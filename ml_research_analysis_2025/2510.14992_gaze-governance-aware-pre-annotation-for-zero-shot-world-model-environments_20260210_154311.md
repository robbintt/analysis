---
ver: rpa2
title: GAZE:Governance-Aware pre-annotation for Zero-shot World Model Environments
arxiv_id: '2510.14992'
source_url: https://arxiv.org/abs/2510.14992
tags:
- video
- audio
- detection
- nsfw
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "GAZE automates long-form video pre-annotation to accelerate world-model\
  \ training by integrating multimodal AI models\u2014scene understanding, object\
  \ detection and tracking, audio transcription with PII detection, face/age estimation,\
  \ and NSFW screening\u2014into a unified pipeline. Proprietary formats are normalized,\
  \ video is segmented into clips, and multiple AI modules generate dense, structured\
  \ evidence."
---

# GAZE:Governance-Aware pre-annotation for Zero-shot World Model Environments

## Quick Facts
- arXiv ID: 2510.14992
- Source URL: https://arxiv.org/abs/2510.14992
- Authors: Leela Krishna; Mengyang Zhao; Saicharithreddy Pasula; Harshit Rajgarhia; Abhishek Mukherji
- Reference count: 6
- Primary result: Automates long-form video pre-annotation to accelerate world-model training with >80% reduction in human review volume

## Executive Summary
GAZE is a governance-aware pre-annotation pipeline that automates long-form video processing to accelerate world-model training. The system integrates multimodal AI models—scene understanding, object detection and tracking, audio transcription with PII detection, face/age estimation, and NSFW screening—into a unified pipeline. By normalizing proprietary video formats, segmenting content into clips, and generating dense structured evidence, GAZE enables review-by-exception that reduces human review volume by over 80% while maintaining governance compliance. The approach produces high-fidelity, privacy-aware datasets directly consumable for training cross-modal dynamics and action-conditioned prediction in world models.

## Method Summary
The pipeline ingests raw video (1-8h activity footage), normalizes proprietary 360° dual-fisheye formats to MP4/H.264, and renders them into four rectilinear views to reduce peripheral distortion. Video is segmented into 60-second overlapping clips with 2-3 second hops for parallel inference. Multiple AI modules run in parallel: Cosmos-Reason1-7B for scene captioning, YOLO+BoT-SORT for detection and tracking, Faster-Whisper+Presidio for ASR and PII detection, DeepFace for age estimation, and NSFW classifiers. The fusion layer consolidates heterogeneous outputs into a prioritized timeline with governance flags, enabling reviewers to validate only flagged segments rather than exhaustive watching.

## Key Results
- Reduces human review volume by over 80% through conservative auto-skipping of low-salience segments
- Saves approximately 19 minutes per review hour on activity videos
- Improves label density and consistency while embedding privacy safeguards and auditability

## Why This Works (Mechanism)

### Mechanism 1: Multi-View Rectilinear Rendering for 360° Content
Rendering dual-fisheye captures into four rectilinear views (~90° FOV each) improves downstream detection and tracking quality compared to processing equirectangular projection directly. This approach reduces peripheral distortion and perspective stretch that degrade detector performance, stabilizes bounding boxes across yaw transitions, and extends track continuity. The core assumption is that detection and tracking models trained on standard perspective imagery generalize poorly to highly distorted panoramic projections.

### Mechanism 2: Conservative Auto-Skip with Review-by-Exception
Pre-annotating low-salience segments as "auto-skip" and surfacing only flagged governance items reduces human review volume by >80% without sacrificing recall on critical classes. The fusion layer consolidates heterogeneous detector outputs into prioritized timeline items, allowing reviewers to validate flagged segments rather than exhaustively watching. This assumes idle/empty spans and low-salience content can be reliably identified via frame-difference energy, black-frame ratio, and audio loudness descriptors.

### Mechanism 3: Overlapping Clip Segmentation with Parallel Inference
Segmenting long-form video into 60-second overlapping clips (2-3s hop) enables embarrassingly parallel multi-task inference while preserving temporal context for cross-clip tracking. Short clips bound per-clip latency, overlap ensures tracks and events spanning boundaries remain coherent, and parallel processing saturates multi-GPU nodes. The core assumption is that 60-second windows capture sufficient activity context while overlap handles boundary cases without excessive recomputation.

## Foundational Learning

- **Equirectangular to Rectilinear Projection**
  - Why needed here: Understanding how dual-fisheye → ERP → multi-view conversion works is essential for debugging detection failures at yaw transitions.
  - Quick check question: Given a 360° dual-fisheye frame, how would you extract a 90° FOV rectilinear view centered at yaw=180°?

- **Multi-Object Tracking with IoU-Guided Association**
  - Why needed here: Person tracks feed governance (minor detection, dwell time) and world-model training; tracking quality directly affects downstream utility.
  - Quick check question: If a person exits and re-enters frame within δ frames, should they receive the same track ID? What tradeoff does this represent?

- **Audio-Visual Temporal Alignment**
  - Why needed here: PII spans from audio must align with visual cues (e.g., speaker's face) for effective redaction and evidence linking.
  - Quick check question: How would you synchronize a PII detection at audio timestamp 01:23.5–01:27.2 with the corresponding video frames showing the speaker?

## Architecture Onboarding

- **Component map:**
  Ingest: TLS upload → content hash → format normalization (MP4/H.264, AAC) → session.json manifest
  Preprocessing: Dual-fisheye → ERP dewarp → 4 rectilinear views → 60s overlapping clips + descriptors (black_ratio, loudness, frame_diff)
  Multi-task inference: Scene captioning (Cosmos-Reason1), Detection+Tracking (YOLO+BoT-SORT), Audio (pyannote+Whisper+Presidio), Face/Age (DeepFace), NSFW (ONNX), Motion (frame differencing)
  Fusion: Consolidate → prioritized timeline with governance flags + evidence links
  Review UI: Flag-centric navigation → accept/adjust/override → audit log
  Export: Governance-filtered video + final_labels.jsonl + redaction rendering

- **Critical path:** Upload → normalization → multi-view render → clip segmentation → parallel inference → fusion → human review → export with redactions

- **Design tradeoffs:**
  Clip length (60s) bounds latency but may fragment long activities; overlap mitigates but adds compute
  Conservative thresholds on PII/NSFW increase human review load but reduce false-negative risk
  Multi-view (4x) improves detection quality at cost of 4x inference compute on video streams

- **Failure signatures:**
  Age estimation oscillation: aggregate per-track with conservative minimum (not mean)
  Track fragmentation at yaw boundaries: mitigate with multi-view; expect higher IDF1 on rectilinear vs. ERP
  Negative RTR on short clips: model overhead dominates; threshold tuning or batch processing required
  PII false positives from common words: calibrate NER confidence; add domain-specific allowlists

- **First 3 experiments:**
  1. Baseline RTR measurement: Process 10 diverse 1-hour videos, measure RTR vs. manual watch-all, compare indoor retail vs. outdoor campus domains.
  2. Multi-view ablation: Run detection+tracking on ERP-only vs. 4-view rectilinear; measure IDF1, ID switches, false positives near fisheye seams.
  3. FP burden sensitivity: Vary detection thresholds systematically; plot RTR vs. FP minutes/hour to identify optimal operating point per governance class (PII, NSFW, minor-risk).

## Open Questions the Paper Calls Out

### Open Question 1
Can a learned fusion policy, such as a lightweight transformer trained on human adjudication data, improve the detection of complex, cross-modal events compared to the current heuristic-based approach?
- Basis in paper: The authors state that their "heuristic-based temporal fusion... may miss complex, cross-modal events" and identify replacing this with a learned policy as a "promising direction."
- Why unresolved: The current system relies on hand-crafted rules to consolidate signals, which limits the pipeline's ability to generalize to novel or subtle interactions between modalities.
- What evidence would resolve it: Comparative benchmarks on a held-out set of complex, multi-modal events showing improved recall/precision of the transformer model over the heuristic baseline.

### Open Question 2
To what extent does pushing initial processing modules (e.g., motion detection, face blurring) to edge devices reduce latency and bandwidth usage while maintaining governance accuracy?
- Basis in paper: The authors identify the "cloud-centric architecture" as a limitation introducing latency constraints and propose developing a "distributed version" for edge devices.
- Why unresolved: The current cloud-dependency creates a bottleneck for real-time world-model applications; the feasibility of offloading heavy governance tasks to weaker edge hardware remains untested.
- What evidence would resolve it: Latency and bandwidth metrics from a prototype edge deployment, alongside accuracy comparisons for edge-processed vs. cloud-processed pre-annotations.

### Open Question 3
Can efficient foundational models (e.g., small language models) and quantization techniques maintain sufficient accuracy for governance tasks while eliminating the prohibitive computational costs of large models?
- Basis in paper: The authors note that "computational cost driven by large models like Cosmos and Whisper prohibits real-time processing" and list exploring efficient models as a priority.
- Why unresolved: It is unproven whether compressed architectures can match the performance of large vision-language models (like Cosmos-Reason1) in detecting nuanced PII or generating reliable scene descriptions.
- What evidence would resolve it: Ablation studies comparing the inference speed versus the governance recall/precision of quantized models against the current full-precision baseline.

## Limitations

- The paper lacks detailed specification of fusion logic and confidence thresholds used to merge multi-modal signals and determine auto-skip segments, making reproduction of the claimed >80% review volume reduction uncertain.

- While the approach shows promising efficiency gains on long-form activity videos (1-8h), performance on shorter clips (<5 minutes) degrades significantly due to model initialization overhead, requiring a minimum runtime threshold for positive ROI.

- The multi-view rectilinear rendering pipeline assumes dual-fisheye source formats; effectiveness on natively rectilinear or narrow-FOV content is not demonstrated, and preprocessing overhead may provide negligible benefit in those cases.

## Confidence

- **High confidence**: Core mechanism of multi-modal pre-annotation pipeline (multi-view rendering + parallel inference + review-by-exception) and its ability to generate structured governance-aware timelines.
- **Medium confidence**: The claimed RTR of 0.28-0.31 and ~19 minutes saved per review hour, as these depend heavily on unspecified fusion thresholds and conservative auto-skip logic.
- **Low confidence**: Generalization to video domains outside the reported activity learning and training environments, given the specific focus on governance requirements and absence of cross-domain validation.

## Next Checks

1. **Fusion threshold ablation study**: Systematically vary confidence thresholds for PII, NSFW, and minor-risk detection, and measure the resulting RTR and false-positive burden on a diverse video sample set to identify the optimal operating point.

2. **Multi-view vs. ERP-only detection quality**: Process a subset of 360° videos through both the 4-view rectilinear pipeline and a baseline ERP-only approach, and compare IDF1 tracking scores and governance flag recall to quantify the quality improvement from dewarping.

3. **Minimum viable runtime threshold**: Test the pipeline on progressively shorter video clips (5-15 minutes) to empirically determine the minimum duration where the RTR becomes positive, and assess whether clip batching or threshold adjustment can extend the effective range.