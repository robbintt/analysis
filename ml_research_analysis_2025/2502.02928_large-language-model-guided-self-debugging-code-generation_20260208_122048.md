---
ver: rpa2
title: Large Language Model Guided Self-Debugging Code Generation
arxiv_id: '2502.02928'
source_url: https://arxiv.org/abs/2502.02928
tags:
- code
- generation
- pycapsule
- agent
- debugging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents PyCapsule, a Python code generation framework
  that uses a two-agent architecture to improve computational efficiency and robustness.
  The framework employs a programmer agent for code generation and debugging, and
  an executor agent for code validation, case testing, and error analysis.
---

# Large Language Model Guided Self-Debugging Code Generation

## Quick Facts
- arXiv ID: 2502.02928
- Source URL: https://arxiv.org/abs/2502.02928
- Authors: Muntasir Adnan; Zhiwei Xu; Carlos C. N. Kuhn
- Reference count: 40
- Primary result: Up to 5.7% improvement on HumanEval, 10.3% on HumanEval-ET, 24.4% on BigCodeBench over state-of-the-art methods

## Executive Summary
This paper introduces PyCapsule, a two-agent Python code generation framework that improves computational efficiency and robustness through specialized deterministic modules. The system combines a programmer agent for code generation and debugging with an executor agent for validation and error analysis, achieving significant performance gains on multiple benchmarks. The framework demonstrates that strategic offloading of non-reasoning tasks to deterministic modules can enhance both efficiency and stability compared to pure LLM-based multi-agent approaches.

## Method Summary
PyCapsule implements a two-agent architecture where a programmer agent handles code generation and debugging while an executor agent validates code and provides feedback. Three specialized modules—signature converter, example call detector, and error handler—perform deterministic preprocessing and postprocessing tasks to improve efficiency and stability. The system uses a Markov Decision Process approach that maintains only the most recent problem-solution-error state, preventing context dilution. Code generation occurs in up to five self-debugging attempts, with each iteration involving code generation, execution, and targeted refinement based on filtered error feedback.

## Key Results
- Achieves up to 5.7% improvement in success rate on HumanEval benchmark
- Demonstrates 10.3% improvement on HumanEval-ET subset
- Shows 24.4% improvement on BigCodeBench compared to state-of-the-art methods
- Normalized debugging success rate decreases with more attempts due to limited and noisy error feedback

## Why This Works (Mechanism)

### Mechanism 1: Deterministic Module Offloading
Shifting non-reasoning tasks from LLM agents to deterministic scripts improves computational efficiency and stability. The system replaces general-purpose LLM calls with specialized modules (Signature Converter, Example Call Detector) that handle tasks like function signature inference and safety checks algorithmically, reducing coordination overhead and token consumption.

### Mechanism 2: State-Truncated Iterative Refinement (MDP)
Limiting conversation history to the single most recent problem-solution-error tuple prevents context dilution and improves debugging success rates. The Markov Decision Process approach assumes optimal next code patches depend only on the current state rather than full history, mitigating attention issues in long contexts.

### Mechanism 3: Error Feedback Compression
Raw runtime errors are too noisy for effective LLM consumption; refining them into natural language summaries increases debugging loop utility. The Error Handler module filters stack traces to focus on relevant information, helping the LLM concentrate on logic errors rather than environmental noise.

## Foundational Learning

- Concept: **Markov Decision Process (MDP)**
  - Why needed here: To understand why the system discards conversation history after each turn, assuming optimal next code patches depend only on current state
  - Quick check question: If a bug was introduced three turns ago but isn't visible in the most recent error message, will this architecture likely fix it?

- Concept: **Context Dilution / Attention Drain**
  - Why needed here: Explains why the two-agent, limited-history design was chosen due to performance degradation in long contexts
  - Quick check question: Why does PyCapsule retain only the most recent solution pair instead of the last three?

- Concept: **Sandboxed Execution (Docker)**
  - Why needed here: The Executor Agent relies on containers to run untrusted code safely
  - Quick check question: What specific risks does the Example Call Detector mitigate that the Docker container cannot?

## Architecture Onboarding

- Component map: Input -> Signature Converter -> Programmer Agent (Gen Mode) -> Example Call Detector -> Executor Agent -> (If Fail) -> Error Handler -> Programmer Agent (Fix Mode)
- Critical path: The pipeline flows from problem input through deterministic preprocessing, LLM-based generation, safety filtering, execution validation, and iterative refinement based on error feedback
- Design tradeoffs: The system trades deep historical context for efficiency and attention stability, using only 2 agents versus 4+ in competitors, lowering cost but potentially limiting performance on complex planning tasks
- Failure signatures: Infinite loops from recursive calls without base cases, error amnesia from truncated messages, early stopping after 5 attempts even if solution is close
- First 3 experiments:
  1. History ablation: Run MBPP with 1 vs. 2 vs. 3 retained conversation pairs to validate MDP assumption
  2. Debugging limit validation: Plot success rate vs. attempt number on HumanEval to verify exponential decay
  3. Module ablation: Disable Error Handler to compare raw vs. refined error feedback effectiveness

## Open Questions the Paper Calls Out

1. Can advanced semantic error analysis or causal reasoning techniques mitigate the "noisy error feedback" issue to sustain debugging effectiveness beyond observed exponential decay?
2. Does integrating a dynamic or summarized conversation history mechanism improve success rates for complex problems compared to the current single-pair approach?
3. Can the specialized modules (signature converter, example call detector) generalize effectively to statically typed or compiled languages like Java or C++?

## Limitations

- The paper does not disclose exact hyperparameter configurations or baseline versions used, limiting reproducibility
- The "exponential decay" claim in debugging success is inferred from aggregated curves but not statistically validated per problem category
- The MDP-based state truncation may artificially cap performance on complex multi-step bugs requiring historical context
- The three deterministic modules are not rigorously tested in isolation to determine individual contributions

## Confidence

- **High** for computational efficiency gains (reduces agent count, token usage)
- **Medium** for debugging success rates and diminishing returns claim
- **Low** for claims about module necessity and MDP optimality

## Next Checks

1. **Module Ablation Study:** Disable each of the three deterministic modules individually and rerun HumanEval to isolate their marginal contributions to success rate and token efficiency
2. **Extended History Test:** Modify the MDP truncation to retain the last 3 conversation pairs instead of 1, then measure performance on MBPP to test the "lost in the middle" hypothesis
3. **Robustness to Malformed Inputs:** Construct test cases with missing signatures or ambiguous test cases to evaluate how often the pipeline fails at the Signature Converter stage