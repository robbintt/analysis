---
ver: rpa2
title: Neural Message-Passing on Attention Graphs for Hallucination Detection
arxiv_id: '2509.24770'
source_url: https://arxiv.org/abs/2509.24770
tags:
- attention
- computational
- these
- graph
- charm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CHARM, a neural message-passing framework
  for hallucination detection in large language models (LLMs). CHARM represents LLM
  computational traces as attributed graphs, where tokens are nodes connected by attention-induced
  edges, enriched with features from attention scores and activations.
---

# Neural Message-Passing on Attention Graphs for Hallucination Detection

## Quick Facts
- arXiv ID: 2509.24770
- Source URL: https://arxiv.org/abs/2509.24770
- Authors: Fabrizio Frasca; Guy Bar-Shalom; Yftah Ziser; Haggai Maron
- Reference count: 40
- Primary result: CHARM achieves strong results in token-level and response-level hallucination detection, handles different types of hallucinations, and shows promising zero-shot cross-dataset transfer capabilities

## Executive Summary
This paper introduces CHARM, a neural message-passing framework for hallucination detection in large language models (LLMs). The method represents LLM computational traces as attributed graphs, where tokens are nodes connected by attention-induced edges, enriched with features from attention scores and activations. By casting hallucination detection as a graph learning task and using Graph Neural Networks (GNNs) to process these graphs, CHARM provably subsumes prior attention-based heuristics and consistently outperforms existing methods across diverse benchmarks. The approach demonstrates strong performance in both token-level and response-level hallucination detection while showing promising zero-shot cross-dataset transfer capabilities.

## Method Summary
CHARM operates by first constructing attributed graphs from LLM computational traces. Each token in the input sequence becomes a node, and edges are created based on attention weights between tokens. These graphs are enriched with node features derived from attention scores and activation values. The method then applies a Graph Neural Network to learn representations that can distinguish between factual and hallucinated content. This graph-based approach allows CHARM to capture complex dependencies and relationships in the LLM's reasoning process that simpler attention-based heuristics might miss.

## Key Results
- CHARM consistently outperforms existing methods across diverse hallucination detection benchmarks
- Achieves strong performance in both token-level and response-level hallucination detection tasks
- Demonstrates promising zero-shot cross-dataset transfer capabilities
- Shows robustness to graph sparsification, allowing efficiency trade-offs without significant accuracy loss

## Why This Works (Mechanism)
CHARM works by leveraging the structural information captured in attention graphs to detect inconsistencies and hallucinations. The GNN architecture enables message passing between nodes, allowing the model to aggregate information from the entire computational trace. This approach captures both local patterns (through attention weights) and global dependencies (through the graph structure), providing a more comprehensive view of the LLM's reasoning process than previous methods that rely solely on attention scores or isolated activations.

## Foundational Learning
- **Graph Neural Networks (GNNs)**: Why needed - to process structured graph representations of LLM traces; Quick check - verify GNN can propagate information across graph edges
- **Attention Mechanisms**: Why needed - to understand how LLMs focus on different input parts; Quick check - confirm attention weights form meaningful edges in the graph
- **Attributed Graphs**: Why needed - to enrich graph structure with additional features beyond topology; Quick check - ensure node features capture relevant activation information
- **Message Passing**: Why needed - to enable information flow between connected nodes; Quick check - verify messages contain relevant local and global context
- **Hallucination Detection**: Why needed - to identify factual vs. fabricated content in LLM outputs; Quick check - establish ground truth labels for training and evaluation
- **Zero-shot Transfer**: Why needed - to evaluate model generalization across different datasets; Quick check - test on held-out datasets with different distributions

## Architecture Onboarding

**Component Map**: Input traces -> Graph Construction -> GNN layers -> Classification

**Critical Path**: The most critical components are the graph construction phase (which must accurately capture attention relationships) and the GNN architecture (which must effectively learn from the graph structure). The classification layer's performance depends entirely on the quality of representations learned by the GNN.

**Design Tradeoffs**: The method balances expressiveness (using rich graph features) against efficiency (through sparsification). More complex graphs with additional features may improve accuracy but increase computational cost. The framework allows trading off between these factors by adjusting graph density.

**Failure Signatures**: Poor performance may arise from: 1) Inadequate graph construction that misses important attention relationships, 2) GNN architecture that fails to learn meaningful representations from the graph structure, or 3) Insufficient or biased training data that doesn't capture the full range of hallucination types.

**3 First Experiments**:
1. Test graph construction with varying levels of sparsification to identify the optimal trade-off between efficiency and accuracy
2. Perform ablation studies removing specific node features to quantify their individual contributions
3. Evaluate zero-shot transfer performance on datasets with known distribution shifts

## Open Questions the Paper Calls Out
None

## Limitations
- Specific datasets and their characteristics are not fully described, limiting assessment of generalizability
- The trade-off curves between efficiency and accuracy under graph sparsification are not provided
- Claims about handling "different types of hallucinations" lack specificity about which types were tested
- Zero-shot cross-dataset transfer capabilities are mentioned as "promising" but quantitative results are not provided

## Confidence

**High confidence**: The core methodology of representing LLM computational traces as attributed graphs and using GNNs for hallucination detection is technically sound

**Medium confidence**: Claims about subsuming prior attention-based heuristics and outperforming existing methods, pending full experimental details

**Medium confidence**: Zero-shot transfer capabilities, as only "promising" results are mentioned without quantitative backing

## Next Checks

1. Conduct ablation studies removing specific graph features (attention scores, activations) to quantify their individual contributions to performance

2. Test CHARM on datasets with known distribution shifts to rigorously evaluate zero-shot cross-dataset transfer claims

3. Perform detailed efficiency analysis by systematically varying graph sparsification levels and measuring both computational cost and detection accuracy trade-offs