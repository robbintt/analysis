---
ver: rpa2
title: 'Tonguescape: Exploring Language Models Understanding of Vowel Articulation'
arxiv_id: '2501.17643'
source_url: https://arxiv.org/abs/2501.17643
tags:
- vowel
- tongue
- vowels
- positions
- position
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether large language models (LLMs) can
  understand vowel articulation based on tongue positions observed in real-time MRI
  videos and images. The researchers created a dataset from MRI recordings of Japanese
  vowel articulation and tested various vision and multimodal LLMs on tasks predicting
  vowels from videos, static images, and images with visual guides.
---

# Tonguescape: Exploring Language Models Understanding of Vowel Articulation

## Quick Facts
- arXiv ID: 2501.17643
- Source URL: https://arxiv.org/abs/2501.17643
- Reference count: 31
- Key outcome: Large language models struggle to predict vowels from tongue articulation videos but show improvement with few-shot examples, indicating partial ability to associate tongue positions with vowel articulation when provided reference examples.

## Executive Summary
This study investigates whether large language models can understand vowel articulation by analyzing tongue positions from real-time MRI videos and images. Researchers created a dataset from Japanese vowel articulation recordings and tested various vision and multimodal LLMs on tasks predicting vowels from videos, static images, and images with visual guides. Results show models struggle with zero-shot prediction but some models improve with few-shot examples, suggesting they can use relative tongue position information rather than absolute positioning. Fine-tuned models showed limited improvement due to small training data, and overall models demonstrate partial ability to associate tongue positions with vowel articulation when provided reference examples.

## Method Summary
The researchers created three datasets from rtMRIDB: VowelVideo (1,773 videos), VowelImage (120 images), and VowelImageWithGuide (120 images with ellipse guides). They tested multimodal LLMs (GPT-4o, Gemini 1.5 Pro, Qwen2-VL-7B/72B-Instruct, LLaVA-NeXT-Interleave, Phi-3.5-vision-instruct, VideoLLaMA2, LLaVA-Med, Qwen-VL-Chat) using zero-shot, one-shot, and five-shot prompting strategies. Fine-tuning was attempted with LoRA on VideoLLaMA2 and Qwen-VL-Chat using a 5-sample training set. Evaluation used classification accuracy across five Japanese vowels with random baseline at 20%.

## Key Results
- Zero-shot accuracy remained at chance level (~20%) for most models
- GPT-4o and Qwen2-VL-72B-Instruct showed improvement with few-shot examples, achieving 35-40% accuracy
- Fine-tuned models showed limited improvement due to extremely small training set (5 samples)
- Models generally performed worse at backness discrimination than height discrimination

## Why This Works (Mechanism)

### Mechanism 1: Relative Position Reasoning via Few-Shot Examples
- Claim: When provided reference images of known vowel-tongue pairings, some multimodal LLMs can infer unknown vowels by comparing relative tongue positions rather than relying on absolute positional understanding.
- Mechanism: Few-shot examples establish a coordinate system—models compare test images against provided exemplars (e.g., "this tongue is higher than /a/ but lower than /i/") rather than directly mapping absolute positions to phonetic categories.
- Core assumption: Models possess latent visual comparison capabilities that can be activated through in-context demonstration, even without explicit positional encoding.
- Evidence anchors:
  - [abstract] "some models, particularly GPT-4o and Qwen2-VL-72B-Instruct, show improvement with few-shot examples, indicating they can use relative tongue position information"
  - [section 4.2] "We can evaluate the capability of recognizing relative positions by using few-shot examples... If there are more than five images that contain each of the five vowels, they can determine which vowel the given image is closest to"
- Break condition: When exemplar set does not span the full articulatory space (e.g., one-shot), models fail—GPT-4o defaulted to /a/ or /i/ rather than performing true relative comparisons.

### Mechanism 2: Textual-Visual Alignment Asymmetry
- Claim: Multimodal LLMs encode strong textual knowledge of vowel articulation mechanisms but exhibit weak grounding between visual tongue representations and their textual phonetic concepts.
- Mechanism: Training data includes extensive linguistic and medical text describing vowel-tongue relationships, enabling accurate textual explanations. However, visual pretraining lacks sufficient articulatory imagery to form robust cross-modal bindings between MRI patterns and phonetic categories.
- Core assumption: The alignment bottleneck occurs at the vision-language interface, not within individual modalities.
- Evidence anchors:
  - [abstract] "Models generally perform poorly at zero-shot prediction, suggesting limited ability to directly map visual tongue positions to vowels without training examples"
  - [section 2.4/Table 1] GPT-4o correctly explains /i/ articulation: "Position your tongue high in your mouth... Move your tongue towards the front"
  - [section 5.2] "most models in the zero-shot setting perform worse than CLIP... The underperformance of LMs compared to CLIP in the zero-shot setting suggests that LMs have difficulty predicting vowels based on absolute positions"
- Break condition: Tasks requiring fine-grained spatial reasoning (tongue height and backness simultaneously) exceed current alignment capacity—even improved models achieve only ~47% backness accuracy.

### Mechanism 3: Guide Marker Salience (Conditional Enhancement)
- Claim: Visual guides (elliptical markers highlighting oral cavity regions) improve performance only for models already capable of leveraging spatial attention cues; for others, guides introduce noise.
- Mechanism: Guides direct model attention to task-relevant regions, reducing visual search complexity. However, if the model lacks the spatial reasoning capacity to interpret guided regions, the markers become distractors.
- Core assumption: Guide effectiveness depends on pre-existing attention and spatial reasoning capabilities.
- Evidence anchors:
  - [section 3.2] "we added guide markers to all images... to facilitate the identification of tongue position within the oral cavity"
  - [section 5.1] "the guideline results in poor performance in both zero-shot and one-shot settings when using GPT-4o, VideoLLaMA2, LLaV A-Med, and Qwen2-VL-7B-Instruct while the performances improve or remain unchanged when using the other models"
- Break condition: Models without robust spatial attention mechanisms will show degraded performance with guides—test with smaller models first.

## Foundational Learning

- Concept: **Tongue height and backness as phonetic features**
  - Why needed here: These two parameters define the vowel space; all predictions hinge on correctly extracting these from images.
  - Quick check question: Given an MRI showing tongue positioned mid-height and central-front, which Japanese vowel is being articulated?

- Concept: **Cardinal vowel system and relative positioning**
  - Why needed here: The paper's few-shot approach mirrors Jones's (1917) method of establishing reference vowels for comparison.
  - Quick check question: Why might a one-shot example fail to establish an effective reference frame for five-way classification?

- Concept: **Zero-shot vs. few-shot multimodal prompting**
  - Why needed here: The performance differential (20% → 40%) between these settings is the central empirical finding.
  - Quick check question: What does it imply when a model improves with few-shot but remains below human performance (62-72%)?

## Architecture Onboarding

- Component map: rtMRIDB -> frame extraction -> manual annotation -> model prompting -> accuracy aggregation
- Critical path:
  1. Frame selection quality directly determines task difficulty—poorly chosen frames (non-articulatory moments) introduce noise
  2. Prompt design (explicit instruction to read height/backness) gates whether models attempt spatial reasoning
  3. Few-shot example selection (which speaker's vowels to use) affects transfer due to physiological variation
- Design tradeoffs:
  - Video vs. image input: Video provides temporal context but requires more compute; some models (VideoLLaMA2) limited to 16 frames
  - Guide markers: Can help or hurt depending on model; requires per-model validation
  - Training data size: Fine-tuning failed due to small dataset (5 training samples); larger data collection needed but MRI data is scarce
- Failure signatures:
  - **Constant output**: Model outputs same vowel regardless of input (LLaVA-NeXT-Interleave outputs all /o/)
  - **Refusal patterns**: Medical-trained models (LLaVA-Med) reject non-clinical queries
  - **Mid-vowel collapse**: /e/ and /o/ consistently misclassified as /a/—suggests binary high/non-high distinction only
- First 3 experiments:
  1. Establish baseline with CLIP on VowelImage to confirm task solvability (expected: 20-30% accuracy)
  2. Run GPT-4o zero-shot vs. five-shot on VowelImageWithGuide to test guide utility for high-capability model
  3. Analyze confusion matrices for mid-vowel (/e/, /o/) performance to diagnose whether models learn height but not backness

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can language models generalize their understanding of tongue position and vowel articulation to languages with complex vowel inventories beyond the five-vowel system?
- **Basis in paper:** [explicit] The authors acknowledge they used the Japanese five-vowel system because it is "relatively easy to predict" and note that "there are many languages that have more or fewer than five vowel phonemes." They explicitly state, "We intend to address languages with more complex vowel systems once the current challenges have been resolved."
- **Why unresolved:** The current study only validates performance on a relatively coarse-grained phonetic task (5 distinct categories). It remains unknown if LLMs can discern subtle articulatory differences required for languages with dense vowel spaces or phonemic contrasts based on slight tongue movements.
- **What evidence would resolve it:** Testing the models on MRI datasets of languages with larger vowel inventories (e.g., German, Thai) or different phonological features to see if accuracy degrades or holds.

### Open Question 2
- **Question:** Can increasing the scale of training data or using advanced prompting strategies (e.g., Chain-of-Thought) improve the "absolute position" understanding of models in zero-shot settings?
- **Basis in paper:** [explicit] The authors note that fine-tuning failed likely due to "small training data" and that exploring strategies like "chain-of-thought... lies outside the boundaries of our study." They conclude models currently "have difficulties" without few-shot examples.
- **Why unresolved:** The study established that models rely on relative positioning (few-shot) rather than absolute positioning (zero-shot). It is undetermined if this is a fundamental lack of visual grounding capability or simply a data/prompting deficiency.
- **What evidence would resolve it:** Experiments utilizing large-scale MRI datasets for fine-tuning or employing reasoning-based prompts to test if zero-shot absolute prediction accuracy can exceed the current ~20% baseline.

### Open Question 3
- **Question:** What underlying architectural factors cause visual guides (markers) to improve performance in some vision-language models (VLMs) while acting as noise for others?
- **Basis in paper:** [explicit] The authors observe in Section 5.1 that "the guideline is probably noise for some models and is a helpful guide for others" (specifically degrading performance in Phi-3.5 but improving it in GPT-4o).
- **Why unresolved:** The paper documents the inconsistent effect of visual scaffolding but does not investigate the attention mechanisms or visual encoders responsible for this divergence.
- **What evidence would resolve it:** An ablation study analyzing cross-attention maps in the respective models to determine if the "guide" markers are being focused on as relevant features or treated as visual distractions.

## Limitations
- Dataset size extremely limited for fine-tuning (only 5 training samples)
- rtMRIDB dataset access requires research license from NIJL, limiting reproducibility
- Manual selection of characteristic frames introduces potential bias
- Study focuses exclusively on Japanese vowels, limiting generalizability

## Confidence
- **High Confidence**: Zero-shot performance at chance levels (~20%) is well-supported; few-shot improvement for certain models is strongly supported
- **Medium Confidence**: Relative vs. absolute positioning inference is supported by patterns but not directly verified; guide marker effects are partially understood
- **Low Confidence**: Fine-tuning results are difficult to interpret due to extremely small training set

## Next Checks
1. **Frame Selection Validation**: Replicate study using frames selected by different annotators or automated criteria to test sensitivity to frame choice
2. **Cross-Modal Ablation**: Test whether models can achieve similar performance using only height or only backness information by creating datasets with reduced spatial complexity
3. **Training Data Scaling**: Conduct fine-tuning experiments with incrementally larger training sets (10, 20, 50 samples) to determine minimum dataset size for meaningful improvement