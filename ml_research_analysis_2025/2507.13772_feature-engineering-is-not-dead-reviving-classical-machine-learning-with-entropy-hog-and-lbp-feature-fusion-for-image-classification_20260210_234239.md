---
ver: rpa2
title: 'Feature Engineering is Not Dead: Reviving Classical Machine Learning with
  Entropy, HOG, and LBP Feature Fusion for Image Classification'
arxiv_id: '2507.13772'
source_url: https://arxiv.org/abs/2507.13772
tags:
- feature
- image
- features
- entropy
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a hybrid feature engineering approach combining
  permutation entropy, HOG, and LBP descriptors for classical machine learning-based
  image classification. The method computes permutation entropy across rows, columns,
  diagonals, anti-diagonals, and patches, then fuses these with shape and texture
  descriptors HOG and LBP into a 780-dimensional feature vector.
---

# Feature Engineering is Not Dead: Reviving Classical Machine Learning with Entropy, HOG, and LBP Feature Fusion for Image Classification

## Quick Facts
- **arXiv ID:** 2507.13772
- **Source URL:** https://arxiv.org/abs/2507.13772
- **Reference count:** 40
- **Primary result:** Achieves 91.23% accuracy on Fashion-MNIST with SVM, outperforming all previously reported classical SVM results without deep learning

## Executive Summary
This paper proposes a hybrid feature engineering approach that combines permutation entropy, Histogram of Oriented Gradients (HOG), and Local Binary Patterns (LBP) descriptors for classical machine learning-based image classification. The method computes permutation entropy across multiple dimensions (rows, columns, diagonals, anti-diagonals, and patches) and fuses these with shape and texture descriptors into a 780-dimensional feature vector. Tested on benchmark datasets including Fashion-MNIST, KMNIST, EMNIST, and CIFAR-10 using SVM classifiers, the approach achieves 91.23% accuracy on Fashion-MNIST—outperforming all previously reported classical SVM results without deep learning. The method demonstrates that interpretable, handcrafted features can deliver competitive performance while remaining computationally efficient and transparent compared to deep learning models.

## Method Summary
The proposed method employs a multi-faceted feature extraction strategy. Permutation entropy is computed across rows, columns, diagonals, anti-diagonals, and patches of input images, capturing temporal complexity and order patterns. These entropy features are then combined with HOG descriptors, which capture edge orientation and shape information, and LBP descriptors, which encode texture patterns. The resulting 780-dimensional feature vector is fed into an SVM classifier with RBF kernel for image classification tasks. The approach leverages the complementary strengths of statistical (entropy), structural (HOG), and textural (LBP) features to create a rich representation suitable for classical ML models.

## Key Results
- Achieves 91.23% accuracy on Fashion-MNIST dataset using SVM classifier
- Outperforms all previously reported classical SVM results on Fashion-MNIST without deep learning
- Demonstrates competitive performance across multiple benchmark datasets (KMNIST, EMNIST, CIFAR-10)
- Validates the effectiveness of handcrafted feature fusion for image classification

## Why This Works (Mechanism)
The success of this approach stems from the complementary nature of the three feature types. Permutation entropy captures the complexity and ordering of pixel intensities across different orientations and patches, providing a statistical measure of image structure. HOG extracts edge orientation histograms, encoding shape and gradient information that is crucial for object recognition. LBP captures local texture patterns through binary comparisons of neighboring pixels. By fusing these distinct feature types, the method creates a comprehensive representation that leverages statistical, structural, and textural information simultaneously. This multi-dimensional feature space enables classical ML models like SVM to achieve performance previously thought to require deep learning architectures.

## Foundational Learning
- **Permutation Entropy:** A complexity measure that quantifies the unpredictability of time series by analyzing the order patterns of values. Needed to capture temporal complexity in images across multiple orientations; quick check: verify entropy values differ significantly between structured and random patterns.
- **Histogram of Oriented Gradients (HOG):** A feature descriptor that counts occurrences of gradient orientation in localized portions of an image. Needed to capture shape and edge information; quick check: confirm HOG captures dominant edge directions in simple geometric shapes.
- **Local Binary Patterns (LBP):** A texture operator that labels pixels by thresholding the neighborhood and considers the result as a binary number. Needed to encode local texture patterns; quick check: verify LBP distinguishes between uniform and non-uniform texture regions.
- **Feature Fusion Techniques:** Methods for combining multiple feature sets into a unified representation. Needed to integrate complementary information from different descriptors; quick check: ensure feature scaling is appropriate before fusion to prevent dominance by high-magnitude features.
- **Support Vector Machines with RBF Kernel:** A classification algorithm that finds optimal decision boundaries in high-dimensional space. Needed as the classification engine for the fused features; quick check: validate that kernel parameters are properly tuned through cross-validation.

## Architecture Onboarding

**Component Map:** Image → Permutation Entropy (rows, cols, diags, anti-diags, patches) → HOG → LBP → Feature Fusion → 780D Vector → SVM(RBF) → Classification

**Critical Path:** The most critical processing sequence is: raw image → multi-orientation permutation entropy computation → HOG and LBP extraction → feature normalization → SVM classification. The permutation entropy computation across five different orientations is particularly crucial as it provides the primary statistical characterization of image complexity.

**Design Tradeoffs:** The approach trades computational simplicity for feature richness, using classical ML rather than deep learning. The 780-dimensional feature space balances comprehensiveness with computational tractability for SVM. The choice of SVM with RBF kernel provides good generalization but requires careful hyperparameter tuning. The method sacrifices some automation in favor of interpretability and transparency.

**Failure Signatures:** Performance degradation may occur with highly complex scenes where handcrafted features cannot capture sufficient discriminative information. Poor results might indicate inadequate feature normalization or suboptimal SVM hyperparameters. Failure to outperform baselines could suggest that certain feature types (e.g., permutation entropy) are not contributing meaningful information for specific dataset characteristics.

**First Experiments:**
1. Compute permutation entropy values for simple synthetic patterns (vertical lines, checkerboards) to verify expected behavior
2. Visualize HOG feature maps on edge-rich images to confirm gradient orientation capture
3. Compare classification performance with individual feature types (only entropy, only HOG, only LBP) to quantify fusion benefits

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation scope is narrow, testing limited to only four datasets despite claims of broader applicability
- Reliance on SVM with RBF kernel as the sole classifier restricts generalizability assessments
- Lack of comprehensive ablation studies to verify individual contributions of permutation entropy, HOG, and LBP components
- Claims about computational efficiency and interpretability advantages over deep learning are not empirically validated

## Confidence
- **High confidence:** Reported benchmark results and comparison to previous classical ML approaches on Fashion-MNIST
- **Medium confidence:** Claims about computational efficiency and interpretability advantages over deep learning
- **Low confidence:** Claims of broader applicability across diverse image domains without supporting evidence

## Next Checks
1. Conduct ablation studies to quantify the individual contribution of permutation entropy, HOG, and LBP features to classification accuracy
2. Test the feature fusion approach across a more diverse set of 10-15 image classification datasets with varying characteristics (medical imaging, satellite imagery, etc.)
3. Evaluate performance with alternative classifiers (Random Forest, Gradient Boosting, k-NN) to assess classifier dependency of the results