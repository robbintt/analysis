---
ver: rpa2
title: Memory Bank Compression for Continual Adaptation of Large Language Models
arxiv_id: '2601.00756'
source_url: https://arxiv.org/abs/2601.00756
tags:
- memory
- bank
- adaptation
- codebook
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of continual adaptation of Large
  Language Models (LLMs) when new data arrives, focusing on the challenge of memory
  bank scalability. As the memory bank grows with incoming documents, storage costs
  and inference latency increase, while maintaining performance becomes difficult.
---

# Memory Bank Compression for Continual Adaptation of Large Language Models

## Quick Facts
- arXiv ID: 2601.00756
- Source URL: https://arxiv.org/abs/2601.00756
- Authors: Thomas Katraouras; Dimitrios Rafailidis
- Reference count: 40
- Key outcome: MBC compresses memory bank to 0.3% size vs. baseline while maintaining high retention accuracy

## Executive Summary
This paper addresses the challenge of continual adaptation of Large Language Models (LLMs) when new data arrives, focusing on memory bank scalability issues. As memory banks grow with incoming documents, storage costs and inference latency increase while maintaining performance becomes difficult. The authors propose MBC (Memory Bank Compression), which compresses the memory bank through a codebook optimization strategy. Instead of storing full document representations, MBC stores indices to a learned codebook. To prevent codebook collapse and ensure balanced utilization, an online resetting mechanism is introduced using exponential moving average (EMA) of code usage. Additionally, Key-Value Low-Rank Adaptation (KV-LoRA) is employed in the attention layers to efficiently utilize the compressed memory representations without full fine-tuning.

Experiments on StreamingQA, SQuAD, and ArchivalQA datasets show that MBC reduces memory bank size to 0.3% compared to the most competitive baseline while maintaining high retention accuracy during online adaptation. Specifically, MBC improves Exact Match (EM) and F1 scores over MAC, achieving average improvements of 11.84% in EM and 12.99% in F1. The memory footprint is reduced by 97.3% on average for the same number of adapted documents. Furthermore, MBC achieves high F1 retention rates, demonstrating reduced catastrophic forgetting during online adaptation.

## Method Summary
The MBC approach addresses memory bank scalability in continual LLM adaptation by introducing a codebook-based compression mechanism. Instead of storing full document representations in the memory bank, MBC learns a compact codebook where each document is represented by an index pointing to its corresponding codebook entry. This dramatically reduces storage requirements while maintaining retrieval capabilities. To prevent codebook collapse (where certain codes are overused while others remain underutilized), the authors implement an online resetting mechanism that uses exponential moving average of code usage to dynamically rebalance the codebook distribution. Additionally, Key-Value Low-Rank Adaptation (KV-LoRA) is integrated into the attention layers, allowing the model to efficiently leverage the compressed memory representations without requiring full fine-tuning. This combination of codebook compression and selective parameter adaptation enables effective continual learning while maintaining memory efficiency.

## Key Results
- MBC achieves 97.3% memory reduction compared to competitive baselines
- Exact Match (EM) scores improved by 11.84% and F1 scores by 12.99% over MAC
- Memory bank compressed to only 0.3% of original size while maintaining high retention accuracy
- Demonstrated high F1 retention rates showing reduced catastrophic forgetting during online adaptation

## Why This Works (Mechanism)
MBC works by fundamentally changing how document representations are stored in the memory bank. Instead of keeping full high-dimensional document embeddings that grow linearly with the number of documents, MBC compresses this information into a fixed-size codebook. Each incoming document is mapped to an index in this codebook, and the model learns to retrieve relevant information using these compact indices. The codebook learning process is optimized to capture the essential semantic content of documents while maintaining distinguishability between different entries. The online resetting mechanism with EMA ensures that no single codebook entry becomes dominant, maintaining diversity and preventing catastrophic forgetting by regularly rebalancing the usage distribution across all codebook entries.

## Foundational Learning
- Codebook Learning: A compression technique that maps high-dimensional vectors to a fixed set of learned codewords. Needed to reduce memory footprint from O(N) to O(1) where N is the number of documents. Quick check: Verify codebook size remains constant regardless of document count.
- Exponential Moving Average (EMA): A technique for calculating weighted averages that gives more importance to recent observations while still considering historical data. Needed to track code usage patterns and detect when rebalancing is required. Quick check: Monitor EMA decay rate impact on codebook utilization balance.
- Key-Value Low-Rank Adaptation (KV-LoRA): A parameter-efficient fine-tuning method that modifies only specific low-rank matrices in attention layers. Needed to adapt to compressed memory representations without full model retraining. Quick check: Measure parameter count reduction compared to full fine-tuning.

## Architecture Onboarding
**Component Map:** Input Documents -> Codebook Encoder -> Codebook Entries -> Memory Bank (indices) -> KV-LoRA Attention -> Model Output

**Critical Path:** Incoming documents are encoded and mapped to codebook indices, which are stored in the compressed memory bank. During inference, the model retrieves relevant codebook entries through KV-LoRA adapted attention mechanisms to generate outputs.

**Design Tradeoffs:** The primary tradeoff is between compression ratio and retrieval accuracy. Smaller codebooks provide greater memory savings but may lose semantic granularity. The online resetting mechanism adds computational overhead during training but prevents catastrophic forgetting and maintains performance over time.

**Failure Signatures:** Codebook collapse (overuse of certain entries), increased retrieval latency due to suboptimal index mapping, degradation in performance when EMA decay rate is too high or too low, and catastrophic forgetting when online resetting is insufficient.

**First Experiments:**
1. Measure memory usage reduction as codebook size varies from 1K to 100K entries
2. Test retrieval accuracy degradation when codebook utilization becomes imbalanced
3. Evaluate catastrophic forgetting rates with and without online resetting mechanism

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation to QA-style tasks, effectiveness on generation tasks remains unexplored
- No analysis of codebook size selection impact across different dataset characteristics
- Lacks granular per-document or per-topic retention analysis for catastrophic forgetting patterns

## Confidence
- High confidence: Memory reduction claims (97.3% reduction, 0.3% size compared to baseline)
- Medium confidence: Performance retention claims (11.84% EM, 12.99% F1 improvements)
- Medium confidence: Catastrophic forgetting reduction claims

## Next Checks
1. Evaluate MBC on non-QA tasks including open-ended generation, summarization, and code completion to assess generalization across task types
2. Conduct ablation studies varying codebook sizes and EMA decay rates across datasets with different knowledge distribution characteristics
3. Perform detailed per-topic retention analysis to identify specific knowledge domains where catastrophic forgetting is most/least effectively mitigated