---
ver: rpa2
title: Revisiting Multivariate Time Series Forecasting with Missing Values
arxiv_id: '2509.23494'
source_url: https://arxiv.org/abs/2509.23494
tags:
- missing
- forecasting
- time
- data
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the limitations of the prevailing imputation-then-prediction
  paradigm for multivariate time series forecasting with missing values. The authors
  show empirically that unsupervised imputation can corrupt data distribution and
  degrade prediction accuracy.
---

# Revisiting Multivariate Time Series Forecasting with Missing Values

## Quick Facts
- **arXiv ID**: 2509.23494
- **Source URL**: https://arxiv.org/abs/2509.23494
- **Reference count**: 40
- **Primary result**: CRIB achieves 18% average improvement over SOTA methods across 11 datasets, especially at high missing rates

## Executive Summary
This paper challenges the prevailing imputation-then-prediction paradigm for multivariate time series forecasting with missing values. The authors demonstrate empirically that unsupervised imputation can corrupt data distribution and degrade prediction accuracy. To address this, they propose CRIB, a direct-prediction method that bypasses imputation entirely. CRIB integrates the Information Bottleneck principle with unified-variate attention and consistency regularization to learn robust representations directly from partially observed data. Experiments on 11 real-world datasets demonstrate that CRIB significantly outperforms state-of-the-art methods by an average of 18% across varying missing rates, particularly under high missing rates.

## Method Summary
CRIB is a direct-prediction method for multivariate time series forecasting with missing values that bypasses the problematic imputation stage. The method processes partially observed data through a patching embedding layer (using TCN with temporal encoding), followed by unified-variate attention that treats all variates and time steps uniformly. An Information Bottleneck layer compresses the representation while maintaining predictive power, and consistency regularization enforces invariance to missingness patterns. The model is trained end-to-end with a composite loss function balancing compression, prediction, and consistency terms, using Adam optimizer with learning rate 10⁻³.

## Key Results
- CRIB outperforms state-of-the-art methods by an average of 18% across varying missing rates
- Performance gains are particularly pronounced at high missing rates (60-70%)
- Ablation studies confirm the importance of each component: removing consistency regularization causes training instability, while improper IB weighting leads to over-regularization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Bypassing explicit imputation prevents the propagation of hallucinated values into the forecasting head
- **Mechanism**: The model operates directly on partially observed data ($X_o$) and uses an Information Bottleneck to learn a representation $Z$ that is maximally informative about the target $Y$ while compressing the input $X_o$. This compression acts as a filter for the noise introduced by missingness
- **Core assumption**: The noise introduced by missing values is largely uninformative for the future target $Y$, and a compressed latent representation can sufficiently capture the signal from observed points without reconstructing the missing ones
- **Evidence anchors**: [abstract], [section 3], [corpus]
- **Break condition**: If missingness patterns are strongly correlated with the target variable (informative missingness), compressing this "noise" may discard predictive signal

### Mechanism 2
- **Claim**: Unified-variate attention recovers disrupted inter-variable correlations better than separate temporal/variable modules
- **Mechanism**: The architecture flattens patches from all variates and time steps into a single token sequence ($N \times T/P$ tokens) and applies standard self-attention across this unified set, allowing the model to dynamically attend to informative signals regardless of whether they come from the same variate or a different variate
- **Core assumption**: Treating time and variate dimensions uniformly allows the model to "fill in" semantic gaps via attention to other variates, without explicit value imputation
- **Evidence anchors**: [abstract], [section 3.2]
- **Break condition**: If the sequence length ($N \times T/P$) becomes excessive, quadratic attention costs may bottleneck performance or force aggressive patching that loses granularity

### Mechanism 3
- **Claim**: Consistency regularization stabilizes training by enforcing invariance to specific missingness masks
- **Mechanism**: The model creates an augmented view $X_{Aug}$ by adding random masks and noise to the already partial input, then minimizes the L2 distance between the representations $Z$ and $Z_{Aug}$. This forces the encoder to ignore the specific locations of missing values and focus on the underlying stable dynamics
- **Core assumption**: The predictive signal is invariant to the specific realization of the missing mask, and perturbing the observed data slightly should not drastically change the forecast
- **Evidence anchors**: [abstract], [section 3.5]
- **Break condition**: If the augmentation strategy removes critical context that is rare to begin with, the consistency loss may enforce degenerate solutions (e.g., constant outputs)

## Foundational Learning

- **Concept**: **Variational Information Bottleneck (VIB)**
  - **Why needed here**: This is the theoretical core of CRIB. You cannot understand the loss function ($L_{Comp}$ vs $L_{Pred}$) without understanding the trade-off between compression (minimizing $I(Z;X_o)$) and prediction (maximizing $I(Y;Z)$)
  - **Quick check question**: Can you explain why increasing the KL divergence term ($\beta$) in the loss makes the latent representation $Z$ more "compressed" or noisy?

- **Concept**: **Patching in Time Series Transformers**
  - **Why needed here**: CRIB relies on transforming point-level data into patches to reduce sequence length and enhance semantic meaning before the Unified-Variate Attention
  - **Quick check question**: If you increase the patch size $P$, how does that affect the computational complexity of the attention layer and the granularity of the temporal features?

- **Concept**: **Consistency Regularization (e.g., FixMatch, Pi-model)**
  - **Why needed here**: The model uses this to handle high missing rates. Understanding semi-supervised learning principles helps explain why enforcing similarity between two augmented views improves robustness
  - **Quick check question**: In CRIB, if the consistency loss is set too high relative to the prediction loss, what might happen to the model's accuracy on fully observed data?

## Architecture Onboarding

- **Component map**: Input -> **Patching** (reduces computation, aggregates sparse points) -> **Unified Attention** (mixes info across variates/time to compensate for missingness) -> **IB Layer** (filters noise via reparameterization) -> Output
- **Critical path**: Input -> **Patching** (reduces computation, aggregates sparse points) -> **Unified Attention** (mixes info across variates/time to compensate for missingness) -> **IB Layer** (filters noise via reparameterization) -> Output
- **Design tradeoffs**:
  - **Direct Prediction vs. Imputation**: CRIB trades the potential benefit of "filling in" gaps for the certainty of not hallucinating bad data
  - **Unified Attention**: Trades the structural bias of graph-based or separated attentions for the flexibility of full attention, at the cost of potential overfitting on small datasets
  - **IB Weight ($\alpha$)**: Paper notes high $\alpha$ can over-regularize and destroy task-relevant info, especially at high missing rates
- **Failure signatures**:
  - **High Variance/Instability**: Observed in ablation (Table 4) when Consistency Regularization is removed
  - **Performance Collapse**: If IB weight is too high, the model compresses information too aggressively, resulting in poor $L_{Pred}$ (high MAE)
  - **OOM (Out of Memory)**: On datasets with high variate counts ($N$) and long sequences ($T$), the unified attention matrix ($N \times T/P)^2$ can explode memory usage
- **First 3 experiments**:
  1. **Ablation on Missing Rate**: Run CRIB on PEMS-BAY or ETTh1 with missing rates 20%, 40%, 60%. Verify that the performance gap between CRIB and "Imputed" baselines widens as the missing rate increases
  2. **Attention Map Visualization**: Visualize the Unified-Variate Attention maps. Check if the model attends to cross-variate dependencies (off-diagonal) rather than just self-time dependencies, as claimed in Figure 6
  3. **Sensitivity to Augmentation**: Modify the augmentation strength (e.g., mask 20% instead of 10% in consistency reg) and observe the impact on the 70% missing rate scenario to test the robustness boundary

## Open Questions the Paper Calls Out
- **Scalability**: The paper does not extensively test the model on datasets with very large numbers of variates, leaving questions about computational efficiency at scale
- **Generalizability**: While results are strong across 11 datasets, the paper does not provide detailed analysis of which dataset characteristics most influence performance

## Limitations
- **Hyperparameter Sensitivity**: CRIB's performance is sensitive to hyperparameters like the IB weight and augmentation strength, but the paper provides limited guidance on tuning strategies for new datasets
- **Informative Missingness**: The proposed mechanisms assume missingness is largely uninformative, but the paper does not thoroughly test performance when missingness is informative (e.g., sensors fail during extreme events)
- **Preprocessing Details**: The paper mentions data normalization but does not specify the exact preprocessing pipeline, which could impact reproducibility

## Confidence
- **High Confidence**: The core claim that direct prediction can outperform imputation-then-prediction (based on extensive empirical evaluation across 11 datasets and multiple missingness patterns)
- **Medium Confidence**: The mechanism explanations (IB filtering, unified attention, consistency regularization) are plausible given the architecture and theory, but the paper provides limited ablation evidence directly linking these components to performance gains
- **Low Confidence**: The generalizability of the hyperparameter settings and the model's behavior under informative missingness patterns are not well established

## Next Checks
1. **Informative Missingness Test**: Design a synthetic dataset where missingness is correlated with the target (e.g., sensor failure during extreme events). Evaluate CRIB vs. imputation baselines to test the assumption that missingness is uninformative
2. **Hyperparameter Transferability**: Train CRIB on a held-out dataset (not in the original 11) with minimal hyperparameter tuning. Measure the performance drop to assess how much of the reported gain relies on dataset-specific tuning
3. **Attention Pattern Analysis**: Conduct a quantitative analysis of the unified-variate attention maps across different datasets and missingness rates. Verify that the model consistently attends to cross-variate dependencies as claimed, rather than just self-time dependencies