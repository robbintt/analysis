---
ver: rpa2
title: 'TS-SUPERB: A Target Speech Processing Benchmark for Speech Self-Supervised
  Learning Models'
arxiv_id: '2505.06660'
source_url: https://arxiv.org/abs/2505.06660
tags:
- speech
- tasks
- speaker
- target
- ts-asr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TS-SUPERB, a benchmark for evaluating self-supervised
  learning (SSL) models on target-speaker speech processing tasks. Unlike previous
  benchmarks focusing on single-speaker scenarios, TS-SUPERB addresses four target-speaker
  tasks (TSE, PSE, PVAD, TS-ASR) in noisy, multi-talker conditions.
---

# TS-SUPERB: A Target Speech Processing Benchmark for Speech Self-Supervised Learning Models

## Quick Facts
- **arXiv ID**: 2505.06660
- **Source URL**: https://arxiv.org/abs/2505.06660
- **Reference count**: 40
- **Primary result**: Introduces TS-SUPERB, a benchmark evaluating SSL models on four target-speaker tasks in noisy, multi-talker conditions.

## Executive Summary
TS-SUPERB is a benchmark designed to evaluate self-supervised learning (SSL) models on target-speaker speech processing tasks, addressing a critical gap in existing speech benchmarks that focus primarily on single-speaker scenarios. The benchmark evaluates four tasks—target speaker extraction (TSE), speaker extraction (PSE), target speaker voice activity detection (PVAD), and target speaker automatic speech recognition (TS-ASR)—in challenging multi-talker environments. Using a unified SSL-based target speech encoder architecture, TS-SUPERB demonstrates that SSL model performance on target speaker tasks cannot be inferred from their performance on related single-speaker tasks. The WavLM Base+ model achieves the best overall performance, with SI-SDRi of 10.69 dB for TSE, 10.01 average SI-SDRi across overlap conditions for PSE, 95.00 mAP for PVAD, and 20.06% WER for TS-ASR. The study also shows that multi-task learning across TS tasks can improve performance.

## Method Summary
TS-SUPERB introduces a unified architecture for target speaker tasks that leverages SSL models as frozen feature extractors. The approach uses a target speech encoder with a shared SSL backbone, a speaker encoder that extracts embeddings from enrollment speech using multi-head factorized attention pooling, and an extractor that conditions on these embeddings to produce target speech features. Task-specific decoders then process these features for TSE/PSE (signal reconstruction), PVAD (activity detection), or TS-ASR (recognition). The benchmark evaluates multiple SSL models (wav2vec 2.0, HuBERT, WavLM, data2vec) using consistent downstream architectures to enable fair comparison. Performance is measured using SI-SDRi for enhancement tasks, mAP for PVAD, and WER for TS-ASR, with evaluation across different overlap conditions to assess robustness.

## Key Results
- WavLM Base+ achieves the best overall performance with SI-SDRi of 10.69 dB for TSE, 10.01 average SI-SDRi across overlap conditions for PSE, 95.00 mAP for PVAD, and 20.06% WER for TS-ASR.
- SSL model performance on target speaker tasks cannot be inferred from their performance on related single-speaker tasks, highlighting the unique challenges of TS scenarios.
- Multi-task learning across TS tasks improves performance, with joint TSE+TS-ASR improving SI-SDRi to 11.74 vs. 11.16 for TSE alone, and joint PSE+PVAD improving mAP to 0.962 vs. 0.942 for PVAD alone.

## Why This Works (Mechanism)

### Mechanism 1: Speaker Embedding as Conditioning Signal
The enrollment speech provides speaker identity information that conditions the extractor to isolate target-speaker features from mixtures. A speaker encoder (SpkEnc) uses multi-head factorized attention pooling (MHFA) to compress SSL features from enrollment speech into a speaker vector *e*. This vector is then fused with mixture representations via broadcast multiplication, guiding the extractor to emphasize target-speaker characteristics. This works because SSL representations encode sufficient speaker-discriminative information that can be pooled into a compact embedding.

### Mechanism 2: Dual-Objective Processing via Unified Encoder
Target-speaker tasks require simultaneously solving speaker identification and information extraction, which benefits from a shared encoder architecture. The target speech encoder processes both the mixture and enrollment through the same SSL backbone (with separate weighted layer summation). The extractor module receives conditioned features and produces target speech representations *Zx* that are passed to task-specific decoders. This dual-objective approach works because representations useful for speaker discrimination and content extraction can be learned in a shared encoder without catastrophic interference.

### Mechanism 3: Multi-Task Learning Leveraging Mutual Information
Joint training across related TS tasks improves feature quality by exploiting shared sub-objectives. Since TSE+TS-ASR and PSE+PVAD share the target speech encoder, the loss function *L = αLi + (1-α)Lj* enables gradients from one task to regularize the encoder for the other. Enhancement tasks (TSE/PSE) benefit from recognition supervision, and vice versa. This works because tasks share complementary information (e.g., clean speech estimation helps ASR; ASR linguistic supervision helps enhancement).

## Foundational Learning

- **Concept: Self-Supervised Speech Representations (SSL)**
  - **Why needed here**: The entire benchmark evaluates how well SSL models transfer to TS tasks. Understanding that SSL models learn general-purpose features from unlabeled audio without task-specific supervision is essential.
  - **Quick check question**: Can you explain why freezing SSL weights (the default in TS-SUPERB) still enables strong performance on downstream tasks?

- **Concept: Speaker Embeddings**
  - **Why needed here**: The target speech encoder relies on speaker embeddings from enrollment speech to condition extraction. You must understand how embeddings compactly represent speaker identity.
  - **Quick check question**: What is the difference between using a pre-trained speaker encoder (e.g., x-vector) vs. the SSL-based MHFA pooling used here?

- **Concept: Signal-to-Distortion Ratio (SI-SDRi)**
  - **Why needed here**: TSE and PSE are evaluated using SI-SDRi improvement. This metric quantifies extraction quality relative to the mixture.
  - **Quick check question**: If SI-SDRi is 10 dB, what does that imply about the relative power of extracted speech vs. residual interference?

## Architecture Onboarding

- **Component map**: SSL Backbone (frozen) -> Weighted Layer Summation -> Speaker Encoder (SpkEnc) -> Target Speech Encoder -> Extractor (conditioned on speaker embedding) -> Task Decoders

- **Critical path**:
  1. Preprocess enrollment and mixture audio to match SSL input requirements.
  2. Forward enrollment through SSL → SpkEnc → speaker embedding *e*.
  3. Forward mixture through SSL → weighted sum → extractor (conditioned on *e*) → *Zx*.
  4. Pass *Zx* to task decoder → loss computation → backprop through trainable modules only (SSL frozen by default).

- **Design tradeoffs**:
  - **Frozen vs. fine-tuned SSL**: Freezing reduces compute and overfitting risk but may underperform compared to full fine-tuning (TS-ASR WER 20.06% vs. 12.32% in prior work with fine-tuning).
  - **Decoder simplicity vs. performance**: Lightweight decoders (few BLSTM layers) enable fair SSL comparison but limit absolute performance.
  - **Single vs. multi-task training**: Multi-task improves some metrics but requires careful loss weighting and may not help all tasks equally.

- **Failure signatures**:
  - **High Failure Rate (FR) in PSE**: SDRi < 1 dB indicates extractor failed to isolate target; often due to enrollment mismatch or extreme overlap/noise.
  - **Poor TS-ASR WER with good SI-SDRi**: Suggests extractor produces audible speech but residual interference confuses the ASR decoder.
  - **PVAD mAP drop at high overlap**: Frame-level detection degrades when multiple speakers overlap heavily (60% condition).

- **First 3 experiments**:
  1. **Baseline reproduction**: Train TSE downstream model with WavLM Base+ on Libri2Mix-min; verify SI-SDRi ≈ 10.69 dB.
  2. **Ablate speaker encoder**: Replace SSL-based SpkEnc with a pre-trained x-vector extractor; compare SI-SDRi and mAP to quantify SSL speaker representation contribution.
  3. **Multi-task pilot**: Jointly train PSE+PVAD with α=0.5 on Noisy SparseLibri2Mix; compare PSE SI-SDRi and PVAD mAP to single-task baselines to validate mutual benefit.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can incorporating enrollment speech at the SSL pre-training stage, rather than only during downstream fine-tuning, improve performance across all TS-SUPERB tasks?
- **Basis in paper**: [explicit] The paper cites Zhang & Qian [25], who incorporated enrollment speech into pre-training and achieved superior TS-ASR performance, noting "It is thus unclear if the learned representation by these models can also be effective for other TS tasks."
- **Why unresolved**: The referenced approach was only tested on TS-ASR; generalization to TSE, PSE, and PVAD remains unexplored.
- **What evidence would resolve it**: Benchmark results from SSL models pre-trained with enrollment speech auxiliary inputs evaluated on all four TS-SUPERB tasks.

### Open Question 2
- **Question**: Why does TS-ASR exhibit a surprisingly weak correlation with standard ASR, and what architectural modifications could better bridge this gap?
- **Basis in paper**: [explicit] "It is observed that TS-ASR has a surprisingly weak correlation with ASR. This could be attributed to the additional requirement of TS-ASR to separate the mixture speech... making it challenging to simultaneously deal with both tasks effectively."
- **Why unresolved**: The paper offers a hypothesis but provides no validation; the simple ASR decoder may be insufficient for the dual objective.
- **What evidence would resolve it**: Ablation studies with stronger decoder architectures or intermediate separation modules showing improved TS-ASR/ASR correlation.

### Open Question 3
- **Question**: How does the unified target speech encoder perform when jointly trained across all four TS tasks simultaneously rather than only paired combinations?
- **Basis in paper**: [inferred] The paper explores multi-task learning only between two task pairs (TSE+TS-ASR and PSE+PVAD), stating "we can explore jointly training it using multi-task learning across different TS downstream tasks" but does not test all four together.
- **Why unresolved**: Mutual information may exist across all tasks; limiting to pairs may miss cross-task benefits (e.g., TSE helping PVAD).
- **What evidence would resolve it**: Experiments training a single unified encoder on all four tasks with weighted loss aggregation, comparing against single-task and paired baselines.

## Limitations
- **SSL Model Comparison Constraints**: The benchmark uses a fixed lightweight decoder architecture across all SSL models, which may not fully exploit stronger models' potential.
- **Evaluation Metric Limitations**: The benchmark relies heavily on signal-based and recognition metrics but lacks comprehensive subjective quality assessments for target speaker extraction tasks.
- **Speaker Embedding Quality Assumptions**: The paper assumes SSL-based speaker embeddings are sufficient for conditioning but does not thoroughly validate this assumption or test sensitivity to enrollment quality.

## Confidence
- **High Confidence**: The observation that TS task performance cannot be inferred from related single-speaker tasks is well-supported by experimental results across multiple SSL models and tasks.
- **Medium Confidence**: The claim that multi-task learning improves performance is supported by specific improvements in joint training scenarios, but effectiveness varies by task combination.
- **Medium Confidence**: The assertion that WavLM Base+ achieves the best overall performance is supported by quantitative results, but the margin should be interpreted cautiously given the constrained downstream architecture.

## Next Checks
1. **Speaker Embedding Ablation Study**: Replace the SSL-based speaker encoder (MHFA pooling) with a conventional speaker embedding system (e.g., x-vector) to quantify the contribution of SSL representations to task performance.
2. **Fine-tuning vs. Frozen SSL**: Conduct experiments comparing frozen SSL weights against full fine-tuning on target tasks to determine the performance gap and identify whether SSL models are truly ready for TS tasks without adaptation.
3. **Subjective Quality Assessment**: Complement objective metrics with subjective listening tests to evaluate perceptual quality of extracted speech, particularly for cases where SI-SDRi improvements do not correlate with human perception.