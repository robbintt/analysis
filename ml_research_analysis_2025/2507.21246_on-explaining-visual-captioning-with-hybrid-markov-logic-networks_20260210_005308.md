---
ver: rpa2
title: On Explaining Visual Captioning with Hybrid Markov Logic Networks
arxiv_id: '2507.21246'
source_url: https://arxiv.org/abs/2507.21246
tags:
- image
- distribution
- caption
- ground
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present a novel approach to explain visual captioning
  systems using Hybrid Markov Logic Networks (HMLNs). The key idea is to quantify
  how training examples influence caption generation by measuring shifts in the distribution
  over training data when conditioned on the generated caption.
---

# On Explaining Visual Captioning with Hybrid Markov Logic Networks

## Quick Facts
- arXiv ID: 2507.21246
- Source URL: https://arxiv.org/abs/2507.21246
- Reference count: 36
- Primary result: HMLN explanations achieve high interpretability scores in user studies across four captioning models

## Executive Summary
This paper introduces a novel approach to explain visual captioning systems using Hybrid Markov Logic Networks (HMLNs). The key innovation is quantifying how training examples influence caption generation by measuring shifts in the distribution over training data when conditioned on the generated caption. The method combines symbolic rules with real-valued functions relating visual features to text using CLIP embeddings, then learns parameters through contrastive divergence. User studies on Amazon Mechanical Turk show that explanations generated by this method are highly interpretable, with the majority of users rating them positively on a 5-point Likert scale.

## Method Summary
The approach models training data as a distribution using Hybrid Markov Logic Networks. When a caption is generated for a test image, it is treated as "virtual evidence" and injected into the network. The system computes importance weights by comparing the prior distribution against the evidence-conditioned distribution to identify which training instances became significantly more or less likely. The framework uses HMLNs with two template types (Conjunctive and Explanation) parameterized by CLIP-based similarity functions. Contrastive Divergence enables tractable parameter learning without computing the intractable partition function. Explanations are generated by selecting training examples that show contrasting bias patterns based on Hellinger distance calculations.

## Key Results
- AoANet received the highest interpretability scores among four captioning models tested
- Majority of users rated HMLN explanations positively on 5-point Likert scale
- Attention-based explanations were less effective at conveying interpretability compared to HMLN approach
- Method successfully applied to SGAE, AoANet, X-LAN, and M2 Transformer models on MSCOCO dataset

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The system generates explanations by measuring how much a generated caption "biases" the probabilistic distribution over the training data.
- **Mechanism:** The authors model the training data as a distribution using a Hybrid Markov Logic Network (HMLN). When a caption is generated for a test image, it is treated as "virtual evidence." By computing the importance weight—specifically the ratio between the prior distribution and the evidence-conditioned distribution—the system identifies which training instances became significantly more (or less) likely. High divergence indicates high relevance or influence.
- **Core assumption:** The generated caption acts as a valid observation that conditions the probability space of the training data.
- **Evidence anchors:**
  - [abstract] "...infer the shift in distributions over these instances when we condition on the generated sample..."
  - [section 3.3] "The importance weights explain the influence of the virtual evidence on the prior distribution... We compute the distance between the marginal densities..."
- **Break condition:** If the generated caption contains predicates that do not exist in the training data knowledge base, the "reification" mapping fails.

### Mechanism 2
- **Claim:** Integrating symbolic logic with neural embeddings allows the system to reason over visual semantics while maintaining interpretability.
- **Mechanism:** The framework uses "Hybrid" MLNs, where logical formulas (symbolic) are parameterized by real-valued functions derived from CLIP neural embeddings. Specifically, it uses "Explanation" (I) and "Conjunctive" (C) templates. The I-property penalizes dissimilarities between visual features and text embeddings using a Gaussian penalty, effectively softening hard logic rules with perceptual similarity.
- **Core assumption:** CLIP embeddings provide a sufficiently accurate proxy for "semantic similarity" between a logical predicate and visual image regions to act as truth values in a logic network.
- **Evidence anchors:**
  - [abstract] "...combine symbolic rules with real-valued functions relating visual features to text using CLIP embeddings..."
  - [section 3.2] "We relate the symbolic representation with the visual representation... using CLIP... We convert these neural embeddings into real-valued terms for the HMLN."
- **Break condition:** If the visual features are ambiguous, the soft logic values will propagate noise, making the distribution shift unreliable.

### Mechanism 3
- **Claim:** Contrastive Divergence (CD) enables tractable parameter learning for the HMLN without computing the intractable partition function.
- **Mechanism:** Standard maximum likelihood estimation in Markov Logic requires calculating a global normalization constant (partition function), which is #P-hard. The authors use CD, approximating the gradient by comparing the observed data statistics against samples generated from a short Gibbs sampling run, allowing the model to learn weights efficiently.
- **Core assumption:** A small number of Gibbs sampling steps is sufficient for the Markov chain to mix enough to provide a useful gradient direction for weight updates.
- **Evidence anchors:**
  - [abstract] "...learn parameters through contrastive divergence."
  - [section 3.2] "Therefore, we apply contrastive divergence where... we sample worlds using Gibbs sampling and compute the gradient based on the estimated expectations..."
- **Break condition:** If the distribution is multi-modal and the Gibbs sampler gets stuck in a local mode, the estimated gradients will be biased.

## Foundational Learning

- **Concept: Markov Logic Networks (MLNs)**
  - **Why needed here:** This paper builds on MLNs. You must understand that an MLN is a set of weighted First-Order Logic formulas. A higher weight means the formula is more likely to be true (harder constraint), while a lower weight makes it a soft preference.
  - **Quick check question:** If a logical formula has a weight of infinity in an MLN, what does that imply for worlds that violate it?

- **Concept: Importance Sampling**
  - **Why needed here:** The explanation mechanism relies on "importance weighting" to compare the prior distribution against the evidence-conditioned distribution.
  - **Quick check question:** When sampling from a proposal distribution $Q$ to estimate an expectation over $P$, why do we need to weight samples by $P(x)/Q(x)$?

- **Concept: CLIP (Contrastive Language-Image Pre-training)**
  - **Why needed here:** The "Hybrid" part of the HMLN relies on CLIP to map text and images into a shared embedding space to calculate similarity (real-valued terms).
  - **Quick check question:** Does CLIP embed the text "a photo of a dog" closer to an image of a dog or an image of a cat?

## Architecture Onboarding

- **Component map:** Test Image + Generated Caption -> Scene Graph Parser + Faster R-CNN -> CLIP Embedder -> HMLN Knowledge Base -> Gibbs Sampler -> Hellinger Distance Scorer -> Top-3 Training Examples

- **Critical path:**
  1. **Template Instantiation:** Convert training captions into ground predicates and slot them into Conjunctive (C) and Explanation (I) templates.
  2. **Weight Learning:** Use Contrastive Divergence to learn formula weights based on the training data.
  3. **Virtual Evidence:** Inject the test caption as "virtual evidence" (soft constraints) into the network.
  4. **Inference:** Run Gibbs sampling to compute marginal probabilities with and without the virtual evidence.
  5. **Selection:** Compute Hellinger distance between marginals to find training images with the highest "bias shift."

- **Design tradeoffs:**
  - *Chain Length:* The authors limit predicate chains to a max of 2. Longer chains might capture more complex logic but exponentially increase grounding complexity.
  - *Gibbs Thinning:* They use "thinning" (sampling every $m$-th step). This reduces autocorrelation but increases wall-clock time.
  - *Closed World Assumption:* The normalization step assumes the test image only contains objects detected by Faster R-CNN.

- **Failure signatures:**
  - **"Reification" Failure:** If the test caption uses a word not found in the training vocabulary, the system tries to map it to the "closest" training predicate.
  - **Attention Failure:** The authors explicitly note that standard attention-based explanations failed to provide statistically significant interpretability.
  - **Weight Collapse:** If the Gaussian penalty in the I-property is too tight, the potential values might vanish.

- **First 3 experiments:**
  1. **Sanity Check (CLIPScore Correlation):** Verify that higher Hellinger distances correspond to higher CLIPScores on a small validation set.
  2. **Attention Baseline:** Implement the weakly-supervised MIL attention model and verify it fails to distinguish between high/low attention pairs.
  3. **User Study Pilot:** Run a small-scale user study comparing HMLN explanations vs. Random training examples.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can this HMLN-based explanation framework be effectively extended to explain complex generative models in other domains, such as Visual Question Answering (VQA)?
- Basis in paper: [explicit] The conclusion states, "In future, we will build upon this framework to explain other complex generative models such as Visual Question Answering."
- Why unresolved: The current work focuses exclusively on image captioning; VQA introduces different constraints, such as reasoning over a specific question input rather than generating a general description.
- What evidence would resolve it: A demonstration of the framework applied to VQA tasks, showing that user studies yield similar interpretability scores for the generated explanations.

### Open Question 2
- Question: Does restricting explanation templates to pairwise conjunctive and XOR properties limit the ability to capture complex, multi-object relationships in generated captions?
- Basis in paper: [inferred] Section 3.1 states, "We limit the size of the chain to a maximum of two predicates since beyond this, the chains seem to lack coherence."
- Why unresolved: Limiting chains to two predicates assumes that higher-order relationships are incoherent or unnecessary, which may not hold for complex scenes involving multiple interacting objects.
- What evidence would resolve it: An ablation study comparing the interpretability and accuracy of explanations generated using pairwise templates versus those allowing longer predicate chains.

### Open Question 3
- Question: To what extent do the generated training examples faithfully reflect the actual internal decision-making process of the captioning model, compared to merely being plausible to human users?
- Basis in paper: [inferred] The experiments rely on Amazon Mechanical Turk ratings of "interpretability" rather than quantitative metrics measuring the causal influence of the cited training examples on the model's parameters or final output.
- Why unresolved: High user interpretability scores indicate plausibility but do not guarantee that the HMLN distribution shift accurately models the specific neural network's internal behavior.
- What evidence would resolve it: A quantitative evaluation measuring if removing the selected training examples significantly degrades the model's ability to generate the specific test caption.

### Open Question 4
- Question: How robust is the explanation framework to the hyperparameters set for the real-valued functions, such as the softness threshold $\epsilon$ used in the C-value calculation?
- Basis in paper: [inferred] The paper mentions setting $\epsilon$ to 0.7 based on a general heuristic, without analyzing sensitivity to this threshold.
- Why unresolved: If the threshold for determining if text explains an image is manually tuned, the explanations may not generalize well across different datasets or model architectures without retuning.
- What evidence would resolve it: A sensitivity analysis showing the variation in selected explanatory examples and user ratings as the $\epsilon$ threshold is systematically varied.

## Limitations

- The framework relies heavily on specific scene graph parsing that may not transfer well to other datasets or domains
- Computational overhead is significant due to Gibbs sampling and Hellinger distance computation over the entire training set
- User studies have methodological limitations including lack of true baseline comparison in main study and potential sample size imbalances
- The approach assumes training captions can be effectively converted to ground predicates, which may not hold for all captioning styles

## Confidence

- **Interpretability improvements:** Medium - supported by user studies but with methodological limitations
- **Technical novelty of HMLN approach:** High - clearly articulated and builds on established MLN framework
- **Superiority over attention-based explanations:** High - the paper demonstrates statistical significance in distinguishing between attention conditions
- **Model-agnostic applicability:** Medium - shown across four models but limited to one dataset

## Next Checks

1. **Replicate the attention baseline comparison:** Implement the weakly-supervised MIL attention model from Section 4.2 and verify that average scores fail to distinguish between high/low attention pairs (should be approximately 2.99 vs 2.85).

2. **Test CLIPScore correlation:** Validate that higher Hellinger distances correspond to higher CLIPScores on a held-out validation set to confirm the theoretical link between distribution shifts and caption quality.

3. **Scale experiment:** Measure inference time and memory usage when explaining captions for a single image, including Gibbs sampling iterations, Hellinger distance computation over training set, and top-3 selection to quantify computational overhead.