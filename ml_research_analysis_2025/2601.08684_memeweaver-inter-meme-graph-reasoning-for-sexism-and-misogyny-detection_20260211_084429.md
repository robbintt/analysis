---
ver: rpa2
title: 'MEMEWEAVER: Inter-Meme Graph Reasoning for Sexism and Misogyny Detection'
arxiv_id: '2601.08684'
source_url: https://arxiv.org/abs/2601.08684
tags:
- multimodal
- meme
- text
- exist
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MEMEWEAVER is an end-to-end multimodal framework that detects sexism
  and misogyny in memes by modeling inter-meme relationships within training batches.
  It fuses text and image features using multiple strategies and applies a learnable
  graph reasoning module to capture semantic affinities among memes.
---

# MEMEWEAVER: Inter-Meme Graph Reasoning for Sexism and Misogyny Detection

## Quick Facts
- arXiv ID: 2601.08684
- Source URL: https://arxiv.org/abs/2601.08684
- Reference count: 27
- Achieves 77.6% accuracy and 83.4% AUC on MAMI; 73.3% F1 on EXIST

## Executive Summary
MEMEWEAVER introduces an end-to-end multimodal framework for detecting sexism and misogyny in memes by leveraging inter-meme relationships within training batches. The system fuses text and image features using early, late, and hybrid strategies, then applies a learnable graph reasoning module to capture semantic affinities among memes. Evaluated on MAMI and EXIST benchmarks, MEMEWEAVER consistently outperforms strong unimodal and multimodal baselines, with faster convergence and improved robustness.

## Method Summary
MEMEWEAVER operates by first extracting multimodal features from memes using a CLIP-based encoder. These features are fused using multiple strategies (early, late, hybrid) before being passed to a learnable graph reasoning module. This module constructs a similarity graph among memes within a batch, allowing the model to reason about inter-meme semantic relationships. The framework is trained end-to-end and evaluated on sexism/misogyny detection tasks, showing strong performance and robustness gains.

## Key Results
- Achieves 77.6% accuracy and 83.4% AUC on the MAMI benchmark
- Attains 73.3% F1 score on the EXIST benchmark
- Demonstrates faster training convergence and improved robustness over unimodal and multimodal baselines

## Why This Works (Mechanism)
MEMEWEAVER’s strength lies in its learnable graph reasoning module, which explicitly models semantic relationships among memes within a batch. This allows the model to capture contextual cues and shared patterns that are not visible when memes are processed in isolation. The multimodal fusion (early, late, hybrid) ensures rich feature representation, while the graph module refines these representations by integrating relational information, leading to better detection of subtle or context-dependent sexist and misogynistic content.

## Foundational Learning
- **Multimodal Fusion (Early/Late/Hybrid)**: Combines text and image features at different stages; needed to leverage complementary information; quick check: compare fusion strategies via ablation.
- **Graph Neural Networks (GNNs)**: Models relationships among data points; needed to reason about meme-to-meme semantic affinities; quick check: test with and without GNN layer.
- **CLIP-based Encoders**: Extracts aligned text and image features; needed for strong multimodal representations; quick check: swap CLIP with other encoders.
- **Batch-wise Graph Construction**: Builds similarity graphs per training batch; needed for capturing local semantic clusters; quick check: vary batch size and observe graph density.

## Architecture Onboarding
- **Component Map**: CLIP Encoder -> Multimodal Fusion -> Graph Reasoning Module -> Classification Head
- **Critical Path**: CLIP feature extraction → Fusion → Graph Reasoning → Final prediction
- **Design Tradeoffs**: Graph reasoning adds computational overhead but improves relational reasoning; fusion strategy affects feature complementarity; learnable vs static graphs impacts adaptability.
- **Failure Signatures**: Poor performance if graph construction is noisy (batch size too small); overfitting if graph reasoning is too complex; modality imbalance if fusion is not well-tuned.
- **First Experiments**: (1) Ablate graph reasoning to confirm its contribution; (2) Compare early, late, and hybrid fusion strategies; (3) Test with static vs learnable similarity graphs.

## Open Questions the Paper Calls Out
None

## Limitations
- Gains attributed to graph reasoning are not clearly separated from fusion strategy effects
- No ablation of static or random similarity graphs to isolate learnability benefit
- No robustness tests with adversarial or cross-domain memes

## Confidence
| Claim | Confidence |
|-------|------------|
| Benchmark results on MAMI and EXIST | High |
| Inter-meme graph reasoning drives improvement | Medium |
| Robustness improvements over baselines | Low |

## Next Checks
1. Re-run ablations with static (non-learnable) similarity graphs and random graphs to isolate the benefit of learnability.
2. Perform cross-dataset validation (e.g., train on MAMI, test on EXIST) to assess generalization and distributional robustness.
3. Conduct adversarial robustness testing by introducing semantically flipped memes or low-quality images to measure degradation under perturbation.