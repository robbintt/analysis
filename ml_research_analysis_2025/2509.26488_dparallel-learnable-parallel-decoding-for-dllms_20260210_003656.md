---
ver: rpa2
title: 'dParallel: Learnable Parallel Decoding for dLLMs'
arxiv_id: '2509.26488'
source_url: https://arxiv.org/abs/2509.26488
tags:
- decoding
- arxiv
- diffusion
- preprint
- parallel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of slow inference in diffusion
  language models (dLLMs), which, despite their parallel token prediction capability,
  still require nearly token-length decoding steps for good performance. The core
  issue is that token certainty in dLLMs converges sequentially, limiting parallelism.
---

# dParallel: Learnable Parallel Decoding for dLLMs
## Quick Facts
- arXiv ID: 2509.26488
- Source URL: https://arxiv.org/abs/2509.26488
- Authors: Zigeng Chen; Gongfan Fang; Xinyin Ma; Ruonan Yu; Xinchao Wang
- Reference count: 16
- Key outcome: 8.5× speedup on GSM8K (76.1% accuracy) and 10.5× speedup on MBPP (40.8% accuracy) with 30× and 24× fewer decoding steps respectively

## Executive Summary
This paper addresses the slow inference problem in diffusion language models (dLLMs) by introducing dParallel, a learnable parallel decoding method. While dLLMs can predict tokens in parallel, their sequential token certainty convergence still requires nearly token-length decoding steps. dParallel uses certainty-forcing distillation to guide pretrained dLLMs to follow their original generation trajectory while enforcing high certainty on masked tokens more rapidly and in parallel. The approach achieves dramatic efficiency gains while maintaining competitive accuracy on reasoning tasks.

## Method Summary
dParallel employs certainty-forcing distillation to enable parallel token prediction in diffusion language models. The method trains a pretrained dLLM to maintain high certainty on masked tokens early in the generation process, allowing for fewer decoding steps while preserving the model's original generation trajectory. The training uses LoRA adapters and requires only 10 hours on eight A5000 GPUs. The approach is evaluated on GSM8K and MBPP benchmarks, demonstrating significant speedups while maintaining accuracy through reduced decoding steps from 256 to 30-24 tokens.

## Key Results
- Applied to LLaDA-8B-Instruct, dParallel reduces decoding steps from 256 to 30 on GSM8K (8.5× speedup, 76.1% accuracy)
- On MBPP, steps reduced from 256 to 24 (10.5× speedup, 40.8% accuracy)
- Training completed in 10 hours on eight A5000 GPUs using LoRA
- Method maintains competitive performance while dramatically improving inference efficiency

## Why This Works (Mechanism)
The paper demonstrates that dParallel unlocks parallel decoding potential in dLLMs by forcing early certainty convergence through distillation. By training the model to maintain high certainty on masked tokens throughout the generation process, the method enables accurate predictions with fewer decoding steps. This approach leverages the inherent parallel prediction capability of dLLMs while addressing their sequential certainty convergence limitation.

## Foundational Learning
**Diffusion Language Models** - Generative models using denoising diffusion processes for text generation. *Why needed:* Forms the baseline architecture being optimized. *Quick check:* Understand how denoising diffusion works in text generation context.

**Certainty-forcing Distillation** - Training technique that enforces high confidence predictions on specific tokens. *Why needed:* Core mechanism enabling parallel decoding. *Quick check:* Verify how certainty is quantified and enforced during training.

**LoRA Adapters** - Parameter-efficient fine-tuning method using low-rank adaptations. *Why needed:* Enables efficient training without full model retraining. *Quick check:* Confirm LoRA implementation details and parameter counts.

## Architecture Onboarding
**Component map:** dLLM (encoder-decoder) -> LoRA adapter -> Certainty-forcing loss -> Masked token predictions
**Critical path:** Input text -> Denoising diffusion process -> Parallel token prediction -> Certainty enforcement -> Output generation
**Design tradeoffs:** Speed vs accuracy (fewer steps may reduce quality), training time vs inference efficiency, LoRA efficiency vs full fine-tuning flexibility
**Failure signatures:** Accuracy degradation with aggressive step reduction, training instability during certainty forcing, loss of generation coherence
**3 first experiments:** 1) Baseline dLLM performance with full 256 steps, 2) dParallel with progressive step reduction, 3) Ablation study on certainty-forcing strength

## Open Questions the Paper Calls Out
None identified in the paper.

## Limitations
- Limited evaluation to only GSM8K and MBPP benchmarks, lacking broader generalizability testing
- Unclear whether accuracy retention is due to distillation approach or general fine-tuning effects
- No comparative cost analysis against other efficiency methods or original LLaDA training
- Performance drops (though unspecified baselines) may indicate accuracy-robustness tradeoffs

## Confidence
High confidence: Technical approach and parallel decoding mechanism are clearly described; experimental results show specific speedup numbers.
Medium confidence: Efficiency claims lack comparative benchmarks; accuracy maintenance claims need better baseline context.
Low confidence: Generalization claims to other datasets/tasks not supported by experimental evidence.

## Next Checks
1. Run controlled experiments comparing dParallel fine-tuned models against baseline LLaDA-8B-Instruct fine-tuned on same tasks to isolate distillation effect.
2. Test method on broader benchmarks including GLUE, SuperGLUE, and additional reasoning tasks to validate generalizability.
3. Conduct ablation studies to quantify contributions of certainty-forcing versus parallel training approach to speedup and accuracy results.