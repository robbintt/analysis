---
ver: rpa2
title: 'Structural Gender Bias in Credit Scoring: Proxy Leakage'
arxiv_id: '2601.18342'
source_url: https://arxiv.org/abs/2601.18342
tags:
- gender
- credit
- financial
- fairness
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study reveals that removing explicit gender attributes is
  insufficient to eliminate bias in credit scoring models, as gendered signals leak
  through non-sensitive features like marital status, age, and credit limit. Using
  SHAP analysis and adversarial inverse modeling, we demonstrate that these proxy
  variables allow models to reconstruct gender with ROC-AUC scores up to 0.65, even
  when gender labels are excluded.
---

# Structural Gender Bias in Credit Scoring: Proxy Leakage

## Quick Facts
- arXiv ID: 2601.18342
- Source URL: https://arxiv.org/abs/2601.18342
- Authors: Navya SD; Sreekanth D; SS Uma Sankari
- Reference count: 16
- Key outcome: Removing explicit gender attributes is insufficient to eliminate bias in credit scoring models due to proxy leakage through non-sensitive features.

## Executive Summary
This study reveals that removing explicit gender attributes is insufficient to eliminate bias in credit scoring models, as gendered signals leak through non-sensitive features like marital status, age, and credit limit. Using SHAP analysis and adversarial inverse modeling, we demonstrate that these proxy variables allow models to reconstruct gender with ROC-AUC scores up to 0.65, even when gender labels are excluded. Traditional fairness metrics—such as disparate impact and demographic parity—fail to detect this structural bias, leading to a false appearance of fairness. Our findings challenge the "fairness through blindness" doctrine and advocate for causal-aware modeling and structural accountability to ensure equitable AI-driven financial decisions.

## Method Summary
The study employs a comprehensive audit framework on the Taiwan Credit Default dataset (30K instances). It tests 12 experimental configurations combining Logistic Regression and XGBoost models with three balancing strategies (Class Weighting, SMOTE, Subsampling) across two feature conditions (with and without demographic features). The methodology integrates traditional fairness metrics, SHAP-based explainability, and inverse modeling to detect gender reconstruction from non-sensitive features. T-statistics quantify SHAP divergence across gender cohorts to identify proxy features.

## Key Results
- Inverse modeling achieved ROC-AUC of 0.65 for gender reconstruction from non-sensitive features
- Traditional fairness metrics (Disparate Impact, Demographic Parity, Equalized Odds) showed compliance while structural bias persisted
- SHAP analysis revealed significant divergence in feature contributions across gender cohorts (Age T = -6.976, Education T = 2.397, Marital Status T = -2.093)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Non-sensitive features function as gender proxies, enabling reconstruction of protected attributes even after explicit removal
- Mechanism: Variables with socio-economic grounding (marital status, age, credit limit) exhibit statistical correlation with gender distributions. The model learns these conditional relationships during training, creating implicit pathways for gendered predictions without direct access to gender labels
- Core assumption: The correlation between proxy features and gender reflects underlying structural socio-economic patterns rather than spurious noise
- Evidence anchors: [abstract] "gendered predictive signals remain deeply embedded within non-sensitive features... variables such as Marital Status, Age, and Credit Limit function as potent proxies for gender"; [results] Inverse modeling achieved ROC-AUC of 0.65 for gender reconstruction from non-sensitive features
- Break condition: If proxy-gender correlations are purely dataset-specific artifacts without structural basis, reconstruction performance would degrade to random (ROC-AUC ≈ 0.5)

### Mechanism 2
- Claim: Models apply divergent decision logic across gender cohorts using the same features, producing differential treatment invisible to aggregate fairness metrics
- Mechanism: SHAP-Gender T-statistic analysis reveals that features like Age (T = -6.976), Education (T = 2.397), and Marital Status (T = -2.093) contribute differently to predictions for male vs. female cohorts. The model adapts its internal weighting based on demographic signatures implicit in the feature vector
- Core assumption: The observed SHAP divergence reflects learned discriminatory patterns rather than legitimate risk differentiation between groups
- Evidence anchors: [results] "Age exhibits the highest divergence across genders... this significant variance in Age suggests that the model's predictive pathways are fundamentally not gender-neutral"; [table II] T-statistics quantify divergence: Education drives male risk assessment; Age and Marital Status dominate female assessment
- Break condition: If divergence reflects genuine risk differences rather than bias, removing these features would harm predictive accuracy without improving equity

### Mechanism 3
- Claim: Traditional fairness metrics (Disparate Impact, Demographic Parity, Equalized Odds) can indicate compliance while structural bias persists undetected
- Mechanism: Aggregate parity metrics measure outcome distributions across groups but fail to detect whether the model uses different feature pathways to reach similar outcomes. A model can satisfy demographic parity while still employing gender-proxied features that would produce disparate treatment under distribution shift
- Core assumption: Surface-level statistical parity is an incomplete proxy for genuine algorithmic fairness
- Evidence anchors: [abstract] "Traditional fairness metrics—such as disparate impact and demographic parity—fail to detect this structural bias, leading to a false appearance of fairness"; [results] All 12 configurations satisfied fairness thresholds (Disparate Impact near 1.0, parity differences near 0.0) while SHAP analysis revealed hidden bias
- Break condition: If structural bias does not affect real-world outcomes despite proxy usage, traditional metrics may be adequate for regulatory compliance

## Foundational Learning

- Concept: **Redundant Encoding / Proxy Discrimination**
  - Why needed here: Core theoretical foundation explaining why removing protected attributes fails; enables understanding of how features encode social structures
  - Quick check question: If you remove "gender" but keep "height" and "income," can you explain why the model might still discriminate?

- Concept: **SHAP Values and Feature Attribution**
  - Why needed here: Primary diagnostic tool used throughout the paper; required to interpret where bias enters model decisions
  - Quick check question: A feature has SHAP value 0.15 for Class A and 0.02 for Class B—what does this asymmetry suggest?

- Concept: **Adversarial / Inverse Modeling for Bias Detection**
  - Why needed here: Validates proxy leakage by treating bias detection as an attack problem; provides quantitative measure (ROC-AUC) of information leakage
  - Quick check question: If an inverse model achieves ROC-AUC 0.51 vs. 0.65, which indicates more severe proxy leakage?

## Architecture Onboarding

- Component map:
  - Taiwan Credit Default dataset -> Three balancing strategies -> Two model types -> Feature conditions -> Audit layer (fairness metrics, SHAP, inverse modeling) -> Analysis layer (gender-stratified SHAP with T-statistics)

- Critical path:
  1. Train credit default model with gender excluded
  2. Run traditional fairness audit (confirm false negative)
  3. Apply SHAP analysis globally and by-gender
  4. Train inverse model to predict gender from remaining features
  5. Identify proxy features via SHAP on inverse model

- Design tradeoffs:
  - SMOTE vs. Subsampling: SMOTE may introduce synthetic artifacts; Subsampling discards data. Paper tests both to ensure findings are not sampling-method dependent
  - Including vs. excluding demographic features: Full exclusion tests "fairness through blindness"; inclusion reveals direct leakage pathways
  - Logistic Regression vs. XGBoost: Linear model provides interpretable baseline; tree model captures complex proxy interactions but is harder to audit

- Failure signatures:
  - Fairness metrics show compliance (DI ≈ 1.0) but inverse model ROC-AUC > 0.6
  - SHAP values for same feature diverge significantly across gender cohorts (|T| > 2)
  - Demographic features consistently out-rank financial features in importance

- First 3 experiments:
  1. **Baseline replication**: Train XGBoost on financial features only, measure default prediction accuracy and inverse model gender reconstruction ROC-AUC
  2. **Feature ablation**: Remove top-k proxy features identified by SHAP (Age, Marital Status, LIMIT_BAL) and measure change in both default accuracy and gender reconstruction
  3. **Stratified audit**: Compute per-cohort SHAP distributions for top features; validate T-statistic divergence exceeds significance threshold (|T| > 1.96)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can formal causal frameworks be developed to trace and quantify the structural influence of sensitive attributes on credit model outcomes via indirect proxy pathways?
- Basis in paper: [explicit] Authors state: "we aim to develop a formal causal framework to trace and quantify the structural influence of sensitive attributes on model outcomes via indirect pathways. This shift from correlation-based explainability to causal discovery will facilitate a more rigorous understanding of the underlying mechanisms of gender leakage."
- Why unresolved: Current SHAP-based analysis is correlation-based and cannot establish causality; the paper identifies proxy leakage but cannot definitively trace how structural biases propagate through features like credit limit and marital status
- What evidence would resolve it: Development of structural causal models or causal graphs quantifying the magnitude of indirect gender effects through specific proxy variables

### Open Question 2
- Question: How robust is the proxy leakage detection methodology across diverse international credit datasets with varying socio-economic and cultural contexts?
- Basis in paper: [explicit] Authors state they "intend to validate this methodology across a diverse array of international credit datasets" to investigate "how localized socio-economic variations and cultural contexts influence the specific proxy variables the model selects for discrimination."
- Why unresolved: The study uses only the Taiwan Credit Default dataset; it is unknown whether Age, Marital Status, and Credit Limit function as gender proxies in other regulatory and cultural environments
- What evidence would resolve it: Replication of the inverse modeling framework on credit datasets from multiple countries, revealing whether proxy patterns are consistent or context-specific

### Open Question 3
- Question: Can fairness-aware loss functions or causal regularization simultaneously optimize predictive accuracy, demographic parity, and explainability without introducing new biases?
- Basis in paper: [explicit] Authors state: "our future technical work will focus on integrating fairness constraints directly into the model's objective function" through "causal regularization techniques and the development of fairness-aware loss functions."
- Why unresolved: Existing pre-processing mitigation (SMOTE, reweighing) may introduce unquantified biases or degrade performance; no current solution balances all three objectives
- What evidence would resolve it: A loss function achieving comparable accuracy while reducing gender reconstruction to near-random levels (ROC-AUC ≈ 0.5)

## Limitations
- The study demonstrates proxy discrimination occurs but cannot definitively prove these proxy patterns cause discriminatory real-world outcomes
- Findings are based on a single Taiwanese dataset, limiting generalizability to other credit contexts and populations
- Current methodology identifies correlation but cannot establish causal pathways for structural bias

## Confidence
- High confidence: Gender reconstruction from non-sensitive features (ROC-AUC 0.65) is statistically demonstrated
- Medium confidence: Traditional fairness metrics reliably fail to detect structural bias in all tested configurations
- Medium confidence: SHAP divergence between gender cohorts reflects meaningful differences in model decision pathways

## Next Checks
1. Apply inverse modeling to alternative credit datasets (US, European) to test cross-population proxy leakage consistency
2. Conduct counterfactual analysis: systematically perturb proxy features and measure changes in both default predictions and gender reconstruction accuracy
3. Design controlled lending simulation: use models with high proxy leakage to generate synthetic credit decisions, then audit downstream economic outcomes by gender