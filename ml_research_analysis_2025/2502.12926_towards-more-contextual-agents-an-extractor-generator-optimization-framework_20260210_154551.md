---
ver: rpa2
title: 'Towards more Contextual Agents: An extractor-Generator Optimization Framework'
arxiv_id: '2502.12926'
source_url: https://arxiv.org/abs/2502.12926
tags:
- agents
- prompt
- performance
- framework
- llm-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The authors address the challenge of improving Large Language
  Model (LLM) performance in domain-specific tasks where general-purpose models often
  underperform due to a lack of specialized knowledge. To tackle this, they propose
  an Extractor-Generator framework that automates the optimization of LLM-based agents
  by refining their prompts through two stages: feature extraction from gold-standard
  input-output pairs, and prompt generation via iterative self-improvement.'
---

# Towards more Contextual Agents: An extractor-Generator Optimization Framework

## Quick Facts
- **arXiv ID:** 2502.12926
- **Source URL:** https://arxiv.org/abs/2502.12926
- **Reference count:** 5
- **Primary result:** Extractor-Generator framework improves LLM agent performance by 8-17 percentage points in answer relevancy across five domains compared to chain-of-thought prompting

## Executive Summary
This paper addresses the challenge of improving Large Language Model (LLM) performance in domain-specific tasks where general-purpose models often underperform due to a lack of specialized knowledge. The authors propose an Extractor-Generator framework that automates the optimization of LLM-based agents by refining their prompts through two stages: feature extraction from gold-standard input-output pairs, and prompt generation via iterative self-improvement. The framework was evaluated across five domains (finance, healthcare, e-commerce, law, cybersecurity) using 150 optimization and 300 test examples per domain. Compared to chain-of-thought and self-consistency prompting pipelines, the proposed method achieved superior performance, with best results in e-commerce (88.1% answer relevancy) and strong gains in finance (87.4%) and healthcare (82.7%).

## Method Summary
The framework operates through a two-stage optimization process. First, an extractor-LLM with a multi-agent architecture processes gold-standard input-output pairs to extract task-relevant features, creating a feature matrix from N examples. Second, a generator-LLM uses this matrix to iteratively create and refine prompts through batched sampling and self-improvement cycles. The system employs a threshold-based detection mechanism to identify underperforming cases and apply targeted prompt updates. The final optimized prompt is then deployed to the target LLM agent for domain-specific tasks.

## Key Results
- E-commerce domain achieved highest performance with 88.1% answer relevancy
- Finance domain showed strong improvement at 87.4% answer relevancy
- Healthcare domain demonstrated solid gains at 82.7% answer relevancy
- Framework outperformed chain-of-thought and self-consistency baselines by 8-17 percentage points across all tested domains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Extracting task-relevant features from gold-standard input-output pairs enables more precise prompt construction than manual engineering.
- **Mechanism:** A distributed multi-agent Extractor-LLM system processes each input-output pair in parallel, with each agent specializing in capturing distinct feature dimensions. The extracted feature vectors encode contextual signals that inform prompt generation.
- **Core assumption:** Gold-standard examples contain recoverable patterns that generalize to unseen inputs in the same domain.
- **Evidence anchors:**
  - [abstract] "feature extraction from a dataset of gold-standard input-output pairs"
  - [section 2.1] "This extractor operates as a multi-agent system with a fully distributed topology, where each agent is specialized in capturing a distinct feature dimension"
  - [corpus] Limited direct corpus evidence on multi-agent feature extraction specifically for prompt optimization

### Mechanism 2
- **Claim:** Iterative self-improvement with underperformance detection reduces error propagation compared to static prompting.
- **Mechanism:** After initial prompt generation, the framework evaluates each prompt on individual input-output pairs using a metric μ. When μ falls below threshold λ, the Generator LLM updates the prompt and applies self-improvement, repeating up to three times per underperforming case.
- **Core assumption:** The evaluation metric μ reliably signals when prompts fail, and self-improvement iterations converge rather than drift.
- **Evidence anchors:**
  - [abstract] "iteratively identifies underperforming cases and applies self-improvement techniques"
  - [algorithm 2] "if μ(Pi, Ψ(x), y) < λ then Update Pi using G... Self-improve Pi using G"
  - [corpus] OMAC framework (arXiv:2505.11765) supports iterative multi-agent optimization as effective pattern

### Mechanism 3
- **Claim:** Batch-wise prompt sampling and selection from feature matrices stabilizes optimization across diverse inputs.
- **Mechanism:** The framework constructs matrix M of N feature vectors (dimension L), then iteratively samples sub-matrices of B rows to generate candidate prompts. Each prompt is evaluated on the full dataset, and the best-performing prompt is retained and further refined.
- **Core assumption:** Batching provides sufficient diversity for generalization while reducing variance in prompt quality.
- **Evidence anchors:**
  - [section 2.2] "A prompt Pt is then generated iteratively using sub-matrices MB,L, randomly sampled batches of rows from MN,L"
  - [algorithm 1] "Consider the prompt Pb with the best performance sb"
  - [corpus] No direct corpus evidence on batch-wise prompt optimization; this appears novel

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Prompting**
  - Why needed here: The paper benchmarks against CoT and its variants (sequential, self-consistency). Understanding these baselines is essential to contextualize the reported 8-17 percentage point improvements.
  - Quick check question: Can you explain why adding reasoning steps before an answer might improve LLM performance on domain-specific tasks?

- **Concept: Self-Improvement in LLMs**
  - Why needed here: The framework applies self-improvement at three stages. Understanding how LLMs can critique and refine their own outputs clarifies why iterative feedback loops work.
  - Quick check question: What are the risks of a model "improving" its own outputs without external ground truth?

- **Concept: Evaluation Metrics for LLM Outputs**
  - Why needed here: The framework relies on metric μ and threshold λ to detect underperformance. The paper uses "answer relevancy" assessed by an LLM-based judge.
  - Quick check question: Why might using an LLM to evaluate another LLM's outputs introduce bias or circularity?

## Architecture Onboarding

- **Component map:** Extractor-LLM (E) -> Feature Matrix (MN,L) -> Generator-LLM (G) -> Evaluator (μ) -> Threshold (λ)
- **Critical path:**
  1. Prepare gold-standard IO pairs (150 examples used in paper)
  2. Run feature extraction with self-improvement (parallel per pair)
  3. Assemble feature matrix M
  4. For T iterations: sample batch → generate prompt → evaluate → track best
  5. Apply Algorithm 2 refinement to best prompt
  6. Deploy optimized prompt Popt to target agent Ψ
- **Design tradeoffs:**
  - Batch size B: Smaller batches increase diversity but may reduce stability
  - Max iterations I: More iterations improve convergence but increase cost
  - Threshold λ: Lower thresholds trigger more updates (potentially overfitting); higher thresholds may miss improvement opportunities
  - Gold set size: Paper used 150 optimization examples; scaling requirements unclear
- **Failure signatures:**
  - Performance plateaus early: Check if feature extraction produces meaningful distinctions
  - High variance across batches: Increase batch size or reduce feature dimension L
  - Degraded test performance despite good optimization scores: Likely overfitting to gold set; reduce iterations or add regularization
  - Metric μ saturates: Evaluation may be misaligned with actual task success
- **First 3 experiments:**
  1. **Baseline validation:** Replicate paper's domain comparison (finance, healthcare, e-commerce) with CoT vs. Extractor-Generator on 50 optimization / 100 test examples to verify framework behavior
  2. **Ablation on self-improvement:** Run framework with self-improvement disabled at each of the three stages to quantify contribution of each refinement loop
  3. **Gold set sensitivity:** Test with 25, 50, 100, 150 optimization examples to characterize minimum data requirements for stable improvements

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on fixed, modest-sized gold-standard dataset (150 examples) for prompt optimization
- Use of LLM-based evaluators introduces potential circularity and bias
- Batch-based sampling strategy lacks theoretical grounding for optimal batch size selection

## Confidence
- **High Confidence**: Comparative results against CoT and self-consistency baselines are robust with consistent improvements across all five domains
- **Medium Confidence**: Mechanism of feature extraction from gold examples improving prompt specificity is supported but lacks isolated ablation studies
- **Low Confidence**: Self-improvement component's actual impact is difficult to assess due to absent intermediate metric tracking

## Next Checks
1. **Dataset size sensitivity analysis**: Systematically test optimization performance with gold-standard datasets ranging from 25 to 500 examples to determine minimum viable dataset size and identify overfitting thresholds
2. **Evaluator independence test**: Compare performance metrics when using human evaluators versus LLM-based evaluators for answer relevancy scoring to quantify potential circularity bias
3. **Zero-shot transfer evaluation**: Assess whether prompts optimized for one domain (e.g., healthcare) transfer effectively to semantically similar domains (e.g., medical research) without additional optimization