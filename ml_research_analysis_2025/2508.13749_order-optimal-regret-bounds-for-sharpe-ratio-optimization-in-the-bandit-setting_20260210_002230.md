---
ver: rpa2
title: Order Optimal Regret Bounds for Sharpe Ratio Optimization in the Bandit Setting
arxiv_id: '2508.13749'
source_url: https://arxiv.org/abs/2508.13749
tags:
- regret
- bound
- where
- variance
- thompson
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of sequential decision-making for
  Sharpe ratio (SR) maximization in a stochastic multi-armed bandit setting. The authors
  focus on Thompson Sampling (TS), a Bayesian approach, under the assumption of Gaussian
  rewards with unknown parameters.
---

# Order Optimal Regret Bounds for Sharpe Ratio Optimization in the Bandit Setting

## Quick Facts
- arXiv ID: 2508.13749
- Source URL: https://arxiv.org/abs/2508.13749
- Reference count: 40
- One-line primary result: Thompson Sampling achieves order-optimal O(log n) regret for Sharpe ratio maximization in Gaussian multi-armed bandits.

## Executive Summary
This paper addresses the challenge of sequential decision-making for Sharpe ratio (SR) maximization in a stochastic multi-armed bandit setting. The authors focus on Thompson Sampling (TS), a Bayesian approach, under the assumption of Gaussian rewards with unknown parameters. They introduce a novel regret decomposition tailored to the SR objective, highlighting the role of information acquisition about both the mean and variance of reward distributions. The paper establishes an upper bound on the regret for the proposed SRTS algorithm and derives a matching lower bound, proving order-optimality. Empirical simulations demonstrate that SRTS significantly outperforms existing algorithms.

## Method Summary
The paper proposes SRTS (Sharpe Ratio Thompson Sampling), which maintains conjugate Gaussian-Gamma posteriors over mean and precision parameters for each arm. In each round, samples from these posteriors are used to compute a sampled Sharpe ratio, and the arm maximizing this value is selected. After observing the reward, the sufficient statistics for both posteriors are updated. The algorithm requires regularization parameter L₀ ∈ (0,1] to stabilize variance estimates. The theoretical analysis provides both upper and lower bounds on expected regret, establishing that SRTS achieves logarithmic regret that is order-optimal.

## Key Results
- Thompson Sampling for Sharpe ratio optimization achieves O(log n) regret, which is order-optimal as confirmed by matching lower bounds.
- The regret decomposition reveals that SR regret depends on both mean and variance estimation, with coupled uncertainties creating additional complexity beyond standard bandit problems.
- Empirical simulations on K=10 Gaussian arms demonstrate that SRTS significantly outperforms existing UCB-based algorithms for Sharpe ratio optimization.

## Why This Works (Mechanism)

### Mechanism 1: Dual-Parameter Posterior Sampling with Conjugate Priors
SRTS achieves logarithmic regret by simultaneously learning mean and variance parameters through conjugate Bayesian updates, enabling risk-adjusted arm selection. For each arm i, the algorithm maintains a posterior over mean θ_i,t ~ N(μ̂_i,t-1, 1/s_i,t-1) and precision τ_i,t ~ Gamma(α_i,t-1, β_i,t-1). In each round, both parameters are sampled, and the sampled Sharpe ratio ξ̂_i,t = θ_i,t/(L_0 + ρ/τ_i,t) is computed. The arm maximizing this sampled SR is selected. After observing reward X_i(t),t, the sufficient statistics for both posteriors are updated. This dual-parameter sampling naturally balances exploration and exploitation.

### Mechanism 2: Regret Decomposition Through Pull Count Control
Expected regret is bounded by controlling E[s_i,n], the expected number of sub-optimal arm pulls, weighted by Sharpe ratio gaps and variance-aware correction terms. Theorem 1 establishes that E[R_n(π)] ≤ Σ_i E[s_i,n]·[Δ_i + correction_terms] + A_7, where Δ_i = ξ_1 - ξ_i is the SR gap. The decomposition separates: (1) variance estimation error, (2) cross-arm variance heterogeneity, and (3) switching costs from alternating arms. Lemma 1 uses Efron-Stein inequality to bound V[s_i,n] ≤ n/2, ensuring pull counts concentrate. This bridge between concentration failure and regret is critical for the subsequent analysis.

### Mechanism 3: Order Optimality via Matching Lower Bounds
The O(log n) regret rate achieved by SRTS is asymptotically tight, certified by matching lower bounds derived through change-of-measure arguments. Theorem 3 and Lemma 4 construct an alternative model F^i where a sub-optimal arm i becomes optimal by perturbing its distribution while keeping KL divergence I(f_i, f̃_i) close to I(f_i, f_*). For any α-consistent policy, lim inf E[s_i,n]/log n ≥ C_1C_2/I(f_i, f_*). This certifies that no consistent policy can achieve o(log n) regret, making SRTS order-optimal.

## Foundational Learning

- **Concept: Thompson Sampling with Conjugate Priors**
  - Why needed here: SRTS requires understanding simultaneous Bayesian updates for mean and precision. The Normal-Gamma conjugate family is the mathematical foundation enabling closed-form posteriors and tractable analysis.
  - Quick check question: Given s observations with sample mean μ̂ and sum of squared deviations, what are the posterior parameters for θ ~ N(·,·) and τ ~ Gamma(·,·) under a N(μ_0, 1/(λ_0τ)) × Gamma(α_0, β_0) prior?

- **Concept: Sharpe Ratio as Risk-Adjusted Objective**
  - Why needed here: Unlike standard bandits maximizing expected reward, SR optimization requires balancing return against risk. The definition ξ_i = μ_i/(L_0 + ρσ_i²) differs from classical finance (μ/σ) by using variance and requiring regularization L_0 to prevent division instability.
  - Quick check question: Why does the paper use σ² in the denominator rather than σ, and what happens to the SR estimate if L_0 = 0 and empirical variance approaches zero?

- **Concept: Regret Decomposition and Concentration Inequalities**
  - Why needed here: The proof strategy decomposes regret into pull counts bounded via concentration. Understanding Efron-Stein inequality, Gaussian tail bounds, and Gamma tail bounds is essential for Sections V-VI.
  - Quick check question: In Lemma 1, why does Efron-Stein give V[s_i,n] ≤ n/2 rather than n, and how does this bound enable the subsequent regret analysis?

## Architecture Onboarding

- **Component map:**
  ```
  SRTS Pipeline:
  ├── Initialization: μ̂_i,0=0, α_i,0=β_i,0=1/2, s_i,0=0
  ├── Per-round sampling (t > K):
  │   ├── τ_i,t ~ Gamma(α_i,t-1, β_i,t-1)
  │   ├── θ_i,t ~ N(μ̂_i,t-1, 1/s_i,t-1)
  │   └── Compute ξ̂_i,t = θ_i,t/(L_0 + ρ/τ_i,t)
  ├── Selection: i(t) = argmax_i ξ̂_i,t
  └── Update: Increment s_i, update μ̂, α, β for selected arm
  ```

- **Critical path:**
  1. Pull each arm once (t=1 to K) to initialize estimates
  2. Sample (θ, τ) from Normal-Gamma posterior for all arms
  3. Compute SR statistic for each arm
  4. Select arm with maximum sampled SR
  5. Update sufficient statistics for selected arm only

- **Design tradeoffs:**
  - L_0 regularization: Prevents division by near-zero variance but introduces bias; paper uses (0, 1] range
  - Risk parameter ρ: ρ→0 reduces to mean-only optimization; ρ→∞ emphasizes variance minimization; requires domain knowledge
  - Gaussian assumption: Enables conjugate analysis but Assumption: non-robust to heavy tails common in financial data

- **Failure signatures:**
  1. Division instability: σ̂² → 0 causes SR explosion → Fix: Ensure L_0 > 0, α_i,0 ≥ 1/2
  2. Slow convergence on similar arms: Small SR gaps → large regret constants → Detection: Monitor min_i |Δ_i|
  3. Heavy-tailed degradation: Gamma posterior fails to concentrate → Detection: Check empirical kurtosis >> 3

- **First 3 experiments:**
  1. Baseline validation: Implement SRTS on K=10 Gaussian arms with paper's parameters (Fig. 1 setup), n=20000. Verify log-linear regret and compare against UCB baseline.
  2. Gap sensitivity: Systematically vary SR gaps by adjusting means/variances. Confirm smaller gaps produce larger constants in regret bound.
  3. Distribution robustness: Test on Student-t (df=3) and log-normal rewards. Measure regret degradation relative to Gaussian case and theoretical bound.

## Open Questions the Paper Calls Out

### Open Question 1
Can the order-optimal regret guarantees for Sharpe ratio optimization be extended to heavy-tailed or sub-Gaussian reward distributions? The authors state they "focus on... Thompson Sampling... under the assumption of Gaussian rewards" and "provide a formal analysis under Gaussian reward model," implicitly limiting the scope to this specific family. The current proofs rely on the conjugacy of Normal-Gamma priors and specific concentration inequalities for Gaussian/Gamma distributions, which do not directly transfer to heavy-tailed distributions often found in financial data.

### Open Question 2
Is it possible to derive finite-time regret bounds for the standard definition of the Sharpe ratio (μ/σ) without the use of the regularization term L₀? The authors note they define SR as μ/(L₀ + ρσ²) "instead of following the traditional definition," explicitly adding L₀ "to stabilize the value of estimated SR" and facilitate the analysis. The theoretical guarantees depend on the Lipschitz stability provided by L₀ > 0 in the denominator. The standard ratio creates a non-regular statistical problem (division by near-zero variance), which complicates concentration bounds for the posterior.

### Open Question 3
Can the SRTS framework be generalized to the linear or contextual bandit setting to handle feature-dependent rewards? The paper discusses applications in "portfolio selection or algorithmic trading" (Introduction), which typically involve high-dimensional contextual data, but the proposed algorithm and regret bounds are restricted to the stochastic multi-armed bandit setting. In a linear setting, the algorithm must simultaneously learn the linear coefficients for the mean and model the variance/residuals, making the decomposition of the risk-adjusted regret significantly more complex.

## Limitations

- The empirical validation is limited to a single simulation scenario with specific parameters, lacking extensive ablation studies or robustness analysis.
- The theoretical analysis relies heavily on the Gaussian assumption and conjugate priors, which may not generalize to heavy-tailed or non-stationary reward distributions common in financial applications.
- The paper does not discuss computational complexity or scaling behavior for large K or n, leaving open questions about practical applicability in high-dimensional settings.

## Confidence

**High Confidence:** The regret decomposition framework and order optimality proof are mathematically rigorous with clear derivations and standard techniques. The conjugate prior updates for Gaussian rewards are well-established in Bayesian statistics.

**Medium Confidence:** The matching lower bound construction is sound, but the constants and their practical significance are not explored. The empirical results show expected trends but lack statistical rigor (confidence intervals, multiple scenarios).

**Low Confidence:** The practical performance claims are based on limited simulations. The algorithm's behavior under distribution misspecification or in high-dimensional settings remains unknown.

## Next Checks

1. **Robustness Testing:** Implement SRTS on heavy-tailed distributions (Student-t, log-normal) and measure regret degradation compared to the theoretical Gaussian case. Compare against the bound's tightness.

2. **Hyperparameter Sensitivity Analysis:** Systematically vary ρ and L₀ across multiple orders of magnitude. Measure how regret scales and identify regimes where the algorithm becomes unstable or underperforms.

3. **Scaling Study:** Test SRTS on larger bandit problems (K=50, 100 arms) and longer time horizons (n=100,000). Measure computational complexity and verify that the O(log n) regret rate holds while tracking runtime and memory requirements.