---
ver: rpa2
title: Teacher-Guided One-Shot Pruning via Context-Aware Knowledge Distillation
arxiv_id: '2511.16653'
source_url: https://arxiv.org/abs/2511.16653
tags:
- pruning
- sparsity
- accuracy
- importance
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational overhead of iterative train-prune-retrain
  cycles in unstructured pruning by proposing a one-shot global pruning framework
  guided by teacher knowledge distillation. Unlike prior approaches that apply KD
  only for post-pruning recovery, this method integrates KD directly into the importance
  score calculation, using gradient signals informed by the teacher model.
---

# Teacher-Guided One-Shot Pruning via Context-Aware Knowledge Distillation

## Quick Facts
- **arXiv ID**: 2511.16653
- **Source URL**: https://arxiv.org/abs/2511.16653
- **Reference count**: 31
- **Primary result**: Achieves high sparsity with minimal performance loss using one-shot pruning guided by context-aware knowledge distillation

## Executive Summary
This paper addresses the computational inefficiency of iterative train-prune-retrain cycles in unstructured pruning by proposing a one-shot global pruning framework guided by teacher knowledge distillation. Unlike prior approaches that apply KD only for post-pruning recovery, this method integrates KD directly into the importance score calculation, using gradient signals informed by the teacher model. The approach employs Context-Aware Kullback-Leibler Divergence (CA-KLD) loss augmented with logit normalization to guide the pruning process. Experiments on CIFAR-10, CIFAR-100, and TinyImageNet datasets show that the method consistently achieves high sparsity levels with minimal performance degradation, outperforming state-of-the-art baselines like EPG, EPSD, and COLT while offering a more computationally efficient alternative to iterative pruning schemes.

## Method Summary
The proposed method introduces a one-shot global pruning framework that leverages teacher-guided knowledge distillation during the pruning process itself. The key innovation is the Context-Aware Kullback-Leibler Divergence (CA-KLD) loss function, which incorporates logit normalization to guide the selection of important parameters for retention. Rather than applying knowledge distillation only after pruning, the teacher model's gradients inform the importance score calculation during pruning. This enables aggressive pruning while preserving critical parameters. The framework consists of three main phases: initial teacher model training, one-shot pruning guided by CA-KLD-based importance scoring, and sparsity-aware retraining. The approach is designed to achieve high sparsity levels while maintaining model performance, particularly suitable for resource-constrained environments where computational efficiency is paramount.

## Key Results
- Achieves significantly higher sparsity levels compared to state-of-the-art baselines (EPG, EPSD, COLT) while maintaining comparable accuracy
- Demonstrates consistent performance across CIFAR-10, CIFAR-100, and TinyImageNet datasets
- Reduces computational overhead by eliminating iterative train-prune-retrain cycles required by traditional pruning methods
- Shows minimal performance degradation even at aggressive pruning ratios

## Why This Works (Mechanism)
The method works by integrating teacher knowledge into the pruning decision process itself, rather than using KD only for recovery. The CA-KLD loss captures the relative importance of parameters by measuring the divergence between teacher and student model outputs in a context-aware manner. The logit normalization component ensures stable gradient signals during pruning, preventing the collapse of important feature representations. By using the teacher model's gradient information to guide which parameters to retain, the method preserves the most critical connections for the target task. This teacher-guided approach ensures that pruning decisions are informed by learned representations rather than purely heuristic or magnitude-based criteria, resulting in better preservation of model performance at higher sparsity levels.

## Foundational Learning

**Knowledge Distillation**: A technique where a smaller "student" model learns from a larger "teacher" model's output distributions. Needed to transfer learned representations from the teacher to guide pruning decisions. Quick check: Verify that KL divergence is being used between teacher and student logits.

**Unstructured Pruning**: Removing individual weights rather than entire neurons or channels. Needed for fine-grained compression without architectural constraints. Quick check: Confirm that pruning is applied at the weight level, not structured blocks.

**KL Divergence**: Measures the difference between two probability distributions. Needed as the core metric for comparing teacher and student model outputs in CA-KLD. Quick check: Ensure temperature scaling is applied to smooth the probability distributions.

**Logit Normalization**: Scaling technique applied to model outputs before computing divergences. Needed to stabilize gradient signals during the pruning process. Quick check: Verify normalization is applied consistently across all comparisons.

**One-shot Pruning**: Performing pruning in a single step rather than iterative cycles. Needed to reduce computational overhead compared to traditional methods. Quick check: Confirm that pruning ratio is determined before the process begins, not adjusted iteratively.

## Architecture Onboarding

**Component Map**: Teacher Model -> CA-KLD Loss Computation -> Importance Score Calculation -> Pruning Mask Application -> Student Model

**Critical Path**: Teacher model forward pass → CA-KLD loss computation with normalized logits → gradient-based importance scoring → parameter pruning → sparsity-aware retraining

**Design Tradeoffs**: The method trades the potentially higher accuracy of iterative pruning for significant computational efficiency gains. By using teacher-guided pruning in a single step, it sacrifices the fine-tuning capabilities of iterative approaches but gains speed and simplicity. The CA-KLD approach with logit normalization adds complexity to the pruning decision process but improves the quality of parameter selection.

**Failure Signatures**: Poor teacher model quality leading to suboptimal guidance, aggressive pruning ratios causing catastrophic accuracy loss, or improper temperature settings in the CA-KLD loss resulting in unstable gradients during pruning.

**First Experiments**:
1. Verify CA-KLD loss computation with and without logit normalization on a small network
2. Test importance score correlation between magnitude-based and teacher-guided methods
3. Evaluate ablation study comparing one-shot vs iterative pruning with teacher guidance

## Open Questions the Paper Calls Out

None

## Limitations

- Limited evaluation to CIFAR-10, CIFAR-100, and TinyImageNet datasets without testing on larger-scale ImageNet or domain-specific tasks
- No explicit quantification of computational overhead reduction compared to iterative methods
- Missing comparison with recent one-shot pruning methods like PGB that could provide additional context for efficiency claims
- Insufficient exploration of hyperparameter sensitivity, particularly temperature scaling in CA-KLD loss

## Confidence

- **High Confidence**: The integration of knowledge distillation into the pruning process and the use of CA-KLD loss are well-justified and align with existing literature on model compression
- **Medium Confidence**: The performance claims on CIFAR-10, CIFAR-100, and TinyImageNet are supported by experimental results, but the lack of broader dataset evaluation introduces some uncertainty
- **Low Confidence**: The computational efficiency claims relative to iterative methods are plausible but lack detailed benchmarking to fully substantiate the assertion

## Next Checks

1. Evaluate the method on ImageNet to assess scalability and performance on larger-scale datasets
2. Conduct a systematic hyperparameter sensitivity analysis focusing on temperature scaling and pruning ratios
3. Provide detailed benchmarking of training time and resource usage compared to state-of-the-art iterative pruning approaches