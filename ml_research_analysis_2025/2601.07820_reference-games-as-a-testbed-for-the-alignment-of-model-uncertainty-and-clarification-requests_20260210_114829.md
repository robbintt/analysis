---
ver: rpa2
title: Reference Games as a Testbed for the Alignment of Model Uncertainty and Clarification
  Requests
arxiv_id: '2601.07820'
source_url: https://arxiv.org/abs/2601.07820
tags:
- clarification
- accuracy
- uncertainty
- confidence
- requests
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether vision-language models can recognize
  their own uncertainty and signal it through clarification requests in reference
  games. The authors use controlled reference games with color-grid images as a testbed
  to measure models' ability to identify when they are uncertain and ask appropriate
  questions.
---

# Reference Games as a Testbed for the Alignment of Model Uncertainty and Clarification Requests

## Quick Facts
- arXiv ID: 2601.07820
- Source URL: https://arxiv.org/abs/2601.07820
- Reference count: 13
- Primary result: Models struggle to align clarification requests with internal uncertainty in controlled reference games

## Executive Summary
This paper investigates whether vision-language models can recognize their own uncertainty and signal it through clarification requests during reference games. The authors use a controlled testbed with color-grid images where models must identify target cells based on human descriptions. The study evaluates three models (Qwen2.5-VL-7B, Qwen2.5-VL-72B, and GPT-5-mini) on both baseline reference resolution and clarification experiments where models are instructed to request clarification when uncertain.

The results reveal that even in this simple, controlled setting, models show poor alignment between their internal uncertainty levels and their clarification behavior. While GPT-5-mini generates clarification requests more frequently when uncertain and achieves higher accuracy, the Qwen models demonstrate little connection between uncertainty and clarification requests. Critically, across all models, clarification requests rarely improve performance, suggesting fundamental limitations in models' pragmatic competence and their ability to engage in interactive communication when facing uncertainty.

## Method Summary
The study employs a controlled reference game paradigm where models must identify target cells in a 4x4 color grid based on human-provided descriptions. The evaluation consists of two phases: a baseline reference resolution task and a clarification experiment. In the baseline task, models receive descriptions and select target cells, with their uncertainty measured through entropy in the probability distribution over grid cells. The clarification experiment instructs models to request clarification when uncertain, using entropy thresholds as a proxy for uncertainty. The authors evaluate three vision-language models (Qwen2.5-VL-7B, Qwen2.5-VL-72B, and GPT-5-mini) using multiple prompt templates and human-annotated correctness judgments to assess both task performance and the quality of clarification requests.

## Key Results
- Models show poor alignment between internal uncertainty levels and clarification request generation, with Spearman correlations indicating weak relationships
- GPT-5-mini demonstrates better uncertainty-clarification alignment than Qwen models but still with limited effectiveness
- Clarification requests rarely improve task performance across all models, suggesting models either fail to recognize genuine uncertainty or generate uninformative questions
- The study validates reference games as a valuable testbed for examining models' interactional abilities and pragmatic competence

## Why This Works (Mechanism)
Assumption: The paper likely proposes that reference games work as a testbed because they provide controlled, measurable environments where uncertainty and clarification behaviors can be quantified. The mechanism would involve using entropy as a measurable proxy for uncertainty and examining how this correlates with clarification request generation, though specific details are not provided in the summary.

## Foundational Learning
- **Reference game dynamics**: Understanding how speakers and listeners coordinate to identify referents through descriptions; needed to interpret the controlled experimental setup and measure model performance
- **Vision-language model uncertainty quantification**: How models represent and compute uncertainty over multimodal inputs; needed to measure alignment between uncertainty levels and clarification behavior
- **Clarification request pragmatics**: The linguistic and contextual factors that make clarification questions effective; needed to evaluate the quality and usefulness of model-generated clarification requests
- **Entropy as uncertainty proxy**: Using information-theoretic measures to quantify model confidence; needed to operationalize and measure model uncertainty in the reference game context
- **Prompt engineering for clarification**: How different instructions and examples affect models' willingness to request clarification; needed to understand the experimental design and model responses

## Architecture Onboarding

### Component Map
Vision input -> Image encoder -> Text encoder -> Cross-modal attention -> Probability distribution over grid cells -> Entropy calculation -> Uncertainty threshold -> Clarification decision -> Text generation

### Critical Path
1. Image encoding of color grid
- Text processing of human description
- Cross-modal fusion to compute cell probabilities
- Entropy calculation from probability distribution
- Threshold comparison to determine clarification need
- Text generation of clarification request or target selection

### Design Tradeoffs
- Controlled grid environment vs. ecological validity: The color-grid setup provides experimental control but limits generalizability to real-world visual complexity
- Entropy threshold vs. adaptive uncertainty: Fixed thresholds provide consistency but may not capture nuanced uncertainty patterns across different descriptions
- Human annotation vs. automated evaluation: Human judgments provide quality assessment but introduce subjectivity and scalability limitations
- Prompt engineering vs. model capability: Carefully crafted prompts may improve clarification behavior but don't address fundamental limitations in pragmatic competence

### Failure Signatures
- Low correlation between uncertainty and clarification requests indicates poor uncertainty recognition
- Clarification requests that don't reduce uncertainty or improve accuracy suggest ineffective question generation
- High accuracy without clarification requests may indicate overconfidence or luck rather than genuine understanding
- Inconsistent clarification behavior across similar uncertainty levels reveals unreliable uncertainty signaling

### Exactly 3 First Experiments
1. Vary entropy thresholds systematically to find optimal uncertainty-clarification alignment points across different models
2. Test few-shot prompting with examples of effective clarification requests to improve model performance
3. Evaluate models on more complex visual domains (natural images) to assess generalization beyond controlled color grids

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly call out open questions in the summary provided. Based on the results showing poor alignment between uncertainty and clarification behavior, potential open questions might include: how to improve models' pragmatic competence in uncertainty recognition, whether larger or different model architectures would perform better, and how these findings generalize to more complex visual domains.

## Limitations
- Controlled color-grid environment limits generalizability to complex real-world visual domains
- Human-annotated correctness judgments may introduce subjective bias in evaluation
- Small scale (three models, specific prompt templates) raises questions about broader applicability
- Spearman correlation may not fully capture the nuanced relationship between uncertainty and clarification behavior

## Confidence

### High confidence:
- Models struggle to align clarification requests with internal uncertainty levels in controlled reference games
- GPT-5-mini shows better performance than Qwen models but still with limited effectiveness
- Clarification requests rarely improve task performance across all models

### Medium confidence:
- The color-grid reference game serves as a valid testbed for measuring pragmatic competence in uncertainty recognition
- The observed patterns reflect fundamental limitations in models' interactional abilities rather than implementation artifacts

### Low confidence:
- The specific patterns observed would generalize to more complex visual domains
- Alternative prompting strategies could substantially improve alignment between uncertainty and clarification behavior
- The Spearman correlation adequately captures the relationship between uncertainty and clarification requests

## Next Checks
1. Replicate the study with more complex visual domains (natural images, videos) to test generalizability beyond controlled color-grid environments and assess whether models show improved uncertainty alignment in richer contexts

2. Conduct ablation studies testing different prompting strategies, including few-shot examples of effective clarification requests and explicit uncertainty thresholds, to determine whether implementation choices significantly impact model performance

3. Implement automated evaluation metrics for clarification quality (e.g., measuring whether clarification questions reduce entropy in the model's posterior distribution) to supplement human judgments and enable larger-scale validation across more models and domains