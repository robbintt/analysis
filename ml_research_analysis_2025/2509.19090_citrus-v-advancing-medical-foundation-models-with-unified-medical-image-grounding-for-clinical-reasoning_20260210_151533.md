---
ver: rpa2
title: 'Citrus-V: Advancing Medical Foundation Models with Unified Medical Image Grounding
  for Clinical Reasoning'
arxiv_id: '2509.19090'
source_url: https://arxiv.org/abs/2509.19090
tags:
- bbox
- medical
- reasoning
- data
- segmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces Citrus-V, a unified multimodal medical foundation\
  \ model that integrates detection, segmentation, and multimodal chain-of-thought\
  \ reasoning for comprehensive clinical reasoning. By leveraging dual image encoders,\
  \ a segmentation projector, and a novel training strategy, Citrus-V achieves state-of-the-art\
  \ performance across five major medical domains\u2014visual and textual QA, document\
  \ understanding, report generation, and image detection/segmentation."
---

# Citrus-V: Advancing Medical Foundation Models with Unified Medical Image Grounding for Clinical Reasoning

## Quick Facts
- **arXiv ID**: 2509.19090
- **Source URL**: https://arxiv.org/abs/2509.19090
- **Reference count**: 40
- **Primary result**: Unified multimodal medical foundation model achieving state-of-the-art performance across visual/textual QA, document understanding, report generation, and image detection/segmentation with pixel-level grounding and chain-of-thought reasoning

## Executive Summary
Citrus-V introduces a unified multimodal medical foundation model that integrates detection, segmentation, and multimodal chain-of-thought reasoning for comprehensive clinical reasoning. The model employs dual image encoders, a segmentation projector, and a novel four-stage training strategy to achieve state-of-the-art performance across five major medical domains. Extensive evaluations demonstrate Citrus-V surpasses both open-source and commercial models, matching or exceeding expert-level performance while providing precise lesion quantification, automated reporting, and reliable second opinions through physician-like diagnostic inference with high interpretability.

## Method Summary
Citrus-V builds on the Qwen2.5-VL backbone with SAM2 segmentation and a 2-layer MLP projector, employing a four-stage curriculum: (1) Concept Alignment for vision-language mapping, (2) Comprehension Enhancement for medical image interpretation, (3) Instruction Fine-tuning on diverse tasks with gradient scaling (0.001) for segmentation modules, and (4) Segmentation Fine-tuning with frozen MLLM. The model uses dual image encoders for task-specific feature extraction and a special [SEG] token projection mechanism to enable pixel-level grounding without adapter modules. Training leverages 16.8M public and 1.6M synthesized samples across medical domains.

## Key Results
- Achieves state-of-the-art performance across visual and textual QA, document understanding, report generation, and image detection/segmentation tasks
- Matches or exceeds expert-level performance on medical benchmarks, surpassing both open-source and commercial models
- Demonstrates strong generalization across five major medical domains with precise lesion quantification and automated reporting capabilities
- Provides reliable second opinions with physician-like diagnostic inference and high interpretability through chain-of-thought reasoning

## Why This Works (Mechanism)

### Mechanism 1: Unified Segmentation Integration via [SEG] Token Projection
Citrus-V enables pixel-level grounding within a language model framework by projecting the hidden state of a special [SEG] token into a segmentation prompt, eliminating the need for adapter modules like LoRA. During inference, when the MLLM outputs the [SEG] token, its hidden representation encodes the segmentation intent derived from the textual query. A 2-layer MLP projector maps this representation into a latent prompt compatible with SAM2. SAM2 then uses this prompt, along with image features from a dedicated encoder, to generate a pixel-level mask. This decouples reasoning from dense prediction while keeping the MLLM architecture clean.

### Mechanism 2: Dual Image Encoder Architecture for Task-Specific Feature Extraction
Performance across heterogeneous tasks (reasoning vs. segmentation) is maintained by using separate image encoders, one optimized for semantic context and one for spatial fidelity. One image encoder feeds the MLLM, providing high-level semantic features necessary for report generation and VQA. A second, separate encoder feeds the segmentation model (SAM2), preserving the low-level spatial details required for accurate delineation. This prevents the loss of pixel-level information that can occur when a single encoder must generalize across conflicting objectives.

### Mechanism 3: Four-Stage Curriculum for Progressive Modality Alignment
A staged training curriculum aligns visual and textual modalities and integrates segmentation progressively, ensuring stable convergence and preventing catastrophic forgetting. The model is trained in four phases: (1) Concept Alignment for vision-language mapping, (2) Comprehension Enhancement for medical image interpretation, (3) Instruction Fine-Tuning on diverse tasks with specific gradient scaling strategy for segmentation, and (4) Segmentation Fine-Tuning where the MLLM is frozen and only segmentation components are optimized. This isolates complex interactions until the model has stable foundational capabilities.

## Foundational Learning
- **Concept: Chain-of-Thought (CoT) Reasoning** - Why needed: Citrus-V uses multimodal CoT not just for final answers, but to generate interpretable diagnostic reports with explicit steps and bounding-box references, mimicking physician reasoning. Quick check: How does the model's CoT process for report generation differ from simply predicting the final "Impression" section?
- **Concept: Prompt-Driven Segmentation (e.g., SAM2)** - Why needed: The core of Citrus-V's grounding ability relies on SAM2, which generates masks based on "prompts." Understanding how text-derived latent prompts from the [SEG] token condition this process is critical. Quick check: In the Citrus-V architecture, what serves as the "prompt" that the segmentation model uses to identify the region of interest?
- **Concept: Gradient Scaling in Multi-Task Learning** - Why needed: The paper uses a specific technique (gradient scaling by 0.001) to balance the learning dynamics between the MLLM and the newly introduced segmentation components. Quick check: What is the potential consequence of applying full, unscaled gradients from the segmentation loss directly to the MLLM during Stage-3 training?

## Architecture Onboarding
- **Component map**: Image + Text Query -> Qwen2.5-VL (MLLM Backbone) -> [SEG] Token -> 2-layer MLP Projector -> Latent Prompt -> SAM2 (Segmentation Model) -> Binary Mask
- **Critical path**: Input: An image and text query (e.g., "segment the lesion") enter the model. Reasoning: The MLLM processes both and produces an output containing the [SEG] token. Projection: The [SEG] token's final hidden state is extracted and passed through the segmentation projector to create a latent prompt embedding. Segmentation: The SAM2 model uses this latent prompt and the image features from its own encoder to decode a pixel-level mask.
- **Design tradeoffs**: Unified vs. Adapter-Based: Using a unified model with a special token avoids parameter-efficient adapters (like LoRA) but requires careful gradient management to protect pre-trained reasoning capabilities. Dual Encoders: Increases model size and computational cost but is critical for ensuring both high-level semantic understanding and low-level spatial accuracy. Full-Parameter Fine-Tuning: The model uses full-parameter SFT for the MLLM instead of freezing it, allowing for deeper integration of medical knowledge but requiring a robust training pipeline.
- **Failure signatures**: Degraded VQA/Report Performance: Post-training, the model hallucinates or loses factual accuracy. This suggests negative transfer from segmentation training. Check: The gradient scaling factor in Stage-3. No Segmentation Output: The model generates text correctly but never produces a mask. Check: Ensure the [SEG] token is being predicted and that the segmentation projector weights have been updated in Stages 3 & 4. Inaccurate or Noisy Masks: The mask is generated but doesn't align with the query. Check: The quality of the latent prompt from the projector and ensure the SAM2 encoder was properly fine-tuned on medical data in Stage-4.
- **First 3 experiments**: Gradient Scaling Ablation: Retrain Stage-3 with different gradient scaling values (e.g., 0.01, 0.0001) or without scaling, and compare performance on a VQA benchmark vs. a segmentation benchmark to confirm the trade-off. Encoder Ablation: Train a version with a single shared image encoder (instead of dual) and evaluate performance on both report generation (CheXpert Plus) and segmentation (MeCOVQA-G+). CoT Grounding Analysis: Evaluate report generation quality (using hallucination metrics or expert review) between a model trained with the full CoT+grounding data versus a model trained only on final report texts.

## Open Questions the Paper Calls Out

### Open Question 1
Can the dual image encoder design with separate semantic and pixel-level encoders be further unified to reduce computational overhead while maintaining performance across reasoning and segmentation tasks? The paper demonstrates the approach works but does not explore whether a single unified encoder with adaptive attention mechanisms could achieve comparable results with fewer parameters.

### Open Question 2
Does the synthetic chain-of-thought reasoning data generated from CT projections generalize effectively to real-world clinical X-ray interpretation where findings may differ from the "God's-eye" CT view? While the paper argues CT-projection synthesis provides "noise-free" supervision, it does not evaluate whether models trained on this data develop spurious correlations or fail on edge cases where CT and X-ray findings genuinely differ.

### Open Question 3
To what extent does the four-stage training paradigm with gradient scaling (0.001 for segmentation modules during instruction fine-tuning) represent the optimal balance between preserving MLLM reasoning capabilities and developing segmentation proficiency? The 0.001 scaling factor appears chosen empirically without ablation across different values.

### Open Question 4
How does the model's performance on rare pathologies, uncommon imaging modalities (e.g., NIR, OCT), and underrepresented anatomical regions compare to its strong performance on common chest X-ray and CT tasks? The model's robustness across the full spectrum of clinical scenarios is critical for deployment but not thoroughly characterized.

## Limitations
- Heavy reliance on synthetic data augmentation raises concerns about domain shift and generalization to real clinical scenarios
- Gradient scaling factor (0.001) appears heuristic without systematic empirical justification
- Performance comparisons against commercial models based on reported numbers rather than direct benchmarking

## Confidence
- **State-of-the-art performance across five medical domains**: High confidence based on multiple benchmark evaluations
- **Pixel-level grounding via [SEG] token projection**: High confidence in mechanism, Medium confidence in universal applicability
- **Dual encoder architecture necessity**: Medium confidence; supported by design rationale but not thoroughly ablated
- **Four-stage training superiority**: High confidence in positive impact, Medium confidence in optimal configuration

## Next Checks
1. **Synthetic Data Dependency Analysis**: Retrain the model with only real data (no synthetic augmentation) and compare performance across all benchmarks to quantify the contribution of synthetic data to reported results.

2. **Gradient Scaling Sensitivity**: Systematically vary the gradient scaling factor (0.0001, 0.001, 0.01, 1.0) during Stage-3 training and measure the trade-off between VQA accuracy and segmentation quality to determine if 0.001 is optimal.

3. **Commercial Model Direct Benchmarking**: Conduct head-to-head evaluations of Citrus-V against GPT-4V and Gemini-1.5-Pro using identical prompts, evaluation criteria, and medical expert review to verify claimed performance advantages.