---
ver: rpa2
title: Accelerating LLM Inference with Precomputed Query Storage
arxiv_id: '2509.25919'
source_url: https://arxiv.org/abs/2509.25919
tags:
- inference
- queries
- query
- response
- precomputed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: StorInfer introduces a storage-assisted approach to accelerate
  LLM inference by precomputing and storing high-quality query-response pairs offline.
  When user queries match precomputed entries, responses are retrieved instantly instead
  of running full inference, reducing latency by up to 17.3% with no loss in quality.
---

# Accelerating LLM Inference with Precomputed Query Storage

## Quick Facts
- **arXiv ID**: 2509.25919
- **Source URL**: https://arxiv.org/abs/2509.25919
- **Reference count**: 30
- **Primary result**: 17.3% latency reduction with 22.5% hit rate on QA datasets

## Executive Summary
StorInfer accelerates LLM inference by precomputing and storing high-quality query-response pairs offline, enabling instant retrieval when semantic matches occur at runtime. The system uses adaptive query masking and sampling to generate diverse, deduplicated queries, then indexes them in a disk-backed vector database for fast retrieval. Parallel execution of vector search and inference ensures zero overhead on misses while achieving substantial latency reduction on hits. Evaluation across three QA datasets demonstrates the approach's effectiveness in predictable query environments.

## Method Summary
StorInfer introduces a storage-assisted approach where an offline generator creates 150K query-response pairs from 200 knowledge base documents per dataset using adaptive masking and sampling techniques. These pairs are embedded with all-MiniLM-L6-v2 and indexed via DiskANN with MIPS. At runtime, incoming queries trigger parallel vector search (CPU/storage) and LLM inference (GPU). When similarity exceeds threshold Sth_Run, the precomputed response returns immediately and inference cancels. The system achieves 17.3% latency reduction with 22.5% hit rate while maintaining response quality through careful threshold tuning.

## Key Results
- 17.3% latency reduction with 22.5% hit rate on SQuAD dataset
- Vector search achieves 8.6× speedup over full LLM inference (24ms vs 208ms average)
- Storage usage scales proportionally with hit rate: 830 MB for 150K pairs
- Quality preserved: BERTScore and ROUGE-L match LLaMA 3.1 8B baseline at Sth_Run=0.9

## Why This Works (Mechanism)

### Mechanism 1: Storage-Assisted Retrieval Bypass
Precomputed query-response pairs enable bypassing GPU inference when semantic matches exist. Queries are generated offline, embedded into vectors, indexed via DiskANN, and stored with response metadata. At runtime, incoming queries are matched against this index using Maximum Inner Product Search (MIPS). If similarity exceeds Sth_Run threshold, the stored response returns immediately, avoiding both prefill and decode phases. This works when query distributions are predictable enough that a bounded precomputed set achieves meaningful hit rates.

### Mechanism 2: Adaptive Query Deduplication for Coverage Efficiency
Adaptive masking and sampling produce semantically diverse queries with higher hit rates than naive generation. Adaptive Query Masking injects recently generated queries into the LLM context window within token budget, preventing regeneration. Adaptive Sampling dynamically raises temperature (0.7→1.0) when generated queries exceed Sth_Gen=0.99 similarity threshold with existing entries, promoting exploration. This deduplication strategy improves storage efficiency and retrieval precision without sacrificing coverage of the true query distribution.

### Mechanism 3: Parallel Execution with Speculative Inference Cancellation
Concurrent vector search (CPU/storage) and LLM inference (GPU) with early termination yields zero overhead on misses and sub-inference latency on hits. Upon query arrival, both branches initiate. Vector search completes in ~24ms. If a hit occurs (similarity ≥ Sth_Run), a termination signal cancels GPU inference mid-prefill or mid-decode. On miss, LLM inference completes normally—no added latency beyond baseline. This works when vector search and GPU inference do not contend for shared resources.

## Foundational Learning

- **LLM Inference Phases (Prefill vs. Decode)**: Why needed here - StorInfer bypasses both phases on cache hits; understanding this clarifies why prefix caching (prefill-only optimization) is insufficient. Quick check: "Which phase generates tokens sequentially using the KV cache?" (Answer: Decode)

- **Vector Similarity Search and MIPS**: Why needed here - Retrieval correctness depends on embedding quality, similarity metric, and ANN index; threshold tuning (Sth_Run) directly controls hit rate vs. response quality tradeoff. Quick check: "If Sth_Run=0.5 vs. 0.9, which yields higher hit rate but potentially lower response quality?" (Answer: 0.5)

- **Semantic Caching vs. Prefix Caching**: Why needed here - The paper contrasts StorInfer with prefix/KV caching; semantic matching enables non-identical query reuse but introduces quality risk. Quick check: "Why does prefix caching not reduce decode phase latency?" (Answer: Prefix caching reuses KV cache for prefill; decode must still generate tokens.)

## Architecture Onboarding

- **Component map**: Generator (Offline) -> LLM + knowledge base -> adaptive masking + sampling -> embedding model -> DiskANN index + response metadata -> storage (830 MB for 150K pairs). Runtime (Online) -> Query -> [parallel: (1) embedding -> DiskANN search; (2) vLLM inference on GPU] -> if hit (similarity ≥ Sth_Run): return cached response + send termination signal; if miss: return LLM output.

- **Critical path**: 1) Offline: Ensure knowledge base coverage matches deployment domain. 2) Offline: Run deduplicated generation to build precomputed set (0.3–0.6s per pair). 3) Online: Monitor hit rate and response quality; adjust Sth_Run if quality degrades.

- **Design tradeoffs**: Sth_Run (runtime threshold) - higher → fewer hits, better quality; lower → more hits, risk of mismatched responses. Storage vs. hit rate - diminishing returns; 150K pairs yield 22.5% hit rate on SQuAD. Precomputation compute cost - substantial upfront investment amortized over deployment lifetime.

- **Failure signatures**: Low hit rate in production - query distribution diverges from precomputed set; consider online cache augmentation. Response quality degradation - Sth_Run set too low; semantic matches return tangentially related answers. Inference not terminating on hit - termination signal not propagated to vLLM. Storage/index corruption - DiskANN index fails to load; implement checksums and backup regeneration pipeline.

- **First 3 experiments**: 1) Threshold sweep on held-out queries - vary Sth_Run (0.5–0.95) on validation split; plot hit rate vs. BERTScore/ROUGE-L to find operating point. 2) Domain shift robustness test - train precomputed set on one dataset (e.g., SQuAD), evaluate hit rate on another (e.g., NarrativeQA); quantify coverage gap. 3) Parallelism overhead measurement - benchmark end-to-end latency with parallel execution enabled vs. sequential on query misses; confirm no statistically significant overhead.

## Open Questions the Paper Calls Out
- **Generalization to non-QA tasks**: How does StorInfer generalize to code generation, summarization, or dialogue? The introduction lists diverse LLM applications but evaluation is restricted to three QA datasets. Query predictability and response semantics differ substantially across task types.

- **Maintenance of precomputed databases**: How should the precomputed query database be maintained when knowledge bases evolve or query distributions shift over time? The system assumes a static knowledge base with no discussion of incremental updates, staleness detection, or handling distribution drift.

- **Quantifying predictable query environments**: What metric can quantify "predictable query environments" to determine StorInfer's suitability for a given deployment? The paper states the method is "especially effective in predictable query environments" but provides no formal definition or measurement of predictability.

## Limitations
- Effectiveness depends critically on query distribution predictability, which may not generalize beyond QA tasks
- Significant upfront computational cost for generating 150K pairs at 0.3-0.6s each
- Storage and index infrastructure requirements may be prohibitive for resource-constrained edge deployments

## Confidence
- **High Confidence**: Parallel execution mechanism with speculative cancellation is technically sound and well-validated through measured 8.6× speedup
- **Medium Confidence**: Adaptive query deduplication effectively improves hit rates but generalizability to different query distributions remains uncertain  
- **Medium Confidence**: Latency reduction claims are well-supported within tested QA datasets but practical impact in production environments is less certain

## Next Checks
1. **Domain Shift Robustness Test**: Generate precomputed query sets on one domain (e.g., SQuAD) and evaluate hit rates on a different domain (e.g., open-domain customer support queries) to quantify coverage limitations when query distributions diverge from training data.

2. **Resource Contention Analysis**: Conduct controlled experiments measuring memory bandwidth and PCIe utilization during parallel vector search and GPU inference execution to verify the claimed absence of resource contention, particularly under multi-tenant deployment scenarios.

3. **Quality Degradation Threshold Analysis**: Systematically vary Sth_Run from 0.5 to 0.95 on held-out queries and measure the precise relationship between hit rate, response quality (BERTScore/ROUGE-L), and end-to-end latency to establish safe operating boundaries for production deployment.