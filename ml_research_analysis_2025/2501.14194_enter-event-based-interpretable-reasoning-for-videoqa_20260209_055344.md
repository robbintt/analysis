---
ver: rpa2
title: 'ENTER: Event Based Interpretable Reasoning for VideoQA'
arxiv_id: '2501.14194'
source_url: https://arxiv.org/abs/2501.14194
tags:
- event
- graph
- node
- question
- events
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ENTER introduces an interpretable VideoQA system that uses event
  graphs to represent video content and generates Python code for reasoning. The event
  graph nodes represent video events, with edges capturing temporal, causal, and hierarchical
  relationships.
---

# ENTER: Event Based Interpretable Reasoning for VideoQA

## Quick Facts
- arXiv ID: 2501.14194
- Source URL: https://arxiv.org/abs/2501.14194
- Reference count: 40
- Outperforms prior modular approaches while maintaining interpretability

## Executive Summary
ENTER introduces an interpretable VideoQA system that uses event graphs to represent video content and generates Python code for reasoning. The event graph nodes represent video events, with edges capturing temporal, causal, and hierarchical relationships. ENTER integrates visual context into the reasoning process, enabling interpretable and contextually aware predictions. A hierarchical iterative update mechanism refines the event graph by adding denser captions, graphs, or multimodal edges when necessary, improving robustness and accuracy. Experiments on NExT-QA, IntentQA, and EgoSchema show that ENTER outperforms prior modular approaches while maintaining interpretability, achieving competitive or superior performance compared to bottom-up methods.

## Method Summary
ENTER is a zero-shot VideoQA pipeline that converts videos into structured event graphs through a series of LLM and VLM calls. First, Gemini 1.5 Flash generates dense captions emphasizing event relationships (temporal, causal, hierarchical) with consistent entity coreference. An LLM then parses these captions into a graph where nodes represent events and edges capture relationships. For answering questions, another LLM generates Python code that queries this graph using a provided API. If the code detects missing information, a hierarchical iterative update mechanism triggers: first attempting to extract more detail from existing captions, then re-processing the video for missed events, and finally incorporating multimodal video clip data as a last resort.

## Key Results
- ENTER achieves 75.1% accuracy on NExT-QA (4,996 questions, 570 videos) with all update stages
- Outperforms prior modular approaches while maintaining interpretability
- Competitive or superior performance compared to bottom-up methods on multiple benchmarks
- Iterative updates improve accuracy from 63.8% (no update) to 75.1% (all stages)

## Why This Works (Mechanism)

### Mechanism 1: Intermediate Structured Representation
Converting video frames into a structured event graph allows a reasoning system to access rich visual context without processing raw pixels, improving both accuracy and transparency. A Vision Language Model (VLM) generates a dense textual caption that emphasizes event relationships (temporal, causal, hierarchical). An LLM then parses this caption into a graph where nodes are events and edges are typed relations. Code generated to answer a question queries this graph rather than the raw video. The core assumption is that the VLM captioning step captures the salient entities and their relationships, and the LLM can reliably structure this information into a graph that preserves these semantics. If the initial caption misses a critical event or misidentifies a relationship, the resulting graph will be incomplete or incorrect, potentially leading to wrong answers if not caught by the update mechanism.

### Mechanism 2: Context-Aware Code Generation
Generating executable Python code that is conditioned on both the question and the content of the event graph makes the reasoning process transparent and adaptable to the specific visual context. An LLM is provided with the question, multiple-choice answers, the string representation of the relevant event graph, and an API definition (e.g., `EventGraph`, `find_node`, `get_temporal_after`). It generates Python code (e.g., `def execute_command(graph): ...`) that uses this API to traverse the graph, collect relevant info, and select an answer. This differs from top-down approaches that plan based on the question alone. The core assumption is that the provided API is sufficiently expressive to capture the necessary reasoning logic, and the LLM can reliably generate syntactically and logically correct code to implement it. The generated code may contain syntax errors, call non-existent API functions, or implement flawed logic (e.g., traversing the wrong edge type), leading to execution failure or incorrect answers.

### Mechanism 3: Hierarchical Iterative Graph Update for Robustness
A multi-stage, self-correcting update process allows the system to recover from initial information gaps in the event graph, trading compute for higher accuracy on difficult queries. If the generated code detects missing information (e.g., an empty result list), it triggers an update. This proceeds hierarchically: 1) Re-parse existing captions for missed details (`generate_denser_graph`). 2) Re-process the video for missed events (`generate_denser_caption`). 3) Incorporate multimodal (video clip) data directly into the graph as a last resort (`generate_multimodal_edges`). The core assumption is that the system can reliably detect when information is missing, and the cost of re-sampling or multimodal processing is justified by the gain in accuracy. If the "missing info" check (`sufficient(info)`) is flawed, the system might trigger costly updates unnecessarily or fail to trigger them when needed, leading to inefficiency or persistent errors.

## Foundational Learning

**Concept: Event Graph Construction & Traversal**
- Why needed here: This is the core data structure. You must understand how a video is converted into nodes (events) and directed edges (temporal, causal, hierarchical relations) to interpret the system's reasoning.
- Quick check question: Given a caption "The man in the hat threw the ball, causing the window to break," what are the likely nodes and what type of edge would connect them?

**Concept: Program-of-Thoughts / Code-as-Reasoning**
- Why needed here: The system's reasoning isn't a direct text output but an *executable program*. You need to read Python code to understand the reasoning steps (e.g., loops, API calls like `get_temporal_after`) and debug failures.
- Quick check question: How does using an executable program for reasoning differ from a standard Chain-of-Thought prompt in terms of transparency and verifiability?

**Concept: Multimodal Iterative Refinement**
- Why needed here: The system isn't single-pass. It has a self-correction loop. Understanding the trigger conditions (e.g., an empty result) and the hierarchy of refinement actions (re-graph vs. re-caption vs. multimodal) is key to understanding its performance and cost.
- Quick check question: What is the intended order of operations in the update mechanism, and what is the rationale for this specific ordering?

## Architecture Onboarding

**Component map:**
Video -> Caption (VLM) -> Graph (LLM) -> (Question + Graph) -> Code (LLM) -> Execution (Python) -> Answer. Iterative Update: Code -> Update Check -> (Denser Graph / Denser Caption / Multimodal Edges)

**Critical path:**
Video -> Caption -> Graph -> (Question + Graph) -> Code -> Execution -> Answer.
The most critical step is the initial **Caption & Graph Generation**, as all subsequent reasoning is based on this structured representation. Errors here cascade unless caught by the update loop.

**Design tradeoffs:**
- **Interpretability vs. Simplicity:** The graph and code make reasoning steps explicit but add complexity compared to an end-to-end model.
- **Accuracy vs. Compute Cost:** The iterative update mechanism improves accuracy but can trigger expensive re-captioning or multimodal processing. The hierarchical design aims to minimize this cost by trying cheaper fixes first.
- **Text-based Graph vs. Raw Visuals:** Relying on a textual graph abstracts away pixel-level details, which may be missed unless the multimodal update is triggered.

**Failure signatures:**
- **Insufficient Knowledge:** The final answer is wrong because the VLM failed to capture a crucial detail (e.g., misidentifying an object) in the initial caption. Error analysis traces this back to the graph.
- **Inconsistent Referencing:** The graph contains two nodes (e.g., "Boy", "Young Boy") that refer to the same entity, leading to incorrect counting or logic.
- **Missed Information:** The graph is structurally sound but misses a key causal edge or event, leading the code to retrieve an empty list and trigger an update. If the update fails, the answer is wrong.
- **Code Execution Error:** The generated code has syntax errors or logic bugs (e.g., infinite loop, incorrect API usage), causing a crash.

**First 3 experiments:**
1. **Validate the Graph Generation Pipeline:** Input a short video with clear causal events (e.g., a person dropping a glass). Run the Captioner and Graph Generator. Manually inspect the resulting event graph for correctness (nodes, edges, entity references).
2. **Test Code Generation on a Fixed Graph:** Provide a pre-built event graph and a simple question to the Code Generator. Execute the output. Verify that the generated code correctly uses the API (e.g., `find_node`, `get_temporal_after`) to retrieve the right information.
3. **Trigger the Update Mechanism:** Input a video and a complex question where the initial caption is likely to miss a detail. Provide an initial graph that is intentionally incomplete. Run the full pipeline and trace the execution to confirm that the `sufficient(info)` check fails, and the appropriate update function (e.g., `generate_denser_caption`) is called.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ENTER effectively mitigate "Inconsistent Referencing" errors by integrating a dedicated visual object tracker to maintain entity identity across the event graph?
- Basis in paper: [explicit] The error analysis (Figure 9) identifies "Inconsistent Referencing" as a critical failure mode where the model mislabels the same entity differently (e.g., "Boy" vs. "Young boy"), leading to incorrect counting.
- Why unresolved: The current system relies solely on textual prompting to maintain consistency, which proves brittle when visual appearance changes or varies across frames.
- What evidence would resolve it: A reduction in counting-related errors on datasets like NExT-QA when a visual tracker is used to ground entity nodes.

### Open Question 2
- Question: How can the event graph representation be adapted to close the performance gap with bottom-up methods on descriptive, long-form video tasks?
- Basis in paper: [explicit] The results section notes that while ENTER excels at causal/temporal reasoning (NExT-QA), it lags behind bottom-up approaches like VideoINSTA on the EgoSchema dataset, which focuses heavily on long-form description.
- Why unresolved: The event graph's structured, node-based format may struggle to capture the dense, unstructured visual details required for pure description compared to retrieval-based or end-to-end models.
- What evidence would resolve it: Modified graph structures that incorporate dense visual features into nodes, showing improved accuracy on the EgoSchema benchmark.

### Open Question 3
- Question: Is the computational cost of the "Hierarchical Iterative Update" (Fig 2) justified by the performance gain, or could a learned policy optimize the decision to retry?
- Basis in paper: [inferred] The paper highlights a "Cost Flow" and a hierarchical update mechanism (Denser Graph -> Multimodal Edges), but relies on a fixed, heuristic approach to triggering these updates.
- Why unresolved: It is unclear if the latency introduced by repeatedly calling VLMs/LLMs for "Denser Captions" is efficient compared to a single-pass approach, or if a dynamic policy could skip unnecessary iterations.
- What evidence would resolve it: A comparative analysis of accuracy vs. inference time (latency) between the current fixed update loop and a policy-based controller.

## Limitations
- Relies heavily on initial caption quality; errors in event detection or relationship identification propagate through the graph
- Computational overhead from iterative updates, particularly the multimodal edge generation stage
- Performance gap on descriptive, long-form video tasks compared to retrieval-based methods

## Confidence
- **High Confidence:** The core mechanism of using event graphs and generated Python code for interpretable reasoning is well-defined and the paper provides sufficient detail for understanding the pipeline.
- **Medium Confidence:** The effectiveness of the hierarchical iterative update mechanism is demonstrated empirically, but the specific implementation details of the update triggers and multimodal component are underspecified.
- **Low Confidence:** The exact conditions under which the system fails (beyond the examples given) and the precise cost-benefit tradeoff of the update mechanism are not fully explored.

## Next Checks
1. **Test Graph Generation Sensitivity:** Input videos with ambiguous entities or subtle relationships to the Captioner and Graph Generator. Manually inspect the resulting event graphs to identify patterns of failure (e.g., inconsistent entity coreference, missed causal edges) and assess whether the update mechanism reliably corrects them.
2. **Stress Test the Update Trigger:** Design test cases where the initial graph is missing a key piece of information. Verify that the `sufficient(info)` check correctly identifies the gap and triggers the appropriate update stage (denser graph, denser caption, or multimodal).
3. **Benchmark Update Cost vs. Accuracy:** Measure the computational cost (API calls, processing time) of each update stage across a range of questions. Compare the accuracy gains from each stage to quantify the efficiency of the hierarchical design and identify scenarios where the cost is justified.