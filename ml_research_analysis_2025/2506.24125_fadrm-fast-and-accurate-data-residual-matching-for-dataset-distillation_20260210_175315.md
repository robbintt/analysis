---
ver: rpa2
title: 'FADRM: Fast and Accurate Data Residual Matching for Dataset Distillation'
arxiv_id: '2506.24125'
source_url: https://arxiv.org/abs/2506.24125
tags:
- dataset
- fadrm
- data
- distillation
- residual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FADRM, a novel dataset distillation method
  that employs data-level residual connections to mitigate information vanishing during
  synthetic data generation. By integrating adjustable residual connections with multi-resolution
  optimization and mixed-precision training, FADRM achieves a 50% reduction in training
  time and GPU memory usage while maintaining or improving accuracy.
---

# FADRM: Fast and Accurate Data Residual Matching for Dataset Distillation

## Quick Facts
- **arXiv ID:** 2506.24125
- **Source URL:** https://arxiv.org/abs/2506.24125
- **Reference count:** 40
- **Primary result:** Achieves 50.0% accuracy on ImageNet-1K (IPC=10, 0.8% compression ratio), surpassing prior methods by 1.4–5.7% while reducing training time and GPU memory usage by 50%

## Executive Summary
FADRM introduces a novel dataset distillation method that employs data-level residual connections to mitigate information vanishing during synthetic data generation. By integrating adjustable residual connections with multi-resolution optimization and mixed-precision training, FADRM achieves state-of-the-art performance on CIFAR-100, Tiny-ImageNet, and ImageNet-1K while significantly reducing computational requirements. The method demonstrates strong cross-architecture generalization and robust performance in continual learning settings, making it a practical solution for resource-constrained applications.

## Method Summary
FADRM optimizes synthetic images through a novel combination of adjustable residual connections, multi-resolution optimization, and mixed-precision training. The method iteratively refines synthetic data by alternating between low-resolution optimization phases (for efficiency) and high-resolution recovery phases, while preserving original dataset information through data-level skip connections. BatchNorm statistics matching ensures the distilled dataset maintains the global distribution characteristics of the original data. The approach achieves 50% reduction in training time and GPU memory usage while maintaining or improving accuracy compared to existing methods.

## Key Results
- Achieves 50.0% accuracy on ImageNet-1K (IPC=10, 0.8% compression ratio), outperforming prior methods by 1.4–5.7%
- Reduces training time and GPU memory usage by 50% compared to state-of-the-art methods
- Demonstrates strong cross-architecture generalization, with ResNet-18 distilled data achieving comparable accuracy on ResNet-50, MobileNetV2, and Swin-Tiny
- Shows robust performance in continual learning settings, surpassing prior methods by 2.5–3.6% on average across datasets

## Why This Works (Mechanism)

### Mechanism 1: Adjustable Residual Connection (ARC)
Data-level skip connections prevent information vanishing during synthetic data optimization by preserving original dataset features. At regular intervals, intermediate optimized images are merged with resized original patches via: ˜xt = α˜xt + (1−α)Resample(Ps, Dt). This balances newly acquired pixel-space knowledge with core local information from raw data. Optimal performance occurs at α=0.5, with information density recovery demonstrated in Figure 3.

### Mechanism 2: Multi-Resolution Optimization (MRO)
Alternating between downsampled and original resolutions reduces computational cost proportionally to (Dds/Dorig)² while recovering lost details via subsequent upsampling. Images start at low resolution, undergo optimization, upsample to original resolution for refinement, repeat. Optimal Dds=200 (vs. 224 original) achieves best accuracy-speed tradeoff, with Dds too small causing irreversible information loss.

### Mechanism 3: Mixed Precision Training (MPT)
Converting model parameters and forward pass to FP16 while retaining loss gradients and distribution matching in FP32 reduces memory/time ~50% without accuracy degradation. FP16 is used for logits and cross-entropy; FP32 preserves numerical stability for BatchNorm divergence computation and gradient updates. Table 4 confirms 47.7%→47.8% accuracy (no degradation) with 5.3GB→2.9GB memory reduction.

## Foundational Learning

- **Concept: Dataset Distillation Objective**
  - Why needed here: FADRM optimizes synthetic data ˜x to minimize performance gap vs. original training; understanding Eq. 1–2 is prerequisite.
  - Quick check question: Can you explain why |C| ≪ |O| matters for the mutual information bound in Theorem 1?

- **Concept: Residual Connections (Model vs. Data Level)**
  - Why needed here: FADRM extends ResNet-style skip connections from feature maps to raw pixels; distinguishing gradient flow vs. information preservation is essential.
  - Quick check question: What problem does a residual connection solve at the model level, and how does this translate to data-level information vanishing?

- **Concept: BatchNorm Statistics Matching**
  - Why needed here: The optimization objective (Eq. 52–53) aligns synthetic data with pretrained model's running mean/variance; intuition for Dglobal is required.
  - Quick check question: Why does matching BNRM and BNRV preserve "global information" about the original dataset distribution?

## Architecture Onboarding

- **Component map:** Original patches Ps → Resample (Downsample) → Data Residual Blocks (×k) → Final Recovery Phase → Distilled image ˜xB

- **Critical path:**
  1. Initialize: ˜x0 ← Resample(Ps, Dds)
  2. Loop k times: Optimize for b steps → Resample to alternate resolution → Apply Eq. 10 (ARC merge)
  3. Final: Optimize remaining B−kb steps at Dorig
  4. Key hyperparameters: α=0.5, k=3, Dds=200 (for ImageNet-scale), B=2000 total iterations

- **Design tradeoffs:**
  - Higher k (more residual injections): Better local detail preservation but risk of redundant local over global structures (Table 6 Left shows k>3 degrades)
  - Smaller Dds: Greater speedup but irreversible information loss if too aggressive
  - Smaller α: More original data influence → better information density but less optimization benefit

- **Failure signatures:**
  - Blurred/distorted outputs: α too high (≥0.9) or k insufficient → information vanishing not mitigated
  - Excessive training time: Dds too close to Dorig → MRO provides no benefit
  - Numerical instability: MPT applied to KL-divergence without FP32 retention → NaN losses
  - Poor cross-architecture generalization: Overfitting to teacher model; verify with Table 2 protocol

- **First 3 experiments:**
  1. Ablate α on CIFAR-100 IPC=10: Sweep α∈{0.4,0.5,0.6,0.7,0.8,0.9,1.0} with k=3 fixed; expect optimal at 0.5 per Table 5
  2. Verify MPT stability: Run with/without FP16 on ImageNet-1K subset; confirm <0.5% accuracy variance and ~50% memory reduction per Table 4
  3. Cross-architecture validation: Distill with ResNet-18 teacher; evaluate student on ResNet-50, MobileNetV2, Swin-Tiny per Table 2 (Right) to confirm generalization

## Open Questions the Paper Calls Out
None

## Limitations
- Data-level residual mechanism lacks independent external validation, relying on internal ablation studies
- ImageNet-1K state-of-the-art claims face uncertainty due to comparison with unpublished methods and limited standard benchmarks
- Mixed-precision training numerical stability claims are based on marginal accuracy differences (47.7%→47.8%) that may not hold in all scenarios

## Confidence
- **High Confidence**: Multi-resolution optimization (MRO) efficiency claims (50% time/memory reduction)
- **Medium Confidence**: ARC mechanism effectiveness and cross-architecture generalization
- **Low Confidence**: ImageNet-1K state-of-the-art claims due to comparison methodology limitations

## Next Checks
1. **ARC Mechanism Isolation**: Create synthetic datasets with known local/global structure patterns; test whether α=0.5 consistently recovers lost local information across varying optimization trajectories

2. **MPT Numerical Stability**: Run ImageNet-1K distillation with and without MPT across multiple random seeds; verify that FP16 quantization doesn't introduce variance >0.5% accuracy in 95% of runs

3. **Distribution Matching Verification**: Extract and visualize the BatchNorm running mean/variance distributions for distilled vs. original datasets; quantify KL-divergence changes across training iterations to validate the distribution matching objective