---
ver: rpa2
title: 'The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use
  Agents'
arxiv_id: '2601.07264'
source_url: https://arxiv.org/abs/2601.07264
tags:
- agents
- calibration
- confidence
- tools
- tool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates the calibration of LLM-based tool-use agents,
  revealing a fundamental "confidence dichotomy" driven by tool type: evidence tools
  (e.g., web search) induce severe overconfidence due to noisy retrieval, while verification
  tools (e.g., code interpreters) ground reasoning via deterministic feedback. To
  address this, the authors propose the Calibration Agentic RL (CAR) framework, which
  fine-tunes agents via RL to jointly optimize task accuracy and calibration reliability,
  supported by a novel Margin-Separated Calibration Reward (MSCR) that strictly separates
  incentives for correct and incorrect predictions.'
---

# The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents

## Quick Facts
- arXiv ID: 2601.07264
- Source URL: https://arxiv.org/abs/2601.07264
- Reference count: 11
- Primary result: Proposed CAR framework reduces Expected Calibration Error (ECE) by up to 68% in LLM-based tool-use agents while maintaining accuracy.

## Executive Summary
This paper investigates calibration in LLM-based tool-use agents, revealing a fundamental "confidence dichotomy" driven by tool type. Evidence tools (e.g., web search) induce severe overconfidence due to noisy retrieval without answer-level feedback, while verification tools (e.g., code interpreters) ground reasoning through deterministic execution errors. To address this, the authors propose the Calibration Agentic RL (CAR) framework, which fine-tunes agents via RL to jointly optimize task accuracy and calibration reliability. CAR significantly reduces Expected Calibration Error (ECE) by up to 68% compared to baselines while maintaining competitive accuracy, and demonstrates robust generalization from local to noisy web environments and to tool-integrated mathematical reasoning.

## Method Summary
The CAR framework fine-tunes LLM-based tool-use agents via RL to jointly optimize task accuracy and calibration reliability. Agents output confidence scores in `<confidence>` XML tags alongside reasoning and tool calls. The novel Margin-Separated Calibration Reward (MSCR) strictly separates incentives for correct and incorrect predictions, avoiding reward overlap that causes accuracy collapse in Brier-score-based methods. Training uses GRPO (Group Relative Policy Optimization) on NQ and HotpotQA datasets with a 2018 Wikipedia dump retriever, evaluating on both in-domain and out-of-domain benchmarks.

## Key Results
- CAR reduces Expected Calibration Error (ECE) by up to 68% compared to baselines
- Maintains competitive accuracy while improving calibration, avoiding the accuracy collapse seen with standard Brier-score rewards
- Demonstrates robust generalization from controlled local Wikipedia retrieval to noisy real-world web environments (Serper API)
- Shows AUROC improvements up to 17%, indicating genuinely improved ability to distinguish correct from incorrect outputs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Evidence tools (e.g., web search) systematically induce overconfidence, while verification tools (e.g., code interpreters) mitigate miscalibration through deterministic feedback.
- **Mechanism:** The asymmetry lies in feedback signals. Evidence tools return results regardless of relevance or accuracy—no answer-level correctness signal exists. Agents conflate retrieval *presence* with answer *correctness*. Verification tools provide observable failure modes (syntax errors, runtime exceptions), creating partial grounding even when logical errors persist.
- **Core assumption:** Tool-induced calibration dynamics are driven primarily by feedback signal structure rather than model architecture.
- **Evidence anchors:**
  - [abstract] "evidence tools (e.g., web search) systematically induce severe overconfidence due to inherent noise in retrieved information, while verification tools (e.g., code interpreters) can ground reasoning through deterministic feedback"
  - [Section 6] "The absence of negative feedback leads agents to conflate the presence of retrieved information with the correctness of their answer"
  - [corpus] Related work on calibration (CCPS, "Mind the Confidence Gap") confirms overconfidence is a widespread LLM issue but does not address tool-type heterogeneity.
- **Break condition:** If a retrieval system provides explicit relevance/reliability scores that agents are trained to interpret, the evidence-tool overconfidence effect may diminish.

### Mechanism 2
- **Claim:** The Margin-Separated Calibration Reward (MSCR) stabilizes RL training by eliminating reward overlap between correct and incorrect trajectories.
- **Mechanism:** Standard Brier-score-based rewards allow the lowest reward for correct answers to equal the highest reward for incorrect ones, creating incentive ambiguity. MSCR enforces strict separation: correct predictions receive base reward ≥1; incorrect predictions receive ≤0 with penalties for false confidence. This removes the "safe failure" loophole where low-confidence wrong answers are not meaningfully distinguished from correct ones.
- **Core assumption:** Calibration learning requires preserving a correctness margin while shaping confidence within each outcome region.
- **Evidence anchors:**
  - [Section 4.1.2] "This formulation decouples calibration terms for correct and incorrect predictions to guarantee a strict reward margin"
  - [Section 5.1] "MSCR achieves a superior accuracy-calibration trade-off... strict reward separation is essential for robust calibration training"
  - [corpus] RLCR (Damani et al., 2025) showed binary correctness rewards degrade calibration; MSCR extends this principle to multi-step tool-use settings.
- **Break condition:** If the reward margin (β₁, β₂) is set too small relative to format penalties, the separation effect weakens and training may revert to instability.

### Mechanism 3
- **Claim:** Calibration-aware RL generalizes from controlled local environments to noisy real-world settings because it induces genuine confidence reasoning, not superficial score rescaling.
- **Mechanism:** The joint optimization (task accuracy + calibration) forces the model to internalize uncertainty estimation as part of its policy. Temperature scaling or post-hoc methods adjust probabilities without changing underlying representations. AUROC improvements under CAR indicate the model learns to rank correct vs. incorrect outputs more reliably.
- **Core assumption:** The calibration capability learned is a transferable skill, not dataset-specific artifact.
- **Evidence anchors:**
  - [Section 5.1] "AUROC relative improvements of up to 17% in our best setting, confirming that the model has genuinely improved its ability to distinguish correct from incorrect outputs"
  - [Section 5.2] Table 3 shows CAR maintains calibration advantages when switching from local Wikipedia dump to Serper API
  - [corpus] No corpus papers directly test cross-environment generalization of calibration in tool-use agents.
- **Break condition:** If the target environment introduces qualitatively different noise patterns (e.g., adversarial retrieval poisoning), learned calibration may not transfer without domain-specific fine-tuning.

## Foundational Learning

- **Concept: Expected Calibration Error (ECE)**
  - **Why needed here:** The primary metric for evaluating whether confidence reflects actual performance. ECE bins predictions by confidence and measures the accuracy-confidence gap.
  - **Quick check question:** If a model outputs 80% confidence on average but only achieves 60% accuracy, is it overconfident or underconfident?

- **Concept: Brier Score**
  - **Why needed here:** Captures both calibration and refinement. Used in the Weighted Brier Score reward variant. Understanding why it has incentive overlap is critical for appreciating MSCR's design.
  - **Quick check question:** What happens to the Brier score when a wrong prediction has 0% confidence? Does this reward structure create a "safe failure" incentive?

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** The underlying RL algorithm for policy optimization. Understanding how GRPO handles multi-turn trajectories with sparse rewards is essential for debugging training instability.
  - **Quick check question:** How does GRPO differ from standard PPO in terms of advantage estimation?

## Architecture Onboarding

- **Component map:**
  - Agent Backbone (Qwen2.5-3B/7B-Instruct) -> Tool Environment (retrieval/execution) -> Reward Module (MSCR/Weighted Brier + format) -> GRPO Trainer

- **Critical path:**
  1. Prompt engineering to enforce `<confidence>` tag output
  2. Rollout generation via tool-interaction trajectories
  3. Reward computation (format check → calibration scoring)
  4. Policy update via GRPO with correctness-confidence joint objective
  5. Evaluation on ID (NQ/HotpotQA) and OOD (SimpleQA-verified) benchmarks

- **Design tradeoffs:**
  - Weighted Brier (λ=1) vs. MSCR: Brier yields lowest ECE but suffers accuracy collapse (reward hacking). MSCR trades slight calibration for robust accuracy
  - Local vs. API retriever: Training on local Wikipedia enables controlled experiments; real-world deployment requires Serper API which introduces stochastic retrieval noise
  - Model scale: 3B–7B range tested; scaling behavior unknown. Smaller models may struggle with verbalized confidence coherence

- **Failure signatures:**
  - Confidence collapse: All predictions converge to similar confidence values (check AUROC drop)
  - Reward hacking with Weighted Brier (λ=1): Accuracy drops sharply while ECE improves artificially
  - Format violation loops: Model fails to produce `<confidence>` tags; fallback reward applies but learning signal degrades

- **First 3 experiments:**
  1. Reproduce pilot study dichotomy: Run Direct Prompting, Prompting-based Tool-Use, and RL-based Tool-Use on NQ/HotpotQA (evidence) and AIME/MATH-500 (verification). Compute MCIP to confirm opposing calibration effects.
  2. Ablate reward structures: Compare Vanilla Search-R1, Temperature Scaling, MASH, Weighted Brier (λ=1, λ=1/3), and MSCR. Plot accuracy vs. ECE tradeoff curve.
  3. Test cross-environment generalization: Train on local Wikipedia retriever, evaluate on Serper API. Measure ECE and AUROC gap to verify calibration robustness under distribution shift.

## Open Questions the Paper Calls Out

- **Question:** Does the "confidence dichotomy" between evidence and verification tools persist, diminish, or shift as the parameter scale of the underlying language models increases beyond 7B?
- **Basis in paper:** [explicit] The authors explicitly state in the Limitations section: "it remains unclear how this phenomenon evolves with scale" because experiments were restricted to 3B–7B models due to computational constraints.
- **Why unresolved:** Larger models may possess superior internal mechanisms to distinguish noise from signal in evidence tools, potentially reducing the severity of overconfidence without explicit calibration training.
- **What evidence would resolve it:** Replicating the pilot study and CAR training using models with 70B+ parameters to observe if the overconfidence gap between search and code tools narrows.

- **Question:** How do calibration dynamics and the CAR framework perform in open-ended generation scenarios where "correctness" is subjective or ill-defined?
- **Basis in paper:** [explicit] The paper notes that evaluation focused on short-answer QA and math reasoning, whereas "Calibration behavior in more open-ended generation scenarios... may involve underspecified correctness signals... that our current framework does not address."
- **Why unresolved:** Reward mechanisms like MSCR rely on exact match or deterministic execution to separate correct from incorrect predictions, a signal that is absent in tasks like report writing or creative planning.
- **What evidence would resolve it:** An extension of CAR that utilizes model-based or human-feedback reward models for correctness in generative tasks, analyzed for calibration reliability.

- **Question:** Can the overconfidence induced by evidence tools be mitigated by modifying the retrieval environment to provide explicit negative feedback (failure signals) rather than always returning stochastic results?
- **Basis in paper:** [inferred] The Discussion hypothesizes that evidence tools cause overconfidence because "evidence tools offer little answer-level correctness feedback," unlike verification tools which provide execution errors.
- **Why unresolved:** While the paper proposes a training-side solution (CAR), it leaves unexplored whether the root cause is the *absence of negative feedback* in the tool itself, which could be fixed by environment design.
- **What evidence would resolve it:** An ablation study where the search tool is modified to explicitly return "No relevant information found" for low-confidence queries, measuring the subsequent impact on agent calibration without CAR fine-tuning.

## Limitations

- The study is limited to Qwen models (3B-7B scale), leaving unclear how the confidence dichotomy and CAR framework perform with larger or different architectures.
- Cross-environment generalization testing is limited to one real-world retrieval source (Serper API), with unknown robustness to other retrieval systems.
- The MSCR mechanism's superiority over Brier Score variants is demonstrated empirically but lacks complete theoretical dissection of gradient dynamics.

## Confidence

- **High Confidence:** The fundamental confidence dichotomy (evidence tools → overconfidence, verification tools → grounding) is well-supported by pilot study results and mechanism analysis.
- **Medium Confidence:** MSCR's strict reward separation is the key to stabilizing RL calibration; ablation against other reward designs is suggestive but not exhaustive.
- **Medium Confidence:** Generalization from local to noisy retrieval environments is demonstrated, but sample size and diversity of retrieval sources remain limited.

## Next Checks

1. **Multi-source Retrieval Test:** Replicate CAR training and evaluation across three distinct retrieval backends (local dump, Serper API, Bing API) to measure robustness of calibration gains under varying noise profiles.
2. **Scaling Study:** Train CAR on Qwen2.5-7B and Qwen3-4B models, then evaluate whether calibration improvements scale proportionally with model size or plateau.
3. **Reward Ablation with Gradient Analysis:** Replace MSCR with Brier Score (λ=1/3) and log reward gradients to verify whether incentive overlap manifests in training instability or accuracy collapse.