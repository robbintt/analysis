---
ver: rpa2
title: Logic-based Task Representation and Reward Shaping in Multiagent Reinforcement
  Learning
arxiv_id: '2510.23615'
source_url: https://arxiv.org/abs/2510.23615
tags:
- task
- options
- learning
- reward
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multi-agent reinforcement
  learning (MARL) with complex tasks specified in Linear Temporal Logic (LTL), which
  typically suffers from exponential sample complexity and the difficulty of designing
  appropriate reward functions. The proposed method converts LTL task specifications
  into Buchi automata and performs model-free learning by constructing a product Semi-Markov
  Decision Process (SMDP) on-the-fly using sampled transitions.
---

# Logic-based Task Representation and Reward Shaping in Multiagent Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.23615
- Source URL: https://arxiv.org/abs/2510.23615
- Reference count: 13
- This paper proposes LCgMQL, an algorithm that converts LTL task specifications to Büchi automata and uses options-based reward shaping to achieve faster convergence in multi-agent reinforcement learning compared to primitive action approaches.

## Executive Summary
This paper addresses the challenge of multi-agent reinforcement learning (MARL) with complex tasks specified in Linear Temporal Logic (LTL). The proposed method converts LTL specifications into Büchi automata and performs model-free learning by constructing a product Semi-Markov Decision Process (SMDP) on-the-fly using sampled transitions. A novel reward shaping approach based on progress levels in the automaton guides agents efficiently toward task completion. Experiments on deterministic gridworld simulations show that LCgMQL with options achieves faster convergence and better scalability compared to primitive action-only approaches.

## Method Summary
The LCgMQL algorithm works by first converting LTL task specifications into Büchi automata, then annotating automaton states with progress levels based on strongly connected components. During learning, the algorithm constructs augmented states (environment state × automaton state) incrementally as transitions occur, rather than pre-computing the full product MDP. Reward shaping is applied based on progress levels, where agents receive rewards proportional to advances in the automaton. Options (temporally extended actions) are used to reduce the effective search space, with benefits becoming more pronounced as state and action spaces grow. The method assumes known options and deterministic environments with static labeling.

## Key Results
- LCgMQL with options converges in 15,000 iterations on 12x12 grid vs. 30,000 iterations without options
- Reward shaping based on progress levels in Büchi automata provides efficient guidance toward task completion
- On-the-fly product SMDP construction avoids explicit model learning while maintaining task awareness
- Algorithm demonstrates improved scalability as state and action spaces increase

## Why This Works (Mechanism)

### Mechanism 1: LTL-to-Automaton Task Encoding with Progress Annotation
Converting LTL specifications to Büchi automata and annotating states with progress levels enables dense reward signals for temporally extended tasks. LTL formulas are translated to Büchi automata where states represent task progress stages. Strongly connected components (SCCs) are identified, and all states within an SCC receive the same progress level. Transitions crossing SCC boundaries increment progress. This transforms sparse task completion signals into denser feedback proportional to automaton state advances.

### Mechanism 2: On-the-Fly Product SMDP Construction
Augmenting MDP states with automaton states during sampling avoids explicit model learning while maintaining task-aware exploration. Rather than pre-computing the full product MDP, the algorithm constructs augmented states (s_t, q) incrementally as transitions occur. At each step, the algorithm validates whether the next state's label is acceptable to the current automaton state q. Valid transitions update the automaton state; invalid ones are discarded. This bounds exploration to task-relevant regions.

### Mechanism 3: Options-Based Search Space Reduction
Temporally extended actions (options) reduce effective search space, with benefits scaling as state/action spaces grow. Options encapsulate multi-step policies and reduce decision frequency by abstracting over primitive action sequences. When options terminate, rewards cascade backward through the trajectory. This reduces the decision frequency and abstracts over primitive action sequences, making the approach increasingly relevant for larger state spaces.

## Foundational Learning

- **Semi-Markov Decision Processes (SMDPs)**: Why needed: Options span variable numbers of timesteps; SMDPs generalize MDPs to handle temporally extended actions with duration distributions. Quick check: Given an option with termination condition β that depends on reaching a goal state, how would you compute the expected cumulative reward if the option takes 5 timesteps to complete?

- **Linear Temporal Logic (LTL) operators**: Why needed: LTL encodes task constraints (sequencing, safety, liveness) that guide automaton construction and reward shaping. Quick check: For the formula G(p_pick → p_drop), what behavior is enforced? What happens if this formula is violated during execution?

- **Strongly Connected Components (SCCs) in directed graphs**: Why needed: Progress level annotation relies on SCC detection; states within an SCC share the same progress level because cyclic transitions don't represent advancement. Quick check: In a Büchi automaton with states {q0, q1, q2} where q0→q1→q2→q1, which states share a progress level?

## Architecture Onboarding

**Component map:**
LTL Parser (Spot library) → Progress Annotator → Environment Wrapper → Joint Option Selector → Transition Validator → Experience Buffer → Q-Updater

**Critical path:**
1. Define atomic propositions (e.g., `p_pick`, `p_drop`, `p_obs`) and map to environment regions
2. Write LTL specification (e.g., `F p_pick ∧ F p_drop ∧ G ¬p_obs`)
3. Run Spot to generate Büchi automaton; annotate with progress levels
4. Initialize Q-table for all (state × automaton_state, joint_option) pairs
5. Execute episodes: sample options → validate transitions → compute shaped rewards → update Q-values
6. Extract policy via argmax over Q for each augmented state

**Design tradeoffs:**
- **Centralized control**: Simplifies coordination but scales as |S|^n × |O|^n for n agents. Paper shows this becomes prohibitive around 12×12 grids without options.
- **Known options vs. learned options**: Paper assumes handcrafted options (go-to-goal, obstacle-avoid). Learning options adds complexity but may improve generalization.
- **Discount factor γ**: Too low (e.g., 0.5) causes reward decay before reaching earlier states; paper uses 0.9. Fig 2.5a/b illustrates this failure mode.
- **Exploration ε**: Must be tuned per grid size; larger grids require more exploration before exploitation stabilizes.

**Failure signatures:**
- **Reward loop without progress**: Agent learns to cycle within an SCC because shaped rewards don't exceed staying-in-place value. Check if γ is too low or progress multiplier is insufficient.
- **Option deadlock**: Two agents' options create interdependence where neither terminates. Check initiation/termination set overlap.
- **Automaton state stuck**: Labels don't trigger automaton transitions. Verify environment labeling matches LTL atomic propositions.
- **Divergent Q-values**: Replay buffer contains contradictory experiences (non-deterministic transitions not handled). Paper assumes deterministic MDPs.

**First 3 experiments:**
1. **Single-agent 5×5 grid, reach-avoid task, primitive actions only**: Validate basic Q-learning convergence and automaton-guided exploration. Target: <5000 iterations to convergence.
2. **Two-agent 8×8 grid, same task, add go-to-goal + obstacle-avoid options**: Measure speedup (iterations and wall-clock). Expected: ~40% fewer iterations per Table I trends.
3. **Two-agent 12×12 grid, vary ε ∈ {0.1, 0.2, 0.3}**: Characterize exploration sensitivity at scale. Track convergence iterations and final policy optimality (path length vs. known optimal).

## Open Questions the Paper Calls Out
- Can the proposed approach be adapted to handle Computational Tree Logic (CTL) for probabilistic tasks in partially observable systems? The current method relies on Linear Temporal Logic (LTL) and assumes the environment states are statically labeled and fully observable.
- Does the LCgMQL algorithm retain its convergence properties and efficiency when applied to continuous state spaces? The current validation was restricted to discrete gridworld simulations.
- How can the framework be extended to learn or discover options on-the-fly rather than relying on pre-defined option definitions? The current implementation assumes the initiation set, policy, and termination condition for every option are provided.

## Limitations
- Assumes deterministic environments with static labeling, limiting applicability to stochastic or partially observable domains
- Requires known option definitions (initiation sets, policies, termination conditions), adding domain expertise burden
- Centralized approach scales exponentially with number of agents and state space size

## Confidence
- **High confidence**: LTL-to-automaton conversion mechanism, progress level annotation via SCC analysis, and basic reward shaping principles
- **Medium confidence**: On-the-fly product SMDP construction and options-based search space reduction, as these rely on domain-specific option definitions not fully detailed
- **Low confidence**: Exact convergence behavior and iteration counts without the unspecified hyperparameters and option definitions

## Next Checks
1. **Single-agent baseline**: Implement 5x5 grid with primitive actions only to verify basic Q-learning with progress-level rewards converges (<5000 iterations)
2. **Options integration test**: Add handcrafted go-to-goal and obstacle-avoid options to 8x8 grid, measure iteration reduction vs. primitive actions (target ~40% speedup)
3. **Exploration sensitivity**: Run 12x12 grid with ε ∈ {0.1, 0.2, 0.3}, track convergence iterations and final policy optimality (path length vs. known optimal)