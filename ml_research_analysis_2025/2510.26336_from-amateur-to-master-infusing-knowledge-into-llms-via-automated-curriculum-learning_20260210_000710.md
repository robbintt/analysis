---
ver: rpa2
title: 'From Amateur to Master: Infusing Knowledge into LLMs via Automated Curriculum
  Learning'
arxiv_id: '2510.26336'
source_url: https://arxiv.org/abs/2510.26336
tags:
- domain
- domains
- question
- section
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ACER, an automated curriculum learning framework\
  \ that systematically infuses domain-specific knowledge into large language models\
  \ without sacrificing their general capabilities. The approach generates structured,\
  \ textbook-style synthetic corpora guided by Bloom\u2019s taxonomy, creating progressively\
  \ difficult content and question-answer pairs across multiple educational levels."
---

# From Amateur to Master: Infusing Knowledge into LLMs via Automated Curriculum Learning

## Quick Facts
- arXiv ID: 2510.26336
- Source URL: https://arxiv.org/abs/2510.26336
- Authors: Nishit Neema; Srinjoy Mukherjee; Sapan Shah; Gokul Ramakrishnan; Ganesh Venkatesh
- Reference count: 40
- Primary result: 3 percentage point macro-average improvements in MMLU domains with Llama 3.2 (1B/3B)

## Executive Summary
This paper introduces ACER, an automated curriculum learning framework that systematically infuses domain-specific knowledge into large language models without sacrificing their general capabilities. The approach generates structured, textbook-style synthetic corpora guided by Bloom's taxonomy, creating progressively difficult content and question-answer pairs across multiple educational levels. Continual pretraining with an interleaved curriculum schedule aligns learning across both content and cognitive dimensions. Experiments with Llama 3.2 (1B and 3B) show consistent macro-average improvements of 3 percentage points across target MMLU domains, with up to 5-point gains in challenging areas like microeconomics.

## Method Summary
ACER generates synthetic textbook-style corpora using a teacher model (Gemini 2.0 Flash) to create structured content and QA pairs organized by cognitive difficulty according to Bloom's taxonomy. The framework employs a "Cog+Con" curriculum schedule that progresses through personas (High School → Undergraduate → Graduate → Researcher) while ordering content from textbook exposition to easy and hard QA pairs. The synthetic domain data is mixed 1:1 with general replay data to prevent catastrophic forgetting. Continual pretraining uses next-token prediction with hyperparameters including batch size 512, sequence length 8192, and learning rate 2e-5 with cosine decay.

## Key Results
- Consistent 3 percentage point macro-average improvements across target MMLU domains with Llama 3.2 3B model
- Up to 5-point gains in challenging domains like microeconomics
- 0.7-point improvement on non-target domains, demonstrating positive cross-domain transfer
- Over 2 absolute point gains on knowledge-intensive benchmarks (ARC, GPQA)
- Maintains stable performance on general reasoning tasks while improving domain knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured synthetic corpora with Bloom's taxonomy-guided progression improve domain knowledge absorption compared to unstructured web text.
- Mechanism: Generate textbook-style content (ToC → sections → QA pairs) at multiple cognitive levels, ensuring hierarchical concept scaffolding rather than ad-hoc sampling. Progressive difficulty from factual recall → application → analysis mirrors human learning.
- Core assumption: Synthetic content generated by LLMs (Gemini 2.0 Flash) can capture domain expertise accurately enough to serve as training material.
- Evidence anchors:
  - [abstract] "ACER first synthesizes a comprehensive, textbook-style curriculum by generating a table of contents for a subject and then creating question-answer (QA) pairs guided by Bloom's taxonomy."
  - [section] Section 3 describes three-stage pipeline: Domain Detailing → Outline Generation → Synthetic Content Generation with QA pairs divided by difficulty.
  - [corpus] Related work (EDCO, From Atoms to Chains) supports curriculum-based fine-tuning, but evidence for synthetic textbook generation specifically is limited in corpus.
- Break condition: If generated content contains hallucinations or factual errors, model may learn incorrect domain knowledge; decontamination threshold (0.9 cosine similarity) may not catch subtle errors.

### Mechanism 2
- Claim: Cognitive + Content curriculum scheduling (Cog+Con) outperforms random or interleaved ordering for domain infusion.
- Mechanism: Present data in difficulty order (Books → Easy QA → Hard QA) combined with persona progression (High School → Undergraduate → Graduate → Researcher), allowing model to build foundational knowledge before advanced concepts.
- Core assumption: Sequential difficulty ordering transfers from human education to LLM pretraining dynamics.
- Evidence anchors:
  - [abstract] "continual pretraining with an interleaved curriculum schedule, aligning learning across both content and cognitive dimensions."
  - [section] Table 1: Cog+Con achieves Macro_t=0.4407 vs Flat=0.4362 vs Interleaved=0.4223 on 3B model. Section 4.2 notes Interleaved "underperforms relative to other curriculum schedules."
  - [corpus] EDCO paper supports curriculum orchestration for domain-specific fine-tuning; corpus evidence for Cog+Con specifically is weak.
- Break condition: If model capacity is insufficient for domain complexity, curriculum ordering provides diminishing returns; smaller models (1B) show reduced gains (2.6 pp vs 3.0 pp for 3B).

### Mechanism 3
- Claim: 1:1 mixing of synthetic domain data with general replay data prevents catastrophic forgetting while enabling cross-domain transfer.
- Mechanism: Replay data from general corpora (pile, cosmopedia, ultratextbooks, proof-pile-2) maintains broad capabilities; domain data injects specialization. Balance prevents overfitting to narrow domain.
- Core assumption: Equal weighting is optimal; dynamic mixing ratios may not improve results.
- Evidence anchors:
  - [abstract] "ACER not only prevents catastrophic forgetting but also facilitates positive cross-domain knowledge transfer, improving performance on non-target domains by 0.7 points."
  - [section] Table 4 (Appendix A.3): 1:1 ratio achieves Macro_t=0.4407 vs 3:1=0.4284 vs 9:1=0.4135. Non-target Macro declines as replay decreases (0.5821 → 0.5736).
  - [corpus] "How to inject knowledge efficiently?" paper (FMR 0.612) addresses knowledge infusion scaling; supports replay mixing concept.
- Break condition: If target domain is extremely specialized or small, 1:1 ratio may dilute signal; asymmetric ratios (3:1 or higher) showed degraded generalization.

## Foundational Learning

- Concept: Curriculum Learning (Bengio et al., 2009)
  - Why needed here: ACER's core hypothesis depends on ordering training data by difficulty improving convergence and final performance.
  - Quick check question: Can you explain why presenting easy examples before hard ones might improve learning dynamics in neural networks?

- Concept: Bloom's Taxonomy
  - Why needed here: Provides the cognitive framework for structuring QA pairs into easy (recall/comprehension) vs hard (analysis/application) categories.
  - Quick check question: What are the six cognitive levels in Bloom's taxonomy, and which levels does ACER use for "easy" vs "hard" QA?

- Concept: Catastrophic Forgetting
  - Why needed here: Domain-adaptive pretraining risk that ACER specifically addresses via replay mixing; failure mode to monitor.
  - Quick check question: Why does fine-tuning on narrow domain data cause models to lose previously learned general capabilities?

## Architecture Onboarding

- Component map:
  1. Domain Detailing (prompt LLM → JSON schema with description, subtopics, key questions)
  2. Outline Generation (ToC tree: Parts → Chapters → Sections → Subsections)
  3. Content Generation (section-level prose + QA pairs with difficulty labels)
  4. Decontamination (cosine similarity filter, threshold 0.9 vs MMLU)
  5. Curriculum Scheduler (Flat/Cog/Cog+Con/Interleaved variants)
  6. Continual Pretraining (next-token prediction, 1:1 domain:replay mix)

- Critical path: Domain Detailing → ToC Generation → Section Content → QA Generation → Decontamination → Curriculum Assembly → CPT with Replay

- Design tradeoffs:
  - Cog+Con schedule vs Interleaved: Cog+Con yields +1.8 pp Macro_t over Interleaved but requires more sequential processing; Interleaving dilutes signal in CPT setting (contradicts fine-tuning findings from Lee et al., 2024).
  - 1:1 vs 3:1 domain:replay: Higher domain ratios improve target but degrade non-target; 1:1 is Pareto-optimal.
  - 4 personas vs 1: Multiple personas (HS/Undergrad/Grad/Researcher) increase token budget (6.4M tokens/domain) but provide content dimension for curriculum.

- Failure signatures:
  - Non-target Macro drops below baseline: Replay ratio too low; increase general data proportion.
  - Interleaved schedule underperforms Flat: Excessive task-switching overwhelms smaller model capacity.
  - QA pairs hallucinate facts not in section content: Answer generation not strictly grounded; verify prompt enforcement.

- First 3 experiments:
  1. Replicate Cog+Con vs Flat ablation on single domain (e.g., microeconomics) with 1B model to validate curriculum effect at smaller scale.
  2. Test replay ratio sweep (1:3, 1:1, 3:1, 9:1) on target vs non-target Macro to confirm 1:1 Pareto frontier.
  3. Decontamination analysis: Sample 100 QA pairs with similarity >0.8 to MMLU, manually verify if contamination inflates benchmark scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does ACER's effectiveness scale to larger LLMs (7B+ parameters), and do curriculum benefits increase or diminish with model capacity?
- Basis in paper: [explicit] "Due to time and resource constraints, we evaluated the approach on 1B and 3B scale models without extending the evaluation to larger-scale models."
- Why unresolved: The authors acknowledge that more powerful models may yield greater benefits, but this remains untested.
- What evidence would resolve it: Replicating ACER experiments on Llama 3.1 8B or 70B models, comparing curriculum vs. flat schedules.

### Open Question 2
- Question: What accounts for the Interleaved schedule's underperformance compared to Cog+Con in continual pretraining, contrary to its success in instruction tuning?
- Basis in paper: [explicit] "This contrasts with Lee et al. (2024), where interleaving educational content improved performance... frequent task-switching introduced by interleaving likely overwhelms model capacity."
- Why unresolved: The conjecture about fragmented sequencing diluting supervision signals lacks direct empirical validation.
- What evidence would resolve it: Ablation studies varying interleaving granularity (chapter-level vs. section-level vs. paragraph-level) to identify the point where benefits emerge.

### Open Question 3
- Question: Does the choice of teacher model for synthetic corpus generation significantly impact ACER's downstream performance?
- Basis in paper: [explicit] "We did not ablate between different models for content generation."
- Why unresolved: All synthetic content was generated using Gemini 2.0 Flash; sensitivity to generator quality/capabilities is unknown.
- What evidence would resolve it: Generate identical curricula using different teacher models (e.g., GPT-4, Claude, smaller open models) and compare resulting student performance.

## Limitations

- Synthetic Data Quality: The framework depends on Gemini 2.0 Flash generating accurate, hallucination-free textbook content, with no external validation of factual accuracy.
- Curriculum Scheduling Generalization: Cog+Con shows strong results but represents only one curriculum ordering strategy, with superiority potentially specific to Llama 3.2's architecture.
- Mixing Ratio Optimization: While 1:1 appears Pareto-optimal, this assumes equal importance of target domain gains and non-target preservation, not explored for extreme specialization cases.

## Confidence

- High Confidence: Target domain improvements (3.0 pp Macro_t for 3B, 2.6 pp for 1B), catastrophic forgetting prevention (non-target Macro maintained/improved), and knowledge-intensive benchmark gains (ARC/GPQA +2.0 points).
- Medium Confidence: Cross-domain transfer claims (0.7 pp non-target improvement) and mechanism explanations (why Cog+Con outperforms Interleaved).
- Low Confidence: Generalizability to other model architectures, synthetic data generation approaches, or domain areas beyond the five academic domains tested.

## Next Checks

1. **Curriculum Scheduling Robustness**: Replicate the Cog+Con vs Interleaved ablation across multiple domain subsets (not just the full five) and model scales (1B, 3B, 8B) to verify the scheduling effect isn't domain-specific.

2. **Synthetic Data Factuality Audit**: Sample 100-200 generated QA pairs across all domains and personas, then verify factual claims against authoritative sources. Quantify hallucination rate and its correlation with downstream performance.

3. **Replay Ratio Sensitivity**: Conduct a finer-grained sweep (1:3, 1:2, 1:1, 2:1, 3:1) on a single domain (e.g., microeconomics) while tracking both target gains and non-target degradation to map the full Pareto frontier.