---
ver: rpa2
title: 'Not What the Doctor Ordered: Surveying LLM-based De-identification and Quantifying
  Clinical Information Loss'
arxiv_id: '2509.14464'
source_url: https://arxiv.org/abs/2509.14464
tags:
- de-identification
- clinical
- llama-3
- information
- clinicalbert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The authors highlight a significant gap in the evaluation of LLM-based
  clinical de-identification: existing metrics fail to detect clinically relevant
  information loss caused by false positive redactions. They propose a novel LLM-based
  metric, CIRE, to assess clinical information retention by comparing sentence-level
  semantic changes pre- and post-de-identification.'
---

# Not What the Doctor Ordered: Surveying LLM-based De-identification and Quantifying Clinical Information Loss

## Quick Facts
- arXiv ID: 2509.14464
- Source URL: https://arxiv.org/abs/2509.14464
- Reference count: 35
- Primary result: All LLM-based and rule-based de-identification models suffer high rates of clinically significant false positive redactions, with traditional metrics failing to detect this information loss.

## Executive Summary
This study reveals a critical gap in clinical text de-identification evaluation: existing metrics fail to detect clinically relevant information loss caused by over-redaction. Across two clinical datasets, all tested models—including Llama-3.3, ClinicalBERT, and rule-based systems—showed high rates of clinically significant false positives, with most deemed high severity by physicians. The authors introduce CIRE (Clinical Information Retention Evaluation), an LLM-based metric that measures semantic changes at the sentence level and correlates strongly with expert human judgment. Manual validation demonstrated that traditional metrics and ICD-based methods poorly capture actual clinical information loss, while CIRE provides a more reliable assessment of clinical utility retention.

## Method Summary
The authors evaluate de-identification models using two clinical datasets: MIMIC-III discharge summaries (2,000 notes with synthetic PII) and a private AHS rheumatology referral letter corpus (204 notes). They implement CIRE by aligning original and de-identified sentences using Needleman-Wunsch, chunking into 20-token segments, and prompting Llama-3.3 to classify clinically meaningful information changes. Traditional metrics (accuracy, precision, recall, F1) and JSC (Jaccard Similarity Coefficient using ICD-10 predictions) are computed alongside CIRE. Models tested include ClinicalBERT, Presidio, Deidentify library, and Llama-3.3 with various prompts. Manual annotation of 500 false positives per model by physicians provides ground truth for clinical information loss severity.

## Key Results
- All evaluated models showed 70-80% of clinically relevant false positives classified as high severity by physicians
- CIRE scores were similar across models (~90-95%), suggesting comparable clinical retention despite performance differences in standard metrics
- Traditional metrics and JSC showed poor correlation with manual clinical information loss annotations
- CIRE demonstrated strong correlation with manual annotations (Pearson up to 0.86 on MIMIC, 0.73-0.78 on AHS)

## Why This Works (Mechanism)

### Mechanism 1
CIRE correlates more strongly with expert human judgment of clinically relevant changes than prior automated metrics by aligning original and de-identified text at the sentence level using Needleman–Wunsch, chunking aligned pairs into fixed 20-token spans, and prompting an LLM to classify whether each chunk pair alters clinically meaningful information. Per-sentence classifications are averaged into a document-level retention score.

### Mechanism 2
Traditional classification metrics fail to capture the clinical severity of over-redaction because they treat all false positives as equivalent. These metrics count token-level errors without weighting by semantic or clinical impact, unable to distinguish removing "the" from removing "metformin."

### Mechanism 3
ICD-based retention metrics undercount clinically relevant changes because many alterations do not affect predicted ICD codes. JSC uses BioBERT to predict ICD-10 codes from original and de-identified notes, but clinically meaningful changes (e.g., medication dosage, patient history) often do not change ICD codes.

## Foundational Learning

- **Concept: HIPAA/GDPR de-identification requirements and PHI categories**
  - Why needed here: Understanding why PII must be removed and what counts as identifiers frames why over-redaction is tempting but costly
  - Quick check question: Can you list at least three HIPAA identifiers and explain why removing non-identifier clinical terms harms downstream utility?

- **Concept: Precision vs. recall trade-offs in high-stakes NLP**
  - Why needed here: De-identification systems often optimize recall at the expense of precision, leading to over-redaction; the paper quantifies the clinical cost of this trade-off
  - Quick check question: If a system has 0.96 recall and 0.20 precision, what does this imply about the rate of false positives and potential information loss?

- **Concept: LLM-as-a-judge evaluation paradigm**
  - Why needed here: CIRE uses an LLM to classify semantic equivalence/clinical alteration; understanding LLM judges helps interpret the metric's strengths and failure modes
  - Quick check question: What are two ways an LLM judge can fail to align with human expert labels, and how would you detect each?

## Architecture Onboarding

- **Component map**: Preprocessing -> Tokenization/Sentence Splitting -> Alignment (Needleman-Wunsch) -> Chunking (20 tokens) -> LLM Classifier (Llama-3.3) -> Aggregation (Average classifications) -> CIRE Score
- **Critical path**: Accurate alignment → stable chunk pairing → LLM classification quality → aggregation
- **Design tradeoffs**: Fixed-length chunking vs. sentence-level splitting (chunking more robust to OCR noise but can split clinical concepts); LLM choice (larger models provide higher judge quality but increase cost); prompt design (examples and explicit categories affect precision/recall)
- **Failure signatures**: Low correlation with human labels suggests prompt misalignment or concept drift; high variance across LLM judges indicates sensitivity to model choice; systematic false negatives may stem from ambiguous prompts or domain gaps
- **First 3 experiments**:
  1. Reproduce CIRE on a small held-out set (10-20 notes) with manual sentence-level labels; compute Pearson/Spearman vs. ground truth to verify correlation claims
  2. Ablate the alignment method: compare Needleman-Wunsch vs. simple string matching on noisy OCR data to quantify alignment error impact on CIRE
  3. Swap the judge LLM (e.g., Llama 3.3 vs. Qwen-72B vs. a smaller model) on the same labeled set to measure judge sensitivity and calibrate prompt wording

## Open Questions the Paper Calls Out

### Open Question 1
Can a multi-stage de-identification process that explicitly assesses proposed removals for clinical relevance reduce inappropriate clinical information loss while maintaining privacy protection? Current single-pass models treat all false positives equally; 70-80% of false positives caused clinically significant changes deemed high severity by physicians, but no existing system validates removals against clinical relevance.

### Open Question 2
Can techniques from paraphrase detection improve CIRE's ability to identify clinically relevant semantic changes? CIRE currently achieves moderate recall (0.54-0.86 depending on model/dataset); paraphrase detection methods may better capture semantic equivalence vs. clinically meaningful alteration.

### Open Question 3
Do de-identification model rankings and clinical information retention patterns generalize across different clinical specialties, institutional contexts, and non-English languages? Only tested on rheumatology referral notes and MIMIC-III discharge summaries; specialty-specific terminology and documentation styles may affect both PII detection and clinical information preservation differently.

## Limitations
- Evaluation limited to two datasets (MIMIC-III and private AHS rheumatology letters), potentially limiting generalizability
- Manual annotation ground truth based on relatively small human-labeled sets (30 AHS + 10 MIMIC notes)
- Alignment method introduces complexity and potential errors in chunk pairing
- Similarity of CIRE scores across models may mask clinically meaningful differences in specific scenarios

## Confidence

- **High confidence**: Traditional metrics fail to capture clinically relevant information loss; CIRE correlates strongly with manual annotations; over-redaction is prevalent and clinically significant
- **Medium confidence**: CIRE is more reliable than JSC or accuracy-based metrics for assessing clinical information retention; ICD-based metrics systematically undercount clinically relevant changes
- **Low confidence**: The equivalence of CIRE scores across models implies similar clinical utility; the LLM judge can reliably generalize to diverse clinical note types without further prompt tuning or validation

## Next Checks

1. **Validate CIRE with task-specific utility**: Measure the impact of de-identification on a downstream clinical NLP task (e.g., readmission prediction or phenotyping) to confirm that high CIRE scores correspond to preserved task performance

2. **Replicate alignment robustness**: Conduct an ablation study comparing Needleman-Wunsch alignment to simpler methods (e.g., fuzzy string matching) on a noisy OCR dataset, quantifying alignment error rates and their downstream effects on CIRE

3. **Cross-judge evaluation**: Apply CIRE using multiple LLM judges (e.g., Llama 3.3, Qwen-72B, GPT-4) to the same validation set and compute inter-judge agreement and correlation with human labels, identifying potential model-dependent biases