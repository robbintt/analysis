---
ver: rpa2
title: 'DAOpt: Modeling and Evaluation of Data-Driven Optimization under Uncertainty
  with LLMs'
arxiv_id: '2511.11576'
source_url: https://arxiv.org/abs/2511.11576
tags:
- optimization
- problem
- data
- should
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes DAOpt, a framework for modeling and evaluating
  data-driven optimization under uncertainty using large language models (LLMs). The
  key idea is to incorporate domain knowledge from stochastic and robust optimization
  into LLMs to enhance their modeling capabilities in uncertain environments.
---

# DAOpt: Modeling and Evaluation of Data-Driven Optimization under Uncertainty with LLMs

## Quick Facts
- arXiv ID: 2511.11576
- Source URL: https://arxiv.org/abs/2511.11576
- Reference count: 40
- Key result: DAOpt achieves out-of-sample feasibility rates above 70% with distributionally robust optimization, significantly outperforming direct and chain-of-thought prompting baselines.

## Executive Summary
DAOpt introduces a framework for modeling and solving data-driven optimization problems under uncertainty using large language models (LLMs). The key innovation is integrating domain knowledge from stochastic and robust optimization into LLMs through a multi-agent decision-making pipeline. The framework includes a new dataset (OptU), a domain knowledge learner, and a simulation environment for evaluating out-of-sample feasibility and robustness. Experiments demonstrate that DAOpt significantly improves solution validity compared to direct prompting approaches, particularly when using distributionally robust optimization models.

## Method Summary
DAOpt employs a three-module framework: (1) OptU dataset with decoupled natural language problem descriptions and historical samples of uncertain parameters; (2) a multi-agent decision module with an optimization identifier, OR domain knowledge learner using RSOME integration, and a reflexion-based checker; (3) a simulation module for out-of-sample evaluation. The framework generates optimization models (deterministic, robust, or distributionally robust) and evaluates them on new instances. Performance is measured by successful rate (SR), out-of-sample feasibility rate (FR), and over-optimistic rate (OpR).

## Key Results
- DAOpt achieves feasibility rates above 70% using distributionally robust optimization, compared to below 50% for direct prompting baselines.
- The framework significantly improves successful rates in obtaining valid solutions, especially when using GPT-4o.
- Out-of-sample performance of DAOpt models is consistently better than in-sample objectives, indicating reduced over-optimism.

## Why This Works (Mechanism)
The framework's effectiveness stems from three key mechanisms: First, the OptU dataset's decoupled structure allows LLMs to focus on problem semantics without being distracted by historical data patterns. Second, the multi-agent architecture with specialized roles (identifier, coder, checker) decomposes the complex task of optimization model generation into manageable subtasks with quality control through Reflexion cycles. Third, the simulation-based evaluation on unseen instances provides robust feedback that drives model refinement, preventing overfitting to training data distributions.

## Foundational Learning
- **RSOME integration**: Required for translating LLM-generated optimization formulations into executable code. Needed to bridge natural language descriptions with mathematical programming syntax. Quick check: Verify RSOME can correctly parse uncertainty sets and constraints from LLM outputs.
- **Multi-agent orchestration**: The decision module requires coordinated interaction between identifier, coder, and checker agents. Needed to handle the complexity of model identification, code generation, and verification. Quick check: Trace the feedback loop where the checker agent provides corrections to the coder.
- **Out-of-sample evaluation**: Critical for assessing real-world performance beyond training data. Needed because in-sample feasibility does not guarantee robustness to unseen uncertainty realizations. Quick check: Confirm test instances are independently generated from training samples.

## Architecture Onboarding

**Component Map**: OptU dataset -> Multi-agent decision module (Identifier -> Coder -> Checker) -> Simulation module -> Evaluation metrics

**Critical Path**: Natural language problem description → Identifier extracts problem structure → Coder generates RSOME optimization model → Checker verifies and refines → Simulation tests out-of-sample feasibility → Metrics computed

**Design Tradeoffs**: Multi-agent approach adds computational overhead but improves solution validity compared to single-prompt methods. Reflexion cycles increase runtime but reduce invalid solutions. Distributionally robust models provide better out-of-sample performance but require additional Wasserstein radius tuning.

**Failure Signatures**: Low successful rate indicates LLM cannot generate valid RSOME code; low feasibility rate suggests incorrect uncertainty set specification; high over-optimistic rate indicates model overfits to training data distribution.

**Three First Experiments**:
1. Test DAOpt on a simple warehouse location problem with known ground truth to verify basic functionality.
2. Compare feasibility rates of deterministic, robust, and distributionally robust models on the same problem to understand paradigm effectiveness.
3. Measure the impact of Reflexion cycles by running with 0, 1, and 2 correction loops and tracking successful rate improvements.

## Open Questions the Paper Calls Out
The paper identifies several open questions for future research: How can the framework be extended to handle sequential decision-making problems with dynamic uncertainty? What are the theoretical guarantees on solution quality when using LLMs for optimization modeling? Can the multi-agent approach be scaled to more complex optimization paradigms beyond those currently tested? How sensitive is performance to the choice of uncertainty set radius and other hyperparameters?

## Limitations
- Proprietary OptU dataset details limit reproducibility and understanding of problem diversity.
- Specific few-shot prompting templates for the OR Domain Knowledge Learner are not disclosed.
- Computational resource requirements and hyperparameter sensitivity are not fully characterized.

## Confidence

**High Confidence**: Framework architecture and RSOME integration methodology are well-specified and reproducible.

**Medium Confidence**: Reported performance improvements over baselines, as methodology is clear but depends on undisclosed dataset details and prompt templates.

**Medium Confidence**: Claim that distributionally robust optimization consistently outperforms other paradigms, as this is supported by experiments but may be sensitive to Wasserstein radius choice.

## Next Checks

1. Recreate OR Domain Knowledge Learner prompts using public optimization examples to assess impact of few-shot examples on code generation quality.

2. Generate synthetic OptU-style problems with varying uncertainty levels to test whether performance gains hold across different problem scales.

3. Benchmark computational costs of multi-agent pipeline versus simpler prompting baselines and verify RSOME correctly implements robust formulations.