---
ver: rpa2
title: 'Layer-Aware Task Arithmetic: Disentangling Task-Specific and Instruction-Following
  Knowledge'
arxiv_id: '2502.20186'
source_url: https://arxiv.org/abs/2502.20186
tags:
- task
- dare
- ties
- lata
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Layer-Aware Task Arithmetic (LATA), a method
  that improves model merging by assigning layer-specific weights to task vectors
  based on their alignment with instruction-following versus task-specific knowledge.
  By amplifying layers that enhance the target task and attenuating those tied to
  general instruction-following, LATA achieves better multi-task learning and selective
  task forgetting.
---

# Layer-Aware Task Arithmetic: Disentangling Task-Specific and Instruction-Following Knowledge

## Quick Facts
- arXiv ID: 2502.20186
- Source URL: https://arxiv.org/abs/2502.20186
- Authors: Yan-Lun Chen; Yi-Ru Wei; Chia-Yi Hsu; Chia-Mu Yu; Chun-Ying Huang; Ying-Dar Lin; Yu-Sung Wu; Wei-Bin Lee
- Reference count: 40
- Primary result: Layer-Aware Task Arithmetic (LATA) improves model merging by assigning layer-specific weights to task vectors based on their alignment with instruction-following versus task-specific knowledge, achieving better multi-task learning and selective task forgetting.

## Executive Summary
This paper introduces Layer-Aware Task Arithmetic (LATA), a method that improves model merging by assigning layer-specific weights to task vectors based on their alignment with instruction-following versus task-specific knowledge. By amplifying layers that enhance the target task and attenuating those tied to general instruction-following, LATA achieves better multi-task learning and selective task forgetting. Experiments on benchmarks including WikiText-2, GSM8K, and HumanEval show LATA consistently outperforms standard Task Arithmetic, DARE, and TIES in both task accuracy and model utility, with minimal degradation in output quality. It also effectively removes harmful capabilities while preserving overall performance, demonstrating the importance of layer-wise analysis in disentangling task-specific from general-purpose knowledge.

## Method Summary
LATA builds on Task Arithmetic by computing per-layer cosine similarity between task-specific complex vectors and instruction-following vectors, then assigning layer weights based on this similarity ranking. The method generates "pure vectors" that emphasize task-specific layers while suppressing instruction-following interference. LATA offers three approaches: Linear-Drop-by-Rank, Logarithmic-Drop-by-Rank, and Drop-with-Threshold (σ=0.95). The merged model is computed as θ_merged = θ_target + Σλ_i * τ'_i, where τ'_i are layer-weighted pure vectors and λ_i are scaling coefficients. The method requires computing instruction vectors (τ_instr = θ_pre - θ_base) and complex vectors (τ_comp = θ_ft - θ_base) for each task, then splitting these vectors by layer to compute similarity rankings.

## Key Results
- LATA consistently outperforms standard Task Arithmetic, DARE, and TIES on WikiText-2 perplexity, GSM8K accuracy, and HumanEval pass rates
- Layer analysis shows task-specific knowledge concentrates in later layers (26-30) while instruction-following knowledge dominates earlier layers
- Drop-with-Threshold effectively removes harmful capabilities while preserving model utility with minimal parameter changes (~10% retention)
- Synergistic improvements observed when merging related tasks (math + code) due to shared directional alignment in task-specific layers

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Layers exhibiting high cosine similarity between the complex vector and instruction vector encode primarily instruction-following behavior, while low-similarity layers encode task-specific knowledge.
- **Mechanism:** LATA computes per-layer cosine similarity between τ^i_comp and τ^i_instr. Layers ranked with low similarity receive larger weights in the "pure vector," amplifying their contribution to the target task during merging; high-similarity layers are attenuated to reduce instruction-following interference.
- **Core assumption:** Instruction-following and task-specific knowledge occupy distinct, separable parameter subspaces that can be identified via directional similarity rather than magnitude.
- **Evidence anchors:**
  - [section 4, Step 2]: "layers showing higher similarity primarily capture instruction-following capabilities. Assigning smaller weights to these layers during TA reduces their impact on the merged model"
  - [section 6, Figure 5]: "Layers with lower similarity (thus more impact on the target task) generally appear after layer 20, especially between layers 26 and 30"
  - [corpus]: Limited direct corpus support—neighbor papers address forgetting/interference but not layer-wise disentanglement via cosine similarity specifically.
- **Break condition:** If task-specific and instruction-following knowledge are highly co-located in the same layers (similarity distribution is uniform), the ranking-based weighting collapses to uniform weighting and LATA degrades to standard TA.

### Mechanism 2
- **Claim:** A small subset of layer vectors (<10%) carries the majority of task-specific signal, enabling effective task forgetting with minimal parameter changes.
- **Mechanism:** Using Drop-with-Threshold (σ=0.95), LATA discards ~90% of layer vectors that exceed the similarity threshold, retaining only the low-similarity "pure" subset. These few layers are sufficient to modify task behavior without rescaling by the full 1/(1-0.9) factor that DARE would require.
- **Core assumption:** Task vectors contain a sparse core of critical parameters; most parameters are redundant or encode general capabilities already present in the base model.
- **Evidence anchors:**
  - [section 6]: "only about 10% of the layer vectors were retained as pure vectors, while the remaining 90% had similarities greater than 0.95"
  - [section 6]: "a complete task vector indeed contains a subset of parameters that are highly critical to the target task, while a substantial portion is less significant"
  - [corpus]: DARE (Yu et al., 2024) sparsification principle is cited, supporting sparsity assumptions in task vectors.
- **Break condition:** If a task's critical parameters are distributed across many layers rather than concentrated, threshold-based dropping removes essential signal and performance degrades sharply.

### Mechanism 3
- **Claim:** Merging task vectors with high intrinsic similarity (e.g., math and code) can produce synergistic improvements on both tasks.
- **Mechanism:** When two task vectors share directional alignment in their low-similarity layers, adding them concurrently reinforces shared task-relevant subspaces rather than causing destructive interference.
- **Core assumption:** Task interference in TA stems primarily from overlapping instruction-following components, not from task-specific parameter conflicts when tasks are cognitively related.
- **Evidence anchors:**
  - [section 6]: "significant overlap in similarity rankings for math and code tasks...when merging them simultaneously (math + code, UA + math + code), both tasks outperform their single-task scenarios"
  - [corpus]: "To See a World in a Spark of Neuron" (arxiv 2503.05320) addresses multi-task interference in model merging, providing related context on disentanglement.
- **Break condition:** If unrelated tasks have conflicting directions in shared task-specific layers, merging produces interference and individual task performance drops below single-task baselines.

## Foundational Learning

- **Concept:** Task Arithmetic fundamentals (task vectors as θ_ft - θ_pre, additive merging, subtractive forgetting)
  - **Why needed here:** LATA builds directly on TA; without understanding what a task vector represents, the layer-wise weighting rationale is opaque.
  - **Quick check question:** Given θ_pre and θ_ft, can you compute τ and explain what subtracting τ from θ_target would accomplish?

- **Concept:** Cosine similarity as a directional alignment metric (range [-1,1], invariant to magnitude)
  - **Why needed here:** LATA's core filtering relies on per-layer cosine similarity; misinterpreting it as magnitude-based will lead to incorrect implementation.
  - **Quick check question:** If two vectors have cosine similarity 0.95, what does that imply about their directional relationship vs. their relative magnitudes?

- **Concept:** Transformer layer functional specialization (early layers → input processing; later layers → output generation)
  - **Why needed here:** Interpreting LATA's layer-wise results (Figure 5) requires intuition for why later layers show lower instruction-following similarity.
  - **Quick check question:** Why might parameter changes in later layers have greater impact on task-specific outputs than early layers?

## Architecture Onboarding

- **Component map:**
  1. Base model (no instruction-following, e.g., Llama-3-8B) → θ_base
  2. Pre-trained model (instruction-tuned, e.g., Llama-3-8B-Instruct) → θ_pre, serves as target model
  3. Fine-tuned models (task-specific) → θ_ft for each task
  4. Instruction vector → τ_instr = θ_pre - θ_base
  5. Complex vector → τ_comp = θ_ft - θ_base (per task)
  6. Task vector → τ = θ_ft - θ_pre (per task)
  7. Pure vector → τ' = layer-weighted τ via similarity ranking
  8. Merged model → θ_merged = θ_target + Σλ_i τ'_i

- **Critical path:**
  1. Load all three model checkpoints (base, pre-trained, fine-tuned) — memory-intensive
  2. Compute τ_instr once (reusable across tasks)
  3. For each task: compute τ_comp, compute per-layer cosine similarities, rank layers, generate τ'
  4. Aggregate pure vectors with scaling coefficients λ_i
  5. Apply to target model

- **Design tradeoffs:**
  - Linear-Drop-by-Rank vs. Logarithmic-Drop-by-Rank: Linear emphasizes top-ranked layers more aggressively; Logarithmic smooths differences (preferred for smaller models with higher layer interdependence per paper's Llama-3-8B results)
  - Drop-with-Threshold: Most aggressive sparsification; effective for forgetting but requires tuning σ (0.95 used in paper)
  - Combining LATA with DARE/TIES: Paper finds LATA alone outperforms LATA+DARE+TIES because DARE/TIES may zero out layers LATA identifies as critical

- **Failure signatures:**
  - Perplexity spikes (>15 on WikiText-2): Indicates merged model lost general language capability, likely from overly aggressive weighting or scaling coefficient too large
  - Task accuracy drops below baseline: Suggests pure vector removed critical task-specific layers (threshold too high) or instruction-following interference not properly suppressed (similarity ranking ineffective)
  - Gibberish outputs (per Table 9 at extreme drop rates): Catastrophic parameter destruction from coefficient instability

- **First 3 experiments:**
  1. Single-task pure vector validation: Compute τ' for one task (e.g., math), merge with λ=0.5, verify GSM8K accuracy improves over raw τ merging while WikiText-2 perplexity stays within 5% of baseline
  2. Layer distribution visualization: Reproduce Figure 5 for your target task/model to confirm late-layer task-specific concentration exists before committing to threshold choice
  3. Forgetting smoke test: Apply Drop-with-Threshold (σ=0.95) to subtract unalignment vector from a multilingual model, verify harm score drops while TMMLU+/JSQuAD utility remains stable

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LATA be extended to support model merging across different architecture families (cross-family merging)?
- Basis in paper: [explicit] The Limitations section states that because LATA relies on task arithmetic, all models must share the same architecture (identical hidden dimensions and layer structures), which limits cross-family applications.
- Why unresolved: The current method relies on direct element-wise operations on aligned parameter matrices, which is impossible when dimensions differ.
- What evidence would resolve it: A mechanism to map or align layer-specific vectors between models with different structures (e.g., different layer counts or hidden sizes) without losing the disentanglement benefits.

### Open Question 2
- Question: How can the scaling coefficient (λ) be determined automatically to prevent model instability or catastrophic forgetting?
- Basis in paper: [explicit] The Limitations section notes that improper scaling coefficients can lead to instability, potentially degrading performance.
- Why unresolved: The current work relies on manual tuning of λ for different tasks (e.g., 0.5 for Math vs. 1.5 for UA), implying a lack of a universal or automated calibration method.
- What evidence would resolve it: A theoretical analysis or an automated algorithm that predicts the optimal λ based on the magnitude of the task vector or the model's current weights.

### Open Question 3
- Question: Does the hypothesis that "later layers are task-specific" while "earlier layers are instruction-focused" hold universally across all task types?
- Basis in paper: [inferred] The Discussion section hypothesizes that layers with lower similarity (usually later layers) are more crucial to the target task, while earlier layers process input instructions. This is inferred from observations on Math and Code tasks.
- Why unresolved: The paper validates the method on specific capabilities (Math, Code, Unalignment), but the layer-wise functional hypothesis is not proven for broader cognitive tasks or modalities.
- What evidence would resolve it: A comprehensive layer-wise probing study across diverse tasks (e.g., sentiment analysis, translation) to verify if the correlation between layer index and task specificity persists.

## Limitations
- Architecture constraint: LATA requires identical model architectures across base, pre-trained, and fine-tuned models, preventing cross-family merging
- Scaling coefficient sensitivity: Improper λ values can cause model instability or catastrophic forgetting
- Limited task validation: Results primarily validated on Math, Code, and Unalignment tasks, with uncertain generalization to other domains

## Confidence

**High Confidence**: The core empirical results showing LATA outperforms standard Task Arithmetic on WikiText-2 perplexity, GSM8K accuracy, and HumanEval pass rates are well-supported with clear baselines and multiple comparison methods (DARE, TIES). The layer-wise analysis in Figure 5 demonstrating late-layer task-specific concentration is reproducible and theoretically plausible.

**Medium Confidence**: The mechanism claims about disentanglement via cosine similarity are supported by directional evidence but lack formal proof. While the paper shows that layers with low similarity to instruction vectors contribute more to task performance, it doesn't establish causality or rule out alternative explanations for the observed patterns.

**Low Confidence**: The generalization claims to arbitrary task combinations and the assertion that LATA will work across diverse model architectures without modification are not adequately validated. The paper's success on specific task pairs (math/code) doesn't guarantee similar performance for unrelated tasks.

## Next Checks

1. **Cross-model validation**: Test LATA on a third architecture (e.g., Mistral-7B) with both similar and dissimilar task pairs to verify the layer-wise disentanglement mechanism generalizes beyond Llama-3-8B and Gemma-2-9B.

2. **Ablation on weighting schemes**: Systematically compare Linear vs Logarithmic Drop-by-Rank across different model sizes to identify conditions where each performs better, and develop a principled selection criterion rather than ad-hoc choice.

3. **Robustness to threshold variation**: Conduct a parameter sensitivity analysis for σ in Drop-with-Threshold, mapping the relationship between threshold choice, retained parameter percentage, and downstream utility to establish safe operating bounds.