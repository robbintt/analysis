---
ver: rpa2
title: 'HGOT: Self-supervised Heterogeneous Graph Neural Network with Optimal Transport'
arxiv_id: '2506.02619'
source_url: https://arxiv.org/abs/2506.02619
tags:
- graph
- transport
- optimal
- heterogeneous
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of self-supervised learning
  on heterogeneous graphs, which requires effective strategies for graph augmentation
  and sampling positive/negative pairs. The proposed HGOT method eliminates the need
  for graph augmentation and positive/negative sample selection by employing optimal
  transport theory.
---

# HGOT: Self-supervised Heterogeneous Graph Neural Network with Optimal Transport

## Quick Facts
- **arXiv ID**: 2506.02619
- **Source URL**: https://arxiv.org/abs/2506.02619
- **Reference count**: 16
- **Primary result**: Achieves state-of-the-art performance on self-supervised heterogeneous graph learning, with >6% accuracy improvement on node classification

## Executive Summary
This paper introduces HGOT, a self-supervised learning method for heterogeneous graphs that leverages optimal transport theory to eliminate the need for traditional graph augmentation and positive/negative sample selection. Unlike conventional approaches that rely on graph perturbations or careful sampling strategies, HGOT aggregates semantic information from different meta-paths into a central view and aligns optimal transport plans between meta-path views and the aggregated view to learn high-quality node representations. The method demonstrates significant performance gains across four real-world datasets, achieving an average accuracy improvement of more than 6% compared to existing methods on node classification tasks.

## Method Summary
HGOT addresses the fundamental challenge in self-supervised heterogeneous graph learning by removing the dependency on graph augmentation and positive/negative sample selection. The method operates by first extracting multiple views from different meta-paths, then aggregating these views into a central representation. Instead of using contrastive learning with sampled pairs, HGOT employs optimal transport theory to align the distribution of meta-path views with the aggregated central view. This alignment process serves as the self-supervised objective, enabling the model to learn meaningful node representations without requiring explicit augmentation strategies or careful pair selection. The approach simplifies the training pipeline while maintaining effectiveness across diverse heterogeneous graph structures.

## Key Results
- Achieves state-of-the-art performance on self-supervised heterogeneous graph learning
- Demonstrates >6% average accuracy improvement on node classification tasks across four real-world datasets
- Eliminates need for graph augmentation and positive/negative sample selection
- Shows effectiveness across diverse heterogeneous graph structures

## Why This Works (Mechanism)
The method works by leveraging the inherent semantic diversity in heterogeneous graphs through meta-paths, which capture different types of relationships between nodes. By aggregating information from multiple meta-paths and using optimal transport to align these views with a central representation, the model learns to preserve both local and global structural information without explicit augmentation. The optimal transport framework provides a principled way to measure and minimize the discrepancy between different views, creating a robust self-supervised signal that doesn't require carefully curated positive and negative samples.

## Foundational Learning

**Optimal Transport Theory**
- Why needed: Provides a principled way to measure distribution alignment without requiring paired samples
- Quick check: Verify that Wasserstein distance computation is stable across different meta-path view distributions

**Meta-path Extraction**
- Why needed: Captures semantic relationships between different node types in heterogeneous graphs
- Quick check: Ensure meta-paths are meaningful and cover diverse relationship patterns in the dataset

**Graph Neural Networks**
- Why needed: Aggregates neighborhood information while preserving node and edge type information
- Quick check: Verify that GNN layers can effectively handle heterogeneous graph structures

## Architecture Onboarding

**Component Map**
Input Graphs -> Meta-path Extraction -> Multiple View Generation -> GNN Processing -> Central View Aggregation -> Optimal Transport Alignment -> Node Representations

**Critical Path**
Meta-path extraction → Multiple view generation → GNN processing → Optimal transport alignment

**Design Tradeoffs**
- Complexity vs. performance: OT alignment adds computational overhead but eliminates augmentation complexity
- Meta-path selection: Number and quality of meta-paths directly impact performance but require domain knowledge
- Aggregation strategy: Choice of how to combine meta-path views affects the quality of the central representation

**Failure Signatures**
- Poor performance when meta-paths are noisy or unrepresentative of true graph semantics
- Computational bottlenecks when dealing with large numbers of node types and edges
- Suboptimal alignment when optimal transport plans are not well-conditioned

**First Experiments**
1. Test on small synthetic heterogeneous graphs with known meta-path structures to verify OT alignment works as expected
2. Evaluate sensitivity to different numbers of meta-paths on a benchmark dataset
3. Compare performance with and without optimal transport alignment on a validation set

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability concerns with large heterogeneous graphs containing thousands of node types and edges due to OT computational complexity
- Sensitivity to meta-path selection and the number of meta-paths used, requiring systematic evaluation across graph domains
- Lack of comprehensive ablation studies comparing OT-based alignment with simpler contrastive methods to justify the added complexity

## Confidence
**Medium**: The claimed >6% accuracy improvement is based on experiments across four datasets, but the relatively small number of benchmarks limits generalizability. The comparison could be strengthened by including more recent methods, and potential overfitting to specific evaluation datasets is not adequately addressed.

## Next Checks
1. Conduct scalability experiments on larger heterogeneous graphs (10K+ nodes with 50+ node types) to measure computational overhead and memory requirements of the OT-based alignment process
2. Perform systematic ablation studies comparing HGOT against simplified versions using basic contrastive alignment (without OT) across multiple datasets to quantify the marginal benefit of optimal transport
3. Test robustness by evaluating HGOT's performance when using different subsets and combinations of meta-paths, including cases with minimal or noisy meta-path information, to assess sensitivity to input design choices