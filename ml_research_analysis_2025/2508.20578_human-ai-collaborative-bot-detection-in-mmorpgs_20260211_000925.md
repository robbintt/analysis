---
ver: rpa2
title: Human-AI Collaborative Bot Detection in MMORPGs
arxiv_id: '2508.20578'
source_url: https://arxiv.org/abs/2508.20578
tags:
- detection
- bots
- arxiv
- data
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an unsupervised framework for detecting auto-leveling
  bots in MMORPGs using time-series representation learning and LLM-based verification.
  The approach leverages contrastive representation learning and DBSCAN clustering
  to group characters with similar leveling patterns, followed by LLM-assisted refinement
  to filter false positives.
---

# Human-AI Collaborative Bot Detection in MMORPGs

## Quick Facts
- **arXiv ID:** 2508.20578
- **Source URL:** https://arxiv.org/abs/2508.20578
- **Authors:** Jaeman Son; Hyunsoo Kim
- **Reference count:** 40
- **Primary Result:** An unsupervised LLM-augmented framework for MMORPG bot detection using contrastive representation learning

## Executive Summary
This paper introduces a hybrid approach to detect auto-leveling bots in MMORPGs by combining time-series representation learning with LLM-based verification. The method uses TS2Vec to encode level-up intervals into fixed-dimensional vectors, clusters similar behaviors using DBSCAN, and validates results with GPT-4o to reduce false positives. The approach addresses legal concerns by providing explainable detection through behavioral similarity metrics rather than opaque rule-based systems. Experimental results show the framework effectively identifies bot groups while minimizing labeling costs through its unsupervised design.

## Method Summary
The framework employs contrastive time-series representation learning (TS2Vec) to convert variable-length level-up intervals into fixed-dimensional embeddings. These embeddings are clustered using DBSCAN with density-based parameters tuned for detecting multi-character farming units. An LLM (GPT-4o) serves as a verification layer, analyzing clustered groups to filter out false positives by examining the homogeneity of level-up patterns. The system operates without labeled training data, making it adaptable to evolving bot behaviors while maintaining interpretability for legal compliance.

## Key Results
- TS2Vec embeddings successfully capture systematic bot leveling patterns distinct from human play
- DBSCAN clustering with min_sample=3 effectively isolates multi-character farming units while treating solo players as noise
- LLM verification reduces false positives by analyzing behavioral homogeneity within clusters
- The framework demonstrates high access information homogeneity in detected bot groups, indicating reliable identification

## Why This Works (Mechanism)

### Mechanism 1: Behavioral Homogeneity via Contrastive Embeddings
- Claim: If auto-leveling bots follow optimized scripts, contrastive learning maps their time-series data into tight clusters distinct from dispersed human patterns.
- Mechanism: The framework converts variable-length level-up intervals into fixed vectors using TS2Vec. Because bots repeat efficient routes, their vectors converge. Humans, playing irregularly, produce noisy embeddings that fall into low-density regions.
- Core assumption: Bot behavior is sufficiently systematic to produce distinct "fingerprints" in the latent space compared to the variance of human play.
- Evidence anchors:
  - [abstract] "...leveraging contrastive representation learning... to identify groups of characters with similar level-up patterns."
  - [section 3.2.1] "...bots—whose leveling behavior is highly systematic—to form dense clusters, while human players... remain isolated."
  - [corpus] Related work "On the efficacy of old features for the detection of new bots" suggests static features decay in utility; learned representations offer an adaptive alternative.
- Break condition: If bots introduce sufficient random noise (jitter) into their leveling intervals, the embedding distinctiveness degrades.

### Mechanism 2: Density-Based "Farming Unit" Isolation
- Claim: Configuring DBSCAN with `min_sample=3` specifically targets multi-boxing farming units while treating solo players (bots or humans) as noise.
- Mechanism: The algorithm requires a minimum density to form a cluster. By setting the threshold to 3, it exploits the operational constraint that farmers often run "farming units" of 3+ characters simultaneously.
- Core assumption: The majority of auto-leveling activity involves coordinated groups rather than solo accounts, and `eps` (epsilon) is correctly tuned to local data density.
- Evidence anchors:
  - [abstract] "...DBSCAN clustering to group characters with similar leveling patterns..."
  - [section 3.2.2] "In our study, we set min_sample = 3, based on the observation that bot activity in the field typically occurs in groups of three or more."
  - [corpus] "A Framework for Mining Collectively-Behaving Bots in MMORPGs" [90084] validates the focus on collective behavior, though corpus evidence for the specific "group of 3" heuristic is derived primarily from the paper's internal domain knowledge.
- Break condition: If farming strategies shift to solo-account operations or highly dispersed networks, this density filter will generate false negatives.

### Mechanism 3: LLM-Based Semantic Verification
- Claim: Large Language Models (LLMs) can serve as a semantic layer to catch false positives that statistical clustering misses, provided they receive structured numerical summaries.
- Mechanism: Clustering output (grouped level-up intervals) is formatted as text. The LLM (GPT-4o) uses Chain-of-Thought (CoT) reasoning to compare intervals within a cluster. It flags clusters where "homogeneity" implies bot behavior or rejects clusters where legitimate play is detected.
- Core assumption: LLMs possess sufficient reasoning capability to interpret numerical time-series differences and "simulate" human moderator judgment without hallucinating constraints.
- Evidence anchors:
  - [abstract] "...incorporate a Large Language Model (LLM) as an auxiliary reviewer to validate the clustered groups..."
  - [section 4.4.2] "Experimental results show that the LLM effectively filtered out normal users... leading to a meaningful reduction in the access information homogeneity score."
  - [corpus] "Can LLMs Serve As Time Series Anomaly Detectors?" [4] (cited in paper) provides the theoretical basis, while "RoBCtrl" [65618] warns generally of adversarial attacks on detectors, suggesting the LLM adds a robust (though not invulnerable) verification layer.
- Break condition: If the prompt context window is exceeded by massive clusters or the LLM hallucinates a pattern match, false positives may increase.

## Foundational Learning

### Concept: Contrastive Time-Series Representation (e.g., TS2Vec)
- Why needed here: Raw time-series data varies in length and is difficult to cluster directly. Representation learning projects this data into a fixed-dimensional vector space where similarity equates to Euclidean distance.
- Quick check question: Does the model map two identical leveling sequences to the exact same vector? (A: Ideally close, but robust to slight perturbations).

### Concept: DBSCAN (Density-Based Spatial Clustering)
- Why needed here: Unlike K-Means, DBSCAN does not require specifying the number of clusters beforehand and naturally handles outliers (noise), which is critical for isolating human players from bot clusters.
- Quick check question: What happens to a solo bot character if `min_sample` is set to 3? (A: It is labeled as noise/outlier and not clustered).

### Concept: Access Information Homogeneity
- Why needed here: This is the primary metric for "bot-ness." It measures if characters in a cluster share login/access metadata (e.g., IP, device), implying a single controller. Lower scores = higher similarity = better detection.
- Quick check question: If a cluster has high behavioral similarity but high access information homogeneity, is it likely a bot group? (A: No, it might be a legitimate guild; high homogeneity implies different humans).

## Architecture Onboarding

### Component Map:
Input level-up logs → Preprocessing (cap at Level 50, convert to intervals) → TS2Vec Encoder → Fixed-dimensional embeddings → DBSCAN Clustering → LLM Verification (GPT-4o) → Output: Bot cluster groups

### Critical Path:
The quality of the TS2Vec embeddings determines the cluster purity. If the encoder fails to separate bots from humans, the LLM verification stage becomes overwhelmed.

### Design Tradeoffs:
- **Unsupervised vs. Supervised:** Zero labeling cost vs. potential for concept drift (bots changing behavior unnoticed)
- **Recall vs. Precision:** Setting `eps` too low misses bots (low recall); setting it too high mixes humans (low precision). The paper prioritizes precision (low false positives) for legal safety

### Failure Signatures:
- **Cluster Contamination:** Access Info Homogeneity remains high after LLM filtering
- **Empty Output:** `eps` is too strict for the specific game data density
- **Hallucinated Explanations:** LLM output contradicts the numerical interval data provided

### First 3 Experiments:
1. **Baseline Representation:** Compare TS2Vec vs. Autoencoder vs. DTW using Kendall's Tau on perturbed sequences (Table 1)
2. **Parameter Sensitivity:** Run DBSCAN with varying `eps` (fixed vs. quantile-based) to maximize Acc_info scores (Table 2)
3. **LLM Ablation:** Compare clustering results with and without the LLM verification step to measure the reduction in false positives (Table 2, G1/G2/G3 comparison)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the framework enable Large Language Models (LLMs) to process raw, high-granularity time-series logs directly without suffering from input length limitations?
- Basis in paper: [explicit] Page 3 notes that evaluating GPT-4o with raw logs "suffered from input length limitations and model constraints, resulting in unreliable outputs," necessitating the hybrid approach.
- Why unresolved: The current context window and numerical reasoning capabilities of standard LLMs cannot handle the sheer volume of raw temporal data required for primary detection.
- What evidence would resolve it: Successful application of long-context window models or specialized time-series foundation models on unprocessed level-up logs.

### Open Question 2
- Question: What is the quantitative precision and recall of the LLM verification step compared to human expert ground truth?
- Basis in paper: [inferred] The paper evaluates LLM effectiveness via proxy metrics like access information homogeneity but does not provide a direct accuracy comparison between the LLM's judgments and human game masters regarding false positives.
- Why unresolved: The authors rely on behavioral similarity metrics to imply correctness rather than validating the LLM's specific decisions against a labeled "gold standard" of banned players.
- What evidence would resolve it: A comparative study measuring the agreement rate (e.g., Cohen's Kappa) and error types between the LLM and human moderators on the same cluster sets.

### Open Question 3
- Question: Can the representation model effectively detect bots that intentionally inject noise or randomization into their leveling patterns?
- Basis in paper: [inferred] The method relies on the assumption that bot behavior is "highly systematic" (Page 2) and forms dense clusters; it is unclear how the model handles adversarial attacks where bots mimic the "irregular progression patterns" of humans.
- Why unresolved: The evaluation perturbation strategy (Page 3) adds noise mathematically but may not simulate the strategic, non-random variability used by sophisticated bot developers to evade detection.
- What evidence would resolve it: Testing the framework against bots specifically programmed to vary their leveling routes and timelines stochastically.

## Limitations

- The approach relies heavily on the assumption that bot leveling behavior produces sufficiently distinct temporal patterns for contrastive learning to capture
- The specific choice of `min_sample=3` for DBSCAN is based on domain knowledge rather than systematic analysis of all possible farming configurations
- The LLM verification step introduces potential variability in decision-making that isn't fully characterized

## Confidence

- **High Confidence:** The fundamental architecture (TS2Vec + DBSCAN) is technically sound and well-established in the literature
- **Medium Confidence:** The specific parameter choices (eps quantile method, min_sample=3) are reasonable but may require tuning for different game environments
- **Medium Confidence:** The LLM verification mechanism improves precision, but the extent of this improvement and its consistency across different scenarios requires further validation

## Next Checks

1. Test the framework across multiple MMORPG games with different leveling mechanics to assess generalizability
2. Conduct adversarial testing by introducing controlled noise into bot leveling patterns to measure the robustness of the contrastive embeddings
3. Perform a systematic ablation study varying DBSCAN parameters (min_sample, eps) across different population densities to identify optimal configurations for diverse scenarios