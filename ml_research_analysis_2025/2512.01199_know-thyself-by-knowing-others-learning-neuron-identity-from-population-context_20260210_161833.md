---
ver: rpa2
title: 'Know Thyself by Knowing Others: Learning Neuron Identity from Population Context'
arxiv_id: '2512.01199'
source_url: https://arxiv.org/abs/2512.01199
tags:
- neurons
- neuron
- across
- representations
- population
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NuCLR, a self-supervised framework for learning
  neuron-level representations from large-scale neural population activity. The core
  method uses a spatiotemporal transformer with a contrastive objective to learn representations
  that capture neuron identity (cell type, brain region) from activity alone.
---

# Keep Thyself by Knowing Others: Learning Neuron Identity from Population Context

## Quick Facts
- arXiv ID: 2512.01199
- Source URL: https://arxiv.org/abs/2512.01199
- Reference count: 40
- One-line result: NuCLR learns neuron identity representations from population activity that generalize across animals and sessions

## Executive Summary
This paper introduces NuCLR, a self-supervised framework for learning neuron-level representations from large-scale neural population activity. The core method uses a spatiotemporal transformer with a contrastive objective to learn representations that capture neuron identity (cell type, brain region) from activity alone. By integrating population context through permutation-equivariant attention and enforcing temporal stability, NuCLR produces embeddings that generalize robustly across animals and sessions.

Key results show NuCLR achieves state-of-the-art performance on cell type and brain region decoding tasks across multiple electrophysiology and calcium imaging datasets. The method demonstrates strong zero-shot generalization to unseen animals, requires minimal labeled data (only 12.5% needed for competitive performance), and shows consistent performance gains with increased pretraining data scale. This work presents the first systematic scaling analysis for neuron-level representation learning and establishes a foundation for identity-aware neural models that can transfer across experimental conditions.

## Method Summary
NuCLR learns neuron representations through self-supervised contrastive learning on population activity. The method takes population recordings (N neurons × T timesteps), bins spikes to 20ms resolution, and partitions into 1s patches. Two views of each neuron are sampled from nearby time windows, and a contrastive objective pulls representations of the same neuron together while pushing apart all other neurons in the population. A spatiotemporal transformer processes these views, with initial layers handling temporal dynamics per neuron, followed by spatial attention layers that integrate population context in a permutation-equivariant manner. The resulting embeddings capture neuron identity and can be used for downstream tasks like cell type and brain region classification through linear probes.

## Key Results
- NuCLR achieves state-of-the-art performance on cell type and brain region decoding across electrophysiology and calcium imaging datasets
- Strong zero-shot generalization to unseen animals, with 12.5% labeled data achieving competitive performance
- Consistent performance improvements with increased pretraining data scale across animals
- Ablation studies show population context and spatial attention are critical for performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive objectives over temporal views of the same neuron yield identity-specific representations
- Mechanism: Two views of each neuron's activity are sampled from nearby time windows. An InfoNCE loss pulls representations of the same neuron together while pushing apart all other neurons in the population, creating embeddings that are both temporally stable and discriminative within the population
- Core assumption: Neuron identity (cell type, brain region) is invariant across time and behavioral conditions within a session
- Evidence anchors:
  - [abstract] "NuCLR brings together views of the same neuron observed at different times... and uses a contrastive objective to pull these representations together."
  - [section 2.2] Eq. 7 defines the loss where same-neuron pairs are positives and all other neurons in the population are negatives
  - [corpus] Related work "Neuron Platonic Intrinsic Representation From Dynamics Using Contrastive Learning" also uses contrastive learning for neuron representations (FMR unavailable, minimal direct overlap)
- Break condition: If neuron response properties change substantially across time or states (e.g., state-dependent modulation), single-neuron temporal stability may not hold, weakening the positive pair assumption

### Mechanism 2
- Claim: Population context, not just single-neuron activity, is necessary to resolve neuron identity
- Mechanism: Spatial attention layers allow each neuron to attend to all other neurons at each time point, integrating population-wide activity patterns. This enables the model to learn how each neuron's activity relates to others, which carries identity information beyond intrinsic firing statistics
- Core assumption: Neurons of the same type or region exhibit coordinated activity patterns that are informative for classification
- Evidence anchors:
  - [abstract] "To capture population context without assuming any fixed neuron ordering, we build a spatiotemporal transformer that integrates activity in a permutation-equivariant manner."
  - [section 3.5] Ablating spatial attention drops Allen VC cell-type F1 from 0.72 to 0.55 and IBL brain-region F1 from 0.53 to 0.36
  - [corpus] No direct corpus evidence on population context specifically; related papers focus on single-neuron representations
- Break condition: If population recordings are sparse (few simultaneously recorded neurons) or lack coordinated structure, spatial attention provides weak signal

### Mechanism 3
- Claim: Scaling pretraining data across animals improves zero-shot generalization
- Mechanism: More animals during pretraining expose the model to a broader distribution of neuron types and recording conditions, yielding representations that transfer to unseen subjects without retraining
- Core assumption: Neuron identity features are shared across individuals of the same species despite inter-animal variability
- Evidence anchors:
  - [abstract] "Increasing the number of animals used during pretraining consistently improves downstream performance."
  - [section 3.4, Figure 2B] Doubling pretraining sessions improves cell-type decoding more than doubling labeled data on Allen VC
  - [corpus] No corpus papers evaluate scaling laws for neuron-level representations
- Break condition: If test animals have systematically different recording conditions, species, or pathology, cross-animal transfer may degrade

## Foundational Learning

- Concept: **Contrastive Learning (InfoNCE)**
  - Why needed here: The self-supervised objective relies on distinguishing same-neuron pairs from all other neurons. Understanding how positives/negatives are defined and how temperature affects embedding geometry is essential
  - Quick check question: Given a batch with 100 neurons per view, how many negative samples does each positive pair have in standard SimCLR vs. NuCLR's per-population formulation?

- Concept: **Permutation Equivariance in Transformers**
  - Why needed here: The spatial transformer must process neuron sets without assuming a fixed ordering (neurons have no natural sequence). Equivariance ensures the same neuron gets the same representation regardless of input ordering
  - Quick check question: If you shuffle the input neuron order, which output changes and which stays the same for an equivariant vs. invariant function?

- Concept: **Temporal vs. Spatial Attention Factorization**
  - Why needed here: NuCLR separates temporal processing (within-neuron dynamics) from spatial processing (across-neuron context). This factorization reduces computation and allows specialized positional encodings
  - Quick check question: Why is the attention complexity O(N²·P) for naive joint spatiotemporal attention vs. O(N·P² + N²·P) for the factorized version with N neurons and P patches?

## Architecture Onboarding

- Component map: Spike binning (20ms) → temporal patching (1s patches) → linear projection to D=256 tokens → 2 temporal-only self-attention layers → 2 spatio-temporal layers (alternating spatial and temporal attention) → mean pooling → 256-dim neuron embedding → projection head (MLP)

- Critical path:
  1. Load population activity matrix (N neurons × T timesteps)
  2. Bin and patch to create N × P token sequences
  3. Apply temporal layers independently per neuron
  4. Apply spatio-temporal layers with cross-neuron attention
  5. Pool and project for contrastive loss

- Design tradeoffs:
  - Temporal-only initial layers: Lower compute (P≈10 << N≈100), but ablation shows minimal accuracy difference (Section A.5)
  - Per-population negatives vs. global negatives: Limits negatives to hundreds rather than thousands, avoiding easy-negative saturation; uses decoupled contrastive loss (DCL) to compensate
  - Neuron dropout (up to 50%): Regularizes for partial observations; helps small datasets (Allen VC) more than large ones (IBL)

- Failure signatures:
  - Embeddings cluster by session/subject rather than cell type: Indicates insufficient pretraining diversity (seen in small datasets like Bugeon, Steinmetz in Section C)
  - Probe accuracy plateaus early with flat loss: Check if negatives are too easy (increase population size) or positives are misaligned (check view sampling window ∆Tmax)
  - Spatial ablation causes <10% drop: Population context may be uninformative; verify dataset has coordinated activity

- First 3 experiments:
  1. Reproduce the spatial attention ablation on IBL (Table 3): Train full NuCLR vs. temporal-only variant and compare brain-region F1. Expect ~0.17 absolute drop
  2. Vary pretraining scale on Allen VC: Train with 6, 20, and 42 sessions and plot zero-shot cell-type F1 vs. number of labeled neurons (Figure 2B replication)
  3. Test bin-size sensitivity: Sweep 10ms, 20ms, 50ms on Allen VC and IBL; expect <5% F1 variation per Figure 3

## Open Questions the Paper Calls Out

- Can NuCLR maintain robust neuron identity decoding when tested on datasets with fundamentally different stimulus distributions, sensory modalities, or behavioral states than those seen during pretraining? [explicit] The authors state: "testing and building robustness to changes in the underlying stimulus distribution will be an interesting line of future research" and note that "our current evaluations use datasets with similar underlying task structure." [basis] All evaluated datasets involve visual stimulation tasks with related structures; no experiments tested cross-modal or cross-task generalization. [what evidence would resolve it] Pretraining on visual cortex recordings and evaluating on auditory, somatosensory, or motor cortex datasets without modification.

- Can a single unified model jointly learn transferable neuron representations across both electrophysiology and calcium imaging modalities? [explicit] The authors note they "currently need two different models for each modality" and state that "building a unified pretraining approach for both modalities would allow for a joint model pretraining." [basis] Current architecture handles modalities separately with different preprocessing (binning vs. direct fluorescence patching), and no unified training was attempted. [what evidence would resolve it] Training a single model on combined electrophysiology and calcium datasets, then evaluating whether cross-modality transfer improves or degrades performance on either modality.

- What additional latent neuronal attributes beyond cell type and brain region are captured in NuCLR's learned representations? [inferred] The paper states representations should capture "cell type, brain region, connectivity, and potentially other latent attributes" but only evaluates the first two. The scaling results suggest information content increases with more data, implying unexplored attributes may be encoded. [basis] Only cell type and brain region classification were systematically evaluated; connectivity patterns, projection targets, and other identity features were not probed. [what evidence would resolve it] Training classifiers or probes for known connectivity patterns (e.g., from tracer studies) or projection targets using NuCLR embeddings, and correlating embedding distances with functional connectivity measures.

## Limitations

- Contrastive learning mechanism critically depends on neuron identity being temporally stable within sessions, but the paper provides limited validation of this assumption across different behavioral states or recording conditions
- Claims about population context being necessary (vs. helpful) lack direct ablation studies comparing against strong single-neuron baselines
- Cross-animal generalization improvements assume consistent feature representation across subjects, but inter-animal variability effects are not quantified

## Confidence

- High confidence: Contrastive loss formulation, transformer architecture implementation, and the basic finding that population context improves performance relative to temporal-only models
- Medium confidence: Scaling benefits from increased pretraining data and zero-shot generalization capabilities across animals
- Low confidence: Claims about population context being *necessary* (vs. helpful) and the specific mechanisms by which contrastive learning captures neuron identity

## Next Checks

1. Test temporal stability assumption by comparing NuCLR performance when training on active vs. passive behavioral states within the same recording session
2. Implement and compare against a strong single-neuron contrastive baseline to directly assess whether population context provides essential signal vs. redundant information
3. Quantify inter-animal variability by measuring embedding similarity for the same cell type across different subjects and correlating with cross-animal transfer performance