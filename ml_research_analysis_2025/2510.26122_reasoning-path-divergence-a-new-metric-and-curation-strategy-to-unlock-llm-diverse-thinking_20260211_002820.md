---
ver: rpa2
title: 'Reasoning Path Divergence: A New Metric and Curation Strategy to Unlock LLM
  Diverse Thinking'
arxiv_id: '2510.26122'
source_url: https://arxiv.org/abs/2510.26122
tags:
- solutions
- solution
- pass
- problem
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of low diversity in LLM reasoning
  outputs caused by one-problem-one-solution (1P1S) training, which limits test-time
  scaling (TTS) performance. The authors introduce a one-problem-multiple-solutions
  (1PNS) paradigm that trains models on diverse reasoning trajectories per problem.
---

# Reasoning Path Divergence: A New Metric and Curation Strategy to Unlock LLM Diverse Thinking

## Quick Facts
- **arXiv ID:** 2510.26122
- **Source URL:** https://arxiv.org/abs/2510.26122
- **Reference count:** 40
- **Key outcome:** Training on diverse reasoning paths (1PNS) with RPD curation yields +2.80% pass@16 gain over 1P1S baseline, proving diversity boosts test-time scaling.

## Executive Summary
This paper addresses the bottleneck of low diversity in LLM reasoning outputs, which limits test-time scaling (TTS) performance. The authors introduce a "one problem, multiple solutions" (1PNS) paradigm and Reasoning Path Divergence (RPD), a step-level metric that uses LLM summaries and asymmetric matching to quantify semantic differences between reasoning strategies. By curating maximally diverse solution sets from the OpenThought3 dataset and fine-tuning Qwen3-4B-Base, they demonstrate that RPD-selected 1PNS training yields more varied outputs and higher pass@k scores—up to +4.99% on AIME24—validating that diversity-centric training unlocks better TTS.

## Method Summary
The authors propose a two-stage curation pipeline to train LLMs on diverse reasoning paths. First, they use an LLM to summarize each solution into 3-5 logical steps, then embed these steps with a separate embedding model. The RPD metric computes asymmetric semantic distance between solution pairs by finding minimum step-wise cosine distances. Problems are ranked by their maximum average pairwise RPD, and for each top problem, a greedy algorithm selects the M most diverse solutions. This 1PNS dataset trains Qwen3-4B-Base (via QLoRA) for 12 epochs, contrasting with 1P1S baselines of equal size. Inference uses nucleus sampling (temp=0.6, top_p=0.95) to evaluate pass@k performance.

## Key Results
- RPD-based 1P3S training achieves +2.80% average pass@16 gain over 1P1S baseline and +4.99% on AIME24.
- The method also increases output diversity, measured by RPD and Div-Self-BLEU.
- RPD outperforms raw embedding baselines in selecting diverse solution pairs (53% vs 40% success rate).
- Post-RL fine-tuning, RPD-1P3S initialization consistently outperforms 1P1S baseline across all benchmarks.

## Why This Works (Mechanism)

### Mechanism 1: Diversity-Centric Data Curation
Standard 1P1S training causes "mode collapse," pushing models toward a single canonical reasoning path and limiting exploration during test-time sampling. 1PNS training on maximally diverse reasoning paths per problem teaches the model that multiple valid strategies exist, expanding its output distribution. This expanded distribution provides better coverage of the solution space during sampling, increasing the probability that at least one path is correct (higher pass@k). The gains depend on the domain benefiting from sampling and problems having multiple valid strategies.

### Mechanism 2: Reasoning Path Divergence (RPD) as a Semantic Distance Metric
RPD works by summarizing solutions into 3-5 logical steps, embedding each step, and asymmetrically matching steps between solutions to find minimum distances. This targets the "logic" or "strategy" of solutions, distinguishing genuinely different reasoning methods from mere rewording. The asymmetric design is robust to differences in solution verbosity and mitigates the impact of step order. RPD's effectiveness relies on the LLM summarizer reliably extracting strategic steps and the embedding model capturing their semantic meaning.

### Mechanism 3: Improved Initialization for Reinforcement Learning
1P1S training leads to a "homogenous solution path," severely limiting RL exploration and confining it to narrow local optima. By training on diverse 1PNS data first, the initial policy for RL already has broader solution space coverage. This expanded prior allows the RL agent to explore more effectively from the start, leading to faster convergence and higher final performance. The benefits depend on the RL algorithm responding to a more diverse initialization.

## Foundational Learning

- **Concept: Test-Time Scaling (TTS)**
  - **Why needed here:** TTS methods like Best-of-N rely on sampling multiple outputs at inference time. The paper argues TTS effectiveness is fundamentally limited by the *diversity* of these outputs.
  - **Quick check question:** Can you explain why generating 16 identical reasoning paths at test time is less useful than generating 16 semantically distinct paths, even if they are all incorrect?

- **Concept: Supervised Fine-Tuning (SFT) and Mode Collapse**
  - **Why needed here:** SFT with one-solution-per-problem datasets causes low output diversity. Understanding this "mode collapse" is essential to appreciating why a multi-solution training paradigm is necessary.
  - **Quick check question:** If you fine-tune a model on a dataset where every math problem has only a single canonical solution, what behavior would you expect when you sample from that model at a high temperature?

- **Concept: Greedy Selection Algorithm**
  - **Why needed here:** The curation pipeline uses a greedy algorithm to select a set of solutions that are maximally distant from each other. This is a practical, actionable component of their method.
  - **Quick check question:** Given a set of 10 candidate solutions and a distance metric, describe a greedy process to select the 3 solutions that are the most diverse from one another.

## Architecture Onboarding

- **Component map:** Summarizer LLM -> Embedding Model -> RPD Calculator -> Curation Pipeline
- **Critical path:** The RPD Calculator is most critical; the method's success hinges on this metric accurately reflecting semantic reasoning diversity, not just textual difference. The quality of the Summarizer LLM is a direct dependency.
- **Design tradeoffs:**
  - **Summarizer Model Size vs. Cost:** RPD metric is robust to using a smaller 7B summarizer instead of 14B, reducing computational overhead.
  - **Solutions per Problem (M):** There's a tradeoff between diversity depth and problem breadth. The paper found 3 solutions per problem (1P3S) worked best, with performance declining at 1P4S or 1P5S.
- **Failure signatures:**
  - **RPD Metric Failure:** If RPD scores don't correlate with human judgment of strategic diversity, the entire selection pipeline is flawed. Check by manual validation on sample pairs.
  - **Summarization Failure:** If the LLM summarizer produces overly abstract or incorrect step descriptions, the RPD score is meaningless. Mitigated by detailed prompt design.
  - **Low-N Pass@k Gains:** Gains are most significant at higher k values (e.g., pass@16). If your application requires high pass@1 accuracy, benefits may be limited.
- **First 3 experiments:**
  1. **Validate the RPD Metric:** Manually compare RPD scores against raw embedding distance and your own judgment on a small set of problems with known diverse solutions.
  2. **Run a Small-Scale 1P3S SFT Experiment:** Curate 100 problems × 3 solutions = 300 samples using the RPD pipeline and fine-tune a base model. Compare pass@k against a randomly curated 1P1S baseline of same size.
  3. **Analyze Output Diversity:** Generate multiple samples from your 1P3S-fine-tuned model and 1P1S baseline for test problems. Manually inspect whether the 1P3S model produces genuinely different reasoning paths.

## Open Questions the Paper Calls Out
- How does the optimal number of solutions per problem (e.g., 1P3S vs 1P5S) vary across different datasets, domains, and model scales?
- Does the 1PNS paradigm with RPD curation scale effectively to much larger models (70B+ parameters)?
- How does RPD-curated 1PNS training interact with different RL algorithms beyond GRPO?
- Can RPD or similar step-level diversity metrics be adapted for multi-modal reasoning (vision-language models) or structured outputs beyond text?

## Limitations
- **Metric Validity:** RPD's reliance on an LLM summarizer introduces uncertainty about consistency and generalizability across domains.
- **Dataset Dependency:** Results are based on OpenThought3 math problems; effectiveness for other domains remains untested.
- **Statistical Reporting:** Inconsistent numbers of runs across benchmarks make it difficult to assess statistical significance of all reported gains.

## Confidence
- **High Confidence:** The core mechanism that 1P1S training limits output diversity, and that 1PNS training can improve pass@k performance, is well-supported.
- **Medium Confidence:** RPD is superior to raw embedding distance for quantifying reasoning path diversity on OpenThought3 dataset.
- **Low Confidence:** The hypothesis that diverse SFT initialization improves RL exploration is plausible but not definitively proven.

## Next Checks
1. **Cross-Domain Applicability:** Test RPD and 1PNS training on a non-math dataset (e.g., code generation) to assess generalizability.
2. **Human Validation of RPD:** Conduct a human study comparing RPD scores against expert judgment of solution diversity on a sample of problems.
3. **Statistical Robustness:** Report results with consistent numbers of runs across all benchmarks and provide confidence intervals or significance tests.