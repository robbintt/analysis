---
ver: rpa2
title: On Discovering Algorithms for Adversarial Imitation Learning
arxiv_id: '2510.00922'
source_url: https://arxiv.org/abs/2510.00922
tags:
- reward
- learning
- policy
- functions
- dail
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the instability problem in Adversarial Imitation
  Learning (AIL) by focusing on the often-overlooked reward assignment (RA) function,
  which maps discriminator outputs to rewards for policy optimization. The authors
  propose discovering RA functions via LLM-guided evolutionary search, resulting in
  Discovered Adversarial Imitation Learning (DAIL), the first meta-learned AIL algorithm.
---

# On Discovering Algorithms for Adversarial Imitation Learning

## Quick Facts
- arXiv ID: 2510.00922
- Source URL: https://arxiv.org/abs/2510.00922
- Reference count: 40
- One-line primary result: Discovered reward assignment function achieves 20% reduction in Wasserstein distance and 12.5% improvement in normalized returns compared to GAIL across unseen environments.

## Executive Summary
This work addresses the instability problem in Adversarial Imitation Learning (AIL) by focusing on the often-overlooked reward assignment (RA) function, which maps discriminator outputs to rewards for policy optimization. The authors propose discovering RA functions via LLM-guided evolutionary search, resulting in Discovered Adversarial Imitation Learning (DAIL), the first meta-learned AIL algorithm. DAIL outperforms human-designed baselines (GAIL, AIRL, FAIRL, GAIL-heuristic) across unseen Brax and Minatar environments, achieving 20% reduction in Wasserstein distance and 12.5% improvement in normalized returns compared to GAIL. The discovered RA function, a bounded S-shaped function, provides more informative learning signals, leading to sharper policy entropy and improved training stability. DAIL also generalizes to unseen policy optimization algorithms like A2C.

## Method Summary
The paper proposes discovering reward assignment functions for adversarial imitation learning through LLM-guided evolutionary search. The framework treats RA function discovery as a meta-learning problem where an LLM acts as an intelligent crossover and mutation operator. A population of RA functions (including baselines like GAIL, AIRL, FAIRL) is evolved using Wasserstein distance between learned and expert policies as the fitness metric. The search space consists of code snippets representing different RA functions, with the LLM generating new candidates by intelligently combining parent functions. The best-performing function discovered is a bounded S-shaped function that significantly outperforms human-designed alternatives.

## Key Results
- Discovered RA function achieves 20% reduction in Wasserstein distance compared to GAIL across unseen environments
- 12.5% improvement in normalized returns over GAIL baseline
- DAIL generalizes to unseen policy optimization algorithms (A2C) without retraining
- RA function provides more informative learning signals, leading to sharper action distributions and greater training stability

## Why This Works (Mechanism)

### Mechanism 1
The discovered reward assignment function ($r_{disc}$) produces more stable training by filtering out noisy learning signals from low-quality state-action pairs. $r_{disc}$ is bounded in [0, 1] and saturates near zero for highly negative log-density ratios (which correspond to near-random policy behavior). This prevents the policy from being rewarded for uninformative actions, unlike GAIL which assigns high positive rewards in this regime, leading to noisy gradients and instability. Low log-density ratios are indicative of random or low-quality behavior and should not be rewarded. This links reward magnitude to action quality.

### Mechanism 2
An LLM-guided evolutionary search can discover novel, high-performing reward assignment functions that generalize beyond human-designed heuristics. The framework uses an LLM to perform intelligent crossover and mutation over a population of RA functions represented as code. The search is guided by a fitness metric—the Wasserstein distance between the learned policy and expert—selecting for functions that minimize this divergence effectively. The LLM's pre-trained knowledge helps propose viable candidates.

### Mechanism 3
Decomposing AIL into Density Ratio (DR) estimation and Reward Assignment (RA) allows for independent optimization of the learning signal. By fixing DR estimation (standard discriminator) and optimizing the RA function, the authors directly shape the learning signal. DAIL's specific function ($r_{disc}$) provides a more informative gradient profile (S-shaped, sharper) than baselines, leading to faster and more stable policy improvement.

## Foundational Learning

- **Concept:** f-divergence minimization in Imitation Learning
  - **Why needed here:** AIL is theoretically grounded in minimizing a divergence (like Jensen-Shannon in GAIL) between expert and policy occupancy measures. Different algorithms correspond to different $f$-divergences and thus different RA functions.
  - **Quick check question:** Which $f$-divergence does GAIL minimize and what is its corresponding Reward Assignment function?

- **Concept:** Generative Adversarial Networks (GANs) and training instability
  - **Why needed here:** AIL is a GAN-like game. DAIL's core motivation is addressing the inherent instability of this min-max optimization. Understanding GAN dynamics (mode collapse, vanishing gradients) is crucial for stability analysis.
  - **Quick check question:** In a GAN, what is the role of the discriminator's loss and how does a poor discriminator affect the generator's training?

- **Concept:** Evolutionary Algorithms and Meta-Learning
  - **Why needed here:** DAIL is discovered via LLM-guided evolutionary search, a form of black-box meta-learning. Understanding concepts like population, crossover, and fitness evaluation is necessary to comprehend the discovery process.
  - **Quick check question:** In an evolutionary algorithm, what is the purpose of "crossover" and how does the LLM facilitate it here?

## Architecture Onboarding

- **Component map:** Discriminator (Critic) -> Reward Assignment Function (RA) -> Policy (Actor)

- **Critical path:**
  1. Initialize: Create population of RA functions (GAIL, AIRL, FAIRL code)
  2. Evolutionary Loop:
     a. Crossover/Mutation: Prompt LLM with parent functions and scores to generate new candidate RA code
     b. Fitness Evaluation: For each candidate, train full AIL system on source environment, compute Wasserstein distance between final policy and expert
     c. Selection: Rank by Wasserstein distance, select top K for next generation
  3. Deploy: Extract best-performing RA function ($r_{disc}$) as static DAIL component

- **Design tradeoffs:**
  - Static vs. Adaptive RA: DAIL uses a fixed function; time-aware adaptive functions could be more flexible but add complexity
  - Boundedness vs. Expressiveness: [0,1] bounds promote stability; unbounded rewards (FAIRL) risk destabilizing gradients
  - Search Cost vs. Performance: Evolutionary search is expensive but yields a superior, generalizable component

- **Failure signatures:**
  - No convergence during search: Wasserstein distance doesn't decrease—LLM generating poor code or search hyperparameters too low
  - High policy entropy: Policy entropy remains high—RA function providing weak/noisy learning signal
  - Unstable training loss: Oscillating/diverging discriminator or value loss—RA function producing exploding rewards

- **First 3 experiments:**
  1. Reproduction & Generalization: Implement DAIL with $r_{disc}(x) = 0.5 \cdot sigmoid(x) \cdot [tanh(x) + 1]$. Train on held-out Brax environment (e.g., `Ant`), compare against GAIL/AIRL baselines
  2. Ablation Study: Test component functions individually (e.g., just $sigmoid(x)$ or $0.5 \cdot [tanh(x) + 1]$) to verify combined form's superiority
  3. Cross-Optimizer Validation: Train with different policy optimizer (e.g., A2C instead of PPO) to test generalization claim across optimizers

## Open Questions the Paper Calls Out

### Open Question 1
Can dynamic, time-aware reward assignment functions (RA) that adapt to the training state outperform the static function discovered by DAIL? The authors note that DAIL's RA function is static and suggest exploring "time-aware RA functions—those that condition on the training state" to yield richer signals. The current meta-learning search space was restricted to functions dependent only on the current log-density ratio, explicitly excluding temporal dynamics like the number of remaining updates.

### Open Question 2
Is it possible to derive theoretical convergence guarantees for meta-learned RA functions like DAIL that do not correspond to valid $f$-divergences? The conclusion states that because the discovered function $r_{disc}$ does not correspond to a valid $f$-divergence, "it therefore lacks theoretical guarantees." The authors relaxed the constraint of matching known divergences to maximize empirical performance and expressiveness, trading off theoretical safety for data-driven efficacy.

### Open Question 3
Does incorporating environment-specific information and training dynamics into the LLM context improve the efficiency or quality of RA function discovery? The authors suggest that "including more information into the LLM's context such as environment information and training state may facilitate more effective crossovers." The current framework uses a generalized prompt strategy to encourage generalization, leaving the specific impact of context-rich prompts on the evolutionary search process untested.

## Limitations
- The effectiveness of LLM-guided search relies on LLM code quality and fitness function appropriateness; poor outputs or inadequate metrics could lead to suboptimal discovery
- Study focuses on specific environments (Brax and MinAtar); generalization to more complex, high-dimensional tasks or sparse reward settings remains untested
- The specific discovered function, while empirically effective, lacks theoretical guarantees since it doesn't correspond to a valid $f$-divergence

## Confidence
- **High Confidence:** The core claim that the reward assignment function is a critical, often-overlooked component for AIL stability is well-supported by ablation studies and theoretical grounding in divergence minimization. The empirical improvement of DAIL over baselines (20% W-distance reduction, 12.5% normalized return gain) is clearly demonstrated.
- **Medium Confidence:** The specific form of the discovered reward function (bounded S-shaped) and its superiority over baselines is supported, but the exact reasons for its performance (beyond filtering noisy signals) could benefit from deeper theoretical analysis. The generalization claim to unseen optimizers (A2C) is demonstrated but limited to one example.
- **Low Confidence:** The robustness of the LLM-guided evolutionary search framework itself is not extensively validated. The paper does not explore alternative search strategies, the impact of different LLM prompting strategies, or the sensitivity to hyperparameters like population size or crossover rate.

## Next Checks
1. **Cross-Domain Generalization:** Test DAIL on a significantly different task domain, such as a vision-based robotic manipulation task from the Meta-World benchmark, to assess robustness beyond MuJoCo-style control and arcade games.
2. **Search Ablation:** Replace the LLM-guided crossover with a simpler, rule-based crossover (e.g., random code splicing) and compare the quality and diversity of discovered RA functions. This would validate the specific contribution of the LLM's "intelligence."
3. **Adversarial Robustness:** Evaluate DAIL's performance when the expert data is corrupted with adversarial noise or when the number of expert trajectories is drastically reduced (e.g., from 10 to 2-3). This would test the method's robustness to data quality and quantity, a critical real-world concern.