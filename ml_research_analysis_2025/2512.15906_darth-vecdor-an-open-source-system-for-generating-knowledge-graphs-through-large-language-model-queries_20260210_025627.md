---
ver: rpa2
title: 'Darth Vecdor: An Open-Source System for Generating Knowledge Graphs Through
  Large Language Model Queries'
arxiv_id: '2512.15906'
source_url: https://arxiv.org/abs/2512.15906
tags:
- knowledge
- response
- responses
- more
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Darth Vecdor (DV) is an open-source system that generates knowledge
  graphs by querying large language models (LLMs) for structured healthcare relationships.
  The system addresses key LLM challenges including erroneous responses, overly general
  outputs, and the need for multi-element parsing.
---

# Darth Vecdor: An Open-Source System for Generating Knowledge Graphs Through Large Language Model Queries
## Quick Facts
- arXiv ID: 2512.15906
- Source URL: https://arxiv.org/abs/2512.15906
- Reference count: 0
- Primary result: Open-source system generating knowledge graphs from LLM queries with browser-based GUI for domain experts

## Executive Summary
Darth Vecdor (DV) is an open-source system that generates knowledge graphs by querying large language models (LLMs) for structured healthcare relationships. The system addresses key LLM challenges including erroneous responses, overly general outputs, and the need for multi-element parsing. DV provides a browser-based GUI enabling domain experts without technical backgrounds to create and use knowledge graphs. The system automatically re-queries LLMs to improve response specificity and employs vector-based matching to map free-text outputs to medical terminology codes. Knowledge graphs are stored as SQL triple stores for efficient querying.

## Method Summary
Darth Vecdor generates knowledge graphs through systematic LLM querying with automated re-query mechanisms for improving response specificity. The system uses vector-based matching to map free-text LLM outputs to standardized medical terminology codes. Knowledge graphs are stored as SQL triple stores, enabling efficient querying and integration with existing database systems. The browser-based GUI allows domain experts to interact with the system without requiring technical expertise in programming or database management.

## Key Results
- Enables domain experts to create and use knowledge graphs without technical backgrounds
- Automatically improves LLM response specificity through re-query mechanisms
- Provides cost-effective, faster, and more explainable access to LLM-encoded knowledge compared to direct LLM queries

## Why This Works (Mechanism)
The system works by systematically querying LLMs for structured healthcare relationships and addressing common LLM limitations through automated re-querying for specificity and vector-based mapping to medical codes. The SQL triple store implementation provides efficient querying capabilities while maintaining compatibility with existing database infrastructure.

## Foundational Learning
- LLM querying strategies - why needed: LLMs often provide overly general or erroneous responses that need systematic refinement; quick check: measure response specificity improvement rates
- Vector-based code mapping - why needed: Free-text outputs must be mapped to standardized medical terminology; quick check: evaluate mapping accuracy across different terminology systems
- SQL triple store implementation - why needed: Efficient storage and querying of graph relationships; quick check: benchmark query performance vs. specialized graph databases

## Architecture Onboarding
Component map: Browser GUI -> LLM Query Engine -> Vector Mapping -> SQL Triple Store -> Query Interface

Critical path: User query → LLM query generation → Response parsing → Vector-based code mapping → Triple store insertion → Queryable knowledge graph

Design tradeoffs: Browser-based GUI enables domain expert use but may limit scalability; SQL triple store provides compatibility but may face performance constraints vs. specialized graph databases

Failure signatures: Overly general LLM responses, inaccurate code mappings, slow query performance with large graphs

First experiments:
1. Test GUI usability with domain experts creating simple knowledge graphs
2. Measure LLM response specificity improvement through re-query mechanism
3. Benchmark vector-based mapping accuracy across different medical terminology systems

## Open Questions the Paper Calls Out
None

## Limitations
- Absence of formal validation results for precision, recall, and F1 scores
- Unknown performance across different healthcare domains and terminology systems
- Scalability limitations of browser-based GUI and SQL triple store for very large graphs

## Confidence
The Darth Vecdor system demonstrates promising capabilities for LLM-based knowledge graph generation, but several key limitations affect confidence in its broader applicability. The absence of formal validation results represents the most significant gap - while informal review suggested high accuracy, there is no quantitative evaluation of precision, recall, or F1 scores for generated relationships or code mappings. The system's performance across different healthcare domains, medical terminology systems, and LLM models remains unknown. The reliance on vector-based matching for free-text to code mapping may introduce errors that scale with graph size, though this has not been characterized. The browser-based GUI, while enabling domain expert use, may face scalability limitations for very large knowledge graphs. The effectiveness of the automatic re-query mechanism for improving specificity has not been systematically measured. Confidence in the core claims is Medium - the approach is technically sound and addresses real challenges, but empirical validation is needed to confirm the reported benefits of cost-effectiveness, speed, and explainability. The SQL triple store implementation may face performance constraints for very large graphs compared to specialized graph databases.

## Next Checks
1. Conduct systematic evaluation measuring precision, recall, and F1 scores for generated medical relationships and code mappings across multiple healthcare domains
2. Benchmark system performance and accuracy when using different LLM models and medical terminology systems
3. Test scalability limits by generating and querying knowledge graphs of increasing size, measuring response times and accuracy degradation points