---
ver: rpa2
title: 'RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression
  Comprehension'
arxiv_id: '2512.06276'
source_url: https://arxiv.org/abs/2512.06276
tags:
- object
- visual
- arxiv
- reasoning
- referring
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces RefBench-PRO, a new benchmark for referring
  expression comprehension (REC) that evaluates both perceptual and reasoning capabilities
  of multimodal large language models (MLLMs). The benchmark is divided into six tasks:
  attribute, position, interaction (perceptual), and relation, commonsense, reject
  (reasoning).'
---

# RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension

## Quick Facts
- **arXiv ID:** 2512.06276
- **Source URL:** https://arxiv.org/abs/2512.06276
- **Reference count:** 40
- **Key outcome:** RefBench-PRO exposes significant gaps in current MLLM grounding performance, particularly on reasoning tasks, while Ref-R1 achieves strong gains across all categories.

## Executive Summary
This paper introduces RefBench-PRO, a new benchmark for referring expression comprehension (REC) that evaluates both perceptual and reasoning capabilities of multimodal large language models (MLLMs). The benchmark is divided into six tasks: attribute, position, interaction (perceptual), and relation, commonsense, reject (reasoning). A fine-grained annotation pipeline generates 200k high-quality training samples and 6k test pairs. The authors also propose Ref-R1, a two-stage training framework using chain-of-thought supervision and DyIoU-GRPO, which improves localization accuracy and reasoning ability. Experiments show RefBench-PRO exposes significant gaps in current MLLM grounding performance, particularly on reasoning tasks, while Ref-R1 achieves strong gains across all categories.

## Method Summary
RefBench-PRO introduces a two-stage training framework (Ref-R1) based on Qwen2.5-VL-7B for referring expression comprehension. The method combines Chain-of-Thought (CoT) supervised fine-tuning (SFT) with reinforcement learning using DyIoU-GRPO. The SFT phase trains on 180k samples with reasoning traces formatted as "ðŸ’­reasoning... <answer>[x1, y1, x2, y2]</answer>". The RL phase uses a reward function combining format compliance, dynamic IoU (threshold scales with step and box size), and group quality metrics. The benchmark itself consists of 6k test samples across six sub-tasks: attribute, position, interaction (perception), and relation, commonsense, reject (reasoning).

## Key Results
- RefBench-PRO exposes significant gaps in current MLLM grounding performance, particularly on reasoning tasks
- Proprietary models like GPT-4o achieve only 12.1% accuracy versus open-source Qwen2.5-VL-7B at 57.6%
- Ref-R1 framework improves rejection accuracy from near-random chance to 58.2% while maintaining strong gains across all categories

## Why This Works (Mechanism)
The two-stage training approach works by first establishing reasoning patterns through CoT supervision during SFT, then refining localization accuracy through DyIoU-GRPO reinforcement learning. The dynamic IoU threshold prevents gradient starvation for small objects by adjusting requirements based on object size and training progress. The group quality reward encourages diverse and challenging sample selection during RL, preventing overfitting to easy cases. The chain-of-thought supervision explicitly teaches models to verbalize reasoning steps before generating bounding boxes, improving compositional reasoning ability.

## Foundational Learning

**Multimodal Grounding**: Understanding how language models localize objects in images
- *Why needed*: REC requires mapping textual descriptions to specific spatial regions
- *Quick check*: Can the model correctly identify "the red car on the left" in a street scene?

**Dynamic IoU Thresholding**: Adaptive evaluation criteria that scale with object size and training progress
- *Why needed*: Prevents small objects from being unfairly penalized during training
- *Quick check*: Verify threshold decreases for smaller bounding boxes using Î±=0.5, Î²=0.8, d_max=0.15

**Group Quality Sampling**: Contrastive learning strategy that selects challenging negative samples
- *Why needed*: Prevents model from exploiting easy patterns and improves generalization
- *Quick check*: Confirm diverse sample selection by analyzing reward distribution across training batches

## Architecture Onboarding

**Component Map**: Qwen2.5-VL-7B base -> SFT (180k CoT samples) -> RL (DyIoU-GRPO) -> RefBench-PRO evaluation

**Critical Path**: Data generation (Qwen2.5-VL-72B + Grounding DINO) -> SFT fine-tuning -> RL optimization -> Benchmark evaluation

**Design Tradeoffs**: The authors trade computational cost (reinforcement learning on 80k samples) for improved reasoning capability versus pure supervised learning approaches. The CoT supervision adds reasoning structure but may introduce bias from training data generation.

**Failure Signatures**: Models exhibiting "grounding hallucination" fail to reject non-existent objects (low RejAcc). Small object degradation appears as accuracy drops below 10% object area. Reasoning failures show as poor performance on relation and commonsense tasks despite strong perceptual scores.

**First Experiments**:
1. Validate Dynamic IoU implementation by testing threshold adjustment on synthetic boxes of varying sizes
2. Test GRPO reward function independently using pre-trained model outputs
3. Benchmark model on individual task categories to identify specific failure modes

## Open Questions the Paper Calls Out

**Open Question 1**: How can training be adjusted to resolve the "grounding hallucination" problem where models consistently fail to reject non-existent objects? Current RL improved rejection accuracy to 58.2% but remains significantly lower than perception tasks, indicating persistent bias from positive instance training.

**Open Question 2**: Can explicit reasoning traces (Chain-of-Thought) alone bridge the performance gap between simple visual perception and complex compositional reasoning? Despite CoT supervision, persistent performance gaps remain between perceptual and reasoning tasks, suggesting current implementations may not fully integrate visual evidence with textual logic.

**Open Question 3**: Is poor performance of proprietary MLLMs primarily due to insufficient spatial resolution or lack of domain-specific grounding instruction tuning? Proprietary models like GPT-4o vastly underperform open-source alternatives, but the paper doesn't isolate whether failure stems from architecture, tokenization, or absence of specific grounding data.

## Limitations

- Incomplete methodological detail in GRPO implementation, particularly group sampling strategy and threshold scheduling
- Reasoning traces generated by same model being evaluated may introduce bias and limit assessment of genuine reasoning
- Selection criteria for 180k SFT subset from 200k generated samples not explicitly defined

## Confidence

- **High Confidence**: Benchmark construction methodology, six-task categorization, and evaluation metrics
- **Medium Confidence**: SFT phase training procedure and overall Ref-R1 architecture
- **Low Confidence**: GRPO implementation details, particularly group sampling and reward integration

## Next Checks

1. Verify GRPO implementation by reproducing "hard group" sampling mechanism with Ï„_q initialization and N=4 contrastive sampling to confirm reasoning improvements

2. Implement controlled ablation study training on negative examples to improve RejAcc and distinguish reasoning from pattern matching

3. Conduct detailed performance breakdown by target object area percentage, isolating objects < 10% of image area to validate Dynamic IoU effectiveness