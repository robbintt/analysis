---
ver: rpa2
title: A Comprehensive Analysis on Machine Learning based Methods for Lung Cancer
  Level Classification
arxiv_id: '2501.18294'
source_url: https://arxiv.org/abs/2501.18294
tags:
- learning
- cancer
- lung
- figure
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses lung cancer classification using multiple machine
  learning algorithms. It collects and preprocesses datasets from WHO, Kaggle, and
  Google datasets, performing correlation analysis and feature selection.
---

# A Comprehensive Analysis on Machine Learning based Methods for Lung Cancer Level Classification

## Quick Facts
- arXiv ID: 2501.18294
- Source URL: https://arxiv.org/abs/2501.18294
- Reference count: 0
- Near-perfect accuracy (100%) achieved by XGBoost, LGBM, Logistic Regression, CatBoost, and Random Forest

## Executive Summary
This study investigates lung cancer level classification using nine machine learning models on a combined dataset from WHO, Kaggle, and Google datasets. The research focuses on multi-class classification (low, medium, high) using demographic, environmental, and lifestyle features. Through correlation-based feature selection, SMOTE balancing, and hyperparameter tuning, the study achieves near-perfect classification performance with traditional ML models. Notably, the DNN model underperforms compared to ensemble methods, challenging the assumption that deep learning always provides superior performance in medical classification tasks.

## Method Summary
The study collected lung cancer datasets from multiple sources and preprocessed them using MinMaxScaler normalization and SMOTE for class balancing. Correlation analysis was applied for feature selection, with thresholds set at >0.5 for merging and <0.4 for scrutiny. Nine models were evaluated: XGBoost, LGBM, AdaBoost, Logistic Regression, Decision Tree, Random Forest, CatBoost, k-NN, and a DNN. Hyperparameter tuning focused on minimum child weight and learning rate (>0.06 for boosting models), with 5-fold cross-validation used throughout. The dataset consisted of approximately 1,095 samples (876 training, 219 testing).

## Key Results
- Traditional ML models (XGBoost, LGBM, Logistic Regression, CatBoost, Random Forest) achieved 100% accuracy, precision, recall, and F1-score
- DNN model underperformed compared to traditional models despite its complexity
- Hyperparameter tuning (learning rate and minimum child weight) significantly reduced overfitting
- Feature selection through correlation analysis improved model efficiency without sacrificing performance

## Why This Works (Mechanism)
The superior performance of traditional ML models over DNNs in this context can be attributed to several factors. First, the dataset size (~1,095 samples) is relatively small for deep learning applications, which typically require thousands of samples to effectively learn complex patterns. Second, the feature engineering and selection process, particularly correlation-based feature selection, may have already extracted the most informative features, reducing the need for deep feature learning. Third, ensemble methods like XGBoost and Random Forest excel at capturing non-linear relationships and interactions between features without requiring extensive parameter tuning. The hyperparameter optimization, especially for learning rate and minimum child weight in boosting models, helped prevent overfitting while maintaining high performance.

## Foundational Learning
- Correlation-based feature selection: Why needed - Reduces dimensionality and removes redundant features; Quick check - Verify correlation matrix and threshold values used
- SMOTE oversampling: Why needed - Balances class distribution to prevent bias toward majority class; Quick check - Confirm minority class samples are generated appropriately
- MinMaxScaler normalization: Why needed - Ensures all features contribute equally to distance-based algorithms; Quick check - Verify all features are scaled to [0,1] range
- 5-fold cross-validation: Why needed - Provides robust performance estimates and reduces overfitting risk; Quick check - Confirm fold stratification and variance in performance metrics
- Hyperparameter tuning: Why needed - Optimizes model performance and prevents overfitting; Quick check - Review parameter search space and final selected values

## Architecture Onboarding

**Component Map**
Data Preprocessing -> Feature Selection -> Model Training (9 models) -> Hyperparameter Tuning -> Evaluation

**Critical Path**
Feature Selection -> Model Training -> Hyperparameter Tuning -> Evaluation (most critical for achieving high performance)

**Design Tradeoffs**
- Traditional ML vs DNN: Traditional models performed better with smaller datasets, while DNNs typically require larger datasets
- Feature engineering vs deep feature learning: Manual feature selection may have captured most relevant information, reducing need for DNN
- Ensemble complexity vs interpretability: Ensemble models achieved high performance but lack transparency compared to simpler models

**Failure Signatures**
- Perfect accuracy (100%) may indicate data leakage or insufficient validation
- DNN underperformance suggests inadequate dataset size or improper architecture
- Inconsistent performance across models may indicate feature-target relationships not captured uniformly

**3 First Experiments**
1. Train XGBoost with default parameters to establish baseline performance
2. Apply correlation analysis to identify and remove highly correlated features
3. Implement SMOTE and compare performance before/after balancing

## Open Questions the Paper Calls Out

**Open Question 1**
Can Deep Neural Networks (DNNs) surpass traditional machine learning models in lung cancer classification when utilizing significantly larger datasets and extended hyperparameter tuning? The current study was limited by dataset size, which restricted the DNN's ability to maximize feature extraction, resulting in traditional models outperforming the DNN. A comparative study using high-dimensional datasets (>10,000 samples) demonstrating DNN performance exceeding that of XGBoost or Logistic Regression would resolve this question.

**Open Question 2**
Do the near-perfect classification metrics (100% accuracy) reported across multiple models generalize to external, independent clinical cohorts? The paper reports near-perfect scores for 6 out of 9 models on a relatively small aggregated dataset (~1,095 samples), which raises concerns regarding potential overfitting or dataset-specific artifacts despite cross-validation efforts. Validation results from applying the trained models to completely separate, unseen datasets from different medical institutions or geographic regions would resolve this question.

**Open Question 3**
How can the "black box" nature of the high-performing ensemble models be interpreted to provide actionable clinical explanations for lung cancer staging? The authors argue these models are valuable for "decision-making" and "personalized therapy," yet the top-performing models (XGBoost, LGBM, Random Forest) are complex ensemble methods lacking inherent interpretability compared to Logistic Regression or Decision Trees. The integration of explainability frameworks (e.g., SHAP or LIME) that quantify feature importance for specific predictions, validated by domain experts, would resolve this question.

## Limitations
- DNN architecture details were not specified, making it difficult to assess proper optimization
- Reported 100% accuracy raises concerns about potential data leakage or insufficient validation
- Dataset composition and exact feature set remain unclear, affecting reproducibility

## Confidence
- DNN performance comparison: Medium confidence - Architecture details missing, making fair comparison difficult
- Generalization of results: High confidence - Near-perfect accuracy suggests need for external validation
- Feature selection process: Medium confidence - Correlation thresholds provided but final feature set unclear

## Next Checks
1. Verify dataset integrity by checking for target leakage and confirming stratified train-test splits
2. Test the reported models on an independent lung cancer dataset to confirm generalization performance
3. Implement and tune a DNN with explicit architecture specifications to provide a fair comparison with traditional ML models