---
ver: rpa2
title: 'See What You Are Told: Visual Attention Sink in Large Multimodal Models'
arxiv_id: '2503.03321'
source_url: https://arxiv.org/abs/2503.03321
tags:
- visual
- tokens
- attention
- sink
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a phenomenon called "visual attention sink"
  in large multimodal models (LMMs), where models consistently allocate high attention
  weights to irrelevant visual tokens. The authors find this behavior arises from
  massive activation of specific hidden state dimensions, similar to attention sinks
  in language models.
---

# See What You Are Told: Visual Attention Sink in Large Multimodal Models

## Quick Facts
- arXiv ID: 2503.03321
- Source URL: https://arxiv.org/abs/2503.03321
- Authors: Seil Kang; Jinyeong Kim; Junhyeok Kim; Seong Jae Hwang
- Reference count: 40
- Primary result: Visual attention sink phenomenon identified in LMMs; VAR method improves performance across general VL, hallucination, and vision-centric tasks without training

## Executive Summary
This paper identifies a phenomenon called "visual attention sink" in large multimodal models (LMMs), where models consistently allocate high attention weights to irrelevant visual tokens. The authors find this behavior arises from massive activation of specific hidden state dimensions, similar to attention sinks in language models. They propose Visual Attention Redistribution (VAR), a method that recycles attention from these sink tokens and redistributes it to enhance focus on relevant image regions. VAR improves performance across general vision-language tasks, visual hallucination tasks, and vision-centric tasks without requiring additional training or inference steps.

## Method Summary
Visual Attention Redistribution (VAR) is a training-free method that modifies attention weights during inference to address visual attention sink in LMMs. The method identifies sink tokens via massive activation in specific hidden state dimensions inherited from base LLMs, then selectively redistributes attention from these tokens to relevant visual regions. VAR operates by calculating a visual non-sink ratio for each attention head, selecting image-centric heads based on task-specific thresholds, and applying attention budget redistribution only to these heads while preserving the last layer's specialized function.

## Key Results
- VAR consistently improves performance across general VL tasks (VQAv2, GQA, VizWiz), visual hallucination tasks (CHAIR, POPE, MMHal), and vision-centric tasks (MMVP, CV-Bench2D/3D)
- Improvements achieved without additional training or inference overhead
- Method effective across various LMMs including LLaVA, VILA, Qwen2-VL, and InternVL2
- Visual sink tokens can be masked with minimal performance impact, confirming they provide no functional contribution

## Why This Works (Mechanism)

### Mechanism 1: Visual Attention Sink Identification via Massive Activation
- Claim: Irrelevant visual tokens receive high attention weights due to extreme activation in specific hidden state dimensions inherited from base LLMs
- Mechanism: Certain fixed dimensions (D_sink) exhibit massive activation values that persist across multimodal fine-tuning. Tokens with normalized sink dimension value ϕ(x) ≥ τ are classified as visual sink tokens. For LLaVA-1.5-7B, D_sink = {1415, 2533} (same as base LLaMA-2).
- Core assumption: The sink dimensions from base language models remain valid after multimodal training and are consistent across layers/inputs
- Evidence anchors:
  - [abstract]: "this behavior arises due to the massive activation of certain hidden state dimensions, which resembles the attention sink found in language models"
  - [section 4.1]: "the sink dimension value of the irrelevant visual token is significantly higher than that of the relevant visual token" with scatter plot showing clear separation at τ=20
  - [corpus]: "What are you sinking?" paper provides geometric foundation for attention sink emergence

### Mechanism 2: Image-Centric Head Selection via Visual Non-Sink Ratio
- Claim: Heads that genuinely process visual information can be identified by measuring the proportion of attention allocated to visual non-sink tokens
- Mechanism: Calculate r_ℓ,h_i = (sum of attention to visual non-sink tokens) / (total visual attention). Heads with r ≥ ρ are selected as image-centric. High-ratio heads focus on semantically relevant regions; low-ratio heads show scattered attention.
- Core assumption: Visual non-sink tokens represent the true image content that should be attended to for task performance
- Evidence anchors:
  - [section 5.1]: "heads with high visual non-sink ratio are more likely to concentrate on the important visual tokens"
  - [figure 4]: Visualization confirms high-ratio heads attend to relevant regions (bird, skateboard) while low-ratio heads have vague patterns
  - [corpus]: Limited direct corpus evidence for this specific ratio-based selection; "To Sink or Not to Sink" explores related pathway concepts

### Mechanism 3: Attention Budget Redistribution Without Retraining
- Claim: Attention from sink tokens can be recycled and redistributed to visual non-sink tokens, improving performance without training/inference overhead
- Mechanism: (1) Reduce sink token attention by factor (1-p), accumulate p=0.6 into budget Ω, (2) Redistribute Ω to visual non-sink tokens proportional to their existing attention weights, (3) Apply only to image-centric heads, excluding last layer
- Core assumption: Sink token attention is surplus that provides no functional contribution to outputs
- Evidence anchors:
  - [section 4.2]: "masking the visual sink tokens has little impact on the model's performance" while random masking causes significant drop
  - [table 4]: Redistribution without head selection causes complete failure (0.0 scores), confirming selective application is critical
  - [corpus]: "LLaVA-PruMerge" supports token reduction approaches; limited evidence for redistribution specifically

## Foundational Learning

- Concept: Attention Sink in Language Models
  - Why needed here: Visual attention sink extends the attention sink phenomenon from LLMs where semantically meaningless tokens (BOS, punctuation) receive disproportionate attention due to architectural properties
  - Quick check question: Why do tokens with massive activation in specific hidden dimensions attract high attention regardless of their semantic relevance?

- Concept: Multi-Head Attention Specialization
  - Why needed here: VAR requires understanding that different attention heads serve different functions—some process visual information, others handle text-only operations
  - Quick check question: What evidence suggests that transformer attention heads can specialize for different modalities or functions?

- Concept: Visual Token Representation in LMMs
  - Why needed here: Understanding how visual encoders (CLIP) convert images into discrete tokens helps explain why certain visual tokens become sinks (often background regions)
  - Quick check question: How does patch-based tokenization in vision transformers create tokens that vary in semantic importance?

## Architecture Onboarding

- Component map:
  Visual Encoder (CLIP) -> Projector -> LLM Decoder with multi-head attention
  Sink dimensions D_sink: fixed per base LLM (LLaMA-2-7B: {1415, 2533}, Qwen2-VL: {458, 2570})
  Visual non-sink ratio calculation per head during forward pass
  Attention redistribution module (training-free, inference-time only)

- Critical path:
  1. Extract hidden states, compute ϕ(x) for each visual token to identify sink tokens
  2. Calculate visual non-sink ratio r per head using current attention weights
  3. Select heads with r ≥ ρ as image-centric (task-specific ρ)
  4. Apply redistribution: qα = (1-p)·α for sinks, redistribute Ω = p·Σ(sink attention) to non-sinks
  5. Skip redistribution for last layer (specialized role per Lad et al., 2024)

- Design tradeoffs:
  - Higher ρ → fewer heads modified, more conservative but may miss beneficial redistribution
  - Higher p → more aggressive redistribution (p=0.6 optimal); p=1.0 would zero out sink attention entirely
  - Task-specific ρ required: hallucination tasks benefit from lower ρ (more heads), vision-centric tasks need higher ρ (fewer, more specialized heads)

- Failure signatures:
  - Redistribution applied to all heads → complete generation failure (Table 4: 0.0 scores)
  - Wrong sink dimensions for model variant causes no improvement
  - Including last layer in redistribution → potential degradation (untested in paper)
  - τ too low → relevant tokens incorrectly classified as sinks

- First 3 experiments:
  1. Sink dimension validation: Plot hidden state activations for BOS token vs. visual tokens across multiple layers; confirm massive activation at identical dimensions (Fig. 2, Fig. 7 pattern)
  2. Token masking ablation: Compare masking identified sink tokens vs. equal-count random tokens on POPE; expect minimal degradation for sink masking but significant drop for random (Fig. 3b)
  3. Head visualization: Sort heads by visual non-sink ratio, visualize attention maps for high vs. low ratio heads; verify high-ratio heads attend to query-relevant regions (Fig. 4 pattern)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What mechanisms cause certain visual tokens to be recognized as visual sink tokens during multimodal training?
- Basis in paper: [explicit] Appendix A.2 states: "the mechanisms behind how LMMs select visual sink tokens and how visual attention sinks emerge during the training process remain open questions" and "investigating how LMMs identify certain visual tokens as visual sink tokens during the multimodal training process could be an intriguing direction for future research."
- Why unresolved: The paper demonstrates that visual sink tokens exhibit massive activation at the same sink dimensions as text sink tokens (inherited from base LLMs), but does not explain why specific visual tokens acquire these properties during training.
- What evidence would resolve it: Ablation studies tracing when during pre-training/fine-tuning visual sink tokens emerge; analysis of gradient flow to specific visual tokens; controlled experiments varying training data composition to observe how visual sink token selection changes.

### Open Question 2
- Question: How does VAR perform on larger multimodal models beyond the 7B-13B parameter scale?
- Basis in paper: [explicit] Appendix C (Limitations) states: "it is still worth exploring our methodology on different and larger LMMs."
- Why unresolved: The paper evaluates VAR only on LLaVA-1.5-7B/13B, VILA-13B, Qwen2-VL-7B, and InternVL2-8B. Scaling behavior is unknown.
- What evidence would resolve it: Benchmark results of VAR applied to models like LLaVA-OneVision, GPT-4V-scale models, or models exceeding 70B parameters on the same suite of tasks (VQAv2, GQA, POPE, MME, etc.).

### Open Question 3
- Question: Can visual sink tokens be exploited for applications beyond attention redistribution?
- Basis in paper: [explicit] Appendix C states: "Visual sink tokens can be utilized for various applications. For example, masking visual sink tokens removes noise and produces more reliable and interpretable visual attention map. Moreover, free attention budget from sink tokens can be exploited to emphasize the specific part of the image, depending on the user's interest."
- Why unresolved: The paper focuses solely on VAR for performance improvement but does not implement or evaluate these alternative applications.
- What evidence would resolve it: Experiments demonstrating (1) improved visual grounding accuracy when masking visual sink tokens, and (2) user-guided attention steering by redirecting sink token attention to user-specified image regions with quantitative improvements in task-specific metrics.

### Open Question 4
- Question: What causes the emergence of massive activation in sink dimensions at early layers?
- Basis in paper: [inferred] Section 4.1 and Appendix A.1 document that massive activation in sink dimensions emerges at layer 2 and persists through all subsequent layers, similar to language models. The paper references prior work suggesting FFNs in early layers are responsible, but does not verify this mechanism in LMMs.
- Why unresolved: The causal mechanism—whether early-layer FFNs write these activations, whether residual connections preserve them, and why specific dimensions are affected—remains unconfirmed in the multimodal setting.
- What evidence would resolve it: Layer-wise ablation of early FFN outputs to measure impact on sink dimension activation; tracing activation propagation through residual connections; analysis of FFN weight patterns associated with sink dimensions in base LLMs vs. LMMs.

## Limitations

- The claim that sink dimensions from base LLMs remain valid after multimodal fine-tuning is asserted but not empirically validated across all tested models
- VAR's effectiveness relies on the specific attention mechanism design of transformer-based LMMs and may not generalize to alternative architectures
- While VAR shows consistent improvements, the magnitude varies significantly across tasks, suggesting it addresses specific failure modes rather than providing universal enhancement

## Confidence

**High Confidence**: The existence of visual attention sink phenomenon itself—supported by multiple ablation studies showing sink token masking has minimal impact while random token masking degrades performance substantially. The geometric mechanism via massive activation in specific hidden dimensions is well-grounded in prior work on attention sinks.

**Medium Confidence**: The head selection mechanism based on visual non-sink ratio. While the paper provides qualitative visualizations and ablation studies, the optimal ρ thresholds are determined empirically per task type rather than derived from theoretical principles. The stability of these thresholds across different model variants and datasets needs further validation.

**Medium Confidence**: The task-specific ρ thresholds (0.8 for general VL, 0.5 for hallucination, 0.9 for vision-centric) are presented as optimal but derived from limited experimentation. The sensitivity of performance to small variations in these thresholds is not explored comprehensively.

## Next Checks

**Validation Check 1**: Replicate sink dimension analysis across all model variants (Qwen2-VL-7B, InternVL2-8B, and larger LLaVA models). For each model, extract hidden states from the BOS token and visualize activation magnitudes across all dimensions to confirm the specified sink dimensions ({458, 2570} for Qwen2-VL, {3584} for InternVL2) show massive activation. Compare these distributions to the base LLM dimensions to quantify how much they shift after multimodal training.

**Validation Check 2**: Conduct systematic ablation studies on head selection sensitivity. Test ρ values in 0.1 increments around the proposed thresholds (e.g., 0.7-0.9 for general VL tasks) and measure performance degradation curves. Additionally, test whether the optimal ρ correlates with any measurable properties of the visual encoder output (e.g., token entropy, semantic segmentation agreement).

**Validation Check 3**: Evaluate VAR's effectiveness on non-decoder architectures. Apply VAR to an encoder-decoder LMM (e.g., GIT, or a CLIP-ViT model with a separate decoder) or to a model using approximate attention (e.g., Mamba-based approaches). Measure whether the same sink dimension identification and attention redistribution principles apply, or whether the method requires fundamental modifications for different architectural paradigms.