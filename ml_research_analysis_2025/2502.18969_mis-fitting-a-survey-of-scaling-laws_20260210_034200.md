---
ver: rpa2
title: '(Mis)Fitting: A Survey of Scaling Laws'
arxiv_id: '2502.18969'
source_url: https://arxiv.org/abs/2502.18969
tags:
- reported
- scaling
- hoffmann
- power
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper surveys over 50 scaling law studies across multiple\
  \ domains, revealing that essential reproducibility details are frequently underreported,\
  \ with only 19 of 42 papers providing analysis code and 23 omitting optimization\
  \ process descriptions. The authors identify eight critical decision points in scaling\
  \ law research\u2014including equation form, training setup, data collection, and\
  \ fitting algorithms\u2014each of which can significantly impact results."
---

# (Mis)Fitting: A Survey of Scaling Laws

## Quick Facts
- arXiv ID: 2502.18969
- Source URL: https://arxiv.org/abs/2502.18969
- Reference count: 40
- Over 50 scaling law studies reviewed, finding critical reproducibility gaps and methodological sensitivities

## Executive Summary
This paper systematically examines reproducibility and methodological choices in scaling law research through a comprehensive survey of over 50 studies and controlled experiments. The authors identify eight critical decision points—including equation form, training setup, data collection, and fitting algorithms—that can significantly impact scaling predictions. Through systematic experiments varying these choices, they demonstrate that different methodological decisions can lead to vastly different optimal parameter-to-token ratios and scaling predictions. To address this, the paper proposes a comprehensive checklist for researchers to improve reproducibility and transparency in scaling law studies.

## Method Summary
The authors survey 42 papers on scaling laws, extracting details about equation forms, training setups, data collection, and fitting algorithms. They conduct experiments using three datasets: reconstructed Chinchilla data (245 datapoints), Poria et al. data (5M-901M param models), and their own trained models (12M-1B params on FineWeb). Models are trained with varied architectures and hyperparameters, then fitted using L-BFGS optimization with Huber/log-Huber loss across 4,500 initialization grid points. The study systematically varies methodological choices to quantify their impact on predicted optimal N/D ratios and scaling predictions.

## Key Results
- Only 19 of 42 surveyed papers provide analysis code, and 23 omit optimization process descriptions
- Different methodological choices (equation form, training setup, data collection, fitting algorithm) lead to systematically different optimal parameter-to-token ratio predictions
- Initialization choice significantly affects fitted scaling laws, with different basins of attraction producing divergent predictions
- Data range coverage constrains extrapolation validity—under-sampling key regions produces biased fits rather than just wider confidence intervals

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Methodological choices at each decision point causally determine scaling law predictions, not just add noise
- Mechanism: Eight decision categories independently shift optimal D/N ratio predictions. Fixing learning rate vs. sweeping, using final vs. mid-training checkpoints, and including vs. excluding embedding parameters each produce systematically different optimal ratio curves.
- Core assumption: Observed variation reflects causal impact of methodological choices, not dataset-specific artifacts
- Evidence anchors: [abstract] "demonstrate that different methodological decisions can lead to vastly different optimal parameter-to-token ratios and scaling predictions"; [section 7.2] "Results vary dramatically across these settings"; [corpus] Related papers confirm scaling law sensitivity

### Mechanism 2
- Claim: Initialization creates path-dependency in fitted scaling laws through local optima trapping
- Mechanism: The fitting landscape contains multiple local minima. L-BFGS primarily shifts parameters near initialization values. Grid search over 4,500 initializations produces different laws than single-initialization optimization.
- Core assumption: Local optima are sufficiently separated that initialization choice non-trivially constrains final solution
- Evidence anchors: [section 7.4] "Optimizing over the full grid yields the power law that diverges most from the Hoffmann et al. (2022) law"; [section 6] "most known optimization methods for scaling laws are only able, in practice, to shift parameters near to their initialization values"
- Break condition: If optimization landscape were convex or initializations equivalent, all methods would converge to same solution

### Mechanism 3
- Claim: Data range coverage constrains extrapolation validity by limiting discoverable optima
- Mechanism: If training runs don't span the true optimal D/N ratio region, the fitted law cannot correctly identify optima outside that region. Filtering to only small models or biased D/N ranges systematically shifts predictions at higher compute budgets.
- Core assumption: True scaling relationship is smooth and extrapolable; under-sampling key regions produces biased fits
- Evidence anchors: [section 7.2] "The minimum or maximum D/N ratio tested does skew results"; [section 4] "The specific N and D values also skew the optimization process towards a certain range of N/D ratios"
- Break condition: If extrapolation were robust to range coverage, fits from narrow ranges would correctly predict optima in unexplored regions

## Foundational Learning

- **Power law functional forms**: L(N,D) = E + A/N^α + B/D^β vs. Kaplan form with interaction terms
  - Why needed here: The paper shows form choice alone changes predicted optima; understanding what each form assumes is prerequisite to choosing appropriately
  - Quick check question: Does the Hoffmann form assume N and D interact additively or multiplicatively on loss?

- **L-BFGS optimization and loss functions**: How quasi-Newton methods fit nonlinear functions, what Huber vs. MSE vs. MAE prioritize
  - Why needed here: Over half of papers don't report fitting details; the paper shows loss choice affects results and optimizer initialization is critical
  - Quick check question: Why might Huber loss be preferred over MSE for scaling law fitting given outliers in small-model regimes?

- **IsoFLOP vs. direct fitting approaches**: Trade-offs between fitting optimal N*(C), D*(C) curves vs. full L(N,D) surface
  - Why needed here: The paper classifies papers into ratio optimization vs. performance prediction approaches; each has different data requirements and stability properties
  - Quick check question: What extra data does IsoFLOP require compared to direct L(N,D) fitting, and when is it worth the cost?

## Architecture Onboarding

- **Component map**: Hypothesis specification -> Training setup -> Data collection -> Fitting algorithm
- **Critical path**: Training setup → Data collection → Fitting. Training decisions (LR sweep strategy, N/D range coverage) propagate through evaluation to final fits. Mid-training checkpoint usage can partially compensate for limited model count.
- **Design tradeoffs**:
  - Fewer parameters to fit → more stability but potentially miss true functional form
  - Fixed learning rate → less noise per-paper-observation but may not match true per-scale optimum
  - Mid-training checkpoints → more data points cheaply but adds training-dynamics complexity
  - Dense initialization grid → more thorough search but 4,500x compute overhead
- **Failure signatures**:
  - Predicted D/N ratio matches training range bounds → likely undercoverage
  - Fits change dramatically when adding/removing small models → small-scale contamination
  - Full-grid optimization diverges from initialized-from-prior-law → likely in different basin; check which has lower loss
  - No validation reported → treat extrapolated predictions as untested hypotheses
- **First 3 experiments**:
  1. Replicate a known scaling law (e.g., Hoffmann Approach 3) on available data with full checklist documentation to establish baseline sensitivity to your specific setup
  2. Ablate single decisions: run same data with (a) final checkpoints only vs. all checkpoints, (b) with/without embedding parameters in N, (c) different initializations—quantify impact magnitude
  3. Validate extrapolation: hold out highest-compute models from fitting, compare predicted vs. actual performance to establish trustworthy extrapolation range

## Open Questions the Paper Calls Out

- **Open Question 1**: How can researchers reliably forecast the limits of scaling law extrapolation and the emergence of new abilities at unseen scales?
  - Basis in paper: [explicit] Section 4 states, "Forecasting limits to extrapolation and the appearance of new abilities at new scales is an open question."
  - Why unresolved: Most surveyed studies train models at modest scales (typically < 1B parameters), making it difficult to validate predictions for significantly larger models where new capabilities often emerge.
  - What evidence would resolve it: Empirical studies that fit laws on lower scales and rigorously validate predictive accuracy against actual performance in models exceeding 7B parameters.

- **Open Question 2**: Does sweeping for optimal learning rates introduce noise that destabilizes scaling law fits compared to using fixed schedules?
  - Basis in paper: [inferred] Appendix D notes the authors "hypothesize that this may be because the true optimal learning rate... is not any of the options we consider," but they were "unable to fully test this hypothesis... at significantly larger scale."
  - Why unresolved: Resource constraints limited the ability to determine if the observed instability from sweeping learning rates persists or changes nature at the compute scales relevant to frontier models.
  - What evidence would resolve it: Controlled ablation studies at large scales comparing the variance of scaling law coefficients derived from swept versus fixed learning rate settings.

- **Open Question 3**: How does scaling law sensitivity vary when fitting across heterogeneous model architectures rather than a single family?
  - Basis in paper: [inferred] Appendix D states, "It is probable that variations would be even harder to predict when varying model architectures," and the survey notes an overrepresentation of Transformers, limiting generalizability.
  - Why unresolved: The paper's analysis focuses primarily on Transformer-based language models, leaving the interaction between architectural inductive biases and the fragility of fitting methods under-explored.
  - What evidence would resolve it: A comparative analysis applying the paper's reproducibility checklist to diverse architectures (e.g., CNNs, State Space Models) to measure fit variance.

## Limitations

- Most experiments use autoregressive language models with similar training recipes, limiting claims about universality across model families
- Key implementation details remain underspecified: initialization grid bounds, Huber loss δ parameter, and outlier filtering criteria
- The paper does not systematically validate extrapolation accuracy beyond training data ranges

## Confidence

- **Reproducibility claims: Medium-High** - Extensive experimental validation but missing key implementation details
- **Universality claims: Low-Medium** - Demonstrated sensitivity across datasets but limited to similar model architectures
- **Extrapolation reliability: Low** - Shows range coverage affects predictions but lacks systematic validation of extrapolation accuracy

## Next Checks

1. **Cross-architecture validation**: Apply the checklist methodology to non-transformer architectures (CNNs, MLPs, or vision transformers) on ImageNet or similar benchmarks to test whether identified decision sensitivities transfer beyond language modeling.

2. **Extrapolation error quantification**: Systematically withhold high-compute models from fitting, then measure prediction error versus actual performance across multiple datasets. Quantify how range coverage relates to extrapolation accuracy.

3. **Community adoption tracking**: Monitor citation patterns and methodological changes in subsequent scaling law papers to assess whether the proposed checklist improves reproducibility and transparency in practice.