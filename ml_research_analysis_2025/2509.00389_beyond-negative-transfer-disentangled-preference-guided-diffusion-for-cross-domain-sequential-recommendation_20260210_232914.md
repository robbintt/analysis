---
ver: rpa2
title: 'Beyond Negative Transfer: Disentangled Preference-Guided Diffusion for Cross-Domain
  Sequential Recommendation'
arxiv_id: '2509.00389'
source_url: https://arxiv.org/abs/2509.00389
tags:
- cross-domain
- user
- sequential
- diffusion
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses Cross-Domain Sequential Recommendation (CDSR),
  where user behaviors across multiple domains are leveraged to enhance recommendations.
  The core challenge is disentangling domain-invariant preferences, domain-specific
  preferences, and noise, which can lead to negative transfer when combined naively.
---

# Beyond Negative Transfer: Disentangled Preference-Guided Diffusion for Cross-Domain Sequential Recommendation

## Quick Facts
- arXiv ID: 2509.00389
- Source URL: https://arxiv.org/abs/2509.00389
- Reference count: 16
- Primary result: DPG-Diff achieves up to 28.2% relative improvement in Hit@10 on Movie dataset for cross-domain sequential recommendation

## Executive Summary
This paper addresses the challenge of negative transfer in Cross-Domain Sequential Recommendation (CDSR) by proposing DPG-Diff, the first diffusion-based approach tailored for this task. The method disentangles user preferences into domain-invariant, domain-specific, and noise components, using these to guide a reverse diffusion process for robust cross-domain knowledge transfer. Extensive experiments on real-world datasets demonstrate consistent performance improvements over state-of-the-art baselines, with particular strength in handling noisy interactions and maintaining performance even with significant noise injection.

## Method Summary
DPG-Diff employs a three-component architecture: (1) a Disentangled Encoder that processes domain-specific sequences through independent Transformers to produce domain-specific and domain-invariant representations, (2) a DPG Denoiser that uses cross-attention to condition the denoising decoder on fused preference guidance, and (3) Tri-view Contrastive Learning that aligns representations across denoised cross-domain, fused domain-aware, and augmented views. The model is trained using a combination of diffusion loss, recommendation loss, and contrastive loss on Amazon datasets with filtering requirements (users/items with <10 interactions removed; users must interact with both domains and ≥3 items per domain).

## Key Results
- Achieves up to 28.2% relative improvement in Hit@10 on Movie dataset
- Demonstrates robust performance under noise injection (10-30% random item substitutions)
- Shows consistent improvements across multiple metrics (MRR@10, NDCG@5,10, Hit@5,10)
- Outperforms state-of-the-art baselines including C2DSR, C2DREIF, and DREAM

## Why This Works (Mechanism)

### Mechanism 1: Disentangled Preference Encoding
Separating domain-invariant from domain-specific preferences reduces negative transfer by preventing conflicting signals from contaminating shared representations. Two independent Transformer encoders process domain-specific sequences to produce g_x and g_y, which are fused into g_d as structured guidance, preserving domain semantics while enabling cross-domain knowledge flow. The core assumption is that domain-specific and domain-invariant preferences are linearly separable in learned representation space.

### Mechanism 2: Preference-Guided Reverse Diffusion
Conditioning the denoising decoder on disentangled preferences enables iterative refinement that suppresses noise while reconstructing behaviorally consistent user embeddings. The DPG Denoiser uses cross-attention to align noisy inputs x_t with guidance g_d, selectively fusing semantic priors and filtering irrelevant signals during each denoising step. The core assumption is that noise in user sequences is approximately Gaussian-distributed and can be progressively removed through learned reverse transitions.

### Mechanism 3: Tri-View Contrastive Alignment
Enforcing consistency across three views (denoised cross-domain, fused domain-aware, augmented) improves representation coherence and cross-domain transfer. Contrastive loss pulls intra-user pairs together while pushing apart inter-user pairs, with augmentations including Crop, Mask, Reorder, Substitute, and Insert. The core assumption is that different views of the same user's behavior should converge to similar embeddings while preserving semantic identity.

## Foundational Learning

- **Diffusion Models (DDPM)**
  - Why needed here: Core generative framework for iterative denoising of user representations
  - Quick check question: Can you explain the forward/reverse Markov chains and how β_t controls noise variance?

- **Cross-Attention in Transformers**
  - Why needed here: Mechanism for conditioning the decoder on disentangled guidance g_d
  - Quick check question: How does cross-attention differ from self-attention, and what are the query/key/value sources here?

- **Contrastive Learning (InfoNCE-style)**
  - Why needed here: Tri-view CL aligns representations across views
  - Quick check question: Given batch of N users, how many positive/negative pairs are formed in standard contrastive loss?

## Architecture Onboarding

- **Component map:** Disentangled Encoder -> DPG Denoiser (Encoder_c + Decoder with CA) -> Tri-view CL
- **Critical path:** 1) Extract s_x, s_y from cross-domain sequence s_c, 2) Encode -> g_x, g_y -> fuse to g_d, 3) Sample t, noise ϵ -> corrupt x_0 to x_t, 4) DPG Denoiser reconstructs x̂_0 conditioned on g_d, 5) Compute all three losses, backprop
- **Design tradeoffs:** More diffusion steps (T) -> better denoising but slower inference; stronger contrastive weight -> better alignment but may over-regularize domain-specific nuances; unsupervised CL vs. supervised: unsupervised reduces O(b²·d) complexity but may miss label-informed positives
- **Failure signatures:** Performance plateaus early -> check if guidance g_d is collapsing (g_x ≈ g_y for all users); degradation under noise -> verify augmentation types aren't introducing semantic drift; inference too slow -> reduce T (paper shows strong early-step performance in Figure 4)
- **First 3 experiments:** 1) Ablation by component: Remove Tri-CL, then remove guidance, then replace CA with concatenation—reproduce Table 3 to validate each module's contribution, 2) Noise robustness test: Inject 10-30% random item substitutions in test sequences; compare NDCG degradation against DREAM (reproduce Figure 3), 3) Inference step sweep: Run inference with T ∈ {1, 2, 5, 10, 20} steps; plot NDCG vs. steps to verify early convergence (reproduce Figure 4 pattern)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DPG-Diff scale to multi-domain scenarios involving three or more domains with varying degrees of overlap and heterogeneity?
- Basis in paper: The conclusion states: "As future work, we plan to extend our framework to multi-domain scenarios to explore its scalability and generalization."
- Why unresolved: The current formulation only handles two domains (X and Y); the disentanglement mechanism and fusion strategy are designed for binary domain relationships and may not generalize to N-domain settings with complex interaction patterns.
- What evidence would resolve it: Experiments on datasets with 3+ domains, demonstrating consistent performance gains and analyzing how domain-invariant preferences are extracted when multiple overlapping domains exist.

### Open Question 2
- Question: Can the disentanglement quality between domain-invariant, domain-specific, and noise signals be quantitatively evaluated and improved?
- Basis in paper: The paper claims successful disentanglement but relies solely on downstream recommendation metrics (Hit@K, NDCG@K) without direct measurement of how well the three signal types are separated.
- Why unresolved: Without quantitative disentanglement metrics (e.g., mutual information-based measures), it remains unclear whether performance gains stem from better disentanglement or other architectural improvements.
- What evidence would resolve it: Introduction of disentanglement evaluation metrics, visualization of learned representations showing clear separation between signal types, or ablation controlling for disentanglement quality specifically.

### Open Question 3
- Question: How robust is DPG-Diff to noise in the training data, as opposed to the inference-time noise injection tested in the paper?
- Basis in paper: The robustness study (RQ3) only evaluates noise injection during inference: "we simulate noise by randomly inserting and substituting items within test sequences." Training-time noise remains unexplored.
- Why unresolved: Real-world training data inherently contains misclicks and impulsive actions; the model's ability to learn clean representations from noisy training sequences is critical but untested.
- What evidence would resolve it: Experiments where synthetic noise is injected into training sequences (not just test), comparing performance degradation against baselines trained on the same corrupted data.

## Limitations

- Diffusion hyperparameters (T steps, noise schedule β_t, inference step count) are unspecified, creating potential for significant performance variation
- Architecture specifics including Transformer depth, attention heads, and cross-attention configuration are not detailed
- Loss weighting ratios between reconstruction, diffusion, and contrastive components remain unclear
- Augmentation intensity parameters could substantially impact contrastive learning stability

## Confidence

- **High Confidence:** The disentanglement mechanism reducing negative transfer (supported by extensive ablation studies and domain-specific performance gains)
- **Medium Confidence:** The tri-view contrastive alignment effectiveness (theoretically sound but augmentation impact not fully characterized)
- **Low Confidence:** Generalization to domains with minimal overlap (methodology assumes meaningful cross-domain signals exist)

## Next Checks

1. **Ablation reproduction:** Systematically remove Tri-CL, then guidance mechanism, then cross-attention to validate individual contribution claims against Table 3 results
2. **Noise injection testing:** Replicate Figure 3 by injecting 10-30% random item substitutions into test sequences to verify robustness claims
3. **Inference step analysis:** Run with T ∈ {1, 2, 5, 10, 20} steps to confirm early convergence pattern shown in Figure 4 and identify optimal inference efficiency tradeoff