---
ver: rpa2
title: 'When Bias Backfires: The Modulatory Role of Counterfactual Explanations on
  the Adoption of Algorithmic Bias in XAI-Supported Human Decision-Making'
arxiv_id: '2505.14377'
source_url: https://arxiv.org/abs/2505.14377
tags:
- bias
- participants
- trust
- recommendations
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study examines how biased AI recommendations, with and without
  counterfactual explanations (CEs), influence human decision-making in hiring scenarios.
  Participants made 60 hiring decisions across three phases: baseline, with biased
  AI, and post-interaction.'
---

# When Bias Backfires: The Modulatory Role of Counterfactual Explanations on the Adoption of Algorithmic Bias in XAI-Supported Human Decision-Making

## Quick Facts
- arXiv ID: 2505.14377
- Source URL: https://arxiv.org/abs/2505.14377
- Reference count: 40
- Primary result: Counterfactual explanations reversed bias adoption rather than preventing it

## Executive Summary
This study examines how biased AI recommendations, with and without counterfactual explanations (CEs), influence human decision-making in hiring scenarios. Participants made 60 hiring decisions across three phases: baseline, with biased AI, and post-interaction. The research tested whether exposure to biased AI recommendations leads to biased decisions in subsequent independent evaluations, and whether CEs affect decision alignment and trust.

Results show participants followed AI recommendations 70% of the time when qualifications were comparable, yet only 8 out of 294 detected gender bias. Without CEs, participants adopted AI bias in later decisions; with CEs, they shifted decisions in the opposite direction of the bias. Trust levels did not differ significantly across conditions, and confidence varied depending on AI bias type. The study highlights the unintended effects of CEs, showing they can reverse rather than prevent bias adoption, underscoring the need for careful design of XAI systems.

## Method Summary
Participants (N=294) completed 60 hiring decisions across three phases: baseline (20 trials, no AI), intervention (20 trials with AI recommendations), and post-interaction (20 trials, no AI). Four conditions were tested: male-biased AI, female-biased AI, and each with/without counterfactual explanations. AI recommendations were pre-generated to always favor the biased gender in mixed-gender pairs. Counterfactual explanations highlighted why the non-recommended candidate scored lower on a specific feature. Post-interaction bias shift was measured as the difference in male selection rates between phases. Quality controls included attention checks, reversed survey items, and exclusion of participants showing disengagement patterns.

## Key Results
- Participants followed AI recommendations 70% of the time when qualifications were comparable
- Only 8 out of 294 participants detected gender bias in the AI system
- Without CEs, participants adopted AI bias direction in post-interaction decisions; with CEs, they reversed it
- Trust levels remained stable across all conditions
- Confidence patterns varied by AI bias type, with significant interaction between biased gender and experimental phase

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Counterfactual explanations (CEs) trigger psychological reactance rather than neutralizing bias transmission.
- Mechanism: CEs expose decision logic sufficiently for participants to develop implicit awareness of bias direction, prompting over-compensatory adjustment in the opposite direction. Without CEs, participants unconsciously adopt AI bias patterns; with CEs, they reverse them.
- Core assumption: The reversal effect stems from reactance rather than improved objectivity (paper acknowledges this interpretation is plausible but not definitively proven).
- Evidence anchors:
  - [abstract] "participants adopted bias direction without CEs but reversed bias direction when CEs were provided"
  - [section 4.3] Significant interaction between biased gender and XAI (F(1, 126) = 9.343, p = .003); AI condition showed alignment with AI bias, XAI condition showed opposite pattern
  - [corpus] Weak corpus support; neighbor papers focus on CEs for transparency/actionability, not reactance effects
- Break condition: If CEs were explicitly calibrated to highlight bias rather than justify recommendations, reactance might convert to calibrated skepticism instead of reversal.

### Mechanism 2
- Claim: High AI alignment combined with low bias detection creates "invisible influence" where algorithmic bias propagates without conscious oversight.
- Mechanism: Participants actively incorporate AI recommendations (70% alignment) while rarely detecting systematic bias (8/294). Trust remains stable across conditions, suggesting users maintain confidence in biased systems they cannot audit.
- Core assumption: Low detection reflects genuine failure of oversight rather than inattentiveness (paper argues quality controls rule out the latter).
- Evidence anchors:
  - [abstract] "participants followed the AI recommendations 70% of the time when the qualifications of the given candidates were comparable. Yet, only a fraction of participants detected the gender bias (8 out of 294)"
  - [section 5.1] "When bias operates without conscious awareness, traditional approaches to bias mitigation relying on human oversight may prove ineffective"
  - [corpus] arXiv:2509.04404 finds biased LLM hiring recommendations alter human decision-making and limit autonomy—consistent with invisible influence pattern
- Break condition: If explanation formats explicitly flagged demographic patterns (e.g., "over 20 trials, 85% of recommendations favored male candidates"), detection rates might increase.

### Mechanism 3
- Claim: Confidence modulation depends on interaction between AI bias direction and pre-existing societal biases.
- Mechanism: Confidence varied by phase only for male-biased AI conditions, suggesting alignment with dominant societal biases produces different certainty dynamics than counter-normative bias.
- Core assumption: The gender-specific effect reflects interaction with broader cognitive schemas/social norms (paper acknowledges this as interpretation).
- Evidence anchors:
  - [section 4.4] "interaction between biased gender and phase was significant, F(2, 580) = 4.244, p = .015"
  - [section 5.3] "confidence patterns varied systematically in response to male-biased but not female-biased AI recommendations"
  - [corpus] No direct corpus evidence for this specific gender-asymmetry effect
- Break condition: If participants held strong counter-stereotypic priors, the asymmetry might reverse or disappear.

## Foundational Learning

- Concept: Counterfactual explanations (CEs)
  - Why needed here: CEs are the core manipulation; they present "if feature X were different, outcome Y would change" statements. Understanding their normative intent (transparency) vs. actual effect (reactance) is essential.
  - Quick check question: Given a hiring AI recommending Candidate A, can you construct a CE that highlights why Candidate B was not selected without implicitly justifying the recommendation?

- Concept: Automation bias
  - Why needed here: Explains why participants defer to AI even when recommendations conflict with fair decision-making. Time pressure and authority cues amplify this effect.
  - Quick check question: In a high-stakes decision task, what design cues would reduce reflexive AI deference without undermining appropriate reliance?

- Concept: Bias shift measurement (pre-post design)
  - Why needed here: The study isolates persistent effects of AI exposure by comparing decisions before and after interaction. Distinguishes temporary compliance from internalized bias.
  - Quick check question: If post-interaction bias persists 24 hours later but not 7 days later, what does this imply about the mechanism?

## Architecture Onboarding

- Component map:
  - Phase 1 (baseline): 20 trials, no AI, establishes pre-existing bias
  - Phase 2 (intervention): 20 trials with (X)AI recommendations; 4 conditions: FB-AI, MB-AI, FB-XAI, MB-XAI
  - Phase 3 (post-interaction): 20 trials, no AI, measures bias shift
  - Phase 4 (survey): Trust scale (10 items), confidence ratings, open-ended detection questions

- Critical path:
  1. Generate candidate profiles with matched aptitude scores (mean difference ≈ 0, SD ≈ 5)
  2. Assign gender via images only (not explicit markers)
  3. For bias conditions: always recommend the bias-favored candidate in mixed-gender pairs
  4. For XAI conditions: generate CEs by identifying a feature where disadvantaged candidate scored lower
  5. Compute bias shift: (post-phase male selection rate) − (pre-phase male selection rate)

- Design tradeoffs:
  - Controlled (no real ML model) vs. ecological validity: paper chooses control to isolate CE effects; tradeoff is reduced realism
  - Equal-aptitude subsample (N=130) vs. full sample (N=294): equal-aptitude removes confounds but reduces power
  - Between-subjects design prevents carryover but requires larger N

- Failure signatures:
  - High straight-lining (>10 consecutive same-side selections) indicates disengagement
  - Uniform survey responses suggest inattention
  - Failed attention check item invalidates trust data
  - Contradictory responses on reversed-wording items flags low-quality data

- First 3 experiments:
  1. Replicate with real ML model (e.g., fine-tuned resume scorer) to test whether controlled recommendations generalize to actual algorithmic bias.
  2. Vary CE framing: compare justification-style CEs ("Candidate A was selected because...") vs. audit-style CEs ("Over 20 trials, 80% of recommendations favored males") to test reactance vs. awareness mechanisms.
  3. Add delayed post-test (24h, 7d) to distinguish transient reactance from durable attitude change.

## Open Questions the Paper Calls Out
None

## Limitations
- The reactance mechanism (Mechanism 1) is plausible but not definitively proven; CEs could alternatively induce fatigue or cognitive overload leading to opposite-bias decisions. Confidence: Medium.
- The gender-specific confidence effect (Mechanism 3) lacks corpus support and may reflect task-specific confounds rather than generalizable social-norm interactions. Confidence: Low.
- The study's controlled, non-ML-generated recommendations trade ecological validity for experimental control, limiting generalizability to real-world AI systems. Confidence: High.

## Confidence
- **High**: Invisible influence pattern (Mechanism 2) - robust empirical support (70% alignment, 8/294 detection, stable trust) with corpus validation from arXiv:2509.04404.
- **Medium**: Reactance mechanism (Mechanism 1) - theoretically sound but requires direct testing of alternative explanations (fatigue, overload).
- **Low**: Gender-specific confidence modulation (Mechanism 3) - significant result but no theoretical or empirical support for the asymmetry.

## Next Checks
1. Test whether audit-style CEs ("80% of recommendations favored males") increase bias detection vs. justification-style CEs, distinguishing reactance from awareness effects.
2. Introduce delayed post-tests (24h, 7d) to determine whether opposite-bias shifts represent transient reactance or durable attitude change.
3. Replicate with real ML models (e.g., fine-tuned resume scoring) to assess whether controlled experimental effects generalize to actual algorithmic bias propagation.