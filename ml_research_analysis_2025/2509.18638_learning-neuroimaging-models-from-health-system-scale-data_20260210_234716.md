---
ver: rpa2
title: Learning neuroimaging models from health system-scale data
arxiv_id: '2509.18638'
source_url: https://arxiv.org/abs/2509.18638
tags:
- prima
- data
- brain
- sequence
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Prima is the first vision-language model trained on health system-scale
  neuroimaging data, using over 220,000 MRI studies and radiology reports to learn
  general, transferable MRI representations. It employs a hierarchical vision transformer
  architecture that processes 3D MRI sequences in subvolumes, with multimodal input
  including sequence names, to align MRI features with summarized radiology reports
  via a contrastive objective.
---

# Learning neuroimaging models from health system-scale data

## Quick Facts
- arXiv ID: 2509.18638
- Source URL: https://arxiv.org/abs/2509.18638
- Reference count: 40
- Prima achieves 92.0 ± 5.5% mean AUROC across 52 radiologic diagnoses in health system-wide study

## Executive Summary
Prima represents the first vision-language model trained on health system-scale neuroimaging data, using over 220,000 MRI studies and radiology reports to learn general, transferable MRI representations. The model employs a hierarchical vision transformer architecture that processes 3D MRI sequences in subvolumes, with multimodal input including sequence names, to align MRI features with summarized radiology reports via a contrastive objective. In a comprehensive 1-year, health system-wide diagnostic study of 29,435 patients, Prima demonstrated superior performance compared to all tested state-of-the-art general and medical vision-language models.

The model provides practical clinical applications including explainable differential diagnoses, radiology worklist prioritization, and clinical referral recommendations, while demonstrating algorithmic fairness across diverse populations and mitigating health system biases such as prolonged turnaround times in rural areas. Prima's features also transfer effectively to out-of-domain tasks like brain age estimation, autism and dementia classification, and glioma or stroke detection, validating its foundation model capabilities for neuroimaging applications.

## Method Summary
Prima uses a hierarchical vision transformer architecture to process 3D MRI sequences by breaking them into subvolumes for efficient computation. The model takes multimodal inputs including MRI sequences and sequence names, then aligns these visual features with summarized radiology reports through a contrastive learning objective. Trained on 220,000+ MRI studies from Stanford Health Care, the model learns general MRI representations that transfer across multiple diagnostic and research tasks. The training approach combines large-scale pretraining with contrastive objectives to bridge the vision-language gap specific to medical imaging.

## Key Results
- Achieved 92.0 ± 5.5% mean AUROC across 52 radiologic diagnoses in health system-wide study
- Outperformed all tested state-of-the-art general and medical vision-language models
- Successfully transferred to out-of-domain tasks including brain age estimation, autism classification, dementia detection, and glioma/stroke identification

## Why This Works (Mechanism)
The model's success stems from its ability to learn general MRI representations at health system scale, capturing the full distribution of imaging patterns and pathologies encountered in real clinical practice. By processing 3D sequences as subvolumes, Prima efficiently handles the computational complexity of volumetric data while maintaining spatial relationships. The multimodal input design, incorporating both image data and sequence metadata, provides additional context that improves diagnostic accuracy. The contrastive objective between MRI features and radiology reports enables the model to learn clinically meaningful representations aligned with radiologist interpretations.

## Foundational Learning
- Vision transformers for 3D medical imaging: Needed to process volumetric MRI data efficiently; Quick check: model can handle varying sequence dimensions
- Multimodal learning with sequence metadata: Needed to incorporate technical context beyond raw images; Quick check: sequence names improve diagnostic accuracy
- Contrastive learning from radiology reports: Needed to align visual features with clinical interpretations; Quick check: learned representations correlate with radiologist annotations
- Hierarchical processing of subvolumes: Needed to manage computational complexity of 3D data; Quick check: model maintains performance with different subvolume sizes
- Health system-scale pretraining: Needed to capture real-world imaging variability; Quick check: model performs well across diverse pathology types
- Transfer learning validation: Needed to demonstrate general representation learning; Quick check: model succeeds on out-of-domain classification tasks

## Architecture Onboarding

Component Map:
Raw MRI sequences + sequence names -> Subvolume extraction -> Hierarchical vision transformer -> Feature aggregation -> Contrastive alignment with radiology reports -> Diagnostic predictions

Critical Path:
MRI sequences and metadata → Subvolume processing → Vision transformer encoding → Contrastive objective with reports → Clinical output generation

Design Tradeoffs:
- Subvolume size vs computational efficiency: Smaller subvolumes reduce memory requirements but may lose spatial context
- Contrastive vs supervised learning: Contrastive approach leverages unlabeled reports but may be noisier than direct supervision
- Model size (700M parameters) vs performance: Balances accuracy with practical deployment constraints
- Single-center vs multi-center training: Stanford-only data provides consistency but limits generalizability

Failure Signatures:
- Performance degradation on rare pathologies not well-represented in training data
- Misalignment between model predictions and radiologist reports due to report quality issues
- Computational bottlenecks when processing very large or complex MRI sequences
- Potential bias amplification if training data underrepresents certain demographic groups

3 First Experiments:
1. Diagnostic accuracy comparison across different pathology categories to identify strengths and weaknesses
2. Ablation study removing sequence metadata to quantify its contribution to performance
3. Fairness audit across demographic subgroups to assess algorithmic bias

## Open Questions the Paper Calls Out
None provided in source material.

## Limitations
- Single-center training data may limit generalizability to other health systems
- Reliance on radiology reports introduces potential bias from documentation inconsistencies
- Computational requirements for fine-tuning may restrict accessibility for smaller institutions
- Fairness claims need more comprehensive intersectional analysis across all demographic combinations

## Confidence
- Diagnostic performance on internal Stanford cohort: High
- Generalizability to external populations: Medium
- Fairness across demographic subgroups: Medium
- Transfer learning capabilities: Medium

## Next Checks
1. External validation across multiple health systems with different scanner manufacturers and protocols
2. Prospective clinical implementation study measuring real-world diagnostic accuracy and workflow impact
3. Comprehensive bias audit across additional demographic factors including socioeconomic status and insurance type