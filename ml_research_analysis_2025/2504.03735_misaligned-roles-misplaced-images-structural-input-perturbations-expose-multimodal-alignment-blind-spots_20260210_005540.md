---
ver: rpa2
title: 'Misaligned Roles, Misplaced Images: Structural Input Perturbations Expose
  Multimodal Alignment Blind Spots'
arxiv_id: '2504.03735'
source_url: https://arxiv.org/abs/2504.03735
tags:
- layer
- arxiv
- attacks
- refusal
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Role-Modality Attacks (RMA), a novel class
  of adversarial attacks that exploit structural vulnerabilities in multimodal language
  models (MMLMs) by manipulating input prompt structures rather than query content.
  The attacks consist of Role Confusion (swapping user and assistant roles) and Modality
  Manipulation (altering image token positions), which expose weaker alignment in
  the user role compared to the assistant role.
---

# Misaligned Roles, Misplaced Images: Structural Input Perturbations Expose Multimodal Alignment Blind Spots

## Quick Facts
- arXiv ID: 2504.03735
- Source URL: https://arxiv.org/abs/2504.03735
- Reference count: 37
- One-line primary result: Novel structural adversarial attacks on VLMs via role confusion and modality manipulation bypass safety mechanisms; mitigated by adversarial training.

## Executive Summary
This paper introduces Role-Modality Attacks (RMA), a novel class of adversarial attacks that exploit structural vulnerabilities in multimodal language models by manipulating input prompt structures rather than query content. The attacks consist of Role Confusion (swapping user and assistant roles) and Modality Manipulation (altering image token positions), which expose weaker alignment in the user role compared to the assistant role. Tested across three vision-language models on eight attack settings, RMA successfully bypasses refusal mechanisms, with compositional attacks showing higher Attack Success Rates (ASR). Analysis of residual stream activations reveals that attack vectors align with the negative refusal feature direction, enabling effective refusal bypass. The authors propose adversarial training to mitigate these vulnerabilities, achieving significant ASR reduction while preserving model utility.

## Method Summary
The authors develop Role-Modality Attacks by manipulating chat template structures: swapping user/assistant role tokens (Role Confusion) and repositioning image tokens within prompts (Modality Manipulation). They evaluate on three VLMs (Qwen2-VL-7B-Instruct, LLaVA-1.5-7B, Phi-3.5-vision-instruct) using AdvBench and HarmBench datasets, measuring Attack Success Rate via target-string matching and Llama-Guard-3-8B judge. Adversarial training applies all 8 attack perturbations to harmful and harmless queries, training models to refuse harmful and answer harmless regardless of structure. Training uses QLoRA with frozen vision encoders, and utility is measured via VQA-V2 accuracy and reward metrics.

## Key Results
- RMA successfully bypasses refusal mechanisms across all tested models, with compositional attacks showing higher ASR than individual attacks
- Attack vectors align with the negative refusal feature direction in residual stream activations, enabling effective bypass
- Adversarial training reduces ASR from 21.25% to 0% for Qwen, 75.04% to 2.60% for LLaVA, and 47.38% to 2.31% for Phi-3.5 while preserving utility
- Role Confusion attacks show higher ASR than baselines, indicating weaker alignment in user role compared to assistant role

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Swapping user and assistant roles exposes weaker alignment in the user role, enabling harmful outputs.
- **Mechanism:** Post-training alignment (RLHF, safety training) focuses primarily on the assistant role during generation, leaving the user role under-aligned. When roles are swapped, the model generates from a perspective with substantially less safety training coverage.
- **Core assumption:** Safety behaviors are role-conditioned and do not fully transfer across role embeddings.
- **Evidence anchors:**
  - [abstract] "alignment stages focus primarily on the assistant role, leaving the user role unaligned"
  - [Table 1] Swap settings show significantly higher ASR than no-swap baselines (e.g., Qwen: 8.08% vs 0.58% on AdvBench TS)
  - [corpus] Corpus lacks direct replications of role-based alignment asymmetry; findings are paper-specific
- **Break condition:** If future models apply uniform safety training across all role embeddings, role confusion attacks should lose effectiveness.

### Mechanism 2
- **Claim:** Altering image token positions from default locations bypasses refusal mechanisms by creating out-of-distribution structural inputs.
- **Mechanism:** Models are conditioned on fixed chat template structures during alignment. Moving the image token (e.g., from default position to end of prompt or into assistant turn) produces structural patterns not seen during safety training, causing the learned refusal circuitry to fail to activate.
- **Core assumption:** Alignment generalizes poorly to structurally perturbed inputs that preserve query content.
- **Evidence anchors:**
  - [abstract] "stick to a fixed input prompt structure of special tokens, leaving the model vulnerable when inputs deviate from these expectations"
  - [Table 1] img_end and img_out settings show elevated ASR across models (e.g., LLaVA img_end: 87.69% TS)
  - [corpus] Adjacent work on adversarial structural perturbations (AlignTree) suggests similar template vulnerabilities, though not role-specific
- **Break condition:** If models are trained with diverse structural augmentations or template-agnostic safety objectives, position-based attacks should weaken.

### Mechanism 3
- **Claim:** Attack effectiveness correlates with the projection strength of attack vectors onto the negative refusal feature direction in residual stream activations.
- **Mechanism:** The refusal feature is a linear direction in activation space, computed via difference-in-means between harmful and harmless prompt activations. Successful attacks shift representations toward harmless regions. Projection strength (not merely cosine similarity) better predicts compositional attack success.
- **Core assumption:** Refusal is mediated by linearly represented features that can be steered via input perturbations.
- **Evidence anchors:**
  - [abstract] "attack vectors align with the negative refusal feature direction, enabling effective refusal bypass"
  - [Figure 3] Composed attacks show stronger projection coefficients (~1.0 at intermediate layers) than individual attacks
  - [corpus] Related interpretability work (Arditi et al. 2024, cited in paper) supports linear refusal direction hypothesis
- **Break condition:** If refusal is mediated nonlinearly or distributed across disentangled features, projection-based analysis may not fully explain bypass.

## Foundational Learning

- **Concept: Residual Stream Activations**
  - Why needed here: The paper analyzes attack effects via residual stream activations at the final prompt token, where refusal decisions are made.
  - Quick check question: At which token position does the model decide whether to refuse, and why does layer selection matter?

- **Concept: Difference-in-Means for Feature Extraction**
  - Why needed here: Refusal feature directions are extracted by computing mean activation differences between harmful and harmless prompt sets.
  - Quick check question: How would using different harmless/harmful datasets affect the extracted refusal direction?

- **Concept: Linear Representation Hypothesis**
  - Why needed here: The paper assumes features like refusal are linearly represented, enabling cosine similarity and projection analyses.
  - Quick check question: If refusal were nonlinearly represented, would projection strength still predict attack success?

## Architecture Onboarding

- **Component map:** Vision encoder -> Projection layer -> LLM backbone -> Chat template -> Tokenization -> Residual stream at final token -> Refusal decision -> Generation
- **Critical path:** Input → Chat template wrapping → Tokenization → Vision encoder (if image) → Projection → LLM backbone → Residual stream at final token → Refusal decision → Generation
- **Design tradeoffs:**
  - Static vs. dynamic templates: Static templates simplify training but create structural rigidity
  - Role-specific vs. unified alignment: Training only assistant role reduces compute but leaves user role vulnerable
  - Adversarial training coverage: Broader perturbation coverage improves robustness but may affect utility
- **Failure signatures:**
  - ASR spikes under structural perturbations (role swap, image repositioning)
  - Low refusal rates on harmful prompts with swapped roles
  - Projection coefficients near 1.0 onto negative refusal direction at intermediate layers
- **First 3 experiments:**
  1. Replicate role confusion attack on a target VLM: measure ASR with and without role swap across 100 harmful prompts.
  2. Extract refusal direction via difference-in-means and compute projection strength for each RMA setting; correlate with ASR.
  3. Apply adversarial training with all 8 RMA perturbations on a held-out harmful dataset; evaluate ASR reduction and VQA accuracy preservation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adversarial training approaches scale to handle the combinatorial explosion of token position permutations as MMLMs incorporate additional modalities beyond vision?
- Basis in paper: [explicit] The authors state: "As the number of modality tokens increases, the space of possible position permutations grows combinatorially, making the problem increasingly challenging to address."
- Why unresolved: The current work only tests vision modality with three image positions; extending to audio, video, or other modalities exponentially increases attack surface.
- What evidence would resolve it: Experiments with 3+ modalities showing whether adversarial training on a subset of permutations generalizes to unseen structural combinations, or whether exhaustive coverage becomes computationally infeasible.

### Open Question 2
- Question: What mechanistic explanation accounts for the observation that cosine similarity with the negative refusal direction does not fully predict compositional attack effectiveness?
- Basis in paper: [explicit] "Cosine similarity alone does not fully explain RMA's compositional advantages. In some cases, composed attacks exhibit equal or even slightly lower cosine similarity with the negative refusal direction, despite achieving higher ASR."
- Why unresolved: The paper introduces projection strength as a complementary metric but does not provide a theoretical account of why directional alignment decouples from effectiveness in composition.
- What evidence would resolve it: A formal analysis linking projection magnitude to refusal bypass probability, validated across diverse attack compositions and model architectures.

### Open Question 3
- Question: How can alignment procedures ensure consistent safety across all supported roles (user, assistant, system) rather than predominantly focusing on the assistant role?
- Basis in paper: [explicit] "Post-training alignment and safety training often focus exclusively on the assistant role, leaving the user role unaligned."
- Why unresolved: Current alignment datasets and RLHF pipelines implicitly privilege assistant responses; no established methodology exists for multi-role safety alignment.
- What evidence would resolve it: Development and validation of a multi-role alignment training scheme that reduces ASR gap between roles to within a small tolerance (e.g., <5%).

### Open Question 4
- Question: Do structural vulnerabilities generalize to closed-source or API-based MMLMs where users cannot directly manipulate prompt templates?
- Basis in paper: [inferred] The limitations section acknowledges users may lack access to structured prompts in production systems, but open-source adoption makes this increasingly relevant; the paper only tests open models.
- Why unresolved: It remains unclear whether API sanitization layers or internal template handling in closed models mitigate these attacks.
- What evidence would resolve it: Red-team evaluations on API-only models (e.g., GPT-4V, Gemini) testing whether role confusion or modality manipulation can be induced through indirect prompt engineering.

## Limitations

- Role Alignment Assumptions: The paper assumes safety behaviors are role-conditioned without direct empirical verification that safety training explicitly excludes user role tokens.
- Structural Vulnerability Generalization: Findings focus on chat template perturbations, but it's uncertain whether these vulnerabilities extend to other VLM architectures or non-chat-based interfaces.
- Refusal Direction Stability: The linear refusal direction extracted via difference-in-means may not be stable across different model versions or training runs.

## Confidence

**High Confidence**: Attack effectiveness measurements (ASR across 8 settings and 3 models), adversarial training efficacy (ASR reduction from 21.25% to 0% for Qwen, 75.04% to 2.60% for LLaVA, 47.38% to 2.31% for Phi-3.5), and utility preservation metrics (VQA accuracy maintenance).

**Medium Confidence**: The mechanism explanations linking role confusion to weaker user alignment and structural perturbations to refusal bypass, based on observed correlations rather than direct causal evidence from model internals.

**Low Confidence**: The generalizability of findings to production systems, different VLM architectures, or safety training protocols not examined in the study.

## Next Checks

1. **Role Alignment Verification**: Conduct ablation studies where user and assistant roles receive symmetric safety training, then measure whether role confusion attacks lose effectiveness, directly testing the role-conditioned alignment hypothesis.

2. **Cross-Architecture Testing**: Apply RMA attacks to VLMs with different architectural designs (e.g., SigLIP-based encoders, different chat template structures) to determine whether structural vulnerabilities are architecture-specific or universal.

3. **Refusal Direction Stability Analysis**: Extract refusal directions across multiple random seeds and model checkpoints, measuring variance and correlation between directions to assess the stability of the linear representation assumption.