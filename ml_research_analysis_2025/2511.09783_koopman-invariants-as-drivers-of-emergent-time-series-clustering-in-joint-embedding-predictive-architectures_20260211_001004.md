---
ver: rpa2
title: Koopman Invariants as Drivers of Emergent Time-Series Clustering in Joint-Embedding
  Predictive Architectures
arxiv_id: '2511.09783'
source_url: https://arxiv.org/abs/2511.09783
tags:
- jepa
- koopman
- dynamical
- regimes
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates why Joint-Embedding Predictive Architectures
  (JEPAs) spontaneously cluster time-series data by dynamical regimes, an unexplained
  phenomenon in self-supervised learning. The authors propose that JEPA's predictive
  objective implicitly drives it to learn invariant subspace representations tied
  to the system's Koopman operator eigenfunctions.
---

# Koopman Invariants as Drivers of Emergent Time-Series Clustering in Joint-Embedding Predictive Architectures

## Quick Facts
- arXiv ID: 2511.09783
- Source URL: https://arxiv.org/abs/2511.09783
- Authors: Pablo Ruiz-Morales; Dries Vanoost; Davy Pissoort; Mathias Verbeke
- Reference count: 26
- Primary result: JEPA learns invariant subspace representations tied to Koopman eigenfunctions, achieving 65.48% cluster purity vs 38.81% for autoencoder

## Executive Summary
This paper investigates why Joint-Embedding Predictive Architectures (JEPAs) spontaneously cluster time-series data by dynamical regimes, an unexplained phenomenon in self-supervised learning. The authors propose that JEPA's predictive objective implicitly drives it to learn invariant subspace representations tied to the system's Koopman operator eigenfunctions. Under specific assumptions, they prove that the idealized JEPA loss is minimized when the encoder learns regime indicator functions, which are Koopman eigenfunctions with eigenvalue 1. Empirical validation on synthetic data with known regimes confirms this: JEPA embeddings show clear clustering aligned with ground-truth regimes (65.48% cluster purity), while a comparable autoencoder achieves only 38.81%. Analysis of the learned linear predictor reveals eigenvalues near 1.0 and near-identity behavior on regime cluster centroids, supporting the theoretical prediction.

## Method Summary
The method trains a JEPA on synthetic time-series data with 18 known dynamical regimes (sinusoids, AR/MA/ARMA processes, trends, square/sawtooth waves, pulses). The architecture consists of a 4-layer 1D CNN encoder mapping input windows to 32-dimensional latents, a linear predictor (initialized to identity), and an EMA target encoder. Training uses L₂ prediction loss on overlapping context-target pairs (768-step windows, 256-step horizon). The theoretical analysis proves that JEPA loss minimization requires the encoder to learn Koopman eigenfunctions, specifically regime indicator functions. Empirical validation measures cluster purity via K-Means (K=18), predictor matrix properties (eigenvalues, Frobenius error from identity), and centroid preservation.

## Key Results
- JEPA achieves 65.48% cluster purity on 18-regime synthetic data vs 38.81% for autoencoder baseline
- Learned predictor M has eigenvalues near 1.0 and shows near-identity behavior on regime cluster centroids
- JEPA loss achieves global minimum when encoder represents regime indicator functions (Koopman eigenfunctions with eigenvalue 1)
- Identity-initialized predictor yields interpretable M; random initialization finds entangled but equivalent solutions

## Why This Works (Mechanism)

### Mechanism 1: Predictive Loss Drives Koopman Eigenfunction Learning
The JEPA predictive objective implicitly drives the encoder to learn regime indicator functions, which are Koopman eigenfunctions with eigenvalue 1. The idealized JEPA loss decomposes into prediction error and stochasticity error terms. When encoder outputs span the invariant subspace of regime indicators, both terms achieve zero because these functions satisfy Kψ = ψ and ψ(x_t+Δ) = ψ(x_t) almost surely under dynamical immiscibility. This relies on Assumption 3.1 (Finite Mixture of Ergodic Regimes) where trajectories remain confined to disjoint regime sets.

### Mechanism 2: Near-Identity Predictor Selects Interpretable Basis
Constraining the linear predictor toward identity is the key inductive bias that forces interpretable, disentangled regime representations. The JEPA loss is invariant to invertible linear transformations of the latent space—random predictor initialization finds entangled but equally valid optima. Identity initialization selects the canonical solution where M ≈ I on the regime subspace, making latent dimensions directly interpretable as regime indicators. Control experiments show both identity and random init achieve similar loss, but only identity yields interpretable M.

### Mechanism 3: EMA Target Provides Stable Learning Signal
The EMA target encoder approximation (f_EMA ≈ f_θ) enables the theoretical analysis and stabilizes learning. Slow-moving EMA parameters provide quasi-stationary targets, preventing collapse and allowing the predictor to learn meaningful transformations rather than chasing rapidly shifting representations. This approximation is validated by empirical success of similar self-supervised methods, though formal analysis of EMA's stabilizing role remains an open question.

## Foundational Learning

- Concept: Koopman Operator Framework
  - Why needed here: The central theoretical contribution links JEPA to Koopman eigenfunctions. You cannot understand the paper's claims without grasping that the Koopman operator K transforms observables ψ via (Kψ)(x) = E[ψ(x_t+Δ)|x_t=x], and its eigenfunctions with eigenvalue 1 represent invariant quantities under dynamics.
  - Quick check question: Given a system with 3 immiscible regimes, what is the dimension of the Koopman eigenspace for eigenvalue 1?

- Concept: Ergodic Decomposition
  - Why needed here: Assumption 3.1 relies on ergodic theory—each regime μ_i is ergodic (time averages equal ensemble averages within that regime), and the invariant measure decomposes as a convex mixture. This justifies why regime indicators are eigenfunctions.
  - Quick check question: Why does dynamical immiscibility (trajectories confined to regimes) guarantee that regime indicators are Koopman eigenfunctions?

- Concept: JEPA Architecture Components
  - Why needed here: The paper analyzes a specific architecture with online encoder, predictor, and EMA target encoder. Understanding the loss L(θ,φ) = E[‖g_φ(f_θ(x_t)) - f_EMA(x_t+Δ)‖²] is prerequisite to following the theoretical derivation.
  - Quick check question: What role does the EMA encoder play versus a standard target network?

## Architecture Onboarding

- Component map:
  - Input window → ConvEmbedder (4-layer 1D CNN) → Latent z_t ∈ R^32
  - Latent z_t → LinearPredictor (M initialized as I_32) → Prediction
  - Target: EMA encoder output f_EMA(x_t+Δ)
  - Loss: L₂ prediction error

- Critical path:
  1. Generate synthetic data with r=18 known regimes (ARMA, sinusoids, trends, pulses, combinations)
  2. Train JEPA with linear predictor initialized to identity
  3. Analyze: (a) cluster purity via K-Means, (b) predictor eigenvalues, (c) M action on centroids
  4. Compare against autoencoder baseline and random-initialized predictor

- Design tradeoffs:
  - Linear vs non-linear predictor: Linear enables theoretical analysis and interpretability; non-linear may improve downstream performance but obscures mechanism
  - Identity vs random initialization: Identity yields interpretable M; random finds equivalent but entangled solutions
  - Latent dimension k: Must satisfy k ≥ r (number of regimes) for encoder to represent all regime indicators

- Failure signatures:
  - No clustering observed → regimes may not be immiscible; check data generation
  - M not near-identity → try identity initialization or reduce predictor capacity
  - Cluster purity similar to autoencoder (~39%) → prediction objective may not be enforced; check loss convergence
  - Eigenvalues not near 1.0 → prediction horizon Δ may be inappropriate for regime timescales

- First 3 experiments:
  1. Replicate synthetic experiment: Train JEPA with linear predictor (identity init, k=32) on 18-regime data. Verify cluster purity >60% and M eigenvalues near 1.0.
  2. Ablate predictor initialization: Compare identity init vs random init on same data. Confirm both achieve similar loss but only identity yields interpretable M.
  3. Test boundary condition: Reduce prediction horizon Δ or introduce regime transitions. Measure when clustering degrades (purity drops toward autoencoder baseline).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the emergent clustering behavior persist in systems with gradual regime transitions rather than the disjoint, immiscible regimes currently assumed?
- Basis in paper: Section 6 states that real-world time series feature "complex phenomena like gradual transitions," which deviate from the "dynamical immiscibility" assumed in the theory.
- Why unresolved: The theoretical derivation relies on Assumption 3.1 where trajectories are strictly confined to disjoint sets, prohibiting continuous transitions between modes.
- What evidence would resolve it: Testing JEPA on synthetic data with soft probabilistic regime switching or real-world physical systems to see if distinct clusters form or if representations capture transition dynamics.

### Open Question 2
- Question: Can JEPA architectures implicitly learn Koopman eigenfunctions corresponding to non-unit eigenvalues, such as oscillatory modes or slow decay?
- Basis in paper: Section 6 suggests future work should explore "whether the architecture can learn other Koopman eigenfunctions corresponding to different dynamical modes."
- Why unresolved: The current theory and proof specifically target the eigenvalue-1 subspace (invariants), leaving the discovery of dynamic modes with complex or decaying spectra unaddressed.
- What evidence would resolve it: Analyzing the learned predictor's action on embeddings of oscillatory data to determine if it captures complex eigenvalues corresponding to the system's oscillation frequencies.

### Open Question 3
- Question: How does replacing the linear predictor with a non-linear predictor formally impact the stability and emergence of the Koopman invariant subspace?
- Basis in paper: Section 5 notes that while intuition extends to non-linear predictors, "a more formal treatment... would provide a more complete understanding of its role."
- Why unresolved: Theorem 3.4 is proven under Assumption 3.3 (Linear Predictor) for analytical tractability, leaving the interaction between non-linear predictive capacity and the linear Koopman structure theoretically undefined.
- What evidence would resolve it: Theoretical analysis or empirical ablations comparing how constraints on predictor complexity affect the linearity of the latent dynamics and the purity of the resulting clusters.

## Limitations
- Theoretical analysis assumes perfect ergodicity and dynamical immiscibility of regimes, which may not hold in real-world data
- Empirical validation relies on synthetic data with perfectly separated regimes, leaving questions about robustness to noise and gradual transitions
- Connection between identity-initialized predictor behavior and interpretability lacks rigorous theoretical grounding

## Confidence
- **High Confidence**: The empirical observation that JEPAs cluster time-series by dynamical regimes (65.48% vs 38.81% purity for autoencoder)
- **Medium Confidence**: The theoretical proof that JEPA loss minimization requires Koopman eigenfunction learning under idealized assumptions
- **Low Confidence**: The mechanism by which identity predictor initialization selects interpretable solutions over other equivalent minima

## Next Checks
1. Test JEPA clustering performance on real-world datasets with known dynamical regimes (e.g., physiological signals, financial data) to assess robustness beyond synthetic conditions
2. Investigate the effect of predictor initialization schemes on learned representations using ablation studies with different random seeds and initialization scales
3. Extend theoretical analysis to systems with gradual regime transitions or hierarchical structure to identify when and how the Koopman eigenfunction framework breaks down