---
ver: rpa2
title: 'EQuARX: Efficient Quantized AllReduce in XLA for Distributed Machine Learning
  Acceleration'
arxiv_id: '2506.17615'
source_url: https://arxiv.org/abs/2506.17615
tags:
- equarx
- allreduce
- quantization
- speedup
- while
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "EQuARX introduces a TPU-optimized quantized AllReduce implementation\
  \ for distributed machine learning that achieves 1.8\xD7 speedup over BF16 baseline\
  \ by using block-wise quantization and deep pipelining of communication and compute.\
  \ The method employs 8\xD7128 scale factors per shard for VPU-friendly quantization\
  \ and supports both full-loop and semi-loop ring variants."
---

# EQuARX: Efficient Quantized AllReduce in XLA for Distributed Machine Learning Acceleration

## Quick Facts
- arXiv ID: 2506.17615
- Source URL: https://arxiv.org/abs/2506.17615
- Authors: Ibrahim Ahmed; Clemens Schaefer; Gil Tabak; Denis Vnukov; Zenong Zhang; Felix chern; Anatoliy Yevtushenko; Andy Davis
- Reference count: 36
- Primary result: 1.8× speedup over BF16 baseline with small accuracy loss on Gemma 3 models

## Executive Summary
EQuARX introduces a TPU-optimized quantized AllReduce implementation for distributed machine learning that achieves 1.8× speedup over BF16 baseline by using block-wise quantization and deep pipelining of communication and compute. The method employs 8×128 scale factors per shard for VPU-friendly quantization and supports both full-loop and semi-loop ring variants. Applied to Gemma 3 models, EQuARX accelerates prefill stage by 1.25× (27B) and 1.1× (12B) with small to negligible accuracy loss across 10 benchmarks. The approach balances performance gains against quantization error through tunable parameters, achieving near-optimal bandwidth utilization while maintaining numerical stability.

## Method Summary
EQuARX implements native quantized AllReduce within XLA compiler for TPUs by decomposing AllReduce into reduce-scatter followed by all-gather stages, each optionally quantized. The method uses dynamic block-wise quantization with 8×128 scale factors per shard (VPU-aligned), two-phase quantization (Qp1 for metadata, Qp2 for scaling), and deep pipelining across microshards to overlap quantization/dequantization with communication. Users can select between full-loop (bandwidth-optimal, N-1 steps) and semi-loop (shorter adder tree, N/2 steps) variants, with quantization applied to reduce-scatter only, all-gather only, or both stages. Block size is controlled via minishards, and microshard count determines pipeline depth.

## Key Results
- Achieves 1.8× speedup over BF16 baseline on TPU v5e
- 1.25× prefill latency reduction for 27B Gemma 3 model, 1.1× for 12B model
- MSE of 0.001 vs 0.13 for naive FP8 quantization on synthetic tensors
- Small to negligible accuracy loss across 10 benchmarks, with some exceptions (HellaSwag -1.84%, AGIEval -2.58% for 27B)

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Block-wise Quantization with VPU-Native Dimensions
- Claim: Using 8×128 scale factors per shard (instead of a single scale) reduces quantization error while leveraging TPU's native vector register shape to avoid expensive reshapes.
- Mechanism: Each shard is divided into 8×128 chunks. For each chunk, an absolute-max reduction produces one scale factor. This yields 1024 scales per shard for a shard of shape (512, 512). Quantization is symmetric per-block, and minishards control granularity independently of shard size.
- Core assumption: Tensor magnitudes vary spatially enough that localized scaling meaningfully outperforms global scaling.
- Evidence anchors:
  - [abstract]: "dynamic block-wise quantization"
  - [Section III.B]: "We picked 8x128 to match the native 2D register vectors used by the TPU... enables us to only use the TPU's VPU to perform the absolute and max operations across different 8x128 chunks... without needing any expensive reshapes"
  - [corpus]: Weak direct evidence; corpus papers address quantization broadly (e.g., LQ-SGD, layer-wise quantization) but not block-wise collectives on TPUs.

### Mechanism 2: Deep Pipelining of Quantization Phases with Communication
- Claim: Splitting quantization into two phases (metadata Qp1, scaling Qp2) and pipelining across microshards hides most Q/Dq overhead, achieving ~90% of the theoretical 2× speedup from halving data volume.
- Mechanism: Divide shards into microshards. Send metadata first, then microshards. While receiving microshard i+1, dequantize i and begin Qp1. Because Qp1, Dq, and Add have no cross-microshard dependencies, they pipeline. Qp2 must wait until all microshards finish Qp1 if shard-wide metadata is needed; microsharding reduces bubble time.
- Core assumption: Network bandwidth is the bottleneck and compute has slack to absorb Q/Dq work during transfer; topology allows sufficient overlap depth.
- Evidence anchors:
  - [abstract]: "deep pipelining of communication and compute, EQuARX... achieves a 1.8X speedup"
  - [Section III.A]: "we divide each shard into u microshards and pipeline the communication and compute of the different microshards when possible... reduce the overhead of these bubbles"
  - [Fig. 2]: Baseline vs pipelined timeline showing bubble reduction
  - [Section IV]: "only 10% longer than the ideal execution time that would have been achieved by a 2X compression"
  - [corpus]: FlashCommunication V2 (arXiv:2508.03760) targets efficient GPU communication via bit splitting but does not address this specific Qp1/Qp2 pipelining.

### Mechanism 3: Configurable Ring Algorithm (Full-loop vs Semi-loop) with Stage Selection
- Claim: Providing full-loop (bandwidth-optimal unidirectional) and semi-loop (shorter adder tree, fewer Q/Dq steps) plus per-stage quantization (RS/AG/both) lets users navigate speed vs error tradeoffs.
- Mechanism: Full-loop reduce-scatter uses N−1 ring steps (bandwidth lower bound (N−1)D/2NB). Semi-loop uses both ring directions in parallel and only N/2 steps, producing a balanced adder tree with fewer Q/Dq invocations. Users can further quantize only RS, only AG, or both.
- Core assumption: Different workloads have different error tolerances; exposing knobs enables better Pareto selection than a one-size approach.
- Evidence anchors:
  - [Section III.C]: Full-loop vs semi-loop descriptions and bandwidth analysis
  - [Fig. 7]: Speedup vs MSE tradeoff showing semi-loop has lower error at slightly reduced speedup; AG-only quantization yields lowest MSE
  - [Table I]: Semi-loop generally shows smaller accuracy diffs than full-loop for the tested models
  - [corpus]: "Efficient AllReduce with Stragglers" addresses stragglers but not these algorithmic knobs.

## Foundational Learning

- Concept: **AllReduce as Reduce-Scatter + All-Gather**
  - Why needed here: EQuARX decomposes AllReduce into these stages and selectively quantizes them. Understanding that reduce-scatter includes summations (hence Q/Dq and overflow risk), while all-gather is communication-only, is prerequisite to reasoning about error accumulation.
  - Quick check question: On a 4-device ring, after a reduce-scatter completes, what does each device hold, and what operation is needed next to complete an AllReduce?

- Concept: **Quantization Scaling, Overflow, and Error Accumulation**
  - Why needed here: Naively quantizing before multi-hop summations can overflow low-precision ranges and accumulate error. EQuARX dequantizes to FP32 for accumulation and re-quantizes for transmission; block-wise scaling limits dynamic range per block.
  - Quick check question: If you quantized BF16 tensors to int8 and performed N−1 ring additions in int8 without intermediate dequantization, what failure modes would you expect as N grows?

- Concept: **TPU VPU and 8×128 Vector Register Shape**
  - Why needed here: The 8×128 block choice is hardware-aligned to avoid reshapes. Without this, computing per-block scale factors could introduce costly data movements that defeat pipelining gains.
  - Quick check question: If you changed the scaling block granularity to an arbitrary size not aligned to the TPU's 2D vector registers, where would you expect performance regressions?

## Architecture Onboarding

- Component map: XLA-native AllReduce with optional quantization on RS and/or AG stages → Quantization pipeline: Qp1 → send metadata → send quantized microshards → Dq → Add → Qp2 → send → Ring algorithms: Full-loop (N-1 steps) and Semi-loop (N/2 steps) → Knobs: (full/semi) × (RS-only, AG-only, both); block size via minishards; microshard count for pipeline depth

- Critical path:
  1. RS phase: Receive → Dq → Add → Qp1 (pipelined across microshards) → Qp2 (after all Qp1 complete) → Send
  2. AG phase: Quantize → Send → Receive → Dq
  3. In LLM prefill with model parallelism, AllReduce is on the critical path; in training, gradient AllReduce can sometimes overlap with backward compute

- Design tradeoffs:
  - Block size (via minishard count m): Smaller blocks → lower error, more metadata traffic
  - Microshard count (u): More microshards → better pipeline fill, but more management overhead
  - Full-loop vs Semi-loop: Full-loop → better bandwidth utilization; Semi-loop → shallower adder tree, fewer Q/Dq steps, less error
  - Stage selection: Both stages quantized → maximum speedup (~1.8× int8 vs BF16); AG-only → lowest observed MSE

- Failure signatures:
  - Latency-bound regime: Tensors < 2 MiB show negligible speedup (hop latency dominates)
  - Overflow with naive quantization: FP8-only AllReduce can produce MSE ~0.13 and risk complete corruption
  - Accuracy drops: For 27B model, HellaSwag and AGIEval showed statistically significant degradation (~1.8–2.58%); verify per-benchmark sensitivity
  - Underutilization bubbles: Without sufficient microsharding, Q/Dq stalls the network (Fig. 2 baseline)

- First 3 experiments:
  1. Microbenchmark RS+AG on a 2×2 or 4×2 topology: Sweep tensor sizes (1–16 MiB) comparing BF16 AllReduce vs int8 EQuARX; identify crossover where bandwidth benefits dominate (~≥2 MiB) and measure gap vs ideal 2× compression.
  2. Error characterization on synthetic N(0,1) tensors: Run all six flavors (full/semi × RS/AG/both) plus naive FP8; record speedup vs MSE to build a Pareto front and validate Fig. 7 trends on your topology.
  3. End-to-end LLM prefill integration: Enable EQuARX on prefill-stage AllReduce for a sharded model; report prefill latency and benchmark accuracy on tasks like MMLU/HumanEval to confirm "small to negligible" impact in your setting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can EQuARX be effectively applied to training workloads where AllReduce operations must handle gradient updates over many iterations?
- Basis in paper: [explicit] The paper notes that in training, "AllReduce for gradient synchronization can sometimes be effectively overlapped with backward pass computation," but all experiments are limited to inference (prefill stage only).
- Why unresolved: Error accumulation from repeated quantization/dequantization across thousands of training steps may behave differently than in single-pass inference.
- What evidence would resolve it: Training convergence curves for models using EQuARX compared to BF16 baseline across multiple epochs.

### Open Question 2
- Question: Can the EQuARX approach be ported to GPU architectures with comparable efficiency and accuracy?
- Basis in paper: [explicit] The authors state "In this work, we focus on TPUs, which have a different architecture and network topology" and contrast with GPU-specific approaches like gZCCL.
- Why unresolved: The deep pipelining and VPU-friendly quantization are co-designed for TPU's 8x128 vector registers and systolic arrays.
- What evidence would resolve it: Implementation and benchmarking of adapted EQuARX on NVIDIA GPUs with NVLink.

### Open Question 3
- Question: How does EQuARX perform on models outside the Gemma family, particularly with different activation distributions or sharding strategies?
- Basis in paper: [inferred] Evaluation is limited to Gemma 3 12B and 27B with a specific sharding pattern (two AllReduce operations per layer).
- Why unresolved: Different model architectures may have different sensitivity to quantization error in collectives.
- What evidence would resolve it: Benchmarks across diverse model families (e.g., LLaMA, Mistral) and alternative parallelism strategies.

### Open Question 4
- Question: Can the decode stage benefit from quantized AllReduce with optimizations targeting smaller tensor sizes?
- Basis in paper: [explicit] The paper states "the AllReduce of the decode stage is mostly latency bound" and thus EQuARX is applied "only to the AllReduce of the prefill stage."
- Why unresolved: Latency-bound collectives may require different quantization strategies than bandwidth-bound ones.
- What evidence would resolve it: Analysis of decode-stage-specific quantization schemes and their latency/error trade-offs.

## Limitations
- Hardware alignment dependency: 8×128 block size tuned for TPU VPU's native vector registers may not translate to GPUs
- Tuning parameter opacity: Exact microshard and minishard values not specified, creating reproducibility gap
- Accuracy impact variability: Statistically significant degradations on specific benchmarks (HellaSwag -1.84%, AGIEval -2.58%)

## Confidence
- High confidence: Mechanisms 1 (block-wise quantization) and 3 (configurable ring algorithms) have clear theoretical backing and experimental validation
- Medium confidence: Mechanism 2 (deep pipelining) depends heavily on specific hardware characteristics and tensor sizes not fully characterized
- Low confidence: Universal applicability claims regarding hardware portability and consistent accuracy preservation across all model types

## Next Checks
1. **Hardware portability test**: Implement EQuARX on both TPU and GPU systems using identical ring topologies. Measure speedup vs BF16 baseline for tensors ≥2 MiB, comparing the claimed 1.8× TPU speedup against GPU performance with and without reshape overhead.

2. **Accuracy sensitivity analysis**: Run end-to-end inference on Gemma 3 27B across all 10 reported benchmarks using each of the six algorithm variants (full/semi × RS/AG/both). Document which specific tasks show significant accuracy degradation to identify safety-critical failure modes.

3. **Small tensor characterization**: For tensor sizes from 1-8 MiB, measure actual speedup vs the theoretical 2× compression limit. Identify the crossover point where pipelining benefits emerge and quantify the performance gap at each size tier.