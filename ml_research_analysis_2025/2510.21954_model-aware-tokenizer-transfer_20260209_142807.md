---
ver: rpa2
title: Model-Aware Tokenizer Transfer
arxiv_id: '2510.21954'
source_url: https://arxiv.org/abs/2510.21954
tags:
- language
- arxiv
- tokenizer
- embeddings
- matt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Model-Aware Tokenizer Transfer (MATT) addresses the bottleneck
  of predefined tokenizers in large language models when adapting to lower-resource
  or distinct-script languages. Existing methods rely on semantic heuristics to initialize
  new embeddings, ignoring higher-layer model dynamics and limiting transfer quality.
---

# Model-Aware Tokenizer Transfer

## Quick Facts
- arXiv ID: 2510.21954
- Source URL: https://arxiv.org/abs/2510.21954
- Reference count: 40
- Outperforms heuristic baselines by substantial margins when transferring tokenizers to new languages

## Executive Summary
Model-Aware Tokenizer Transfer (MATT) addresses the bottleneck of predefined tokenizers in large language models when adapting to lower-resource or distinct-script languages. Existing methods rely on semantic heuristics to initialize new embeddings, ignoring higher-layer model dynamics and limiting transfer quality. MATT introduces Attention Influence Modeling (AIM), which distills inter-token communication patterns from a source model into a target model with a new tokenizer by matching attention-layer interactions. This provides a richer initialization than embedding similarity alone. Experiments across diverse linguistic settings show that MATT recovers substantial fractions of original model performance within a few GPU hours, outperforming heuristic baselines.

## Method Summary
MATT transfers tokenizers by training a target model with a new tokenizer to mimic attention dynamics from a frozen source model. First, a target tokenizer is trained on target-language data and merged with the source vocabulary. Shared tokens are initialized from the source model, while new tokens use semantic-based initialization (FOCUS). The AIM objective then distills attention patterns by matching segment-level outputs between teacher and student models, training only new token embeddings while freezing all other parameters. This distillation is applied at a single higher layer (typically the last) to balance performance and efficiency.

## Key Results
- MATT achieves 54.41% average accuracy compared to 35.99% for the best heuristic approach when transferring Gemma 3 12B to Ukrainian
- Requires only 16.6 GiB VRAM and 4 hours for 4B models
- Performance gains saturate around one-third model depth for AIM objective
- AIM with MSE loss recovers ~70% of original IT performance vs. ~57% for cosine embedding loss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aligning segment-level attention outputs transfers inter-token communication patterns more effectively than embedding similarity alone.
- Mechanism: The AIM objective computes weighted value states `v*_{i,j} = A_{i,j} * v_j` for each token pair, aggregates them into segment-level representations `s_{i,k}` using a shared segmentation function, then minimizes `L*(s_{i,k}, s'_{i,k})` between teacher and student models. This distills structural knowledge about how tokens influence each other through attention layers.
- Core assumption: Attention outputs encode transferable relational information about token semantics that is more informative than static embedding vectors.
- Evidence anchors:
  - [abstract] "MATT introduces Attention Influence Modeling (AIM) objective that distills inter-token communication patterns from a source model into a target model"
  - [section 3.3-3.4] Full mathematical derivation of segment-level attention aggregation and loss formulation
  - [corpus] Limited direct evidence; corpus papers focus on vocabulary transfer via semantic/alignment methods rather than attention dynamics
- Break condition: If new tokenizer's vocabulary is so fragmented that segment boundaries cannot align meaningfully (e.g., character-level vs. word-level extremes), the aggregation loses semantic coherence.

### Mechanism 2
- Claim: Freezing overlapping token embeddings while training only new tokens preserves learned representations and accelerates convergence.
- Mechanism: Tokens shared between source and target tokenizers are initialized from the original model and kept fixed. Only non-overlapping token embeddings receive gradients during AIM training. This anchors the embedding space while allowing new tokens to find consistent positions relative to existing structure.
- Core assumption: Shared tokens retain valid representations across tokenizers; the model's internal dynamics for known tokens generalize.
- Evidence anchors:
  - [section 3.5] "tokens that are shared between the old and new tokenizers are initialized from the original model and kept fixed, while only the embeddings of new, non-overlapping tokens are updated"
  - [table 1] Models with copied embeddings (†) consistently outperform those without
  - [corpus] "Dictionaries to the Rescue" paper similarly leverages overlap-based initialization but uses bilingual dictionaries rather than attention signals
- Break condition: If new tokenizer radically restructures even "shared" tokens (e.g., different normalization/byte-fallback rules), their embeddings may become inconsistent with new tokenization patterns.

### Mechanism 3
- Claim: Applying AIM at higher layers (up to ~⅓ model depth) captures more transferable dynamics, but with diminishing returns beyond that point.
- Mechanism: The AIM objective is computed at a configurable layer depth. Higher layers encode more abstract, context-dependent token interactions. The paper finds performance saturates around one-third depth while memory/time grow linearly, suggesting earlier layers encode tokenizer-specific patterns less useful for transfer.
- Core assumption: Higher transformer layers contain more semantic/relational information relevant to transfer than lower layers which may encode surface token statistics.
- Evidence anchors:
  - [appendix C, figure 5] "gains saturate once the objective reaches roughly one third of the model's total depth"
  - [table 5] Ablation showing last-layer AIM outperforms all-layers AIM with lower resource cost
  - [corpus] No direct corpus comparison; related work does not analyze layer-wise transfer dynamics
- Break condition: If model architecture uses pre-norm differently or has non-standard attention patterns, the optimal layer depth may shift significantly.

## Foundational Learning

- Concept: **Decoder-only transformer attention mechanism**
  - Why needed here: AIM requires computing and manipulating Q, K, V states and understanding how `A_{i,j}` weights aggregate value vectors into outputs. Without this, the segmentation and distillation logic is opaque.
  - Quick check question: Given attention matrix A and value matrix V, can you compute the output for token position 5?

- Concept: **Tokenization and vocabulary overlap**
  - Why needed here: MATT's efficiency depends on partial vocabulary sharing. Understanding how BPE/SentencePiece builds vocabularies determines whether overlap is meaningful or superficial.
  - Quick check question: If source vocab has `["▁form", "ula"]` and target has `["▁for", "mula"]`, what tokens are "shared" vs. "new"?

- Concept: **Knowledge distillation objectives**
  - Why needed here: AIM is a distillation variant. Understanding standard KD (logits matching, feature matching) provides intuition for why matching attention dynamics transfers knowledge.
  - Quick check question: How does matching intermediate representations (features/attention) differ from matching final output probabilities?

## Architecture Onboarding

- Component map:
  ```
  Input text → [Source Tokenizer T] → Source tokens → [Frozen Teacher Model] → Attention states Q,K,V,O → Segment aggregation → Teacher segment vectors s_{i,k}
           ↓
           → [Target Tokenizer T'] → Target tokens → [Student Model (frozen except new embeddings)] → Attention states → Segment aggregation → Student segment vectors s'_{i,k}
                                                                                                                              ↓
                                                                                                                    AIM Loss: L*(s_{i,k}, s'_{i,k})
  ```

- Critical path:
  1. Train target tokenizer on target language corpus
  2. Merge with source tokenizer to create extended vocabulary
  3. Identify shared vs. new tokens; initialize shared from source, new via FOCUS/FastText
  4. Run AIM training: freeze teacher + student non-embedding weights, train only new embeddings
  5. Evaluate without further training (or use as warm-start for continual pretraining)

- Design tradeoffs:
  - **Layer depth vs. efficiency**: Deeper AIM captures more signal but scales memory/time linearly. Paper recommends last layer only or ~⅓ depth.
  - **AIM* vs. AIM**: Simplified objective `L* = (1/m) Σ L(o_i, o'_i)` is faster (~⅔ VRAM) but slightly lower accuracy. Full AIM matches per-segment weighted values.
  - **Loss function**: MSE favored for generative tasks; cosine embedding loss for discriminative (paper's tentative finding, not conclusive).
  - **Freezing shared embeddings**: Faster convergence but assumes tokenizer changes don't invalidate existing representations.

- Failure signatures:
  - **Near-random performance on target language**: Likely embedding initialization failure—verify FOCUS/FastText has sufficient target corpus coverage.
  - **High discriminative but near-zero generative performance**: Suggests embedding similarity transfer (FOCUS baseline); indicates AIM not being applied or learning.
  - **OOM at low batch sizes**: Check if using all-layers AIM; switch to last-layer only or AIM*.
  - **Segment mismatch errors**: Offset-based segmentation failing—verify both tokenizers use compatible normalization (check Appendix A algorithm).

- First 3 experiments:
  1. **Reproduce single-language transfer** (Ukrainian on Gemma 3 4B): Use provided hyperparameters (lr=1e-4, AdamW, no weight decay), ~50M tokens, AIM on last layer. Verify average accuracy reaches ~51% vs. ~40% FOCUS baseline. Target: confirm setup correctness with documented numbers.
  2. **Layer depth ablation**: Run AIM at depths 3, 6, 9, 12 layers on Gemma 3 4B. Plot accuracy vs. VRAM/time. Expect saturation around layer 12 (~⅓ of 34 total layers for 4B). Target: calibrate depth/efficiency tradeoff for your hardware.
  3. **Loss function comparison**: MSE vs. cosine embedding loss on a generative task (translation BLEU). Paper suggests MSE recovers ~70% of original IT performance vs. ~57% for cosine. Target: validate loss choice for your downstream task type.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can MATT be effectively extended to models with untied input and output embeddings?
- Basis in paper: [explicit] The authors state "handling untied settings is left for future work" and in Appendix D note that "the search space remains large" for strategies addressing untied embeddings.
- Why unresolved: MATT relies on tied embeddings to propagate improvements from input embeddings to the LM head. Preliminary mapping experiments underperformed, and no systematic solution has been validated.
- What evidence would resolve it: A modified MATT variant that achieves comparable performance recovery on models like GPT-2 (untied) versus tied-embedding baselines, with ablations showing which architectural adaptations matter.

### Open Question 2
- Question: How does MATT perform when applied to encoder-only architectures such as BERT or RoBERTa?
- Basis in paper: [explicit] The Limitations section states: "we have not tested MATT on encoder-only architectures. In principle, applying it would only require removing the causal constraint in the AIM definition."
- Why unresolved: The causal mask in AIM assumes autoregressive generation; bidirectional attention may require different alignment strategies or segment definitions.
- What evidence would resolve it: Experiments transferring tokenizers in BERT-family models to new languages, comparing AIM with and without causal constraints on classification and retrieval tasks.

### Open Question 3
- Question: What are the performance gains when using MATT as initialization for full continual pretraining versus heuristic baselines?
- Basis in paper: [explicit] "We do not perform continual pretraining due to computational constraints and instead evaluate only the initialized model."
- Why unresolved: MATT is designed as a warm-up, but its advantage over heuristics may converge, widen, or disappear after extended training with a language modeling objective.
- What evidence would resolve it: A comparison of downstream task performance after 10B–50B tokens of continual pretraining, tracking whether MATT's initial lead persists and whether it reduces total tokens needed to reach a target quality threshold.

## Limitations

- The method's effectiveness depends heavily on having a reasonable amount of shared vocabulary between source and target tokenizers, with unclear performance when tokenizers are completely disjoint.
- The choice of segmentation granularity is not rigorously validated across different script typologies, which could affect the quality of attention pattern distillation.
- The paper's suggestion that AIM provides a path toward robust tokenizer transfer for "any" lower-resource language overstates the evidence, as experiments focus on high-resource or medium-resource languages.

## Confidence

**High Confidence**: The claim that AIM outperforms heuristic embedding initialization (FOCUS, WECHSEL) is well-supported by controlled experiments across multiple model sizes and languages. The layer-depth saturation finding is directly measured and clearly reported.

**Medium Confidence**: The assertion that AIM is broadly applicable to diverse language families is supported by multilingual experiments, but the evaluation focuses on a relatively narrow set of languages. The generalizability claim would be stronger with broader linguistic coverage.

**Low Confidence**: The paper's suggestion that AIM provides a path toward robust tokenizer transfer for "any" lower-resource language overstates the evidence. The experiments are primarily on high-resource or medium-resource languages with existing pretraining data, not truly low-resource scenarios.

## Next Checks

1. **Cross-Script Extreme Transfer**: Apply MATT to transfer a model from Latin script to a completely different script (e.g., English to Chinese or Arabic) where tokenizer overlap is minimal. Measure whether AIM still recovers performance or if the method fails without shared subword units. Target: Determine the minimum viable vocabulary overlap threshold.

2. **Segmentation Granularity Sensitivity**: Systematically vary the segment size and measure impact on downstream accuracy for languages with different morphological typologies (e.g., English vs. Turkish). Target: Identify optimal segmentation strategies per language type and validate the claim that segment-level attention transfer is universally effective.

3. **Untied Embedding Architectures**: Extend MATT to models with untied input/output embeddings (e.g., LLaMA-style). Validate the preliminary mapping experiments mentioned in the paper and measure performance degradation compared to tied-embedding models. Target: Confirm whether MATT can be generalized beyond the tied-embedding assumption.