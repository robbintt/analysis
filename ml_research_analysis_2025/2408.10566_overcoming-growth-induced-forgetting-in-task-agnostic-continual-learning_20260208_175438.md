---
ver: rpa2
title: Overcoming Growth-Induced Forgetting in Task-Agnostic Continual Learning
arxiv_id: '2408.10566'
source_url: https://arxiv.org/abs/2408.10566
tags:
- forgetting
- growth
- learning
- expansion
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies growth-induced forgetting as a novel form
  of catastrophic forgetting that arises specifically from model growth in continual
  learning. Unlike traditional forgetting due to data distribution shifts, this phenomenon
  occurs when expanding a model's parameter capacity disrupts retention of previously
  learned knowledge.
---

# Overcoming Growth-Induced Forgetting in Task-Agnostic Continual Learning

## Quick Facts
- **arXiv ID**: 2408.10566
- **Source URL**: https://arxiv.org/abs/2408.10566
- **Reference count**: 13
- **Primary result**: Introduces SparseGrow method combining layer expansion, gradient gating, and sparse training to address growth-induced forgetting in continual learning.

## Executive Summary
This paper identifies growth-induced forgetting as a novel form of catastrophic forgetting that occurs specifically from model growth in continual learning scenarios. Unlike traditional forgetting caused by data distribution shifts, this phenomenon arises when expanding a model's parameter capacity disrupts retention of previously learned knowledge. The authors propose SparseGrow, a method that addresses this issue by combining layer expansion with gradient gating and sparse training/initialization. Experiments on diverse datasets demonstrate that SparseGrow significantly outperforms existing methods while maintaining parameter efficiency.

## Method Summary
SparseGrow addresses growth-induced forgetting through three key mechanisms: layer expansion to increase gradient sparsity, gradient gating to protect critical parameters via preservation masks, and sparse training with on-data initialization to control plasticity. The method expands network width before capacity-constrained tasks, gates gradients to preserve important parameters, and uses threshold-based sparsification during training. A ResNet-18 base network is used with expansion adding n=2 channels, and preservation masks are generated from non-zero parameters after sparse training.

## Key Results
- SparseGrow achieves higher average accuracy than baselines across multiple datasets, including 0.5256 on 12-task Permuted MNIST
- Layer expansion strategy outperforms lateral connection and in-depth growth approaches, achieving 0.679 AAC vs 0.424 and 0.373 respectively on 4-domain Permuted MNIST
- The method demonstrates better backward transfer while maintaining parameter efficiency through sparse network optimization

## Why This Works (Mechanism)

### Mechanism 1: Layer Expansion Increases Gradient Sparsity
Widening layers increases gradient sparsity during training by distributing updates across more parameters, minimizing interference with prior knowledge. As networks grow wider, fewer parameters receive significant gradients, reducing forgetting of previously learned tasks.

### Mechanism 2: Gradient Gating Protects Critical Parameters via Preservation Masks
Preservation masks gate gradients to prevent updates to important parameters, retaining prior knowledge while allowing selective adaptation. The mask P_ij = I(|W_ij| > 0) blocks gradients for preserved parameters during backpropagation.

### Mechanism 3: Sparse Training and On-Data Initialization Controls Plasticity
Data-driven sparsification during training combined with specialized warm-up initialization of expanded parameters improves adaptability while reducing forgetting. Learnable thresholds and regularization encourage sparsity while maintaining task performance.

## Foundational Learning

- **Concept: Catastrophic vs. Growth-Induced Forgetting**
  - Why needed here: Understanding the distinction between traditional forgetting and growth-induced forgetting is essential for proper diagnosis and mitigation
  - Quick check question: Can you explain why forgetting from adding neurons differs from forgetting caused by training on new data distributions?

- **Concept: Gradient Sparsity in Wide Networks**
  - Why needed here: The core insight is that layer expansion increases gradient sparsity, protecting against forgetting
  - Quick check question: Why would adding neurons to a layer result in fewer parameters receiving significant gradient updates during backpropagation?

- **Concept: Sparse Neural Network Training**
  - Why needed here: SparseGrow relies on threshold-based data-driven pruning for implementation
  - Quick check question: How can learnable threshold parameters be optimized via backpropagation to balance sparsity and task performance?

## Architecture Onboarding

- **Component map**: Base Network -> Layer Expansion Module -> Gradient Gating System -> Sparse Training Engine -> Sparse Initialization Handler

- **Critical path**: 1) Train first dataset with sparse training, 2) Generate preservation mask P, 3) Trigger layer expansion before capacity-constrained task, 4) Initialize expanded parameters with sparse warm-up, 5) Train new tasks with gradient gating, 6) Update preservation mask

- **Design tradeoffs**: Expansion timing (before last task optimal), expansion size (n=3 optimal for capacity-limited models), sparsity weight (α = 1/(total training iterations)), growth strategy (layer expansion preferred)

- **Failure signatures**: Rapid post-expansion accuracy drop (improper initialization), stagnant new-task accuracy (excessive preservation), inconsistent BWT (seed sensitivity), EWC breakdown on later tasks (strong distribution shifts), LwF+LayExp initial decline (distillation conflicts)

- **First 3 experiments**: 1) Reproduce Table 1 comparing no growth, layer expansion, lateral connection, in-depth growth on 4-domain Permuted MNIST, 2) Ablate sparse initialization comparing random vs zero vs sparse on-data initialization on FreshStale, 3) Vary expansion hyperparameters (n ∈ {1,2,3}, timing) on DomainNet

## Open Questions the Paper Calls Out

- **Optimal expansion timing**: Future research could aim to optimize the timing of model growth rather than relying on manual scheduling
- **Neural Architecture Search integration**: Leveraging techniques such as neural architecture search to further enhance the model's adaptability
- **Adaptive expansion magnitude**: Addressing insufficient model capacity expansion in high-complexity class-incremental scenarios

## Limitations

- Implementation details underspecified: Optimizer configuration, learning rates, batch sizes, and epoch counts not provided
- Expansion trigger mechanism lacks precise specification despite claiming "before last task" timing
- Claims about parameter efficiency gains require careful scrutiny as sparse networks may need more total parameters for equivalent performance

## Confidence

- **High confidence**: The distinction between traditional catastrophic forgetting and growth-induced forgetting is well-founded and supported by experimental evidence
- **Medium confidence**: Experimental results showing SparseGrow's superiority are compelling but may be sensitive to implementation details and hyperparameter tuning
- **Low confidence**: Claims about parameter efficiency gains require careful scrutiny as sparse networks may achieve comparable accuracy but could still require more total parameters for equivalent performance

## Next Checks

1. **Gradient sparsity ablation**: Systematically vary layer expansion sizes (n=1,2,3) on Permuted MNIST and measure gradient sparsity patterns alongside forgetting metrics to establish causal relationship

2. **Expansion timing sensitivity**: Test multiple expansion schedules (before task 1, 3, 6, 12) on DomainNet to identify optimal timing and verify the "before last task" heuristic holds across datasets

3. **Parameter efficiency audit**: Compare total parameters and FLOPs for SparseGrow versus dense baselines across all datasets, ensuring efficiency claims hold when accounting for sparse network overhead