---
ver: rpa2
title: 'LARC: Towards Human-level Constrained Retrosynthesis Planning through an Agentic
  Framework'
arxiv_id: '2508.11860'
source_url: https://arxiv.org/abs/2508.11860
tags:
- larc
- planning
- retrosynthesis
- routes
- route
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LARC is the first LLM-based agentic framework for constrained retrosynthesis
  planning. It integrates an Agent-as-a-Judge evaluator with specialized chemistry
  tools to dynamically guide synthetic route generation under user-specified constraints
  like avoiding carcinogens or pyrophoric substances.
---

# LARC: Towards Human-level Constrained Retrosynthesis Planning through an Agentic Framework

## Quick Facts
- arXiv ID: 2508.11860
- Source URL: https://arxiv.org/abs/2508.11860
- Reference count: 40
- Primary result: LARC achieves 72.9% success rate on constrained retrosynthesis, outperforming general-purpose LLMs and approaching human expert-level performance.

## Executive Summary
LARC is the first LLM-based agentic framework for constrained retrosynthesis planning that integrates an Agent-as-a-Judge evaluator with specialized chemistry tools to dynamically guide synthetic route generation under user-specified constraints. The framework modifies classical search algorithms by augmenting the value function with constraint-aware scores, allowing smaller models to match or exceed larger models through tool-grounded reasoning. Evaluated on 48 tasks across three constraint types, LARC demonstrates that agentic tooling enables reliable, faster constrained synthesis planning that approaches human expert performance.

## Method Summary
LARC wraps a classical retrosynthesis planner (MEEA* using MCTS or A*) with an LLM-based EVALUATOR that uses external chemistry tools to score reactions based on user constraints. The EVALUATOR generates a static rubric and tool-selection strategy during an initial planning phase, then evaluates candidate reactions by calling specialized APIs (carcinogenicity prediction, pyrophoricity similarity search, molecule identification). These constraint scores are combined with the base planner's feasibility values using an additive value function: V'(m, R) = V(m, R) + λ ΣS(r) with λ=2. The SYNTHESIZER manages the search tree, expanding routes while the EVALUATOR provides tool-grounded feedback to ensure constraint satisfaction.

## Key Results
- LARC achieves 72.9% success rate on 48 constrained retrosynthesis tasks, vastly outperforming general-purpose LLMs
- Performance approaches human expert-level benchmarks while being substantially faster
- Ablation tests show tool integration is critical, with performance dropping from 72.9% to 45.8% when tooling is removed
- Smaller models (Mistral) can outperform larger ones (Claude) on specific constraint types through better tool integration

## Why This Works (Mechanism)

### Mechanism 1: Constraint-Aware Value Function Augmentation
The framework resolves the tension between chemical feasibility and user constraints by modifying the search heuristic rather than retraining the planner. LARC injects a dynamic constraint score, S(r), directly into the value function of A* or MCTS search. By combining the pre-trained feasibility value V(m, R) with the agentic evaluation ΣS(r), the search is penalized for expanding routes that violate constraints while retaining chemical plausibility. The additive linear combination V' = V + λ ΣS(r) preserves the optimization landscape without destabilizing convergence.

### Mechanism 2: Tool-Grounded Agentic Feedback Loop
The system achieves high reliability by offloading verification from the LLM's internal weights to deterministic, specialized cheminformatics tools. The EVALUATOR acts as a semantic router, interpreting the constraint, selecting the appropriate tool (e.g., CarcinogenicityPredictor), and translating the tool's raw output into a calibrated score (1-5). This grounds the reasoning in external truth, correcting the LLM's tendency to hallucinate chemical properties.

### Mechanism 3: Hierarchical Evaluation Planning
Decoupling "how to evaluate" from the act of evaluation reduces context overload and improves consistency across long search horizons. The EVALUATOR first runs an "Evaluation Planning" phase to generate a static rubric and tool-selection strategy for the specific constraint. This plan is cached and reused for all subsequent reaction evaluations in that task, preventing the LLM from shifting its criteria mid-search.

## Foundational Learning

- **Monte Carlo Tree Search (MCTS) and A* Search**
  - Why needed here: LARC wraps a classical search algorithm rather than generating routes end-to-end via LLM. Understanding expansion, simulation, and backpropagation is required to debug route prioritization.
  - Quick check question: Can you explain how the Upper Confidence Bound (UCB) in MCTS balances exploration vs. exploitation in this context?

- **SMILES Notation & Molecular Fingerprints**
  - Why needed here: The system relies on passing SMILES strings to tools and calculating similarity via fingerprints (e.g., Morgan fingerprints). Understanding this representation is vital for debugging tool inputs.
  - Quick check question: How would a syntax error in a SMILES string affect the CarcinogenicityPredictor tool output?

- **LLM Tool Use / Function Calling**
  - Why needed here: The core innovation is the EVALUATOR's ability to select and execute the right tool API based on the constraint.
  - Quick check question: What happens if the LLM tries to call a tool that does not exist in the toolbox?

## Architecture Onboarding

- **Component map:** User Prompt -> SYNTHESIZER (MEEA* wrapper) -> EVALUATOR (LLM agent) -> Toolbox (external APIs) -> Search State (visited molecules, routes, cached scores)
- **Critical path:**
  1. Init: EVALUATOR generates a scoring rubric (1-5 scale) based on the constraint
  2. Simulation (MCTS): SYNTHESIZER simulates routes using optimistic default score (5) for unevaluated reactions
  3. Evaluation: Top K candidate routes passed to EVALUATOR
  4. Tool Execution: EVALUATOR calls tools on molecules in the route
  5. Scoring: EVALUATOR returns S(r) for each reaction
  6. Selection (A*): SYNTHESIZER picks best route using V' and expands one step
- **Design tradeoffs:** Latency vs. Accuracy - system limits evaluations to 300 to control cost; Model Size - tool integration flattens capability curve, making smaller models viable
- **Failure signatures:** "Blind Pruning" (valid route scored 1 due to tool false positive), "Constraint Drift" (shifting criteria without evaluation planning), "Infinite Loops" (search hits 500 expansion limit)
- **First 3 experiments:** 1) Unit Test the Evaluator with known carcinogens/safe molecules, 2) Lambda Sweep varying λ on single task, 3) Ablation (No Tools) on 5-task benchmark

## Open Questions the Paper Calls Out

- Can LARC-generated synthetic routes be validated through laboratory experiments, and what fraction of routes deemed "successful" in silico are actually feasible in practice? The authors note that ultimately, testing AI-generated synthetic routes in a laboratory will be needed to validate results and translate advantages into real impacts.

- How can reaction feasibility be reliably assessed within agentic frameworks at scale, beyond current tool-based evaluations? While the authors suggest incorporating physics-based models like molecular dynamics simulations, they note this is not readily scalable.

- How does LARC performance generalize to a broader range of practical constraints beyond the three tested (carcinogens, pyrophoric substances, user-specified substances)? The authors acknowledge LARC addresses only a simplified version of practical constrained retrosynthesis planning.

- How can tool interpretation inconsistencies across different base LLMs be systematically addressed to ensure reliable constraint evaluation? The authors observe different LLMs interpret the same tool outputs differently, affecting evaluation reliability without clear calibration standards.

## Limitations

- The framework addresses only a simplified version of practical constrained retrosynthesis planning with limited constraint types tested
- Large-scale laboratory validation of AI-generated routes remains challenging and unproven
- Physics-based models for reaction feasibility assessment are not scalable with current approaches
- Tool interpretation inconsistencies across different LLMs can affect evaluation reliability

## Confidence

- **High confidence:** Tool-grounded agentic feedback substantially improves constraint satisfaction compared to vanilla LLM planners (72.9% vs 45.8% success rate)
- **Medium confidence:** Claims of "human-level" performance, as human baseline comparison is not fully detailed
- **Low confidence:** Claims about general applicability beyond specific constraint types tested

## Next Checks

1. **Edge Case Testing:** Validate the framework on retrosynthesis tasks with multiple overlapping constraints to test the limits of the additive scoring mechanism.

2. **Tool Reliability Audit:** Conduct systematic validation of external tools across diverse chemical scaffolds to quantify false positive/negative rates and their impact on success rates.

3. **Cross-Domain Generalization:** Apply LARC to retrosynthesis tasks from different chemical spaces not represented in the original 48-task benchmark to assess performance generalization.