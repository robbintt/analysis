---
ver: rpa2
title: 'From Emergence to Control: Probing and Modulating Self-Reflection in Language
  Models'
arxiv_id: '2506.12217'
source_url: https://arxiv.org/abs/2506.12217
tags:
- self-reflection
- reasoning
- arxiv
- reflection
- hidden
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Self-reflection in language models is typically attributed to reinforcement
  learning with verifiable rewards (RLVR), but this work reveals it already emerges,
  albeit rarely, in pretrained models. The authors propose Reflection-Inducing Probing,
  which injects reflection-triggering reasoning traces from fine-tuned models into
  pretrained models, increasing self-reflection frequency from 0.6% to 18.6%.
---

# From Emergence to Control: Probing and Modulating Self-Reflection in Language Models

## Quick Facts
- arXiv ID: 2506.12217
- Source URL: https://arxiv.org/abs/2506.12217
- Reference count: 40
- Self-reflection emerges in pretrained models at 0.6%, amplified to 18.6% via Reflection-Inducing Probing

## Executive Summary
Self-reflection in language models is typically attributed to reinforcement learning with verifiable rewards (RLVR), but this work reveals it already emerges, albeit rarely, in pretrained models. The authors propose Reflection-Inducing Probing, which injects reflection-triggering reasoning traces from fine-tuned models into pretrained models, increasing self-reflection frequency from 0.6% to 18.6%. Analysis of hidden states shows both pretrained and fine-tuned models maintain distinct neural signatures for reflective and non-reflective contexts. Leveraging this, they construct a self-reflection vector and demonstrate bidirectional control: enhancing reflection improves reasoning accuracy by up to 12% on benchmarks, while suppressing it reduces computational cost by over 32% without significant performance degradation.

## Method Summary
The method extracts hidden states at tokens immediately preceding reflection keywords ("wait") from fine-tuned model outputs, contrasts them with identical tokens in non-reflective contexts, and computes a difference-in-means vector per layer. This self-reflection vector is injected into pretrained models during inference by modifying hidden states: h̃^(ℓ) = h^(ℓ) + α·v^(ℓ)·⟨h^(ℓ), v^(ℓ)⟩. Positive α enhances reflection (improving accuracy by up to 12%), while negative α suppresses it (reducing response length by 32-50% with minimal accuracy loss). Layer 14 injection optimally balances control and stability.

## Key Results
- Self-reflection emerges in pretrained models at 0.6% frequency on MATH500, amplified to 18.6% via Reflection-Inducing Probing
- The self-reflection vector is task-agnostic and transferable across domains (extracted from GPQA applied to MATH500)
- Reflection enhancement improves Pass@1 accuracy by up to 12%; suppression reduces computational cost by over 32% with minimal performance degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-reflection capability exists latently in pretrained models and is amplified, not created, by fine-tuning
- Mechanism: Pretraining on diverse text encodes meta-cognitive patterns that remain dormant until triggered. RLVR/distillation increases activation frequency from ~0.6% to near 100% by shaping the model's tendency to invoke these patterns
- Core assumption: The reflection-inducing token's hidden state aggregates contextual reasoning signals that predict subsequent reflection
- Evidence anchors:
  - [abstract]: "self-reflection is not exclusive to RLVR fine-tuned models: it already emerges, albeit rarely, in pretrained models"
  - [Section 3.1]: Qwen2.5-1.5B exhibits 0.6% spontaneous self-reflection on MATH500; Reflection-Inducing Probing raises this to 18.6%
  - [corpus]: "Reflect, Retry, Reward" (arXiv:2505.24726) similarly shows self-reflection can be incentivized post-hoc, supporting the amplification hypothesis

### Mechanism 2
- Claim: The difference-in-means vector between reflective and non-reflective hidden states isolates a task-agnostic reflection direction
- Mechanism: By computing v^(ℓ) = μ_reflect^(ℓ) - μ_non-reflect^(ℓ) at layer ℓ, the method extracts a direction that correlates with the model's internal "should I reflect?" signal. Adding α·v^(ℓ) shifts hidden states along this direction, biasing the model toward or away from reflection
- Core assumption: Features are linearly represented in activation space (linear representation hypothesis)
- Evidence anchors:
  - [Section 4.1]: Equation 5 defines the self-reflection vector as difference-of-means
  - [Figure 3]: UMAP visualizations show separable clusters for reflective vs. non-reflective tokens in both pretrained and fine-tuned models at layer 15
  - [corpus]: "ReflCtrl" (arXiv:2512.13979) uses similar representation engineering for reflection control, validating the approach

### Mechanism 3
- Claim: Intervening at middle layers (notably layer 14) maximally modulates reflection without disrupting token prediction
- Mechanism: Middle layers balance abstract reasoning representation and prediction control. Early layers lack sufficient abstraction; late layers are too close to output, causing interference with generation
- Core assumption: The self-reflection feature is hierarchically constructed across layers, with peak representational clarity at mid-depth
- Evidence anchors:
  - [Figure 7]: Layer-wise ablation shows layer 14 achieves highest Pass@1 with α=0.01
  - [Appendix C]: "Injections into early layers yield only marginal gains... injecting too late often degrades performance"

## Foundational Learning

- **Concept: Residual Stream and Hidden States**
  - Why needed here: Interventions modify h^(ℓ) in the residual stream; understanding how information flows through layers is prerequisite
  - Quick check question: Can you explain why modifying h^(ℓ) at layer 14 affects the final output distribution?

- **Concept: Difference-in-Means for Feature Directions**
  - Why needed here: Core technique for extracting the self-reflection vector from contrastive hidden states
  - Quick check question: Given two sets of activations H_reflect and H_non-reflect, how would you compute a steering vector?

- **Concept: Decoder-Only Transformer Autoregression**
  - Why needed here: Reflection-inducing tokens aggregate context causally; the model predicts reflection based on prior tokens only
  - Quick check question: Why does the hidden state at position i not have access to tokens at positions > i?

## Architecture Onboarding

- **Component map:**
  Input tokens → Embedding → Layers 1-28 (each: Attention → MLP → Residual Add) → Unembedding → Softmax
  Intervention: At target layer ℓ, intercept h^(ℓ), compute modification h̃^(ℓ) = h^(ℓ) + α·v^(ℓ)·⟨h^(ℓ), v^(ℓ)⟩, pass h̃^(ℓ) forward

- **Critical path:**
  1. Collect reflection-inducing token hidden states from A_ft outputs containing "wait" → H_reflect
  2. Collect same-token-type hidden states from non-reflective outputs → H_non-reflect
  3. Compute v^(ℓ) = mean(H_reflect) - mean(H_non-reflect) per layer
  4. At inference, inject v^(ℓ) with scaling α at chosen layer
  5. α > 0 enhances reflection; α < 0 suppresses; α = 0 is baseline

- **Design tradeoffs:**
  - Positive α: Higher accuracy (up to +12%) but longer responses (more compute)
  - Negative α: Shorter responses (−32% to −50%) with minor accuracy drop (1-3%)
  - Layer choice: Middle layers (12-16) balance control and stability

- **Failure signatures:**
  - Over-reflection (α too high): Excessive "wait" tokens, circular reasoning, accuracy decline
  - Wrong layer (too early): Minimal effect; (too late): Incoherent output
  - Token mismatch: Using non-reflection-inducing tokens contaminates the contrastive signal

- **First 3 experiments:**
  1. Replicate 0.6% → 18.6% probing on Qwen2.5-1.5B using MATH500; verify reflection tokens appear
  2. Compute v^(ℓ) at layers 10, 14, 20; test α ∈ {-0.3, 0.0, 0.03} on validation split; plot accuracy vs. response length
  3. Cross-domain transfer: Extract v from GPQA-Diamond, apply to MATH500; compare to within-domain v performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific properties of pretraining data or objectives cause self-reflection capabilities to emerge?
- Basis in paper: [explicit] "Despite these observations, it remains unclear why such reflective behaviors emerge."
- Why unresolved: The paper demonstrates emergence but does not investigate causal factors in pretraining
- What evidence would resolve it: Ablation studies varying pretraining data composition or scale, measuring downstream self-reflection frequency

### Open Question 2
- Question: Can self-reflection be dynamically modulated during inference based on real-time task difficulty signals?
- Basis in paper: [explicit] "users must predefine whether to enhance or suppress self-reflection prior to inference; the model does not yet autonomously adjust its reflective behavior based on task complexity"
- Why unresolved: Current method requires manual α setting before inference
- What evidence would resolve it: Adaptive mechanism that adjusts steering based on uncertainty or task features without human intervention

### Open Question 3
- Question: Does the self-reflection vector approach transfer to closed-source models where hidden states are unavailable?
- Basis in paper: [explicit] "our approach relies on access to internal model activations, which may not be feasible in closed-source or API-limited environments"
- Why unresolved: Method requires hidden state access, limiting applicability to open-weight models
- What evidence would resolve it: Prompt-based or output-based methods achieving comparable control without activation access

### Open Question 4
- Question: Does the self-reflection vector generalize across fundamentally different architectures beyond tested model families?
- Basis in paper: [inferred] Only Qwen, DeepSeek, and Llama families tested; cross-domain transfer shown within these families but broader generalizability unknown
- Why unresolved: Limited architectural diversity leaves universality unconfirmed
- What evidence would resolve it: Cross-model vector transfer experiments across diverse architectures (Mistral, Gemma, etc.)

## Limitations
- Method requires access to internal model activations, limiting applicability to open-weight models
- Optimal intervention layer and scaling parameter must be determined empirically for each model
- Current approach does not enable dynamic, task-dependent modulation during inference

## Confidence
- Method reproducibility: High - detailed procedure with specific benchmarks and models specified
- Mechanism validity: Medium - linear representation assumption plausible but not universally proven
- Cross-domain transfer: Medium - demonstrated within tested families but broader generalizability unknown

## Next Checks
1. Verify reflection frequency baseline: Run DeepSeek-R1-Distill-Qwen-1.5B on MATH500 and confirm 0.6% spontaneous self-reflection rate
2. Replicate UMAP visualization: Extract hidden states at layer 14 for reflection vs non-reflection tokens; verify separable clusters
3. Test intervention effect: Apply v^(14) with α=0.03 to pretrained model; measure Pass@1 accuracy change and response length variation compared to baseline