---
ver: rpa2
title: Overcoming Language Priors for Visual Question Answering Based on Knowledge
  Distillation
arxiv_id: '2501.05690'
source_url: https://arxiv.org/abs/2501.05690
tags:
- question
- visual
- language
- answering
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the issue of language priors in visual question
  answering (VQA), where models rely on spurious linguistic correlations rather than
  truly understanding image content. The authors propose KDAR, a method that leverages
  knowledge distillation and an adaptive sample-wise reweighting strategy to reduce
  overfitting to common answers and improve generalization.
---

# Overcoming Language Priors for Visual Question Answering Based on Knowledge Distillation

## Quick Facts
- **arXiv ID**: 2501.05690
- **Source URL**: https://arxiv.org/abs/2501.05690
- **Reference count**: 40
- **Primary result**: KDAR achieves 71.33% accuracy on VQA-CPv2 using LXMERT, state-of-the-art performance on this challenging out-of-distribution benchmark.

## Executive Summary
This paper addresses the critical issue of language priors in visual question answering (VQA), where models learn to rely on spurious linguistic correlations rather than genuinely understanding image content. The authors propose KDAR (Knowledge Distillation with Adaptive Reweighting), a method that combines knowledge distillation with an adaptive sample-wise reweighting strategy to reduce overfitting to common answers and improve generalization. By using soft labels from a debiased teacher model and dynamically adjusting sample importance during training, KDAR significantly outperforms existing methods on the challenging VQA-CPv2 benchmark, achieving state-of-the-art results while maintaining strong performance on the in-distribution VQAv2 dataset.

## Method Summary
KDAR employs a teacher-student knowledge distillation framework where a debiased teacher model (D-VQA) provides soft label supervision to a student VQA model (e.g., UpDn or LXMERT). The training objective combines three loss components: standard binary cross-entropy with ground truth labels, KL divergence between student and teacher soft logits, and an adaptive reweighting term that dynamically adjusts sample importance based on the relative confidence of teacher and student predictions. The adaptive reweighting mechanism penalizes samples where the student shows overconfidence compared to the teacher, effectively preventing overfitting to common, biased answers. The method is evaluated on both VQA-CPv2 (out-of-distribution) and VQAv2 (in-distribution) benchmarks.

## Key Results
- KDAR achieves 71.33% accuracy on VQA-CPv2 test split using LXMERT backbone, state-of-the-art performance.
- On VQA-CPv2 validation, KDAR with UpDn backbone reaches 62.86% accuracy, surpassing previous best methods.
- The method maintains strong performance on VQAv2 (IID), demonstrating balanced generalization without sacrificing in-distribution accuracy.

## Why This Works (Mechanism)

### Mechanism 1: Soft Label Regularization
Using soft labels from a teacher model acts as a regularizer that penalizes overfitting to high-frequency answers (language priors). Standard one-hot labels force a model to assign 100% probability to a single ground truth (often the most common answer), while soft labels provide probability distributions over answers that contain information about inter-class relationships. This prevents the student from becoming overconfident in the single most frequent class, effectively smoothing the decision boundary.

### Mechanism 2: Semantic Answer Space Narrowing
Soft labels provide implicit semantic guidance that constrains the candidate answer space, improving convergence on ambiguous visual inputs. By observing the distribution of the teacher's output, the student learns to associate question types with specific subsets of the answer vocabulary, effectively filtering out irrelevant answers. For example, the teacher's output distribution for "What color?" questions will emphasize color-related answers, helping the student avoid predicting unrelated categories.

### Mechanism 3: Adaptive Sample-wise Reweighting (L_apt)
Dynamically adjusting the loss weight of specific samples based on the divergence between teacher and student performance mitigates overfitting to long-tailed distributions. If the student model significantly outperforms the teacher on a specific sample (showing high confidence while the teacher does not), the student is likely overfitting or exploiting a shortcut on a common sample. The adaptive loss term adjusts weights to manage this relative confidence gap, forcing the model to focus on samples where learning progression is genuine rather than memorized.

## Foundational Learning

- **Concept: Language Priors & Distribution Bias in VQA**
  - Why needed: To understand what is being distilled - the problem is overfitting to the head of the distribution (e.g., always predicting "tennis" for sports questions regardless of image content).
  - Quick check: If a VQA model achieves 80% accuracy on a dataset where 80% of answers are "yes", is the model effective? (No, it may just be a prior matcher).

- **Concept: Knowledge Distillation (KD)**
  - Why needed: The core method relies on a Teacher-Student paradigm where the goal is for the student to match the logits (probabilities) of the teacher, not just the ground truth label.
  - Quick check: Why use soft labels (logits) instead of just hard labels (argmax) for training the student? (Soft labels encode inter-class relationships/uncertainty).

- **Concept: Out-of-Distribution (OOD) vs. In-Distribution (IID)**
  - Why needed: The paper evaluates on VQAv2 (IID) and VQA-CPv2 (OOD). OOD tests measure if the model actually "looks" at the image or just relies on question statistics.
  - Quick check: Why is VQA-CPv2 the "stress test" for language bias? (Because the answer distribution for a given question type changes between train and test splits).

## Architecture Onboarding

- **Component map**: Teacher (D-VQA, frozen) -> Soft labels + Adaptive weighting -> Student (UpDn/LXMERT) -> Combined loss optimization
- **Critical path**:
  1. Teacher Forward Pass: Input Image+Question -> Get Logits $Z_t$. (Teacher weights frozen).
  2. Softening: Apply Softmax with Temperature $\tau$ to $Z_t$ and Student $Z_s$.
  3. Comparison: Calculate $L_{apt}$ (relative confidence check) and $L_{kd}$ (distribution matching).
  4. Optimization: Update Student weights using $L_{total} = L_{apt} + \beta L_{kd}$.

- **Design tradeoffs**:
  - Teacher Quality: The system relies entirely on the assumption that the Teacher is "debiased." If the Teacher is biased, the distillation amplifies the bias (confirmation bias).
  - Complexity: Introduces a second forward pass (Teacher), doubling inference/training memory or compute requirements during the training phase.

- **Failure signatures**:
  - Metric Divergence: High performance on VQAv2 (IID) but random performance on VQA-CPv2 (OOD) indicates the student learned the dataset bias rather than the teacher's debiasing.
  - Loss Collapse: If $L_{apt}$ goes to zero immediately, check the teacher-student logit scales; the student might be ignoring the teacher.
  - Over-regularization: If accuracy is low on both sets, $\beta$ may be too high, forcing the student to mimic a teacher that is too different from the ground truth.

- **First 3 experiments**:
  1. Sanity Check (Ablation): Train the Student with $L_{kd}$ only (no adaptive weighting) to verify that soft labels alone provide regularization benefits.
  2. Hyperparameter Sensitivity: Sweep $\tau$ (Temperature) and $\beta$ (Loss weight) on VQA-CPv2. The paper notes peaks around $\tau=2.5$ and $\beta=3$.
  3. Backbone Scalability: Apply KDAR to a stronger backbone (LXMERT) to verify if the method scales or if it is dependent on the UpDn architecture.

## Open Questions the Paper Calls Out
- **Open Question 1**: Does incorporating both biased and debiased models simultaneously as teacher models further enhance the generalization of the student model compared to using a single debiased teacher? The current study exclusively utilizes a single well-trained debiased model (D-VQA) as the teacher, and the potential complementary information or regularization effects that a biased teacher might provide remain unexplored.

- **Open Question 2**: To what extent is the method's success dependent on the specific "debiased" quality of the teacher model compared to simply having a strong ensemble or converged teacher? The paper does not ablate the effect of using a standard, non-debiased strong model as the teacher, making it difficult to determine if the student's improvement comes from the distillation process itself or the specific transfer of "debiased" knowledge.

- **Open Question 3**: Are the optimal hyperparameters for the distillation temperature ($\tau$) and loss weight ($\beta$) sensitive to the specific backbone architecture? While optimal values are identified for UpDn, the paper does not confirm if these exact same hyperparameters are optimal for Transformer-based architectures versus Attention-based ones.

## Limitations
- The entire framework relies on the assumption that the teacher model is genuinely debiased, with no extensive validation of the teacher's freedom from language priors.
- Limited ablation studies on hyperparameter sensitivity, particularly for the adaptive weighting mechanism which could be numerically unstable with extreme confidence values.
- All experiments are conducted on VQA benchmarks, with unknown generalizability to other visual reasoning tasks or domains with different bias structures.

## Confidence
**High Confidence** (supported by direct evidence):
- KDAR improves VQA-CPv2 performance compared to baseline models
- The combination of soft label regularization and adaptive reweighting contributes to performance gains
- KDAR achieves state-of-the-art results on VQA-CPv2 when applied to LXMERT

**Medium Confidence** (mechanistic claims with moderate evidence):
- Soft labels specifically reduce overfitting to high-frequency answers rather than just providing general regularization
- The adaptive reweighting mechanism L_apt effectively identifies and penalizes overfitting to common samples
- Semantic answer space narrowing occurs through teacher-student logit matching

**Low Confidence** (claims with limited or no direct evidence):
- KDAR's performance would generalize to other visual reasoning tasks beyond VQA
- The specific mathematical formulation of L_apt optimally balances teacher-student confidence divergence
- The method would maintain effectiveness with teachers of varying quality or debiasing levels

## Next Checks
1. **Teacher Quality Sensitivity Analysis**: Systematically evaluate KDAR performance using teachers with varying levels of bias (from highly biased to highly debiased) to test whether the method amplifies bias when the teacher is imperfect and identify the minimum teacher quality threshold for effectiveness.

2. **OOD Generalization Test**: Apply KDAR to a completely different visual reasoning dataset (e.g., GQA, NLVR2) and evaluate whether the performance gains transfer beyond VQA benchmarks to validate whether the debiasing generalizes to different types of visual reasoning tasks.

3. **Ablation of L_apt Component**: Conduct a more extensive ablation study isolating L_apt by testing multiple variants (fixed weighting, confidence-based without exponential transformation, teacher-only weighting) to clarify whether the specific mathematical formulation provides advantages over simpler adaptive weighting approaches.