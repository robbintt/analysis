---
ver: rpa2
title: On weight and variance uncertainty in neural networks for regression tasks
arxiv_id: '2501.04272'
source_url: https://arxiv.org/abs/2501.04272
tags:
- variance
- uncertainty
- posterior
- regression
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates variance uncertainty in Bayesian neural
  networks for regression tasks. The authors extend Blundell et al.'s (2015) Bayes
  by Backprop framework by introducing a variational posterior distribution over the
  variance parameter, rather than treating it as fixed.
---

# On weight and variance uncertainty in neural networks for regression tasks

## Quick Facts
- arXiv ID: 2501.04272
- Source URL: https://arxiv.org/abs/2501.04272
- Reference count: 20
- Primary result: VBNET-SVAR outperforms fixed-variance BNN and frequentist NN on MSPE and coverage in both synthetic and high-dimensional regression tasks

## Executive Summary
This paper introduces a Bayesian neural network framework that learns uncertainty in both weights and variance parameters simultaneously. By treating variance as a learnable latent variable with its own variational posterior, the method improves generalization and uncertainty quantification compared to traditional approaches that fix variance via cross-validation. The approach is validated through a nonlinear function approximation problem and a high-dimensional riboflavin genetic dataset, demonstrating superior performance in mean squared prediction error and coverage probabilities.

## Method Summary
The method extends Bayes by Backprop by introducing a variational posterior distribution over the variance parameter, replacing the fixed-variance assumption. Variance is parameterized as σ² = log(1 + exp(S)) to ensure positivity while allowing gradient-based optimization. The Evidence Lower Bound (ELBO) objective balances data fit against regularization from priors on both weights and variance. Stochastic optimization with the reparameterization trick enables low-variance gradient estimation for all parameters. The framework supports both Gaussian and spike-and-slab priors for weights, with the latter providing feature selection capabilities for high-dimensional settings.

## Key Results
- VBNET-SVAR achieves lower MSPE than VBNET-FIXED and frequentist neural networks on synthetic nonlinear function approximation
- The method achieves better 95% coverage probability calibration on test data compared to fixed-variance baselines
- On the riboflavin dataset (p=4088, n=71), VBNET-SVAR with spike-and-slab prior demonstrates superior performance in high-dimensional regression

## Why This Works (Mechanism)

### Mechanism 1
Treating variance as a learnable latent variable with its own posterior distribution improves generalization compared to fixing it via cross-validation. The model jointly infers weights W and variance parameter S through variational inference. Variance is parameterized as σ² = log(1 + exp(S)), ensuring positivity while allowing gradient-based optimization. The ELBO objective balances fitting the data against regularization from priors on both weights and variance.

### Mechanism 2
The reparameterization trick enables low-variance gradient estimation for both weight and variance parameters, making joint optimization tractable. Instead of directly optimizing expectations over q(θ|η), the method samples ε ~ N(0, I) and deterministically transforms to parameters: W = µw + εw ⊙ σw and S = µL + εLσL. This allows backpropagation through samples, providing unbiased gradient estimates with lower variance than score-function estimators.

### Mechanism 3
Spike-and-slab priors combined with variance uncertainty provide both feature selection and adaptive noise modeling for high-dimensional regression. The spike-and-slab prior p(W) = Π[zj N(Wj; 0, σ²₁) + (1-zj) N(Wj; 0, σ²₂)] drives irrelevant weights toward zero (spike) while allowing signal weights flexibility (slab). Simultaneously, variance uncertainty prevents the model from overconfidently underestimating noise when many features are irrelevant.

## Foundational Learning

- **Concept: Variational Inference and ELBO**
  - Why needed here: The entire method frames posterior approximation as optimization of the Evidence Lower Bound (ELBO) rather than sampling. Understanding ELBO = E_q[log p(x,θ) - log q(θ)] is essential to interpret the objective function f(W,S,η).
  - Quick check question: Given q(θ) and true posterior p(θ|x), what does KL[q||p] = 0 imply about the ELBO?

- **Concept: Reparameterization Trick**
  - Why needed here: Algorithm 1 relies on sampling ε ~ N(0,I) and computing W = µ + ε·σ. Without understanding why this enables gradient flow, you cannot debug training instability or extend to other distributions.
  - Quick check question: Why can't we directly take gradients through a sample θ ~ q(θ|η), but can through θ = g(ε, η) where ε ~ p(ε)?

- **Concept: Aleatoric vs. Epistemic Uncertainty**
  - Why needed here: The paper models variance (aleatoric noise) as uncertain, distinct from weight uncertainty (epistemic). Distinguishing these clarifies what the model learns and how to interpret prediction intervals.
  - Quick check question: If you collect more data, which uncertainty type should decrease and which should remain roughly constant?

## Architecture Onboarding

- **Component map:**
  Input X → Network ϕ(X; W) → Predicted mean ŷ
                    ↓
            Weight samples: W = µw + εw ⊙ log(1+exp(ρw))
            Variance samples: S = µL + εL · log(1+exp(ρL))
                    ↓
            Likelihood: y ~ N(ŷ, log(1+exp(S))·I)
                    ↓
            ELBO loss = E_q[log p(y|W,S,X) + log p(W) + log p(S) - log q(W) - log q(S)]

- **Critical path:**
  1. Initialize µw, ρw (weights), µL, ρL (variance) with sensible defaults (e.g., ρ initialized to small negative values for initial σ ≈ log(1+exp(ρ)) < 1).
  2. Sample εw, εL each iteration; compute W, S via reparameterization.
  3. Forward pass to compute ŷ and log-likelihood with dynamic variance.
  4. Compute ELBO gradients w.r.t. all variational parameters.
  5. Update with Adam or similar; learning rates for variance parameters (γl) may need separate tuning from weight rates (γw).

- **Design tradeoffs:**
  - Fixed vs. learned variance: Fixed variance is simpler but requires cross-validation; learned variance adds 2 parameters but adapts automatically.
  - Gaussian vs. spike-and-slab prior: Gaussian is simpler; spike-and-slab enables feature selection but increases computational cost and requires handling mixture distributions.
  - Number of samples per iteration: More samples reduce gradient variance but increase compute; typical range 1-10.

- **Failure signatures:**
  - Variance collapsing to near-zero: Check if µL → -∞ (σ² → 0); may indicate overfitting or learning rate too high for variance parameters. Solution: Add prior on S centered at reasonable noise level.
  - Prediction intervals too wide/narrow: Examine coverage probabilities (Figure 3); if coverage far from nominal 95%, variance posterior may be miscalibrated.
  - Training instability: Gradients for ρL can explode if ρL → large positive values; consider gradient clipping or constraining ρL range.

- **First 3 experiments:**
  1. Replicate nonlinear function approximation (Section 6.1): Use 800 train / 200 test samples from y = x + 2sin(2π(x+ε)) + 2sin(4π(x+ε)) + ε. Compare VBNET-SVAR vs. VBNET-FIXED on MSPE and 95% coverage. Verify variance uncertainty improves out-of-sample generalization.
  2. Ablation on variance prior: Test different priors p(S)—e.g., N(0, 1) vs. N(log(σ²_true), 0.1)—to assess sensitivity to variance prior choice. Document when learned variance hurts vs. helps.
  3. High-dimensional stress test: Apply to riboflavin dataset with PCA-BNN (25 components) vs. dropout-BNN (spike-and-slab). Compare computational cost and MSPE; verify spike-and-slab provides feature selection benefits in p >> n regime.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, focusing instead on demonstrating the proposed method's effectiveness for regression tasks with variance uncertainty.

## Limitations
- Computational overhead from learning additional variance parameters may not justify gains in simpler settings
- No comparison to modern uncertainty quantification methods like ensemble approaches or deep ensembles
- Potential instability when variance posterior becomes degenerate

## Confidence
- Mechanism 1 (variance learning improves generalization): Medium
- Mechanism 2 (reparameterization trick enables joint optimization): High
- Mechanism 3 (spike-and-slab priors enable feature selection): Low

## Next Checks
1. Test sensitivity to variance prior hyperparameters by varying prior variance on S across orders of magnitude and measuring MSPE degradation.
2. Compare VBNET-SVAR against deep ensembles and MC dropout on the synthetic function task, controlling for computational budget.
3. Evaluate variance uncertainty in a sparse-signal setting (e.g., sparse linear regression with Gaussian noise) where variance learning should help vs. hurt.