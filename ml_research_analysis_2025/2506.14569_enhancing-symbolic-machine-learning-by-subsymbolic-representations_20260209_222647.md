---
ver: rpa2
title: Enhancing Symbolic Machine Learning by Subsymbolic Representations
arxiv_id: '2506.14569'
source_url: https://arxiv.org/abs/2506.14569
tags:
- embeddings
- symbolic
- tilde
- learning
- subsymbolic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes integrating subsymbolic neural embeddings into
  the TILDE symbolic learning system, using similarity predicates based on embeddings
  (e.g., word2vec, GloVe, GenePT). The approach includes: (1) compiling symbolic predicates
  and adding a subsymbolic similar/2 predicate for semantic similarity; (2) learning
  decision trees with TILDE that use both symbolic and embedding-based similarities;
  (3) refining embeddings via Logic Tensor Networks (LTN) to better satisfy learned
  rules.'
---

# Enhancing Symbolic Machine Learning by Subsymbolic Representations

## Quick Facts
- **arXiv ID:** 2506.14569
- **Source URL:** https://arxiv.org/abs/2506.14569
- **Reference count:** 27
- **Primary result:** Integrating subsymbolic embeddings into TILDE with LTN fine-tuning improves F1 scores across hate speech, spam, and drug response tasks.

## Executive Summary
This paper presents a neuro-symbolic learning system that integrates subsymbolic neural embeddings into the TILDE symbolic learning framework. The approach introduces similarity predicates grounded in embedding cosine similarity, allowing symbolic decision trees to generalize beyond exact lexical matches. The system further refines embeddings via Logic Tensor Networks to better satisfy learned rules, achieving substantial F1 score improvements across three diverse domains while maintaining interpretability.

## Method Summary
The method involves compiling symbolic predicates and adding a subsymbolic `similar/2` predicate for semantic similarity. TILDE learns decision trees using both symbolic and embedding-based similarities, with constants learned as Y-arguments of the similarity predicate. Logic Tensor Networks then refine the embeddings to maximize rule satisfaction, correcting suboptimal constant selections. The approach uses precomputed similarity groundings filtered by symbolic predicates for efficiency.

## Key Results
- F1 score improvements across all three domains when using embedding-based similarity predicates
- LTN fine-tuning corrects suboptimal constant selections and enhances classification accuracy
- Outperforms baselines including hand-crafted rules and TILDE alone
- Fine-tuning all embeddings yields higher performance than restricting to constants only

## Why This Works (Mechanism)

### Mechanism 1
Integrating subsymbolic similarity allows symbolic learners to generalize beyond exact lexical matches. The `similar/2` predicate grounded in cosine similarity between pre-trained embeddings enables TILDE to identify entities with high semantic similarity, applying rules to related concepts without explicit enumeration. This works when the embedding space's geometric proximity correlates with task-specific semantic interchangeability.

### Mechanism 2
Fine-tuning embeddings via Logic Tensor Networks corrects suboptimal constant selections made during initial symbolic learning. TILDE trees convert to fuzzy logic rules, and gradient descent adjusts embeddings to maximize rule satisfaction. This shifts embeddings toward predictive clusters, rewriting the rule's semantic focus without changing symbolic structure. The approach assumes the initial tree structure is directionally correct.

### Mechanism 3
Filtering subsymbolic computation via symbolic predicates improves memory efficiency without information loss. Symbolic predicates like `contains_word` act as masks, retrieving embeddings only when needed. This prevents unnecessary gradient calculations for irrelevant entities, assuming symbolic predicates provide a sufficiently tight bound on relevant entities.

## Foundational Learning

- **First-Order Logic (FOL) and Inductive Logic Programming (ILP):** TILDE builds decision trees using FOL queries at nodes. Understanding variable binding in predicates like `similar(X, Y)` is essential.
  - *Quick check:* How does a first-order decision tree differ from a standard propositional tree regarding feature testing at a node?

- **Vector Space Models (Embeddings):** The method assumes words/genes map to vectors where distance implies similarity. Without this, `similar/2` is just a black box.
  - *Quick check:* Why is cosine similarity generally preferred over Euclidean distance for high-dimensional text embeddings?

- **Fuzzy Logic & Real-valued Logic Satisfaction:** LTN bridges discrete logic and continuous optimization by treating logical connectives as differentiable operations.
  - *Quick check:* In LTN, how does the "satisfaction level" of a formula relate to the loss function used for training?

## Architecture Onboarding

- **Component map:** Data Preprocessor -> Embedding Store -> TILDE (ACE) -> LTN Refiner -> Inference Engine
- **Critical path:** Grounding (generate `similar/2` facts) → Structure Learning (TILDE tree) → Translation (fuzzy rules) → Refinement (LTN updates) → Deployment (tree + refined embeddings)
- **Design tradeoffs:** Constants-only vs. all embeddings fine-tuning (performance vs. semantic preservation); hand-crafted vs. learned rules (general vs. specific performance).
- **Failure signatures:** Semantic collapse (embeddings converge to trivial solution); threshold brittleness (similarity predicate too sensitive/narrow).
- **First 3 experiments:** 1) Ablation: TILDE vs. TILDE+`similar/2` on hate speech; 2) LTN validation: F1 before/after refinement on spam; 3) Embedding visualization: "urgent" before/after refinement via SOM.

## Open Questions the Paper Calls Out

1. **Dynamic predicate leveraging:** Would extending TILDE to dynamically leverage subsymbolic predicates during rule induction improve semantic flexibility and classification performance?
2. **Learnable similarity thresholds:** Would making similarity thresholds learnable parameters improve predictive performance over fixed user-defined thresholds?
3. **Scalability limits:** Can the approach scale to domains with significantly larger numbers of constants without excessive computational overhead from precomputing all pairwise similarities?
4. **Semantic preservation trade-off:** To what extent does fine-tuning all embeddings versus only constants trade off between task performance and preservation of original semantic relationships?

## Limitations

- Performance bounded by quality and coverage of pre-trained embeddings, especially in specialized domains
- LTN refinement assumes initial TILDE structure is sound; structural errors may propagate as false patterns
- Shifted sigmoid and fuzzy logic operators for LTN are not explicitly defined, leaving implementation variance
- Precomputing all pairwise similarities may become computationally prohibitive in large-scale domains

## Confidence

- **High:** Integration of symbolic and subsymbolic representations improves F1 scores across multiple domains
- **Medium:** LTN fine-tuning corrects suboptimal constant selections, given lack of explicit fuzzy logic operator definitions
- **Low:** Claims about filtering subsymbolic computation improving memory efficiency are weakly supported by direct evidence

## Next Checks

1. Reproduce the hate speech ablation experiment to isolate the contribution of the `similar/2` predicate
2. Implement and validate the LTN refinement pipeline, ensuring shifted sigmoid and fuzzy logic operators are consistent with paper intent
3. Visualize embedding trajectories before and after LTN fine-tuning to confirm semantic shifts toward predictive clusters