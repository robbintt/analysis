---
ver: rpa2
title: 'SYMPHONY: Synergistic Multi-agent Planning with Heterogeneous Language Model
  Assembly'
arxiv_id: '2601.22623'
source_url: https://arxiv.org/abs/2601.22623
tags:
- agent
- symphony
- search
- reasoning
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SYMPHONY addresses the limited diversity in LLM-based planning
  by introducing a multi-agent framework that integrates heterogeneous language models
  for Monte Carlo Tree Search. The method combines UCB-based agent scheduling, entropy-modulated
  confidence scoring, and pool-wise memory sharing to enhance exploration and robustness.
---

# SYMPHONY: Synergistic Multi-agent Planning with Heterogeneous Language Model Assembly

## Quick Facts
- arXiv ID: 2601.22623
- Source URL: https://arxiv.org/abs/2601.22623
- Reference count: 40
- SYMPHONY achieves state-of-the-art performance on HotpotQA, WebShop, and MBPP using heterogeneous LLM ensembles with fewer node expansions and lower token costs than prior methods.

## Executive Summary
SYMPHONY addresses the limited diversity in LLM-based planning by introducing a multi-agent framework that integrates heterogeneous language models for Monte Carlo Tree Search. The method combines UCB-based agent scheduling, entropy-modulated confidence scoring, and pool-wise memory sharing to enhance exploration and robustness. Experiments on HotpotQA, WebShop, and MBPP show SYMPHONY achieves state-of-the-art performance, even with open-source models on consumer hardware, while requiring fewer node expansions and lower token costs than prior methods. The approach demonstrates strong efficiency, adaptability, and scalability in complex reasoning and planning tasks.

## Method Summary
SYMPHONY extends Monte Carlo Tree Search by integrating a heterogeneous pool of language models that work synergistically through a UCB-based scheduling mechanism. The framework employs entropy-modulated confidence scoring to stabilize value propagation and uses pool-wise memory sharing to retain successful action patterns across agents. Three variants are evaluated: SYMPHONY-S (open-source models on consumer hardware), SYMPHONY-L (API-based models), and SYMPHONY-M (mixed). The method demonstrates that combining diverse reasoning patterns through coordinated multi-agent planning improves exploration, reduces model-specific biases, and enhances performance on complex, multi-hop tasks.

## Key Results
- SYMPHONY-S (Qwen2.5-7B + Mistral-7B + Llama-3.1-8B) achieves 64.1% accuracy on HotpotQA, outperforming single-agent baselines and matching API-based models
- SYMPHONY-L (GPT-4 + Qwen2.5-72B + Llama-3.1-70B) reaches 73.1% accuracy on HotpotQA, surpassing all prior methods including GPT-4-only baselines
- On MBPP, SYMPHONY achieves 92.7% pass@1 accuracy while requiring fewer node expansions and lower token costs than single-agent approaches

## Why This Works (Mechanism)

### Mechanism 1: Heterogeneous Agent Pool Diversity
- **Claim:** A heterogeneous agent pool generates more diverse search tree branches, leading to more effective exploration and reduced model-specific bias.
- **Mechanism:** The framework uses multiple language models with different pretraining sources and reasoning styles (e.g., Qwen2.5-7B, Mistral-7B, Llama-3.1-8B for local deployment). Each agent independently proposes candidate actions at each search node, ensuring generated rollouts are not dominated by a single model's learned patterns. This structural diversity increases the likelihood of finding complementary or novel reasoning paths.
- **Core assumption:** The performance gains depend on the assumption that agents have meaningfully different inductive biases. If models are too similar, diversity benefits diminish.
- **Evidence anchors:**
  - [abstract] "By leveraging diverse reasoning patterns across agents, SYMPHONY enhances rollout diversity and facilitates more effective exploration."
  - [section 3.2] "This diversity increases the likelihood of generating complementary reasoning paths, reduces model-specific biases, and improves performance on complex, multi-hop tasks."
  - [section 4.5, Figure 2] Moving from single-agent to a three-model ensemble raises 4-Unique node expansions from <20% to >80% on MBPP, correlating with ~30% accuracy gains.
  - [corpus] The paper "Rethinking the Value of Multi-Agent Workflow" (arXiv:2601.12307) questions whether multi-agent gains stem from workflow or prompt diversity, suggesting heterogeneity claims require careful agent selection.

### Mechanism 2: UCB-Based Adaptive Scheduling
- **Claim:** A UCB-based adaptive scheduler dynamically selects the most historically effective agent for each task, improving coordination and planning efficiency.
- **Mechanism:** Agent selection is framed as a multi-armed bandit problem. Each agent maintains a cumulative utility score (`Q̄`) and invocation count (`N_i`). At each step, the agent with the highest Upper Confidence Bound (`UCB = Q̄ + α√(ln(N_total)/(N_i+1))`) is selected, balancing exploitation of high-performing agents with exploration of underutilized ones.
- **Core assumption:** Theoretical proof (Theorem 1) relies on two strong assumptions: (1) **correct coverage**—at each step, at least one agent outputs the correct action; (2) **non-triviality**—no single agent is correct on all steps. These are idealized and may not hold for complex, open-ended tasks.
- **Evidence anchors:**
  - [section 3.3] "This formulation favors agents that exhibit either superior historical returns or low invocation frequency..."
  - [Appendix B, Theorem 1] Formal proof that ensemble sampling has strictly lower expected error than any single deterministic agent, conditional on correct coverage and non-triviality.
  - [section 4.6, Figure 3] In SYMPHONY-L, GPT-4 is invoked in ~40% of calls, yet the system outperforms GPT-4-only baseline, showing effective load distribution.

### Mechanism 3: Entropy-Modulated Confidence Scoring
- **Claim:** Entropy-Modulated Confidence Scoring (EMCS) down-weights value estimates of uncertain nodes, stabilizing MCTS value propagation.
- **Mechanism:** After expanding a node, a scheduled agent outputs both a value estimate (`Z(s_t) ∈ [0,1]`) and a confidence score (`C(s_t) ∈ (0,1)`). Confidence is interpreted as Bernoulli success probability; entropy `E(s_t) = -C·ln(C) - (1-C)·ln(1-C)` is maximal at 0.5 and minimal near 0 or 1. Final node reward: `R(s_t) = Z(s_t)·(1 - E(s_t))`, penalizing high-uncertainty evaluations.
- **Core assumption:** The agent's self-reported confidence score is a reliable proxy for actual uncertainty. This depends on model calibration, which can be imperfect.
- **Evidence anchors:**
  - [section 3.5] "This formulation preserves confident evaluations while suppressing uncertain ones..."
  - [Table 5, ablation] Removing EMCS causes MBPP pass@1 to drop from 0.927 to 0.892.

## Foundational Learning

- **Concept:** Monte Carlo Tree Search (MCTS)
  - **Why needed here:** MCTS is the core planning algorithm SYMPHONY extends. It iteratively builds a search tree through selection (UCB-guided traversal), expansion (adding child nodes), simulation (rollout to estimate value), and backpropagation (updating node statistics).
  - **Quick check question:** Can you explain how UCT balances exploration and exploitation in node selection?

- **Concept:** Upper Confidence Bound (UCB) / Multi-Armed Bandits
  - **Why needed here:** SYMPHONY adapts UCB for both MCTS node selection and agent scheduling. Understanding the exploration-exploitation trade-off and how confidence bounds are computed is essential.
  - **Quick check question:** How does increasing the exploration constant `α` affect agent selection behavior in SYMPHONY's scheduler?

- **Concept:** LLM Calibration and Uncertainty Quantification
  - **Why needed here:** The EMCS mechanism relies on LLM-generated confidence scores being meaningful proxies for uncertainty. Understanding model calibration issues helps diagnose when this assumption might fail.
  - **Quick check question:** Why might an LLM be systematically overconfident, and how would that affect entropy-modulated scoring?

## Architecture Onboarding

- **Component map:**
  Initial State → [UCB Scheduler] → [Heterogeneous Agent Pool] → Node Expansion
                              ↓
                    [EMCS Evaluator] → Entropy-Modulated Reward
                              ↓
                    [MCTS Selection/Simulation] → Next Node
                              ↓
                    [Reflection Generator] (on failure) → [Pool-wise Memory]

- **Critical path:**
  1. **Agent Pool Configuration:** Select models with genuinely diverse architectures or training. The paper shows Qwen+Mistral+Llama works; mixing similar models won't.
  2. **Scheduler Warm-up:** The UCB scheduler needs initial exploration before exploiting. Use a higher `α` (paper uses 20) early, potentially annealing.
  3. **Confidence Calibration:** Ensure agents output calibrated confidence. Monitor if certain agents are systematically over/under confident.
  4. **Memory Management:** The FIFO memory buffer size affects how much historical reflection is retained. Too small = no learning; too large = context overflow.

- **Design tradeoffs:**
  - **Agent Count vs. Latency:** More agents increase diversity but linearly increase parallel API calls or GPU memory usage.
  - **Exploration (`α`) vs. Convergence:** High `α` ensures all agents are tried but delays convergence to the best; low `α` may lock onto a suboptimal agent.
  - **Memory Buffer Size:** Larger buffers retain more failure reflections but consume more context tokens.
  - **SYMPHONY-S vs. SYMPHONY-L:** Consumer-grade models (S) are cheaper but weaker; API models (L) are stronger but costlier. The paper shows S can be competitive.

- **Failure signatures:**
  - **All agents converge to similar outputs:** Agent pool lacks heterogeneity.
  - **Scheduler over-exploits one agent:** `α` too low or early noisy rewards misled UCB.
  - **Value estimates are noisy/unstable:** EMCS not suppressing uncertainty effectively; check agent calibration.
  - **Memory overflow:** FIFO buffer too large for context window; reduce size.
  - **No improvement over single-agent:** Heterogeneity or scheduling not providing benefit; verify models are truly different.

- **First 3 experiments:**
  1. **Ablate agent heterogeneity:** Run with 1 agent, 2 similar agents, 2 diverse agents, 3 diverse agents. Plot "4-Unique" branch percentage and task accuracy. Should replicate Figure 2.
  2. **Validate UCB scheduler:** Log which agent is selected at each step. Verify that the best agent(s) are selected more often over time but under-explored agents still get chances. Compare against random scheduling.
  3. **Test EMCS contribution:** Run with and without entropy modulation. Plot the distribution of final rewards `R(s_t)` and measure variance. The EMCS version should have tighter reward distribution and better final accuracy, especially on uncertain nodes. This validates Table 5 ablation.

## Open Questions the Paper Calls Out
None

## Limitations
- **Heterogeneity Assumption Fragility:** The claimed diversity benefits depend critically on the agent pool containing models with genuinely different inductive biases. If models are too similar, performance gains from ensemble methods diminish significantly.
- **UCB Scheduling Optimality:** While Theorem 1 proves theoretical advantages under idealized assumptions, these conditions rarely hold in practice. The exploration-exploitation trade-off managed by hyperparameter α requires careful tuning.
- **Confidence Score Reliability:** EMCS assumes agents' self-reported confidence scores accurately reflect true uncertainty, but LLM calibration is notoriously difficult and systematic overconfidence or underconfidence would cause the entropy penalty to be misapplied.

## Confidence

- **High Confidence:** The architectural design combining MCTS with heterogeneous agent pools is technically sound and the empirical results showing improved accuracy on multiple benchmarks are reproducible.
- **Medium Confidence:** The claimed efficiency improvements (fewer node expansions, lower token costs) are supported by ablation studies but require careful hyperparameter tuning.
- **Low Confidence:** The scalability claims to "complex, open-ended tasks" extend beyond the tested domains, and generalization to radically different problem spaces remains unproven.

## Next Checks
1. **Agent Pool Diversity Validation:** Systematically vary the agent pool composition from identical models to increasingly diverse models, measuring both the "4-Unique" branch percentage and task accuracy to establish the minimum heterogeneity threshold required for performance gains.
2. **UCB Scheduler Robustness Testing:** Create controlled environments where one agent is deliberately weakened after initial strong performance, then measure whether the UCB scheduler recovers and switches to better agents versus becoming locked into the suboptimal choice.
3. **Confidence Calibration Assessment:** Evaluate the entropy-modulated scoring across agents with known calibration properties (overconfident vs. well-calibrated), measuring whether EMCS actually improves accuracy on poorly calibrated agents or simply amplifies their errors.