---
ver: rpa2
title: 'Surfer 2: The Next Generation of Cross-Platform Computer Use Agents'
arxiv_id: '2510.19949'
source_url: https://arxiv.org/abs/2510.19949
tags:
- arxiv
- agents
- agent
- task
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Surfer 2 introduces a unified, hierarchical architecture for cross-platform
  GUI agents that achieves state-of-the-art performance across web, desktop, and mobile
  environments using purely visual interaction. The system decouples high-level planning
  (Orchestrator) from low-level execution (Navigator) and employs multi-stage validation
  with adaptive recovery.
---

# Surfer 2: The Next Generation of Cross-Platform Computer Use Agents

## Quick Facts
- **arXiv ID:** 2510.19949
- **Source URL:** https://arxiv.org/abs/2510.19949
- **Reference count:** 40
- **Primary result:** Achieves state-of-the-art performance across web, desktop, and mobile environments using purely visual interaction, surpassing human baselines on OSWorld and AndroidWorld.

## Executive Summary
Surfer 2 introduces a unified, hierarchical architecture for cross-platform GUI agents that achieves state-of-the-art performance across web, desktop, and mobile environments using purely visual interaction. The system decouples high-level planning (Orchestrator) from low-level execution (Navigator) and employs multi-stage validation with adaptive recovery. Without task-specific fine-tuning, Surfer 2 attains 97.1% on WebVoyager, 69.6% on WebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, surpassing human baselines on OSWorld and AndroidWorld. Multiple-attempt scaling enables it to exceed human performance across all benchmarks.

## Method Summary
Surfer 2 employs a hierarchical architecture separating high-level planning (Orchestrator) from low-level execution (Navigator). The Orchestrator decomposes complex tasks into verifiable subtasks, while the Navigator executes them via a perception-action loop using ReAct methodology. The system uses purely visual interaction through screenshots, with a specialized localization model (Holo1.5) grounding textual action descriptions to pixel coordinates. Multi-stage validation with an LLM-as-a-Judge module enables self-correction and recovery. The architecture operates without task-specific fine-tuning, using different frontier models (o3, Claude Sonnet 4.5, GPT-4.1) based on benchmark requirements.

## Key Results
- Achieves 97.1% on WebVoyager, 69.6% on WebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld without task-specific fine-tuning
- Surpasses human baselines on OSWorld and AndroidWorld benchmarks
- Multiple-attempt scaling enables performance exceeding human baselines across all benchmarks
- Purely visual interaction enables unified cross-platform operation without environment-specific interfaces

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Separation of Planning and Execution
Decoupling high-level planning from low-level execution improves performance on long-horizon, cross-platform tasks. The Orchestrator decomposes complex tasks into verifiable subtasks, while the Navigator executes them via a perception-action loop. This separation reduces cognitive load on each component and creates natural retry boundaries. Core assumption: Complex, multi-step tasks benefit from explicit decomposition. Evidence: Abstract and Section 3.1 explicitly describe this separation. Break condition: Explicit task decomposition may add overhead for tasks with ambiguous or highly interdependent subtask boundaries.

### Mechanism 2: Multi-Stage Validation and Adaptive Recovery
A dedicated validation module reduces premature termination and error propagation. After a `answer` action, a Validator (VLM-as-a-Judge) reviews execution traces and recent screenshots. If validation fails, the Navigator or Orchestrator receives feedback and replans or continues, enabling self-correction. Core assumption: The Validator's judgment is accurate enough to provide useful corrective feedback. Evidence: Abstract mentions "self-verification with adaptive recovery," Section 3.5 states validation intercepts 15–20% of errors. Break condition: If the Validator is unreliable, its feedback could degrade performance or cause unnecessary replanning loops.

### Mechanism 3: Purely Visual Interaction with Specialized Localization
Operating on screenshots and using a specialized localization model enables a unified, cross-platform agent without environment-specific interfaces. The Navigator takes screenshots as input and outputs textual action descriptions. A Localizer model (Holo1.5) grounds these descriptions to pixel coordinates. This visual grounding works across web, desktop, and mobile without DOM or accessibility tree reliance. Core assumption: Vision-language models can provide sufficiently accurate spatial grounding for reliable GUI interaction. Evidence: Abstract mentions "unified architecture operating purely from visual observations," Section 3.4 describes the localization process. Break condition: Localization failures (5–8% per Section 6) on small or ambiguous UI elements can cascade into incorrect actions.

## Foundational Learning

- **Concept: Hierarchical Planning / Plan-and-Act**
  - Why needed: To understand how the Orchestrator decomposes tasks into subtasks
  - Quick check: Can you explain one benefit and one drawback of separating a planning module from an execution module?

- **Concept: ReAct (Reason+Act) Loop**
  - Why needed: The Navigator operates on this paradigm, producing `thought`, `note`, and `action` at each step
  - Quick check: What is the output at each step of a ReAct loop, and how does it differ from a single, monolithic action generation?

- **Concept: Visual Grounding**
  - Why needed: To understand the Localizer's role in converting textual descriptions to pixel coordinates
  - Quick check: Why is accurate visual grounding critical for an agent that interacts via mouse clicks?

## Architecture Onboarding

- **Component map:** User Task -> Orchestrator (Optional) -> Navigator -> Localizer -> GUI Actions
- **Critical path:** 1) User provides task 2) Simple Task: Navigator executes directly with Validator feedback 3) Complex Task: Orchestrator creates plan, delegates subtasks 4) Navigator executes subtask with Localizer grounding 5) Validator reviews outcome 6) Orchestrator updates plan or terminates
- **Design tradeoffs:** Orchestrator On/Off enables complex task handling but adds cost/latency; Model Selection uses different frontier models for cost-capability balance; Visual-only vs. DOM/a11y provides cross-platform generality but may be less robust; Test-time Scaling (pass@k) improves success rates but increases computational cost
- **Failure signatures:** Localization errors clicking wrong elements; Context overflow exceeding context windows; Validator unreliability causing premature termination; Stuck in exploration traps repeating actions
- **First 3 experiments:** 1) Ablate the Orchestrator: Run Surfer 2 on WebArena/OSWorld with Orchestrator disabled to measure contribution 2) Swap the Localizer: Substitute Holo1.5 72B with 7B and UI-TARS 7B on OSWorld to analyze performance change 3) Probe Validator Accuracy: Manually inspect Validator decisions on AndroidWorld to estimate false positive/negative rates

## Open Questions the Paper Calls Out

### Open Question 1
Can smaller, specialized models achieve Pareto-optimal cost-efficiency for GUI agents without sacrificing the performance of frontier models? The paper explicitly calls for a next-generation vision language model to achieve cost-efficiency, as current results rely on expensive frontier models costing $1–5 per complex task.

### Open Question 2
How can agent architectures effectively maintain reasoning coherence over task horizons significantly exceeding 50 steps? Section 6 identifies that long-horizon tasks face context window limits and compounding errors as primary constraints.

### Open Question 3
Can visual grounding error rates be reduced below the observed 5–8% floor for dynamic or ambiguous UI elements? Section 6 notes that even state-of-the-art localizers fail on 5–8% of UI elements due to dynamic content and ambiguous descriptions.

### Open Question 4
To what extent can agent performance be decoupled from prompt engineering sensitivity? Section 6 states that prompt engineering proves surprisingly impactful, with minor wording changes yielding 5–10% accuracy swings.

## Limitations

- Absence of task-specific fine-tuning leaves open questions about whether comparable performance could be achieved with simpler approaches
- Evaluation relies on vision-language models as both executors and judges, introducing inherent bias with 5-10% error rates in LLM-based evaluation
- Test-time scaling through multiple attempts raises significant concerns about practical deployment costs and real-world applicability
- Lack of systematic investigation into trade-offs between purely visual approaches versus structured interfaces

## Confidence

- **High Confidence:** Hierarchical separation between planning and execution is well-demonstrated and mechanistically sound, with consistent performance improvements across all benchmarks
- **Medium Confidence:** Specific performance numbers are credible but should be interpreted cautiously due to acknowledged limitations in LLM-based evaluation and prompt sensitivity
- **Low Confidence:** Claim of "state-of-the-art" performance requires additional context, as direct comparisons with identical evaluation protocols are not systematically presented

## Next Checks

1. **Prompt Sensitivity Analysis:** Systematically vary Orchestrator, Navigator, and Validator prompts using controlled perturbations to quantify actual impact of prompt engineering on performance across all four benchmarks

2. **Cross-Platform Capability Validation:** Test Surfer 2 on WebVoyager subset using DOM-based interaction alongside visual approach to validate whether purely visual architecture genuinely enables superior cross-platform generalization

3. **Cost-Performance Trade-off Analysis:** Measure computational cost (token usage, inference time) for each benchmark under pass@1 and pass@k configurations, calculating marginal cost per percentage point improvement to assess practical viability of test-time scaling