---
ver: rpa2
title: 'Artificial Intelligence-Driven Network-on-Chip Design Space Exploration: Neural
  Network Architectures for Design'
arxiv_id: '2512.07877'
source_url: https://arxiv.org/abs/2512.07877
tags:
- design
- performance
- parameter
- reverse
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of Network-on-Chip (NoC) design
  space exploration, where finding optimal configurations for performance targets
  is computationally expensive using traditional simulation-based methods. The authors
  propose an AI-driven framework that uses machine learning to predict NoC parameters
  (such as virtual channels, buffer size, injection rate, and packet size) directly
  from target latency and throughput specifications.
---

# Artificial Intelligence-Driven Network-on-Chip Design Space Exploration: Neural Network Architectures for Design

## Quick Facts
- arXiv ID: 2512.07877
- Source URL: https://arxiv.org/abs/2512.07877
- Authors: Amogh Anshu N; Harish BP
- Reference count: 19
- One-line primary result: Conditional Diffusion Model achieves lowest MSE (0.463) predicting NoC parameters from performance targets, outperforming MLP (1.412) and CVAE (5.144)

## Executive Summary
This paper addresses the computational expense of Network-on-Chip (NoC) design space exploration by proposing an AI-driven framework that directly predicts NoC configuration parameters from target latency and throughput specifications. The framework integrates BookSim simulations with three neural network architectures: Multi-Layer Perceptron (MLP), Conditional Variational Autoencoder (CVAE), and Conditional Diffusion Model. By reformulating the optimization as a supervised learning problem, the approach bypasses iterative forward simulations and achieves superior predictive accuracy, particularly with the diffusion model. The method demonstrates significant potential for accelerating NoC design exploration while handling the inherent many-to-one mapping between performance targets and valid configurations.

## Method Summary
The framework generates a comprehensive dataset of 150,000+ BookSim 2.0 simulations on 4×4 mesh topologies with DOR routing and uniform traffic, varying parameters including virtual channels (2-20), buffer size (2-10), injection rate (0.001-0.1), and packet size (2-10 flits). Data undergoes MinMax scaling and 80/20 train/validation splitting. Three neural architectures are implemented: MLP (2→64→64→4 with ReLU), CVAE (encoder 6→64→32→16, latent z∈R¹⁶, decoder 18→64→32→4 with β-annealing), and Conditional Diffusion (T=1000 timesteps, sinusoidal time embeddings, denoiser 128→128→output). Models predict parameters from target metrics, with BookSim simulation used for final performance evaluation. The Conditional Diffusion Model achieves highest accuracy (MSE 0.463) through iterative denoising conditioned on performance targets.

## Key Results
- Conditional Diffusion Model achieves lowest predictive error with MSE of 0.463 on unseen data
- MLP baseline shows reasonable performance (MSE 1.412) but limited by single-output constraint
- CVAE underperforms significantly (MSE 5.144), likely due to posterior collapse
- Diffusion model better handles the many-to-one mapping inherent in inverse NoC design problems
- Framework reduces design exploration time compared to traditional simulation-based methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Inverse prediction outperforms forward simulation for design exploration by directly mapping performance targets to configuration parameters.
- Mechanism: The framework reformulates NoC optimization as a supervised learning problem where f: P → X (performance to parameters), bypassing iterative forward simulations. The model learns from 150,000+ simulation pairs to predict configurations that minimize ||P - P'||.
- Core assumption: The simulation data sufficiently covers the design space and the mapping is learnable.
- Evidence anchors:
  - [abstract] "predict optimal NoC parameters given target performance metrics"
  - [section III.A] "we learn a mapping function f: P → X such that when X is used in a NoC simulation, the resulting performance P' minimizes the distance ||P - P'||"
  - [corpus] Related work (AC-Refiner) applies similar conditional diffusion to circuit optimization, suggesting pattern transferability; however, direct corpus evidence for NoC-specific inverse modeling is limited.
- Break condition: If target performance lies outside the training distribution (e.g., extreme latency requirements), predictions may be unreliable.

### Mechanism 2
- Claim: Generative models handle many-to-one mappings inherent in inverse design better than deterministic models.
- Mechanism: Multiple valid configurations can produce identical performance targets. MLP outputs a single prediction; CVAE and diffusion sample multiple candidates, selecting the best via simulation validation.
- Core assumption: Sampling diversity correlates with finding valid configurations.
- Evidence anchors:
  - [section IV.C] "Generative models (CVAE and diffusion) generate 10 samples per target, and the best-performing configuration is selected based on simulation error"
  - [table III] Diffusion achieves MSE 0.463 vs MLP's 1.412 despite higher training loss, validating sampling strategy
  - [corpus] Weak corpus support; neighbor papers don't explicitly analyze many-to-one mapping in generative design contexts.
- Break condition: If the generative model collapses to limited modes (posterior collapse in CVAE), diversity degrades.

### Mechanism 3
- Claim: Iterative denoising with performance conditioning enables more accurate navigation of high-dimensional parameter spaces.
- Mechanism: The diffusion model progressively refines noisy parameter vectors over T=1000 timesteps, conditioned on latency/throughput targets. Each denoising step corrects accumulated errors, yielding better final configurations.
- Core assumption: The noise schedule and conditioning mechanism preserve solution validity through denoising.
- Evidence anchors:
  - [section III.C.3] "A neural network learns to predict and remove noise conditioned on performance targets" with sinusoidal time embeddings
  - [section IV.D] "Diffusion balances diversity and accuracy, enabling it to better handle the many-to-one nature of the reverse mapping"
  - [corpus] AC-Refiner and diffusion autoencoder papers suggest iterative denoising transfers across hardware design domains.
- Break condition: If discrete parameters (num_vcs, buffer sizes) are rounded incorrectly during denoising, invalid configurations emerge.

## Foundational Learning

- Concept: Network-on-Chip (NoC) parameters
  - Why needed here: The framework predicts four specific parameters—virtual channel count, buffer size, injection rate, packet size—from performance targets. Understanding their interactions is essential.
  - Quick check question: Which parameter most directly affects throughput vs. latency?

- Concept: Inverse problems vs. forward simulation
  - Why needed here: Traditional NoC design simulates forward (parameters → performance). This work inverts that mapping, which introduces many-to-one degeneracy.
  - Quick check question: Why might multiple configurations yield identical latency/throughput?

- Concept: Conditional generative models (CVAE, Diffusion)
  - Why needed here: These architectures condition generation on target metrics, enabling diverse, valid outputs for the same condition.
  - Quick check question: How does conditioning differ between CVAE (latent space) and Diffusion (timestep embeddings)?

## Architecture Onboarding

- Component map:
  Configuration Generator → BookSim Simulator (parallel via joblib) → Output Parser → Data Pipeline (MinMax scaling, 80/20 split) → Neural Model (MLP/CVAE/Diffusion) → Post-hoc BookSim validation

- Critical path:
  Data quality and coverage (150K samples across mesh topologies) → Model selection (diffusion preferred for inverse tasks) → Post-processing (quantization of discrete params) → Simulation-based evaluation

- Design tradeoffs:
  - MLP: Fast inference, single output, poor for many-to-one mappings
  - CVAE: Probabilistic, but prone to posterior collapse and higher error variance
  - Diffusion: Highest accuracy (MSE 0.463), slower inference due to iterative denoising, best for diverse valid outputs

- Failure signatures:
  - Training/validation loss low but BookSim evaluation high → model overfitting to parameter values, not performance
  - CVAE generating similar outputs → latent space collapse, adjust KL weight β
  - Diffusion producing invalid discrete parameters → quantization/clamping errors

- First 3 experiments:
  1. Reproduce the 150K simulation dataset on a smaller subset (e.g., 4×4 mesh, uniform traffic) to validate data pipeline.
  2. Train MLP baseline to establish forward/inverse mapping baseline; verify BookSim evaluation gap.
  3. Implement conditional diffusion with reduced timesteps (T=100 vs. T=1000) to assess accuracy/speed tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a differentiable surrogate model or "BookSim-in-the-loop" approach effectively integrate performance metrics directly into the training objective?
- Basis in paper: [explicit] The authors identify a mismatch where models are trained on parameter MSE but evaluated on simulation performance. They state future work could "incorporate latency and throughput optimization directly into the training loop" using surrogates.
- Why unresolved: The current reliance on standard MSE loss creates a disconnect between the optimization target and the actual design goal (latency/throughput), but BookSim itself is non-differentiable.
- What evidence would resolve it: Demonstration of an end-to-end trained model where gradients are derived from a differentiable surrogate, resulting in lower simulation error than the current post-hoc approach.

### Open Question 2
- Question: Can the proposed diffusion framework be extended to holistically optimize for power, area, and thermal constraints alongside latency and throughput?
- Basis in paper: [explicit] The authors list the exclusion of "power, area, and thermal constraints" as a specific limitation that restricts the framework's applicability in holistic NoC co-design.
- Why unresolved: The current dataset and models were constructed exclusively around latency and throughput targets, ignoring other critical physical design metrics.
- What evidence would resolve it: A multi-objective generative model that successfully predicts configurations satisfying specified power and area budgets while maintaining target throughput.

### Open Question 3
- Question: What architectural modifications are required to stabilize the Conditional VAE (CVAE) and prevent posterior collapse in the context of NoC parameter generation?
- Basis in paper: [explicit] The authors note the CVAE underperformed significantly and speculate this was "likely due to posterior collapse or insufficient latent conditioning, warranting future investigation."
- Why unresolved: The standard CVAE architecture failed to effectively model the conditional distribution, resulting in high error variance compared to the diffusion model.
- What evidence would resolve it: A modified CVAE architecture that achieves competitive simulation accuracy (MSE) with the diffusion model, demonstrating stable latent space utilization.

## Limitations

- The dataset focuses exclusively on 4×4 mesh topologies with uniform traffic, limiting generalization to other topologies and traffic patterns
- Current framework optimizes only for latency and throughput, ignoring power, area, and thermal constraints critical for holistic NoC design
- The many-to-one nature of inverse problems means multiple valid configurations may exist, but the framework doesn't guarantee finding all optimal solutions

## Confidence

- High confidence in the core mechanism: inverse design via conditional diffusion is theoretically sound and supported by corpus evidence (AC-Refiner, diffusion autoencoders)
- Medium confidence in empirical results: MSE values are reported, but without confidence intervals or variance across runs. The superiority of diffusion over CVAE/MLP is plausible but requires reproduction
- Low confidence in dataset coverage: 150K simulations on 4×4 mesh with uniform traffic may not capture extreme or corner-case performance targets

## Next Checks

1. Reproduce a small subset of the 150K simulation dataset (e.g., 1,000 samples) and verify BookSim output consistency
2. Implement the conditional diffusion model with T=100 timesteps first, then scale to T=1000, comparing accuracy and inference speed
3. Evaluate model predictions on held-out extreme targets (e.g., latency < 5 cycles) to test generalization beyond training distribution