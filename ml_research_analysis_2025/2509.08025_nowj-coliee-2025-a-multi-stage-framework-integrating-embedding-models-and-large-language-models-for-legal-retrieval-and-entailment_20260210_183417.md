---
ver: rpa2
title: 'NOWJ@COLIEE 2025: A Multi-stage Framework Integrating Embedding Models and
  Large Language Models for Legal Retrieval and Entailment'
arxiv_id: '2509.08025'
source_url: https://arxiv.org/abs/2509.08025
tags:
- legal
- case
- task
- coliee
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The NOWJ team participated in all five tasks of COLIEE 2025, focusing
  on legal information processing. They proposed multi-stage frameworks integrating
  traditional information retrieval techniques (BM25, BERT, monoT5) with advanced
  embedding models (BGE-m3, LLM2Vec) and large language models (Qwen-2, QwQ-32B, DeepSeek-V3).
---

# NOWJ@COLIEE 2025: A Multi-stage Framework Integrating Embedding Models and Large Language Models for Legal Retrieval and Entailment

## Quick Facts
- arXiv ID: 2509.08025
- Source URL: https://arxiv.org/abs/2509.08025
- Reference count: 24
- The NOWJ team achieved first place in COLIEE 2025 Task 2 (Legal Case Entailment) with an F1 score of 0.3195

## Executive Summary
The NOWJ team participated in all five tasks of COLIEE 2025, developing multi-stage frameworks that integrate traditional information retrieval techniques with advanced embedding models and large language models. Their approach combines BM25 lexical retrieval, fine-tuned neural re-rankers, and LLM-based verification to address various legal information processing challenges. The team's best performance was first place in Task 2 (Legal Case Entailment), demonstrating the effectiveness of their hybrid approach that leverages both classical IR methods and contemporary generative models for legal text processing.

## Method Summary
The framework employs a three-stage pipeline: initial lexical pre-ranking using BM25 to generate candidate paragraphs, semantic re-ranking using fine-tuned models (mBERT, monoT5-base, monoT5-3B) with cross-entropy loss, and LLM-based verification using QwQ-32B and DeepSeek-V3 with voting consensus. For tasks requiring case summarization, Qwen-2.5-14B compresses lengthy legal cases while preserving key facts. Ensemble methods combine multiple retrieval models through weighted combinations or majority voting to improve robustness. The approach varies by task, with some using summarization preprocessing and others employing direct multi-model retrieval.

## Key Results
- First place in Task 2 (Legal Case Entailment) with F1 score of 0.3195
- Third place rankings in Tasks 3 and 5
- Task 2 best run: BM25 → mBERT/monoT5 fine-tuning → ensemble → LLM voting (top-35 candidates)
- Summarization preprocessing improved Task 1 recall@200 from 0.7387 to 0.7784
- Grid-search ensemble of 3 models outperformed LightGBM with all models (F2: 0.7702 vs 0.7311) in Task 3

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage lexical-semantic filtering followed by LLM contextual analysis improves legal entailment precision.
- Mechanism: BM25 retrieves high-lexical-overlap candidates; fine-tuned monoT5/mBERT re-ranks using semantic similarity; LLMs (QwQ-32B, DeepSeek-V3) evaluate inter-paragraph relationships via joint prompting to filter false positives.
- Core assumption: LLMs can distinguish topical similarity from entailment when processing multiple candidates together.
- Evidence anchors:
  - [abstract] "two-stage retrieval system combined lexical-semantic filtering with contextualized LLM analysis, achieving first place with an F1 score of 0.3195"
  - [section 3.2] "rather than evaluating each (q, pi) pair separately, we construct a single prompt that includes the decision q and all k candidate paragraphs"
  - [corpus] Limited external validation; neighboring papers focus on retrieval, not entailment pipelines.
- Break condition: If top-k candidates lack the entailing paragraph, LLM refinement cannot recover it.

### Mechanism 2
- Claim: Majority voting across diverse retrieval models improves robustness over single-model approaches.
- Mechanism: Combine outputs from pre-ranking (BGE-m3), fine-tuned re-rankers (BGE-m3-ft, LLM2Vec) via voting to aggregate complementary retrieval signals.
- Core assumption: Different models make independent errors; aggregation reduces variance.
- Evidence anchors:
  - [section 2.4] "the ensemble run yields the best results across all metrics, with a particularly notable improvement in recall"
  - [section 4.4] "Run 2 achieved the highest efficacy F2... leveraging a strategy centered on the linear combination of scores from only three top-performing base models"
  - [corpus] UQLegalAI@COLIEE2025 similarly uses ensemble methods for legal retrieval (FMR=0.505).
- Break condition: If all models share systematic bias (e.g., lexical over-reliance), voting provides limited benefit.

### Mechanism 3
- Claim: LLM-based summarization preserves key legal facts for improved retrieval representation.
- Mechanism: Qwen-2.5-14B compresses 4K-10K word cases into structured summaries (Introduction, Facts, Relevant provisions, Analyses, Conclusion) within 131K token context window.
- Core assumption: Summary retains legally salient information; compression noise is tolerable.
- Evidence anchors:
  - [section 2.2] "By leveraging the in-context learning ability of LLMs, we compressed legal cases while preserving key facts"
  - [section 2.4, Table 1] Summarization improves recall at R@200 from 0.7387 to 0.7784
  - [corpus] Segment First, Retrieve Better paper uses rhetorical role segmentation for legal search (FMR=0.512).
- Break condition: If critical entailment signals reside in discarded details, retrieval degrades.

## Foundational Learning

- Concept: BM25 lexical retrieval
  - Why needed here: First-stage filtering for all tasks; enables high-recall candidate selection before expensive neural re-ranking.
  - Quick check question: Can you explain why BM25 provides better recall than dense retrieval as an initial filter?

- Concept: Cross-encoders vs. bi-encoders
  - Why needed here: Task 3 ensembles both architectures; cross-encoders provide precision, bi-encoders enable scalable similarity computation.
  - Quick check question: When would you choose a bi-encoder over a cross-encoder for a retrieval pipeline?

- Concept: InfoNCE contrastive loss
  - Why needed here: Training objective for BGE-m3 fine-tuning (Equation 2); distinguishes positive/easy-negative/hard-negative samples.
  - Quick check question: How do hard negatives differ from random negatives in contrastive learning?

## Architecture Onboarding

- Component map:
  - Task 2 (first place): BM25 → (mBERT, monoT5-base, monoT5-3B) → weighted ensemble → (QwQ-32B, DeepSeek-V3) → voting
  - Task 1: Preprocessing → Qwen-2.5 summarization → BGE-m3 pre-ranking → (BGE-m3-ft, LLM2Vec) → majority voting
  - Task 3: Bi-encoders (BGE, E5, Stella, NV-Embed) + Cross-encoders (BGE-reranker, GTE-reranker) → LightGBM/grid-search ensemble

- Critical path: Candidate generation (recall-critical) → neural re-ranking (precision-critical) → LLM verification (false-positive reduction). Failures in early stages cascade.

- Design tradeoffs:
  - Top-k selection: Higher k (35 vs. 20) improves recall but increases LLM inference cost and noise
  - Ensemble complexity: LightGBM with all models underperformed grid-search with 3 top models (F2: 0.7311 vs. 0.7702), suggesting noise from weak models
  - LLM consensus: Two-model voting improved precision but may reject valid entailing paragraphs

- Failure signatures:
  - Task 4: All three runs achieved identical accuracy (0.7397), indicating LLM prediction variance was suppressed—suggests prompt engineering or model diversity insufficient
  - Task 5 clustering approach: 55.9% F1 vs. 69.2% for hierarchical model—semantic clustering oversimplified legal argument structure

- First 3 experiments:
  1. Replicate Task 2 three-stage pipeline with top-20 BM25 candidates; measure precision/recall tradeoffs at each stage
  2. Ablate summarization step on Task 1; compare recall@200 with vs. without compression
  3. Test ensemble strategy on Task 3: start with grid-search weighted combination of 3 models, then add LightGBM to identify noise sources from weaker models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What data augmentation and fine-tuning strategies can reduce inconsistency in LLM-based legal textual entailment predictions?
- Basis in paper: [explicit] "Future work should focus on enhancing the performance and robustness of LLM-based methods through data augmentation and fine-tuning, particularly in handling complex legal reasoning and reducing inconsistencies in model predictions."
- Why unresolved: All three submitted runs for Task 4 achieved identical accuracy (0.7397), and few-shot prompting was found "inconsistent and unreliable" despite LLMs' strong in-context learning abilities.
- What evidence would resolve it: Comparative experiments showing that specific augmentation or fine-tuning methods reduce variance in LLM predictions across multiple runs on legal entailment benchmarks.

### Open Question 2
- Question: How can semantic clustering approaches balance explainability with performance in legal judgment prediction?
- Basis in paper: [inferred] The clustering-based approach underperformed significantly (59.7% TP accuracy) compared to the hierarchical model with heuristics (67.1%), yet the authors explicitly note clustering "provides explainability" while "structural dependencies" are important.
- Why unresolved: The 7.4 percentage point gap suggests current clustering methods oversimplify complex argument relationships, but abandoning clustering sacrifices interpretability.
- What evidence would resolve it: Development of structured clustering methods that preserve inter-claim dependencies while maintaining explainability, demonstrated through improved F1 scores on rationale extraction.

### Open Question 3
- Question: What techniques can reliably quantify query similarity for dynamic ensemble weighting in legal retrieval?
- Basis in paper: [explicit] The similarity-informed voting ensemble underperformed (F2=0.7069), with the authors noting "difficulties in precisely quantifying query similarity or in effectively leveraging historical performance data."
- Why unresolved: Simple bi-encoder similarity failed to capture task-relevant legal query characteristics needed for intelligent model selection.
- What evidence would resolve it: A retrieval benchmark showing that a proposed similarity metric correlates with per-model performance and enables dynamic weighting that outperforms static grid-search ensembles.

## Limitations

- Limited ablation studies prevent isolating individual component contributions to overall performance
- Voting mechanism for LLM outputs may mask systematic errors rather than reducing them
- Performance variability across tasks (55.9% F1 on Task 5 clustering vs. 69.2% for baseline) suggests limited generalizability

## Confidence

- High confidence: Task 2 first-place result (0.3195 F1) and the basic effectiveness of ensemble retrieval models
- Medium confidence: The specific mechanisms of LLM-based entailment verification and the claimed benefits of summarization for retrieval
- Low confidence: Generalization of the multi-stage framework across all five COLIEE tasks given the variable performance

## Next Checks

1. Conduct ablation study on Task 2 pipeline: measure F1 when removing LLM stage, varying k in top-k selection, and testing different voting thresholds
2. Cross-domain validation: apply the same framework to a non-COLIEE legal dataset (e.g., European Court of Human Rights cases) to test generalizability
3. Error analysis of LLM outputs: categorize false positives/negatives to determine whether voting consensus suppresses valid entailments or genuinely reduces errors