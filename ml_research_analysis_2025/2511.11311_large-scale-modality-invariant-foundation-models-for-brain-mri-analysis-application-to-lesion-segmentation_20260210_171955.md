---
ver: rpa2
title: 'Large-scale modality-invariant foundation models for brain MRI analysis: Application
  to lesion segmentation'
arxiv_id: '2511.11311'
source_url: https://arxiv.org/abs/2511.11311
tags:
- segmentation
- pre-training
- lesion
- learning
- brain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a modality-invariant self-supervised learning
  framework for brain MRI analysis, integrating contrastive learning with masked image
  modeling to capture anatomical priors across multiple imaging modalities. Despite
  successfully aligning cross-modality embeddings during pre-training, lesion segmentation
  performance remained primarily driven by modality-specific features rather than
  modality-invariant representations.
---

# Large-scale modality-invariant foundation models for brain MRI analysis: Application to lesion segmentation

## Quick Facts
- **arXiv ID:** 2511.11311
- **Source URL:** https://arxiv.org/abs/2511.11311
- **Reference count:** 13
- **Primary result:** Modality-invariant pre-training did not improve lesion segmentation performance compared to modality-specific approaches

## Executive Summary
This study introduces a modality-invariant self-supervised learning framework for brain MRI analysis, integrating contrastive learning with masked image modeling to capture anatomical priors across multiple imaging modalities. Despite successfully aligning cross-modality embeddings during pre-training, lesion segmentation performance remained primarily driven by modality-specific features rather than modality-invariant representations. The findings suggest that accurate lesion segmentation depends on fine-grained, modality-specific tissue contrasts, indicating that modality invariance may be more beneficial for global classification or regression tasks rather than segmentation.

## Method Summary
The framework combines modality-invariant contrastive learning (MCL) with masked image modeling (MIM) using a hybrid Swin Transformer encoder. Pre-training uses 60,529 scans from 11,187 subjects across 16 sources, with positive pairs defined as co-localized 3D patches from same-anatomical regions across different modalities. Downstream evaluation uses lesion segmentation on ATLAS v2 (stroke), ISLES 2022 (stroke), and FCD BONN (epilepsy) datasets. The approach aims to learn subject-specific anatomical representations invariant to imaging sequence while maintaining localization capabilities for dense prediction tasks.

## Key Results
- Modality-invariant pre-training achieved successful cross-modality alignment (cosine distance <0.01) but showed no improvement in lesion segmentation DSC scores
- Segmentation performance remained modality-specific: DWI best for stroke, FLAIR best for FCD, regardless of pre-training strategy
- CL + MIM achieved highest segmentation performance (DSC 0.4936 at 100% training data), outperforming MCL + MIM (0.4782)
- The study confirms that modality-specific tissue contrasts are critical for lesion detection, while modality-invariant features may emphasize global anatomical structure

## Why This Works (Mechanism)

### Mechanism 1
Cross-modality contrastive alignment successfully unifies embeddings across MRI modalities but does not transfer to improved lesion segmentation. MCL defines positive pairs as co-localized 3D patches from the same anatomical region across different modalities within a single acquisition session, while treating patches from different subjects as negatives regardless of modality. This forces the encoder to learn subject-specific anatomical representations that are invariant to imaging sequence. Core assumption: Subject-specific anatomical structure persists across modalities and can be disentangled from modality-specific tissue contrast information. Evidence: Cross-modality alignment measured via cosine distance showed MCL: 0.0066±0.0104 compared to default CL: 0.5995±0.1576.

### Mechanism 2
Masked Image Modeling (MIM) provides stronger localization priors than contrastive learning for dense prediction tasks like segmentation. Random patch masking at the bottleneck (ratio 0.5-0.75) with reconstruction objectives forces the encoder to retain fine-grained spatial information necessary to predict masked regions. Unlike contrastive learning which optimizes for global embedding similarity, MIM requires voxel-level feature fidelity. Core assumption: Reconstruction of masked brain regions requires learning local tissue patterns and spatial relationships rather than just global anatomical structure. Evidence: CL + MIM achieves highest DSC (0.4936 at 100% training data) compared to CL alone (0.4678) or MCL alone (0.4693).

### Mechanism 3
Combining MCL with MIM can degrade performance because the invariance objective removes texture features necessary for reconstruction. MCL's contrastive loss dominates optimization through higher gradient norms, while simultaneously eliminating modality-specific texture that the MIM decoder needs to reconstruct. The two objectives operate at cross-purposes when fine-grained tissue contrast encodes pathology. Core assumption: Gradient magnitudes from contrastive objectives exceed those from reconstruction objectives under the tested hyperparameters (τ = 0.2). Evidence: MCL + MIM (0.4782) underperforms CL + MIM (0.4936) on ATLAS v2 at 100% training data.

## Foundational Learning

- **Concept: Self-Supervised Learning (SSL) Pre-training**
  - Why needed here: The paper builds on the paradigm that large-scale unlabeled data can provide anatomical priors before task-specific fine-tuning. Without understanding SSL, the distinction between "from scratch" baselines and pre-trained models would be unclear.
  - Quick check question: Can you explain why a model trained on 60k unlabeled scans might generalize better than one trained only on 400 labeled examples?

- **Concept: Contrastive Learning (CL) vs. Masked Image Modeling (MIM)**
  - Why needed here: The core contribution involves comparing these two SSL paradigms and their combination. The paper's negative result depends on understanding that CL optimizes for embedding-level discrimination while MIM optimizes for pixel/voxel-level reconstruction.
  - Quick check question: Given a brain MRI patch, what would a contrastive loss encourage vs. what would a reconstruction loss encourage?

- **Concept: Modality Invariance in Medical Imaging**
  - Why needed here: The central hypothesis (that cross-modality representations could improve segmentation) and its failure mode (that lesion detection requires modality-specific contrasts) require understanding why one might want invariance and what information it sacrifices.
  - Quick check question: If you trained a model to produce the same embedding for T1, T2, and FLAIR images of the same patient, what information would necessarily be discarded?

## Architecture Onboarding

- **Component map:** FOMO60k pre-training data -> Hybrid Swin UNetR-v2 encoder -> MoCo v2 contrastive queue (16,384) + MIM bottleneck masking (0.5-0.75) -> FPN decoder -> ATLAS v2/ISLES 2022/FCD BONN segmentation tasks

- **Critical path:** 1) Data preprocessing: skull-stripping (HD-BET) → 1mm³ resampling → RAS reorientation → intensity clipping (0.5-99.5%) → z-score normalization; 2) Pre-training: 200k steps with AdamW (lr=1e-4, weight decay=0.01, batch size=16); 3) Fine-tuning: 5k steps, encoder frozen for first 30%, FPN decoder trained from scratch

- **Design tradeoffs:** MCL alone: Good cross-modality alignment but lacks localization priors; CL + MIM: Best segmentation performance but no cross-modality transfer; MCL + MIM: Intended to combine benefits, but gradient dominance and texture loss cause underperformance; From scratch: No pre-training overhead but requires more labeled data

- **Failure signatures:** Embedding collapse: Monitored via cross-subject separability (maintained at ~0.6 cosine distance); Negative transfer: Pre-trained models showing no improvement over from-scratch baselines on specific modalities; Modality coupling: Performance varying by modality regardless of pre-training strategy (DWI best for stroke, FLAIR for FCD)

- **First 3 experiments:** 1) Baseline comparison: Train from scratch vs. CL vs. MCL vs. CL+MIM vs. MCL+MIM on ATLAS v2 (T1 only), measuring DSC at 100%/75%/50%/25% training data fractions to assess sample efficiency; 2) Cross-modality alignment validation: Compute mean cosine distance between embeddings from different modalities of same subject on held-out FOMO60k validation set, confirming MCL achieves <0.01 distance vs. >0.5 for standard CL; 3) Multi-modal downstream evaluation: Test all pre-training variants on ISLES 2022 (DWI/ADC/FLAIR) and FCD BONN (FLAIR/T1) to determine whether modality-invariant pre-training reduces dependency on optimal imaging sequences

## Open Questions the Paper Calls Out

- Would modality-invariant pre-training improve performance on global classification or regression tasks (e.g., brain age prediction) compared to segmentation tasks? Future work should investigate the role of modality invariance in global classification or regression tasks, where such representations may prove more beneficial.

- Would alternative reconstruction objectives (e.g., iBOT with token-level modality-agnostic targets and higher InfoNCE temperature) improve segmentation while preserving modality invariance? The use of a higher τ value in InfoNCE, combined with a token-level modality-agnostic reconstruction objective, such as iBOT, could circumvent these limitations and surpass the default baseline.

- Does the observed coupling between imaging modality and segmentation performance reflect intrinsic tissue contrast properties or annotation bias from single-modality ground truth creation? The paper notes best-performing modalities matched clinical diagnostic standards but acknowledges ground truth annotations being created on the same modalities as a potential confound.

## Limitations

- The negative finding that modality-invariant pre-training does not improve lesion segmentation represents a fundamental limitation of the approach for this specific task, suggesting that the current formulation of modality invariance may be misaligned with lesion detection requirements.

- The study focuses specifically on lesion segmentation where modality-specific contrasts are clinically relevant, and does not explore whether modality invariance might be beneficial for other types of medical imaging tasks.

- The paper does not explore alternative invariance definitions (e.g., pathology-invariant representations) that might better serve segmentation objectives, focusing only on subject-anatomy invariance.

## Confidence

- **High confidence:** Cross-modality alignment metrics (cosine distances), segmentation DSC measurements, and the core finding that modality-invariant pre-training does not improve lesion segmentation
- **Medium confidence:** The mechanism explanation for why modality invariance fails (gradient dominance and texture loss in MCL+MIM), as this involves complex optimization dynamics without extensive ablation studies
- **Medium confidence:** The generalizability of findings to other segmentation tasks, as the study focuses specifically on lesion segmentation where modality-specific contrasts are clinically relevant

## Next Checks

1. **Gradient contribution analysis:** Quantify and compare gradient norms from MCL vs MIM branches during combined training to verify the claimed gradient dominance effect, and test whether gradient clipping or branch-specific learning rates can rescue the combined approach.

2. **Cross-task validation:** Evaluate modality-invariant pre-training on non-lesion segmentation tasks (e.g., organ segmentation, brain parcellation) to determine whether the modality invariance limitation is specific to lesion detection or represents a broader constraint on the approach.

3. **Alternative invariance formulations:** Test whether pathology-specific invariance (e.g., stroke-lesion invariant features rather than subject-anatomy invariant features) improves segmentation performance compared to the current subject-anatomy invariance objective.