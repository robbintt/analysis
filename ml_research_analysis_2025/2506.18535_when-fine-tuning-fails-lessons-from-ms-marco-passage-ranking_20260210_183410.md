---
ver: rpa2
title: 'When Fine-Tuning Fails: Lessons from MS MARCO Passage Ranking'
arxiv_id: '2506.18535'
source_url: https://arxiv.org/abs/2506.18535
tags:
- fine-tuning
- training
- base
- hard
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper demonstrates that fine-tuning pre-trained models on
  the MS MARCO passage ranking task consistently degrades performance rather than
  improving it. Through experiments with five model variants including full fine-tuning
  and LoRA adaptations, all approaches underperformed the base sentence-transformers/all-MiniLM-L6-v2
  model (MRR@10: 0.3026), with degradations ranging from 13.5% to 32.3%.'
---

# When Fine-Tuning Fails: Lessons from MS MARCO Passage Ranking

## Quick Facts
- **arXiv ID**: 2506.18535
- **Source URL**: https://arxiv.org/abs/2506.18535
- **Reference count**: 14
- **Primary result**: Fine-tuning pre-trained models on MS MARCO passage ranking consistently degrades performance, with all variants underperforming the base model (MRR@10: 0.3026)

## Executive Summary
This paper demonstrates that fine-tuning pre-trained models on the MS MARCO passage ranking task consistently degrades performance rather than improving it. Through experiments with five model variants including full fine-tuning and LoRA adaptations, all approaches underperformed the base sentence-transformers/all-MiniLM-L6-v2 model. The authors attribute these failures to the base model's extensive pre-training on 1 billion sentence pairs including 9.1 million MS MARCO samples, creating a saturated benchmark where additional fine-tuning introduces harmful noise rather than beneficial signal.

## Method Summary
The study evaluates five model variants on MS MARCO passage ranking: Base model (sentence-transformers/all-MiniLM-L6-v2), Full Fine-Tuning with Random/Hard negatives, and LoRA Fine-Tuning with Random/Hard negatives. All models use triplet loss with margin 0.2, AdamW optimizer (lr=2e-5), and 128 batch size. Random triplets sample 1M examples from training triples, while hard negatives retrieve top-200 candidates per query and sample from ranks 51-200. LoRA uses rank 16, α=32 targeting attention Q/V projections (~3.9% parameters). Performance is measured via MRR@10/100 on dev set, with UMAP visualizations analyzing embedding space structure degradation.

## Key Results
- All five fine-tuned variants underperformed the base model, with degradations ranging from 13.5% to 32.3% in MRR@10
- Full Fine-Tuning Hard achieved lowest performance (0.2074 MRR@10, 32.3% degradation)
- UMAP visualizations revealed progressive flattening of embedding space structure across fine-tuning variants
- Hard negative sampling paradoxically harmed performance rather than improving it
- LoRA with hard negatives showed catastrophic degradation (32%+ drop) compared to other variants

## Why This Works (Mechanism)
Fine-tuning degrades because the base model is already saturated through extensive pre-training on MS MARCO data (9.1M samples from 1B total). Additional fine-tuning on this saturated benchmark introduces harmful noise that progressively flattens the embedding space structure, destroying the beneficial pre-trained representations. Hard negative sampling exacerbates this by pulling semantically similar but non-relevant passages too close in the embedding space, creating conflicts with the base model's established relationships.

## Foundational Learning
- **MS MARCO Passage Ranking**: Task of retrieving and ranking relevant passages for queries from 8.8M passages corpus. Why needed: Understanding the specific benchmark where fine-tuning fails.
- **Triplet Loss**: Margin-based ranking loss optimizing distance between anchor-positive and anchor-negative pairs. Why needed: The loss function driving all fine-tuning experiments.
- **UMAP Visualization**: Dimensionality reduction technique for visualizing high-dimensional embedding spaces. Why needed: Primary method for observing embedding space degradation.
- **LoRA (Low-Rank Adaptation)**: Parameter-efficient fine-tuning method updating small trainable matrices within pre-trained layers. Why needed: One of the fine-tuning approaches tested.
- **Hard Negative Mining**: Strategy selecting negatives that are semantically similar but not relevant. Why needed: Central to the paradoxical finding that harder negatives harm performance.
- **Model Saturation**: State where pre-training has already captured most useful signal from target domain. Why needed: Core explanation for why fine-tuning fails.

## Architecture Onboarding

**Component Map**: MS MARCO Collection (8.8M passages) -> Triplet Dataset (Random/Hard) -> Base Model (all-MiniLM-L6-v2) -> Fine-Tuning Variants (Full/LoRA) -> Triplet Loss -> UMAP Visualization -> Performance Metrics

**Critical Path**: Base model → Triplet dataset construction → Fine-tuning training → UMAP visualization → MRR@10/100 evaluation

**Design Tradeoffs**: Using pre-saturated base model trades transfer learning benefits for degradation risk; hard negative mining trades potential precision gains for catastrophic performance loss; LoRA trades parameter efficiency for greater degradation.

**Failure Signatures**: Fine-tuned models underperforming base by 13-32%; UMAP showing progressive embedding space flattening; hard negatives causing worse performance than random negatives.

**3 First Experiments**:
1. Verify base model MRR@10 matches 0.3026 without modification
2. Test whether removing MS MARCO samples from pre-training data allows fine-tuning to improve performance
3. Compare triplet loss to MultipleNegativesRankingLoss to assess if alternative losses mitigate degradation

## Open Questions the Paper Calls Out
- **Generalization**: Do these fine-tuning failure patterns generalize beyond MS MARCO to other retrieval tasks or less saturated domains? Results may not extend to benchmarks where base models had limited pre-training exposure.
- **Loss Function Alternatives**: Would alternative loss functions (MultipleNegativesRankingLoss, ContrastiveLoss) mitigate fine-tuning degradation on saturated models? The paper exclusively used triplet loss.
- **Cross-Encoder Comparisons**: Do cross-encoder architectures exhibit similar degradation when fine-tuned on saturated benchmarks? The study focused only on dual-encoder models.
- **Regularization Techniques**: Can regularization techniques preserve beneficial pre-trained structure during fine-tuning of saturated models? No regularization methods were explored.

## Limitations
- Single dataset focus limits generalizability to other retrieval tasks or less saturated domains
- Exclusive reliance on triplet loss represents a significant methodological limitation
- Focus on MiniLM-based models excludes comparisons with other model families
- No exploration of regularization techniques to preserve pre-trained structure

## Confidence
- **High**: Performance degradation observations across all fine-tuning variants; base model superiority with MRR@10=0.3026
- **Medium**: Attribution of failures to benchmark saturation rather than optimization issues; universality of findings beyond MiniLM models
- **Low**: Claims about fundamental limitations of fine-tuning on saturated benchmarks without exploring alternative approaches

## Next Checks
1. Verify whether the same degradation pattern appears with alternative base models (e.g., Contriever, ANCE) that were not pre-trained on MS MARCO data
2. Test whether increased regularization strength, learning rate scheduling, or alternative loss formulations can prevent embedding space flattening while maintaining or improving performance
3. Conduct ablation studies removing the MS MARCO samples from the base model's pre-training data to quantify their contribution to saturation and evaluate whether fine-tuning then provides meaningful improvements