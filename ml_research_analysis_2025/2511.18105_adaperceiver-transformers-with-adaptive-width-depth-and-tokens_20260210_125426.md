---
ver: rpa2
title: 'AdaPerceiver: Transformers with Adaptive Width, Depth, and Tokens'
arxiv_id: '2511.18105'
source_url: https://arxiv.org/abs/2511.18105
tags:
- tokens
- depth
- adaperceiver
- token
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AdaPerceiver introduces a unified transformer architecture with
  adaptivity across depth, width, and tokens. It enables a single model to adjust
  its computational footprint at inference time, supporting trade-offs between accuracy
  and efficiency.
---

# AdaPerceiver: Transformers with Adaptive Width, Depth, and Tokens

## Quick Facts
- arXiv ID: 2511.18105
- Source URL: https://arxiv.org/abs/2511.18105
- Authors: Purvish Jajal; Nick John Eliopoulos; Benjamin Shiue-Hal Chou; George K. Thiruvathukal; Yung-Hsiang Lu; James C. Davis
- Reference count: 40
- Primary result: Unified transformer achieving 85.4% ImageNet-1K accuracy with 36% higher throughput than FlexiViT-L while supporting runtime adaptation across tokens, depth, and width

## Executive Summary
AdaPerceiver introduces a transformer architecture with adaptive width, depth, and token counts, enabling a single model to adjust its computational footprint at inference time. Built on the Perceiver paradigm, it employs block-masked attention, Matryoshka FFNs, and early-exiting trained jointly via a once-for-all regime. This allows smooth trade-offs between accuracy and efficiency without requiring separate models or multiple forward passes.

The model achieves 85.4% top-1 accuracy on ImageNet-1K with 36% higher throughput than FlexiViT-L, and maintains competitive performance on dense tasks (semantic segmentation and depth estimation) with ~26× fewer encoder FLOPs. With learned policies, it can reduce FLOPs by 24-33% while preserving accuracy within ±0.1 percentage points.

## Method Summary
AdaPerceiver extends the PerceiverIO architecture with three axes of adaptivity: tokens (input sequence length), depth (number of layers), and width (embedding dimension). The key innovation is a unified training regime that enables all configurations to be optimized jointly in a single forward pass. Block-masked attention constrains token groups to attend only within and to earlier groups, enabling efficient multi-granularity training. Matryoshka FFNs allow width adaptation through per-sample slicing, and early-exiting is implemented via intermediate decoder readouts. The model is trained in three curriculum stages: first token adaptivity, then token+depth, and finally all three axes with distillation from a ViT-H/14 teacher.

## Key Results
- Achieves 85.4% ImageNet-1K accuracy with 36% higher throughput than FlexiViT-L
- Reduces dense prediction encoder FLOPs by ~26× while maintaining competitive performance
- With learned policies, reduces FLOPs by 24-33% while preserving accuracy within ±0.1 percentage points
- Maintains strong performance across segmentation (mIoU) and depth estimation (RMSE) tasks

## Why This Works (Mechanism)

### Mechanism 1: Block-masked attention for single-pass multi-granularity training
Block masking constrains attention within token groups such that later groups can attend to earlier ones but not vice versa, ensuring that computing the first t tokens yields identical results to what the model would produce if t were the actual sequence length. This avoids the O(|T|) cost of separate forward passes per granularity. The structured mask creates causal isolation between granularity levels while allowing beneficial information flow from earlier to later tokens.

### Mechanism 2: Once-for-all training with lightweight decoder readouts
The encoder produces intermediate latent representations {zl}l∈D in a single pass, which are then sliced and decoded at multiple token granularities t∈T and depths l∈D via cross-attention readouts (≈2% of parameters). This approach is tractable because output cross-attention constitutes ≈2% of total parameters, making multiple readouts computationally negligible compared to the full encoder forward pass.

### Mechanism 3: Implicit width training through per-sample sampling
Width adaptivity is learned implicitly by sampling a width configuration wi∼Uniform(W) for each training example, making a separate width loss unnecessary. Since width affects the encoder forward pass itself, gradients from both token and depth losses propagate through the width-sliced computation, training all width sub-networks. Matryoshka linear layers enable this by masking/slicing along the embedding dimension.

## Foundational Learning

- **Perceiver encode-process-decode paradigm**: Understanding that latents are modality-agnostic processing buffers is essential for grasping why token adaptivity can be applied to latents independently of input size. Quick check: If you double the input image resolution, what happens to the number of latent tokens in a standard Perceiver? (Answer: unchanged—latents are fixed-size and decoupled from input.)

- **Rotary Positional Embeddings (RoPE) and extrapolation**: AdaPerceiver uses 1D RoPE to distinguish broadcast latent tokens and enable extrapolation beyond training token counts. Quick check: Why does RoPE support sequence length extrapolation better than learned absolute position embeddings? (Answer: RoPE encodes relative positions via rotation, not absolute indices, so positions beyond training range remain well-defined.)

- **Once-for-all / elastic neural networks**: Understanding that OFA trains a supernetwork containing many sub-networks with shared weights clarifies why a single model can be configured at inference time. Quick check: In an OFA model, if sub-network A uses depth=6 and sub-network B uses depth=12, what is the relationship between their first 6 layers? (Answer: They share identical weights—sub-network A is literally the first 6 layers of sub-network B.)

## Architecture Onboarding

- **Component map**: Input tokens (patches) → Cross-Attention (encode) → Learned latent (broadcast to N) → Latent stream [z0...zL] → AdaPerceiver Blocks × L → Output tokens (task-specific) ← Cross-Attention (decode) → Task head (linear / MLP adapter)

- **Critical path**: 1. Latent broadcast + RoPE (distinguishes N identical tokens) 2. Block mask creation (determines which tokens attend to which) 3. Encoder forward with Matryoshka FFN width sampling 4. Multi-granularity readout via sliced cross-attention 5. Loss aggregation across (token, depth) pairs

- **Design tradeoffs**: Block masking vs. bidirectional attention (efficiency vs. information flow), per-sample vs. per-batch width sampling (convergence vs. memory), learned single latent vs. fixed latent array (extrapolation vs. interpolation complexity)

- **Failure signatures**: Accuracy collapses at low token counts (block mask too restrictive), width sub-networks have inconsistent performance (insufficient width sampling diversity), extrapolation beyond training tokens fails dramatically (RoPE theta/position handling), memory OOM during training (block mask + multiple depth readouts increases memory)

- **First 3 experiments**: 1. Sanity check: Single-configuration baseline (disable adaptivity, fix t=256, w=832, l=21), 2. Ablation: Block mask vs. bidirectional attention (compare convergence speed, final accuracy, training time), 3. Pareto frontier characterization (evaluate all (t, w, l) configurations, plot accuracy vs. FLOPs/latency)

## Open Questions the Paper Calls Out

### Open Question 1
Is it feasible to train the AdaPerceiver architecture from scratch without relying on distillation from a pre-trained teacher model? Section 5 (Limitations) states, "We rely on distillation... do not explore training the model entirely from scratch. This limits the generality of our approach when high-quality pre-trained teachers are unavailable." The authors utilized distillation specifically to ease learning and mitigate training costs, leaving the stability and convergence properties of training from random initialization unvalidated.

### Open Question 2
What is the upper-bound performance of AdaPerceiver on dense prediction tasks when utilizing state-of-the-art decoders rather than linear probes? Section 5 (Limitations) notes, "we evaluated with a linear probe rather than a state-of-the-art decoder. As a result, AdaPerceiver's upper-bound performance on these tasks remains unclear." The paper prioritized characterizing the adaptivity of the encoder rather than maximizing absolute performance on segmentation/depth tasks, leaving the encoder's capacity to feed complex heads untested.

### Open Question 3
Can learned configuration policies be improved to close the substantial performance gap between the implemented RL policy and the theoretical "optimal" oracle? Table 4 shows a large accuracy gap (~8.6%) between the "Optimal" policy (93.6%) and the "RL" policy (85.0%), despite the authors restricting their evaluation to "simple policies." The authors demonstrated that a large efficiency reservoir exists (as shown by the oracle), but the implemented lightweight policy network failed to capture the necessary complexity to access it.

## Limitations

- The block-masking mechanism restricts token-to-token information flow, potentially limiting performance on tasks requiring global context compared to full-attention baselines
- Width adaptivity is trained entirely implicitly without dedicated supervision, which may be suboptimal compared to explicit width supervision approaches
- The model relies on distillation from pre-trained teachers, leaving training from scratch unexplored and limiting generality when high-quality teachers are unavailable

## Confidence

**High confidence** in efficiency claims and basic training methodology. The computational savings from block masking and output cross-attention are straightforward to verify, and the accuracy-throughput Pareto curves are well-supported.

**Medium confidence** in universality across dense prediction tasks. While the paper demonstrates reasonable performance on segmentation and depth estimation, these tasks receive less experimental scrutiny than classification, and the claim of "competitive" performance is supported but not extensively validated.

**Low confidence** in optimality of block masking design. The paper shows it works but doesn't systematically explore alternatives or provide theoretical justification for why this specific masking structure enables effective training.

## Next Checks

1. **Block mask sensitivity analysis**: Systematically evaluate AdaPerceiver with different block mask configurations (varying overlap, block sizes, attention patterns) on ImageNet-1K to identify whether the current design is near-optimal or if significant improvements exist through mask restructuring.

2. **Cross-task generalization test**: Apply AdaPerceiver to a diverse set of dense prediction tasks beyond the three covered (e.g., instance segmentation, panoptic segmentation, optical flow) to measure whether the block masking constraint becomes more limiting on tasks requiring complex spatial reasoning and multi-scale feature fusion.

3. **Implicit vs. explicit width training comparison**: Train two variants of AdaPerceiver—one with implicit width sampling as proposed, another with explicit width supervision using dedicated width loss terms—to compare convergence speed, final accuracy across all width configurations, and robustness to width sampling frequency.