---
ver: rpa2
title: 'SIMBA UQ: Similarity-Based Aggregation for Uncertainty Quantification in Large
  Language Models'
arxiv_id: '2510.13836'
source_url: https://arxiv.org/abs/2510.13836
tags:
- methods
- similarity
- generations
- language
- confidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of uncertainty quantification
  (UQ) for large language models (LLMs) by developing methods to estimate confidence
  in generated outputs. The core contribution is a similarity-based aggregation framework
  that leverages the consistency among multiple sampled generations to infer confidence,
  avoiding reliance on model internals.
---

# SIMBA UQ: Similarity-Based Aggregation for Uncertainty Quantification in Large Language Models

## Quick Facts
- arXiv ID: 2510.13836
- Source URL: https://arxiv.org/abs/2510.13836
- Reference count: 40
- Primary result: Similarity-based aggregation methods outperform baselines in LLM uncertainty quantification, with best calibration (ACE) on nine diverse datasets.

## Executive Summary
This paper introduces SIMBA UQ, a black-box framework for uncertainty quantification in large language models that estimates confidence by analyzing consistency among multiple sampled generations. The core insight is that correct outputs tend to cluster more tightly in similarity space than incorrect ones, enabling confidence estimation without access to model internals. The framework offers three aggregation approaches—simple averaging, Bayesian posterior estimation, and supervised classification—and demonstrates superior calibration performance across question answering, summarization, and text-to-SQL tasks. Notably, simple token-based similarity metrics like Jaccard prove effective even for structured outputs like SQL queries.

## Method Summary
The SIMBA framework generates multiple samples per query across different temperatures (5 samples × 6 temperatures from 0.25 to 1.5), computes pairwise similarities between all generations, and aggregates these similarities into confidence scores. Three aggregation methods are proposed: (1) arith-agg using mean similarity, (2) bayes-beta using Bayesian posterior updating with Beta distributions, and (3) clf-pairs/clf-pairs+gen using supervised random forest classification on similarity features. The method requires a small labeled dataset for training classifiers or learning Bayesian parameters, with 50/50 train-test splits and evaluation on low-temperature samples only. Correctness is determined via Rouge-L thresholds for text tasks and SQL execution matching for code generation.

## Key Results
- SIMBA methods achieve significantly lower Adaptive Calibrated Error (ACE) than baselines across all nine datasets, with clf-pairs reaching ACE of 0.041 on CoQA versus 0.272 for arith-agg baseline
- Simple Jaccard similarity performs surprisingly well for structured SQL outputs, comparable to specialized metrics
- Classification-based aggregation (clf-pairs) consistently outperforms both simple averaging and Bayesian approaches on calibration metrics
- The framework maintains strong performance across diverse task types: QA, summarization, and text-to-SQL generation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If correct generations exhibit higher pairwise similarity to other sampled generations than incorrect ones, then aggregated similarity can serve as a proxy for confidence.
- **Mechanism:** Multiple samples are generated from an LLM at varying temperatures. Pairwise similarities between all samples are computed using a chosen metric (e.g., Jaccard, Rouge-L). An aggregation function maps the similarity vector for each generation to a confidence score.
- **Core assumption:** The "consistency hypothesis" holds: correct generations cluster more tightly than incorrect ones.
- **Evidence anchors:** [abstract]: "consistency between a generated output and other sampled generations is used as a proxy for confidence"
- **Break condition:** If the task has multiple valid but syntactically dissimilar correct answers (e.g., paraphrased summaries), similarity-based clustering may incorrectly penalize correct responses.

### Mechanism 2
- **Claim:** If pairwise similarities are informative features, then training a probabilistic classifier on a small labeled dataset can yield better-calibrated confidence estimates than simple aggregation heuristics.
- **Mechanism:** The aggregation function f(·) is learned via supervised classification. Features include all pairwise similarities (clf-pairs), mean similarity plus generative score (clf-mean+gen), or both (clf-pairs+gen). A random forest outputs P(correct | similarities).
- **Core assumption:** The training distribution reflects test-time generation conditions (same sampling procedure, similar query distribution).
- **Evidence anchors:** [Section 3.4]: "treating confidence estimation as a classification task... train a probabilistic classifier for whether a response is correct using supervised learning with features based on similarities"
- **Break condition:** Distribution shift between training and deployment data degrades classifier calibration; small training sets may not cover edge cases.

### Mechanism 3
- **Claim:** If similarities are conditionally independent given correctness, Bayesian posterior updating can provide well-calibrated confidence estimates with minimal parameter learning.
- **Mechanism:** Prior p₀ and Beta distribution parameters for P(similarity | correct) and P(similarity | incorrect) are learned from a small training set. Posterior P(correct | similarities) is computed via Bayes rule.
- **Core assumption:** Conditional independence of pairwise similarities; Beta distributions adequately model the conditional similarity distributions.
- **Evidence anchors:** [Section 3.4]: "The formula makes two important assumptions: 1) similarities in sᵢ depend only on whether yᵢ is correct, and 2) similarities in sᵢ are conditionally independent"
- **Break condition:** When similarities are correlated (e.g., from similar temperature samples), the independence assumption fails, leading to miscalibrated posteriors.

## Foundational Learning

- **Concept:** Calibration metrics (ACE, AUROC)
  - **Why needed here:** To evaluate whether confidence estimates align with empirical accuracy; the paper optimizes for ACE specifically.
  - **Quick check question:** If a model assigns 0.8 confidence to 100 predictions, how many should be correct for the model to be perfectly calibrated?

- **Concept:** Pairwise similarity metrics for text (Jaccard, Rouge, embedding cosine)
  - **Why needed here:** The entire framework depends on computing s(yᵢ, yⱼ); choice of metric affects clustering behavior.
  - **Quick check question:** Why might Jaccard be preferred over semantic embeddings for SQL query similarity?

- **Concept:** Probabilistic classification with limited training data
  - **Why needed here:** The classification-based aggregation methods require training a model on small datasets (50% split of ~1000 queries).
  - **Quick check question:** What regularization or model constraints help prevent overfitting when training on small similarity-feature datasets?

## Architecture Onboarding

- **Component map:** Sampler -> Similarity Computer -> Aggregator -> Evaluator
- **Critical path:**
  1. Verify sampling produces meaningful diversity (if all samples identical, UQ fails)
  2. Select similarity metric appropriate to task (token-based for SQL, semantic for summarization)
  3. Train aggregator on in-domain labeled data before deployment

- **Design tradeoffs:**
  - Black-box (similarity-only) vs. white-box (include generative scores): white-box adds signal but requires logit access
  - Simple vs. learned aggregation: simple is zero-shot; learned requires training data but improves ACE
  - Metric choice: token-based metrics (Jaccard) work surprisingly well even for structured outputs; semantic metrics (SBERT) add computational cost

- **Failure signatures:**
  - ACE remains high despite training → check training/test distribution mismatch
  - AUROC near 0.5 → similarity signal may be uninformative; verify sampling diversity
  - bayes-beta underperforms on ACE → conditional independence assumption violated

- **First 3 experiments:**
  1. **Baseline check:** Run arith-agg on a held-out subset; if ACE > 0.3, the consistency hypothesis may not hold for your task
  2. **Metric ablation:** Compare Jaccard vs. Rouge-L vs. embedding-based similarity on a validation set; select best-performing metric before training classifiers
  3. **Sample budget test:** Vary number of samples per query (e.g., 5 vs. 10 vs. 20) to find point of diminishing returns for ACE improvement

## Open Questions the Paper Calls Out
- **Question:** How robust is the SIMBA framework when applied to out-of-domain data involving distribution shifts?
- **Basis in paper:** [explicit] The authors state, "Investigating out-of-domain performance with such methods is important for future work as it involves challenges such as distribution shift and domain adaptation..."
- **Why unresolved:** The study explicitly restricted experiments to in-domain settings, assuming the training and test sets were not "too dissimilar," leaving generalization capabilities unverified.
- **What evidence would resolve it:** Empirical results from experiments where the similarity aggregation models are trained on one domain (e.g., Wikipedia-based QA) and tested on a structurally different domain (e.g., medical or legal text).

- **Question:** Under what theoretical conditions does the consistency hypothesis fail to correlate similarity with correctness?
- **Basis in paper:** [explicit] The limitations section notes, "Further work is required to provide theoretical justification and a more complete understanding of when the assumption may or may not hold."
- **Why unresolved:** The paper relies on empirical validation of the consistency hypothesis but lacks a theoretical framework explaining why high similarity sometimes occurs in incorrect generations.
- **What evidence would resolve it:** A formal analysis or counter-examples defining specific linguistic or logical scenarios where incorrect answers exhibit high pairwise similarity (stability of error).

- **Question:** Can learnable similarity functions improve performance over static token-based metrics like Jaccard?
- **Basis in paper:** [explicit] The methodology section suggests, "Future work could explore adapting learnable similarity functions over higher level concepts, such as those using graph neural networks..."
- **Why unresolved:** The current work relies on pre-defined metrics (e.g., Jaccard, Rouge-L) that treat outputs as sets of tokens rather than complex structures.
- **What evidence would resolve it:** Comparative experiments integrating semantic or structural similarity metrics (e.g., GNN-based matching for SQL or semantic embeddings for summarization) into the SIMBA aggregation pipeline.

## Limitations
- The consistency hypothesis may fail for tasks with multiple valid but syntactically diverse correct answers
- The method requires generating 30 samples per query, creating significant computational overhead
- Performance depends on having a representative training set; domain shift can degrade calibration quality

## Confidence
- **High Confidence**: Experimental results showing SIMBA's superior ACE performance compared to baselines are well-supported by methodology and metrics
- **Medium Confidence**: Effectiveness of simple similarity metrics like Jaccard for structured outputs is supported but underlying reasons are not fully explored
- **Low Confidence**: Bayesian aggregation's poor ACE performance suggests violated independence assumptions, but paper lacks deep investigation of when/why this occurs

## Next Checks
1. **Distribution Shift Test**: Evaluate SIMBA's calibration when trained on one dataset (e.g., CoQA) and tested on a different but related dataset (e.g., Natural Questions) to quantify robustness to domain shift

2. **Sample Efficiency Analysis**: Systematically vary the number of samples per query (2, 5, 10, 20) to identify the point of diminishing returns for ACE improvement and determine practical sample budgets

3. **Task-Specific Similarity Analysis**: For tasks where SIMBA underperforms (e.g., BIRD), conduct qualitative analysis of generation clusters to identify patterns where the consistency hypothesis fails, potentially revealing task-specific failure modes