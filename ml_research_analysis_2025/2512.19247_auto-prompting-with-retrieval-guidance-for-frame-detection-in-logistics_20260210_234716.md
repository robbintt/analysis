---
ver: rpa2
title: Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics
arxiv_id: '2512.19247'
source_url: https://arxiv.org/abs/2512.19247
tags:
- prompt
- logistics
- frame
- prompts
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel prompt optimization pipeline for frame
  detection in logistics texts, combining retrieval-augmented generation (RAG), few-shot
  prompting, chain-of-thought (CoT) reasoning, and automatic CoT synthesis (Auto-CoT)
  to generate highly effective task-specific prompts. The approach uses an LLM-based
  prompt optimizer agent that iteratively refines prompts using retrieved examples,
  performance feedback, and internal self-evaluation.
---

# Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics

## Quick Facts
- arXiv ID: 2512.19247
- Source URL: https://arxiv.org/abs/2512.19247
- Reference count: 32
- Key result: Optimized prompts improve logistics frame detection accuracy by up to 15% over baselines using RAG + Auto-CoT

## Executive Summary
This paper introduces a prompt optimization pipeline for hierarchical frame detection in logistics texts, leveraging retrieval-augmented generation (RAG), few-shot prompting, chain-of-thought (CoT) reasoning, and automatic CoT synthesis (Auto-CoT). The system uses an LLM-based optimizer to iteratively refine prompts using retrieved examples and performance feedback. Evaluated on Vietnamese logistics messages, the approach achieves up to 15% accuracy improvement over static or zero-shot prompts across multiple LLMs. The method offers a scalable alternative to full fine-tuning for domain-specific NLP tasks.

## Method Summary
The method employs an LLM-based prompt optimizer that iteratively refines task-specific prompts for frame detection in logistics texts. It uses RAG to inject semantically similar k-shot examples into prompts, then applies Auto-CoT to generate and refine Chain-of-Thought reasoning. The optimizer runs for 3 iterations, using validation set feedback to improve prompt clarity and constraint specification. The pipeline is tested on a proprietary dataset of 1,500 Vietnamese logistics messages with 73 unique labels, structured as a hierarchical classification task (Actor, Reason, Cause).

## Key Results
- Optimized prompts with Auto-CoT and RAG improve real-world inference accuracy by up to 15% compared to baseline prompts
- The approach consistently outperforms zero-shot and static few-shot prompts across GPT-4o, Qwen 2.5 (72B), and LLaMA 3.1 (70B)
- RAG-enhanced few-shot prompts provide marginal gains but are less effective than the full Auto-CoT + RAG pipeline

## Why This Works (Mechanism)
The method works by combining retrieval-augmented examples with iterative prompt refinement guided by an LLM optimizer. RAG ensures relevant contextual examples are included, while Auto-CoT synthesizes structured reasoning paths. The optimizer iteratively critiques and refines prompts based on validation performance, focusing on constraint clarity and example relevance. This closed-loop system adapts prompts to domain-specific patterns without requiring full model fine-tuning.

## Foundational Learning
- **RAG (Retrieval-Augmented Generation)**: Why needed - injects relevant examples into prompts to guide LLM responses; Quick check - verify top-k retrieved examples are semantically relevant to the query
- **Few-shot prompting**: Why needed - provides concrete examples within the prompt to demonstrate task expectations; Quick check - ensure examples cover diverse label combinations
- **Chain-of-Thought (CoT)**: Why needed - structures reasoning steps to improve complex decision-making; Quick check - confirm CoT traces follow logical flow without skipping steps
- **Auto-CoT**: Why needed - automatically generates and synthesizes reasoning chains to reduce manual prompt engineering; Quick check - validate synthesized CoT examples match task complexity
- **Prompt optimization loop**: Why needed - iteratively refines prompts based on feedback to improve performance; Quick check - monitor validation accuracy per iteration for convergence
- **Multilingual embeddings**: Why needed - enables effective retrieval for Vietnamese logistics texts; Quick check - test embedding similarity scores for slang vs. standard phrases

## Architecture Onboarding

**Component Map:**
LLM Optimizer -> Prompt Template Engine -> RAG Retriever -> Evaluation Module -> (back to LLM Optimizer)

**Critical Path:**
Input query → RAG retrieves examples → Prompt template injects examples → LLM generates response → Evaluation compares to ground truth → Optimizer refines prompt → Next iteration

**Design Tradeoffs:**
- Fixed 3-iteration loop vs. adaptive convergence criteria: Fixed count ensures predictable runtime but may over-optimize or under-converge
- RAG vs. static few-shot: RAG provides contextual relevance but introduces retrieval noise; static examples are stable but less adaptable
- Multilingual embeddings vs. monolingual: Multilingual supports Vietnamese but may sacrifice precision for low-resource languages

**Failure Signatures:**
- RAG retrieves irrelevant or low-similarity examples (similarity scores <0.5)
- Prompt optimizer generates overly verbose or contradictory instructions
- Validation accuracy plateaus or decreases after iteration 1
- LLM fails to follow CoT reasoning despite clear examples

**Three First Experiments:**
1. Run zero-shot prompt on validation set to establish baseline accuracy
2. Implement RAG retriever with k=3 and test example relevance on sample queries
3. Execute one iteration of the optimization loop and inspect prompt changes

## Open Questions the Paper Calls Out
- Does the pipeline maintain performance gains when transferred to other specialized, low-resource domains (e.g., legal or medical) with distinct terminologies?
- What adaptive mechanisms can replace the fixed 3-step iteration loop to ensure prompt convergence without unnecessary computational overhead?
- How can the RAG component be modified to better handle informal slang, abbreviations, and non-standard syntax to improve example retrieval quality?

## Limitations
- Evaluated only on proprietary Vietnamese logistics dataset, limiting generalizability
- Fixed 3-iteration loop may not guarantee convergence and adds computational latency
- RAG sensitivity to informal input causes suboptimal few-shot selection for slang or non-standard phrasing

## Confidence
- High Confidence: Core methodological contribution and consistent improvements across multiple LLMs
- Medium Confidence: Real-world inference results rely on proprietary dataset; qualitative benefits not quantitatively validated
- Low Confidence: Claims about out-of-distribution performance and cross-domain robustness are untested

## Next Checks
1. Obtain or simulate a logistics text dataset with the same label hierarchy and evaluate the full pipeline
2. Run the optimization loop with k=1, k=3, k=6, and k=0 (no RAG) to quantify marginal benefit of retrieval
3. Apply the Auto-CoT + RAG pipeline to a different domain (e.g., customer service dialogues) and report accuracy and iteration stability