---
ver: rpa2
title: 'VideoRewardBench: Comprehensive Evaluation of Multimodal Reward Models for
  Video Understanding'
arxiv_id: '2509.00484'
source_url: https://arxiv.org/abs/2509.00484
tags:
- mrms
- video
- reward
- wang
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces VideoRewardBench, the first comprehensive\
  \ benchmark for evaluating multimodal reward models (MRMs) in video understanding\
  \ across four dimensions: perception, knowledge, reasoning, and safety. The benchmark\
  \ includes 1,563 high-quality preference pairs from 10 existing video datasets,\
  \ covering 1,559 distinct questions\u201415 times more than prior benchmarks."
---

# VideoRewardBench: Comprehensive Evaluation of Multimodal Reward Models for Video Understanding

## Quick Facts
- arXiv ID: 2509.00484
- Source URL: https://arxiv.org/abs/2509.00484
- Reference count: 40
- 28 MRMs evaluated; best models achieve only moderate accuracy (63.6% for Gemini-2.5-Pro, 53.3% for best open-source)

## Executive Summary
This paper introduces VideoRewardBench, the first comprehensive benchmark for evaluating multimodal reward models (MRMs) in video understanding across four dimensions: perception, knowledge, reasoning, and safety. The benchmark includes 1,563 high-quality preference pairs from 10 existing video datasets, covering 1,559 distinct questions—15 times more than prior benchmarks. A thorough evaluation of 28 MRMs across three categories (generative, discriminative, and semi-scalar) reveals that even the best models achieve only moderate accuracy: proprietary models like Gemini-2.5-Pro and Claude-3.7-Sonnet reach 63.6% and 63.2% overall accuracy respectively, while the top open-source model Qwen2.5-VL-72B attains just 53.3%.

## Method Summary
VideoRewardBench evaluates MRMs using 1,563 preference pairs from 10 video datasets, covering 4 dimensions (perception, knowledge, reasoning, safety) with 1,559 unique questions. Three evaluation protocols are used: generative MRMs perform pairwise ranking with specific prompt templates, discriminative MRMs compute pointwise scalar scores, and semi-scalar MRMs generate critiques then predict scalar scores. The benchmark uses Overall Accuracy (sample-weighted) and Macro Average Accuracy (dimension-balanced) as metrics. Models are evaluated across varying frame counts and inference-time scaling parameters to assess robustness and scaling behavior.

## Key Results
- Proprietary models (Gemini-2.5-Pro: 63.6%, Claude-3.7-Sonnet: 63.2%) significantly outperform open-source models (Qwen2.5-VL-72B: 53.3%)
- RL-trained MRMs show weaker cross-modal generalization than SFT-trained models (R1-Reward drops 15.6% compared to base model)
- Inference-time scaling improves generative and semi-scalar MRMs but not discriminative MRMs
- Critic-trained generative MRMs benefit substantially from increased frame counts while semi-scalar MRMs show performance decline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Inference-time scaling improves generative and semi-scalar MRM accuracy through response diversity aggregation, but fails for discriminative MRMs due to deterministic scoring.
- Mechanism: For generative MRMs, sampling K responses with temperature > 0 creates varied judgment distributions; majority voting filters out inconsistent evaluations. Semi-scalar MRMs benefit similarly via score merging across independent runs. Discriminative MRMs output identical scalar scores regardless of sampling, making aggregation ineffective.
- Core assumption: The judgment variance across samples reflects meaningful uncertainty rather than noise.
- Evidence anchors:
  - [abstract] "(ii) except for discriminative MRMs, other types of MRMs across varying model capacities can benefit from inference-time scaling"
  - [section 5.1] "Claude-3.7-Sonnet improves by 10.6% as K increases from 1 to 9, and Qwen2.5-VL-72B improves by 2% as K rises from 1 to 5"
  - [corpus] Related work on inference-time scaling (Snell et al., 2024) shows similar benefits for LLMs, suggesting mechanism generalizes across modalities.
- Break condition: If temperature is set too low (e.g., 0.2), variance collapses and scaling fails—as occurred in VL-RewardBench experiments.

### Mechanism 2
- Claim: Critic-trained generative MRMs benefit more from increased frame counts than non-critic or semi-scalar MRMs because critic training improves visual evidence aggregation.
- Mechanism: Critic training teaches models to systematically evaluate visual details. When more frames are available, these models better integrate temporal evidence for judgment. Non-critic models lack this training and struggle with high-dimensional visual input. Semi-scalar MRMs may over-rely on textual critique generation, making additional frames less impactful.
- Core assumption: Critic training improves visual attention and evidence synthesis rather than just evaluation pattern matching.
- Evidence anchors:
  - [section 5.2] "LLaVA-Critic-72B improves from 52.0% to 63.0% as the frame count rises from 1 to 64" while "non-critic-trained generative MRMs (Qwen2.5-VL-7B/72B) exhibit a relatively less pronounced upward trend"
  - [section 5.2] "semi-scalar MRM, MM-RLHF-Reward is the least affected by frame count variation, showing a slight initial drop before stabilizing"
  - [corpus] No directly comparable mechanism in corpus; this appears to be a novel finding specific to video MRMs.
- Break condition: If critic training was performed on image-only data without temporal reasoning, frame benefits may not transfer (as seen with R1-Reward's weak cross-modal generalization).

### Mechanism 3
- Claim: RL-trained MRMs show weaker cross-modal generalization than SFT-trained models because RL optimizes for specific reward distributions that may not transfer across modalities.
- Mechanism: SFT on critic instruction-following data teaches general evaluation patterns applicable across domains. RL fine-tunes these patterns toward specific reward distributions, potentially overfitting to training modality characteristics. When evaluated on video after image-only RL training, the reward signal becomes miscalibrated.
- Core assumption: RL optimization creates modality-specific reward calibration rather than generalizable evaluation reasoning.
- Evidence anchors:
  - [abstract] "MRMs trained with reinforcement learning (RL) do not necessarily exhibit stronger cross-modal generalization than those trained without RL"
  - [section 4.3] "R1-Reward drops by 15.6% compared to Qwen2.5-VL-7B" and "Flex-Judge, trained only on text and evaluated on video, shows the largest drop—20% compared to its base model"
  - [corpus] Related work R1-Reward (Zhang et al.) focuses on stable RL training but doesn't address cross-modal transfer, suggesting this is an underexplored failure mode.
- Break condition: If RL training explicitly includes multi-modal data with proper regularization, the degradation may be mitigated.

## Foundational Learning

- Concept: Preference-based reward modeling
  - Why needed here: The benchmark evaluates MRMs that must distinguish chosen vs. rejected responses—understanding this foundational paradigm is essential.
  - Quick check question: Can you explain why reward models are trained on preference pairs rather than absolute quality scores?

- Concept: Three MRM architecture paradigms
  - Why needed here: The benchmark treats generative, discriminative, and semi-scalar MRMs differently in evaluation methodology and scaling behavior.
  - Quick check question: What is the key output difference between a discriminative MRM and a generative MRM when evaluating a response?

- Concept: Cross-modal generalization
  - Why needed here: The paper reveals that training modality (image vs. text vs. video) critically impacts video evaluation performance.
  - Quick check question: Why might a model trained on image-text pairs fail to evaluate video-text pairs, even though video contains images?

## Architecture Onboarding

- Component map:
  - Source datasets → Multi-stage filtering → Response generation → Preference annotation → 1,563 triplets (video-text prompt, chosen response, rejected response)
  - Evaluation Dimensions: Perception (long-form, short-form), Knowledge, Reasoning, Safety—each requiring different response types and annotation methods
  - MRM Categories: Generative (pairwise ranking via text output), Discriminative (pointwise scalar scoring), Semi-scalar (critique + scalar)
  - Metrics: Overall Accuracy (sample-weighted), Macro Average Accuracy (dimension-balanced)

- Critical path: Start with short-form perception subset (413 samples, ground-truth-based labels) to quickly validate MRM baseline → progress to knowledge and reasoning dimensions (require complex evaluation) → finish with safety (highest variance in model performance).

- Design tradeoffs:
  - **Sample size vs. quality**: 1,563 samples is 15x larger than prior work, but multi-stage filtering required to ensure difficulty (removing questions answerable without video)
  - **Dimension balance vs. domain coverage**: Equal distribution across 5 dimensions (18-26% each) limits depth in any single domain
  - **Human annotation vs. scalability**: Long-form perception and safety required 3-annotator majority voting; short-form perception used ground-truth labels to reduce cost

- Failure signatures:
  - **Length bias**: If chosen/rejected responses differ significantly in word count, models may use length as proxy for quality (dataset analysis shows 102.9 vs. 104.6 avg words—appropriately balanced)
  - **Position bias**: Models may prefer first-presented response; benchmark randomizes order and uses reversal during inference-time scaling
  - **Safety dimension collapse**: Qwen2.5-VL-72B shows 80.1% → 44.7% accuracy drop on safety when scaling from 7B to 72B, suggesting overfitting or emergent misalignment

- First 3 experiments:
  1. **Baseline evaluation**: Run 3 representative MRMs (one from each category) on short-form perception subset to establish evaluation pipeline and identify any data loading issues.
  2. **Inference-time scaling ablation**: Test LLaVA-Critic-7B with K ∈ {1, 3, 5, 9} at temperatures {0.5, 1.0} to reproduce scaling benefits and determine optimal settings.
  3. **Frame count sensitivity**: Evaluate Qwen2.5-VL-7B and LLaVA-Critic-7B with frame counts {1, 8, 32, 64} on the reasoning subset to confirm differential frame effects before full benchmark runs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do multimodal reward models trained with reinforcement learning exhibit weaker cross-modal generalization (image→video) compared to those trained with supervised fine-tuning alone?
- Basis in paper: [explicit] The paper states "MRMs trained with reinforcement learning (RL) do not necessarily exhibit stronger cross-modal generalization than those trained without RL," noting that R1-Reward drops 15.6% compared to its base model, and Flex-Judge (trained only on text) shows a 20% drop.
- Why unresolved: The paper identifies this counterintuitive finding but does not investigate the underlying mechanisms causing RL-trained models to lose cross-modal transferability.
- What evidence would resolve it: Ablation studies isolating training data modality composition, analysis of learned representations in RL-trained vs SFT-trained models, and experiments with mixed-modality RL training data.

### Open Question 2
- Question: Why do semi-scalar MRMs show performance decline with increased frame counts while critic-trained generative MRMs benefit substantially?
- Basis in paper: [explicit] Section 5.2 states "For the semi-scalar MRM, MM-RLHF-Reward... showing a slight initial drop before stabilizing," while "LLaVA-Critic-72B improves from 52.0% to 63.0% as the frame count rises from 1 to 64."
- Why unresolved: The paper documents this divergent behavior across MRM categories but does not explain the architectural or algorithmic causes.
- What evidence would resolve it: Controlled experiments varying frame count while measuring attention patterns, noise accumulation in critique generation, and score prediction mechanisms across architectures.

### Open Question 3
- Question: What specific training data compositions or architectural modifications would enable open-source MRMs to close the 10+ percentage point gap with proprietary models like Gemini-2.5-Pro (63.6%) and Claude-3.7-Sonnet (63.2%)?
- Basis in paper: [inferred] The evaluation shows top open-source model Qwen2.5-VL-72B achieves only 53.3%, with most models struggling on short-form perception, knowledge, and reasoning. Specialized reward modeling training does not close this gap.
- Why unresolved: The benchmark identifies performance deficiencies but does not investigate whether the gap stems from training data quality, scale, model architecture, or proprietary techniques.
- What evidence would resolve it: Systematic comparison of training data characteristics, experiments with scaled open training data, and ablation of architectural differences between top proprietary and open-source models.

## Limitations
- Exact multi-stage filtering thresholds and human annotation guidelines are incompletely specified
- RJScore computation for safety dimension is not fully detailed
- Proprietary model API specifications beyond date listings are unclear

## Confidence
- **High Confidence**: Benchmark design and evaluation methodology are well-specified; accuracy findings (63.6% for Gemini-2.5-Pro, 53.3% for Qwen2.5-VL-72B) are directly supported
- **Medium Confidence**: Key insights about RL generalization, scaling behavior, and frame count effects are supported but some mechanisms rely on external comparisons
- **Low Confidence**: Cross-modal generalization findings depend on external R1-Reward specifications not fully detailed in this paper

## Next Checks
1. **Cross-validation of safety dimension**: Replicate the Qwen2.5-VL-72B safety performance drop (80.1% → 44.7%) with different random seeds and human annotator subsets to verify this is not an outlier effect.
2. **Frame count ablation on reasoning**: Systematically evaluate Qwen2.5-VL-7B and LLaVA-Critic-7B across {1, 8, 32, 64} frames on the reasoning subset to confirm differential frame effects before scaling to full benchmark.
3. **Inference-time scaling robustness**: Test LLaVA-Critic-7B with K ∈ {1, 3, 5, 9} at temperatures {0.5, 1.0} on the short-form perception subset to reproduce scaling benefits and identify optimal settings.