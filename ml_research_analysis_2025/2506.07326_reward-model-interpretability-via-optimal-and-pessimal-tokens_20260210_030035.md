---
ver: rpa2
title: Reward Model Interpretability via Optimal and Pessimal Tokens
arxiv_id: '2506.07326'
source_url: https://arxiv.org/abs/2506.07326
tags:
- love
- reward
- human
- tokens
- happiness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Reward models trained to encode human preferences exhibit systematic
  heterogeneity, asymmetry, and framing sensitivity, undermining their assumed interchangeability
  and revealing unintended biases toward identity groups and frequent tokens. By exhaustively
  scoring all single-token responses to value-laden prompts across ten open-source
  reward models, the authors find that: (1) models rank tokens inconsistently even
  with similar architectures and objectives; (2) high-scoring tokens show greater
  sensitivity than low-scoring ones, with sentiment and prompt framing modulating
  this effect in a human-like manner; (3) more frequent tokens are systematically
  overvalued, suggesting base model leakage; and (4) identity group references are
  systematically undervalued, likely due to harmlessness training artifacts.'
---

# Reward Model Interpretability via Optimal and Pessimal Tokens

## Quick Facts
- arXiv ID: 2506.07326
- Source URL: https://arxiv.org/abs/2506.07326
- Authors: Brian Christian; Hannah Rose Kirk; Jessica A. F. Thompson; Christopher Summerfield; Tsvetomira Dumbalska
- Reference count: 40
- Key outcome: Reward models trained to encode human preferences exhibit systematic heterogeneity, asymmetry, and framing sensitivity, undermining their assumed interchangeability and revealing unintended biases toward identity groups and frequent tokens.

## Executive Summary
Reward models trained to encode human preferences exhibit systematic heterogeneity, asymmetry, and framing sensitivity, undermining their assumed interchangeability and revealing unintended biases toward identity groups and frequent tokens. By exhaustively scoring all single-token responses to value-laden prompts across ten open-source reward models, the authors find that: (1) models rank tokens inconsistently even with similar architectures and objectives; (2) high-scoring tokens show greater sensitivity than low-scoring ones, with sentiment and prompt framing modulating this effect in a human-like manner; (3) more frequent tokens are systematically overvalued, suggesting base model leakage; and (4) identity group references are systematically undervalued, likely due to harmlessness training artifacts. These biases persist when optimizing for multi-token sequences and diverge from independent human preference rankings on EloEveRything. The findings challenge the reliability of reward models as proxies of human values and highlight risks for alignment and downstream deployment.

## Method Summary
The authors exhaustively score every token in the vocabulary (~128K–256K tokens) against fixed value-laden prompts for ten open-source reward models. They analyze distribution moments (skewness), rank tokens by reward, correlate rankings across models using Kendall's τ, and validate against external human preference data (EloEverything). They extend to multi-token sequences via modified GCG optimization and test framing effects by running prompts with positive ("greatest thing ever") and negative ("worst thing ever") valence.

## Key Results
- Models rank tokens inconsistently even with similar architectures and objectives, with Kendall's τ ranging ~0.2–0.7
- High-scoring tokens show greater sensitivity to sentiment and framing than low-scoring ones, with sensitivity inverting between positive and negative prompts
- More frequent tokens are systematically overvalued, suggesting base model distribution leakage
- Identity group references (e.g., "homosexuals," "Black people") are systematically undervalued across all models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Exhaustive single-token search reveals systematic asymmetries in reward model behavior that would be invisible to conventional sampling-based analysis.
- Mechanism: By scoring every token in the vocabulary (~128K–256K tokens) against a fixed prompt, the full reward distribution is mapped. Positive skew emerges consistently—most tokens receive low scores while a small subset forms a high-scoring tail. This asymmetry suggests reward models allocate discriminative capacity preferentially to positive-valued regions.
- Core assumption: Single-token responses, while artificial, surface value-encoding patterns that generalize to multi-token sequences.
- Evidence anchors:
  - [abstract]: "systematic asymmetries in how models encode high- vs low-scoring tokens"
  - [Section 2.2]: "all score distributions exhibit a positive skew... most tokens receive low rewards, while a small number of tokens score substantially higher than average"
  - [corpus]: PILAF (arXiv:2502.04270) addresses optimal preference sampling but does not validate exhaustive search generalizability
- Break condition: If downstream RLHF performance is insensitive to token-level distributional shape, the diagnostic value of exhaustive search would be limited.

### Mechanism 2
- Claim: Reward models exhibit framing sensitivity that mirrors human cognitive biases—sensitivity to positive tokens increases under positive framing and to negative tokens under negative framing.
- Mechanism: When prompts are positively framed ("greatest thing ever"), the slope relating sentiment to score is steeper for positive-sentiment tokens. Under negative framing ("worst thing ever"), this inverts. This suggests reward models do not encode value as symmetric but as context-dependent, with framing shifting attentional focus.
- Core assumption: Sentiment lexicons (Bing, AFINN-111) validly approximate token-level valence for model scoring.
- Evidence anchors:
  - [abstract]: "significant sensitivity to prompt framing that mirrors human cognitive biases"
  - [Section 3.2]: "models are, on average, more sensitive to negative-sentiment tokens relative to positive-sentiment ones... when the prompt is framed negatively"
  - [corpus]: No corpus papers directly validate framing transfer to human behavior
- Break condition: If framing effects disappear when controlling for prompt length or syntactic structure, the cognitive analogy would weaken.

### Mechanism 3
- Claim: Identity-group token devaluation emerges as an unintended consequence of harmlessness training objectives, producing "linguistic erasure."
- Mechanism: Tokens like "homosexuals," "Black people," and "Jews" score systematically lower even for positively framed prompts. The paper hypothesizes this stems from overrepresentation of identity terms in unsafe/rejected training examples, causing models to associate identity references with negative reward regardless of context.
- Core assumption: The devaluation pattern is attributable to harmlessness training data rather than base model pretraining or architecture.
- Evidence anchors:
  - [abstract]: "concerning linguistic erasure effects, with identity group references systematically devalued across models, likely stemming from unintended biases in harmlessness training objectives"
  - [Section 2.1]: "These patterns likely stem from artifacts in reward model training data, where identity groups are disproportionately represented in unsafe or 'rejected' examples"
  - [corpus]: Interpretable Reward Model via Sparse Autoencoder (arXiv:2508.08746) discusses RM interpretability but does not confirm harmlessness-training causation
- Break condition: If devaluation persists after retraining on identity-balanced preference data, alternative causes (base model biases, tokenizer artifacts) would require investigation.

## Foundational Learning

- Concept: **Reward Models in RLHF**
  - Why needed here: The entire methodology depends on understanding that reward models are trained on pairwise human preferences to output scalar scores representing "preferability," which are then used to fine-tune LLMs via PPO.
  - Quick check question: Can you explain why reward models are described as "where the human value rubber meets the road" in the RLHF pipeline?

- Concept: **Bradley-Terry Scoring**
  - Why needed here: The paper assumes familiarity with how pairwise preference comparisons are converted to scalar rewards through Bradley-Terry models, as this underpins what the reward scores represent.
  - Quick check question: What does it mean that reward scalars are "based on the Bradley-Terry score"?

- Concept: **Token Vocabulary Exhaustive Search**
  - Why needed here: The core methodological innovation is scoring all ~128K–256K tokens in a model's vocabulary, which is computationally tractable only for single tokens but becomes combinatorially intractable for multi-token sequences.
  - Quick check question: Why is exhaustive search feasible for single tokens but not for two-token sequences?

## Architecture Onboarding

- Component map:
  - Input layer: Value-laden prompts (e.g., "What, in one word, is the greatest thing ever?") fed to reward model
  - Reward model: Transformer that takes prompt-response pair and outputs scalar; study uses 10 models from RewardBench (Gemma-2, Llama-3/3.1 based, 2B–27B parameters)
  - Vocabulary tokenizer: Model-specific tokenizers (Gemma ~256K tokens, Llama ~128K tokens)
  - Scoring loop: For each token in vocabulary, append to prompt, compute reward score, store
  - Analysis layer: Sort/rank tokens, compute distribution moments (skewness), correlate across models and with sentiment lexicons (Bing, AFINN-111), compare to EloEverything human rankings
  - Multi-token extension: GCG (Greedy Coordinate Gradient) optimization for 2–5 token sequences

- Critical path:
  1. Select reward models from RewardBench leaderboard
  2. Define value-laden prompt variants (positive/negative framing)
  3. Score entire vocabulary per model per prompt
  4. Rank tokens, compute distribution statistics
  5. Cross-correlate models using Kendall's τ; validate against external human preference data (EloEverything)
  6. Optionally extend to multi-token sequences via modified GCG

- Design tradeoffs:
  - Single-token vs. multi-token: Exhaustive search is only tractable for single tokens; GCG provides approximate optimization for longer sequences but cannot characterize full distributions
  - Prompt design: "In one word" constraints elicit clear valence but limit ecological validity; removing constraint enables longer responses but complicates analysis
  - Sentiment lexicon choice: Bing (binary) vs. AFINN-111 (-5 to +5 scale)—tradeoff between granularity and coverage

- Failure signatures:
  - Heterogeneity without explanation: Models with similar training objectives show divergent token rankings (Kendall's τ ranging ~0.2–0.7), suggesting uncontrolled variables
  - Linguistic erasure detection: Identity tokens appearing in pessimal rankings for both positive and negative prompts indicates systematic bias
  - Mere-exposure effect: Frequent tokens scoring higher after controlling for sentiment suggests base model distribution leakage

- First 3 experiments:
  1. Replicate exhaustive single-token scoring on one reward model (e.g., R-Gem-2B) with the "greatest thing" prompt to verify positive skew and identify top-10/bottom-10 tokens; compare to paper's Table 2
  2. Test framing effect by running same model on "greatest thing," "best thing," and "worst thing" prompts; verify that sentiment-score slope inverts between positive and negative frames
  3. Check for identity-token devaluation by extracting rank positions of tokens like "homosexuals," "Black people," "Jews" across models; confirm systematic low-ranking even under positive framing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the "mere-exposure effect" in reward models interact with the KL-divergence regularizer during the downstream fine-tuning of LLMs?
- Basis in paper: [explicit] The authors state, "More work is needed... and also the degree to which this 'mere-exposure effect' interacts with the downstream KL-divergence regularizer typically used when fine-tuning LLMs against the reward model."
- Why unresolved: The study analyzes reward models in isolation rather than evaluating their behavior within the full RLHF training loop where KL constraints are applied.
- What evidence would resolve it: An analysis of the final token distributions of LLMs fine-tuned with these reward models to determine if the overvaluation of frequent tokens persists or is mitigated by the regularizer.

### Open Question 2
- Question: What specific development choices (e.g., training data composition, base model selection) drive the substantial heterogeneity observed in token rankings across different reward models?
- Basis in paper: [explicit] The conclusion notes that the "significant heterogeneity in token rankings among reward models invites further study of how these differences arise as a function of the design choices made by developers."
- Why unresolved: The paper identifies the heterogeneity but attributes the difficulty in explaining it to the opacity of training processes and poor documentation of the models studied.
- What evidence would resolve it: A controlled ablation study training reward models on identical base architectures but varying data sources or annotation guidelines to isolate the causal factors of ranking divergence.

### Open Question 3
- Question: To what extent do the systematic framing effects and identity-group devaluations identified in reward models propagate to the final behavior of downstream fine-tuned LLMs?
- Basis in paper: [explicit] The authors state, "It remains unclear how their behaviors interact with pre-trained models and KL constraints during RLHF" and warn of "distortions that risk propagating through the downstream large language models."
- Why unresolved: The methodology focuses on the reward model as a standalone artifact, abstracting away from the generative model and the alignment process.
- What evidence would resolve it: Comparing the outputs of LLMs fine-tuned using these reward models against the independent human preference baselines (like EloEverything) to see if the models' specific biases are inherited by the final system.

## Limitations
- The exhaustive single-token approach relies on artificial prompts that may not reflect real-world reward model behavior
- The causal mechanism for identity token devaluation is speculative, with alternative explanations unexplored
- The framing sensitivity findings depend on sentiment lexicon validity for model scoring

## Confidence
- **High Confidence**: The heterogeneity finding (Kendall's τ ranging 0.2–0.7) and identity token devaluation are well-supported by exhaustive scoring and cross-model correlation analysis
- **Medium Confidence**: The framing sensitivity mechanism and mere-exposure effect are statistically significant but rely on assumptions about sentiment lexicon validity and generalizability to multi-token behavior
- **Low Confidence**: The hypothesis that identity token devaluation specifically stems from harmlessness training objectives is speculative without empirical validation

## Next Checks
1. **Validate framing transfer to multi-token sequences**: Replicate the exhaustive single-token analysis but extend to 2–3 token sequences using GCG optimization. Test whether framing effects persist when tokens are combined, and whether the sensitivity pattern (positive tokens more sensitive under positive framing) holds for longer responses.

2. **Test identity token devaluation under controlled conditions**: Retrain a reward model from scratch on preference data where identity group references are balanced across safe/unsafe examples. Compare the resulting token rankings to the original model to isolate whether harmlessness training specifically causes the devaluation pattern.

3. **Cross-validate sentiment lexicon choice**: Repeat the sentiment-score correlation analysis using multiple sentiment lexicons (e.g., SentiWordNet, VADER) and compare results. Additionally, test whether model-specific tokenization affects sentiment assignment by examining how sentiment scores distribute across subword units versus whole words.