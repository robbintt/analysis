---
ver: rpa2
title: Towards Scalable Web Accessibility Audit with MLLMs as Copilots
arxiv_id: '2511.03471'
source_url: https://arxiv.org/abs/2511.03471
tags:
- page
- accessibility
- pages
- mllms
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces AAA, a scalable web accessibility audit framework
  that operationalizes WCAG-EM through a human-AI partnership model. The framework
  features two core innovations: GRASP, a graph-based multimodal sampling method that
  uses learned embeddings of visual, textual, and relational cues to ensure representative
  page coverage; and MaC, a multimodal large language model-based copilot that assists
  auditors with cross-modal reasoning and intelligent support in high-effort tasks.'
---

# Towards Scalable Web Accessibility Audit with MLLMs as Copilots

## Quick Facts
- arXiv ID: 2511.03471
- Source URL: https://arxiv.org/abs/2511.03471
- Authors: Ming Gu; Ziwei Wang; Sicen Lai; Zirui Gao; Sheng Zhou; Jiajun Bu
- Reference count: 40
- One-line primary result: AAA framework operationalizes WCAG-EM through human-AI partnership using multimodal sampling (GRASP) and MLLM copilots (MaC)

## Executive Summary
This paper introduces AAA, a scalable web accessibility audit framework that operationalizes WCAG-EM through a human-AI partnership model. The framework features two core innovations: GRASP, a graph-based multimodal sampling method that uses learned embeddings of visual, textual, and relational cues to ensure representative page coverage; and MaC, a multimodal large language model-based copilot that assists auditors with cross-modal reasoning and intelligent support in high-effort tasks. Together, these components enable end-to-end, scalable web accessibility auditing. The authors also contribute four novel datasets for benchmarking different stages of the audit pipeline. Extensive experiments demonstrate the effectiveness of their methods, revealing that small-scale language models, when fine-tuned, can serve as capable accessibility experts.

## Method Summary
The AAA framework operationalizes the WCAG-EM audit process through two key innovations. GRASP performs multimodal sampling by constructing website graphs where nodes are pages, then generating embeddings via BERT (text), ViT (visual), and GNNs (linkage). A structure learning module prunes noisy edges and recovers missing semantic links, with centroids defining the sample. The MaC component uses MLLMs to ingest page screenshots for cross-modal reasoning, mapping visual elements to WCAG criteria. For accessibility tasks, the framework employs fine-tuned small-scale MLLMs (approx. 8B parameters) that can match or exceed larger models through domain-specific optimization. The pipeline includes crawling, automated checking, multimodal sampling, MLLM-assisted auditing, and human validation.

## Key Results
- GRASP achieves lowest inter-cluster similarity ($S_{sampled}$ 44.31 vs ~55 for baselines), indicating highest diversity
- Small-scale MLLMs (8B parameters) significantly outperform larger baselines on accessibility tasks after fine-tuning
- MLLMs achieve up to 99.88% accuracy in violation judgment on the CCT dataset despite lower classification scores
- Four novel datasets created: TPS (495 websites, 97K pages), APR (968 pages), CCT (1,985 CAPTCHA images), CPE (1,199 pages)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal graph-based sampling yields more representative page subsets than text-only clustering.
- Core assumption: Visual layout and hyperlink topology contain orthogonal signals of "representativeness" not captured by DOM text alone.
- Evidence: GRASP-IGNN achieves lowest inter-cluster similarity ($S_{sampled}$ 44.31 vs ~55 for baselines), indicating higher diversity.

### Mechanism 2
- Claim: MLLMs enable automation of "cognitive" accessibility checks requiring visual reasoning.
- Core assumption: MLLMs possess sufficient world-knowledge and visual capability to approximate human cognitive barriers.
- Evidence: MLLMs achieve high accuracy (up to 99.88%) in violation judgment on CCT dataset despite lower classification scores.

### Mechanism 3
- Claim: Small-scale MLLMs, when fine-tuned, can match or exceed large models in specialized accessibility tasks.
- Core assumption: Accessibility tasks are sufficiently bounded to be compressed into smaller models without losing critical nuance.
- Evidence: Intern2-VL-8B significantly outperforms larger baselines, achieving nearly 100% in WCAG conformance.

## Foundational Learning

- **WCAG-EM (Website Accessibility Conformance Evaluation Methodology)**: Why needed? AAA framework explicitly operationalizes this 5-step standard. Quick check: Can you name the three sub-steps of WCAG-EM Step 3 (Select a Representative Sample)?
- **Graph Neural Networks (GNNs) & Homophily**: Why needed? GRASP relies on GNNs to propagate "representativeness" across page nodes. Quick check: Why might a standard GCN fail on a website graph where homepage links to both "Login" page and "Legal" page?
- **Multimodal Alignment**: Why needed? MaC requires mapping visual pixels to semantic concepts. Quick check: What visual features would an MLLM look for to determine if a "Search Box" exists?

## Architecture Onboarding

- **Component map**: Crawler -> Auto Check (Axe-core + AI) -> GRASP (BERT + ViT + GNN) -> MaC (MLLM Assistant/Auditor/Consultant) -> Human-in-the-loop
- **Critical path**: The bottleneck is the MaC -> Manual Check interface. Too many false positives overwhelm human auditors; too many false negatives reduce coverage.
- **Design tradeoffs**: Text-only sampling (faster, cheaper) vs. Multimodal sampling (more robust, requires vision encoding). Large models (turn-key, expensive) vs. Small fine-tuned models (cheaper, require data curation).
- **Failure signatures**: Linkage noise may cause redundant sampling if structure learning fails to prune navigational links. Small models may produce unstructured/verbose outputs without fine-tuning.
- **First 3 experiments**: 1) Implement text-only SDC baseline and run against TPS dataset to replicate $S_{sampled}$ gap. 2) Zero-shot prompt standard MLLM on CCT dataset without fine-tuning to establish formatting failure baseline. 3) Swap IGNN for GCN in GRASP to verify heterophily claim.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can MLLMs effectively serve in the consultant role for automated accessibility remediation? The paper identifies this as a key avenue for future research in large-scale remediation.
- **Open Question 2**: How can the accuracy and reliability of MLLM-based accessibility judgments be validated against hallucination risks? The paper acknowledges hallucinations as a known issue but does not propose mitigation strategies.
- **Open Question 3**: Can GRASP multimodal sampling approach generalize effectively to single-page applications and highly dynamic web content? The method's effectiveness on sites with minimal traditional hyperlink structures remains untested.
- **Open Question 4**: What is the optimal division of labor between automated tools, MLLM-based assistants, and human auditors across different accessibility conformance levels? The framework integrates components but leaves task allocation optimization unexplored.

## Limitations

- Dependence on visual input quality and assumptions about graph structure may limit GRASP effectiveness on SPAs and non-traditional websites
- Fine-tuning advantage for small MLLMs is highly task-dependent and may not generalize to complex reasoning scenarios
- Scalability claims assume MaC's false positive rate remains low enough to avoid overwhelming human auditors

## Confidence

- **High Confidence**: Multimodal sampling framework demonstrably outperforms text-only baselines on provided datasets
- **Medium Confidence**: Small MLLMs can match or exceed large models, but advantage is task-specific and not universal
- **Low Confidence**: Scalability claims in real-world deployment scenarios, as paper demonstrates effectiveness on curated datasets but not deployment challenges

## Next Checks

1. Implement GCN ablation study by swapping IGNN for standard GCN in GRASP to verify heterophily claim and measure performance degradation
2. Test GRASP's sampling quality on diverse website architectures not represented in TPS dataset (SPAs, e-commerce sites, news sites with infinite scroll)
3. Conduct comprehensive audit of MaC's output quality by having human experts categorize all suggestions into true positives, false positives, and "requires domain expertise" categories to quantify cognitive load on auditors