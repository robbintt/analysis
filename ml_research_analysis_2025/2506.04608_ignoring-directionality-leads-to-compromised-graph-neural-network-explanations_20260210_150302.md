---
ver: rpa2
title: Ignoring Directionality Leads to Compromised Graph Neural Network Explanations
arxiv_id: '2506.04608'
source_url: https://arxiv.org/abs/2506.04608
tags:
- graph
- information
- explanation
- neural
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies that the common graph symmetrization preprocessing
  step in Graph Neural Networks (GNNs) discards directional information, leading to
  compromised explanation fidelity. The authors provide theoretical analysis showing
  that symmetrization reduces structural complexity and mutual information, and demonstrate
  through experiments that preserving edge directionality improves explanation quality.
---

# Ignoring Directionality Leads to Compromised Graph Neural Network Explanations

## Quick Facts
- arXiv ID: 2506.04608
- Source URL: https://arxiv.org/abs/2506.04608
- Reference count: 40
- Key outcome: Graph symmetrization discards directional information, degrading explanation fidelity; preserving edge directionality via Laplacian normalization improves explanation quality on both synthetic and real-world graphs.

## Executive Summary
This paper demonstrates that the standard graph symmetrization preprocessing step in GNNs discards critical directional information, leading to compromised explanation quality. The authors provide theoretical analysis showing that symmetrization reduces structural complexity and mutual information, and empirically validate that preserving edge directionality via Laplacian normalization significantly improves explanation fidelity. Their findings highlight the importance of direction-aware explainability for trustworthy GNN deployment in critical applications.

## Method Summary
The paper compares two graph preprocessing methods: symmetric relaxation (converting directed adjacency A to undirected A + A^T) and Laplacian normalization (preserving asymmetric structure). The authors train GCN-based models on both preprocessed versions, then apply post-hoc explainers (GNNExplainer, PGExplainer) to generate explanations. They evaluate using synthetic graphs with ground-truth explanations (AUC) and real-world citation networks (Fidelity, Characterization Score), demonstrating that Laplacian normalization consistently yields better explanations when directionality carries semantic meaning.

## Key Results
- DiLink-Motif explanation AUC improves from 0.793 (symmetric) to 0.914 (Laplacian) with GNNExplainer
- BA-Shapes remains stable at ~0.925 AUC for both methods, confirming undirected cases see no harm
- Real-world citation networks show moderate improvements in Fidelity and Characterization Score when preserving directionality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Symmetrization reduces structural complexity, degrading explanation quality.
- Mechanism: Converting A to A + A^T removes asymmetric edge patterns. Von Neumann entropy Hᵥ(G) ≥ Hᵥ(Gᵤ) formally captures this reduction—meaningful directional structures collapse into undirected connections.
- Core assumption: Directional patterns encode task-relevant semantics (true in financial, citation, and flow-based domains).
- Evidence anchors:
  - [abstract] "common practice of graph symmetrization discards directional information, leading to significant information loss and misleading explanations"
  - [section 3.1] Definition 3.1 and inequality (5): Hᵥ(G) ≥ Hᵥ(Gᵤ)
  - [corpus] SVDformer (arXiv:2508.13435) notes directed graphs model asymmetric relationships but existing methods "struggle to jointly capture directional semantics"
- Break condition: If the original graph is truly undirected, symmetrization causes no harm—empirically confirmed by BA-Shapes results.

### Mechanism 2
- Claim: Mutual information between subgraph explanations and predictions is upper-bounded by directional information preservation.
- Mechanism: Explanation methods maximize MI(Y, Gₛ). Theorem 3.3 proves max MI(Y, Gₛ) ≥ max MI(Y, Gᵤₛ) because symmetrization conditions the subgraph space, reducing H(Y|Gₛ) to H(Y|Gᵤₛ).
- Core assumption: The ground-truth explanation depends on directed paths (e.g., money flows from source to destination).
- Evidence anchors:
  - [section 3.2] Theorem 3.3 (Directional Semantic Gain) with proof via conditional entropy inequality
  - [section 4] DiLink-Motif AUC improves from 0.793 to 0.914 with GNNExplainer
  - [corpus] Flow Matters (arXiv:2509.00772) shows directional modeling improves classification on heterophilic graphs
- Break condition: If prediction depends only on node features or undirected structure, directional gain is negligible.

### Mechanism 3
- Claim: Laplacian normalization enables spectral GNNs to operate on directed graphs without symmetric approximation.
- Mechanism: Standard GCN requires symmetric Laplacian L = D^{-1/2}AD^{-1/2}. Laplacian normalization (referencing Tong et al. 2020, DiGCN) constructs a directed Laplacian that preserves asymmetric adjacency structure.
- Core assumption: Directed Laplacian decomposition remains numerically stable for the graph's spectral properties.
- Evidence anchors:
  - [section 2] "Laplacian Normalization (Lap-Norm) [14], a novel method that preserves asymmetric structural relationships"
  - [section 4 Table 1] Laplacian normalization matches or exceeds symmetric relaxation across all synthetic datasets
  - [corpus] Sheaves Reloaded (arXiv:2506.02842) discusses directional sheaf neural networks as GNN generalization
- Break condition: Highly irregular directed graphs with poor spectral concentration may require alternative approaches.

## Foundational Learning

- Concept: **Mutual Information for Explanation Objective**
  - Why needed here: GNN explainers formulate explanation as maximizing MI(Y, Gₛ)—understanding this connects preprocessing choices to explanation quality.
  - Quick check question: If MI(Y, Gₛ) = 0, what does that imply about the subgraph's relationship to the prediction?

- Concept: **Graph Laplacian and Spectral Filtering**
  - Why needed here: Understanding why GCN requires symmetric inputs explains why symmetrization became standard practice.
  - Quick check question: Why does an asymmetric Laplacian complicate eigenvalue-based convolution?

- Concept: **Post-hoc Explanation Fidelity Metrics (Fid⁺, Fid⁻, Char)**
  - Why needed here: The paper evaluates using Fidelity and Characterization Score—knowing these ensures you can reproduce and extend experiments.
  - Quick check question: Should a good explanation have high Fid⁺ and high Fid⁻, or high Fid⁺ and low Fid⁻?

## Architecture Onboarding

- Component map: Graph Preprocessing -> Base GNN Model -> Explainer Module -> Evaluation Layer
- Critical path: Preprocessing choice -> GNN training -> Explainer optimization -> Fidelity evaluation. The preprocessing decision at step 1 irreversibly constrains explanation quality at step 4.
- Design tradeoffs:
  - Symmetric relaxation: Broader model compatibility but discards directional semantics
  - Laplacian normalization: Preserves directionality but may require architectural adjustments
  - Spatial methods (GAT, GraphSAGE): Theoretically handle direction but under-evaluated on directed benchmarks
- Failure signatures:
  - Explanation AUC drops significantly on directed datasets while model accuracy remains acceptable → likely symmetrization issue
  - Explainer highlights bi-directional paths when domain expects unidirectional flow → directionality lost
  - Fid⁺ and Fid⁻ both high → explanation captures spurious correlations rather than causal structure
- First 3 experiments:
  1. Baseline replication: Train GCN with symmetric relaxation on BA-Shapes, verify GNNExplainer achieves ~0.92 AUC
  2. Directed stress test: Construct simple directed graph where explanation must identify unidirectional path; compare symmetric vs. Laplacian preprocessing
  3. Real-world validation: Apply both preprocessing methods to Cora citation network using PGExplainer, report Fid⁺/Fid⁻/Char deltas

## Open Questions the Paper Calls Out
1. How can direction-aware explanation frameworks be effectively generalized to complex structures like hypergraphs and heterogeneous networks? The current theoretical analysis and empirical validation are restricted to standard directed and undirected graphs.
2. Can differential privacy be integrated into direction-aware GNN pipelines without compromising the gains in explanation fidelity? Adding noise for privacy often reduces data utility; it is unknown if this would negate the structural information recovered by preserving directionality.
3. What specific visualization tools or symbolic reasoning methods best enhance human accessibility for directed graph explanations? While the paper improves explanation fidelity, it does not address the cognitive gap for human users interpreting these directional subgraphs.

## Limitations
- The primary limitation is the assumption that directional semantics are task-relevant across domains, validated on flow-based synthetic graphs and citation networks but not systematically tested on semantically neutral networks.
- Laplacian normalization implementation details are not fully specified, potentially affecting reproducibility across different graph structures.
- The evaluation focuses on post-hoc explainers (GNNExplainer, PGExplainer) without exploring whether directional preservation benefits other explanation paradigms.

## Confidence
- **High confidence**: The theoretical analysis connecting symmetrization to information loss (Mechanisms 1-2) is mathematically rigorous and well-supported by proofs.
- **Medium confidence**: The empirical gains on real-world datasets are promising but could benefit from larger-scale validation across diverse graph types and explanation methods.
- **Medium confidence**: The claim that Laplacian normalization is universally superior to symmetrization assumes stable spectral properties across all directed graphs, which requires further validation.

## Next Checks
1. Apply both preprocessing methods to a domain where directionality is known to be semantically neutral (e.g., undirected social networks) to confirm the theoretical prediction that gains should be minimal.
2. Test directional vs. symmetric preprocessing with GraphSVX or GraphLIME to determine if the benefit extends beyond gradient-based explainers.
3. Compare different directed Laplacian normalization approaches (PageRank-based, random walk-based) to isolate which preservation mechanism drives the performance gains.