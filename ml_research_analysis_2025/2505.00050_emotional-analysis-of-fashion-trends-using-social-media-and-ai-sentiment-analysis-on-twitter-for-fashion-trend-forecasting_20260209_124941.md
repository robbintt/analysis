---
ver: rpa2
title: 'Emotional Analysis of Fashion Trends Using Social Media and AI: Sentiment
  Analysis on Twitter for Fashion Trend Forecasting'
arxiv_id: '2505.00050'
source_url: https://arxiv.org/abs/2505.00050
tags:
- fashion
- sentiment
- trend
- social
- media
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research addresses the challenge of forecasting fashion trends
  using social media sentiment analysis. The study applies natural language processing
  and machine learning to Twitter data, developing a framework for extracting fashion-related
  content, performing sentiment classification with improved normalization, conducting
  time series decomposition, and establishing causal relationships through Granger
  causality testing.
---

# Emotional Analysis of Fashion Trends Using Social Media and AI: Sentiment Analysis on Twitter for Fashion Trend Forecasting

## Quick Facts
- arXiv ID: 2505.00050
- Source URL: https://arxiv.org/abs/2505.00050
- Authors: Aayam Bansal; Agneya Tharun
- Reference count: 40
- Sentiment classification achieved 78.35% balanced accuracy with improved normalization techniques

## Executive Summary
This research develops a comprehensive framework for forecasting fashion trends using Twitter sentiment analysis. The study applies natural language processing and machine learning to analyze 40,094 fashion-related tweets, developing improved sentiment normalization techniques that reduce positive skew and yield more reliable trend classification. The methodology combines fashion theme identification, sentiment analysis with enhanced normalization, time series decomposition, and Granger causality testing to track and forecast fashion trend trajectories. Statistical analysis reveals accessories and streetwear as statistically significant rising trends, with sustainability and streetwear identified as primary trend drivers through causal analysis.

## Method Summary
The methodology involves merging T4SA sentiment data with complementary tweet text, filtering for fashion-related content using 23 keywords, and applying multilabel keyword-based classification across seven fashion themes. Sentiment scores undergo improved normalization using a sigmoid transformation with neutrality dampening to create balanced sentiment categories. Time series analysis employs synthetic timestamps, weekly aggregation, and additive decomposition with a 13-week period to extract trend components. ARIMA/SARIMA forecasting and Granger causality testing identify predictive relationships between themes. Random Forest classification with TF-IDF features and stratified cross-validation produces the final sentiment predictions.

## Key Results
- Improved predictive model achieved 78.35% balanced accuracy in sentiment classification across positive, neutral, and negative categories
- Accessories and streetwear themes showed statistically significant rising trends
- Granger causality analysis identified sustainability and streetwear as primary trend drivers with bidirectional relationships
- Cross-platform analysis revealed platform-specific sentiment patterns with clear brand-specific sentiment hierarchies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Improved sentiment normalization reduces positive skew in fashion-related Twitter sentiment scores, yielding more reliable trend classification.
- Mechanism: A sigmoid transformation (`tanh(2 × (POS − NEG)) × (1 − NEU × 0.7)`) compresses extreme values while a neutrality dampening factor reduces signal strength when tweets exhibit high neutral scores. This shifts the distribution from 77–84% "very positive" toward a balanced spread with 73.6% neutral, 20.6% positive, and 5.8% negative.
- Core assumption: Fashion discourse sentiment follows a distribution where most content is informational rather than emotionally extreme, and extreme skew in precomputed scores reflects annotation artifacts rather than genuine sentiment patterns.
- Evidence anchors:
  - [abstract]: "sentiment classification with improved normalization techniques"
  - [section III.C]: Details sigmoid transformation formula and rebalanced categorization thresholds; Figure 1 shows distribution shift.
  - [corpus]: Weak direct corpus support; neighbor papers use sentiment analysis but do not address normalization techniques specifically.
- Break condition: If neutral-class prevalence exceeds ~85% after normalization, the dampening factor may be over-suppressing genuine sentiment variance, requiring threshold recalibration.

### Mechanism 2
- Claim: Granger causality testing identifies directional predictive relationships between fashion themes that go beyond simple correlation.
- Mechanism: After stationarity transformation, pairwise regression tests whether lagged values of theme X improve predictions of theme Y beyond Y's own history. Multi-lag analysis (1–4 weeks) captures both immediate and delayed effects. Significance at p < 0.05 establishes that one theme's past discourse volume/sentiment predicts another's future trajectory.
- Core assumption: Time series constructed from synthetic timestamps (due to missing actual dates) sufficiently approximate real temporal ordering for causal inference. The paper acknowledges this as a limitation.
- Evidence anchors:
  - [abstract]: "Granger causality analysis establishes sustainability and streetwear as primary trend drivers, showing bidirectional relationships"
  - [section III.G]: Full methodology for Granger testing with stationarity transformation; Table VI shows validated relationships (e.g., Streetwear → Vintage, p < 2.1×10⁻⁵).
  - [corpus]: Neighbor paper "Sentiment Analysis of Social Media Data for Predicting Consumer Behavior Trends" applies ML to Twitter sentiment but does not use Granger causality; limited external validation of this specific approach in fashion domain.
- Break condition: If Augmented Dickey-Fuller tests show non-stationarity that differencing cannot resolve, Granger results become unreliable. Also breaks if synthetic timestamps do not reflect actual temporal patterns.

### Mechanism 3
- Claim: Time series decomposition separates genuine trend direction from seasonal cycles and residual noise, enabling statistically validated trend classification.
- Mechanism: Additive decomposition (Yt = Tt + St + Rt) with 13-week period extracts quarterly fashion cycle seasonality. Linear regression on trend components with p-value thresholds classifies trends as "Strongly/Moderately/Slightly Rising/Falling" or "Stable." This prevented misclassification of minimalist fashion (initially "Strongly Rising," validated as "Slightly Falling" with p=0.0112, R²=0.080).
- Core assumption: A 13-week period accurately captures fashion seasonality aligned with fashion weeks and retail cycles; residual spikes represent unpredictable external events (celebrity endorsements, brand announcements).
- Evidence anchors:
  - [abstract]: "accessories and streetwear themes showing statistically significant rising trends"
  - [section III.D-IV.D]: Decomposition methodology; Figures 2-4 show sustainability, luxury, vintage components; Table V validates trend significance.
  - [corpus]: No direct corpus validation for 13-week decomposition period in fashion contexts.
- Break condition: If R² values remain below 0.05 even with significant p-values, trend magnitude may be too weak for actionable forecasting.

## Foundational Learning

- Concept: **TF-IDF Vectorization with N-grams**
  - Why needed here: Converts raw tweet text into numerical features capturing both single terms and two-word phrases for sentiment classification.
  - Quick check question: Can you explain why n-grams (1-2) might capture sentiment signals that unigrams miss in fashion discourse?

- Concept: **Stationarity and Differencing**
  - Why needed here: Both ARIMA forecasting and Granger causality require stationary time series (constant mean/variance over time).
  - Quick check question: If a fashion theme's mention count grows 10% monthly, what differencing order (d) would ARIMA likely require?

- Concept: **Balanced Accuracy vs. Raw Accuracy**
  - Why needed here: With sentiment classes imbalanced (5.8% negative, 73.6% neutral, 20.6% positive), raw accuracy (92%) masks poor minority-class performance. Balanced accuracy (78.35%) accounts for per-class recall.
  - Quick check question: A classifier achieves 95% accuracy on a dataset with 90% negative examples. Why is this metric potentially misleading?

## Architecture Onboarding

- Component map: Data ingestion -> Theme labeling -> Sentiment scoring -> Temporal analysis -> Causal inference -> Predictive modeling
- Critical path: Fashion keyword extraction → sentiment normalization → time series construction → statistical validation. Errors in keyword filtering propagate through all downstream analysis.
- Design tradeoffs:
  - Keyword-based theme extraction (simple, interpretable) vs. ML classifier (more nuanced, requires labeled training data)
  - Synthetic timestamps (enables temporal analysis) vs. actual timestamps (unavailable, limits causal reliability)
  - Random Forest (handles non-linear relationships, high-dimensional data) vs. deep learning (higher capacity, less interpretable)
- Failure signatures:
  - Sentiment distribution remains heavily skewed after normalization → check for hashtag/mention noise in text preprocessing
  - Granger tests return all non-significant → verify stationarity transformation, inspect lag selection
  - Trend significance contradicts visual inspection → examine R² values; low R² with significant p-value indicates weak effect
- First 3 experiments:
  1. Validate sentiment normalization on a held-out subset: manually label 200 tweets, compare original vs. improved compound scores against human annotations.
  2. Test temporal stability: randomly shuffle synthetic timestamps within seasonal windows, rerun Granger analysis to assess sensitivity to timestamp artifacts.
  3. Benchmark theme extraction: compare keyword-based labeling against a small BERT-based classifier on 500 tweets to estimate precision/recall gaps.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do seasonal fashion trend patterns identified via time series decomposition change when using authentic temporal metadata compared to synthetic timestamp simulation?
- Basis in paper: [explicit] The authors state in Section V.F that their temporal analysis "relied on simulated timestamps rather than actual posting dates" and that future work must incorporate "genuine temporal data to validate these patterns."
- Why unresolved: The T4SA dataset used lacked timestamps, forcing the authors to simulate a temporal dimension based on general fashion industry cycles (e.g., fashion weeks), which may not reflect the actual timing of consumer discourse.
- What evidence would resolve it: Replicating the analysis on a dataset with verified timestamps to compare the resulting seasonal components against the simulated findings.

### Open Question 2
- Question: To what extent do the identified Granger-causal relationships between social media sentiment themes (e.g., sustainability driving streetwear) predict actual retail sales or market performance?
- Basis in paper: [explicit] Section V.F notes that while Granger causality suggests predictive relationships, "establishing true causality would require... longitudinal validation against market data."
- Why unresolved: The study establishes that one social media topic can predict another, but it does not verify if these social signals precede or correlate with real-world economic purchasing behavior.
- What evidence would resolve it: A longitudinal study correlating the rise of specific sentiment themes with corresponding sales figures or search volume data over the same period.

### Open Question 3
- Question: Does the integration of computer vision techniques for analyzing images alongside text improve the accuracy of fashion trend forecasting beyond the text-only approach used in this study?
- Basis in paper: [explicit] Section V.F lists "Visual Component Exclusion" as a limitation, noting that "incorporating image analysis through computer vision techniques would provide a more complete understanding."
- Why unresolved: Fashion is a highly visual domain, yet the model relies exclusively on textual features (hashtags, keywords), potentially missing critical trend signals conveyed only through images.
- What evidence would resolve it: A multimodal study that combines the existing NLP framework with image classification to measure performance differences in trend prediction.

### Open Question 4
- Question: Do authentic cross-platform sentiment hierarchies match the simulated platform-specific patterns observed in this study (e.g., Instagram showing higher positivity than Reddit)?
- Basis in paper: [explicit] Section V.F states that the cross-platform analysis "used simulated data rather than actual multi-platform collection," limiting the reliability of the platform-specific conclusions.
- Why unresolved: The authors modeled sentiment for platforms like Instagram and TikTok based on "established platform characteristics" rather than analyzing raw data collected from those specific networks.
- What evidence would resolve it: Scraping and analyzing authentic fashion-related posts from Twitter, Instagram, TikTok, and Reddit to verify if the predicted sentiment hierarchies hold true.

## Limitations

- Reliance on synthetic timestamps due to missing actual dates significantly limits the validity of causal inferences from Granger causality testing
- Keyword-based fashion theme extraction may miss nuanced theme associations that machine learning classifiers could capture
- Cross-platform analysis findings based on simulated data rather than authentic multi-platform collection

## Confidence

- **High Confidence**: Sentiment classification performance metrics (78.35% balanced accuracy, macro F1 = 0.81)
- **Medium Confidence**: Trend identification for accessories and streetwear - statistically significant but dependent on synthetic timestamp construction
- **Medium Confidence**: Granger causality relationships - methodology is sound but temporal data limitations reduce reliability
- **Low Confidence**: Cross-platform analysis findings - synthetic data generation methodology for other platforms not fully specified

## Next Checks

1. Implement a small-scale manual annotation study (200-300 tweets) to validate the improved sentiment normalization formula against human judgments, comparing both original and improved compound scores
2. Conduct sensitivity analysis on synthetic timestamp generation by varying seasonal distribution weights and observing changes in Granger causality significance patterns
3. Benchmark keyword-based theme extraction against a BERT-based multilabel classifier on a held-out sample to quantify precision/recall gaps and assess whether ML-based extraction would yield materially different trend forecasts