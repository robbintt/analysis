---
ver: rpa2
title: 'CRAM: Large-scale Video Continual Learning with Bootstrapped Compression'
arxiv_id: '2508.05001'
source_url: https://arxiv.org/abs/2508.05001
tags:
- video
- learning
- task
- memory
- codes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses catastrophic forgetting in video continual
  learning by compressing long videos into neural codes and storing them in a rehearsal
  buffer. The key innovation is a code-refreshing scheme that maintains representation
  stability by carefully decompressing and recompressing stored codes with updated
  models.
---

# CRAM: Large-scale Video Continual Learning with Bootstrapped Compression

## Quick Facts
- **arXiv ID:** 2508.05001
- **Source URL:** https://arxiv.org/abs/2508.05001
- **Reference count:** 40
- **Primary result:** Outperforms prior art on long-video continual learning while using <2GB memory.

## Executive Summary
CRAM addresses catastrophic forgetting in video continual learning by compressing long videos into neural codes and storing them in a rehearsal buffer. The key innovation is a code-refreshing scheme that maintains representation stability by carefully decompressing and recompressing stored codes with updated models. Experiments on Epic-Kitchens-100 and Kinetics-700 show that the method outperforms prior art while using less than 2GB memory, even for videos over 10x longer than previously demonstrated.

## Method Summary
CRAM compresses video clips into neural codes using a VQ-VAE encoder-decoder pair and stores these codes in a rehearsal buffer instead of raw frames. During incremental learning, it refreshes stored codes by decoding them with a previous decoder snapshot and re-encoding with the current encoder, preventing representation drift. The method supports both online learning (training compressor from scratch) and pre-trained settings (freezing the compressor after initial training), with the latter offering greater stability.

## Key Results
- Achieves state-of-the-art performance on Epic-Kitchens-100 and Kinetics-700 with less than 2GB memory usage
- Handles videos over 10x longer than previous methods (20 minutes average vs. 10 seconds)
- Outperforms compressed rehearsal baselines by maintaining representation stability through code refreshing

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Storing compressed neural codes instead of raw RGB pixels enables effective rehearsal for long-video Continual Learning (CL) under strict memory constraints (< 2GB), provided the classifier can operate in the latent space.
- **Mechanism:** A VQ-VAE compressor ($c = \phi, \psi$) reduces video clips into compact codes. The rehearsal buffer stores these codes ($e_{j,k}$) rather than high-dimensional frames. This allows the buffer to retain temporal information for videos that are 10x longer than raw-video baselines permit, preventing the "data starvation" that causes catastrophic forgetting.
- **Core assumption:** The compressed latent space retains sufficient semantic information for the downstream classification task, and reconstruction quality is sufficient for feature stability.
- **Evidence anchors:** [Abstract] "...store video codes (embeddings) instead of raw inputs... storing thousands of relatively long videos in under 2 GB." [Section 4.2.1] Defines the buffer $B_{i-1}$ as containing codes $e_{j,k} = \phi_{t-1}(x_{j,k})$.

### Mechanism 2
- **Claim:** "Code Refreshing" prevents representation drift by aligning stale codes (generated by previous encoders) with the current encoder's manifold using a single previous decoder snapshot.
- **Mechanism:** As the compressor trains online, the meaning of latent codes shifts (drift). To fix this without storing all previous compressor versions (which grows memory linearly), CRAM uses a "bootstrap" step. It decodes stored codes using the decoder from step $t-1$ ($\psi_{t-1}$) to get pixel-space reconstructions, then immediately recompresses them with the new encoder $\phi_t$. This "refreshes" the buffer to match the current model.
- **Core assumption:** The reconstruction from the previous decoder ($\psi_{t-1}$) is accurate enough that re-encoding it with the new encoder ($\phi_t$) produces a valid representation for the new classifier, effectively bridging the temporal gap.
- **Evidence anchors:** [Abstract] "...refreshing video codes, which requires careful decompression with a previous version of the network and recompression with a new one." [Section 4.2.2] Equations 7 and 8 explicitly define the decompression of buffer samples using $\psi_1$ and recompression with $\phi_2$.

### Mechanism 3
- **Claim:** Freezing the compressor after a pre-training phase eliminates representation drift entirely, removing the need for code refreshing and improving stability at the cost of adaptability.
- **Mechanism:** If the model is pre-trained on a subset of classes (e.g., Kinetics-400) before the CL stream starts, the compressor learns general visual features. By freezing $\phi$ and $\psi$ during the incremental phase, the distribution of the latent codes remains stationary. This simplifies the CL problem to a linear (or lightweight) classifier adaptation problem.
- **Core assumption:** The pre-training dataset is representative enough of the visual world that the frozen features are sufficient for novel classes encountered later (transferability).
- **Evidence anchors:** [Section 4.3] "In the first phase, we pre-train the compressor... and in the second phase... freeze the compressor... circumvent these issues [representation drift]."

## Foundational Learning

- **Concept: VQ-VAE (Vector Quantized Variational Autoencoder)**
  - **Why needed here:** CRAM relies on compressing video into discrete codes. Understanding VQ-VAE explains *how* the video is reduced to codes and why a decoder is available to "refresh" them.
  - **Quick check question:** How does the decoder reconstruct a video from a discrete latent code during the refresh step?

- **Concept: Catastrophic Forgetting & Rehearsal Buffers**
  - **Why needed here:** The core problem being solved. One must understand that neural networks tend to overwrite weights for old tasks when trained on new data, and "rehearsal" (re-playing old data) is the primary defense.
  - **Quick check question:** Why does simply storing compressed codes solve the memory issue but create a "representation drift" issue?

- **Concept: Representation Drift**
  - **Why needed here:** As the online compressor updates, the "meaning" of a latent vector changes. This is the specific problem the "refresh" mechanism solves.
  - **Quick check question:** If you encode an image with Encoder V1, can you use the resulting code directly with Classifier V5? (Answer: Not without refresh/alignment).

## Architecture Onboarding

- **Component map:** Encoder ($\phi$) -> Decoder ($\psi$) -> Classifier ($q$) -> Buffer ($B$)
- **Critical path:**
  1. **Input:** Receive video clip $x$ and label $y$ for task $t$.
  2. **Encode:** Compress $x \to e$ using current $\phi$.
  3. **Refresh (if online):** Retrieve old codes from $B$. Decode with old $\psi_{old}$, re-encode with new $\phi_{new}$.
  4. **Train:** Update Classifier $q$ (and Compressor $\phi, \psi$ if online) using new batch + refreshed buffer samples.
  5. **Store:** Add new codes to $B$.

- **Design tradeoffs:**
  - **Online vs. Pre-trained:** Online (scratch) allows learning new visual domains but requires expensive refreshing and is less stable. Pre-trained is stable/cheap but may fail on out-of-distribution visual data.
  - **Compression Rate:** 256× is standard. Lower rates cost more memory; higher rates lose semantic detail (Sec 5.8).

- **Failure signatures:**
  - **Flickering/Drift:** If accuracy on old tasks drops rapidly, check if "Code Refreshing" is actually running. A common bug is using the *current* decoder to decode old codes instead of the *stored* previous decoder.
  - **Memory Overflow:** If buffer exceeds 2GB, check if raw frames are being stored accidentally instead of codes.

- **First 3 experiments:**
  1. **Sanity Check (IID):** Train Compressor + Classifier on all data at once (no CL) to establish the "Upper Bound" accuracy (see Table 1).
  2. **Baseline (Drift):** Train incrementally with a compressed buffer but *disable* code refreshing. Observe the drop in accuracy (AvgF) to quantify the drift penalty.
  3. **Full Ablation:** Run full CRAM (refresh enabled) on Kinetics-700. Verify memory stays <1.5GB and accuracy matches or exceeds Table 1.

## Open Questions the Paper Calls Out
- **Question 1:** Can the CRAM framework be effectively extended to dense video understanding tasks, such as object detection or temporal segmentation, which require localization rather than global classification?
- **Question 2:** What is the specific computational overhead (training time and latency) introduced by the bootstrapped decompression-recompression cycle compared to standard rehearsal buffers?
- **Question 3:** Can the stability of the online compressor in the "incremental from scratch" setting be improved to close the performance gap with the pre-trained setting?

## Limitations
- The S3D classifier's adaptation to compressed codes follows external specifications not detailed in this paper
- The buffer management strategy (reservoir sampling vs. FIFO) is not explicitly stated
- Pre-trained frozen compressor may fail on visually distinct domains due to transferability assumptions

## Confidence
- **High Confidence:** The memory efficiency claim (<2GB for long videos) is directly supported by the compression mechanism and experimental memory measurements
- **Medium Confidence:** The superiority over prior art (AvgF scores) is demonstrated, but the ablation study isolating the code refreshing mechanism's contribution is not fully explicit
- **Low Confidence:** The claim that freezing the compressor eliminates representation drift entirely assumes perfect transferability from pre-training data

## Next Checks
1. **Ablation Validation:** Implement and compare three variants: (a) with code refreshing enabled, (b) with refreshing disabled, and (c) with a frozen pre-trained compressor. Measure the exact contribution of each component to average forgetting.
2. **Out-of-Distribution Test:** Evaluate the pre-trained frozen compressor variant on a dataset with visual domains significantly different from Kinetics-400 (e.g., medical or satellite imagery) to test the transferability assumption.
3. **Compression Rate Sensitivity:** Systematically vary the compression ratio (e.g., 128×, 256×, 512×) and measure the tradeoff between memory usage and classification accuracy to identify the break condition where semantic information is lost.