---
ver: rpa2
title: 'MiLe Loss: a New Entropy-Weighed Loss for Mitigating the Bias of Learning
  Difficulties in Large Language Models'
arxiv_id: '2310.19531'
source_url: https://arxiv.org/abs/2310.19531
tags:
- loss
- language
- mile
- tokens
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the issue of token imbalance in language model
  pretraining, where frequent tokens are learned well but infrequent ones are overlooked,
  leading to performance degradation. To mitigate this bias, the authors propose MiLe
  Loss, a new loss function that dynamically assesses token learning difficulty using
  information entropy and scales the training loss adaptively to focus more on difficult-to-learn
  tokens.
---

# MiLe Loss: a New Entropy-Weighed Loss for Mitigating the Bias of Learning Difficulties in Large Language Models

## Quick Facts
- **arXiv ID**: 2310.19531
- **Source URL**: https://arxiv.org/abs/2310.19531
- **Reference count**: 24
- **Primary result**: MiLe Loss improves LLM pretraining by adaptively focusing on difficult tokens, achieving up to 4.17% gains on TriviaQA and 1.05% average improvement on commonsense reasoning tasks

## Executive Summary
This paper addresses the token imbalance problem in large language model pretraining, where frequent tokens are learned well but infrequent ones are overlooked. The authors propose MiLe Loss, a new loss function that uses information entropy to dynamically assess token learning difficulty and adaptively scales the training loss to focus more on difficult-to-learn tokens. Experiments on the Pile dataset with models ranging from 468M to 6.7B parameters demonstrate consistent improvements over Cross-Entropy and Focal Loss across various downstream benchmarks, including commonsense reasoning, closed-book QA, and MMLU.

## Method Summary
MiLe Loss introduces a novel approach to mitigating token imbalance by using information entropy to measure how well each token is being learned during training. The method calculates token-level entropy from the model's probability distribution and uses a threshold (0.75) to identify difficult tokens that the model struggles to predict. For these difficult tokens, the loss is scaled up to provide additional training focus, while easy tokens receive reduced loss weight. This dynamic adaptation allows the model to allocate training resources more effectively across the vocabulary, addressing the inherent bias in standard cross-entropy loss that favors frequent tokens.

## Key Results
- MiLe Loss consistently outperforms Cross-Entropy and Focal Loss across multiple model scales (468M, 1.2B, 6.7B parameters)
- On 6.7B-parameter model, achieves up to 1.05% average improvement on commonsense reasoning tasks
- Demonstrates up to 4.17% gains on TriviaQA in zero-shot settings
- Shows robust performance across diverse downstream benchmarks including MMLU, closed-book QA, and commonsense reasoning

## Why This Works (Mechanism)
MiLe Loss works by recognizing that traditional cross-entropy loss creates a learning bias where frequent tokens dominate the gradient updates, leaving infrequent but semantically important tokens undertrained. By using entropy as a measure of prediction confidence, the method can identify tokens that the model finds challenging and increase their importance in the loss calculation. This creates a dynamic curriculum where the model naturally spends more time on difficult concepts, leading to more balanced learning across the entire vocabulary.

## Foundational Learning

**Information Entropy**
- Why needed: Measures uncertainty in probability distributions to identify difficult tokens
- Quick check: Calculate entropy for a simple probability distribution to verify understanding

**Token Frequency Distribution**
- Why needed: Understanding how token imbalance affects learning dynamics
- Quick check: Plot token frequency vs rank to visualize Zipf's law in real datasets

**Cross-Entropy Loss**
- Why needed: Baseline loss function being improved upon
- Quick check: Implement cross-entropy for a simple classification problem

**Focal Loss**
- Why needed: Alternative method for addressing class imbalance
- Quick check: Compare focal loss behavior with different focusing parameters

## Architecture Onboarding

**Component Map**
Input tokens -> Embedding Layer -> Transformer Blocks -> Logits -> Entropy Calculation -> Loss Scaling -> Total Loss

**Critical Path**
The critical path involves calculating entropy from the softmax probabilities, comparing against threshold, applying scaling factor, and computing final loss. This must happen after each forward pass but before backpropagation.

**Design Tradeoffs**
- Dynamic vs static difficulty assessment: MiLe uses real-time entropy vs predetermined frequency-based weighting
- Computational overhead vs performance gain: Entropy calculations add computation but improve learning efficiency
- Threshold selection: Fixed threshold (0.75) vs adaptive thresholds for different training stages

**Failure Signatures**
- Vanishing gradients if entropy threshold too high
- Overfitting to rare tokens if scaling factor too aggressive
- Degraded performance if entropy calculation implemented incorrectly

**First Experiments**
1. Compare entropy distributions between easy and hard tokens on a small dataset
2. Implement MiLe Loss on a simple language model (few million parameters) with synthetic data
3. Measure training speed impact of entropy calculations vs standard cross-entropy

## Open Questions the Paper Calls Out
None

## Limitations
- Limited analysis of entropy dynamics throughout entire training process (only single checkpoint examined)
- Fixed entropy threshold (0.75) chosen without ablation studies or sensitivity analysis
- Computational overhead of entropy calculations not quantified in terms of training time or memory
- Evaluation focuses primarily on zero-shot/few-shot settings with limited discussion of fine-tuning performance

## Confidence

**High confidence**: The core mechanism of using entropy to identify difficult tokens and adaptively scale loss is clearly defined and correctly implemented.

**Medium confidence**: Empirical improvements over Cross-Entropy and Focal Loss are consistently observed across multiple scales, though magnitude varies by task.

**Low confidence**: The claim of "consistent" outperformance across all settings may overstate results, given modest improvements (0.07% on Hellaswag for 1.2B model).

## Next Checks

1. Conduct ablation studies on the entropy threshold (0.75) to determine sensitivity and optimal range across different datasets and model scales.

2. Analyze entropy evolution across multiple training checkpoints to understand how token difficulty distributions change over time and affect MiLe Loss adaptation.

3. Measure and report computational overhead introduced by entropy calculations in terms of training time and memory usage compared to standard Cross-Entropy Loss.