---
ver: rpa2
title: 'Beyond the limitation of a single query: Train your LLM for query expansion
  with Reinforcement Learning'
arxiv_id: '2510.10009'
source_url: https://arxiv.org/abs/2510.10009
tags:
- search
- query
- expansion
- information
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of single-query search agents
  in multi-hop reasoning tasks by training a query expansion capability via reinforcement
  learning. The method generates multiple semantically-enriched query variants in
  parallel to maximize retrieval recall, then uses a squeezer model to distill retrieved
  content into reasoning-relevant summaries.
---

# Beyond the limitation of a single query: Train your LLM for query expansion with Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.10009
- Source URL: https://arxiv.org/abs/2510.10009
- Reference count: 18
- Primary result: Query expansion via RL improves multi-hop QA by 4.4% average EM over state-of-the-art

## Executive Summary
This paper addresses the limitations of single-query search agents in multi-hop reasoning tasks by training a query expansion capability via reinforcement learning. The method generates multiple semantically-enriched query variants in parallel to maximize retrieval recall, then uses a squeezer model to distill retrieved content into reasoning-relevant summaries. Experiments across seven QA benchmarks show the approach achieves an average 4.4% improvement over state-of-the-art baselines, with particularly strong gains on multi-hop reasoning tasks. Notably, even a 3B-parameter model outperforms larger 7B baselines when equipped with this query expansion capability.

## Method Summary
The method trains an LLM-based search agent with query expansion capability using reinforcement learning. The policy model generates multiple query variants (expansion) to improve retrieval recall, which are then processed by a frozen squeezer model to condense retrieved chunks into reasoning-relevant summaries. The distilled content is fed back to the policy model for final answer generation. Training uses PPO via the Search-R1 framework with veRL, optimizing for exact-match reward. The approach is evaluated on seven QA benchmarks, demonstrating significant improvements over single-query baselines.

## Key Results
- 4.4% average improvement in Exact Match accuracy across seven benchmarks
- 6.7% gain from single-query (n=1) to dual-query (n=2) expansion
- 3B-parameter models with query expansion outperform 7B baselines without it
- Performance drops significantly without the squeezer model (-8.2% EM on complex tasks)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Generating multiple query variants significantly improves retrieval recall over single-query methods.
- **Mechanism:** Parallel queries covering syntax variations and semantic breadth mitigate the semantic brittleness of single-vector embeddings, capturing more relevant information.
- **Core assumption:** Relevant evidence is sparse enough that a single embedding vector will likely miss it.
- **Evidence anchors:** Abstract states simultaneous search covers more relevant information; Section 4.3 shows consistent improvements from n=1 to n=2 expansions.

### Mechanism 2
- **Claim:** Distilling retrieved content via a frozen squeezer model prevents context overload and training instability.
- **Mechanism:** The squeezer filters noise from high-volume retrievals, preserving only reasoning-critical facts for the policy model, preventing information overload and GPU memory issues.
- **Core assumption:** Relevant evidence constitutes a small fraction of retrieved text; a frozen LLM can reliably identify and extract this signal.
- **Evidence anchors:** Abstract mentions squeezer distills content; Section 3.1 notes compressed information saves GPU memory; Section 4.4 shows dramatic performance drop without squeezer.

### Mechanism 3
- **Claim:** End-to-end RL is required to align query generation with final answer objectives.
- **Mechanism:** RL allows the model to learn the complex interplay between generating diverse queries and utilizing distilled results to maximize exact-match reward.
- **Core assumption:** The model cannot intuitively predict effective query variations without trial-and-error feedback on answer correctness.
- **Evidence anchors:** Section 4.3 shows naive expansion without RL yields no improvement; Section 1 emphasizes RL training for query expansion capability.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO)**
  - **Why needed here:** Core algorithm used to train the ExpandSearch agent, updating policy using exact-match reward without destabilizing learning.
  - **Quick check question:** How does the "clip" parameter in PPO prevent the policy from changing too drastically during a single update?

- **Concept: Dense Retrieval & Embedding Limitations**
  - **Why needed here:** Motivates expansion to fix semantic brittleness of single-vector dense retrieval where different terminology yields low similarity scores.
  - **Quick check question:** Why might a standard embedding model return low similarity scores for a document that answers a query using different terminology?

- **Concept: Context Window & "Lost in the Middle" Phenomenon**
  - **Why needed here:** Explains squeezer necessity as LLMs struggle to extract information buried in the middle of long contexts.
  - **Quick check question:** If you retrieve 50 chunks of text, why is simply concatenating them into a 10k-token prompt likely to yield worse reasoning performance than summarizing first?

## Architecture Onboarding

- **Component map:** User Query -> Policy Model (Generate `<search>q1 ## q2 ## q3</search>`) -> Search Engine (Retrieve chunks) -> Squeezer API (Distill chunks -> Summary) -> Policy Model (Read Summary -> Generate Answer) -> Reward Calculation -> PPO Update

- **Critical path:** User Query -> Policy Model (Generate query variants) -> Search Engine (Retrieve chunks for all queries) -> Squeezer API (Distill chunks to summaries) -> Policy Model (Read Summary -> Generate Answer) -> Reward Calculation (Exact Match) -> PPO Update

- **Design tradeoffs:**
  - Squeezer Size: Larger squeezers (70B) help general QA; smaller, specialized models (17B) may excel at multi-hop synthesis
  - Base vs. Instruct: 3B Instruct models outperform 3B Base; 7B Base models outperform 7B Instruct
  - Parallel Queries (n): Increasing n from 1 to 2 yields +6.7% gain; gains diminish after n=3

- **Failure signatures:**
  - Naive Expansion: Adding expansion prompts without RL training causes performance to drop
  - Squeezer Bottleneck: Weak squeezer leads to garbage summaries and hallucinated answers
  - GPU OOM: Without squeezer, context length explodes during training, causing memory errors

- **First 3 experiments:**
  1. Baseline Validation: Run Search-R1 vs. ExpandSearch (n=3, k=5) on HotpotQA to reproduce 4.4% avg improvement
  2. Ablation Study (Squeezer): Remove squeezer and feed raw chunks to policy model to verify performance drop
  3. Squeezer Substitution: Train with LLaMA-4-17b squeezer, but inference with LLaMA-3.1-8b to check policy generalization

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- Dependency on frozen, large-scale squeezer (LLaMA-4-17b) raises deployment feasibility concerns
- Only compares against single baseline (Search-R1), limiting relative superiority claims
- Optimal number of query expansions is fixed at n=3 rather than adaptive per query

## Confidence
- **High Confidence:** Multiple query expansion improves retrieval recall (4.4% average improvement)
- **Medium Confidence:** Squeezer model is essential for multi-hop tasks (demonstrated through ablation)
- **Medium Confidence:** RL training is required for query expansion (naive expansion fails without it)

## Next Checks
1. **Squeezer Generalization:** Train policy with LLaMA-4-17b squeezer, test inference with LLaMA-3.1-8b to verify generalization
2. **Retrieval-Only Ablation:** Remove squeezer entirely and feed raw chunks to policy model to confirm reported performance drop
3. **Hyperparameter Sensitivity:** Test RL training with alternative PPO clipping values to assess robustness to hyperparameter choices