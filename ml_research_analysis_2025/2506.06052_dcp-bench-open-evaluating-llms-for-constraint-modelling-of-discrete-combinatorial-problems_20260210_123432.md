---
ver: rpa2
title: 'DCP-Bench-Open: Evaluating LLMs for Constraint Modelling of Discrete Combinatorial
  Problems'
arxiv_id: '2506.06052'
source_url: https://arxiv.org/abs/2506.06052
tags:
- constraint
- modelling
- problem
- llms
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces DCP-Bench-Open, a novel benchmark for evaluating
  LLM-driven constraint modelling of discrete combinatorial problems. The dataset
  contains 164 diverse problems from the CP and OR communities, structured for solution-level
  evaluation.
---

# DCP-Bench-Open: Evaluating LLMs for Constraint Modelling of Discrete Combinatorial Problems

## Quick Facts
- arXiv ID: 2506.06052
- Source URL: https://arxiv.org/abs/2506.06052
- Reference count: 22
- Key outcome: Benchmark reveals Python-based frameworks (CPMpy, OR-Tools) outperform MiniZinc for LLM-driven constraint modelling, with inference-time compute boosting accuracy up to 91%.

## Executive Summary
This work introduces DCP-Bench-Open, a novel benchmark for evaluating LLM-driven constraint modelling of discrete combinatorial problems. The dataset contains 164 diverse problems from the CP and OR communities, structured for solution-level evaluation. The study systematically compares LLM performance across three modelling frameworks—MiniZinc (domain-specific), CPMpy, and OR-Tools (Python-based)—and evaluates the impact of system prompts and inference-time compute methods. Results show that Python-based frameworks yield higher accuracy (up to 75% for OR-Tools) compared to MiniZinc (57.3%). Adding detailed documentation and guidelines consistently improves performance. Inference-time compute methods, particularly repeated sampling combined with self-verification, further enhance accuracy, reaching up to 91%. However, multi-instance evaluation reveals that models often overfit to the default prompt instance, with strict robustness dropping by 8.7–30%. DCP-Bench-Open enables rigorous evaluation of future LLM-driven modelling approaches and highlights the importance of inference-time compute and multi-instance awareness for robust generalisation.

## Method Summary
The authors created DCP-Bench-Open by curating 164 discrete combinatorial problems from CP and OR repositories, including texts like CSPLib and MiniZinc Challenge instances. Each problem includes 2-3 data instances with known solutions. They evaluated LLMs (GPT-4, Claude 3.5 Sonnet, Llama-3.1-8B) across three frameworks (MiniZinc, CPMpy, OR-Tools) using zero-shot prompting, system prompts with documentation, and inference-time compute methods (sampling, self-verification). Performance was measured via solution accuracy against known optimal solutions, with additional multi-instance robustness testing using hidden instances.

## Key Results
- Python-based frameworks (CPMpy, OR-Tools) achieve up to 75% accuracy versus 57.3% for MiniZinc
- Detailed documentation and guidelines consistently improve performance across all frameworks
- Inference-time compute methods (repeated sampling + self-verification) boost accuracy to 91%
- Multi-instance evaluation reveals 8.7–30% drop in robustness when models face hidden instances

## Why This Works (Mechanism)
The benchmark's strength lies in its systematic evaluation of LLM constraint modelling across multiple dimensions: framework choice, prompting strategy, and inference-time compute. By including diverse problem sources and solution-level evaluation, it captures both correctness and robustness. The structured approach allows isolation of individual factors (e.g., documentation impact) while testing generalizability through multi-instance evaluation.

## Foundational Learning

**Constraint Programming Basics** - Why needed: Understanding CP concepts is essential for interpreting model structures and constraints. Quick check: Can you identify constraint types in a MiniZinc model?

**LLM Prompt Engineering** - Why needed: Different prompting strategies significantly impact model performance. Quick check: Explain the difference between zero-shot and system prompt approaches.

**Inference-Time Compute Methods** - Why needed: Sampling and self-verification are critical for improving LLM outputs. Quick check: Describe how repeated sampling with verification works.

## Architecture Onboarding

**Component Map**: Problem Repository -> Framework Parser -> LLM Generator -> Solution Validator -> Accuracy Metric

**Critical Path**: Problem Description → Framework-Specific Prompt → LLM Generation → Model Execution → Solution Verification

**Design Tradeoffs**: MiniZinc offers domain-specific expressiveness but requires specialized knowledge, while Python frameworks trade some expressiveness for broader accessibility and better LLM performance.

**Failure Signatures**: Overfitting to default instances, framework-specific syntax errors, constraint formulation errors, and timeout failures during model execution.

**First Experiments**:
1. Run single instance evaluation on a simple scheduling problem across all three frameworks
2. Compare zero-shot versus system prompt performance on a graph coloring problem
3. Test inference-time compute methods on a vehicle routing problem with known optimal solution

## Open Questions the Paper Calls Out

**Open Question 1**: Can multi-instance-aware inference-time compute methods reduce overfitting to the default prompt instance and improve Multiple Instance Accuracy (MIA)?
- Basis in paper: The authors state: "Further work on multi-instance aware prompting, for example, multi-instance inference time computation could remedy this" regarding the observed 8.7–30% drop in robustness when evaluating on hidden instances.
- Why unresolved: Current inference-time methods (sampling, self-verification) were evaluated only on single instances; whether they generalize across multiple data instances per problem remains untested.
- What evidence would resolve it: Experiments applying inference-time compute strategies (e.g., sampling across multiple instance outputs) and measuring improvement in MIA scores.

**Open Question 2**: Why does retrieval-augmented in-context learning (RAICL) fail to improve performance when combined with detailed documentation prompts, contrary to prior findings?
- Basis in paper: The authors report that RAICL "was shown ineffective for most LLMs or occasionally slightly effective" and "degraded" performance compared to zero-shot baseline, contrasting with Michailidis et al. (2024).
- Why unresolved: The paper speculates that LLMs may "utilize structured framework documentation more effectively than they generalize from retrieved examples," but this hypothesis is not empirically tested.
- What evidence would resolve it: Ablation studies isolating the interaction between documentation prompts and retrieved examples, potentially with attention analysis.

**Open Question 3**: Can supervised fine-tuning of LLMs on constraint modelling datasets outperform zero-shot and inference-time compute methods on DCP-Bench-Open?
- Basis in paper: The authors mention: "The existence of DCP-Bench-Open allows for... supervised fine-tuning of LLMs. The latter would require a training dataset of constraint models and problem descriptions."
- Why unresolved: All experiments use off-the-shelf LLMs with prompting strategies; no fine-tuning experiments were conducted.
- What evidence would resolve it: Training data construction from the benchmark and evaluation of fine-tuned models against the best inference-time compute baselines (90%+ accuracy).

**Open Question 4**: How does LLM-driven constraint modelling scale to large industrial problems with more constraints, larger data, and more elaborate descriptions than textbook problems?
- Basis in paper: The authors note: "Large-scale industrial problems typically involve more data, many constraints and objectives, and elaborate descriptions, though these are rarely publicly available."
- Why unresolved: DCP-Bench-Open problems are sourced from textbooks and research repositories with relatively small instance sizes (most solved in under 5 seconds).
- What evidence would resolve it: Curating or synthetically generating larger-scale problem instances and measuring accuracy, timeout rates, and computational cost.

## Limitations

- Benchmark relies on textbook and research repository problems, which may not reflect real-world industrial complexity
- Multi-instance evaluation reveals significant overfitting, suggesting robustness limitations
- Focus on three specific frameworks may limit generalizability to other constraint programming languages

## Confidence

- High: Comparative performance between Python-based and MiniZinc frameworks is well-established
- High: Improvement from detailed documentation and guidelines is consistently observed
- Medium: Robustness findings across multiple instances are supported but show concerning overfitting effects
- Low: Generalizability of inference-time compute benefits to other problem types or LLM models remains uncertain

## Next Checks

1. Test the benchmark across additional constraint modelling frameworks and programming languages to assess generalizability of the findings
2. Evaluate model performance on problems with varying levels of complexity and different constraint types to determine robustness boundaries
3. Compare inference-time compute methods across multiple LLM architectures (beyond GPT-4) to validate the scalability of observed improvements