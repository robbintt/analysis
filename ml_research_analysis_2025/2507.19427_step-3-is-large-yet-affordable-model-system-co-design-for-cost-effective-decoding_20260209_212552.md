---
ver: rpa2
title: 'Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective
  Decoding'
arxiv_id: '2507.19427'
source_url: https://arxiv.org/abs/2507.19427
tags:
- attention
- step-3
- decoding
- hardware
- dsv3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Step-3, a 321B-parameter large language model
  optimized for cost-effective decoding through hardware-aware model-system co-design.
  The key innovations are a Multi-Matrix Factorization Attention (MFA) mechanism that
  reduces KV cache size and computation while maintaining expressiveness, and an Attention-FFN
  Disaggregation (AFD) distributed inference system that decouples attention and FFN
  layers into specialized subsystems.
---

# Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding

## Quick Facts
- arXiv ID: 2507.19427
- Source URL: https://arxiv.org/abs/2507.19427
- Reference count: 38
- 321B-parameter model achieves up to 4,039 tokens/second/GPU under 50ms TPOT SLA on Hopper GPUs

## Executive Summary
Step-3 is a 321B-parameter large language model optimized for cost-effective decoding through hardware-aware model-system co-design. The key innovations are Multi-Matrix Factorization Attention (MFA) that reduces KV cache size and computation while maintaining expressiveness, and Attention-FFN Disaggregation (AFD) that decouples attention and FFN layers into specialized subsystems. Step-3 achieves significant decoding cost reduction compared to models like DeepSeek-V3 and Qwen3 MoE 235B, while activating 38B parameters per token. On Hopper GPUs, it achieves up to 4,039 tokens/second/GPU under 50ms TPOT SLA (4K context, FP8), outperforming DeepSeek-V3's 2,324 tokens/second/GPU in the same setup.

## Method Summary
Step-3 combines MFA attention (64 query heads with 1 shared KV head, low-rank query projection 7168→2048→16384) with a MoE FFN (61 layers, 8/256 sparsity ≈ 0.08) deployed through AFD - a 3-stage pipeline (16.6ms per stage) that separates attention (DP) and FFN (TP/EP hybrid) onto different GPU sets using StepMesh RDMA communication. The model uses FP8 quantization throughout and achieves 50ms TPOT with 4,039 tokens/second/GPU on Hopper GPUs at 4K context with 6144 total batch size.

## Key Results
- Achieves 4,039 tokens/second/GPU under 50ms TPOT SLA on Hopper GPUs (4K context, FP8)
- Outperforms DeepSeek-V3 (2,324 tokens/second/GPU) in identical hardware setup
- MFA attention achieves arithmetic intensity of 128, positioned between H20 (74) and A800/910B (156-175) rooflines
- Minimum sparsity threshold of 0.058 for H800 prevents network bandwidth bottlenecks

## Why This Works (Mechanism)

### Mechanism 1
MFA's arithmetic intensity of 128 positions it between hardware rooflines, avoiding compute-bottleneck (DSv3 MLA at 512) and memory-bottleneck (Qwen3 GQA at 32) extremes. This enables consistent efficiency across hardware generations.

### Mechanism 2
AFD's 3-stage pipeline (A→comm→F) with 16.6ms/stage hides communication latency under computation, allowing independent batch sizing for attention (context-dependent) and FFN (accumulated globally).

### Mechanism 3
Minimum viable sparsity S ≥ (H × FLOPs × L) / (Net × Bandwidth × 11.1ms) prevents network bottlenecks. Step-3 uses S ≈ 0.08 to stay above H800 threshold while maintaining efficiency.

## Foundational Learning

- **Roofline model and arithmetic intensity**: Understanding why MFA's intensity of 128 matters requires knowing hardware rooflines; check: Given 100 GFLOPs and 1GB KV cache, what's arithmetic intensity?
- **MoE routing and sparsity**: Section 5.4's minimum sparsity derivation requires understanding activated vs total experts; check: If 8 of 256 experts plus 1 shared expert activate, what's sparsity?
- **Pipeline latency hiding**: AFD's efficiency depends on balanced stage times; check: If attention=20ms, FFN=10ms, comm=5ms per batch, where's the bottleneck in 50ms TPOT?

## Architecture Onboarding

- **Component map:**
  Attention Instance --FP8 tokens + metadata--> FFN Instance
  ↑ ↓
  BF16 activations <--RDMA-- Expert outputs + reduce

  Attention Instance: vLLM-based, DP across GPUs, manages KV cache
  FFN Instance: Lightweight C++/Python, TP or EP or hybrid
  Communication: StepMesh library (GPUDirect RDMA, zero SM usage)

- **Critical path**: Attention computation → FP8 quantization → RDMA dispatch → FFN all-gather → Expert computation → reduce-scatter → RDMA return → BF16 dequantization → next layer (272μs per layer for 61 layers)

- **Design tradeoffs**: Higher arithmetic intensity benefits H800 but may overshoot A800/910B rooflines; larger EP reduces per-node traffic but increases expert imbalance; MTP doubles attention intensity but increases FFN load.

- **Failure signatures**: TPOT > 50ms with low utilization → check batch size vs Bdense/BMoE thresholds; attention idle while FFN busy → pipeline imbalance; network congestion at high batch → sparsity below threshold; MFU < 40% on FFN → insufficient batch for MoE.

- **First 3 experiments**:
  1. **Latency breakdown profiling**: Instrument per-layer attention, communication, and FFN times on 4K context with batch=256. Target: all stages within ±10% of 272μs budget.
  2. **Sparsity threshold validation**: Deploy Step-3 FFN with 4, 8, 16 activated experts at batches 1024/2048/4096. Target: confirm ~0.058 threshold on H800.
  3. **Heterogeneous hardware test**: Compare 4×L20 vs 1×H800 with identical batch context. Target: L20 within 25% of H800 performance.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can hybrid linear attention models be redesigned to maintain balanced layer-wise computation times for distributed pipeline efficiency while preserving their KV cache savings?
- **Open Question 2**: How does MoE routing restriction (limiting token routing to adjacent experts) quantitatively impact model expressiveness and downstream task performance?
- **Open Question 3**: Is aggressive KV cache quantization (e.g., 4-bit storage with 8-bit computation) feasible for full attention layers in hybrid models without significant accuracy degradation on long-context tasks?

## Limitations

- Critical information missing: exact MoE expert topology, training procedure, and MFA mechanism details require external paper
- Hardware-specific: results focus exclusively on Hopper GPUs, limiting generalizability
- Theoretical cost comparisons assume perfect hardware utilization and ignore deployment overheads

## Confidence

**High Confidence**: MFA arithmetic intensity positioning, AFD 50ms TPOT achievement, DeepSeek-V3 performance comparison, hardware-specific sparsity thresholds

**Medium Confidence**: Theoretical decoding cost advantage, "cost-effective" claims based on idealized calculations, minimum sparsity threshold derivation

**Low Confidence**: Exact MoE expert topology, training methodology, MFA mechanism verification, long-context performance validation

## Next Checks

- **Check 1**: Profile per-layer latency breakdown for attention, RDMA communication, and FFN execution on 4K context with batch size 256 to verify all stages complete within 272μs budget.
- **Check 2**: Deploy Step-3 FFN with controlled expert activation counts (4, 8, 16 experts) at various batch sizes to confirm ~0.058 sparsity threshold on H800 hardware.
- **Check 3**: Compare Step-3 performance on 4×L20 GPUs versus 1×H800 GPU with identical configurations to validate bandwidth ratio assumptions in sparsity analysis.