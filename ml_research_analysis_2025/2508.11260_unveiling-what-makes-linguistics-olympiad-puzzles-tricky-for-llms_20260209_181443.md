---
ver: rpa2
title: 'UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?'
arxiv_id: '2508.11260'
source_url: https://arxiv.org/abs/2508.11260
tags:
- puzzles
- data
- llms
- language
- linguistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes large language models' performance on linguistics
  Olympiad puzzles across 41 low-resource languages. The authors annotate 629 problems
  with 50 linguistic features to identify patterns behind poor LLM performance.
---

# UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?

## Quick Facts
- arXiv ID: 2508.11260
- Source URL: https://arxiv.org/abs/2508.11260
- Reference count: 40
- Key outcome: Models struggle with higher morphological complexity and perform better on English-like features; morpheme splitting improves performance.

## Executive Summary
This paper analyzes why large language models struggle with Linguistics Olympiad puzzles across 41 low-resource languages. The authors annotate 629 problems with 50 linguistic features to identify patterns behind poor LLM performance. They find that models struggle with puzzles involving higher morphological complexity and perform better on puzzles with features also found in English. They also show that explicitly splitting words into morphemes as a pre-processing step improves solvability, indicating a need for more language-specific tokenizers. These findings provide insights into challenges in linguistic reasoning and modeling of low-resource languages.

## Method Summary
The study evaluates 10 LLMs on 64 curated Linguistics Olympiad puzzles (629 problems) across 41 low-resource languages. Puzzles are annotated with ~50 WALS features across 8 categories. Models are tested with 6 prompt styles measuring exact-match accuracy. The analysis computes correlations between accuracy and linguistic feature counts. A morpheme-splitting intervention is tested to improve performance. Contamination is checked via NoContext prompts.

## Key Results
- Higher morphological complexity correlates with lower LLM performance (all p-values ≤ .001).
- Models perform better on puzzles featuring linguistic structures also found in English.
- Explicitly marking morpheme boundaries in the input improves LLM puzzle-solving accuracy.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Higher morphological complexity in a puzzle correlates with lower LLM performance.
- Mechanism: LLMs struggle segmenting and composing morphologically complex words because standard tokenizers split based on frequency, not linguistic morpheme boundaries, obscuring underlying grammatical rules.
- Core assumption: Tokenizer's inability to isolate morphemes hinders pattern extraction from few-shot examples.
- Evidence anchors:
  - [abstract] "Our analyses show that LLMs struggle with puzzles involving higher morphological complexity."
  - [section 3.1] "higher morphological feature count correlates with lower exact match scores across all LLMs (all significant p-values ≤ .001)."
  - [corpus] Corpus provides weak direct evidence but confirms broader LLM struggles with morphological reasoning in low-resource languages.

### Mechanism 2
- Claim: LLMs perform better on puzzles featuring linguistic structures that also exist in English.
- Mechanism: LLMs, predominantly trained on English-centric corpora, may have stronger internal representations for syntactic and morphological patterns common in English, creating a "typological proximity" bias.
- Core assumption: Pre-training data distribution has biased the model's representational space towards English-like linguistic abstractions.
- Evidence anchors:
  - [abstract] "perform better on puzzles involving linguistic features that are also found in English."
  - [section 3.2] "a higher average similarity positively correlates with higher exact match scores."
  - [corpus] Corpus evidence is indirect; papers like "LingBench++" highlight the need for cross-linguistic evaluation but do not confirm this specific bias.

### Mechanism 3
- Claim: Explicitly marking morpheme boundaries in the input improves LLM puzzle-solving accuracy.
- Mechanism: Pre-processing to separate morphemes aligns tokenization with linguistic structure, reducing vocabulary burden and making compositional rules more transparent for the model to learn from few-shot context.
- Core assumption: The primary bottleneck is tokenization misalignment, not the model's fundamental reasoning capacity.
- Evidence anchors:
  - [abstract] "splitting words into morphemes as a pre-processing step improves solvability."
  - [section 3.5] "all models that were performing above 15% on the subset solved more problems accurately after splitting."
  - [corpus] Corpus provides no direct evidence for this specific intervention but supports the general idea.

## Foundational Learning

- Concept: **Morphological Typology**
  - Why needed here: To understand why languages with complex morphology (agglutinative, polysynthetic) pose a greater challenge for tokenizers and LLMs than isolating languages like English.
  - Quick check question: In an agglutinative language, would a word likely be tokenized into fewer or more subword units compared to a similar concept in English?

- Concept: **Subword Tokenization (e.g., BPE, WordPiece)**
  - Why needed here: To grasp the root cause of the problem—these algorithms segment text based on statistical frequency in the training corpus, not on linguistic meaning, often breaking morphemes into meaningless fragments in low-resource languages.
  - Quick check question: A tokenizer sees the made-up word "unhappiness" frequently. How might it tokenize it? Now, how might it tokenize a complex, unseen word from a low-resource language?

- Concept: **Linguistic Feature Annotation (e.g., WALS)**
  - Why needed here: This is the methodological core of the paper. Understanding how to systematically describe language properties (like word order, case marking) allows for structured analysis of LLM weaknesses.
  - Quick check question: If you were annotating a puzzle, which WALS feature would capture the difference between "the dog bites the man" and "the man bites the dog"?

## Architecture Onboarding

- Component map: Dataset Curation & Contamination Check -> Feature Annotation Pipeline -> Evaluation Harness -> Analysis & Intervention

- Critical path: The pipeline is sequential. An error in contamination removal would invalidate downstream performance correlations. The annotation quality is crucial for the analysis to hold.

- Design tradeoffs:
  1. Tokenizer vs. Pre-processing: The paper intervenes at the pre-processing level (adding spaces). A deeper but harder fix would be retraining or adapting the tokenizer itself.
  2. Broad vs. Fine-grained Features: The 50 WALS features provide interpretability but may not capture all nuances. Broader classes (like b_Morphology) are used for robust analysis but lose detail.
  3. Exact Match vs. Partial Credit: Using exact-match accuracy is strict but unambiguous. It may penalize models that grasp the rule but make minor surface errors.

- Failure signatures:
  1. Inconsistent gains from morpheme splitting: If only some models benefit, the issue may be architecture-specific.
  2. High performance on NoContext prompts: A sign of severe data contamination.
  3. Correlations vanish for a new model family: Suggests the weaknesses are not universal but tied to specific training data or architectures.

- First 3 experiments:
  1. Tokenizer Ablation: Compare performance on morphologically complex puzzles using standard tokenizer vs. morpheme-aware tokenizer for the target low-resource languages.
  2. Feature Ablation Study: Create synthetic puzzles that vary only in one linguistic attribute (e.g., only morphological complexity) to isolate its causal impact on performance.
  3. Cross-Lingual Transfer Test: Evaluate if a model fine-tuned on one set of morphologically complex low-resource languages generalizes better to new languages than a baseline model.

## Open Questions the Paper Calls Out

- Question: Do LLMs exhibit differential performance limitations when solving puzzles requiring derivational morphology inference versus inflectional morphology?
  - Basis in paper: [explicit] Section 5 states future analyses "could include addressing different kinds of morphology, e.g. inflectional as well as derivational, to better understand the observed effects."
  - Why unresolved: The current study aggregates morphological features into a single complexity count, obscuring potential specific weaknesses in handling meaning-changing (derivational) versus grammatical (inflectional) structures.
  - What evidence would resolve it: Annotating the dataset for specific morphology types and correlating them with model error rates.

- Question: Does scaling inference-time compute (ITC) effectively mitigate the "morphology gap," or is the performance gain observed in models like DeepSeek-R1 limited to specific linguistic features?
  - Basis in paper: [explicit] Section 3.4 and Appendix C.1 report results for DeepSeek-R1 but conclude that "future work must extend our study to a wider set of ITC models" to determine the impact of ITC on the gains.
  - Why unresolved: While DeepSeek-R1 showed superior performance, it is unclear if this is a general capability of ITC architectures or specific to that model's training, and how it interacts with the negative correlation to morphological complexity.
  - What evidence would resolve it: Benchmarking diverse ITC-enabled models against the specific morphological features identified as difficult in this paper.

- Question: To what extent do different prompting strategies alter the consistency of LLM responses for puzzles with high morphological complexity?
  - Basis in paper: [explicit] Section 5 suggests, "We could also explore how different prompting strategies change LLM responses and their consistency in solving LO puzzles."
  - Why unresolved: The paper averages performance over prompts but does not deeply analyze the variance in success rates between Chain-of-Thought and Minimal prompts specifically for complex morphology versus other features.
  - What evidence would resolve it: A comparative analysis of prompt strategy effectiveness mapped against the linguistic attributes (e.g., morphology vs. syntax).

## Limitations

- The study's findings are based on a relatively small sample of 64 puzzles across 41 low-resource languages, which may not fully represent linguistic diversity.
- The morpheme-splitting intervention relies on language-specific annotations that may contain errors or inconsistencies, particularly for less-documented languages.
- The study focuses primarily on translation puzzles, potentially missing other linguistic reasoning challenges present in Linguistics Olympiads.

## Confidence

- **High Confidence**: The correlation between morphological complexity and LLM performance degradation is well-supported by statistical evidence (all p-values ≤ 0.001) and aligns with known limitations of subword tokenization. The morpheme-splitting intervention's positive effect is clearly demonstrated across multiple models and puzzle subsets.
- **Medium Confidence**: The finding that English-typological similarity improves performance is supported by correlation analysis but could be influenced by other confounding factors. The claim that this represents a "typological proximity bias" requires further validation.
- **Low Confidence**: The assertion that these findings reveal fundamental challenges in modeling low-resource languages may overstate the implications. While the results are meaningful for Linguistics Olympiad-style reasoning tasks, they may not generalize to broader NLP applications.

## Next Checks

1. **Tokenizer Ablation Study**: Compare standard BPE tokenization against morpheme-aware tokenizers (rule-based or trained) for the same puzzles to isolate the impact of tokenization quality on performance.

2. **Cross-Lingual Generalization Test**: Evaluate whether models fine-tuned on morphologically complex low-resource languages show improved performance on new, unseen languages compared to baseline models.

3. **Controlled Feature Manipulation**: Create synthetic puzzles that systematically vary individual linguistic features (e.g., morphological complexity alone) while holding other variables constant to establish causal relationships.