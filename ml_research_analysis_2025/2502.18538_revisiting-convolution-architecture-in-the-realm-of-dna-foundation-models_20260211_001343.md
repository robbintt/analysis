---
ver: rpa2
title: Revisiting Convolution Architecture in the Realm of DNA Foundation Models
arxiv_id: '2502.18538'
source_url: https://arxiv.org/abs/2502.18538
tags:
- convnova
- tasks
- dilation
- hyenadna
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper revisits convolutional networks for DNA foundation
  modeling and shows they remain competitive with transformer and SSM methods. The
  authors propose ConvNova, a CNN-based model with three key designs: dilated convolutions
  for extended receptive fields, gated convolutions for selective feature extraction,
  and a dual-branch structure for gating mechanisms.'
---

# Revisiting Convolution Architecture in the Realm of DNA Foundation Models

## Quick Facts
- arXiv ID: 2502.18538
- Source URL: https://arxiv.org/abs/2502.18538
- Reference count: 40
- Primary result: ConvNova CNN achieves SOTA on >50% of DNA foundation model benchmarks with 5.8% histone task improvement

## Executive Summary
This paper challenges the assumption that transformers and state space models (SSMs) are superior for DNA foundation modeling by demonstrating that carefully designed convolutional architectures can achieve state-of-the-art performance. The authors propose ConvNova, a CNN-based model with three key innovations: dilated convolutions for extended receptive fields, gated convolutions for selective feature extraction, and a dual-branch structure for gating mechanisms. ConvNova outperforms or matches leading methods across multiple DNA benchmarks while typically using fewer parameters and offering faster computation, demonstrating that CNNs remain competitive with modern architectures in genomics.

## Method Summary
ConvNova uses gated convolutional blocks (GCBs) with dual branches - one for feature extraction and one for gating signal generation. The architecture employs dilated convolutions with exponentially increasing dilation rates ([1, 1, d, d², d³...]) to achieve large receptive fields without downsampling, preserving spatial resolution critical for DNA sequence modeling. Pretraining uses a masked language modeling (MLM) objective on the HG38 human reference genome with 10% nucleotide masking, followed by fine-tuning with pooling and linear classification heads. The dual-branch gating mechanism uses sigmoid activation to create soft masks that suppress irrelevant sequence regions while amplifying informative motifs.

## Key Results
- Achieves state-of-the-art performance on more than half of tasks across multiple DNA foundation model benchmarks
- 5.8% average improvement on histone-related tasks (H3K4me3: 60.20 MCC vs. 54.45 for HyenaDNA)
- 55% gene-finding accuracy versus 35% for HyenaDNA on GENCODE
- Typically uses fewer parameters than transformer and SSM baselines
- Demonstrates catastrophic performance degradation (50+ points) with downsampling architectures on splice site tasks

## Why This Works (Mechanism)

### Mechanism 1: Dilated Convolutions for Receptive Field Extension Without Information Loss
Dilated convolutions expand context exponentially across layers while preserving spatial resolution, avoiding the information loss from downsampling that severely degrades CNN performance on DNA tasks. The exponential dilation pattern ([1, 1, d, d², d³...]) achieves large receptive fields without reducing sequence length, maintaining the precise positional information that DNA regulatory signals require.

### Mechanism 2: Gated Convolutions for Irrelevant Segment Suppression
The gating mechanism uses sigmoid-activated branches to produce values in [0,1] that multiplicatively gate the main feature path, creating learned soft masks that suppress non-informative regions. This selective feature extraction is particularly effective for DNA sequences containing substantial "irrelevant segments" (non-coding, non-regulatory regions) that shouldn't contribute equally to predictions.

### Mechanism 3: Dual-Branch Architecture for Specialized Gating Signal Generation
Separating gating signal generation into a dedicated branch allows for specialized, independent representations optimized for "what to suppress." Two parallel branches process inputs independently, with updates: A = A + h ⊙ g, B = B + g. This specialization improves over single-branch gated convolutions by allowing the gating branch to develop distinct representations optimized for suppression decisions.

## Foundational Learning

- **Dilated/Atrous Convolutions**: Understanding how ConvNova achieves large receptive fields without information loss from downsampling. Quick check: Can you explain why a dilation rate of 4 with kernel size 9 gives a larger receptive field than kernel size 9 alone, without more parameters?

- **Gating Mechanisms (Sigmoid-Based Multiplicative Gating)**: The core innovation of ConvNova; explains how the model selectively amplifies/suppresses sequence features. Quick check: Why use sigmoid (range [0,1]) rather than GELU for the gating branch activation?

- **DNA Foundation Model Benchmarks (Histone Prediction, Gene Finding, Splice Sites)**: Context for interpreting results; different histone marks have distinct spatial distributions. Quick check: Why might different histone modification tasks require different optimal receptive field sizes?

## Architecture Onboarding

- **Component map**: Input (one-hot DNA) → Stem Conv (k=9) → [GCB × N] → MLP → Task Head
- **Critical path**: 
  1. Initialize dual-branch GCBs with proper LayerNorm placement (before convolutions)
  2. Set dilation schedule: authors use rate 4 as default
  3. Pretrain with MLM objective (10% masking, predict masked nucleotides)
  4. Fine-tune with pooling + linear head for classification

- **Design tradeoffs**:
  - Dilation rate: Higher = larger receptive field, but some tasks prefer smaller receptive fields
  - Kernel size: Authors chose 9 after ablation; larger kernels give marginal gains but increase parameters
  - Number of GCBs: Gene finding uses 10 GCBs; NT benchmark tasks use 5

- **Failure signatures**:
  - Catastrophic 50-point drop on splice site tasks with downsampling architecture
  - Histone task accuracy drops with full-sequence receptive field
  - Training instability on small datasets: reduce model width or add dropout

- **First 3 experiments**:
  1. Receptive field ablation: Run ConvNova on H3K4me3 with dilation rates [1, 2, 4, 8] to map non-monotonic performance
  2. Gating mechanism validation: Compare no gating, single-branch gating, and dual-branch gating on H3K9ac
  3. Architecture sanity check: Compare dilation vs. U-Net downsampling on splice site prediction to confirm catastrophic downsampling failure

## Open Questions the Paper Calls Out

1. **Multi-species pre-training**: Would pre-training on multi-species datasets significantly enhance downstream performance compared to single-species HG38 approach? The study doesn't test if increased data diversity from multi-species corpora provides benefits.

2. **H3K4me2 receptive field paradox**: What biological mechanism explains H3K4me2's better performance with small receptive fields despite enrichment in gene middles suggesting long-range dependency? The authors hypothesize "Contextual Locality" or "Gene Position Context" but don't verify.

3. **Architecture generalization to non-classification tasks**: Does ConvNova's efficiency generalize to regression or generation tasks beyond classification benchmarks? The paper demonstrates classification performance but doesn't test continuous value prediction or generative capabilities.

## Limitations

- Evaluation methodology raises concerns - modest absolute performance differences (1-3%) and aggregated 5.8% improvement may mask significant variation across individual tasks
- Computational efficiency claims need context - 400 epochs pretraining represents substantial compute investment despite fewer parameters
- Non-monotonic dilation rate performance suggests careful hyperparameter tuning is required rather than universal superiority

## Confidence

- **High confidence**: ConvNova architecture design and implementation are sound; represents a novel and well-executed approach
- **Medium confidence**: Performance improvements are real but modest; vary considerably across individual tasks
- **Medium confidence**: Computational efficiency claims are valid but require task-specific tuning of dilation rates

## Next Checks

1. **Dilation Rate Sensitivity Analysis**: Systematically test ConvNova across dilation rates [1, 2, 4, 8, 16] on each histone task individually to map non-monotonic performance curves and identify optimal settings per task type.

2. **Cross-Architecture Ablation Study**: Implement the same dilation and gating mechanisms within a Transformer/SSM baseline to isolate whether performance gains come from CNN architecture versus architectural innovations that could be applied to other architectures.

3. **Small Dataset Performance Evaluation**: Test ConvNova's robustness on smaller training sets (10%, 1%, 0.1% of training data) to evaluate whether dual-branch complexity provides benefits when data is limited, addressing assumptions about DNA's "irrelevant segments" and gating utility.