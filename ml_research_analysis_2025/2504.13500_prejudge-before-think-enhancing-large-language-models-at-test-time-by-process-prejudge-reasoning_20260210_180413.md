---
ver: rpa2
title: 'Prejudge-Before-Think: Enhancing Large Language Models at Test-Time by Process
  Prejudge Reasoning'
arxiv_id: '2504.13500'
source_url: https://arxiv.org/abs/2504.13500
tags:
- prejudge
- reasoning
- step
- language
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a process prejudge strategy for enhancing
  large language model (LLM) reasoning by teaching models to anticipate and avoid
  errors before they occur, rather than relying on trial and error. The authors define
  prejudge nodes in reasoning chains and develop an automated framework using dynamic
  tree search to synthesize training data with these prejudge elements.
---

# Prejudge-Before-Think: Enhancing Large Language Models at Test-Time by Process Prejudge Reasoning

## Quick Facts
- arXiv ID: 2504.13500
- Source URL: https://arxiv.org/abs/2504.13500
- Reference count: 29
- Primary result: Prejudge reasoning strategy improves LLM accuracy by 2.4-3.5% on competition math tasks

## Executive Summary
This paper introduces a process prejudge strategy for enhancing large language model (LLM) reasoning by teaching models to anticipate and avoid errors before they occur, rather than relying on trial and error. The authors define prejudge nodes in reasoning chains and develop an automated framework using dynamic tree search to synthesize training data with these prejudge elements. They employ a two-phase training approach combining supervised fine-tuning and reinforcement learning. Experiments on competition-level mathematics and logic tasks show consistent improvements over baselines like chain-of-thought training and self-refine methods, with average accuracy gains of 3.5-2.4% on Qwen2.5-7B/32B models, and further improvements when combining with CoT data or scaling test-time compute.

## Method Summary
The method synthesizes training data through dynamic tree search that identifies "prejudge nodes" - reasoning steps that precede inevitable errors. The framework uses one LLM to perform multiple functions: answer judging, response critiquing, prejudge generation, and thought completion. It employs a two-phase training approach: Phase 1 uses expensive dynamic tree search (~5 min/query) to create 39k high-quality prejudge-annotated rationales for supervised fine-tuning (SFT) cold start. Phase 2 uses this cold-start model to generate 195k additional rationales via zero-shot prompting with self-consistency filtering, then trains the final model on combined data. Reinforcement learning (GRPO/DPO) further refines the capability. The approach integrates special tokens (`<|think|>`, `<|prejudge|>`, `<|verify|>`) to mark different reasoning stages in the training data.

## Key Results
- Prejudge-Before-Think (PBT) improves Qwen2.5-7B accuracy by 3.5% on GSM8K, 2.4% on MATH-500, and 1.6% on AIME-2024 compared to standard CoT
- PBT shows consistent improvements across diverse datasets including GSM8K, MATH-500, AQuA, SVAMP, TheoremQA, AIME-2024, GAOKAO-2023, and GPQA-Diamond
- Performance scales with test-time compute, with 32B model showing greater improvements than 7B model
- Combining PBT with CoT data provides additional benefits on certain tasks, while o1-like data shows mixed results

## Why This Works (Mechanism)

### Mechanism 1: Prejudge Node Detection via Value Backpropagation
- Identifying reasoning steps that precede inevitable errors enables targeted anticipatory guidance
- Tree search explores multiple continuations from each reasoning step. A "prejudge value" vâ‚š(záµ¢) = v(záµ¢) Ã— ğŸ™(min(child_values) = 0) flags steps where at least one continuation has zero probability of reaching the correct answer
- Core assumption: The model can reliably distinguish correct from incorrect final answers via LLM-as-a-judger; errors cluster at predictable reasoning junctures

### Mechanism 2: Error Pattern Extraction and Prejudge Hint Generation
- Synthesizing explicit error analyses from failed trajectories produces actionable anticipatory guidance
- After identifying a prejudge node, the framework collects all incorrect child paths and prompts the LLM-as-a-critic to generate error analysis. A second prompt converts this analysis into a "prejudge hint" - forward-looking guidance about what mistakes to avoid
- Core assumption: The model can generalize from specific failure patterns to forward-looking avoidance strategies; verbalized error analysis transfers to preventive reasoning

### Mechanism 3: Two-Phase Training for Scalable Capability Transfer
- Bootstrapping from tree-search-generated data, then distilling to larger corpora, embeds prejudge reasoning into model parameters
- Phase 1 uses expensive dynamic tree search (~5 min/query) to create 39k high-quality prejudge-annotated rationales for SFT cold start. Phase 2 uses this cold-start model to generate 195k additional rationales via zero-shot prompting with self-consistency filtering
- Core assumption: The cold-start model sufficiently internalizes prejudge patterns to generate reasonable-quality distillation data; SFT+RL can compress test-time search into parametric knowledge

## Foundational Learning

- **Monte Carlo Tree Search (MCTS) / Tree Search for Reasoning**: The entire prejudge detection system depends on understanding how tree search explores branching reasoning paths, assigns values to nodes, and backpropagates success/failure signals. Quick check: Can you explain how the UCT algorithm balances exploration vs. exploitation, and how value backpropagation differs from rollout-based evaluation?

- **LLM-as-a-Judge / Critique Models**: The framework relies entirely on the model's ability to judge answer correctness and critique reasoning errors. Understanding failure modes of LLM judges is critical. Quick check: What are three common failure modes of LLM-as-a-judge on mathematical reasoning tasks (e.g., surface-level pattern matching, overconfidence on incorrect answers)?

- **Supervised Fine-Tuning (SFT) + Reinforcement Learning from Verifiable Rewards**: The two-phase training combines SFT for cold start with GRPO/DPO for refinement. Understanding when each is appropriate and how they interact is essential. Quick check: What is the difference between DPO (offline) and GRPO (online) in terms of data requirements and optimization stability?

## Architecture Onboarding

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Dynamic Tree Search                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚Thought Gen   â”‚â†’ â”‚Process Est.  â”‚â†’ â”‚Reasoning     â”‚      â”‚
â”‚  â”‚(low temp)    â”‚  â”‚(prejudge? vp)â”‚  â”‚Critique      â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                           â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚Prejudge Est. â”‚â† â”‚Hint Gen      â”‚                        â”‚
â”‚  â”‚(2nd search)  â”‚  â”‚(LLM-critic)  â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚           â†“                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚Thought Expandâ”‚ â†’ repeat until final answer              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“ (outputs: rationale with <think|>, <prejudge|>, <verify|> tags)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Two-Phase Training                        â”‚
â”‚  Phase 1: Tree Search â†’ 39k samples â†’ SFT (cold start)      â”‚
â”‚  Phase 2: Cold model â†’ 195k samples (self-consistency) â†’    â”‚
â”‚           Combined SFT + GRPO/DPO                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- **Critical path**: Implement LLM-as-a-judger (binary correct/incorrect) â†’ Implement tree search with value backpropagation â†’ Implement prejudge node detection â†’ Implement critique â†’ prejudge hint generation prompts â†’ Implement second tree search for hint validation â†’ Build SFT data pipeline with special tokens â†’ Implement two-phase training loop

- **Design tradeoffs**: Tree branching factor vs. compute (more branches improve prejudge detection but increase ~5 min/query cost exponentially); Verify step inclusion (ablation shows verification critical but adds overhead); Phase 1 vs. Phase 2 data ratio (39k vs. 195k - more Phase 1 data is higher quality but dramatically more expensive); CoT data mixing (32B benefits from combining PBT with CoT, but 7B shows mixed results)

- **Failure signatures**: Low prejudge hint quality (hints are generic or incorrect â†’ no Pass@N improvement); Judge accuracy degradation (if LLM-as-a-judger errors increase, prejudge nodes are misidentified); RL instability (GRPO reward hacking where model generates verbose but incorrect "prejudge" text); Distillation collapse (Phase 2 model produces repetitive, low-quality prejudge patterns)

- **First 3 experiments**: 1) Validate prejudge detection signal: Run tree search on 100 held-out problems; manually annotate whether detected prejudge nodes genuinely precede error-prone steps (target: >70% precision); 2) Ablate hint generation: Compare (a) no hint, (b) generic hint, (c) LLM-generated hint (measure Pass@N on 500 problems; expect (c) > (b) > (a) with ~3% gap); 3) Phase 1 vs. Phase 2 data quality audit: Sample 50 rationales from each phase; human-evaluate hint relevance and correctness (if Phase 2 quality <80% of Phase 1, increase self-consistency sampling or add rejection filtering)

## Open Questions the Paper Calls Out

- **Can we design a reward function or other strategies that allow LLMs to make prejudgments and combine them with some novel modes (e.g., aha moments) to enhance reasoning capabilities?**: The authors ask about designing reward functions to trigger or blend distinct cognitive modes dynamically, noting the current work uses standard RL techniques without exploring specific reward shaping.

- **How can the computational efficiency of the dynamic tree-searching algorithm be optimized to mitigate the high latency of data synthesis?**: The paper notes the ~5 minute per query cost as a bottleneck and suggests future work should focus on time efficiency in the searching strategy.

- **Why does combining PBT with o1-like data (LIMO) lead to performance degradation on specific benchmarks like GPQA-Diamond?**: Table 3 shows mixing PBT with LIMO data causes accuracy drops on MATH500 and GPQA-Diamond despite gains on other tasks, suggesting underlying conflicts between reasoning styles.

## Limitations

- **Computational Cost**: The dynamic tree search requires ~5 minutes per query to generate training data, making the data synthesis pipeline computationally expensive and potentially limiting scalability.

- **Domain Specificity**: All evaluation tasks are mathematical or logical reasoning problems; the prejudge mechanism may not transfer well to domains where errors are more subtle or reasoning paths are less deterministic.

- **Judge Reliability Dependency**: The framework assumes the LLM-as-a-judge can reliably identify correct/incorrect answers, but this accuracy may degrade on domain-specific problems or competition-level reasoning tasks.

## Confidence

- **High Confidence**: The core framework architecture (dynamic tree search + two-phase training) is well-specified and experimental results show consistent improvements on tested mathematical reasoning tasks.

- **Medium Confidence**: The effectiveness of prejudge hints in improving reasoning quality - while Pass@N metrics show improvements, the actual quality and usefulness of generated prejudge hints are not directly validated.

- **Low Confidence**: The claim that process prejudge reasoning can be effectively distilled into parametric knowledge for all problem types - success depends on cold-start model generating high-quality rationales, but this quality isn't validated.

## Next Checks

1. **Judge Accuracy Validation**: Run the LLM-as-a-judge on 100 held-out problems from each evaluation dataset and measure binary classification accuracy. Target: >90% accuracy on GSM8K/MATH, >80% on AIME/GAOKAO.

2. **Prejudge Hint Quality Audit**: Sample 100 generated prejudge hints from the final model and evaluate them on a 3-point scale (generic/incorrect/useful). Target: >70% of hints should be classified as useful.

3. **Domain Transfer Experiment**: Apply the same framework to a non-mathematical reasoning task (e.g., commonsense reasoning on HellaSwag or strategy game planning). Compare performance against standard CoT baseline. Target: â‰¥5% improvement.