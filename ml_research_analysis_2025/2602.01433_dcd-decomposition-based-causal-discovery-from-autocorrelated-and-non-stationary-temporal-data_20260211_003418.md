---
ver: rpa2
title: 'DCD: Decomposition-based Causal Discovery from Autocorrelated and Non-Stationary
  Temporal Data'
arxiv_id: '2602.01433'
source_url: https://arxiv.org/abs/2602.01433
tags:
- causal
- time
- seasonal
- discovery
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses causal discovery in multivariate time series
  with strong non-stationarity and autocorrelation, which confounds traditional methods
  by inducing spurious edges. It proposes DCD, a decomposition-based framework that
  separates each time series into trend, seasonal, and residual components, and performs
  component-specific causal analysis: trend components are tested for stationarity,
  seasonal components for deterministic periodicity via HSIC, and residuals for short-term
  causal links using constraint-based methods.'
---

# DCD: Decomposition-based Causal Discovery from Autocorrelated and Non-Stationary Temporal Data

## Quick Facts
- arXiv ID: 2602.01433
- Source URL: https://arxiv.org/abs/2602.01433
- Reference count: 40
- One-line primary result: DCD achieves TPR of 0.79±0.04 and FDR of 0.38±0.14 on synthetic benchmarks, outperforming PCMCI+, CD-NOD, and others.

## Executive Summary
This paper addresses causal discovery in multivariate time series with strong non-stationarity and autocorrelation, which confounds traditional methods by inducing spurious edges. It proposes DCD, a decomposition-based framework that separates each time series into trend, seasonal, and residual components, and performs component-specific causal analysis: trend components are tested for stationarity, seasonal components for deterministic periodicity via HSIC, and residuals for short-term causal links using constraint-based methods. These component-level graphs are integrated into a unified multi-scale causal graph. Theoretical guarantees show identifiability under spectral separability and bounded leakage, validated empirically. On synthetic benchmarks, DCD achieves a mean TPR of 0.79±0.04 and FDR of 0.38±0.14, outperforming PCMCI+, CD-NOD, CFCI, CPC, and BOSS-LiNGAM, especially as sample size, lag, and dimensionality increase. On real-world climate data, DCD uncovers sparse, interpretable, physically plausible structures, recovering key lagged interactions masked by trends in baseline methods.

## Method Summary
DCD decomposes multivariate time series into trend, seasonal, and residual components using STL. Trend components are assessed for stationarity using ADF/KPSS tests. Seasonal components are analyzed for deterministic periodicity using HSIC with RBF kernel. Residual components are analyzed for short-term causal links using constraint-based methods like PCMCI+. The component-specific causal graphs are integrated into a unified multi-scale causal graph. The method is validated on synthetic and real-world datasets.

## Key Results
- DCD achieves TPR of 0.79±0.04 and FDR of 0.38±0.14 on synthetic benchmarks.
- Outperforms PCMCI+, CD-NOD, CFCI, CPC, and BOSS-LiNGAM, especially as sample size, lag, and dimensionality increase.
- On real-world climate data, DCD uncovers sparse, interpretable, physically plausible structures.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Decomposing time series into trend, seasonal, and residual components isolates frequency-specific causal signals, reducing spurious edges caused by shared non-stationarity.
- **Mechanism**: STL (Seasonal-Trend decomposition using LOESS) acts as a spectral filter. It assigns low-frequency drift to trend ($T$), deterministic periodic patterns to seasonality ($S$), and high-frequency noise to residuals ($R$). By applying causal discovery separately to $R$, the method avoids misinterpreting shared trends (e.g., two variables drifting upward together) as direct causal links.
- **Core assumption**: Spectral Separability (A2) assumes components have disjoint support in the frequency domain.
- **Evidence anchors**:
  - [abstract]: "...separates each time series into trend, seasonal, and residual components and performs component-specific causal analysis... isolates long- and short-range causal effects, reduces spurious associations..."
  - [section 3.2.1]: "We select STL... because it adapts to local non-linearities via robust LOESS smoothing, thereby minimizing spectral leakage..."
  - [corpus]: Neighboring paper "Preserving Seasonal and Trend Information" supports the utility of decomposition for non-stationary learning.
- **Break condition**: If the system exhibits cross-frequency coupling (e.g., a trend modulating the amplitude of a seasonal cycle), the disjoint frequency assumption fails, and leakage may re-introduce confounding.

### Mechanism 2
- **Claim**: Removing non-stationary components restores the weak dependence (mixing) conditions required for constraint-based causal discovery on residuals.
- **Mechanism**: Raw non-stationary data violates the $\beta$-mixing assumption required for valid conditional independence (CI) tests because dependencies do not decay over time. By filtering out the non-decaying trend and seasonal components, the remaining residuals approximate a stationary process with decaying autocovariance, restoring the statistical validity of the CI tests used in the final step.
- **Core assumption**: Causal Invariance (A1) assumes the structural mechanism of the residual process is time-invariant and satisfies mixing conditions.
- **Evidence anchors**:
  - [section 3.1.3]: Lemma 2 states "...the estimated residual component $\hat{R}(t)$ satisfies the weak dependence (mixing) condition... with deviation bounded by the leakage parameter $\epsilon$."
  - [abstract]: "Trend components are assessed using stationarity tests... residual components using constraint-based causal discovery."
- **Break condition**: If the "residual" component itself contains structural breaks or hidden non-stationarity not captured by the trend decomposition, the mixing assumption remains violated, leading to inconsistent CI tests.

### Mechanism 3
- **Claim**: A unified causal graph can be constructed via the conflict-free union of component-specific subgraphs.
- **Mechanism**: The method constructs three distinct edge sets: $E_T$ (trend-driven), $E_S$ (seasonal-driven), and $E_R$ (short-term mechanistic). It relies on the assumption that causal mechanisms operate at distinct scales. The integration step simply unions these sets ($E_T \cup E_S \cup E_R$) without needing complex conflict resolution logic because the frequencies are theoretically disjoint.
- **Core assumption**: Causal Modularity (A4) assumes cross-scale influences are negligible (e.g., a short-term shock $R$ does not directly cause a long-term trend shift $T$).
- **Evidence anchors**:
  - [section 3.1.1]: "We define the Multi-Scale Causal Modularity hypothesis... the edge set $E^*$ can be partitioned into disjoint subsets..."
  - [section 3.2.3]: "Since the components operate in effectively disjoint frequency bands... the edge sets are additive and conflict-free."
- **Break condition**: If a short-term intervention (residual) causes a regime shift (trend), the modularity assumption is violated, and the union graph would miss this cross-scale causal link.

## Foundational Learning

- **Concept**: **Weak Dependence ($\beta$-mixing)**
  - **Why needed here**: Constraint-based algorithms (like PC or PCMCI+) rely on conditional independence tests. These tests generally assume samples are independent or "mix" well over time (dependencies decay). Non-stationary trends break this, causing tests to find "dependence" that is just shared drift.
  - **Quick check question**: If a time series has a unit root (random walk), does the autocorrelation decay to zero as lag increases? (Answer: No, it persists, violating mixing.)

- **Concept**: **Spectral Leakage**
  - **Why needed here**: The theory relies on perfectly separating frequencies. In practice, digital filters/decomposition are imperfect. "Leakage" is when trend energy bleeds into the residual. The paper bounds this by $\epsilon$; if $\epsilon$ is too high, the method fails.
  - **Quick check question**: Why does the paper use LOESS (STL) instead of a simple moving average? (Answer: Simple averages can induce artifacts and handle non-linear trends poorly, increasing leakage.)

- **Concept**: **Granger Causality vs. Structural Causality**
  - **Why needed here**: Standard Granger causality fails here because if $X$ and $Y$ share a trend, $X$ "Granger-causes" $Y$ predictively, even without a structural mechanism. This paper targets Structural Causality (mechanism) by removing the shared trend first.
  - **Quick check question**: If variable $X$ and $Y$ both grow exponentially over time, will Granger causality likely report a link? (Answer: Yes, likely a spurious one due to shared trend.)

## Architecture Onboarding

- **Component map**: Input -> STL Decomposition -> Component Analysis (Trend/Seasonal/Residual) -> Integration -> Output
- **Critical path**: The **Decomposition** step is the bottleneck. If the variance threshold for seasonality is set incorrectly, or if STL fails to separate trend from residual (leakage), the subsequent causal discovery on residuals is invalid.
- **Design tradeoffs**:
  - **STL vs. Fourier**: STL is robust to non-linear trends but requires selecting a seasonal period $P$. The paper uses a variance-maximization search for $P$.
  - **Sparsity vs. Recall**: The method reduces False Discovery Rate (FDR) significantly compared to baselines, but may miss "direct" long-term causal links ($T_X \to T_Y$) because it models trends as exogenous ($t \to X$).
- **Failure signatures**:
  - **High Correlation in Residuals**: If the residual heatmap shows strong autocorrelation, decomposition failed to remove the trend.
  - **Spurious Seasonal Edges**: If HSIC $p$-values are universally low, check if the seasonal period $P$ was tuned correctly.
- **First 3 experiments**:
  1. **Orthogonality Check**: Replicate Figure 2(d). Run STL on synthetic data and compute the correlation between the estimated Seasonal driver ($\hat{S}_X$) and the Estimated Residual ($\hat{R}_Y$). Ensure $\rho \approx 0$.
  2. **Leakage Sensitivity**: Generate data where trend variance $\gg$ residual variance. Run DCD and measure if FDR increases (indicating leakage is overwhelming the CI tests).
  3. **Baseline Comparison**: Run DCD vs. PCMCI+ on a dataset with a strong linear trend. Verify that PCMCI+ produces a "hairball" graph (all connected) while DCD produces a sparse graph.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the framework be extended to handle cross-frequency coupling where low-frequency trends modulate the amplitude of seasonal components?
- **Basis in paper**: [explicit] The Conclusion states future work aims to extend the framework to handle cases where "trends modulate seasonal amplitudes."
- **Why unresolved**: The theoretical identifiability guarantees (Theorem 1) rely on Assumption A2 (Spectral Separability), which assumes disjoint frequency support between components. Cross-frequency coupling violates this separation.
- **What evidence would resolve it**: A modified theoretical framework that relaxes A2 and an empirical demonstration showing accurate causal recovery on synthetic data with known amplitude modulation.

### Open Question 2
- **Question**: How can the method identify direct trend-to-trend causal mechanisms ($T_i \rightarrow T_j$) without inducing spurious cointegration?
- **Basis in paper**: [explicit] Section 5 states the method "conservatively models trends as exogenous ($t \rightarrow X_i$)... potentially missing direct long-term mechanisms ($T_i \rightarrow T_j$)."
- **Why unresolved**: The current design treats trends as functions of time to avoid false positives caused by non-stationary confounding, leaving inter-trend causality unexplored.
- **What evidence would resolve it**: A causal discovery algorithm capable of distinguishing between spurious correlations and true mechanistic links between non-stationary trend components.

### Open Question 3
- **Question**: Can deep learning-based decomposition techniques reduce spectral leakage ($\epsilon$) more effectively than STL in highly non-linear systems?
- **Basis in paper**: [explicit] The Conclusion proposes "exploring deep learning-based decomposition to further reduce leakage in highly non-linear systems."
- **Why unresolved**: The identifiability bounds depend on minimizing the leakage parameter (Assumption A3), and classical STL may not optimally separate components in complex, non-linear settings.
- **What evidence would resolve it**: Comparative experiments showing that learned decomposition minimizes cross-component covariance ($\lambda$) and improves TPR/FDR over the STL baseline on non-linear datasets.

## Limitations

- **Spectral separability assumption**: The theoretical guarantees hinge on disjoint frequency support between trend, seasonal, and residual components. Real-world data with cross-frequency coupling (e.g., trend-modulated seasonality) violates this assumption, potentially reintroducing spurious edges.
- **Decomposition stability**: STL hyperparameters (smoothness, seasonal windows) are not fully specified. Poor tuning can cause spectral leakage, where trend energy contaminates residuals and invalidates subsequent causal tests.
- **Causal modularity**: The method assumes no cross-scale causal links (e.g., short-term shocks cannot cause long-term trends). If violated, the union graph misses important mechanisms.

## Confidence

- **High confidence**: Synthetic benchmark performance (TPR 0.79±0.04, FDR 0.38±0.14) and qualitative interpretability of real-world results. The decomposition + component-specific analysis mechanism is sound and well-supported by the ablation showing raw CD fails.
- **Medium confidence**: Theoretical identifiability guarantees, which rely on idealized conditions (perfect spectral separation, bounded leakage). The bounds are derived but depend on assumptions that may not hold exactly in practice.
- **Low confidence**: Handling of unknown seasonal periods. While the paper proposes a variance-maximization search, the sensitivity to period selection and the candidate set range are not fully characterized.

## Next Checks

1. **Spectral Leakage Test**: Generate synthetic data with known trend vs. residual variance ratio (e.g., trend variance 10x residual). Apply DCD and measure if FDR increases, indicating leakage is confounding the residual CI tests.
2. **Cross-Scale Causality Test**: Create a dataset where a short-term residual shock directly causes a long-term trend shift. Verify if DCD misses this edge (violating Causal Modularity), while a baseline that analyzes raw data might capture it (albeit with many false positives).
3. **Period Selection Robustness**: Apply DCD to real data (e.g., Sea Ice) with varying seasonal period candidates. Assess stability of the seasonal component and downstream causal graph structure as the period changes.