---
ver: rpa2
title: 'How Explanations Leak the Decision Logic: Stealing Graph Neural Networks via
  Explanation Alignment'
arxiv_id: '2506.03087'
source_url: https://arxiv.org/abs/2506.03087
tags:
- graph
- explanation
- target
- explanations
- stealing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates model stealing attacks on explainable
  Graph Neural Networks (GNNs), revealing that explanations can leak decision logic
  and facilitate model extraction. The authors propose EGSteal, a novel framework
  that exploits this vulnerability through two key components: (1) rank-based explanation
  alignment, which ensures the surrogate model captures the target''s reasoning patterns,
  and (2) explanation-guided data augmentation, which generates diverse training samples
  without additional queries.'
---

# How Explanations Leak the Decision Logic: Stealing Graph Neural Networks via Explanation Alignment

## Quick Facts
- arXiv ID: 2506.03087
- Source URL: https://arxiv.org/abs/2506.03087
- Reference count: 40
- Primary result: Explanation-based model stealing framework achieves 80.74% AUC on NCI1 and 93.38% on AIDS while capturing decision logic through rank alignment

## Executive Summary
This paper presents EGSteal, a novel framework that exploits explanations from Graph Neural Networks to extract functional clones more effectively than previous model stealing approaches. The key insight is that explanations reveal decision logic that can be aligned between surrogate and target models, rather than just matching predictions. By combining rank-based explanation alignment with explanation-guided data augmentation, EGSteal achieves superior performance under limited query budgets and maintains effectiveness even when target explanations use different methods than the surrogate.

## Method Summary
EGSteal extracts a surrogate GNN by querying a black-box target model for predictions and explanations, then training with two components: (1) rank-based explanation alignment that ensures the surrogate learns similar node importance rankings through a pairwise ranking loss, and (2) explanation-guided data augmentation that generates additional training samples by modifying low-importance (style) nodes while preserving high-importance (causal) structure. The framework uses Graph-CAM for surrogate explanations and supports cross-explanation-method alignment (e.g., target uses PGExplainer while surrogate uses Graph-CAM).

## Key Results
- Achieves 80.74% AUC on NCI1 and 93.38% on AIDS molecular datasets
- Maintains 87.78% prediction fidelity and 42.38% explanation rank correlation
- Outperforms baseline methods across all query budgets (10-50%), with larger gains at lower budgets
- Shows robustness to cross-distribution scenarios, though performance degrades with significant domain shift

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aligning node importance rankings between surrogate and target models transfers decision logic more effectively than matching raw explanation scores.
- **Mechanism:** A RankNet-inspired loss (Eq. 6-7) enforces pairwise ranking consistency: for any node pair (i,j), if the target ranks i more important than j, the surrogate should learn the same ordering via sigmoid-based ranking loss. This sidesteps scale/distribution mismatches between different explanation methods.
- **Core assumption:** Relative node importance rankings are stable across models and explanation mechanisms, even when absolute scores vary.
- **Evidence anchors:**
  - [Section 4.2.1]: "raw importance scores produced by different models or explanation methods often vary widely in scale and distribution, limiting the effectiveness of direct alignment"
  - [Table 3]: PGExplainer shows 65% rank correlation gain despite being edge-based, confirming cross-method ranking stability
  - [corpus]: Weak direct corpus support—ADAGE discusses GNN extraction defenses but not explanation alignment specifically
- **Break condition:** If target explanations are adversarially perturbed or deliberately noisy, ranking signals become unreliable and alignment may degrade surrogate performance below baseline.

### Mechanism 2
- **Claim:** Explanations enable query-free data augmentation by identifying "style" nodes whose modification preserves predictions.
- **Mechanism:** Low-importance nodes (bottom α fraction by rank) are treated as non-causal style components. Augmentation strategies (node dropping, edge perturbation) intervene on these nodes while preserving high-importance causal structure. Labels for augmented graphs are inherited from original queries via style invariance (Eq. 9).
- **Core assumption:** The causal factorization GE → Y holds; style interventions don't alter p(Y|GE).
- **Evidence anchors:**
  - [Section 4.1]: SCM formulation with GE → Y and GS having no direct causal effect on Y
  - [Eq. 3-4]: Formal invariance property pdo(GS=Gi_S)(Y|GE; θ) = p(Y|GE; θ)
  - [corpus]: No direct corpus validation of this specific augmentation strategy for GNNs
- **Break condition:** If explanations incorrectly identify causal nodes (e.g., explanations are unfaithful), style interventions may accidentally modify decision-critical structure, introducing label noise.

### Mechanism 3
- **Claim:** Combining explanation alignment with explanation-guided augmentation provides complementary benefits—alignment captures logic, augmentation improves prediction accuracy under query limits.
- **Mechanism:** The total loss L_total = L_pred + λL_align jointly optimizes prediction matching and explanation consistency. Augmentation expands training data (1+α)×Q samples without additional queries, while alignment ensures the surrogate reasons similarly to the target.
- **Core assumption:** Query budget constraints are the primary bottleneck; explanations provide sufficient signal to guide both data efficiency and logic transfer.
- **Evidence anchors:**
  - [Figure 7]: Ablation shows removing alignment causes largest logic drop; removing augmentation hurts AUC/fidelity more than alignment removal
  - [Figure 4]: Consistent gains over TS baseline across all query budgets (10-50%), with larger gaps at lower budgets
  - [corpus]: Weak support—related work on GNN extraction (MEA-GNN, EfficientGNN) doesn't incorporate explanations
- **Break condition:** If λ is set too high, explanation alignment may dominate prediction loss, harming task performance; if α is too aggressive, augmentation may generate unrealistic graphs.

## Foundational Learning

- **Concept: Message-passing in GNNs**
  - Why needed here: Understanding how node embeddings aggregate neighborhood information (Eq. 1) is prerequisite to grasping why node importance explanations reveal decision logic.
  - Quick check question: Can you explain why a node's importance score depends on its neighbors' embeddings?

- **Concept: Gradient-based explanation methods (Graph-CAM, Grad-CAM)**
  - Why needed here: EGSteal uses Graph-CAM for the surrogate; understanding how class-specific weights backpropagate to node features (Eq. 5, 11-13) is essential for implementing alignment.
  - Quick check question: How does Graph-CAM compute node importance from final-layer features and classifier weights?

- **Concept: Model stealing threat model**
  - Why needed here: The black-box setting (unknown architecture/parameters, limited query budget) defines constraints; distinguishing in-distribution vs. cross-distribution shadow data matters for robustness analysis.
  - Quick check question: What information does the attacker have access to, and what remains unknown?

## Architecture Onboarding

- **Component map:**
  Query phase -> Explanation extraction -> Augmentation module -> Training loop -> Surrogate model

- **Critical path:**
  1. Explanation extraction from target (quality directly affects both alignment signal and augmentation correctness)
  2. Style node identification (α parameter controls augmentation quality vs. quantity tradeoff)
  3. Ranking loss computation over all node pairs (O(|V|²) per graph—computational bottleneck)

- **Design tradeoffs:**
  - Graph-CAM vs. other explainers: Graph-CAM chosen for efficiency (no additional training) and gradient compatibility; PGExplainer may provide better edge-level explanations but requires conversion
  - Ranking loss vs. MSE: Ranking handles scale mismatch but loses magnitude information
  - Augmentation ratio α: Higher α = more data but riskier interventions; paper finds 0.1-0.3 optimal

- **Failure signatures:**
  - Low rank correlation despite high fidelity: Surrogate mimics predictions without capturing logic → check if alignment loss is being optimized
  - AUC drops with augmentation: Style interventions corrupting causal structure → reduce α or verify explanation faithfulness
  - Cross-distribution failure: Shadow data too divergent → may need domain adaptation or more queries

- **First 3 experiments:**
  1. **Baseline comparison:** Replicate Table 1 results on NCI1/NCI109 with GIN target/surrogate, measuring AUC, fidelity, and rank correlation vs. TS baseline
  2. **Ablation study:** Remove augmentation (wo Aug.) and alignment (wo Align.) separately to verify complementary contributions per Figure 7
  3. **Hyperparameter sweep:** Vary λ ∈ {0.01, 0.1, 1, 10, 100} and α ∈ {0.1, 0.2, 0.3, 0.4, 0.5} on NCI109 to reproduce Figure 8 sensitivity analysis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What defense mechanisms can effectively protect explainable GNNs from explanation-based model stealing attacks while preserving explanation utility?
- Basis in paper: [explicit] Conclusion states: "Future work will extend our investigation to more graph learning tasks and develop effective defense mechanisms against explanation-based model stealing attacks."
- Why unresolved: The paper focuses on attack methodology but does not propose or evaluate any defensive strategies.
- What evidence would resolve it: Development and empirical evaluation of defense methods (e.g., explanation perturbation, differential privacy, query throttling) that reduce attack success rates while maintaining explanation faithfulness metrics.

### Open Question 2
- Question: Does EGSteal generalize to other graph learning tasks beyond graph classification?
- Basis in paper: [explicit] Conclusion states: "Future work will extend our investigation to more graph learning tasks..."
- Why unresolved: All experiments are limited to graph classification on molecular datasets; node classification, link prediction, and graph generation remain unexplored.
- What evidence would resolve it: Experiments applying EGSteal to node classification (e.g., citation networks), link prediction, and other graph tasks with appropriate metric adaptations.

### Open Question 3
- Question: How can explanation-based model stealing attacks maintain effectiveness under significant distribution shift between shadow and target data?
- Basis in paper: [inferred] Cross-distribution experiments (Figure 5) show explanation correlation dropping from 35.58% to 6.68% when using AIDS shadow data for NCI109 target.
- Why unresolved: The paper demonstrates vulnerability to distribution shift but does not propose methods to address this limitation.
- What evidence would resolve it: Modified frameworks incorporating domain adaptation or style-invariant learning that maintain extraction performance under distribution shift.

## Limitations
- Explanation faithfulness validation: The framework assumes target explanations are faithful, but this is not independently verified before using them for alignment/augmentation
- Cross-domain generalization: Experiments are limited to molecular graphs, raising questions about performance on non-chemical domains like social networks or citation graphs
- Computational scalability: The quadratic complexity of rank-based alignment (O(|V|²) per graph) creates scalability challenges for large graphs

## Confidence
- **High:** EGSteal framework architecture and implementation details (query generation, augmentation strategies, ranking loss formulation)
- **Medium:** Effectiveness claims for NCI1/NCI109 benchmarks (AUC scores 80.74% and 93.38% with corresponding fidelity and rank correlation)
- **Low:** Generalizability to non-molecular graph domains and robustness under adversarial explanation perturbations

## Next Checks
1. **Faithfulness validation:** Independently measure explanation faithfulness on target models before using them for alignment/augmentation
2. **Adversarial robustness:** Test EGSteal performance when target explanations are deliberately perturbed or made noisy
3. **Cross-domain transfer:** Evaluate on non-molecular graph datasets (e.g., social networks, citation graphs) to assess domain generalization