---
ver: rpa2
title: 'Demand for LLMs: Descriptive Evidence on Substitution, Market Expansion, and
  Multihoming'
arxiv_id: '2504.15440'
source_url: https://arxiv.org/abs/2504.15440
tags:
- other
- fast
- cheap
- gemini
- demand
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes demand patterns for Large Language Models
  (LLMs) using OpenRouter marketplace data from January to April 2025. The study finds
  three key patterns: (1) new models see rapid adoption within weeks of release, (2)
  different model releases have varying effects on market expansion versus substitution
  of existing models, and (3) multi-homing is common with apps using multiple models
  simultaneously.'
---

# Demand for LLMs: Descriptive Evidence on Substitution, Market Expansion, and Multihoming

## Quick Facts
- arXiv ID: 2504.15440
- Source URL: https://arxiv.org/abs/2504.15440
- Authors: Andrey Fradkin
- Reference count: 12
- Primary result: LLM marketplace data shows rapid adoption, varied substitution vs market expansion, and common multi-homing patterns

## Executive Summary
This paper analyzes LLM demand patterns using OpenRouter marketplace data from January to April 2025. The study identifies three key findings: new models experience rapid adoption that stabilizes within weeks, model releases have varying effects on market expansion versus substitution, and multi-homing is prevalent with apps using multiple models simultaneously. These patterns suggest significant horizontal and vertical differentiation in the LLM market, allowing providers to maintain pricing power despite technological advances through factors beyond raw benchmark performance.

## Method Summary
The analysis uses daily token usage data scraped from OpenRouter model pages, covering 249 models after filtering. The study employs interrupted time-series case studies around three model releases (Claude 3.7 Sonnet, Gemini 2.0 Flash, Gemini 2.5 Pro) and visual analysis of token usage patterns. The research also calculates diversion ratios and substitution patterns, along with app-level model mix analysis for multi-homing behavior.

## Key Results
- New LLM models see rapid adoption within weeks of release, followed by stabilization
- Model releases differ substantially in whether they expand the market or substitute existing models
- Multi-homing is common, with apps simultaneously using multiple models for different tasks

## Why This Works (Mechanism)

### Mechanism 1: Rapid Adoption and Quick Stabilization
- **Claim**: New LLMs see rapid spike in demand that stabilizes within weeks of release
- **Mechanism**: A unified API marketplace dramatically lowers integration costs for developers, enabling apps to test and integrate new models almost immediately after release
- **Core assumption**: Switching costs for developers are low enough to permit rapid re-evaluation of models
- **Evidence anchors**: [abstract] "...new models experience rapid initial adoption that stabilizes within a few weeks"; [section 3.2, Fact 1] "adoption of the new model occurs quickly, with an initial jump in log tokens that stabilizes within a few weeks"
- **Break condition**: Deep model-specific optimizations would increase switching costs and slow adoption

### Mechanism 2: Differentiated Substitution and Market Expansion
- **Claim**: New model releases do not uniformly substitute for existing ones; effects vary between cannibalizing related models and expanding the total user base
- **Mechanism**: Substitution patterns are governed by user perception of model similarity - upgrades draw from predecessors while novel capability-price points attract new use cases
- **Core assumption**: LLMs are not commodity goods; users have heterogeneous preferences leading to differentiated demand
- **Evidence anchors**: [abstract] "...model releases differ substantially in whether they primarily attract new users or substitute demand from competing models"; [section 3.2, Fact 2] "We see quite different trends... with some models expanding the market and other models creating obvious substitution"
- **Break condition**: Perfect commoditization would lead to uniform price-based substitution

### Mechanism 3: Persistent Demand via Horizontal Differentiation and Multi-homing
- **Claim**: Providers can retain demand and pricing power because models are differentiated on non-benchmark factors, and users commonly employ multiple models (multi-home) for different sub-tasks
- **Mechanism**: Horizontal differentiation means models can appeal to different user preferences (coding "vibes," tool integration, context window) even at similar price/performance points
- **Core assumption**: Benchmark performance does not fully capture user-perceived quality or utility
- **Evidence anchors**: [abstract] "...findings suggest significant horizontal and vertical differentiation in the LLM market"; [section 3.2, Fact 3] "There is substantial multi-homing, with users of the same app employing a mix of models"
- **Break condition**: A single provider could consolidate demand with dominant multi-dimensional lead

## Foundational Learning

- **Concept: Horizontal vs. Vertical Differentiation**
  - **Why needed here**: This is the core economic framework used to explain how LLM providers can compete without a pure price war
  - **Quick check question**: If one model is faster and cheaper but another is preferred for creative writing despite lower benchmark scores, which dimension of differentiation does the writing preference reflect? (Answer: Horizontal)

- **Concept: Multi-homing**
  - **Why needed here**: This concept challenges the idea of a single "winner-take-all" model
  - **Quick check question**: According to the paper, does the coding app "Cline" rely exclusively on one model? (Answer: No, the data shows it uses a mix of models)

- **Concept: Interrupted Time-Series Analysis**
  - **Why needed here**: The paper uses this method to draw conclusions from case studies of model releases
  - **Quick check question**: For the analysis of a new model's release to be valid, what must be true about other events in the market during that same time period? (Answer: There should be no concurrent events that would similarly affect demand)

## Architecture Onboarding

- **Component map**: Provider (Anthropic, Google, OpenAI) -> OpenRouter Platform -> App (Cline, Roo Code, SillyTavern) -> User
- **Critical path**: Model Release -> OpenRouter Lists Model -> Apps Integrate via Unified API -> Token Usage Data Generated -> Data Scraped from Public Pages -> Patterns Analyzed
- **Design tradeoffs**:
  - Scope vs. Depth: Using OpenRouter provides detailed API usage data but misses native app segments
  - Descriptive vs. Causal: Case study method provides intuitive evidence but relies on untestable assumptions
  - Public vs. Complete Data: Analysis limited to apps that opt-in to OpenRouter
- **Failure signatures**:
  - Major app starts/stops using OpenRouter during case study window, creating spurious demand shock
  - Incorrect generalization from developer-heavy dataset to broader consumer market
  - Over-interpreting correlation as causation without ruling out concurrent market events
- **First 3 experiments**:
  1. Cross-Platform Validation: Replicate analysis using data from other marketplaces or direct provider APIs
  2. Price Elasticity Measurement: Track demand changes in response to provider-initiated price changes
  3. User Intent Survey: Survey developers to identify which features drive model selection

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does demand for LLMs respond to price changes?
- **Basis in paper**: [explicit] "The first, and perhaps the most obvious, question is how demand for LLMs responds to price."
- **Why unresolved**: Dataset lacks sufficient within-model price variation to estimate price elasticities
- **What evidence would resolve it**: Observational data from providers enacting sustained price changes

### Open Question 2
- **Question**: What specific dimensions drive horizontal differentiation among LLMs?
- **Basis in paper**: [explicit] "A second question concerns the dimensions of horizontal differentiation... [such as] ‘vibes’ and the presence of app-model integration."
- **Why unresolved**: Paper observes aggregate usage outcomes but cannot disentangle causal impact of subjective qualities
- **What evidence would resolve it**: User-level survey data on preferences or experimental A/B testing

### Open Question 3
- **Question**: Do demand patterns observed on marketplaces generalize to direct provider interactions?
- **Basis in paper**: [explicit] "It remains an open question whether users that directly interface with model providers such as Anthropic, Google, and OpenAI exhibit similar demand patterns."
- **Why unresolved**: Study relies solely on OpenRouter data, excluding native app usage
- **What evidence would resolve it**: Comparative analysis using proprietary data from model providers

### Open Question 4
- **Question**: Does persistent demand for certain models reflect unmeasured quality or behavioral lock-in?
- **Basis in paper**: [explicit] "A key question for competition is whether persistent demand reflects important differences in quality not measured by benchmarks... [or] other behavioral factors."
- **Why unresolved**: High-performing models on benchmarks do not always dominate market share
- **What evidence would resolve it**: Studies correlating usability metrics with retention or analysis of user churn

## Limitations

- Findings based on OpenRouter marketplace data, missing native app usage and other major intermediaries
- Interrupted time-series approach relies on untestable assumptions about absence of confounding events
- Multi-homing analysis limited to top-20 ranked apps, potentially undercounting usage diversity
- No direct causal evidence for price sensitivity or drivers of horizontal differentiation

## Confidence

- **High confidence**: Rapid adoption patterns and stabilization within weeks (directly observed in data)
- **Medium confidence**: Differentiated substitution vs market expansion patterns (requires assumptions about no concurrent shocks)
- **Medium confidence**: Multi-homing as evidence of horizontal differentiation (observational correlation, not causal proof)

## Next Checks

1. **Cross-platform validation**: Replicate the analysis using data from alternative marketplaces or direct provider APIs to assess generalizability beyond OpenRouter
2. **Confounding event audit**: Systematically catalog all major marketplace and app events during each case study period to quantify potential violations of interrupted time-series assumptions
3. **Developer survey validation**: Conduct targeted surveys with app developers to verify whether observed multi-homing patterns reflect deliberate model specialization or technical constraints