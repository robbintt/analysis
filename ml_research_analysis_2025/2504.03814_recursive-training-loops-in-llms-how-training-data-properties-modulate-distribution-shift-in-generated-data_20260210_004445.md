---
ver: rpa2
title: 'Recursive Training Loops in LLMs: How training data properties modulate distribution
  shift in generated data?'
arxiv_id: '2504.03814'
source_url: https://arxiv.org/abs/2504.03814
tags:
- data
- quality
- diversity
- distribution
- shifts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how properties of human training data affect
  distribution shifts in large language models during recursive fine-tuning. The authors
  first confirm that different human datasets lead to varying magnitudes of distribution
  shifts.
---

# Recursive Training Loops in LLMs: How training data properties modulate distribution shift in generated data?

## Quick Facts
- arXiv ID: 2504.03814
- Source URL: https://arxiv.org/abs/2504.03814
- Authors: Grgur Kovač; Jérémy Perez; Rémy Portelas; Peter Ford Dominey; Pierre-Yves Oudeyer
- Reference count: 40
- One-line primary result: Different human datasets produce varying distribution shifts during recursive fine-tuning, with lexical diversity amplifying shifts and semantic diversity/quality mitigating them.

## Executive Summary
This paper investigates how properties of human training data affect distribution shifts in large language models during recursive fine-tuning. Through extensive experiments across 800 clusters from four datasets, the authors identify key data properties that influence shift magnitudes. Lexical diversity amplifies shifts, while semantic diversity and quality mitigate them. Notably, the effects are highly modular, with each domain experiencing independent shifts. The study also reveals that political bias shifts depend on the initial political lean of the human data.

## Method Summary
The study simulates recursive training loops using iterative chains: 20 generations where each generation fine-tunes a fresh base model on a mixture of accumulated synthetic data and fresh human samples. Four datasets are processed into 800 clusters using UMAP + clustering algorithms. Models are fine-tuned using LoRA with specified hyperparameters, and synthetic data is generated at controlled ratios. Distribution shifts are measured using LLM-as-judge evaluations for quality and political lean, along with embedding-based metrics for semantic diversity.

## Key Results
- Different human datasets lead to varying magnitudes of distribution shifts during recursive fine-tuning
- Lexical diversity is the most robust predictor of shift magnitude, amplifying degradation
- Semantic diversity and data quality mitigate distribution shifts
- Distribution shift dynamics are highly modular, with minimal cross-domain influence
- Political bias shifts depend on initial political lean, with direction varying by dataset

## Why This Works (Mechanism)

### Mechanism 1: Lexical Diversity Amplifies Distribution Shifts
Higher lexical diversity correlates with more severe quality/diversity degradation during recursive fine-tuning. Lexical diversity captures surface-level variation without semantic depth, causing models to overfit to patterns that lack robust semantic grounding. This creates fragile representations that destabilize under recursion.

### Mechanism 2: Semantic Diversity and Quality Provide Resilience
Semantic diversity (embedding-space spread) and data quality are associated with smaller distribution shifts. Semantically diverse data spans a richer representational space, providing more stable training signal. High-quality data reduces noise that could otherwise compound under recursion.

### Mechanism 3: Domain-Level Modularity in Shift Dynamics
Distribution shifts are highly modular—data from one domain minimally influences generation in another domain. Models maintain semi-independent representations for different domains, suggesting factorized internal representations where domain properties primarily affect generation within that domain.

## Foundational Learning

- **Concept: Iterative Chain Paradigm**
  - Why needed: This is the core experimental setup—understanding how data pools evolve across generations is essential.
  - Quick check: Can you explain why each generation uses a *fresh* base model rather than continuing from the previous fine-tuned model?

- **Concept: Distribution Shift vs. Model Collapse**
  - Why needed: The paper distinguishes general distribution mismatch from *detrimental* collapse (quality/diversity loss).
  - Quick check: What makes a distribution shift "detrimental" versus merely different?

- **Concept: Synthetic-to-Human Data Ratio**
  - Why needed: This ratio controls the "contamination" level and directly modulates shift magnitude.
  - Quick check: At what ratio does the paper observe plateauing of degradation effects for the 100M_tweets dataset?

## Architecture Onboarding

- **Component map:** Human dataset → clustering (UMAP + k-means/HDBSCAN) → 200 clusters per dataset → mixed clusters for multi-domain experiments → Fresh base model → LoRA fine-tuning → generate synthetic data → accumulate with fresh human data → LLM-as-judge evaluation

- **Critical path:** 1) Prepare human datasets with proper filtering (pre-GPT-3 cutoff, English-only, token length bounds) → 2) Generate 800 clusters across 4 datasets with controlled property variation → 3) Run iterative chains with synthetic ratios 1/8 and 1/4 for 20 generations each → 4) Fit regression models mapping properties → shift magnitudes

- **Design tradeoffs:** Small models (1-2B parameters) enable 800+ experiments but may not generalize to frontier models; LLM-as-judge enables scalable evaluation but correlates ~0.52 with human judgments; discrete generations approximate continuous real-world feedback loops

- **Failure signatures:** Non-monotonic diversity loss curves (Wikipedia U-shape) indicate model prior–data misalignment; high VIF scores (>5) in early regression attempts required predictor reduction; R² values 0.1–0.5 indicate substantial unexplained variance

- **First 3 experiments:** 1) Replicate synthetic ratio sweep on a single dataset to validate plateau behavior at r ≥ 0.5 → 2) Run controlled quality manipulation (split dataset by quality quartiles) to isolate quality effects from confounds → 3) Test modularity hypothesis by training on mixed-domain data and measuring cross-domain coefficient significance

## Open Questions the Paper Calls Out

### Open Question 1
Do the effects of specific data properties on distribution shift scale to larger models and different training methods? The study exclusively utilizes small language models (1B–2B parameters) and LoRA fine-tuning. It remains unclear if the predictive power of properties like lexical diversity holds for foundation models or reinforcement learning-based training.

### Open Question 2
How does the topology of the training loop (e.g., networked vs. linear) influence the modularity and severity of distribution shifts? The experimental design uses discrete, linear chains, whereas real-world recursive training happens in networks of LLMs rather than in linear chains.

### Open Question 3
Can actively manipulating dataset properties function as a reliable control mechanism to prevent model collapse? The paper identifies correlation-based predictors but does not test the active engineering of these properties as an intervention to causally prevent collapse.

## Limitations
- Use of small 1-2B parameter models limits generalizability to frontier LLMs
- Synthetic-to-human ratio control (1/8 and 1/4) may not capture full spectrum of real-world contamination levels
- LLM-as-judge evaluation shows only moderate correlation (~0.52) with human judgments and hasn't been validated for social media content
- Regression analysis explains only 10-50% of variance in distribution shifts, suggesting important unmeasured factors

## Confidence

**High Confidence:** Different human datasets produce varying magnitudes of distribution shifts; domain-specific properties primarily affect generation within their own domain.

**Medium Confidence:** Lexical diversity amplifies distribution shifts (significant in 11/18 regression conditions); semantic diversity and quality mitigate shifts (supported by regression coefficients).

**Low Confidence:** Political bias findings are limited by binary classification approach and specific datasets used; direction-dependent amplification/inversion effects require further validation.

## Next Checks

1. **Replicate synthetic ratio sweep** on a single dataset to validate plateau behavior at r ≥ 0.5, ensuring the iterative chain implementation correctly captures accumulation dynamics.

2. **Run controlled quality manipulation** by splitting a dataset into quality quartiles and training chains on each subset separately, isolating quality effects from lexical and semantic confounds.

3. **Test modularity hypothesis** by training on mixed-domain data and measuring cross-domain coefficient significance—if >20% of domain pairs show significant effects, the current modularity assumption requires revision.