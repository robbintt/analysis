---
ver: rpa2
title: Towards Audio Token Compression in Large Audio Language Models
arxiv_id: '2511.20973'
source_url: https://arxiv.org/abs/2511.20973
tags:
- audio
- arxiv
- speech
- language
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of high token rates and quadratic
  attention complexity in Large Audio Language Models (LALMs), which limits their
  scalability for long-form audio and deployment on resource-constrained devices.
  The authors propose compressing audio tokens before they are fed into the LLM decoder
  using techniques such as unsupervised segmentation, uniform average pooling, and
  uniform downsampling.
---

# Towards Audio Token Compression in Large Audio Language Models

## Quick Facts
- arXiv ID: 2511.20973
- Source URL: https://arxiv.org/abs/2511.20973
- Reference count: 40
- This paper proposes compressing audio tokens before feeding them to LLMs using pooling/sampling techniques, achieving 3x token reduction with minimal performance loss in ASR and S2TT tasks.

## Executive Summary
This paper addresses the scalability challenge in Large Audio Language Models (LALMs) caused by quadratic attention complexity and high audio token rates. The authors propose compressing audio tokens before they reach the LLM decoder using techniques like uniform average pooling, uniform downsampling, and unsupervised segmentation. To maintain performance after compression, they employ low-rank adapters (LoRA) to finetune the LLM. Experiments on ASR and S2TT tasks demonstrate that compressed LALMs can reduce input token counts by up to 3× while maintaining performance close to frame-level models, with WER increases of less than 1 point at 2× compression.

## Method Summary
The approach involves three main components: (1) audio preprocessing to 16kHz mel-spectrograms, (2) token compression using uniform average pooling, uniform sampling, or unsupervised segmentation before the LLM, and (3) LoRA-based finetuning of the LLM's attention layers to realign with compressed representations. The system uses Whisper-large-v3 as the audio encoder and Qwen2-7B as the LLM backbone. Compression factors of 2-3× achieve the best trade-off between token reduction and performance maintenance, with uniform average pooling showing superior results to uniform sampling.

## Key Results
- Uniform average pooling with compression factor 3 increased WER by less than 1 point compared to baseline
- Token compression achieved up to 3× reduction in input audio token count
- Cross-task transfer showed ASR training does not generalize well to S2TT without task-specific finetuning
- Multilingual ASR models trained on English data showed good transfer to German, Spanish, and other languages

## Why This Works (Mechanism)

### Mechanism 1: Token Compression Reduces Quadratic Attention Complexity
Reducing audio token count before the LLM backbone mitigates the quadratic scaling of transformer self-attention. Audio encoders output ~50 features/second, so compression modules (uniform average pooling, uniform sampling, or unsupervised segmentation) reduce this rate by factors of 2–6× before LLM processing, lowering the sequence length that attention must process. At compression factor ≥6, WER approximately doubles, indicating information loss exceeds recoverable redundancy.

### Mechanism 2: LoRA Realigns Compressed Audio Embeddings to LLM Input Space
Adding low-rank adapters to attention layers restores alignment between compressed audio representations and the LLM's learned input distribution. LoRA adds ~9M learnable parameters (rank=16, α=32) to key and query projections across all attention layers, allowing efficient finetuning without modifying the frozen LLM backbone or audio encoder. If compression alters temporal structure too aggressively (e.g., uniform sampling at high factors), LoRA alone may not recover alignment.

### Mechanism 3: Token Merging Preserves Information Better Than Token Dropping
Uniform average pooling (token merging) outperforms uniform sampling (token dropping) for preserving lexical content under compression. Average pooling aggregates information from K adjacent frames into one representation, smoothing while retaining signal. Sampling discards K−1 of K frames, losing frame-specific detail. Beyond moderate compression (3–4×), even averaging accumulates error.

## Foundational Learning

- **Quadratic Attention Complexity**
  - Why needed here: Understanding why token reduction matters requires grasping that self-attention computes pairwise interactions, scaling as O(n²) in sequence length.
  - Quick check question: If you halve the audio token count from 500 to 250, by what factor does attention computation decrease? (Answer: ~4×)

- **Mel-Spectrogram Features and Frame Rates**
  - Why needed here: The paper assumes knowledge that audio is converted to spectral features at fixed frame rates (e.g., 100 frames/sec before pooling), which determines token density.
  - Quick check question: A 16kHz audio signal with 10ms stride produces how many frames per second? (Answer: 100)

- **Low-Rank Adaptation (LoRA)**
  - Why needed here: The method relies on LoRA to efficiently adapt a frozen LLM to new input distributions without full finetuning.
  - Quick check question: LoRA adds learnable matrices A and B such that ΔW = BA. If the original weight is 4096×4096 and rank r=16, how many new parameters does LoRA add per weight matrix? (Answer: 4096×16 + 16×4096 = 131,072)

## Architecture Onboarding

- **Component map:**
  Audio Input (16kHz) -> Mel-Spectrogram (128-dim, 25ms window, 10ms stride) -> Audio Encoder (Whisper-large-v3, outputs 50 tok/s) -> Compression Module (UniAvg / UniSamp / UnSeg) -> LoRA-Adapted LLM Backbone (Qwen2-7B, rank=16, α=32) -> Output (ASR transcript or S2TT translation)

- **Critical path:** Audio preprocessing -> encoder feature extraction -> compression (this is where token reduction occurs) -> LoRA-modified attention layers -> token generation. Errors in compression choice or LoRA configuration directly affect downstream performance.

- **Design tradeoffs:**
  - Compression factor vs. WER: Table I shows 2× compression adds <1 WER; 3× adds ~1–1.5 WER; 6× doubles WER.
  - Merging vs. sampling: UniAvg is more robust; UniSamp degrades faster but is computationally simpler.
  - Training data scope: Section V-B shows English-only training transfers to other languages for ASR, but Section V-C shows ASR training does not transfer well to S2TT—task-specific finetuning is still required.

- **Failure signatures:**
  - WER jumps disproportionately (>2 points) when switching from UniAvg to UniSamp at same compression factor -> indicates sampling is losing critical frames.
  - Cross-task transfer fails (e.g., ASR-trained model underperforms on S2TT) -> LoRA has overfit to source task; add task-specific data.
  - Meaning/readability scores drop (Table III) despite moderate WER -> compression may be disrupting prosodic/structural cues; consider unsupervised segmentation instead of uniform methods.

- **First 3 experiments:**
  1. Apply UniAvg with factors [2, 3, 4, 5, 6] on LibriSpeech test-clean; plot WER vs. tok/s to identify acceptable operating point for your compute budget.
  2. Train with LoRA on key+query projections vs. key-only vs. full attention; measure WER recovery gap to confirm the paper's choice is optimal for your LLM.
  3. Finetune LoRA on English-only CommonVoice, then evaluate on German, Spanish, and Mandarin to quantify transfer degradation before committing to multilingual training data collection.

## Open Questions the Paper Calls Out

### Open Question 1
Can audio token compression be achieved in Large Audio Language Models (LALMs) without requiring Low-Rank Adapters (LoRA) or additional training to realign the embedding space?
- Basis in paper: The Conclusion states a future goal to "explore methods that do not require training the model with additional LoRA weights for rebalancing the attention weights."
- Why unresolved: The current study relies on LoRA to bridge the distribution mismatch between compressed audio embeddings and the frozen LLM backbone, adding training overhead.
- Evidence: A "training-free" compression mechanism that maintains baseline ASR/S2TT performance without adapter modules or parameter updates.

### Open Question 2
How does aggressive token compression impact performance on non-lexical tasks like audio classification or music understanding compared to the lexical tasks studied?
- Basis in paper: The authors explicitly list a desire to "benchmark the compressed LALMs on other tasks, such as audio classification and understanding" in their future work.
- Why unresolved: The current evaluation is restricted to ASR and speech-to-text translation, which rely heavily on lexical content; it is unclear if downsampling destroys acoustic features required for non-speech reasoning.
- Evidence: Benchmarking results on datasets like AudioSet or music captioning using the proposed uniform average pooling and unsupervised segmentation methods.

### Open Question 3
To what extent is paralinguistic information (e.g., prosody, speaker identity) preserved under the proposed uniform average pooling and unsupervised segmentation techniques?
- Basis in paper: The Introduction argues against transcription-based methods because they "discard important paralinguistic information," yet the paper does not evaluate if the proposed pooling methods similarly smooth out these subtle features.
- Why unresolved: The study focuses exclusively on Word Error Rate (WER) and BLEU scores, leaving the survival of non-semantic cues unverified.
- Evidence: Evaluation of speaker verification or emotion recognition accuracy using compressed tokens compared to frame-level features.

## Limitations
- Only three compression techniques evaluated on a single encoder-LLM combination, leaving generalizability to other architectures uncertain
- LoRA configuration sensitivity not explored; optimal parameters may vary with compression factor and task complexity
- Cross-task transfer boundaries identified but not deeply investigated; compression doesn't overcome fundamental task alignment requirements

## Confidence
- **High confidence** in the core compression benefit: The quadratic attention complexity reduction is mathematically sound, and experimental results consistently show WER/BLEU degradation stays below 1-2 points at 2-3× compression across multiple datasets and tasks.
- **Medium confidence** in LoRA effectiveness: While the paper demonstrates LoRA improves compressed model performance over frozen LLM baselines, it doesn't compare against other adaptation methods or explore optimal LoRA hyperparameters.
- **Medium confidence** in token merging superiority: The paper shows uniform average pooling outperforms uniform sampling in controlled experiments, but doesn't test other merging strategies or investigate whether this advantage holds at different compression factors or with different audio characteristics.

## Next Checks
1. Apply UniAvg compression with factors [2, 3, 4, 5, 6] on your target dataset, measuring WER/BLEU against token rate reduction. Plot the trade-off curve to identify your optimal compression factor, as the paper's 2-3× sweet spot may shift with different audio characteristics or computational constraints.

2. Train compressed models with LoRA rank values [8, 16, 32] and α values [16, 32, 64] on your target task. Compare WER/BLEU recovery against parameter count to identify the most parameter-efficient configuration for your use case, as the paper's rank=16, α=32 may not be optimal.

3. Implement the compression + LoRA pipeline with an alternative audio encoder (e.g., HuBERT or Wav2Vec2) while keeping the same LLM architecture. Evaluate whether the 2-3× compression sweet spot and LoRA effectiveness transfer across encoders, as the paper only tests Whisper-large-v3.