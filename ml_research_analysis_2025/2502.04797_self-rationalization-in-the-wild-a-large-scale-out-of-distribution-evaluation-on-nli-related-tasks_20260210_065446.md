---
ver: rpa2
title: 'Self-Rationalization in the Wild: A Large Scale Out-of-Distribution Evaluation
  on NLI-related tasks'
arxiv_id: '2502.04797'
source_url: https://arxiv.org/abs/2502.04797
tags:
- datasets
- human
- score
- association
- acceptability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper conducts a large-scale out-of-distribution (OOD) evaluation
  of self-rationalization models across 19 diverse datasets spanning three tasks:
  natural language inference (NLI), fact-checking (FC), and hallucination detection
  in abstractive summarization (HDAS). The authors fine-tune T5-Large and OLMo-7B
  models using few-shot learning from two source datasets (e-SNLI and e-FEVER) and
  evaluate their generalization performance on OOD datasets with varying domains,
  text structures, and lengths.'
---

# Self-Rationalization in the Wild: A Large Scale Out-of-Distribution Evaluation on NLI-related tasks

## Quick Facts
- **arXiv ID**: 2502.04797
- **Source URL**: https://arxiv.org/abs/2502.04797
- **Reference count**: 40
- **Primary result**: Few-shot fine-tuning yields comparable OOD performance to full-shot training across 19 diverse datasets spanning NLI, FC, and HDAS tasks.

## Executive Summary
This paper presents a large-scale out-of-distribution evaluation of self-rationalization models across 19 diverse datasets spanning three tasks: natural language inference (NLI), fact-checking (FC), and hallucination detection in abstractive summarization (HDAS). The authors fine-tune T5-Large and OLMo-7B models using few-shot learning from two source datasets (e-SNLI and e-FEVER) and evaluate their generalization performance on OOD datasets with varying domains, text structures, and lengths. The study introduces an acceptability-based sample selection method and evaluates explanations using both human judgments and reference-free metrics, finding that few-shot models achieve similar human scores to their full-shot counterparts and that the Acceptability score (T5-11B) shows the strongest correlation with human evaluations across all tasks.

## Method Summary
The authors fine-tune T5-Large and OLMo-7B models on source datasets (e-SNLI, e-FEVER) using prompt-based fine-tuning with few-shot learning. They employ acceptability filtering (threshold < 0.3) using T5-Large to improve data quality, combined with sample selection strategies like FastVote-k or random selection. Models are evaluated on 19 OOD datasets across NLI, FC, and HDAS tasks using macro F1 for label accuracy and Acceptability score (T5-11B) plus human evaluation for explanation quality. The paper introduces a new acceptability-based sample selection method and compares the impact of data source choice versus sample selection strategy on OOD performance.

## Key Results
- Fine-tuning on few annotated examples yields comparable OOD performance to full-shot training across all tasks
- The choice of fine-tuning data source has a larger impact on OOD performance than sample selection strategies
- Models with higher label prediction accuracy tend to produce better explanations, as measured by the Acceptability score
- T5-Large models show more stable OOD performance while OLMo-7B models generally excel in explanation generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot fine-tuning enables effective OOD generalization for self-rationalization comparable to full-shot fine-tuning.
- Mechanism: Prompt-based fine-tuning on a small, curated subset of source data allows the model to learn the joint generation of labels and explanations without overfitting to source dataset artifacts, preserving transferability to OOD tasks.
- Core assumption: Base model possesses sufficient pre-existing reasoning and linguistic capabilities that only need a few examples to learn the self-rationalization task format.
- Evidence anchors:
  - [abstract] "Our findings reveal: 1) few annotated examples effectively adapt models for OOD explanation generation..."
  - [section 5.2.4, Table 4] "Overall, few-shot models achieved similar human scores to their full-shot counterparts...".
  - [corpus] A related paper explores OOD generalization, providing broader context though not directly confirming this specific mechanism.
- Break condition: Effectiveness is contingent on base model's pre-training quality; catastrophic forgetting shows too much fine-tuning can be detrimental.

### Mechanism 2
- Claim: Quality of fine-tuning data source is a more significant driver of OOD performance than sample selection strategy.
- Mechanism: Fundamental characteristics of source dataset (explanation style, domain, reasoning type) establish upper bound for learned behavior. e-FEVER transfers better to FC and HDAS tasks than e-SNLI due to real-world claims and evidence.
- Core assumption: OOD target tasks share underlying semantic or structural properties with chosen fine-tuning data source, creating favorable transfer learning path.
- Evidence anchors:
  - [abstract] "...fine-tuning data source has a larger impact on OOD performance than sample selection strategies."
  - [section 5.3.2, Table 5] Shows e-FEVER models generally outperform e-SNLI models on FC and HDAS tasks in terms of F1.
- Break condition: May not hold if OOD target domain is radically different from available fine-tuning source; very poor selection strategy could undermine good data source.

### Mechanism 3
- Claim: Acceptability Score (T5-11B) serves as reliable, reference-free proxy for human judgment of free-text explanation quality and correlates positively with label prediction accuracy.
- Mechanism: Acceptability model provides robust, model-based metric. Correlation with label accuracy suggests models that "understand" task well enough to make correct predictions are also better positioned to generate coherent and plausible explanations.
- Core assumption: Acceptability model's training on SNLI data generalizes sufficiently to evaluate explanations across NLI, Fact-Checking, and HDAS tasks.
- Evidence anchors:
  - [abstract] "...the Acceptability score (T5-11B) shows the strongest correlation with human judgments..."
  - [section 5.2.3, Table 3] Quantifies correlation, showing Acceptability (T5-11B) outperforms other LLM-based metrics.
- Break condition: Correlation is general trend, not perfect rule; models could achieve high accuracy via spurious correlations while generating plausible-sounding but unfaithful explanations.

## Foundational Learning

- **Concept: Self-Rationalization**
  - Why needed here: This is the core taskâ€”a model must jointly predict a label and generate a free-text explanation for its decision.
  - Quick check question: Can you explain the difference between self-rationalization and simply generating a label followed by a separate post-hoc explanation?

- **Concept: Out-of-Distribution (OOD) Generalization**
  - Why needed here: The central evaluation challenge. You must understand that models are tested on 19 datasets with different domains, text lengths, and tasks than the fine-tuning data.
  - Quick check question: Why is evaluating on OOD data critical for claims about a model's general reasoning ability versus its ability to memorize a specific dataset's patterns?

- **Concept: Acceptability Filtering**
  - Why needed here: A key practical technique in the paper. You need to understand that training data quality is improved by using a model to filter out low-quality explanations.
  - Quick check question: How does using the Acceptability model (T5-Large) as a filter for training data relate to using the Acceptability model (T5-11B) as an evaluation metric?

## Architecture Onboarding

- **Component map**: Source Datasets (e-SNLI, e-FEVER) -> Acceptability Filtering -> Sample Selector (e.g., FastVote-k) -> Student Model (T5-Large or OLMo-7B) -> OOD Datasets (19 benchmarks) -> Evaluator (Acceptability Score T5-11B and other LLM-based metrics)

- **Critical path**: Source Data -> [Acceptability Filtering] -> [Sample Selection] -> [Prompt-based Fine-Tuning] -> [Inference on OOD Data] -> [Evaluation via Acceptability Score]. Selection of source dataset is critical decision point that influences downstream OOD performance.

- **Design tradeoffs**:
  - **Base Model Choice**: T5-Large offers more stable OOD performance out-of-the-box (due to NLI pre-training), while OLMo-7B has higher potential but is less stable and suffers from catastrophic forgetting with full-shot fine-tuning on certain data.
  - **Data Source vs. Selection**: Investing effort in finding more domain-relevant source dataset yields higher returns than engineering complex sample selection strategies.
  - **Few-shot vs. Full-shot**: For many configurations, few-shot fine-tuning (e.g., 128 shots) is computationally cheaper and yields performance on par with or better than full-shot fine-tuning.

- **Failure signatures**:
  - **Catastrophic Forgetting**: Observed in OLMo model fine-tuned on full e-SNLI dataset, leading to severe drop in explanation quality.
  - **Template Extraction Failure**: Models, especially OLMo with few shots, may fail to follow output JSON/text template, requiring robust post-processing logic to extract labels and explanations.

- **First 3 experiments**:
  1. Reproduce the Pareto Front: Fine-tune T5-Large on e-SNLI and e-FEVER with varying few-shot counts (e.g., 16, 64, 128 shots) and plot the Pareto front of F1 vs. Acceptability score to confirm trade-off between accuracy and explanation quality.
  2. Ablate the Data Source: Fine-tune same model on e-SNLI and e-FEVER, then evaluate on subset of FC and HDAS tasks. Quantify performance difference to validate claim that data source impact exceeds sample selection impact.
  3. Metric Correlation Check: Generate explanations using held-out model, evaluate with both Acceptability Score and human evaluation on small sample, and calculate correlation to confirm metric's reliability for specific use case.

## Open Questions the Paper Calls Out
None

## Limitations

- **Data Generalization Limits**: Evaluation covers 19 datasets but all are text-based NLI-related tasks; transferability to completely different modalities or reasoning types (e.g., visual reasoning, mathematical proof) remains unproven.
- **Metric Reliability Concerns**: Acceptability Score shows strongest correlation with human judgments but this correlation is particularly pronounced in lower score ranges; performance at evaluating high-quality explanations remains less certain.
- **Base Model Constraints**: Acknowledges catastrophic forgetting in OLMo-7B with full-shot fine-tuning but doesn't systematically explore whether this limitation applies to other base models or whether architectural modifications could mitigate this issue.

## Confidence

- **High Confidence**: Claims about general effectiveness of few-shot fine-tuning for OOD generalization are well-supported by quantitative results across multiple datasets; empirical finding that e-FEVER generally outperforms e-SNLI as source dataset for FC and HDAS tasks is consistently demonstrated.
- **Medium Confidence**: Assertion that data source choice has larger impact than sample selection strategy is supported but based primarily on comparisons between acceptability filtering and FastVote-k; relative importance could vary with different selection methods or source datasets not tested.
- **Low Confidence**: Correlation between label accuracy and explanation quality (Acceptability score) is presented as general principle but paper acknowledges this correlation is stronger in lower score ranges and may not hold universally; mechanism explaining why models that predict better also explain better remains somewhat speculative.

## Next Checks

1. **Cross-Domain Transfer Test**: Evaluate best-performing T5-Large and OLMo-7B models from this study on completely different reasoning task (e.g., visual question answering or mathematical problem solving) to test true limits of their OOD generalization capabilities beyond NLI-related domains.

2. **Metric Stress Test**: Design adversarial test cases where models achieve high label accuracy through spurious correlations but generate poor explanations, then evaluate whether Acceptability Score correctly identifies these cases as low-quality to validate whether metric truly measures reasoning quality or just surface coherence.

3. **Architectural Ablation Study**: Systematically compare T5-Large and OLMo-7B performance when fine-tuned with identical hyperparameters and data sources, controlling for catastrophic forgetting effects; additionally test whether intermediate fine-tuning on related tasks before target OOD task improves performance to identify specific mechanisms enabling successful transfer.