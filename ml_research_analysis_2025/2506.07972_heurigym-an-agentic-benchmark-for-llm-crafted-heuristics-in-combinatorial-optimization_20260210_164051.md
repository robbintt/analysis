---
ver: rpa2
title: 'HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial
  Optimization'
arxiv_id: '2506.07972'
source_url: https://arxiv.org/abs/2506.07972
tags:
- problem
- name
- conference
- design
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HeuriGym, an agentic benchmark for evaluating
  large language models' ability to generate heuristics for combinatorial optimization
  problems. The core method idea involves an iterative feedback loop where LLMs generate
  complete heuristic algorithms, receive execution feedback, and refine their solutions.
---

# HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization

## Quick Facts
- arXiv ID: 2506.07972
- Source URL: https://arxiv.org/abs/2506.07972
- Reference count: 40
- Primary result: Even top LLMs achieve only 0.6 QYI score, well below expert baseline of 1.0

## Executive Summary
This paper introduces HeuriGym, an agentic benchmark for evaluating large language models' ability to generate heuristics for combinatorial optimization problems. The framework implements an iterative feedback loop where LLMs generate complete heuristic algorithms, receive execution feedback, and refine their solutions. Across nine diverse optimization problems spanning domains like computer systems, logistics, and biology, even the strongest models like GPT-o4-mini-high and Gemini-2.5-Pro achieve only 0.6 QYI scores, revealing substantial gaps in current LLMs' reasoning, tool use, planning, and adaptive capabilities for realistic problem-solving.

## Method Summary
The HeuriGym framework evaluates LLMs on generating heuristics for 9 combinatorial optimization problems through an iterative agentic loop. LLMs receive natural language problem descriptions and generate Python code, which is executed in an isolated sandbox with 8 CPU cores and specified libraries. Execution logs, verification outcomes, and evaluation costs from a small demonstration set are appended back to the prompt for iterative refinement (up to 10 iterations). Performance is quantified using the Quality-Yield Index (QYI), the harmonic mean of solution pass rate (yield) and quality relative to expert baselines. The benchmark intentionally selects problems with fewer than 1,000 citations and large solution spaces to resist memorization.

## Key Results
- GPT-o4-mini-high achieves QYI of 0.4574, Gemini-2.5-Pro reaches 0.6170, both well below expert baseline of 1.0
- Even with 10 iterations of feedback, LLMs struggle with constraint satisfaction and algorithmic efficiency
- Temperature-controlled sampling reveals fundamental trade-off between solution yield and quality
- Evolutionary frameworks with candidate sampling improve performance but face scalability challenges

## Why This Works (Mechanism)

### Mechanism 1: Iterative Feedback-Driven Refinement
- Claim: Providing execution feedback to LLMs allows them to iteratively refine heuristics, improving solution quality over single-shot generation.
- Mechanism: The framework executes LLM-generated code on a demonstration set. Errors, constraint violations, and objective costs are captured as logs. This information is fed back into the LLM's prompt in subsequent iterations, enabling it to debug logic, fix API usage, and adopt more sophisticated algorithms based on past performance.
- Core assumption: LLMs possess sufficient in-context learning capabilities to effectively interpret error messages and performance metrics to improve their generated code.
- Evidence anchors:
  - [abstract] "HeuriGym empowers LLMs to propose heuristics, receive evaluative feedback via code execution, and iteratively refine their solutions."
  - [section 3.1.3] "The framework incorporates a feedback loop: execution logs, verification outcomes, and evaluation costs from a small demonstration set are appended back to the prompt, enabling iterative refinement."
  - [corpus] Corpus evidence on iterative refinement specifically for heuristics is weak; general LLM code generation literature supports it.
- Break condition: The feedback is too noisy or complex for the model to interpret, or context window limitations prevent the model from effectively using the history of attempts, leading to repeated failures or regression.

### Mechanism 2: Task Selection to Resist Memorization
- Claim: Selecting optimization problems with limited academic citation counts and large solution spaces forces LLMs to engage in genuine reasoning rather than pattern matching.
- Mechanism: By excluding ubiquitous problems like TSP or SAT and selecting problems with fewer than 1,000 citations, the benchmark minimizes the likelihood that direct solutions are present in the model's pretraining data. The vast solution spaces, often combinatorially large, further preclude memorization of specific solutions.
- Core assumption: The model's reasoning capabilities can generalize from known algorithms and principles to novel problem formulations and constraints.
- Evidence anchors:
  - [abstract] "...assess whether LLMs can produce high-quality solutions to novel yet foundational problems... characterized by clearly defined objectives and expansive solution spaces."
  - [section 4.1] "...we intentionally exclude ubiquitous problems... This 1,000-citation cutoff is a practical criterion to exclude heavily standardized textbook CO problems that are almost certainly present in LLM training corpora..."
  - [corpus] Weak corpus evidence on the efficacy of this specific selection criterion; relies on general principles of generalization.
- Break condition: The problem, despite being "novel," is compositionally similar to well-known problems, allowing the model to apply a known template without deeper reasoning.

### Mechanism 3: Unified Quality-Yield Metric (QYI)
- Claim: The Quality-Yield Index provides a more holistic evaluation of LLM performance than traditional metrics like pass@k.
- Mechanism: The QYI metric is the harmonic mean of two factors: Yield (the proportion of instances for which a feasible solution is found) and Quality (how close the solution cost is to an expert baseline). This balances the ability to produce *any* valid solution with the ability to produce a *good* solution, avoiding the ceiling effect of simple accuracy metrics.
- Core assumption: An expert-provided heuristic represents a reasonable upper bound for "quality" and that there is a meaningful trade-off between solution yield and quality for current models.
- Evidence anchors:
  - [abstract] "To quantify performance, we propose the Quality-Yield Index (QYI), a metric that captures both solution pass rate and quality."
  - [section 3.2] "We define a unified metric, the Quality-Yield Index (QYI), as the harmonic mean of quality and yield. This formulation... penalizes imbalanced values more strongly than the arithmetic mean."
  - [corpus] No direct corpus evidence supports the superiority of QYI over other metrics; it is a novel contribution of this paper.
- Break condition: The expert baseline is either unreasonably weak (inflating QYI) or unreasonably strong (deflating QYI), making comparisons less meaningful.

## Foundational Learning

- Concept: Combinatorial Optimization (CO)
  - Why needed here: The entire benchmark is built around CO problems. You must understand that these problems involve finding an optimal solution from a finite set of possibilities, often NP-hard.
  - Quick check question: Can you explain the difference between an exact solver (like ILP) and a heuristic approach, and why heuristics are preferred for large-scale CO problems?

- Concept: In-Context Learning & Iterative Refinement
  - Why needed here: This is the core mechanism by which LLMs improve their solutions in this framework. You need to grasp how models use information in their prompt window (like previous errors and code) to adjust their output without weight updates.
  - Quick check question: How does appending error logs to a prompt enable an LLM to "debug" its own previously generated code?

- Concept: F-Score / Harmonic Mean
  - Why needed here: The primary evaluation metric, QYI, is the harmonic mean of Quality and Yield. You must understand why the harmonic mean (which penalizes extreme values) is chosen over the arithmetic mean for this combined metric.
  - Quick check question: If a model has a Quality of 0.9 but a Yield of 0.1, how will the harmonic mean-based QYI compare to the arithmetic mean, and what does that signify?

## Architecture Onboarding

- Component map: Problem Descriptions -> LLM Agent -> Execution Environment -> Verifier & Evaluator -> Feedback Loop -> LLM Agent
- Critical path: A new engineer should first focus on understanding the prompt construction, specifically how problem descriptions are structured. Next, they should trace the execution of generated code in the sandbox and see what information is produced by the Verifier and Evaluator. Finally, they must understand how this output is formatted and injected back into the LLM's prompt for subsequent iterations.
- Design tradeoffs:
  - Feedback Granularity: Providing full execution traces may overwhelm the context window but offers detailed debugging info. Summaries save tokens but may miss critical details. The paper uses logs from a small demonstration set as a trade-off.
  - Problem Novelty vs. Difficulty: Selecting obscure problems resists memorization but may make the benchmark too hard for current models, providing little signal. The 1,000 citation cutoff is a heuristic balance.
- Failure signatures:
  - Timeouts: Generated code is algorithmically inefficient or falls into an infinite loop.
  - Constraint Violations: The code runs but produces solutions that don't satisfy hard problem constraints (e.g., scheduling conflicts).
  - Hallucinated APIs: The code attempts to import libraries or call functions that are not available in the specified environment.
  - Regression: A solution in a later iteration is worse than an earlier one, indicating the model has misinterpreted feedback or overcorrected.
- First 3 experiments:
  1. Baseline Evaluation: Run a strong model (e.g., GPT-o4-mini or Gemini-2.5-Pro) on a single problem from the suite with 1, 5, and 10 iterations to observe the SOLVE@i and QYI improvement curves.
  2. Ablation on Feedback: Remove the error logs and objective scores from the feedback loop and measure the performance drop to quantify the value of iterative refinement.
  3. Temperature Sweep: Run the same model with different temperatures (e.g., 0.0, 0.5, 1.0) on a problem to explore the trade-off between solution yield and quality.

## Open Questions the Paper Calls Out

- Can multi-agent architectures with specialized agents for problem decomposition significantly improve LLM performance on HeuriGym tasks compared to single-agent approaches?
  - Basis: Section 6 states multi-agent designs have shown strong potential but remain unexplored in the current framework.
  - Why unresolved: The current agentic pipeline uses only a standard single-agent configuration; multi-agent strategies were not tested.
  - What evidence would resolve it: Comparative experiments between single-agent baseline and multi-agent architectures across all nine problems, measuring SOLVE@i and QYI improvements.

- What is the optimal balance between temperature-controlled sampling diversity and constraint satisfaction for combinatorial optimization tasks?
  - Basis: Section 5.2 and Appendix E.3 reveal a fundamental trade-off: higher temperature increases quality but lowers yield due to more invalid outputs.
  - Why unresolved: The paper identifies the trade-off empirically but does not propose or test mechanisms to overcome it.
  - What evidence would resolve it: Experiments with adaptive temperature schedules or hybrid strategies that maintain high yield while achieving expert-level quality.

- How can iterative feedback mechanisms be scaled to effectively consolidate and reconcile conflicting refinements from multiple candidate programs?
  - Basis: Section 5 notes improved mechanisms are needed to consolidate and reconcile feedback from different sampled candidates so that refinements do not conflict with one another.
  - Why unresolved: Current approaches of simply appending sampled programs to prompts do not scale; population size is limited by context length.
  - What evidence would resolve it: A feedback aggregation mechanism that achieves higher QYI than baseline Gemini-2.5-Pro (0.6170) with larger population sizes.

## Limitations

- The evaluation framework depends on black-box API access to proprietary LLMs, making reproducibility dependent on specific model versions and API configurations.
- The QYI metric, while novel, lacks external validation and relies on expert baselines that are not fully transparent.
- The iterative feedback mechanism assumes LLMs can meaningfully interpret execution logs, but the quality and structure of this feedback significantly impact performance and are not extensively characterized.

## Confidence

- High Confidence: The iterative feedback loop mechanism and the QYI metric formulation are well-specified and internally consistent with the framework's design.
- Medium Confidence: The problem selection strategy is logically sound but lacks direct empirical validation for this specific benchmark.
- Medium Confidence: The performance gap between models and expert baselines is clearly demonstrated, but the exact cause (reasoning, tool use, planning) requires further decomposition.

## Next Checks

1. Ablation Study on Feedback: Systematically remove different components of the feedback (execution logs, verification results, objective costs) in the iterative loop to quantify the contribution of each element to the improvement in SOLVE@i and QYI.

2. Cross-Problem Generalization: Evaluate a single, strong model on a held-out problem not in the benchmark suite to test whether performance improvements on HeuriGym problems transfer to genuinely novel optimization challenges.

3. Expert Baseline Sensitivity: Conduct a sensitivity analysis on the QYI metric by varying the expert baseline difficulty (e.g., using weaker or stronger reference algorithms) to assess how robust the metric is to changes in the assumed "optimal" performance level.