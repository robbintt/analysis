---
ver: rpa2
title: Cost-Aware Optimal Pairwise Pure Exploration
arxiv_id: '2503.07877'
source_url: https://arxiv.org/abs/2503.07877
tags:
- exploration
- pure
- which
- have
- arms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a general framework for pure exploration in
  multi-armed bandits, focusing on identifying pairwise relationships between arms
  while optimizing cumulative costs. A novel algorithm, CAET, is proposed, leveraging
  track-and-stop principles with a novel forced exploration method to handle zero-cost
  arms.
---

# Cost-Aware Optimal Pairwise Pure Exploration

## Quick Facts
- arXiv ID: 2503.07877
- Source URL: https://arxiv.org/abs/2503.07877
- Authors: Di Wu; Chengshuai Shi; Ruida Zhou; Cong Shen
- Reference count: 40
- Primary result: Introduces CAET algorithm for cost-aware pure exploration in MAB with theoretical optimality guarantees and forced exploration for zero-cost arms

## Executive Summary
This work introduces a general framework for pure exploration in multi-armed bandits, focusing on identifying pairwise relationships between arms while optimizing cumulative costs. A novel algorithm, CAET, is proposed, leveraging track-and-stop principles with a novel forced exploration method to handle zero-cost arms. Theoretical analyses prove CAET asymptotically approaches the fundamental performance lower bound. Extensions to regret minimization via explore-then-commit schemes are also discussed, achieving optimal performance. Experimental results corroborate the algorithm's effectiveness and efficiency under various settings.

## Method Summary
CAET implements a track-and-stop algorithm for cost-aware pure exploration. The method tracks the optimal sampling proportion ω* derived from a cost-weighted KL divergence optimization problem, using a plug-in estimator. For zero-cost arms, a forced exploration mechanism via mixing parameter α ensures sufficient sampling. The stopping rule uses generalized likelihood ratio statistics Z_{a,b}(t) compared to a theoretically derived threshold β(t,δ). The algorithm maintains empirical estimates of means and costs, projects the sampling distribution to ensure feasibility, and outputs a partition of arms when the stopping condition is met. For regret minimization, the framework is extended by treating sub-optimality gaps as costs in an explore-then-commit scheme.

## Key Results
- CAET algorithm achieves asymptotic optimality, matching the theoretical lower bound on cumulative cost as δ→0
- Novel forced exploration method successfully handles zero-cost arms that would otherwise create degeneracy in the optimization
- Extensions to regret minimization via explore-then-commit achieve optimal performance
- Experimental results validate effectiveness across various synthetic bandit settings

## Why This Works (Mechanism)

### Mechanism 1: Cost-Weighted Optimal Allocation
The algorithm minimizes cumulative cost by adapting sampling frequency proportional to the inverse of arm costs. The theoretical lower bound suggests optimal sampling proportion ω* must maximize a function involving terms like ω_a d(μ_a, λ_a)/c_a. CAET uses a plug-in estimator to track this optimal proportion u*(c,μ), effectively pulling expensive arms less frequently than cheap ones for the same information gain.

### Mechanism 2: Forced Exploration via Mixing
The presence of zero-cost arms creates a degeneracy in the optimization objective, resolved by forcing a specific, decaying rate of exploration on these arms. Standard cost-weighting fails for zero-cost arms (division by zero). CAET introduces mixing parameter α (dependent on δ) to split sampling between uniform exploration over zero-cost arms and standard optimal tracking for costly arms.

### Mechanism 3: Track-and-Stop with Generalized Likelihood Ratio (GLRT)
The algorithm ensures δ-PAC correctness by stopping only when statistical evidence for a specific pairwise ordering exceeds a theoretical threshold. It uses track-and-stop principle: sampling rule tracks optimal proportion while stopping rule calculates GLRT statistic Z_{a,b}(t) for all critical pairs, stopping only when min Z_{a,b}(t) > β(t, δ).

## Foundational Learning

- **Concept: Pure Exploration vs. Regret Minimization**
  - Why needed: CAET operates in "Pure Exploration" setting (identifying best arm/ranking with high probability) rather than "Regret Minimization" (maximizing rewards online), but connects the two by treating sub-optimality gaps as costs
  - Quick check: Can you explain why minimizing cumulative cost of identifying best arm is mathematically distinct from minimizing regret incurred while learning to pull it?

- **Concept: Kullback-Leibler (KL) Divergence**
  - Why needed: Core difficulty metric (information gain) and stopping statistic rely on KL divergence d(μ, λ) between distributions
  - Quick check: If two arms have very similar means (μ_a ≈ μ_b), how does KL divergence affect required sample size compared to arms with distinct means?

- **Concept: Asymptotic Optimality**
  - Why needed: Paper proves CAET is optimal in limit as δ→0, meaning guarantees hold for high-confidence regimes but may not be tight for "moderate" confidence
  - Quick check: Does "asymptotic optimality" guarantee algorithm is best choice for finite budget of 100 pulls? (Answer: No)

## Architecture Onboarding

- **Component map:** Input (δ, c, A) -> Optimizer (solve max-min problem for ω*) -> Sampler (C-tracking following u_α) -> Accumulator (update μ̂, ĉ) -> Stopping Module (compute GLRT statistics)

- **Critical path:** The Optimizer (Step 2) is the computational bottleneck, requiring solving a constrained optimization problem involving cost-weighted KL divergences every round.

- **Design tradeoffs:**
  - Computational vs. Statistical Efficiency: Complex optimization problem solved each round vs. batched version trading reactivity for speed
  - Sensitivity to Zero-Cost: Parameter α handles zero-cost arms; incorrect settings lead to slow convergence (oversampling free arms) or high error (undersampling them)

- **Failure signatures:**
  - Infinite Loop: Optimization solver fails to find feasible ω* due to numerical instability
  - Early Stopping: Threshold β(t, δ) underestimated, violating δ-PAC guarantee
  - Cost Explosion: Forced exploration logic disabled for zero-cost arms, algorithm ignores them and fails to resolve comparisons

- **First 3 experiments:**
  1. Verify Lower Bound: Plot expected cumulative cost E[f(c,μ;τ_δ)] normalized by log(1/δ) as δ→0; curve should flatten to theoretical constant T*(c,μ)
  2. Stress Test Zero-Costs: Create synthetic bandit with 50% zero-cost arms; compare CAET against baseline treating costs as uniform
  3. Regret Minimization Mapping: Run CAET in Explore-then-Commit setup, using sub-optimality gaps as costs; compare regret against standard UCB/Thompson Sampling over horizon T

## Open Questions the Paper Calls Out

1. Can a batched version of the CAET algorithm be developed to reduce the computational complexity of solving the optimization problem at every round? (Section 9 mentions batching as future direction)

2. Can the cost-aware framework be extended to handle general pure exploration tasks that cannot be formulated as pairwise comparisons? (Appendix B.2 notes current work focuses on pairwise tasks)

3. How does the cost-aware objective change the complexity and algorithm design for the fixed-budget pure exploration setting? (Appendix B.2 identifies fixed-budget setting as important future topic)

## Limitations

- Theoretical guarantees assume stationary and known arm costs; performance may degrade with non-stationary costs
- Forced exploration mechanism requires careful tuning of decay rates r and r' with limited guidance provided
- Asymptotic optimality results only hold as δ→0, meaning performance for moderate confidence levels may deviate
- Numerical optimization procedure for computing ω* and exact values of truncation parameters are not fully specified

## Confidence

**High Confidence:** Cost-weighted optimal allocation mechanism and track-and-stop with GLRT for δ-PAC correctness are well-established in bandit literature.

**Medium Confidence:** Forced exploration method for zero-cost arms is novel and theoretically justified but depends on correct hyperparameter tuning; regret minimization extension is conceptually sound but relies on specific assumptions.

**Low Confidence:** Practical implementation details including numerical optimization procedure, exact truncation parameter values, and experimental protocol details are not fully specified.

## Next Checks

1. Sensitivity Analysis: Systematically vary decay rates r and r' for forced exploration parameter α across range (e.g., r ∈ {0.3, 0.4, 0.45}) and evaluate impact on convergence speed and error rates for synthetic bandits with varying proportions of zero-cost arms.

2. Non-Stationary Cost Verification: Implement variant where arm costs change dynamically during experiment (e.g., following slow random walk); measure degradation in CAET's performance compared to stationary case.

3. Optimization Bottleneck Profiling: Benchmark computational time required to solve constrained optimization problem for ω* at each round using different numerical solvers; compare to overall runtime to identify bottleneck and evaluate batching strategies.