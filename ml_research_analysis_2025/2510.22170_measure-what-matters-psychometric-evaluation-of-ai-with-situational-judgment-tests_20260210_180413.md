---
ver: rpa2
title: 'Measure what Matters: Psychometric Evaluation of AI with Situational Judgment
  Tests'
arxiv_id: '2510.22170'
source_url: https://arxiv.org/abs/2510.22170
tags:
- your
- level
- trait
- persona
- sjts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses limitations in current AI psychometric evaluation
  methods that rely on human trait inventories and ad hoc personas, which fail to
  capture behavioral realism and domain relevance. We introduce a framework using
  situational judgment tests (SJTs) with structured generation, industrial-organizational
  and personality psychology integration, and demographically grounded personas for
  law enforcement.
---

# Measure what Matters: Psychometric Evaluation of AI with Situational Judgment Tests

## Quick Facts
- arXiv ID: 2510.22170
- Source URL: https://arxiv.org/abs/2510.22170
- Reference count: 40
- Key outcome: Framework using situational judgment tests with demographically grounded personas achieves R² ≈ 0.8-0.9 between HEXACO trait scores and SJT performance

## Executive Summary
This work addresses critical limitations in current AI psychometric evaluation methods by introducing a framework that uses situational judgment tests (SJTs) with demographically grounded personas for law enforcement. The approach combines base scenarios from domain experts with controlled attribute sampling and LLM-based trait bleed correction to create psychometrically robust SJTs. The resulting dataset includes 8,500 personas, 4,000 SJTs, and 300,000 responses, demonstrating high predictive validity between HEXACO personality scores and SJT performance.

## Method Summary
The framework generates personas using demographic priors, memoir-inspired narratives, and Pydantic schemas across 8 archetypes, then administers HEXACO-100 questionnaires and SJTs via structured generation. SJTs are created by varying 11 attributes from 20 base expert-designed scenarios, with LLM-as-judge refinement reducing trait bleed between response options. The approach uses high-temperature sampling (temp=2.0 for personas, temp=1.5 for SJTs) with vLLM for evaluation, and includes iterative trait-bleed correction until response options achieve strong trait fit scores.

## Key Results
- High predictive power (R² ≈ 0.8-0.9) between HEXACO scores and SJT performance
- 8,500 personas, 4,000 SJTs, and 300,000 responses generated with good diversity (MSTTR-100=0.802)
- Expert case studies demonstrate construct validity through consistent trait-behavior mappings
- Framework is modular and extensible to other domains beyond law enforcement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SJT response patterns predict HEXACO trait scores with high explanatory power
- Mechanism: SJTs present domain-specific scenarios with six response options, each designed to align with one HEXACO trait. When personas respond, their choice distributions serve as behavioral proxies for underlying personality dimensions. A linear regression model using HEXACO scores as predictors and aggregated SJT scores as dependent variables yields high adjusted R² values, indicating that trait configurations explain behavioral outcomes.
- Core assumption: Response options genuinely map to single traits without significant cross-trait contamination
- Evidence anchors:
  - [abstract] "high predictive power (R² ≈ 0.8-0.9) between HEXACO scores and SJT performance"
  - [Section 9, Table 3] Trait-wise regression models show R² values ranging from 0.864 (Emotionality) to 0.993 (Honesty–Humility)
  - [corpus] Related work (Walsh et al., 2025; Mittelstädt et al., 2024) finds LLMs can match or surpass human performance on SJTs, supporting feasibility
- Break condition: If trait bleed between options is substantial, R² values would degrade as responses no longer isolate single traits.

### Mechanism 2
- Claim: Demographically grounded, memoir-inspired personas produce consistent trait expression across questionnaires and behavioral scenarios
- Mechanism: Personas are constructed by sampling demographic attributes from census-derived PGMs, assigning an archetype, generating a memoir-grounded narrative excerpt (180–250 words), and enforcing schema consistency via Pydantic. This multi-source grounding creates internal coherence: demographics lock literal values, memoir provides tonal/behavioral priors, and archetype soft-guides tendencies without rigid constraints.
- Core assumption: Narrative grounding translates into measurable behavioral consistency during evaluation
- Evidence anchors:
  - [Section 6] "The generation proceeds as follows: Demographic Selection... Archetype Assignment... Memoir Grounding... Schema-Constrained Prompting"
  - [Section 8, Tables 1–2] Case studies show HEXACO Z-scores align with SJT response percentages (e.g., Officer Wong: Conscientiousness +5.38 Z-score → 40% SJT selection)
  - [corpus] Weak direct evidence; related work (Park et al., 2023, 2024) shows persona consistency but lacks formal psychometric validation
- Break condition: If memoir/archetype conditioning is too weak, personas would revert to base model priors rather than exhibiting differentiated behaviors.

### Mechanism 3
- Claim: Iterative LLM-as-judge refinement reduces trait bleed in SJT response options
- Mechanism: After initial SJT generation, an LLM evaluator scores each response option's fit to its intended trait on a 1–5 scale. Options scoring below 5 are fed back for rewriting to sharpen trait specificity. This iterative correction creates cleaner behavioral probes that better isolate single HEXACO dimensions.
- Core assumption: LLM-as-judge can reliably detect trait overlap and suggest effective corrections
- Evidence anchors:
  - [Section 7, p.6] "To mitigate trait bleeding, we use an LLM-as-a-judge to evaluate each option's Trait Fit, assigning scores from 1 (Poor) to 5 (Very Strong)"
  - [Appendix G.1] Rubric 1 defines trait alignment scoring; Appendix K shows the evaluation prompt template
  - [corpus] No direct corpus evidence on trait bleed correction; this appears novel to this work
- Break condition: If the judge's trait calibration is noisy or biased toward certain traits, corrections could introduce systematic distortion rather than clarity.

## Foundational Learning

- **HEXACO Personality Model**
  - Why needed here: All SJT response options map to one of six HEXACO traits; understanding the distinction between (e.g.) Honesty–Humility vs. Agreeableness is required to interpret alignment results
  - Quick check question: In HEXACO, which trait captures fairness and ethical conduct vs. which captures cooperative social behavior?

- **Situational Judgment Tests (SJTs)**
  - Why needed here: SJTs are the core evaluation instrument; they differ from trait inventories by probing behavior through realistic scenarios rather than self-report items
  - Quick check question: Why might an SJT reveal behavioral tendencies that a HEXACO questionnaire does not?

- **Structured Generation (Pydantic/JSON schemas)**
  - Why needed here: Both persona and SJT generation enforce output schemas to ensure field consistency and parseability at scale
  - Quick check question: What happens if a generated persona's demographic fields contradict the memoir narrative?

## Architecture Onboarding

- **Component map:**
  1. Persona Generator (Section 6): PGM → demographics → archetype → memoir excerpt → Pydantic schema → full persona
  2. SJT Generator (Section 7): Base scenarios (expert-authored) → attribute seed sampling → controlled variation → LLM generation
  3. Trait Bleed Corrector (Section 7, Appendix K): LLM-as-judge evaluates trait fit → rewrites low-scoring options
  4. Evaluation Framework (Section 5, 9): vLLM structured generation → HEXACO questionnaire + SJT administration → regression/correlation analysis

- **Critical path:**
  1. Define 8 archetypes + 11 SJT attributes (requires domain expert collaboration)
  2. Generate 20 base SJT scenarios with trait-mapped options
  3. Run attribute sampling to produce 4,000 SJT variants
  4. Apply trait bleed correction pipeline
  5. Generate 8,500 personas using demographic PGM + memoir seeds
  6. Administer HEXACO-100 + SJTs to each persona via structured generation
  7. Compute trait-behavior correlations and regression models

- **Design tradeoffs:**
  - High-temperature sampling (temp=1.5–2.0) increases diversity but may reduce consistency
  - Memoir grounding adds narrative richness but introduces potential copyright/licensing concerns
  - LLM-as-judge scales evaluation but may inherit model biases

- **Failure signatures:**
  - Low Cohen's κ between human and LLM judges on SJT quality → rubric or judge model needs calibration
  - Negative correlation between HEXACO scores and SJT selection for a trait (e.g., Honesty–Humility showed –0.122) → response options may be default-attractive regardless of persona
  - MSTTR-100 < 0.70 → insufficient lexical diversity in generated content

- **First 3 experiments:**
  1. **Baseline comparison:** Run HEXACO-100 + SJTs on base model (no persona conditioning) to establish control distributions
  2. **Trait ablation:** Remove memoir grounding and compare persona consistency—does SJT-HEXACO alignment degrade?
  3. **Cross-model validation:** Administer the same persona+SJT set to GPT-4.1, Qwen 2.5-7B, and Llama 3.1-8B to assess framework portability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do demographic biases manifest in LLM persona simulations, and what mechanisms drive the observed differences (e.g., minority groups showing ~52% Conscientiousness preference vs. ~24% population mean)?
- Basis in paper: [explicit] Section 10 states: "LLM simulations evidence markedly different behaviors for minorities, exhibiting a jump in Conscientiousness scores from 0.24 to 0.52. We believe that the data we share is a treasure trove for further such investigations."
- Why unresolved: The authors note the observation but did not conduct systematic analysis of demographic conditioning effects on trait expression.
- What evidence would resolve it: Controlled experiments varying demographic attributes while holding other persona features constant, combined with bias attribution analysis across training data and model architecture.

### Open Question 2
- Question: How does the framework perform when extended to domains beyond law enforcement (e.g., healthcare, education)?
- Basis in paper: [explicit] Abstract states the framework is "modular and extensible to other domains"; Section 10 mentions "customization of personas and SJTs to other verticals."
- Why unresolved: Only law enforcement was tested as a case study; no empirical validation in other professional contexts.
- What evidence would resolve it: Application of the same methodology to generate domain-specific SJTs and personas in healthcare or education, with expert validation of construct validity.

### Open Question 3
- Question: How do single-shot SJT responses compare to dynamic, multi-turn decision-making processes in capturing trait-consistent behavior?
- Basis in paper: [explicit] Section 12 (Limitations): "our current SJT design captures single-shot judgments, which may not reflect the dynamic, multi-turn decision-making processes characteristic of real-world policing."
- Why unresolved: Current framework only evaluates isolated scenario responses; real-world decisions unfold across multiple interactions.
- What evidence would resolve it: Longitudinal evaluation where personas engage in multi-turn scenarios, with behavioral consistency tracked across conversation depth.

### Open Question 4
- Question: Does independent expert validation replicate the trait-behavior alignment observed in author-validated assessments?
- Basis in paper: [explicit] Section 12: "we have not employed independent expert raters to cross-validate score rubrics or persona classifications"; Section 11 calls for "integrating independent expert validation."
- Why unresolved: Current validation was conducted by authors with domain expertise, introducing potential confirmation bias.
- What evidence would resolve it: Blinded inter-rater reliability study with independent psychologists and law enforcement professionals evaluating personas and SJT responses.

## Limitations

- Generalizability to non-law enforcement domains remains untested
- LLM-as-judge reliability lacks external validation across different model architectures
- Copyright implications of using memoir excerpts for persona grounding need formal assessment

## Confidence

- **High Confidence**: Construct validity demonstrated through consistent HEXACO-SJT mappings in case studies; framework architecture is technically sound and reproducible
- **Medium Confidence**: Predictive power claims (R² ≈ 0.8-0.9) are supported by case studies but limited to specific archetypes; trait bleed correction mechanism appears effective but lacks external validation
- **Low Confidence**: Generalizability to non-law enforcement domains; LLM-as-judge reliability across different models; long-term stability of persona-SJT alignment

## Next Checks

1. Cross-domain validation: Apply the framework to healthcare or education scenarios and measure HEXACO-SJT alignment degradation
2. Multi-judge reliability: Compare trait fit scores across different LLM judges (GPT-4.1, Claude, Qwen) to assess consistency
3. Copyright audit: Conduct formal legal review of memoir excerpt usage and develop synthetic narrative alternatives if needed