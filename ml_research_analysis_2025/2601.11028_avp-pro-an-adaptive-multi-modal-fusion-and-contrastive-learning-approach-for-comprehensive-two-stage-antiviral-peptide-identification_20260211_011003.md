---
ver: rpa2
title: 'AVP-Pro: An Adaptive Multi-Modal Fusion and Contrastive Learning Approach
  for Comprehensive Two-Stage Antiviral Peptide Identification'
arxiv_id: '2601.11028'
source_url: https://arxiv.org/abs/2601.11028
tags:
- learning
- feature
- sequence
- samples
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AVP-Pro introduces a two-stage deep learning framework that combines
  adaptive feature fusion with biologically-guided data augmentation to identify antiviral
  peptides (AVPs) and predict their functional subtypes. The method employs ESM-2
  embeddings and ten classical descriptors, fused via a hierarchical attention architecture
  that integrates CNNs and BiLSTMs with an adaptive gating mechanism.
---

# AVP-Pro: An Adaptive Multi-Modal Fusion and Contrastive Learning Approach for Comprehensive Two-Stage Antiviral Peptide Identification

## Quick Facts
- arXiv ID: 2601.11028
- Source URL: https://arxiv.org/abs/2601.11028
- Reference count: 40
- Primary result: 0.9531 accuracy and 0.9064 MCC on general AVP identification

## Executive Summary
AVP-Pro is a two-stage deep learning framework for identifying antiviral peptides (AVPs) and predicting their functional subtypes. It combines adaptive feature fusion with biologically-guided data augmentation to handle both general AVP detection and fine-grained subtype classification under small-sample conditions. The model leverages ESM-2 embeddings and classical descriptors, fused via a hierarchical attention architecture with adaptive gating. An Online Hard Example Mining (OHEM) strategy with BLOSUM62-guided augmentation improves discrimination between similar samples. Transfer learning from a large general dataset enables accurate subtype prediction for six viral families and eight specific viruses.

## Method Summary
AVP-Pro employs a dual-branch architecture combining CNN-extracted local motifs and BiLSTM-extracted global dependencies, fused via an adaptive gating mechanism. The input consists of ESM-2 embeddings and ten classical descriptors. Stage 1 trains on a large balanced dataset (2,662 positives + 2,662 negatives) using a combined loss of focal loss, OHEM-driven contrastive loss, and consistency regularization. Stage 2 applies transfer learning by fine-tuning the pre-trained encoder on small subtype datasets (as few as 87 positive samples). BLOSUM62-guided augmentation and TTA are used to enhance robustness. The model is implemented in PyTorch with specific hyperparameter settings for learning rate, weight decay, and optimizer choice.

## Key Results
- Stage 1 achieves 0.9531 accuracy and 0.9064 MCC on general AVP identification
- Stage 2 successfully predicts functional subtypes across six viral families and eight viruses under small-sample conditions
- Outperforms existing methods on multiple metrics including MacroP, MacroR, MacroF, G-mean, AUROC, and AUPRC

## Why This Works (Mechanism)

### Mechanism 1
Adaptive gating improves classification by dynamically weighting local versus global sequence features based on input context. The gating network learns a scalar weight λ that blends CNN-extracted local motifs and BiLSTM-extracted global dependencies. For AVP samples, λ clusters near zero (favoring global context); for non-AVP samples, λ shifts toward one (favoring local motif detection of "inactive fragments"). Core assumption: Antiviral activity depends more on overall sequence patterns than isolated motifs for positive samples, while non-AVPs can be rejected via local inactive signatures. Evidence anchors: Abstract states the architecture integrates adaptive gating to dynamically modulate weights; Interpretability section shows bimodal λ distribution for AVP vs non-AVP samples. Break condition: If λ distribution becomes unimodal or converges to a constant across all samples, the gating mechanism is not learning discriminative fusion and the model degrades to static concatenation.

### Mechanism 2
OHEM-driven contrastive learning sharpens decision boundaries by actively pushing hard negative samples away from positive prototypes in embedding space. A positive prototype P₊ is computed as the mean of a positive sample queue Q₊. Negative samples in Q₋ are ranked by cosine similarity to P₊; the most similar (hardest) negatives are sampled for contrastive loss. During training, positive pair similarity rises to ~0.9 while hard negative similarity drops from >0.6 to <0.2. Core assumption: Ambiguous samples near the decision boundary are the primary source of false positives and can be identified via proximity to the positive prototype. Evidence anchors: Abstract mentions OHEM-driven contrastive learning targeting blurred decision boundaries; Ablation Study shows hard negative similarity decay from >0.6 to <0.2 during training. Break condition: If hard negative similarity does not decrease during training, or if the positive prototype drifts excessively, the mining strategy may be selecting uninformative negatives.

### Mechanism 3
Transfer learning from a large general AVP dataset enables accurate subtype prediction under small-sample conditions by initializing feature extractors with pre-trained representations. Stage 1 trains on 2,662 positive + 2,662 negative samples. Stage 2 loads the pre-trained encoder and fine-tunes only the classifier head on subtype data (as few as 87 positive samples for HPIV3). Core assumption: General antiviral features learned in Stage 1 transfer to virus-specific recognition tasks. Evidence anchors: Abstract states transfer learning enables accurate subtype prediction under small-sample conditions; Efficacy section shows MCC scores above 0.88 versus suboptimal ~0.58 from scratch training. Break condition: If Stage 2 fine-tuning exhibits catastrophic forgetting or fails to improve over the frozen-encoder baseline, the transfer is not effective.

## Foundational Learning

- **Contrastive Learning with Hard Negative Mining**
  - Why needed here: Standard cross-entropy loss does not explicitly structure the embedding space; confusing samples near class boundaries remain misclassified.
  - Quick check question: Can you explain why sampling the hardest negatives (those most similar to positives) is more effective than random negative sampling for learning discriminative embeddings?

- **Adaptive Gating Mechanisms**
  - Why needed here: Static feature concatenation cannot capture that some sequences require local motif emphasis while others need global context.
  - Quick check question: How does a learned scalar gate λ differ from simply concatenating features and letting the classifier learn weights?

- **Transfer Learning for Few-Shot Classification**
  - Why needed here: Subtype datasets have 87–995 positive samples—insufficient for training deep networks from scratch without severe overfitting.
  - Quick check question: Why is it preferable to freeze the encoder and fine-tune only the classifier versus end-to-end fine-tuning with a small learning rate?

## Architecture Onboarding

- **Component map**: ESM-2 embedding + 10 classical descriptors → Parallel CNN + BiLSTM branches with self-attention → Adaptive gating fusion → MLP classifier → Prediction
- **Critical path**: ESM-2 embedding quality → CNN/BiLSTM feature extraction → gating weight learning → contrastive mining of hard negatives. If any stage fails, downstream performance degrades.
- **Design tradeoffs**: Computing 10 classical descriptors adds preprocessing overhead but captures physicochemical properties ESM-2 may miss. OHEM requires maintaining positive/negative queues (memory cost) and computing prototype similarity each batch. Two-stage training doubles experiment time but enables few-shot subtype prediction.
- **Failure signatures**: λ converging to ~0.5 for all samples → gating not discriminative; check gate network capacity and gradient flow. Hard negative similarity not decreasing → queue may contain only easy negatives; verify queue refresh rate and similarity threshold. Stage 2 accuracy lower than random baseline → negative class dominance; check class weights in focal loss and apply oversampling if needed.
- **First 3 experiments**: 1) Reproduce ablation: Compare Full vs. No_Attention vs. Baseline on Set 1-nonAVP to validate that MCC improves from ~0.858 (Baseline) to ~0.906 (Full). 2) Visualize gating distribution: Plot λ histograms for AVP vs. non-AVP on test set; confirm bimodal separation as in Figure 4. 3) Hard negative analysis: Track positive/hard negative similarity curves during training; verify the crossover pattern shown in Figure 3.

## Open Questions the Paper Calls Out

### Open Question 1
Can incorporating 3D structural information (e.g., AlphaFold-predicted structures) enhance the predictive robustness of AVP-Pro beyond the capabilities of current 1D sequence-based features? Basis in paper: The Conclusion states future work will focus on incorporating three-dimensional structural information to further enhance the model's robustness. Why unresolved: The current framework relies exclusively on 1D sequence features, which may fail to capture spatial folding or binding site geometry critical for antiviral activity. What evidence would resolve it: A comparative study showing improved MCC or Accuracy on the existing benchmark datasets when structural tokens or graph-based representations are added to the multimodal fusion module.

### Open Question 2
Would active learning strategies improve performance on viral subfamilies with extreme data scarcity more effectively than the current transfer learning approach? Basis in paper: The Conclusion notes the model has room for improvement in predicting certain viral subfamilies with sparse data and explicitly proposes exploring active learning strategies as future work. Why unresolved: While transfer learning boosts performance on small datasets, the high specificity but lower sensitivity in some subclasses suggests the model may still struggle to generalize from the limited positive samples available. What evidence would resolve it: Experiments demonstrating that an active learning sampling loop significantly increases sensitivity (SN) for sparse viral families compared to the standard fine-tuning transfer learning method used in Stage 2.

### Open Question 3
To what extent does the BLOSUM62-guided augmentation strategy bias the model against discovering novel antiviral peptides with non-standard evolutionary patterns? Basis in paper: The paper asserts that BLOSUM62 ensures biological plausibility, but if a peptide's antiviral mechanism relies on a novel or rare amino acid substitution not captured well by standard evolutionary matrices, the augmentation might force the representation closer to "standard" proteins, potentially increasing false negatives for novel candidates. Why unresolved: The ablation study compares BLOSUM62 against random mutation, but it does not test against other advanced augmentation matrices or assess the retrieval rate of novel, non-homologous peptides. What evidence would resolve it: A comparative analysis of recall rates for experimentally validated novel AVPs (low homology to training set) when trained with BLOSUM62 augmentation versus a position-specific scoring matrix (PSSM) or ungoverned mutation strategy.

## Limitations
- Architectural hyperparameters (CNN/BiLSTM depth, attention heads) and training settings (batch size, dropout, exact OHEM queue sizes) are not fully specified, creating reproducibility gaps.
- Loss weighting coefficients (λ₁, λ₂) and contrastive learning parameters (temperature τ, hard negative count K) are unspecified, leaving critical design choices ambiguous.
- The study relies on external datasets (AVP-HNCL) without full disclosure of preprocessing steps, potentially limiting independent validation.

## Confidence
- **High confidence** in the two-stage framework concept and transfer learning effectiveness, supported by consistent performance improvements across metrics.
- **Medium confidence** in the adaptive gating mechanism's contribution, as the reported bimodal weight distributions are compelling but lack ablation at the architectural granularity.
- **Medium confidence** in OHEM-driven contrastive learning's impact, given strong similarity curve trends but no comparison to standard contrastive baselines.

## Next Checks
1. Reproduce Stage 1 ablation results (MCC ~0.906 vs ~0.858 for Baseline) on Set 1-nonAVP to verify adaptive gating contribution.
2. Implement and compare against a frozen-encoder Stage 2 baseline to isolate transfer learning benefits on small subtype datasets.
3. Test robustness to BLOSUM62 augmentation by measuring semantic drift via UMAP/t-SNE on augmented vs original samples.