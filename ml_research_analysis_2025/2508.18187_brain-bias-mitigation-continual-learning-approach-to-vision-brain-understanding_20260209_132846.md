---
ver: rpa2
title: 'BRAIN: Bias-Mitigation Continual Learning Approach to Vision-Brain Understanding'
arxiv_id: '2508.18187'
source_url: https://arxiv.org/abs/2508.18187
tags:
- learning
- arxiv
- brain
- continual
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of brain signal bias due to memory
  decay in vision-brain understanding (VBU). As participants repeatedly view visual
  stimuli across sessions, their recognition confidence declines, leading to weaker
  and more inconsistent brain signals in later sessions.
---

# BRAIN: Bias-Mitigation Continual Learning Approach to Vision-Brain Understanding

## Quick Facts
- **arXiv ID:** 2508.18187
- **Source URL:** https://arxiv.org/abs/2508.18187
- **Reference count:** 40
- **Primary result:** 1-4% improvement in 200-way Top-1 retrieval accuracy for brain-to-image and image-to-brain tasks using continual learning approach

## Executive Summary
This paper addresses the problem of brain signal bias in vision-brain understanding (VBU) caused by memory decay across fMRI scanning sessions. As participants repeatedly view visual stimuli, their recognition confidence and corresponding brain signals weaken in later sessions, introducing representation bias that degrades model performance. The authors propose a Bias-Mitigation Continual Learning (BRAIN) approach that incrementally learns brain signals while mitigating this bias through two key mechanisms: De-bias Contrastive Learning (DCL) to address signal quality differences, and Angular-based Forgetting Mitigation (AFM) to prevent catastrophic forgetting.

The method is evaluated on the Natural Scenes Dataset using 200-way Top-1 retrieval accuracy for both brain-to-image and image-to-brain tasks. BRAIN achieves state-of-the-art performance, outperforming previous methods including non-continual learning approaches across various continual learning benchmarks. The approach demonstrates robust performance improvements of 1-4% over baseline methods, effectively addressing the representation bias problem while maintaining knowledge across sessions.

## Method Summary
The BRAIN approach consists of two main components: De-bias Contrastive Learning (DCL) and Angular-based Forgetting Mitigation (AFM). DCL weights the contrastive alignment loss by the inverse of response accuracy using an exponential weighting scheme (w_t = e^(1-r(t))), where r(t) represents the average accuracy for each session. This mechanism counteracts the declining signal quality in later sessions by giving more importance to high-quality brain signals. AFM prevents catastrophic forgetting by preserving knowledge through angular feature direction constraints, using L2 distance on normalized angular features between consecutive steps.

The method is trained incrementally using continual learning protocols with settings like (N_init, N_s) where N_init represents initial sessions and N_s represents session increments. The total loss function combines DCL and AFM losses (L = L_DCL + λ_CL L_AFM with λ_CL=1). The visual encoder uses frozen OpenCLIP ViT-B/16, while the fMRI encoder architecture is based on MindEye but specific details are not provided in the paper. Training uses AdamW optimizer with learning rate 2.5e-4 (cosine decay), batch size 16, and 50 epochs per step.

## Key Results
- Achieves state-of-the-art performance on Natural Scenes Dataset with 1-4% improvement in 200-way Top-1 retrieval accuracy
- Outperforms previous methods including non-continual learning approaches across various continual learning benchmarks
- Demonstrates superior performance in both brain-to-image and image-to-brain retrieval tasks
- Effectively mitigates representation bias while preventing catastrophic forgetting across scanning sessions

## Why This Works (Mechanism)
The BRAIN approach works by addressing two fundamental challenges in vision-brain understanding: representation bias due to memory decay and catastrophic forgetting in continual learning scenarios. The De-bias Contrastive Learning component recognizes that brain signals degrade in quality as participants become less engaged across sessions, and compensates for this by weighting the contrastive loss based on response accuracy. This ensures that high-quality signals from early sessions are preserved while still learning from noisier later sessions.

The Angular-based Forgetting Mitigation component works by constraining the angular directions of feature representations between consecutive training steps. By preserving the angular relationships between features, the model maintains knowledge from previous sessions while adapting to new data. This angular constraint is more effective than traditional L2 regularization because it preserves the discriminative directions in the feature space that are crucial for retrieval tasks, rather than just the magnitude of features.

## Foundational Learning

**Continual Learning** - Why needed: Prevents catastrophic forgetting when learning across multiple fMRI sessions with changing signal quality. Quick check: Model maintains performance on earlier sessions while learning new ones.

**Contrastive Learning** - Why needed: Aligns brain and image representations in a shared embedding space for effective retrieval. Quick check: Brain and corresponding image representations are closer than non-corresponding pairs.

**Angular Feature Preservation** - Why needed: Maintains discriminative directions in feature space across training steps. Quick check: Angular distances between corresponding features remain consistent across sessions.

## Architecture Onboarding

**Component Map:** fMRI Encoder -> DCL Module -> AFM Module -> Retrieval Head

**Critical Path:** The critical path flows from the fMRI encoder through both DCL and AFM modules before reaching the retrieval head. The DCL module processes session-specific accuracy weights, while AFM maintains angular consistency between consecutive steps.

**Design Tradeoffs:** The approach trades computational efficiency for accuracy by using a two-stage loss function and maintaining checkpoints between steps. The frozen visual encoder limits adaptation but ensures stable image representations.

**Failure Signatures:** Performance degradation in later sessions indicates DCL weighting issues; catastrophic forgetting indicates AFM implementation problems; poor alignment suggests issues with the base contrastive learning setup.

**First Experiments:**
1. Validate DCL weighting by training with varying accuracy weights and measuring session-wise performance
2. Test AFM effectiveness by comparing angular feature preservation with and without the constraint
3. Evaluate retrieval performance with different visual encoder backbones to assess impact of frozen vs. trainable visual features

## Open Questions the Paper Calls Out
None specified in the provided text.

## Limitations
- fMRI encoder architecture details are not fully specified, relying on external references
- Response accuracy metadata assumes accurate labels without addressing potential noise
- Computational overhead of maintaining checkpoints and angular constraints between steps
- Limited exploration of alternative bias mitigation strategies beyond exponential weighting

## Confidence

**High confidence:** The general framework of using continual learning to address session-wise bias in VBU, and the specific contributions of DCL and AFM as separate mechanisms.

**Medium confidence:** The experimental results showing 1-4% improvements, as the exact implementation details required for full replication are missing.

**Low confidence:** The precise numerical improvements reported, as they depend heavily on unspecified architectural choices and hyperparameter tuning.

## Next Checks

1. **Architecture Specification Validation:** Contact authors or examine MindEye [23] to determine the exact fMRI encoder architecture, including voxel selection method and layer dimensions used in their experiments.

2. **AFM Feature Layer Validation:** Implement AFM with multiple layer choices (e.g., last hidden layer, attention outputs) and compare performance to determine the most effective configuration.

3. **DCL Weighting Robustness Validation:** Test the DCL weighting scheme with alternative decay functions (e.g., linear vs. exponential) and analyze sensitivity to response accuracy noise to verify the robustness of the bias mitigation approach.