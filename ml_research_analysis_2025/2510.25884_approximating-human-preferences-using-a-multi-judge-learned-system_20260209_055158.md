---
ver: rpa2
title: Approximating Human Preferences Using a Multi-Judge Learned System
arxiv_id: '2510.25884'
source_url: https://arxiv.org/abs/2510.25884
tags:
- human
- should
- data
- answer
- judge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a learned aggregation framework for combining
  multiple LLM-based judges to better approximate human preferences. The method uses
  persona-based synthetic annotations as ground truth and trains either a GAM or MLP
  to map judge scores to preference labels.
---

# Approximating Human Preferences Using a Multi-Judge Learned System

## Quick Facts
- arXiv ID: 2510.25884
- Source URL: https://arxiv.org/abs/2510.25884
- Reference count: 40
- Primary result: Learned aggregation achieves R² = 0.578 vs. 0.498 for naive mean judge baseline

## Executive Summary
This paper introduces a learned aggregation framework that combines multiple LLM-based judges to better approximate human preferences. The method uses persona-based synthetic annotations as ground truth and trains either a GAM or MLP to map judge scores to preference labels. Experiments show that learned aggregation outperforms naive baselines with the GAM providing interpretable feature importance rankings. The work demonstrates that learned aggregation improves alignment between LLM judges and human preferences while highlighting limitations around synthetic ground truth and sensitivity to training data quality.

## Method Summary
The method learns an aggregation function f_θ that maps scores from 10 specialized LLM judges (Truthfulness, Helpfulness, Harmlessness, Honesty, Clarity, Conciseness, Instruction-Following, Logical-Consistency, Explanatory-Depth, Creativity) to a single preference score approximating synthetic human feedback. Using 2,000 samples from UltraFeedback, each judge outputs a [0-4] score, and 14 personas generate ground-truth preference scores [0-10] via LLM evaluation. The system trains either a GAM with 5-10 splines and regularization λ ∈ [0.1, 100] or an MLP with one hidden layer (32-128 units) using MSE loss with Adam optimizer and early stopping. The 80/20 train/test split evaluates performance via R² against held-out ground truth.

## Key Results
- Learned aggregation achieves R² = 0.578 vs. 0.498 for naive mean judge baseline
- GAM provides interpretable feature importance with Truthfulness and Instruction Following consistently ranking as top contributors
- System shows robustness to judge-level perturbations but sensitivity to systematic training data contamination (R² < 0.40 at 50% contamination)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Learned aggregation outperforms heuristic baselines by modeling judge-specific calibration functions.
- **Mechanism:** The MLP/GAM learns implicit or explicit weights for each judge dimension, compensating for heterogeneous scales and systematic biases that naive averaging assumes away.
- **Core assumption:** Judge scores carry predictive signal but are miscalibrated relative to ground-truth preferences.
- **Evidence anchors:**
  - [abstract]: "Experiments show that learned aggregation outperforms naive baselines (R² = 0.578 vs. 0.498 for mean judge)"
  - [Section 4.3.2]: "Learned models estimate judge-specific calibration functions and importance weights during training, enabling them to compensate for monotonic distortions and heterogeneous biases."
  - [corpus]: Corpus neighbor "On Evaluating LLM Alignment by Evaluating LLMs as Judges" notes LLM judges suffer from various biases requiring calibration methods.
- **Break condition:** If judge outputs become perfectly calibrated or if ground-truth preferences are inconsistent (high inter-annotator disagreement), learned aggregation gains diminish.

### Mechanism 2
- **Claim:** Persona-based synthetic ground truth enables scalable training, but preference diversity constrains achievable performance.
- **Mechanism:** LLM personas simulate diverse human evaluators (14 personas from Child to CEO), providing preference labels without costly human annotation. However, heterogeneous persona preferences create variance in targets.
- **Core assumption:** Persona-generated preferences approximate human preference distributions well enough for training signal.
- **Evidence anchors:**
  - [abstract]: "The method uses persona-based synthetic annotations as ground truth"
  - [Section 4.2]: "When trained on the persona mean rather than sampled individuals, the aggregator achieves notably higher performance (GAM R² = 0.695... approximately 20% better than our baseline approach."
  - [Section 5]: "We rely on synthetic persona labels rather than genuine human annotations, potentially missing authentic preference complexity."
  - [corpus]: Weak corpus signal on persona-based preference synthesis specifically; related work focuses on RLAIF/Constitutional AI as alternatives to human labels.
- **Break condition:** If personas fail to capture real user diversity, or if circularity in LLM-as-judge degrades signal quality (preference leakage).

### Mechanism 3
- **Claim:** GAM architecture provides interpretable feature importance while maintaining competitive performance.
- **Mechanism:** Generalized Additive Model decomposes f(x) = Σsj(xj) where each sj is a spline function for one judge dimension, allowing direct inspection of each judge's contribution.
- **Core assumption:** Judge contributions are approximately additive (no strong interaction effects between judges).
- **Evidence anchors:**
  - [abstract]: "GAM providing interpretable feature importance rankings"
  - [Section 4.1.1]: "Truthfulness, Instruction Following, Clarity, Conciseness and Logical Consistency consistently rank as the most important judges... with low variance in importance scores indicating reliable interpretability."
  - [corpus]: No direct corpus evidence on GAM interpretability for judge aggregation; this is a methodological contribution.
- **Break condition:** If judge interactions are strong (e.g., Truthfulness only matters when Helpfulness is high), GAM will miss these effects.

## Foundational Learning

- **Concept: Ensemble diversity and error correlation**
  - **Why needed here:** The paper aggregates 10 specialized judges, relying on uncorrelated errors for statistical benefits. Understanding Q-statistic and double-fault measures helps diagnose when ensemble benefits break down.
  - **Quick check question:** If two judges (e.g., Truthfulness and Honesty) always agree, does adding both improve the ensemble?

- **Concept: Calibration in LLM-as-judge systems**
  - **Why needed here:** Judges suffer from "rubric sensitivity, bias, and instability" (abstract). Learned aggregation partially addresses calibration but assumes systematic (not random) miscalibration.
  - **Quick check question:** If a judge systematically scores 1 point higher than ground truth, can learned aggregation compensate? What if the bias varies by domain?

- **Concept: Synthetic preference data quality**
  - **Why needed here:** The method trades human labels for persona-generated labels. Understanding RLAIF/Constitutional AI helps assess when this substitution is valid vs. when it introduces circularity.
  - **Quick check question:** If your persona generator and your judges share the same base model, what failure mode should you monitor for?

## Architecture Onboarding

- **Component map:**
  Input: (prompt, answer) pairs from UltraFeedback dataset -> 10 LLM judges -> 10-dimensional score vector -> Persona evaluator (14 personas) -> Ground truth preference label -> GAM or MLP aggregator -> Aggregated preference score

- **Critical path:**
  1. Generate prompt-answer pairs (2,000 samples from UltraFeedback)
  2. Each judge scores each pair → 10-dimensional score vector
  3. Persona evaluator generates ground-truth preference label (sampled or averaged across personas)
  4. Train aggregator (MSE loss, Adam optimizer, early stopping patience=15) on 80/20 split
  5. Evaluate via R² against held-out ground truth

- **Design tradeoffs:**
  - GAM vs. MLP: GAM offers interpretability (feature importance via spline p-values) at slight performance cost (R² 0.575 vs 0.578)
  - Persona sampling vs. persona mean: Sampling diversity increases training variance but may better reflect real user heterogeneity; mean yields higher R² (0.695) but smoother targets
  - Judge count: 10 judges used; ablation not reported but Section 4.1.1 shows some judges contribute minimally (Harmlessness, Explanatory-Depth)

- **Failure signatures:**
  - Scale compression contamination: R² drops below 0.40 at 50% contamination (Figure 5)
  - Systematic bias contamination: Gradual degradation, more severe than random noise
  - Judge perturbation (rubric drift): Naive mean degrades up to 40%; learned aggregators remain stable
  - Persona-ground-truth mismatch: Child persona shows R²=0.442 vs. Student at 0.693 (Figure 4), suggesting some preference profiles are inherently harder to model

- **First 3 experiments:**
  1. **Baseline replication:** Run the 10-judge setup on UltraFeedback with mixed-persona ground truth; verify MLP R²≈0.578 and GAM R²≈0.575 vs. mean judge R²≈0.498
  2. **Robustness probe:** Inject systematic bias (±2 points) into 30% of training personas; confirm R² remains above 0.50 for random noise but drops below 0.45 for scale compression
  3. **Judge importance audit:** Train GAM 20 times with hyperparameter variation (±20% regularization, ±2 splines); verify Truthfulness and Instruction-Following consistently rank as top contributors with low coefficient of variation

## Open Questions the Paper Calls Out

- **Question:** Can aggregator models trained on synthetic persona-based labels successfully generalize to predict the preferences of actual human users?
- **Basis in paper:** [explicit] The authors state in Section 5 that they "do not yet calibrate to a held-out human-labeled set" and identify "small, carefully sampled human evals" to verify rank agreement as necessary future work.
- **Why unresolved:** The entire experimental framework relies on a synthetic ground truth (LLM-based personas), creating a potential "proxy mismatch" where the model learns to predict AI-generated scores rather than genuine human utility.
- **What evidence would resolve it:** A comparative evaluation where the aggregator, trained on synthetic labels, is tested against a dataset containing both LLM-judge scores and gold-standard human preference annotations.

- **Question:** Does incorporating uncertainty estimates or filtering low-confidence persona responses improve the robustness of the aggregator against noise?
- **Basis in paper:** [explicit] Section 4.2 notes that the methodology "do[es] not filter persona responses by confidence scores," and Section 5 lists "uncertainty-aware training" as a specific avenue for future work.
- **Why unresolved:** The current model treats all synthetic annotations equally, potentially introducing noise from "uncertain or arbitrary ratings" that could degrade the learning signal.
- **What evidence would resolve it:** Ablation experiments that filter out synthetic labels below a certain confidence threshold or use confidence-weighted loss functions, comparing the resulting R² and robustness against the baseline.

- **Question:** How does the aggregator's performance and judge weighting change when the persona set is dynamically weighted or learned from data rather than uniformly sampled?
- **Basis in paper:** [explicit] Section 5 identifies the "uniform sampling across personas" as a "strong assumption" and suggests the need to "learn a persona prior from data" and expand demographics.
- **Why unresolved:** The current approach assumes a heterogeneous mix of 14 personas represents the user base, but real-world applications may have skewed user distributions that the aggregator fails to model.
- **What evidence would resolve it:** Experiments utilizing a validation set of real user preferences to learn optimal persona weights, subsequently analyzing if the feature importance of judges shifts compared to the uniform sampling baseline.

## Limitations
- Reliance on synthetic persona-based ground truth introduces potential circularity and uncertainty about real human preference alignment
- System shows sensitivity to systematic training data contamination with R² dropping below 0.40 at 50% contamination
- Key implementation details like LLM model versions and inference parameters are unspecified, creating reproducibility challenges

## Confidence
- **High Confidence:** GAM vs. MLP performance comparison (R² = 0.575 vs. 0.578), judge importance rankings (Truthfulness, Instruction Following as top contributors), baseline aggregation comparison (R² = 0.578 vs. 0.498)
- **Medium Confidence:** Robustness to judge-level perturbations, persona-ground-truth alignment variance, systematic bias sensitivity
- **Low Confidence:** Real-world human preference alignment, synthetic ground truth quality, complete reproducibility without model specifications

## Next Checks
1. **Human Validation Study:** Run the learned aggregator on 200 held-out samples with human preference annotations (not personas) to measure actual alignment gap versus synthetic ground truth performance
2. **Judge Ablation Analysis:** Systematically remove judges (starting with lowest importance) to determine the minimum viable judge set and interaction effects missed by GAM's additive assumption
3. **Contamination Threshold Testing:** Vary contamination types (random noise vs. systematic bias) and levels (10%-50%) to map performance degradation curves and identify early warning signals