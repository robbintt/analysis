---
ver: rpa2
title: Towards Effective Open-set Graph Class-incremental Learning
arxiv_id: '2507.17687'
source_url: https://arxiv.org/abs/2507.17687
tags:
- classes
- graph
- samples
- unknown
- u1d461
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces OGCIL, a novel framework for open-set graph
  class-incremental learning (GCIL), addressing the dual challenges of catastrophic
  forgetting and unknown class detection. OGCIL generates pseudo samples directly
  in the embedding space using a prototypical conditional variational autoencoder
  (CVAE) to replay old-class knowledge and synthesize out-of-distribution (OOD) samples.
---

# Towards Effective Open-set Graph Class-incremental Learning

## Quick Facts
- arXiv ID: 2507.17687
- Source URL: https://arxiv.org/abs/2507.17687
- Reference count: 40
- Primary result: Achieves up to 17.6% higher OSCR than best baseline on five real-world datasets

## Executive Summary
The paper introduces OGCIL, a novel framework for open-set graph class-incremental learning (GCIL) that addresses catastrophic forgetting and unknown class detection. OGCIL generates pseudo samples directly in the embedding space using a prototypical conditional variational autoencoder (CVAE) to replay old-class knowledge and synthesize out-of-distribution (OOD) samples. The framework introduces a prototypical hypersphere classification loss that encloses in-distribution samples within class-specific hyperspheres while rejecting OOD and out-of-class samples as outliers. Extensive experiments on five real-world datasets show that OGCIL significantly outperforms existing GCIL and open-set GNN methods in open-set classification rate (OSCR), closed-set accuracy, and open-set AUC-ROC.

## Method Summary
OGCIL operates by decoupling structure and features in graph data, then reconstructing node embeddings to avoid catastrophic forgetting. The framework uses a Prototypical CVAE to synthesize pseudo-ID embeddings for old classes and generates pseudo-OOD samples via mixing embeddings from different classes. During training, it combines three losses: Prototypical Hypersphere Classification loss for open-set discrimination, CVAE reconstruction loss for knowledge replay, and knowledge distillation loss for stability. The method processes data sequentially across tasks with disjoint classes, treating unseen classes as "unknown" during training but evaluating their detection capability.

## Key Results
- Achieves up to 17.6% higher OSCR compared to the best baseline across five datasets
- Outperforms existing GCIL and open-set GNN methods in closed-set accuracy and open-set AUC-ROC
- Ablation study confirms effectiveness of each component (CVAE, HSC loss, mixing strategy)
- Visualizations demonstrate superior separation between known and unknown classes

## Why This Works (Mechanism)

### Mechanism 1
Generating pseudo-samples in the embedding space, rather than the raw graph space, may enable efficient knowledge replay while avoiding the complexity of reconstructing graph topology. A Prototypical Conditional Variational Autoencoder (CVAE) learns to reconstruct node embeddings by regularizing the latent space to align with class-specific prototypes. During incremental tasks, these prototypes serve as priors to sample "pseudo-ID" embeddings, approximating the distribution of historical data without storing raw graphs. The core assumption is that node embeddings capture sufficient semantic and structural information such that the decision boundary learned from reconstructed embeddings generalizes to real graph data. Evidence shows "synthesis node embeddings for old classes, enabling knowledge replay without storing raw graph data" and "prototypical CVAE is proposed to synthesize node embeddings." Break condition: If embeddings are too low-dimensional to preserve topological nuances, or if CVAE suffers from posterior collapse, generated boundaries will drift, failing to prevent catastrophic forgetting.

### Mechanism 2
Treating unknown classes as "outliers" distributed outside known class hyperspheres, rather than mapping them to a single "unknown" cluster, appears to improve open-set detection in dynamic environments. The Prototypical Hypersphere Classification (HSC) loss anchors known samples close to their respective prototypes (minimizing distance) while explicitly repelling out-of-distribution (OOD) and off-class samples. This defines a rejection region rather than a specific class label for unknowns. The core assumption is that samples from unknown classes form distributions that do not perfectly overlap with the dense regions (hyperspheres) of known class prototypes. Evidence shows "explicitly models them as outliers through prototype-aware rejection regions" and "avoids the restrictive assumption of grouping all unknowns into a single prototype... repelling OOD embeddings away." Break condition: If an unknown class is semantically indistinguishable from a known class (e.g., a sub-class), it will fall inside the hypersphere and be misclassified.

### Mechanism 3
Linear interpolation between embeddings of different classes may synthesize effective pseudo-OOD samples to train the rejection boundary. A mixing strategy creates $z_{mix}$ by combining embeddings from two distinct classes. Because these mixed samples lack a strong affinity to any single class prototype, they serve as negative training examples for the HSC loss. The core assumption is that the manifold space between known classes represents a valid approximation of "potential unknown" territories that the model should reject. Evidence shows "employ a mixing-based strategy to generate out-of-distribution (OOD) samples" and "mixed embeddings... unlikely to align with any single class prototype." Break condition: If interpolation creates unrealistic samples that lie on the manifold of a valid known class, it may cause the model to incorrectly reject actual known data.

## Foundational Learning

- **Concept: Catastrophic Forgetting in GNNs**
  - Why needed here: The paper addresses the tendency of GNNs to overwrite weights for old classes when learning new ones.
  - Quick check question: Can you explain why updating weights on Task B performance degrades performance on Task A?

- **Concept: Variational Autoencoders (VAEs)**
  - Why needed here: The core replay mechanism relies on a CVAE to model the distribution of embeddings.
  - Quick check question: How does the KL-divergence term in a VAE loss function regularize the latent space?

- **Concept: Open-Set Recognition (OSR)**
  - Why needed here: The model must distinguish "known" from "unknown," rather than just classifying among fixed labels.
  - Quick check question: In standard classification, how does the Softmax layer fail to handle inputs from unseen classes?

## Architecture Onboarding

- **Component map:**
  - Encoder (GCN) -> Prototypical CVAE (Encoder/Decoder) -> OOD Generator -> Loss Aggregator

- **Critical path:**
  1. Forward Pass: GNN generates embeddings $z$
  2. Replay: For old classes, sample $h \sim N(p_c, I)$ and decode to pseudo-embeddings $\hat{z}$
  3. Mixing: Generate $z_{mix}$ from current and pseudo-embeddings
  4. Optimization: Compute HSC loss (pulling known $z$ to $p_c$, pushing $z_{mix}$ away) and update all components

- **Design tradeoffs:**
  - Embedding-level generation: Avoids the computational cost of generating raw graphs but may lose fine-grained topological details
  - Hypersphere vs. Softmax: Rejects unknowns better than Softmax but requires tuning the radius/distance threshold for the hypersphere

- **Failure signatures:**
  - High Reconstruction Error: If CVAE loss diverges, pseudo-samples are noise, leading to immediate forgetting
  - All-Class Rejection: If HSC loss is too aggressive, the hyperspheres shrink, and the model classifies everything as "unknown"

- **First 3 experiments:**
  1. Sanity Check (CVAE): Train on Task 1, then generate pseudo-samples. Visualize (t-SNE) if they overlap with real Task 1 embeddings
  2. Ablation (HSC Loss): Replace Prototypical HSC with standard Cross-Entropy. Measure the drop in Open-Set Classification Rate (OSCR)
  3. Scalability: Run the full OGCIL pipeline on the full sequence of tasks (CoraFull/Arxiv) to verify that memory overhead remains constant while accuracy is maintained

## Open Questions the Paper Calls Out

### Open Question 1
Can the generation mechanism be extended to synthesize raw graph topologies instead of just node embeddings? Basis in paper: [explicit] The authors state that generating pseudo-samples directly in the embedding space is a "compromise" made because generating raw graph data is "inherently challenging." Why unresolved: The current method relies on decoupling structure and features, but this may lose critical topological information present in the original graph. What evidence would resolve it: A variant of the framework that generates adjacency matrices or edge lists alongside node features while maintaining OSCR performance.

### Open Question 2
How does OGCIL perform if unknown classes persist across multiple tasks without becoming labeled? Basis in paper: [explicit] The paper assumes "any class designated as unknown in task $T_{t-1}$ becomes known in the subsequent task $T_t$" to maximize data efficiency. Why unresolved: The current evaluation loop resets the "unknown" status, failing to test the model's stability against accumulating, persistent unknown classes over long time horizons. What evidence would resolve it: Experiments on task sequences where unknown classes remain unlabeled for >1 task, measuring stability in the open-set score ($S_{open}$).

### Open Question 3
Is the linear mixing strategy sufficient to model complex, non-linear out-of-distribution (OOD) manifolds? Basis in paper: [inferred] The method synthesizes OOD samples via linear interpolation ($z_{mix} = \alpha * z_1 + (1-\alpha) * z_2$), assuming unknown regions lie convexly between known class embeddings. Why unresolved: Linear interpolation may fail to generate diverse OOD samples that lie on distinct, non-linear manifolds, potentially leaving "holes" in the rejection regions. What evidence would resolve it: A comparison using non-linear generative models (e.g., GANs) for OOD synthesis showing improved AUC-ROC over the mixing strategy.

## Limitations
- The paper does not specify CVAE architecture details (encoder/decoder depth, activation functions), optimizer settings, or prototype initialization strategy
- The hypersphere rejection threshold is not explicitly defined in the main text
- While ablation studies validate component contributions, the OOD mixing strategy lacks direct corpus validation for graph data

## Confidence
- High confidence in the CVAE-based replay mechanism's effectiveness for mitigating catastrophic forgetting, supported by strong empirical results across five datasets
- Medium confidence in the hypersphere classification approach for open-set detection, though the corpus evidence is limited to general OSR literature
- Medium confidence in the mixing strategy for generating pseudo-OOD samples, as this specific approach appears novel without direct corpus precedent

## Next Checks
1. **CVAE Stability Test**: Visualize pseudo-embeddings (t-SNE) after each task to verify they maintain class separation and do not collapse to single points
2. **Rejection Threshold Sensitivity**: Systematically vary the hypersphere distance threshold to identify optimal rejection performance across datasets
3. **Topology Preservation Evaluation**: Compare node classification accuracy on original vs. reconstructed graphs to quantify information loss from embedding-level replay