---
ver: rpa2
title: QuarkMed Medical Foundation Model Technical Report
arxiv_id: '2508.11894'
source_url: https://arxiv.org/abs/2508.11894
tags:
- medical
- data
- knowledge
- arxiv
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QuarkMed is a medical foundation model developed to address the
  limitations of general-purpose language models in healthcare applications. It leverages
  curated medical data, Retrieval-Augmented Generation (RAG), and a large-scale, verifiable
  reinforcement learning pipeline.
---

# QuarkMed Medical Foundation Model Technical Report

## Quick Facts
- arXiv ID: 2508.11894
- Source URL: https://arxiv.org/abs/2508.11894
- Reference count: 40
- Primary result: Achieved 70% accuracy on the Chinese Medical Licensing Examination

## Executive Summary
QuarkMed is a medical foundation model designed to address the limitations of general-purpose language models in healthcare applications. It leverages curated medical data, Retrieval-Augmented Generation (RAG), and a large-scale, verifiable reinforcement learning pipeline. The model achieved 70% accuracy on the Chinese Medical Licensing Examination, demonstrating strong generalization across diverse medical benchmarks. QuarkMed offers a robust and versatile AI solution, serving millions of users at ai.quark.cn.

## Method Summary
QuarkMed employs a five-stage training pipeline: medical continued pre-training on a curated corpus, instruction fine-tuning (IFT) across 112 tasks, supervised fine-tuning (SFT), Stage 1 reinforcement learning (RL) for reasoning-focused verifiable rewards, and Stage 2 RL for general alignment. The model uses a hybrid "Rule + Model" reward verifier and Group Relative Policy Optimization (GRPO) to balance accuracy and safety. RAG is integrated for real-time knowledge retrieval, reducing hallucination rates in clinical applications.

## Key Results
- Achieved 70% accuracy on the Chinese Medical Licensing Examination
- Demonstrated strong performance on MedQA and MedMCQA benchmarks
- Reduced hallucination rates through RAG integration and verifiable reinforcement learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A hybrid "Rule + Model" reward verifier stabilizes reinforcement learning in medical domains where pure rules are brittle and pure models are prone to hallucination.
- Mechanism: The system uses a "Verifier" that prioritizes objective medical rules (e.g., ICD code matching) but augments them with a model-based component to handle synonyms and incomplete labels. This hybrid signal guides the policy (GRPO) more accurately than a scalar reward model alone.
- Core assumption: Medical truth is often discrete (verifiable) but linguistic expression is variable; assuming the model-based component can generalize semantic equivalence without significant bias.
- Evidence anchors:
  - [Section 3.3] "We designed a hybrid reward model... that prioritizes rules but is augmented by a model-based component... improved disease diagnosis performance by 3 percentage points."
  - [Abstract] Mentions a "large-scale, verifiable reinforcement learning pipeline."
  - [Corpus] Evidence is weak for this specific hybrid implementation; however, [arXiv:2508.11894v1] generally aligns with RLVR trends in reasoning models.
- Break condition: The model-based reward component begins to "reward hack" by rewarding semantically plausible but clinically incorrect outputs, or the rule-base fails to cover edge cases, causing a conflict in gradient signals.

### Mechanism 2
- Claim: Translating structured knowledge (SPO triples) into synthetic natural language creates "grounding tokens" that reduce hallucination rates in parametric memory.
- Mechanism: The architecture transforms structured Subject-Predicate-Object triples into fluent text via a translation model. This synthetic data is used during continued pre-training, effectively mapping dense knowledge graph relationships into the model's latent space.
- Core assumption: The translation model preserves semantic fidelity without introducing noise, and the base model treats this synthetic text with the same weight as authoritative literature.
- Evidence anchors:
  - [Section 2.2] "Knowledge translation techniques are employed... convert structured SPO data into unstructured natural language sentences."
  - [Section 2.2] "Accuracy increases from 39% to 60.57%... particularly notable in concept-based tasks."
  - [Corpus] [arXiv:2510.16973] confirms Foundation Models leverage massive datasets for improved performance, supporting the utility of data synthesis.
- Break condition: The generated natural language lacks the nuance of the original structured data (information loss), or the volume of synthetic data overwhelms the model's ability to distinguish between high-authority and low-authority sources.

### Mechanism 3
- Claim: Decoupling "Reasoning" (Stage 1 RL) from "General Alignment" (Stage 2 RL) prevents capability forgetting while improving safety and style.
- Mechanism: The model first optimizes purely for reasoning accuracy using verifiable rewards (Stage 1). It then undergoes a second RL phase (GRPO) using a multi-dimensional Reward Model (Honesty, Helpfulness, etc.) to align with human preferences, maintaining a 3:1 ratio of reasoning-to-general data to preserve the initial capability gains.
- Core assumption: The "Reasoning" capability acquired in Stage 1 is robust enough to survive the distribution shift introduced by the preference optimization in Stage 2.
- Evidence anchors:
  - [Section 3.4] "The training data... divided into reasoning-intensive and general-purpose categories... ratio of approximately 3:1."
  - [Section 1] "Dual-Stage Reinforcement Learning... reasoning-focused verifiable reward phase followed by general alignment."
  - [Corpus] [arXiv:2508.09148] (Motif) suggests similar staged training is a common, effective pattern in foundation model development.
- Break condition: The Stage 2 alignment process causes "alignment tax," where the model becomes safer but significantly less capable of complex diagnosis or logical inference (catastrophic forgetting).

## Foundational Learning

- Concept: **Reinforcement Learning from Verifiable Rewards (RLVR)**
  - Why needed here: Essential for understanding QuarkMed's Stage 1 training, where rewards are derived from objective ground truth (e.g., correct diagnosis) rather than subjective human preference.
  - Quick check question: Can you distinguish between a reward signal derived from a database match (Verifiable) vs. a reward signal derived from a human ranking (Preference)?

- Concept: **Knowledge Graph Triple Translation (SPO → Text)**
  - Why needed here: QuarkMed does not just "read" text; it synthesizes text from structured databases to fill knowledge gaps. Understanding this translation is key to debugging data quality.
  - Quick check question: How would you represent the medical fact "Aspirin treats Headache" (Subject-Predicate-Object) as a natural language training sample?

- Concept: **GRPO (Group Relative Policy Optimization)**
  - Why needed here: This is the specific RL algorithm selected over DPO for the alignment phase. It normalizes rewards within groups of samples to stabilize training.
  - Quick check question: Why might normalizing rewards across a group of 8 candidate responses be more stable than comparing a single "chosen" vs. "rejected" pair (DPO)?

## Architecture Onboarding

- Component map:
  - Data Layer: Medical Materials + Knowledge Graph (SPO) + EHRs -> [Translation/Synthesis] -> Training Corpus
  - Training Sequence: Base LLM -> [IFT] -> [SFT] -> [Stage 1 RL (Verifier)] -> [Stage 2 RL (Reward Model)]
  - Inference: User Query -> [RAG Retriever] -> [QuarkMed Model] -> Response

- Critical path: The **Stage 1 RL "Cold Start"**. The paper emphasizes using high-quality SFT data (700 examples) to initialize the RL policy. Without this, the RL explorer policy is too random to effectively utilize the Verifier.

- Design tradeoffs:
  - **Rule vs. Model Verifier:** Rules are exact but brittle; Models are flexible but hackable. QuarkMed chooses a hybrid approach.
  - **GRPO vs. DPO:** DPO is simpler but performed worse on "Honesty" and "Harmlessness" metrics. GRPO requires more computation (group sampling) but yielded better alignment.

- Failure signatures:
  - **Low Entropy in RL:** If the cold-start SFT is too long, the model memorizes answers and stops exploring, failing to learn reasoning.
  - **Reward Hacking:** In Stage 2, if the Reward Model is not updated via active learning, the policy may generate verbose, style-heavy but factually empty responses to maximize "Helpfulness" scores.
  - **Verifiability Bias:** The model may excel at multiple-choice exams (verifiable) but fail at open-ended patient counseling (hard to verify).

- First 3 experiments:
  1. **Verifier Ablation:** Train three Stage 1 models: (A) Rule-only reward, (B) Model-only reward, (C) Hybrid. Compare diagnosis accuracy on the held-out test set.
  2. **RAG vs. Parametric Recall:** Evaluate the model on questions regarding recently updated guidelines (post-training cutoff) with RAG enabled vs. disabled to measure hallucination reduction.
  3. **Algorithm Comparison (DPO vs. GRPO):** Reproduce the Table 5 results on a smaller scale to validate that GRPO provides superior "Honesty" scores before committing to a full training run.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multi-modal capabilities be integrated into QuarkMed to enable the interpretation of medical images such as X-rays and pathology slides?
- Basis in paper: [explicit] The authors state in the Future Directions (Section 5.3) that future work will concentrate on developing multi-modal capabilities, as the current model focuses only on text-based data.
- Why unresolved: The current architecture and training pipeline are restricted to text, lacking the necessary encoders or alignment objectives for visual data.
- What evidence would resolve it: A new model iteration capable of ingesting image inputs and demonstrating high accuracy on radiology or pathology interpretation benchmarks.

### Open Question 2
- Question: Can "semi-verifiable composite rewards" effectively optimize LLMs for nuanced clinical tasks where correctness is gradient, such as lifestyle tailoring and patient education?
- Basis in paper: [explicit] Section 5.2 notes that current verifiers under-optimize "nuanced counseling" and suggests fusing probabilistic factuality estimators with discourse/coherence scoring.
- Why unresolved: Current RL training relies on discrete, automatable verifiers (e.g., ICD matching) which do not capture subjective or contextual quality.
- What evidence would resolve it: Improved performance on benchmarks specifically measuring counseling quality or patient education effectiveness compared to models trained with purely discrete rewards.

### Open Question 3
- Question: How can reinforcement learning be adapted to handle temporal reasoning and longitudinal management planning where feedback is sparse or delayed?
- Basis in paper: [explicit] Section 5.2 identifies "Reward Coverage Gaps" regarding temporal reasoning and suggests integrating simulation or synthetic patient state transitions for temporal credit assignment.
- Why unresolved: Current feedback mechanisms are dense and immediate, failing to model long-term treatment trajectories or de-escalation strategies effectively.
- What evidence would resolve it: Successful application of simulation-based RL pipelines that demonstrate improved accuracy in longitudinal case management scenarios.

## Limitations
- Data source specificity: The proprietary 1T token corpus and fine-grained content structuring model are not disclosed, limiting reproducibility and external validation of the knowledge grounding claims.
- Verifiability dependency: The RLVR pipeline's performance is tightly coupled to the quality of verifiable ground truth; in ambiguous or novel medical scenarios, the system may degrade.
- Generalization beyond exams: While the model achieves high scores on structured medical exams, real-world clinical reasoning often involves unstructured, multimodal, and incomplete data that may not be fully represented in the training corpus.

## Confidence
- **High confidence**: The staged RL training pipeline (RLVR → GRPO) is technically sound and aligns with established foundation model practices; the reported exam performance is verifiable and comparable to peer models.
- **Medium confidence**: The hybrid "Rule + Model" verifier improves accuracy, but the relative contribution of each component and its robustness to edge cases is not fully quantified.
- **Low confidence**: Claims about real-time hallucination reduction via RAG are based on internal metrics; independent studies are needed to validate long-term stability and clinical safety.

## Next Checks
1. **Verifier Ablation Study**: Train and compare models using only rule-based, only model-based, and hybrid reward signals to isolate the contribution of each component to diagnostic accuracy.
2. **Clinical Scenario Generalization**: Test the model on open-ended, multimodal clinical vignettes (not multiple-choice) to assess performance beyond structured exam formats.
3. **RAG Hallucination Audit**: Conduct a longitudinal study measuring hallucination rates on time-sensitive or guideline-updated topics, comparing RAG-enabled vs. disabled inference in real deployment conditions.