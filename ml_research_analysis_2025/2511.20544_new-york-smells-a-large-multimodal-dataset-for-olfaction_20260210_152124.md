---
ver: rpa2
title: 'New York Smells: A Large Multimodal Dataset for Olfaction'
arxiv_id: '2511.20544'
source_url: https://arxiv.org/abs/2511.20544
tags:
- smell
- olfactory
- dataset
- image
- olfaction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the New York Smells dataset, a large-scale
  multimodal dataset pairing visual and olfactory signals captured in natural environments
  across New York City. The dataset contains 7,000 smell-image pairs from 3,500 distinct
  objects across indoor and outdoor settings, collected using an electronic nose (Cyranose
  320) and synchronized cameras.
---

# New York Smells: A Large Multimodal Dataset for Olfaction

## Quick Facts
- arXiv ID: 2511.20544
- Source URL: https://arxiv.org/abs/2511.20544
- Reference count: 40
- Key outcome: Large-scale multimodal dataset pairs visual and olfactory signals, demonstrating visual supervision enables effective contrastive learning of olfactory representations that outperform hand-crafted features.

## Executive Summary
This paper introduces the New York Smells dataset, a large-scale multimodal dataset pairing visual and olfactory signals captured in natural environments across New York City. The dataset contains 7,000 smell-image pairs from 3,500 distinct objects across indoor and outdoor settings, collected using an electronic nose (Cyranose 320) and synchronized cameras. To demonstrate its utility, the authors establish three benchmark tasks: cross-modal smell-to-image retrieval, recognition of scenes, objects, and materials from smell alone, and fine-grained discrimination between grass species. They show that visual supervision enables effective contrastive learning of olfactory representations, significantly outperforming widely-used hand-crafted features like smellprints on all tasks. These results suggest that visual data can provide strong supervisory signals for learning generalizable, semantically rich olfactory representations from raw sensor data.

## Method Summary
The paper proposes Contrastive Olfaction-Image Pretraining (COIP), a method that learns joint embeddings between olfactory and visual modalities using a symmetric contrastive loss (SymInfoNCE). Raw e-nose sensor data (T×32 matrix from 32 Cyranose 320 sensors) is processed by either a CNN, MLP, or Transformer encoder, paired with a standard vision encoder for images. The model is trained to maximize similarity between embeddings of paired smell-image samples while minimizing similarity with non-matching samples. Three downstream tasks evaluate the learned representations: cross-modal retrieval, classification of scenes/objects/materials from smell alone, and fine-grained grass species discrimination.

## Key Results
- COIP with visual supervision significantly outperforms models using hand-crafted smellprint features on all benchmark tasks
- Raw time-series olfactory data contains more discriminative information than smellprints, which discard 2nd-order statistics and temporal dynamics
- Models trained on the in-the-wild New York Smells dataset show strong performance across diverse scene, object, and material classification tasks

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Contrastive Learning for Olfactory Representation
- **Claim:** Visual supervision can be used to learn generalizable olfactory representations from raw e-nose sensor data.
- **Mechanism:** COIP learns a joint embedding space by maximizing similarity between paired smell-image embeddings while minimizing similarity with non-matching pairs using SymInfoNCE loss.
- **Core assumption:** Objects that look similar also smell similar, or visual context provides strong supervisory signals for olfactory signatures.
- **Evidence anchors:** Inspired by CLIP, trained joint embedding between smell and images termed COIP; learned representations significantly outperform hand-crafted features.
- **Break condition:** Would fail if visual similarity doesn't correlate with olfactory similarity (e.g., visually identical containers with different contents).

### Mechanism 2: End-to-End Learning on Raw E-Nose Time-Series Data
- **Claim:** End-to-end learning on raw, time-series olfactory data outperforms traditional hand-crafted features ("smellprints").
- **Mechanism:** Neural networks process the raw T×32 matrix of sensor readings over time, potentially learning richer representations including 2nd-order statistics and temporal dynamics.
- **Core assumption:** Raw sensor signal contains more useful information for discrimination than handcrafted features.
- **Evidence anchors:** Smellprint discards many signals including 2nd-order statistics and correlations between different sensors; raw signal has potential to discover powerful representations.
- **Break condition:** Less effective if signal-to-noise ratio in raw temporal data is very low or critical information is fully captured by handcrafted approach.

### Mechanism 3: Scalable, In-the-Wild Multimodal Data Collection
- **Claim:** A large-scale, diverse dataset collected in natural environments is crucial for developing machine olfaction models for real-world application.
- **Mechanism:** Custom rig pairs electronic nose with RGB-D cameras; captures synchronized olfactory and visual data from thousands of distinct objects across diverse NYC locations.
- **Core assumption:** Models trained on controlled lab data don't generalize well to complex, real-world scenarios; natural diversity and noise is prerequisite for robust machine olfaction.
- **Evidence anchors:** Dataset is collected "in-the-wild" across 8 scene categories, 49 object categories, materials from Matador taxonomy; contrasts with lab-controlled data limitations.
- **Break condition:** If data collection introduces unmanageable systematic noise or diversity is too overwhelming without significantly more data.

## Foundational Learning
- **Concept:** Electronic Nose (E-Nose)
  - **Why needed here:** Source of olfactory signal; understanding array of sensors whose resistance changes in response to volatile compounds is key to understanding raw data representation (T×32 matrix).
  - **Quick check question:** What physical change in the Cyranose's sensors is measured to produce the olfactory signal?
- **Concept:** Contrastive Learning (CLIP-style)
  - **Why needed here:** Core learning method (COIP) used to train models; understanding how loss function trains two different encoders to map different modalities into shared embedding space.
  - **Quick check question:** In the contrastive loss, is the goal to maximize or minimize the dot product between embeddings of a paired smell and image?
- **Concept:** Handcrafted vs. Learned Features
  - **Why needed here:** Central claim that learned features on raw data outperform traditional handcrafted "smellprints"; understanding this distinction is critical for appreciating contribution.
  - **Quick check question:** According to the paper, what specific type of information does the "smellprint" discard that the raw signal contains?

## Architecture Onboarding
- **Component map:**
  1. Data Source: Custom rig with Cyranose 320 e-nose (raw T×32 matrix) + synchronized cameras (RGB image)
  2. Olfaction Encoder (fθS): Neural network (CNN, MLP, or Transformer) processing raw e-nose matrix into fixed-size embedding
  3. Vision Encoder (fθI): Standard vision model processing RGB image into fixed-size embedding
  4. Training Objective: Contrastive loss (SymInfoNCE variant) aligning embeddings of paired smell-image samples

- **Critical path:**
  1. Load batch of B paired samples: (Raw E-Nose Matrix, Image)
  2. Pass E-Nose data through Olfaction Encoder to get smell embeddings Z_s
  3. Pass Images through Vision Encoder to get image embeddings Z_i
  4. Compute contrastive loss (LI,S + LS,I) between Z_s and Z_i
  5. Backpropagate to update both encoders

- **Design tradeoffs:**
  - Olfaction Encoder Choice: CNNs provide inductive bias for local temporal patterns vs. Transformers for global context modeling
  - Raw Data vs. Smellprint: Raw data more computationally expensive but preserves information vs. smellprints faster but act as information bottleneck

- **Failure signatures:**
  - Overfitting: Dataset (7K pairs) relatively small for deep learning; model might memorize training pairs without proper regularization
  - Poor Generalization: Weak or inconsistent visual-olfactory correlation leads to noisy learned representations
  - Sensor Drift: E-nose sensors prone to drift over time, could corrupt learned representations if not accounted for

- **First 3 experiments:**
  1. Reproduce Baseline Comparison: Train MLP on provided "smellprint" features, compare performance against randomly initialized baseline
  2. Raw Data Encoder Ablation: Train simple 1D-CNN on raw T×32 olfactory data using contrastive loss, compare against smellprint MLP
  3. Linear Probe on Downstream Tasks: Take learned features from contrastive training, train linear classifier for scene/object/material recognition

## Open Questions the Paper Calls Out
- Can visual supervision utilizing temporal changes and 3D spatial cues improve olfactory representation learning over static image supervision?
- Do learned olfactory representations generalize across different electronic nose hardware or sensor arrays?
- Does incorporating active sensing dynamics, such as biological "sniffing" airflow patterns, improve recognition accuracy compared to passive sampling?

## Limitations
- Core claims hinge on assumption that visual appearance strongly correlates with chemical signatures, which may not hold uniformly for objects with identical visual properties but different odors
- Dataset size (7K pairs) is relatively small for deep learning, raising concerns about overfitting and robustness of learned representations
- Specific architectures for both vision and smell encoders are not fully specified, limiting reproducibility

## Confidence
- **High Confidence:** Dataset collection methodology and establishment of benchmark tasks are well-documented and reproducible; general finding that visual supervision improves olfactory representation learning is supported
- **Medium Confidence:** Superiority of end-to-end learning on raw e-nose data over handcrafted smellprints is demonstrated, but exact architectural choices and hyperparameters leading to result are underspecified
- **Low Confidence:** Paper doesn't address how sensor drift was handled during collection or provide evidence that learned representations are robust to this known issue in e-nose technology

## Next Checks
1. **Correlation Analysis:** Quantitatively measure visual-olfactory correlation by computing similarity metrics between learned smell and image embeddings for matched and mismatched pairs
2. **Robustness to Sensor Drift:** Retrain models on subsets collected at different times and measure performance degradation, or simulate drift by adding noise to raw signals
3. **Architectural Ablation Study:** Conduct systematic ablation study varying smell encoder architecture (CNN kernel sizes, Transformer depth) and vision encoder backbone, reporting impact on downstream task performance