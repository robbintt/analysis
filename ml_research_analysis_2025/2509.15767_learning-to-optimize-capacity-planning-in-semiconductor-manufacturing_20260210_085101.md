---
ver: rpa2
title: Learning to Optimize Capacity Planning in Semiconductor Manufacturing
arxiv_id: '2509.15767'
source_url: https://arxiv.org/abs/2509.15767
tags:
- time
- machine
- semiconductor
- actions
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a deep reinforcement learning framework for
  capacity planning in semiconductor manufacturing, addressing the challenge of optimizing
  resource allocation across complex production systems. The authors propose using
  a heterogeneous graph neural network (HGNN) to capture the intricate relationships
  between machines and operations, enabling proactive decision-making for machine-level
  actions including dedication, uptime, and efficiency adjustments.
---

# Learning to Optimize Capacity Planning in Semiconductor Manufacturing

## Quick Facts
- arXiv ID: 2509.15767
- Source URL: https://arxiv.org/abs/2509.15767
- Reference count: 34
- Primary result: HGNN-based DRL framework improves throughput and cycle time by ~1.8% over heuristics in large-scale semiconductor manufacturing

## Executive Summary
This paper presents a deep reinforcement learning framework for capacity planning in semiconductor manufacturing that uses a heterogeneous graph neural network (HGNN) to capture complex relationships between machines and operations. The framework addresses the challenge of optimizing resource allocation across production systems by enabling proactive decision-making for machine-level actions including dedication, uptime, and efficiency adjustments. The policy is trained using n-step Proximal Policy Optimization (PPO) with parallelized simulation instances to handle the vast action space. Experiments on Intel's Minifab testbed and the larger SMT2020 scenario demonstrate substantial improvements over heuristic baselines while maintaining better performance metrics than random actions and no-action controls.

## Method Summary
The framework represents the factory as a heterogeneous graph with machine and operation nodes connected by operation-operation edges (process flow dependencies) and operation-machine edges (assignments). A heterogeneous GNN encoder processes this graph to generate embeddings, while an attention-based decoder maps these embeddings to action probabilities across the vast machine-operation space. The policy is trained via n-step PPO using a variance reduction technique where rewards are computed as the difference between simulation outcomes with and without the proposed action. The method addresses the challenge of scaling to large action spaces through attention mechanisms and handles stochasticity through branching simulation trajectories.

## Key Results
- In the largest tested scenario (SMT2020), the trained policy increases throughput and decreases cycle time by approximately 1.8% each compared to heuristic baselines
- The framework demonstrates scalability from Intel's Minifab (5 machines) to SMT2020 (1,314 machines) while maintaining performance advantages
- The policy consistently outperforms random action baselines and no-action controls across all tested scenarios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Representing the factory as a heterogeneous graph allows the policy to capture complex, non-local dependencies between machines and operations that heuristics miss.
- **Mechanism:** The Heterogeneous Graph Neural Network (HGNN) constructs a graph with machine nodes and operation nodes. "Operation-operation" edges encode process flow dependencies, while "operation-machine" edges encode assignments. This structure allows the model to propagate information regarding bottlenecks (e.g., a downstream queue affecting upstream release) across the network via message passing.
- **Core assumption:** The topological structure of the factory (machines, operations, and flows) is sufficiently stable for a learned graph policy to generalize across varying demand and downtime scenarios.
- **Evidence anchors:** [abstract] "...heuristic rules... cannot easily account for the complex interactions along the process flow... By representing the policy using a heterogeneous graph neural network, the model directly captures the diverse relationships..." [section 4.1] "Different types of edges capture key relationships... operation-operation edges reflect logical dependencies... operation-machine edges denote assignment relations." [corpus] "Unsupervised Anomaly Prediction... Graph Neural Network" confirms GNNs are effective for modeling interdependent parameters in semiconductor time-series, supporting the architectural choice.
- **Break condition:** If the factory topology changes rapidly (e.g., constant reconfiguration) or the graph depth (L) is insufficient to bridge the distance between cause and effect (e.g., re-entrant flows requiring deep propagation), the mechanism fails.

### Mechanism 2
- **Claim:** Defining the reward as the difference between a simulation with the proposed action and a baseline "no-action" simulation isolates the specific impact of the action from environmental noise.
- **Mechanism:** Instead of optimizing an absolute KPI, the framework uses a counterfactual approach. It forks the simulation state: one branch applies the action (KPI₁), and one does not (KPI₀). The reward is rₜ = KPI₁ - KPI₀. This variance reduction technique helps the agent distinguish between "lucky" random demand and actual improvement caused by capacity decisions.
- **Core assumption:** The simulation engine is deterministic enough when seeded, or the simulation horizon is short enough, that variance between the two branches remains primarily attributable to the action.
- **Evidence anchors:** [section 3.1] "...better distinguish the effects of actions from the simulations' stochasticity by defining the reward as rₜ = KPI₁ - KPI₀..." [section 4.3] "Branching... allows us to periodically realign a no-action baseline trajectory for variance reduction." [corpus] Evidence in the corpus is weak for this specific counterfactual reward mechanism in semiconductor planning; no direct neighbors were found using differential simulation rewards.
- **Break condition:** If the simulation horizon is too long or stochasticity is too high, the two simulation branches diverge chaotically, rendering the KPI₁ - KPI₀ signal noise.

### Mechanism 3
- **Claim:** An attention-based decoder enables scalability to vast action spaces by dynamically weighting the compatibility of machine-operation pairs.
- **Mechanism:** The action space involves selecting machines (|M|) and operations (|O|), creating a combinatorial explosion. The decoder computes a probability πᵈᵢⱼ for machine-operation pairs using attention (dot product of embeddings) rather than enumerating all combinations directly. This allows the policy to efficiently sample dedication actions (d⁺, d⁻) from the valid subset of the space.
- **Core assumption:** The learned embeddings successfully compress the "fitness" of a machine for an operation into a vector space where dot products correlate with throughput gains.
- **Evidence anchors:** [abstract] "...measures taken to achieve sufficient scalability to tackle the vast space of possible machine-level actions." [section 4.2] "For operation-machine dedication actions, we use an attention mechanism to model pairwise interactions... yielding independent one-dimensional probabilities." [corpus] Corpus papers like "Physics-Informed Neural Networks" focus on process control, not planning; no specific evidence validates attention decoders for capacity planning specifically, suggesting this is a novel application in this domain.
- **Break condition:** If the "scaling factor" C in the attention mechanism is improperly tuned, probabilities may flatten (lack of exploration) or spike (premature convergence), causing the agent to select sub-optimal dedications.

## Foundational Learning

- **Concept:** **Markov Decision Process (MDP) Formulation**
  - **Why needed here:** The paper maps the dynamic, continuous factory environment into discrete steps (sₜ, aₜ, rₜ). Understanding this abstraction is required to see why "Future Change Lists" are treated as "Actions" and factory stats as "States."
  - **Quick check question:** Can you identify the State, Action, and Reward specifically defined in Section 3.1?

- **Concept:** **Graph Neural Networks (GNNs) & Message Passing**
  - **Why needed here:** The core engine of the policy is not a standard dense network but a GNN. You must understand that nodes (machines) update their state based on neighbors (operations) to grasp how "bottleneck propagation" works mathematically.
  - **Quick check question:** How does the "Edge-aware Attention" in Section 4.2 differ from a standard fully connected layer when processing machine features?

- **Concept:** **Proximal Policy Optimization (PPO)**
  - **Why needed here:** The agent is trained via n-step PPO. This algorithm clips gradient updates to prevent the policy from changing too drastically, a critical feature for maintaining stability in the complex simulation environment.
  - **Quick check question:** Why might standard Deep Q-Learning (DQN) struggle with the continuous-like nature of uptime/efficiency actions compared to the PPO approach used here?

## Architecture Onboarding

- **Component map:** D-SIMCON (Discrete Event Simulator) -> Heterogeneous Graph Construction -> HGNN Encoder -> Attention-based Decoder -> Action Sampling -> Simulation Branching -> PPO Trainer
- **Critical path:** The State Branching & Rollout (Section 4.3, Figure 2). The system must fork the simulation state, run two parallel trajectories (Action vs. No-Action), and compute the differential reward. Latency in the simulation engine here dictates training wall-clock time.
- **Design tradeoffs:**
  - **Depth (L) vs. Overfitting:** Table 2 shows L=1 outperformed L=2 on Minifab. Deeper graphs increase computational cost and risk overfitting to noise in smaller datasets, but may be needed for large factories (SMT2020 used L=2).
  - **Exploration Scaling (C):** The paper sets C=10 in the attention mechanism. Lower values encourage exploration but may slow convergence; higher values exploit known paths but risk local optima.
- **Failure signatures:**
  - **Policy Collapse/Decline:** Figure 3 and 5 show KPIs improving for ~40 epochs then degrading. This suggests the policy may be overfitting to specific transient states or "policy collapse" (cited as [6] in text).
  - **Variance Explosion:** If the gap between KPI₁ and KPI₀ fluctuates wildly without convergence, the "Branching" variance reduction is failing.
- **First 3 experiments:**
  1. **Minifab Hyperparameter Scan:** Replicate Table 2 by varying hidden dimension d and layer count L on the small Intel Minifab model to validate the pipeline without large compute costs.
  2. **Baseline Reward Check:** Run the agent with a standard absolute reward (just KPI₁) vs. the differential reward (KPI₁ - KPI₀) to quantify the value of the variance reduction mechanism.
  3. **SMT2020 Stability Test:** Run the full training loop on SMT2020 but stop at epoch 40 (the peak in Figure 3) vs. epoch 100 to confirm the degradation phenomenon and test proposed regularization fixes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can training stability be improved to prevent the observed performance decline during long-term training?
- Basis in paper: [explicit] The authors note a decline in KPIs after epoch 40 in SMT2020 and state they are investigating regularization measures in future work.
- Why unresolved: The current PPO-based training exhibits policy degradation over longer horizons, potentially due to policy collapse or overfitting.
- What evidence would resolve it: Training curves showing sustained or improved KPIs in SMT2020 scenarios running for significantly more than 40 epochs without decline.

### Open Question 2
- Question: How can the framework explicitly account for the financial costs of actions to ensure profitability?
- Basis in paper: [explicit] The conclusion states that future work must account for the costs incurred by different actions to avoid unprofitable decisions.
- Why unresolved: The current reward function optimizes physical metrics (throughput, cycle time) but ignores the economic trade-offs of machine modifications or uptime changes.
- What evidence would resolve it: A modified reward function that results in a policy where the economic value of performance gains exceeds the cost of the implemented actions.

### Open Question 3
- Question: Does incorporating information about future wafer starts improve the policy's ability to make proactive decisions?
- Basis in paper: [explicit] The authors plan to incorporate future wafer starts into the fab state representation to support targeted proactive decisions in dynamic settings.
- Why unresolved: The current state representation relies on current factory status, limiting the model's ability to anticipate future bottlenecks proactively.
- What evidence would resolve it: A performance comparison showing that a policy trained with future start information outperforms the current baseline in scenarios with highly variable demand.

## Limitations
- The paper relies on commercial D-SIMCON software and SMT2020 benchmark, which are not publicly available for exact reproduction
- The specific normalization schemes for node features and action masking strategy for invalid machine-operation pairs are not explicitly defined
- The framework's training stability degrades over long horizons, with KPIs declining after approximately 40 epochs in large-scale scenarios

## Confidence
- **High Confidence:** The core claim that a heterogeneous GNN can capture complex machine-operation relationships is well-supported by the architectural description and experimental results showing consistent improvements over baselines
- **Medium Confidence:** The variance reduction mechanism (KPI₁ - KPI₀ reward) is logically sound and described clearly, but its effectiveness depends heavily on simulator determinism assumptions that cannot be fully verified without access to the original environment
- **Medium Confidence:** The attention-based decoder's ability to scale to vast action spaces is theoretically justified, but the novel application to capacity planning lacks direct corpus evidence for validation

## Next Checks
1. **Simulator Fidelity Test:** Implement the Minifab model and verify basic dynamics (WIP accumulation, queue behavior) match expected semiconductor manufacturing patterns before attempting RL training
2. **Reward Mechanism Validation:** Run controlled experiments comparing the differential reward (KPI₁ - KPI₀) against absolute reward baselines to quantify variance reduction benefits on a simpler problem
3. **Policy Stability Monitoring:** During full SMT2020 training, implement early stopping based on evaluation metrics and compare results at epoch 40 (peak) versus epoch 100 (decline) to confirm the policy collapse phenomenon