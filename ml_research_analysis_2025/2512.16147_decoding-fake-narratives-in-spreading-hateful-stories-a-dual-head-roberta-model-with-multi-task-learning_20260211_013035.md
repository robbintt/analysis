---
ver: rpa2
title: 'Decoding Fake Narratives in Spreading Hateful Stories: A Dual-Head RoBERTa
  Model with Multi-Task Learning'
arxiv_id: '2512.16147'
source_url: https://arxiv.org/abs/2512.16147
tags:
- task
- hate
- classification
- fake
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a dual-head RoBERTa model for the Faux-Hate
  shared task, focusing on detecting fake narratives in hate speech within code-mixed
  Hindi-English text. The system uses a multi-task learning approach with two parallel
  classification heads: one for binary classification of fake and hate speech (Task
  A), and another for predicting the target and severity of hateful content (Task
  B).'
---

# Decoding Fake Narratives in Spreading Hateful Stories: A Dual-Head RoBERTa Model with Multi-Task Learning

## Quick Facts
- arXiv ID: 2512.16147
- Source URL: https://arxiv.org/abs/2512.16147
- Reference count: 3
- Primary result: Dual-head RoBERTa model achieves 0.76 F1 for fake/hate detection and 0.56 F1 for target/severity prediction on code-mixed Hindi-English text

## Executive Summary
This paper presents a dual-head RoBERTa architecture for detecting fake narratives in hate speech within code-mixed Hindi-English text. The system employs multi-task learning with two parallel classification heads: one for binary fake/hate detection and another for predicting target and severity of hateful content. The model incorporates layer normalization, GELU activation, staged dropout, and residual connections in the classification heads. Results demonstrate competitive performance with F1 scores of 0.76 for Task A and 0.56 for Task B, highlighting the effectiveness of multitask learning and architectural innovations for handling the complexities of code-mixed text.

## Method Summary
The approach uses a shared RoBERTa-base encoder with two task-specific classification heads. The encoder processes input text and extracts the [CLS] token representation, which is passed through each classification head. Task A performs binary classification (fake/hate vs not), while Task B predicts target and severity across four classes. Both heads use similar architectures with linear transformations, layer normalization, GELU activation, and dropout layers (0.2 → 0.1 staged). Losses are computed independently for each task and averaged during training. The model includes residual connections in the classification heads to improve gradient flow and training stability.

## Key Results
- Task A (binary fake/hate detection): Achieved F1 score of 0.76
- Task B (target/severity prediction): Achieved F1 score of 0.56 across four classes
- Residual connections improved Task A performance by 0.03 F1 compared to without residual connections
- Staged dropout (0.2 → 0.1) and layer normalization contributed to training stability
- Model demonstrated competitive results on the Faux-Hate shared task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-task learning enables knowledge transfer between fake/hate detection and target/severity prediction tasks.
- Mechanism: The shared RoBERTa-base encoder produces contextualized representations that both classification heads draw from. Task-specific losses are computed independently then averaged, allowing gradient signals from both tasks to update the shared encoder weights while dedicated parameters capture task-unique patterns.
- Core assumption: Fake narrative detection and hate speech target/severity prediction share underlying linguistic cues (e.g., sentiment polarity, rhetorical structures) that benefit from joint optimization.
- Evidence anchors:
  - [abstract] "The system achieved competitive results, demonstrating the efficacy of leveraging multi-task learning for this complex problem."
  - [section 3.2.2] "Independent Loss Computation: Each head computes its own task-specific loss, which are combined through averaging for multi-task learning."
  - [corpus] Related work shows RoBERTa-based architectures are effective for hate speech tasks, though no direct evidence confirms multi-task transfer benefits specific to this code-mixed domain.
- Break condition: If tasks are fundamentally antagonistic (e.g., fake narratives require opposite features from hate severity cues), shared encoder updates could degrade both tasks.

### Mechanism 2
- Claim: Residual connections in classification heads improve gradient flow and capture subtle task distinctions.
- Mechanism: Skip connections around intermediate layers allow gradients to bypass non-linear transformations during backpropagation, reducing vanishing gradient risk and enabling the model to retain raw [CLS] representations alongside transformed features.
- Core assumption: The classification task benefits from both processed and unprocessed representations of the encoder output.
- Evidence anchors:
  - [section 5] "The variant with residual connections achieved a test set F1 score of 0.76, outperforming the variant without residual connections (0.73)."
  - [section 3.2.2] "Residual Connections: While not enabled in the current configuration, residual connections can be added to facilitate gradient flow and improve training."
  - [corpus] No corpus papers directly validate residual connections for hate speech classification heads.
- Break condition: If residual connections propagate noise or irrelevant features from early layers, they could increase false positives.

### Mechanism 3
- Claim: Dual regularization (layer normalization + staged dropout) stabilizes training on noisy code-mixed text.
- Mechanism: Layer normalization normalizes activations across features, reducing internal covariate shift. Staged dropout (0.2 → 0.1) applies stronger regularization early in the classification head, then lighter regularization closer to output, balancing underfitting prevention with information preservation.
- Core assumption: Code-mixed Hindi-English text has higher variance and spurious correlations requiring aggressive regularization.
- Evidence anchors:
  - [section 3.2.1] "Layer Normalization: Applied after the input layer to improve training stability and convergence... A dropout layer with a probability of 0.2 is used... Another dropout layer with a lower probability (0.1) is applied."
  - [corpus] Multilingual Hate Speech Detection paper notes code-mixed text challenges, though no direct evidence links this regularization strategy to performance gains.
- Break condition: If regularization is too aggressive for the dataset size, the model may underfit complex patterns.

## Foundational Learning

- **Transformer encoder architectures ([CLS] token pooling)**
  - Why needed here: The model extracts the [CLS] representation from RoBERTa's final hidden state as the summary vector for classification. Understanding what [CLS] encodes is essential for debugging representation quality.
  - Quick check question: Can you explain why the [CLS] token, rather than mean-pooling all tokens, is used for classification tasks in BERT-family models?

- **Multi-task learning with soft parameter sharing**
  - Why needed here: The dual-head architecture shares the encoder but maintains independent head parameters. Understanding how loss averaging affects gradient dynamics is critical for tuning learning rates and loss weights.
  - Quick check question: If Task A loss is consistently higher than Task B loss, what adjustment could prevent Task A from dominating gradient updates?

- **Code-mixed text processing challenges**
  - Why needed here: Hindi-English code-mixed text contains non-standard grammar, transliteration variations, and script mixing that standard tokenizers may poorly segment.
  - Quick check question: How might RoBERTa's Byte-Pair Encoding tokenizer handle Romanized Hindi words like "bahut" or "accha"?

## Architecture Onboarding

- **Component map:**
```
Input Text → RoBERTa Tokenizer → RoBERTa-base Encoder (12 layers, 768 hidden)
         → [CLS] Extraction → Shared Dropout
         → ├─ Task A Head: Linear(768→768) → LayerNorm → GELU → Dropout(0.2)
         │              → Linear(768→384) → LayerNorm → GELU → Dropout(0.1)
         │              → Linear(384→2) → Binary Output
         └─ Task B Head: [Same architecture] → Linear(384→4) → Multi-class Output
```

- **Critical path:**
  1. Tokenization quality for code-mixed text (directly affects encoder representations)
  2. [CLS] pooling integrity (if positional embeddings are misaligned, pooled output degrades)
  3. Classification head residual connection configuration (paper shows 0.03 F1 swing on Task A)
  4. Loss balancing (currently equal averaging; imbalanced tasks may need weighted loss)

- **Design tradeoffs:**
  - Residual connections: Enabled vs disabled trades gradient flow quality against potential noise propagation. Paper empirically favors enabled.
  - Shared vs separate encoders: Current design shares encoder (parameter efficient, enables transfer) vs separate encoders would allow task-specific fine-tuning but increase memory.
  - Dropout staging (0.2→0.1): Aggressive early dropout reduces overfitting but may discard useful features; paper does not ablate this.

- **Failure signatures:**
  - Task A shows "slightly higher false positives in some cases" — suggests model conflates overlapping features of fake narratives and hate speech.
  - Task B F1 (0.56) significantly lower than Task A (0.76) — multi-class severity/target prediction may suffer from class imbalance or insufficient discriminative features.
  - No dataset size reported; if small, model may memorize rather than generalize.

- **First 3 experiments:**
  1. **Ablation on residual connections with statistical significance testing:** Replicate Table 1 with multiple random seeds (n≥5) to confirm the 0.03 F1 difference is not noise. Log per-class precision/recall to identify which classes benefit most.
  2. **Class distribution analysis for Task B:** Report class frequencies for the 4-class target/severity task. If imbalanced, test weighted cross-entropy loss with inverse class frequencies.
  3. **Tokenizer quality audit on code-mixed samples:** Extract 50 random examples, print tokenizer outputs, and measure unknown token ([UNK]) frequency. If high, consider vocabulary augmentation or a code-mixed pretrained model (e.g., mBERT, XLM-R).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating diverse datasets and refined fine-tuning strategies significantly enhance the robustness of the dual-head RoBERTa model?
- Basis in paper: [explicit] The authors state future work "will focus on refining the model with additional data and fine-tuning techniques to further improve its accuracy."
- Why unresolved: The current study is limited to the specific shared task dataset provided and standard training parameters.
- What evidence would resolve it: Experiments benchmarking the model on broader, external code-mixed datasets demonstrating improved F1 scores.

### Open Question 2
- Question: What specific architectural modifications are required to close the performance gap between binary detection (Task A) and target/severity prediction (Task B)?
- Basis in paper: [inferred] Task B yielded an F1 of 0.56 compared to Task A's 0.76, suggesting the multi-task approach struggled more with the complex multi-class component.
- Why unresolved: The paper attributes the lower score to task complexity but does not propose specific solutions to balance the performance disparity.
- What evidence would resolve it: Ablation studies showing architectural changes (e.g., hierarchical classification) that raise Task B F1 closer to Task A levels.

### Open Question 3
- Question: How can the model better distinguish overlapping features between fake narratives and hate speech to minimize false positives?
- Basis in paper: [inferred] The analysis section attributes observed false positives to the "inherent challenges of the dataset, such as the overlapping features of fake narratives and hate speech."
- Why unresolved: The current model relies on standard classification heads which may conflate these intertwined phenomena.
- What evidence would resolve it: Integration of feature disentanglement mechanisms or specialized loss functions showing a reduced false positive rate.

## Limitations

- The paper lacks dataset statistics including training sample size, class distributions, and validation methodology, making it difficult to assess model performance and potential overfitting.
- No statistical significance testing is provided for the ablation results showing residual connections improve performance by 0.03 F1.
- The paper does not validate tokenizer quality for code-mixed Hindi-English text, leaving uncertainty about the reliability of encoder representations.

## Confidence

- **High Confidence**: The architectural description is detailed and internally consistent, with clear specification of layer normalization, GELU activation, dropout staging, and residual connections. The multi-task learning framework is well-established in the literature.
- **Medium Confidence**: The reported F1 scores are presented as competitive results, but without dataset statistics, baseline comparisons beyond unspecified "baselines," or statistical significance testing, the true performance advantage remains uncertain.
- **Low Confidence**: Claims about knowledge transfer benefits from multi-task learning and the effectiveness of the regularization strategy for code-mixed text are not directly supported by experimental evidence. No comparison against single-task models or tokenizer quality assessment is provided.

## Next Checks

1. **Statistical significance testing for ablation results**: Re-run the residual connection ablation study with at least 5 random seeds and report mean F1 scores with 95% confidence intervals. This will determine whether the 0.03 F1 improvement is statistically significant or within expected variation.

2. **Class distribution and imbalance analysis for Task B**: Report the exact frequency distribution of the 4 target/severity classes. If any class represents less than 15% of the data, test weighted cross-entropy loss with inverse class frequencies and compare against the current equal-weighted loss.

3. **Code-mixed tokenizer quality audit**: Sample 100 random training examples and manually examine the tokenizer outputs. Count unknown tokens ([UNK]) and identify systematic tokenization failures. If the unknown token rate exceeds 2%, experiment with vocabulary augmentation or evaluate the model using mBERT/XLM-R which may handle code-mixing better.