---
ver: rpa2
title: 'FedSKD: Aggregation-free Model-heterogeneous Federated Learning using Multi-dimensional
  Similarity Knowledge Distillation'
arxiv_id: '2503.18981'
source_url: https://arxiv.org/abs/2503.18981
tags:
- knowledge
- learning
- data
- fedskd
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FedSKD is a model-heterogeneous federated learning framework that
  enables direct knowledge exchange between clients with heterogeneous model architectures
  without centralized aggregation. It introduces multi-dimensional similarity knowledge
  distillation (batch-wise, pixel/voxel-wise, and region-wise) to facilitate bidirectional
  cross-client knowledge transfer while mitigating model drift and knowledge dilution.
---

# FedSKD: Aggregation-free Model-heterogeneous Federated Learning using Multi-dimensional Similarity Knowledge Distillation

## Quick Facts
- **arXiv ID**: 2503.18981
- **Source URL**: https://arxiv.org/abs/2503.18981
- **Reference count**: 40
- **Primary result**: FedSKD achieves 76.66% mean AUC on Local Test and 67.10% on Global Test for fMRI-based autism spectrum disorder classification, outperforming FedCross by 5.39% and 3.44% respectively

## Executive Summary
FedSKD introduces a model-heterogeneous federated learning framework that eliminates the need for centralized model aggregation by enabling direct knowledge exchange between clients with heterogeneous architectures. The framework employs multi-dimensional similarity knowledge distillation at batch-wise, pixel/voxel-wise, and region-wise levels to facilitate bidirectional cross-client knowledge transfer. Experiments on two medical imaging tasks demonstrate significant performance improvements over state-of-the-art baselines while maintaining fairness across sensitive subgroups.

## Method Summary
FedSKD addresses the challenge of model heterogeneity in federated learning by replacing traditional aggregation with a knowledge distillation approach that works across different model architectures. The framework enables clients to directly exchange knowledge without requiring identical model structures, using similarity-based distillation at multiple granularities. The multi-dimensional approach captures different aspects of knowledge representation - from overall batch statistics to fine-grained pixel/voxel relationships and higher-level regional patterns. This allows for effective bidirectional knowledge transfer while mitigating model drift and knowledge dilution that commonly plague heterogeneous federated learning systems.

## Key Results
- On FedASD task: 76.66% mean AUC (Local Test) and 67.10% (Global Test), outperforming FedCross by 5.39% and 3.44% respectively
- On FedSkin with non-IID data (Î±=1): 88.83% mean AUC (Local Test) and 84.26% (Global Test), improving over FedCross by 3.14% and 3.05%
- Demonstrated improvements in fairness metrics across sensitive subgroups in both medical imaging tasks

## Why This Works (Mechanism)
The multi-dimensional similarity knowledge distillation enables effective knowledge transfer between heterogeneous models by capturing different levels of representation similarity. Batch-wise distillation transfers global patterns, pixel/voxel-wise distillation captures fine-grained spatial relationships, and region-wise distillation handles higher-level semantic understanding. This comprehensive approach ensures that knowledge is preserved and transferred across architectural differences, while the bidirectional transfer mechanism prevents model drift by maintaining balance between local and transferred knowledge.

## Foundational Learning

1. **Knowledge Distillation**: Why needed - enables model compression and transfer between different architectures; Quick check - verify distillation loss calculation and temperature scaling
2. **Federated Learning Aggregation**: Why needed - traditional FL requires model averaging which fails with heterogeneous architectures; Quick check - confirm aggregation-free operation through direct knowledge exchange
3. **Similarity Metrics**: Why needed - quantifies knowledge transfer effectiveness across different model representations; Quick check - validate similarity calculation methods for each dimension
4. **Bidirectional Transfer**: Why needed - prevents model drift by maintaining balance between local and transferred knowledge; Quick check - verify symmetric knowledge exchange implementation
5. **Non-IID Data Handling**: Why needed - real-world federated scenarios involve data heterogeneity; Quick check - confirm performance under varying data distribution parameters
6. **Fairness Metrics**: Why needed - ensures equitable performance across sensitive subgroups; Quick check - validate subgroup performance measurement and reporting

## Architecture Onboarding

**Component Map**: Clients (heterogeneous models) -> Multi-dimensional Knowledge Distillation -> Knowledge Exchange -> Performance Evaluation

**Critical Path**: Model training -> Similarity computation (3 dimensions) -> Knowledge distillation loss calculation -> Parameter update -> Knowledge exchange with peers

**Design Tradeoffs**: Aggregation-free approach eliminates central coordination but requires more complex similarity calculations; Multi-dimensional distillation captures comprehensive knowledge but increases computational overhead; Bidirectional transfer prevents drift but may introduce oscillation without proper balancing.

**Failure Signatures**: Poor similarity metrics indicate architectural incompatibility; Degraded local performance suggests excessive knowledge transfer; Inconsistent subgroup performance reveals fairness issues; High computational overhead may limit scalability.

**First 3 Experiments**:
1. Verify basic knowledge distillation between two simple heterogeneous models on synthetic data
2. Test single-dimension distillation (batch-wise only) to establish baseline performance
3. Evaluate bidirectional vs unidirectional transfer impact on model drift

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to two medical imaging datasets, raising generalizability concerns to other domains
- Performance variability across test conditions suggests sensitivity to evaluation protocols
- No computational overhead analysis for the multi-dimensional distillation process under different scales

## Confidence

**High confidence**: Methodological framework design and core distillation mechanism
**Medium confidence**: Absolute performance improvements (limited dataset diversity)
**Medium confidence**: Fairness metric improvements (sparse subgroup analysis details)
**Low confidence**: Scalability claims without computational cost analysis

## Next Checks
1. Evaluate FedSKD on non-medical datasets (e.g., CIFAR-100 or NLP tasks) to assess cross-domain applicability
2. Conduct ablation studies to quantify individual contributions of each distillation dimension
3. Measure computational overhead and communication costs under varying client numbers and model sizes