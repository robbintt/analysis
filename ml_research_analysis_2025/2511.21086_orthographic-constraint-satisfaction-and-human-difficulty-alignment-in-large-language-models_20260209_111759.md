---
ver: rpa2
title: Orthographic Constraint Satisfaction and Human Difficulty Alignment in Large
  Language Models
arxiv_id: '2511.21086'
source_url: https://arxiv.org/abs/2511.21086
tags:
- uni00000013
- uni00000048
- uni00000011
- words
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work systematically evaluates 28 configurations of three\
  \ language model families on 58 word puzzles requiring strict orthographic constraint\
  \ satisfaction. The study reveals that architectural differences produce substantially\
  \ larger performance gaps (2.0\u20132.2\xD7 F1) than parameter scaling within families\
  \ (83% gain from eightfold scaling), suggesting specialized architectural features\
  \ may be required beyond standard language model scaling."
---

# Orthographic Constraint Satisfaction and Human Difficulty Alignment in Large Language Models

## Quick Facts
- arXiv ID: 2511.21086
- Source URL: https://arxiv.org/abs/2511.21086
- Authors: Bryan E. Tuck; Rakesh M. Verma
- Reference count: 0
- 28 model configurations systematically evaluated on 58 word puzzles requiring strict orthographic constraint satisfaction

## Executive Summary
This work evaluates 28 configurations of three language model families on 58 word puzzles requiring strict orthographic constraint satisfaction. The study reveals that architectural differences produce substantially larger performance gaps (2.0–2.2× F1) than parameter scaling within families (83% gain from eightfold scaling), suggesting specialized architectural features may be required beyond standard language model scaling. Heterogeneous thinking budget sensitivity shows high-capacity models benefit strongly (+0.102 to +0.136 F1) while mid-sized variants saturate or degrade. Using difficulty ratings from 10,000 human solvers per puzzle, models show modest calibration (r=0.24–0.38) but systematically fail on common words with unusual orthography ("data", "poop", "loll": 89–96% model miss rate despite 86–95% human success), revealing over-reliance on distributional plausibility that penalizes orthographically atypical but constraint-valid patterns.

## Method Summary
The study evaluates 28 configurations across Qwen3 (4B, 8B, 14B, 32B dense, 30B MoE), Claude Haiku 4.5, and GPT-5-mini on 58 consecutive NYT Spelling Bee puzzles. Models are tested in direct and thinking modes at 4K/8K/16K token budgets using zero-shot evaluation. Performance is measured via precision, recall, and F1 against verified solution sets, with human difficulty calibration from 10,000 solvers per puzzle. The task requires generating English words using 7 specified letters including a mandatory center letter, minimum 4 letters, with repetition permitted.

## Key Results
- Architectural differences produce 2.0–2.2× F1 performance gaps (0.761 vs. 0.343) with 68% vs. 23% recall but only ~9% precision difference
- High-capacity models benefit strongly from thinking budgets (+0.102 to +0.136 F1) while mid-sized variants show negative returns or saturation
- Models fail systematically on common words with unusual orthography at 89–96% rate despite 86–95% human success
- 14B models show puzzling degradation with increased budget while 30B MoE variants improve dramatically with sufficient allocation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Cross-architecture differences in constraint satisfaction manifest primarily through recall rather than precision.
- **Mechanism:** Proprietary models discover more of the valid solution space through architectural or training advantages, while open-source models with similar precision fail to enumerate constraint-valid candidates.
- **Core assumption:** Architectural features enable more systematic exploration of combinatorial spaces.
- **Evidence anchors:** 2.0–2.2× F1 gap with 68% vs. 23% recall; proprietary models discover more valid solution space rather than higher-quality guesses.

### Mechanism 2
- **Claim:** Thinking budget benefits are capacity-dependent and architecturally heterogeneous.
- **Mechanism:** High-capacity models decompose constraint satisfaction into productive sub-steps; mid-sized models lack representational capacity to leverage additional compute.
- **Core assumption:** Extended reasoning requires minimum capacity thresholds before computational investment yields returns.
- **Evidence anchors:** 14B shows performance degradation with increased budget; 30B MoE exhibits dramatic budget dependence, doubling performance in 4K-to-8K range.

### Mechanism 3
- **Claim:** Models over-rely on distributional plausibility, systematically missing orthographically atypical but constraint-valid patterns.
- **Mechanism:** Training on distributional patterns creates priors that penalize unusual letter combinations even when they satisfy explicit constraints.
- **Core assumption:** Constraint-checking mechanisms are not structurally independent from statistical priors.
- **Evidence anchors:** 89–96% model miss rate on words like "data", "poop", "loll" despite 86–95% human success; near-universal failures on doubled consonants and symmetric patterns.

## Foundational Learning

- **Concept: Constrained text generation**
  - Why needed here: The task requires satisfying discrete character-level rules distinct from semantic pattern matching.
  - Quick check question: Can you explain why generating "wagon" from {A,G,I,L,N,O,W} with mandatory W is fundamentally different from predicting the next token in a sentence?

- **Concept: Distributional plausibility vs. structural validity**
  - Why needed here: These two objectives conflict for orthographically unusual words; understanding this tension is prerequisite to interpreting systematic failure patterns.
  - Quick check question: Why might "poop" have lower generation probability than a less common word under standard language modeling, despite satisfying puzzle constraints?

- **Concept: Mixture-of-experts routing dynamics**
  - Why needed here: The MoE variant shows dramatically different budget sensitivity than dense models.
  - Quick check question: Why would a 30B MoE model with 3B active parameters underperform at low thinking budgets but improve substantially with more tokens?

## Architecture Onboarding

- **Component map:** Prompt design → Mode selection (direct/thinking) → Budget allocation → Word list extraction → Constraint verification → Scoring against ground truth
- **Critical path:** Input (7-letter set + mandatory center letter + constraints) → Model selection (Qwen3/Claude/GPT variants) → Generation mode (direct/thinking) → Token budget allocation → Output verification → Performance scoring
- **Design tradeoffs:** Open-source (inspectable, controllable) vs. proprietary (2× better F1, unknown architecture); small models + high budget (may degrade) vs. large models + appropriate budget (responsive); zero-shot (isolates intrinsic capability) vs. few-shot (may mask limitations)
- **Failure signatures:** High precision / low recall (search breadth failure); performance degradation with increased budget (mid-capacity model); systematic misses on doubled consonants / repeated letters (distributional plausibility bias); catastrophic length-dependent decline (working memory / verification complexity limit)
- **First 3 experiments:** Budget calibration run (4K/8K/16K for single model); Orthographic pattern probe (held-out set with doubled consonants, symmetric repeats); Length-stratified recall analysis (4/5/6/7+ letter words separately)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural features enable superior constraint satisfaction in proprietary models, and can these be replicated in open architectures?
- Basis in paper: Authors state architectural differences produce 2.0–2.2× performance gaps but "cannot disentangle these factors without access to proprietary details."
- Why unresolved: Proprietary model internals are inaccessible; the paper cannot isolate whether gains stem from architecture, training data, or objectives.
- What evidence would resolve it: Ablation studies on open models incorporating hypothesized features with controlled training regimes.

### Open Question 2
- Question: Do the identified failure patterns generalize to non-English orthographic systems?
- Basis in paper: The distributional plausibility mechanism may transfer to other alphabetic systems without testing.
- Why unresolved: No cross-linguistic experiments were conducted; morphological and orthographic conventions differ substantially across languages.
- What evidence would resolve it: Replication on spelling bee analogues in languages with varying orthographic depth.

### Open Question 3
- Question: What training objectives could effectively reward orthographically unusual but constraint-valid solutions without degrading general language modeling performance?
- Basis in paper: Conclusion lists "(2) training objectives rewarding orthographically unusual but valid solutions" as a research direction.
- Why unresolved: The paper identifies the problem but does not propose, implement, or evaluate any specific training modifications.
- What evidence would resolve it: Controlled experiments comparing models trained with auxiliary losses penalizing distributional bias.

## Limitations
- Proprietary model access with incomplete architectural details limits understanding of performance gaps
- Human solver data from 10,000 users per puzzle cannot be independently verified or reproduced
- Thinking budget analysis assumes linear scaling benefits that may not hold across different task domains
- Findings specific to English orthography may not generalize to other languages

## Confidence
- **High confidence**: Architectural vs. scaling distinction (2.0–2.2× vs. 83% gain), capacity-dependent thinking budget effects, distributional plausibility bias on orthographically atypical words
- **Medium confidence**: Specific numerical performance gaps and human difficulty alignment correlations, MoE budget sensitivity curves
- **Low confidence**: Universal applicability of "search breadth vs. verification accuracy" distinction across different constraint satisfaction tasks

## Next Checks
1. **Architectural ablation study**: Test open-source models with modified attention patterns to isolate whether the recall gap stems from architectural features rather than scale alone.

2. **Cross-domain constraint transfer**: Evaluate the same model configurations on Sudoku or ZebraLogic puzzles to verify whether thinking budget sensitivity and recall characteristics generalize beyond orthographic constraints.

3. **Distributional bias quantification**: Create a synthetic test set with controlled orthographic properties matched to human word frequency to independently verify the 89–96% failure rate on atypical patterns.