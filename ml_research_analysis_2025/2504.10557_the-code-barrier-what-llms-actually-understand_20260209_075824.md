---
ver: rpa2
title: 'The Code Barrier: What LLMs Actually Understand?'
arxiv_id: '2504.10557'
source_url: https://arxiv.org/abs/2504.10557
tags:
- code
- llms
- description
- obfuscation
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates how well large language models (LLMs) comprehend
  obfuscated code, addressing whether LLMs understand code semantics beyond pattern
  recognition. Using code obfuscation as a testing framework, the research systematically
  evaluates 13 state-of-the-art models on tasks involving code description generation
  and deobfuscation across three obfuscation types (variable renaming, dead code injection,
  and literal encryption) using a benchmark of 250 Java programming problems.
---

# The Code Barrier: What LLMs Actually Understand?
## Quick Facts
- arXiv ID: 2504.10557
- Source URL: https://arxiv.org/abs/2504.10557
- Reference count: 40
- Primary result: General-purpose LLMs outperform code-specialized models on code description tasks when code is obfuscated

## Executive Summary
This study evaluates how well large language models comprehend obfuscated code, addressing whether LLMs understand code semantics beyond pattern recognition. Using code obfuscation as a testing framework, the research systematically evaluates 13 state-of-the-art models on tasks involving code description generation and deobfuscation across three obfuscation types using a benchmark of 250 Java programming problems.

Key findings reveal that general-purpose models like GPT-4o and DeepSeek-R1 outperform code-specialized models in code description tasks, challenging assumptions about domain-specific training benefits. LLMs demonstrate heavy reliance on meaningful variable names and clear literal values, with description accuracy declining significantly when these are altered. The study also identifies that LLMs struggle most with concise, structurally complex code and that code-specialized models generally perform better on deobfuscation tasks than general-purpose ones.

## Method Summary
The study uses an automated obfuscation pipeline to create three types of code transformations (variable renaming, dead code injection, and literal encryption) applied to 250 Java programming problems from CodeNet. Thirteen LLMs (both general-purpose and code-specialized) are evaluated on two tasks: generating code descriptions and deobfuscating code. Semantic similarity between generated descriptions and ground truth is assessed using LLM judges (GPT-4o and Claude Sonnet 3.7), while deobfuscation success is measured through compilation and functional testing. The methodology includes both quantitative performance metrics and qualitative analysis of failure modes.

## Key Results
- General-purpose models like GPT-4o and DeepSeek-R1 outperform code-specialized models in code description tasks
- LLMs show heavy reliance on meaningful variable names, with description accuracy declining significantly when these are altered
- LLMs struggle most with concise, structurally complex code (high cyclomatic complexity with few tokens)
- Code-specialized models generally perform better on deobfuscation tasks than general-purpose ones

## Why This Works (Mechanism)
### Mechanism 1
- **Claim:** LLM code comprehension relies more heavily on lexical semantics (variable names) than structural control flow for description tasks.
- **Mechanism:** The model's attention mechanism anchors onto meaningful identifiers as semantic shortcuts, bypassing the computationally expensive simulation of runtime logic. When these anchors are removed (obfuscation), the model cannot easily reconstruct the intent.
- **Core assumption:** The performance drop is caused by the loss of semantic hints in names, not an inability to parse the obfuscated syntax itself.
- **Evidence anchors:**
  - [abstract] "LLMs demonstrate heavy reliance on meaningful variable names... with description accuracy declining significantly when these are altered."
  - [section 4.2.1] Variable renaming caused the highest performance degradation (average 18.6% drop), whereas dead code injection had the least impact.
  - [corpus] "When Names Disappear: Revealing What LLMs Actually Understand About Code" confirms that removing the naming channel severely impacts performance.
- **Break condition:** If models trained specifically on nameless abstract syntax trees (ASTs) outperform standard models on these tasks, this mechanism would be invalidated.

### Mechanism 2
- **Claim:** General-purpose models outperform code-specialized models on semantic description because they leverage broader reasoning capabilities rather than overfitting to syntactic patterns.
- **Mechanism:** Generalist models (e.g., GPT-4o) apply "world knowledge" and reasoning to infer context from partial code structures, whereas specialized models may rely on rigid pattern matching learned from clean codebases.
- **Core assumption:** The performance delta is due to reasoning capability rather than simply model size (though GPT-4o is also the largest model tested).
- **Evidence anchors:**
  - [abstract] "General-purpose models like GPT-4o... outperform code-specialized models in code description tasks, challenging assumptions about domain-specific training benefits."
  - [section 4.1.1] Specialized models like StarChat and CodeStral showed counterintuitive performance drops or stagnation compared to generalist models.
  - [corpus] Corpus evidence is weak regarding the specific "generalist vs. specialist" inversion; related papers focus more on semantic recovery than model type comparison.
- **Break condition:** If code-specialized models consistently outperform generalist models on *functionally equivalent* but novel obfuscation techniques, the "overfitting" hypothesis breaks.

### Mechanism 3
- **Claim:** LLMs fail non-linearly on high "information density" code—specifically, concise code with high cyclomatic complexity.
- **Mechanism:** Transformers struggle to distribute attention effectively when complex logic is compressed into few tokens. The attention mechanism dilutes over structural keywords, missing the implicit logic required for "dense" algorithms.
- **Core assumption:** Failure is driven by the ratio of complexity-to-tokens, not just the absolute complexity.
- **Evidence anchors:**
  - [section 4.2.2] "Samples with high information density—implementing complex logic with fewer tokens—present a unique challenge... Failed samples exhibit higher average cyclomatic complexity... while containing substantially fewer tokens."
  - [abstract] "LLMs struggle most with concise, structurally complex code."
- **Break condition:** If increasing context window size or using "chain-of-thought" prompting eliminates this failure mode, the bottleneck is context capacity rather than the density mechanism itself.

## Foundational Learning
- **Concept:** Code Obfuscation Types (Lexical, Structural, Data)
  - **Why needed here:** The paper uses these specific perturbations to decouple "pattern matching" from "semantic understanding." You cannot interpret the results without distinguishing *renaming* (semantic hints) from *dead code* (structural noise) and *encryption* (data hiding).
  - **Quick check question:** If a model fails on "Dead Code Injection" but succeeds on "Variable Renaming," what does that imply about its reliance on control flow vs. naming conventions?

- **Concept:** Semantic Equivalence vs. Syntactic Similarity
  - **Why needed here:** The core evaluation relies on judging if a generated description matches the *intent* (d') of the code, not just the tokens. The paper uses LLMs as judges for this, assuming they can detect semantic equivalence.
  - **Quick check question:** Why is functional testing (input/output execution) necessary in addition to semantic description comparison when evaluating deobfuscation?

- **Concept:** Cyclomatic Complexity
  - **Why needed here:** This metric is used in Section 4.2.2 to identify "blind spots" where LLMs fail. It quantifies the density of control flow.
  - **Quick check question:** According to the paper, does high cyclomatic complexity alone predict LLM failure, or must it be combined with another factor (token count)?

## Architecture Onboarding
- **Component map:** Obscura Dataset (d, d', s, s_obf) -> Obfuscator Tool -> LLM Subject (13 models) -> LLM Evaluator (GPT-4o/Claude) -> Execution Environment (Java compiler/runner)
- **Critical path:**
  1. **Benchmark Creation:** Generating d' (context-free descriptions) is critical; original descriptions contained narrative fluff (e.g., Batman stories) that biased evaluations.
  2. **Evaluation Loop:** LLM Evaluator scoring is the primary metric for description accuracy. Execution testing is the primary metric for deobfuscation correctness.
- **Design tradeoffs:**
  - **LLM-as-Judge:** Using LLMs to evaluate descriptions introduces circularity and potential bias. *Mitigation:* The paper uses binary classification checks and dual-model evaluation (GPT-4o + Claude), but acknowledges subjectivity.
  - **Dataset Scope:** Limiting to Java and algorithmic competition code (CodeNet). *Tradeoff:* High correctness ground truth, but low representativeness of "spaghetti" enterprise code.
- **Failure signatures:**
  - **Hallucination (Domain Shift):** StarChat (specialized) exhibited a 55-78% rate of generating non-code-related content (hallucination) when failing, whereas top models just gave inaccurate descriptions.
  - **Functional Decoupling:** Code often compiles (syntactic success) but fails functional tests (semantic failure), especially with literal encryption.
- **First 3 experiments:**
  1. **Baseline Description (No Obfuscation):** Prompt subject LLMs with clean code (s) to generate descriptions. Evaluate against d' to establish a "pattern matching" baseline without interference.
  2. **Obfuscation Stress Test:** Prompt subject LLMs with s_ren, s_dead, and s_enc. Measure the degradation slope. *Look for:* Does accuracy drop correlate with specific obfuscation types? (Expected: Renaming > Encryption > Dead Code).
  3. **Functional Deobfuscation:** Ask LLMs to rewrite s_obf to clean code. Compile and run I/O tests. *Look for:* Gap between "Code looks clean" and "Code runs correctly."

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can adversarial training specifically on obfuscated code improve the robustness of LLMs against syntactic distortion?
- Basis in paper: [explicit] Section 5.4 explicitly lists "investigating whether adversarial training with obfuscated code improves robustness" as a primary direction for future research.
- Why unresolved: The current study only evaluates pre-trained models off-the-shelf; it does not attempt to re-train or fine-tune models using the obfuscated samples to see if performance improves.
- What evidence would resolve it: An experiment where models are fine-tuned on the Obscura dataset (or similar obfuscated data) and then re-evaluated on the description and deobfuscation tasks to measure performance gains.

### Open Question 2
- Question: How effective are hybrid approaches that combine LLMs with traditional program analysis for semantic code understanding?
- Basis in paper: [explicit] Section 5.4 calls for "developing hybrid approaches that combine LLM capabilities with traditional program analysis techniques."
- Why unresolved: Current LLMs struggle with semantic preservation (Finding 7) and rely on surface patterns (Finding 3); the paper suggests traditional tools might bridge this gap but provides no data on such a combination.
- What evidence would resolve it: A study evaluating a pipeline where static analysis tools pre-process code (e.g., extracting control flow graphs) before LLM analysis, compared against LLM-only baselines.

### Open Question 3
- Question: Do the findings regarding generalist vs. specialist model performance generalize to non-object-oriented languages like C or Python?
- Basis in paper: [inferred] Section 5.3 identifies "Language specificity" as a threat to validity, noting the "Java-only focus limits generalizability to languages with different paradigms."
- Why unresolved: The observed phenomenon where general-purpose models outperform code-specialized ones on descriptions (Finding 1) might be specific to Java's syntax or the tokenization of its constructs.
- What evidence would resolve it: Replicating the Obscura benchmark methodology on the CodeNet Python800 or C++ benchmarks and comparing the performance rankings of the 13 models.

### Open Question 4
- Question: What distinct internal mechanisms allow code-specialized models to excel at deobfuscation while failing at description generation compared to generalist models?
- Basis in paper: [inferred] Findings 1 and 6 show a paradox where specialized models generally lose at description but win at deobfuscation, which Section 5.1 speculates is due to "different aspects of code understanding" without proving the mechanism.
- Why unresolved: The paper identifies the performance discrepancy but does not perform mechanistic interpretability (e.g., attention head analysis) to explain why specialization helps structural reconstruction but hurts abstract semantic articulation.
- What evidence would resolve it: An analysis of attention patterns or internal representations in specialized vs. general models when processing obfuscated tokens to identify divergent processing strategies.

## Limitations
- Dataset limited to Java algorithmic competition code, which may not generalize to real-world enterprise codebases
- Evaluation methodology uses LLMs as judges, introducing potential circularity and bias
- Model size differences confound comparisons between general-purpose and code-specialized models

## Confidence
- **Medium**: Primary claim about lexical reliance on variable names - systematic obfuscation experiments provide strong evidence, but LLM-as-judge evaluation introduces bias
- **Low-Medium**: Comparative performance between general-purpose and code-specialized models - confounded by model size differences, though the pattern is observed consistently
- **Medium-High**: Struggle with high information density code - robust empirical evidence with cyclomatic complexity and token count correlation

## Next Checks
1. **Cross-Domain Validation**: Test the same obfuscation techniques on enterprise codebases and open-source projects to verify if the lexical reliance pattern holds across different coding styles and naming conventions.

2. **Size-Normalized Model Comparison**: Conduct controlled experiments comparing generalist and specialized models of similar parameter counts to isolate whether performance differences stem from reasoning capability or model scale.

3. **Hybrid Evaluation Framework**: Implement a mixed evaluation approach combining LLM-as-judge with human annotation and execution-based functional testing to reduce evaluation bias and validate semantic equivalence judgments.