---
ver: rpa2
title: 'Multi-Label Classification with Generative AI Models in Healthcare: A Case
  Study of Suicidality and Risk Factors'
arxiv_id: '2507.17009'
source_url: https://arxiv.org/abs/2507.17009
tags:
- label
- match
- performance
- sets
- suicide
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the use of generative large language models
  (GPT-3.5, GPT-4.5) for multi-label classification of suicidality-related factors
  (SI, SA, ES, NSSI) from psychiatric EHR notes. Fine-tuned GPT-3.5 achieved 0.94
  partial match accuracy and 0.91 F1 score, while GPT-4.5 with guided prompting showed
  superior performance across label sets, including rare ones.
---

# Multi-Label Classification with Generative AI Models in Healthcare: A Case Study of Suicidality and Risk Factors

## Quick Facts
- arXiv ID: 2507.17009
- Source URL: https://arxiv.org/abs/2507.17009
- Reference count: 0
- Primary result: GPT-3.5 fine-tuning achieved 0.94 partial match accuracy and 0.91 F1 score for classifying suicidality-related factors from psychiatric EHR notes

## Executive Summary
This study explores using generative large language models (GPT-3.5, GPT-4.5) for multi-label classification of suicidality-related factors from psychiatric EHR notes. The approach treats the combination of labels as distinct semantic units, enabling the capture of co-occurring clinical factors that binary classifiers miss. Fine-tuned GPT-3.5 showed high accuracy on dominant classes, while GPT-4.5 with guided prompting demonstrated superior performance across all label sets, including rare ones. The work introduces novel label-set-level metrics and a multi-label confusion matrix to reveal systematic error patterns, particularly the conflation of suicidal ideation and attempts, and a tendency toward cautious over-labeling.

## Method Summary
The study classified four suicidality-related factors (SI, SA, ES, NSSI) from 500 Initial Psychiatric Evaluation notes using generative LLMs. Three conditions were tested: zero-shot prompting, zero-shot with guideline definitions, and fine-tuning (GPT-3.5 only). The models were evaluated using exact-match and partial-match accuracy, along with micro/macro F1 scores at label, instance, and label-set levels. A custom 16×16 multi-label confusion matrix was developed to analyze error patterns. Five-fold cross-validation was used for fine-tuning experiments, with three runs per condition.

## Key Results
- GPT-3.5 fine-tuning achieved 0.94 partial match accuracy and 0.91 F1 score
- GPT-4.5 with guided prompting showed superior performance across all label sets, including rare or minority label sets
- The models exhibited a systematic tendency toward cautious over-labeling, with 86 hallucination errors versus 31 omission errors
- Error analysis revealed consistent conflation between Suicidal Ideation and Suicide Attempts

## Why This Works (Mechanism)

### Mechanism 1: Multi-Label vs Binary Classification
Generative models capture co-occurring clinical factors more effectively than binary classifiers by treating label combinations as distinct semantic units. The architecture predicts the full label set simultaneously rather than independent probabilities for each class, learning the conditional probability of correlated factors directly from text context.

### Mechanism 2: Pre-trained Model Generalization
Advanced general-purpose models (GPT-4.5) with guided prompting demonstrate superior calibration for rare or minority label sets compared to fine-tuned smaller models. Larger pre-trained models leverage extensive prior knowledge to maintain representation of rare concepts, requiring only a "guideline" prompt to align their output format.

### Mechanism 3: Cautious Safety Screening
The system acts as a cautious safety screener by exhibiting a systematic bias toward "hallucination" (over-labeling) rather than "omission" (under-labeling). The model interprets ambiguous clinical narratives through a lens of maximum sensitivity, preferring to flag potential risk rather than miss a threat.

## Foundational Learning

- **Concept: Multi-Label Classification (MLC) vs. Multi-Class**
  - Why needed here: Understanding this distinction is critical for interpreting the "Exact Match" vs. "Partial Match" metrics used in the paper.
  - Quick check question: If a model predicts `{SI, SA}` but the true label is `{SI, SA, ES}`, is this a complete failure in an Exact Match scenario?

- **Concept: Label-Set-Level Evaluation (Powerset)**
  - Why needed here: The paper introduces metrics that treat the *combination* of labels as the unit of analysis, revealing "blind spots" that micro-averaged scores hide.
  - Quick check question: Why would a model have a high Micro-F1 score but a low Macro-F1 score on label sets in this specific dataset?

- **Concept: Prompt Engineering vs. Fine-Tuning**
  - Why needed here: Understanding the trade-off—fine-tuning optimizes for the specific data distribution, while prompting leverages the model's general reasoning—is key to replicating the results.
  - Quick check question: Based on the paper, which approach is better for finding rare risk factors like "Exposure to Suicide" (ES), and why?

## Architecture Onboarding

- **Component map:** Input (IPE notes) -> Processing Core (LLM: GPT-3.5-Tune or GPT-4.5-Guide) -> Control Logic (Prompt Engineering or Fine-tuning) -> Output (Binary vector [SI, SA, ES, NSSI]) -> Evaluation Layer (Custom Label-Set Metrics + 16×16 Confusion Matrix)

- **Critical path:**
  1. Data Pre-processing: Cleaning IPE notes while retaining clinical nuance
  2. Prompt Design: Formulating the "Guideline" with label definitions is the highest-ROI step for zero-shot models
  3. Metric Selection: Prioritize Partial Match F1 to capture nuance in multi-label scenarios

- **Design tradeoffs:**
  - GPT-3.5-Tune: Highest accuracy on dominant classes, requires labeled training data, lower inference cost, susceptible to class imbalance
  - GPT-4.5-Guide: Superior balance on rare classes, zero-shot (no training data), likely higher inference cost, better generalization

- **Failure signatures:**
  - The "SA-only" Conflation: Consistently misclassifies "Suicide Attempt Only" cases as "Ideation & Attempt"
  - NSSI False Associations: Falsely associates Non-Suicidal Self-Injury with Suicide Attempts
  - Hallucination Cascade: Predicts maximum label set for ambiguous notes

- **First 3 experiments:**
  1. Sanity Check (Zero-Shot): Run GPT-3.5 without guidelines on 50 notes to establish baseline conflation rate between SI and SA
  2. Ablation (Guideline): Add ES and NSSI definitions to prompt, measure delta in Macro-F1 for rare labels
  3. Error Profiling: Generate 16×16 confusion matrix, calculate ratio of Hallucinations to Omissions to verify cautious over-labeling hypothesis

## Open Questions the Paper Calls Out

- **Open Question 1:** Can targeted prompt engineering or fine-tuning strategies specifically mitigate the systematic conflation of Suicide Attempts (SA) and Non-Suicidal Self-Injury (NSSI) without reducing overall sensitivity?

- **Open Question 2:** Do the reported high performance metrics generalize to psychiatric notes from institutions with different documentation styles and patient demographics?

- **Open Question 3:** Does the structured data extracted via this multi-label classification pipeline produce valid results when used as input for downstream large-scale observational studies?

## Limitations
- Small dataset (500 notes) from a single clinical setting limits generalizability
- Absence of detailed hyperparameter specifications and complete prompt templates prevents exact replication
- Clinical validation of predictions (whether flagged cases required intervention) is not addressed

## Confidence
- **High confidence:** Comparative performance between GPT-3.5 fine-tuned and GPT-4.5 guided prompting models
- **Medium confidence:** Clinical utility of cautious over-labeling approach without longitudinal outcome data
- **Medium confidence:** Mechanism explaining why GPT-4.5 performs better on rare classes

## Next Checks
1. **Clinical outcome validation:** Implement blinded review process where clinicians assess model predictions against actual clinical outcomes
2. **Cross-institutional replication:** Test approaches on psychiatric notes from a different healthcare system to assess generalizability
3. **Threshold optimization study:** Conduct ROC analysis on label-set level to identify optimal decision thresholds balancing safety benefit against workflow disruption