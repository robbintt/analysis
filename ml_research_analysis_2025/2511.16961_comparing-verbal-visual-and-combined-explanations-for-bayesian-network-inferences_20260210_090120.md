---
ver: rpa2
title: Comparing verbal, visual and combined explanations for Bayesian Network inferences
arxiv_id: '2511.16961'
source_url: https://arxiv.org/abs/2511.16961
tags:
- verbal
- visual
- figure
- participants
- explanatory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares three explanation modalities (verbal, visual,
  combined) for Bayesian Networks (BNs) against a baseline. Users performed better
  with all explanation types than baseline for questions about observation impact,
  path contributions, and interaction effects (p<0.05).
---

# Comparing verbal, visual and combined explanations for Bayesian Network inferences

## Quick Facts
- arXiv ID: 2511.16961
- Source URL: https://arxiv.org/abs/2511.16961
- Reference count: 40
- One-line primary result: Users performed better with all explanation types than baseline for questions about observation impact, path contributions, and interaction effects (p<0.05).

## Executive Summary
This study evaluates three explanation modalities (verbal, visual, combined) for Bayesian Networks against a baseline interface. Using 124 participants, the research demonstrates that all explanation types significantly improve user performance on questions about observation impact, path contributions, and interaction effects compared to a baseline interface. Combined explanations (verbal + visual) outperform single-modality explanations for specific question types, particularly finding questions and path questions, though no significant differences emerge for control questions requiring only BN knowledge.

## Method Summary
The study employed a Qualtrics survey with 124 participants recruited via Prolific Academic. Participants completed a training task with a "Car Battery" BN, then answered questions about two test BNs ("Rats" and "Podunk") across four interface conditions: Baseline, Verbal, Visual, and Combined. Questions covered four types: Finding (impact of observations), CommonEffect (interaction effects), Path (contribution paths), and PathThink (control questions). Mixed-effects logistic regression analyzed accuracy scores, with fixed effects for explanatory condition, question type, and tutorial score, and random effects for participant ID and question ID.

## Key Results
- All three explanation types (verbal, visual, combined) significantly outperformed baseline for questions about observation impact, path contributions, and interaction effects (p<0.05)
- Combined explanations outperformed visual-only explanations for finding questions and verbal-only explanations for path questions
- No significant differences between conditions for control questions requiring only BN knowledge
- System usability scores were highest for explanation conditions, with combined explanations receiving the highest SUS scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-modal explanations (verbal + visual) improve comprehension of Bayesian Network inferences over single-modality or no explanation.
- Mechanism: Verbal and visual modalities provide complementary cognitive pathways. Verbal explanations articulate abstract reasoning (e.g., paths, contributions), while visual highlighting and animations make spatial and temporal relationships concrete. Together, they reduce cognitive load for tasks requiring tracking evidence propagation through network structures.
- Core assumption: Users can integrate information across modalities without excessive split-attention; the mappings between text and visuals are clear.
- Evidence anchors:
  - [abstract] "users did better with all three types of extensions than with the baseline UI for questions about the impact of an observation, the paths that enable this impact, and the way in which an observation influences the impact of other observations; and (2) using verbal and visual modalities together is better than using either modality alone for some of these question types."
  - [section] Section 5.1, H4: "The effect size of Both explanations was larger than that of Visual ones only for Finding questions, and larger than that of Verbal explanations only for Path questions (p-value<0.05), supporting 2 out of 8 sub-hypotheses."
  - [corpus] No direct corpus support for this specific multi-modal finding; related BN work focuses on structure learning and forecasting, not explanation interfaces.
- Break condition: If modalities are poorly synchronized (e.g., animation timing mismatches text), split-attention effects may increase cognitive load, reducing or eliminating the benefit.

### Mechanism 2
- Claim: Explanations that explicitly quantify contributions of findings and arcs improve performance on impact-related questions.
- Mechanism: The system computes absolute contribution differences (e.g., +25%, -17%) and conveys them via a color-coded scale and verbal descriptors ("moderately increases"). This makes abstract probabilistic shifts concrete and comparable.
- Core assumption: Users interpret verbal magnitude descriptors consistently and map them correctly to visual color intensities.
- Evidence anchors:
  - [section] Section 3.1: "These contributions are expressed as absolute differences (e.g., +25%,−17%), conveyed verbally using a similar scale to that in [27], and visually through matching color coding."
  - [abstract] "users did better with all three types of extensions than with the baseline UI for questions about the impact of an observation..."
  - [corpus] Weak/missing; corpus papers do not address contribution quantification for explanations.
- Break condition: If contribution scales are non-linear or inconsistently applied across variables, users may misinterpret magnitude relationships.

### Mechanism 3
- Claim: Counterfactual explanations and path highlighting help users understand interaction effects in common-effect structures.
- Mechanism: For interacting findings (e.g., Peeling and Mutation), the system sequentially shows (i) the contribution without the interacting finding, (ii) how adding the interaction changes the situation, and (iii) the final contribution. Animations highlight arcs along active paths, fading irrelevant nodes.
- Core assumption: Users can mentally simulate counterfactual scenarios presented in sequence and correctly attribute changes to specific interactions.
- Evidence anchors:
  - [section] Section 3.2: "A user can find out more in a detailed mode that (i) describes the paths that make a difference to a finding's contribution (Figure 3(c)), and (ii) offers a counterfactual explanation about the interaction between this finding and others (Figure 4(c))."
  - [section] Section 5.1, H1: Participants in explanation conditions outperformed baseline for CommonEffect questions (p-value≪0.001).
  - [corpus] No corpus papers address counterfactual explanations for BN user interfaces.
- Break condition: If counterfactual sequences are too long or involve multiple interacting variables, users may lose track of the baseline comparison.

## Foundational Learning

- Concept: Conditional independence and d-separation in Bayesian Networks
  - Why needed here: Understanding when observations can or cannot influence target probabilities depends on d-separation rules. Without this, users cannot interpret path explanations or predict impact.
  - Quick check question: If two nodes are not directly connected but share a child, does observing the child create a dependency between them? (Answer: Yes, via the common-effect pattern.)

- Concept: Belief propagation (probability updates given evidence)
  - Why needed here: The explanations assume users understand that probabilities propagate through the network when evidence is added; animations visualize this process.
  - Quick check question: If you observe a parent node, does it change the probability of its child? (Answer: Yes, by Bayes' rule applied to the child's CPT.)

- Concept: Multi-modal interface design (split-attention vs. dual-coding)
  - Why needed here: Combining verbal and visual modalities can reduce cognitive load if well-integrated, but cause split-attention if poorly synchronized.
  - Quick check question: If an animation shows path highlighting while unrelated text is displayed, what risk does this introduce? (Answer: Split-attention, increasing cognitive load.)

## Architecture Onboarding

- Component map:
  - Contribution computation module -> Verbal explanation generator -> Visual explanation engine -> Synchronization layer -> User interaction controller
  - Each component handles specific aspects of explanation generation and delivery

- Critical path:
  1. User observes a finding → Contribution module computes impact → Verbal and visual outputs generated
  2. User requests detailed path explanation → Path contributions computed → Animation triggers sequentially with verbal descriptions
  3. For interacting findings → Counterfactual sequence generated and displayed step-by-step

- Design tradeoffs:
  - Template-based verbal generation vs. LLM: Templates are predictable but brittle; LLMs offer fluency but risk misrepresenting inferences (Section 6)
  - Full path explanation vs. summary: Full paths scale poorly for large BNs; summaries may omit critical details
  - Manual vs. automated contribution computation: Study used manual crafting; production systems need automated computation

- Failure signatures:
  - Mismatched animation timing with text segments causing user confusion
  - Inconsistent contribution scale application across variables
  - Overly complex counterfactual sequences for dense interaction networks

- First 3 experiments:
  1. Replicate with larger, non-binary BNs to test scalability of explanation methods
  2. A/B test template-based vs. LLM-generated verbal explanations for accuracy and user trust
  3. Measure split-attention effects by varying synchronization granularity between verbal and visual modalities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do these explanation modalities improve performance for expert users or students formally training in Bayesian Networks?
- Basis in paper: [explicit] The authors note the limitation of using "mainly participants with no prior BN experience" and suggest evaluating the implemented system with "different cohorts, e.g., students who are learning BNs."
- Why unresolved: The study relied on crowd-workers who lacked domain expertise; it is unclear if visual animations or verbal redundancy would distract experts who already possess strong mental models of probabilistic inference.
- What evidence would resolve it: A comparative user study replicating the experimental protocol with a cohort of data science students or professional analysts.

### Open Question 2
- Question: How can these explanation methods be adapted to scale effectively for large, complex Bayesian Networks?
- Basis in paper: [explicit] The authors state their system is "well suited only to small BNs" because explaining the impact of every arc and displaying counterfactuals "scale poorly," warranting investigation into adaptation for larger networks.
- Why unresolved: The study only tested small, binary networks; applying the current "arc-by-arc" visual and verbal methods to networks with dozens of nodes would likely cause cognitive overload and screen clutter.
- What evidence would resolve it: A study testing summary-based or filterable explanation interfaces on large networks (e.g., >20 variables), measuring user accuracy and subjective cognitive load.

### Open Question 3
- Question: Can Large Language Models (LLMs) generate these explanations accurately without hallucinating probabilistic inferences?
- Basis in paper: [explicit] The authors identify LLMs as a "natural next step" to overcome the limitations of template-based generation, but note that "high-quality training and response validation are essential."
- Why unresolved: Template-based approaches are rigid but accurate, while LLMs offer fluency but are prone to generating factually incorrect reasoning (hallucinations) regarding specific probability calculations.
- What evidence would resolve it: An evaluation comparing the factual accuracy of LLM-generated BN explanations against ground-truth inferences derived directly from the network.

### Open Question 4
- Question: Does tailoring the explanation modality to a user's spatial reasoning ability improve task performance?
- Basis in paper: [inferred] The authors found that participants in the Visual condition had lower spatial reasoning scores (paper-folding task) than the control group, leading them to posit it is "worth... tailoring explanation types to users' abilities."
- Why unresolved: It is unknown if the Visual modality performed worse relative to Verbal due to the interface design itself or because that specific cohort struggled with the spatial demands of the animations.
- What evidence would resolve it: A study that assigns explanation modalities based on pre-screened spatial reasoning scores to test for an interaction effect between ability and interface type.

## Limitations
- Scalability concerns: The study only tested two small, binary BNs, raising questions about the method's effectiveness for larger, more complex networks
- Template-based verbal explanations may lack the nuance and fluency that LLM-generated text could provide, though this trade-off was acknowledged
- Reliance on Prolific participants limits ecological validity, as participants lacked domain expertise in Bayesian Networks

## Confidence

- High: Core finding that all explanation conditions outperform baseline for observation impact, path contributions, and interaction effects (p<0.05)
- High: Combined explanations provide additional benefit over single modalities for specific question types
- Medium: Usability findings (SUS scores) and generalizability to other BN structures
- Low: Claims about scalability and the potential benefits of LLM-generated explanations

## Next Checks

1. **Scalability Test:** Replicate the study with larger, non-binary BNs (e.g., 10+ nodes, multi-valued variables) to assess whether explanation benefits persist
2. **LLM Integration:** A/B test template-based vs. LLM-generated verbal explanations for accuracy, user trust, and comprehension
3. **Split-Attention Analysis:** Systematically vary synchronization granularity between verbal and visual modalities to measure the impact of split-attention effects on cognitive load and performance