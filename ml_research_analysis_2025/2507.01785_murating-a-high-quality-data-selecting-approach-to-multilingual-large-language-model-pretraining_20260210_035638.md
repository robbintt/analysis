---
ver: rpa2
title: 'MuRating: A High Quality Data Selecting Approach to Multilingual Large Language
  Model Pretraining'
arxiv_id: '2507.01785'
source_url: https://arxiv.org/abs/2507.01785
tags:
- multilingual
- data
- language
- quality
- murater
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of multilingual data selection
  for large language model pretraining, where existing model-based approaches focus
  almost exclusively on English. The proposed method, MuRating, aggregates multiple
  English-language quality raters via pairwise comparisons using a Bradley-Terry model,
  then transfers these quality judgments across 17 target languages through translation.
---

# MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining

## Quick Facts
- arXiv ID: 2507.01785
- Source URL: https://arxiv.org/abs/2507.01785
- Reference count: 40
- Primary result: Improves multilingual LLM pretraining by 1-3.4 points on English tasks and 1.8 points on multilingual evaluations through pairwise quality aggregation and translation-based transfer

## Executive Summary
This work addresses the challenge of multilingual data selection for large language model pretraining, where existing model-based approaches focus almost exclusively on English. The proposed method, MuRating, aggregates multiple English-language quality raters via pairwise comparisons using a Bradley-Terry model, then transfers these quality judgments across 17 target languages through translation. This creates a unified multilingual evaluator trained on monolingual, cross-lingual, and parallel text pairs. When applied to pretrain 1.2B and 7B parameter LLaMA models, MuRating consistently improves performance across both English and multilingual benchmarks compared to strong baselines including QuRater, AskLLM, and FineWeb2-HQ, achieving average gains of 1-3.4 points on English tasks and 1.8 points on multilingual evaluations. Analysis shows that pairwise supervision provides more stable multilingual scoring than pointwise methods, and incorporating cross-lingual and parallel pairs improves language-agnostic quality assessment.

## Method Summary
MuRating constructs a multilingual quality evaluator through a three-stage pipeline. First, it aggregates 300,000 English text pairs annotated by four raters (AskLLM, DCLM, FineWeb-Edu, GPT-4o) using a Bradley-Terry pairwise preference model to learn unified quality scores. Second, it translates these pairs to 17 languages via GPT-4o, creating monolingual, cross-lingual, and parallel training pairs with neutral labels for translations. Third, it fine-tunes a BGE-M3 encoder on this multilingual dataset using pairwise loss and parallel regularization, then scores FineWeb-2 documents to select the top 10% per language for pretraining 1.2B and 7B LLaMA models. The approach anchors quality assessment in English and transfers preferences rather than absolute scores to handle translation artifacts.

## Key Results
- MuRater(E) improves 1.2B model performance by 1.3 points on English tasks and 2.3 points on multilingual tasks compared to QuRater
- MuRater(M) improves 7B model performance by 1.5 points on English tasks and 1.8 points on multilingual tasks compared to QuRater
- Pairwise supervision shows more stable cross-lingual transfer than pointwise methods, with better correlation preservation across language pairs
- Cross-lingual and parallel pair regularization improves language-agnostic scoring, with slopes closer to 1.0 and lower MSE on parallel document scoring

## Why This Works (Mechanism)

### Mechanism 1: Bradley-Terry Aggregation for Rater Consolidation
Aggregating multiple noisy English quality raters through pairwise comparison modeling yields more stable, unified quality scores than any individual rater. The Bradley-Terry model converts rater disagreement into probability estimates (PA>B), then trains a single scorer via binary cross-entropy loss to predict pairwise preferences. This filters idiosyncratic biases of individual raters while preserving consensus quality signals. Core assumption: Rater disagreements are noise rather than orthogonal quality dimensions.

### Mechanism 2: Pairwise Supervision for Translation-Robust Multilingual Transfer
Transferring pairwise preference labels across languages via translation is more robust than pointwise score transfer because relative rankings degrade more gracefully under translation artifacts. English pairs (tAen, tBen) with preference probability PAen>Ben are translated to (tAm, tBm), preserving PAen>Ben ≈ PAm>Bm. Pointwise scores require absolute calibration across languages, which translation perturbs. Pairwise only requires rank preservation. Core assumption: Translation preserves relative quality rankings even when absolute quality shifts.

### Mechanism 3: Cross-Lingual and Parallel Pair Regularization for Language-Agnostic Scoring
Training on cross-lingual pairs (tAm, tBm') and parallel pairs (tAm, tAm') with neutral labels forces the model to learn language-invariant quality representations. Parallel pairs with P=0.5 create an explicit constraint: translated texts receive identical scores. Cross-lingual pairs extend this by requiring quality comparisons across languages. The parallel loss penalizes score divergence between translations. Core assumption: Quality is fundamentally language-agnostic.

## Foundational Learning

- **Bradley-Terry Model:**
  - Why needed here: Core mechanism for converting multi-rater preferences into scalar scores; foundation of the entire pipeline
  - Quick check question: Given preferences [A>B, A>B, B>A], what probability does Bradley-Terry estimate for A>B? (Answer: 2/3, then refined via MLE)

- **Preference Learning / Reward Modeling (RLHF-style):**
  - Why needed here: The pairwise loss is borrowed from RLHF reward modeling; understanding this framing clarifies why the method works
  - Quick check question: How does pairwise preference learning differ from pointwise regression? (Answer: Relative comparisons vs. absolute scores; reduces calibration burden)

- **Cross-Lingual Transfer via Translation:**
  - Why needed here: The entire multilingual extension depends on translating English supervision; need to understand translation fidelity trade-offs
  - Quick check question: What types of content are most vulnerable to translation quality degradation? (Answer: Idioms, cultural references, technical terminology)

## Architecture Onboarding

- **Component map:**
  [English Raters: AskLLM, DCLM, FineWeb-Edu, QuRater]
          ↓ (score pairs)
  [Bradley-Terry Aggregation] → Unified English preference scores
          ↓ (translate pairs)
  [Multilingual Pair Generator]
      ├─ Monolingual pairs (same language)
      ├─ Cross-lingual pairs (different languages)
      └─ Parallel pairs (neutral label)
          ↓
  [MuRater: BGE-M3 encoder + linear head]
          ↓ (score documents)
  [Top-10% selection per language] → Pretraining corpus
          ↓
  [LLaMA-architecture LLM pretraining]

- **Critical path:**
  1. English pair generation and multi-rater annotation (300K pairs, ~$9.7K GPT-4o cost)
  2. Bradley-Terry training on preference aggregation
  3. Translation of pairs to 17 languages (~$18.7K GPT-4o cost, 600K pairs total)
  4. MuRater fine-tuning on combined multilingual data (3 epochs, batch 512, lr 2e-5)
  5. Document scoring and selection (top 10% per language)

- **Design tradeoffs:**
  - MuRater(E) vs. MuRater(M): English-anchored outperforms multilingual-anchored by 0.5-1.0 points. Trade-off: MuRater(E) assumes English corpora have richer topical diversity.
  - Pairwise vs. Pointwise: Pairwise more robust but requires 2× scoring operations; pointwise simpler but noisier cross-lingually.
  - Parallel pair weight (λ=0.5): Higher λ forces stronger language-invariance but may reduce discrimination ability.
  - Base encoder choice (BGE-M3): Selected for multilingual coverage (100+ languages) and efficiency; may inherit embedding space biases.

- **Failure signatures:**
  - Low performance on narrative/creative content: English raters focus on "educational value" and factuality; creative writing undervalued.
  - Language-specific score drift: Japanese/Thai show lower translation quality and correspondingly lower selection scores; may under-sample high-quality content.
  - Domain collapse: Over-selection of "Health," "Science," "People and Society" may reduce diversity.

- **First 3 experiments:**
  1. Validate Bradley-Terry aggregation benefit: Train single-rater baselines vs. aggregated rater on held-out preference pairs. Metric: preference prediction accuracy (Table 3 shows 93% validation accuracy—replicate this breakdown).
  2. Ablate cross-lingual vs. parallel contributions: Train three MuRater variants—(a) monolingual pairs only, (b) +cross-lingual, (c) +parallel. Measure slope/MSE on held-out parallel documents (Figure 4 replication).
  3. Test translation quality sensitivity: Replace GPT-4o translation with Qwen3-8B; measure correlation between translation quality scores and downstream benchmark performance per language (Section 6.5 provides correlation data—extend to performance impact).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the MuRating framework be adapted to effectively select high-quality narrative and creative content?
- Basis in paper: The Limitations section states the auto-rater exhibits "limited performance on narrative and creative domains" because the underlying English raters prioritized factual and informational content.
- Why unresolved: The current aggregation of English raters biases the selection against creative writing, potentially filtering out valuable fiction or dialogue data necessary for well-rounded LLM capabilities.
- What evidence would resolve it: A study measuring downstream performance on creative writing benchmarks (e.g., story completion) using models pretrained on data selected by a "narrative-aware" variant of MuRater.

### Open Question 2
- Question: To what extent do the biases of the teacher model (GPT-4o) propagate into the final multilingual data selection?
- Basis in paper: The authors explicitly note that "reliance on GPT-4o introduces potential biases and idiosyncrasies inherent to proprietary large language models."
- Why unresolved: It remains unclear if the quality scores reflect intrinsic document value or merely the specific preferences of the GPT-4o annotation and translation process.
- What evidence would resolve it: A comparative analysis of MuRaters trained with different annotation sources (e.g., human raters vs. open-source LLMs) to isolate the variance attributed to the teacher model's bias.

### Open Question 3
- Question: Can a unified multilingual rater outperform language-specific raters for culturally distinct or low-resource languages?
- Basis in paper: The Limitations section suggests that "further research could explore language-specific rater designs or culturally aligned data selection strategies."
- Why unresolved: While a unified rater offers scalability, it may suppress language-specific nuances or cultural contexts that a localized rater would capture.
- What evidence would resolve it: An ablation study comparing the unified MuRater against individual, monolingual raters trained on native high-quality corpora for specific low-resource languages within the set.

## Limitations
- Data construction cost and scalability: English pairwise annotation requires ~300K judgments at ~$0.03 per judgment using GPT-4o, totaling ~$9.7K, plus ~$18.7K for translation, creating significant barriers for broader adoption.
- Ground truth quality ambiguity: No validation against human-annotated ground truth exists; Bradley-Terry aggregation may simply consolidate rater biases rather than identify objective quality.
- Domain collapse risk: Consistent over-selection of "People and Society," "Health," and "Science" domains may reduce corpus diversity and create unintended biases in pretrained models.

## Confidence
**High Confidence:**
- Bradley-Terry aggregation improves preference prediction accuracy over individual raters (Table 3 shows 93% validation accuracy)
- MuRater(E) outperforms MuRater(M) by 0.5-1.0 points on English tasks
- Pairwise supervision shows more stable cross-lingual transfer than pointwise methods (Figure 5 correlation analysis)

**Medium Confidence:**
- Cross-lingual and parallel pair regularization improves language-agnostic scoring (Figure 4 slope/MSE improvements)
- Top-10% selection consistently outperforms strong baselines across model scales (1.2B and 7B)
- Translation-based transfer works for 17 languages but untested beyond this set

**Low Confidence:**
- Quality is fundamentally language-agnostic (assumed rather than tested)
- Bradley-Terry model is optimal for multi-rater aggregation (no comparison to alternative methods)
- Performance gains generalize to other model architectures or pretraining objectives

## Next Checks
1. **Bradley-Terry aggregation validation:** Replicate the core aggregation experiment by training single-rater baselines and comparing their preference prediction accuracy against the aggregated model. Break down performance by individual rater pairs to identify which disagreements the aggregation resolves most effectively.

2. **Translation quality sensitivity analysis:** Systematically vary translation quality by replacing GPT-4o with Qwen3-8B or another high-quality but lower-cost LLM. Measure the correlation between translation quality scores and downstream benchmark performance per language, then quantify the performance impact of translation degradation.

3. **Cross-lingual regularization ablation:** Train three MuRater variants—(a) monolingual pairs only, (b) +cross-lingual pairs, (c) +parallel pairs with λ=0.5. Compare their performance on held-out parallel documents using slope and MSE metrics, and assess whether cross-lingual regularization actually improves language-agnostic quality assessment or simply regularizes the model.