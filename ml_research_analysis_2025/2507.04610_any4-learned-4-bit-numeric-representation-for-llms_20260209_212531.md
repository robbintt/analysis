---
ver: rpa2
title: 'any4: Learned 4-bit Numeric Representation for LLMs'
arxiv_id: '2507.04610'
source_url: https://arxiv.org/abs/2507.04610
tags:
- quantization
- any4
- numeric
- values
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: any4 introduces a learned 4-bit quantization scheme for LLMs that
  adaptively determines per-row numeric representations using weighted K-means clustering,
  avoiding preprocessing or outlier preservation. Evaluated across multiple model
  families and sizes (1B-70B Llama, Mistral, Mixtral), any4 achieves the highest accuracy
  compared to int4, fp4, and nf4 formats while remaining competitive with methods
  requiring preprocessing (AWQ, GPTQ).
---

# any4: Learned 4-bit Numeric Representation for LLMs

## Quick Facts
- **arXiv ID**: 2507.04610
- **Source URL**: https://arxiv.org/abs/2507.04610
- **Reference count**: 31
- **Primary result**: any4 achieves highest accuracy among 4-bit formats while remaining competitive with preprocessing methods

## Executive Summary
any4 introduces a learned 4-bit quantization scheme for LLMs that adaptively determines per-row numeric representations using weighted K-means clustering, avoiding preprocessing or outlier preservation. Evaluated across multiple model families and sizes (1B-70B Llama, Mistral, Mixtral), any4 achieves the highest accuracy compared to int4, fp4, and nf4 formats while remaining competitive with methods requiring preprocessing (AWQ, GPTQ). It supports arbitrary 4-bit codes per row, incurs only 0.0625 bits overhead per weight, and can be calibrated using a single diverse sample. An accompanying GPU-optimized library, tinygemm, enables efficient inference, with 4-bit kernels providing up to 3× speedup over 16-bit baselines.

## Method Summary
any4 is a post-training weight-only quantization method that learns per-row codebooks through weighted K-means clustering. For each weight matrix row, it computes group-wise scales (α, β) for group size g=128, then applies weighted K-means with 16 clusters where sample weights equal α×E[|x|] (activation magnitude from calibration). The method uses a single hand-crafted 88-word prompt covering fiction, news, code, math, and facts for calibration, achieving activation statistics comparable to large datasets. Storage requires 4.3125 bits/weight (4 data + 0.25 metadata + 0.0625 LUT overhead).

## Key Results
- Achieves highest accuracy among 4-bit formats (int4, fp4, nf4) across Llama, Mistral, and Mixtral models
- Competitive with preprocessing methods (AWQ, GPTQ) while avoiding their computational overhead
- Single diverse sample calibration matches performance of 128-sample dataset calibration
- tinygemm library provides up to 3× speedup over 16-bit baselines for 4-bit inference
- Method extends competitively to 3-bit and 2-bit quantization

## Why This Works (Mechanism)

### Mechanism 1: Data-Adaptive Codebook Learning via Weighted K-Means
any4 learns 16 arbitrary bf16 values per matrix row through weighted K-means clustering, where weights combine group_scale and expected_activation_magnitude. This outperforms fixed numeric formats by matching each weight row's distribution. The approach assumes weight distributions vary sufficiently across matrix rows to justify per-row codebook overhead of 0.0625 bits/weight.

### Mechanism 2: Output-Aware Quantization Objective
The clustering objective minimizes E[|ŷ - y|] (output activation error) rather than weight reconstruction error, incorporating input activations x_j and group scales α_i,j. This improves downstream accuracy by optimizing for the actual task objective. The approach assumes calibration activation statistics approximate runtime distributions.

### Mechanism 3: Single-Sample Calibration Sufficiency
A hand-crafted 88-word prompt covering five diverse domains (fiction, news, code, math, facts) provides sufficient activation diversity to estimate E[|x|] per channel. This achieves calibration quality matching hundreds of dataset samples, with topic diversity covering major activation distribution modes.

## Foundational Learning

- **Concept: Lloyd's Algorithm (K-Means)**
  - Why needed here: Core of any4's quantization—alternating assignment (E-step) and centroid update (M-step)
  - Quick check question: Can you trace how Eq. 18 (E-step assignment) differs from Eq. 23 (M-step centroid)?

- **Concept: Groupwise Quantization**
  - Why needed here: any4 uses group size g=128 with shared asymmetric scale α and zero-point β per group
  - Quick check question: Given weights in [-0.1, 0.05] and int4 range [-8, 7], compute α and β using Eq. 2.

- **Concept: Lookup Table Dequantization**
  - Why needed here: tinygemm stores 16-entry bf16 LUTs per row in registers; 4-bit codes index via warp shuffle
  - Quick check question: How many register bytes for one bf16 LUT (16 entries × 2 bytes)?

## Architecture Onboarding

- **Component map:** Calibration → Scaling → K-means → Storage → tinygemm → Inference
- **Critical path:** 1) Calibration: Run forward pass, collect mean(|activation|) per channel 2) Quantization: Scale weights → run weighted K-means per row → pack indices + LUTs 3) Inference: Load 4-bit tiles → LUT lookup via warp shuffle → dequantize → mma
- **Design tradeoffs:** Group size: g=64 (best accuracy, more metadata) vs g=128 (default) vs g=1024 (any4 still stable; nf4/fp4 collapse); Scaling: Asymmetric (default) vs symmetric (better for Llama3-70B per Table 1); Initialization: k-means++ clearly outperforms random/int4/nf4 seeding (Table A4)
- **Failure signatures:** Perplexity >100: Check scale factor numerical stability; verify LUT indexing correct; any4 worse than nf4: Confirm activation weights applied (Table A3 ablation); Slow inference: Ensure weights pre-transposed to mma tile format; verify LUT in registers, not shared memory
- **First 3 experiments:** 1) Replicate Table 1 baseline: Quantize Llama3-8B with int4/fp4/nf4/any4 (g=128), measure WikiText-2 and C4 perplexity. Expected: any4≈6.51 < nf4≈6.63. 2) Calibration ablation (Table 3): Compare single diverse sample vs 128 C4 samples. Expected: within 0.1 perplexity. 3) Group size sweep (Table 4): Test g∈{64,128,256,512,1024} on Llama3.2-1B. Expected: any4 degrades gracefully; nf4/fp4 collapse at g=1024.

## Open Questions the Paper Calls Out

- **Question 1:** Can combining any4 with orthogonal pre-processing techniques (like AWQ, GPTQ, or QuaRot) yield accuracy improvements that exceed either method in isolation?
- **Question 2:** To what extent does the use of a single, hand-crafted calibration sample influence model bias, truthfulness, or domain-specific performance compared to standard large-scale datasets like C4?
- **Question 3:** Why does the performance of the learned representation degrade relative to incoherence-processing methods at 2-bit precision, and can this gap be closed?

## Limitations

- Convergence criteria for K-means clustering loop not specified (uses "till values converge")
- Symmetric vs asymmetric scaling selection criteria not clearly defined for new models
- Single-sample calibration mechanism lacks theoretical justification for sufficiency
- tinygemm kernel implementation details limited, making independent verification difficult

## Confidence

**High Confidence** in core contribution: any4 demonstrably achieves better accuracy than fixed 4-bit formats across multiple benchmarks and model families, with competitive results to preprocessing methods.

**Medium Confidence** in single-sample calibration claim: Empirical results show parity with 128-sample calibration, but mechanism isn't fully explained and could be domain-dependent.

**Medium Confidence** in mechanism explanations: Weighted K-means approach clearly described, but theoretical justification for output-aware objective could be more rigorous; some implementation details underspecified.

**Low Confidence** in tinygemm performance claims: 3× speedup claim depends heavily on specific GPU architectures and tensor core utilization patterns not fully characterized.

## Next Checks

1. **Convergence Criteria Validation**: Reproduce K-means implementation with explicit convergence thresholds (e.g., 1e-4 weight movement or max 100 iterations) and verify impact on perplexity scores across different model sizes.

2. **Domain Generalization Test**: Evaluate any4-calibrated models on specialized domains not represented in calibration prompt (medical literature, legal documents) to test limits of single-sample calibration sufficiency.

3. **Kernel Performance Benchmarking**: Implement simplified tinygemm kernel on different GPU architectures (A100, H100, RTX 4090) to independently verify claimed 3× speedup and identify architectural dependencies.