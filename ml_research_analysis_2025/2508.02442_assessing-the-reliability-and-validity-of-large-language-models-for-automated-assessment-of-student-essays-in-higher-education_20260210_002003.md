---
ver: rpa2
title: Assessing the Reliability and Validity of Large Language Models for Automated
  Assessment of Student Essays in Higher Education
arxiv_id: '2508.02442'
source_url: https://arxiv.org/abs/2508.02442
tags:
- human
- scores
- scoring
- each
- essay
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examined the reliability and validity of five Large
  Language Models (LLMs) for automated essay scoring in a higher education context.
  Using a four-criterion rubric, 67 Italian-language student essays were scored by
  five models (Claude 3.5, DeepSeek v2, Gemini 2.5, GPT-4, and Mistral 24B) across
  three replications each.
---

# Assessing the Reliability and Validity of Large Language Models for Automated Assessment of Student Essays in Higher Education

## Quick Facts
- arXiv ID: 2508.02442
- Source URL: https://arxiv.org/abs/2508.02442
- Reference count: 0
- Large Language Models (LLMs) failed to reliably replicate human judgment in automated essay scoring, showing consistently low agreement with human raters and weak within-model reliability.

## Executive Summary
This study evaluated five LLMs (Claude 3.5, DeepSeek v2, Gemini 2.5, GPT-4, and Mistral 24B) for automated essay scoring using a four-criterion rubric on 67 Italian-language student essays. Despite high internal consistency, models showed consistently low agreement with human raters (Quadratic Weighted Kappa near zero) and weak within-model reliability (median Kendall's W < 0.30). Systematic scoring biases emerged, with models inflating Coherence scores while struggling with context-sensitive criteria like Pertinence and Feasibility. These findings highlight the current limitations of LLMs for interpretive academic assessment and underscore the need for continued human oversight in this domain.

## Method Summary
The study used 67 Italian-language student essays (2,500-3,000 words) from a master's-level psychology course. Five LLMs were evaluated using a four-criterion rubric (Pertinence, Coherence, Originality, Feasibility) across three replications each. Models were prompted via OpenRouter API with fixed parameters (temperature=0.25, max tokens=3,050) in JSON output mode. Statistical analysis included Quadratic Weighted Kappa for human-LLM agreement, Kendall's W for within-model reliability, and error metrics (MAE, bias).

## Key Results
- Models showed consistently low agreement with human raters (QWK near zero, non-significant)
- Within-model reliability was weak (median Kendall's W < 0.30)
- Models exhibited systematic scoring biases, inflating Coherence scores and struggling with context-sensitive criteria like Pertinence and Feasibility
- Inter-model agreement was moderate for Coherence and Originality but negligible for Pertinence and Feasibility

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs rely on surface-level linguistic markers rather than deep disciplinary reasoning when evaluating essays
- **Mechanism:** Models tokenize text and identify explicit connective patterns (e.g., transition words) to proxy "Coherence," whereas "Feasibility" requires simulating real-world constraints unavailable in the immediate context window
- **Core assumption:** High scores for Coherence are driven by the presence of structural formatting and connectors rather than logical validity
- **Evidence anchors:** Abstract notes tendency to inflate Coherence and inconsistent handling of context-dependent dimensions; Section 5 discusses how Coherence can be partially inferred from structural markers

### Mechanism 2
- **Claim:** Low within-model reliability suggests zero-shot LLM evaluation is a generative process, not a deterministic scoring function
- **Mechanism:** Even with low temperature (0.25), probabilistic selection of attention heads during inference leads to different interpretations of the rubric on each run
- **Core assumption:** The system prompt defines a persona but does not freeze the model's internal weights to a fixed evaluation policy
- **Evidence anchors:** Section 4.3 reports Kendall's W coefficients generally low (median W < .30) and not statistically significant; fixed parameters failed to ensure consistency

### Mechanism 3
- **Claim:** Models converge on "Originality" but diverge on "Pertinence" because they share a linguistic definition of novelty but lack specific disciplinary context
- **Mechanism:** "Originality" maps to distinct lexical variation patterns common in general training data, while "Pertinence" requires mapping content to specific course objectives unavailable in pre-training
- **Core assumption:** Inter-model agreement implies shared training data distributions or architectural biases regarding specific concepts
- **Evidence anchors:** Abstract reports moderate convergence for Coherence and Originality but negligible concordance for Pertinence and Feasibility; Discussion notes models align in evaluating structural aspects but diverge with context-sensitive dimensions

## Foundational Learning

- **Concept: Quadratic Weighted Kappa (QWK)**
  - **Why needed here:** The study uses QWK to measure human-LLM agreement, accounting for agreement between ordinal categories weighted by distance between them
  - **Quick check question:** If Model A scores an essay 10 and Model B scores it 9, while the Human scores it 5, does QWK penalize the gap between 9 and 5 more than 9 and 10?

- **Concept: Zero-Shot Prompting**
  - **Why needed here:** LLMs were not trained on specific university grading data, relying entirely on the system prompt to understand the task
  - **Quick check question:** Without providing examples of graded essays in the prompt, what information must the model rely on to determine if a score is "high" or "low"?

- **Concept: Temperature Parameter**
  - **Why needed here:** The study set temperature to 0.25 to reduce randomness, but results showed even this failed to ensure consistency
  - **Quick check question:** Does setting temperature to 0 guarantee the exact same output for every run of a complex prompt, or does it merely make the output more deterministic?

## Architecture Onboarding

- **Component map:** Input (Essay + System Prompt) -> LLM Inference Engine (Claude/GPT/Gemini via OpenRouter API) -> Generation Config (Temp=0.25, Max Tokens=3050, JSON mode) -> Output (Structured JSON) -> Validation (Parser + Statistical Analysis in R)

- **Critical path:**
  1. Prompt Engineering: The prompt must explicitly define JSON schema and scoring scales to prevent malformed outputs
  2. Replication Loop: Each essay must run 3 times per model to measure variance
  3. Parsing Logic: Must handle "unstructured, malformed, or incomplete outputs" without crashing the evaluation pipeline

- **Design tradeoffs:**
  - Zero-Shot vs. Fine-Tuning: Chose Zero-Shot for ease/speed, costing reliability; fine-tuning would improve reliability but requires labeled dataset
  - JSON Enforcement: Helps automated analysis but may constrain reasoning capacity or lead to syntactically correct but semantically empty justifications

- **Failure signatures:**
  - Score Inflation: Claude/Gemini cluster near 29/30, indicating prompt fails to enforce critical stance
  - Negligible Agreement: QWK near 0 means model is essentially guessing relative to human rubric
  - Parsing Errors: "Missing values" (4.5%) suggest JSON enforcement prompt occasionally ignored or token limits hit

- **First 3 experiments:**
  1. Few-Shot Calibration: Add 3-5 human-graded essays to prompt to see if QWK improves from ~0.00 to >0.40
  2. Chain-of-Thought Enforcement: Modify prompt to require justification text before score generation to force reasoning prior to valuation
  3. Ensemble Averaging: Average scores of all 3 runs (or 3 models) to see if "wisdom of crowds" reduces error metrics closer to human levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do variations in prompt formulation and exemplar conditioning (few-shot) influence the reliability and validity of LLM-based automated essay scoring?
- Basis in paper: [explicit] Discussion states systematic investigation of prompt design, exemplar conditioning, and rubric structure is necessary
- Why unresolved: Study utilized single structured zero-shot prompt without testing different prompting strategies
- What evidence would resolve it: Comparative study manipulating prompt types (zero-shot vs. few-shot) across same essay set, measuring changes in QWK

### Open Question 2
- Question: Do LLMs demonstrate higher agreement with human raters in academic disciplines that rely on more objective criteria or in languages other than Italian?
- Basis in paper: [explicit] Authors note study limited by focus on single discipline (psychology) and language (Italian)
- Why unresolved: Unclear if low agreement is specific to interpretive nature of psychology rubric or Italian language processing capabilities
- What evidence would resolve it: Replication using STEM essays or English-language corpora to compare agreement metrics

### Open Question 3
- Question: Can specific calibration techniques improve LLM performance on context-dependent criteria like Pertinence and Feasibility?
- Basis in paper: [inferred] Results showed negligible inter-model agreement for Pertinence and Feasibility, suggesting models lack disciplinary insight to judge practical applicability
- Why unresolved: Paper identifies failure to assess these dimensions but does not test if domain-specific context or fine-tuning could correct this bias
- What evidence would resolve it: Intervention study providing models with background domain knowledge before scoring, followed by comparison of Kendall's W for these criteria

## Limitations

- Reliance on zero-shot prompting without calibration examples fundamentally constrains LLM reliability
- Use of Italian-language essays limits generalizability to other languages and educational contexts
- Absence of inter-rater reliability calculations for human scores introduces uncertainty about ground truth quality

## Confidence

- **High Confidence:** Statistical finding of low human-LLM agreement (QWK near zero) and poor within-model reliability (Kendall's W < 0.30) is robust given replication design and clear statistical reporting
- **Medium Confidence:** Interpretation that models inflate Coherence scores due to reliance on structural markers rather than content quality is plausible but requires additional qualitative analysis
- **Low Confidence:** Claims about systematic biases being inherent to LLM architecture rather than prompt design require further testing with alternative prompt structures

## Next Checks

1. **Few-Shot Calibration Test:** Add 3-5 human-graded essays to the system prompt and re-run the full analysis to determine if agreement metrics improve significantly
2. **Cross-Language Replication:** Conduct the same study with English-language essays to assess whether observed patterns are language-dependent or architecture-bound
3. **Prompt Structure Manipulation:** Test a chain-of-thought variant requiring explicit reasoning steps before score generation to evaluate if the "generative process" mechanism can be mitigated