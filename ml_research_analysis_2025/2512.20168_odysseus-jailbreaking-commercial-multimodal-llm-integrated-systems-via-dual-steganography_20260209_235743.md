---
ver: rpa2
title: 'Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual
  Steganography'
arxiv_id: '2512.20168'
source_url: https://arxiv.org/abs/2512.20168
tags:
- image
- systems
- jailbreak
- content
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reveals a critical vulnerability in commercial multimodal
  large language model (MLLM)-integrated systems that rely on safety filters to detect
  and block harmful content. These systems assume malicious content is explicitly
  visible in inputs or outputs, an assumption that breaks down in MLLM-integrated
  systems due to their ability to process multiple modalities.
---

# Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography

## Quick Facts
- arXiv ID: 2512.20168
- Source URL: https://arxiv.org/abs/2512.20168
- Reference count: 40
- Commercial MLLM-integrated systems vulnerable to dual steganography jailbreak attacks

## Executive Summary
This paper reveals a critical vulnerability in commercial multimodal large language model (MLLM)-integrated systems that rely on safety filters to detect and block harmful content. These systems assume malicious content is explicitly visible in inputs or outputs, an assumption that breaks down in MLLM-integrated systems due to their ability to process multiple modalities. To exploit this, the authors propose Odysseus, a novel jailbreak paradigm using dual steganography to covertly embed malicious queries and responses into benign-looking images.

## Method Summary
The Odysseus attack consists of four stages: malicious query encoding, steganography embedding, model interaction via function calling, and response extraction. The method encodes malicious queries as binary matrices with Hamming error correction, embeds them into cover images using a GAN-based steganography model, leverages MLLM function-calling capabilities to automatically decode and process the hidden content, and extracts harmful responses embedded in output images. The attack achieves high success rates against four commercial systems while maintaining visual stealthiness.

## Key Results
- Achieves up to 99% attack success rate across commercial systems
- Maintains high stealthiness (SSIM up to 0.8361, PSNR up to 68.6 dB)
- Robust against common defenses including input filtering, steganalysis, and image transformations
- Outperforms existing baselines in both attack success and stealthiness

## Why This Works (Mechanism)

### Mechanism 1: Exploiting the Explicit-Content Assumption in Safety Filters
Current commercial MLLM-integrated systems' safety filters rely on the assumption that malicious content must be explicitly visible in input or output. Odysseus bypasses this by using dual steganography to covertly embed both malicious queries and harmful responses into benign-looking images, meaning harmful content is never present in plain text or obvious visual forms that filters are designed to detect. This would degrade if safety filters are augmented to perform exhaustive decoding of common encoding schemes or integrate advanced cross-modal steganalysis.

### Mechanism 2: Robust Information Hiding via Neural Steganography
The attack uses a GAN-based steganographic model that can reliably encode binary data into images with high extraction accuracy even under common image transformations. The model optimizes for reconstruction accuracy, resistance to noise (simulated transformations), and visual stealthiness through adversarial training. This mechanism could fail if target systems apply novel, aggressive image transformations not seen during training or highly effective adaptive steganalysis.

### Mechanism 3: Leveraging Function Calling for Automated Execution
Function calling in modern MLLM-integrated systems creates an unmonitored execution path that can be co-opted to perform decoding, inference, and re-encoding steps. The attacker provides benign-seeming images with instructions directing the MLLM to call attacker-defined extract and hide functions. This attack vector would be mitigated if systems sandbox or prohibit user-defined function execution, perform real-time semantic analysis on all data passed to/from function calls, or require strict whitelisting of callable functions.

## Foundational Learning

**Concept: Steganography & Neural Steganographic Models**
- Why needed: Understanding how information is covertly embedded into images is essential to grasping how the malicious payload evades detection.
- Quick check: How does a neural steganography model's use of a discriminator during training contribute to the attack's stealthiness?

**Concept: MLLM Safety Filters (Input & Output)**
- Why needed: The attack is fundamentally designed to exploit the limitations of these filters. Knowing what they typically check for clarifies the vulnerability.
- Quick check: What is the key assumption the authors claim current safety filters make, which Odysseus exploits?

**Concept: Function Calling / Tool Use in LLM Agents**
- Why needed: This is the mechanism that allows the attack to execute automatically within the target system.
- Quick check: In the Odysseus pipeline, what are the two key functions the model is instructed to call, and what is the purpose of each?

## Architecture Onboarding

**Component Map:** Attacker's Offline Stage (Query Encoder -> Steganography Encoder) -> Target MLLM-Integrated System (Input Filters -> MLLM with Function Calling -> Output Filters) -> Attacker's Local Decoder

**Critical Path:** The attack succeeds if: (1) encoded image passes Input Filters -> (2) MLLM successfully calls extract function to decode query -> (3) MLLM generates harmful response and calls hide function to embed it -> (4) resulting image passes Output Filters -> (5) attacker successfully decodes image locally

**Design Tradeoffs:** Adding more redundancy improves robustness against transformations but may increase perceptible image changes or payload size. Using complex, multi-step function calls makes the attack easy to automate but creates a longer behavioral signature that could be audited.

**Failure Signatures:**
- Input/Output Block: Safety filters return refusal message or flag content
- Function Call Failure: Model fails to invoke specified extract or hide functions, or invokes them incorrectly
- Decoding Failure: Local steganographic decoder cannot recover binary matrix, or recovered matrix fails error correction

**First 3 Experiments:**
1. Reproduce steganography robustness: Train HiDDeN-inspired steganography model and test extraction accuracy against suite of image transformations
2. Evaluate filter evasion (simulated): Test encoded images with malicious and benign payloads against proxy safety filter to verify evasion
3. End-to-end prototype attack: Implement full Odysseus pipeline against target model using single harmful query from SafeBench

## Open Questions the Paper Calls Out

**Open Question 1:** How can future defenses effectively account for implicit, cross-modal threats and adopt comprehensive detection mechanisms beyond standard safety filters?
- Basis: Authors conclude that "future defenses need to account for implicit, cross-modal threats and to adopt more comprehensive detection mechanisms beyond safety filters alone."
- Why unresolved: Current safety filters operate on assumption that malicious content is explicitly visible, which is violated by dual steganography
- What evidence would resolve: Development and validation of new defense architectures capable of detecting hidden semantics within non-textual modalities

**Open Question 2:** Can the robustness of the Odysseus attack be further improved by incorporating advanced redundancy mechanisms or error-correction coding beyond Hamming codes?
- Basis: In Appendix F2, authors note "attack could be further strengthened by incorporating more advanced redundancy mechanisms or error-correction coding"
- Why unresolved: Character-level extraction accuracy degrades significantly under combined transformations (dropping to ~40% after three transformations)
- What evidence would resolve: Comparative study measuring extraction accuracy and ASR when utilizing more robust codes against combined transformation defenses

**Open Question 3:** Is it feasible to develop lightweight, format-agnostic auditing mechanisms to inspect user-defined function calls for malicious payloads?
- Basis: Appendix G states developing such mechanisms is "an important direction for future research"
- Why unresolved: Current auditing is hindered by heterogeneity of function outputs and computational overhead required
- What evidence would resolve: System design and performance benchmark demonstrating monitoring tool that can parse diverse data formats and detect steganographic content in real-time

## Limitations
- Exact steganography model architecture details are not fully specified, making exact replication challenging
- Results are based on four specific commercial systems; different implementations may vary in resistance
- Effectiveness for Gemini and Grok systems requires prompt adaptation beyond provided GPT-style prompts

## Confidence
- High Confidence: Core mechanism of dual steganography for covert communication and success in bypassing explicit-content safety filters
- Medium Confidence: Robustness of steganography model against various transformations (internal validation, needs independent testing)
- Medium Confidence: Function-calling exploitation mechanism (conceptually sound but needs cross-system validation)

## Next Checks
1. Train the proposed steganography model on an independent dataset and systematically test extraction accuracy against comprehensive suite of image transformations
2. Test complete Odysseus pipeline against additional commercial MLLM-integrated systems not evaluated in original study
3. Implement and evaluate encoded images against multiple independent safety filter APIs to verify consistent evasion across different filtering mechanisms