---
ver: rpa2
title: Enhancing DR Classification with Swin Transformer and Shifted Window Attention
arxiv_id: '2504.15317'
source_url: https://arxiv.org/abs/2504.15317
tags:
- image
- classification
- retinopathy
- swin
- shifted
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a Swin Transformer-based approach for multi-class
  diabetic retinopathy classification, addressing challenges of class imbalance, pixel-level
  similarities, and subtle retinal variations. The proposed method incorporates image
  cropping, Contrast-Limited Adaptive Histogram Equalization (CLAHE), and targeted
  data augmentation to improve model robustness.
---

# Enhancing DR Classification with Swin Transformer and Shifted Window Attention

## Quick Facts
- arXiv ID: 2504.15317
- Source URL: https://arxiv.org/abs/2504.15317
- Authors: Meher Boulaabi; Takwa Ben Aïcha Gader; Afef Kacem Echi; Zied Bouraoui
- Reference count: 4
- Primary result: Achieves 89.65% accuracy on Aptos and 97.40% on IDRiD datasets for 5-class DR classification

## Executive Summary
This study presents a Swin Transformer-based approach for multi-class diabetic retinopathy classification, addressing challenges of class imbalance, pixel-level similarities, and subtle retinal variations. The proposed method incorporates image cropping, Contrast-Limited Adaptive Histogram Equalization (CLAHE), and targeted data augmentation to improve model robustness. By leveraging hierarchical token processing and shifted window attention, the model efficiently captures fine-grained features while maintaining linear computational complexity. Validated on Aptos and IDRiD datasets, the approach achieves accuracy rates of 89.65% and 97.40%, respectively, demonstrating strong performance particularly in detecting early-stage DR.

## Method Summary
The method employs a Swin Transformer architecture for 5-class DR classification (grades 0-4) using APTOS and IDRiD datasets. Images undergo preprocessing including cropping to remove background, CLAHE for contrast enhancement, and data augmentation (rotation up to 360°, horizontal flips). The Swin Transformer uses 16×16 patch size, learning rate 1e-3, batch size 32, 200 epochs, AdamW optimizer, and Cross-Entropy loss with early stopping (patience=15). The model leverages hierarchical token processing with PatchMerging and shifted window attention to capture multi-scale features while maintaining computational efficiency.

## Key Results
- Achieves 89.65% accuracy on Aptos dataset and 97.40% on IDRiD dataset
- Grade 4 recall improves from 0.08 to 1.00 with preprocessing
- High specificity and sensitivity across most classes, particularly for severe DR (Grade 4)
- Model demonstrates suitability for real-world clinical applications in automated retinal screening

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shifted window attention enables cross-window information exchange while maintaining linear computational complexity.
- Mechanism: Consecutive transformer blocks alternate between regular window partitioning (W-MSA) and shifted window partitioning (SW-MSA). The shift displaces windows by half their size, allowing tokens from different regions to interact. Cyclic shifting with padding handles boundary windows.
- Core assumption: Local attention within windows is sufficient when periodically bridged across blocks; fine-grained DR features span multiple windows.
- Evidence anchors:
  - [abstract]: "leveraging hierarchical token processing and shifted window attention to efficiently capture fine-grained features while maintaining linear computational complexity"
  - [section 2]: "In consecutive blocks, the first module applies regular window partitioning, while the second module shifts the windows, allowing for improved feature continuity and better spatial representation"
  - [corpus]: SwinECAT applies shifted window attention to fundus disease classification with reported gains (fmr_score: 0.425)
- Break condition: If window size is too small relative to lesion scale, cross-window dependencies may not propagate effectively through depth.

### Mechanism 2
- Claim: CLAHE preprocessing improves model sensitivity to subtle retinal variations by enhancing local contrast.
- Mechanism: CLAHE adaptively redistributes intensity values within localized regions, clipping and redistributing histogram peaks. This amplifies low-contrast microaneurysms and hemorrhages against background retinal tissue.
- Core assumption: Diagnostically relevant structures exist in the images but are suppressed by non-uniform illumination or low contrast.
- Evidence anchors:
  - [abstract]: "CLAHE is used to improve local contrast and highlight subtle structures, aiding feature extraction"
  - [table 1]: Grade 4 recall jumps from 0.08 (without preprocessing) to 1.00 (with preprocessing); Grade 1 F1 improves from 0.00 to 1.00
  - [corpus]: Limited direct evidence—neighboring papers do not explicitly isolate CLAHE contribution in Swin-based architectures
- Break condition: Excessive contrast amplification may introduce artifacts or overfit to illumination patterns in training data.

### Mechanism 3
- Claim: Hierarchical patch merging enables multi-scale feature representation for varying DR lesion sizes.
- Mechanism: The architecture progressively merges adjacent patches (PatchMerging), reducing spatial dimensions while increasing channel depth. Early stages capture fine details (microaneurysms); later stages encode broader patterns (neovascularization).
- Core assumption: DR severity indicators manifest at multiple spatial scales requiring hierarchical aggregation.
- Evidence anchors:
  - [abstract]: "hierarchical token processing... efficiently captures fine-grained features"
  - [section 2 + figure 1]: PatchMerging reduces spatial dimensions before final classification; residual connections preserve gradient flow
  - [corpus]: MV-Swin-T and Iwin Transformer both leverage hierarchical processing for multi-scale feature extraction across domains
- Break condition: If merging is too aggressive, small lesion features may be lost before semantic aggregation completes.

## Foundational Learning

- **Self-Attention Mechanisms**
  - Why needed here: Swin's attention computes weighted relationships between patches; understanding Q/K/V interactions is prerequisite to diagnosing attention failures.
  - Quick check question: Given a 4×4 patch grid with window size 2, how many attention operations occur in W-MSA vs. global self-attention?

- **Layer Normalization and Residual Connections**
  - Why needed here: Each Swin block applies LayerNorm before attention and MLP, with residual additions—critical for training stability in deep hierarchies.
  - Quick check question: What happens to gradient magnitude in a 12-layer Swin without residual connections?

- **Computational Complexity of Attention**
  - Why needed here: The paper explicitly contrasts Ω(MSA) = 4hwC² + 2(hw)²C against Ω(W-MSA) = 4hwC² + 2M²hwC.
  - Quick check question: For a 512×512 image with 16×16 patches and window size 7, calculate the flops ratio between global and window attention.

## Architecture Onboarding

- **Component map**:
  Input → Crop → CLAHE → Augmentation → PatchExtract (16×16) → PatchEmbedding → [SwinBlock×N: (W-MSA → LN → MLP) → (SW-MSA → LN → MLP)] → PatchMerging → GlobalAvgPool → Dense(softmax, 5 classes)

- **Critical path**:
  1. Preprocessing quality directly controls feature visibility (Table 1 shows near-zero recall without it for grades 1, 4)
  2. Patch size 16×16 determines token granularity—too large loses microaneurysms
  3. Shift magnitude (window_size // 2) governs cross-window connectivity

- **Design tradeoffs**:
  - Smaller windows → faster computation but limited receptive field growth per block
  - Aggressive augmentation → improved generalization but risk of unrealistic artifacts
  - Deeper hierarchies → richer semantics but increased vanishing gradient risk (mitigated by residuals)

- **Failure signatures**:
  - Grade 1 recall collapses (0.68 on Aptos) → model struggles with mild NPDR's subtle features; may need stronger contrast enhancement or targeted oversampling
  - High training accuracy, low validation accuracy → overfitting to dataset-specific illumination patterns
  - Disproportionate per-class performance → class imbalance not adequately addressed

- **First 3 experiments**:
  1. **Ablate preprocessing**: Train with crop-only, CLAHE-only, and full pipeline to isolate each component's contribution to per-class recall.
  2. **Window size sweep**: Test window sizes [4, 7, 8, 12] on a held-out validation set; monitor Grade 1-2 F1-scores to find optimal scale for early DR features.
  3. **Patch size analysis**: Compare 8×8 vs. 16×16 patches; measure impact on small lesion detection (microaneurysms) vs. training time.

## Open Questions the Paper Calls Out
- **Open Question 1**: How well does the model generalize to external datasets featuring different demographic profiles and fundus camera hardware?
- **Open Question 2**: Can the model's sensitivity for early-stage and moderate DR (Grades 1 and 3) be improved without sacrificing the high accuracy observed for other classes?
- **Open Question 3**: Do the learned attention maps from the Swin Transformer correspond to clinically relevant retinal lesions?

## Limitations
- Does not specify critical implementation details including train/validation/test split ratios and CLAHE hyperparameters
- Ablation studies lack isolation of shifted window attention contribution versus other architectural choices
- Class imbalance mitigation strategies beyond standard augmentation are not detailed
- Reported metrics may not fully reflect real-world clinical deployment challenges

## Confidence
- **High Confidence**: The foundational mechanism of shifted window attention for cross-window information exchange and its linear computational complexity advantage over global attention.
- **Medium Confidence**: The effectiveness of CLAHE preprocessing for enhancing subtle retinal features, supported by dramatic improvements in Grade 4 recall (0.08→1.00) but lacking direct comparison to alternative contrast enhancement methods.
- **Medium Confidence**: The hierarchical patch merging approach for multi-scale feature representation, with clear architectural description but limited ablation evidence for window size optimization.

## Next Checks
1. **Ablation of Preprocessing Components**: Systematically evaluate the individual and combined contributions of cropping, CLAHE, and augmentation by training separate models with each component removed, measuring impact on per-class recall metrics.
2. **Window Size Sensitivity Analysis**: Conduct a controlled experiment varying window sizes [4, 7, 8, 12] on a held-out validation set, specifically monitoring Grade 1-2 F1-scores to identify the optimal scale for early DR feature detection.
3. **Input Resolution Impact Study**: Test the model's performance across different input resolutions (256×256, 384×384, 512×512) to determine if higher resolution improves detection of small lesions like microaneurysms without excessive computational cost.