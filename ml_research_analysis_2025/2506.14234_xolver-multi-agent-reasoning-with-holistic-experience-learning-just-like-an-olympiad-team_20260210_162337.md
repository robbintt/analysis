---
ver: rpa2
title: 'Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just Like
  an Olympiad Team'
arxiv_id: '2506.14234'
source_url: https://arxiv.org/abs/2506.14234
tags:
- xolver
- reasoning
- problem
- number
- cases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Xolver introduces a training-free, multi-agent reasoning framework
  that mimics expert problem-solving teams by integrating episodic retrieval, dynamic
  shared memory, tool usage, and iterative refinement. It enables a black-box LLM
  to accumulate and apply experiential knowledge during inference, moving beyond isolated
  problem-solving.
---

# Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just Like an Olympiad Team

## Quick Facts
- arXiv ID: 2506.14234
- Source URL: https://arxiv.org/abs/2506.14234
- Reference count: 40
- Primary result: Training-free multi-agent framework that integrates retrieval, collaboration, and iterative refinement to achieve expert-level performance across math and coding benchmarks

## Executive Summary
Xolver introduces a training-free, multi-agent reasoning framework that mimics expert problem-solving teams by integrating episodic retrieval, dynamic shared memory, tool usage, and iterative refinement. It enables a black-box LLM to accumulate and apply experiential knowledge during inference, moving beyond isolated problem-solving. Across diverse math and programming benchmarks, Xolver consistently outperforms specialized agents and even state-of-the-art models like o3 and o4-mini-high, achieving up to 98.1% on GSM8K and 94.4% on AIME'24. The framework demonstrates robustness to backbone model strength, data shuffling, and iteration strategies, with performance gains scaling with agent count and refinement iterations.

## Method Summary
Xolver is a training-free multi-agent framework that orchestrates specialized agents (planner, dynamic agents, judge, verifier) with dual memory architecture (episodic memory DE for cross-problem retrieval and intermediate shared memory DS for within-problem refinement). The system iteratively generates solutions, evaluates them via judge-mediated feedback, updates shared memory with top-ranked reasoning traces, and refines until convergence. It integrates external retrieval (BM25), tool invocation (Python execution), multi-agent collaboration, and iterative refinement within a single inference loop, enabling experience accumulation and transfer without model fine-tuning.

## Key Results
- Achieves 98.1% accuracy on GSM8K and 94.4% on AIME'24, outperforming specialized agents and state-of-the-art models
- Outperforms both specialized agents and general-purpose models on LiveCodeBench v5 and multiple math benchmarks
- Performance gains scale with agent count and iteration count, with ablation showing 16.4-23.7% drops when removing multi-agent or iterative components
- Robust to backbone model strength, data shuffling, and iteration strategies

## Why This Works (Mechanism)

### Mechanism 1
Integrating multiple experience modalities (retrieval, tools, collaboration, refinement) in a unified framework yields better performance than isolated approaches. Xolver combines episodic retrieval, dynamic shared memory, tool invocation, multi-agent collaboration, judge-driven evaluation, and iterative refinement into a single inference loop. Rather than treating these as separate enhancements, they reinforce each other: retrieval provides exemplars, agents specialize and collaborate, tools handle precise computation, and shared memory accumulates successful reasoning paths across iterations.

### Mechanism 2
A dual-memory architecture enables both cross-problem knowledge transfer and within-problem iterative improvement. Episodic memory (DE) stores past problems, solutions, and traces for retrieval. Intermediate shared memory (DS) maintains the top-m highest-quality reasoning-response-feedback tuples from the current problem's iterations. In the first iteration, agents bootstrap from retrieved exemplars; in subsequent iterations, they build on the accumulated contents of DS, enabling progressive refinement without starting from scratch.

### Mechanism 3
External judge-mediated evaluation with structured feedback drives more reliable convergence than self-evaluation. After each agent generates a response, a dedicated judge agent produces a feedback tuple (natural language critique + scalar score). The score determines ranking in shared memory; the critique guides subsequent refinement. This decouples evaluation from generation, reducing self-bias.

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**: Xolver relies on episodic retrieval to provide exemplars for bootstrapping agent reasoning; understanding similarity search (BM25, embedding-based) and retrieval-corpus construction is essential. Quick check: Given a problem query, can you explain how BM25 retrieves relevant exemplars from a corpus, and what tradeoffs exist between lexical and semantic retrieval?

- **Multi-Agent Orchestration**: Xolver dynamically constructs a team of specialized agents (planner, dynamic agents, judge, verifier) and coordinates their interactions across iterations. Quick check: Can you describe how role assignment, message passing, and convergence detection work in a multi-agent LLM system?

- **Iterative Refinement with Feedback**: The core loop generates solutions, evaluates them, updates shared memory, and continues until convergence or max iterations; understanding how feedback drives improvement is critical. Quick check: How would you design a feedback schema that provides both a scalar score for ranking and natural language critique for targeted correction?

## Architecture Onboarding

- **Component map**: Planner Agent → Dynamic Agents → Judge Agent → Intermediate Shared Memory (DS) → Verifier Agent; Episodic Memory (DE) provides retrieval exemplars to Dynamic Agents
- **Critical path**: 1. Planner initializes agent team A. 2. First iteration: Agents receive query + retrieved exemplars from DE → generate responses. 3. Judge evaluates each response → assigns scores + critiques. 4. DS updated with top-m tuples by score. 5. Subsequent iterations: Agents receive query + prior history + DS contents → refine. 6. Repeat until convergence or max iterations reached. 7. Verifier extracts final answer from top-ranked DS entry. 8. DE optionally updated with new problem-solution pair.
- **Design tradeoffs**: Agent count (m) vs. token costs (O(mI) complexity); iteration limit (I) vs. latency; external vs. self-retrieval performance vs. corpus maintenance; DS size vs. context length.
- **Failure signatures**: Stagnant DS scores (retrieval quality issue), high agreement but low accuracy (judge underpowered), excessive token usage (convergence logic issues), format extraction failures (inconsistent output formatting).
- **First 3 experiments**: 1. Reproduce baseline comparison on GSM8K and LiveCodeBench with QWQ-32B backbone. 2. Ablate judge vs. self-judging on math and code tasks. 3. Test retrieval strategies (external, self, none) on AIME'24 subset.

## Open Questions the Paper Calls Out

### Open Question 1
Can self-judging mechanisms be enhanced to eliminate the performance gap with external judge agents in multi-agent reasoning systems? The paper reports that self-judging causes "a 9.9% decrease in coding tasks and a 3.88% decrease in math tasks" due to self-bias where agents "occasionally validating incorrect solutions." The ablation only quantifies the gap but does not explore whether debiasing techniques or calibration methods could close it.

### Open Question 2
What is the minimal agent-iteration configuration that preserves Xolver's performance gains while reducing token overhead? The authors state Xolver "faces limitations in computational efficiency, with substantially higher token consumption than traditional approaches" and aim to "optimize agent interactions to reduce resource requirements." The paper shows performance scales with agents and iterations but does not identify a Pareto-optimal frontier balancing cost vs. accuracy.

### Open Question 3
Can tool invocation be tightly coupled with reasoning to prevent avoidable calculation errors when agents skip available Python execution? Error analysis notes "calculation errors often occur when agents choose not to invoke these tools—highlighting that tool usage remains decoupled from the model's core reasoning process." The paper does not propose or test mechanisms to enforce or encourage tool use during intermediate reasoning steps.

## Limitations
- Model access constraints: o3-mini models are not publicly documented or accessible through standard APIs, creating significant reproducibility barriers
- Judge agent evaluation reliability: Performance depends on LLM-based test simulation and probability estimation, which may not match human evaluation accuracy
- External corpus dependency: 3.5-point performance gain from external retrieval assumes access to curated datasets like OpenMathReason

## Confidence
- **High confidence**: The integrated multi-agent framework with shared memory architecture demonstrably improves performance over isolated approaches, as evidenced by consistent ablation results
- **Medium confidence**: Claims about judge-mediated evaluation superiority are well-supported by ablation data (9.9% and 3.88% drops), but the exact mechanism and judge prompt quality remain partially opaque
- **Medium confidence**: Performance superiority over specialized agents and state-of-the-art models is demonstrated across multiple benchmarks, though some results may depend on specific model access and configuration details

## Next Checks
1. **Reproduce baseline ablation study**: Implement self-judging mode where each dynamic agent evaluates its own response instead of using the dedicated judge agent. Measure the exact performance drop on both math (Math-500 subset) and coding (LiveCodeBench v5 subset) tasks to verify the 9.9% and 3.88% degradation claims.

2. **Validate retrieval strategy comparison**: Implement and test all three retrieval modes (external BM25 on OpenMathReason, self-retrieval via LLM, no retrieval) on a consistent subset of AIME'24 problems. Measure and compare performance differences to quantify the 3.5-point external retrieval advantage and assess self-retrieval viability.

3. **Scale agent count experiment**: Systematically vary the number of dynamic agents (m=1, 2, 3, 4, 5) while keeping iterations fixed at I=2, running on a consistent benchmark subset (e.g., GSM8K 100 problems). Plot performance vs. agent count and token usage to verify the claimed scalability benefits and quantify the computational tradeoff curve.