---
ver: rpa2
title: 'Toward Trusted Onboard AI: Advancing Small Satellite Operations using Reinforcement
  Learning'
arxiv_id: '2507.22198'
source_url: https://arxiv.org/abs/2507.22198
tags:
- satellite
- agent
- onboard
- control
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research developed a Reinforcement Learning (RL) algorithm
  for autonomous command automation onboard a 3U CubeSat, focusing on macro Control
  Action Reinforcement Learning (CARL). The RL agent was trained using Proximal Policy
  Optimization (PPO) in a high-fidelity digital twin simulation, learning to issue
  high-level actions like adjusting attitude for solar pointing based on live telemetry.
---

# Toward Trusted Onboard AI: Advancing Small Satellite Operations using Reinforcement Learning

## Quick Facts
- arXiv ID: 2507.22198
- Source URL: https://arxiv.org/abs/2507.22198
- Reference count: 36
- Primary result: RL agent for autonomous satellite command automation successfully integrated onboard 3U CubeSat; full deployment pending.

## Executive Summary
This work develops a Reinforcement Learning (RL) algorithm for autonomous command automation onboard a 3U CubeSat, using macro Control Action Reinforcement Learning (CARL). The RL agent is trained via Proximal Policy Optimization (PPO) in a high-fidelity digital twin simulation, learning to issue high-level actions (e.g., adjust attitude for solar pointing) based on live telemetry. While full deployment remains future work, development and integration phases demonstrated successful containerization and interface with onboard systems, though initial inference outputs showed inconsistent behavior in extreme scenarios. Improved training with increased simulation difficulty yielded more responsive agent decisions. The work establishes a phased framework for building trust in onboard AI, progressing from simulation to safe inference on orbit.

## Method Summary
The method employs macro Control Action Reinforcement Learning (CARL) where an RL agent outputs discrete high-level commands (Drift, Charge, Desaturate) that trigger existing Flight Software (FSW) routines for execution. Training uses PPO via Ray in a Basilisk-based digital twin simulation with a 18-dimensional observation space (DCM, MRP, angular velocity, position, velocity, battery fraction, wheel speed fraction). The trained policy is containerized in Docker for onboard inference. Deployment follows a phased approach: simulation training, digital twin validation, shadow mode (inference without command authority), and eventual autonomous execution.

## Key Results
- RL model successfully containerized and interfaced with onboard systems
- Inference outputs showed inconsistent behavior in extreme scenarios initially
- Improved training with increased simulation difficulty yielded more responsive agent decisions
- Demonstrated phased framework for building trust in onboard AI

## Why This Works (Mechanism)

### Mechanism 1: Macro-Action Abstraction (CARL)
- **Claim:** Restricting the agent to high-level macro actions rather than low-level actuator commands reduces the search space and improves training stability.
- **Mechanism:** The DRL agent outputs a discrete macro command (e.g., "Charge"), triggering existing FSW routines to handle the complex physics of slewing and stabilization. The agent learns what to do, while FSW handles how to do it.
- **Core assumption:** The underlying FSW algorithms are robust enough to execute the macro commands reliably.
- **Break condition:** If FSW fails to execute a macro command (e.g., stuck reaction wheel), the agent cannot correct the lower-level error.

### Mechanism 2: Digital Twin Domain Randomization
- **Claim:** Training the agent in a high-fidelity simulation with increased environmental difficulty forces learning of proactive survival strategies that transfer to orbit.
- **Mechanism:** By reducing battery capacity and reaction wheel speed thresholds in simulation, the agent encounters failure states more frequently, incentivizing earlier reactions to degradation trends.
- **Core assumption:** The simulation fidelity captures the dominant dynamics and constraints of the physical asset sufficiently well.
- **Break condition:** If the simulation model diverges significantly from physical hardware, the proactive strategies may be maladaptive in reality.

### Mechanism 3: Shadow Mode Validation
- **Claim:** Deploying the trained policy onboard in an isolated environment allows validation of model decisions against real-time telemetry without risking the asset.
- **Mechanism:** The policy runs in a containerized environment receiving live telemetry, generating actions that are logged but not executed. Operators compare these recommendations against actual satellite states to build trust.
- **Core assumption:** The data pipeline successfully transforms raw telemetry into the abstracted observation format the agent expects.
- **Break condition:** If the telemetry abstraction layer introduces latency or unit errors, the agent's decisions will be based on a "hallucinated" state.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO)**
  - **Why needed here:** PPO stabilizes learning by preventing the policy from changing too drastically in a single update, crucial for continuous control tasks.
  - **Quick check question:** Why is PPO often preferred over Deep Q-Learning for continuous action spaces or stability-critical systems?

- **Concept: Observation Space Abstraction (MRP & State Vectors)**
  - **Why needed here:** The agent perceives Modified Rodrigues Parameters for attitude and normalized fractions for battery/wheel speed, not raw voltages.
  - **Quick check question:** Why might a developer choose MRPs over Quaternions or Direction Cosine Matrices (DCM) for a neural network input vector?

- **Concept: Containerization (Docker) in Embedded Systems**
  - **Why needed here:** The inference runs in a Docker container on a resource-constrained CubeSat, balancing isolation (safety) against overhead (storage/computational cost).
  - **Quick check question:** What is the primary risk of running a large, unoptimized Docker container on a flight computer with limited storage and RAM?

## Architecture Onboarding

- **Component map:** LIME Satellite Bus (Raw Telemetry) -> Host-to-Container Port/Mounted Directory -> Python Script (Feature extraction) -> PPO Policy (ONNX/PyTorch) -> Logger (Output actions to file)

- **Critical path:** The Telemetry Abstraction Layer. Structural data misalignment and unit inconsistencies are critical failure modes that this phase helps identify.

- **Design tradeoffs:**
  - *Container Size vs. Flexibility:* Multi-stage Docker builds reduced size but traded against ease of debugging/iterating live.
  - *Simulation Difficulty vs. Convergence:* Making simulation harder forces learning but risks agent failing to converge.
  - *Data Fidelity vs. Bandwidth:* Logging full telemetry is ideal but downlink limits forced compression/deletion, reducing visibility into agent decision-making.

- **Failure signatures:**
  - **The "Drift" Loop:** Agent defaults to "Drift" action regardless of state, indicating reward function didn't sufficiently penalize inaction.
  - **Unresponsive Inference:** Agent outputs random actions in extreme scenarios, suggesting training distribution didn't cover state-space tails.
  - **Unit/Column Misalignment:** Policy inputs NaN values or physically impossible vectors, causing model to output garbage or crash.

- **First 3 experiments:**
  1. **Static Telemetry Replay:** Feed container a recorded log of historical housekeeping data. Verify it reads the file, processes it into correct vector shape, and logs an output without crashing.
  2. **Sanity Check Visualization:** Create "Battery vs. Wheel Speed" plot using newly integrated container outputs. Confirm action boundaries match ground-truth model from training.
  3. **Latency Profiling:** Measure "End-to-End" time from telemetry generation to action log write. Ensure it's significantly faster than satellite's control loop frequency (< 1 second).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can reward structures be evolved beyond simple survival-based schemes to induce proactive, mission-relevant behaviors in the agent?
- **Basis in paper:** Authors state future work will address "limitations of the current reward structure" to facilitate "complex and adaptive agent behavior" rather than just survival.
- **Why unresolved:** Current agent often defaults to "Drift" action and remains unresponsive in non-critical states, failing to adapt actions to varying inputs or future needs.
- **Evidence:** Demonstration of an agent that proactively manages resources or attitude in anticipation of future states, rather than reacting only to prevent terminal failure.

### Open Question 2
- **Question:** How can the model's inference policy be modularized to allow for the granular promotion of specific capabilities from recommendation to autonomous status?
- **Basis in paper:** Future Work section notes that "model functionality will need to be split into portions that can be promoted separately" to support the proposed phased deployment framework.
- **Why unresolved:** Current implementation validates the agent as a whole, but the mechanism for certifying individual action types (e.g., "Charge" vs. "Desaturate") independently is not yet developed.
- **Evidence:** A software architecture allowing operators to selectively enable autonomous execution for specific, trusted action classes while keeping others in recommendation-only mode.

### Open Question 3
- **Question:** What are the trade-offs between digital twin fidelity and algorithm generalizability when training agents for diverse satellite missions?
- **Basis in paper:** Paper outlines a "third vector" for future work to "clarify the trade-offs between digital twin fidelity and algorithm generalizability" to establish practical deployment guidelines.
- **Why unresolved:** It remains unclear how closely the training environment must resemble specific operational conditions versus using a more generic agent.
- **Evidence:** Comparative performance metrics of generic versus mission-tailored agents across varying levels of simulation fidelity and real-world telemetry.

## Limitations
- RL agent has not been granted actual command authority; long-term in-orbit performance remains unvalidated
- Telemetry abstraction layer proved fragile with structural data misalignment and unit inconsistencies identified as critical failure modes
- Simulation-to-reality transfer remains untested beyond shadow mode, raising concerns about distributional shift in extreme scenarios

## Confidence

- **High Confidence:** The core CARL mechanism (macro-action abstraction) is theoretically sound and well-documented. PPO training and digital twin validation approaches are standard, established practices.
- **Medium Confidence:** Simulation-based training results are reproducible given described methodology, but specific PPO hyperparameters and neural network architecture are not fully specified. Shadow mode validation provides strong evidence for inference capability, but real command execution remains future work.
- **Low Confidence:** Long-term autonomous performance on orbit cannot be assessed from current results. Identified failure modes (drift loops, unresponsive inference) suggest the agent may not generalize to all operational scenarios.

## Next Checks
1. **Command Authority Transition:** Implement and test transfer from shadow mode to active command execution on LIME satellite, starting with low-risk scenarios.
2. **Distributional Shift Testing:** Create simulation scenarios that systematically push agent into extreme state-space regions to assess robustness.
3. **Ground vs. Onboard Parity Verification:** Deploy container to ground truth hardware identical to LIME and compare action outputs against training simulation under identical telemetry inputs to quantify reality gap.