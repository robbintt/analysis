---
ver: rpa2
title: Maximizing Uncertainty for Federated learning via Bayesian Optimisation-based
  Model Poisoning
arxiv_id: '2501.08002'
source_url: https://arxiv.org/abs/2501.08002
tags:
- attack
- learning
- data
- uncertainty
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Delphi, a novel model poisoning attack method
  for federated learning that maximizes the uncertainty of the global model output.
  The attack leverages the relationship between uncertainty and the model parameters
  of the first hidden layer, employing two optimization techniques - Bayesian Optimisation
  (Delphi-BO) and Least Squares Trust Region (Delphi-LSTR) - to search for optimal
  poisoned model parameters.
---

# Maximizing Uncertainty for Federated learning via Bayesian Optimisation-based Model Poisoning

## Quick Facts
- **arXiv ID:** 2501.08002
- **Source URL:** https://arxiv.org/abs/2501.08002
- **Reference count:** 40
- **Primary result:** Introduces Delphi, a model poisoning attack for federated learning that maximizes global model uncertainty using Bayesian Optimization targeting the first hidden layer.

## Executive Summary
This paper presents Delphi, a novel model poisoning attack designed to maximize uncertainty in federated learning systems. The attack exploits the relationship between model parameters in the first hidden layer and the output uncertainty, using two optimization techniques: Bayesian Optimization (Delphi-BO) and Least Squares Trust Region (Delphi-LSTR). By reparameterizing significant neurons based on gradient L2 norms, the attack efficiently induces high uncertainty in the global model's predictions. The effectiveness is quantified using KL Divergence and proven to have an upper bound dependent on the number of users in the system.

## Method Summary
Delphi works by poisoning local models during federated learning through manipulation of weights in the first hidden layer. The attack identifies significant neurons using L2 gradient norms and optimizes their weights to maximize KL divergence from an uncertain target distribution. Two optimization approaches are employed: Bayesian Optimization with Gaussian Process surrogate modeling and Least Squares Trust Region. The poisoned models are then aggregated via FedAvg to compromise the global model's predictive confidence.

## Key Results
- Delphi-BO induces significantly higher uncertainty than Delphi-LSTR, reducing mean predictive confidence by half.
- The attack effectiveness is upper-bounded and dependent on the number of users in the federated system.
- Bayesian Optimization outperforms Least Squares Trust Region due to the non-convex relationship between weights and uncertainty.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Targeting the first hidden layer is a high-leverage attack surface for inducing global model uncertainty.
- **Mechanism:** The attack identifies significant neurons in the first hidden layer based on L2 norm of gradients, maximizing sensitivity between input disturbance and output uncertainty while avoiding full parameter space manipulation.
- **Core assumption:** L2 norm of gradient serves as valid proxy for neuron significance, and first layer's feature extraction is critical enough to propagate uncertainty effectively.
- **Evidence anchors:**
  - [abstract]: "...taking advantage of the relationship between the uncertainty and the model parameters of the first hidden layer..."
  - [section III-C]: "We have chosen the first hidden layer because it has the higher sensitivity between the input and the model’s output [29]."
- **Break condition:** If defender employs robust aggregation (like Krum) that filters out weights deviating significantly from benign cluster, or if model architecture renders first layer less sensitive.

### Mechanism 2
- **Claim:** Bayesian Optimization outperforms Least Squares Trust Region because the relationship between weights and uncertainty is non-convex.
- **Mechanism:** Delphi-BO treats weight-to-uncertainty mapping as expensive-to-evaluate black-box function, using Gaussian Process surrogate model and Expected Improvement acquisition function.
- **Core assumption:** Function mapping weights to uncertainty is smooth enough to be modeled by Gaussian Process with Matern kernel.
- **Evidence anchors:**
  - [abstract]: "...Delphi-BO induces significantly higher uncertainty than Delphi-LSTR, with the mean predictive confidence reduced by half."
  - [section VI-A]: "Delphi-LSTR is effective when more than one malicious user exists... shows that Delphi-LSTR does not converge towards the optimal parameters..."
- **Break condition:** If dimensionality of search space increases drastically, cubic complexity O(n³) of GP inversion in BO may become computationally prohibitive.

### Mechanism 3
- **Claim:** Attack effectiveness is mathematically bounded by number of users and perturbation size.
- **Mechanism:** Paper derives upper bound for attack effectiveness (ρ) in FedAvg, showing it is proportional to ε²(3N/A + 4), where ε is expected perturbation, N is benign users, and A is malicious users.
- **Core assumption:** Theoretical bound assumes FedAvg aggregation rule and specific conditions on loss/gradient properties.
- **Evidence anchors:**
  - [section V-C]: "Therefore, based on the eq. 33, we find out that there is an upper bound for the attack effectiveness."
  - [abstract]: "The attack's effectiveness is shown to be upper-bounded and dependent on the number of users..."
- **Break condition:** If aggregation rule changes (e.g., to weighted averaging or clipping), derived bound no longer strictly applies.

## Foundational Learning

- **Concept: Federated Averaging (FedAvg)**
  - **Why needed here:** This is the target environment. Understanding how local weights (wₖ) are aggregated into global model (w) is essential to understanding how poisoned weights (wₐ) influence system.
  - **Quick check question:** If 5 clients have accuracy 90% and 1 attacker has accuracy 10%, does FedAvg strictly average the weights or the accuracies?

- **Concept: Bayesian Optimization (BO) & Gaussian Processes (GP)**
  - **Why needed here:** Delphi-BO relies on GP to model uncertainty landscape. Need to understand how surrogate model replaces true expensive function and how acquisition function balances exploration vs. exploitation.
  - **Quick check question:** Why does BO scale cubically (O(n³)) with number of observations?

- **Concept: Kullback-Leibler (KL) Divergence**
  - **Why needed here:** This is the objective function. Attack works by minimizing KL divergence between model's prediction and uncertain target distribution.
  - **Quick check question:** If target distribution Z is uniform (maximum entropy), what happens to KL divergence as model's prediction becomes more confident?

## Architecture Onboarding

- **Component map:**
  1. FL Simulator (Server -> Client)
  2. Delphi Optimizer (LSTR solver OR BO loop)
  3. Uncertainty Quantifier (KL Divergence calculator)
  4. Neuron Selector (L2 gradient norm analyzer)

- **Critical path:**
  1. Global Model Reception: Malicious client receives wₜ
  2. Neuron Selection: Identifies top k neurons in first hidden layer
  3. Optimization Loop:
     - *Delphi-BO:* Sample weights → Evaluate Uncertainty → Update GP → Select next weights via EI
     - *Delphi-LSTR:* Calculate gradient/Hessian → Solve trust region subproblem
  4. Update & Upload: Replace original weights with optimized poisoned weights θₐ and send to server

- **Design tradeoffs:**
  - **Performance vs. Speed:** Delphi-BO yields higher uncertainty (better attack) but has higher time complexity (O(n³)) compared to Delphi-LSTR (O(d³) per step).
  - **Stealth vs. Impact:** "Dynamic" search for neurons maximizes impact (better attack effectiveness) but may create detectable anomalies in early training rounds vs. "Fixed" scheme.
  - **Attack Scope:** Manipulating only first layer is computationally efficient but may be less potent than full-model attack against robust defenses like Krum.

- **Failure signatures:**
  - **Convergence Stall:** Delphi-LSTR fails to lower objective function, causing attack to flatline.
  - **Early Detection:** Attack effectiveness exceeds theoretical upper bound significantly in early rounds, triggering defensive thresholds.
  - **Benign Overwriting:** In Imbalanced data scenarios, fixed neuron attack is less effective, suggesting global model is washing out poisoned signal.

- **First 3 experiments:**
  1. **Sanity Check (CIFAR10):** Replicate Delphi-BO vs. Delphi-LSTR comparison on simple CNN (AlexNet) with FedAvg. Verify BO reduces predictive confidence by ~50%.
  2. **Neuron Sensitivity:** Run ablation study varying number of manipulated neurons (5 vs. 10) to confirm "sensitivity" hypothesis.
  3. **Defense Evaluation:** Test Delphi-BO against Krum aggregation rule to verify claim that attack "penetrates" Krum but with reduced confidence impact vs. FedAvg.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the mathematical proof of attack effectiveness be generalized to aggregation functions beyond FedAvg?
- **Basis in paper:** [explicit] The conclusion states authors will "derive mathematical proofs for different types of aggregation functions and FL schemes."
- **Why unresolved:** Current mathematical analysis (Section V) is derived specifically for FedAvg update rule, which simplifies bounding of perturbation term (δ).
- **What evidence would resolve it:** Generalized theorem for attack effectiveness (ρ) applicable to robust aggregation rules like Krum or Trimmed Mean, supported by corresponding experimental bounds.

### Open Question 2
- **Question:** How does Delphi perform when applied to Transformer-based architectures?
- **Basis in paper:** [explicit] The conclusion identifies need for "a more comprehensive analysis for a variety of DL models, such as transformers."
- **Why unresolved:** Current evaluation is limited to AlexNet (CNN), and attack relies on perturbing first hidden layer; Transformers utilize self-attention mechanisms which may exhibit different sensitivity profiles.
- **What evidence would resolve it:** Experimental results applying Delphi-BO to Vision Transformers (ViT) on standard datasets, comparing induced uncertainty against CNN baseline.

### Open Question 3
- **Question:** Does extending parameter manipulation to deeper layers increase attack effectiveness against robust aggregators like Krum?
- **Basis in paper:** [explicit] Section VI-E concludes that "the attack needs to be improved through the expansion of the attack in the rest of the layers."
- **Why unresolved:** Current study limits manipulation to first hidden layer; deeper layers may control more abstract features that, if poisoned, could bypass distance-based defenses.
- **What evidence would resolve it:** Comparative study measuring attack success rates when perturbing neurons in varying layer depths against Krum defense.

## Limitations
- Theoretical upper bound on attack effectiveness is derived specifically for FedAvg and may not hold for other aggregation methods like Krum.
- Current evaluation is limited to CNN architectures (AlexNet), with no testing on Transformer-based models or other neural network architectures.
- The assumption that L2 norm of gradients is a valid proxy for neuron significance may break down with different training regimes or initialization schemes.

## Confidence
- **Delphi-BO outperforms Delphi-LSTR (attack effectiveness):** Medium
- **First layer is the optimal attack surface:** Low
- **KL Divergence is a valid uncertainty measure:** High
- **Theoretical upper bound holds:** Medium

## Next Checks
1. **Generalization Test:** Validate Delphi-BO's superiority over Delphi-LSTR on diverse datasets (ImageNet, MNIST) and model architectures (ResNet, Transformer) to confirm claimed non-convexity of weight-uncertainty relationship.
2. **Robustness Evaluation:** Test attack's effectiveness against other robust aggregation rules beyond Krum (Trimmed Mean, Coordinate-wise Median) to assess robustness of theoretical upper bound.
3. **Dynamic vs. Fixed Neuron Selection:** Conduct detailed ablation study comparing "Dynamic" and "Fixed" neuron selection schemes across multiple training rounds to quantify trade-off between attack effectiveness and detectability.