---
ver: rpa2
title: Legal Retrieval for Public Defenders
arxiv_id: '2601.14348'
source_url: https://arxiv.org/abs/2601.14348
tags:
- legal
- retrieval
- public
- dataset
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces the public defense retrieval task: retrieving
  relevant paragraphs from appellate defense briefs to assist defenders in legal research
  and writing. A retrieval system, the NJ BriefBank, was developed in collaboration
  with the New Jersey Office of the Public Defender.'
---

# Legal Retrieval for Public Defenders

## Quick Facts
- arXiv ID: 2601.14348
- Source URL: https://arxiv.org/abs/2601.14348
- Reference count: 40
- Primary result: Existing legal benchmarks fail on public defense retrieval; domain knowledge (IRAC expansion, synthetic data, legal adaptation) improves performance more than model scale.

## Executive Summary
This work introduces the public defense retrieval task: retrieving relevant paragraphs from appellate defense briefs to assist defenders in legal research and writing. A retrieval system, the NJ BriefBank, was developed in collaboration with the New Jersey Office of the Public Defender. Evaluation on 170 defender queries showed that existing legal retrieval benchmarks (BarExam-QA, LePaRD) do not transfer well, with larger models achieving only 37.37% recall@5. Domain knowledge improves retrieval quality: query expansion using IRAC analysis, fine-tuning on carefully curated synthetic data, and legal domain adaptation increased performance. A taxonomy of defender search queries and a manually annotated dataset are released to support future research on realistic legal retrieval. The study highlights that progress in public defense retrieval depends more on domain alignment than model scale.

## Method Summary
The study developed the NJ BriefBank, a retrieval system for public defenders, using a corpus of 856 appellate briefs and directives (96K paragraphs). Retrieval models (e5-large, Qwen3-Embedding) were fine-tuned on synthetic data generated for the corpus, filtered by rerankers and LLMs, and optionally expanded with IRAC-based legal reasoning. Zero-shot baselines were established using existing legal benchmarks, which were shown to underperform on the PD dataset. Query expansion and domain-specific fine-tuning improved recall@5 performance. The system prioritizes verifiable retrieval over generation to avoid hallucinations and unverifiable citations.

## Key Results
- Existing legal benchmarks (BarExam-QA, LePaRD) fail to transfer to public defense retrieval; larger models achieve only 37.37% recall@5 on PD queries.
- IRAC-based query expansion and fine-tuning on optimized synthetic data improve performance, especially for smaller models.
- Fine-tuning on general legal benchmarks degrades performance on PD datasets; domain-specific adaptation is critical.
- A taxonomy of 170 defender queries and a manually annotated dataset are released for future research.

## Why This Works (Mechanism)

### Mechanism 1: Domain Alignment Over Model Scale
- Claim: Progress in public defense retrieval depends more on domain alignment than model scale.
- Mechanism: Existing legal benchmarks (BarExam-QA, LePaRD) exhibit a distribution shift from real-world public defender queries; fine-tuning on these benchmarks degrades performance on the PD dataset, while domain-specific adaptations improve it.
- Core assumption: Public defender queries are structurally and semantically distinct from bar exam questions and case law citations used in prior datasets.
- Evidence anchors:
  - [abstract] "We show that existing legal retrieval benchmarks fail to transfer to public defense search, however adding domain knowledge improves retrieval quality."
  - [section 5] Fine-tuning on BarExam-QA and LePaRD reduces Recall@5 on PD datasets; optimized synthetic data increases it (Figure 4, Table A4).
  - [corpus] Weak direct corpus support; related work on benchmarks lacking real-world utility is cited (Ott et al., 2022; Gu et al., 2025), but no external validation of this specific transfer gap.
- Break condition: If future benchmarks are constructed with realistic defender queries and manually verified targets, transfer may improve.

### Mechanism 2: IRAC-Based Query Expansion
- Claim: Query expansion using the IRAC legal reasoning framework improves retrieval performance, particularly for smaller models.
- Mechanism: IRAC (Issue, Rule, Application, Conclusion) expands queries with legally relevant signals (doctrinal terms, rule structures), increasing semantic overlap with target paragraphs.
- Core assumption: IRAC analysis generates expansion terms that are present in relevant legal paragraphs but not in the original query.
- Evidence anchors:
  - [abstract] "query expansion using IRAC analysis... increased performance."
  - [section 5] "fine-tuned e5-large-v2 model... surpasses the performance of the larger Qwen3-Embedding-8B model... with expanded queries" (Table A4).
  - [corpus] No direct external corpus evidence; Zheng et al. (2025) showed gains for statute retrieval using reasoning-based expansion, which is analogous.
- Break condition: If IRAC-derived terms diverge from the language used in appellate briefs, expansion may introduce noise.

### Mechanism 3: Curated Synthetic Data with Reranker Filtering
- Claim: Fine-tuning retrieval models on carefully curated synthetic query–paragraph pairs improves performance, while naive synthetic data harms it.
- Mechanism: Fine-tuning a generator on annotated (query, paragraph) pairs from the NJ OPD dataset produces more realistic synthetic queries; reranker and LLM filters remove low-quality pairs, yielding training data aligned with defender information needs.
- Core assumption: The synthetic queries capture the distribution of real defender queries and the reranker threshold effectively discards misaligned examples.
- Evidence anchors:
  - [abstract] "fine-tuning on carefully curated synthetic data... increased performance."
  - [section 5] Naive synthetic data decreases Recall@5 on PD datasets by up to 8.4 points; optimized synthetic data increases it (Table A4).
  - [corpus] Weak corpus support; synthetic data curation for legal domains is underexplored in retrieved neighbors.
- Break condition: If synthetic query distribution drifts from real queries or filtering is too aggressive, gains diminish.

## Foundational Learning

- **IRAC Framework (Issue, Rule, Application, Conclusion)**
  - Why needed here: Query expansion uses IRAC to inject legal reasoning structure into search queries.
  - Quick check question: Given the query "Is consent valid if the motor vehicle stop was illegal?", can you identify the issue, applicable rule, and doctrinal terms?

- **Zero-Shot vs. Fine-Tuned Retrieval**
  - Why needed here: The paper evaluates both regimes; zero-shot performance is low, and fine-tuning effects vary dramatically by training dataset.
  - Quick check question: Why does fine-tuning on BarExam-QA improve performance on BarExam-QA but degrade it on the PD dataset?

- **Recall@k as a Practitioner-Facing Metric**
  - Why needed here: Defenders inspect only top results; Recall@5 correlates highly with other IR metrics (Spearman R ≥ 0.97).
  - Quick check question: For a system returning 5 results per query, what does Recall@5 = 37% mean in practical terms?

## Architecture Onboarding

- **Component map:**
  1. Corpus: 856 public documents → 96K paragraphs (LLM semantic segmentation)
  2. Retriever: Embedding models (e5-large, Qwen3-Embedding) → top-k candidates
  3. Query Expander (optional): IRAC-based expansion via Llama-70B
  4. Reranker: Cross-encoder (e.g., Qwen3-reranker-8B, optionally fine-tuned) → re-scored results
  5. Summarizer: LLM-generated issue/facts summaries for each result (generation for summarization only)

- **Critical path:**
  1. Establish zero-shot baseline with multiple retrievers (Table 2)
  2. Generate optimized synthetic data: fine-tune query generator on NJ OPD (query, paragraph) pairs; filter with reranker and LLM
  3. Fine-tune retriever on curated synthetic data; optionally expand queries with IRAC

- **Design tradeoffs:**
  - **Retrieval vs. Generation:** RAG (NotebookLM) had 66% issue rate with hallucinations and unverifiable citations; retrieval-only preserves verifiability.
  - **Keyword vs. Embedding Search:** Embedding-based retrieval handles natural language queries better; keyword queries (e.g., "803(c)(27)") have 38% unhelpful results.
  - **Scale vs. Domain Adaptation:** Larger models (Qwen3-8B) outperform smaller ones zero-shot, but a fine-tuned e5-large with query expansion matches/exceeds it.

- **Failure signatures:**
  - Training on BarExam-QA/LePaRD or naive synthetic data → performance drop on PD dataset (Figure 4)
  - Keyword-style queries (citations, rule numbers) → higher failure rate (38% not useful)
  - Agentic queries (multihop, jurisdiction checks) → not supported by current BriefBank

- **First 3 experiments:**
  1. **Zero-shot baseline sweep:** Run all 8 retrieval models from Table 2 on PD dataset; log Recall@1, Recall@5. Verify Spearman correlation between NJ OPD and PD datasets.
  2. **Synthetic data ablation:** Train e5-large-v2 on (a) naive synthetic data, (b) optimized synthetic data, (c) optimized + IRAC-expanded queries. Compare Recall@5 deltas.
  3. **Reranker evaluation:** Apply off-the-shelf rerankers (bge, Qwen3-8B) and a fine-tuned Qwen3-reranker to top-20 retrievals; compute F1 vs. majority baseline (Table A8).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can agentic search frameworks that integrate multihop retrieval and legal reasoning improve the accuracy of complex public defense queries, such as verifying if a case is still "good law"?
- Basis in paper: [explicit] Section 4.2 identifies "agentic search" for queries like checking case history as an "exciting avenue for future work," and Section 6.3 reiterates this promise.
- Why unresolved: Current systems rely on single-step embedding or keyword retrieval and cannot perform the multi-step logic required for questions like "has Counterman v. Colorado been addressed in a published New Jersey opinion?"
- What evidence would resolve it: Evaluation of agentic architectures on the "agentic" queries identified in the paper's taxonomy compared to current retrieval baselines.

### Open Question 2
- Question: Can synthetic data generation methods be improved to capture nuanced legal contexts without the imprecision or hallucinations observed in current general-purpose LLMs?
- Basis in paper: [explicit] Section 6.3 states that "Further research will be needed to make progress on this front" regarding the inability of current models to handle nuanced legal contexts which "might affect synthetic data."
- Why unresolved: The paper notes that "naive" synthetic data degrades performance, and even optimized data requires filtering because models fail to address nuanced legal contexts.
- What evidence would resolve it: Comparison of retrieval performance (Recall@k) using synthetic data generated by standard LLMs versus models specifically fine-tuned for legal nuance.

### Open Question 3
- Question: Does incorporating metadata regarding case outcomes (e.g., whether a brief contributed to a successful client outcome) into reranking models improve the utility of search results for public defenders?
- Basis in paper: [explicit] Section 3.1 states: "Future versions may incorporate instruction-following rerankers that also account for other metadata, for example whether a brief contributed to a successful outcome for a client."
- Why unresolved: The current system prioritizes recency and semantic similarity but ignores the success rate or specific outcome of the precedent cases in the ranking logic.
- What evidence would resolve it: User studies or relevance assessments where rerankers with access to outcome metadata are compared against the current recency/semantic baselines.

### Open Question 4
- Question: Can retrieval models trained on the Public Defense (PD) dataset generalize effectively to other jurisdictions or legal domains without suffering from the same domain mismatch observed in existing benchmarks?
- Basis in paper: [inferred] Section 6.1 discusses the "distribution mismatch" between existing benchmarks and real-world use, raising the question of whether a new benchmark like the PD dataset solves this broadly or if it remains specific to the NJ OPD context.
- Why unresolved: The paper demonstrates that general legal benchmarks fail to transfer to the NJ OPD context; it is unknown if models trained on NJ OPD data will transfer to other public defender offices.
- What evidence would resolve it: Cross-domain evaluation where models fine-tuned on the PD dataset are tested on datasets from other jurisdictions or legal aid organizations.

## Limitations

- The study relies on synthetic training data, as the optimized generator was fine-tuned on confidential data unavailable to external researchers.
- The PD dataset, though publicly released, contains only 170 queries and 543 annotated paragraphs, which may limit statistical power for some analyses.
- The study focuses on retrieval rather than generation, leaving the potential of hybrid approaches unexplored.

## Confidence

- **High Confidence:** The core finding that domain alignment outperforms model scale is well-supported by ablation studies (Figure 4, Table A4). The negative transfer from existing legal benchmarks is clearly demonstrated.
- **Medium Confidence:** The effectiveness of IRAC-based query expansion is supported by results, but the lack of external corpus validation introduces some uncertainty. The synthetic data curation approach is promising but untested on external datasets.
- **Low Confidence:** The exact impact of the fine-tuning process on the synthetic data generator is uncertain due to the lack of available weights and detailed methodology.

## Next Checks

1. **External Benchmark Transfer:** Test the optimized retrieval model on an external legal retrieval dataset (e.g., a subset of LeSo or other legal corpora) to validate domain generalization.
2. **Synthetic Data Ablation:** Systematically vary the filtering threshold and few-shot examples in the synthetic data generation process to identify optimal configurations and assess robustness.
3. **IRAC Expansion Robustness:** Evaluate IRAC-based expansion on a held-out subset of the PD dataset with manually verified expansion quality to ensure it consistently adds relevant terms.