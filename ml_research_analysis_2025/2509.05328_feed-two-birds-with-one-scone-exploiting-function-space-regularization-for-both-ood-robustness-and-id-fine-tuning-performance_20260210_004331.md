---
ver: rpa2
title: 'Feed Two Birds with One Scone: Exploiting Function-Space Regularization for
  Both OOD Robustness and ID Fine-Tuning Performance'
arxiv_id: '2509.05328'
source_url: https://arxiv.org/abs/2509.05328
tags:
- space
- function
- fine-tuning
- feature
- parameter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of robust fine-tuning for foundation
  models like CLIP, which aims to maintain
---

# Feed Two Birds with One Scone: Exploiting Function-Space Regularization for Both OOD Robustness and ID Fine-Tuning Performance

## Quick Facts
- **arXiv ID:** 2509.05328
- **Source URL:** https://arxiv.org/abs/2509.05328
- **Reference count:** 40
- **Primary result:** Function-Space Regularization (FRR-FT) improves OOD robustness by 3-4% while maintaining ID accuracy on CLIP models.

## Executive Summary
This paper addresses the challenge of robust fine-tuning for foundation models like CLIP, which aims to maintain Out-of-Distribution (OOD) robustness while adapting to specific downstream tasks. Existing methods that regularize parameter or feature spaces often fail to preserve the generalization capabilities of pre-trained models. The authors propose a novel approach that directly optimizes the distance between the fine-tuned model and the pre-trained model in function space, measuring the divergence of their predictions rather than their internal parameters.

The method introduces two complementary regularizations: Functional Alignment Regularization (FAR) and Functional Consistency Regularization (FCR). FAR uses RandAugment to generate simulated OOD samples and minimizes the L2 distance between the predictions of the fine-tuned model and the frozen pre-trained model. FCR ensures prediction stability by minimizing the KL divergence between clean and perturbed inputs. Experiments on multiple CLIP architectures and OOD benchmarks demonstrate that FRR-FT achieves a better trade-off between ID accuracy and OOD robustness compared to existing methods.

## Method Summary
FRR-FT (Functional Regularization Robust Fine-Tuning) directly optimizes the distance between the fine-tuned model and the pre-trained model in function space, using two key regularizations. Functional Alignment Regularization (FAR) generates simulated OOD samples using RandAugment and minimizes the L2 distance between the fine-tuned model's predictions and the frozen pre-trained model's predictions on these samples. Functional Consistency Regularization (FCR) promotes stable predictions by minimizing the KL divergence between the model's outputs on clean and perturbed inputs. The combined loss function is: $\mathcal{L} = \mathcal{L}_{CE} + \lambda_1 \tilde{R}_{FAR} + \lambda_2 \tilde{R}_{FCR}$. The method is evaluated on CLIP models (ViT-B/32, ViT-B/16, ResNet-50, ViT-L/14) fine-tuned on ImageNet-1K, with OOD evaluation on ImageNet-V2, ImageNet-R, ImageNet-A, ImageNet-Sketch, WILDS-iWildCam, and WILDS-FMoW.

## Key Results
- FRR-FT achieves 3-4% improvement in OOD accuracy while maintaining ID accuracy compared to baseline fine-tuning
- Combining FAR and FCR yields higher OOD accuracy (52.2%) than FAR alone (51.7%) on ImageNet benchmarks
- The method outperforms parameter/feature space regularization methods like L2-SP and CaRot on multiple OOD datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Regularizing the distance between the fine-tuned model and the pre-trained model in *function space* (output behavior) preserves Out-of-Distribution (OOD) robustness better than regularizing parameter or feature spaces.
- **Mechanism:** The method minimizes the L2 distance between the predictions of the fine-tuning model ($f_\theta$) and the frozen pre-trained model ($f_0$) using simulated OOD samples. By penalizing deviations in output behavior rather than internal weights, the model retains the generalization capabilities of the foundation model while adapting to the downstream task.
- **Core assumption:** The distribution of "simulated" OOD samples (generated via augmentation) sufficiently approximates the true distribution shifts encountered during testing, such that functional stability on these samples transfers to real OOD data.
- **Evidence anchors:** [abstract] "existing methods might serve as a poor proxy for the optimization in the function space." [section 3.2.1] Uses RandAugment to generate simulated OOD data $\tilde{x}$ to approximate the functional distance $\|f_\theta - f_0\|^2$. [corpus] Related work in robust fine-tuning often focuses on weight decay (L2-SP) or feature distillation; this paper explicitly pivots to output space.
- **Break condition:** Fails if the data augmentation strategy (e.g., RandAugment) produces artifacts unrepresentative of real-world distribution shifts, causing the model to overfit to specific augmentations rather than learning general robustness.

### Mechanism 2
- **Claim:** Enforcing prediction consistency between clean and perturbed inputs (Functional Consistency Regularization) actively enhances robustness beyond merely anchoring to the pre-trained model.
- **Mechanism:** This component minimizes the KL divergence between the model's prediction on a clean sample $x$ and its prediction on a perturbed sample $\tilde{x}$. It smooths the prediction landscape specifically for the downstream task data, treating robustness as an invariance problem.
- **Core assumption:** The decision boundaries learned for the downstream task should be locally flat (invariant) to input perturbations, which directly correlates with better OOD generalization.
- **Evidence anchors:** [abstract] "introduce an additional consistency regularization to promote stable predictions of perturbed samples." [figure 3] Shows that combining Alignment (FAR) with Consistency (FCR) yields higher OOD accuracy (52.2%) than FAR alone (51.7%). [corpus] Weak direct link in provided neighbors, but consistent with general robustness literature (e.g., "D2R" in neighbors mentions dual regularization).
- **Break condition:** Degrades if the perturbation strength is too high, forcing the model to map distinct semantic concepts to the same label (over-smoothing), or if the downstream task requires sensitivity to fine-grained details that the perturbation destroys.

### Mechanism 3
- **Claim:** Parameter/Feature/Logit distances serve as "imperfect proxies" for OOD robustness, whereas functional perturbation analysis correlates more reliably with performance stability.
- **Mechanism:** The paper empirically demonstrates that perturbing model weights or features often leads to erratic performance collapses (sensitivity), whereas perturbations in function space (predictions) remain stable. By optimizing in this stable space, the method avoids the "architecture-dependent" failures of previous SOTA methods like L2-SP or CaRot.
- **Core assumption:** Robustness is fundamentally a property of the input-output mapping (the function), not the specific parameterization of the weights.
- **Evidence anchors:** [figure 2] Shows that perturbations in parameter/feature/logit space cause high variance in loss/accuracy, while function space perturbations are stable. [section 1] "existing methods cannot always improve OOD robustness for different model architectures." [corpus] Neighbor "Directional Gradient Projection" (arXiv:2502.15895) explores similar constraints on gradients/weights, suggesting active research in *how* to constrain fine-tuning.
- **Break condition:** If the functional distance metric (L2) fails to capture semantic drift—for example, if small numerical output differences correspond to large semantic errors—this theoretical advantage disappears.

## Foundational Learning

- **Concept:** **Function-Space vs. Parameter-Space Regularization**
  - **Why needed here:** The core thesis rejects standard weight decay (L2-SP). You must understand that two models can have similar weights but vastly different behaviors (functions), and vice versa.
  - **Quick check question:** Can you explain why constraining the L2 norm of weights ($||\theta - \theta_0||$) does not guarantee the model's output ($f(x)$) stays the same for a specific input?

- **Concept:** **Simulated OOD Data (Data Augmentation as Proxy)**
  - **Why needed here:** The method requires OOD data to calculate the functional distance, but by definition, only In-Distribution (ID) data is available during fine-tuning.
  - **Quick check question:** How does RandAugment bridge the gap between ID training data and unknown OOD test data in this context?

- **Concept:** **Consistency Regularization (Smoothing)**
  - **Why needed here:** This is the "FCR" component. It is distinct from the alignment with the teacher; it forces the student model to be confident and stable on its own augmentations.
  - **Quick check question:** What is the difference between aligning a student's prediction with a *teacher's* prediction (FAR) and aligning a student's prediction with its *own* perturbed prediction (FCR)?

## Architecture Onboarding

- **Component map:** Batch of ID images ($x$) -> RandAugment augmentation -> Dual forward pass through trainable model ($f_\theta$) and frozen pre-trained model ($f_0$) -> Loss computation combining CE, FAR, and FCR
- **Critical path:** The generation of $\tilde{x}$ is the critical dependency. If the augmentation pipeline is slow or generates trivial noise, both regularization terms fail.
- **Design tradeoffs:**
  - **$\lambda_1$ (FAR weight):** High values lock the model to the pre-trained behavior (high OOD, low ID). Low values allow full fine-tuning (high ID, low OOD).
  - **$\lambda_2$ (FCR weight):** Controls local smoothness.
  - **Assumption:** The paper uses a frozen $f_0$. This increases memory usage (storing two sets of activations) but avoids the instability of evolving teachers.
- **Failure signatures:**
  - **ID Accuracy Collapse:** Model matches pre-trained OOD performance but fails to learn the new task (over-regularized).
  - **Architecture Sensitivity:** If gains are seen on ViT but not ResNet (or vice versa), check the augmentation strength, as different architectures have different inductive biases for texture vs. shape.
  - **NaN Loss:** Check stability of KL divergence if $f_\theta(\tilde{x})$ produces zero probabilities.
- **First 3 experiments:**
  1.  **Baseline Verification:** Reproduce Figure 2. Train a vanilla CLIP model on ImageNet, measure OOD drop. Add L2-SP (param) and observe the trade-off.
  2.  **Ablation Study (Replicate Fig 3):** Run three configurations on a single backbone (e.g., ViT-B/32): (A) FAR only, (B) FCR only, (C) FRR-FT (Both). Verify that (C) achieves the highest OOD accuracy.
  3.  **Augmentation Sensitivity:** Replace RandAugment with simple Gaussian noise or Cutout. Determine if the specific *type* of augmentation is critical to the "Simulated OOD" hypothesis.

## Open Questions the Paper Calls Out
- **Open Question 1:** Can Functional Regularization Robust Fine-Tuning (FRR-FT) be effectively applied to non-vision foundation models, such as Large Language Models (LLMs)?
- **Open Question 2:** Is RandAugment the optimal method for generating the "simulated OOD" samples required for the Functional Alignment Regularization (FAR) term?
- **Open Question 3:** Does combining FRR-FT with advanced model-ensemble methods (like Model Soups) yield synergistic improvements in the ID/OOD trade-off?

## Limitations
- The RandAugment-based "simulated OOD" assumption is reasonable but unverified for real-world distribution shifts, creating a potential failure point if augmentation artifacts dominate.
- The method's effectiveness beyond image classification tasks or CLIP-like architectures remains uncertain.
- Specific hyperparameter sensitivity may vary across different model architectures, requiring architecture-specific tuning.

## Confidence
- **High confidence:** The empirical observation that parameter/feature space regularization serves as a poor proxy for OOD robustness (Figure 2 results)
- **Medium confidence:** The claim that functional consistency regularization provides additive benefits beyond alignment alone (requires careful hyperparameter tuning)
- **Medium confidence:** The general transferability across different CLIP architectures, though the specific hyperparameter sensitivity may vary

## Next Checks
1. **Architecture Sensitivity Test:** Replicate the ablation study across ResNet and ViT backbones with varying augmentation strengths to determine if the FAR+FCR combination remains superior or if the optimal λ values are architecture-dependent.
2. **Augmentation Ablation:** Systematically replace RandAugment with simpler perturbation strategies (Gaussian noise, CutMix, no augmentation) to test whether the specific augmentation method or the general principle of functional regularization drives the improvements.
3. **OOD Simulation Fidelity:** Generate synthetic distribution shifts (controlled corruption, style transfer) to compare functional distance stability against true OOD performance, testing whether the RandAugment proxy correlates with actual robustness gains.