---
ver: rpa2
title: 'CFMI: Flow Matching for Missing Data Imputation'
arxiv_id: '2506.09258'
source_url: https://arxiv.org/abs/2506.09258
tags:
- cfmi
- csdi
- ours
- short
- wine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CFMI combines continuous normalising flows with flow-matching to
  impute missing data. The method uses a single conditional model to approximate all
  necessary conditional distributions, avoiding the exponential complexity of learning
  separate models for each missingness pattern.
---

# CFMI: Flow Matching for Missing Data Imputation

## Quick Facts
- **arXiv ID:** 2506.09258
- **Source URL:** https://arxiv.org/abs/2506.09258
- **Reference count:** 40
- **Primary result:** CFMI achieves competitive or superior performance to traditional and deep learning imputation methods across 24 UCI datasets while training faster than diffusion-based approaches

## Executive Summary
CFMI introduces a conditional flow matching approach for missing data imputation that learns a single model to approximate all conditional distributions needed for imputation, avoiding the exponential complexity of learning separate models for each missingness pattern. The method combines continuous normalizing flows with flow-matching techniques to provide accurate imputations while maintaining computational efficiency. Evaluated on 24 UCI tabular datasets and time-series data, CFMI demonstrates strong performance across different missingness mechanisms and levels, matching or exceeding the accuracy of established methods like missForest and modern deep learning approaches while offering faster training times compared to diffusion-based alternatives.

## Method Summary
CFMI uses conditional flow matching to impute missing data by learning a single conditional model that approximates all necessary conditional distributions simultaneously. The approach leverages a vector field conditioned on observed values to transform between observed and missing data through an Optimal Transport path or similar integration scheme. Unlike traditional methods that require separate models for each missingness pattern (leading to exponential complexity), CFMI learns one model that handles all patterns through conditioning. The method is evaluated on tabular datasets using metrics like RMSE, MAE, Wasserstein-2 distance, CRPS, and MMD, as well as on time-series data where it matches the accuracy of state-of-the-art diffusion methods while training significantly faster.

## Key Results
- CFMI achieves competitive or superior performance to traditional methods like missForest and modern deep learning approaches across 24 UCI tabular datasets
- The method maintains strong results across different missingness levels (25%, 50%, 75%) and mechanisms (MCAR, MAR)
- Applied to zero-shot time-series imputation, CFMI matches the accuracy of a state-of-the-art diffusion-based method while training significantly faster
- CFMI demonstrates computational efficiency by avoiding the exponential complexity of learning separate models for each missingness pattern

## Why This Works (Mechanism)
CFMI works by learning a single conditional model that can handle all possible missingness patterns through conditioning, rather than requiring separate models for each pattern. The flow matching framework allows the model to learn the transformation between observed and missing data using a vector field conditioned on available information. This conditioning mechanism enables the model to capture the dependencies between observed and missing values without explicitly enumerating all possible missingness patterns. The continuous normalizing flow formulation provides a principled way to model complex, high-dimensional distributions while maintaining tractability through the flow matching approximation.

## Foundational Learning
- **Flow Matching**: A technique that learns the velocity field of a continuous transformation rather than the transformation itself; needed because it provides stable training and avoids the numerical issues of traditional normalizing flows; quick check: verify the velocity field is conditioned on observed values
- **Conditional Modeling**: Training models that depend on observed context; needed because it allows a single model to handle all missingness patterns; quick check: confirm conditioning is applied before the main transformation layers
- **Optimal Transport Path**: A linear interpolation between distributions (X_t = (1-t)X_0 + tX_1); needed as a simple, tractable path for flow matching; quick check: verify the path formulation in the implementation
- **Wasserstein Distance**: A metric for comparing probability distributions; needed as a principled evaluation metric for imputation quality; quick check: ensure proper implementation using a solver like POT library
- **Missing Data Mechanisms**: MCAR (Missing Completely At Random) and MAR (Missing at Random); needed as the evaluation framework; quick check: verify masking procedure generates the correct mechanism
- **Continuous Normalizing Flows**: ODE-based transformations for density estimation; needed as the theoretical foundation; quick check: verify the ODE solver and integration scheme

## Architecture Onboarding

**Component Map:**
Input features -> Masking layer -> Conditioning mechanism -> MLP vector field -> ODE solver -> Imputed output

**Critical Path:**
The critical path involves the conditioning mechanism that takes observed values and passes them to the MLP vector field, which then generates the velocity field used by the ODE solver to transform the data toward completion.

**Design Tradeoffs:**
- Single conditional model vs. multiple pattern-specific models: CFMI trades potential pattern-specific optimization for exponential complexity reduction
- Flow matching vs. traditional normalizing flows: CFMI gains training stability at the cost of potentially more complex implementation
- Conditioning mechanism choice: Different approaches (concatenation, FiLM, etc.) affect representational capacity and training dynamics

**Failure Signatures:**
- Poor convergence: May indicate incorrect conditioning or inappropriate ODE solver parameters
- Suboptimal imputation quality: Could result from insufficient model capacity or inadequate training data
- Computational inefficiency: May suggest suboptimal architecture choices or integration scheme

**First Experiments:**
1. Implement basic conditioning (concatenation) and verify it can learn simple missing patterns on a small dataset
2. Test different ODE solvers (Euler vs. RK45) and compare convergence profiles and runtime
3. Validate Wasserstein-2 distance calculations using a known distribution pair to ensure correct implementation

## Open Questions the Paper Calls Out
The paper explicitly limits evaluation to MCAR and MAR missingness mechanisms, leaving the performance under Missing Not At Random (MNAR) mechanisms unexplored. The computational complexity and scaling behavior for high-dimensional datasets are not thoroughly investigated, as experiments focus on relatively small tabular datasets. The sensitivity to categorical embedding dimensions is fixed at 4 without ablation studies or justification for this choice.

## Limitations
- Limited evaluation to MCAR and MAR mechanisms, with no verification of performance under MNAR scenarios
- Experiments restricted to small tabular datasets, leaving scaling behavior to high-dimensional data unclear
- Fixed categorical embedding dimension of 4 without ablation studies or justification for this choice

## Confidence

**High Confidence:** The core algorithmic contribution of using a single conditional model for all missingness patterns is clearly described and demonstrated through competitive performance on UCI datasets and time-series tasks.

**Medium Confidence:** The comparative results against baselines like missForest and CSDI are reproducible given the metric definitions, though exact numerical replication requires resolving architectural unknowns.

**Low Confidence:** The specific flow integration scheme (ODE solver, step size) and whether "short" variants were used in experiments could materially affect convergence and runtime claims.

## Next Checks
1. Verify the conditioning mechanism by implementing both concatenation and FiLM-style approaches to match reported performance
2. Test convergence profiles using both OT and VP paths to confirm which formulation was used experimentally
3. Replicate the time-series imputation speed comparison by measuring wall-clock time for equivalent training steps on the same hardware