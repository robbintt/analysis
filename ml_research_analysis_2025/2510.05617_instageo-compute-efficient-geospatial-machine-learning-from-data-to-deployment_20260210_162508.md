---
ver: rpa2
title: 'InstaGeo: Compute-Efficient Geospatial Machine Learning from Data to Deployment'
arxiv_id: '2510.05617'
source_url: https://arxiv.org/abs/2510.05617
tags:
- data
- instageo
- geospatial
- performance
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "InstaGeo addresses the lack of end-to-end workflows for deploying\
  \ geospatial foundation models by providing an open-source framework that integrates\
  \ data curation, task-specific model distillation, and web-based deployment. The\
  \ system automates the creation of satellite image chips from in-situ observations,\
  \ trains lightweight student models via distillation that are up to 8\xD7 smaller\
  \ than standard fine-tuned counterparts, and enables interactive map-based visualization\
  \ of predictions."
---

# InstaGeo: Compute-Efficient Geospatial Machine Learning from Data to Deployment

## Quick Facts
- **arXiv ID:** 2510.05617
- **Source URL:** https://arxiv.org/abs/2510.05617
- **Reference count:** 28
- **Primary result:** End-to-end framework integrating data curation, distillation, and deployment for geospatial ML

## Executive Summary
InstaGeo addresses the lack of end-to-end workflows for deploying geospatial foundation models by providing an open-source framework that integrates data curation, task-specific model distillation, and web-based deployment. The system automates the creation of satellite image chips from in-situ observations, trains lightweight student models via distillation that are up to 8× smaller than standard fine-tuned counterparts, and enables interactive map-based visualization of predictions. Across three tasks—flood mapping, crop segmentation, and desert locust prediction—InstaGeo-replicated datasets closely matched original benchmarks with differences under 2 percentage points mean intersection-over-union (mIoU).

## Method Summary
InstaGeo is an open-source framework for geospatial machine learning that integrates data curation via STAC querying, task-specific model distillation, and web-based deployment. The pipeline converts raw satellite imagery into model-ready chips, trains efficient student models through layer-wise distillation, and enables interactive visualization through a web application. The framework supports both vanilla fine-tuning and task-specific distillation approaches, with the latter significantly reducing model size while maintaining accuracy.

## Key Results
- Task-specific distillation achieved comparable accuracy to vanilla fine-tuning while significantly reducing model size and computational cost, up to 8× smaller for desert locust prediction
- InstaGeo-replicated datasets closely matched original benchmarks with differences under 2 percentage points mIoU across three tasks
- Using InstaGeo's streamlined pipeline, a larger crop segmentation dataset was curated, improving mIoU by 12 percentage points to 60.65%
- Framework enables progression from raw data to deployed model within one working day, facilitating real-time, large-scale environmental monitoring

## Why This Works (Mechanism)

### Mechanism 1
Automated data curation via STAC querying reduces the engineering friction of converting raw satellite imagery into model-ready tensors, potentially preserving model performance if source imagery aligns with pre-training data. The `chip_creator` module queries SpatioTemporal Asset Catalogs (STAC) for specific missions (e.g., HLS, Sentinel-2), filters by cloud cover and temporal tolerance, retrieves spatial chunks, and serializes them into standardized chip-label pairs. This enforces consistency between inference inputs and the model's training distribution.

### Mechanism 2
Task-specific layer-wise distillation allows for the creation of student models that maintain teacher-level performance with significantly reduced parameter counts and FLOPs, conditional on the task complexity not requiring the full depth of the encoder. A standard "Vanilla Fine-tuned" model acts as a frozen teacher. A student model is initialized using only the first N layers of the teacher's encoder. The student is trained using a composite loss: standard cross-entropy with ground truth and KL-divergence against the teacher's logits.

### Mechanism 3
An integrated web-application deployment pipeline reduces the operational latency from "model checkpoint" to "visual insight," assuming the user can containerize the provided microservices. The framework wraps inference scripts in a FastAPI backend that generates Cloud Optimized GeoTIFFs (COGs). These are served via a TiTiler instance to a React frontend. This bypasses the need for manual GIS software imports for basic validation.

## Foundational Learning

- **SpatioTemporal Asset Catalogs (STAC)**
  - **Why needed here:** The entire Data Component relies on STAC for searching granules. Without understanding STAC specifications (Items, Catalogs, Collections), one cannot debug why `chip_creator` fails to find imagery for specific dates.
  - **Quick check question:** Can you explain the difference between querying a STAC API for a specific date range versus a bounding box, and how InstaGeo combines these?

- **Knowledge Distillation (Logit Matching)**
  - **Why needed here:** The efficiency gains depend on distillation. Understanding that the student learns from the *soft probabilities* (logits) of the teacher—not just hard labels—is critical for tuning the loss weights.
  - **Quick check question:** Why might KL-divergence loss be preferred over Mean Squared Error when distilling a classification model's logits?

- **Cloud Optimized GeoTIFFs (COGs)**
  - **Why needed here:** The Application Component uses TiTiler to serve predictions. Understanding COGs (internal tiling and overviews) explains why the visualization is interactive rather than requiring a full raster download.
  - **Quick check question:** How does the internal structure of a COG allow a tile server to fetch only a specific zoom level of a prediction map without reading the entire file?

## Architecture Onboarding

- **Component map:** Data (chip_creator) -> Model (Prithvi Encoder + Segmentation Head) -> Inference (GeoTIFF generation) -> Application (FastAPI + TiTiler + React)
- **Critical path:** Raw Labels (CSV/GeoTIFF) -> STAC Query -> Chip Creation -> Vanilla Fine-tuning (Teacher) -> Distillation (Student) -> Inference on new AOI -> COG Generation -> TiTiler Visualization
- **Design tradeoffs:** Using HLS ensures high temporal revisit time but introduces a ~3pp performance penalty compared to native Sentinel-2 for models pre-trained on S2; reducing student encoder layers lowers FLOPs but risks losing global context
- **Failure signatures:** Empty Dataset (STAC query returned no granules), Spectral Mismatch (model performs poorly on HLS chips but well on S2), Inference Timeout (selected AOI is too large for worker's memory buffer)
- **First 3 experiments:** 1) Run `chip_creator` on provided CSV points to generate single batch of chips and verify pixel alignment; 2) Train vanilla teacher on small split, then run distillation with student layers N in {2, 4, 6} to observe accuracy/FLOP trade-off; 3) Launch Application Component and submit bounding box to verify prediction raster overlays correctly on base map

## Open Questions the Paper Calls Out

### Open Question 1
Can the selection of the optimal student model encoder depth be automated based on task complexity metrics, rather than relying on manual experimental search? The current framework requires manual hyperparameter search to align model capacity with task difficulty.

### Open Question 2
How well does the task-specific distillation approach generalize to dense regression tasks (e.g., biomass estimation) compared to the semantic segmentation tasks evaluated? The distillation loss combines CrossEntropy and KL Divergence; it is unclear if this combination holds for continuous regression targets without modification.

### Open Question 3
Do alternative geospatial foundation model architectures integrate as seamlessly as Prithvi-EO-2.0 while maintaining the reported compute efficiency? The current results rely exclusively on the Prithvi family (V1-100M and V2-300M).

## Limitations
- Performance degradation (~3 pp mIoU) when replicating datasets using different satellite sensors than the model's pre-training corpus
- Lack of specified hyperparameters for distillation (learning rates, loss weights, optimizer settings) creating reproducibility uncertainty
- Infrastructure dependencies for "one working day" deployment claim that may not be universally available

## Confidence
- **High Confidence**: Layer-wise distillation producing smaller models with maintained accuracy (supported by Table 2 results showing 8× reduction with 0.5 pp mIoU drop)
- **Medium Confidence**: Comparative performance claims across three tasks (flood mapping, crop segmentation, locust prediction) are internally supported but lack external validation
- **Low Confidence**: Generalization claim that InstaGeo works "across diverse geospatial applications" based on only three specific tasks

## Next Checks
1. Replicate flood mapping dataset using both HLS and native Sentinel-2 imagery to quantify exact performance gap and measure consistency across geographic regions
2. Systematically vary student encoder depth N {2, 4, 6, 8, 10} across all three tasks to map accuracy/FLOP trade-off curve and document exact hyperparameters achieving reported results
3. Time each component of deployment pipeline (STAC query, chip creation, fine-tuning, distillation, inference, COG generation, tile serving) on different hardware configurations to verify "one working day" claim under realistic conditions