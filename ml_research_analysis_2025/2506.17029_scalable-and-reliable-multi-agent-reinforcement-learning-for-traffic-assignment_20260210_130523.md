---
ver: rpa2
title: Scalable and Reliable Multi-agent Reinforcement Learning for Traffic Assignment
arxiv_id: '2506.17029'
source_url: https://arxiv.org/abs/2506.17029
tags:
- traffic
- assignment
- methods
- demand
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Scalable and Reliable Multi-agent Reinforcement Learning for Traffic Assignment

## Quick Facts
- arXiv ID: 2506.17029
- Source URL: https://arxiv.org/abs/2506.17029
- Authors: Leizhen Wang; Peibo Duan; Cheng Lyu; Zewen Wang; Zhiqiang He; Nan Zheng; Zhenliang Ma
- Reference count: 6
- Primary result: Achieves User Equilibrium in traffic assignment with improved scalability over individual-agent approaches

## Executive Summary
This paper proposes MARL-OD-DA, a scalable and reliable multi-agent reinforcement learning framework for traffic assignment. By redefining agents as origin-destination (OD) pair routers rather than individual travelers, the framework significantly reduces computational complexity while maintaining solution quality. The method employs a Dirichlet-based action space with pruning to enforce sparsity in route assignments and uses local relative gap rewards to align agent learning with the global User Equilibrium objective.

## Method Summary
The framework models traffic assignment as a multi-agent reinforcement learning problem where each agent represents an OD pair rather than individual travelers. Agents use a three-layer MLP (512 hidden units) to output Dirichlet concentration parameters, which are sampled and pruned to produce sparse route distributions. The policy is trained using Independent Proximal Policy Optimization (IPPO) with a learning rate of 4×10⁻⁵ and discount factor of 0.75. The key innovation is the use of local relative gap rewards to guide agents toward User Equilibrium while maintaining scalability through OD-pair consolidation.

## Key Results
- Reduces agent count by at least two orders of magnitude compared to traveler-based approaches
- Achieves User Equilibrium convergence on small and medium-scale networks (OW, SiouxFalls, Anaheim)
- Demonstrates superior scalability, avoiding memory limitations that make individual-agent MARL impractical for larger networks

## Why This Works (Mechanism)

### Mechanism 1: OD-Pair Agent Consolidation
Replacing individual traveler agents with OD pair routers reduces computational complexity and improves scalability in high-demand scenarios. The framework models M OD pairs instead of N individual travelers, where M ≪ N. Each agent outputs route distributions rather than discrete choices, drastically reducing action space dimensionality. This works under the assumption that travelers within an OD pair have homogeneous routing behavior. Break condition: If travelers within an OD pair have significantly diverse preferences not captured by the router, the consolidated agent may produce sub-optimal flow distributions.

### Mechanism 2: Dirichlet Action Space with Pruning
The Dirichlet-based policy output combined with action pruning stabilizes exploration and enforces necessary sparsity. The policy network outputs parameters for a Dirichlet distribution, naturally satisfying probability simplex constraints. Action pruning forces negligible probabilities to zero, ensuring valid path exclusions and mitigating "label smoothing" effects. This works under the assumption that optimal solutions require strictly zero flow on inefficient paths. Break condition: If the pruning threshold is set too aggressively, it may prematurely cutoff viable alternative routes.

### Mechanism 3: Local Relative Gap Reward
Using the change in local relative gap as a reward signal aligns individual agent learning with the global User Equilibrium objective better than simple travel time minimization. The reward is based on the reduction of the difference between average cost on used paths and shortest path cost for that specific OD pair. This works under the assumption that agents have sufficient observability of path costs. Break condition: If agents react myopically to local gaps without anticipating congestion response of other agents, the system may oscillate rather than converge.

## Foundational Learning

- **Concept: User Equilibrium (UE) vs. System Optimal (SO)**
  - Why needed here: The paper explicitly distinguishes between minimizing individual costs (UE) and total system costs (SO). Understanding this is critical because the choice of reward function biases the solution toward one or the other.
  - Quick check question: Does the Braess paradox suggest that adding a link to a network can improve overall travel time in a UE state? (Answer: No, it usually degrades it; UE minimizes individual cost, not total time).

- **Concept: Probability Simplex Constraints**
  - Why needed here: The agent must output valid route splits that sum to 1.0 and are non-negative. Standard network outputs do not guarantee this, motivating the use of Dirichlet distributions or Softmax.
  - Quick check question: Why might a Softmax function be insufficient for traffic assignment compared to a Dirichlet distribution? (Answer: Softmax strictly outputs positive values, preventing the model from assigning exactly zero flow to a path, which is often required for sparsity).

- **Concept: Relative Gap as a Convergence Metric**
  - Why needed here: The paper uses "relative gap" as the primary evaluation metric and reward signal. It quantifies how far the current assignment is from a theoretical equilibrium.
  - Quick check question: If the relative gap is zero, what condition holds for all used paths in an OD pair? (Answer: The travel costs on all used paths are equal, and no unused path has a lower cost).

## Architecture Onboarding

- **Component map:** Static features + Dynamic features -> 3-layer MLP (512) -> Softplus -> Dirichlet Concentration Parameters -> Sample Dirichlet -> Action Pruning -> Normalize -> Flow Assignment

- **Critical path:**
  1. Input: Static features (route topology) + Dynamic features (current travel times/costs)
  2. Forward Pass: Network outputs concentration params α
  3. Sampling: Sample split ratios a ~ Dir(α)
  4. Pruning: Apply threshold τ=10⁻⁴ to force sparsity
  5. Loading: Assign demand to routes based on split ratios; simulate network
  6. Reward: Compute local relative gap reduction

- **Design tradeoffs:**
  - Scalability vs. Granularity: Defining agents as OD-pairs scales to massive demand (360k travelers) but assumes homogeneous routing logic for all travelers in that pair
  - Dirichlet vs. Softmax: Dirichlet allows zero-probability actions (sparsity) but introduces complexity in gradient calculation. Softmax is stable but forces non-zero probability on all paths

- **Failure signatures:**
  - Gradient Explosion: If pruning is not handled correctly, log-prob calculations in the Dirichlet strategy can diverge
  - Oscillation: Agents constantly shifting flow between two equivalent paths if the local gap signal is noisy
  - Memory Overflow (Baseline): Attempting to run a "Traveler-based" MARL on medium networks will crash GPU memory

- **First 3 experiments:**
  1. Sanity Check (OW Network): Validate convergence on a small grid. Compare convergence speed of "Local Relative Gap" reward vs. "Negative Travel Time" reward to verify the alignment with UE.
  2. Scaling Test (SiouxFalls): Measure memory footprint and training time. Confirm that Agent count reduction (OD-pair vs Traveler) allows the model to fit in memory where baselines fail.
  3. Ablation (Pruning): Run MARL-OD-DA with and without action pruning on the Anaheim network to quantify the improvement in convergence stability and final Relative Gap.

## Open Questions the Paper Calls Out
- Can the MARL-OD-DA framework maintain convergence efficiency when applied to large-scale urban networks significantly larger than the Anaheim network?
- How does the framework perform in high-fidelity simulation environments that capture complex, non-stationary traffic dynamics?
- How does the constraint of pre-defining route sets via K-shortest paths impact the optimality of the User Equilibrium solution?

## Limitations
- Limited validation to small- to medium-scale networks, with extension to large-scale urban networks identified as future research
- Relies on simplified analytical traffic assignment framework rather than high-fidelity simulation environments
- Route sets are pre-defined using K-shortest paths, potentially restricting discovery of optimal routes during high congestion

## Confidence
- Scalability claims: High - Directly supported by agent count reduction analysis
- UE convergence: Medium - Validated on limited network sizes but methodology is sound
- Dirichlet+pruning effectiveness: Medium - Mechanism described but empirical validation limited to specific networks

## Next Checks
1. Implement action pruning correctly, ensuring it's applied only to environment actions and not gradient calculations to prevent NaN loss
2. Verify the relative gap calculation implementation matches the paper's equations, particularly ensuring shortest path costs are recalculated at each step
3. Test the framework on a network larger than Anaheim (e.g., Chicago Sketch or similar) to validate scalability claims beyond the current experimental scope