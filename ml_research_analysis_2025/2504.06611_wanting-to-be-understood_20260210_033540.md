---
ver: rpa2
title: Wanting to be Understood
arxiv_id: '2504.06611'
source_url: https://arxiv.org/abs/2504.06611
tags:
- other
- reward
- agent
- agents
- crossing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the hypothesis that humans have an intrinsic
  motivation to understand and be understood, even without extrinsic rewards. The
  authors implement this in reinforcement learning agents through artificial curiosity
  (drive to understand) and imitation, influence/impressionability, and sub-reaction
  time anticipation (drives to be understood).
---

# Wanting to be Understood

## Quick Facts
- arXiv ID: 2504.06611
- Source URL: https://arxiv.org/abs/2504.06611
- Authors: Chrisantha Fernando; Dylan Banarse; Simon Osindero
- Reference count: 23
- One-line primary result: Intrinsic rewards for mutual understanding drive agents to prefer social interaction and enable asymmetric cooperation

## Executive Summary
This paper investigates the hypothesis that humans have an intrinsic motivation to understand and be understood, even without extrinsic rewards. The authors implement this in reinforcement learning agents through artificial curiosity (drive to understand) and imitation, influence/impressionability, and sub-reaction time anticipation (drives to be understood). They test these motivations in a perceptual crossing paradigm where two agents interact on a 1D circular line. The key finding is that while artificial curiosity alone does not drive agents to prefer social interaction, the reciprocal understanding rewards successfully lead agents to prioritize interaction with each other over inanimate objects. Furthermore, the intrinsic motivation enables cooperation in tasks where only one agent receives extrinsic reward for the other's behavior.

## Method Summary
The authors implement a 1D circular line environment where two agents can interact with each other, each other's shadows (offset 0.2 modulo 1), or private stationary objects. Agents receive 4D observations (position, previous crossing, last action, last reward) and can take discrete actions (left, right, no-op) with movement increment 0.05. Four reward variants are tested: artificial curiosity based on prediction error with confidence, imitation/imitation-by rewards comparing 10-step interaction histories, influence/impressionability rewards based on mutual information over rolling buffers, and sub-reaction time anticipation with delayed actions. Training uses PPO with LSTM policies, separate LSTM predictors for crossings, and rolling buffers for reward calculations.

## Key Results
- Artificial curiosity alone does not drive agents to prefer social interaction over inanimate objects
- Reciprocal understanding rewards (imitation + influence/impressionability) successfully lead agents to prioritize interaction with each other
- Intrinsic motivation enables cooperation in asymmetric tasks where only one agent receives extrinsic reward
- The impressionability component specifically is necessary for successful asymmetric cooperation

## Why This Works (Mechanism)

### Mechanism 1: Reciprocal Imitation Rewards
Two complementary reward functions analyze a 10-step interaction history. The "promote imitation" function rewards an agent for actively creating a crossing pattern (first 5 steps) that is then passively observed as matching (second 5 steps). The "imitation" function rewards the reciprocal pattern. Only crossing transitions (0→1, 1→0) are compared. Agents can implicitly distinguish self-caused from other-caused observations through temporal alignment of active vs. passive phases.

### Mechanism 2: Mutual Information-Based Influence and Impressionability
A rolling buffer of 20 4-timestep chunks tracks observations and actions. Impressionability reward = MI(passive observations → active crossings). Influence reward = MI(active crossings → passive observations). Agents sum these, seeking to control and be controlled. Dirichlet priors (pseudo-counts) prevent reward gaming from sparse statistics. High mutual information indicates meaningful social coordination rather than coincidental correlation.

### Mechanism 3: Intrinsic Reward Transfer for Asymmetric Cooperation
When one agent has extrinsic reward for the other's location and the other has only intrinsic motivation for being influenced (impressionability), cooperation emerges. Agent0 learns to "pay" Agent1 in intrinsic reward by producing influenceable patterns. Agent1 follows because impressionability is intrinsically rewarding. This bootstraps cooperation without shared extrinsic reward.

## Foundational Learning

- **Reinforcement Learning with Intrinsic Rewards**
  - Why needed here: All mechanisms add intrinsic reward terms to PPO objective; understanding how shaping rewards interact with policy optimization is essential
  - Quick check question: Why can intrinsic rewards cause "reward hacking" where agents optimize the intrinsic signal at the expense of task performance?

- **Mutual Information Estimation from Samples**
  - Why needed here: The influence/impressionability mechanism requires calculating MI from rolling buffers of discrete events with pseudo-count regularization
  - Quick check question: What problem do Dirichlet priors (pseudo-counts) solve when estimating MI from limited samples?

- **Active Inference / Prediction-Error-Based Curiosity**
  - Why needed here: The "drive to understand" uses an active-inference-style reward that maximizes surprise under high confidence
  - Quick check question: Why does rewarding prediction errors when confidence is high lead to exploration rather than exploitation?

## Architecture Onboarding

- **Component map:** Observation → LSTM policy → Action → Environment → Crossing signal → Rolling buffers → Imitation/MI rewards → Combined reward → PPO update

- **Critical path:** 1. Observation → LSTM policy → discrete action 2. Environment step → crossing signal 3. Rolling buffers updated 4. Imitation and/or MI rewards calculated 5. Combined reward → PPO update

- **Design tradeoffs:** Hardcoded crossing transitions as MI features vs. learned representations (paper notes this limitation); buffer sizes (10 steps for imitation, 20 chunks for MI) chosen empirically; symmetric vs. asymmetric reward: removing impressionability preserves social preference but breaks asymmetric cooperation

- **Failure signatures:** Agents spending time at fixed objects/shadows → artificial curiosity alone; highly stereotyped oscillation patterns → imitation reward overfitting; Agent1 not cooperating in asymmetric task → impressionability component missing; MI reward unstable early → pseudo-counts too low

- **First 3 experiments:** 1. Baseline replication (artificial curiosity only): Verify no preference for other-agent crossings 2. Full influence+impressionability: Confirm social crossing preference emerges 3. Asymmetric task ablation: Run signal-bit location task with/without impressionability component

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed intrinsic motivations scale from dyadic primary intersubjectivity to secondary intersubjectivity involving triadic agent-agent-object interactions?
- Basis in paper: The authors state they focus purely on primary intersubjectivity and leave the "more stringent requirement" of co-inventing external representations for "further work."
- Why unresolved: The current Perceptual Crossing Paradigm explicitly excludes a third object to prevent joint attention tasks, limiting the study to bodily coordination.
- What evidence would resolve it: Successful emergence of referential communication or joint attention protocols in an environment containing shared objects.

### Open Question 2
- Question: Do the learned social policies generalize to novel partners, or are they overfitted to a specific co-training agent?
- Basis in paper: The authors note the agents learn an "other-specific policy" and suggest that achieving an "agent-general policy" via training with random 2-of-N agents is "left for further work."
- Why unresolved: Current simultaneous training (self-play) may result in "private" coordination scripts that fail with unfamiliar agents.
- What evidence would resolve it: Zero-shot coordination results where agents trained in a population successfully interact with unseen partners.

### Open Question 3
- Question: How is the establishment of mutual awareness affected when agents have mismatched intrinsic motivation mechanisms?
- Basis in paper: The paper notes regarding the calculation of mutual information: "We have not explored the effects of mismatch... between the two agents, but leave this for further work."
- Why unresolved: The study assumes symmetric reward functions, but biological systems likely possess asymmetries in social processing.
- What evidence would resolve it: Experiments pairing agents with different hyperparameters for influence/impressionability rewards to observe if coordination collapses or adapts.

### Open Question 4
- Question: Can artificial agents with these intrinsic rewards establish mutual awareness with human partners in real-time interactions?
- Basis in paper: The authors suggest it "would be interesting to test the learned general policies in interaction with real humans on the PCP" provided latency issues are resolved.
- Why unresolved: The complexity and noise of human behavior may not satisfy the rigid temporal constraints of the current mutual information reward calculations.
- What evidence would resolve it: Successful real-time human-agent interaction in the perceptual crossing task where the human subject reports a sense of social connection.

## Limitations
- Hardcoded crossing transitions as features for imitation and MI rewards restricts generality
- Perceptual crossing paradigm is highly constrained compared to real-world social interaction
- Agents may exploit reward structure through stereotyped patterns rather than genuine understanding

## Confidence
- **High**: The finding that artificial curiosity alone does not drive social preference is well-established through direct comparison in Figure 5
- **Medium**: The demonstration that reciprocal understanding rewards successfully drive social interaction is convincing within the perceptual crossing paradigm but may not generalize to more complex tasks
- **Medium**: The asymmetric cooperation results show the mechanism works but rely heavily on the specific design of the impressionability reward component

## Next Checks
1. Test the artificial curiosity baseline with varied exploration strategies to determine if different curiosity formulations could still produce social preference
2. Conduct ablation studies removing either the influence or impressionability component individually to quantify their relative contributions to social interaction and cooperation
3. Evaluate whether the same intrinsic motivation framework can sustain cooperation in tasks where both agents have extrinsic rewards but with conflicting objectives, testing the robustness of the mechanism beyond the asymmetric case