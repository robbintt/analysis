---
ver: rpa2
title: Multimodal RAG for Unstructured Data:Leveraging Modality-Aware Knowledge Graphs
  with Hybrid Retrieval
arxiv_id: '2510.14592'
source_url: https://arxiv.org/abs/2510.14592
tags:
- retrieval
- multimodal
- data
- knowledge
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of effectively retrieving and
  reasoning over unstructured multimodal documents, which combine text, images, tables,
  equations, and graphs, using Retrieval-Augmented Generation (RAG) systems. Current
  RAG systems are primarily unimodal and struggle with cross-modal dependencies.
---

# Multimodal RAG for Unstructured Data:Leveraging Modality-Aware Knowledge Graphs with Hybrid Retrieval

## Quick Facts
- **arXiv ID**: 2510.14592
- **Source URL**: https://arxiv.org/abs/2510.14592
- **Reference count**: 19
- **Primary result**: MAHA achieves ROUGE-L score of 0.486 and complete modality coverage (1.00), substantially outperforming baseline methods.

## Executive Summary
This paper addresses the challenge of effectively retrieving and reasoning over unstructured multimodal documents, which combine text, images, tables, equations, and graphs, using Retrieval-Augmented Generation (RAG) systems. Current RAG systems are primarily unimodal and struggle with cross-modal dependencies. To overcome this, the authors propose MAHA, a Modality-Aware Hybrid retrieval Architecture. MAHA integrates dense vector retrieval with structured graph traversal over a modality-aware knowledge graph that captures cross-modal semantics and relationships. Evaluations on benchmark datasets demonstrate that MAHA substantially outperforms baseline methods, achieving a ROUGE-L score of 0.486 and complete modality coverage (1.00). These results highlight MAHA’s ability to combine semantic embeddings with explicit document structure, enabling effective multimodal retrieval and reasoning.

## Method Summary
The MAHA system processes multimodal documents by first parsing them into chunks of different modalities (text, tables, images, equations, graphs). Each modality is encoded using modality-specific models: text uses OpenAI text-embedding-3-small, images use CLIP, tables are converted to HTML, and equations to LaTeX. These embeddings are indexed in a vector store for semantic retrieval. Simultaneously, a modality-aware knowledge graph is constructed where nodes represent chunks and edges capture cross-modal relationships (e.g., HAS-IMAGE, HAS-TABLE, NEXT-FORMULA). During query processing, MAHA performs parallel dense vector similarity search and graph traversal, fusing the results before feeding them to an LLM for answer generation.

## Key Results
- MAHA achieves a ROUGE-L score of 0.486, representing a 72% improvement over vector-only retrieval and a 44% improvement over graph-only retrieval.
- The system attains complete modality coverage (1.00), ensuring retrieved answers span all modalities present in the source documents.
- MAHA demonstrates strong recall and ranking performance with Recall@3 of 0.79 and MRR of 0.74.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modality-aware knowledge graphs enable cross-modal retrieval that pure vector similarity cannot achieve alone.
- Mechanism: The knowledge graph explicitly encodes cross-modal relationships using edges such as HAS-IMAGE, HAS-TABLE, NEXT-FORMULA, and NEXT-TEXT. These edges capture structural and semantic dependencies between modalities that would be invisible to dense vector similarity alone, allowing retrieval to traverse from a text chunk to its supporting table or equation.
- Core assumption: Documents have inherent cross-modal structure that can be extracted during parsing and represented as explicit graph relationships.
- Evidence anchors:
  - [abstract] "the knowledge graph encodes cross-modal semantics and relationships"
  - [section 3.1] "The nodes in the graph represent entities such as text, equations, images, and tables. The edges in the graph capture semantic relationships such as... HAS-IMAGE, HAS-TABLE, HAS-FORMULA"
  - [section 5.3] Ablation shows graph-only achieves ROUGE-L 0.337 vs vector-only 0.282, confirming structural relations provide complementary information
  - [corpus] mKG-RAG paper (0.649 FMR) supports multimodal KG-enhanced retrieval for visual QA tasks
- Break condition: If document parsing fails to correctly identify modality boundaries or if cross-modal relationships are mislabeled, graph traversal will retrieve irrelevant context, degrading rather than improving retrieval quality.

### Mechanism 2
- Claim: Hybrid retrieval combining dense vector search with graph traversal yields higher retrieval coverage and ranking quality than either method alone.
- Mechanism: Dense vectors capture semantic similarity (finding conceptually related content), while graph traversal follows explicit structural links (finding contextually related content). The fusion strategy integrates both without sacrificing relevance or coverage, ensuring answers spread across related sections or modalities are surfaced.
- Core assumption: The fusion strategy correctly balances when to prioritize semantic similarity versus structural traversal for a given query type.
- Evidence anchors:
  - [abstract] "MAHA integrates dense vector retrieval with structured graph traversal"
  - [section 3.1] "Combining these approaches ensures both modality coverage and contextual depth"
  - [section 5.3] MAHA achieves ROUGE-L 0.486 (72% gain over vector-only, 44% over graph-only), Recall@3 0.79, MRR 0.74; all exceed individual components
  - [corpus] LinearRAG paper discusses graph-based RAG for large-scale corpora, supporting graph+vector combination approaches
- Break condition: If the fusion strategy over-prioritizes one retrieval path (e.g., always favoring graph traversal), the system may miss semantically relevant but structurally distant evidence, reducing recall.

### Mechanism 3
- Claim: Modality-specific encoding preserves the distinct semantics of different content types, enabling accurate similarity matching within and across modalities.
- Mechanism: Text uses OpenAI text-embedding-3-small embeddings; images and graphs use CLIP (openai-clip-vit-base-patch32) with base64 encoding; tables are converted to HTML; equations are encoded as LaTeX. Non-textual data is also summarized and embedded. This preserves modality-specific semantics rather than forcing all content into a single text representation.
- Core assumption: Each modality's encoding model adequately captures its semantic content, and cross-modal embeddings (like CLIP) enable meaningful alignment between text and visual elements.
- Evidence anchors:
  - [section 3.1] "text chunks are converted into embeddings using language models... visual elements such as images and graphs are encoded using CLIP"
  - [section 5.1] MAHA outperforms CLIP (image-only) and BM25 (text-only), suggesting modality-specific encodings combined with cross-modal linking are effective
  - [corpus] Weak direct corpus evidence for this specific encoding strategy; neighbor papers do not detail comparable modality-specific encoding choices
- Break condition: If CLIP embeddings fail to capture domain-specific visual semantics (e.g., scientific graphs, equations rendered as images), retrieval quality for visual modalities will degrade.

## Foundational Learning

### Concept: Knowledge Graphs for Retrieval
- Why needed here: MAHA's core innovation is a modality-aware KG; understanding how entities and edges represent document structure is essential to grasping the hybrid retrieval mechanism.
- Quick check question: Given a document with a paragraph describing a table's findings, what edge type would connect the text node to the table node in MAHA's schema?

### Concept: Dense vs. Sparse Retrieval (BM25 vs. FAISS/SBERT)
- Why needed here: The paper compares against BM25 (sparse/lexical) and FAISS+SBERT (dense/semantic); understanding their tradeoffs explains why hybrid approaches are needed.
- Quick check question: Why would BM25 fail to retrieve a document using the query "revenue growth" if the document only contains "sales increase"?

### Concept: Multimodal Embeddings (CLIP)
- Why needed here: CLIP enables joint embedding of images and text, allowing cross-modal retrieval; understanding its limitations helps evaluate MAHA's visual retrieval robustness.
- Quick check question: What type of visual content might CLIP struggle to embed accurately, and how might this affect MAHA's performance on equation-heavy documents?

## Architecture Onboarding

### Component map:
Process Module -> Vectorstore Indexing + Knowledge Graph -> Query Module -> LLM Generator -> Assistant Module

### Critical path:
1. Document ingestion → multimodal parsing → modality-specific encoding
2. Dual indexing: vectorstore + knowledge graph construction with cross-modal edges
3. Query encoding → parallel retrieval (vector similarity + graph traversal)
4. Fusion of retrieval results → LLM synthesis with retrieved context and metadata
5. Response generation with modality coverage validation

### Design tradeoffs:
- **Parsing complexity vs. accuracy**: More sophisticated parsers (e.g., equation OCR, table structure detection) improve chunk quality but increase latency and error points.
- **Graph schema granularity vs. scalability**: Fine-grained edges (e.g., NEXT-FORMULA) improve precision but increase graph size and traversal cost.
- **Fusion strategy tuning**: Over-weighting vector retrieval may miss structural context; over-weighting graph traversal may miss semantically relevant but distant content.
- Assumption: The paper does not specify the exact fusion algorithm (e.g., weighted score, re-ranking), leaving this as an implementation decision.

### Failure signatures:
- **Low modality coverage (<1.0)**: Indicates graph schema or parsing is missing cross-modal edges; review edge definitions and parsing output.
- **High Recall@K but low MRR**: Suggests relevant content is retrieved but not ranked highly; fusion strategy may need tuning.
- **ROUGE-L degradation on equation-heavy documents**: May indicate LaTeX encoding or CLIP limitations for rendered equations.
- **Graph traversal returns irrelevant nodes**: Likely indicates incorrect edge creation during parsing or schema mismatch.

### First 3 experiments:
1. **Single-document baseline test**: Ingest one multimodal PDF (text + table + image); query for cross-modal information (e.g., "What does Figure 3 show about the data in Table 2?"); verify modality coverage and edge existence in KG.
2. **Ablation comparison**: Run vector-only, graph-only, and full MAHA on a small benchmark subset (e.g., 50 queries from UDA or MRAMG-Bench); compare ROUGE-L, Recall@3, and modality coverage to replicate paper findings.
3. **Edge schema validation**: Manually inspect the knowledge graph for a sample document; confirm HAS-IMAGE, HAS-TABLE, NEXT-FORMULA edges correctly link related chunks; test queries that specifically require these edges to succeed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can advanced automated methods be developed to construct knowledge graphs for highly unstructured data formats without relying on manual schema tailoring?
- Basis in paper: [explicit] The authors state, "we believe there is significant potential in exploring more advanced, automated methods for knowledge graph construction to handle highly unstructured data formats."
- Why unresolved: The current implementation necessitates extending text-centric schemas with specific modality-aware relationships (e.g., HAS-IMAGE), which may require manual intervention or tailored strategies for different document structures.
- What evidence would resolve it: A demonstration of a fully automated pipeline that generates high-quality, modality-aware knowledge graphs from unseen document layouts with minimal manual configuration.

### Open Question 2
- Question: How can a dynamic query router be designed to intelligently adapt retrieval paths based on real-time question complexity?
- Basis in paper: [explicit] The conclusion proposes that "future research could focus on developing a more dynamic query router that can intelligently adapt to the complexity of a user’s question in real-time."
- Why unresolved: The current architecture likely employs a static or less adaptive mechanism for coordinating hybrid retrieval (vector + graph), which may not be optimal for all query types.
- What evidence would resolve it: Comparative analysis showing improved latency or accuracy when using a complexity-aware router versus the standard MAHA retrieval mechanism on a diverse query dataset.

### Open Question 3
- Question: What is the optimal fusion strategy for balancing semantic vector retrieval and structural graph traversal to maximize relevance?
- Basis in paper: [inferred] The authors identify "Coordinating hybrid retrieval" as a key challenge, noting the difficulty in "Balancing when to prioritize semantic vector retrieval versus graph traversal."
- Why unresolved: While the proposed fusion strategy succeeded, the authors frame the balance as a challenge they had to design for, leaving the theoretical optimality of this trade-off unexplored.
- What evidence would resolve it: Ablation studies varying the weights of vector versus graph signals in the fusion layer to identify a theoretical performance upper bound across different multimodal benchmarks.

## Limitations
- **Fusion strategy unspecified**: The paper mentions a fusion strategy but does not detail the exact algorithm used to balance vector and graph retrieval.
- **Parsing heuristics unclear**: The logic for chunk boundaries and edge creation (e.g., linking images to specific text) is not fully specified.
- **Limited corpus validation**: While results are strong, direct validation of modality-specific encoding (especially for equations and scientific visuals) is sparse in the literature.

## Confidence
- **Knowledge Graph Mechanism**: High - Clear evidence from ablation showing graph-only outperforms vector-only and achieves complementary results.
- **Hybrid Retrieval Performance**: High - Substantial empirical gains (72% ROUGE-L improvement) with controlled ablation experiments.
- **Modality-Specific Encoding**: Medium - Effective in practice, but limited direct corpus validation for domain-specific visual content like equations.
- **Overall System Claims**: High - Multiple metrics (ROUGE-L, Recall@3, MRR, modality coverage) consistently support the paper's conclusions.

## Next Checks
1. Reproduce single-document baseline test with one multimodal PDF to verify modality coverage and KG edge construction.
2. Run ablation comparison on a small benchmark subset (50 queries) to replicate ROUGE-L, Recall@3, and modality coverage improvements.
3. Validate edge schema construction by manually inspecting the KG for sample documents and testing queries requiring specific cross-modal edges.