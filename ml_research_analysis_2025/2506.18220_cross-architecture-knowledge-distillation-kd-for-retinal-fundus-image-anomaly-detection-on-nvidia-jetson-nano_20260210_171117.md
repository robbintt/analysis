---
ver: rpa2
title: Cross-Architecture Knowledge Distillation (KD) for Retinal Fundus Image Anomaly
  Detection on NVIDIA Jetson Nano
arxiv_id: '2506.18220'
source_url: https://arxiv.org/abs/2506.18220
tags:
- student
- images
- training
- teacher
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addressed the challenge of deploying high-performance
  retinal disease detection models on resource-constrained edge devices like the NVIDIA
  Jetson Nano. It proposed a cross-architecture knowledge distillation framework that
  transfers diagnostic expertise from a large Vision Transformer (ViT) teacher model
  to a compact CNN student model, achieving a 97.4% reduction in parameters while
  retaining 93% of the teacher's diagnostic performance.
---

# Cross-Architecture Knowledge Distillation (KD) for Retinal Fundus Image Anomaly Detection on NVIDIA Jetson Nano

## Quick Facts
- arXiv ID: 2506.18220
- Source URL: https://arxiv.org/abs/2506.18220
- Reference count: 0
- Primary result: 89% student accuracy (93% retention) with 97.4% parameter reduction for retinal disease detection on Jetson Nano

## Executive Summary
This study tackles the challenge of deploying high-performance retinal disease detection models on resource-constrained edge devices like the NVIDIA Jetson Nano. The authors propose a cross-architecture knowledge distillation framework that transfers diagnostic expertise from a large Vision Transformer (ViT) teacher model to a compact CNN student model. By employing specialized projectors for attention and feature alignment, along with multi-view robust training, the framework achieves 89% overall accuracy across four diagnostic categories (Normal, Diabetic Retinopathy, Glaucoma, Cataract) while reducing parameters by 97.4%. This compression enables scalable, AI-driven triage solutions for under-resourced clinical settings.

## Method Summary
The framework transfers knowledge from a ViT-base teacher (pre-trained with I-JEPA self-supervised learning, then fine-tuned) to a CNN student via three specialized loss components: Partially Cross-Attention (PCA) projector learns to mimic transformer-style attention patterns through KL divergence; Group-Wise Linear (GL) projector aligns heterogeneous feature representations using channel-grouped linear transformations; and a multi-view robust training method with adversarial feature matching improves generalization. The total loss combines these with standard cross-entropy. The student model (MobileNetV2 or similar) is trained on 6,727 fundus images across four diagnostic classes, achieving 89% accuracy while reducing parameters from 85.8M to 2.2M.

## Key Results
- Student model achieves 89% overall accuracy on retinal fundus image classification (vs. 92.87% for teacher)
- 97.4% reduction in parameters (85.8M → 2.2M) enables deployment on Jetson Nano
- 93% retention of teacher performance while enabling edge inference
- Performance varies by class: Normal (97.4%), DR (91.6%), Glaucoma (68.7%), Cataract (81.6%)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Partially Cross-Attention (PCA) projector enables a CNN student to approximate transformer-style global attention patterns by learning to mimic the teacher's attention distributions.
- **Mechanism:** Three parallel convolutional layers project CNN feature maps into query (Q), key (K), and value (V) representations. Student attention maps are computed via scaled dot-product attention and aligned to teacher attention maps through KL divergence loss: $L_{PCA} = D_{KL}(A_t || A_s)$. This forces the CNN to learn which spatial regions should influence each other, compensating for CNNs' inherent local receptive field bias.
- **Core assumption:** That attention distributions—rather than raw feature values—encode the most transferable diagnostic knowledge between architectures.
- **Evidence anchors:** [abstract] "Partitioned Cross-Attention (PCA) projector"; [section 2.3.1] "The PCA loss minimized the KL divergence between the attention distributions"; [corpus] Hybrid Knowledge Transfer paper (arxiv:2504.16128) confirms attention-based distillation improves ViT→CNN transfer in agricultural IoT.

### Mechanism 2
- **Claim:** The Group-Wise Linear (GL) projector aligns heterogeneous feature spaces between CNN and ViT architectures while maintaining parameter efficiency.
- **Mechanism:** CNN features are segmented into g groups along the channel dimension. Each group undergoes a separate learnable linear transformation: $GL(F_s) = Concat[W_1 F_s^1, W_2 F_s^2, ..., W_g F_s^g]$. The GL loss minimizes MSE between projected student features and spatially-resized teacher features. This grouped approach reduces parameters compared to full linear projection, making it suitable for edge deployment.
- **Core assumption:** That channel groups can be independently transformed and that spatial resizing preserves semantic alignment between architectures.
- **Evidence anchors:** [abstract] "Group-Wise Linear (GL) projector, which aligns the heterogeneous feature representations between architectures"; [section 2.3.2] "The group-wise format has significantly lower parameter count than the full linear projection, it's good for device deployment".

### Mechanism 3
- **Claim:** Multi-view robust training with an adversarial component improves generalization to varied imaging conditions by forcing the student to produce teacher-like feature distributions across augmentations.
- **Mechanism:** Each input generates multiple views via cropping, scaling, and color augmentation. A discriminator network D attempts to distinguish teacher from student features, while the student is trained to fool D via adversarial loss: $L_{adv} = E[\log D(F_t) + \log(1 - D(GL(F_s)))]$. This distributional matching complements point-wise losses (PCA, GL).
- **Core assumption:** That augmentations used during training reflect the variability encountered in real-world clinical imaging.
- **Evidence anchors:** [section 2.3.3] "A discriminator D attempts to differentiate between teacher and student features, which the student aims to fool"; [section 4.1.4.4] "All losses are averaged over views, and the total loss is: total_loss = PCA loss + λ × GL loss + robust loss".

## Foundational Learning

- **Concept:** Self-supervised learning with masked prediction (I-JEPA)
  - **Why needed here:** The teacher ViT is pre-trained using I-JEPA to learn semantic representations from unlabeled fundus images before supervised fine-tuning. This addresses the scarcity of labeled medical data.
  - **Quick check question:** Can you explain why predicting masked regions in latent space (rather than pixel space) might preserve clinically relevant features better than reconstruction-based pre-training?

- **Concept:** Knowledge distillation fundamentals (soft targets, temperature scaling)
  - **Why needed here:** The paper extends traditional KD to cross-architecture settings. Understanding baseline KD (Hinton et al. 2015) is prerequisite to appreciating why direct logit matching fails between ViT and CNN.
  - **Quick check question:** Why would soft targets from a ViT teacher be more informative than hard labels for training a CNN student?

- **Concept:** Architectural inductive biases: CNN locality vs. Transformer global attention
  - **Why needed here:** The core challenge is bridging fundamentally different feature extraction paradigms. CNNs process images through local convolutional kernels; ViTs use global self-attention over patch sequences.
  - **Quick check question:** What specific diagnostic patterns in retinal images might require global context (transformer advantage) versus local texture analysis (CNN advantage)?

## Architecture Onboarding

- **Component map:**
  Input Image → [Multi-View Generator] → Multiple Augmented Views → [Teacher ViT] → Teacher Features + Attention Maps → [GL Projector] → Aligned Features → [Loss Computation] → [Student Update Only]
                                        ↓                                           ↓
                    [Student CNN]                             [PCA Projector] → Student Attention
                    ↓                                           ↓
            Student Features                         [GL Projector] → Projected Features
                                                        ↓
                                                [Loss Computation]

- **Critical path:**
  1. Teacher ViT must be properly pre-trained (I-JEPA SSL → supervised fine-tuning) before distillation begins.
  2. Feature extraction hooks must be correctly registered on both models to capture intermediate representations.
  3. PCA projector requires extracting teacher attention maps from the final transformer encoder.
  4. GL projector must handle channel dimension mismatches via padding without introducing artifacts.
  5. Loss weighting (α, λ₁, λ₂) must be tuned.

- **Design tradeoffs:**
  - Student architecture selection: MobileNetV2 chosen for edge deployment among tested options.
  - Group count in GL projector: Higher g increases expressiveness but also parameters.
  - Number of views in multi-view training: More views improve robustness but increase training time.
  - Augmentation strength: Must balance clinical relevance preservation with generalization gains.

- **Failure signatures:**
  - Student accuracy stuck at ~20%: suggests distillation signals not propagating.
  - Large accuracy gap between training and validation: overfitting to training augmentations.
  - Confusion between Cataract and Glaucoma: shared image characteristics may require additional discriminative features.

- **First 3 experiments:**
  1. **Baseline sanity check:** Train student CNN from scratch (no KD) on the same dataset. Confirm paper's reported 20% baseline.
  2. **Ablation on projectors:** Train three variants—PCA only, GL only, both combined. Isolate each projector's contribution to the 69 percentage point improvement.
  3. **Teacher quality sensitivity:** Train with a randomly-initialized teacher (no I-JEPA pre-training) versus the pre-trained teacher. Quantify how much SSL pre-training contributes to final student performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific clinical or technical factors cause the significant performance discrepancy in Glaucoma classification (68.7% accuracy) compared to Normal (97.4%) or DR (91.6%) cases?
- Basis in paper: [explicit] The authors state in Section 5.3 that "These differences call for clinical and technical investigation to determine the cause," specifically noting the confusion between Cataract and Glaucoma.
- Why unresolved: The paper hypothesizes reasons (shared image characteristics, class imbalance) but does not perform the ablation studies or feature analysis required to isolate the root cause.
- What evidence would resolve it: Ablation studies controlling for class balance and feature saliency maps (e.g., GradCAM) highlighting the specific anatomical regions causing confusion for the student model.

### Open Question 2
- Question: Can the framework be extended to handle simultaneous multi-anomaly detection using object detection architectures rather than single-label classification?
- Basis in paper: [explicit] Section 6 asks, "We can try to perform a kind of object detection with multiple classes and bounding boxes. However, getting the data for such a model is a challenging task."
- Why unresolved: The current implementation is limited to single-label classification and cannot detect co-occurring diseases or localize anomalies.
- What evidence would resolve it: A modified architecture with bounding box outputs trained on a multi-label dataset, evaluating if the cross-architecture distillation holds for localization tasks.

### Open Question 3
- Question: Does applying correct tensor quantization significantly reduce the deployed model size and inference latency on the Jetson Nano compared to the unquantized student?
- Basis in paper: [inferred] Section 5.1.3 notes a caveat in Figure 23: "The model sizes were the same so quantization might not have been saved properly," leaving the true benefits of quantization unverified.
- Why unresolved: The technical implementation of quantization failed, resulting in identical file sizes for quantized and standard models.
- What evidence would resolve it: Re-running the Jetson Nano deployment with verified INT8 quantization to measure actual model size reduction (MB) and FPS throughput improvement.

## Limitations

- **Dataset provenance**: The 6,727 fundus images are not attributed to a specific public dataset, raising questions about reproducibility and generalizability.
- **Hyperparameter opacity**: Key training parameters (λ₁, λ₂, α, group count g, number of views) are unspecified, making exact replication difficult.
- **Model architecture selection**: While multiple CNN options are tested, the final student architecture used for reported results is not stated.
- **Quantization efficacy**: The paper notes that standard post-training quantization failed to compress the student model, yet does not explore quantization-aware training or alternative compression techniques.

## Confidence

- **High confidence**: The core cross-architecture KD framework (PCA, GL, adversarial components) is well-defined and mechanistically sound. The 89% student accuracy and 93% retention metric are directly stated.
- **Medium confidence**: The claimed 97.4% parameter reduction depends on the unspecified CNN architecture choice. The effectiveness of I-JEPA pre-training is supported by SSL literature but not independently validated here.
- **Low confidence**: Claims about model robustness to real-world clinical variability are inferred from augmentation strategy but not empirically tested on out-of-distribution data.

## Next Checks

1. **Ablation study on projector components**: Train and evaluate student models with only PCA loss, only GL loss, and the full combined loss. Quantify the individual and synergistic contributions of each projector to the 69 percentage point improvement over the baseline.

2. **Teacher quality sensitivity analysis**: Compare student performance when trained with the pre-trained I-JEPA teacher versus a randomly initialized teacher. Measure the impact of self-supervised pre-training on final diagnostic accuracy.

3. **Edge deployment stress test**: Deploy the student model on an actual NVIDIA Jetson Nano and measure inference latency, memory usage, and thermal performance under continuous operation. Verify that the model meets real-time clinical screening requirements.