---
ver: rpa2
title: Replay Can Provably Increase Forgetting
arxiv_id: '2506.04377'
source_url: https://arxiv.org/abs/2506.04377
tags:
- forgetting
- replay
- task
- samples
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proves that experience replay, a standard technique
  for mitigating catastrophic forgetting in continual learning, can actually increase
  forgetting in certain settings. The authors provide both theoretical and empirical
  evidence for this surprising phenomenon.
---

# Replay Can Provably Increase Forgetting

## Quick Facts
- arXiv ID: 2506.04377
- Source URL: https://arxiv.org/abs/2506.04377
- Reference count: 40
- Primary result: Experience replay can increase forgetting in continual learning settings

## Executive Summary
This paper proves that experience replay, a standard technique for mitigating catastrophic forgetting in continual learning, can actually increase forgetting in certain settings. The authors provide both theoretical and empirical evidence for this surprising phenomenon. They show that replay's effectiveness depends critically on the geometric relationship between task subspaces and the specific samples chosen for replay, with some task sequences showing increased forgetting when replay is applied.

## Method Summary
The paper analyzes sample replay in continual linear regression where tasks share a common solution. Two main settings are considered: worst-case (adversarial replay samples) and average-case (randomly chosen replay samples from specific task subspaces). Theoretical analysis uses over-parameterized linear regression with gradient descent converging to minimum-norm solutions. Empirical validation uses MLPs trained on linear task sequences and MNIST classification tasks with various replay strategies.

## Key Results
- Adversarially chosen replay samples can transform vanishing forgetting (O(1/T)) into catastrophic forgetting (Θ(1))
- Randomly chosen replay samples from specific task subspaces can increase forgetting in expectation, even when tasks are relatively close under natural distance metrics
- Replay effectiveness is highly dependent on task relationships and sample selection, with non-monotonic effects as replay sample count increases
- Similar non-monotonic forgetting patterns observed when training nonlinear MLPs on sequences of linear tasks

## Why This Works (Mechanism)

### Mechanism 1: Null Space Projection Accumulation
- Claim: Forgetting results from sequential orthogonal projections of parameter error into task null spaces
- Mechanism: Parameter error evolves as wt - w* = Pt(wt-1 - w*), where Pt is projection onto task t's null space
- Core assumption: Over-parameterization (kt := rank(Xt) < d) and realizability (all tasks share common solution w*)
- Evidence: Equation (2) showing wt - w* = Pt(wt-1 - w*) and Equation (4) forgetting formula
- Break condition: When tasks have orthogonal null spaces (principal angle → 0), forgetting vanishes

### Mechanism 2: Replay Modifies Effective Task Subspaces
- Claim: Replay samples modify task 2's effective null space in direction-dependent manner
- Mechanism: Replay samples from task 1 projected into task 2's null space with Gaussian-distributed directions
- Core assumption: Random replay sample selection; replay reduces task 2's null space rank
- Evidence: Theorem 3.5 showing random replay can increase forgetting in expectation
- Break condition: Principal angles > π/4 guarantee benign replay (Proposition 3.6)

### Mechanism 3: Intra-Task Sample Interference Amplification
- Claim: Replay introduces intra-task interference previously absent, harming non-replayed samples
- Mechanism: Partial replay creates gradient updates that harm non-replayed samples from same task
- Core assumption: Replay uses m < d samples per task
- Evidence: Theorem 3.2 showing adversarial replay can cause catastrophic forgetting
- Break condition: If forgetting without replay is already zero, replay cannot harm

## Foundational Learning

- Concept: **Orthogonal Projections and Null Spaces**
  - Why needed: Theoretical framework uses projections onto row spans (Πt) and null spaces (Pt)
  - Quick check: Given matrix X with rank k < d, what is dimension of its null space?

- Concept: **Principal Angles Between Subspaces**
  - Why needed: Forgetting depends non-monotonically on angle between task null spaces, maximizing near π/4
  - Quick check: If two 1D null spaces have angle θ between them, what is ∥P2P1∥op?

- Concept: **Minimum-Norm Solutions in Over-Parameterized Regression**
  - Why needed: GD/SGD implicitly finds solution closest to initialization, determining error evolution
  - Quick check: Why does gradient descent from zero initialization yield minimum-norm solution?

## Architecture Onboarding

- Component map: Task generator -> Replay buffer -> Continual learner -> Forgetting evaluator
- Critical path: Construct task sequence → Train on task 1 → Train on task 2 with replay → Measure forgetting on task 1
- Design tradeoffs:
  - Number of replay samples: Non-monotonic effect (small m can increase forgetting, larger m decreases it)
  - Sample selection strategy: Random vs. adversarial vs. gradient-based
  - Task relationship: Tasks with principal angles > π/4 guarantee benign replay
- Failure signatures:
  - Forgetting increases with 1-3 replay samples then decreases
  - High variance in replay effectiveness across sample choices
  - Particular task orderings show systematic harm
- First 3 experiments:
  1. Replicate 3D average case construction: Create two tasks in R³ with specific null space geometry, verify non-monotonic forgetting with replay
  2. Angle sensitivity test: Vary angle between two 1D null spaces from 0 to π/2, measure forgetting with/without single-sample replay
  3. MNIST class-dependent replay: For Rotated MNIST, replay single samples from each digit class, plot forgetting reduction per class

## Open Questions the Paper Calls Out

1. What are necessary and sufficient conditions under which replay is guaranteed to be benign?
2. How can we develop methods to identify task sequences in datasets where replay could increase forgetting before training?
3. How do regularization techniques and training hyperparameters affect replay-induced forgetting?

## Limitations

- Theoretical analysis is highly specific to over-parameterized linear regression setting
- Results may not generalize to practical neural network scenarios with unconstrained architectures
- Non-monotonic replay effects (1-2 samples increase forgetting, more samples decrease it) are counterintuitive and may be difficult to reproduce
- Paper proves replay can be harmful or beneficial depending on conditions, not that it's universally harmful

## Confidence

**High confidence**: Theoretical results on linear regression forgetting mechanisms (orthogonal projections, subspace angle dependence)
**Medium confidence**: Average-case results showing random replay can increase forgetting in expectation
**Medium confidence**: Empirical results on MLPs showing similar non-monotonic behavior

## Next Checks

1. **Angle Sensitivity Verification**: Systematically vary principal angle between two 1D null spaces from 0 to π/2 and measure forgetting with/without single-sample replay to verify π/4 peak
2. **Replay Sample Count Sensitivity**: For average-case 3D construction, measure forgetting across broader range of replay sample counts (m = 0, 1, 2, 4, 8, 16) to confirm non-monotonic relationship
3. **Cross-Dataset Generalization**: Test phenomenon on third dataset (e.g., Fashion-MNIST or CIFAR-10) with task-incremental learning to verify non-monotonic replay effects generalize beyond MNIST