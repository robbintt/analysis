---
ver: rpa2
title: 'Thought of Search: Planning with Language Models Through The Lens of Efficiency'
arxiv_id: '2404.11833'
source_url: https://arxiv.org/abs/2404.11833
tags:
- state
- block
- word
- states
- successor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the gap in analyzing soundness, completeness,
  and efficiency of LLM-based planning methods. It proposes using LLMs to generate
  code for search components (successor function and goal test) rather than calling
  LLMs at every search step.
---

# Thought of Search: Planning with Language Models Through The Lens of Efficiency

## Quick Facts
- arXiv ID: 2404.11833
- Source URL: https://arxiv.org/abs/2404.11833
- Reference count: 40
- Uses LLMs to generate search code components, achieving 100% accuracy on four domains with minimal LLM calls

## Executive Summary
This paper addresses the inefficiency of LLM-based planning methods that call language models at every search step. Instead of using LLMs for reasoning at each state expansion, the authors propose using LLMs to generate Python code for critical search components (successor functions and goal tests). This shift from per-step LLM calls to one-time code generation dramatically improves efficiency while inheriting soundness and completeness guarantees from standard search algorithms. Experiments on 24 Game, Mini Crosswords, BlocksWorld, and PrOntoQA show the approach solves entire datasets in seconds with 100% accuracy, compared to days and thousands of dollars for existing methods.

## Method Summary
The method uses LLMs to generate Python implementations of successor functions and goal tests based on natural language problem descriptions. A human iteratively validates and refines the generated code until it correctly captures all state transitions and goal conditions. The validated code is then plugged into standard search algorithms (BFS/DFS) to solve individual problem instances. This approach shifts the complexity from O(b×L×T) per-step LLM calls to O(1) per domain code generation, while maintaining the theoretical guarantees of the underlying search algorithms.

## Key Results
- Solves 1362 instances of 24 Game in 6.83 seconds with 100% accuracy using only 2 LLM calls
- Completes 20 Mini Crossword puzzles in 1.92 seconds with 100% accuracy
- Solves 502 BlocksWorld instances in 0.56-9.7 seconds with 100% accuracy
- Demonstrates 1000x speedup and cost reduction compared to per-step LLM approaches

## Why This Works (Mechanism)

### Mechanism 1: Code Generation for Search Components
LLMs can produce correct Python implementations of successor functions and goal tests when given natural language problem descriptions. The LLM translates textual specifications into executable code that defines state transitions and goal conditions, which are then used in standard search algorithms. This works because search algorithms' theoretical guarantees transfer to the overall system when executing LLM-generated code symbolically.

### Mechanism 2: Shift from Per-Step LLM Calls to One-Time Code Generation
Moving LLM evaluation from inside the search loop to a pre-search code generation phase reduces complexity from O(b×L×T) to O(1) per domain. Traditional approaches call LLMs at each state expansion and evaluation, while this method calls the LLM only to generate reusable code that is amortized across all problem instances in a domain.

### Mechanism 3: Human-in-the-Loop Code Validation
With minor human feedback (1-3 iterations), GPT-4 can produce correct search component code. The human reviews generated code, identifies errors, and provides specific feedback. The LLM revises, and this iterative debugging converges quickly. The assumption is that a human validator can recognize correct code and provide actionable feedback.

## Foundational Learning

- **State Space Search (BFS/DFS)**: Why needed: The approach relies on plugging generated code into standard search algorithms. You must understand what successor functions and goal tests do, and how BFS/DFS guarantee completeness. Quick check: Given a state representation and successor function, can you trace the first three levels of a BFS tree?

- **LLM Prompting for Code Generation**: Why needed: Quality of generated search components depends entirely on prompt design. Quick check: Write a prompt asking an LLM for a successor function given a state representation—does it specify input/output types?

- **Soundness vs. Completeness in Search**: Why needed: The paper's main claim is restoring these properties lost in per-step LLM approaches. Quick check: If a successor function sometimes generates invalid states, which property is violated?

## Architecture Onboarding

- Component map: Natural Language Problem Description -> [LLM Query] ←→ [Human Feedback Loop] -> Generated Python Code (successor_states, is_goal) -> Standard Search Algorithm (BFS/DFS) -> Solution Plan

- Critical path: The LLM-to-code step. If generated code is incorrect, no amount of search will help. Focus validation effort here first.

- Design tradeoffs:
  - Human validation vs. automated testing: Current approach requires human; automation is future work. Trade reliability for engineering effort.
  - BFS vs. DFS: BFS guarantees optimal solutions; DFS uses less memory. Paper uses BFS for BlocksWorld (optimal plans matter), DFS for Crosswords (constraint satisfaction).
  - General code vs. domain-specific optimizations: Generated code is correct but may be inefficient (paper notes 1.92s-6.83s variance on same problem from different code variants).

- Failure signatures:
  - IndexError/string manipulation errors: LLM didn't handle edge cases (see Crossword experiments—words not exactly 5 characters)
  - Shallow copy bugs: State mutation across successor generation (see BlocksWorld experiments)
  - Missing "clear" predicate updates: Incorrect state representation after actions (BlocksWorld stacking)
  - Predicate parsing confusion: "on X Y" vs "on-table X" both start with "on"

- First 3 experiments:
  1. **24 Game validation**: Run provided successor/goal code on 1362-instance dataset. Verify 100% accuracy. Time should be <7 seconds total. This confirms basic system setup.
  2. **Manual code review**: Before running search, inspect generated successor functions. Check for: (a) all operators covered, (b) edge cases (division by zero, empty lists), (c) correct state copying. Log iteration count to convergence.
  3. **Efficiency comparison**: On BlocksWorld, count expanded states. Compare your time to paper's 0.56-9.7s range. If significantly slower, profile successor function—representation choice (strings vs. dictionaries) affects performance per Section 4.

## Open Questions the Paper Calls Out

### Open Question 1
Can the need for human feedback in validating LLM-generated search components be fully automated while maintaining soundness and completeness guarantees? Current approach requires human validation; automation is noted as future work. What evidence would resolve it: A fully automated pipeline that generates correct code with equivalent accuracy to human-validated code across diverse domains.

### Open Question 2
How effectively can LLMs generate heuristic functions or search pruning techniques to improve efficiency on larger problem instances? Current approach uses uninformed search; heuristics are left for future work. What evidence would resolve it: Experiments showing LLM-generated heuristics enabling efficient search on problems with state spaces beyond 10^6 states.

### Open Question 3
How does the LLM-generated Python code approach compare to generating symbolic planning models (e.g., PDDL) in terms of correctness, efficiency, and applicability? Paper intentionally focuses on direct code generation without empirical comparison to PDDL-based approaches. What evidence would resolve it: Head-to-head comparison on identical problem sets measuring accuracy, development effort, and runtime performance.

## Limitations
- Human-in-the-loop validation requirement limits scalability and automation potential
- Generated code efficiency varies significantly depending on implementation choices
- Only tested on domains with clear state transition rules and structured representations
- No evaluation of generalization to novel problem instances beyond training datasets

## Confidence

- **High**: Soundness and completeness claims (inherited from standard search algorithms)
- **High**: Efficiency improvements vs. per-step LLM approaches (verified through runtime comparisons)
- **Medium**: 100% accuracy claim (depends on human validation quality and code review rigor)
- **Low**: Claims about generalizability to arbitrary domains (only four domains tested)

## Next Checks

1. **Code Verification**: Independently verify that generated successor functions and goal tests correctly implement problem semantics across all test instances

2. **Runtime Scaling**: Test efficiency claims on larger datasets (10x current size) to confirm sublinear scaling behavior

3. **Error Injection**: Deliberately introduce subtle bugs in generated code to confirm the search algorithm fails appropriately rather than producing incorrect solutions