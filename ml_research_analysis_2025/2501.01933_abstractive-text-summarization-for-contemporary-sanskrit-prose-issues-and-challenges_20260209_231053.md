---
ver: rpa2
title: 'Abstractive Text Summarization for Contemporary Sanskrit Prose: Issues and
  Challenges'
arxiv_id: '2501.01933'
source_url: https://arxiv.org/abs/2501.01933
tags:
- summarization
- text
- language
- data
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis initiates abstractive text summarization for contemporary
  Sanskrit prose, a low-resource language. It develops datasets and trains ten transformer-based
  models, finding that BERT encoder-based combinations perform best for coherence
  and keyword capture, while RoBERTa and GPT-2 decoders underperform.
---

# Abstractive Text Summarization for Contemporary Sanskrit Prose: Issues and Challenges

## Quick Facts
- **arXiv ID:** 2501.01933
- **Source URL:** https://arxiv.org/abs/2501.01933
- **Reference count:** 40
- **Key outcome:** BERT encoder-based models perform best for Sanskrit abstractive summarization, with coherence and keyword capture outperforming RoBERTa and GPT-2 decoders, though factual consistency remains challenging.

## Executive Summary
This thesis pioneers abstractive text summarization for contemporary Sanskrit prose, addressing the significant challenges of low-resource, morphologically rich languages. The work develops novel datasets and trains ten transformer-based encoder-decoder models to generate coherent summaries from Sanskrit texts. Through systematic experimentation with monolingual language models (BERT, GPT-2, RoBERTa) and their combinations, the research identifies optimal architectures for Sanskrit text processing. Despite computational constraints and limited training data, the results demonstrate that Sanskrit abstractive summarization is feasible using transfer learning approaches, though challenges remain in factual consistency and content compression.

## Method Summary
The research develops a comprehensive pipeline for Sanskrit abstractive summarization, beginning with data collection from OSCAR, Wikipedia, Mann Ki Baat translations, and the Anantaa journal. The corpus undergoes preprocessing including regex-based sandhi and saṃyoga splitting to handle Sanskrit morphology. Three monolingual language models (BERT, GPT-2, RoBERTa) are pre-trained on 482,517 Sanskrit sentences using HuggingFace. Ten encoder-decoder combinations are then fine-tuned on 15,268 document-summary pairs using the Rothe et al. (2020) methodology with early stopping. Models are evaluated using ROUGE metrics (1, 2, L) and human assessment across coherence, factual consistency, and keyword capture dimensions, with the best-worst ranking method employed for comparative analysis.

## Key Results
- BERT encoder-based combinations (particularly BERT2BERT) outperform other architectures in both ROUGE scores and human evaluation metrics
- RoBERTa and GPT-2 decoder models show significantly lower performance, with RoBERTa particularly affected by BPE tokenization unsuitability
- Human evaluation reveals BERT2BERT produces the most coherent and relevant summaries, though factual consistency remains problematic across all models
- Factual consistency and compression challenges persist, with models generating overly long summaries containing hallucinated content
- Multilingual models (MURIL, mBART) fail to condition on input, producing identical outputs regardless of source text

## Why This Works (Mechanism)
The success of BERT encoder-based models stems from their bidirectional context understanding, which is crucial for Sanskrit's complex morphological structure and sandhi phenomena. The pre-training on large Sanskrit corpora enables the models to learn language-specific patterns before fine-tuning on summarization tasks. The encoder-decoder architecture allows for abstractive generation rather than mere extraction, capturing semantic relationships beyond surface-level matching. Transfer learning from pre-trained models compensates for the limited availability of parallel Sanskrit summarization data, while the early stopping mechanism prevents overfitting on the small training set.

## Foundational Learning
**Sanskrit morphology and sandhi:** Understanding the combination and splitting of phonemes in Sanskrit is essential for proper tokenization and vocabulary construction. Quick check: Verify that sandhi splitting reduces out-of-vocabulary tokens by at least 30%.
**Transformer architecture fundamentals:** Knowledge of self-attention mechanisms and positional encoding is crucial for understanding model behavior. Quick check: Confirm that attention weights show meaningful patterns across different Sanskrit script characters.
**ROUGE metric interpretation:** Understanding precision, recall, and F-measure relationships helps evaluate summary quality beyond simple overlap. Quick check: Compare ROUGE scores with human judgment correlation coefficients.
**Encoder-decoder training dynamics:** Understanding how pre-training transfers to fine-tuning tasks is key for optimization. Quick check: Monitor training loss divergence between pre-training and fine-tuning phases.
**Human evaluation methodology:** Knowledge of best-worst scaling and inter-annotator agreement is crucial for result interpretation. Quick check: Calculate Krippendorff's alpha for annotator consistency.

## Architecture Onboarding

**Component map:** Sanskrit corpus -> LM pretraining (BERT/GPT-2/RoBERTa) -> Encoder-decoder combinations -> Fine-tuning -> Evaluation (ROUGE/human)

**Critical path:** Data preprocessing (sandhi splitting) → Monolingual LM pretraining → Encoder-decoder initialization → Fine-tuning with early stopping → ROUGE evaluation → Human assessment

**Design tradeoffs:** The choice between BERT's bidirectional encoding versus GPT-2's causal generation impacts Sanskrit's morphological complexity handling. BERT2BERT prioritizes coherence over novelty (ROUGE-2), while decoder-only models sacrifice contextual understanding for generation fluency. The sandhi splitting approach balances between reducing vocabulary size and preserving linguistic meaning.

**Failure signatures:** Poor compression manifests as overly long summaries with repetitive content. Factual inconsistency appears as hallucinated entities not present in source text. Tokenization failures show as high out-of-vocabulary rates or incorrect morphological segmentation. Multilingual model failures result in identical outputs regardless of input variation.

**First experiments:**
1. Test sandhi splitting regex patterns on a small Sanskrit corpus to verify OOV reduction targets
2. Run multilingual models (MURIL, mBART) on identical inputs to confirm conditioning failure
3. Perform bootstrap resampling on ROUGE scores to establish statistical significance baselines

## Open Questions the Paper Calls Out
None

## Limitations
- Severe data constraints with only 15,268 training pairs and no validation split
- Incomplete sandhi pattern dictionary with unspecified criteria for manual corrections
- Human evaluation methodology lacks inter-annotator agreement statistics and statistical significance testing
- Compute infrastructure details omitted, making training duration estimates unreliable
- Failure of multilingual models attributed to conditioning issues without diagnostic testing

## Confidence

**High confidence:** Sanskrit abstractive summarization feasibility using transfer learning from pre-trained transformers is clearly demonstrated through consistent performance patterns.

**Medium confidence:** BERT encoder-based models outperform other combinations, though small test set size (155 pairs) and lack of statistical significance testing weaken this conclusion.

**Low confidence:** Claims about factual consistency and compression challenges are based on human evaluation without systematic error analysis or comparison to extractive baselines.

## Next Checks
1. Implement partial sandhi splitting regex patterns and evaluate OOV token rates against reported ~201K unique tokens
2. Test whether MURIL and mBART produce identical outputs for varied inputs before training to confirm conditioning failure hypothesis
3. Perform bootstrap resampling on 155 test pairs to calculate confidence intervals for ROUGE scores and human evaluation rankings