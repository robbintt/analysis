---
ver: rpa2
title: 'Programmable-Room: Interactive Textured 3D Room Meshes Generation Empowered
  by Large Language Models'
arxiv_id: '2506.17707'
source_url: https://arxiv.org/abs/2506.17707
tags:
- room
- panorama
- image
- generation
- programmable-room
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Programmable-Room is a framework for interactive generation and
  editing of 3D room meshes from natural language instructions. It decomposes the
  complex task into subtasks like generating 3D coordinates, texture images, and arranging
  furniture, supported by visual programming with LLMs.
---

# Programmable-Room: Interactive Textured 3D Room Meshes Generation Empowered by Large Language Models

## Quick Facts
- arXiv ID: 2506.17707
- Source URL: https://arxiv.org/abs/2506.17707
- Reference count: 40
- Primary result: Interactive 3D room mesh generation from natural language using visual programming and a diffusion-based panorama texture generator

## Executive Summary
Programmable-Room is a framework for interactive generation and editing of 3D room meshes from natural language instructions. It decomposes the complex task into subtasks like generating 3D coordinates, texture images, and arranging furniture, supported by visual programming with LLMs. The authors introduce PRIG, a diffusion-based model that generates panorama textures conditioned on text and multiple visual prompts (layout, depth, semantic maps), enhanced with bidirectional LSTM and multi-scale injection. Experiments show Programmable-Room outperforms state-of-the-art models in perceptual quality and 3D structure, with FID of 65.68 and KID of 0.02354 for texture generation, and PQ of 3.57 for mesh generation. The framework is highly extensible, enabling continuous improvement as new models are released.

## Method Summary
Programmable-Room leverages a visual programming approach where GPT-4 receives natural language instructions and generates a Python-like program composed of module calls. The framework includes modules for generating 3D coordinates (GenShape), creating visual prompts (GenLayout, GenDepth, GenSemantic), generating panorama textures (PRIG), and reconstructing 3D meshes (GenEmptyRoom). PRIG is a diffusion model that generates equirectangular panorama images conditioned on text and three visual prompts simultaneously. The model uses multi-scale feature injection with Feature Denormalization and an auxiliary BiLSTM-based loss to improve structural coherence. The framework also includes furniture placement using LayoutGPT and merging capabilities to combine rooms and furniture.

## Key Results
- PRIG achieves FID of 65.68 and KID of 0.02354 for panorama texture generation
- 3D mesh generation achieves PQ of 3.57 and 3DS of 3.82 in user studies
- Multi-prompt conditioning (layout, depth, semantic) significantly outperforms single-prompt approaches
- BiLSTM auxiliary loss improves structural coherence in generated panoramas

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Natural language instructions can be systematically decomposed into ordered, executable module sequences for 3D room generation.
- Mechanism: GPT-4 receives user instructions alongside pre-selected in-context examples (instruction-program pairs) and a task description labeled with roles (system/assistant/user). The LLM infers a Python-like program where each line calls a module with parsed arguments; outputs from earlier lines become inputs to subsequent lines.
- Core assumption: Descriptive module names and sufficient in-context examples enable GPT-4 to generalize to unseen instructions without fine-tuning.
- Evidence anchors:
  - [abstract]: "VP is a method that utilizes a large language model (LLM) to write a Python-like program which is an ordered list of necessary modules."
  - [section III.A]: "Programmable-Room leverages in-context learning ability of LLMs... GPT-4 can act as a specialized program generator for our framework without additional training."
- Break condition: Instructions outside the distribution of in-context examples, or ambiguous natural language that GPT-4 cannot unambiguously map to module signatures, will produce incorrect programs.

### Mechanism 2
- Claim: Conditioning panorama generation on multiple visual prompts (layout, depth, semantic maps) simultaneously yields higher fidelity and structural coherence than single-prompt conditioning.
- Mechanism: PRIG concatenates layout L, depth D, and semantic map M into a 9-channel tensor V. A multi-scale injection strategy extracts features at four resolutions (64×64 to 8×8) via feature extractors F1, F2, followed by zero convolution and Feature Denormalization (FDN) to modulate the diffusion U-Net's normalized noise features. The decoder is trained while encoder/middle block weights are frozen from Stable Diffusion.
- Core assumption: Each visual prompt provides complementary geometric information; their combination resolves ambiguities that any single prompt leaves.
- Evidence anchors:
  - [abstract]: "generates panorama images conditioned on text and visual prompts (i.e., layout, depth, and semantic map) simultaneously."
  - [section IV.E, Table III]: FID drops from 125.73 (layout only) to 65.68 (all three); KID drops from 0.23876 to 0.02354.
- Break condition: If any visual prompt is noisy or inconsistent with others (e.g., depth map contradicts layout), injected features may conflict and degrade output quality.

### Mechanism 3
- Claim: A BiLSTM-based auxiliary loss on 1D layout representations improves panorama structural coherence by capturing long-range horizontal dependencies.
- Mechanism: BiLSTM encodes panorama layout coordinates S into a 1D representation S₁D. During training, an auxiliary L2 loss L_BiLSTM = ||S₁D - Ŝ₁D||² penalizes deviation between ground-truth and predicted 1D layouts derived from the generated panorama. This loss is combined with the standard diffusion latent loss via weighted coefficients.
- Core assumption: Panorama layouts exhibit continuous left-right structure amenable to bidirectional sequence modeling; 1D encoding preserves sufficient geometric information.
- Evidence anchors:
  - [abstract]: "enhance the panorama image generation quality by optimizing the training objective with a 1D representation of a panorama scene obtained from bidirectional LSTM."
  - [Table I]: FID improves from 84.89 (w/o BiLSTM) to 65.68 (w/ BiLSTM); KID from 0.03811 to 0.02354.
- Break condition: Highly irregular or non-rectilinear room layouts may not compress cleanly to 1D, limiting the auxiliary loss's effectiveness.

## Foundational Learning

- Concept: Latent diffusion models and conditional generation
  - Why needed here: PRIG builds on Stable Diffusion's frozen encoder/middle block and requires understanding of latent spaces, conditioning mechanisms (CLIP text embeddings, visual prompt injection), and the denoising objective.
  - Quick check question: Can you explain how FDN modulates normalized features differently from standard cross-attention conditioning?

- Concept: In-context learning with LLMs
  - Why needed here: Visual Programming