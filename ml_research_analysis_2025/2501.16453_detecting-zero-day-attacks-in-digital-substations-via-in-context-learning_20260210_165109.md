---
ver: rpa2
title: Detecting Zero-Day Attacks in Digital Substations via In-Context Learning
arxiv_id: '2501.16453'
source_url: https://arxiv.org/abs/2501.16453
tags:
- weak
- attack
- data
- transformer
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes using transformer-based in-context learning
  (ICL) to detect zero-day attacks in IEC-61850 digital substations. The core idea
  is to leverage weak classifiers to generate pseudo-labels/distributions for incoming
  packets, then use a transformer model to learn and adapt to novel attacks based
  on a few examples without retraining.
---

# Detecting Zero-Day Attacks in Digital Substations via In-Context Learning

## Quick Facts
- arXiv ID: 2501.16453
- Source URL: https://arxiv.org/abs/2501.16453
- Reference count: 40
- Key outcome: Transformer-based in-context learning achieves over 85% detection accuracy on zero-day attacks in IEC-61850 digital substations while meeting 3ms latency requirements

## Executive Summary
This paper proposes using transformer-based in-context learning (ICL) to detect zero-day attacks in IEC-61850 digital substations. The approach leverages weak classifiers to generate pseudo-labels for incoming packets, then uses a transformer model to adapt to novel attacks based on a few examples without retraining. Two transformer variants are proposed: a Simple Transformer (TF) using hard labels and a Distributional Transformer (DTF) using probability distributions from weak classifiers. Experiments on the ERENO-IEC-61850 dataset demonstrate that the proposed method achieves over 85% detection accuracy on zero-day attacks, while existing baselines fail. The DTF model can be deployed in real substations with an inference time of 2.44 ms per sample on a GPU, meeting the 3 ms latency requirement.

## Method Summary
The method trains GPT-2 style decoder-only transformers to detect zero-day attacks in IEC-61850 digital substations using in-context learning. Weak classifiers (DNNs) generate pseudo-labels for incoming packets, which serve as context for the transformer. Two variants are proposed: TF uses hard labels from weak classifiers, while DTF uses full probability distributions. The transformers are trained on multi-mixed synthetic attack data to enhance ICL capability. During inference, the model processes sequences of (packet features, pseudo-labels) pairs and classifies new packets based on learned patterns. Mixed training strategies combine ground truth and weak labels during training, with optimal ratios of 40:60 for TF and 5:95 for DTF.

## Key Results
- DTF achieves over 85% detection accuracy on zero-day attacks while TF requires more shots to reach comparable performance
- Both models maintain 100% accuracy on normal traffic (zero false positives)
- DTF outperforms weak classifiers even when their accuracy drops to 40-50%
- The DTF model achieves 2.44 ms inference time per sample on GPU, meeting IEC-61850's 3ms latency requirement

## Why This Works (Mechanism)

### Mechanism 1
Transformers detect previously unseen attack types by conditioning on recent packet-label sequences without parameter updates. The model processes sequences of (packet features, pseudo-labels) as in-context data. When the query packet arrives, the transformer infers its label by recognizing pattern deviations from the contextual normal baseline—adapting to novel attacks as they appear in the context window (n-shot learning). This works because zero-day attacks share latent structural or statistical regularities that emerge when multiple attack packets accumulate in context, even if individual weak classifier predictions are noisy.

### Mechanism 2
Probability distributions from weak classifiers (DTF) provide more robust context than hard labels (TF), especially when weak classifiers are unreliable. DTF concatenates full probability vectors from W weak classifiers, preserving uncertainty information. The attention mechanism can then weight reliable classifiers more heavily per-packet, rather than committing to potentially incorrect hard labels early. This approach leverages the partial signal captured by weak classifiers even when individually inaccurate (~40-50% accuracy).

### Mechanism 3
Training diversity via multi-mixing synthetic attacks enhances ICL capability for zero-day detection. Multi-mixing creates K_syn synthetic attack classes by linearly combining features from K_ori original attacks. This exposes the transformer to a broader attack feature space during training, improving its ability to recognize novel patterns at inference. The synthetic combinations create meaningful latent representations that transfer to real zero-day attacks.

## Foundational Learning

- **In-Context Learning (ICL)**: Transformers can adapt to new tasks from input sequences alone, without gradient updates. This is central to why this approach works for zero-day detection. Quick check: Can you explain why ICL differs from fine-tuning or transfer learning?

- **IEC-61850 Protocol (GOOSE/SV messaging)**: Deployment constraints (3ms latency) and packet structures are protocol-specific. Misunderstanding these leads to infeasible system designs. Quick check: What is the maximum message transmission time for Type 1A/P2 trip applications per IEC-61850?

- **Weak Classifier Ensembling**: The transformer's context includes pseudo-labels from weak classifiers; understanding ensemble uncertainty helps diagnose detection failures. Quick check: Why might probability distributions from weak classifiers outperform hard voting?

## Architecture Onboarding

- **Component map**: Incoming packet → Preprocessing → Weak classifier inference → Batch accumulation → Transformer inference → Decision logic
- **Critical path**: Incoming packet → Preprocessing (T_pre) → Weak classifier inference → Batch accumulation (T_batch) → Transformer inference (T_average × BS) → Decision. Total latency must satisfy T_total < 3ms for IEC-61850 compliance.
- **Design tradeoffs**: TF vs DTF: TF adapts faster with more shots; DTF has higher zero-shot accuracy and tolerates weaker classifiers. Mixed training (ground truth + weak labels): Improves both zero-shot and n-shot performance but requires labeled training data; optimal mix ratio differs (40:60 for TF, 5:95 for DTF). Batch size (BS): Larger batches improve throughput but increase latency; BS=1 required to meet 3ms constraint.
- **Failure signatures**: Weak classifier accuracy drops below 40-50%: DTF performance collapses sharply. Context window too short (N << 11): Insufficient ICL signal. Ground truth ratio too high during DTF training (>10%): Model instability, degrading accuracy with shots.
- **First 3 experiments**: 
  1. Baseline validation: Train weak classifiers and transformer on ID attacks only; measure zero-shot accuracy on all 4 OOD attacks from ERENO dataset. Expect MDTF > MTF > traditional ML baselines.
  2. Sensitivity stress test: Synthetically degrade weak classifier accuracy (0.3 to 0.9) and plot zero-shot MDTF/MTF accuracy. Verify 0.4-0.5 threshold behavior.
  3. Latency compliance check: Deploy trained DTF on target hardware (GPU with TensorRT), measure T_average with BS=1. Confirm T_total < 3ms under F_GOOSE=250Hz, F_SV=4800Hz conditions.

## Open Questions the Paper Calls Out

- **Distinguishing ID vs OOD attacks**: The models currently lack explicit mechanisms to distinguish in-distribution (ID) from out-of-distribution (OOD) attacks, limiting operators' ability to select appropriate responses. Authors state: "Although our models have capability of performing multi-classification, they currently lack explicit mechanisms to distinguish in-distribution (ID) from out-of-distribution (OOD) attacks."

- **Minimum weak classifier threshold**: The critical threshold at which MDTF's parabolic performance curve collapses remains uncharacterized. Authors note "performance may vary if weak classifiers dip below the 40–50% accuracy range" but sensitivity analysis only covered 0.4–1.0 accuracy range.

- **Alternative transformer architectures**: Only GPT-2 decoder-only architecture was tested; encoder-only or more efficient architectures may offer better accuracy-latency trade-offs. Authors suggest: "Exploring other parameters such as sequence length and architectural variants could further refine detection accuracy and robustness."

## Limitations

- Critical design parameters unspecified: GPT-2 architecture details (layers, hidden dimension, attention heads), weak classifier ensemble structure, packet feature extraction method, and training hyperparameters are not provided.

- Limited empirical scope: Experiments only test four specific zero-day attacks from one dataset, raising questions about generalizability to other attack types or datasets.

- Weak baseline comparison: The claim that DTF "outperforms" baselines is not directly tested against existing IEC-61850 anomaly detection methods in the literature.

## Confidence

- **High Confidence**: The core mechanism of using transformer in-context learning for zero-day detection is well-supported by experimental results showing >85% accuracy on OOD attacks. The latency analysis appears reasonable given the 3ms IEC-61850 constraint.

- **Medium Confidence**: The comparative advantage of DTF over TF is demonstrated, but the experimental design doesn't isolate whether the probability distribution input or the training mix ratio (5:95) drives the improvement.

- **Low Confidence**: Claims about multi-mixing synthetic attacks enhancing ICL capability are supported only by a single ablation showing accuracy improvements with more synthetic classes (K_syn=700).

## Next Checks

1. **Architecture reproduction**: Implement the exact transformer architecture (GPT-2 decoder-only with N=11 context) and weak classifier ensemble on ERENO-IEC-61850 dataset. Measure zero-shot and n-shot accuracy on the four OOD attacks. Verify the 85%+ accuracy claim and the sharp performance drop below 40-50% weak classifier accuracy.

2. **Latency compliance verification**: Deploy the trained DTF model on target GPU hardware using TensorRT optimization. Measure inference time per sample with batch size 1 under realistic GOOSE (250Hz) and SV (4800Hz) traffic loads. Confirm total latency including preprocessing remains below 3ms IEC-61850 requirement.

3. **Generalization stress test**: Evaluate the trained DTF model on two additional zero-day attack datasets or synthetically generated attack types not present in ERENO-IEC-61850. Measure zero-shot accuracy to assess whether the ICL capability transfers beyond the specific OOD attacks tested in the paper.