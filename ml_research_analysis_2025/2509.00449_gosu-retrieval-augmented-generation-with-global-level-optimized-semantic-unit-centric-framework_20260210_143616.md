---
ver: rpa2
title: 'GOSU: Retrieval-Augmented Generation with Global-Level Optimized Semantic
  Unit-Centric Framework'
arxiv_id: '2509.00449'
source_url: https://arxiv.org/abs/2509.00449
tags:
- arxiv
- preprint
- gosu
- semantic
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GOSU is a retrieval-augmented generation framework that addresses
  semantic ambiguity and fragmented relationships in graph-based RAG by globally optimizing
  semantic units (SUs) and balancing fine-grained binary and coarse-grained n-ary
  relations. It extracts SUs from text chunks, merges and disambiguates them at the
  corpus level using an LLM, and constructs an SU-centric knowledge graph.
---

# GOSU: Retrieval-Augmented Generation with Global-Level Optimized Semantic Unit-Centric Framework

## Quick Facts
- arXiv ID: 2509.00449
- Source URL: https://arxiv.org/abs/2509.00449
- Reference count: 12
- Primary result: GOSU achieves average win rates of +71.0% (Comprehensiveness), +51.8% (Diversity), +69.0% (Empowerment), and +73.4% (Overall) over baselines.

## Executive Summary
GOSU addresses semantic ambiguity and fragmented relationships in graph-based RAG by globally optimizing semantic units (SUs) and balancing fine-grained binary and coarse-grained n-ary relations. The framework extracts SUs from text chunks, merges and disambiguates them at the corpus level using an LLM, and constructs an SU-centric knowledge graph. Experiments across five domains show GOSU consistently outperforms baselines like LightRAG, HiRAG, and HyperGraphRAG.

## Method Summary
GOSU operates in three stages: (1) Global-level Semantic Unit Optimization extracts candidate SUs from chunks, then aggregates and deduplicates them using cosine similarity filtering followed by LLM-based equivalence judgment; (2) SU-centric Knowledge Graph Construction builds entities, relations, and n-ary relations around each SU node; (3) Hierarchical keyword-based retrieval with SU completion combines entity, semantic, and topic-level query decomposition with graph traversal to retrieve comprehensive context for generation.

## Key Results
- Outperforms NaiveRAG, LightRAG, HiRAG, and HyperGraphRAG on five domain corpora
- Achieves +71.0% win rate on Comprehensiveness, +51.8% on Diversity, +69.0% on Empowerment, +73.4% on Overall metrics
- Shows particular strength in multi-party interaction retrieval through n-ary relation modeling

## Why This Works (Mechanism)

### Mechanism 1: Global Semantic Unit Disambiguation
The framework aggregates local SUs into a global pool and uses two-stage filtering (cosine similarity + LLM judgment) to cluster equivalent units and resolve coreferences before graph construction. This reduces fragmentation compared to purely local approaches. Break condition: If similarity threshold is too high, distinct events sharing lexical patterns may be falsely merged.

### Mechanism 2: SU-Centric Hybrid Graph Topology
GOSU structures the graph around Semantic Units as explicit nodes representing n-ary relations while retaining standard entity nodes. This hybrid approach balances coarse-grained event context with fine-grained entity precision, allowing queries to traverse directly to events or their constituent entities. Break condition: If SUs are extracted at wrong granularity, the graph becomes overly generic with star-schema-like structures.

### Mechanism 3: Hierarchical Retrieval with SU Completion
The query is decomposed into three keyword levels (Low entities, High topics, Semantic event phrases) and retrieves both specific entities and relevant SUs. "SU Completion" then traverses from these SUs to fetch associated chunks and relations missed by pure vector search. Break condition: If the query is ambiguous or LLM extracts poor keywords, irrelevant SUs may be retrieved, leading to hallucination.

## Foundational Learning

- **N-ary Relations vs. Binary Relations**
  - Why needed: GOSU solves the "binary bottleneck" where standard triplets fail to capture events involving 3+ parties
  - Quick check: How does GOSU represent "Alice, Bob, and Charlie met in Paris" differently than a standard Knowledge Graph?

- **Coreference Resolution**
  - Why needed: The "Global Optimization" step is a massive coreference resolution task determining if "the deal" in paragraph 1 is the same as "the acquisition" in paragraph 10
  - Quick check: In Section 3.1 Eq. (6), what mechanism serves as the final arbiter for deciding if two semantic units are the same?

- **Hierarchical Keyphrase Extraction**
  - Why needed: The retrieval logic splits query intent into three distinct levels required to debug retrieval success/failure patterns
  - Quick check: What are the three keyword levels extracted from a query in Section 3.3 Eq. (17)?

## Architecture Onboarding

- **Component map:** DocSplit -> LLM SemExt (Local SU extraction) -> Global Merger (Cluster/Dedup) -> EntRelExt (Entity/Relation extraction) -> Graph Assembly; KeyExt (Query -> 3-level Keywords) -> Retriever (Parallel Entity/Semantic search) -> SU Completion (Graph traversal) -> LLM Gen

- **Critical path:** The Global Semantic Unit Optimization (Step 2) is most fragile and expensive, relying on LLM judging pairs of text strings for equivalence. Failure here corrupts the entire graph structure.

- **Design tradeoffs:** Cost vs. Consistency - significantly higher indexing costs due to pairwise LLM calls; Granularity - trades flat chunk simplicity for heterogeneous graph complexity (SUs + Entities).

- **Failure signatures:** Slow Indexing - pairwise comparison scales poorly (O(N^2)); Empty Retrieval - if Retriever_sem returns nothing, LLM may have failed to extract matching semantic keywords.

- **First 3 experiments:** (1) Ablate Global Optimization - run pipeline with local SUs only to measure performance drop; (2) Stress Test Ambiguity - ingest corpus with heavy synonyms to measure node count reduction after Global Merger; (3) Cost/Latency Profile - measure TPQ during KeyExt vs AnsGen phases.

## Open Questions the Paper Calls Out

- Can GOSU effectively integrate multimodal inputs (images, tables) within the semantic unit graph to prevent information omission?
- How can GOSU enhance capacity to uncover deep chains of reasoning for complex inferential tasks beyond standard n-ary relations?
- Can computational overhead of global semantic unit optimization be reduced to facilitate scalability for large-scale corpora?

## Limitations

- Computationally expensive pairwise LLM-based semantic unit filtering that doesn't scale well to large corpora
- Unspecified critical hyperparameters (cosine similarity threshold Ï„, retrieval budgets, chunk size) making replication non-trivial
- Evaluation protocol relies on judge LLMs introducing potential subjectivity and variance

## Confidence

- **High Confidence**: General framework architecture and core mechanisms are well-specified and technically sound
- **Medium Confidence**: Specific implementation details of LLM prompts and thresholds are missing, requiring significant engineering
- **Low Confidence**: Evaluation methodology (judge prompts, corpus-level query generation) lacks sufficient detail for exact replication

## Next Checks

1. **Ablation Study**: Remove global optimization step and measure performance degradation on multi-document summarization to validate core contribution
2. **Scalability Test**: Profile indexing time and memory usage on corpora of increasing size to identify practical scaling limits
3. **Robustness Check**: Test framework on corpus with high synonymy (legal contracts) to measure deduplication effectiveness and detect false merges