---
ver: rpa2
title: Accuracy and Consumption analysis from a compressed model by CompactifAI from
  Multiverse Computing
arxiv_id: '2507.08836'
source_url: https://arxiv.org/abs/2507.08836
tags:
- tokens
- compressed
- energy
- consumption
- full-size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the CompactifAI compression method from Multiverse
  Computing, applied to the Llama 3.1 8B model, focusing on energy efficiency and
  accuracy. Using CodeCarbon for power measurement and Ragas for accuracy evaluation,
  the compressed model was compared to the full-size version.
---

# Accuracy and Consumption analysis from a compressed model by CompactifAI from Multiverse Computing

## Quick Facts
- arXiv ID: 2507.08836
- Source URL: https://arxiv.org/abs/2507.08836
- Authors: Damien Fovet; Shashank Chamoli; Sarah Oury; Srishti Singhal
- Reference count: 10
- Primary result: CompactifAI compressed Llama 3.1 8B achieves 30-39% energy savings with minimal accuracy loss versus full-size model.

## Executive Summary
This study evaluates CompactifAI, a tensor-network-based compression method from Multiverse Computing, applied to Llama 3.1 8B. Using CodeCarbon for energy measurement and Ragas for accuracy evaluation, the compressed model demonstrated substantial energy savings (30-39% total, 43-51% GPU) while maintaining or improving most accuracy metrics compared to the full-size model. The compressed model also generated more succinct responses, contributing to additional energy savings through fewer tokens.

## Method Summary
The study compared CompactifAI-compressed Llama 3.1 8B against the full-size model on question-answering tasks. Energy consumption was measured using CodeCarbon, while accuracy was evaluated using the Ragas framework against GPT-4o-generated ground truth. Two generation configurations were tested: Set 1 (temperature 0.5, max 200 tokens) and Set 2 (temperature 0.1, max 1000 tokens). The evaluation used 104 curated questions across five domains, with 10 questions tested both with and without context.

## Key Results
- Compressed model reduced processing time by 5.68% (200 tokens) and 18.17% (1000 tokens)
- Lowered CO2 emissions by 30.03% (200 tokens) and 39.05% (1000 tokens)
- Decreased total energy consumption by 30.04% and 39.09% respectively
- GPU energy savings most significant: 43.55% (200 tokens) and 50.5% (1000 tokens)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tensor-network-based parameter reduction lowers per-inference compute without proportional accuracy degradation.
- Mechanism: CompactifAI applies tensor network decomposition to compress weight matrices in Llama 3.1 8B, reducing parameters loaded and multiplied during inference, directly reducing FLOPs per token.
- Core assumption: Tensor decomposition preserves sufficient information in weight subspace to maintain semantic reasoning.
- Evidence anchors: [abstract] "tensor-network-based compression method... maintained the model accuracy"; [section 2] "uses tensor networks with other techniques that reduces the amount of parameters"
- Break condition: If downstream tasks require rare knowledge stored in compressed-away weight dimensions, factual correctness may degrade beyond acceptable thresholds.

### Mechanism 2
- Claim: Energy savings compound through both lower per-token energy and shorter average response length.
- Mechanism: Compressed model consumes 25.60% less energy per generated token AND generates 18.11% fewer tokens on average (more succinct responses), multiplying to ~39% total energy reduction.
- Core assumption: Succinct responses remain adequate for user intent; token reduction is not a failure mode.
- Evidence anchors: [section 4.1.3] "compressed model consumes -25.60% energy to generate one token"; [section 4.1.3] "on average the compressed model generates 18,11% fewer tokens"
- Break condition: If succinctness causes incomplete answers in high-stakes domains (medical, legal), energy savings trade-off may be unacceptable.

### Mechanism 3
- Claim: GPU energy reduction disproportionately exceeds CPU energy reduction.
- Mechanism: Compression primarily reduces matrix multiplication load (GPU-heavy), while CPU overhead (orchestration, data movement) remains relatively constant.
- Core assumption: Inference is compute-bound on GPU; memory bandwidth is not primary bottleneck.
- Evidence anchors: [section 4.1.3] GPU energy reduced 43.55-50.5% vs. CPU energy reduced 5.79-18.15%; [section 4.1.3] "most of the power consumption comes from the GPU"
- Break condition: On memory-bandwidth-bound systems (smaller GPUs, edge devices), GPU energy savings may be smaller than reported.

## Foundational Learning

- Concept: Tensor Networks (Matrix Product States/Tensor Trains)
  - Why needed here: CompactifAI's core compression technique; understanding helps diagnose compression quality vs. accuracy trade-offs.
  - Quick check question: Can you explain how a low-rank tensor decomposition reduces parameter count while approximately preserving the original function?

- Concept: CodeCarbon Energy Measurement
  - Why needed here: Entire energy claim rests on this tool; knowing its limitations (RAPL proxies, GPU power models) is critical for interpreting results.
  - Quick check question: What are the main uncertainty sources in CodeCarbon's GPU energy estimates?

- Concept: ROUGE/BLEU/Semantic Similarity Metrics
  - Why needed here: Understanding what each metric captures (lexical overlap, n-gram fluency, embedding similarity) prevents over-claiming "accuracy maintained."
  - Quick check question: Why might BLEU improve post-compression even if factual correctness slightly decreases?

## Architecture Onboarding

- Component map: Base model (Llama 3.1 8B) -> Compression layer (CompactifAI tensor-network decomposition) -> Inference runtime (PyTorch 2.5.1) -> Measurement (CodeCarbon 2.5.0) -> Evaluation (Ragas framework + ChatGPT-4o ground truth)

- Critical path: 1) Acquire CompactifAI-compressed Llama 3.1 8B checkpoint 2) Load model into PyTorch inference server 3) Instrument inference loop with CodeCarbon tracker 4) Run Q&A tasks with controlled token limits (200/1000) 5) Compute Ragas metrics vs. ground truth

- Design tradeoffs:
  - Succinctness vs. completeness: Compressed model generates shorter answers by default; may require prompt engineering for verbose tasks.
  - Factual correctness vs. efficiency: ~3-5% factual correctness reduction observed; assess per-domain acceptability.
  - Framework lock-in: Tests only in PyTorch; vLLM or other runtimes may yield different efficiency profiles.

- Failure signatures:
  - Hallucination increase on context-dependent queries (factual correctness metric sensitive)
  - Response truncation at 200-token limit causing incomplete answers
  - Energy measurement noise on shared infrastructure (virtualization, background processes)

- First 3 experiments:
  1. Replicate energy measurement on different GPU hardware (e.g., A100, consumer RTX) to validate GPU-specific savings.
  2. Test on domain-specific Q&A (medical, legal) to characterize factual correctness degradation bounds.
  3. Compare against 4-bit or 8-bit quantization baseline on same task set for accuracy/efficiency Pareto curve.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the energy consumption of the compression process itself affect the total amortization period for the operational energy savings?
- Basis in paper: [explicit] Section 5 states "We do not have information about the impacts of the compression itself," noting this is required to "calculate how long... it would take to amortize the compression."
- Why unresolved: Study measured inference energy exclusively, excluding computational overhead of initial model compression.
- What evidence would resolve it: Measurements of energy and compute resources required to execute CompactifAI compression on Llama 3.1 8B.

### Open Question 2
- Question: Do larger models yield proportionally greater efficiency and accuracy benefits when compressed with CompactifAI compared to the 8B parameter model tested?
- Basis in paper: [explicit] Section 5 notes authors "were not able to test other compressed models" to verify claim that "the bigger the model, the bigger the benefits of the compression."
- Why unresolved: Experimental scope limited to Llama 3.1 8B, leaving scaling laws for this specific compression technique unverified.
- What evidence would resolve it: Comparative benchmark applying same methodology to larger models (e.g., 70B parameters).

### Open Question 3
- Question: How does the compressed model's performance compare when deployed using the vLLM framework versus the PyTorch framework used in this study?
- Basis in paper: [explicit] Section 5 lists as limitation: "We were not able to compare the results... with other framework than PyTorch. For instance... VLLM framework seems to be more efficient."
- Why unresolved: Tests strictly run on PyTorch, leaving interaction between this compression method and optimized inference engines unknown.
- What evidence would resolve it: Running inference benchmarks on vLLM endpoint to measure relative energy consumption against PyTorch baseline.

## Limitations

- Proprietary compression method (CompactifAI) limits reproducibility and independent verification of claimed energy savings and accuracy retention
- Energy measurement uncertainty from CodeCarbon's RAPL and GPU power model approximations, especially for short inference runs
- Small dataset scope (104 questions) and lack of publicly available dataset limits generalizability to other question types

## Confidence

- **High Confidence**: Energy consumption reduction claims (30-39% total, 43-51% GPU) supported by direct measurement, though with acknowledged tool uncertainty
- **Medium Confidence**: Accuracy metric improvements (ROUGE, BLEU, semantic similarity) statistically clear, but factual correctness slight decline (~3-5%) requires domain-specific risk assessment
- **Low Confidence**: Generalizability to other models, tasks, or hardware configurations due to proprietary method and limited dataset

## Next Checks

1. Independent energy replication: Replicate energy measurements on different GPU (A100 or RTX 4090) to verify GPU-specific savings and assess hardware dependency
2. Domain-specific factual accuracy: Test compressed model on larger, publicly available, domain-specific QA dataset (PubMedQA, CodeXGLUE) to quantify factual correctness degradation in high-stakes contexts
3. Comparison with open compression baselines: Benchmark CompactifAI against open-source 4-bit/8-bit quantization or pruning methods on same task set to establish relative Pareto efficiency