---
ver: rpa2
title: LLMs Can Plan Only If We Tell Them
arxiv_id: '2501.13545'
source_url: https://arxiv.org/abs/2501.13545
tags:
- block
- table
- input
- l1-0
- list
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether large language models (LLMs) can
  autonomously generate long-horizon plans rivaling human performance. It introduces
  AoT+ (Algorithm-of-Thoughts+), an enhanced prompting technique that builds on AoT
  by incorporating periodic state regeneration and random solution traces to reduce
  state hallucinations and computational complexity.
---

# LLMs Can Plan Only If We Tell Them

## Quick Facts
- arXiv ID: 2501.13545
- Source URL: https://arxiv.org/abs/2501.13545
- Authors: Bilgehan Sel; Ruoxi Jia; Ming Jin
- Reference count: 40
- LLMs can achieve state-of-the-art planning performance through structured prompting without external tools

## Executive Summary
This paper demonstrates that large language models can autonomously generate long-horizon plans rivaling human performance when guided by appropriate prompting techniques. The authors introduce AoT+ (Algorithm-of-Thoughts+), which enhances the original AoT method through periodic state regeneration and random solution traces. This approach significantly reduces state hallucinations and computational complexity while achieving state-of-the-art results across multiple planning benchmarks. The key insight is that LLMs possess latent planning capabilities that can be unlocked through structured prompting rather than requiring external verification tools or post-training.

## Method Summary
AoT+ is a prompting technique that builds on the Algorithm-of-Thoughts framework by incorporating two key innovations: periodic state regeneration and random solution traces. The method uses hierarchical state identifiers (1.2.1) to cache current states at decision points, reducing the cognitive load on the model during long planning traces. Random trajectories are generated by combining successful paths with unsuccessful ones, preventing the model from fixating on specific patterns while still demonstrating valid search structure. The approach works across planning domains including Blocksworld, Logistics, List Functions, and ACRE, requiring only a single API call per instance without external tools.

## Key Results
- AoT+ achieves state-of-the-art results across multiple planning benchmarks, exceeding human performance on Blocksworld (88% vs 78%)
- The method consistently outperforms prior approaches, including those using external verification tools
- AoT+ demonstrates significant efficiency gains with lower token usage and faster execution times compared to iterative approaches
- Random trajectories perform comparably to human-curated examples, achieving 70% vs 71% on Game of 24 and 54% vs 52% on Crossword benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Periodic State Regeneration Reduces Cognitive Load
LLMs hallucinate states during long planning traces because maintaining accurate state across many steps overwhelms attention capacity. AoT+ periodically restates the current world state with hierarchical identifiers (1.2.1), allowing the model to "cache" state rather than recompute from entire context history. This reduces state error rates from ~40% to ~15% at depth 20.

### Mechanism 2: Random Trajectories Activate Search Behavior Without Human Intuition
The benefit comes from demonstrating the structure of search (exploration, backtracking) rather than curated human heuristics. Random exploration paths that always terminate with correct goal-achievement steps create implicit bias toward valid solutions while preventing fixation on specific patterns. Random trajectories match human-curated AoT performance across all benchmarks.

### Mechanism 3: Hierarchical State Identifiers Enable Non-Linear Reasoning
Structured numbering (1.2.1.1) lets LLMs "jump" to previous decision points without external state tracking. The identifier system creates addressable nodes within the generated trace, making backtracking a matter of jumping to "state 1.2" rather than re-explaining from scratch.

## Foundational Learning

- Concept: **Non-ergodic planning problems** - Why needed: Planning problems where single missteps lead to unrecoverable states (unlike games with undo). CoT assumes ergodicityâ€”each step can be reversed or corrected. Quick check: In your task, can the agent always return to a previous state? If not, you need explicit backtracking demonstration.

- Concept: **State hallucination in LLMs** - Why needed: LLMs lose track of accumulated state changes during multi-step reasoning. They answer "what state would make sense here?" rather than "what state actually resulted from prior actions?" Quick check: Can your LLM correctly answer "what is the current state?" after 20 actions, without being shown intermediate states?

- Concept: **In-context search process vs. in-context solutions** - Why needed: Standard few-shot shows *solutions*. AoT+ shows *search processes* including dead ends and backtracking. This teaches the LLM how to explore, not just what correct answers look like. Quick check: Do your examples show only successful paths, or do they demonstrate exploration of alternatives?

## Architecture Onboarding

- Component map: Prompt Template -> System: Domain rules + action constraints -> In-context examples (AoT+ format) -> User: Problem instance (start/goal states)

- Critical path: 1. Define domain actions and constraints in system prompt, 2. Generate 3-5 in-context examples using random trajectory generator, 3. Ensure each example terminates with correct goal-state achievement, 4. Apply memoization every N steps, 5. Parse output for PDDL solution segment

- Design tradeoffs: More examples = better performance but higher token cost (AoT+ uses ~2000 input tokens vs 650 for CoT). Frequent state regeneration = fewer hallucinations but longer traces. Random trajectories = easier prompt creation but slightly lower ceiling than human-curated heuristics.

- Failure signatures: State drift (increase memoization frequency), Action illegality (strengthen constraint examples), Premature termination (require explicit goal-state matching), Identifier confusion (enforce strict identifier format).

- First 3 experiments: 1. Baseline comparison: Run your task with CoT vs AoT+ prompts on 50 instances. Expect 2-3x improvement on planning-heavy tasks. 2. State regeneration ablation: Test memoization at intervals of 3, 5, 10 steps. Plot state-error-rate vs depth. 3. Trajectory source test: Compare random-generated examples vs manually-curated examples. If gap is <5%, use random for scalability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do random search trajectories perform comparably to human-curated intuitive traces in AoT+?
- Basis in paper: The authors note that AoT+R (random traces) achieves performance very close to the original AoT, describing this result as "contrary to our expectations."
- Why unresolved: The paper hypothesizes that randomness prevents the model from fixating on specific patterns but does not provide a mechanistic explanation for why random, potentially inefficient paths successfully guide the model.
- What evidence would resolve it: A detailed analysis of attention patterns comparing how models process random versus curated trajectories to identify specific regularization effects.

### Open Question 2
- Question: Can AoT+ effectively generalize to planning domains with continuous or infinite action spaces?
- Basis in paper: The authors acknowledge that real-world applications often involve "action spaces [that] are often vast or infinite," yet the experiments are restricted to discrete benchmarks like Blocksworld and Logistics.
- Why unresolved: The "memoization" technique relies on regenerating discrete symbolic states (e.g., PDDL); it is unclear if this natural language caching works for continuous state vectors.
- What evidence would resolve it: Evaluating AoT+ on continuous control benchmarks or robotics simulations where states are numerical rather than purely linguistic.

### Open Question 3
- Question: Does the memoization technique permanently eliminate state hallucinations or merely delay them in very deep planning problems?
- Basis in paper: Figure 4 shows a dramatic reduction in error rates, but the discussion highlights the "inherent difficulty in self-verifying outputs" and errors are not reduced to absolute zero at depth 20.
- Why unresolved: The paper demonstrates improvement up to depth 20, but it does not test if the "cognitive load" eventually overwhelms the model in significantly longer horizons.
- What evidence would resolve it: Stress-testing AoT+ on planning instances requiring solution depths greater than 50 steps to observe if error rates remain stable or accumulate.

## Limitations
- Random trajectory generation hyperparameters (step counts, trajectory selection criteria) are not specified, affecting reproducibility
- LLM inference parameters (temperature, top-p) are not reported, though results are claimed to be consistent across models
- 4-5 in-context examples per benchmark may not represent the final configuration used for all experiments

## Confidence
- **High confidence**: AoT+ achieves state-of-the-art results across multiple planning benchmarks; periodic state regeneration demonstrably reduces state hallucinations; random trajectories perform comparably to human-curated examples
- **Medium confidence**: Claims about surpassing human performance on Blocksworld (88% vs 78%) - this specific comparison appears robust but depends on exact benchmark configuration
- **Low confidence**: Generalizability to domains with >50 state elements or branching factors >10, as these exceed the tested conditions

## Next Checks
1. **State regeneration ablation study**: Test memoization intervals of 3, 5, and 10 steps on your target domain and plot state-error-rate vs depth to replicate Figure 4 conditions
2. **Trajectory source comparison**: Generate 50 test instances using both random trajectories and manually-curated examples to verify the <5% performance gap claimed in Section 4.1
3. **Cross-LLM validation**: Run the same benchmark suite on at least two different LLMs (e.g., GPT-4 and Claude) to confirm the "consistent across various LLMs" claim and identify model-specific failure patterns