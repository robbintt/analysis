---
ver: rpa2
title: Are Hallucinations Bad Estimations?
arxiv_id: '2509.21473'
source_url: https://arxiv.org/abs/2509.21473
tags:
- hallucination
- loss
- definition
- probability
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper provides a novel theoretical perspective on hallucinations\
  \ in generative models, framing them as a fundamental misalignment between the loss-minimizing\
  \ training objective and human-acceptable outputs. The authors formalize this as\
  \ \u03B4-hallucination, where an estimator's output fails to align with any plausible\
  \ input cause."
---

# Are Hallucinations Bad Estimations?

## Quick Facts
- arXiv ID: 2509.21473
- Source URL: https://arxiv.org/abs/2509.21473
- Reference count: 4
- Primary result: This paper provides a novel theoretical perspective on hallucinations in generative models, framing them as a fundamental misalignment between the loss-minimizing training objective and human-acceptable outputs.

## Executive Summary
This paper provides a novel theoretical perspective on hallucinations in generative models, framing them as a fundamental misalignment between the loss-minimizing training objective and human-acceptable outputs. The authors formalize this as δ-hallucination, where an estimator's output fails to align with any plausible input cause. Crucially, they prove that even optimal estimators minimizing expected loss can still hallucinate, demonstrating that hallucination is not merely a limitation of model capacity or data but a structural feature of estimation itself.

The authors derive a general lower bound on the probability of δ-hallucination and validate their theory through controlled experiments on synthetic coin-flipping, open-ended QA, and text-to-image generation tasks. The results consistently show that minimizing loss does not eliminate hallucination, supporting the theoretical claims. This work reframes hallucination as an intrinsic property of probabilistic estimation rather than a solvable bug, suggesting that effective mitigation requires rethinking training objectives to better align with human standards of correctness.

## Method Summary
The paper formalizes hallucination as δ-hallucination, where an estimator's output fails to align with any plausible input cause by having low probability density under all latent states. The authors prove that even Bayes-optimal estimators under expected quadratic loss can produce such outputs when the true conditional distribution is multimodal. They derive a lower bound on δ-hallucination probability using Chebyshev and Paley-Zygmund inequalities under mild distributional assumptions.

Validation experiments include: synthetic coin-flipping (8-layer transformer predicting total heads from coin labels with N∈{2,3,5} coins, M∈[20000,40000] flips), open-ended QA (fine-tuning Qwen models on 300 questions with 2 answers each, measuring resemblance to TruthfulQA incorrect answers), and text-to-image generation (fine-tuning Stable Diffusion v1.5 on AFHQ dataset, measuring HCDR membership at 10% quantile threshold).

## Key Results
- Even Bayes-optimal estimators under quadratic loss can produce outputs with zero probability under all plausible latent states
- Training loss decreases monotonically while hallucination rate remains stable or increases across all three experimental domains
- A non-zero lower bound on δ-hallucination probability exists under mild distributional assumptions about conditional means

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Loss-minimizing optimal estimators can produce outputs that fail to match any plausible outcome mode.
- Mechanism: Under expected quadratic loss, the Bayes-optimal estimator computes the conditional expectation A*(X) = E[A(X)]. When the true conditional distribution is multimodal, this expectation averages across modes and may land in a low-probability region that aligns with none of them.
- Core assumption: The target distribution contains multiple valid modes indexed by latent states Z_i with non-negligible probability mass.
- Evidence anchors:
  - [abstract]: "We show that even loss-minimizing optimal estimators still hallucinate."
  - [section 1, p.3]: "If the true conditional distribution Pr[A(X)] = Pr[A(x)|X=x] is multimodel, then A*(X) averages across all those possible outcomes and may fall in a low-probability region."
  - [corpus]: Kalai et al. (2025) "Why Language Models Hallucinate" supports persistence under scale and likelihood training.
- Break condition: If the true conditional distribution is unimodal, δ-hallucination by this mechanism reduces to standard estimation error outside HDRs.

### Mechanism 2
- Claim: δ-hallucination is an attribution failure where the output has low probability density under every latent state.
- Mechanism: The formal definition requires f(A(X;Z_i) = A*(X)) ≤ δ for all i ∈ [N]. When this holds, the estimate cannot be attributed to any plausible cause consistent with the data distribution.
- Core assumption: A latent variable Z exists with discrete states that partition the output space into plausible categories.
- Evidence anchors:
  - [section 4, p.7]: Definition 4.2 formalizes δ-hallucination as failure to exceed δ threshold under any Z_i.
  - [section 4, p.7, Remark 4.2]: "δ-hallucination occurs when the estimator outputs a value that has low likelihood under every plausible latent state."
  - [corpus]: Aithal et al. (2024) link diffusion hallucinations to mode interpolation between latent states—consistent with this attribution framing.
- Break condition: If no meaningful latent structure exists (N=1), the definition collapses to standard density-based outlier detection.

### Mechanism 3
- Claim: Hallucination probability has a non-zero lower bound under mild distributional assumptions.
- Mechanism: Theorem 6.1 derives P^δ_H > ∏(P_i K^μ_i) using Chebyshev and Paley-Zygmund inequalities, where the bound depends on variance of conditional means and high-density region coverage.
- Core assumption: Conditional means across latent states are independently distributed with identical expected mean (Assumption 6.1).
- Evidence anchors:
  - [section 6, p.10]: Theorem 6.1 provides the formal lower bound.
  - [section 7, p.11-14]: Experiments show training loss decreases while hallucination rate does not converge to zero.
  - [corpus]: Corpus evidence for quantitative lower bounds is weak; no direct replications found.
- Break condition: If variance of conditional means is zero (all modes share the same center), the bound becomes vacuous.

## Foundational Learning
- Concept: Bayes-optimal estimator under quadratic loss
  - Why needed here: Central to the proof that optimal estimators hallucinate; requires understanding why conditional expectation minimizes expected squared error.
  - Quick check question: Given a multimodal distribution over outputs, where does the conditional expectation lie relative to the modes?
- Concept: Multimodal conditional distributions and latent variables
  - Why needed here: The theory relies on latent states Z inducing multiple valid output modes; understanding mixture distributions is essential.
  - Quick check question: If Pr[A|X] is a mixture of two Gaussians with means μ₁ and μ₂, what is E[A|X]?
- Concept: Highest Density Regions (HDRs) and their conditional generalization
  - Why needed here: δ-hallucination is defined via exclusion from high-density regions under all latent states.
  - Quick check question: For a unimodal Gaussian, what shape is the 90% HDR?

## Architecture Onboarding
- Component map: Estimator A*(X) -> Latent states {Z_i} -> High-density regions {U^δ_i} -> Hallucination detector
- Critical path:
  1. Identify latent states Z_i relevant to your task (manual annotation or clustering).
  2. Estimate conditional densities f(A|X; Z_i) per state (GMMs, KDE, or parametric models).
  3. Compute δ-thresholds defining U^δ_i (e.g., percentile-based).
  4. For each prediction, verify membership in at least one U^δ_i.
- Design tradeoffs:
  - Tighter δ (lower threshold) → fewer hallucinations flagged but higher false negatives.
  - More latent states → finer-grained attribution but increased estimation complexity.
  - Density estimation quality directly affects hallucination detection accuracy.
- Failure signatures:
  - Training loss decreases but hallucination rate remains constant or increases (observed in Section 7).
  - Predictions consistently fall in low-density regions across all latent states.
  - Outputs blend features from multiple incompatible modes (e.g., cat-dog chimeras in text-to-image).
- First 3 experiments:
  1. Replicate coin-flipping aggregation (Section 7.1): Train a small transformer to predict total heads from coin labels; verify that loss decrease does not correlate with conditional probability of predictions under latent states.
  2. Open-ended QA probe (Section 7.2): Fine-tune an LLM on multi-answer questions; measure resemblance to known incorrect answers (TruthfulQA) as proxy for δ-hallucination.
  3. HCDR calibration for text-to-image (Section 7.3): Fit GMMs to CLIP embeddings of known classes; generate images with ambiguous prompts and compute fraction falling outside all class HDRs.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can tighter lower bounds on δ-hallucination probability be derived under more relaxed distributional assumptions than those in Assumption 6.1 (identical means and independence)?
- Basis in paper: [explicit] "The current lower bound for δ-hallucination is relatively loose and relies on certain assumptions, leaving room for tighter bounds under more relaxed conditions."
- Why unresolved: The current proof relies on Chebyshev and Paley-Zygmund inequalities with independence assumptions; removing these requires different mathematical machinery.
- What evidence would resolve it: A new theoretical bound that holds without the independence assumption, or a counterexample showing the bound cannot be improved without additional structure.

### Open Question 2
- Question: How do alternative loss functions (beyond quadratic and cross-entropy) affect hallucination rates, and can loss functions be designed to explicitly minimize δ-hallucination?
- Basis in paper: [explicit] "Extend our theoretical framework to other loss functions to investigate how the choice of training objective influences hallucination rates."
- Why unresolved: The paper only derives results for quadratic loss in the main text and sketches cross-entropy in the appendix; other loss families remain unexplored.
- What evidence would resolve it: Theoretical analysis or empirical measurements showing monotonic relationships between loss function properties and hallucination rates.

### Open Question 3
- Question: What practical training schemes (e.g., HDR-guided sampling, mixed-objective fine-tuning) can scale the theoretical insights to large foundation models while explicitly penalizing outputs outside HCDRs?
- Basis in paper: [explicit] "Design practical strategies that scale our insights, such as HDR-guided sampling or mixed-objective fine-tuning that explicitly penalizes implausible outputs."
- Why unresolved: The paper validates the theory but does not propose or test mitigation methods; mode-seeking objectives may conflict with standard likelihood training.
- What evidence would resolve it: An algorithm that reduces measured δ-hallucination rates on benchmarks without degrading overall task performance.

## Limitations
- The theoretical lower bound on δ-hallucination probability relies on Assumption 6.1 (identical expected mean across conditional distributions), which may not hold in practice and is not empirically validated
- The experiments demonstrate correlation between decreasing training loss and non-decreasing hallucination rates, but do not conclusively prove causation or that the observed phenomenon is truly δ-hallucination rather than other forms of model error
- The definition of δ-hallucination depends critically on the choice of latent states Z_i and density estimation quality, which are task-dependent and potentially subjective

## Confidence
- High: The core theoretical result that Bayes-optimal estimators under multimodal distributions produce outputs outside all plausible modes
- Medium: The experimental evidence showing training loss decreases while hallucination metrics remain stable or increase
- Medium: The proposed formalization of hallucination as an attribution failure rather than a training deficiency

## Next Checks
1. **Parameter sensitivity analysis**: Systematically vary δ thresholds, latent state definitions, and density estimation parameters across all three experiments to assess robustness of reported hallucination rates
2. **Ablation on latent structure**: Remove or modify the latent state structure in the coin-flipping and text-to-image experiments to test whether observed effects persist without explicit attribution requirements
3. **Cross-task generalization**: Apply the δ-hallucination framework to at least two additional task types (e.g., structured prediction, sequence-to-sequence translation) to test whether the theoretical insights generalize beyond the current experimental scope