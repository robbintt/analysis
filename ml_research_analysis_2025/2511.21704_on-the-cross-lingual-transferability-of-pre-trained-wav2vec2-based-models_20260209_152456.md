---
ver: rpa2
title: On the Cross-lingual Transferability of Pre-trained wav2vec2-based Models
arxiv_id: '2511.21704'
source_url: https://arxiv.org/abs/2511.21704
tags:
- performance
- languages
- xls-r
- xlsr-53
- pre-trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the cross-lingual transferability of pre-trained
  wav2vec2-based models across 18 languages and 15 different pre-trained models. The
  study focuses on understanding how the diversity and quantity of pre-training data
  impact model performance in speech recognition tasks, particularly comparing Indo-European
  and non-Indo-European language families.
---

# On the Cross-lingual Transferability of Pre-trained wav2vec2-based Models

## Quick Facts
- arXiv ID: 2511.21704
- Source URL: https://arxiv.org/abs/2511.21704
- Authors: Jonatas Grosman; Cassio Almeida; Guilherme Schardong; Hélio Lopes
- Reference count: 40
- Pre-trained wav2vec2 models show stronger cross-lingual transfer within language families, with multilingual models outperforming monolingual ones

## Executive Summary
This paper investigates cross-lingual transfer of pre-trained wav2vec2-based models across 18 languages and 15 different pre-trained models. The study systematically compares monolingual and multilingual models, revealing that multilingual models with diverse pre-training data outperform those with larger but less varied datasets. A key finding shows Indo-European languages consistently achieve better performance than non-Indo-European languages across all evaluated models, suggesting pre-training data imbalance affects downstream performance. The research also demonstrates positive cross-lingual transfer in monolingual models, particularly when pre-training and fine-tuning languages share linguistic families.

## Method Summary
The study fine-tuned 15 pre-trained wav2vec2 LARGE models on 18 languages from Common Voice 7.0, using exactly 5 hours of training data per language. Models were evaluated using Character Error Rate (CER) with frozen feature extractors and CTC loss. Training used Adam optimizer (lr=6e-5, 4k steps) with tri-stage learning rate scheduling. The vocabulary was built from characters appearing at least 0.001% in training data. Three runs with different seeds were performed for each configuration. Statistical significance was tested using Kruskal-Wallis and Conover-Iman tests.

## Key Results
- Multilingual models (XLS-R) achieved the best performance for 8/18 languages tested
- Models pre-trained on diverse data sources outperformed those with larger but homogeneous datasets
- Indo-European languages consistently outperformed non-Indo-European languages across all models
- Cross-lingual transfer was stronger when pre-training and fine-tuning languages shared linguistic families

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training data diversity improves downstream ASR performance more than raw data quantity.
- Mechanism: Diverse acoustic environments and speaker variations create more robust latent speech representations that generalize better to unseen domains. Models exposed to multiple domains learn to extract phonetically-relevant features invariant to speaker/channel characteristics.
- Core assumption: The acoustic features learned from diverse sources transfer more effectively than those learned from homogeneous but larger datasets.
- Evidence anchors:
  - [Section 4.1]: XLSR-53 (56k hours diverse) outperformed VoxPopuli-100k (100k hours single-domain).
  - [Section 4.1]: UniSpeech-ML trained on 2k hours Common Voice showed exceptional results due to domain matching.
  - [corpus]: Weak direct corpus evidence; related NLP work (mBERT transfer) exists but in different modality.

### Mechanism 2
- Claim: Cross-lingual transfer is stronger when pre-training and fine-tuning languages share linguistic family/subgroup membership.
- Mechanism: Languages within the same family share phonological inventories, phonotactic constraints, and prosodic patterns. The convolutional encoder learns acoustic-phonetic patterns that are partially language-universal and partially language-specific; similar languages activate overlapping learned representations.
- Core assumption: The latent representations capture language-specific phonetic units that transfer within language families.
- Evidence anchors:
  - [Section 4.3]: Portuguese performed best with VP-es, VP-it, VP-fr (same Italic subgroup).
  - [Section 4.3]: German performed best with VP-nl, VP-sv (same Germanic subgroup).
  - [corpus]: Cross-lingual knowledge transfer patterns observed in multilingual LLMs show similar language proximity effects.

### Mechanism 3
- Claim: Pre-training data imbalance toward Indo-European languages creates performance disparities favoring Indo-European downstream tasks.
- Mechanism: When pre-training data is dominated by certain languages, model weights optimize for acoustic patterns prevalent in those languages. The transformer contextual representations encode priors biased toward Indo-European phonological structures.
- Core assumption: The model learns language-specific priors during self-supervised pre-training that persist through fine-tuning.
- Evidence anchors:
  - [Section 4.2]: Indo-European languages consistently outperformed non-Indo-European languages across all models.
  - [Section 4.2]: Performance gap persisted even excluding Japanese and Chinese.
  - [corpus]: Debiasing work in multilingual LLMs confirms representation biases persist across languages.

## Foundational Learning

- Concept: Self-supervised speech representation learning (wav2vec 2.0 objective)
  - Why needed here: The entire paper assumes understanding of how models learn representations from unlabeled audio via masked prediction and contrastive learning.
  - Quick check question: Can you explain why wav2vec 2.0 can learn useful representations without transcriptions?

- Concept: Connectionist Temporal Classification (CTC) loss
  - Why needed here: The fine-tuning experiments use CTC for the ASR task; understanding alignment-free training is essential.
  - Quick check question: Why does CTC allow training on unaligned audio-transcript pairs?

- Concept: Language family taxonomy (Indo-European, Uralic, Sino-Tibetan, etc.)
  - Why needed here: The paper's central finding relates transfer performance to language family relationships; you need to understand what makes languages linguistically similar.
  - Quick check question: Why might Spanish transfer better to Portuguese than to German, even though all three are Indo-European?

## Architecture Onboarding

- Component map:
  Raw Audio (X) → Convolutional Encoder → Latent Representations (Z) → Quantization Module → Transformer Network → Contextual Representations (C) → Fine-tuning Head (Linear) → Character Probabilities (|V| classes)

- Critical path:
  1. Select appropriate pre-trained model based on target language family (Section 4.3 rankings)
  2. Build vocabulary V from target language training data (characters with frequency ≥0.001%)
  3. Initialize linear projection layer randomly on top of frozen encoder output
  4. Fine-tune with CTC loss, tri-stage learning rate schedule (warmup 10%, constant 40%, decay 50%)
  5. Evaluate using Character Error Rate (CER)

- Design tradeoffs:
  - Multilingual vs. monolingual pre-trained model: Multilingual generally better (XLS-R ranked best in 8/18 languages), but monolingual models pre-trained on same-family languages can be competitive for specific targets
  - Data diversity vs. quantity: 56k hours diverse data outperformed 100k hours single-domain
  - Encoder freezing: The paper freezes encoder during fine-tuning to "leverage features already learned"; this trades adaptation capacity for training stability and efficiency

- Failure signatures:
  - Very high CER (>0.5) on non-Indo-European languages: Indicates pre-training data doesn't cover target phonology; consider multilingual models with broader language coverage or language-specific continued pre-training
  - No improvement over random baseline: Check if vocabulary extraction failed, or if training data has <5 hours (below paper's threshold)
  - Large variance across seeds (±0.05+ CER): Increase fine-tuning steps or use larger learning rate warmup

- First 3 experiments:
  1. Replicate the baseline comparison: Fine-tune XLS-R vs. random initialization on one Indo-European language (e.g., German) using 5 hours of Common Voice data to verify your pipeline matches reported CER (~0.056 test).
  2. Test language family transfer: Fine-tune VoxPopuli monolingual models (VP-es, VP-nl) on Portuguese and German respectively to confirm the family/subgroup ranking pattern from Section 4.3.
  3. Measure diversity vs. quantity effect: Compare XLSR-53 (diverse, 56k hours) vs. VP-100k (homogeneous, 100k hours) on a held-out language not in either pre-training set (e.g., Indonesian) to validate the diversity advantage.

## Open Questions the Paper Calls Out

- **Question**: What are the underlying causes for the superior performance of Indo-European languages compared to non-Indo-European languages in wav2vec2-based models?
  - **Basis**: Authors state in Future Work section they want to investigate why Indo-European languages generally performed better.
  - **Why unresolved**: Paper identifies the gap and hypothesizes it relates to data imbalance/diversity but doesn't provide empirical evidence from balanced pre-training experiments.
  - **What evidence would resolve it**: An ablation study using models pre-trained from scratch with controlled, balanced data across language families.

- **Question**: Does the gender imbalance in pre-training data (predominantly male voices) significantly impact the final performance of the models?
  - **Basis**: Authors note most pre-training data comes from male voices and this gender imbalance may affect performance.
  - **Why unresolved**: Study did not control for or analyze gender distribution in pre-training or fine-tuning data.
  - **What evidence would resolve it**: Comparative evaluation of model accuracy on male vs. female test sets.

- **Question**: Do the findings regarding cross-lingual transferability hold true across different speech domains outside of read speech?
  - **Basis**: Limitations section states findings may not remain valid in different domains since only Common Voice (read speech) was used.
  - **Why unresolved**: Common Voice consists largely of read speech; conclusions about data diversity vs. quantity may not apply to noisy, conversational, or broadcast domains.
  - **What evidence would resolve it**: Replicating fine-tuning experiments on datasets from different domains (e.g., conversational telephone speech, broadcast news).

- **Question**: How sensitive are the comparative rankings of pre-trained models to changes in fine-tuning hyperparameters?
  - **Basis**: Authors note resource constraints prevented testing whether findings held with different hyperparameter combinations.
  - **Why unresolved**: Results rely on specific hyperparameter configuration; it's unknown if superiority of diverse multilingual models is robust to different optimization settings.
  - **What evidence would resolve it**: Running fine-tuning experiments with varying learning rates and schedules to observe if relative performance changes.

## Limitations
- Limited evidence for diversity vs. quantity claim: Only a single comparison pair (XLSR-53 vs. VoxPopuli-100k) rather than systematic variation of both factors
- Potential data imbalance bias: Indo-European performance advantage may reflect pre-training data imbalance rather than inherent model limitations
- Domain specificity: Findings may not generalize beyond read speech since only Common Voice dataset was used

## Confidence

- **High Confidence**: Multilingual models generally outperform monolingual ones for cross-lingual transfer (XLS-R ranked best for 8/18 languages). Directly supported by presented results with statistical significance testing.
- **Medium Confidence**: Pre-training data diversity improves performance more than quantity. Supported by XLSR-53 vs. VoxPopuli-100k comparison but limited to a single data point.
- **Medium Confidence**: Cross-lingual transfer is stronger within language families. Supported by systematic comparisons across language pairs but relies on family membership as proxy for acoustic-phonetic similarity.

## Next Checks

1. **Controlled diversity vs. quantity experiment**: Create synthetic pre-training datasets that vary only in data diversity while holding total hours constant, then compare downstream performance to validate the primary mechanism beyond the single XLSR-53 vs. VoxPopuli-100k comparison.

2. **Balanced multilingual pre-training replication**: Re-train or simulate a multilingual model with balanced language representation across families to test whether the Indo-European performance advantage persists when pre-training data is linguistically balanced.

3. **Fine-grained phonological distance analysis**: Measure actual phonological and phonotactic distances between language pairs (beyond family membership) and correlate with transfer performance to validate the assumed mechanism linking family membership to acoustic-phonetic similarity.