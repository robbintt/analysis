---
ver: rpa2
title: Improve Rule Retrieval and Reasoning with Self-Induction and Relevance ReEstimate
arxiv_id: '2505.10870'
source_url: https://arxiv.org/abs/2505.10870
tags:
- retrieval
- rule
- rules
- query
- siar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of rule retrieval, where semantic
  misalignment between instantiated query facts and abstract rule representations
  leads to poor retrieval accuracy. The authors propose Self-Induction Augmented Retrieval
  (SIAR), which uses LLMs to abstract and induce potential inferential rules from
  queries, improving semantic alignment for retrieval.
---

# Improve Rule Retrieval and Reasoning with Self-Induction and Relevance ReEstimate

## Quick Facts
- **arXiv ID:** 2505.10870
- **Source URL:** https://arxiv.org/abs/2505.10870
- **Reference count:** 17
- **Primary result:** SIAR improves Recall@1 by up to 67.47% over vanilla retrieval; R³ further enhances performance to 88.67% Recall@1 in some settings

## Executive Summary
This paper addresses the challenge of rule retrieval, where semantic misalignment between instantiated query facts and abstract rule representations leads to poor retrieval accuracy. The authors propose Self-Induction Augmented Retrieval (SIAR), which uses LLMs to abstract and induce potential inferential rules from queries, improving semantic alignment for retrieval. They also introduce Rule Relevance ReEstimate (R³), which re-ranks retrieved rules by assessing their applicability to the query. Experiments across three datasets (Clutrr, ULogic, CAIL2018) show SIAR improves Recall@1 by up to 67.47% over vanilla retrieval, while R³ further enhances performance, achieving up to 88.67% Recall@1 in some settings. The methods are effective across model sizes and rule formats, demonstrating robustness and generalizability.

## Method Summary
SIAR addresses semantic misalignment in rule retrieval by using LLMs to generate self-induced rules from instantiated queries through few-shot prompting, then using these abstract rules (or concatenated with original queries) to retrieve from a rule library. The approach works with both sparse (BM25) and dense (BGE) retrievers, with different query formulation strategies for each. R³ then re-ranks the retrieved rules by having an LLM assess their logical applicability to the query. The method requires no training, relying instead on prompting, and achieves significant improvements in both retrieval accuracy (Recall@1 up to 88.67%) and downstream reasoning performance across multiple datasets and model sizes.

## Key Results
- SIAR improves Recall@1 by up to 67.47% over vanilla retrieval on CLUTRR dataset
- R³ further enhances performance, achieving up to 88.67% Recall@1 in some settings
- The approach shows consistent improvements across different model sizes (7B, 72B) and rule formats (natural language, formal language)
- R³ provides significant benefits for larger models but inconsistent results for smaller models on complex datasets

## Why This Works (Mechanism)

### Mechanism 1: Self-Induction as Semantic Projection
The method uses few-shot in-context learning to prompt an LLM to summarize and abstract facts in the query, hypothesizing inferential relationships to form a "self-induced rule" (SI). This SI is then used as (or concatenated with) the search query. The theoretical insight is that the query subspace (concrete) and rule subspace (abstract) have minimal overlap; SIAR bridges this gap by transforming the query into the rule's representational space. The mechanism fails if the LLM's inductive capability is insufficient for the query's complexity, resulting in a self-induced rule that deviates significantly from the true underlying logic, thereby failing to bridge the semantic gap. Performance gains from SIAR are notably smaller for 7B models versus 72B models, evidencing this constraint.

### Mechanism 2: Relevance Re-estimation via Logical Instantiability
R³ prompts an LLM to assess if the abstract knowledge in a retrieved rule can be "instantiated to align with the facts in the queries" and if it is "helpful for reasoning." Instead of relying solely on vector similarity (dense) or keyword overlap (sparse), this step performs a logical verification, filtering for semantic applicability. It uses a listwise ranking prompt to output a re-ordered list. The mechanism fails if the LLM's ability to assess relevance is no better than the initial retriever's similarity score. The paper's ablation study on smaller models (Table 1 & 6, discussion in Section 4.1) shows that R³'s benefit is inconsistent for 7B models on complex datasets (CLUTRR), indicating a capacity threshold for this meta-task.

### Mechanism 3: Augmented Query Formulation for Retriever Type
The paper identifies two strategies: using SI as the new query (w/ SI) or concatenating SI with the original input (w/ SI + input).
- For dense retrievers, using SI alone is generally better. Dense models map text to a unified semantic vector space; concatenating the concrete original query can disrupt the abstract semantic coherence of the SI, lowering similarity to the abstract rules.
- For sparse retrievers, concatenating SI with the original input is generally better. Sparse models rely on keyword matching; the original query provides specific keywords (e.g., "California") that the abstract SI (e.g., "Region Z") lacks, increasing the BM25 score.
The mechanism's guidance fails if the rule library's language format contradicts the assumption. For instance, if rules are in formal language, the paper notes (Section D) that dense retrieval performance can drop, potentially altering which query formulation is optimal.

## Foundational Learning

### Concept: Semantic Misalignment in Retrieval
**Why needed here:** The paper's core problem is that standard retrieval fails because instantiated facts (in a query) and abstract rules (in a library) exist in different semantic spaces, leading to low similarity scores despite logical equivalence.
**Quick check question:** Can you explain why a traditional dense retriever might assign a low similarity score to a query about "Alice in California" and a rule about "PersonX in RegionZ"?

### Concept: Inductive vs. Deductive Reasoning
**Why needed here:** SIAR relies on the LLM's **inductive** capability (generalizing a rule from a specific query instance) before the final reasoning step, which is **deductive** (applying a retrieved rule to the query).
**Quick check question:** In the context of this paper, is the process of generating the "Self-Induced Rule" an example of induction or deduction? What about the final step of answering the question?

### Concept: Retrieve-then-Reason Paradigm
**Why needed here:** This is the overarching architecture. Understanding this sequential dependency is crucial because the paper's contribution is to improve the first stage (retrieval) to prevent error propagation into the second (reasoning).
**Quick check question:** If the retrieval stage returns an irrelevant rule, how does this typically affect the downstream reasoning performance according to the paper?

## Architecture Onboarding

**Component map:**
User Query -> SIAR (LLM-based inductor) -> Self-Induced Rule (SI) -> Query Formulation (SI or SI+input) -> Retriever (BM25/BGE) -> Rule Library -> Retrieved Rules -> R³ (LLM-based re-ranker) -> Top-k Rules -> Downstream Reasoner

**Critical path:** The correctness of the final retrieved rule depends on a chain of successful transformations: (1) Accurate self-induction of a rule that mirrors the true logic. (2) Formulation of a query that maximizes retriever-specific signals. (3) Accurate assessment of logical instantiability by the re-ranker.

**Design tradeoffs:**
- **Performance vs. Latency/Cost:** Adding R³ increases accuracy but requires an additional LLM call per query, adding latency and computational cost.
- **Precision vs. Robustness:** Using a more capable LLM (e.g., 72B vs 7B) significantly improves both SIAR and R³, but at higher inference cost. Smaller models may not consistently benefit from R³.
- **Query Formulation:** For sparse retrieval, including the original query adds precision via keywords but adds length; for dense, it may introduce noise into the semantic vector.

**Failure signatures:**
- **Persistent Low Recall@1:** Likely due to weak inductive capability of the chosen LLM, failing to generate a representative self-induced rule. The paper shows 7B models struggle on CLUTRR.
- **Dense Retrieval Degradation:** Occurs if the query formulation is mismatched (e.g., using SI+input for a dense retriever on an abstract rule corpus).
- **Reasoning Degradation:** If vanilla retrieval is used, retrieved noise can actively harm reasoning performance compared to having no rules at all (Figure 1c).

**First 3 experiments:**
1. **Baseline Verification:** Replicate the vanilla retrieval performance drop on the CLUTRR or ULogic dataset using a standard dense retriever (like BGE) to confirm the semantic misalignment problem exists.
2. **SIAR Efficacy Ablation:** Implement SIAR alone. Compare Recall@1 using the two query formulations (w/ SI vs. w/ SI + input) on both sparse (BM25) and dense (BGE) retrievers to validate the retriever-specific tuning claim.
3. **R³ Contribution Ablation:** Implement the full SIAR-R³ pipeline. Measure the improvement in Recall@1 and the downstream reasoning Match score over SIAR alone. Test with at least two model sizes (e.g., 7B and 72B) to observe the capability-dependent benefit of re-ranking.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** How does the performance of SIAR and R³ degrade when scaling to industrial-sized rule libraries containing millions of rules with significantly higher distractor ratios?
**Basis in paper:** [explicit] The authors explicitly state in the Limitations section that the current rule libraries are small (max 1,048 rules) and that "The smaller number of rules reduces the difficulty of the benchmark." They propose introducing more irrelevant rules in future work.
**Why unresolved:** It is unclear if the self-induction mechanism remains effective when the search space is vastly larger, potentially increasing the likelihood of retrieving "hard negatives" that share semantic similarity with the induced rule but are logically irrelevant.
**What evidence would resolve it:** Experiments on expanded datasets (e.g., 100k+ rules) where the density of relevant rules is lowered, measuring the drop in Recall@k compared to the current benchmarks.

### Open Question 2
**Question:** Can smaller parameter models (e.g., 7B scale) be enhanced via fine-tuning to reliably perform the Rule Relevance ReEstimate (R³) required for complex reasoning tasks?
**Basis in paper:** [inferred] The results show that on the CLUTRR dataset, the 7B model saw no performance gain from R³, while the 72B model improved significantly. The authors suggest this indicates smaller models "lack the capacity to effectively rerank rules."
**Why unresolved:** The paper relies on prompting general-purpose LLMs. It leaves unresolved whether this failure is an inherent limitation of the model size or a failure of the zero-shot prompting strategy for smaller models.
**What evidence would resolve it:** A comparative study showing whether fine-tuning a 7B model specifically on a rule-relevance ranking task can close the performance gap with the 72B model observed in the paper.

### Open Question 3
**Question:** What specific mechanisms can align the "SI + input" query format with dense retrievers to prevent the loss of semantic coherence observed in the paper?
**Basis in paper:** [inferred] The authors note that while sparse retrievers benefit from concatenating the original input with the self-induced rule (SI + input), dense retrievers suffer because this concatenation "disrupts the semantic coherence of the SI" in the vector space.
**Why unresolved:** The paper identifies the trade-off but offers no solution for dense retrievers other than reverting to using the SI alone, which may lose specific query details.
**What evidence would resolve it:** A proposed query fusion method (e.g., late interaction or weighted embedding averaging) that allows dense retrievers to utilize both the concrete query and abstract rule effectively.

## Limitations
- **Model-size dependency:** R³ shows inconsistent benefits for smaller models (7B) on complex datasets, suggesting a capability threshold for the re-ranking task.
- **Small rule libraries:** Current experiments use small rule libraries (max 1,048 rules), limiting generalizability to industrial-scale applications with millions of rules.
- **Unproven retriever-specific guidance:** The recommendation to use SI for dense and SI+input for sparse retrievers lacks direct corpus validation and may not hold for all rule formats.

## Confidence

**High confidence:** The core problem of semantic misalignment between instantiated queries and abstract rules is well-established and empirically demonstrated across all three datasets.

**Medium confidence:** The retriever-specific query formulation guidance (SI for dense, SI+input for sparse) is theoretically sound but lacks direct corpus validation.

**Medium confidence:** The R³ re-ranking mechanism shows consistent improvements on larger models but demonstrates model-size-dependent effectiveness that requires further investigation.

## Next Checks

1. **Model-size capability threshold:** Systematically test SIAR and R³ across a broader range of model sizes (e.g., 7B, 13B, 34B, 72B) on the same datasets to quantify the exact capability threshold where R³ transitions from harmful to beneficial.

2. **Rule library format robustness:** Evaluate SIAR's performance when applied to rule libraries in formal language versus natural language to test the assumption that abstract semantic projection works across different rule representations.

3. **Cross-domain generalization:** Apply SIAR to rule retrieval tasks outside the RuleBench datasets (e.g., medical reasoning rules or legal inference rules) to validate whether the semantic misalignment problem and proposed solution generalize to other reasoning domains.