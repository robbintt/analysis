---
ver: rpa2
title: 'SG-OIF: A Stability-Guided Online Influence Framework for Reliable Vision
  Data'
arxiv_id: '2511.19466'
source_url: https://arxiv.org/abs/2511.19466
tags:
- influence
- training
- learning
- sg-oif
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reliably estimating training-point
  influence on test predictions in deep learning vision models, which is critical
  for tasks like noisy label detection and out-of-distribution detection. The authors
  introduce SG-OIF, a Stability-Guided Online Influence Framework that treats algorithmic
  stability as a real-time controller for online inverse-curvature computation.
---

# SG-OIF: A Stability-Guided Online Influence Framework for Reliable Vision Data

## Quick Facts
- **arXiv ID**: 2511.19466
- **Source URL**: https://arxiv.org/abs/2511.19466
- **Reference count**: 40
- **Primary result**: Achieves 91.1% accuracy in top 1% predictions on CIFAR-10 with 20% asymmetric noise using stability-guided online influence estimation

## Executive Summary
This paper introduces SG-OIF, a framework that addresses the challenge of reliably estimating training-point influence on test predictions in deep learning vision models. The method treats algorithmic stability as a real-time controller for online inverse-curvature computation, maintaining lightweight anchor inverse Hessian-vector products via stochastic Richardson and preconditioned Neumann iterations. SG-OIF modulates per-example influence scores using stability-guided residual thresholds, anomaly gating, and confidence calibration. The framework demonstrates state-of-the-art performance on noise-label and out-of-distribution detection tasks across multiple datasets while maintaining low computational overhead.

## Method Summary
SG-OIF maintains a bank of K anchor inverse Hessian-vector products (IHVPs) updated via stochastic Richardson and preconditioned Neumann iterations. The framework computes a stability proxy β̃t based on learning rate ηt, average gradient norm Ḡt, and weight decay, which sets a time-varying residual tolerance τt. During IHVP computation, if the solver residual rv exceeds this tolerance, the confidence weight cv is reduced, effectively gating unreliable influence scores. The method uses a low-rank subspace decomposition with a diagonal curvature backend and monitors Gram matrix conditioning to refresh anchors when coverage degrades.

## Key Results
- Achieves 91.1% accuracy in top 1% prediction samples on CIFAR-10 with 20% asymmetric noise
- Obtains 99.8% AUPR score on MNIST for OOD detection
- Reduces noisy sample detection time by 97% compared to batch influence computation
- Demonstrates 31-34 point AUPR improvement when stability gating is active

## Why This Works (Mechanism)

### Mechanism 1: Stability-Guided Residual Gating
The framework monitors solver residuals rv = gv - Htφv and modulates each influence score by a confidence weight cv ∈ [0,1]. When rv exceeds the tolerance τt derived from the stability proxy β̃t, cv is clipped, suppressing unreliable influence scores during non-stationary training phases.

### Mechanism 2: Low-Rank Subspace Acceleration
Each IHVP is decomposed as φv = Qr av + uv, where Qr captures dominant curvature directions and uv is handled via a diagonal backend. This Woodbury-style factorization enables fast inverse applications while preserving directional accuracy for the most influential curvature dimensions.

### Mechanism 3: Anchor-Based Projection Coverage
The framework maintains a set of anchor vectors Φ and monitors the minimum eigenvalue λmin(G) of the Gram matrix G = Φ⊤Φ. When λmin(G) drops below 0.1, anchors are refreshed to ensure new gradients can be well-approximated by existing anchors, bounding projection error.

## Foundational Learning

- **Inverse Hessian-Vector Products (IHVP)**: Required because influence functions need H⁻¹v, but the Hessian is massive and often singular. Understanding iterative solvers is key to grasping Richardson and Neumann updates.
  - Quick check: Why can't we just compute `torch.inverse(Hessian)` for a ResNet-50?

- **Algorithmic Stability (Uniform Stability)**: The paper uses stability theory as a proxy for estimation reliability. You must understand that a stable algorithm has bounded sensitivity to input changes.
  - Quick check: If a model is "uniformly stable," does changing one training label drastically change the test loss?

- **Stochastic Iterative Solvers (Richardson & Neumann)**: These iterations update influence estimates online without recomputing from scratch. Richardson iteration is a fixed-point method.
  - Quick check: What happens to a Richardson iteration if the step size ρt is set too high relative to the spectral radius of the Hessian?

## Architecture Onboarding

- **Component map**: Controller -> Solver -> Backend -> Scorer
- **Critical path**: Sample Minibatch → Compute Gradients gi → Update Curvature Surrogate Ht → Update Anchors (solve for φv) → Gate (compute residual rv and confidence cv) → Score (aggregate influence scores)
- **Design tradeoffs**: Backend Fidelity vs. Speed (Diagonal fastest but highest Ecurv; Hybrid Low-Rank most accurate but O(r²d) overhead); Anchor Bank Size K (larger K improves coverage but increases memory/compute)
- **Failure signatures**: Confidence Collapse (cv drops to 0 for all anchors); Anchor Stagnation (λmin(G) remains low); Influence Oscillation (scores fluctuate early in training)
- **First 3 experiments**: 1) Noise Detection Baseline on CIFAR-10 with 20% asymmetric noise to verify ~91% P@1%; 2) Ablation on Gating by removing confidence gate and plotting variance; 3) Backend Comparison swapping Diagonal to Low-Rank on MNIST to observe time vs AUPR trade-off

## Open Questions the Paper Calls Out

### Open Question 1
How well does SG-OIF generalize to non-vision domains such as NLP, tabular data, or time series? All experiments are on vision benchmarks; curvature properties and gradient dynamics may differ substantially across modalities.

### Open Question 2
How sensitive is SG-OIF to hyperparameters κ (stability scale), K (anchor count), and r (low-rank dimension)? The paper treats κ as a tunable constant without sensitivity analysis, and optimal configurations may be dataset-dependent.

### Open Question 3
How robust are theoretical guarantees when Polyak-Łojasiewicz conditions or Hessian moment bounds are violated? The paper assumes these conditions without empirical validation, but deep networks often exhibit non-convex landscapes where PL conditions may not hold.

### Open Question 4
How does SG-OIF perform on additional governance tasks like data poisoning localization, selective unlearning, and curriculum reweighting? Only noisy-label and OOD detection are evaluated experimentally, though the framework claims broader applicability.

## Limitations
- The theoretical justification for stability as a direct controller for inverse-curvature estimation accuracy remains incompletely established
- The framework is only evaluated on vision datasets, limiting generalizability claims to other domains
- No rigorous error bounds connect stability metrics to actual influence ranking fidelity

## Confidence
- **High Confidence**: Empirical performance claims (AUPR scores, P@1% results) are well-supported by experimental tables and ablation studies
- **Medium Confidence**: Stability-guided gating effectiveness is demonstrated empirically, but theoretical relationship requires further validation
- **Low Confidence**: Claim that online anchor maintenance achieves bounded projection error lacks complete proof

## Next Checks
1. **Ablation on Stability Gating**: Remove confidence gating entirely and measure AUPR degradation across all datasets to quantify gating contribution
2. **Stability Proxy Sensitivity**: Systematically vary κ and preconditioner P to map hyperparameter sensitivity space and identify optimal configurations
3. **Convergence Analysis**: Track solver residual norms ||rv|| over training epochs to verify they remain within claimed bounds and correlate with influence score stability