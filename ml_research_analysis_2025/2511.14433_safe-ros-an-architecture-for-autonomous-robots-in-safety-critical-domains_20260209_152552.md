---
ver: rpa2
title: 'Safe-ROS: An Architecture for Autonomous Robots in Safety-Critical Domains'
arxiv_id: '2511.14433'
source_url: https://arxiv.org/abs/2511.14433
tags:
- safety
- autonomous
- verification
- system
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents Safe-ROS, an architecture for autonomous robots
  in safety-critical domains. The approach features two distinct subsystems: (1) a
  Safety-Related Autonomous System (SRAS) for normal operations, and (2) a Safety
  System implementing Safety Instrumented Functions (SIFs) for independent oversight.'
---

# Safe-ROS: An Architecture for Autonomous Robots in Safety-Critical Domains

## Quick Facts
- arXiv ID: 2511.14433
- Source URL: https://arxiv.org/abs/2511.14433
- Reference count: 40
- Primary result: Safe-ROS provides verifiable safety oversight for deploying autonomous robots in safety-critical domains using dual subsystems

## Executive Summary
This paper presents Safe-ROS, an architecture for autonomous robots operating in safety-critical domains. The approach features two distinct subsystems: a Safety-Related Autonomous System (SRAS) for normal operations and a Safety System implementing Safety Instrumented Functions (SIFs) for independent oversight. The SIF is implemented as a cognitive agent programmed to stop the robot when too close to obstacles. Verification is achieved through model checking in MCAPL and deductive verification in Dafny, ensuring the SIF meets formal safety requirements. The architecture is validated in Gazebo simulation and lab testing using an AgileX Scout Mini robot for nuclear inspection tasks.

## Method Summary
Safe-ROS implements a dual-subsystem architecture where the Safety-Related Autonomous System (SRAS) uses standard ROS navigation stack while the Safety System implements a BDI agent for independent safety oversight. Requirements are formalized in FRET, translated to LTL, and verified using AJPF model checking. The safety logic is implemented in Gwendolen (Java) and integrated via java_rosbridge. A cmd_vel_interceptor orchestrator node, verified in Dafny, ensures safety commands override navigation commands. The system is tested in Gazebo simulation and physical lab environments using an AgileX Scout Mini robot platform.

## Key Results
- Formal verification confirms the SIF satisfies safety properties through model checking and deductive verification
- Gazebo simulation and physical testing demonstrate effective collision avoidance in nuclear inspection scenarios
- Architecture successfully separates safety-critical functions from navigation logic while maintaining operational effectiveness

## Why This Works (Mechanism)

### Mechanism 1: Diverse Oversight for Probabilistic Systems
- Separating SS from SRAS mitigates lack of guarantees in probabilistic motion planning
- Safety Wrapper implemented as BDI agent in Gwendolen subscribes to sensor data independently
- Agent commands stop via high-priority topic when safety bounds breached
- Break condition: Shared hardware/middleware failures compromise independent oversight

### Mechanism 2: Traceable Formal Verification
- Natural language requirements translated to formal logic for verification
- FRET requirements converted to LTL and verified against Gwendolen agent code
- Confirms properties like "Globally, if too_close implies Eventually stopped"
- Break condition: Mischaracterization of "eventually" timing vs real-world stopping distance

### Mechanism 3: Hierarchical Control Interception
- Verified software interceptor guarantees safety commands override navigation
- Python cmd_vel_interceptor node mathematically guarantees zero-velocity output on stop signal
- Break condition: ROS network latency could delay stop signal reaching interceptor or motors

## Foundational Learning

**Concept: Belief-Desire-Intention (BDI) Agents**
- Why needed: Safety System is cognitive agent in Gwendolen, not simple script
- Quick check: How does agent update belief base when too_close percept received?

**Concept: Linear Temporal Logic (LTL)**
- Why needed: Verification relies on mapping "robot shall stop" to G(too_close â†’ F stopped)
- Quick check: Why does "Eventually" instead of "Next" impose timing limitation?

**Concept: Safety Instrumented Functions (SIFs)**
- Why needed: Industrial concept defining SS role as function bringing system to safe state
- Quick check: Is SIF part of navigation stack or external supervisor?

## Architecture Onboarding

**Component map:** FRET Requirements -> Gwendolen Agent -> AJPF Model Checking; ROS Noetic -> SRAS Navigation Stack -> cmd_vel_interceptor -> Robot Hardware

**Critical path:**
1. Requirement elicitation in FRET
2. Agent implementation in Gwendolen
3. Model checking in AJPF
4. Integration logic verification in Dafny
5. Runtime interception via cmd_vel_interceptor

**Design tradeoffs:**
- Latency vs. Verifiability: High-level BDI agent adds overhead but allows formal verification
- Abstraction vs. Reality: Dafny proof covers logic, not ROS implementation details

**Failure signatures:**
- False Positives: Robot stops frequently in open space (check LiDAR noise thresholds)
- Silent Failures: Robot fails to stop (check java_rosbridge connection or interceptor priority)

**First 3 experiments:**
1. Verify Orchestrator: Run Dafny code to confirm cmd_vel_callback post-conditions hold
2. Simulate Dynamic Obstacles: In Gazebo, move obstacle within 5cm to trigger too_close percept
3. Stress Test Interceptor: Command forward while publishing True to /gwendolen_control to confirm immediate halt

## Open Questions the Paper Calls Out

**Open Question 1:** How can architecture extend to verify real-time safety properties given model checker limitations on explicit time dependencies?
- Basis: AJPF doesn't support LTL "next" operator, forcing reliance on "eventually" semantics
- Why unresolved: Current approach approximates timing constraints, insufficient for strict reaction latency requirements
- Resolution evidence: Verification workflow integrating metric temporal logic or code-extraction pipeline for timing analysis

**Open Question 2:** How can SIF upgrade to execute complex recovery behaviors while remaining formally verifiable?
- Basis: Current SIF limited to stopping; future work explores returning to safe locations
- Why unresolved: Intelligent recovery requires complex logic that must still be proven safe
- Resolution evidence: Verified Gwendolen agent with navigation goals demonstrated in simulation

**Open Question 3:** How can architecture establish formal guarantees regarding system-level safety from SRAS-SS interaction?
- Basis: Architecture doesn't formally establish how system-level safety emerges from interaction
- Why unresolved: Verifying components in isolation doesn't prove combined runtime interaction prevents all unsafe outcomes
- Resolution evidence: Compositional verification argument or system-level model proving SS mitigates all SRAS hazards

**Open Question 4:** How can correctness of abstraction layer mapping ROS sensor data to agent beliefs be formally validated?
- Basis: Abstracting ROS topics into beliefs raises questions about sensor validity and translation correctness
- Why unresolved: Verification assumes beliefs match reality; buggy parsing means agent operates on false premises
- Resolution evidence: Formal contract or test harness proving environment correctly updates beliefs from ground truth data

## Limitations
- Safety verification gaps between formal models and unverified ROS middleware components
- Hardware independence assumption not physically validated in current implementation
- Single-obstacle verification limits safety guarantees in dynamic environments

## Confidence

**High Confidence:**
- Architectural separation between SRAS and SS is clearly implemented and demonstrated
- Dafny verification of cmd_vel_interceptor logic is mathematically sound and reproducible
- AJPF model checking workflow from FRET to LTL is well-documented

**Medium Confidence:**
- Safety guarantee claims depend on unverified ROS middleware bridging formal verification to physical execution
- "Eventually" timing semantics may not provide sufficient guarantees for physical robot dynamics

**Low Confidence:**
- Real-world performance beyond controlled simulation and lab testing not demonstrated
- Industrial SIF independence requirements architecturally described but not physically validated

## Next Checks
1. Middleware Timing Analysis: Instrument java_rosbridge and cmd_vel_interceptor to measure message latency under various load conditions
2. Hardware Isolation Test: Implement SIF on separate computing platform to validate independent stopping capability
3. Dynamic Obstacle Scenario: Extend Gazebo tests to include moving obstacles with varying velocities for realistic validation