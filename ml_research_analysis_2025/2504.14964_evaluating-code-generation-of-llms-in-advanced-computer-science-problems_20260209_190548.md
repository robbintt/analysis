---
ver: rpa2
title: Evaluating Code Generation of LLMs in Advanced Computer Science Problems
arxiv_id: '2504.14964'
source_url: https://arxiv.org/abs/2504.14964
tags:
- problem
- code
- programming
- llms
- solutions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates how well Large Language Models (LLMs) like
  GitHub Copilot, CodePal, and Hugging Face chatbots can solve advanced programming
  assignments in Java, Python, and C. The authors test 12 problems from introductory
  and advanced computer science courses, using 1000 random test cases per problem
  to measure accuracy.
---

# Evaluating Code Generation of LLMs in Advanced Computer Science Problems

## Quick Facts
- arXiv ID: 2504.14964
- Source URL: https://arxiv.org/abs/2504.14964
- Reference count: 0
- LLMs struggle with advanced CS problems but perform well on introductory ones

## Executive Summary
This paper evaluates how well Large Language Models (LLMs) like GitHub Copilot, CodePal, and Hugging Face chatbots can solve advanced programming assignments in Java, Python, and C. The authors test 12 problems from introductory and advanced computer science courses, using 1000 random test cases per problem to measure accuracy. While LLMs perform well on introductory problems, they struggle with advanced assignments—only a few achieve 100% accuracy. However, many LLMs identify the correct algorithms but fail to handle specific constraints or implement full solutions, often defaulting to heuristics. The study shows that GitHub Copilot performs best, and that programming language choice affects success rates. These insights can help educators design assignments that better assess student understanding beyond LLM capabilities.

## Method Summary
The study generates code solutions for 12 programming problems (3 CS1 baseline, 9 CS4-CS5 advanced) across multiple algorithms including bin-packing, TSP, MST, shortest path variants, and dynamic programming. Four LLMs (GitHub Copilot, CodePal, Llama3-70b, Mistral-Nemo-Instruct-2407) are queried in Python, Java, and C using three-part prompts (problem description, input format, output format). Solutions are verified against reference implementations using 1000 random test cases per problem with fixed random seeds. Minor fixes are permitted (output formatting, missing imports, main() calls) but no algorithm changes are allowed.

## Key Results
- LLMs show clear performance gaps between introductory (CS1) and advanced (CS4/CS5) problems
- GitHub Copilot consistently outperforms other models, achieving highest accuracy rates
- Programming language choice significantly affects success rates, with C producing more compilation/runtime errors
- LLMs frequently identify correct algorithms but implement heuristic approximations or ignore specific constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs solve introductory programming problems effectively but struggle with advanced problems, often producing partial solutions or heuristics that ignore stated constraints.
- Mechanism: During training, models ingest vast amounts of public code. Introductory problems (e.g., CS1 tasks) are well-represented, allowing models to retrieve and adapt solutions. For advanced problems (CS4/CS5), the models often identify the general algorithmic class from the prompt (e.g., "bin-packing," "shortest path") but fail to implement the full solution respecting all constraints, defaulting instead to simpler, common heuristics present in the training data.
- Core assumption: The accuracy of LLM-generated code is correlated with the prevalence and similarity of problem/solution pairs in the training data. More unique or constrained problems are less likely to be solved correctly without specific guidance.
- Evidence anchors:
  - [abstract] "LLMs perform well on introductory problems, they struggle with advanced assignments... often defaulting to heuristics."
  - [section 3] The motivating example shows GitHub Copilot generated a greedy heuristic for the bin-packing problem, not an optimal solver.
  - [section 5.1] "The results in Table 2 show that there is a clear difference between the baseline CS1 problems, P1 to P3, and the problems from more advanced courses, P4 to P12."
  - [corpus] From "Ensuring Computer Science Learning in the AI Era," the challenge of "undermining student learning through cognitive offloading" implies LLMs can solve standard tasks. "From Bugs to Breakthroughs" focuses on novice errors, suggesting the problems where LLMs excel are well-documented in educational research.
- Break condition: This mechanism would not hold if an LLM consistently solved novel, highly constrained advanced problems without prior exposure or with minimal prompt engineering. The paper notes extensive prompt engineering was not performed, a key limitation.

### Mechanism 2
- Claim: The choice of programming language affects LLM code generation success, with C proving significantly more problematic than Python or Java.
- Mechanism: Performance disparity stems from the volume and pedagogical focus of code in training corpora and language complexity. Python and Java have massive open-source and educational codebases. C, often used in specialized systems programming, has fewer educational examples and introduces complex memory management, leading to more compilation and runtime errors.
- Core assumption: The quantity and quality of a language's code in training data, combined with its syntactic complexity, directly impact an LLM's ability to generate runnable code.
- Evidence anchors:
  - [abstract] "...programming language choice affects success rates."
  - [section 5.3] "LLM tools fail to generate functional C code. In particular, 12 solutions result in either compilation or runtime errors... LLMs produced twelve solutions with Python, ten with Java, and seven with C that partially solved the problems."
  - [corpus] Corpus evidence on this specific language-based mechanism is weak or missing in the provided neighbors, which focus on general education and plagiarism.
- Break condition: If all tested LLMs performed equally well across all languages, or if C outperformed Python/Java, this mechanism would be invalidated.

### Mechanism 3
- Claim: Specialized code-generation models (ACGs) outperform general-purpose chatbots on advanced programming tasks.
- Mechanism: ACGs like GitHub Copilot and CodePal are fine-tuned on code corpora, whereas general chatbots are trained on a broader mix of text and code. This specialization gives ACGs a stronger internal representation of syntax and semantics, making them more effective at translating complex descriptions into functional code, even for problems they cannot solve perfectly.
- Core assumption: Specialized training on code provides a measurable advantage for code generation over general-purpose training, especially for tasks requiring precise algorithmic implementation.
- Evidence anchors:
  - [abstract] "...GitHub Copilot performs best..."
  - [section 5.3] "Out of 36 solutions, GC produces only a single non-functional solution... while Llama and Mistral generate a larger number of non-functional solutions."
  - [section 5.1] "In all these cases, Github Copilot is the LLM that generates correct results" for the few advanced problems solved.
  - [corpus] Corpus evidence on this specific comparison between ACGs and chatbots is weak or missing in the provided neighbors.
- Break condition: If a general-purpose chatbot consistently outperformed a specialized ACG on a diverse set of advanced problems, this mechanism would not hold.

## Foundational Learning

- Concept: **Algorithmic Complexity and NP-Completeness**
  - Why needed here: To understand why LLMs struggle with certain problems. The "bin-packing" problem is NP-complete; thus, optimal solutions are computationally hard, and training data likely contains more heuristic implementations.
  - Quick check question: Why would an LLM generate a greedy heuristic for an NP-complete problem like "bin-packing" instead of an optimal solver?

- Concept: **Prompt Engineering and Constraint Specification**
  - Why needed here: The paper's central finding is that LLMs often identify the correct algorithm but fail on specific constraints. Understanding how to craft precise prompts to enforce these constraints is critical for effective use.
  - Quick check question: Based on the paper, what is the primary reason LLMs fail on advanced problems even when they identify the correct algorithm?

- Concept: **Training Data Bias and Common Patterns**
  - Why needed here: The core mechanism for failure is that LLMs default to common heuristics found in their training data, which may not satisfy a problem's specific requirements. Users must understand that LLMs are probabilistic engines, not logical reasoners.
  - Quick check question: An LLM generates a correct but suboptimal solution for a constrained shortest-path problem. What is a likely explanation for this behavior based on the study's findings?

## Architecture Onboarding

- Component map: User Prompt -> LLM Core -> IDE/Interface -> Verifier/Test Suite
- Critical path:
  1. **Problem Definition**: An advanced programming assignment with specific constraints is defined.
  2. **Prompt Formulation**: The problem is translated into a text prompt for the LLM.
  3. **Code Generation**: The LLM produces source code. This is the primary failure point (heuristic defaults, syntax errors).
  4. **Verification**: The generated code is executed against the test suite to measure accuracy.
- Design tradeoffs:
  - **Specialization vs. Generalization**: Specialized ACGs (Copilot) yield fewer errors and higher accuracy than general chatbots.
  - **Language Choice**: Python/Java produce more runnable code and partial solutions; C produces more compilation/runtime errors, which may make it useful for designing "LLM-resistant" assessments.
  - **Evaluation Method**: A large, random test suite measures partial correctness but may miss subtle, deterministic logical flaws that a curated suite of edge cases would catch.
- Failure signatures:
  - **Heuristic Default**: Code solves a simplified problem, ignoring constraints. Accuracy is often high (e.g., 75.4%) but not 100%.
  - **Constraint Violation**: Correct algorithm is used, but a specific rule is missed (e.g., TV-Zapping's consecutive change rule).
  - **Non-functional Code**: Compilation errors (CE) or runtime errors (RE), especially in C.
  - **Infinite Loop (IL)**: Code fails to terminate on some inputs.
- First 3 experiments:
  1. **Baseline Replication**: Test the paper's advanced problem prompts against a newer model (e.g., GPT-4o). Observe if it better handles the specific constraints that caused older models to fail.
  2. **Constraint Isolation**: For a problem with a known constraint failure (e.g., TV-Zapping), run two trials: (A) original prompt, (B) rewritten prompt emphasizing the single constraint. Measure accuracy improvement.
  3. **Language Cross-Check**: Select a problem where the Python solution is a heuristic and the C solution has a compilation error. Fix the C error manually, then compare the algorithmic approach and accuracy of the fixed C code to the Python version.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent can systematic prompt engineering improve LLM accuracy on advanced programming assignments compared to the baseline prompts used in course materials?
- Basis in paper: [explicit] "In this paper, we have not performed extensive prompt engineering but rather used the original or slightly modified exercises... It is possible that prompt engineering may improve the results of LLMs in advanced courses. We leave this as a future work."
- Why unresolved: The study deliberately used original course prompts without optimization, leaving the potential gains from refined prompting unknown.
- What evidence would resolve it: A controlled experiment comparing baseline prompts against systematically engineered prompts across the same problem set, measuring accuracy improvements.

### Open Question 2
- Question: Why do LLMs frequently identify correct algorithms but then implement heuristic approximations instead of complete solutions?
- Basis in paper: [inferred] The paper documents cases where LLMs recognize problems (e.g., MST, dynamic programming) but generate suboptimal heuristics or ignore constraints. The authors note: "LLMs identify the base problem and provide partial solutions" and "often tend to implement popular heuristics instead of following the prompt instructions."
- Why unresolved: The behavioral analysis does not explain whether this stems from training data bias toward common implementations, inability to parse complex constraints, or other factors.
- What evidence would resolve it: Analysis of LLM internal reasoning or ablation studies varying constraint complexity to identify failure triggers.

### Open Question 3
- Question: What assessment modifications effectively distinguish genuine student understanding from LLM-assisted solutions in advanced CS courses?
- Basis in paper: [explicit] "We believe that there is a need to adjust CS course assessments for a fair assessment of the students' knowledge." The authors suggest incorporating "variations or constraints to known algorithms in advanced programming assignments."
- Why unresolved: The paper identifies the need but does not propose or test specific assessment redesign strategies.
- What evidence would resolve it: Empirical evaluation of redesigned assignments where LLMs struggle but students with genuine understanding succeed.

## Limitations

- The evaluation focuses on a specific set of 12 problems from particular courses, which may not generalize to all advanced CS assignments.
- Extensive prompt engineering was not performed, which could significantly affect results.
- The test suite uses random inputs with 1000 cases per problem, but the quality and distribution of these cases could influence accuracy measurements.
- The study doesn't explore hybrid approaches where human intervention could guide LLMs toward correct solutions.

## Confidence

**High Confidence**: The finding that LLMs struggle with advanced problems while performing well on introductory ones is well-supported by the empirical results showing clear performance gaps between CS1 and CS4/CS5 problems. The observation that GitHub Copilot performs best among tested models is also robust given the consistent pattern across problems.

**Medium Confidence**: The claim about language-specific performance differences (C being more problematic) is supported by the data but could be influenced by the specific test suite and prompt formulations used. The mechanism explaining why LLMs default to heuristics—training data bias toward common patterns—is plausible but requires further validation.

**Low Confidence**: The assertion that specialized code-generation models significantly outperform general-purpose chatbots, while observed in this study, may not hold across all problem types or with different model versions, given the rapid evolution of LLM capabilities.

## Next Checks

1. **Constraint Isolation Test**: For the TV-Zapping problem where LLMs failed on the consecutive change constraint, create two prompt variants—one original, one emphasizing this specific constraint. Measure accuracy improvement to validate the mechanism that LLMs identify correct algorithms but miss constraints.

2. **Cross-Language Algorithm Analysis**: For problems where Python solutions are heuristics but C solutions have compilation errors, manually fix the C compilation issues and compare the algorithmic approaches and accuracy between the corrected C and Python versions to test the language-performance hypothesis.

3. **Modern Model Comparison**: Replicate the evaluation with a current state-of-the-art model (e.g., GPT-4o or Claude 3.5) to determine if the performance gaps between CS1 and advanced problems persist, and whether newer models better handle the specific constraints that challenged older models.