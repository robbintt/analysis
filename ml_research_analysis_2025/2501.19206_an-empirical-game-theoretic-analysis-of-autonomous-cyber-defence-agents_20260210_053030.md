---
ver: rpa2
title: An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents
arxiv_id: '2501.19206'
source_url: https://arxiv.org/abs/2501.19206
tags:
- blue
- agents
- 'true'
- response
- vf-pbrs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of developing robust and generalizable
  autonomous cyber-defence (ACD) agents capable of handling dynamic, adversarial environments.
  The authors propose an empirical game-theoretic analysis using the principled double
  oracle (DO) algorithm, extended to multiple response oracles (MRO), to evaluate
  state-of-the-art deep reinforcement learning (DRL) approaches for ACD.
---

# An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents

## Quick Facts
- arXiv ID: 2501.19206
- Source URL: https://arxiv.org/abs/2501.19206
- Reference count: 40
- Key outcome: ACD agents trained via adversarial learning are robust against learning attackers; VF-PBRS oracles converge upon significantly stronger ACD policies compared to vanilla approaches.

## Executive Summary
This work addresses the challenge of developing robust and generalizable autonomous cyber-defence (ACD) agents capable of handling dynamic, adversarial environments. The authors propose an empirical game-theoretic analysis using the principled double oracle (DO) algorithm, extended to multiple response oracles (MRO), to evaluate state-of-the-art deep reinforcement learning (DRL) approaches for ACD. To expedite convergence, they introduce a value-function potential-based reward shaping (VF-PBRS) method that leverages value functions from previous iterations as a potential function. The evaluation, conducted on two cyber-defence gym environments (CC2 and CC4), demonstrates that ACD agents trained via adversarial learning are robust against learning attackers.

## Method Summary
The approach uses Multiple Response Oracles (MRO) to iteratively expand the strategy space by querying multiple heterogeneous DRL oracles against the opponent's current Nash mixture. Value-Function Potential-Based Reward Shaping (VF-PBRS) accelerates convergence by using value functions from previous iterations as a potential function, providing denser feedback during training. Pre-trained Models (PTMs) are used to reduce wall-time by initializing oracles from successful policies, though full random-initialization iterations may be needed to escape local optima. The framework constructs an empirical payoff matrix, computes Nash Equilibrium mixtures, and checks exploitability to determine convergence.

## Key Results
- ACD agents trained via adversarial learning are robust against learning attackers
- VF-PBRS oracles converge upon significantly stronger ACD policies compared to vanilla approaches
- The study highlights the importance of using multiple oracle approaches to identify diverse and effective ACD strategies

## Why This Works (Mechanism)

### Mechanism 1: Iterative Adversarial Expansion (MRO)
The MRO algorithm reduces defender exploitability by forcing the agent to adapt to the worst-case opponent mixture iteratively. It constructs an empirical payoff matrix and queries multiple heterogeneous oracles (different DRL architectures) against the opponent's current Nash mixture, adding successful responses to the policy pool until no oracle can find an exploit.

### Mechanism 2: Value-Function Potential-Based Reward Shaping (VF-PBRS)
VF-PBRS accelerates best-response oracle convergence by mitigating temporal credit assignment problems. It uses value functions from previous successful iterations as a potential function, generating shaping rewards that guide agents toward previously identified high-value states, effectively "warm-starting" the learning process with historical strategic knowledge.

### Mechanism 3: Pre-trained Model (PTM) Sampling
PTMs reduce wall-time by initializing oracles from successful policies based on current mixture weights. This allows refinement of existing successful policies rather than rediscovering basic strategies, though periodic full random-initialization iterations may be needed to escape local optima.

## Foundational Learning

- **Double Oracle (DO) Algorithm & Nash Equilibrium**: This is the theoretical backbone that outputs a mixture (distribution) over policies constituting a Nash Equilibrium. Understanding this is required to interpret why the system cycles through training and evaluation. Quick check: Why does the system add a new policy to the matrix only when the current mixture is "exploitable"?

- **Potential-Based Reward Shaping (PBRS)**: Ensures the "acceleration" mechanism is valid by distinguishing between generic reward hacking and potential-based shaping. Without this understanding, one might incorrectly modify the reward function and break convergence guarantees. Quick check: If I add a shaping reward that encourages the agent to stay alive longer, does that preserve the optimal policy? (Answer: Only if formulated as a potential difference).

- **Partially Observable Markov Games (POMGs)**: The agents operate with partial views of the network state, requiring the architecture to handle uncertainty and non-stationarity (opponent changes). This is distinct from standard single-agent MDPs. Quick check: How does the agent construct a state representation if it cannot see the entire network topology or the attacker's location?

## Architecture Onboarding

- **Component map**: Environment Wrapper (CybORG) -> Oracles (DRL training loops) -> Game Theory Engine (empirical payoff matrix, Nash equilibrium, exploitability) -> Meta-Controller (orchestrates loop)

- **Critical path**: Initialize matrix with base policies → Solve matrix for Nash Mixture → Check Exploitability → Trigger oracles if exploitable → Oracles train against sampled opponent mixture → Evaluate new policies to augment matrix → Repeat until exploitability threshold met

- **Design tradeoffs**: MRO vs. ADO (MRO evaluates multiple oracle types per iteration for higher quality/diversity but increases computational cost exponentially); PTM vs. Random Init (PTM saves wall-time but risks cycling in local optima if opponent shifts strategy drastically)

- **Failure signatures**: Matrix Explosion (payoff matrix grows too large to solve/manage), False Convergence (oracle fails to find better response due to bugs or insufficient training steps), Reward Hacking (incorrect PBRS implementation that breaks theoretical guarantees)

- **First 3 experiments**: 1) Baseline ADO with random initializations and no reward shaping on CC2 to establish convergence baseline; 2) Same ADO loop with VF-PBRS enabled to measure environment steps reduction; 3) MRO with two distinct Blue architectures (e.g., GPPO vs. Cardiff PPO) to analyze final mixture weighting

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the choice of the discount factor γ in VF-PBRS affect convergence and stability of the double oracle algorithm?
- **Basis**: Appendix C notes finding a suitable γ is challenging and states "Evaluating the impact of the value of γ on VF-PBRS for double oracles represents an interesting avenue for future research."
- **Why unresolved**: Authors set γ=1 to satisfy specific potential function properties but acknowledge standard discount rates often violate necessary conditions.
- **What evidence would resolve it**: An ablation study comparing empirical convergence rates and final policy exploitability when varying γ in the VF-PBRS formulation.

### Open Question 2
- **Question**: What mechanisms can effectively manage exponential growth of the empirical payoff matrix in MRO without permanently discarding potentially useful strategies?
- **Basis**: Section 8 discusses increased complexity of augmenting the payoff matrix and notes that pruning dominated strategies risks discarding policies that may become relevant in future iterations.
- **Why unresolved**: Paper identifies complexity issue and risk of premature pruning but does not implement or test specific dynamic pruning or re-evaluation strategy.
- **What evidence would resolve it**: Comparative analysis of pruning heuristics showing reduction in computational overhead without degrading quality of final Nash equilibrium.

### Open Question 3
- **Question**: How does the choice of PTM (generalist policies vs. historical best responses) impact robustness and convergence speed of the MRO framework?
- **Basis**: Section 7.1 observes that a "generalist" Red policy was a "poor PTM choice against advanced Blue mixtures" compared to full runs.
- **Why unresolved**: While paper demonstrates PTMs expedite training, it leaves open optimization of which PTM to sample to balance exploration and exploitation.
- **What evidence would resolve it**: Experiments contrasting performance of oracles initialized with generalist PTMs against those initialized with specific, high-performing historical policies across multiple iterations.

## Limitations
- Analysis focuses on specific cyber-defence scenarios (CC2/CC4) that may not generalize to real-world network topologies and attack patterns
- Payoff matrix construction relies on sampled evaluation episodes, introducing variance that may affect equilibrium detection
- Computational requirements for MRO are substantial, limiting scalability to larger strategy spaces

## Confidence
- **High Confidence**: The theoretical framework combining double oracle with value-function potential-based reward shaping is sound and well-grounded in game theory literature
- **Medium Confidence**: Empirical results showing improved convergence with VF-PBRS are convincing but need independent validation
- **Medium Confidence**: Claim that MRO produces more robust and generalizable policies is supported by experiments but would benefit from additional attack types and network configurations

## Next Checks
1. Conduct ablation studies removing VF-PBRS to quantify its exact contribution to convergence speed and final policy quality
2. Test learned policies against novel attacker strategies not seen during training to evaluate true robustness
3. Scale the approach to a larger strategy space with more oracle types to assess computational tractability in more complex scenarios