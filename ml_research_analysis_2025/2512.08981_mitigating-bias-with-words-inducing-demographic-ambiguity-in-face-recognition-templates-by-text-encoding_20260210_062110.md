---
ver: rpa2
title: 'Mitigating Bias with Words: Inducing Demographic Ambiguity in Face Recognition
  Templates by Text Encoding'
arxiv_id: '2512.08981'
source_url: https://arxiv.org/abs/2512.08981
tags:
- demographic
- bias
- face
- embeddings
- utie
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses demographic bias in face recognition (FR)
  systems, where face embeddings encode demographic-specific information that leads
  to performance disparities across groups. The authors propose UTIE, a novel approach
  that enriches face embeddings with features from other demographic groups to induce
  demographic ambiguity.
---

# Mitigating Bias with Words: Inducing Demographic Ambiguity in Face Recognition Templates by Text Encoding

## Quick Facts
- arXiv ID: 2512.08981
- Source URL: https://arxiv.org/abs/2512.08981
- Reference count: 40
- This paper proposes UTIE, a method that reduces demographic bias in face recognition by adding mean text embeddings of non-predicted demographic groups to face embeddings.

## Executive Summary
This paper addresses demographic bias in face recognition (FR) systems, where face embeddings encode demographic-specific information that leads to performance disparities across groups. The authors propose UTIE, a novel approach that enriches face embeddings with features from other demographic groups to induce demographic ambiguity. By leveraging Vision-Language Models (VLMs), UTIE adds the mean text embedding of all demographic groups (excluding the predicted one) to the image embedding, resulting in a more balanced representation across demographics. Evaluated on RFW and BFW datasets with three VLMs (CLIP, OpenCLIP, SigLIP), UTIE consistently reduces bias metrics (STD and SER) while maintaining or improving verification accuracy.

## Method Summary
The UTIE method works by first extracting an image embedding from a face using a frozen VLM backbone, then extracting text embeddings for demographic prompts (e.g., "A photo of a {label} person"). The model predicts the dominant demographic class via maximum cosine similarity, computes the mean of text embeddings from all other demographic groups, and adds this mean vector to the image embedding. This creates a more neutral representation that distributes similarity more evenly across demographic directions. The approach is evaluated using three VLMs (CLIP ViT-B/16, OpenCLIP ViT-B/16, SigLIP ViT-B/16) on RFW and BFW datasets, measuring verification accuracy, STD (standard deviation across group accuracies), and SER (Skewed Error Ratio).

## Key Results
- On RFW with CLIP, UTIE reduces STD from 4.81 to 4.46 and SER from 1.50 to 1.45 while improving accuracy from 72.20% to 72.25%.
- Similar bias reduction trends observed for gender bias on BFW dataset across all three VLMs.
- UTIE consistently outperforms baseline IE and IE+PTE configurations in reducing demographic bias metrics.
- Zero-shot demographic prediction accuracy ranges from 92-98% across models and datasets.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding the mean of non-predicted demographic text embeddings to image embeddings reduces demographic dominance while preserving identity information.
- Mechanism: Given an image embedding I, the model predicts the demographic class î via max cosine similarity to text embeddings Ti. It then computes T̄ = mean(Ti for i≠î) and produces I' = I + T̄. This shifts the embedding away from its dominant demographic direction toward a more neutral region where similarity is distributed across all demographic classes.
- Core assumption: Demographic attributes and identity features occupy partially separable directions in the joint embedding space, allowing demographic influence to be diluted without erasing identity signals.
- Evidence anchors:
  - [abstract] "we enrich the facial embeddings of each demographic group with text-derived demographic features extracted from other demographic groups. This encourages a more neutral representation in terms of demographic attributes."
  - [section 3.2] "By adding the mean embedding T̄ to the image embedding I, this operation aims to reduce the dominance of the predicted demographic class Tî. As a result, I' distributes more evenly across the directions associated with all demographic groups."
  - [corpus] Limited direct validation; corpus neighbors address bias in language models and face obfuscation but do not confirm this specific embedding arithmetic mechanism.
- Break condition: If demographic and identity features are highly entangled (same directions), adding T̄ could degrade verification accuracy or fail to reduce bias.

### Mechanism 2
- Claim: Zero-shot demographic prediction using text-image alignment is sufficiently accurate to identify the dominant demographic direction for exclusion.
- Mechanism: VLMs trained with contrastive objectives align semantically related image-text pairs. By encoding prompts like "A photo of a {label} person" for each demographic class, the text embeddings Ti capture demographic-specific semantic directions. The predicted class î is determined by maximum cosine similarity to the image embedding I.
- Core assumption: The VLM's cross-modal alignment generalizes to demographic attributes without task-specific fine-tuning.
- Evidence anchors:
  - [section 5.3] CLIP achieves 92.78% zero-shot accuracy on RFW and 98.42% on gender prediction on BFW; OpenCLIP reaches 92.94% and 97.42% respectively.
  - [section 3.2] "Since the prompt specifies only the demographic classes, the text embeddings Ti capture information that is ideally exclusively related to those demographics."
  - [corpus] DemoBias (arXiv:2508.19298) confirms demographic biases persist in vision foundation models but does not validate zero-shot prediction quality for this method.
- Break condition: If zero-shot prediction errors are frequent and systematic, the wrong demographic direction may be excluded, potentially increasing bias rather than reducing it.

### Mechanism 3
- Claim: IE+PTE (adding the predicted demographic embedding instead of excluding it) amplifies demographic bias, validating the directional logic of UTIE.
- Mechanism: I* = I + Tî reinforces alignment with the predicted demographic class, increasing the dominance of that attribute in the embedding. Higher STD and SER in IE+PTE compared to baseline demonstrate that demographic text embeddings carry meaningful bias-related signal.
- Core assumption: The text embedding Tî encodes a direction that, when added, increases demographic specificity rather than noise.
- Evidence anchors:
  - [section 5.1] IE+PTE consistently yields higher STD and SER across models; e.g., CLIP on RFW: STD increases from 4.81 (IE) to 5.01 (IE+PTE).
  - [section 4] "This configuration allows us to examine whether explicitly incorporating the predicted demographic attribute amplifies demographic bias in the embeddings."
  - [corpus] No direct corpus validation of this contrastive validation technique.
- Break condition: If IE+PTE does not increase bias metrics, the directional interpretation of Tî is unreliable and UTIE's mechanism is unsubstantiated.

## Foundational Learning

- **Vision-Language Models (VLMs) and Contrastive Learning**
  - Why needed here: UTIE depends on VLMs producing aligned image-text embeddings where demographic semantics in text transfer meaningfully to image space.
  - Quick check question: Can you explain why CLIP's contrastive loss creates a shared embedding space for images and text?

- **Zero-Shot Prediction with Text Prompts**
  - Why needed here: UTIE requires identifying the dominant demographic class without a trained classifier, using only prompt-based text embeddings.
  - Quick check question: Given text embeddings T1, T2, T3, T4 for four classes and an image embedding I, how would you compute the zero-shot predicted class?

- **Bias Metrics in Face Recognition (STD, SER)**
  - Why needed here: Evaluating UTIE requires interpreting STD (variation across group accuracies) and SER (ratio of max to min error rates).
  - Quick check question: If STD decreases from 4.81 to 4.46 and SER decreases from 1.50 to 1.45, does this indicate more or less demographic bias?

## Architecture Onboarding

- **Component map:**
  - Image encoder f_i(·) -> image embedding I
  - Text encoder f_t(·) -> text embeddings Ti
  - Zero-shot predictor -> predicted class î
  - Mean combiner -> T̄ = mean(Ti for i≠î)
  - Embedding fusion -> I' = I + T̄

- **Critical path:**
  1. Pre-compute Ti for all demographic classes using text encoder (once per class)
  2. Extract I from input face image
  3. Compute cos(I, Ti) for all i → identify î
  4. Compute T̄ excluding Tî
  5. Return I' as final template for verification

- **Design tradeoffs:**
  - Accuracy vs. fairness: UTIE reduces STD/SER but may slightly affect accuracy (observed changes are minimal but non-zero)
  - Inference overhead: One additional vector addition and N cosine similarity computations per image
  - Prompt sensitivity: Results depend on prompt phrasing ("A photo of a {label} person")

- **Failure signatures:**
  - Zero-shot prediction fails (low accuracy): T̄ computed with wrong exclusion, bias reduction unreliable
  - Identity degradation: Significant accuracy drop suggests T̄ overwrites identity-relevant directions
  - Asymmetric bias reduction: STD/SER improve for race but not gender (or vice versa), indicating demographic-specific effectiveness

- **First 3 experiments:**
  1. Reproduce baseline vs. UTIE comparison on RFW using CLIP ViT-B/16, reporting mean accuracy, STD, and SER to validate implementation.
  2. Ablate prompt design: test "A photo of a {label} person" vs. "{label} person" vs. "{label}" to measure prompt sensitivity on zero-shot prediction accuracy and bias metrics.
  3. Test failure mode: randomly shuffle demographic predictions (simulate zero-shot failure) and measure impact on STD/SER to quantify dependence on prediction quality.

## Open Questions the Paper Calls Out
None

## Limitations
- The paper's core claim that adding mean text embeddings of non-predicted demographics induces demographic ambiguity is based on cosine similarity changes rather than explicit validation of demographic disentanglement.
- The assumption that demographic features and identity features are disentangled enough for this approach to work without identity degradation is not empirically tested.
- The paper does not address whether the induced ambiguity might obscure legitimate demographic cues needed for certain downstream applications.

## Confidence
- **High confidence**: Verification accuracy maintenance and bias metric improvements (STD/SER reductions) are directly measurable and consistently reported across models and datasets.
- **Medium confidence**: The directional mechanism of UTIE (reducing dominance of predicted demographic class) is plausible given the observed bias amplification in IE+PTE, but lacks direct validation of embedding space geometry changes.
- **Low confidence**: The assumption that demographic and identity features occupy separable directions in embedding space is not empirically verified.

## Next Checks
1. Visualize embedding space before/after UTIE using t-SNE or UMAP to verify that demographic clusters become less distinct while identity neighborhoods remain intact.
2. Conduct ablation studies varying the number of demographic classes included in T̄ computation (e.g., using only 2 vs. all 4 race classes) to test the robustness of the demographic ambiguity induction.
3. Test cross-dataset generalization by training on one demographic subset and evaluating bias mitigation on held-out groups to verify that UTIE doesn't simply memorize dataset-specific demographic patterns.