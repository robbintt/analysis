---
ver: rpa2
title: 'NoReGeo: Non-Reasoning Geometry Benchmark'
arxiv_id: '2601.10254'
source_url: https://arxiv.org/abs/2601.10254
tags:
- tasks
- geometric
- task
- arxiv
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NoReGeo is a benchmark designed to evaluate whether large language
  models (LLMs) and vision-language models (VLMs) possess native geometric understanding,
  rather than relying on algebraic reasoning. It includes 2,500 elementary geometry
  problems across 25 categories, presented in both text-only and image-based (dotted/full)
  formats.
---

# NoReGeo: Non-Reasoning Geometry Benchmark

## Quick Facts
- **arXiv ID**: 2601.10254
- **Source URL**: https://arxiv.org/abs/2601.10254
- **Reference count**: 40
- **Primary result**: NoReGeo evaluates whether LLMs/VLMs have native geometric understanding; even best models achieve ~65% accuracy, well below human performance (~74.5%).

## Executive Summary
NoReGeo is a benchmark designed to evaluate whether large language models (LLMs) and vision-language models (VLMs) possess native geometric understanding, rather than relying on algebraic reasoning. It includes 2,500 elementary geometry problems across 25 categories, presented in both text-only and image-based (dotted/full) formats. Evaluations across 45+ models show that even the best models (e.g., GPT-4.1, Phi-3.5-Vision) achieve at most 65% accuracy, well below human performance (~74.5%). Full images significantly boost VLM performance compared to dotted or text-only inputs, but some models show minimal gains or even degrade, indicating weak visual grounding. Linear probing on frozen vision encoders reveals that geometric features are already embedded in visual representations, but LLMs do not effectively access or utilize them. These findings highlight a substantial gap in current models' geometric intuition and provide a tool for probing and advancing spatial reasoning in AI.

## Method Summary
NoReGeo evaluates 45+ models on 2,500 elementary geometry problems across 25 categories, using three input modalities: text-only, text + dotted images (points only), and text + full images (with connecting lines). The benchmark distinguishes between Classification, Numeric, and Unstable task types. Models must produce structured JSON outputs enforced via Outlines/xgrammar libraries. Linear probing experiments use CLIP-ViT-B/32 embeddings to assess whether geometric features are linearly separable in vision encoder representations. Performance is measured via accuracy (multiple-choice), soft accuracy (±0.5 tolerance for numeric answers), and MSE for numeric regression analysis.

## Key Results
- Even the best models (GPT-4.1, Phi-3.5-Vision) achieve at most 65% accuracy, below human performance (~74.5%).
- Full images significantly boost VLM performance on curved shapes and global geometry tasks (+40-100%), but some models (InternVL-3, G-LLaVA-13, URSA) show minimal gains or regression, indicating weak visual grounding.
- Linear probing on frozen CLIP-ViT-B/32 embeddings achieves 97.4% accuracy on full-image geometry tasks, while the same encoder's VLM achieves only 55-66% accuracy on identical inputs, revealing that geometric features exist in vision encoders but are not utilized by LLMs.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Geometric features are linearly accessible in vision encoder representations but remain unutilized by downstream LLM components in current VLM architectures.
- Mechanism: A simple linear probe trained on frozen CLIP-ViT-B/32 embeddings achieves 97.4% accuracy on full-image geometry tasks, while the same encoder's VLM achieves only 55-66% accuracy on identical inputs. This gap indicates that vision-language alignment during pretraining does not propagate geometrically salient features to the language head.
- Core assumption: The linear separability of geometric tasks in embedding space reflects the presence of task-relevant information, not just spurious correlations.
- Evidence anchors:
  - [section] "A fine-tuned ViT-B/32 linear probe achieves 97% accuracy on full images but drops to 58% on dot-only tests."
  - [section] "Through fine-tuning and linear-probing studies, we demonstrate that geometric knowledge exists latently in vision encoders but fails to naturally emerge in current LLM training paradigms."
  - [corpus] Related work "Do Large Language Models Truly Understand Geometric Structures?" similarly finds that final-answer metrics cannot measure true geometric structure understanding.
- Break condition: If probing accuracy dropped dramatically with larger/more diverse training sets, or if probes required highly non-linear transformations, this would suggest features are not readily accessible.

### Mechanism 2
- Claim: Full visual context (points plus connecting lines) significantly improves VLM geometric performance, but the magnitude varies across tasks and models.
- Mechanism: Tasks involving curved shapes, area comparisons, and global geometry show the largest gains (+40-100% accuracy) when full images replace dotted images. Linear or axis-aligned tasks (parallelism, perpendicularity) show minimal or no improvement, suggesting that sparse point representations already encode sufficient information for simpler spatial alignment.
- Core assumption: The difference between dotted and full images isolates the contribution of explicit geometric structure (edges, shapes) beyond point locations alone.
- Evidence anchors:
  - [section] "The largest average gains occur in tasks involving curved shapes and global geometry, such as Area comparison (ACM), Basic Coordinates Tasks (BCT) and Circle Properties (CPR)."
  - [section] "By contrast, linear or axis-aligned tasks, such as Parallelism, Perpendicularity, or Geometric Transformations (GTR), show minimal or no improvement."
  - [corpus] "Med-Scout" identifies "geometric blindness" in MLLMs for medical perception, reinforcing that visual grounding deficits generalize beyond synthetic benchmarks.
- Break condition: If models trained end-to-end from scratch showed no difference between dotted and full images, the mechanism would be about architectural capacity rather than visual representation quality.

### Mechanism 3
- Claim: Domain-specific fine-tuning (e.g., math-specialized models) can degrade instruction-following and structured output adherence, reducing geometric task performance.
- Mechanism: Math-specialized text-only models (Qwen2.5-Math-7B-Instruct) frequently ignore structured JSON prompts and misformat outputs, performing worse on classification and unstable tasks than general instruction-tuned peers. This suggests narrow fine-tuning can overfit to rigid formats and erode broader instruction-following capabilities.
- Core assumption: The performance drop is attributable to instruction-following degradation rather than reduced geometric reasoning capacity per se.
- Evidence anchors:
  - [section] "Math-specialized text-only models (e.g., Qwen2.5-Math-7B-Instruct) often ignore structured prompts and misformat outputs, performing worse on classification and unstable tasks than general instruction-tuned peers."
  - [abstract] "Further, our ablation experiments demonstrate that such geometric understanding does not emerge through fine-tuning alone, indicating that effective training for geometric comprehension requires a specialized approach from the outset."
  - [corpus] Weak direct evidence in corpus; related papers focus on surface equations and proof synthesis rather than fine-tuning degradation.
- Break condition: If math-specialized models performed well on geometric tasks when prompted without strict formatting, the mechanism would shift from instruction-following to format-compatibility.

## Foundational Learning

- Concept: **Linear Probing**
  - Why needed here: Used to assess whether geometric features exist in frozen vision encoder embeddings; directly informs the claim that information is present but inaccessible to LLMs.
  - Quick check question: If a linear classifier on frozen embeddings achieves 97% accuracy, what does this imply about the information content of those embeddings?

- Concept: **Vision-Language Grounding**
  - Why needed here: The paper's central finding is a grounding gap—visual features encode geometry but language components do not retrieve or reason with them effectively.
  - Quick check question: What architectural components mediate between the vision encoder output and the LLM input in a typical VLM?

- Concept: **Modalities in Geometry (Text vs. Visual)**
  - Why needed here: NoReGeo explicitly compares text-only, dotted-image, and full-image conditions to isolate which geometric properties are "textually recoverable" versus "inherently visual."
  - Quick check question: For collinearity detection, would you expect full images to substantially outperform text-only coordinate descriptions? Why or why not?

## Architecture Onboarding

- Component map:
  Vision Encoder -> Embeddings -> (Optionally) Frozen/Fine-tuned -> Linear Probe or VLM Adapter -> LLM Backbone -> Structured Output (JSON)

- Critical path:
  1. Input image → Vision encoder → Embeddings (49 patches + 1 CLS token)
  2. Embeddings → (Optionally) frozen or fine-tuned → Linear probe for probing experiments
  3. Embeddings → VLM's adapter → LLM → Structured output (JSON)
  The failure point identified: step 3's adapter/LLM does not effectively leverage geometric information present at step 2.

- Design tradeoffs:
  - **Frozen vs. fine-tuned encoder**: Freezing costs ~5 points in probe accuracy but prevents overfitting; the paper suggests limited adaptation is beneficial.
  - **Dotted vs. full images**: Full images boost performance on curved/complex tasks but may confuse undertrained VLMs on simpler tasks.
  - **General vs. math-specialized training**: Specialization can improve numeric precision but risks degrading instruction-following.

- Failure signatures:
  - Models that regress on full images compared to dotted images (e.g., InternVL-3, G-LLaVA-13, URSA) signal weak visual grounding or poor instruction adherence.
  - High MSE on numeric tasks with low classification accuracy indicates approximate but imprecise reasoning.
  - Frequent JSON formatting errors suggest instruction-following degradation, not necessarily geometric incompetence.

- First 3 experiments:
  1. **Benchmark a baseline VLM** on all three conditions (text-only, dotted, full) to establish performance gaps for your architecture.
  2. **Run linear probes** on your vision encoder across the 8 binary tasks to quantify latent geometric information; compare frozen vs. fine-tuned backbones.
  3. **Cross-task transfer analysis**: Train probes on one task (e.g., parallelism) and evaluate on related tasks (e.g., perpendicularity, right angles) to determine if geometric features generalize within families.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural or training modifications would enable LLMs to effectively access and utilize the geometric features already embedded in frozen vision encoder representations?
- Basis in paper: [explicit] The authors state: "a simple linear probe on a frozen vision encoder solves the tasks almost perfectly — suggesting that geometric features are present in embeddings but are not accessed by current LLM architectures or training regimes."
- Why unresolved: The paper demonstrates the disconnect exists (97% probe accuracy vs. ~65% best model accuracy) but does not investigate specific mechanisms for bridging the vision-encoder-to-LLM information gap.
- What evidence would resolve it: Ablation studies testing different cross-attention mechanisms, adapter modules, or projection layers that successfully transfer geometric features to the language model backbone while maintaining performance on language tasks.

### Open Question 2
- Question: What causes certain VLMs (e.g., InternVL-3, G-LLaVA-13, URSA) to show minimal gains or even performance degradation when provided with full images versus dotted images?
- Basis in paper: [explicit] "InternVL-3, G-LLaV A-13, and URSA gain little or regress, signaling weak visual grounding or instruction-following most pronounced on numeric items."
- Why unresolved: The paper identifies the phenomenon and speculates about causes (poor handling of complex images, degraded instruction adherence, overfitting to irrelevant patterns) but provides no empirical investigation.
- What evidence would resolve it: Systematic analysis correlating degradation magnitude with architectural choices, attention pattern visualizations on geometric vs. non-geometric image regions, and controlled experiments varying image complexity.

### Open Question 3
- Question: Can specialized training from the outset (rather than fine-tuning) develop native geometric understanding, and what form should such training take?
- Basis in paper: [explicit] "Our ablation experiments demonstrate that such geometric understanding does not emerge through fine-tuning alone, indicating that effective training for geometric comprehension requires a specialized approach from the outset."
- Why unresolved: The paper rules out post-hoc fine-tuning as sufficient but does not explore alternative training paradigms, data curricula, or objective functions.
- What evidence would resolve it: Comparison of models trained from scratch with geometric objectives (e.g., contrastive learning on geometric relations, auxiliary prediction of spatial properties) against standard pretraining regimes on NoReGeo performance.

## Limitations
- **Task Generalization**: NoReGeo focuses on elementary geometry concepts without compound reasoning requirements, leaving unclear whether models' geometric reasoning transfers to more complex, real-world spatial reasoning tasks.
- **Format vs. Reasoning**: Performance degradation in math-specialized models could stem from format adherence issues rather than genuine geometric understanding deficits, as the paper enforces strict structured JSON output.
- **Architecture Specificity**: Results are based primarily on CLIP-based VLMs; it's uncertain whether findings extend to other vision-language architectures or whether architectural modifications could bridge the identified grounding gap.

## Confidence
- **High Confidence**: Linear probing results demonstrating latent geometric features in vision encoders (97.4% accuracy vs. 55-66% VLM performance) are well-supported by direct experimental evidence and clear methodology.
- **Medium Confidence**: The claim that domain-specific fine-tuning degrades instruction-following is supported but relies on indirect comparisons across different model families rather than controlled experiments.
- **Medium Confidence**: The mechanism explaining why full images help more for curved shapes and global geometry than for linear tasks is plausible but based on aggregated performance patterns rather than task-specific ablation studies.

## Next Checks
1. **Format Ablation Study**: Evaluate the same models on NoReGeo tasks without strict JSON formatting requirements to determine whether performance improvements reflect genuine geometric understanding gains or format compliance.
2. **Multi-Step Geometry Extension**: Create compound geometry problems requiring sequential reasoning (e.g., using collinearity detection to infer shape properties) to test whether NoReGeo's elementary tasks capture models' full geometric reasoning capabilities.
3. **Cross-Architecture Comparison**: Benchmark SigLIP-based VLMs and native vision-reasoning models on NoReGeo to determine whether CLIP-specific architectural limitations drive the observed grounding gap.