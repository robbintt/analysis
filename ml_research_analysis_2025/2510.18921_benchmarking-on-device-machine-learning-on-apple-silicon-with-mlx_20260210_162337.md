---
ver: rpa2
title: Benchmarking On-Device Machine Learning on Apple Silicon with MLX
arxiv_id: '2510.18921'
source_url: https://arxiv.org/abs/2510.18921
tags:
- inputs-char-no
- batch-size
- apple
- roberta-base
- bert-base-uncased
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks the MLX framework for on-device machine learning
  on Apple Silicon devices. The authors created MLX-Transformers, a library enabling
  direct execution of transformer models from Hugging Face on Apple hardware without
  checkpoint conversion.
---

# Benchmarking On-Device Machine Learning on Apple Silicon with MLX

## Quick Facts
- arXiv ID: 2510.18921
- Source URL: https://arxiv.org/abs/2510.18921
- Reference count: 13
- Primary result: MLX-Transformers enables direct execution of Hugging Face transformer models on Apple Silicon with sublinear latency scaling, narrowing the performance gap with CUDA GPUs.

## Executive Summary
This paper benchmarks the MLX framework for on-device machine learning on Apple Silicon devices, introducing MLX-Transformers as a library that enables direct execution of transformer models from Hugging Face without checkpoint conversion. The authors evaluated BERT, RoBERTa, and XLM-RoBERTa models on M1 and M2 Max MacBooks and compared performance against an NVIDIA A10 GPU. Results demonstrate that while CUDA GPUs maintain superior absolute speed, the M2 Max significantly narrows the performance gap compared to M1, with inference latency scaling sublinearly with batch size on Apple Silicon hardware.

## Method Summary
The study benchmarks MLX framework inference latency on Apple Silicon devices versus PyTorch on CUDA GPU, using BERT (base/large), RoBERTa-base, and XLM-RoBERTa-base models from Hugging Face. The evaluation employs input lengths of 50/100/200/500 characters and batch sizes of 1/16/32, measuring inference latency (ms) across mlx-gpu, mlx-cpu, torch-cpu, and torch-cuda backends. Hardware configurations include 8GB M1 MacBook Pro, 32GB M2 Max MacBook Pro, and NVIDIA A10 (24GB) on AWS EC2. The methodology executes 10 iterations per configuration with 5 iterations for operations, incorporating warmup runs to mitigate JIT compilation effects.

## Key Results
- CUDA GPU outperforms Apple Silicon in absolute speed, but M2 Max narrows the gap compared to M1 (BERT-base: 23.46ms vs 38.23ms vs 179.35ms)
- Inference latency scales sublinearly with batch size on Apple Silicon, indicating good parallel processing efficiency
- MLX-Transformers demonstrates strong potential for efficient on-device ML applications on consumer devices, offering accessibility and cost advantages despite lower peak performance than dedicated GPUs

## Why This Works (Mechanism)
The MLX framework leverages Apple Silicon's unified memory architecture and GPU acceleration to execute machine learning workloads efficiently on consumer hardware. The sublinear scaling behavior indicates effective parallelization across the M2 Max's enhanced GPU cores and memory bandwidth compared to M1. MLX-Transformers provides seamless integration with Hugging Face's model ecosystem, eliminating the need for checkpoint conversion while maintaining competitive inference performance.

## Foundational Learning
- **Unified Memory Architecture**: Shared memory pool between CPU and GPU eliminates data transfer bottlenecks; needed to understand Apple Silicon's memory efficiency advantage; quick check: verify memory usage during peak batch processing
- **JIT Compilation Effects**: Initial slowdowns due to just-in-time compilation impact first-run measurements; needed to properly interpret latency variance; quick check: include multiple warmup iterations before timing measurements
- **Transformer Model Efficiency**: Encoder-only architectures (BERT/RoBERTa) have different computational characteristics than decoder models; needed to contextualize benchmark results; quick check: compare encoder vs decoder model performance patterns

## Architecture Onboarding
- **Component Map**: MLX Framework -> MLX-Transformers Library -> Hugging Face Models -> Apple Silicon Hardware -> Performance Metrics
- **Critical Path**: Model loading → Tokenization → GPU computation → Result synchronization → Latency measurement
- **Design Tradeoffs**: Direct model execution without conversion (simplicity) vs. potential optimization opportunities (performance); consumer hardware accessibility (cost) vs. peak performance limitations (speed)
- **Failure Signatures**: High first-run latency (JIT compilation), memory exhaustion with large batch sizes, backend selection errors causing CPU-only execution
- **Three First Experiments**: 1) Verify MLX is using GPU backend on Mac via device placement check, 2) Test warmup iteration count impact on stabilized latency measurements, 3) Monitor memory usage during peak batch size tests to identify bottlenecks

## Open Questions the Paper Calls Out
- **Open Question 1**: How does inference performance of MLX on Apple Silicon compare across different data modalities, such as vision or audio, relative to NLP transformers? The paper mentions intention to extend future work to include models of different modalities for comprehensive assessment, but current study strictly benchmarks text-based transformer models.
- **Open Question 2**: To what extent can on-device training of smaller models be optimized within the MLX framework on consumer Apple Silicon devices? The conclusion explicitly suggests exploring on-device training potential as future research direction, though paper focuses entirely on inference latency without training benchmarks.
- **Open Question 3**: What specific optimizations are required to enable larger, state-of-the-art generative models (e.g., LLaMA) to run efficiently on Apple Silicon? Authors mention supporting LLaMA in the library and discuss larger models in conclusion, yet only benchmark comparatively smaller encoder models.
- **Open Question 4**: How does the Unified Memory Model impact performance bottlenecks relative to raw compute limitations during high-load batch processing? Paper attributes M2 Max's superior performance partly to better hardware configuration and notes sublinear scaling, but does not isolate memory bandwidth as specific variable.

## Limitations
- Evaluation focuses on a narrow set of transformer models (BERT, RoBERTa, XLM-RoBERTa) and relatively small input sizes, potentially not representing full range of on-device ML workloads
- Hardware comparison uses different generations and configurations (M1 vs M2 Max vs A10) introducing confounding factors beyond software framework differences
- Relies on inferred details about input tokenization and measurement methodology rather than explicitly specified protocols

## Confidence
- **High confidence**: MLX-Transformers enables direct execution of Hugging Face transformer models on Apple Silicon without checkpoint conversion, and MLX inference latency scales sublinearly with batch size on Apple Silicon devices
- **Medium confidence**: M2 Max demonstrates significantly improved performance compared to M1, narrowing the gap with CUDA GPU performance, though absolute GPU performance remains superior
- **Low confidence**: Claims about practical implications for on-device ML applications would benefit from testing with more diverse model architectures, larger batch sizes, and more demanding inference scenarios

## Next Checks
1. **Reproduce core latency measurements** using the provided MLX-Transformers repository with identical model configurations, input lengths (50/100/200/500 characters), and batch sizes (1/16/32), ensuring proper GPU backend selection and including sufficient warmup iterations
2. **Validate scaling behavior** by testing additional batch sizes (64, 128) and input lengths to confirm sublinear scaling pattern holds across broader range of configurations and to identify any memory-bound or compute-bound regimes
3. **Extend model diversity** by benchmarking additional transformer variants (distilled models, encoder-decoder architectures) and non-transformer models (CNNs, MLPs) to assess whether observed performance characteristics generalize beyond initial BERT/RoBERTa/XLM-RoBERTa subset