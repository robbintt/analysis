---
ver: rpa2
title: 'Cartan Networks: Group theoretical Hyperbolic Deep Learning'
arxiv_id: '2505.24353'
source_url: https://arxiv.org/abs/2505.24353
tags:
- hyperbolic
- group
- solvable
- networks
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Cartan Networks, a new deep learning architecture
  based on hyperbolic spaces viewed as both Riemannian manifolds and solvable Lie
  groups. The key insight is to exploit the metric equivalence between hyperbolic
  spaces and solvable Lie groups, alternating group homomorphisms with metric-preserving
  diffeomorphisms in the network layers.
---

# Cartan Networks: Group theoretical Hyperbolic Deep Learning

## Quick Facts
- arXiv ID: 2505.24353
- Source URL: https://arxiv.org/abs/2505.24353
- Reference count: 40
- Primary result: Novel deep learning architecture using hyperbolic spaces as solvable Lie groups, showing competitive performance on classification and regression tasks without requiring exponential/logarithmic maps.

## Executive Summary
This paper introduces Cartan Networks, a deep learning architecture that leverages the metric equivalence between hyperbolic spaces and solvable Lie groups. The key insight is to perform group-theoretic operations instead of Riemannian exponential/logarithmic maps, alternating group homomorphisms with metric-preserving diffeomorphisms in network layers. The architecture shows competitive performance on synthetic and real-world datasets while maintaining geometric properties, with notably good depth scaling even without pointwise activation functions.

## Method Summary
The method exploits the metric equivalence M[1,1+q] ≃ SO(1,1+q)/SO(q) ≃ Exp(Solv[1,q]) to parametrize hyperbolic space using solvable Lie group coordinates. Networks are constructed by composing group homomorphisms with isometries, using solvable coordinates Υ = [Υ₁, Υ₂] with a single-chart domain ℝⁿ. The architecture alternates group operations (multiplication, fiber rotations) with linear transformations, and includes optional pointwise activation functions. Classification uses hyperbolic hyperplanes derived from geodesically complete submanifolds, with distance-based decision boundaries.

## Key Results
- Competitive performance on MNIST, Fashion-MNIST, KMNIST, and CIFAR-10 classification tasks
- Strong results on synthetic regression tasks with 5 toy datasets (200 train/1000 test samples)
- Cartan networks without pointwise activations show excellent depth scaling, suggesting intrinsic representational strength
- Numerical stability improvements over standard hyperbolic neural networks using exponential/logarithmic maps

## Why This Works (Mechanism)

### Mechanism 1
Hyperbolic spaces can be represented as solvable Lie groups, enabling group-theoretic operations instead of Riemannian exponential/logarithmic maps. The metric equivalence M[1,1+q] ≃ SO(1,1+q)/SO(q) ≃ Exp(Solv[1,q]) allows parametrization via solvable coordinates Υ = [Υ₁, Υ₂] with a single-chart domain ℝⁿ, avoiding numerical instability of Lorentz/Poincaré models at coordinate singularities.

### Mechanism 2
Composing group homomorphisms with isometries creates expressive layer transformations without requiring tangent-space nonlinearities. Each layer computes f_lin(Υ) = R_u(β ∗ [Υ₁; WΥ₂ + b]) where R_u is fiber rotation, β ∗ is group multiplication, and [Υ₁; WΥ₂ + b] is the homomorphism. The fiber rotation mixes Cartan and paint coordinates nonlinearly, and Υ₁ gets exponentiated in subsequent layers.

### Mechanism 3
Hyperbolic hyperplanes for classification are naturally derived from geodesically complete submanifolds, with distance-based decision boundaries. Hyperplanes H_{α,β,w} are isometric immersions of M[1,q] into M[1,1+q]. The classification function h(Υ) = αe^{-Υ₁} + ⟨w,Υ₂⟩ + βe^{Υ₁}(1+|Υ₂|²) relates to distance via d(Υ,H) = ½arccosh(1 + 2h²/(|w|²-4αβ)).

## Foundational Learning

- **Lie Groups and Lie Algebras**: The entire architecture relies on group operations (multiplication, inversion, homomorphisms) and their algebraic generators. Solvable Lie algebras define the coordinate structure. Quick check: Can you explain why solvable algebras have derived series terminating at zero, and how this relates to upper-triangular matrix representations?

- **Riemannian Manifolds and Isometries**: Hyperbolic space has intrinsic curvature affecting distance computation. Isometries preserve this structure while enabling flexible transformations. Quick check: What distinguishes a general diffeomorphism from an isometry, and why does the paper restrict to isometries for certain transformations?

- **Symmetric Spaces**: Hⁿ is a coset manifold G/H with specific algebraic decomposition. Understanding this structure explains why the solvable group equivalence exists. Quick check: Given the decomposition g = h ⊕ m with commutator relations [m,m] ⊂ h, what does this imply about geodesic structure?

## Architecture Onboarding

- **Component map**: Input embedding: x ∈ ℝᵈ → [0; x] ∈ M[1,1+d] → [linear]×L → classify
- **Critical path**: Implement solvable coordinate representation with matrix form (Eq. 25) → Implement group operation ∗ (Eq. 7) and inverse → Implement fiber rotation R_u (Eq. 11) - numerically sensitive due to log → Chain layers: embed → [linear]×L → classify → Use Riemannian Adam via Geoopt for optimization
- **Design tradeoffs**: No activations: Better depth scaling, worse per-parameter efficiency. With DiLU: Improved classification accuracy on complex datasets, breaks geometric purity. Dimension change: W can be non-square to project between manifold dimensions, useful for bottleneck architectures
- **Failure signatures**: Gradient explosion when Υ₁ becomes large (exponential terms in Eq. 11). Numerical instability in fiber rotation log if argument approaches zero. Poor convergence on non-hierarchical data (Euclidean baseline outperforms on some tasks)
- **First 3 experiments**: 1) Implement the group operation (Eq. 7) and verify associativity with random Υ,Ψ ∈ ℝⁿ; test inverse property Υ ∗ Υ⁻¹ = 0. 2) Train Cartan network on synthetic regression (Table 1 tasks) with varying depth (1-5 layers), measuring R² with/without DiLU to confirm depth-expressivity claim. 3) Embed uniform random points, verify distance computation (Eq. 41) matches expected hyperbolic distance distribution; compare to Poincaré ball implementation for numerical stability

## Open Questions the Paper Calls Out

- **Can the Cartan network framework be effectively generalized to other non-compact symmetric spaces beyond the hyperbolic geometry tested in this study?**
  - Basis: The discussion section states that "exploring more complex manifolds" is a major direction for future research, noting that hyperbolic space is merely the "simplest representative" of this family
  - Why unresolved: The current implementation and experiments are restricted to the solvable group structure of hyperbolic space
  - What evidence would resolve it: Successful implementation and benchmarking on other symmetric spaces (e.g., Grassmannians) demonstrating competitive performance

- **How can the group-theoretic operations of Cartan networks be specialized to handle convolutional or sequential data architectures?**
  - Basis: The authors list "investigating how to specialize our architecture to convolutional or sequential data" as a primary avenue for future work
  - Why unresolved: The current work focuses on fully connected architectures; the group homomorphisms and isometries defined have not been adapted for the local connectivity or recurrence required for these tasks
  - What evidence would resolve it: The derivation of group-theoretic convolutional filters or recurrent cells and their evaluation on standard image or sequence datasets

- **Can the computational overhead of Cartan networks be reduced through software optimization to match the efficiency of highly optimized Euclidean libraries?**
  - Basis: The paper acknowledges that "architectural modifications... lead to increased computational overhead" and identifies improving computational performance as "an important step to ensure its adoption"
  - Why unresolved: The current implementation suffers from latency due to the complex geometric operations compared to standard libraries optimized for Euclidean tensor operations
  - What evidence would resolve it: A optimized software library implementation showing training latency per epoch comparable to standard PyTorch Euclidean layers on identical hardware

## Limitations
- Current scope restricted to hyperbolic spaces; no validation on compact symmetric spaces where solvable group equivalence breaks down
- Expressivity claims rely on depth scaling rather than formal universal approximation results
- Numerical stability remains a concern, particularly in fiber rotation operations with large coordinates
- Performance degradation on complex datasets when geometric purity is maintained (no activations)

## Confidence
- **High confidence**: Mathematical foundations connecting hyperbolic spaces to solvable Lie groups - well-established in differential geometry literature
- **Medium confidence**: Architectural design and classification framework - novel constructions with empirical but limited theoretical validation
- **Low confidence**: Generalization beyond hyperbolic spaces and performance on highly complex datasets - only tested on simple image tasks and synthetic regression

## Next Checks
1. **Numerical stability audit**: Systematically test fiber rotation operations with varying input magnitudes, implementing gradient clipping and alternative retractions to identify failure thresholds
2. **Expressivity benchmark**: Compare depth scaling on synthetic hierarchical data with varying complexity, measuring both accuracy and gradient flow statistics across layers
3. **Cross-space validation**: Implement a minimal version for a compact symmetric space (e.g., sphere S²) to test framework extensibility and identify where the solvable group approach breaks down