---
ver: rpa2
title: 'LLMSR@XLLM25: An Empirical Study of LLM for Structural Reasoning'
arxiv_id: '2505.12328'
source_url: https://arxiv.org/abs/2505.12328
tags:
- reasoning
- arxiv
- language
- preprint
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The LLMSR@XLLM25 shared task evaluates large language models on
  fine-grained, controllable, and interpretable reasoning. The challenge requires
  extracting problem conditions, segmenting Chain-of-Thought rationales into statement-evidence
  pairs, and verifying logical validity at each step.
---

# LLMSR@XLLM25: An Empirical Study of LLM for Structural Reasoning

## Quick Facts
- arXiv ID: 2505.12328
- Source URL: https://arxiv.org/abs/2505.12328
- Authors: Xinye Li; Mingqi Wan; Dianbo Sui
- Reference count: 7
- Primary result: Achieved 75.26 macro F1 for condition extraction and 33.04 for statement verification without fine-tuning

## Executive Summary
This paper presents a lightweight approach to fine-grained structural reasoning using an off-the-shelf Meta-Llama-3-8B-Instruct model. The system tackles the LLMSR@XLLM25 shared task through a two-stage pipeline that first extracts problem conditions, then segments Chain-of-Thought rationales into statement-evidence pairs while verifying logical validity at each step. By carefully designing few-shot prompts and implementing a lightweight regex-based post-processor, the team achieved competitive results without model fine-tuning or external retrieval. The approach demonstrates that constrained reasoning with structured output can rival more complex pipelines, highlighting the potential of lightweight methods for structural reasoning tasks.

## Method Summary
The method employs a two-stage pipeline using Meta-Llama-3-8B-Instruct without fine-tuning. First, a Question Parsing (QP) stage with 2-shot demonstrations extracts all problem conditions from input questions. Second, a CoT Parsing & Verification (CP) stage with 3-shot demonstrations receives these conditions as context and segments rationales into statement-evidence pairs, adjudicating logical entailment at each step. Outputs are constrained to strict JSON format using fenced code blocks, with a lightweight regex post-processor handling schema validation and normalization. The approach relies on few-shot in-context learning patterns rather than weight updates, with stage-specific shot counts optimized to prevent context dilution.

## Key Results
- Achieved 75.26 macro F1 score on condition extraction (QP stage)
- Achieved 33.04 macro F1 score on statement verification (CP stage)
- Ranked 5th overall in the LLMSR@XLLM25 shared task
- Reduced JSON unparsable rate from 16% to 2% using fenced code blocks

## Why This Works (Mechanism)

### Mechanism 1: Few-shot In-Context Learning
- **Claim:** Few-shot demonstrations establish output patterns that guide structured generation without weight updates
- **Mechanism:** Hand-picked exemplars covering major logical patterns prime the model to produce structured JSON by pattern-matching
- **Core assumption:** Selected demonstrations span the distribution of test patterns; pattern exhaustion causes degradation
- **Evidence anchors:** 2-shot QP and 3-shot CP demonstrations jointly cover most patterns; adding shots beyond optimal causes context dilution (F1 drops from 0.3304 to 0.2978)

### Mechanism 2: Stage-separated Pipelines
- **Claim:** Stage-separated pipelines with cascaded context improve per-stage optimization
- **Mechanism:** QP extracts conditions first; CP receives those conditions as additional context
- **Core assumption:** QP errors propagate to CP; condition quality bounds downstream verification accuracy
- **Evidence anchors:** QP F1 (75.26) vs. verification F1 (33.04) gap suggests bottleneck at verification stage

### Mechanism 3: Constrained Output Formatting
- **Claim:** Constrained output formatting reduces generation failures
- **Mechanism:** Fenced code blocks and explicit JSON-only instructions reduce malformed output rate
- **Core assumption:** Model has sufficient instruction-following capacity to respect format constraints
- **Evidence anchors:** Unparsable rate reduced from 16% to 2% with fenced blocks; post-processor handles schema validation

## Foundational Learning

- **Few-shot In-Context Learning:**
  - Why needed: The entire system relies on demonstration-based pattern transfer without gradient updates
  - Quick check question: Can you explain why adding more shots (4-shot) degraded performance in Table 2?

- **Chain-of-Thought Decomposition:**
  - Why needed: The task requires segmenting CoT into statement–evidence pairs and verifying entailment at each step
  - Quick check question: What is the difference between outcome supervision and process supervision (PRM)?

- **Text Parsing with Regular Expressions:**
  - Why needed: Post-processing normalizes spans and enforces JSON schema deterministically
  - Quick check question: How would you handle a case where #statements ≠ #evidence in the output?

## Architecture Onboarding

- **Component map:**
Input Question -> [QP Stage: 2-shot prompt → Llama-3-8B → Extracted Conditions] -> [CP Stage: 3-shot prompt + QP output → Llama-3-8B → Statement–Evidence Pairs + Verifications] -> [Post-processor: Regex normalizer → Schema validator → JSON output]

- **Critical path:** QP accuracy → CP context quality → Verification F1. The bottleneck is logical adjudication, not extraction (75.26 vs. 33.04 F1).

- **Design tradeoffs:**
  - Lightweight (no fine-tuning/retrieval) vs. accuracy ceiling
  - Stage-specific shot counts vs. uniform prompting
  - Regex post-processing vs. neural validators

- **Failure signatures:**
  - JSON parse errors → check for unclosed braces, extra quotes
  - Hallucinated evidence → model paraphrases conditions instead of citing
  - Negation mishandling → verification verdicts inverted
  - Context dilution → performance drops when shot count exceeds optimal

- **First 3 experiments:**
  1. Replicate the shot-count ablation (k=1,2,3,4) for QP on the dev set to verify the 2-shot optimum
  2. Run CP with fixed 2-shot QP, varying CP shots to confirm 3-shot peak
  3. Ablate the JSON fencing instruction: measure unparsable rate with/without ```json``` blocks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would a dedicated verifier model (e.g., GPT-4o, Claude 3) decoupled from the generator significantly improve verification F1 scores?
- Basis in paper: "Verification may need a more capable judge (e.g., GPT-4o, Claude 3) detached from the generator"
- Why unresolved: Only evaluated single 8B model for both generation and verification
- What evidence would resolve it: Ablation experiments comparing same-model vs. decoupled generator-verifier setups

### Open Question 2
- Question: Can a lightweight, specialized entailment critic trained on synthetic pairs improve step-level verification accuracy?
- Basis in paper: "Training a small, dedicated critic on synthetic entailment pairs—à la CoT-Critic—could boost step-level faithfulness"
- Why unresolved: No such critic was implemented or tested
- What evidence would resolve it: Training a small critic model on synthetic entailment data and evaluating impact

### Open Question 3
- Question: How effectively can extracted structures serve as dense supervision signals for Process Reward Models (PRMs)?
- Basis in paper: "The extracted structures are ideal supervisory signals for PRMs"
- Why unresolved: Paper produces structures but does not integrate them into PRM training
- What evidence would resolve it: Training a PRM using extracted structures as step-level labels

### Open Question 4
- Question: What specific interventions would mitigate Llama-3-8B's failures on negation and evidence hallucination?
- Basis in paper: Explicitly notes Llama-3-8B "hallucinates evidence, merely paraphrases conditions, and mishandles negation"
- Why unresolved: Study demonstrates problem but doesn't investigate targeted solutions
- What evidence would resolve it: Targeted experiments with logic-augmented training data

## Limitations

- The paper lacks publicly available prompt demonstrations, creating a fundamental reproducibility gap
- The narrow evaluation dataset (24 training instances) raises concerns about overfitting
- The approach shows systematic limitations on negation handling with Reasoning F1 of only 7.82
- Significant performance drop from QP F1 (75.26) to verification F1 (33.04) suggests error propagation issues

## Confidence

**High Confidence**: Few-shot in-context learning for structured generation is well-established; JSON fencing reducing unparsable output from 16% to 2% is concrete and measurable; performance degradation with excessive shots is statistically robust.

**Medium Confidence**: Staged pipeline design's effectiveness is supported but not rigorously validated against alternatives; claims about lightweight approaches rivaling complex pipelines are suggestive but limited by narrow evaluation scope.

**Low Confidence**: Specific selection criteria for demonstration examples remain unspecified; negation handling failure mode is identified but not addressed, suggesting fundamental limitations.

## Next Checks

1. **Prompt Content Validation**: Reconstruct exact 2-shot QP and 3-shot CP demonstrations from code repository and systematically vary content to test pattern coverage effects.

2. **End-to-End System Integration**: Implement complete pipeline and measure actual error propagation from condition extraction to verification to quantify the 75.26→33.04 F1 drop.

3. **Negation Handling Stress Test**: Create controlled test suite targeting logical negations and contradictions to validate whether 7.82 Reasoning F1 reflects systematic model limitations.