---
ver: rpa2
title: Analyzing the Instability of Large Language Models in Automated Bug Injection
  and Correction
arxiv_id: '2509.06429'
source_url: https://arxiv.org/abs/2509.06429
tags:
- similarity
- temperature
- code
- test
- problems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically examines the instability of Large Language
  Models (LLMs) in automated bug fixing tasks. Using the ChatGPT (GPT-4) model, the
  authors tested 20 buggy code samples from the QuixBugs dataset, generating nine
  fixes per problem at three temperature settings (0.0, 0.5, 1.0), yielding 540 total
  outputs.
---

# Analyzing the Instability of Large Language Models in Automated Bug Injection and Correction

## Quick Facts
- arXiv ID: 2509.06429
- Source URL: https://arxiv.org/abs/2509.06429
- Reference count: 40
- Key outcome: This study systematically examines the instability of Large Language Models (LLMs) in automated bug fixing tasks. Using the ChatGPT (GPT-4) model, the authors tested 20 buggy code samples from the QuixBugs dataset, generating nine fixes per problem at three temperature settings (0.0, 0.5, 1.0), yielding 540 total outputs. Stability was measured via Levenshtein distance for syntactic similarity and Output Equivalence Rate (OER) for functional consistency. Results show that higher temperatures significantly increase output variability and reduce functional reliability, with the average OER dropping from 0.70 to 0.62 as temperature rises. Certain problems (e.g., template-based or math-based tasks) remained stable across all settings, while graph-based and control-flow intensive tasks degraded rapidly. Notably, even at temperature 0.0, full determinism was not achieved, indicating inherent LLM instability beyond temperature effects. The findings highlight that while LLMs can assist in bug fixing, their outputs are often inconsistent and require robust verification methods like multi-sample generation and test-based selection to ensure reliability.

## Executive Summary
This paper investigates the stability of Large Language Models (LLMs) in automated bug fixing by systematically measuring output variability across temperature settings. Using GPT-4 to generate multiple fixes for 20 buggy Python programs from the QuixBugs dataset, the authors find that higher temperatures dramatically increase syntactic and functional instability. Even at the lowest temperature setting (0.0), outputs remain non-deterministic, suggesting inherent randomness in LLM generation beyond temperature control. The study highlights the need for robust verification methods in production systems that rely on LLMs for code generation.

## Method Summary
The study uses the QuixBugs dataset containing 20 buggy Python programs and their test oracles. For each program, GPT-4 is queried nine times at each of three temperature settings (0.0, 0.5, 1.0), yielding 540 total outputs. Each query uses an identical prompt template: "The following code contains an error. Please fix the error and provide the correct working version: \n\npython\n<code>\n". Output stability is measured through normalized Levenshtein distance for syntactic similarity and Output Equivalence Rate (OER) via test case execution. The results are aggregated to categorize problems as Fully Successful (100% success), Partially Successful (66.67%), or Failed (≤33.33%).

## Key Results
- Temperature increase from 0.0 to 1.0 reduces average Output Equivalence Rate from 0.70 to 0.62
- Certain problem types (template-based, math-based) remain stable across all temperatures, while graph-based and control-flow intensive tasks degrade rapidly
- Even at temperature 0.0, outputs show non-zero variance, indicating inherent LLM instability beyond temperature effects
- Multi-sample generation with test-based selection is proposed as a mitigation strategy for unreliable outputs

## Why This Works (Mechanism)

### Mechanism 1: Temperature-Modulated Sampling Diversity
- Claim: Increasing temperature amplifies output variability and reduces functional correctness in bug-fixing tasks.
- Mechanism: Temperature controls the softmax sharpness during token sampling; higher values flatten probability distributions, allowing lower-probability tokens to be selected more frequently. This expands the solution search space but also increases the probability of sampling functionally incorrect code paths.
- Core assumption: The observed OER decline (0.70 → 0.62) is primarily attributable to temperature-driven sampling rather than prompt ambiguity or model state changes.
- Evidence anchors:
  - [abstract] "results demonstrate that the model's outputs become much more unstable and variable as the temperature rises, with high temperatures showing especially high rates of functional failure"
  - [section 4.2, Tables 5-7] OER drops from 0.70 (T=0.0) to 0.67 (T=0.5) to 0.62 (T=1.0)
  - [corpus] Related work [10,16] confirms residual variability persists even at low temperatures, suggesting temperature is one of multiple stochastic factors.
- Break condition: If model outputs at T=0.0 showed zero variance, temperature would fully explain instability. The paper shows this is NOT the case.

### Mechanism 2: Task-Structure Sensitivity to Sampling Noise
- Claim: Problems with canonical solutions (template-based, arithmetic) resist temperature-induced degradation, while graph-based and control-flow-intensive problems degrade rapidly.
- Mechanism: Template-like problems have fewer valid solution paths in the model's learned distribution—sampling variance has limited "room" to deviate. Graph/control-flow problems have multiple valid algorithmic strategies; sampling noise more easily shifts between incompatible approaches, breaking correctness.
- Core assumption: Stability differences across problem types reflect structural properties of the solution space, not differences in training data coverage.
- Evidence anchors:
  - [abstract] "Certain problems (e.g., template-based or math-based tasks) remained stable across all settings, while graph-based and control-flow intensive tasks degraded rapidly"
  - [section 4.2, Table 2] `flatten` and `kth` show 0.99-1.0 similarity across all temperatures; `breadth_first_search` stays at 0.40-0.45
  - [corpus] Weak direct evidence; related papers don't systematically compare task-type stability. This is a gap for future validation.
- Break condition: If unstable problems had poor test oracle coverage rather than genuine solution-space complexity, the mechanism would be confounded.

### Mechanism 3: Residual Non-Determinism Beyond Temperature
- Claim: Temperature=0 does NOT guarantee deterministic outputs; other stochastic elements in the generation stack persist.
- Mechanism: Even with temperature set to zero, sources of variability may include: API-level load balancing across model replicas, floating-point non-associativity in GPU computations, hidden sampling in post-processing, and model version drift between API calls.
- Core assumption: The observed Levenshtein variance at T=0.0 (e.g., 0.40 for `breadth_first_search`, 0.81 for `bitcount` with high std dev) reflects generation-stack randomness rather than measurement error.
- Evidence anchors:
  - [abstract] "even at temperature 0.0, full determinism was not achieved, indicating inherent LLM instability beyond temperature effects"
  - [section 5] "the variations observed in Levenshtein similarity values and the failures recorded for certain problems indicate that LLMs depend not only on the temperature parameter but also on sampling methods and systemic factors"
  - [corpus] [9,15,16] document that "lowering temperature reduces spread while leaving residual randomness intact" and attribute this to "other stochastic elements in the generation stack"
- Break condition: If API calls were made to identical model replicas with fixed seeds and variance persisted, this would suggest architectural randomness. The paper does not control for this explicitly.

## Foundational Learning

- **Concept: Levenshtein Distance (Edit Distance)**
  - Why needed here: Primary metric for syntactic similarity; quantifies how many character-level operations (insert, delete, substitute) separate two generated code outputs. Normalized to [0,1] for interpretability.
  - Quick check question: If two code snippets have Levenshtein similarity of 0.65, are they guaranteed to be functionally equivalent? (No—syntactic similarity does not imply semantic equivalence.)

- **Concept: Output Equivalence Rate (OER)**
  - Why needed here: Functional correctness metric; measures the fraction of test inputs where two programs produce identical outputs. Essential for detecting semantic divergence that Levenshtein misses.
  - Quick check question: If Program A passes all tests and Program B has OER=0.8 relative to A, what does the 0.2 represent? (Test cases where B produces different outputs—potential bugs or behavioral changes.)

- **Concept: Temperature in Softmax Sampling**
  - Why needed here: Core hyperparameter controlling output diversity. Understanding its effect is critical for tuning reliability vs. exploration tradeoffs in production systems.
  - Quick check question: At temperature → 0, what happens to the sampling distribution? (Approaches greedy decoding—always selecting the highest-probability token. But this paper shows it does NOT guarantee determinism.)

## Architecture Onboarding

- **Component map:**
  Input layer -> Generation layer -> Sampling layer -> Evaluation layer -> Aggregation layer

- **Critical path:**
  1. Receive buggy code from QuixBugs dataset
  2. Send identical prompt to LLM API N times (3 trials × 3 temperatures = 9 outputs per problem)
  3. Pairwise Levenshtein comparison → syntactic similarity matrix
  4. Execute test oracle against each generated fix → OER calculation
  5. Categorize: Fully Successful (100%), Partially Successful (66.67%), Failed (≤33.33%)

- **Design tradeoffs:**
  - Multi-sample generation: Increases computational cost linearly but enables variance-aware reliability estimates. The paper used 9 samples; production may need more for statistical power.
  - Low temperature (T=0.0): Maximizes reproducibility but does NOT eliminate non-determinism. May also reduce solution diversity, missing valid alternative fixes.
  - OER threshold selection: Higher thresholds (e.g., 0.9) increase confidence but reduce acceptance rates. The paper uses binary pass/fail; graded thresholds may better reflect partial correctness.

- **Failure signatures:**
  - High Levenshtein variance at T=0.0 → residual non-determinism in API/stack
  - 0% success rate across all temperatures for specific problems (e.g., `rpn_eval`, `shortest_path_length_test`) → systematic model limitation, not temperature issue
  - Success rate decreases monotonically with temperature (e.g., `breadth_first_search`: 100%→100%→33%) → temperature-sensitive problem class
  - Non-monotonic behavior (e.g., `bucketsort`: 33%→33%→100%) → small stochasticity may occasionally escape brittle deterministic errors

- **First 3 experiments:**
  1. **Baseline replication**: Reproduce the temperature sweep (T=0.0, 0.5, 1.0) on a subset of 5 stable + 5 unstable problems from QuixBugs. Verify OER and Levenshtein patterns match reported values (±5% tolerance). This validates your pipeline before extending.
  2. **Seed control isolation**: If your LLM API supports seed parameters, run T=0.0 trials with fixed seeds to quantify how much variance is attributable to temperature vs. other stack randomness. Compare against unseeded runs.
  3. **Multi-sample selection protocol**: For each problem, generate 10 samples at T=0.5 and apply test-aware selection (accept first sample that passes all tests). Measure acceptance rate and compare to single-shot success rates. This evaluates the practical viability of multi-sample ensembling as suggested in the paper.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does LLM instability in bug fixing scale linearly with codebase size, and does it persist across different programming languages?
- Basis in paper: [explicit] Section 6 states future work "should extend this research to larger-scale and multi-file projects [and] different programming languages."
- Why unresolved: The current study relies solely on small, single-function Python snippets from QuixBugs, limiting generalizability to complex software systems.
- What evidence would resolve it: Applying the OER and Levenshtein methodology to multi-file Java or C++ repositories with complex inter-file dependencies.

### Open Question 2
- Question: Do structural metrics like AST-based similarity reveal different stability patterns than the character-level Levenshtein distance used here?
- Basis in paper: [explicit] Section 6 suggests research "should also be supported by more advanced analyses, such as AST-based or semantic similarity metrics."
- Why unresolved: Syntactic character distance may not accurately reflect logical consistency (functional equivalence) or structural refactoring in the generated code.
- What evidence would resolve it: A comparative analysis correlating Levenshtein scores against Abstract Syntax Tree (AST) edit distances for the same set of generated patches.

### Open Question 3
- Question: How does model version drift impact the longitudinal stability of automated bug fixes?
- Basis in paper: [explicit] Section 6 identifies "examining the effects of model version changes" as an important research area.
- Why unresolved: The study relies on a specific GPT-4 snapshot, whereas production APIs update frequently, potentially altering non-deterministic behaviors or fix success rates.
- What evidence would resolve it: A longitudinal study measuring OER and stability variance across multiple GPT-4 version iterations (e.g., gpt-4-0613 vs. gpt-4-turbo) using identical prompts.

## Limitations

- The study does not fully isolate sources of non-determinism at temperature=0.0, leaving uncertainty about which stack components contribute most to instability
- Analysis is limited to Python code from the QuixBugs dataset, constraining generalizability to other programming languages and bug patterns
- The paper acknowledges that factors beyond temperature contribute to variability but lacks explicit experimental isolation of these components

## Confidence

- **High Confidence**: Temperature-temperature correlation with output variability (T=0.0 → 0.70 OER, T=1.0 → 0.62 OER) - well-supported by systematic measurements across 540 samples
- **Medium Confidence**: Task-structure sensitivity claims - plausible mechanism but limited direct evidence comparing different problem types within this study alone
- **Medium Confidence**: Residual non-determinism beyond temperature - documented phenomenon but exact stack components not isolated in this work

## Next Checks

1. **Multi-sample selection protocol validation**: Generate 10 samples at T=0.5 for each problem and implement test-aware selection (accept first sample passing all tests). Measure acceptance rate and compare to single-shot success rates to evaluate practical viability of the proposed mitigation strategy.

2. **Cross-dataset replication**: Apply the same experimental protocol to a different buggy code dataset (e.g., Defects4J for Java) to test whether temperature sensitivity and task-structure patterns generalize beyond QuixBugs.

3. **API parameter isolation**: If seed parameters are available, run controlled experiments at T=0.0 with and without fixed seeds to quantify the relative contribution of temperature vs. other stochastic elements in the generation stack.