---
ver: rpa2
title: Achieving Fairness Without Harm via Selective Demographic Experts
arxiv_id: '2511.06293'
source_url: https://arxiv.org/abs/2511.06293
tags:
- fairness
- group
- groups
- sensitive
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles algorithmic bias in high-stakes domains like\
  \ healthcare by proposing FairSDE, a method to achieve fairness without sacrificing\
  \ performance for any group. FairSDE learns distinct representations for different\
  \ demographic groups through demographic experts\u2014group-specific representations\
  \ paired with personalized classifiers\u2014and dynamically selects between expert\
  \ models and a pooled baseline using a no-harm constrained optimization."
---

# Achieving Fairness Without Harm via Selective Demographic Experts

## Quick Facts
- arXiv ID: 2511.06293
- Source URL: https://arxiv.org/abs/2511.06293
- Reference count: 40
- Primary result: FairSDE achieves fairness without harming any group's performance across medical imaging and facial datasets.

## Executive Summary
This paper tackles algorithmic bias in high-stakes domains like healthcare by proposing FairSDE, a method to achieve fairness without sacrificing performance for any group. FairSDE learns distinct representations for different demographic groups through demographic experts—group-specific representations paired with personalized classifiers—and dynamically selects between expert models and a pooled baseline using a no-harm constrained optimization. Evaluated on three real-world medical imaging datasets and two facial image datasets, FairSDE consistently achieves fairness without harm, outperforming existing methods that typically degrade performance for certain groups.

## Method Summary
FairSDE addresses algorithmic bias by learning group-specific representations and classifiers while ensuring no group is harmed relative to an ERM baseline. The method uses a ResNet-18 backbone with demographic experts—group-specific classifiers—and enforces separability between groups through a discriminator that links representations to sensitive attributes. Virtual centers learn class-group distribution means, while pairwise alignment and diversity losses create compact, separable clusters. A selection module uses greedy or integer programming optimization to choose between expert models and ERM per group, guaranteeing fairness without harm.

## Key Results
- FairSDE consistently achieves fairness without harm across all tested datasets
- Outperforms existing methods that typically degrade performance for certain groups
- Improves both fairness and accuracy across all demographic groups
- Addresses gap in prior work focused mainly on simpler tabular data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Learning group-specific representations with enforced separability enables demographic experts to capture distributional differences that pooled models cannot.
- **Mechanism:** A discriminator D maximizes P(A|f(X)) to link representations to sensitive attributes, while virtual centers V_{a,y} serve as proxies for class-group distribution means. Pairwise alignment within groups reduces variance, creating compact, separable clusters in latent space.
- **Core assumption:** Demographic groups exhibit meaningful distributional differences in feature space that can be exploited for better prediction.
- **Evidence anchors:** [abstract]: "learning distinct representations for different demographic groups"; [section 4.1]: "We explicitly enforce a group-wise dependence between the representation f(X) and the sensitive attribute A, i.e., by maximizing P(A|f(X))"
- **Break condition:** When group distributions are nearly identical, separability enforcement may degrade performance by forcing artificial distinctions.

### Mechanism 2
- **Claim:** Decoupling both representations and classifiers (rather than classifiers alone) is necessary to achieve fairness without harm on high-dimensional image data.
- **Mechanism:** Group-specific classifiers h_a are trained on their corresponding decoupled representations. This two-level decoupling captures patterns unique to each demographic that shared representations with decoupled classifiers would miss.
- **Core assumption:** High-dimensional features like medical images are not easily separable by sensitive attributes in standard latent spaces.
- **Evidence anchors:** [section 4.1]: "naively personalized classifiers may underperform pooled ERM when group distributions are similar, as the pooled model benefits from more data"; [page 6]: "decoupled classifiers fail to outperform ERM on both gender and age attributes, leading to a trivial solution"
- **Break condition:** When groups are small, decoupled classifiers may overfit; no-harm selection defaults to ERM.

### Mechanism 3
- **Claim:** Constrained combinatorial optimization for model selection guarantees no group is harmed relative to ERM baseline while minimizing inter-group performance gaps.
- **Mechanism:** Binary variables v_a select between expert h_a ∘ f and ERM h_erm for each group. Integer programming minimizes accuracy gap Δ subject to α_a ≥ α_{erm,a} (no-harm constraint). Greedy strategy alternatively maximizes worst-group performance.
- **Core assumption:** Validation and test sets share similar distributions (stated explicitly in paper).
- **Evidence anchors:** [section 4.2]: "the trivial solution where we choose the ERM model for all groups... is feasible"; [page 7, Table 3]: Shows IP reducing performance gap to 1.81 with no group underperforming ERM
- **Break condition:** Under distribution shift between validation and test, selection decisions may harm groups unexpectedly.

## Foundational Learning

- **Concept: Group fairness notions (demographic parity, equalized odds, max-min fairness)**
  - Why needed here: The paper optimizes for overall accuracy parity and max-min fairness; understanding what each metric captures is essential for selecting the right selection strategy.
  - Quick check question: Can you explain why equalizing accuracy across groups might conflict with equalizing true positive rates?

- **Concept: Representation learning with contrastive/divergence objectives**
  - Why needed here: L_virt and L_div use similarity-based alignment and contrastive-style penalties; understanding how these shape latent geometry is prerequisite to debugging representation quality.
  - Quick check question: What happens to virtual center learning if one group has 10× more samples than another?

- **Concept: Constrained combinatorial optimization / integer programming**
  - Why needed here: The overall accuracy parity selection requires solving a binary optimization with linear constraints; understanding feasibility and trivial solutions is critical.
  - Quick check question: Why is the "always select ERM" solution always feasible in the IP formulation?

## Architecture Onboarding

- **Component map:** Image x_i → ResNet-18 backbone f → representation z_i → (group discriminator D predicts A, virtual centers V_{a,y}, group classifiers h_a) → per-group predictions

- **Critical path:**
  1. Train representation learning with L_disc + L_virt + L_div jointly
  2. Train group classifiers h_a on frozen representations
  3. Train pooled ERM baseline separately
  4. Run selection on validation set to assign each group to expert or ERM
  5. Apply selected model per group at test time

- **Design tradeoffs:**
  - More groups → more classifier parameters (linear in |A|), but minimal overhead for typical |A| ≤ 5
  - Strong separability enforcement (high λ_1) may harm transferability if groups share meaningful structure
  - Greedy selection optimizes worst-group; IP optimizes gap—choose based on deployment priority

- **Failure signatures:**
  - Selection always returns ERM → representations insufficiently discriminative or groups too similar
  - Validation performance good but test degrades → distribution shift violated
  - High discriminator accuracy but poor downstream fairness → separability achieved but not leveraged by classifiers

- **First 3 experiments:**
  1. Reproduce single-dataset result (e.g., Ham10000 with age attribute) comparing FairSDE vs. ERM vs. Decoupled-only; verify no-harm property holds
  2. Ablate L_disc, L_virt, L_div separately (as in Appendix Table 6) to isolate which loss term drives fairness gains on your target dataset
  3. Stress-test selection: artificially shift validation set (e.g., subsample one group) and measure whether no-harm guarantee breaks on test set

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How robust is the no-harm selection strategy when the validation set distribution differs significantly from the test set distribution?
- **Basis in paper:** [explicit] Page 6 states the assumption that distributions are identical between validation and test sets is "critical," noting the strategy "may fail to perform as intended under a significant distribution shift."
- **Why unresolved:** The current evaluation relies on held-out test sets sampled from the same distribution as the training/validation data.
- **What evidence would resolve it:** Experiments evaluating selection performance under covariate shift or domain generalization benchmarks where validation and test distributions diverge.

### Open Question 2
- **Question:** Does the integer programming approach for model selection scale effectively to settings with high-dimensional sensitive attributes or numerous intersecting demographic subgroups?
- **Basis in paper:** [inferred] Page 5 notes that integer programming is feasible because the "number of sensitive attributes is relatively small in fairness studies," implying potential computational limits.
- **Why unresolved:** The combinatorial optimization solves for every group; exponential growth in groups (intersectionality) could make exact solutions intractable.
- **What evidence would resolve it:** Scalability analysis measuring optimization time and memory usage as the number of sensitive groups increases.

### Open Question 3
- **Question:** Can the fairness-without-harm guarantee be maintained without requiring explicit access to sensitive attributes during the inference/selection phase?
- **Basis in paper:** [inferred] The method requires sensitive attribute $a$ to select the specific expert model ($v_a \cdot \alpha$), and Page 7 justifies this only by citing specific medical contexts where such data is collected.
- **Why unresolved:** In many real-world deployments, sensitive attributes are legally protected or unavailable at inference, limiting the proposed method's applicability.
- **What evidence would resolve it:** A variant of FairSDE that utilizes inferred or blind selection mechanisms to achieve similar no-harm guarantees without ground-truth attributes at inference.

## Limitations

- The no-harm guarantee critically depends on validation and test sets sharing similar distributions, which may not hold in real-world deployment.
- The paper does not specify exact hyperparameter values (λ coefficients) used for reported results, requiring re-tuning for reproduction.
- Virtual center architecture details and discriminator layer specifications are underspecified, potentially affecting implementation fidelity.

## Confidence

- **High confidence:** The core mechanism of decoupling representations and classifiers is well-specified and theoretically sound; the no-harm optimization framework is clearly defined.
- **Medium confidence:** The empirical claims of achieving fairness without harm across all tested datasets are supported by presented results, though exact hyperparameter values are missing.
- **Low confidence:** Claims about performance under distribution shift and the specific architectural details needed for exact reproduction.

## Next Checks

1. **Distribution shift validation:** Intentionally create validation-test distribution mismatch (e.g., by subsampling one group) and verify whether the no-harm guarantee holds in the test set.
2. **Ablation study replication:** Reproduce the ablation experiments from Appendix Table 6 to isolate which loss components (L_disc, L_virt, L_div) drive fairness gains on a target dataset.
3. **Selection strategy comparison:** Compare greedy (max-min) vs. integer programming (accuracy parity) selection on the same dataset to validate their distinct fairness impacts as claimed.