---
ver: rpa2
title: 'Cmprsr: Abstractive Token-Level Question-Agnostic Prompt Compressor'
arxiv_id: '2511.12281'
source_url: https://arxiv.org/abs/2511.12281
tags:
- compression
- information
- prompt
- gpt-4
- cmprsr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Cmprsr is the first abstractive, question-agnostic prompt compressor
  that outperforms both extractive and vanilla abstractive methods. Built by post-training
  a small LLM (Qwen3-4B) with SFT and GRPO, it achieves higher QA and summarization
  performance across a wide range of compression rates, closely adheres to the user-specified
  target CR, and generalizes well to long inputs and out-of-domain datasets.
---

# Cmprsr: Abstractive Token-Level Question-Agnostic Prompt Compressor

## Quick Facts
- arXiv ID: 2511.12281
- Source URL: https://arxiv.org/abs/2511.12281
- Authors: Ivan Zakazov; Alexander Sharipov; Berke Argin; Oussama Gabouj; Kamel Charaf; Alexi Semiz; Lorenzo Drudi; Nicolas Baldwin; Robert West
- Reference count: 40
- Key outcome: Cmprsr is the first abstractive, question-agnostic prompt compressor that outperforms both extractive and vanilla abstractive methods. Built by post-training a small LLM (Qwen3-4B) with SFT and GRPO, it achieves higher QA and summarization performance across a wide range of compression rates, closely adheres to the user-specified target CR, and generalizes well to long inputs and out-of-domain datasets.

## Executive Summary
Cmprsr introduces a novel abstractive, question-agnostic prompt compression approach that outperforms existing extractive and vanilla abstractive methods. By post-training Qwen3-4B with a combination of supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), Cmprsr achieves superior performance in both question answering and summarization tasks across various compression rates. The key innovation lies in the dual-objective training that simultaneously optimizes for compression rate adherence and downstream task performance, addressing the limitations of previous approaches that either lack semantic preservation or fail to maintain the requested compression ratio.

## Method Summary
Cmprsr employs a two-stage training approach on Qwen3-4B-Instruct: first, supervised fine-tuning with synthetic data generated by gpt-4.1-mini, using length-conditioned prompting inspired by hindsight instruction relabelling; second, GRPO optimization on a subset of 10k examples with dual rewards for compression rate adherence (R_len) and quality preservation (R_qual). The model is trained on MeetingBank transcripts and evaluated across multiple datasets including MeetingBank, LongBench, LongBench V2, InfiniteBench, Ruler, and GSM8K. The approach achieves superior performance by learning to compress prompts while preserving semantic information necessary for downstream tasks.

## Key Results
- Cmprsr outperforms both extractive methods (LLMLingua-2, DAC) and vanilla abstractive approaches across all compression rates
- The model achieves higher QA precision and BERTScore-F1 for summarization compared to baselines
- Cmprsr demonstrates strong generalization to long inputs and out-of-domain datasets while maintaining requested compression rates
- Abstractive compression proves particularly effective at high compression rates where extractive methods struggle with coherence

## Why This Works (Mechanism)

### Mechanism 1: Dual-objective GRPO training enables simultaneous compression-rate adherence and semantic preservation through complementary reward signals.
The combined reward R = R_qual + R_len creates opposing pressures: R_len = 1 - max(0, r_C - r_T) penalizes over-compression, while R_qual (cross-entropy based) rewards preservation of task-relevant information. The policy learns to navigate this trade-off rather than optimizing either objective in isolation.

### Mechanism 2: Length conditioning during SFT via hindsight instruction relabelling (HIR) establishes precise CR control that vanilla models lack.
By appending the actual token count to the prompt during training (Original + Length Conditioned Prompt(len(Compression)) + Compression), the model learns to condition its generation on a target length. This counters the vanilla LLM tendency to converge toward a fixed CR regardless of the requested rate.

### Mechanism 3: Abstractive compression outperforms extractive methods at high compression rates because paraphrase capacity allows semantic condensation beyond token removal.
Extractive methods (LLMLingua-2, DAC) operate on order-preserving subsequences, which fragments coherence at aggressive CRs. Abstractive methods can rephrase, reorder, and synthesize, preserving local phrase structure while reducing token count.

## Foundational Learning

- **Concept: Reinforcement Learning from Policy Optimization (GRPO/DPO/ORPO variants)**
  - Why needed here: GRPO replaces standard RLHF for fine-tuning the compressor; understanding reward shaping, policy gradients, and preference optimization is essential for debugging training dynamics.
  - Quick check question: Given rewards [R_len=0.8, R_qual=0.6] for sample A and [R_len=0.5, R_qual=0.9] for sample B, which would GRPO prefer if the objective weights are equal?

- **Concept: Knowledge Distillation (Teacher-Student Paradigm)**
  - Why needed here: SFT phase distills compression strategies from gpt-4.1-mini (teacher) to Qwen3-4B (student); evaluating distillation quality vs. direct training trade-offs is critical.
  - Quick check question: If teacher outputs at CR=0.3 but student at CR=0.5, should you upsample, downsample, or reweight the training data?

- **Concept: Prompt Compression Taxonomy (Extractive vs. Abstractive, Question-Aware vs. Question-Agnostic)**
  - Why needed here: Cmprsr's design choices (abstractive + question-agnostic) define its use-cases and limitations; misunderstanding leads to misapplication.
  - Quick check question: For a RAG system where retrieval is query-dependent, should you use question-agnostic or question-aware compression? What trade-off does Cmprsr make?

## Architecture Onboarding

- **Component map:** Input Text → Chunking (512 tokens max) → [Compressor: Qwen3-4B SFT+GRPO] → Concatenated Output

- **Critical path:**
  1. Data generation: gpt-4.1-mini generates compressions at CR ∈ [0.1, 0.7]; rebalance for uniform distribution
  2. SFT training: Length-conditioned fine-tuning with lr=10^-5 on upsampled data
  3. GRPO training: Dual-reward optimization on 10k examples, 4 rollouts per input, lr=5×10^-6
  4. Evaluation: Summarization (BERTScore-F1) and QA (precision) on MeetingBank, LongBench, GSM8K, OOD benchmarks

- **Design tradeoffs:**
  - Model size (4B) vs. latency: Autoregressive generation adds overhead; viable only when paired with large target models or pre-computed compressions
  - Single-dataset training (MeetingBank) vs. OOD generalization: Works well on LongBench/GSM8K but may fail on specialized domains (code, medical)
  - Summary-based R_qual vs. QA-based: Summary reward prevents reward hacking (outputting answers) but may bias toward summarization tasks

- **Failure signatures:**
  - CR drift: ∆CR consistently positive/negative → length conditioning failed; check SFT data distribution
  - Semantic loss on short inputs: GSM8K performance drops → abstraction degrades on already-condensed text
  - Over-compression on OOD: Specialized domains fail → training data coverage insufficient; consider domain-specific SFT
  - Reward hacking: Model outputs possible answers instead of compressions → switch from QA-based to summary-based R_qual

- **First 3 experiments:**
  1. **Ablate GRPO rewards:** Train separate models with R_len only and R_qual only; measure CR adherence vs. task performance to validate dual-objective necessity
  2. **Vary teacher model for distillation:** Compare SFT from gpt-4.1-mini vs. gpt-4.1 vs. Qwen2.5-7B to isolate teacher quality effects on student performance
  3. **OOD stress test:** Evaluate on code (CodePromptZip-style data) and medical texts; measure failure modes and identify minimum domain-specific fine-tuning requirements

**Assumption:** The cross-entropy reward formulation (Eq. 2) assumes a frozen solver model; if the target model changes, reward calibration may need re-tuning.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can a more sophisticated reward design in the RL stage mitigate the current model's bias toward summarization tasks?
  - Basis: The authors state that "Further performance gains can also be reached through more elaborate GRPO reward design, as the current reward is biased toward summarization."
  - Why unresolved: The current quality reward relies on a summary-based proxy, which optimizes for semantic coherence in summaries but may not optimally preserve factual details required for complex QA or reasoning tasks.
  - Evidence needed: Comparing performance of Cmprsr with QA-specific or multi-objective rewards against the current summary-biased baseline on reasoning benchmarks like GSM8k.

- **Open Question 2:** Does training on a multi-domain corpus significantly improve compression quality for specialized domains like coding or medicine?
  - Basis: The paper notes that the current model was "trained on a single dataset," resulting in "potentially sub-optimal behavior on more specialized OOD data, e.g. from coding or medical domain."
  - Why unresolved: While Cmprsr generalizes well to varying input lengths, the domain shift from meeting transcripts to highly technical data remains untested.
  - Evidence needed: Evaluating a variant of Cmprsr fine-tuned on a mixture of code and medical data against the baseline on domain-specific benchmarks like HumanEval or MedQA.

- **Open Question 3:** How does the inference latency of the autoregressive Cmprsr compare to extractive methods in real-time applications?
  - Basis: The authors acknowledge that "the autoregressive nature of Cmprsr leads to the latency overhead," restricting prime use-cases to those allowing for pre-computed compression.
  - Why unresolved: The paper demonstrates cost savings via token reduction but does not quantify the wall-clock time penalty introduced by the 4B-parameter Compressor model.
  - Evidence needed: A latency benchmark comparing the end-to-end time of Cmprsr versus extractive baselines on a standardized throughput stream.

## Limitations

- Dual-objective reward formulation sensitivity may exhibit unstable training dynamics where quality signal dominates, causing CR adherence to degrade
- Teacher distillation quality ceiling fundamentally bounds Cmprsr's performance to the abstractive compression quality of gpt-4.1-mini
- OOD domain generalization gaps may cause semantic drift when paraphrasing domain-specific terminology in specialized domains like medical or code

## Confidence

**High confidence**: The claim that Cmprsr outperforms extractive baselines at high compression rates (CR ≥ 0.3) is well-supported by direct comparisons in Tables 3 and 6, showing consistent gains in both BERTScore-F1 and QA precision across multiple datasets.

**Medium confidence**: The assertion that length conditioning via hindsight instruction relabelling establishes precise CR control is plausible given the quantitative CR adherence results, but lacks ablation studies showing the necessity of this specific approach.

**Low confidence**: The claim that dual-objective GRPO is necessary for balancing compression rate adherence and semantic preservation lacks direct experimental support. No ablations compare single-objective variants.

## Next Checks

1. **Reward ablation study**: Train three variants - R_len only, R_qual only, and combined R_len + R_qual - and measure CR adherence, semantic preservation, and task performance trade-offs.

2. **Teacher model sensitivity analysis**: Repeat the full training pipeline using different teacher models (gpt-4.1, Qwen2.5-7B, Claude) to isolate the contribution of the distillation phase from the fine-tuning improvements.

3. **Domain-specific stress testing**: Evaluate Cmprsr on specialized corpora (medical abstracts, legal documents, programming code) and conduct error analysis to identify failure modes.