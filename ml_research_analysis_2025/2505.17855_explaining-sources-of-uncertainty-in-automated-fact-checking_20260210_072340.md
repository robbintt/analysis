---
ver: rpa2
title: Explaining Sources of Uncertainty in Automated Fact-Checking
arxiv_id: '2505.17855'
source_url: https://arxiv.org/abs/2505.17855
tags:
- uncertainty
- evidence
- claim
- explanations
- clue-span
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLUE is a framework for generating natural language explanations
  of model uncertainty in multi-evidence fact-checking. It identifies conflicts and
  agreements between claims and evidence by extracting span-level interactions, then
  produces explanations via prompting or attention steering to verbalize these interactions.
---

# Explaining Sources of Uncertainty in Automated Fact-Checking

## Quick Facts
- arXiv ID: 2505.17855
- Source URL: https://arxiv.org/abs/2505.17855
- Reference count: 40
- CLUE improves faithfulness to model uncertainty (point-biserial correlations up to +0.102) and label-explanation consistency over prompting baseline

## Executive Summary
This paper introduces CLUE, a framework for generating natural language explanations of model uncertainty in multi-evidence fact-checking. CLUE identifies conflicts and agreements between claims and evidence by extracting span-level interactions, then produces explanations via prompting or attention steering to verbalize these interactions. Evaluated across three models and two datasets, CLUE improves faithfulness to model uncertainty and label-explanation consistency compared to a prompting baseline. Human evaluators found CLUE explanations more helpful, informative, less redundant, and more consistent than the baseline.

## Method Summary
CLUE generates uncertainty explanations through a two-stage process: first extracting span-level interactions between claims and evidence, then producing natural language explanations via either prompting or attention steering mechanisms. The framework requires no fine-tuning and operates on existing model outputs, making it applicable to any fact-checking model that produces attention weights. The approach focuses on identifying and verbalizing conflicts and agreements between multiple pieces of evidence and the claim being fact-checked.

## Key Results
- Improves faithfulness to model uncertainty (point-biserial correlations up to +0.102)
- Outperforms prompting baseline on label-explanation consistency
- Human evaluators prefer CLUE explanations for helpfulness, informativeness, and consistency

## Why This Works (Mechanism)
CLUE works by systematically identifying and verbalizing the span-level interactions that drive model uncertainty in multi-evidence fact-checking. By focusing on the specific conflicts and agreements between evidence pieces and claims, the framework can generate targeted explanations that directly address why the model is uncertain about its prediction.

## Foundational Learning
- **Span-level interaction extraction**: Needed to identify specific textual conflicts/agreement between evidence and claims; quick check: verify extracted spans correspond to actual textual discrepancies
- **Attention weight interpretation**: Required to understand which evidence interactions influence model uncertainty; quick check: ensure attention weights correlate with meaningful evidence relationships
- **Natural language generation from structured interactions**: Essential for converting extracted interactions into human-readable explanations; quick check: validate generated explanations accurately represent underlying interactions

## Architecture Onboarding

Component map: Claim -> Evidence Collection -> Span Extraction -> Interaction Analysis -> Attention Steering/Prompting -> Explanation Generation

Critical path: Span Extraction -> Interaction Analysis -> Attention Steering

Design tradeoffs: No fine-tuning vs. potential for more tailored explanations; generic framework vs. task-specific optimization

Failure signatures: Generic explanations when interactions are ambiguous; over-reliance on attention weights when evidence is contradictory

First experiments:
1. Test span extraction accuracy on simple claim-evidence pairs with clear conflicts
2. Validate interaction analysis identifies known conflicts in FEVER dataset
3. Compare attention steering vs. prompting for different types of uncertainty

## Open Questions the Paper Calls Out
None

## Limitations
- Modest improvements in point-biserial correlations (+0.102) may not translate to practical impact
- Human evaluation focused on preference rather than objective accuracy verification
- Computational efficiency and runtime overhead not addressed

## Confidence
- Medium: Framework's effectiveness in improving faithfulness and consistency metrics
- High: Technical implementation of span-level interaction extraction and attention steering
- Low: Generalizability claims to other reasoning tasks without systematic evaluation

## Next Checks
1. Evaluate CLUE on fact-checking datasets from different domains (scientific claims, health misinformation) to assess robustness
2. Conduct human accuracy validation studies comparing CLUE explanations to ground truth evidence conflicts
3. Measure computational overhead and latency introduced by CLUE's explanation generation process