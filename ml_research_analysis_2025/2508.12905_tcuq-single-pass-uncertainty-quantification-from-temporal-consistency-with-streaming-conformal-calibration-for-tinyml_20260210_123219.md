---
ver: rpa2
title: 'TCUQ: Single-Pass Uncertainty Quantification from Temporal Consistency with
  Streaming Conformal Calibration for TinyML'
arxiv_id: '2508.12905'
source_url: https://arxiv.org/abs/2508.12905
tags:
- tcuq
- deep
- uncertainty
- temporal
- calibration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TCUQ, a single-pass, label-free uncertainty
  quantification method for TinyML. It uses lightweight temporal consistency signals
  on model posteriors and features to create a calibrated risk score, then applies
  streaming conformal calibration to enable budgeted accept/abstain decisions without
  online labels or extra forward passes.
---

# TCUQ: Single-Pass Uncertainty Quantification from Temporal Consistency with Streaming Conformal Calibration for TinyML

## Quick Facts
- arXiv ID: 2508.12905
- Source URL: https://arxiv.org/abs/2508.12905
- Reference count: 40
- On resource-constrained MCUs, TCUQ is 30–60% faster and 50–60% smaller than early-exit ensembles and deep ensembles, while improving accuracy-drop detection by 3–7 AUPRC points under corrupted streams.

## Executive Summary
This paper introduces TCUQ, a single-pass, label-free uncertainty quantification method for TinyML. It uses lightweight temporal consistency signals on model posteriors and features to create a calibrated risk score, then applies streaming conformal calibration to enable budgeted accept/abstain decisions without online labels or extra forward passes. On resource-constrained MCUs, TCUQ is 30–60% faster and 50–60% smaller than early-exit ensembles and deep ensembles, while improving accuracy-drop detection by 3–7 AUPRC points under corrupted streams and reaching up to 0.86 AUPRC at high severities; failure detection achieves up to 0.92 AUROC. TCUQ delivers strong in-distribution calibration and enables practical, on-device monitoring under tight memory and compute constraints.

## Method Summary
TCUQ builds on standard TinyML backbones (e.g., ResNet-8, DSCNN) augmented with *Temporal Assistance (TA)* auxiliary exits at intermediate layers, trained using a weighted loss that emphasizes intermediate predictions. After training, TA heads are discarded. At inference, TCUQ maintains a ring buffer of recent feature and prediction history and computes four lightweight signals: multi-lag predictive divergence (JSD), feature stability (Cosine), decision persistence, and a confidence proxy. These signals are fed into a pre-trained logistic regression combiner to estimate the probability of misclassification. A streaming conformal layer then updates an online quantile threshold, enabling budgeted abstention. The method is evaluated on vision (MNIST, CIFAR-10, TinyImageNet) and audio (SpeechCommands) tasks under various corruptions.

## Key Results
- TCUQ is 30–60% faster and 50–60% smaller than early-exit ensembles and deep ensembles on MCUs.
- Accuracy-drop detection AUPRC improves by 3–7 points over baselines under corrupted streams.
- Calibration quality (Brier Score, NLL, ECE) is strong across datasets and corruption levels.
- Failure detection achieves up to 0.92 AUROC for ID× vs ID✓.

## Why This Works (Mechanism)
TCUQ leverages temporal consistency across recent inferences as a proxy for uncertainty. By tracking how predictions and features evolve over time, it can detect abrupt changes indicative of accuracy drops or distribution shift. The streaming conformal layer provides adaptive, label-free calibration, enabling on-device risk-aware decision making.

## Foundational Learning
- **Temporal consistency for uncertainty**: Leveraging changes in predictions and features over time to infer model reliability. *Why needed*: TinyML lacks the compute for expensive Bayesian or ensemble methods. *Quick check*: Verify that predictive divergence and feature stability signals are indeed higher under corrupted inputs.
- **Streaming conformal calibration**: Online update of quantile thresholds for risk-calibrated abstention. *Why needed*: Labels are unavailable in real deployment; calibration must be label-free. *Quick check*: Monitor calibration metrics (BS, NLL, ECE) over time under streaming conditions.
- **Auxiliary exit training (TA)**: Training intermediate heads alongside the main output to enrich feature representations. *Why needed*: Intermediate features carry useful signals for uncertainty detection. *Quick check*: Ensure TA heads improve intermediate feature discriminativeness.

## Architecture Onboarding
- **Component map**: Backbone (with TA exits) -> Ring buffer (W=16) -> Signal extractor (4 signals) -> Logistic combiner -> Streaming conformal layer (online quantile q) -> Risk score U_t.
- **Critical path**: For each inference, update ring buffer, compute 4 signals, run logistic regression, update quantile, output U_t. Memory bottleneck is the ring buffer; compute bottleneck is signal extraction.
- **Design tradeoffs**: Fixed ring buffer window vs. adaptive memory use; single-pass vs. multi-pass methods; streaming vs. offline calibration. TCUQ favors low memory and latency over maximal detection accuracy.
- **Failure signatures**: Poor CID detection (low AUPRC) if combiner overfits; high latency/memory if high-dimensional features are stored; calibration drift if quantile tracker is not sufficiently adaptive.
- **Exactly 3 first experiments**:
  1. Run TCUQ on a single sequence (e.g., CIFAR-10-C, severity 1) and plot the evolution of U_t over time; check for clear separation between ID and CID samples.
  2. Measure the latency and memory usage of the signal extraction step for different feature dimensionalities; confirm that low-dimensional (e.g., GAP) features fit within MCU constraints.
  3. Evaluate calibration (BS, NLL, ECE) on a clean ID stream after a short burn-in period; confirm stability.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can adaptive windowing or learnable lag schedules effectively reduce memory state size without compromising detection latency or calibration? Basis: Future work will explore adaptive windowing and learnable lag schedules to shrink state further. Why unresolved: TCUQ currently relies on a fixed ring buffer window (W) and fixed lag set (L), creating a static trade-off between RAM usage and signal quality.
- **Open Question 2**: How does TCUQ performance degrade or improve on purely semantic out-of-distribution (OOD) tasks when applied to high-capacity backbones compared to specialized detectors? Basis: Specialized OOD detectors may remain preferable for purely semantic OOD on high-capacity models, whereas TCUQ is most impactful under CID. Why unresolved: The evaluation focuses primarily on TinyML-scale models and corrupted-in-distribution (CID) tasks, leaving the interaction with semantic shift on larger models less certain.
- **Open Question 3**: Does the streaming conformal layer maintain calibration and stability over long-horizon deployments involving real-world, non-stationary drift? Basis: Future work includes long-horizon field studies to assess stability under real deployment drift. Why unresolved: Experiments use concatenated streams of ID, CID, and OOD data; performance over extended, unbroken real-world operation remains unverified.

## Limitations
- The exact architecture of Temporal Assistance exit heads is not specified, limiting exact replication.
- The streaming conformal quantile update rule is not fully detailed.
- The composition and size of the development set for combiner training are not specified.

## Confidence
- **Efficiency Claims (Latency, Memory, Flash):** High
- **Uncertainty Detection Performance (AUPRC, AUROC):** Medium
- **Calibration Quality (BS, NLL, ECE):** High
- **Streaming Conformal Calibration and Budgeted Abstention:** Medium

## Next Checks
1. **Validate the Impact of Ring Buffer Size (W):** Systematically vary W (e.g., 8, 16, 32) and measure the effect on both uncertainty detection (AUPRC) and efficiency (latency, memory).
2. **Test on Unseen Corruption Types:** Evaluate TCUQ on at least one corruption type not used in training (e.g., a new type of noise or transformation) to assess generalization beyond the benchmark corruptions.
3. **Reproduce Logistic Combiner Training Details:** Re-implement the combiner training using the paper's reported procedure, but explicitly vary the ratio of in-distribution to corrupted data in the development set. This will reveal the sensitivity of detection performance to training data composition.