---
ver: rpa2
title: The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models
  for Recipe Generation
arxiv_id: '2508.14718'
source_url: https://arxiv.org/abs/2508.14718
tags:
- generation
- recipe
- gpt-2
- recipes
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We established a rigorous benchmark for text-based recipe generation,
  a fundamental task in natural language generation. We present a comprehensive comparative
  study contrasting a fine-tuned GPT-2 large (774M) model against the GPT-2 small
  (124M) model and traditional LSTM/RNN baselines on the 5-cuisine corpus from RecipeDB.
---

# The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models for Recipe Generation

## Quick Facts
- arXiv ID: 2508.14718
- Source URL: https://arxiv.org/abs/2508.14718
- Reference count: 10
- Primary result: Fine-tuned GPT-2 large yields >20% relative improvement in BERTScore F1 (0.92 vs 0.72) over best recurrent baseline while reducing perplexity by 69.8%

## Executive Summary
This study establishes a rigorous benchmark for text-based recipe generation using a 5-cuisine corpus from RecipeDB. The authors compare fine-tuned GPT-2 large (774M) against GPT-2 small (124M) and traditional LSTM/RNN baselines, introducing a targeted tokenization strategy that preserves numerical quantities and structural markers. Their custom tokenizer adds 23 common fraction tokens and special boundary tokens to address limitations of generic BPE tokenizers in domain-specific text. Results show the large transformer-based approach significantly outperforms recurrent models across multiple automatic metrics including fluency, coherence, semantic relevance, and diversity.

## Method Summary
The study fine-tunes GPT-2 large and small models on structured recipes from a 5-cuisine dataset (~51K recipes), comparing against LSTM/RNN baselines. Recipes are preprocessed with custom special tokens marking sections (title, ingredients, instructions) and augmented with 23 fraction tokens to preserve numerical quantities. The models are trained using causal language modeling with AdamW optimizer (LR=3e-5), evaluated using BLEU-4, METEOR, ROUGE-L, BERTScore, diversity metrics, and perplexity. Generation uses top-p=0.95, top-k=50, and temperature=0.7 sampling.

## Key Results
- GPT-2 large achieves BERTScore F1 of 0.92 versus 0.72 for best recurrent baseline (>20% relative improvement)
- Perplexity reduced by 69.8% compared to recurrent models
- Transformer-based approach demonstrates superior semantic coherence and long-range dependency tracking
- Custom tokenization preserves numerical quantities and structural boundaries effectively

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Targeted tokenization with fraction tokens and structural markers improves numerical fidelity and recipe structure preservation
- **Mechanism:** Adding 23 common fraction tokens prevents BPE from splitting them into meaningless sub-tokens. Custom boundary tokens are treated as indivisible semantic units, signaling recipe sections explicitly to the model
- **Core assumption:** Standard BPE tokenization degrades numerical meaning and structure recognition in domain-specific text
- **Evidence anchors:** Abstract highlights tokenization strategy as key contribution; Section 3.2.2 details fraction preservation; related work shows limited tokenization innovations in recipe NLG

### Mechanism 2
- **Claim:** Fine-tuning pre-trained transformers yields substantially better semantic coherence than training recurrent models from scratch
- **Mechanism:** GPT-2's pre-training encodes general linguistic patterns; fine-tuning specializes this knowledge to recipe language distributions without losing learned representations. Self-attention maintains long-range dependencies that LSTMs lose
- **Core assumption:** Recipe language shares sufficient structure with general language for transfer learning to be effective
- **Evidence anchors:** Abstract reports >20% BERTScore improvement and 69.8% perplexity reduction; Section 5.2 shows better ingredient tracking; related work confirms fine-tuning effectiveness

### Mechanism 3
- **Claim:** Structured formatting with special tokens enables reliable section boundary generation and improves downstream parsing
- **Mechanism:** Concatenating recipe components with explicit boundary markers creates unambiguous training signals. The model learns to predict these tokens as structural anchors during generation
- **Core assumption:** Models can learn hierarchical structure from sequential token patterns when explicitly marked
- **Evidence anchors:** Section 3.2.1 shows explicit formatting with special tokens; Section 5.2 reports reliable section generation; structural token approach is novel contribution

## Foundational Learning

- **Concept: Byte Pair Encoding (BPE) Tokenization**
  - Why needed here: Understanding how standard tokenizers split text explains why fractions and special symbols get fragmented, motivating the custom augmentation strategy
  - Quick check question: How would standard BPE tokenize "3/4 cup"—and why might this harm a recipe model?

- **Concept: Causal Language Modeling (CLM) Objective**
  - Why needed here: The paper uses CLM for fine-tuning; understanding P(t_i|t_1,...,t_{i-1}) explains both training and generation behavior
  - Quick check question: Why does masking padding tokens (labels = -100) matter for loss calculation?

- **Concept: Perplexity as Intrinsic Evaluation**
  - Why needed here: The 69.8% perplexity reduction is a key result; perplexity measures model confidence in predicting test sequences
  - Quick check question: If perplexity is low but BLEU is also low, what might this indicate about generation quality?

## Architecture Onboarding

- **Component map:** RecipeDB (5-cuisine corpus) → Preprocessing (cleaning, structuring with special tokens) → Custom Tokenizer (+23 fraction tokens, +structural markers) → GPT-2 Large (774M) with resized embeddings → Fine-tuning (CLM objective, AdamW, LR=3e-5) → Generation (Top-p=0.95, Top-k=50, Temperature=0.7)

- **Critical path:** Tokenization augmentation → embedding resize → fine-tuning on structured sequences → sampling-based generation with ingredient prompts

- **Design tradeoffs:**
  - Model size: 774M vs 124M—large model shows +9% BERTScore F1 improvement but requires more GPU memory
  - Temperature: Lower (0.7) improves coherence; higher would increase diversity but risk incoherence
  - Beam vs sampling: Paper chose sampling for creativity; beam would improve consistency but reduce novelty

- **Failure signatures:**
  - Factual inaccuracies: "bake fish at 500°F for 3 hours" (Section 5.3)
  - Quantity hallucinations: "20 cloves of garlic for single serving"
  - Ingredient omissions/introductions not in prompt
  - Repetitive loops in longer generations (less frequent than LSTM but still present)

- **First 3 experiments:**
  1. **Ablate tokenization:** Train identical GPT-2 model without fraction tokens or structural markers; compare BERTScore and manual inspection of numerical accuracy
  2. **Scale comparison:** Fine-tune GPT-2 small (124M) with identical setup to isolate parameter count effects (already partially done—extend with statistical significance testing)
  3. **Constrained decoding pilot:** Implement simple ingredient-usage constraint (check all prompt ingredients appear in output) and measure reduction in omissions; report precision/recall tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent do high automatic metric scores (e.g., BERTScore) correlate with human judgments of recipe feasibility, safety, and clarity?
- **Basis in paper:** [explicit] The authors explicitly state in Section 6 that "Systematic Human Evaluation" is a "critical next step" to assess qualities like safety and creativity that automatic metrics miss
- **Why unresolved:** The study relies entirely on automatic metrics (BLEU, METEOR, etc.), which failed to penalize nonsensical or dangerous instructions (e.g., "bake at 500°F for 3 hours") identified in the qualitative analysis (Section 5.3)
- **What evidence would resolve it:** A user study evaluating generated recipes on specific functional criteria (feasibility, safety) to validate or correct the automatic metric findings

### Open Question 2
- **Question:** Can Retrieval-Augmented Generation (RAG) or constrained decoding effectively eliminate factual hallucinations regarding quantities and cooking physics without reducing the novelty of the generated recipes?
- **Basis in paper:** [explicit] Section 6 lists "Retrieval-Augmented Generation" and "Constrained Decoding" as future work to address the "Factual Inaccuracy and Common Sense" limitations detailed in Section 5.3
- **Why unresolved:** The current fine-tuned model generates statistically probable but factually incorrect text (e.g., unrealistic quantities like "5 cups of salt") because it lacks true world knowledge or physical constraints
- **What evidence would resolve it:** Experiments integrating RAG with culinary knowledge bases, measuring the reduction in factual errors while tracking diversity metrics to ensure creativity is maintained

### Open Question 3
- **Question:** How do encoder-decoder architectures (e.g., T5, BART) compare to the fine-tuned decoder-only GPT-2 approach in handling the structured constraints of recipe generation?
- **Basis in paper:** [explicit] Section 6 suggests "Benchmarking Additional Architectures," specifically mentioning encoder-decoder models like T5 and BART, to provide a "more complete picture of architectural trade-offs"
- **Why unresolved:** The current study focuses exclusively on decoder-only transformers (GPT-2) vs. RNNs, leaving the performance of other dominant architectures on this specific tokenization strategy unknown
- **What evidence would resolve it:** A comparative benchmark on the 5-cuisine corpus using T5/BART models adapted with the same custom fraction-aware tokenization

## Limitations

- **Limited human evaluation scope:** No systematic human preference studies validate whether automated metric improvements translate to perceptually better recipes
- **Factual accuracy remains unresolved:** The model generates implausible instructions but evaluation framework doesn't measure this critical failure mode
- **Tokenization innovation unproven in isolation:** No ablation study isolates the effect of custom tokenization from larger model architecture improvements

## Confidence

- **High confidence:** Causal language modeling framework and general architecture implementation (training setup, tokenization approach, evaluation metrics are clearly specified)
- **Medium confidence:** Relative performance improvements (BERTScore, perplexity gains) are well-documented, but absolute quality and practical utility remain uncertain without human evaluation
- **Low confidence:** Claims about tokenization strategy being the "key contribution" addressing a "critical limitation" lack direct experimental validation through ablation

## Next Checks

1. **Ablation study on tokenization:** Train identical GPT-2 large model with and without the custom fraction tokens and structural markers, keeping all other hyperparameters constant. Compare not just automated metrics but also manual inspection of numerical accuracy and structural coherence to isolate tokenization effects.

2. **Human preference evaluation:** Conduct a blind pairwise comparison study where human raters evaluate recipe quality across the three model variants (large, small, LSTM) on dimensions like plausibility, coherence, and appeal. This would validate whether automated metric improvements correspond to perceived quality gains.

3. **Factual consistency measurement:** Implement a simple constraint-checking system that verifies ingredient usage (all prompt ingredients appear in output) and sanity checks (temperature/time plausibility). Report precision/recall tradeoff and measure how much generation quality degrades when these constraints are enforced, quantifying the factual accuracy problem the paper acknowledges but doesn't solve.