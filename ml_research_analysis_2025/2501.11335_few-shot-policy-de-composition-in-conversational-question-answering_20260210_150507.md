---
ver: rpa2
title: Few-shot Policy (de)composition in Conversational Question Answering
arxiv_id: '2501.11335'
source_url: https://arxiv.org/abs/2501.11335
tags:
- question
- policy
- questions
- logic
- logical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Logical Decomposition for Policy Compliance
  (LDPC), a neuro-symbolic framework that uses large language models (LLMs) to perform
  policy compliance detection (PCD) in conversational settings. The approach decomposes
  policies into yes/no questions, composes them into explicit logical formulas, and
  uses three-valued logic to evaluate compliance, requiring no task-specific fine-tuning.
---

# Few-shot Policy (de)composition in Conversational Question Answering

## Quick Facts
- arXiv ID: 2501.11335
- Source URL: https://arxiv.org/abs/2501.11335
- Reference count: 26
- Achieves 79.0% micro accuracy and 79.7% macro accuracy on ShARC benchmark

## Executive Summary
This paper introduces Logical Decomposition for Policy Compliance (LDPC), a neuro-symbolic framework that performs policy compliance detection in conversational question answering without requiring task-specific fine-tuning. The approach leverages large language models to decompose policies into yes/no questions, compose them into explicit logical formulas, and evaluate compliance using three-valued logic. The system achieves competitive performance on the ShARC benchmark, ranking near state-of-the-art while maintaining interpretability through its logical formula representation.

## Method Summary
LDPC operates through a neuro-symbolic pipeline that transforms conversational policies into executable logical formulas. The system uses LLMs to generate logical decompositions via self-consistency sampling, producing consistent logical formulas from policy text. These formulas are then composed into explicit logical expressions that can be evaluated against user responses. A RoBERTa model handles the question answering component, while three-valued logic (true, false, unknown) accommodates the uncertainty inherent in conversational interactions. The framework requires no task-specific fine-tuning, relying instead on the general reasoning capabilities of pre-trained models.

## Key Results
- Achieves 79.0% micro accuracy and 79.7% macro accuracy on ShARC benchmark
- Ranks near state-of-the-art despite not being fine-tuned on the dataset
- Demonstrates competitive performance while maintaining interpretability

## Why This Works (Mechanism)
The approach works by leveraging the reasoning capabilities of large language models to bridge the gap between natural language policies and formal logical representations. By decomposing complex policies into simpler yes/no questions, the system reduces the reasoning burden on any single component. The use of three-valued logic appropriately handles the uncertainty and incompleteness common in conversational interactions. Self-consistency sampling helps ensure the reliability of the generated logical formulas by sampling multiple times and selecting consistent outputs.

## Foundational Learning
- Three-valued logic: Needed to handle uncertainty in conversational responses; Quick check: Verify system correctly handles unknown responses
- Logical decomposition: Breaks complex policies into simpler components; Quick check: Validate that decomposed questions accurately represent original policy intent
- Self-consistency sampling: Improves reliability of LLM outputs; Quick check: Compare consistency rates across different sampling parameters
- Neuro-symbolic integration: Combines neural reasoning with symbolic logic; Quick check: Test performance with and without symbolic components
- Policy compliance detection: Core task of determining if user responses meet policy requirements; Quick check: Verify accuracy on edge cases in policy interpretation

## Architecture Onboarding

Component Map: LLM Decomposition -> Logical Formula Composition -> RoBERTa QA -> Three-valued Logic Evaluation

Critical Path: The logical formula generation phase is critical, as errors here propagate through the entire system. The RoBERTa model's question answering accuracy directly impacts final compliance detection performance.

Design Tradeoffs: The framework trades potential fine-tuning gains for generalization and interpretability. The self-consistency sampling approach balances computational cost against formula quality.

Failure Signatures: Errors typically manifest as incorrect logical formula generation or QA model misclassification. Ambiguous policies or conversational contexts are particularly challenging.

First Experiments:
1. Test logical formula generation consistency across multiple LLM runs
2. Evaluate RoBERTa QA performance on isolated policy questions
3. Measure three-valued logic evaluation accuracy on synthetically generated test cases

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the scalability of the approach to more complex policy domains, the impact of conversational context on logical formula generation, and the potential for extending the framework to multilingual settings.

## Limitations
- LLM-generated logical formulas may produce inconsistent decompositions across runs
- Three-valued logic may not capture all nuances of complex policy relationships
- Limited evaluation scope focused primarily on ShARC benchmark

## Confidence
- Performance claims (High): Verified accuracy metrics align with methodology
- Interpretability claims (Medium): Limited empirical evidence provided for practical interpretability
- Generalization claims (Low): Narrow evaluation scope constrains generalization assertions

## Next Checks
1. Conduct ablation studies comparing LLM-generated vs manually crafted logical formulas
2. Perform inter-rater reliability testing on generated logical formulas
3. Test system performance on additional policy domains beyond ShARC