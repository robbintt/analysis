---
ver: rpa2
title: 'Neural PDE Solvers with Physics Constraints: A Comparative Study of PINNs,
  DRM, and WANs'
arxiv_id: '2510.09693'
source_url: https://arxiv.org/abs/2510.09693
tags:
- uni00000013
- neural
- uni00000003
- methods
- equation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically compares three neural PDE solvers\u2014\
  physics-informed neural networks (PINNs), the deep Ritz method (DRM), and weak adversarial\
  \ networks (WANs)\u2014on Poisson equations (up to 5D) and time-independent Schr\xF6\
  dinger equations (1D/2D infinite well and harmonic oscillator). All methods achieve\
  \ low L2 errors (10\u207B\u2076-10\u207B\u2079) when paired with forced boundary\
  \ conditions (FBCs), forced nodes (FNs), and orthogonality regularization (OG)."
---

# Neural PDE Solvers with Physics Constraints: A Comparative Study of PINNs, DRM, and WANs

## Quick Facts
- arXiv ID: 2510.09693
- Source URL: https://arxiv.org/abs/2510.09693
- Reference count: 0
- Key outcome: PINNs are most reliable for accuracy and recovery of excited spectra; DRM offers best accuracy-runtime trade-off on stationary problems; WAN is more sensitive but competitive with proper tuning

## Executive Summary
This study systematically compares three neural PDE solvers—physics-informed neural networks (PINNs), the deep Ritz method (DRM), and weak adversarial networks (WANs)—on Poisson equations (up to 5D) and time-independent Schrödinger equations (1D/2D infinite well and harmonic oscillator). All methods achieve low L2 errors (10⁻⁶-10⁻⁹) when paired with forced boundary conditions (FBCs), forced nodes (FNs), and orthogonality regularization (OG). PINNs are the most reliable for accuracy and recovery of excited spectra; DRM offers the best accuracy-runtime trade-off on stationary problems; WAN is more sensitive but competitive when weak-form constraints and FN/OG are used effectively. Sensitivity analyses show that FBC removes boundary-loss tuning, network width matters more than depth for single-network solvers, and most gains occur within 5000-10,000 epochs. The same toolkit solves the KH case, indicating transfer beyond canonical benchmarks.

## Method Summary
The study compares three neural PDE solvers: PINNs minimize PDE residuals directly, DRM minimizes an energy functional via variational formulation, and WANs operate on the weak form using adversarial training. All use fully connected networks (3 hidden layers, 50 neurons, tanh activation) with automatic differentiation for derivative computation. Key techniques include forced boundary conditions (hard enforcement), forced nodes (prior knowledge of nodal locations), and orthogonality regularization (preventing ground state collapse for excited states). The solvers are evaluated on Poisson equations (1D-5D) and Schrödinger equations (1D/2D) for eigenstates n=0-5, measuring L2 error, eigenvalue error, runtime, and PDE residual norm.

## Key Results
- All three methods achieve L2 errors of 10⁻⁶-10⁻⁹ on benchmark problems with proper regularization
- PINNs are most reliable for recovering excited state spectra when combined with OG and FN
- DRM provides the best accuracy-runtime trade-off for stationary elliptic problems
- Most accuracy gains occur within 5,000-10,000 training epochs
- The same toolkit (FBC, FN, OG) successfully extends to the Kramers-Henneberger potential

## Why This Works (Mechanism)
The methods work by representing the solution as a neural network and using automatic differentiation to compute required derivatives for the physics constraints. PINNs directly minimize the PDE residual at collocation points, DRM minimizes an energy functional that has the PDE solution as its minimizer, and WANs minimize a functional derived from the weak form. The key innovations—FBC for hard boundary enforcement, FN for guiding excited states through known nodal locations, and OG for preventing ground state collapse—address the fundamental challenge of spectral bias in neural networks, enabling them to capture higher-frequency components of the solution that correspond to excited states.

## Foundational Learning

- **Concept: Variational Formulations of PDEs**
  - Why needed here: To understand DRM, which doesn't minimize the PDE residual directly. Instead, it minimizes an energy functional (e.g., $I[u] = \int (1/2 |\nabla u|^2 - f u) dx$). The minimizer of this functional is the solution to the PDE.
  - Quick check question: How does converting a differential equation into an integral minimization problem change the requirements for the solution's differentiability?

- **Concept: Weak Form of a PDE**
  - Why needed here: To understand WANs, which operate on the weak form. This involves multiplying the PDE by a test function $v$, integrating, and using integration by parts to reduce derivative requirements on the solution $u$.
  - Quick check question: Why is a solution to the weak form often more general than a classical solution to the strong form?

- **Concept: Automatic Differentiation (AD)**
  - Why needed here: All three methods (PINNs, DRM, WANs) use neural networks to represent the solution. Computing the PDE residual or energy functional requires calculating derivatives of the network's output with respect to its inputs (e.g., $\nabla u$). AD allows for exact and efficient computation of these derivatives without symbolic math or numerical finite differences.
  - Quick check question: Why is automatic differentiation essential for training "physics-informed" neural networks compared to standard backpropagation used for training classifiers?

## Architecture Onboarding

- **Component map:** Sampler -> Network -> AD Engine -> Physics Loss Module -> Optimizer
- **Critical path:** The primary critical path for all methods starts with the Sampler generating points. These points are fed into the Network to produce solution values. AD is then used to calculate required derivatives (first-order for DRM, second-order for PINNs on Schrödinger). These derivatives are used in the Physics Loss Module to compute the loss. The gradients of this loss are backpropagated to update the network parameters. For WANs, a second adversarial network introduces a second critical path for its own update step, creating an interleaved training loop.
- **Design tradeoffs:**
  - PINNs are general but can be slow to converge. They require balancing PDE residual, boundary, and data losses.
  - DRM is often faster and more stable for elliptic problems with variational forms but is less general. It requires the PDE to be cast as a minimization problem.
  - WANs are theoretically powerful for handling discontinuous solutions via the weak form but are more complex to implement and tune due to adversarial training instabilities.
- **Failure signatures:**
  - Trivial Solution Collapse: The network converges to the zero function or a constant, especially in DRM without proper normalization or boundary constraints.
  - Spectral Bias: Difficulty in learning high-frequency components of the solution, leading to poor recovery of excited states. OG and FN are designed to mitigate this.
  - Training Instability: In WANs, this manifests as oscillating or diverging losses. In PINNs, it can be caused by imbalanced loss term magnitudes.
- **First 3 experiments:**
  1. **Poisson Baseline:** Implement all three solvers on a 1D Poisson equation with a known analytical solution. Compare their L2 error and runtime using a simple soft boundary condition. This establishes a baseline for comparison.
  2. **FBC vs. BC Ablation:** Re-run the 1D Poisson experiment using Forced Boundary Conditions (FBC) for all three methods. Quantify the improvement in both final accuracy and convergence speed. This isolates the impact of hard boundary constraints.
  3. **Excited State Recovery:** Task the solvers with finding the first excited state of a 1D Schrödinger equation (e.g., infinite well). Start with a baseline PINN/DRM, then add Orthogonality Regularization (OG) and Forced Nodes (FN) separately. Compare which method successfully converges to the excited state and not the ground state. This tests the mechanisms for handling multi-modal solutions.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can spacetime variational formulations effectively extend DRM and WAN to solve fully time-dependent PDEs? The study focused on time-independent problems; empirical benchmarks on standard time-dependent PDEs would resolve this.

- **Open Question 2:** Can neural domain decomposition replace forced nodes (FN) to handle potentials with irregular nodal structures? FN requires prior knowledge of nodal locations and failed on irregular potentials; domain decomposition could enforce boundary conditions locally without this prior knowledge.

- **Open Question 3:** How can orthogonality regularization (OG) be stabilized to prevent the mixing of degenerate eigenstates in higher dimensions? Standard OG penalties inadvertently mix degenerate states, requiring data-driven disambiguation rather than solving purely from physics constraints.

- **Open Question 4:** Does adaptive, residual-driven sampling improve convergence rates relative to the uniform sampling used in this study? The paper notes gains appeared within fixed epochs, but efficiency might be improved by dynamically placing points where the PDE residual is highest.

## Limitations
- The study relies on careful tuning of collocation points, network width, and loss weights, raising questions about scalability to more complex problems
- All benchmark problems are stationary, linear, and have known analytical solutions, limiting claims about real-world applicability
- The comparative advantage of WANs for weak solutions is asserted but not demonstrated on problems with singularities or shocks

## Confidence
- **High Confidence**: PINNs are the most reliable for accuracy and recovery of excited spectra when equipped with FBC, FN, and OG. DRM offers the best accuracy-runtime trade-off on stationary problems. Most accuracy gains occur within 5,000-10,000 epochs.
- **Medium Confidence**: WANs are competitive when weak-form constraints and FN/OG are used effectively, but are more sensitive to tuning. The same toolkit (FBC, FN, OG) solves the KH case, indicating transfer beyond canonical benchmarks.
- **Low Confidence**: The study does not provide sufficient evidence that the observed method rankings hold for nonlinear, time-dependent, or high-gradient problems. The claim that DRM is "less general" is not substantiated with examples where PINNs succeed and DRM fails.

## Next Checks
1. **Nonlinear and Time-Dependent Extension:** Validate all three methods on a nonlinear PDE (e.g., Burgers' equation) and a time-dependent PDE (e.g., heat equation) to test claims about general applicability. Measure L2 error, runtime, and stability across a range of initial/boundary conditions.

2. **Adaptive Sampling and Gradient-Driven Refinement:** Implement an adaptive sampling strategy that refines collocation points based on local PDE residual magnitude. Compare the final L2 error and total runtime against uniform sampling for a problem with a sharp gradient or singularity (e.g., a step function source term in Poisson's equation).

3. **Robustness to Network Architecture and Initialization:** Conduct a systematic ablation study on network width (10, 50, 100, 200 neurons) and depth (2, 3, 4 hidden layers) for each method on a fixed problem (e.g., 1D Schrödinger infinite well, n=2). Report the variance in L2 error and runtime across 10 random initializations to quantify the impact of architecture and initialization on stability.