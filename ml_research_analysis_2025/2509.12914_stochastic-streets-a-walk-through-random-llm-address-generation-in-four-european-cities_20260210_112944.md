---
ver: rpa2
title: 'Stochastic Streets: A Walk Through Random LLM Address Generation in four European
  Cities'
arxiv_id: '2509.12914'
source_url: https://arxiv.org/abs/2509.12914
tags:
- streets
- llms
- numbers
- random
- madrid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether Large Language Models (LLMs) can
  generate random street addresses for European cities. Four major European cities
  (Amsterdam, Madrid, Paris, and Rome) were selected, and five LLM models were tested
  using 1,000 prompts per city per model.
---

# Stochastic Streets: A Walk Through Random LLM Address Generation in four European Cities

## Quick Facts
- arXiv ID: 2509.12914
- Source URL: https://arxiv.org/abs/2509.12914
- Reference count: 8
- Primary result: LLMs fail to generate uniformly random street addresses, repeatedly selecting a limited set of streets rather than sampling uniformly across city street databases

## Executive Summary
This study investigates whether Large Language Models (LLMs) can generate random street addresses for European cities. Four major European cities (Amsterdam, Madrid, Paris, and Rome) were selected, and five LLM models were tested using 1,000 prompts per city per model. The results show that LLMs exhibit significant bias, repeatedly selecting a limited set of streets rather than sampling uniformly. For example, in Madrid, the most frequent street appeared over 200 times, and in some cases more than 600 times, despite the city having approximately 10,000 streets. Additionally, some models generated invalid addresses. While address numbers showed more uniform distribution than street names, certain models exhibited specific number biases (e.g., 14, 12, 17, 45). The study concludes that LLMs struggle with random address generation, highlighting another limitation of these models in handling seemingly simple tasks.

## Method Summary
The study tested five LLM models (GPT-4.1, Gemini 2.5 Flash, Gemma 3-12B, LLaMA 3.1-8B, Mistral-7B, Qwen3-8B) using 1,000 prompts per city per model. Each prompt explicitly requested random street selection with uniform probability, followed by random number selection. Temperature was set to 1.0 for all models. Outputs were parsed into street-number pairs, validated against official street registries, and visualized as heatmaps using OpenStreetMap geocoding. The primary metrics were frequency distribution of streets (top-5 most frequent), address number distribution variance, overlap with iconic streets, and address validity against official sources.

## Key Results
- Street selection bias: Single streets appeared 200-600+ times in 1,000 samples (expected ~1-2 for uniform distribution)
- Address hallucination: Qwen3-8B generated 4/5 top streets not in official Madrid street list
- Number distribution: More uniform than streets, but specific biases observed (Gemma3 over-generated "14", Qwen3 over-generated "12" and "123")

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** First-token selection heavily biases subsequent street name generation, constraining output diversity before the model considers the full street vocabulary.
- **Mechanism:** When prompted for an address, models preferentially generate street-type tokens (e.g., "Calle," "Rue," "Via") as the first token. This initial token conditions the probability distribution for all subsequent tokens, effectively filtering the candidate space to streets matching that prefix.
- **Core assumption:** The model's token probability distribution at position 1 captures its structural bias toward certain street types.
- **Evidence anchors:**
  - [Page 4]: "A possible reason for the biases is that when asking the LLM for an address, the model tends to prefer naming streets over avenues, boulevards, squares, etc. This causes the first output token to be 'street' ('calle', 'rue', 'vie') which already influences the final response."
  - [Page 4]: "We computed the log probabilities of the first token generated by the GPT-4.1 model. We found that the token 'C' has a probability of 99.87%, whereas the token 'Gran' has a probability of less than 0.0005%."
  - [corpus]: Weak direct evidence; related work on graph traversals and sampling (Breaking the Static Graph) discusses context-aware path selection but not token-level conditioning.
- **Break condition:** If prompt engineering forces the model to begin with a neutral token or enumerates all street types explicitly, the first-token bias should diminish.

### Mechanism 2
- **Claim:** LLMs sample from learned training data distributions rather than generating uniformly random outputs, causing high-frequency training examples to dominate responses.
- **Mechanism:** LLMs predict the most probable next token given the prompt and training corpus statistics. Streets appearing more frequently in training data (or in contexts associated with addresses) receive higher probability mass, regardless of prompt instructions to sample uniformly.
- **Core assumption:** Training corpus street-name frequency correlates with generation frequency.
- **Evidence anchors:**
  - [Page 2]: "This may occur because their output is conditioned on learned statistical patterns from training data rather than on intrinsic randomness."
  - [Page 4]: "This suggests that importance or popularity is not the only factor that influences LLM responses, hinting at the presence of more complex underlying biases."
  - [corpus]: Related work on random number generation in LLMs (referenced as [4] in paper) supports deterministic patterns from training statistics.
- **Break condition:** If streets have equal training frequency, or if fine-tuning explicitly reweights street probabilities, generation should approach uniformity.

### Mechanism 3
- **Claim:** Address format ordering determines when randomness can be introduced, with number-first formats amplifying bias by forcing early decisions from fixed distributions.
- **Mechanism:** In autoregressive generation, each token conditions on all prior tokens. Paris addresses (e.g., "17 Rue de Charonne") require the number first, meaning the model samples from a fixed number distribution before any street-specific context can introduce variation.
- **Core assumption:** Later tokens benefit from accumulated context that dilutes initial distributional biases.
- **Evidence anchors:**
  - [Page 7]: "A typical address appears as '17 Rue de Charonne, 75011 Paris, France' where the address number comes at the very beginning. This means that the model selects the number before selecting the street which limits the model's ability to introduce randomness incrementally through its autoregressive generation process."
  - [Page 7]: "Gemma3 and Qwen3 exhibited a disproportionately high frequency of selecting the numbers '14' and '12' respectively when generating addresses in Paris."
  - [corpus]: No direct corpus evidence on format-order effects in LLM generation.
- **Break condition:** If prompts restructure output format to place street names before numbers, number distribution should become more uniform.

## Foundational Learning

- **Concept: Autoregressive token prediction**
  - **Why needed here:** Understanding that each token is predicted conditional on all prior tokens explains why early choices (first token, number before street) constrain later diversity.
  - **Quick check question:** If you generate 100 tokens autoregressively starting from token A vs. token B, will the final token distributions differ?

- **Concept: Temperature and sampling diversity**
  - **Why needed here:** The study sets temperature=1; understanding how temperature scales logits before softmax clarifies why even "neutral" temperature doesn't guarantee uniformity.
  - **Quick check question:** At temperature → ∞, what happens to the probability distribution over tokens?

- **Concept: Training data distributional bias**
  - **Why needed here:** LLMs are not random number generators; they are pattern completers. Their "random" outputs reflect training corpus statistics.
  - **Quick check question:** Why might "Calle de Alcalá" appear 600 times even if the model was never explicitly trained on address generation?

## Architecture Onboarding

- **Component map:** Prompt template -> First-token probability distribution -> Street-type prefix selection -> Street name completion -> Number generation -> Address validation

- **Critical path:**
  1. Prompt construction (city, language) → 2. First-token probability distribution → 3. Street-type prefix selection → 4. Street name completion → 5. Number generation → 6. Address validation

- **Design tradeoffs:**
  - Higher temperature (>1) increases diversity but risks hallucinations (invalid streets)
  - Explicit street-type enumeration in prompts may reduce first-token bias but constrains natural generation
  - Post-hoc filtering removes invalid addresses but reduces effective sample size

- **Failure signatures:**
  - Concentration on few streets: Top street appears >200 times in 1,000 generations (expected ~1-2 for uniform)
  - Invalid streets: Qwen3-8B generated 4/5 top streets not in official Madrid street list
  - Number spikes: Gemma3 over-generates "14"; Qwen3 over-generates "12" and "123"

- **First 3 experiments:**
  1. **First-token intervention:** Modify prompt to force specific street types (e.g., "Generate a random boulevard in Madrid") and measure distribution shift
  2. **Format reordering:** Request street-first format for Paris addresses and compare number distribution variance to baseline
  3. **Hallucination audit:** Validate top-10 generated streets per city against official street registries; compute invalid-address rate per model

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can LLMs be fine-tuned to produce statistically uniform random addresses, and does this capability generalize to cities not included in the fine-tuning set?
- **Basis in paper:** [explicit] The authors conclude that the "next step will be to check if the models can learn to produce more random like addresses if they are fine-tuned with examples and if so, whether the improvement generalizes to other cities."
- **Why unresolved:** The current study only evaluated pre-trained models using in-context learning (prompts), without attempting to modify model weights to correct for the observed distributional biases.
- **What evidence would resolve it:** A follow-up study showing that fine-tuned models achieve a near-uniform distribution of street selections in a target city and maintain this behavior when tested on a new, unseen city.

### Open Question 2
- **Question:** Does the syntactic order of address components (e.g., number preceding the street name vs. following it) causally increase numerical bias in autoregressive generation?
- **Basis in paper:** [inferred] The paper notes that Parisian addresses (where numbers precede streets) showed higher variance/bias for numbers than other cities. The authors suggest this happens because "the model selects the number before selecting the street which limits the model's ability to introduce randomness."
- **Why unresolved:** The study identified a correlation between address format and bias, but did not isolate syntax as the independent variable by testing shuffled formats or synthetic address structures.
- **What evidence would resolve it:** Experiments using standardized prompts that force the model to generate street names before numbers in all languages, or comparing bias levels across languages with different standard address formats.

### Open Question 3
- **Question:** To what extent does the frequency of a language in the training data correlate with the "randomness" quality of geographic generation for that locale?
- **Basis in paper:** [inferred] The authors observed worse performance for Amsterdam and hypothesized this is "due to Dutch having a smaller presence in the training datasets of most LLMs," implying a link between data representation and task performance.
- **Why unresolved:** The study sampled only four cities, making it difficult to distinguish between city-specific quirks (e.g., Amsterdam's canals/gracht naming conventions) and broader training data deficiencies.
- **What evidence would resolve it:** A broader analysis correlating the training token count for various languages (if available) with the entropy/uniformity scores of address generation in those respective regions.

## Limitations
- Translation uncertainty: Prompt translations for Spanish, French, Italian, and Dutch are assumed accurate but unverified
- API version ambiguity: Exact API endpoints and parameters for proprietary models unspecified
- Registry completeness: Official street databases may have different inclusion criteria across cities

## Confidence
**High Confidence**: The core observation that LLMs exhibit non-uniform street selection is well-supported by empirical data showing extreme concentration (e.g., single streets appearing 200-600 times in 1,000 samples).

**Medium Confidence**: Proposed mechanisms explaining bias (first-token conditioning, training data distribution, format ordering) are plausible and supported by specific evidence, but remain speculative without controlled experiments.

**Low Confidence**: Generalization of findings beyond the four studied cities and five models is uncertain. Different languages, address formats, or model architectures might behave differently.

## Next Checks
1. **First-token intervention experiment**: Modify prompts to explicitly specify street types and measure changes in distribution concentration to test Mechanism 1
2. **Format reordering validation**: Restructure Paris prompts to request street-first format and compare number distribution variance to baseline to test Mechanism 3
3. **Cross-city generalization study**: Repeat experiment with additional European cities having different address formats to assess universality of observed biases