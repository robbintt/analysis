---
ver: rpa2
title: Adaptive Multi-task Learning for Probabilistic Load Forecasting
arxiv_id: '2512.20232'
source_url: https://arxiv.org/abs/2512.20232
tags:
- load
- learning
- forecasting
- entities
- ieee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of simultaneous load forecasting
  across multiple entities (e.g., regions, buildings) in power systems, which is complicated
  by uncertainties in load demand, dynamic changes in consumption patterns, and correlations
  among entities. Existing multi-task learning methods for load forecasting are limited
  to offline learning, preventing them from adapting to evolving consumption patterns.
---

# Adaptive Multi-task Learning for Probabilistic Load Forecasting

## Quick Facts
- **arXiv ID**: 2512.20232
- **Source URL**: https://arxiv.org/abs/2512.20232
- **Reference count**: 40
- **Primary result**: Introduces Multi-APLF, an adaptive multi-task learning method using vector-valued HMMs that dynamically updates model parameters to capture evolving consumption patterns and correlations among entities, achieving lower RMSE and MAPE than existing methods across multiple real-world datasets.

## Executive Summary
This paper addresses the challenge of simultaneous probabilistic load forecasting across multiple entities in power systems, where uncertainties in load demand, dynamic consumption patterns, and inter-entity correlations complicate traditional forecasting approaches. Existing multi-task learning methods are limited to offline learning and cannot adapt to changing patterns. The authors propose Multi-APLF, an adaptive method based on vector-valued hidden Markov models that recursively updates parameters using forgetting factors to capture evolving patterns and correlations while providing reliable uncertainty assessments. Experiments demonstrate superior performance in forecasting accuracy and probabilistic calibration across multiple real-world datasets compared to existing methods.

## Method Summary
The paper proposes an adaptive multi-task learning framework for probabilistic load forecasting using vector-valued hidden Markov models (HMMs). The method performs online learning by recursively updating mean vectors and covariance matrices with forgetting factors (λ_s=0.9, λ_r=0.9) to capture changing consumption patterns. For prediction, it generates 24-hour-ahead forecasts and uncertainty estimates through recursive fusion of observation and state models. The approach handles multiple entities simultaneously, with computational complexity growing quadratically with entity count but remaining constant with respect to historical data size. The method includes specific mechanisms for maintaining positive definite covariance matrices through thresholding and rank-1 matrix compensation.

## Key Results
- Multi-APLF achieves lower RMSE and MAPE across multiple entities compared to existing methods on real-world datasets (GEFCom2017, ISO New England, PJM, Australian Electricity Demand, NSW)
- The method demonstrates superior probabilistic performance with better calibration error, CRPS, and Pinball Loss metrics
- Computational efficiency scales well, with complexity quadratic in entity count and constant in historical data size
- The approach shows robustness to delayed load demand and scalability to large-scale scenarios with many entities

## Why This Works (Mechanism)
The method works by combining vector-valued HMMs with online learning to simultaneously model multiple entities while capturing their correlations and temporal dynamics. The recursive parameter updates with forgetting factors allow the model to adapt to changing consumption patterns without requiring full retraining. The joint modeling of entities enables information sharing across tasks, improving forecasting accuracy for entities with limited historical data. The probabilistic framework provides uncertainty quantification that reflects both aleatoric uncertainty (inherent randomness) and epistemic uncertainty (model uncertainty).

## Foundational Learning

**Hidden Markov Models**: Probabilistic models where the system being modeled is assumed to be a Markov process with unobservable states. Why needed: Forms the theoretical foundation for modeling temporal dependencies in load patterns. Quick check: Verify understanding of state transition probabilities and emission distributions.

**Vector-Valued HMMs**: Extension of HMMs where observations are vectors rather than scalars, allowing modeling of multiple correlated variables. Why needed: Enables joint modeling of multiple entities and their correlations. Quick check: Understand how joint covariance matrices capture inter-entity relationships.

**Online/Recursive Learning**: Learning algorithms that update model parameters incrementally as new data arrives rather than batch retraining. Why needed: Allows adaptation to evolving consumption patterns without full model retraining. Quick check: Verify understanding of forgetting factors and their impact on parameter stability.

**Probabilistic Forecasting**: Providing full predictive distributions rather than point estimates, including uncertainty quantification. Why needed: Critical for power system operations where risk management requires understanding prediction uncertainty. Quick check: Understand proper scoring rules like CRPS and Pinball Loss.

## Architecture Onboarding

**Component map**: Historical loads → Feature engineering → Vector-Valued HMM → Recursive parameter updates → Probabilistic forecasts

**Critical path**: Data collection and preprocessing → Feature vector construction → Online parameter updates → Forecast generation → Uncertainty quantification

**Design tradeoffs**: The quadratic complexity in entity count enables joint modeling of correlations but may limit scalability to extremely large numbers of entities. The online learning approach sacrifices some statistical efficiency compared to batch methods but gains adaptability to changing patterns.

**Failure signatures**: Non-positive definite covariance matrices due to thresholding operations, numerical instability in matrix inversions, poor performance for entities with vastly different scales, failure to capture long-term temporal dependencies due to independence assumptions between hidden states.

**First experiments**:
1. Implement temperature shift features as defined in reference [24] and verify their impact on individual entity MAPE
2. Test the rank-1 matrix compensation procedure to ensure positive definiteness is maintained after multiple updates
3. Reproduce forecasting accuracy metrics (RMSE, MAPE) for the PJM dataset with the specified train/test split

## Open Questions the Paper Calls Out
None

## Limitations
- The recursive updates of joint covariance matrices may become numerically unstable as entity count increases
- The assumption of temporal independence between hidden states is a significant simplification that may not capture long-term load patterns
- Performance metrics focus on point estimates and standard probabilistic measures but do not address potential bias in uncertainty quantification for rare events or extreme weather conditions

## Confidence

**High confidence**: The mathematical formulation of the Vector-Valued HMM framework and the computational complexity analysis are well-specified and reproducible.

**Medium confidence**: The experimental setup and baseline comparisons are detailed, but the exact implementation of feature engineering (temperature shifts, mean past temperature) requires reference to external work [24].

**Low confidence**: The scalability claims for massive numbers of entities are not empirically validated beyond the reported datasets.

## Next Checks

1. **Reproduce Table 1**: Replicate the forecasting accuracy metrics (RMSE, MAPE) for the PJM dataset using the same train/test split and baseline methods.

2. **Test positive definiteness**: Implement the rank-1 matrix compensation procedure and verify that the covariance matrix remains positive definite after multiple recursive updates, particularly for K > 10 entities.

3. **Validate feature engineering**: Implement the temperature shift features as defined in reference [24] and confirm that the MAPE for individual entities is not dominated by scale differences.