---
ver: rpa2
title: 'DTCCL: Disengagement-Triggered Contrastive Continual Learning for Autonomous
  Bus Planners'
arxiv_id: '2512.18988'
source_url: https://arxiv.org/abs/2512.18988
tags:
- learning
- policy
- autonomous
- disengagement
- driving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a Disengagement-Triggered Contrastive Continual\
  \ Learning (DTCCL) framework for improving autonomous bus planning policies through\
  \ real-world operational feedback. The key innovation is using disengagement events\u2014\
  moments when human drivers override the system\u2014as high-value supervision signals\
  \ to iteratively refine driving policies."
---

# DTCCL: Disengagement-Triggered Contrastive Continual Learning for Autonomous Bus Planners

## Quick Facts
- arXiv ID: 2512.18988
- Source URL: https://arxiv.org/abs/2512.18988
- Authors: Yanding Yang; Weitao Zhou; Jinhai Wang; Xiaomin Guo; Junze Wen; Xiaolong Liu; Lang Ding; Zheng Fu; Jinyu Miao; Kun Jiang; Diange Yang
- Reference count: 37
- Primary result: 48.6% improvement in planning performance over direct retraining using disengagement-triggered contrastive learning

## Executive Summary
This paper presents a Disengagement-Triggered Contrastive Continual Learning (DTCCL) framework for improving autonomous bus planning policies through real-world operational feedback. The key innovation is using disengagement events—moments when human drivers override the system—as high-value supervision signals to iteratively refine driving policies. The framework collects disengagement logs, generates positive and negative samples through safety-aware augmentation, and employs contrastive learning to enhance policy robustness and generalization. Experiments demonstrate that DTCCL achieves significant gains in safety, stability, and efficiency in complex urban scenarios, with a 48.6% improvement over direct retraining approaches.

## Method Summary
DTCCL uses a cloud-edge architecture where disengagement events trigger cloud-based policy updates. The system extracts temporal segments around disengagement events, generates positive samples through safety-constrained perturbations and agent dropout, and creates negative samples via collision-inducing scenarios. A joint training objective combines behavior cloning, triplet contrastive loss, and stability constraints to refine the policy. The continual learning loop enables scalable fleet-wide improvements without manual intervention, with validation showing improved performance on both nominal routes and previously problematic disengagement scenarios.

## Key Results
- 48.6% improvement in overall planning performance compared to direct retraining on disengagement cases
- Achieves best scores in safety and stability metrics (TTC, Collision avoidance) while maintaining reasonable progress
- Demonstrates effective handling of long-tail scenarios including red-light violations and unsafe lateral maneuvers
- Shows progressive improvement across continual learning iterations with diminishing returns after 5-10 cycles

## Why This Works (Mechanism)

### Mechanism 1: Sparse-Signal Densification via Safety-Aware Augmentation
Direct fine-tuning on sparse disengagement logs fails due to data sparsity; safety-aware augmentation expands the effective training set by generating positive variants (safe ego micro-perturbations) and negative variants (collision-inducing perturbations). This creates dense gradient signals from single failure events, preventing overfitting while maintaining physical safety boundaries.

### Mechanism 2: Representation Sharpening via Triplet Contrastive Learning
Contrastive loss structures the latent space to widen margins between recoverable and failure states. The model processes anchor (disengagement), positive (safe variant), and negative (unsafe variant) through a shared Transformer encoder, pulling positive embeddings toward anchors while pushing negative embeddings away. This forces the encoder to learn explicit safety features rather than just mimicking motion.

### Mechanism 3: Closed-Loop Continual Refinement
The "Deploy → Collect → Augment → Update" cycle treats deployment as data collection, enabling iterative reduction of failure rates at geographic hotspots. The joint objective ($L_{BC} + L_{CL} + L_{stab}$) ensures stability constraints prevent catastrophic forgetting while adapting to specific corner cases.

## Foundational Learning

- **Imitation Learning (IL) & Behavior Cloning**: The base policy is initialized via IL (PLUTO framework). Understanding the BC loss ($L_{BC}$) is essential for grasping what the system preserves while adding contrastive signals. Quick check: Can you explain why pure IL fails to generalize to out-of-distribution disengagement cases (the "covariate shift" problem)?

- **Contrastive Learning (Triplet Loss)**: This is the core improvement engine. Understanding anchor, positive, and negative sample selection is critical for debugging the augmentation pipeline. Quick check: Why is the "negative" sample (unsafe trajectory) not used for behavioral cloning ($L_{BC}$), but only for contrastive loss ($L_{CL}$)?

- **Continual Learning & Stability**: The system updates a deployed model, requiring understanding of the plasticity-stability trade-off. Quick check: What metric or mechanism prevents the updated policy from degrading performance on nominal routes?

## Architecture Onboarding

- **Component map**: Edge Node (Bus) with sensors → Perception → Planner ($\pi_t$) → Control → Trigger (30 cruise + 20 off frames) → Extracts Segment $S$ → Cloud Pipeline → Augmentation Engine (generates $D_{pos}$, $D_{neg}$) → Trainer (joint loss $L_{BC} + L_{CL} + L_{stab}$) → Validator → Updated policy $\pi_{t+1}$ → Fleet deployment

- **Critical path**: The definition of the "Safety Checker" in the Augmentation Engine. If this logic allows physically impossible or unsafe "positive" samples, the entire representation learning collapses.

- **Design tradeoffs**: Safety vs. Comfort shows a slight decrease in comfort scores (58.70 vs 60.88) for significantly better safety/TTC performance (85.67 vs 60.04), prioritizing collision avoidance over ride smoothness. The augmentation is synthetic, based on real logs but simulating risky scenarios.

- **Failure signatures**: Catastrophic Forgetting (fixes specific intersection but fails on highways), Augmentation Leakage (learns to detect augmented artifacts rather than safety semantics), Conservative Stalling (becomes too afraid to move due to excessive negative samples).

- **First 3 experiments**: 1) Baseline Reproduction: Run "Direct IL" baseline on 756 disengagement cases to verify overfitting behavior. 2) Latent Space Visualization: Verify safe vs. unsafe behavior clusters separate distinctly before/after training. 3) Ablation on Augmentation: Disable lead-agent insertion and measure TTC/Progress metric drops.

## Open Questions the Paper Calls Out

- How can adaptive weighting strategies for sample importance be integrated into the DTCCL framework to improve learning efficiency?
- How can contrastive learning be extended to explicitly model multi-agent interactions in dense traffic?
- Can real-time online adaptation be achieved while maintaining safety guarantees and avoiding catastrophic forgetting?
- How should the safety-comfort trade-off be systematically optimized for autonomous public transport?

## Limitations

- The specific formulation of the stability loss $L_{stab}$ is not provided, making it unclear how strongly the method prevents catastrophic forgetting
- Exact hyperparameters for contrastive loss (temperature $\tau$) and augmentation perturbation bounds are unspecified
- Claims about long-tail scenario handling are demonstrated only in case studies, not statistically validated across diverse geographic conditions

## Confidence

- **High**: The framework's use of disengagement events as high-value supervision signals is technically sound and well-supported by literature
- **Medium**: The 48.6% improvement over direct retraining is plausible given ablation results, but exact hyperparameter sensitivity is unknown
- **Low**: Long-tail scenario handling claims lack statistical validation across diverse geographic conditions

## Next Checks

1. **Latent Space Validation**: Visualize embeddings of Anchor, Positive, and Negative samples before/after training to confirm that safety vs. unsafe clusters separate distinctly

2. **Ablation on Augmentation Strategy**: Disable the lead-agent insertion (negative generation) and measure the drop in TTC/Progress metrics to quantify the value of unsafe scenario simulations

3. **Generalization Stress Test**: After DTCCL training, evaluate the policy on a held-out set of nominal (non-disengagement) nuPlan scenarios to detect catastrophic forgetting or degradation of basic driving skills