---
ver: rpa2
title: 'PI-MFM: Physics-informed multimodal foundation model for solving partial differential
  equations'
arxiv_id: '2512.23056'
source_url: https://arxiv.org/abs/2512.23056
tags:
- training
- error
- physics-informed
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Physics-informed multi-operator learning is challenging because
  existing approaches are data-hungry and neglect governing equations during training.
  This work introduces PI-MFM, a framework that enforces PDE residuals directly during
  pretraining and adaptation.
---

# PI-MFM: Physics-informed multimodal foundation model for solving partial differential equations

## Quick Facts
- **arXiv ID**: 2512.23056
- **Source URL**: https://arxiv.org/abs/2512.23056
- **Reference count**: 40
- **Primary result**: PI-MFM consistently outperforms purely data-driven models on 13 parametric 1D time-dependent PDEs, especially with sparse data or partial temporal domains.

## Executive Summary
PI-MFM addresses the challenge of physics-informed multi-operator learning by enforcing PDE residuals directly during pretraining and adaptation. The framework takes symbolic PDE representations as input, automatically constructs derivative-based loss terms via vectorized computation, and works with any PDE-encoding multimodal foundation model. It achieves superior data efficiency and generalization compared to purely data-driven approaches, particularly when labeled data is sparse or noisy. The method also demonstrates zero-shot transfer capabilities, achieving ~1% error on unseen PDEs using only physics residuals for fine-tuning.

## Method Summary
PI-MFM is a physics-informed multimodal foundation model that learns to solve parametric 1D time-dependent PDEs. It uses a PROSE backbone with data encoder, symbol encoder, fusion module, and cross-attention decoder. The framework takes initial conditions and symbolic PDE expressions (in Polish notation) as input, outputs spatiotemporal solutions, and enforces physics constraints via automatic derivative computation and PDE residual losses. Training combines data fidelity with physics losses, and the model supports both automatic differentiation and finite difference methods for derivative computation.

## Key Results
- Consistently outperforms purely data-driven counterparts, especially with sparse labeled spatiotemporal points
- Achieves ~1% error on unseen PDE families using zero-shot physics-informed fine-tuning
- Resampling collocation points at each iteration improves accuracy and reduces outliers
- Physics losses improve robustness to label noise and partial temporal domain training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Enforcing PDE residuals during training improves data efficiency and generalization compared to purely data-driven approaches.
- **Mechanism:** The PDE residual loss $L_{PDE}(\theta) = \mathbb{E}_{s,u_0}[\|F(G_\theta(u_0, s); s)\|^2]$ constrains the solution space to physically admissible functions. This acts as a regularizer when labeled data is sparse or noisy, reducing overfitting by requiring solutions to satisfy governing equations explicitly.
- **Core assumption:** The symbolic PDE expression correctly encodes the governing physics; the neural network can represent solutions that simultaneously satisfy both data fidelity and PDE residuals.
- **Evidence anchors:**
  - [abstract] "PI-MFM...directly enforces governing equations during pretraining and adaptation...consistently outperforms purely data-driven counterparts, especially with sparse labeled spatiotemporal points"
  - [Section 4.1.1] "At lower resolutions, the model effectively leverages the underlying physical laws to compensate for the scarcity of labeled data"
  - [corpus] Related work (gST-PINN, RAMS) confirms physics-informed constraints improve convergence and accuracy, though these are single-operator settings.

### Mechanism 2
- **Claim:** Forward-mode automatic differentiation is more efficient than reverse-mode for computing coordinate derivatives in multi-operator learning with shared query points.
- **Mechanism:** The Jacobian of outputs with respect to coordinates $(t, x)$ has block-diagonal structure when the forward pass is pointwise in query index $m$ (no cross-$m$ coupling). Forward-mode AD computes all $B \times M$ derivatives in $O(\text{derivatives})$ sweeps, while reverse-mode requires $B \times M$ VJPs. Layer normalization preserves this structure; batch normalization breaks it.
- **Core assumption:** The network architecture maintains pointwise independence across query coordinates (no attention or aggregation across $m$).
- **Evidence anchors:**
  - [Section 3.3.1] "The block-diagonal structure is the key reason forward-mode AD can be applied in a fully vectorized manner"
  - [Section 3.3.1] "Extracting all BM per-output coordinate derivatives via reverse mode would require BM such one-hot cotangents"
  - [corpus] No direct corpus comparison of AD modes for foundation models; evidence is primarily theoretical within the paper.

### Mechanism 3
- **Claim:** Resampling collocation points at each iteration improves accuracy and reduces outliers compared to fixed collocation sets.
- **Mechanism:** Dynamic resampling enforces PDE residuals across the full spatiotemporal domain over training, preventing overfitting to local residual minima. Fixed points can leave regions undersupervised, causing solution degradation elsewhere.
- **Core assumption:** The PDE residual is informative across the domain; resampling doesn't introduce harmful variance.
- **Evidence anchors:**
  - [Section 4.3] "Resampling shifts the error distributions downward and, importantly, reduces their spread across most families"
  - [Section 4.3] "For any given number of collocation points, randomly resampling these points at each step of training yields a substantially more accurate solution"
  - [corpus] RAMS paper similarly uses adaptive/residual-based sampling for PINNs, supporting the general principle.

## Foundational Learning

- **Concept: Polish (prefix) notation for symbolic PDE encoding**
  - **Why needed here:** The framework requires PDEs as symbolic token sequences to automatically parse expression trees and compute derivatives. Understanding how $u_t + 0.5u_x = 0$ becomes `[add, u_t, mul, N514, E-3, u_x]` is essential for debugging loss assembly.
  - **Quick check question:** Can you trace how the advection equation's symbolic tokens map to an expression tree?

- **Concept: Physics-informed loss decomposition (PDE, IC, IC', data)**
  - **Why needed here:** Training combines four loss terms with weights $\omega_{PDE}, \omega_{IC}, \omega_{IC'}, \omega_{data}$. Understanding their roles helps diagnose which constraint is failing when convergence stalls.
  - **Quick check question:** For a second-order-in-time PDE (e.g., Wave equation), which additional loss term is required compared to first-order PDEs?

- **Concept: Block-diagonal Jacobian structure in operator networks**
  - **Why needed here:** This structure enables efficient forward-mode AD. Operations that aggregate across query points (attention, batch norm) break it. Essential for architectural choices.
  - **Quick check question:** Why does layer normalization preserve the Jacobian structure while batch normalization does not?

## Architecture Onboarding

- **Component map:**
  - Data encoder -> Symbol encoder -> Fusion module -> Decoder
  - Derivative backend -> Loss assembler -> Physics loss

- **Critical path:**
  1. Symbolic PDE $\rightarrow$ Polish tokens $\rightarrow$ expression tree (parse once per PDE family)
  2. Forward pass on collocation points $\rightarrow$ solution + derivatives
  3. Residual evaluation via tree recursion $\rightarrow$ $L_{PDE}$
  4. Backprop through combined losses

- **Design tradeoffs:**
  - **FDM float32 vs. Forward AD float16:** FDM is ~2-4x faster and uses ~50% less memory but requires step-size tuning ($\Delta x \in [0.001, 0.01]$ for float32). AD is tuning-free but costlier.
  - **Resample vs. fixed collocation:** Resampling improves accuracy but adds per-iteration overhead; fixed points are faster but risk domain coverage gaps.
  - **Collocation count:** More points reduce error but scale compute; 500 points per PDE is a reasonable starting point.

- **Failure signatures:**
  - **Exploding gradients with physics loss:** Often indicates incorrect derivative computation or mismatched PDE encoding
  - **Plateauing error with data loss only:** Physics loss may be underweighted or collocation strategy insufficient
  - **H1 error >> L2 error:** Derivative accuracy degraded—check FDM step size or AD precision

- **First 3 experiments:**
  1. **Validate derivative backend:** Compare AD vs. FDM on a single PDE family (e.g., Advection) with dense data; verify L2 and H1 errors match within 0.1%
  2. **Ablate physics loss:** Train with and without $L_{PDE}$ on sparse data (e.g., 4×16 grid); confirm physics-informed model achieves lower error
  3. **Test zero-shot transfer:** Pretrain on 10 PDE families, fine-tune on unseen family (e.g., Burgers) using only PDE+IC losses; verify error drops to ~1% within 3200 iterations

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the PI-MFM framework maintain its data efficiency and computational tractability when scaling to complex 2D and 3D time-dependent PDEs?
- **Basis in paper:** [explicit] The authors state in the Conclusions, "we will scale PI-MFM to two- and three-dimensional, time-dependent PDEs," noting that current experiments are limited to 1D benchmarks.
- **Why unresolved:** The current vectorized loss computation and model architecture were tested on 1D domains; it is unclear if memory usage or training stability will degrade in higher-dimensional spaces due to the curse of dimensionality.
- **What evidence would resolve it:** Successful benchmarking of PI-MFM on standard 2D/3D datasets (e.g., Navier-Stokes) demonstrating comparable relative error reductions and data efficiency to the 1D results.

### Open Question 2
- **Question:** Do adaptive sampling strategies or gradient-enhanced losses significantly improve convergence speed and solution fidelity compared to the simple random resampling strategy currently used?
- **Basis in paper:** [explicit] The authors propose to "develop adaptive sampling strategies" and "investigate training loss enhancements such as gradient-enhanced PINNs" to accelerate convergence and alleviate the ill-conditioning of the PINN loss landscape.
- **Why unresolved:** Section 4.3 demonstrates that resampling collocation points improves accuracy, but the authors suggest that allocating budgets toward regions with large residuals (adaptive sampling) could yield further efficiency gains.
- **What evidence would resolve it:** A comparative study showing that adaptive or gradient-enhanced methods reduce training iterations or test error relative to the random resampling baseline established in the paper.

### Open Question 3
- **Question:** Can PI-MFM be effectively integrated with uncertainty quantification (UQ) methods to provide reliable confidence intervals during out-of-distribution extrapolation?
- **Basis in paper:** [explicit] The authors list "reliable extrapolation and uncertainty quantification" as a necessary step for the "more reliable deployment" of PDE-encoding foundation models.
- **Why unresolved:** While the model shows robustness to noise in Section 4.2, it currently lacks a mechanism to quantify prediction uncertainty, which is critical for safety in scientific applications.
- **What evidence would resolve it:** Implementation of a UQ technique (e.g., Bayesian inference or ensembling) that provides calibrated error bars for predictions on unseen PDE families or extrapolated time domains.

## Limitations
- All experiments are confined to 1D spatiotemporal domains; scalability to 2D/3D PDEs remains unproven
- Architecture specifics of the PROSE backbone are not fully detailed, requiring assumptions for reproduction
- Loss weight sensitivity analysis is not provided, leaving optimal hyperparameter configurations unclear

## Confidence
- **High confidence:** Data efficiency improvements with physics losses, resampling collocation points, and zero-shot transfer performance
- **Medium confidence:** Forward-mode AD efficiency claims (theoretically sound but lacks direct comparative benchmarks)
- **Low confidence:** FDM vs. AD precision trade-offs (theoretical analysis without comprehensive empirical validation)

## Next Checks
1. **Architecture sensitivity analysis:** Systematically vary embedding dimensions, attention layers, and hidden sizes to determine which PROSE components most impact physics-informed performance versus data-only baselines.

2. **Loss weight optimization:** Conduct ablation studies on ω_PDE, ω_IC, ω_IC', and ω_data to identify optimal weight configurations for different PDE families and data sparsity levels.

3. **Cross-dimensional scalability:** Test PI-MFM on 2D Burgers' equation and 2D wave equation to evaluate whether the physics-informed approach maintains its data efficiency advantages in higher-dimensional settings.