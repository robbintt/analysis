---
ver: rpa2
title: AI for pRedicting Exacerbations in KIDs with aSthma (AIRE-KIDS)
arxiv_id: '2511.01018'
source_url: https://arxiv.org/abs/2511.01018
tags:
- asthma
- visit
- index
- data
- patients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed and validated the AIRE-KIDS models to predict
  repeat asthma exacerbations in children with a prior emergency department (ED) visit.
  Using retrospective Epic EMR data linked with environmental and neighborhood data,
  machine learning models (boosted trees and large language models) were trained and
  validated on two cohorts (pre- and post-COVID).
---

# AI for pRedicting Exacerbations in KIDs with aSthma (AIRE-KIDS)

## Quick Facts
- arXiv ID: 2511.01018
- Source URL: https://arxiv.org/abs/2511.01018
- Reference count: 0
- Primary result: LGBM model achieved AUC 0.712, F1 0.51 for predicting repeat pediatric asthma ED visits

## Executive Summary
This study developed and validated the AIRE-KIDS models to predict repeat asthma exacerbations in children with prior emergency department (ED) visits. Using retrospective Epic EMR data linked with environmental and neighborhood data, machine learning models (boosted trees and large language models) were trained and validated on two cohorts (pre- and post-COVID). The best-performing LGBM model achieved an AUC of 0.712 and F1 score of 0.51 for predicting repeat asthma-related ED visits, significantly outperforming current clinical decision rules. Key predictors included prior asthma ED visits, medical complexity, food allergy, and triage acuity. These models offer a practical tool to identify high-risk children for targeted asthma management from the ED.

## Method Summary
The AIRE-KIDS models were developed using retrospective Epic EMR data from Children's Hospital of Eastern Ontario. The training cohort included 2,716 children with asthma-related ED visits (Feb 2017–Feb 2019), while validation used 1,237 children from a post-COVID period (Jul 2022–Apr 2023). The task was binary classification to predict repeat asthma-related ED visits or hospitalizations within 1 year of an index asthma ED visit. Models included LightGBM gradient boosting and fine-tuned large language models. Features spanned demographics, prior healthcare utilization, clinical acuity, allergies, environmental exposures, and neighborhood marginalization. Parsimonious models using 5-6 key features were developed based on SHAP importance analysis. Thresholds were optimized to maximize F1 score for resource-constrained clinical referral decisions.

## Key Results
- LGBM model achieved AUC 0.712 and F1 score 0.51 on validation data
- Model significantly outperformed current clinical decision rules (F1 0.334)
- Key predictors included prior asthma ED visits, medical complexity, food allergy, and triage acuity
- Environmental and neighborhood features showed minimal contribution and were excluded from final models
- Model maintained performance across pre- and post-COVID cohorts with modest degradation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Tree-based gradient boosting (LGBM) outperforms LLMs on structured clinical tabular data for this prediction task.
- **Mechanism:** LGBM iteratively learns decision splits on tabular features (e.g., prior ED visits, CTAS), directly capturing non-linear interactions without the information loss incurred by serializing structured data into text for LLM tokenization. LLMs, fine-tuned via LoRA with textual encoders, showed lower discrimination (AUC 0.53–0.60 vs 0.67 LGBM for ED visits on training data).
- **Core assumption:** Feature-value pair text encoding preserves discriminative signal comparably to native tabular representation.
- **Evidence anchors:**
  - [abstract] "The LGBM ML model performed best... despite the popularity of LLMs, the LGBM approach yielded the best performing models."
  - [section] Table 4 shows LGBM AUC 0.666 (ED) vs Llama1b 0.530; Table 5 shows LGBM F1 0.503 vs Llama1b 0.435 on validation.
  - [corpus] Neighbor papers on pediatric asthma detection use audio embeddings, not tabular-to-text LLM approaches; no corpus evidence directly supports tabular LLM superiority.
- **Break condition:** If feature dimensionality scales dramatically (hundreds to thousands of sparse features) or if longitudinal event sequences dominate predictive signal, RNN/LSTM architectures may outperform static tabular models.

### Mechanism 2
- **Claim:** Prior healthcare utilization and clinical acuity markers (not environmental/social variables) drive repeat exacerbation prediction.
- **Mechanism:** SHAP analysis ranked prior asthma ED visit, CTAS triage acuity, medical complexity, food allergy, and prior non-asthma respiratory ED visits as top contributors. Environmental (AQHI, NO2, PM2.5) and neighborhood marginalization features showed minimal contribution and were excluded from final parsimonious models.
- **Core assumption:** Individual-level clinical history captures more variance in exacerbation risk than contextual environmental exposures measured at postal-code resolution.
- **Evidence anchors:**
  - [abstract] "Key predictors included prior asthma ED visits, medical complexity, food allergy, and triage acuity."
  - [section] "Although environmental and social variables (e.g., air quality, marginalization index) were considered, they did not significantly contribute to model performance and were excluded from the final models."
  - [corpus] Weak direct evidence; no corpus neighbor analyzed environmental vs clinical feature importance for asthma prediction.
- **Break condition:** If environmental exposures are measured at individual-level (personal sensors) or at finer spatial-temporal granularity, their predictive contribution may increase substantially.

### Mechanism 3
- **Claim:** F1-optimized threshold selection enables clinically actionable risk stratification under resource constraints.
- **Mechanism:** The asthma program has limited referral capacity. Thresholds maximizing F1 (harmonic mean of precision/recall) were selected via 5-fold cross-validation (thresholds: 0.250 for ED, 0.170 for hospitalization) rather than optimizing AUC alone. This prioritizes identifying true high-risk patients for scarce intervention slots while avoiding over-referral.
- **Core assumption:** The cost of missed high-risk patients (false negatives) and over-referrals (false positives) is roughly balanced, justifying F1 optimization.
- **Core assumption:** Prevalence in training (~29%) approximates deployment prevalence.
- **Evidence anchors:**
  - [abstract] LGBM achieved "F1 score of 0.51... significantly outperforming current clinical decision rules" (F1=0.334).
  - [section] "Capacity within the CHEO asthma program is resource-limited... the F1 score was deemed to be the most appropriate evaluation metric."
  - [corpus] No corpus neighbor addresses threshold optimization for constrained clinical resources.
- **Break condition:** If prevalence drops substantially (e.g., <10%) or if false negative costs far exceed false positive costs, F1 optimization may yield suboptimal thresholds; recall-oriented metrics (e.g., F2) may be preferred.

## Foundational Learning

- **Concept: Gradient Boosting Decision Trees (LGBM)**
  - **Why needed here:** LGBM iteratively fits residuals from prior trees, capturing non-linear interactions in clinical data without feature scaling. Understanding leaf-wise growth vs level-wise growth informs hyperparameter tuning.
  - **Quick check question:** Given 68 features and ~2,700 training samples, would increasing `num_leaves` from 4–5 (final model values) likely improve or overfit validation performance?

- **Concept: Beta Calibration for Probability Calibration**
  - **Why needed here:** Boosted trees output scores on arbitrary scales, not calibrated probabilities. Beta calibration maps scores to well-calibrated probabilities, critical for threshold-based clinical decisions.
  - **Quick check question:** If a model outputs uncalibrated score 0.8 that calibrates to probability 0.45, what threshold setting error would occur without calibration?

- **Concept: SHAP (Shapley Additive Explanations)**
  - **Why needed here:** SHAP values quantify each feature's contribution to individual predictions, enabling parsimonious model selection (6 features for ED model) and clinical interpretability.
  - **Quick check question:** If "prior asthma ED visit" has a mean absolute SHAP value of 0.12 and "air quality" has 0.01, which should be retained in a 6-feature model?

## Architecture Onboarding

- **Component map:** Epic EMR → structured features (demographics, visits, labs, allergies, operational) + external joins (AQHI, ON-Marg via postal code) → 68 features → missing indicator encoding for booleans, categorical encoding, temporal aggregation (1-year lookback) → LGBM with Bayesian hyperparameter tuning (5-fold CV), beta calibration post-hoc → threshold optimization (F1-max), external validation on post-COVID cohort → SHAP for feature importance ranking → parsimonious model re-training

- **Critical path:**
  1. Define index visit (first asthma ED in accrual period)
  2. Extract 1-year lookback features (prior visits, comorbidities, labs)
  3. Link environmental/marginalization via postal code
  4. Train LGBM with nested CV, calibrate probabilities
  5. Select threshold maximizing F1 on training folds
  6. Validate on temporally distinct cohort (post-COVID)

- **Design tradeoffs:**
  - Parsimony vs performance: Final models use 5–6 features (dropping 62+) for deployment ease; AUC dropped modestly (0.666→0.712 for ED, paradoxically improved via better generalization).
  - F1 vs AUC: F1 prioritized for resource-limited referral decisions; AUC reported for literature comparability.
  - LGBM vs LLM: LGBM chosen for interpretability, local inference, and superior performance; LLMs tested but underperformed.

- **Failure signatures:**
  - Environmental features consistently low SHAP importance → suggests spatial resolution mismatch or exposure measurement error
  - LLM AUC near 0.50 for hospitalization (Llama1b) → serialization may lose discriminative structure
  - High missingness (27–44%) on environmental/marginalization features for Quebec patients (excluded from ON-Marg linkage)

- **First 3 experiments:**
  1. **Temporal stability test:** Re-train LGBM on post-COVID data alone; compare SHAP rankings to pre-COVID to assess feature drift.
  2. **Environmental resolution ablation:** Aggregate environmental features at finer temporal windows (e.g., 6-hour intervals) and test predictive gain.
  3. **Threshold sensitivity analysis:** Plot precision-recall curve across thresholds; evaluate F2 and F0.5 scores to understand recall-weighted vs precision-weighted operating points.

## Open Questions the Paper Calls Out
- **Open Question 1:** Can the AIRE-KIDS models be generalized to other pediatric centers without the need for extensive retraining? (Basis: Explicit statement in Limitations about need for additional validation)
- **Open Question 2:** How does the AIRE-KIDS model perform when deployed prospectively as a real-time clinical decision support system? (Basis: Conclusion identifies local deployment and prospective evaluation as next steps)
- **Open Question 3:** Does the inclusion of missing behavioral data, specifically medication adherence or smoke exposure, significantly improve predictive performance? (Basis: Discussion notes these variables were missed but are clinically relevant)

## Limitations
- Single-center, provincial data (CHEO, Ontario) with strong temporal drift (3-5 point AUC drop pre- vs post-COVID)
- Missing data patterns differ substantially between cohorts (27-44% for environmental features in pre-COVID)
- Exclusion of Quebec patients from marginalization analysis introduces geographic bias
- 6-feature parsimonious models may underperform in populations with different clinical practice patterns

## Confidence
- **High confidence:** LGBM outperforms LLM for tabular clinical data (AUC 0.666-0.712 vs 0.530-0.600), clinical features dominate over environmental variables, F1-optimized thresholds enable resource-constrained referrals
- **Medium confidence:** Parsimonious models (6 features) maintain predictive performance; environmental features contribute negligibly when measured at postal-code resolution
- **Low confidence:** Threshold optimization balances false positive/negative costs appropriately for all clinical contexts; findings generalize beyond CHEO population

## Next Checks
1. Re-train models on post-COVID cohort alone and compare feature importance rankings to assess temporal stability and feature drift
2. Test environmental feature aggregation at finer spatial-temporal resolutions (e.g., 6-hour intervals) to determine if measurement granularity explains null findings
3. Conduct cost-effectiveness analysis comparing F1-optimized thresholds against recall-weighted (F2) and precision-weighted (F0.5) operating points for different clinical resource constraints