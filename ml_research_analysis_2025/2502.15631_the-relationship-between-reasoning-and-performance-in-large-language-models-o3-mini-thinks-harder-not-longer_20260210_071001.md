---
ver: rpa2
title: The Relationship Between Reasoning and Performance in Large Language Models
  -- o3 (mini) Thinks Harder, Not Longer
arxiv_id: '2502.15631'
source_url: https://arxiv.org/abs/2502.15631
tags:
- reasoning
- o3-mini
- arxiv
- accuracy
- difficulty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether higher-performing reasoning models
  generate longer reasoning chains to achieve better accuracy. By analyzing OpenAI's
  o1-mini, o3-mini (m), and o3-mini (h) on the Omni-MATH benchmark, we find that o3-mini
  (m) achieves superior accuracy without using longer reasoning chains than o1-mini,
  suggesting more effective reasoning.
---

# The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer

## Quick Facts
- arXiv ID: 2502.15631
- Source URL: https://arxiv.org/abs/2502.15631
- Authors: Marthe Ballon; Andres Algaba; Vincent Ginis
- Reference count: 40
- Primary result: Higher-performing models achieve better accuracy through more effective reasoning per token, not longer reasoning chains.

## Executive Summary
This study investigates whether improved reasoning model performance stems from longer chain-of-thought or more efficient reasoning. By analyzing OpenAI's o1-mini, o3-mini (m), and o3-mini (h) on the Omni-MATH benchmark, we find that o3-mini (m) achieves superior accuracy without using longer reasoning chains than o1-mini, suggesting more effective reasoning. Across all models and compute settings, accuracy generally declines as reasoning chain length increases, even after controlling for question difficulty. This accuracy drop is significantly smaller in more proficient models, indicating they use test-time compute more effectively.

## Method Summary
The study evaluates four models (gpt-4o, o1-mini, o3-mini m, o3-mini h) on the Omni-MATH benchmark (4,428 Olympiad-level math problems) via OpenAI Batch API. Reasoning tokens are extracted from API responses, and correctness is evaluated using Omni-Judge. Accuracy is analyzed by token bins and stratified by difficulty/domain, with logistic regression controlling for difficulty and domain fixed effects to estimate token-accuracy relationships. The analysis compares token distributions, conditional error rates, and average marginal effects across models and compute settings.

## Key Results
- o3-mini (m) achieves superior accuracy without using longer reasoning chains than o1-mini, suggesting more effective reasoning
- Accuracy generally declines as reasoning chain length increases across all models and compute settings, even when controlling for difficulty
- o3-mini (h) achieves only marginal accuracy gains over o3-mini (m) by allocating substantially more reasoning tokens across all problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Higher-performing reasoning models achieve better accuracy through more effective reasoning per token, not longer reasoning chains.
- Mechanism: Improved internal representations or training procedures enable newer model generations (o3-mini vs. o1-mini) to extract more utility from each reasoning token, achieving superior accuracy with statistically identical token distributions for correctly-solved problems.
- Core assumption: The observed efficiency gains transfer across tasks beyond mathematical reasoning benchmarks.
- Evidence anchors:
  - [abstract] "o3-mini (m) achieves superior accuracy without requiring longer reasoning chains than o1-mini"
  - [Results] "the almost identical token distributions show that o3-mini (m) does not use more reasoning tokens to achieve its superior performance to o1-mini"
  - [corpus] Related benchmarking studies evaluate reasoning-focused models across specialized domains but do not directly test the token-efficiency hypothesis.
- Break condition: If future models require proportionally more tokens to achieve accuracy gains, the "thinks harder" mechanism would weaken; this study's cross-generational comparison (o1→o3-mini) shows the opposite trend.

### Mechanism 2
- Claim: Accuracy decreases as reasoning chain length increases, even after controlling for question difficulty.
- Mechanism: Models that require more tokens for a given problem difficulty tier are more likely to be struggling with that problem; extended reasoning chains may accumulate compounding errors or indicate the model has entered an unproductive reasoning path.
- Core assumption: The correlation reflects a causal relationship between reasoning struggle and token usage, rather than an artifact of the benchmark.
- Evidence anchors:
  - [abstract] "accuracy generally declines as reasoning chains grow across all models and compute settings, even when controlling for difficulty of the questions"
  - [Results] "The accuracy decrease per 1000 reasoning tokens is 3.16% for o1-mini, 1.96% for o3-mini (m), and 0.81% for o3-mini (h)"
  - [Results] "stratifying by tier level shows that, within difficulty tiers, accuracy also decreases with higher reasoning token usage"
  - [corpus] No corpus papers directly test this inverse relationship between chain length and accuracy.
- Break condition: If longer chains improve accuracy for high-difficulty problems in future models, the negative correlation would invert for specific problem classes.

### Mechanism 3
- Claim: Higher compute settings (reasoning_effort=high) allocate tokens uniformly across all problems rather than selectively to difficult ones.
- Mechanism: o3-mini (h) uses roughly 2x tokens compared to o3-mini (m) across all questions, including problems o3-mini (m) already solves correctly, yielding only ~4% accuracy gain at substantially higher cost.
- Core assumption: The uniform token scaling is a property of the reasoning_effort parameter rather than an adaptive allocation mechanism.
- Evidence anchors:
  - [abstract] "o3-mini (h) achieves a marginal accuracy gain over o3-mini (m) by allocating substantially more reasoning tokens across all problems, even the ones that o3-mini (m) can already solve"
  - [Results] "the token distribution of o3-mini (h) is stretched linearly with respect to the one of o3-mini (m)"
  - [corpus] Corpus papers compare model performance but do not analyze token allocation patterns across compute settings.
- Break condition: If adaptive compute allocation were implemented (varying tokens by problem-specific signals), this mechanism would change significantly.

## Foundational Learning

- Concept: Test-time compute scaling
  - Why needed here: The paper's central question is whether improved performance comes from more test-time compute (longer chains) or better reasoning quality.
  - Quick check question: Can you explain why test-time compute scaling differs from training-time compute scaling?

- Concept: Chain-of-thought reasoning tokens
  - Why needed here: The study measures reasoning tokens specifically, distinct from completion tokens, to isolate the thinking process.
  - Quick check question: How do reasoning tokens differ from output tokens in the models studied?

- Concept: Logistic regression with fixed effects
  - Why needed here: The paper uses controlled regression to isolate token effects from difficulty and domain confounders.
  - Quick check question: Why use fixed effects for difficulty tiers rather than including difficulty as a continuous variable?

## Architecture Onboarding

- Component map:
  - Omni-MATH problems -> OpenAI Batch API (gpt-4o, o1-mini, o3-mini m, o3-mini h) -> API responses (reasoning tokens) -> Omni-Judge evaluation -> Accuracy metrics

- Critical path:
  1. Query models via Batch API with standardized prompt
  2. Extract reasoning token counts from API response
  3. Evaluate correctness using Omni-Judge
  4. Bin responses by token count, stratify by difficulty/domain
  5. Fit logistic regression with fixed effects to estimate token-accuracy relationship

- Design tradeoffs:
  - Token limits: 25K for o1-mini/o3-mini (m), 100K for o3-mini (h); constrains maximum reasoning depth
  - Automated evaluation: Omni-Judge achieves 91.78% consistency with gpt-4o-as-judge but may diverge from human evaluation
  - Benchmark scope: Olympiad-level math may not generalize to other reasoning domains

- Failure signatures:
  - Very high token counts (>30K) correlate with >50% error probability across all models
  - Discrete Mathematics domain shows anomalous performance patterns across all models
  - gpt-4o shows inverted difficulty-performance relationship (performs better on Tier 4 than Tiers 2-3)

- First 3 experiments:
  1. Replicate the token distribution comparison between o1-mini and o3-mini (m) on a held-out subset of Omni-MATH to verify the "same length, better accuracy" finding.
  2. Test whether constraining max_completion_tokens improves o1-mini accuracy more than o3-mini (m) accuracy, as the Discussion section suggests weaker models benefit more from chain length constraints.
  3. Implement early-exit token thresholds based on the conditional error rate curves (e.g., halt if tokens exceed the 50% error-rate threshold for the given model) and measure accuracy/cost tradeoffs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the causal mechanism behind the accuracy decline as reasoning chains lengthen—do models allocate more tokens to inherently unsolvable problems, or do longer chains themselves introduce higher error probability?
- Basis in paper: [explicit] "A possible hypothesis for this accuracy drop is that models tend to reason more on problems they cannot solve. Another possibility is that longer reasoning chains inherently have a higher probability of leading to a wrong final solution, highlighting the need for mathematical benchmarks with reference reasoning templates."
- Why unresolved: The observational analysis cannot disentangle whether token usage is a signal of problem difficulty or a cause of errors.
- What evidence would resolve it: Benchmarks with gold-standard reasoning templates enabling comparison of model reasoning paths to correct paths at matched lengths; controlled experiments forcing models to generate specific token counts on the same problems.

### Open Question 2
- Question: How do alternative prompting strategies interact with test-time compute scaling in reasoning models?
- Basis in paper: [explicit] "Our prompting strategy employed here may not generalize to alternative approaches or more constrained prompt settings, and their interaction with test-time compute warrants further investigation. Many studies on prompting and reasoning were conducted on LLMs without test-time compute, so the broader implications for the latest generation of reasoning models remain to be fully understood."
- Why unresolved: Prior prompting research targeted models without test-time compute; systematic evaluation across prompting methods for o1/o3-style models is lacking.
- What evidence would resolve it: Controlled experiments comparing zero-shot, few-shot, chain-of-thought, and constrained prompting across reasoning models with matched compute budgets.

### Open Question 3
- Question: Does Omni-Judge evaluation validity extend to o3-mini models without data leakage or systematic bias?
- Basis in paper: [explicit] "Omni-Judge has only been validated for data leakage checks on o1-mini; extending these checks to o3-mini remains future work, though we assume minimal overlap."
- Why unresolved: The automated judge's consistency with human evaluation has not been verified specifically for o3-mini outputs.
- What evidence would resolve it: Human evaluation of a sample of o3-mini responses compared against Omni-Judge judgments; leakage detection analysis on o3-mini training data.

### Open Question 4
- Question: Do the observed relationships between reasoning length and accuracy generalize beyond mathematical reasoning to other domains?
- Basis in paper: [explicit] "Future evaluations may consider additional benchmarks that incorporate broader language understanding or real-world reasoning challenges, though the current focus remains on math and coding due to the relative ease of implementing objective reward models."
- Why unresolved: Mathematical reasoning has uniquely verifiable answers; domains with ambiguous or multi-step real-world reasoning may exhibit different token-accuracy dynamics.
- What evidence would resolve it: Replication of the analysis on coding benchmarks, multi-hop reasoning tasks, and open-ended reasoning domains with established evaluation protocols.

## Limitations
- Domain Specificity: Findings are based exclusively on Olympiad-level mathematics problems and may not generalize to other reasoning domains.
- Evaluation Methodology: Omni-Judge achieves 91.78% consistency with human evaluation but the 8.22% discrepancy represents meaningful measurement error.
- Token Count Limitations: Fixed maximum token limits (25K for o1-mini/o3-mini m, 100K for o3-mini h) may artificially constrain the upper bound of reasoning chains.

## Confidence
- High Confidence: The core finding that o3-mini achieves superior accuracy without using longer reasoning chains than o1-mini is well-supported by the data.
- Medium Confidence: The claim that accuracy generally declines as reasoning chain length increases is supported but requires careful interpretation due to potential confounding factors.
- Medium Confidence: The observation that o3-mini h allocates tokens uniformly across all problems rather than selectively is well-documented but the "inefficient" interpretation depends on cost-benefit analysis.

## Next Checks
1. **Cross-Domain Replication**: Replicate the token distribution and accuracy analysis on a non-mathematical reasoning benchmark (e.g., logical reasoning or code generation) to test whether the "thinks harder, not longer" efficiency pattern holds across different reasoning domains.

2. **Adaptive Compute Evaluation**: Implement an adaptive compute allocation strategy that varies max_completion_tokens based on problem difficulty predictions, then compare accuracy-cost tradeoffs against the uniform scaling observed in o3-mini h.

3. **Human Evaluation Validation**: Select a stratified sample of problems across the full token range and have human experts evaluate both the correctness and quality of reasoning chains, comparing these judgments against Omni-Judge scores to quantify potential systematic biases in automated evaluation.