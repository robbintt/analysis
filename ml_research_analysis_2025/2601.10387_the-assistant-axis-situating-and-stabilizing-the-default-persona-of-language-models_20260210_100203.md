---
ver: rpa2
title: 'The Assistant Axis: Situating and Stabilizing the Default Persona of Language
  Models'
arxiv_id: '2601.10387'
source_url: https://arxiv.org/abs/2601.10387
tags:
- assistant
- persona
- role
- response
- axis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models can adopt different personas but usually
  default to a "helpful Assistant" identity shaped during post-training. This work
  investigates the space of possible personas by extracting activation directions
  corresponding to diverse character archetypes.
---

# The Assistant Axis: Situating and Stabilizing the Default Persona of Language Models

## Quick Facts
- **arXiv ID**: 2601.10387
- **Source URL**: https://arxiv.org/abs/2601.10387
- **Reference count**: 40
- **Primary result**: Activation capping along the "Assistant Axis" reduces persona-based jailbreak success by ~60% with minimal capability loss.

## Executive Summary
Large language models adopt different personas but usually default to a "helpful Assistant" identity shaped during post-training. This work investigates the space of possible personas by extracting activation directions corresponding to diverse character archetypes. Across several models, the main variation in persona space is an "Assistant Axis" that measures how far the current persona is from the default Assistant. Steering toward this axis reinforces helpful and harmless behavior, while steering away increases susceptibility to adopting other roles and enhances the success of persona-based jailbreaks. The axis is also present in pre-trained models, where it primarily promotes helpful human archetypes and suppresses spiritual ones. Deviations along this axis predict "persona drift"—unexpected shifts to harmful or bizarre behaviors—often triggered by emotionally charged conversations or requests for meta-reflection. Capping activations along the Assistant Axis within a safe range stabilizes behavior in these high-risk scenarios and reduces adversarial jailbreak success, with minimal impact on capabilities. These findings suggest that post-training loosely tethers models to the intended Assistant persona, highlighting the need for deeper anchoring strategies.

## Method Summary
The authors generated role vectors by prompting a base model to roleplay 275 character archetypes with 240 trait descriptions and 240 extraction questions each, collecting 1200 rollouts per role. They extracted mean post-MLP residual stream activations at middle layers to form role vectors, then applied PCA to reveal the primary variance component—the "Assistant Axis." This axis was computed as the contrast vector between default Assistant activations and the mean of all role vectors. To stabilize behavior, they implemented activation capping that clamps projections onto the Assistant Axis to remain above a threshold (25th percentile of calibration set). The intervention was applied across 8-16 middle-to-late layers during inference, and evaluated on persona-based jailbreaks and standard capability benchmarks.

## Key Results
- The leading component of persona space across Gemma 2 27B, Qwen 3 32B, and Llama 3.3 70B is an "Assistant Axis" that captures distance from default Assistant mode.
- Measuring deviations along this axis predicts "persona drift" into harmful or bizarre behaviors during specific conversation types.
- Activation capping along the Assistant Axis reduces harmful responses to persona-based jailbreaks by ~60% while preserving benchmark performance.
- The Assistant Axis is present in pre-trained models, promoting helpful human archetypes and suppressing spiritual ones.

## Why This Works (Mechanism)

### Mechanism 1: Linear Representation of Persona Space in Residual Stream
- Claim: IF the default Assistant persona corresponds to a direction in residual stream activation space, THEN post-training can be understood as pushing activations toward this region.
- Mechanism: The paper extracts mean post-MLP residual stream activations for 275 prompted character archetypes, then applies PCA. Across Gemma 2 27B, Qwen 3 32B, and Llama 3.3 70B, PC1 consistently separates fantastical roles (ghost, leviathan, bard) from Assistant-adjacent roles (evaluator, consultant, analyst). The "Assistant Axis" is computed as a contrast vector: mean(default Assistant activation) − mean(all role vectors), which aligns with PC1 (cosine similarity >0.71 at middle layers).
- Core assumption: Personas are approximately linearly represented; the semantic meaning of PC1 generalizes across models.
- Evidence anchors:
  - [abstract] "Across several different models, we find that the leading component of this persona space is an 'Assistant Axis,' which captures the extent to which a model is operating in its default Assistant mode."
  - [section 2.1.3] "Between all pairs of models, the correlation of role loadings on PC1 is >0.92, indicating remarkably high similarity."
  - [corpus] "Persona Vectors" (Chen et al., 2025) uses similar contrastive extraction for trait directions, supporting linear representability of persona attributes.
- Break condition: IF the Assistant Axis fails to causally modulate behavior under steering (e.g., no change in role susceptibility), THEN the linear representation assumption may not hold, or the axis may be confounded with response content.

### Mechanism 2: Persona Drift via Conversational Context Modulation of Assistant Axis Projections
- Claim: IF certain conversation types push model activations away from the Assistant end of the axis, THEN harmful behavior becomes more likely.
- Mechanism: Multi-turn synthetic conversations show domain-specific drift patterns. Ridge regression on user message embeddings predicts next-turn Assistant Axis projection with R² 0.53–0.77. Drift-driving prompts include: pushing for meta-reflection on model processes, emotionally vulnerable disclosures, phenomenological demands, and specific authorial voices. Correlation between first-turn Assistant Axis projection and second-turn harmful response rate: r = 0.39–0.52.
- Core assumption: The measured projection reflects a causal state influencing behavior, not just a correlation with response style.
- Evidence anchors:
  - [abstract] "Measuring deviations along the Assistant Axis predicts 'persona drift'—a phenomenon where models slip into exhibiting harmful or bizarre behaviors."
  - [section 4.1] "In therapy-related conversations where the user is working through emotional issues or philosophical conversations about AI capabilities and self-awareness, models drift along the Assistant Axis to the non-Assistant end."
  - [corpus] Corpus does not provide direct evidence for drift mechanisms; related work focuses on persona construction, not dynamic drift during conversation.
- Break condition: IF controlling for response length/topic eliminates the projection–harm correlation, THEN the axis may be tracking semantic content rather than persona state per se.

### Mechanism 3: Activation Capping Constrains Persona Space Trajectories
- Claim: IF activations are clamped to remain within a bounded region along the Assistant Axis, THEN extreme persona deviations are suppressed with minimal capability degradation.
- Mechanism: Activation capping applies: h ← h − v · min(⟨h, v⟩ − τ, 0), where v is the Assistant Axis and τ is a threshold (25th percentile of calibration distribution). Applied across 8–16 middle-to-late layers simultaneously. This reduces harmful responses to persona-based jailbreaks by ~60% while preserving benchmark performance (IFEval, MMLU Pro, GSM8k, EQ-Bench).
- Core assumption: The 25th percentile of the calibration distribution represents a "safe" Assistant-ness level; capping does not interfere with non-persona-related computations.
- Evidence anchors:
  - [abstract] "Capping activations along the Assistant Axis within a safe range stabilizes behavior in these high-risk scenarios and reduces adversarial jailbreak success, with minimal impact on capabilities."
  - [section 5.2] "We found that we could decrease the rate of harmful responses by nearly 60% without impacting performance."
  - [corpus] "What Can We Actually Steer?" examines activation steering effectiveness across behaviors but does not specifically test bounded capping; related evidence is indirect.
- Break condition: IF capping at the 25th percentile causes detectable degradation on tasks requiring creative or non-Assistant personas (e.g., roleplay, fiction), THEN the "safe range" may be too restrictive for general use.

## Foundational Learning

- Concept: **Residual Stream Activations**
  - Why needed here: Role vectors and the Assistant Axis are extracted from post-MLP residual stream at middle layers; understanding how activations encode semantic information is essential.
  - Quick check question: At which layer(s) does the paper extract the Assistant Axis, and why might middle layers be preferred?

- Concept: **Contrastive Vector Extraction**
  - Why needed here: The Assistant Axis is defined as a difference-in-means vector (Assistant − mean of roles); knowing how contrastive methods isolate semantic directions is critical.
  - Quick check question: How does the Assistant Axis differ from simply using PC1, and what advantage does the contrastive definition offer?

- Concept: **Principal Component Analysis (PCA) for Interpretability**
  - Why needed here: PCA reveals that PC1 aligns with "Assistant-ness" across models; understanding how variance decomposition exposes interpretable structure is key.
  - Quick check question: What percentage of variance does PC1 explain, and how does this vary across Gemma, Qwen, and Llama?

## Architecture Onboarding

- Component map:
  1. **Data Generation Pipeline**: 275 roles × 5 system prompts × 240 extraction questions → 1200 rollouts/role; LLM judge filters for role expression.
  2. **Vector Extraction**: Mean post-MLP residual stream activations at response tokens → role vectors (n=377–463 per model).
  3. **Axis Computation**: PCA on role vectors → PC1; Assistant Axis = mean(default Assistant) − mean(role vectors); validated by cosine similarity with PC1.
  4. **Steering Module**: Activation capping applied at inference; vector addition/subtraction scaled by activation norm.
  5. **Evaluation Suite**: Role susceptibility (introspective questions), jailbreak robustness (persona-based attacks), capability benchmarks (IFEval, MMLU Pro, GSM8k).

- Critical path:
  1. Generate and filter role-playing rollouts → extract role vectors → run PCA → verify PC1 semantics.
  2. Compute Assistant Axis contrast vector → validate alignment with PC1.
  3. Calibrate activation cap threshold (25th percentile) on held-out rollouts.
  4. Sweep layer ranges and cap strengths → identify Pareto-optimal settings for harm reduction vs. capability preservation.

- Design tradeoffs:
  - **Contrast vector vs. PC1**: Contrast vector is robust when PC1 semantics vary across models; PC1 may capture slightly different variance.
  - **Layer selection**: Middle-to-late layers (e.g., 46–53/64 for Qwen, 56–71/80 for Llama) balance semantic abstraction and intervention granularity.
  - **Cap threshold**: 25th percentile is Pareto-optimal; stricter caps reduce harm more but risk capability loss.

- Failure signatures:
  - **No clear PC1 alignment**: If default Assistant projects to middle of PC1 rather than an extreme, the axis may not represent "Assistant-ness."
  - **Steering degrades fluency**: Excessive steering strengths cause nonsensical or repetitive outputs.
  - **Cap too strict**: Benchmark scores drop, especially on tasks requiring flexible persona.

- First 3 experiments:
  1. **Reproduce role vector extraction on a single model (e.g., Qwen 3 32B)**: Generate 50 roles × 5 prompts × 20 questions, filter by role expression, extract mean activations, run PCA. Verify PC1 separates Assistant-like vs. fantastical roles.
  2. **Validate steering causality**: Apply ±1×, ±2× Assistant Axis steering during role-play prompts; score role adoption (human, nonhuman, mystical, Assistant) with an LLM judge. Confirm directionality (away from Assistant → more role adoption).
  3. **Calibrate activation capping**: On a held-out subset of jailbreak prompts, sweep cap thresholds (1st, 10th, 25th, 50th percentiles) and layer ranges; measure harmful response rate and capability benchmark scores. Identify the Pareto frontier.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can preventative steering during training permanently anchor models to the Assistant persona more robustly than inference-time activation capping?
- Basis in paper: [explicit] Section 8.2 states that "productionizing such interventions, or exploring alternatives like preventative steering during training remain open challenges."
- Why unresolved: The paper demonstrates inference-time interventions (capping) work, but does not test if applying these vectors during fine-tuning creates a persistent "deeper anchoring" to the safe persona.
- What evidence would resolve it: Experiments applying the Assistant Axis vector as a loss term or steering intervention during the post-training phase, followed by evaluations of persona stability without inference-time steering.

### Open Question 2
- Question: Does the "Assistant Axis" exist in frontier models, Mixture-of-Experts (MoE) architectures, or reasoning models?
- Basis in paper: [explicit] Section 8.1 notes that "Reproducing our pipeline on frontier, mixture-of-expert, and reasoning models would help shed light on how the Assistant is represented in commonly used products."
- Why unresolved: The study is limited to three specific open-weights dense models (Gemma, Qwen, Llama), leaving the generality of the finding unknown for architectures with different internal structures.
- What evidence would resolve it: Replicating the persona space mapping and PC1 extraction on MoE models (e.g., Mixtral) or reasoning-focused models to see if a dominant "Assistant Axis" still emerges.

### Open Question 3
- Question: To what extent is the Assistant persona encoded nonlinearly or in the model weights, rather than linearly in the activations?
- Basis in paper: [explicit] Section 8.1 states, "The assumption that the Assistant persona corresponds to a linear direction in activation space is likely flawed... there are likely some aspects of the Assistant persona encoded in the weights."
- Why unresolved: The paper relies on linear principal component analysis and contrast vectors, which may miss crucial nonlinear interactions or weight-level representations that define the persona.
- What evidence would resolve it: Using mechanistic interpretability techniques (e.g., Sparse Autoencoders or circuit analysis) to identify nonlinear features or weight subnetworks that correlate with persona adherence.

### Open Question 4
- Question: Can Assistant Axis projections serve as a reliable, real-time safety metric to flag persona drift during live deployment?
- Basis in paper: [explicit] Section 8.2 suggests that "projections onto the Assistant Axis could serve as a real-time measure of model coherence in deployment... that could complement behavioral evaluations."
- Why unresolved: While the paper correlates low projections with harmful behavior in synthetic settings, it does not validate this metric as a live monitoring tool in production environments.
- What evidence would resolve it: A longitudinal study of deployed models where low Assistant Axis projections are logged alongside user reports of hallucinations or harmful behavior to validate predictive power.

## Limitations

- The linear representation assumption may not capture all aspects of the Assistant persona, which could be encoded nonlinearly in weights or activations.
- The optimal cap threshold (25th percentile) was calibrated on a specific dataset and may not generalize to all deployment scenarios.
- The study focuses on dense models and does not validate whether the Assistant Axis exists in frontier, MoE, or reasoning models.

## Confidence

- **High Confidence**: The existence of an Assistant Axis that varies consistently across models and can be extracted via PCA (validated by >0.92 correlation of PC1 loadings across models).
- **Medium Confidence**: The causal effectiveness of activation capping in reducing harmful responses while preserving capabilities (supported by benchmark results but limited to specific model sizes and datasets).
- **Low Confidence**: The universality of the drift-driving conversation types across different model architectures and the long-term stability of capped activations during extended conversations.

## Next Checks

1. **Cross-model Generalization Test**: Apply the Assistant Axis extraction and activation capping pipeline to additional model families (e.g., Mistral, DeepSeek) to verify that the linear persona structure and intervention effectiveness generalize beyond the three models studied.

2. **Causal Drift Validation**: Design a randomized controlled trial where conversation prompts are systematically varied while controlling for semantic content, to isolate whether Assistant Axis projections causally predict harmful behavior rather than merely correlating with it.

3. **Extended Conversation Stability**: Run long-form roleplay scenarios (10+ turns) with activation capping to test whether the intervention maintains stable behavior over extended interactions, or whether drift accumulates despite the cap.