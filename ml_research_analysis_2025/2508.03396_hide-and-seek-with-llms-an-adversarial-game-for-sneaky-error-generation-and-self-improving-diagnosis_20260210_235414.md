---
ver: rpa2
title: 'Hide and Seek with LLMs: An Adversarial Game for Sneaky Error Generation and
  Self-Improving Diagnosis'
arxiv_id: '2508.03396'
source_url: https://arxiv.org/abs/2508.03396
tags:
- errors
- error
- diagnosis
- training
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Hide and Seek Game (HSG) is a novel adversarial framework for\
  \ training LLMs to generate and diagnose complex reasoning errors. HSG employs two\
  \ adversarial roles\u2014Sneaky, which generates subtle, deceptive errors, and Diagnosis,\
  \ which detects them\u2014allowing both to improve through co-evolution."
---

# Hide and Seek with LLMs: An Adversarial Game for Sneaky Error Generation and Self-Improving Diagnosis

## Quick Facts
- **arXiv ID**: 2508.03396
- **Source URL**: https://arxiv.org/abs/2508.03396
- **Reference count**: 5
- **Primary result**: HSG achieves 16.8%–31.4% higher diagnostic accuracy than baselines like GPT-4o on three math reasoning datasets.

## Executive Summary
Hide and Seek Game (HSG) is a novel adversarial framework for training LLMs to generate and diagnose complex reasoning errors. HSG employs two adversarial roles—Sneaky, which generates subtle, deceptive errors, and Diagnosis, which detects them—allowing both to improve through co-evolution. Experiments on three math reasoning datasets show that HSG achieves 16.8%–31.4% higher diagnostic accuracy than baselines like GPT-4o, and generates significantly stealthier errors by avoiding common error patterns. The method also produces a challenging public dataset of deceptive errors and diagnostics, offering a new benchmark for error detection research.

## Method Summary
HSG uses a Qwen3-4B base model with two prompt-driven, parameter-shared roles: Sneaky (S) generates erroneous answers, and Diagnosis (D) identifies reasoning flaws. Both roles use GRPO with hierarchical rewards—S receives adversarial feedback based on D's success at detection and correction, while D receives collaborative feedback based on correction success. The framework trains on mixed datasets (GSM8K, MATH, NuminaMath-TIR) for 600 steps, selecting checkpoints by lowest correction failure rates. Evaluation uses correction success rate (ACCcorr) and GPT-4o win rate for diagnostic quality, with error type analysis distinguishing stealthy from trivial errors.

## Key Results
- HSG achieves 16.8%–31.4% higher diagnostic accuracy than baselines like GPT-4o across three math datasets.
- Generated errors show 92.2% classified as "neither" (stealthy), compared to 32.5% for baselines, avoiding common error patterns.
- Public dataset of 7,200 deceptive errors and diagnostics created, providing a challenging benchmark for error detection research.

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Co-evolution Through Role-Based Feedback
- **Claim**: HSG improves both error stealth and diagnostic accuracy through competitive pressure between generator and detector roles.
- **Mechanism**: The Sneaky role (S) generates incorrect answers, receives adversarial feedback based on whether the Diagnosis role (D) succeeds at detection and correction. D receives collaborative feedback based on correction success. This creates pressure toward harder errors and better diagnostics.
- **Core assumption**: Models can learn transferable diagnostic patterns from adversarially-generated errors rather than static error datasets.
- **Evidence anchors**:
  - [abstract]: "HSG involves two adversarial roles: Sneaky...and Diagnosis...Through adversarial co-evolution, both error stealth and diagnostic precision are enhanced."
  - [page 6]: "HSG (56.17 ± 6.00%) ≻ LLM-rater Adv+RL (38.51 ± 4.76%) ≻ RL only (27.43 ± 3.52%)" showing correction failure rates.
  - [corpus]: SPC paper shows similar self-play adversarial gains for critic evolution, supporting the adversarial co-evolution approach.
- **Break condition**: If Sneaky generates only trivial errors (Type A/B) that don't require genuine diagnostic skill to catch, co-evolution collapses into pattern matching.

### Mechanism 2: Hierarchical Reward Design Prevents Reward Hacking
- **Claim**: The hierarchical reward R(rmain, rsecondary) = max(rmain, τ) · [β + (1 − β)rsecondary] stabilizes training by ensuring auxiliary signals persist even when main rewards fail.
- **Mechanism**: Main rewards (error correctness for S, diagnostic accuracy for D) act as amplifiers. The threshold τ prevents exploration collapse when main rewards are low. Weight modulation via β balances objectives dynamically.
- **Core assumption**: Independent reward components (format, length) provide meaningful guidance without conflicting with primary objectives.
- **Evidence anchors**:
  - [page 3]: "Threshold clipping...prevents the policy from ceasing exploration due to failures in the main objective."
  - [page 5]: "β = 0.01 is used for S (due to initial difficulty in generating stealthy errors), and β = 0.04 for D."
  - [corpus]: Weak corpus evidence—no direct comparisons of hierarchical vs. flat reward designs found.
- **Break condition**: If auxiliary rewards correlate inversely with quality (e.g., longer answers always score lower regardless of content), the amplification mechanism backfires.

### Mechanism 3: GRPO Reduces Training Complexity for Long-Chain Reasoning
- **Claim**: Group Relative Policy Optimization eliminates value function training, making adversarial training feasible for reasoning tasks with long outputs.
- **Mechanism**: GRPO uses relative scores among G sampled candidates as baselines rather than learned value functions. This reduces memory and implementation complexity while maintaining stable policy updates.
- **Core assumption**: Group-relative advantages provide sufficient baseline quality for credit assignment in multi-step reasoning.
- **Evidence anchors**:
  - [page 4]: "GRPO eliminates the need for value function training, reducing both memory usage and implementation complexity...particularly advantageous for long-chain reasoning tasks."
  - [page 5]: Uses G = 8 samples per round with group mean/standard deviation normalization.
  - [corpus]: No direct corpus comparison of GRPO vs. PPO for adversarial training found.
- **Break condition**: If sample diversity within groups is low, relative advantages become noisy, causing unstable updates.

## Foundational Learning

- **Concept: Adversarial Training in LLMs**
  - Why needed here: HSG is fundamentally a competitive game between generator and discriminator. Understanding GAN-style dynamics helps anticipate mode collapse, oscillation, and convergence issues.
  - Quick check question: Can you explain why a discriminator that's too strong can prevent the generator from learning meaningful representations?

- **Concept: Policy Gradient Methods with KL Regularization**
  - Why needed here: GRPO optimizes policies with KL divergence constraints. Understanding why KL terms prevent catastrophic forgetting helps diagnose training instability.
  - Quick check question: What happens to sample diversity if the KL penalty coefficient β is set too high?

- **Concept: Correction-Based Evaluation Signals**
  - Why needed here: HSG uses correction success (rcorr(aC)) as the ultimate feedback signal rather than human annotation. This assumes correction models encode diagnostic quality.
  - Quick check question: What failure modes could arise if the correction model πcorrect has systematic blind spots?

## Architecture Onboarding

- **Component map**:
  - Qwen3-4B base model → Sneaky role (S) and Diagnosis role (D) with shared parameters → Correction model πcorrect → Reward calculator → GRPO optimizer
  - S generates G=8 erroneous answers → D diagnoses each → πcorrect attempts correction → Compute hierarchical rewards → Update S and D alternately

- **Critical path**:
  1. S samples G erroneous answers → D diagnoses each → πcorrect attempts correction
  2. Compute Radversarial_S for all G samples, select highest-reward sneaky answer
  3. Train D on the hardest sneaky answer with Rcollaborative_D feedback
  4. Alternate S and D training steps (Eqs. 15, 16)

- **Design tradeoffs**:
  - Parameter sharing between S and D reduces memory but may cause interference
  - Using πcorrect as fixed evaluator provides consistent feedback but cannot improve
  - Selecting only hardest sample for D training focuses capacity but may reduce diversity
  - β settings (0.01 for S, 0.04 for D) reflect asymmetric difficulty but require tuning

- **Failure signatures**:
  - **Type A errors dominate**: Sneaky just shifts final answers without reasoning changes → indicates reward not penalizing trivial strategies
  - **Type B errors dominate**: Explicit contamination persists → D is not providing useful feedback signal
  - **Training oscillation**: S and D accuracies bounce without convergence → check KL penalty strength
  - **Low correction failure rates**: Generated errors too easy → adversarial pressure insufficient

- **First 3 experiments**:
  1. **Baseline comparison**: Train HSG vs. LLM-rater Adv+RL vs. RL-only on NuminaMATH-TIR subset, measure correction failure rates across 5 baseline models (replicate Fig. 4)
  2. **Ablate hierarchical reward**: Compare R(rmain, rsecondary) vs. flat reward sum on error type distribution (target: Type A/B rates without hierarchy should increase)
  3. **Scaling test**: Train on Qwen3-4B, 8B, 14B with identical hyperparameters; verify if larger models show diminishing improvements from HSG diagnostics (as suggested by Table 1 trends)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the HSG framework maintain its effectiveness when adapted for open-domain reasoning tasks that lack objectively verifiable ground truth answers?
- **Basis in paper**: [explicit] The Conclusion states, "In future work, we plan to extend HSG to broader open-domain and complex reasoning tasks, especially in intelligent education."
- **Why unresolved**: The current implementation relies on mathematical datasets specifically because they possess "structured logic and well-defined answers" (Page 1) required for the correctness reward function $r_{corr}$. Open-domain tasks lack these binary signals.
- **What evidence would resolve it**: Successful application of HSG to tasks like qualitative logical inference or summarization, utilizing alternative evaluation metrics such as LLM-as-a-judge or human evaluation to replace $\Gamma_{correct}$.

### Open Question 2
- **Question**: To what extent does the capability of the fixed auxiliary correction model ($\pi_{correct}$) act as a bottleneck or ceiling for the co-evolution of the Sneaky and Diagnosis agents?
- **Basis in paper**: [inferred] The framework relies on a static $\pi_{correct}$ to determine correction success (Eq. 8) and generate feedback rewards, but this model "does not participate in optimization" (Page 3).
- **Why unresolved**: If the correction model is weak, it may fail to correct errors identified by Diagnosis, penalizing the framework unfairly. Conversely, it may fail to correct truly "sneaky" errors, inadvertently rewarding the Sneaky agent for flaws that are actually trivial.
- **What evidence would resolve it**: An ablation study comparing HSG performance using correction models of varying sizes (e.g., 7B vs. 70B parameters) to analyze the correlation between the corrector's ceiling and the adversaries' improvement.

### Open Question 3
- **Question**: Does the shared-parameter architecture between the adversarial Sneaky and Diagnosis roles lead to optimization interference or gradient conflict during training?
- **Basis in paper**: [inferred] The paper notes that $\pi_{\theta_S}$ and $\pi_{\theta_D}$ are "prompt-driven, parameter-shared models" (Page 4), implying a single set of weights is updated by opposing objectives.
- **Why unresolved**: While parameter sharing reduces memory usage, updating the same weights to simultaneously "hide" errors and "seek" errors creates a conflict in the gradient direction (one maximizes detection difficulty, the other minimizes it), potentially leading to sub-optimal convergence.
- **What evidence would resolve it**: A comparative analysis of training dynamics and final performance between the proposed shared-parameter model and a decoupled model where Sneaky and Diagnosis roles have independent parameters.

## Limitations
- **Role-specific prompts missing**: Exact formulations for Sneaky and Diagnosis prompts are referenced but not fully specified, creating potential reproducibility gaps.
- **Fixed evaluator assumption**: Using πcorrect as a static oracle assumes its diagnostic judgments are comprehensive and unbiased—no validation of its coverage of subtle reasoning errors is provided.
- **Scaling assumptions untested**: While results show HSG works on Qwen3-4B, no evidence confirms the method transfers to larger models or different base architectures.

## Confidence

| Claim | Confidence |
|-------|------------|
| Adversarial co-evolution mechanism | **High** - Strong experimental evidence (16.8%–31.4% accuracy gains) and alignment with SPC paper's self-play critic results |
| Hierarchical reward design | **Medium** - Plausible mechanism but lacks direct ablation studies comparing hierarchical vs. flat rewards |
| GRPO advantages | **Medium** - Theoretically sound but no corpus evidence directly compares GRPO vs. PPO for this adversarial setting |

## Next Checks

1. **Prompt dependency test**: Train HSG with three different prompt variants for S and D roles to quantify how sensitive performance is to prompt engineering.
2. **Evaluator bias audit**: Generate 100 HSG errors, manually annotate diagnostic quality, and compare against πcorrect judgments to identify systematic blind spots.
3. **Cross-architecture transfer**: Replicate HSG on Llama-3 or GPT-2 architecture to test if adversarial gains generalize beyond Qwen family.