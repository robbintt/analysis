---
ver: rpa2
title: 'One Head, Many Models: Cross-Attention Routing for Cost-Aware LLM Selection'
arxiv_id: '2509.09782'
source_url: https://arxiv.org/abs/2509.09782
tags:
- routing
- cost
- performance
- router
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a cross-attention-based routing framework\
  \ for selecting the most appropriate large language model (LLM) from a diverse pool\
  \ for each query, aiming to optimize the balance between response quality and computational\
  \ cost. Unlike prior methods that rely on static features or query-only embeddings,\
  \ the approach explicitly models query\u2013model interactions using a single-head\
  \ cross-attention mechanism that learns joint representations of prompts and LLMs."
---

# One Head, Many Models: Cross-Attention Routing for Cost-Aware LLM Selection

## Quick Facts
- **arXiv ID:** 2509.09782
- **Source URL:** https://arxiv.org/abs/2509.09782
- **Reference count:** 36
- **Primary result:** Introduces cross-attention routing framework achieving up to 6.6% improvement in AIQ and 2.9% higher maximum performance over existing predictors

## Executive Summary
This paper presents a novel cross-attention-based routing framework for selecting optimal large language models from a diverse pool for each query, balancing response quality against computational cost. Unlike traditional static feature-based approaches, the method explicitly models query-model interactions using a single-head cross-attention mechanism that learns joint representations of prompts and LLMs. The framework employs a dual-predictor architecture estimating both quality and cost, using an exponential reward function to robustly balance these objectives across varying user preferences. Evaluated on RouterBench across multiple domains, the attention-based router achieves significant performance improvements while maintaining stability across different cost preferences.

## Method Summary
The framework uses cross-attention routing to select the most appropriate LLM from a diverse pool for each query by explicitly modeling query-model interactions. It employs a single-head cross-attention mechanism that learns joint representations of prompts and LLMs, enabling dynamic selection based on task requirements and cost constraints. The system uses a dual-predictor architecture to estimate both response quality and generation cost, with an exponential reward function to balance these objectives. The approach is evaluated on RouterBench across multiple LLM pools and domains, demonstrating improved cost-performance trade-offs compared to static feature-based predictors.

## Key Results
- Achieves up to 6.6% improvement in Average Improvement in Quality (AIQ) over existing predictors
- Delivers 2.9% higher maximum performance while maintaining better stability across user cost preferences
- Attention-based predictors consistently outperform regression and MLP variants in ablation studies

## Why This Works (Mechanism)
The cross-attention mechanism explicitly models the interaction between queries and LLMs by learning joint representations, rather than relying on static features or query-only embeddings. This allows the router to capture nuanced relationships between task requirements and model capabilities. The dual-predictor architecture separates quality and cost estimation, enabling more precise control over the trade-off between performance and computational resources. The exponential reward function provides robust balancing across varying user preferences, preventing extreme swings in model selection that could occur with linear weighting schemes.

## Foundational Learning
- **Cross-attention mechanisms:** Why needed - to capture query-model interactions explicitly; Quick check - compare performance against static feature-based approaches
- **Dual-predictor architecture:** Why needed - separate quality and cost estimation for precise trade-off control; Quick check - validate independent predictor performance
- **Exponential reward functions:** Why needed - robust balancing across varying user preferences; Quick check - test sensitivity to different preference distributions
- **RouterBench evaluation framework:** Why needed - standardized benchmark for LLM routing systems; Quick check - assess generalizability across different benchmark datasets

## Architecture Onboarding
- **Component map:** Query embeddings -> Cross-attention module -> Dual predictors (Quality + Cost) -> Exponential reward function -> Model selection
- **Critical path:** Input query → Cross-attention encoding → Quality prediction → Cost prediction → Reward calculation → Optimal model selection
- **Design tradeoffs:** Cross-attention provides better query-model interaction modeling but increases computational overhead; dual predictors enable precise trade-off control but require separate training
- **Failure signatures:** Suboptimal routing on out-of-distribution queries, sensitivity to reward function hyperparameters, potential overfitting to RouterBench-specific patterns
- **First experiments:** 1) Baseline comparison using static features vs cross-attention on RouterBench, 2) Ablation study of reward function types (linear vs exponential), 3) Sensitivity analysis to varying user preference distributions

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation conducted primarily on RouterBench, a single benchmark dataset that may not represent all real-world deployment scenarios
- Limited analysis of failure cases and edge conditions where the router might make suboptimal selections
- Exponential reward function introduces hyperparameters requiring tuning for different deployment contexts

## Confidence
- **High confidence:** Technical implementation of cross-attention routing and dual-predictor architecture
- **Medium confidence:** Performance improvements over baselines due to benchmark-specific evaluation methodology
- **Medium confidence:** Generalizability across different LLM pools and domains given limited evaluation scope

## Next Checks
1. Test the routing framework on multiple independent benchmarks beyond RouterBench to assess generalizability across different task distributions and query types
2. Conduct ablation studies specifically examining the cross-attention mechanism's behavior on out-of-distribution queries and failure case analysis
3. Evaluate the system's performance under varying computational constraints and with dynamically changing LLM pools to assess robustness in production environments