---
ver: rpa2
title: 'RAPID-Serve: Resource-efficient and Accelerated P/D Intra-GPU Disaggregation'
arxiv_id: '2601.11822'
source_url: https://arxiv.org/abs/2601.11822
tags:
- decode
- prefill
- rapid-serve
- serving
- cache
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RAPID-Serve is a novel LLM inference serving system that enables
  concurrent execution of prefill and decode phases on the same GPU(s) to improve
  resource utilization while meeting latency SLOs. Unlike hybrid batching that couples
  phases in lockstep (causing latency spikes) or disaggregated serving that requires
  expensive KV-cache transfers, RAPID-Serve uses concurrent execution with adaptive
  resource allocation via CU masking.
---

# RAPID-Serve: Resource-efficient and Accelerated P/D Intra-GPU Disaggregation

## Quick Facts
- arXiv ID: 2601.11822
- Source URL: https://arxiv.org/abs/2601.11822
- Authors: Amna Masood; Pratishtha Gaur; Nuwan Jayasena
- Reference count: 22
- Primary result: Enables concurrent prefill and decode execution on same GPU(s) for up to 4.1x throughput improvement while maintaining latency SLOs

## Executive Summary
RAPID-Serve is a novel LLM inference serving system that enables concurrent execution of prefill and decode phases on the same GPU(s) to improve resource utilization while meeting latency SLOs. Unlike hybrid batching that couples phases in lockstep (causing latency spikes) or disaggregated serving that requires expensive KV-cache transfers, RAPID-Serve uses concurrent execution with adaptive resource allocation via CU masking. This allows prefill and decode to make independent progress while minimizing interference. Evaluations show RAPID-Serve achieves up to 4.1x (average 1.7x) unconstrained throughput improvement and up to 32x (average 4.9x) goodput improvement under SLO constraints compared to state-of-the-art approaches, while maintaining p95 ITL latencies up to 6x lower than chunked hybrid batching.

## Method Summary
RAPID-Serve implements concurrent prefill and decode execution using multiprocessing on AMD Instinct GPUs, sharing model weights and KV cache via IPC handles. The system leverages CU masking to partition compute units between phases, allocating more CUs to compute-bound prefill and fewer to memory-bound decode. An Adaptive Resource Manager dynamically switches between overallocation (both phases get 100% CUs at low decode load) and partitioned allocation (based on offline profiles of minimum CUs needed per decode batch size) to balance SLO compliance and utilization. The design uses FCFS scheduling with async_scheduling running one step ahead of GPU execution, and a lock-free KV cache manager residing only in the decode process.

## Key Results
- Achieves up to 4.1x (average 1.7x) unconstrained throughput improvement over state-of-the-art approaches
- Delivers up to 32x (average 4.9x) goodput improvement under SLO constraints
- Maintains p95 ITL latencies up to 6x lower than chunked hybrid batching while meeting SLOs
- Shows robust performance across LMSYS, arXiV, and Loogle datasets with varying prompt lengths

## Why This Works (Mechanism)

### Mechanism 1: Intra-GPU Prefill-Decode Concurrency via Multiprocessing
Running prefill and decode as separate processes on the same GPU eliminates lockstep coupling while sharing memory, achieving concurrent execution without KV cache transfer overhead. Prefill and decode execute in independent processes with separate GPU kernels that overlap temporally. KV cache is shared via IPC handles; decode process allocates blocks and notifies prefill process, avoiding locks. This breaks the iteration-level batching constraint where prefill completion gates decode progress.

### Mechanism 2: Compute Unit Partitioning via CU Masking
Fine-grained spatial partitioning of GPU compute units allows memory-bound decode to maintain performance with fewer CUs, freeing resources for compute-bound prefill. AMD Instinct GPUs expose CU masking—per-command-queue bitmasks restricting kernel execution to specific compute units. Decode kernels achieve similar performance with 40-50% of CUs (memory-bandwidth bound), while prefill performance scales near-linearly with CU count (compute-bound).

### Mechanism 3: Adaptive Resource Allocation via Profiling-Driven Policy
Dynamically switching between resource overallocation and partitioned allocation based on decode workload intensity maintains SLO compliance while maximizing utilization. At low decode load, both phases get 100% CUs (overallocation), letting hardware scheduler arbitrate; as decode load increases and ITL SLOs are at risk, the system switches to partitioned allocation based on offline profiles of minimum CUs needed per decode batch size to meet SLOs.

## Foundational Learning

- **Concept: Compute-Bound vs. Memory-Bandwidth-Bound Kernels**
  - Why needed here: Understanding that prefill is compute-bound (benefits from more CUs) while decode is memory-bandwidth-bound (needs fewer CUs) is foundational to RAPID-Serve's resource partitioning strategy.
  - Quick check question: If you reduce compute units by 50% for a kernel and performance drops by 50%, is it compute-bound or memory-bound?

- **Concept: KV Cache Structure and Lifecycle**
  - Why needed here: The paper assumes familiarity with KV cache (stores keys/values for all tokens), its growth with sequence length, and why it's central to both prefill (produces it) and decode (consumes it incrementally).
  - Quick check question: Why does disaggregated serving require KV cache transfer between instances while RAPID-Serve does not?

- **Concept: Service Level Objectives (SLOs) for LLM Serving**
  - Why needed here: RAPID-Serve optimizes for TTFT (Time-To-First-Token) and ITL (Inter-Token Latency) under constraints; goodput is defined as throughput under SLO attainment.
  - Quick check question: A system achieves 1000 tokens/sec throughput but 90% of requests exceed 200ms ITL. Is this goodput or raw throughput?

## Architecture Onboarding

- **Component map:** Scheduler -> Prefill Process & Decode Process -> KV Cache Manager (in decode only) -> GPU with Adaptive Resource Manager
- **Critical path:** Request arrives → decode process allocates KV blocks → notifies prefill process → prefill executes concurrently with ongoing decode batches → prefill populates KV cache → notifies decode → decode moves request to running batch → generates tokens auto-regressively → Resource Manager monitors decode batch size → adjusts CU allocation if ITL SLO at risk
- **Design tradeoffs:** Lock-free vs. consistency (only decode manages KV cache to avoid locks; this creates dependency but eliminates serialization); Overallocation vs. partitioning (overallocation maximizes utilization but risks SLO violation under load; partitioning guarantees SLOs but may leave CUs idle if decode finishes early); No prefill chunking vs. long prompts (prefill runs to completion in one step, reducing overhead but potentially stalling new requests under very long prompts)
- **Failure signatures:** ITL spike under load (indicates Resource Manager failed to switch to partitioned mode; check decode batch size vs. threshold); GPU idle gaps (async_scheduling disabled or CPU bottleneck; profile CPU-side scheduling); KV cache allocation failure (decode process out of blocks; check block pool size and request preemption logic); CU masking ignored (HIP Graphs may cache CU masks; verify queue configuration before graph capture)
- **First 3 experiments:** 1) Baseline comparison: Run RAPID-Serve vs. chunked hybrid batching on LMSYS trace; measure throughput, TTFT, ITL, goodput under SLO (ITL < 100ms LLaMA-3.1 70B, 50ms Mixtral 8×7B); confirm reported gains (1.7x-4.1x throughput, up to 32x goodput); 2) Ablation on CU masking: Disable CU masking (use overallocation only) vs. partitioned allocation; measure ITL degradation under high decode batch sizes; verify Figure 7 behavior (D100-P100 exceeds SLO at larger batches); 3) Stress test with long prompts: Run on arXiV (8K avg) and Loogle (20K avg) datasets; verify that non-chunked prefill does not cause unacceptable TTFT queuing delays and that memory capacity is not exceeded

## Open Questions the Paper Calls Out

### Open Question 1
Can the memory subsystem interference observed between concurrent prefill and decode kernels be mitigated through software-level scheduling or partitioning techniques without relying on hardware virtualization? The paper identifies memory contention as a source of performance degradation (2-5% for decode) but explicitly excludes addressing it from the current design due to hardware limitations and the complexity of virtualization.

### Open Question 2
How does the assumption of single-step prefill execution (no chunking) impact the system's ability to handle requests with extremely long contexts that exceed single-iteration memory or compute limits? While the design avoids chunking to reduce overhead, this creates a hard limit on prompt size that can be processed in a single iteration, potentially causing memory errors or excessive latency for very long contexts (e.g., >20k tokens) relative to available CUs.

### Open Question 3
Does the intra-GPU disaggregation approach of RAPID-Serve retain its performance advantage over traditional disaggregated serving when scaled to large, multi-node clusters with high-bandwidth interconnects? The paper proves efficacy in resource-constrained environments, but it is unclear if the intra-GPU concurrency model outperforms optimized inter-node disaggregation (e.g., Splitwise/Mooncake) when network bandwidth is no longer the primary bottleneck.

## Limitations
- Architecture dependency on AMD Instinct GPUs and ROCm, limiting generalization to other GPU vendors
- Evaluation scope restricted to specific datasets (LMSYS, arXiV, Loogle) and two model architectures (LLaMA-3.1 70B, Mixtral 8×7B)
- Single-step prefill execution creates hard limits on context length without chunking mechanism

## Confidence
- **High Confidence:** The fundamental observation that prefill is compute-bound while decode is memory-bandwidth-bound, enabling resource partitioning; the general approach of concurrent execution via multiprocessing to avoid lockstep coupling
- **Medium Confidence:** The specific performance gains (1.7x-4.1x throughput, 32x goodput) reported under the tested conditions; the effectiveness of the adaptive resource manager in maintaining SLOs across varying workloads
- **Low Confidence:** Generalization of results to other GPU architectures, model families, and workload distributions; the robustness of lock-free KV cache management under extreme contention scenarios

## Next Checks
1. **Cross-Architecture Validation:** Implement RAPID-Serve on NVIDIA GPUs using MPS for concurrent execution and test whether equivalent performance gains are achievable without CU masking. Measure if alternative spatial partitioning methods (e.g., CUDA MPS context affinity) can replicate the results.

2. **Profile-Based SLO Prediction:** Conduct extensive offline profiling across diverse model architectures (different attention mechanisms, sequence lengths) to validate the adaptive resource manager's decision boundaries. Test whether the policy maintains SLOs when deployed on workloads with different characteristics than the training profiles.

3. **Long Context Stress Test:** Evaluate RAPID-Serve under extreme sequence lengths (>16K tokens) and concurrent high-QPS workloads to identify breaking points in the lock-free KV cache design and resource allocation policy. Measure memory fragmentation and allocation latency impacts.