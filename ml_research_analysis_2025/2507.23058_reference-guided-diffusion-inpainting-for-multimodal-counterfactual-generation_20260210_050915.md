---
ver: rpa2
title: Reference-Guided Diffusion Inpainting For Multimodal Counterfactual Generation
arxiv_id: '2507.23058'
source_url: https://arxiv.org/abs/2507.23058
tags:
- image
- object
- reference
- diffusion
- cited
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This thesis introduces two novel methods for controllable counterfactual
  generation across perceptual domains: MObI for multimodal object inpainting in autonomous
  driving, and AnydoorMed for reference-guided anomaly inpainting in mammography scans.
  Both methods leverage latent diffusion models conditioned on reference images and
  bounding boxes to achieve high-fidelity, semantically consistent insertions while
  preserving multimodal coherence.'
---

# Reference-Guided Diffusion Inpainting For Multimodal Counterfactual Generation
## Quick Facts
- arXiv ID: 2507.23058
- Source URL: https://arxiv.org/abs/2507.23058
- Reference count: 40
- Introduces two novel methods for counterfactual generation in autonomous driving and medical imaging

## Executive Summary
This thesis presents MObI and AnydoorMed, two reference-guided diffusion inpainting methods for generating multimodal counterfactuals in safety-critical domains. MObI enables controlled object manipulation in autonomous driving scenes through reference-guided inpainting conditioned on bounding boxes, while AnydoorMed performs anomaly inpainting in mammography scans using similar reference-based techniques. Both methods leverage pretrained latent diffusion models and CLIP embeddings to ensure semantic consistency between reference images and generated content. The methods achieve state-of-the-art results in their respective domains, demonstrating high-fidelity object insertions that preserve multimodal coherence and realistic appearance.

## Method Summary
The thesis introduces two complementary approaches for counterfactual generation using reference-guided diffusion inpainting. MObI (Multimodal Object Inpainting) manipulates objects in autonomous driving scenes by conditioning a latent diffusion model on reference images and bounding boxes, enabling both object reinsertion and replacement tasks. AnydoorMed extends this framework to medical imaging, specifically mammography anomaly detection, where it inserts, replaces, or reinserts abnormalities while maintaining anatomical consistency. Both methods use a two-stage process: first extracting CLIP embeddings from reference images, then conditioning the diffusion model on these embeddings along with spatial masks derived from bounding boxes. The approach preserves background integrity while generating semantically consistent new objects that match the reference content.

## Key Results
- MObI achieves FID scores of 6.60 and 9.00 for reinsertion and replacement tasks respectively
- AnydoorMed demonstrates realism with FID of 1.83±0.16, 2.78±0.21, and 4.78±0.14 for reinsertion, replacement, and insertion tasks
- CLIP-I scores exceed 90 across all settings, indicating strong semantic alignment
- Object detection performance remains high on reinserted objects in MObI

## Why This Works (Mechanism)
The methods succeed by leveraging pretrained vision-language models (CLIP) to capture high-level semantic features from reference images, which are then used to condition diffusion models during inpainting. This reference-guided approach ensures that generated objects maintain semantic coherence with the reference while being spatially constrained by bounding boxes. The latent diffusion framework enables efficient manipulation of image features without degrading background quality. By combining semantic guidance from CLIP with spatial constraints, the methods can generate realistic counterfactuals that preserve multimodal consistency across domains.

## Foundational Learning
- Latent Diffusion Models: Why needed - Enable efficient image generation through latent space manipulation; Quick check - Verify training uses VQ-VAE encoder/decoder architecture
- CLIP Embeddings: Why needed - Provide semantic guidance bridging vision and language; Quick check - Confirm CLIP model used for both reference encoding and generated content evaluation
- Object Detection/Segmentation: Why needed - Provide spatial constraints for inpainting; Quick check - Validate detection accuracy on target dataset
- Multimodal Consistency: Why needed - Ensure generated content aligns semantically across domains; Quick check - Review CLIP-I score calculation methodology
- FID Score Calculation: Why needed - Quantify realism of generated images; Quick check - Verify proper implementation of Fréchet Inception Distance

## Architecture Onboarding
Component Map: CLIP Reference Encoder -> Latent Diffusion Model -> VQ-VAE Decoder -> Output Image
Critical Path: Reference image → CLIP embedding extraction → Bounding box masking → Diffusion inpainting → Final image reconstruction
Design Tradeoffs: The methods prioritize semantic fidelity over fine-grained detail accuracy, using CLIP guidance rather than pixel-level supervision. This enables better multimodal coherence but may sacrifice photorealistic textures.
Failure Signatures: Poor semantic alignment when reference CLIP embeddings mismatch target context, inaccurate bounding box detection leading to spatial artifacts, and background degradation when inpainting regions are too large.
First Experiments:
1. Baseline: Unconditional diffusion inpainting without reference guidance
2. Ablation: Removing CLIP conditioning to measure semantic coherence impact
3. Stress Test: Extreme object size variations to evaluate spatial constraint robustness

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on accurate object detection and segmentation preprocessing
- Assumption of paired reference images and bounding boxes availability
- Automated metrics may not fully capture subjective realism and semantic coherence
- Limited validation across diverse object types and environmental conditions

## Confidence
High confidence in technical implementation
Medium confidence in generalizability across domains
Medium confidence in multimodal coherence claims

## Next Checks
1. Conduct user studies with domain experts to validate practical utility beyond automated metrics
2. Test method robustness across varying object sizes, lighting conditions, and occlusion scenarios
3. Evaluate computational efficiency and inference time for real-time deployment feasibility