---
ver: rpa2
title: 'Toward Reliable Clinical Coding with Language Models: Verification and Lightweight
  Adaptation'
arxiv_id: '2510.07629'
source_url: https://arxiv.org/abs/2510.07629
tags:
- code
- clinical
- codes
- coding
- verification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the problem of accurate clinical coding using\
  \ large language models (LLMs). The authors find that off-the-shelf LLMs perform\
  \ poorly on exact match metrics and that many errors are near-misses\u2014codes\
  \ that are hierarchically close but incorrect."
---

# Toward Reliable Clinical Coding with Language Models: Verification and Lightweight Adaptation

## Quick Facts
- arXiv ID: 2510.07629
- Source URL: https://arxiv.org/abs/2510.07629
- Reference count: 15
- One-line primary result: Verification pipeline improves end-to-end F1 by up to 16 points (Haiku-3: 41.6→57.6) via lightweight expansion and LLM-based verification.

## Executive Summary
This paper addresses the challenge of accurate clinical coding using large language models (LLMs). While off-the-shelf LLMs struggle with exact match metrics, the authors find that many errors are near-misses—hierarchically close but incorrect codes. To improve performance, they propose lightweight interventions including prompt engineering, small-scale fine-tuning, and a clinical code verification pipeline that expands candidate codes using ICD-10-CM hierarchy and verifies them via LLM-based contextual revision. The paper releases a new expert double-annotated outpatient clinical notes dataset and demonstrates significant improvements through verification, achieving up to 16-point gains in F1 scores.

## Method Summary
The method combines prompt engineering, lightweight fine-tuning, and a generate-expand-verify pipeline. Initial code generation uses various prompt strategies (baseline, detailed instructions, chain-of-thought, prompt decomposition). For verification, candidate codes are expanded using ICD-10-CM hierarchical relationships (siblings, cousins, 1-hop and 2-hop neighbors), then verified via LLM-based multiple-choice scoring using description-only prompts. Fine-tuning uses paired code-description outputs to improve model performance. The pipeline is evaluated on a new expert-annotated outpatient clinical notes dataset with ICD-10-CM codes.

## Key Results
- Verification pipeline improves end-to-end F1 scores by up to 16 points (Haiku-3: 41.6→57.6)
- Description-only prompts achieve highest verification accuracy (Sonnet-3.5v2: 90.3% on cousin expansions)
- Fine-tuning with code→description format significantly improves performance, while code-only training causes collapse
- Lightweight interventions substantially improve accuracy without heavy computational overhead

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Candidate Expansion
Expanding LLM-predicted codes using ICD-10-CM structural relationships increases the probability that the correct code enters the candidate pool for verification. The ICD-10-CM tabular list forms a tree where siblings, cousins, and index graph neighbors provide semantically proximate alternatives. For a predicted code, the expanded set creates a bounded search space (~0.05–0.5% of 72k billable codes), targeting near-miss errors where models generate codes from the correct branch but wrong leaf. Core assumption: Most LLM errors occur within structured proximity rather than across unrelated branches.

### Mechanism 2: Description-Only Verification Prompts
Presenting candidate codes via their text descriptions (not alphanumeric tokens) improves LLM verification accuracy. LLMs are pretrained on natural language, not ICD code tokens. Description-only prompts align candidate presentation with the model's pretraining distribution, enabling semantic matching between clinical note context and code meaning. This yielded 90.3% accuracy on cousin expansions (Sonnet-3.5v2), outperforming code-only and code+description formats. Core assumption: The LLM's semantic understanding transfers better through descriptive text than through code identifiers.

### Mechanism 3: Paired Code-Description Fine-Tuning
Small-scale fine-tuning on paired code→description outputs improves performance substantially, but output format is critical—code-only training causes collapse. Fine-tuning on 67 examples with code→description format improved Haiku-3 from 40.6 to 56.9 F1. The paired output creates an implicit consistency constraint, forcing the model to align code tokens with semantic meaning. Training on code alone collapsed performance to 0.0, likely due to token distribution shift without grounding. Core assumption: High-quality paired examples provide sufficient signal for lightweight adaptation.

## Foundational Learning

- **Concept: ICD-10-CM Tree Structure (Billable vs. Non-Billable)**
  - Why needed here: The expansion mechanism relies on siblings/cousins relationships; only leaf nodes are billable, so near-miss errors often occur at leaf or leaf-1 levels.
  - Quick check question: Given code I11.0 ("Hypertensive heart disease with heart failure"), what defines its siblings, and why might a model confuse it with I11.9?

- **Concept: Hierarchical Evaluation Metrics (Prefix Match, POR)**
  - Why needed here: Exact match F1 obscures near-miss patterns; prefix-n and Prefix Overlap Ratio quantify partial correctness.
  - Quick check question: If a model predicts I11.0 and gold is I11.9, would prefix-1 match count this as correct? What about prefix-2?

- **Concept: Multiple-Choice LLM Scoring**
  - Why needed here: The verification step scores candidates rather than generating freely; understanding MC vs. generation trade-offs is essential.
  - Quick check question: Why might asking an LLM to "select the most relevant description from these 10 options" outperform asking it to "generate the correct ICD code"?

## Architecture Onboarding

- **Component map**: Generator (LLM) → Expander (ICD-10-CM hierarchy) → Verifier (LLM with description-only MC prompt) → Evaluator
- **Critical path**: Generator accuracy → Expansion coverage (is gold in candidates?) → Verifier discrimination (can LLM pick gold from near-misses?)
- **Design tradeoffs**: Broader expansion increases coverage but adds distractors, lowering conditional verification accuracy; description-only prompts require maintaining code→description lookup; fine-tuning is lightweight but format-sensitive
- **Failure signatures**: High verification accuracy, flat end-to-end F1 → expansion not covering gold codes; fine-tuned model outputs invalid codes → trained on code-only format; laterality errors persist → sibling descriptions too similar
- **First 3 experiments**:
  1. Baseline: Run off-the-shelf LLM with single-line prompt on your notes. Compute exact match F1 and prefix-2 F1. Gap indicates near-miss rate to target.
  2. Expansion ablation: Test S(c), C(c), N1(c), N2(c) independently. Measure coverage (% gold-in-candidates) and verification accuracy. Identify which relations help vs. add noise.
  3. Fine-tuning format check: On small held-out set, compare code-only vs. code→description training. Confirm Table 3 pattern (code-only collapse) before committing resources.

## Open Questions the Paper Calls Out
- Can the verification pipeline generalize effectively to inpatient clinical coding settings, where notes are longer, more structured, and span multiple departments?
- How does verification accuracy vary across patient demographic groups, clinical conditions, and healthcare settings, and can fairness-aware interventions mitigate disparities?
- What is the optimal balance between candidate expansion breadth (coverage) and distractor proliferation in the verification step, and does this optimum vary by model capacity?

## Limitations
- The lightweight adaptation results are highly sensitive to output format, but exact training setup and hyperparameters remain unspecified
- Hierarchical expansion may not capture all clinically relevant near-misses when correct codes fall outside the expanded candidate set
- Evaluation focuses on exact match and prefix-level metrics but does not measure clinical impact of retrieved codes

## Confidence
- **High confidence**: The verification pipeline improves performance when gold codes are in the candidate pool; the format-sensitivity of fine-tuning (code→description vs code-only) is well-demonstrated
- **Medium confidence**: The lightweight interventions generalize beyond the specific model versions tested; the claim that 67 examples suffice for meaningful adaptation
- **Low confidence**: That hierarchical expansion covers the majority of near-miss errors in practice; that description-only prompts will consistently outperform alternatives across different medical domains

## Next Checks
1. Oracle upper bound analysis: Measure end-to-end performance when expansion always includes gold codes to isolate generator vs verifier contributions
2. Format transfer test: Apply the code→description fine-tuning protocol to a different LLM (e.g., Claude-3.5-Sonnet) to verify format sensitivity generalizes
3. Expansion coverage audit: For generator failures, manually classify whether gold codes fall within siblings/cousins or require deeper structural expansion