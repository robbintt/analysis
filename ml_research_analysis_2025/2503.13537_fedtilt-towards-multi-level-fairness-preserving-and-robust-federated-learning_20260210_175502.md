---
ver: rpa2
title: 'FedTilt: Towards Multi-Level Fairness-Preserving and Robust Federated Learning'
arxiv_id: '2503.13537'
source_url: https://arxiv.org/abs/2503.13537
tags:
- client
- fairness
- data
- loss
- fedtilt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of achieving both fairness and
  robustness in federated learning (FL), where existing methods often focus on only
  one of these aspects. The authors propose FedTilt, a novel FL framework inspired
  by tilted empirical risk minimization (TERM), which introduces tilt hyperparameters
  to flexibly tune fairness and robustness.
---

# FedTilt: Towards Multi-Level Fairness-Preserving and Robust Federated Learning

## Quick Facts
- **arXiv ID**: 2503.13537
- **Source URL**: https://arxiv.org/abs/2503.13537
- **Reference count**: 40
- **Primary result**: FedTilt achieves better trade-offs between fairness, robustness, and accuracy than state-of-the-art fair FL methods on realistic federated datasets.

## Executive Summary
This paper introduces FedTilt, a novel federated learning framework that addresses the challenge of achieving both fairness and robustness simultaneously. Built on tilted empirical risk minimization (TERM), FedTilt introduces tunable tilt hyperparameters that allow flexible control over two levels of fairness (client fairness and client data fairness) while maintaining robustness to persistent outliers. The framework outperforms existing fair FL methods like Ditto on benchmark datasets including MNIST and CIFAR10, demonstrating its effectiveness in complex real-world scenarios.

## Method Summary
FedTilt extends federated learning with a two-level tilted loss framework. At the local level, clients optimize personalized models using two tilt parameters (τ for class-level fairness, λ for sample-level robustness) combined with proximal regularization (μ). At the global level, the server aggregates client models using a third tilt parameter (q) that controls client fairness. The framework allows flexible tuning between fairness and robustness by adjusting these hyperparameters, with positive values promoting fairness and negative values enhancing robustness to outliers. The method is evaluated across MNIST, FashionMNIST, and CIFAR10 datasets with non-IID data partitions.

## Key Results
- FedTilt achieves 98.25% accuracy on MNIST with persistent Gaussian noise while maintaining low fairness deviations
- Outperforms state-of-the-art fair FL methods like Ditto and FedAvg on realistic federated datasets
- Successfully balances client fairness (uniform performance across clients) and client data fairness (uniform performance across classes within clients)
- Demonstrates robustness to persistent outliers through negative tilt hyperparameter tuning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Increasing global tilt q towards positive values enforces uniform performance across clients (Client Fairness)
- **Mechanism:** As q → +∞, the global loss approaches maximum distance between any client model and global model. Minimizing this forces the server to prioritize worst-performing clients, equalizing performance across federation.
- **Core assumption:** Client divergence is primarily driven by data heterogeneity rather than malicious noise
- **Evidence anchors:** Abstract mentions achieving client fairness via tuning tilt values; Section III-C shows minimizing max loss ensures client fairness
- **Break condition:** High positive q may overfit global model to bad actors if dataset has extreme outliers

### Mechanism 2
- **Claim:** Increasing local tilt hyperparameters (τ, λ) to positive values enforces uniform performance across data classes within a client
- **Mechanism:** Positive τ forces focus on class with highest loss; positive λ forces focus on hardest samples within that class. This minimizes variance of performance across different data labels inside client.
- **Core assumption:** Local data class imbalance is primary cause of poor performance, and minority classes are not pure noise
- **Evidence anchors:** Abstract mentions achieving client data fairness; Section III-C shows τ > 0, λ > 0 promotes very high client data fairness
- **Break condition:** If specific class is inherently noisy, high positive τ will force model to fit this noise, degrading generalizability

### Mechanism 3
- **Claim:** Decreasing tilt hyperparameters (specifically λ) to negative values mitigates influence of persistent outliers
- **Mechanism:** Negative tilt (e.g., λ → -∞) makes tilted loss approach minimum loss rather than average or maximum. This shifts optimization focus to "easy" or "clean" samples, effectively ignoring high-loss contributions from outliers.
- **Core assumption:** Outliers generate significantly higher loss values than clean data, allowing threshold-based cutoff via tilt mechanism
- **Evidence anchors:** Abstract states FedTilt is robust to persistent outliers via tuning; Section III-C concludes negative λ suppresses effect of outliers in complex datasets
- **Break condition:** If outliers are actually rare but critical edge cases, negative tilting will suppress learning these critical features entirely

## Foundational Learning

- **Concept**: **Tilted Empirical Risk Minimization (TERM)**
  - **Why needed here**: FedTilt is built entirely on TERM. Understanding how tilt t re-weights samples exponentially based on loss values is essential for grasping loss functions (Eq. 3, 10, 11)
  - **Quick check question**: If sample has loss 5.0 and another has 0.5, with tilt t=10, does high-loss sample get significantly more or less weight than in standard averaging?

- **Concept**: **Bi-level Optimization**
  - **Why needed here**: FedTilt solves for global model w and personalized client models v_n simultaneously. Understanding that update of v_n depends on current w (and vice versa) is critical for debugging convergence
  - **Quick check question**: Does global model w depend on personalized models v_n, or on intermediate client models w_n? (Check Algorithm 1)

- **Concept**: **Persistent Outliers vs. Static Outliers**
  - **Why needed here**: Paper specifically claims novelty in handling "persistent" outliers (injected every round). Standard robust aggregation assumes outliers are static or limited in number
  - **Quick check question**: Why would outlier appearing in every communication round break defense assuming outliers are minority class?

## Architecture Onboarding

- **Component map**: Server (Global Model w) -> Client (Personalized Model v_n, Intermediate Model w_n) -> Server
- **Critical path**: 
  1. Server Broadcast: Send global w^(t-1) to selected clients
  2. Client Update: Clients compute gradients for intermediate w_n (using local data tilt) and personalized v_n (using regularization relative to global model)
  3. Tilted Aggregation: Server receives w_n and updates global w by minimizing tilted global loss (Eq. 10), down-weighting or up-weighting clients based on divergence (tilt q)
- **Design tradeoffs**:
  - Fairness vs. Robustness: High positive tilts (τ, λ) favor fairness but reduce robustness to noise. Negative tilts favor robustness but reduce fairness for minority groups with naturally high loss
  - Global vs. Local: Regularization term μ controls how close personalized models stay to global model. High μ = more uniformity, less personalization
- **Failure signatures**:
  - Divergence on Noisy Data: If using standard FedAvg or Ditto on noisy CIFAR10, loss may never converge
  - Over-constraining: If tilt values (q, τ) are set too high without proper step-size adjustment, gradient descent may oscillate or explode
- **First 3 experiments**:
  1. Baseline Reproduction (Clean Data): Run FedAvg and FedTilt (q=0, τ=0, λ=0) on MNIST to verify FedTilt reduces to FedAvg as per Proposition 1
  2. Sensitivity Analysis (Tilts): On Non-IID split (e.g., only 2 classes per client), sweep τ and λ from -1 to +1. Plot Test Accuracy vs. "Client Data Fairness" (μσ) to observe tradeoff
  3. Robustness Stress Test: Inject 30% random pixel corruption into CIFAR10 training data. Compare FedTilt (using negative λ) vs. Ditto to verify if FedTilt maintains accuracy while Ditto collapses

## Open Questions the Paper Calls Out

- **Open Question 1**: Can FedTilt be extended to federated learning on graph-structured data while maintaining convergence guarantees?
  - **Basis in paper**: [explicit] Conclusion states future work includes extending method to federated learning on graph data
  - **Why unresolved**: Current theoretical analysis and empirical evaluation rely on image datasets and standard MLP/CNN architectures
  - **What evidence would resolve it**: Theoretical convergence bounds for GNNs within FedTilt framework and empirical benchmarks on graph datasets

- **Open Question 2**: How robust is FedTilt against sophisticated optimization-based poisoning attacks?
  - **Basis in paper**: [explicit] Conclusion identifies investigating robustness against stronger attacks as future work
  - **Why unresolved**: Paper currently evaluates robustness against unintentional outliers rather than adaptive adversaries
  - **What evidence would resolve it**: Performance metrics when subjected to state-of-the-art optimization-based poisoning attacks

- **Open Question 3**: Can model ownership protection strategies, such as watermarking, be integrated into FedTilt without degrading fairness or accuracy?
  - **Basis in paper**: [explicit] Conclusion lists exploring model ownership protection strategies as future research avenue
  - **Why unresolved**: Watermarking techniques may conflict with tilted loss objectives or regularization terms used to ensure fairness
  - **What evidence would resolve it**: Modified FedTilt framework that successfully embeds robust watermarks while maintaining comparable accuracy and fairness metrics

## Limitations
- Theoretical analysis is limited to strongly convex and smooth loss functions, not covering deep neural networks used in experiments
- Convergence proof relies on specific assumptions about data heterogeneity and gradient boundedness that may not hold in realistic federated settings
- Experimental evaluation focuses on synthetic noise injection rather than real-world data bias or corruption scenarios

## Confidence
- **High Confidence**: Core mechanism of using tilted losses for fairness is well-grounded in TERM literature and mathematical formulation is sound
- **Medium Confidence**: Robustness claims are supported by experiments but theoretical justification for negative tilts is less rigorous than for positive tilts
- **Medium Confidence**: Experimental results showing trade-offs between fairness, robustness, and accuracy are convincing, though evaluation could benefit from more diverse datasets

## Next Checks
1. **Convergence Analysis Extension**: Test FedTilt on non-convex deep learning tasks beyond current experiments to validate if convergence properties hold in practice
2. **Real-World Bias Evaluation**: Apply FedTilt to datasets with documented demographic biases (e.g., medical imaging with demographic disparities) to assess fairness improvements in realistic scenarios
3. **Sensitivity to Hyperparameter Tuning**: Conduct comprehensive ablation study on how different combinations of tilt values (q, τ, λ) affect fairness-accuracy-robustness trade-off across multiple datasets