---
ver: rpa2
title: 'Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations'
arxiv_id: '2512.19456'
source_url: https://arxiv.org/abs/2512.19456
tags:
- essay
- scoring
- prompt
- different
- activations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether intermediate-layer activations of
  large language models (LLMs) can serve as discriminative features for cross-prompt
  automated essay scoring (AES), addressing the challenge of prompt-dependent scoring
  variability. Instead of using only final model outputs, the authors fit linear and
  nonlinear probes to LLM activations and analyze the discriminative power of these
  representations across different models, input content types, and traits.
---

# Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations

## Quick Facts
- arXiv ID: 2512.19456
- Source URL: https://arxiv.org/abs/2512.19456
- Authors: Jinwei Chi; Ke Wang; Yu Chen; Xuanye Lin; Qiang Xu
- Reference count: 10
- Primary result: Linear probes on LLM activations achieve 0.654 QWK cross-prompt scoring, outperforming nonlinear probes and state-of-the-art baselines

## Executive Summary
This paper investigates whether intermediate-layer activations of large language models (LLMs) can serve as discriminative features for cross-prompt automated essay scoring (AES), addressing the challenge of prompt-dependent scoring variability. Instead of using only final model outputs, the authors fit linear and nonlinear probes to LLM activations and analyze the discriminative power of these representations across different models, input content types, and traits. They also compute and compare directional representations of essays to understand how LLMs adapt evaluation perspectives across prompts and traits. Results show that linear probes achieve strong cross-prompt scoring performance (e.g., 0.654 QWK with Qwen3-8B, 9.55% improvement over state-of-the-art), outperforming nonlinear probes and demonstrating that essay quality is largely linearly encoded. Sensitivity analysis reveals that instructions have more impact than prompts on evaluation, and directional analysis shows LLMs adjust scoring perspectives based on essay type and trait, indicating flexible adaptation to diverse scoring criteria.

## Method Summary
The method extracts activation vectors from individual attention heads at the final token position of frozen LLMs, then fits ridge regression probes (λ=0.01) to predict essay trait scores. The authors train one probe per head per trait, select the best-performing head on held-out prompts, and evaluate cross-prompt generalization using prompt-wise cross-validation. They also compute directional representations by taking weighted differences of activation centroids across score levels to analyze how LLMs adapt evaluation perspectives. The input template concatenates prompt description, essay text, and trait-specific instruction to guide evaluation.

## Key Results
- Linear probes achieve 0.654 QWK cross-prompt scoring with Qwen3-8B, outperforming nonlinear probes (MLP) and PAES baseline
- Middle-layer attention heads (layers 10-16) show stronger discriminative power than lower or upper-middle layers
- Removing instructions causes larger QWK drops than removing prompts, indicating instructions have more impact on evaluation
- Cross-prompt direction cosine similarity ranges from 0.6 to 0.9+, showing LLMs adjust scoring perspectives based on prompt-trait combinations

## Why This Works (Mechanism)

### Mechanism 1: Linear Encoding of Essay Quality Traits
- Claim: Essay quality traits are largely linearly encoded in LLM activation space, allowing simple linear probes to extract scoring-relevant information.
- Mechanism: Each trait dimension (e.g., Content, Organization, Sentence Fluency) corresponds to a direction in the activation space of specific attention heads. A linear combination of activation dimensions can predict trait scores, where the weight vector captures the "quality direction" for that trait.
- Core assumption: The linear hypothesis from representation engineering (concepts as directions) extends to complex, domain-specific constructs like writing quality.
- Evidence anchors:
  - [abstract] "Results show that linear probes achieve strong cross-prompt scoring performance... outperforming nonlinear probes and demonstrating that essay quality is largely linearly encoded."
  - [section: Results and Discussion - Performance Under Different Probes] Tables 4 and 5 show Ridge regression consistently outperforms MLP probes across all three tested models, with MLP often dropping below PAES baseline on traits like Organization and Word Choice.
  - [corpus] Related work on linear representations (Gurnee & Tegmark 2023, Hollinsworth et al. 2024) demonstrates sentiment and abstract concepts are linearly represented; this paper extends that finding to AES-specific traits.
- Break condition: If nonlinear probes significantly outperform linear probes on a new trait or prompt type, the linear encoding assumption may not hold for that dimension.

### Mechanism 2: Attention Head Specialization for Trait Discrimination
- Claim: Different attention heads encode distinct evaluation perspectives, enabling fine-grained trait discrimination without explicit trait-specific training.
- Mechanism: Attention heads at different layers specialize in different linguistic and semantic features. Middle layers (approximately layers 10-16 in 32-layer models) show stronger discriminative power for essay traits compared to lower (1-5) or upper-middle (21-25) layers. Each head's probe weight vector captures its specialized evaluation criterion.
- Core assumption: Pre-training on diverse text corpora induces head-level functional specialization that transfers to essay evaluation without task-specific fine-tuning.
- Evidence anchors:
  - [section: Visualization] Figure 2c heatmaps show most attention heads can distinguish traits, with middle layers exhibiting stronger discriminative ability.
  - [section: Trait-Specific Preferences of Heads] Figure 3 demonstrates that different heads assign different scores to the same words, indicating varied evaluation perspectives.
  - [corpus] No direct corpus evidence for AES-specific head specialization; existing probing literature focuses on generic concepts rather than domain-specific task decomposition.
- Break condition: If head-level probe performance shows no variation across layers or heads (uniform QWK distribution), specialization is not the driver.

### Mechanism 3: Instruction-Guided Contextualization of Evaluation Perspectives
- Claim: LLMs dynamically adjust their evaluation perspectives based on input context, with instructions exerting stronger influence than essay prompts on activation discriminability.
- Mechanism: The input template (prompt + essay + instruction) conditions the model's internal state. Instructions specifying the evaluation trait (e.g., "evaluate for Organization") steer the activation space toward trait-relevant dimensions. Removing instructions causes greater performance decay than removing prompts, suggesting instructions gate trait-specific feature extraction.
- Core assumption: Instruction-following behavior trained into chat/instruct models generalizes to producing trait-specific activations, not just output text.
- Evidence anchors:
  - [section: Sensitivity to Input Content] Tables 6 and 7 show removing instructions ("w/o i") causes larger QWK drops than removing prompts ("w/o p") across all traits; removing both drops average QWK from 0.652 to 0.527.
  - [abstract] "Sensitivity analysis reveals that instructions have more impact than prompts on evaluation."
  - [corpus] No corpus papers directly address instruction effects on internal activations for AES; this appears to be a novel finding in this work.
- Break condition: If performance remains unchanged when instructions are removed (essay-only input matches full input), the instruction-guided mechanism is not operative.

## Foundational Learning

- **Linear Probing and Representation Geometry**
  - Why needed here: The entire method rests on fitting linear classifiers to activations. Without understanding why linear probes work (and when they fail), you cannot interpret the results or debug failures.
  - Quick check question: Given activations X ∈ R^(n×d) and labels y ∈ R^n, write the closed-form ridge regression solution and explain why L2 regularization (λ=0.01) matters when d > n.

- **Attention Head Outputs and Residual Stream**
  - Why needed here: Activations are extracted from individual attention heads at the final token position. You need to understand what this vector represents and why the final token aggregates sequence information.
  - Quick check question: In a transformer with L layers and H heads per layer, how many separate probes must be trained? Why might the final token position capture global essay semantics?

- **Cross-Prompt Evaluation Protocol (Prompt-Wise CV)**
  - Why needed here: The core claim is cross-prompt generalization. Understanding the evaluation setup is critical for interpreting whether results actually demonstrate generalization or leakage.
  - Quick check question: In prompt-wise cross-validation with 8 prompts, how many training/testing splits are evaluated? Why is this harder than within-prompt scoring?

## Architecture Onboarding

- **Component map:**
  - Input Template -> Forward pass through frozen LLM -> Activation extraction from attention heads -> Ridge regression probe training -> Head selection on test set -> Directional analysis for interpretability

- **Critical path:**
  1. Tokenize input with full template → 2. Forward pass to extract head activations → 3. Train one probe per head on source prompts → 4. Select head with highest test QWK on held-out prompt → 5. (Optional) Compute directional vectors for interpretability

- **Design tradeoffs:**
  - Head selection on test set: The paper acknowledges this inflates reported performance; results demonstrate discriminative power but do not establish superiority over baselines in deployment.
  - Linear vs. nonlinear probes: Linear enables interpretability (directional analysis) but may underfit complex trait interactions; MLP probes underperform suggesting linear is sufficient but not necessarily optimal.
  - Model selection vs. size: Qwen3-4B (4B params) matches DeepSeek-R1-Distill-Llama-8B (8B params), suggesting architecture and training matter more than scale.

- **Failure signatures:**
  - Low QWK on Prompt 7 (~0.35-0.57 across models): Prompt 7 essays may be outliers or fundamentally different; removing P7 from training improves other prompt scores.
  - MLP probe collapse on traits like Organization and Word Choice (dropping to 0.29-0.39): Suggests these traits have weaker linear structure or require more data.
  - Large variance in cross-prompt direction cosine similarity (0.6 to 0.9+): Indicates inconsistent generalization; some prompt-trait combinations may not transfer.

- **First 3 experiments:**
  1. **Reproduce single-head probing on one model (Llama-2-7b-chat-hf) with prompt-wise CV:** Extract activations, train Ridge probes per head, verify you can replicate the ~0.65 average QWK baseline before testing other models.
  2. **Ablate input components (prompt/instruction/both) on one prompt-trait pair:** Confirm the instruction > prompt importance finding on your infrastructure; this validates the full pipeline.
  3. **Test zero-shot generalization to a held-out prompt not in ASAP/ASAP++:** This probes whether the method works on genuinely unseen prompt types, a key claim not tested in the paper.

## Open Questions the Paper Calls Out
None

## Limitations
- Head selection on test set inflates performance metrics and makes comparisons to baselines unfair
- Cross-prompt generalization claims may not hold for genuinely unseen prompt types given limited prompt diversity
- Instruction effect interpretation lacks corpus validation for the mechanism by which instructions condition activation space

## Confidence

- **High confidence**: Linear encoding of essay quality traits (supported by consistent Ridge probe superiority over MLP across models and traits)
- **Medium confidence**: Head specialization for trait discrimination (heatmap evidence is suggestive but not conclusive)
- **Medium confidence**: Instruction-guided contextualization of evaluation perspectives (novel finding with strong sensitivity analysis but limited corpus support)

## Next Checks
1. **Head selection ablation**: Retrain all models with fixed head selection (e.g., always use head 17 at layer 12) rather than best-head-on-test-set to establish fair baseline performance and assess true generalization capability.

2. **Zero-shot prompt transfer**: Evaluate the method on a held-out prompt from a different dataset (e.g., not in ASAP/ASAP++) to test whether cross-prompt generalization holds for genuinely unseen prompt types.

3. **Instruction content analysis**: Systematically vary instruction content (e.g., different wording for the same trait) to determine whether instructions act as semantic gates or merely trigger known patterns in the model.