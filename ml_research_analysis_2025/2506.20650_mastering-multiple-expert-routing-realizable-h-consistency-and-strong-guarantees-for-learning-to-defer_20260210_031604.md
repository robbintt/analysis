---
ver: rpa2
title: 'Mastering Multiple-Expert Routing: Realizable $H$-Consistency and Strong Guarantees
  for Learning to Defer'
arxiv_id: '2506.20650'
source_url: https://arxiv.org/abs/2506.20650
tags:
- loss
- surrogate
- deferral
- learning
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning to defer with multiple
  experts, where input instances must be optimally assigned to experts to balance
  accuracy and computational cost. The authors introduce novel surrogate loss functions
  and efficient algorithms with strong theoretical guarantees for both single-stage
  (jointly learning predictor and deferral function) and two-stage (learning only
  the deferral function with a fixed expert) learning scenarios.
---

# Mastering Multiple-Expert Routing: Realizable $H$-Consistency and Strong Guarantees for Learning to Defer

## Quick Facts
- arXiv ID: 2506.20650
- Source URL: https://arxiv.org/abs/2506.20650
- Authors: Anqi Mao; Mehryar Mohri; Yutao Zhong
- Reference count: 40
- Primary result: Novel surrogate losses achieve realizable H-consistency for learning to defer with multiple experts, outperforming baselines in synthetic realizable settings while matching state-of-the-art in non-realizable scenarios.

## Executive Summary
This paper addresses the problem of learning to defer with multiple experts, where input instances must be optimally assigned to experts to balance accuracy and computational cost. The authors introduce novel surrogate loss functions and efficient algorithms with strong theoretical guarantees for both single-stage (jointly learning predictor and deferral function) and two-stage (learning only the deferral function with a fixed expert) learning scenarios. For single-stage deferral, they introduce a family of new realizable H-consistent surrogate losses and prove H-consistency for a selected member. For two-stage deferral, they derive new surrogate losses that achieve realizable H-consistency, H-consistency bounds, and Bayes-consistency for both two-expert and multiple-expert scenarios under natural assumptions.

## Method Summary
The method introduces novel surrogate loss functions for learning to defer with multiple experts. In single-stage deferral, a family of realizable H-consistent surrogate losses $L_\Psi$ is proposed, where the auxiliary function $\Psi$ must satisfy specific boundary conditions. The specific loss $L_{mae}$ (Mean Absolute Error) provides non-asymptotic bounds linking surrogate error to target deferral loss. For two-stage deferral, cost-sensitive surrogate losses $L_\Phi$ are derived, generalizing binary and multi-class cost-sensitive classification. The approach uses ResNet-16 architectures for both predictor and deferral functions, trained with Adam optimizer (weight decay $1 \times 10^{-3}$, batch size 1024, 200 epochs) on CIFAR-10, CIFAR-100, SVHN, and Tiny ImageNet datasets.

## Key Results
- Proposed surrogate losses match state-of-the-art performance in non-realizable settings while outperforming existing methods in realizable settings
- Achieved system accuracy of 91.98-96.08% across benchmark datasets with different deferral behaviors
- In realizable synthetic settings, method achieved close to 100% system accuracy compared to baselines failing to find near-zero-error solutions
- Provided enhanced theoretical guarantees under low-noise assumptions for both single-stage and two-stage scenarios

## Why This Works (Mechanism)

### Mechanism 1: Realizable Consistency via Constrained Comp-Sum Losses
The proposed surrogate loss family ($L_\Psi$) is designed to guarantee that if a perfect solution exists within the hypothesis set, minimizing this surrogate will converge to it. Standard cross-entropy extensions often fail in multiple-expert scenarios because they do not penalize incorrect deferral paths sufficiently when a perfect path exists. The "comp-sum" formulation requires the auxiliary function $\Psi$ to satisfy specific boundary limits ($\lim_{u\to 0^+} \Psi(u)=1$ and $\lim_{u\to 1^-} \Psi(u)=0$), ensuring that scaling the weights of a correct hypothesis drives the loss to zero, aligning the surrogate minimum with the target deferral loss minimum.

### Mechanism 2: Generalization Guarantees via H-Consistency Bounds
The specific surrogate loss $L_{mae}$ provides a non-asymptotic bound linking the estimation error of the surrogate to the target deferral loss, ensuring validity across arbitrary distributions. The paper derives an H-consistency bound stating that the generalization error of the deferral loss is bounded by a linear function of the surrogate error. Unlike Bayes-consistency (which applies only to infinite data/function classes), this bound accounts for the specific hypothesis set $H$ (e.g., neural networks), ensuring that optimizing the surrogate improves the target metric even with limited model capacity.

### Mechanism 3: Two-Stage Routing with Cost-Sensitive Surrogates
In settings where the predictor is fixed (two-stage), the proposed surrogate $L_\Phi$ enables learning a router that optimally selects experts by generalizing cost-sensitive binary/multi-class losses. The method treats the deferral decision as a cost-sensitive classification problem over experts. By deriving a surrogate that admits an R-consistency bound, the router learns to select the expert (or the fixed predictor) that minimizes the specific instance cost, avoiding the "realizability gap" of prior methods that failed to account for classification-error-based costs.

## Foundational Learning

**Concept: Realizable vs. Agnostic Learning**
Why needed here: The paper distinguishes between performance when a perfect solution exists (realizable) vs. noisy data. Understanding this explains why the method outperforms baselines in realizable synthetic tests.
Quick check question: Does the system assume there is always a perfect expert for every input, or must it handle uncertainty?

**Concept: Consistency (Bayes vs. H-Consistency)**
Why needed here: This is the core theoretical contribution. You must understand that Bayes-consistency guarantees correctness with infinite data, while H-consistency guarantees correctness relative to the specific model architecture used.
Quick check question: If a loss is Bayes-consistent but not H-consistent, does minimizing the loss guarantee the best performance within a fixed neural network class?

**Concept: Surrogate Loss Functions**
Why needed here: The deferral loss (0-1 style) is non-differentiable. The mechanism relies on convex/smooth approximations (surrogates) to train the model via gradient descent.
Quick check question: Why can't we directly minimize the classification error of the system during backpropagation?

## Architecture Onboarding

**Component map:** Input $x$ -> Shared Backbone (ResNet-16) -> Predictor Head (outputs logits for $n$ classes) + Deferral Head (outputs logits for $n_e$ experts) -> Loss Layer (computes $L_{mae}$ or $L_\Psi$ combining predictor softmax and expert costs)

**Critical path:** The implementation of the surrogate loss function (Eq. 4 in text). It is not a standard cross-entropy. It requires calculating the sum of costs and the specific "comp-sum" structure over predictor probabilities and expert scores.

**Design tradeoffs:**
- **MAE ($q=1$) vs. Generalized CE ($q<1$):** The paper suggests MAE provides linear H-consistency bounds (stronger guarantee) but notes that standard logistic losses (baselines) are often optimized for non-realizable settings. The trade-off is theoretical robustness vs. empirical optimization smoothness.
- **Single-stage vs. Two-stage:** Single-stage learns predictor and router jointly (complex, end-to-end). Two-stage assumes a fixed predictor (simpler, applicable to fixed LLMs/classifiers).

**Failure signatures:**
- **Underfitting in Realizable Settings:** If using standard baselines (Verma et al., Mao et al. 2024a), the model may fail to reach near-zero error on synthetic data because the loss landscape doesn't align with the zero-cost solution.
- **Deferral Collapse:** The router might over-defer or under-defer if the cost weightings in the loss formulation are not scaled correctly against the predictor's loss.

**First 3 experiments:**
1. **Realizable Synthetic Test:** Train on the Mixture-of-Gaussians dataset (Section 6, Figure 1). Success is indicated by the system accuracy approaching 100% as sample size increases, while baselines flatline.
2. **Expert Ablation:** Run on CIFAR-10/100 with the "Simulated Expert" setup (Expert 1 correct on classes 1-3, Expert 2 on 4-6, etc.). Measure System Accuracy and Deferral Ratio to verify the router learns the specific domains of expertise.
3. **Two-Stage Validation:** Fix a pre-trained predictor (e.g., standard ResNet) and train *only* the router using the Two-Stage loss (Eq. 7). Compare against the single-stage setup to quantify the trade-off in flexibility vs. stability.

## Open Questions the Paper Calls Out

**Open Question 1:** Can the proposed surrogate loss framework be adapted to handle previously unseen experts at test time? The authors state in Section 7 that "It would be interesting to adapt our framework to handle previously unseen experts at test time, as studied by Tailor et al. (2024)." This is unresolved because the current theoretical analysis and algorithms assume a fixed set of $n_e$ pre-defined experts available during training.

**Open Question 2:** How do the proposed methods perform in real-world settings where expert domains overlap and data is heterogeneous? Section 7 notes the authors plan to "dedicate future work to conducting a more extensive empirical analysis in real-world settings, particularly in important scenarios where expert domains overlap and the data is heterogeneous." This is unresolved because the current experiments rely on simulated experts with distinct, non-overlapping domains of expertise.

**Open Question 3:** Can the learning-to-defer guarantees be maintained in scenarios where expert predictions are unavailable during training? The authors acknowledge in Section 7 that extending methods to "scenarios where expert predictions are unavailable during training" is a "promising direction for future work." This is unresolved because the current loss functions rely on access to expert predictions $g_j(x)$ and costs $c_j(x,y)$ during the training process.

**Open Question 4:** Does realizable R-consistency for two-stage multiple-expert deferral hold without the assumption that at most one expert has zero cost per instance? Theorem 4.3 proves realizable R-consistency under the specific assumption that "for any $(x, y)$, there is at most one expert $j^*$ for which $c_{j^*}(x, y) = 0$." This is unresolved because this is a restrictive "single-expert perfection" assumption.

## Limitations
- Theoretical guarantees rely on strong assumptions about perfect solutions existing within the hypothesis set and closure under scaling
- Experimental validation focuses on synthetic and relatively simple vision datasets with limited testing on complex real-world expert scenarios
- Performance guarantees may degrade under realistic noise levels and when expert costs are not perfectly correlated with classification accuracy

## Confidence
**High Confidence:**
- The surrogate loss formulation and its mathematical properties are well-defined
- The distinction between realizable and agnostic settings is clearly established
- The experimental methodology and dataset choices are appropriate for the claims

**Medium Confidence:**
- The theoretical bounds for H-consistency in the two-stage setting
- The empirical superiority in realizable settings over baselines
- The generalization of results to complex expert systems beyond the tested scenarios

**Low Confidence:**
- The performance guarantees under realistic noise levels
- The scalability of the approach to hundreds of experts
- The robustness of the method when expert costs are not perfectly correlated with classification accuracy

## Next Checks
1. **Realizable Setting Verification**: Implement the Mixture-of-Gaussians experiment (Section 6, Figure 1) to verify that the proposed method achieves near-100% accuracy while baselines fail to converge to zero-error solutions.

2. **Two-Stage Transferability Test**: Fix a pre-trained ResNet-18 on CIFAR-10, then train only the router using the two-stage loss. Compare system accuracy and deferral behavior against the single-stage approach to quantify the trade-off.

3. **Expert Cost Sensitivity Analysis**: Modify the expert cost function to include latency-based costs uncorrelated with accuracy (e.g., constant cost for all predictions). Evaluate whether the method still learns meaningful deferral strategies or collapses to random routing.