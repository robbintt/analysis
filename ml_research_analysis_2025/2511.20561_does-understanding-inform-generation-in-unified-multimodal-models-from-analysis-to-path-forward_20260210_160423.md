---
ver: rpa2
title: Does Understanding Inform Generation in Unified Multimodal Models? From Analysis
  to Path Forward
arxiv_id: '2511.20561'
source_url: https://arxiv.org/abs/2511.20561
tags:
- generation
- arxiv
- reasoning
- symbol
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UniSandbox, a decoupled evaluation framework
  with synthetic, leak-proof datasets to analyze the understanding-generation gap
  in unified multimodal models. The study reveals that most open-source models fail
  on reasoning generation tasks, scoring near zero without Chain-of-Thought (CoT),
  indicating generation degenerates into shallow keyword mapping.
---

# Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward

## Quick Facts
- arXiv ID: 2511.20561
- Source URL: https://arxiv.org/abs/2511.20561
- Authors: Yuwei Niu; Weiyang Jin; Jiaqi Liao; Chaoran Feng; Peng Jin; Bin Lin; Zongjian Li; Bin Zhu; Weihao Yu; Li Yuan
- Reference count: 40
- Most open-source unified multimodal models fail reasoning generation tasks without Chain-of-Thought (CoT), scoring near zero.

## Executive Summary
This paper introduces UniSandbox, a decoupled evaluation framework with synthetic, leak-proof datasets to analyze the understanding-generation gap in unified multimodal models. The study reveals that most open-source models fail on reasoning generation tasks, scoring near zero without Chain-of-Thought (CoT), indicating generation degenerates into shallow keyword mapping. CoT effectively bridges this gap, and self-training can internalize reasoning abilities. For knowledge transfer, models struggle to utilize newly injected knowledge, but CoT improves performance. Query-based architectures show latent CoT-like properties that aid knowledge transfer. The work provides insights for designing architectures and training strategies that truly synergize understanding and generation.

## Method Summary
The authors developed UniSandbox, a decoupled evaluation framework with synthetic datasets for knowledge retrieval and reasoning generation tasks. They systematically evaluated unified multimodal models with and without Chain-of-Thought (CoT) prompting across multiple model families. The STARS (Self-Training with Rejection Sampling) framework was introduced to internalize reasoning capabilities through self-training. Query-based architectures were analyzed to understand their inherent CoT-like properties. Performance was measured across mathematical operations, symbolic mapping, and knowledge transfer tasks.

## Key Results
- Most open-source models score near zero on reasoning generation tasks without CoT, showing generation degenerates into shallow keyword mapping
- CoT improves reasoning generation performance by over 10x, effectively bridging the understanding-generation gap
- Self-training with rejection sampling (STARS) can internalize reasoning abilities, with curriculum learning showing +0.25 average improvement over direct training
- Query-based architectures exhibit latent CoT-like properties that facilitate knowledge transfer from understanding to generation modules

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-Thought (CoT) serves as an explicit bridge that activates latent language priors to guide visual generation in reasoning tasks.
- Mechanism: CoT compels the model to first articulate reasoning steps and derive a conclusion within language space, then uses this explicit outcome as the generative instruction. This transforms complex logical problems into straightforward text-to-image tasks.
- Core assumption: Unified models possess latent reasoning capabilities that cannot be accessed during generation without explicit prompting scaffolds.
- Evidence anchors:
  - [abstract] "explicit Chain-of-Thought (CoT) in the understanding module effectively bridges the gap"
  - [section 3.3] BAGEL performance surged from 0.0283 to 0.5100 with CoT; closed-source models with integrated CoT-like mechanisms showed superior performance
  - [corpus] RealUnify benchmark confirms unified models often lack genuine synergetic interaction between understanding and generation components

### Mechanism 2
- Claim: Self-training with rejection sampling (STARS) can internalize explicit CoT reasoning into implicit model capabilities through distillation.
- Mechanism: Three-stage process: (1) CoT mode generates training samples as "teacher" signals; (2) model's understanding module serves as "verifier" to filter high-quality outputs via consistency scoring; (3) fine-tuning distills reasoning into parameters, enabling implicit reasoning.
- Core assumption: The model's understanding module can accurately assess semantic consistency between generated images and textual instructions—a capability requiring inherent self-verification.
- Evidence anchors:
  - [section 3.4] STARS framework description with equations (1)-(3) formalizing the approach
  - [section 3.4.1, Table 2] Normal mode performance improved from near-zero to 0.29 (Math1), with cross-difficulty generalization observed
  - [corpus] Information-Theoretic Criteria paper provides theoretical grounding for knowledge distillation across modalities

### Mechanism 3
- Claim: Query-based architectures inherently exhibit latent CoT-like properties that facilitate knowledge transfer from understanding to generation modules.
- Mechanism: Queries progressively extract information—related attributes emerge in intermediate queries while target knowledge peaks in later queries—creating implicit sequential reasoning without explicit CoT.
- Core assumption: The query extraction mechanism creates an emergent information flow pattern analogous to step-by-step reasoning chains.
- Evidence anchors:
  - [abstract] "query-based architectures inherently exhibit latent CoT-like properties that affect this transfer"
  - [section 4.4, Figure 4] Visualization shows target knowledge probability peaks at final query (probability ~0.7-0.8) while related attributes appear in intermediate queries
  - [corpus] Weak external evidence—no corpus papers directly validate query-based latent reasoning patterns

## Foundational Learning

- Concept