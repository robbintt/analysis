---
ver: rpa2
title: 'A Hybrid CNN-VSSM model for Multi-View, Multi-Task Mammography Analysis: Robust
  Diagnosis with Attention-Based Fusion'
arxiv_id: '2507.16955'
source_url: https://arxiv.org/abs/2507.16955
tags:
- bi-rads
- hybrid
- learning
- multi-task
- vssm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of early and accurate breast
  cancer detection from mammography images, which remains difficult due to subtle
  imaging findings and high inter-observer variability. The proposed method introduces
  a hybrid CNN-VSSM architecture that combines convolutional neural networks for local
  feature extraction with Visual State Space Models (VSSMs) for global context modeling,
  integrated within a multi-view, multi-task framework that processes all four standard
  mammography views and jointly predicts diagnostic labels and BI-RADS scores.
---

# A Hybrid CNN-VSSM model for Multi-View, Multi-Task Mammography Analysis: Robust Diagnosis with Attention-Based Fusion

## Quick Facts
- arXiv ID: 2507.16955
- Source URL: https://arxiv.org/abs/2507.16955
- Authors: Yalda Zafari; Roaa Elalfy; Mohamed Mabrok; Somaya Al-Maadeed; Tamer Khattab; Essam A. Rashed
- Reference count: 35
- One-line primary result: Hybrid CNN-VSSM with attention fusion achieves AUC 0.9967 and F1 0.9830 on binary BI-RADS classification

## Executive Summary
This study addresses the challenge of early and accurate breast cancer detection from mammography images, which remains difficult due to subtle imaging findings and high inter-observer variability. The proposed method introduces a hybrid CNN-VSSM architecture that combines convolutional neural networks for local feature extraction with Visual State Space Models (VSSMs) for global context modeling, integrated within a multi-view, multi-task framework that processes all four standard mammography views and jointly predicts diagnostic labels and BI-RADS scores. A gated attention-based fusion module dynamically weights information across views to handle missing data and improve robustness. The model is evaluated on the TOMPEI-CMMD dataset across three diagnostic tasks of increasing complexity.

## Method Summary
The proposed method processes four mammography views (L-CC, L-MLO, R-CC, R-MLO) through a hybrid CNN-VSSM architecture that combines a pretrained ResNet-18 for local feature extraction with VSSM blocks for global context modeling. A gated attention fusion module dynamically weights view features based on global context, and the network jointly predicts diagnostic labels and BI-RADS scores through two parallel classification heads. The model is trained using a composite loss function with class-weighted cross-entropy and optimized with AdamW, incorporating dropout, gradient clipping, and data augmentation.

## Key Results
- Binary BI-RADS 1 vs. 5 classification: AUC 0.9967, F1 0.9830
- Ternary classification (BI-RADS 1 vs. 3 vs. 5): F1 score 0.7790
- Five-class BI-RADS task: Best F1 score 0.4904, demonstrating difficulty with fine-grained distinctions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Cascading a CNN with a Visual State Space Model (VSSM) creates a representation that preserves local texture while capturing global anatomical context more efficiently than Transformers.
- **Mechanism**: The architecture utilizes a pretrained ResNet to extract localized feature maps (edges, microcalcifications). These maps are then patched and fed into VSSM blocks equipped with 2D Selective Scan (SS2D). Unlike attention mechanisms with quadratic complexity, SS2D scans features along four directional paths with linear complexity, aggregating global context without losing the spatial inductive biases established by the CNN.
- **Core assumption**: The sequential scanning of flattened visual states in VSSM can effectively model the complex, irregular geometry of breast tissue and lesion dependencies without the explicit pairwise comparison offered by self-attention.
- **Evidence anchors**:
  - [abstract] "...combining convolutional encoders for rich local feature extraction with Visual State Space Models (VSSMs) to capture global contextual dependencies."
  - [Section 2.1.3] "The model follows a cascaded structure... ResNet-18... serves as an efficient and robust feature encoder... passed into the subsequent stages of the VSSM."
  - [corpus] Weak direct validation for CNN-VSSM in mammography specifically; corpus focuses on Transformers (arXiv:2503.13309) or report generation (arXiv:2508.09225), suggesting this hybrid approach is a novel deviation from standard Vision Transformers.
- **Break condition**: If lesions require strictly local, high-frequency edge detection that gets smoothed out during the global state-space scanning sequence, the VSSM component may dilute the CNN's signal.

### Mechanism 2
- **Claim**: Gated attention fusion enables the model to dynamically prioritize the most informative mammographic views while remaining robust to missing data.
- **Mechanism**: The model extracts feature vectors from all four views (L-CC, L-MLO, R-CC, R-MLO). It concatenates these into a global context vector, which passes through an MLP to generate softmax-normalized attention weights ($\alpha$). These weights scale the feature vectors before final fusion, theoretically assigning lower weights to uninformative or missing views (which are zeroed out).
- **Core assumption**: The "global context" vector contains sufficient information to determine the relative importance of a specific view before the final classification decision is made.
- **Evidence anchors**:
  - [abstract] "...gated attention-based fusion module that dynamically weights information across views, effectively handling cases with missing data."
  - [Section 2.2] "This attention-weighted concatenation preserves the full feature space while enabling data-driven, case-specific fusion."
  - [corpus] Corpus neighbor arXiv:2509.10344 (GLAM) highlights the difficulty of multi-view alignment, reinforcing that simple fusion often fails, justifying the need for attention-based weighting.
- **Break condition**: If two views contain contradictory but equally critical diagnostic cues (e.g., one shows mass, the other shows distortion), the softmax normalization might suppress the signal of the "minority" feature in favor of the dominant view.

### Mechanism 3
- **Claim**: Joint training on both diagnostic labels (Normal/Malignant) and BI-RADS scores (1–5) acts as a form of implicit regularization, improving feature robustness.
- **Mechanism**: The network optimizes a composite loss function $L_{total} = 0.5 \cdot L_{label} + 0.5 \cdot L_{birads}$. By forcing the shared backbone to satisfy two distinct but clinically related classification objectives simultaneously, the model learns more generalizable features that capture the underlying pathology rather than dataset-specific noise.
- **Core assumption**: The mapping from image features to "Malignant" status shares significant overlap with the mapping to high BI-RADS scores, and gradients from one task will reinforce rather than destabilize the other.
- **Evidence anchors**:
  - [Section 5] "...multi-task learning can mitigate [overfitting] by acting as an implicit regularizer."
  - [Table 2 & 3] Multi-task models generally outperform single-task models in the binary and ternary experiments.
  - [corpus] arXiv:2511.15968 warns of "destructive task interference" in multi-task learning, suggesting this mechanism relies heavily on the specific task pairing.
- **Break condition**: If the BI-RADS labeling is noisy or inconsistent (e.g., a benign mass labeled BI-RADS 4), the conflicting gradients could degrade the primary malignancy detection performance.

## Foundational Learning

- **Concept**: **State Space Models (SSMs) vs. Transformers**
  - **Why needed here**: The paper positions VSSM as a solution to the quadratic complexity of Vision Transformers. You must understand that SSMs (like Mamba) compress a sequence history into a recurrent state with linear cost, whereas Transformers attend to the entire history at every step.
  - **Quick check question**: How does the computational complexity of the VSSM block scale with the sequence length of image patches compared to a standard Vision Transformer?

- **Concept**: **Multi-View Mammography (CC & MLO)**
  - **Why needed here**: The model fuses four specific views. You need to know that CC (Cranio-Caudal) is top-down while MLO (Medio-Lateral Oblique) captures the pectoral muscle, providing different anatomical contexts.
  - **Quick check question**: Why would a lesion visible in the MLO view potentially be missed or appear differently in the CC view?

- **Concept**: **BI-RADS Lexicon**
  - **Why needed here**: The model predicts scores 1–5. Understanding that these represent a probability of malignancy (1=Normal, 5=Highly suspicious) is crucial for interpreting the "Ternary" and "5-Class" difficulty results.
  - **Quick check question**: Why is distinguishing between BI-RADS 3 (Probably Benign) and BI-RADS 4 (Suspicious) clinically more ambiguous than distinguishing 1 from 5?

## Architecture Onboarding

- **Component map**: Input (4 channels @ 512x512) -> Hybrid CNN-VSSM Backbone -> Gated Attention Fusion -> Dual Classification Heads (Diagnosis + BI-RADS)

- **Critical path**:
  1.  **Shared Encoding**: All 4 images pass through the same CNN-VSSM feature extractor (in the "Shared" model variant which performed best).
  2.  **Global Contextualization**: The VSSM stage scans feature maps horizontally/vertically to aggregate global dependencies.
  3.  **View Weighting**: The Fusion module generates a scalar weight for each view based on the concatenated feature context.
  4.  **Joint Prediction**: The fused vector splits into two heads to predict Malignancy and BI-RADS simultaneously.

- **Design tradeoffs**:
  - **Shared vs. View-Specific**: The "Shared" model forces all views through one encoder (better generalization, handles missing data), while "View-Specific" uses separate encoders for CC/MLO (higher capacity but prone to overfitting).
  - **Speed vs. Context**: Using VSSM allows for global context with linear complexity (faster than Swin Transformers), but may lack the pixel-perfect alignment of quadratic attention mechanisms.

- **Failure signatures**:
  - **Ambiguity Collapse**: Performance drops drastically on the 5-class task (F1 < 0.50), indicating the model cannot resolve fine-grained distinctions between intermediate BI-RADS scores (2, 3, 4).
  - **Overfitting in View-Specific**: The "View-Specific" models often underperform "Shared" models in complex tasks, suggesting they memorize view-specific noise rather than general pathology.

- **First 3 experiments**:
  1.  **Baseline Sanity Check**: Run the Binary task (BI-RADS 1 vs. 5) using the Shared Hybrid model. Success is an AUC > 0.95; failure indicates a data loading or preprocessing pipeline error (as the paper reports near-perfect results here).
  2.  **Missing View Ablation**: Manually zero out the R-MLO view in the validation set and check the attention weights. Verify that the fusion module assigns a near-zero weight to the zero-tensor input and that performance does not collapse.
  3.  **Backbone Dissection**: Compare the "Shared Hybrid" against a "ResNet-only" baseline on the Ternary task. The goal is to confirm that the VSSM component specifically contributes to the lift in F1 score (e.g., from ~0.76 to ~0.78) by analyzing the validation curves.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed Hybrid CNN-VSSM architecture generalize its superior performance to external, multi-center datasets with different imaging protocols and population demographics?
- Basis in paper: [inferred] The study relies exclusively on the TOMPEI-CMMD dataset for training and validation. While the introduction notes that other models have fine-tuned on external datasets, this specific hybrid framework has not yet been evaluated across domain shifts.
- Why unresolved: Models often overfit to the specific noise patterns or class distributions of a single dataset, and the TOMPEI-CMMD dataset has specific class imbalances that may not reflect global screening populations.
- What evidence would resolve it: Reporting AUC and F1-scores after training on TOMPEI-CMMD and testing on independent datasets like VinDr-Mammo or CSAW.

### Open Question 2
- Question: What architectural or optimization strategies are required to improve performance on fine-grained, full-range BI-RADS classification (classes 1–5)?
- Basis in paper: [explicit] The authors highlight that the five-class BI-RADS task proved "most challenging," resulting in low F1-scores (below 0.5). They note this underscores the "inherent difficulty" of distinguishing intermediate categories due to subtle radiological variations.
- Why unresolved: The current hybrid approach, while effective for binary tasks, fails to resolve the high inter-class similarity and intra-class variability present in the full BI-RADS spectrum.
- What evidence would resolve it: Significant improvement in F1-scores for intermediate classes (2, 3, 4) through methods like ordinal regression or hierarchical classification.

### Open Question 3
- Question: Under what conditions does multi-task learning become detrimental rather than beneficial in this framework?
- Basis in paper: [explicit] The paper observes that while multi-task learning improved binary and ternary results, its benefits were "less consistent" in the five-class task, suggesting auxiliary binary tasks may offer "limited benefit" when target distinctions are fine-grained.
- Why unresolved: The mechanism causing this performance ceiling—whether due to gradient interference or insufficient model capacity for conflicting objectives—is not fully isolated.
- What evidence would resolve it: An ablation study analyzing task-specific gradient directions or comparing performance using dynamic task-weighting strategies.

## Limitations
- Struggles with 5-class BI-RADS classification (F1 < 0.50), unable to resolve fine-grained distinctions between intermediate scores
- Requires assumptions about ResNet-18 layer cutoffs and VSSM block configurations not specified in the paper
- Gated attention fusion assumes global context vectors can accurately determine view importance, which may fail with contradictory diagnostic information

## Confidence
- High confidence: Binary BI-RADS classification performance (AUC 0.9967, F1 0.9830) and the overall multi-view fusion framework
- Medium confidence: The VSSM component's contribution to performance gains (requires assumed architectural details)
- Medium confidence: Multi-task learning benefits (effective but potentially sensitive to task interference)
- Low confidence: Ability to handle complex 5-class BI-RADS classification with clinically useful accuracy

## Next Checks
1. Conduct ablation studies comparing hybrid CNN-VSSM against pure ResNet and pure VSSM baselines across all three classification tasks to isolate the hybrid approach's specific contribution
2. Test the gated attention fusion module's robustness by introducing controlled contradictions between views (e.g., one view showing mass, another showing normal tissue) and analyzing attention weight distributions
3. Evaluate model calibration across BI-RADS score predictions using reliability diagrams to assess whether probability outputs align with clinical risk assessments, particularly for intermediate BI-RADS scores (2-4) where performance degrades