---
ver: rpa2
title: 'Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation,
  and Filtering'
arxiv_id: '2505.12662'
source_url: https://arxiv.org/abs/2505.12662
tags:
- knowledge
- retrieval
- reference
- question
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Know\xB3-RAG improves open-domain QA by integrating knowledge\
  \ graphs into three stages of the RAG pipeline: adaptive retrieval, reference generation,\
  \ and reference filtering. It uses KG embeddings to assess answer reliability, enriches\
  \ queries with KG-derived entities, and filters references for both relevance and\
  \ factual consistency."
---

# Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering

## Quick Facts
- **arXiv ID**: 2505.12662
- **Source URL**: https://arxiv.org/abs/2505.12662
- **Reference count**: 40
- **Key outcome**: State-of-the-art open-domain QA across GLM4-9B, Qwen2.5-32B, and GPT-4o-mini models by integrating KG into RAG's three stages.

## Executive Summary
Know³-RAG addresses hallucination and relevance issues in open-domain QA by embedding knowledge graph (KG) signals into three RAG pipeline stages: adaptive retrieval, reference generation, and reference filtering. It uses KG embeddings to assess answer reliability, enriches queries with KG-derived entities, and filters references for both relevance and factual consistency. Evaluated on HotpotQA, 2WikiMultiHopQA, and PopQA, it achieves state-of-the-art results across multiple LLM models while significantly reducing hallucinations.

## Method Summary
Know³-RAG implements a three-module framework: (1) Knowledge-aware Adaptive Retrieval uses ComplEx KGE scores with relative triple scoring to determine retrieval necessity, (2) Knowledge-enhanced Reference Generation augments queries with entity-linked, local neighbor, and globally predicted KG entities, and (3) Knowledge-driven Reference Filter applies dual LLM relevance and KGE factual checks to select top-k references. The process iterates up to T=2 times with escalating thresholds, using 3-shot prompting and temperature=0.

## Key Results
- Achieves state-of-the-art EM/F1 on HotpotQA, 2WikiMultiHopQA, and PopQA across GLM4-9B, Qwen2.5-32B, and GPT-4o-mini models
- Reduces hallucinations through KG-guided adaptive retrieval and dual-filter reference selection
- Improves answer reliability by 0.03-0.11 F1 points through entity-augmented query expansion
- Relevance filtering yields greater improvements than factual filtering in reference selection

## Why This Works (Mechanism)

### Mechanism 1: Relative Triple Scoring for Adaptive Retrieval
KG embeddings quantify answer reliability by measuring how far extracted triples deviate from KG-anchored norms. Lower deviation → higher reliability. Core assumption: KGE models encode meaningful plausibility signals. Break condition: Long-tail entities in PopQA have sparse KG coverage.

### Mechanism 2: Entity-Augmented Query Expansion
Injecting KG-derived entities into queries improves reference relevance by grounding generation in structured knowledge. Sources: query-linked, one-hop neighbors, KGE-predicted tails. Core assumption: Entity linking accuracy and KG completeness are sufficient. Break condition: Ablation shows entity noise can mislead (F1 drops 0.03-0.11).

### Mechanism 3: Dual-Filter Reference Selection
Combining LLM relevance judgment with KGE-based factual checking filters both semantically irrelevant and factually incorrect references. Core assumption: LLMs can reliably judge semantic relevance with entity context. Break condition: Too many references (k>5) degrades performance despite filtering.

## Foundational Learning

- **Knowledge Graph Embeddings (KGE)**
  - Why needed: Core scoring mechanism for reliability assessment
  - Quick check: If a triple scores 0.7 on ComplEx, is it "true"? (No—only meaningful relative to other triples with same head entity.)

- **Entity Linking**
  - Why needed: First step in query augmentation; incorrect linking propagates errors
  - Quick check: Given "Apple released iOS 17," how would you disambiguate "Apple" to a KG entity?

- **Iterative RAG with Dynamic Thresholds**
  - Why needed: Know³-RAG loops until reliability score falls below threshold θt, which escalates each iteration
  - Quick check: Why does θt increase over iterations rather than decrease?

## Architecture Onboarding

- **Component map**: Input Question Q → Entity Linking → Query-linked entities E_q → Adaptive Retrieval (Answer → Triple Extraction → KGE Scoring → Threshold) → Reference Generation (Query + Entities → Knowledge Models → Pseudo-references) → Reference Filtering (LLM relevance + Triple factual check → Top-k) → Accumulated References D_c → QA Model (Q + D_c → Answer)

- **Critical path**: Triple extraction → KGE scoring → threshold check. If this path fails, the loop either exits prematurely or iterates unnecessarily.

- **Design tradeoffs**: Efficiency vs. accuracy (simple spaCy vs. dense entity retrieval), reference count (k=5 balances coverage vs. noise), iteration cap (T=2 prevents infinite loops but may underserve long-tail questions).

- **Failure signatures**: Low KGE coverage on long-tail entities → scores unreliable → excessive iterations; entity linking errors → wrong neighbors retrieved → irrelevant references; LLM relevance check hallucinates "relevant" → false positives pass filter.

- **First 3 experiments**: 1) Baseline sanity check: Run Direct Prompting vs. RAG-D vs. Know³-RAG on PopQA subset; 2) Ablation isolation: Disable each module individually; 3) KGE coverage audit: Sample 100 questions, compute % of extracted triples with valid KGE scores.

## Open Questions the Paper Calls Out
- Can Know³-RAG incorporate search engine retrieval to handle rapidly changing or time-sensitive knowledge?
- How does performance degrade on domains with sparse KG embeddings?
- Can the adaptive threshold θ₀ be automatically calibrated rather than manually tuned?
- What is the trade-off between hallucination reduction and increased inference latency?

## Limitations
- KGE generalization to open-domain QA is empirically validated but not theoretically proven
- Entity linking accuracy directly impacts query augmentation quality but uses simple spaCy models
- Iterative threshold escalation lacks sensitivity analysis and could over-reject valid references

## Confidence
- **High**: Adaptive retrieval with KGE-based confidence scoring improves EM/F1 over baseline RAG
- **Medium**: Entity-augmented query expansion contributes consistently but with varying margins
- **Low**: Factual consistency filtering provides marginal benefit vs. relevance filtering alone

## Next Checks
1. Conduct ablation on entity linking pipeline: replace spaCy with dense retrieval-based entity disambiguation
2. Test KGE score reliability: sample 50 extracted triples from PopQA with low KG coverage, manually verify factual accuracy vs. ComplEx scores
3. Vary iteration cap T: run experiments with T=1, T=3, T=5 on HotpotQA to find optimal balance between convergence and reference quality