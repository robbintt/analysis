---
ver: rpa2
title: 'MCTS-Judge: Test-Time Scaling in LLM-as-a-Judge for Code Correctness Evaluation'
arxiv_id: '2502.12468'
source_url: https://arxiv.org/abs/2502.12468
tags:
- code
- test
- arxiv
- mcts-judge
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reliable code correctness
  evaluation in the LLM-as-a-Judge paradigm, which lacks accuracy for reasoning-intensive
  programming tasks. To overcome this, the authors introduce MCTS-Judge, the first
  framework to integrate test-time computation into LLM-as-a-Judge.
---

# MCTS-Judge: Test-Time Scaling in LLM-as-a-Judge for Code Correctness Evaluation

## Quick Facts
- arXiv ID: 2502.12468
- Source URL: https://arxiv.org/abs/2502.12468
- Reference count: 21
- Primary result: MCTS-Judge doubles accuracy on APPS benchmark from 41% to 80% using 3x fewer tokens than o1-series models

## Executive Summary
This paper introduces MCTS-Judge, the first framework to integrate test-time computation into LLM-as-a-Judge for code correctness evaluation. The authors address the fundamental limitation of direct LLM evaluation for reasoning-intensive programming tasks by leveraging Monte Carlo Tree Search to decompose evaluation into simpler, multi-perspective subtasks. A high-precision, unit-test-level reward mechanism uses simulated execution to encourage line-by-line analysis, achieving state-of-the-art performance on three benchmarks while using significantly fewer tokens than existing methods.

## Method Summary
MCTS-Judge implements a four-stage MCTS loop (Selection, Expansion, Simulation, Backpropagation) where Selection uses a global-local strategy combining UCT and LLM self-assessment, Expansion executes subtasks to generate child nodes, Simulation completes trajectories for reward computation, and Backpropagation propagates rewards up the tree. The reward mechanism uses simulated execution where the LLM acts as an interpreter to execute code line-by-line against pre-generated test cases, with rewards assigned only when trajectory predictions match simulated execution results. The framework operates without training, using test-time computation scaling through increased rollouts, depth, and test cases.

## Key Results
- Doubles accuracy on APPS benchmark from 41% to 80% compared to base models
- Outperforms o1-series models while using 3x fewer tokens
- Achieves state-of-the-art performance across BigCodeBench, APPS, and HumanEval-X benchmarks
- Demonstrates strong robustness without requiring reference code solutions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MCTS decomposes complex code evaluation into simpler, multi-perspective subtasks, reducing cognitive load per inference step.
- Mechanism: The search tree structures evaluation as a sequence of subtask nodes (e.g., logic check, functionality coverage). A global-local selection strategy balances exploitation of high-value regions via UCT with local trajectory coherence via LLM self-assessment.
- Core assumption: The base LLM can reliably judge simpler subtasks even when it fails on holistic evaluation.
- Evidence anchors:
  - [abstract] "MCTS-Judge leverages Monte Carlo Tree Search (MCTS) to decompose problems into simpler, multi-perspective evaluations."
  - [section 3.2] "Through a node-selection strategy that combines self-assessment based on historical actions in the current trajectory and the Upper Confidence Bound for Trees (UCT) algorithm."
  - [corpus] Weak direct corpus support; related work (J1, DAJ) explores test-time scaling for judges but not MCTS-based decomposition specifically.

### Mechanism 2
- Claim: Simulated execution rewards provide unit-test-level signal without actual code execution.
- Mechanism: Test cases are pre-generated and validated by GPT-4o. During reward computation, the same LLM simulates line-by-line execution as an interpreter, comparing simulated outputs to expected outputs. Reward is assigned only when trajectory prediction matches simulated execution result.
- Core assumption: The LLM can simulate execution traces accurately enough to detect correctness bugs.
- Evidence anchors:
  - [abstract] "A high-precision, unit-test-level reward mechanism uses simulated execution to encourage line-by-line analysis."
  - [section 3.3] "We instruct the LLM to simulate a code interpreter, executing the code line by line while tracking variable changes."
  - [corpus] No direct corpus precedent for LLM-as-interpreter reward in judges.

### Mechanism 3
- Claim: Test-time compute scaling (depth, rollouts, test cases) yields monotonic accuracy improvements.
- Mechanism: More rollouts explore diverse reasoning trajectories; deeper trees enable more granular subtask decomposition; more test cases reduce false positives in reward signals.
- Core assumption: The search space contains higher-quality trajectories that base sampling would miss.
- Evidence anchors:
  - [section 4.6] "Fig. 5 demonstrates the impact of these key hyperparameters... MCTS-Judge benefits from extending the test-time computation."
  - [corpus] J1 and DAJ corroborate test-time scaling benefits for LLM judges, though via different mechanisms (best-of-N, data reweighting).

## Foundational Learning

- Concept: Monte Carlo Tree Search (Selection, Expansion, Simulation, Backpropagation)
  - Why needed here: Core algorithm for structured exploration of reasoning trajectories.
  - Quick check question: Can you explain how UCT balances exploration vs. exploitation?

- Concept: LLM-as-a-Judge paradigm
  - Why needed here: Understanding baseline failure modes (bias, superficial reasoning) contextualizes why System-2 search helps.
  - Quick check question: What are the known failure modes of direct LLM evaluation for code?

- Concept: Simulated execution / LLM-as-interpreter
  - Why needed here: Critical for understanding the reward mechanism's design and limitations.
  - Quick check question: Why might an LLM struggle to simulate code execution accurately?

## Architecture Onboarding

- Component map: Preprocessing (GPT-4o test case generation) -> MCTS Loop (Selection -> Expansion -> Simulation -> Backpropagation) -> Reward Module (simulated execution) -> Aggregation (weighted sampling)

- Critical path: Test case quality -> Reward signal accuracy -> MCTS guidance -> Final trajectory selection

- Design tradeoffs:
  - More rollouts/depth increases accuracy but linearly increases token cost
  - Self-assessment weight (wl=0.1) vs. UCT weight (wu=0.9): local coherence vs. global optimization
  - Stricter reward thresholds (all test cases must pass) reduce false positives but may under-reward partially correct trajectories

- Failure signatures:
  - Low accuracy despite scaling: Check test case validity (β self-evaluations)
  - High variance across rollouts: Self-assessment may be unreliable; increase UCT weight
  - Reward always zero: Simulated execution failing; verify interpreter prompts

- First 3 experiments:
  1. Replicate APPS accuracy improvement (41% → 80%) with DeepSeek-Coder-V2-16B using provided hyperparameters (depth=9, rollouts=8).
  2. Ablate reward mechanism: Compare simulated execution vs. self-consistency (RMSC) vs. self-evaluation (RMSE) rewards.
  3. Test scaling curve: Vary rollouts (4, 8, 16) and depth (5, 9, 13) to validate test-time scaling claims.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the simulated execution reward mechanism generalize to non-code evaluation domains?
- **Basis in paper:** [Inferred] The method relies on code execution simulation (Sec 3.3), yet the conclusion positions it as a general "LLM-as-a-Judge" paradigm.
- **Why unresolved:** The reliance on execution traces restricts application to code, leaving other domains (e.g., math, text) unexplored.
- **What evidence would resolve it:** Testing the framework on non-code benchmarks using alternative, non-execution-based reward signals.

### Open Question 2
- **Question:** Does the reliability of the LLM-based "simulated interpreter" limit the framework's scalability on complex code?
- **Basis in paper:** [Inferred] The reward depends on the LLM accurately tracking variable states (Sec 3.3), which is prone to hallucination in complex logic.
- **Why unresolved:** The paper assumes simulation accuracy without bounding the error rate for state-heavy or recursive code.
- **What evidence would resolve it:** A comparison between the simulated rewards and ground-truth sandbox execution results on high-complexity tasks.

### Open Question 3
- **Question:** What is the wall-clock latency trade-off of sequential MCTS rollouts compared to parallelizable baselines?
- **Basis in paper:** [Inferred] The paper claims token efficiency (Sec 4.3) but does not analyze the time cost of sequential System-2 thinking.
- **Why unresolved:** Token count reduction does not guarantee lower latency for interactive use cases where sequential steps block completion.
- **What evidence would resolve it:** A time-to-solution analysis comparing MCTS-Judge against parallel methods like Best-of-N.

## Limitations

- **Subtask action space completeness**: The paper references a "range of subtasks" but only provides one concrete example (logic assessment), with missing enumeration of all subtask types.
- **Test case quality dependence**: Framework performance is highly sensitive to GPT-4o-generated test cases, with no quantitative metrics on test case quality or edge case coverage.
- **Simulated execution accuracy**: The LLM-as-interpreter reward mechanism assumes accurate simulation without providing ablation or error analysis on simulation accuracy itself.

## Confidence

- **APPS accuracy improvement (41% → 80%)**: High confidence - well-documented with clear experimental setup and comparison baselines
- **Test-time scaling benefits**: Medium confidence - supported by ablation studies but lacks comprehensive scaling curves across all dimensions
- **Robustness without reference code**: Medium confidence - claims robustness but limited ablations on reference-based vs reference-free scenarios

## Next Checks

1. **Subtask coverage validation**: Implement and test all subtask types mentioned in the paper, then conduct ablation studies removing individual subtasks to quantify their contribution to overall accuracy.

2. **Test case quality audit**: Generate test cases for 50 problems, then manually verify their validity and edge case coverage. Measure the correlation between test case quality scores and final accuracy.

3. **Simulated execution error analysis**: For 100 randomly selected code snippets, compare LLM-simulated outputs against actual Python execution. Quantify the hallucination rate and its impact on reward signal quality.