---
ver: rpa2
title: Leveraging counterfactual concepts for debugging and improving CNN model performance
arxiv_id: '2501.11087'
source_url: https://arxiv.org/abs/2501.11087
tags:
- filters
- class
- debugging
- counterfactual
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to improve the performance of CNN-based
  image classifiers by leveraging counterfactual concepts. The approach identifies
  crucial filters used in the decision-making process and retrains models to encourage
  activation of class-relevant important filters and discourage activation of irrelevant
  filters.
---

# Leveraging counterfactual concepts for debugging and improving CNN model performance

## Quick Facts
- arXiv ID: 2501.11087
- Source URL: https://arxiv.org/abs/2501.11087
- Reference count: 2
- Primary result: 1-2% accuracy improvement on CUB-2011 dataset by retraining CNNs with counterfactual filter alignment

## Executive Summary
This paper proposes a method to improve CNN-based image classifier performance by leveraging counterfactual concepts. The approach identifies crucial filters used in decision-making through a Counterfactual Explanation (CFE) model, then retrains models to encourage activation of class-relevant important filters while discouraging irrelevant ones. By incorporating counterfactual explanations, the method validates unseen predictions and identifies misclassifications, providing insights into potential weaknesses and biases in the model's learning process. Experimental results demonstrate 1-2% accuracy improvements on publicly available datasets.

## Method Summary
The method trains a CNN classifier, uses a separate CFE model to extract "Minimum Correct" (MC) filters from the last convolutional layer, accumulates global MC filters per class from high-confidence correct training samples, then retrains the classifier with a composite loss that maximizes agreement with global MC filters while minimizing non-MC filter activation. The retraining loss combines standard cross-entropy with terms that encourage class-relevant filter activation and discourage irrelevant filter activation.

## Key Results
- Test accuracy improved from 69.48% to 70.64% (1.16% gain) on CUB-2011 dataset
- Misclassification detection achieved 38% precision (680 of 1769 errors identified)
- Class-specific improvements: Gray Catbird recall improved from 51.39% to 75.81%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minimum Correct (MC) filters identified via counterfactual reasoning capture the essential features required for class predictions.
- Mechanism: The CFE model identifies two filter sets per prediction: MC filters (sparse set that would cause prediction to fail if disabled) and MI filters (filters that would flip prediction to target class if altered). These are extracted from the last convolutional layer where filters encode high-level concepts.
- Core assumption: CFE model faithfully identifies causally relevant filters rather than spurious correlations; filter importance is consistent across images of same class.
- Evidence anchors: Abstract states approach "utilizes counterfactual reasoning to identify crucial filters"; section 3 describes CFE model predicting MC/MI filters; related work provides FMR=0.61.

### Mechanism 2
- Claim: Agreement between local MC filters (test image) and global MC filters (class-level) indicates prediction reliability.
- Mechanism: For each class, accumulate MC filters from high-confidence correct training predictions (probability > 90%). Normalize to create global class-specific filter signature. For test images, compute similarity (recall/F1) between activated local MC filters and global MC set. Low agreement flags potential misclassifications.
- Core assumption: Correct predictions exhibit filter activation patterns that align with class's canonical filter signature; misclassifications deviate because they activate wrong-class filters.
- Evidence anchors: Abstract mentions "minimizes deviation of activation patterns"; section 4.2 shows 38% misclassification detection rate (680/1769 errors).

### Mechanism 3
- Claim: Retraining with filter-alignment losses improves accuracy by encouraging class-relevant and discouraging class-irrelevant filter activations.
- Mechanism: Debugging loss combines three terms: (1) standard cross-entropy, (2) L_MC to maximize agreement with global MC filters, and (3) L_nonMC to minimize non-MC filter activation. Weights λ₁ and λ₂ control regularization strength.
- Core assumption: Models learn spurious correlations via irrelevant filters; constraining filter usage forces learning of class-relevant features.
- Evidence anchors: Abstract states experimental results show 1-2% improvement; section 4.3 shows accuracy improved from 69.48% to 70.64%.

## Foundational Learning

- **Counterfactual Explanation**: Why needed - entire method builds on identifying "what would change the prediction." Quick check - Given a CNN classifying image as "cat," what minimal input changes would make it predict "dog"?
- **Filter/Channel Activation in CNNs**: Why needed - method operates on filter-level activations in last conv layer. Quick check - After global average pooling layer, what does each channel value represent for input image?
- **Sparse Regularization**: Why needed - CFE model uses sparsity loss (λ=2) to identify minimal filter sets; debugging loss encourages sparse activations. Quick check - How does L1 regularization differ from L2 in promoting sparsity, and why might sparsity help interpretability?

## Architecture Onboarding

- **Component map**: Base Classifier (VGG-16) -> CFE Model (Tariq et al. 2022) -> Global MC Accumulator -> Misclassification Detector -> Debugging Trainer
- **Critical path**: 1. Train base classifier → 2. Train/obtain CFE model → 3. Extract global MC filters from training set → 4. Run misclassification analysis on test set → 5. Retrain with debugging loss
- **Design tradeoffs**: Confidence threshold τ (higher values ensure quality but may exclude valid samples); λ₁/λ₂ weights (paper shows λ₁=0.001, λ₂=0.0001 works best); frequency threshold on global MC filters (including only frequently-activated filters reduces noise)
- **Failure signatures**: Large train-test accuracy gap (99% vs 69.5%) suggests global MC filters won't generalize; Table 1 shows 256-370 new errors from aggressive misclassification detection; Table 3 shows some classes lose up to 20% recall
- **First 3 experiments**: 1. Baseline replication - Train VGG-16 on CUB-2011 to verify ~69.5% test accuracy; 2. Global MC extraction - Accumulate MC filters per class with τ=90%, visualize top filters per class using GradCAM; 3. Ablation on λ weights - Grid search λ₁ × λ₂ to replicate Table 2, measure accuracy gain and new error introduction rate

## Open Questions the Paper Calls Out

- **Open Question 1**: How does initial accuracy of pre-trained model quantitatively affect fidelity of identified MC filters and subsequent debugging success? The authors state that less accurate models would produce less faithful MC filters, but experiments only used a high-performing VGG-16 baseline.
- **Open Question 2**: How can global filter extraction mechanism be modified to maintain alignment between training and test set activations when base model suffers from significant overfitting? The method assumes high-confidence training predictions generalize to test distribution, which may fail during overfitting.
- **Open Question 3**: Can loss function be adapted to mitigate severe performance regressions in specific classes while retaining global improvements? Table 3 shows classes like "American Crow" suffered 20% recall drop while global average improved.

## Limitations
- Heavy dependency on external CFE model without implementation details or weights provided
- Assumes global MC filters from training set generalize to test distribution, which fails when models severely overfit
- Class-specific performance regressions observed (up to 20% recall drop for some classes)

## Confidence

- **High confidence**: Filter-alignment mechanism is well-specified with clear loss formulations and ablation results showing 1.16% accuracy improvement
- **Medium confidence**: Counterfactual reasoning foundation depends on external CFE model's faithfulness, which isn't validated in this paper
- **Low confidence**: Generalization claims across different architectures and datasets are unsupported, as all experiments use VGG-16 on CUB-2011

## Next Checks

1. **CFE Model Validation**: Implement or obtain CFE model from Tariq et al. (2022) and verify it achieves FMR ≈ 0.61 on held-out set before proceeding with filter extraction

2. **Global MC Filter Quality**: For top 3 classes, visualize learned global MC filters using GradCAM and verify they correspond to semantically meaningful features (e.g., bird beaks, wings for CUB-2011)

3. **Ablation on Regularization Strength**: Systematically vary λ₁ and λ₂ across ranges {0.0001, 0.0005, 0.001, 0.002} × {0.00001, 0.00005, 0.0001} and measure both accuracy gains and class-level recall changes to identify optimal trade-offs