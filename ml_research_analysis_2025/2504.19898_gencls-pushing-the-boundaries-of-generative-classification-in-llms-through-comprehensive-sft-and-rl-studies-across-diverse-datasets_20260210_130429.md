---
ver: rpa2
title: 'GenCLS++: Pushing the Boundaries of Generative Classification in LLMs Through
  Comprehensive SFT and RL Studies Across Diverse Datasets'
arxiv_id: '2504.19898'
source_url: https://arxiv.org/abs/2504.19898
tags:
- fmt-suc
- macro-f1
- shot
- overall
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates generative classification in LLMs by systematically
  exploring prompt strategies for both training and inference, combined with reinforcement
  learning. It introduces GenCLS++, a framework that fine-tunes LLMs using diverse
  prompt strategies (e.g., few-shot learning, category definitions, uncertainty labels)
  and applies RL to improve classification performance.
---

# GenCLS++: Pushing the Boundaries of Generative Classification in LLMs Through Comprehensive SFT and RL Studies Across Diverse Datasets

## Quick Facts
- arXiv ID: 2504.19898
- Source URL: https://arxiv.org/abs/2504.19898
- Reference count: 40
- Improves generative classification accuracy by 3.46% relative to naive SFT baseline, up to 6.10% on IFLYTEK dataset.

## Executive Summary
This paper investigates generative classification in LLMs by systematically exploring prompt strategies for both training and inference, combined with reinforcement learning. It introduces GenCLS++, a framework that fine-tunes LLMs using diverse prompt strategies (e.g., few-shot learning, category definitions, uncertainty labels) and applies RL to improve classification performance. Experiments on seven datasets show that GenCLS++ improves accuracy by 3.46% relative to a naive SFT baseline and up to 6.10% on the IFLYTEK dataset. Key findings include: (1) switching inference prompts yields gains, (2) RL boosts performance when preceded by SFT warm-up, and (3) classification benefits from direct answer generation rather than explicit reasoning.

## Method Summary
GenCLS++ fine-tunes Qwen-2.5-7B-Instruct using two-phase training: (1) SFT "policy warm-up" with 10 prompt strategies (zero-shot, N-shot, fixed-3-shot, similar-3-shot, definition, definition+1-shot, numerical, uncertainty), (2) RL with Reinforce++ via OpenRLHF framework using rule-based accuracy reward. The framework systematically explores training-inference prompt asymmetry, showing that different prompts at inference time than training improves performance. Direct answer generation without Chain-of-Thought reasoning is preferred for classification tasks.

## Key Results
- GenCLS++ improves accuracy by 3.46% relative to naive SFT baseline across seven datasets
- RL boosts performance by 18.18% relative when preceded by SFT warm-up
- Switching inference prompts yields consistent gains across most dataset-strategy combinations
- Direct answer generation without reasoning outperforms reasoning variants for classification

## Why This Works (Mechanism)

### Mechanism 1: Training-Inference Prompt Asymmetry
Training on one prompt distribution (e.g., definitions) equips the model with category semantics, while inference with a different prompt (e.g., few-shot) leverages in-context learning without semantic repetition—potentially reducing overfitting to training prompt artifacts.

### Mechanism 2: SFT Warm-up Before RL
SFT provides foundational classification capabilities (label format, task understanding) that RL can refine via reward optimization, avoiding exploration in unreasonably large action spaces.

### Mechanism 3: Direct Generation Without Explicit Reasoning
For classification tasks, models trained to output answers directly (no CoT) outperform those trained with reasoning steps. RL training shows response length decreasing over steps—the model learns that reasoning adds no value for discrete label prediction and prunes it.

## Foundational Learning

- **Generative vs. Discriminative Classification**
  - Why needed here: GenCLS++ replaces the traditional value head with token-level generation, leveraging pretrained knowledge
  - Quick check question: Can you explain why a randomly initialized value head might mismatch with a pretrained LLM backbone?

- **Reinforce++ Algorithm**
  - Why needed here: The paper uses Reinforce++ instead of PPO for efficiency; understanding on-policy RL basics is essential for debugging reward hacking
  - Quick check question: What distinguishes Reinforce++ from GRPO in terms of batch sampling requirements?

- **Prompt Strategy Taxonomy (ICL variants, Definitions, Numerical Labels)**
  - Why needed here: The framework systematically explores 10+ prompt types; you need to recognize when to use semantic (definitions) vs. non-semantic (numerical) strategies
  - Quick check question: Why might numerical labels outperform semantic labels in some settings (Table 1: IFLYTEK)?

## Architecture Onboarding

- **Component map:**
Base LLM (Qwen-2.5-7B-Instruct) -> SFT Phase (diverse prompt strategies) -> RL Phase (Reinforce++ with rule-based accuracy reward) -> Inference (prompt switching, optionally perplexity-based decoding)

- **Critical path:**
1. Select training prompt strategy (few-shot variants most robust)
2. Run SFT warm-up (critical—skip at major cost)
3. Apply RL with simple accuracy reward (format reward unnecessary if using no-reasoning setup)
4. At inference, experiment with different prompts than training; default to zero-shot if training had few-shot, and vice versa

- **Design tradeoffs:**
  - Few-shot training improves generalization but increases prompt token costs (Table 5: 5-shot adds ~300-400 tokens)
  - Similar-3-shot retrieval helps when category is retrieval-relevant (TNEWS), hurts when task involves text-pair relationships (EIC)
  - Numerical labels: efficient for many-class datasets (119 classes in IFLYTEK), but require numerical-only inference to work

- **Failure signatures:**
  - Accuracy near random (e.g., 12.41% on EIC): likely prompt mismatch between numerical training and semantic inference
  - RL reward plateaus early: SFT warm-up may have been skipped or insufficient
  - Response length explodes during RL: reasoning prompt still active; switch to no-reasoning prompt

- **First 3 experiments:**
1. Replicate the naive SFT baseline (zero-shot training, zero-shot inference) on one public dataset (e.g., EC) to establish baseline
2. Add SFT warm-up with 3-shot training, test zero-shot inference—verify 1-2% accuracy gain from prompt asymmetry
3. Apply RL (Reinforce++, accuracy reward only) on warmed model—target additional 1-2% gain; monitor response length to confirm reasoning pruning

## Open Questions the Paper Calls Out

### Open Question 1
Do the specific benefits of GenCLS++ (particularly the superiority of direct generation over CoT) generalize to LLMs of varying parameter scales (e.g., 70B+)? The experiments were exclusively conducted using the Qwen-2.5-7B-Instruct model; performance dynamics may shift in larger models with greater intrinsic reasoning capacities.

### Open Question 2
Can cross-sample attention during packed training function as a reliable proxy for explicit In-Context Learning (ICL)? The results in Table 13 were ambiguous; packing improved few-shot performance on some datasets (like QI) but degraded it on others (like EIC), leaving the hypothesis neither fully confirmed nor rejected.

### Open Question 3
What theoretical mechanisms determine the optimal "mismatch" between training prompts and inference prompts? The paper demonstrates the phenomenon empirically but does not explain why certain training strategies (e.g., "Definition") prime the model to perform better with different inference strategies (e.g., "3-shot").

## Limitations
- Missing SFT and RL hyperparameters make exact replication difficult
- Three of seven datasets are proprietary, limiting independent verification
- Mechanisms are empirically demonstrated but lack formal theoretical justification

## Confidence
- **High Confidence**: "RL boosts performance when preceded by SFT warm-up" — supported by large empirical gap (18.18% relative gain)
- **Medium Confidence**: "Switching inference prompts yields gains" — consistent improvements shown, but mechanism is loosely justified
- **Medium Confidence**: "Direct answer generation without CoT outperforms reasoning variants" — supported by ablation results, but "why" is inferential
- **Low Confidence**: Exact reproducibility of results — missing hyperparameters and proprietary datasets prevent faithful reproduction

## Next Checks
1. **Reproduce SFT Baseline Gap**: Implement the naive SFT baseline (zero-shot train/test) and the +SFT variant (3-shot training, zero-shot inference) on a public dataset (e.g., TNEWS). Verify the ~1-2% accuracy gain from prompt switching to confirm Mechanism 1's core claim.

2. **Test SFT Warm-up Necessity**: Run RL directly from the base model (no SFT warm-up) on a small dataset and compare to the SFT→RL pipeline. Confirm the ~18% relative drop in accuracy if warm-up is skipped, validating Mechanism 2.

3. **Validate Reasoning Pruning**: During RL, monitor response length and accuracy when using a CoT prompt vs. a direct answer prompt. Confirm that RL shortens responses and that accuracy improves or plateaus faster without reasoning, supporting Mechanism 3.