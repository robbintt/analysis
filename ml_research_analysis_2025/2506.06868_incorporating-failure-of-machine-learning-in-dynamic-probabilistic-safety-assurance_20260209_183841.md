---
ver: rpa2
title: Incorporating Failure of Machine Learning in Dynamic Probabilistic Safety Assurance
arxiv_id: '2506.06868'
source_url: https://arxiv.org/abs/2506.06868
tags:
- safety
- safeml
- speed
- system
- distance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a probabilistic runtime safety assurance
  framework that integrates SafeML with Bayesian Networks to address the challenge
  of ML failures in autonomous vehicle platooning. The core method uses SafeML to
  detect distributional shifts in ML inputs via Wasserstein distance and bootstrap
  p-values, feeding this reliability information into a BN that performs dynamic safety
  reasoning under uncertainty.
---

# Incorporating Failure of Machine Learning in Dynamic Probabilistic Safety Assurance

## Quick Facts
- arXiv ID: 2506.06868
- Source URL: https://arxiv.org/abs/2506.06868
- Reference count: 23
- Primary result: A probabilistic runtime safety assurance framework using SafeML and Bayesian Networks to detect and mitigate ML failures in AV platooning, achieving 54.08% posterior probability for critical safety state S5 during OOD events.

## Executive Summary
This paper introduces a runtime safety assurance framework that integrates SafeML with Bayesian Networks (BNs) to address ML failures in autonomous vehicle platooning. The core method uses SafeML to detect distributional shifts in ML inputs via Wasserstein distance and bootstrap p-values, feeding this reliability information into a BN that performs dynamic safety reasoning under uncertainty. Experiments on the GTSRB traffic sign dataset showed that SafeML successfully identified misclassifications as out-of-distribution (OOD) with p-values <0.01 across RGB channels, prompting conservative fallback responses. Without SafeML, the system incorrectly reported safe states during ML misclassifications; with SafeML, it transitioned to critical safety states (S5) in 54.08% of OOD cases, demonstrating robust anomaly detection and mitigation.

## Method Summary
The framework trains a CNN on GTSRB for traffic sign classification, then uses SafeML to detect OOD inputs by comparing RGB pixel distributions against training data using Wasserstein distance and bootstrap p-values. A BN integrates the ML prediction, SafeML status, and physical parameters (speed, distance) to infer system safety states (S0–S5). When SafeML flags OOD, the BN overrides optimistic sensor evidence to transition to a critical safety state (S5), triggering fallback control. The method assumes that distributional shifts correlate with classification errors and that conservative fallbacks are always safer.

## Key Results
- SafeML detected OOD inputs with p-values <0.01 across all RGB channels for misclassified GTSRB test samples.
- Without SafeML, the BN incorrectly reported safe states (S0/S1) during ML misclassifications; with SafeML, it transitioned to S5 in 54.08% of OOD cases.
- The framework successfully prevented silent ML failures by forcing fallback control when OOD was detected, even when physical parameters suggested nominal operation.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Distributional divergence between training and operational data correlates with a loss of classification reliability.
- **Mechanism:** SafeML calculates the Wasserstein distance for input data (specifically RGB pixel distributions) against training data distributions. A bootstrap-based p-value is derived to test the null hypothesis that the operational input belongs to the same distribution. If the minimum p-value across channels drops below a threshold (α=0.01), the input is flagged as Out-of-Distribution (OOD).
- **Core assumption:** Assumption: Statistical shifts in pixel distributions serve as a valid proxy for semantic reasoning failures (misclassification).
- **Evidence anchors:**
  - [Section 3.2] "This evaluation uses the Wasserstein distance... followed by a bootstrap-based p-value calculation."
  - [Section 5.1] "P-values for all three RGB channels were below 0.01, strongly rejecting the null hypothesis."
  - [corpus] The paper "Taming Silent Failures" (FMR 0.63) supports the need to detect confident but incorrect outputs, which this mechanism addresses.
- **Break condition:** If the ML fails on data that is statistically similar to the training set (in-distribution failure), this mechanism will not trigger.

### Mechanism 2
- **Claim:** Integrating statistical reliability signals into a Bayesian Network (BN) enables dynamic safety state estimation that contextually overrides deterministic ML outputs.
- **Mechanism:** The BN models causal dependencies between the ML decision, the SafeML status, and physical parameters (speed, distance). It combines these using conditional probability tables. Even if physical parameters appear safe, a low p-value (OOD) from SafeML propagates through the network to increase the posterior probability of a critical system state (S5).
- **Core assumption:** The causal structure of the BN accurately reflects the system's operational dependencies and failure modes.
- **Evidence anchors:**
  - [Section 3.1] "Computing posterior distributions over safety-relevant system states."
  - [Section 5.2] "The OOD flag raised by SafeML prompts the system to transition to fallback control... probability of S5... 54.08%."
  - [corpus] "Towards provable probabilistic safety" aligns with using probabilistic guarantees for embodied AI systems.
- **Break condition:** If the BN Conditional Probability Tables (CPTs) are poorly calibrated, the inference may under- or over-estimate risk.

### Mechanism 3
- **Claim:** Explicitly modeling ML failure as a distinct node allows for safety-preserving fallbacks (state S5) even when sensor data suggests nominal operation.
- **Mechanism:** The system defines discrete safety states (S0–S5). The SafeML status acts as a "gatekeeper." When SafeML flags OOD, the system prioritizes this uncertainty over conflicting evidence (e.g., safe distance), forcing a transition to S5 (Critical ML Failure), which triggers fallback modes like degraded Adaptive Cruise Control (ACC).
- **Core assumption:** A conservative response (fallback) is always safer than attempting to utilize an uncertain ML prediction.
- **Evidence anchors:**
  - [Section 4.2] "S5: Critical ML Failure... activate fallback safety mode."
  - [Section 5.3] "The system's fallback to a safe state is non-disruptive and aligns with safety-first design priorities."
  - [corpus] "AgentGuard" (FMR 0.55) discusses runtime verification triggering fallbacks, mirroring this state-based transition.
- **Break condition:** If the fallback mode (e.g., degraded ACC) is itself unavailable or unsafe in the specific context, the mechanism may reduce utility without guaranteeing safety.

## Foundational Learning

- **Concept:** **Wasserstein Distance (Earth Mover's Distance)**
  - **Why needed here:** This metric quantifies the "cost" to transform one distribution into another. It is the core statistical tool SafeML uses to detect when live camera inputs deviate from training data (e.g., lighting changes, occlusion).
  - **Quick check question:** If two images have the same mean brightness but one has high contrast and the other is flat, would the Wasserstein distance capture this difference?

- **Concept:** **Bayesian Inference (Posterior Update)**
  - **Why needed here:** The system doesn't just use logic rules; it calculates the probability of safety states given new evidence. Understanding how observing the "SafeML Status" updates the probability of "SystemState" is crucial for interpreting the results.
  - **Quick check question:** In this architecture, does a SafeML OOD flag increase or decrease the posterior probability of state S0 (Fully Safe)?

- **Concept:** **Out-of-Distribution (OOD) Detection**
  - **Why needed here:** This is the specific failure mode addressed. It distinguishes between the model making a mistake on familiar data vs. the model failing because the input is fundamentally alien (e.g., a German traffic sign model seeing a British sign).
  - **Quick check question:** Why is checking the p-value against a threshold of 0.01 considered a "conservative" approach to safety?

## Architecture Onboarding

- **Component map:** Camera feeds (RGB) + Telemetry (Speed, LIDAR distance) -> CNN Classifier (Predicts Traffic Sign) -> SafeML Module (Computes Wasserstein distance + p-value against training set) -> Bayesian Network (Fuses prediction + p-value + telemetry) -> System State (S0–S5) -> Control Action (e.g., Maintain speed vs. Fallback/ACC).

- **Critical path:** The **SafeML Module** is the critical path for safety assurance. If the Wasserstein calculation or bootstrap is too slow, the system may act on an unsafe ML prediction before the OOD flag is raised.

- **Design tradeoffs:**
  - **Responsiveness vs. False Alarms:** The paper notes that setting p < 0.01 triggers S5 even for some correct predictions (Table 4, rows 5-6). The tradeoff is accepting "false alarms" (needless deceleration) to avoid "silent failures" (crashing due to misclassification).
  - **Complexity:** Using BNs allows for nuance (S0-S5) but requires defining Conditional Probability Tables (CPTs), which can be complex to tune compared to simple binary logic (Safe/Unsafe).

- **Failure signatures:**
  - **Silent ML Failure:** CNN is confident but wrong; SafeML p-value is low (<0.01); System moves to **S5**.
  - **Physical Safety Breach:** Distance < Threshold; SafeML is ID; System moves to **S3** or **S4**.
  - **Sensor Disagreement:** Distance detection quality is low; System moves to **S1** or **S2**.

- **First 3 experiments:**
  1. **Visual Histogram Analysis:** Plot RGB histograms of training samples vs. misclassified test samples to visually verify the distributional shift SafeML detects (replicate Fig 3).
  2. **Ablation on SafeML:** Run the BN with SafeML disabled (p-value node clamped to "ID") on the misclassified dataset to confirm the system erroneously stays in S0/S1 (Table 3 behavior).
  3. **Threshold Sensitivity:** Vary the p-value threshold (e.g., 0.001, 0.01, 0.05) to observe the shift in System State probabilities and measure the tradeoff between S5 activation rate and false positive rate.

## Open Questions the Paper Calls Out
- How can temporal reasoning be incorporated into the SafeML-BN framework to track distributional shifts over time rather than evaluating each input independently?
- What threshold adaptation mechanism can balance SafeML's false-positive rate against safety-critical miss rates in dynamic operational conditions?
- How does the framework scale to multi-agent platooning scenarios with interdependent ML systems and potential cascading failures?
- How should conditional probability tables in the BN be derived or calibrated when deploying the framework to new perception tasks or domains?

## Limitations
- The exact structure and CPTs of the Bayesian Network are not fully specified, which could affect reproducibility of the 54.08% S5 activation rate.
- The claim that OOD detection via SafeML reliably correlates with classification errors is only validated on GTSRB, not on diverse real-world conditions.
- The safety of the fallback mode (degraded ACC) itself is assumed but not empirically validated under varied operational scenarios.

## Confidence
- **High confidence**: The mechanism of using Wasserstein distance and bootstrap p-values for OOD detection (Mechanism 1) is well-supported by the experimental results and standard statistical practice.
- **Medium confidence**: The BN's ability to integrate ML uncertainty into safety reasoning (Mechanism 2) is plausible but depends on the accuracy of the CPTs and causal structure, which are not fully detailed.
- **Low confidence**: The assumption that the fallback mode is always safe (Mechanism 3) is asserted but not tested for edge cases where degraded ACC might fail.

## Next Checks
1. **Generalize OOD Detection**: Validate SafeML's OOD detection on a dataset with known distributional shifts (e.g., synthetic lighting changes or adversarial examples) to confirm its robustness beyond GTSRB.
2. **Stress-Test BN Reasoning**: Perform sensitivity analysis on the BN CPTs to determine how robust the safety state transitions are to small perturbations in conditional probabilities.
3. **Fallback Safety Audit**: Simulate scenarios where the fallback mode (degraded ACC) might be unsafe (e.g., high-speed highways with short reaction times) to assess the system's true safety net.