---
ver: rpa2
title: 'EAGLE: Efficient Alignment of Generalized Latent Embeddings for Multimodal
  Survival Prediction with Interpretable Attribution Analysis'
arxiv_id: '2506.22446'
source_url: https://arxiv.org/abs/2506.22446
tags:
- survival
- clinical
- imaging
- risk
- eagle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents EAGLE, a multimodal deep learning framework
  for cancer survival prediction that integrates imaging, clinical, and textual data
  through attention-based fusion mechanisms. The framework achieves 99.96% dimensionality
  reduction while maintaining competitive predictive performance, addressing computational
  efficiency and clinical interpretability challenges.
---

# EAGLE: Efficient Alignment of Generalized Latent Embeddings for Multimodal Survival Prediction with Interpretable Attribution Analysis

## Quick Facts
- arXiv ID: 2506.22446
- Source URL: https://arxiv.org/abs/2506.22446
- Reference count: 26
- Multi-modal survival prediction with 99.96% dimensionality reduction while maintaining competitive performance

## Executive Summary
EAGLE is a multimodal deep learning framework that integrates imaging, clinical, and textual data for cancer survival prediction. The framework uses cross-modal attention mechanisms to learn hierarchical relationships between modalities while achieving significant dimensionality reduction. Evaluated on 911 patients across three cancer types (GBM, IPMN, NSCLC), EAGLE demonstrates robust performance with concordance indices ranging from 0.598 to 0.679. The framework provides patient-level attribution analysis revealing modality-specific contribution patterns and identifies clinically meaningful risk groups with 4-5 fold differences in median survival.

## Method Summary
EAGLE processes pre-extracted embeddings from imaging (RadImageNet), clinical data, and text reports through modality-specific encoders that progressively compress data to unified latent dimensions. Cross-modal attention mechanisms (8 heads) learn adaptive relationships between modalities, with results fused through a deep neural network and trained via Cox proportional hazards loss. The framework incorporates three complementary attribution methods (L1-norm, gradient×activation, integrated gradients) for interpretability. An auxiliary binary classification task improves convergence while maintaining primary survival prediction objectives.

## Key Results
- Achieved concordance indices of 0.637-0.679 across three cancer types (GBM, IPMN, NSCLC)
- Maintained competitive predictive performance with 99.96% dimensionality reduction
- Identified clinically meaningful risk groups with 4-5 fold differences in median survival
- Demonstrated modality-specific contribution patterns: text reports dominated GBM (43.7%), balanced contributions for IPMN (31-35%), imaging drove NSCLC predictions (49.0%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-modal attention enables adaptive weighting of modality contributions based on patient-specific relevance
- Mechanism: Multi-head attention computes attention weights between modality pairs, allowing the model to learn which modality combinations are most informative for each patient
- Core assumption: Prognostically relevant information is distributed non-uniformly across modalities and varies by disease type
- Evidence anchors: Cross-modal attention between imaging-text and imaging-clinical pairs with 8 heads; related papers use cross-attention fusion
- Break condition: If attention weights become uniform across modalities or a single modality consistently dominates regardless of patient

### Mechanism 2
- Claim: Progressive dimensionality reduction through modality-specific encoders preserves prognostic signal while enabling computational tractability
- Mechanism: Each modality passes through dedicated encoder networks that progressively compress pre-extracted embeddings to unified latent dimensions
- Core assumption: Pre-extracted embeddings contain redundant information, and survival-relevant features can be captured in low-dimensional representations
- Evidence anchors: Achieves 99.96% dimensionality reduction while maintaining competitive predictive performance; encoder architectures specified with batch normalization and dropout
- Break condition: If C-index degrades substantially compared to high-dimensional baselines without compression

### Mechanism 3
- Claim: Multiple complementary attribution methods provide more robust interpretability than any single method alone
- Mechanism: Three attribution approaches (L1-norm, gradient×activation, integrated gradients) are computed and normalized to percentages, cross-validating modality importance estimates
- Core assumption: Each attribution method has different failure modes; consensus across methods increases confidence in interpretability claims
- Evidence anchors: Three complementary attribution methods providing patient-level interpretability; mathematical formulations provided
- Break condition: If attribution methods produce contradictory rankings, interpretability becomes ambiguous rather than robust

## Foundational Learning

- Concept: Cox proportional hazards loss for censored survival data
  - Why needed here: The model outputs risk scores trained via Cox partial likelihood, which handles right-censored outcomes common in oncology
  - Quick check question: Can you explain why mean squared error on survival time would be inappropriate when 50% of patients are censored?

- Concept: Multi-head attention mechanisms
  - Why needed here: Core fusion mechanism operates via attention; understanding query/key/value projections and attention weight computation is required to debug or modify fusion behavior
  - Quick check question: Given three modality embeddings, how would you formulate cross-attention to compute imaging→text influence?

- Concept: Pre-extracted embeddings from foundation models
  - Why needed here: EAGLE operates on embeddings (RadImageNet, GatorTron), not raw images/text; the quality and domain-specificity of these embeddings constrains downstream performance
  - Quick check question: What information might be lost when using RadImageNet embeddings instead of end-to-end CNN feature learning from raw MRI?

## Architecture Onboarding

- Component map: Pre-extracted embeddings → modality-specific encoders → cross-modal attention → fusion network → Cox risk output + auxiliary binary classification
- Critical path: Embedding quality → encoder compression → attention weight computation → risk score
- Design tradeoffs: Pre-extracted embeddings vs end-to-end (computational efficiency vs potential loss of task-specific features); unified architecture vs cancer-specific (generalizability vs optimal per-cancer performance); three attribution methods vs one (computational overhead for robustness)
- Failure signatures: C-index near 0.5 (random: check data leakage, label corruption, or severe class imbalance); attention weights uniformly distributed (model not learning modality-specific relevance); large discrepancy between attribution methods (attribution unreliable); high variance across folds (insufficient data or unstable architecture)
- First 3 experiments:
  1. Ablation on fusion strategy: Compare attention fusion vs simple concatenation vs late fusion
  2. Single-modality baselines: Run imaging-only, text-only, clinical-only to establish modality importance independently
  3. Compression sweep: Test encoder output dimensions (32, 64, 128, 256) to identify minimal representation size before performance degrades

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would cancer-specific architectural adaptations outperform EAGLE's unified approach across diverse malignancies?
- Basis in paper: "Future work should explore cancer-specific architectural adaptations that could leverage these differences rather than using a unified approach."
- Why unresolved: The unified pipeline prioritizes generalizability but may sacrifice cancer-specific optimization, as evidenced by variable C-indices and differential modality contributions across cancer types
- What evidence would resolve it: Head-to-head comparison of unified vs cancer-specific architectures on held-out cancer types

### Open Question 2
- Question: Can end-to-end learning from raw imaging data improve predictive performance over pre-extracted embeddings?
- Basis in paper: "The current implementation uses pre-extracted embeddings rather than end-to-end learning from raw images, which may limit optimal feature discovery."
- Why unresolved: Pre-extracted embeddings reduce computational burden but may discard task-relevant features
- What evidence would resolve it: Comparative study training identical EAGLE architectures on raw images vs pre-extracted embeddings

### Open Question 3
- Question: How does EAGLE performance generalize to external institutions with different imaging protocols and clinical documentation practices?
- Basis in paper: "the cohorts, while clinically representative, come from single institutions, and external validation would strengthen generalizability claims."
- Why unresolved: Single-institution data may capture site-specific biases that inflate apparent performance
- What evidence would resolve it: Multi-center external validation across 3-5 institutions with varying imaging protocols

### Open Question 4
- Question: Would longitudinal integration of treatment response data improve survival predictions beyond baseline multimodal assessment?
- Basis in paper: "Future directions include extending EAGLE to handle longitudinal data, incorporating treatment response dynamics crucial for adaptive therapy planning."
- Why unresolved: Current model uses baseline data only; treatment response provides powerful prognostic signal but requires temporal architecture modifications
- What evidence would resolve it: Implementation of recurrent or transformer-based temporal fusion comparing baseline-only vs longitudinal predictions

## Limitations

- The claim of state-of-the-art performance lacks direct comparative validation against high-dimensional baselines or other multimodal survival frameworks
- The cross-modal attention mechanism's superiority over simpler fusion strategies remains unproven without ablation studies
- Attribution analysis lacks ground-truth validation to verify modality contribution estimates
- Dataset sizes are relatively small for deep learning, with ±0.087 concordance index variance suggesting potential instability
- The choice of pre-extracted embeddings constrains the framework's feature space without addressing whether task-specific feature learning might improve performance

## Confidence

- **High confidence**: Dimensionality reduction implementation and computational efficiency claims; Cox proportional hazards framework application; risk stratification identifying clinically meaningful survival groups
- **Medium confidence**: Concordance index performance (0.637-0.679) relative to survival prediction baselines; interpretability through multiple attribution methods; clinical relevance of modality contribution patterns
- **Low confidence**: Claims of state-of-the-art performance without comparative baselines; attribution robustness without ground-truth validation; attention mechanism's adaptive benefit over simpler alternatives

## Next Checks

1. Conduct ablation studies comparing cross-modal attention fusion against simple concatenation, late fusion, and other fusion strategies to quantify attention's specific contribution to performance
2. Implement ground-truth validation experiments where known modality importance is artificially manipulated to test whether attribution methods correctly identify modality contributions
3. Compare performance using task-specific feature learning (end-to-end CNN for imaging) against pre-extracted embeddings to assess feature representation quality impact