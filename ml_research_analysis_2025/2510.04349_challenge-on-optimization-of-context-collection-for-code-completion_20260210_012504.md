---
ver: rpa2
title: Challenge on Optimization of Context Collection for Code Completion
arxiv_id: '2510.04349'
source_url: https://arxiv.org/abs/2510.04349
tags:
- code
- context
- completion
- competition
- phase
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The challenge focused on optimizing context collection for fill-in-the-middle
  code completion in Python and Kotlin. Participants developed strategies to gather
  relevant code snippets from repositories to improve completion quality using chrF
  as the evaluation metric across three models (Codestral, Qwen2.5-Coder, Mellum).
---

# Challenge on Optimization of Context Collection for Code Completion

## Quick Facts
- **arXiv ID:** 2510.04349
- **Source URL:** https://arxiv.org/abs/2510.04349
- **Reference count:** 26
- **Primary result:** Optimization of context collection for fill-in-the-middle code completion in Python and Kotlin using chrF evaluation

## Executive Summary
The Challenge on Optimization of Context Collection for Code Completion focused on improving fill-in-the-middle code completion by developing strategies to gather relevant code snippets from repositories. Participants competed across two tracks (Python and Kotlin) using three models (Codestral, Qwen2.5-Coder, Mellum) with chrF as the primary evaluation metric. The challenge attracted significant participation with 19 teams for Python and 8 for Kotlin in the public phase, narrowing to 6 teams submitting private phase solutions.

The competition required participants to develop sophisticated context collection strategies that balanced precision and recall while maintaining computational efficiency. Teams employed various parsing tools and retrieval methods to extract symbol definitions and rank context chunks. The private phase results showed team SpareCodeComplete achieving the highest average chrF scores of 0.725 for Python and 0.753 for Kotlin, while team NoMoreActimel ranked second in both tracks.

## Method Summary
The challenge centered on optimizing context collection strategies for fill-in-the-middle code completion tasks. Participants developed approaches that combined parsing tools (AST, Tree-sitter, PSI) with retrieval methods (BM25, FAISS, Zoekt) to extract and rank relevant code snippets. The methodology involved extracting symbol definitions from prefix and suffix code segments, retrieving definitions from modified files, and assembling context from similar code snippets. Teams employed various strategies including symbol-based extraction, retrieval-based context gathering, and hybrid approaches that combined multiple techniques. The evaluation framework used chrF as the primary metric, measuring the quality of generated completions against ground truth implementations.

## Key Results
- Team SpareCodeComplete achieved highest average chrF scores: 0.725 (Python) and 0.753 (Kotlin) in private phase
- Team NoMoreActimel ranked second with scores of 0.725 (Python) and 0.719 (Kotlin)
- Competition attracted 19 teams for Python and 8 for Kotlin during public phase
- Complete dataset available under CC BY 4.0 license

## Why This Works (Mechanism)
The success of context collection optimization stems from the systematic approach to identifying and ranking relevant code snippets based on symbol definitions and code similarity. By leveraging multiple parsing tools and retrieval methods, teams could extract semantic information from both prefix and suffix code segments, ensuring that the most contextually relevant definitions were included in the completion context. The chrF metric effectively captures the quality of these completions by measuring character-level n-gram precision and recall, providing a reliable evaluation framework for comparing different context collection strategies.

## Foundational Learning
The challenge demonstrated that effective context collection requires a multi-faceted approach combining symbol extraction, retrieval-based methods, and semantic analysis. Teams learned that no single technique suffices; instead, successful strategies integrate multiple approaches including AST-based parsing, Tree-sitter for language-specific syntax, and vector-based similarity search using FAISS. The competition also revealed the importance of balancing computational efficiency with precision, as overly complex context collection methods may not scale well in practical development environments.

## Architecture Onboarding
The context collection architecture follows a pipeline approach where code is first parsed to extract symbol definitions using tools like AST, Tree-sitter, or PSI depending on the programming language. These symbols are then used to query repositories for relevant definitions and similar code snippets using retrieval methods such as BM25 or FAISS. The retrieved chunks are ranked based on relevance scores and assembled into the final context collection. This modular architecture allows teams to experiment with different parsing tools and retrieval methods while maintaining a consistent evaluation framework using the chrF metric.

## Open Questions the Paper Calls Out
- How do different context collection strategies generalize across programming languages beyond Python and Kotlin?
- What is the optimal balance between context collection precision and computational efficiency for real-world development scenarios?
- How do chrF scores correlate with actual developer productivity and code quality in practical usage?
- Can the presented context collection approaches be effectively scaled to larger codebases and more diverse programming patterns?

## Limitations
- chrF metric may not fully capture practical utility of code completions in real development scenarios
- Private test set evaluation introduces uncertainty as specific challenges remain unknown to public
- Reliance on specific parsing tools (AST, Tree-sitter, PSI) may introduce implementation-specific biases
- Competition results may be influenced by dataset characteristics that don't generalize to all development contexts
- The focus on fill-in-the-middle scenarios may not address all code completion use cases encountered in practice

## Confidence
- **High Confidence**: Technical implementation details and competition framework are well-documented
- **Medium Confidence**: chrF scores and rankings accurate for specific evaluation setup but generalizability uncertain
- **Medium Confidence**: Effectiveness of context collection strategies supported by empirical results but relative importance varies by use case

## Next Checks
1. Conduct user study comparing completions generated with different context collection strategies to validate correlation with developer productivity
2. Test presented approaches on independently curated dataset with diverse programming patterns
3. Evaluate impact of different context collection granularities on completion quality across programming languages and coding styles