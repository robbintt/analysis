---
ver: rpa2
title: Deep Reinforcement Learning for Solving the Fleet Size and Mix Vehicle Routing
  Problem
arxiv_id: '2512.24251'
source_url: https://arxiv.org/abs/2512.24251
tags:
- vehicle
- routing
- fleet
- problem
- fsmvrp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a deep reinforcement learning (DRL) approach
  for solving the Fleet Size and Mix Vehicle Routing Problem (FSMVRP), which requires
  simultaneous optimization of fleet composition and vehicle routing. The authors
  formulate FSMVRP as a Markov Decision Process and design a novel policy network
  called FRIPN that integrates fleet composition and routing decisions within a unified
  framework.
---

# Deep Reinforcement Learning for Solving the Fleet Size and Mix Vehicle Routing Problem

## Quick Facts
- arXiv ID: 2512.24251
- Source URL: https://arxiv.org/abs/2512.24251
- Reference count: 40
- The paper proposes a deep reinforcement learning approach that achieves 1.10% average cost gap vs. best-known algorithms while generating solutions within seconds.

## Executive Summary
This paper addresses the Fleet Size and Mix Vehicle Routing Problem (FSMVRP) by proposing a deep reinforcement learning approach that integrates fleet composition and routing decisions within a unified framework. The authors formulate FSMVRP as a Markov Decision Process and design a novel policy network called FRIPN that combines fleet composition and routing decisions. The method incorporates specialized input embeddings, including a remaining graph embedding to facilitate effective vehicle employment decisions. Extensive experiments demonstrate that the proposed approach outperforms traditional methods in terms of solution quality and computational efficiency, particularly for large-scale and time-constrained scenarios.

## Method Summary
The authors formulate FSMVRP as a Markov Decision Process where actions simultaneously select both vehicle type and next customer node. FRIPN uses a Transformer encoder-decoder architecture with node and vehicle encoders, and a decoder that produces joint vehicle-node action probabilities. The method incorporates a remaining graph embedding that captures unvisited customer demand distribution, enabling informed fleet composition decisions. Training uses REINFORCE with a shared baseline and random vehicle-type sampling per batch to promote generalization across diverse fleet configurations. The policy network is trained on randomly generated instances and tested on benchmark datasets without retraining.

## Key Results
- Achieves 1.10% average cost gap compared to best-known algorithms
- Generates solutions within seconds for instances with up to 1000 customers
- Outperforms traditional methods (ALNS, TS, GA) in both solution quality and computational efficiency
- Shows strong generalization capability when tested on benchmark instances without retraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The remaining graph embedding enables more informed fleet composition decisions by providing contextual awareness of unvisited customer demand.
- Mechanism: The remaining graph embedding aggregates node embeddings of all customers still available to each vehicle type at each step. For candidate vehicles (those not yet deployed), this embedding captures the residual demand distribution; for employed vehicles, it is masked to zero. This allows the policy to assess whether remaining demand justifies deploying an additional vehicle of a particular type.
- Core assumption: Fleet composition decisions benefit from knowing the spatial distribution and magnitude of unvisited demand.
- Evidence anchors:
  - [abstract]: "...including a remaining graph embedding to facilitate effective vehicle employment decisions."
  - [section IV-B-2]: Equations 17-19 define the remaining graph embedding and assignment function. "In addition to vehicle fixed costs, our observations indicate that the presence of unvisited nodes plays a critical role in vehicle deployment decisions."
  - [section V-B]: Ablation experiments show convergence to superior results with remaining graph embedding across all problem scales.
- Break condition: If remaining demand is uniformly distributed and vehicle capacities are homogeneous, the remaining graph embedding may provide diminishing marginal utility.

### Mechanism 2
- Claim: Integrating fleet composition and routing decisions in a unified MDP with joint action selection enables coherent optimization of fixed and variable costs.
- Mechanism: Rather than decomposing FSMVRP into separate fleet selection and routing subproblems, the method defines a joint action a_t = (k_j, v_i) at each step, selecting both a vehicle type and a node. Fleet composition emerges implicitly: deploying a vehicle from the depot incurs its fixed cost once; subsequent actions using that vehicle only incur variable travel costs. The fixed cost assignment function (Equation 16) ensures fixed costs are encoded only when a vehicle is a candidate.
- Core assumption: Fleet composition and routing decisions are tightly coupled; their joint optimization yields better solutions than sequential approaches.
- Evidence anchors:
  - [abstract]: "...FRIPN that seamlessly integrates fleet composition and routing decisions."
  - [section III-B]: "Different from some classic routing problems that only have to decide which node to visit... we have to select both the vehicle and the node at each step."
  - [section IV-A]: "Through this network policy framework, we incorporate the fleet composition into the route scheduling process, thereby avoiding the potential high computational complexity caused by introducing additional fleet composition decision structures."
  - [corpus]: Limited direct corpus comparison for FSMVRP-specific joint formulations; neighboring papers address related but distinct VRP variants.
- Break condition: If fixed costs are negligible relative to variable costs, the joint formulation reduces to standard routing and the added complexity may not be justified.

### Mechanism 3
- Claim: Shared baseline REINFORCE with random vehicle-type sampling enables stable policy training across heterogeneous fleet configurations.
- Mechanism: Training uses REINFORCE with a shared baseline (average reward across N sampled trajectories for the same instance), reducing variance compared to greedy rollouts. Critically, the number of vehicle categories is randomly sampled per batch, and training data is generated accordingly. This prevents the policy from overfitting to a specific fleet configuration and encourages generalization.
- Core assumption: Policy generalization across diverse candidate fleets requires exposure to varying fleet configurations during training.
- Evidence anchors:
  - [section IV-C]: "Since the number of vehicle categories varies across instances, we randomly sample the scale of vehicle categories prior to each batch update."
  - [section V-D]: The policy achieves 1.10% average cost gap on benchmark instances without retraining, despite distributional differences in vehicle parameters.
- Break condition: If test instances have vehicle types with cost-capacity relationships dramatically outside training distribution, generalization may degrade.

## Foundational Learning

- Concept: **Markov Decision Processes (MDP) for combinatorial optimization**
  - Why needed here: FSMVRP is formulated as an MDP with state, action, transition, and reward. Understanding how sequential decision-making maps to routing problems is essential for interpreting the policy network's role.
  - Quick check question: Can you explain why the reward is defined as the negative of total cost rather than a step-wise function?

- Concept: **Transformer encoder-decoder architectures with attention**
  - Why needed here: FRIPN uses multi-head attention for node encoding, vehicle encoding, and decoding. The decoder uses cross-attention (vehicle embedding queries, node embedding keys/values) to produce action probabilities.
  - Quick check question: How does the attention mask in Equation 24 enforce constraint satisfaction (e.g., capacity, visitation)?

- Concept: **REINFORCE policy gradient with baseline**
  - Why needed here: Training uses REINFORCE with a shared baseline. Understanding variance reduction through baselines is critical for debugging training instability.
  - Quick check question: Why might a shared baseline outperform a critic baseline for problems with high instance variability?

## Architecture Onboarding

- Component map:
  Node Encoder -> Vehicle Encoder -> Decoder -> Action Selection

- Critical path:
  1. Instance arrives â†’ Node Encoder computes h_NE once.
  2. At each step t: Vehicle Encoder computes h_VE,t using current vehicle states + remaining graph embedding.
  3. Decoder computes compatibility scores u_t,i,j with constraint masking.
  4. Sample or greedily select action; update state; accumulate cost.
  5. At episode end, compute reward; update policy via REINFORCE.

- Design tradeoffs:
  - **Joint vs. decomposed formulation**: Joint action space avoids introducing separate fleet selection modules but increases action space complexity.
  - **Greedy vs. sampling decoding**: Greedy is deterministic and faster; sampling + augmentation yields better solutions but increases inference time.
  - **Remaining graph embedding overhead**: Additional computation per step; justified when fixed costs significantly influence decisions.

- Failure signatures:
  - **Training divergence**: Check baseline stability; ensure shared baseline is computed correctly per instance.
  - **Poor generalization to new fleet configurations**: Verify vehicle-type sampling covers test distribution; consider curriculum over fleet diversity.
  - **Constraint violations**: Inspect masking in Equation 24; ensure availability flags correctly reflect capacity and visitation constraints.

- First 3 experiments:
  1. **Ablation on remaining graph embedding**: Train identical networks with and without this component on 50-node instances; compare convergence curves and final cost gaps.
  2. **Scalability test**: Apply policy trained on 100-node instances directly to 200, 500, and 1000-node instances without retraining; measure cost gap vs. ALNS runtime.
  3. **Fleet diversity generalization**: Train on instances with 3-4 vehicle types; test on instances with 6+ types with out-of-distribution cost-capacity ratios. Assess degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed DRL framework be effectively extended to handle complex FSMVRP variants with time windows or battery constraints?
- Basis in paper: [explicit] The conclusion states that "DRL-based methods can be extended to address other variants of FSMVRP."
- Why unresolved: The current study focuses on the standard FSMVRP formulation without time windows (FSMVRPTW) or electric vehicle charging (E-FSMVRP).
- What evidence would resolve it: Application of the FRIPN architecture to the E-FSMFTW benchmark instances with competitive results.

### Open Question 2
- Question: Can more sophisticated embedding techniques be developed to improve the accuracy of vehicle employment decisions beyond the remaining graph embedding?
- Basis in paper: [explicit] The conclusion calls for "further exploration of techniques related to vehicle employment decisions."
- Why unresolved: The paper introduces "remaining graph embedding" as a novel component but does not claim it is the optimal solution for fleet composition representation.
- What evidence would resolve it: An ablation study showing a new embedding mechanism reduces the cost gap or improves convergence speed relative to the FRIPN baseline.

### Open Question 3
- Question: How does the assumption of distinct variable costs in training affect the model's ability to generalize to real-world instances with uniform variable costs?
- Basis in paper: [inferred] Section V-D notes that while training data uses distinct variable costs, the benchmark dataset assumes uniform costs.
- Why unresolved: The authors test without retraining, achieving a 1.10% gap, but do not analyze if retraining on uniform cost distributions would eliminate this discrepancy.
- What evidence would resolve it: Comparing the performance of models trained on distinct vs. uniform cost distributions when evaluated on the Golden et al. benchmark.

## Limitations
- Critical architectural details are underspecified, including exact number of Transformer layers, attention heads, embedding dimensions, and decoder clipping constant C.
- Lacks comparison against exact solvers or state-of-the-art column generation approaches for FSMVRP, limiting optimality gap assessment.
- Generalization claims rely on random vehicle-type sampling assumption without sensitivity analysis for distributional mismatches.

## Confidence

- **High confidence**: The mechanism of joint fleet composition and routing via a unified MDP (Mechanism 2) is well-supported by the formulation and ablation evidence.
- **Medium confidence**: The remaining graph embedding's effectiveness (Mechanism 1) is supported by ablation experiments, but the exact contribution depends on problem-specific demand distributions.
- **Medium confidence**: The training stability from random vehicle-type sampling (Mechanism 3) is plausible but lacks ablation against fixed fleet configurations.

## Next Checks

1. **Ablation on remaining graph embedding**: Train identical networks with and without this component on 50-node instances; compare convergence curves and final cost gaps.
2. **Scalability test**: Apply policy trained on 100-node instances directly to 200, 500, and 1000-node instances without retraining; measure cost gap vs. ALNS runtime.
3. **Fleet diversity generalization**: Train on instances with 3-4 vehicle types; test on instances with 6+ types with out-of-distribution cost-capacity ratios. Assess degradation.