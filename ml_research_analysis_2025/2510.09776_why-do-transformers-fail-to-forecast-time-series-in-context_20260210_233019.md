---
ver: rpa2
title: Why Do Transformers Fail to Forecast Time Series In-Context?
arxiv_id: '2510.09776'
source_url: https://arxiv.org/abs/2510.09776
tags:
- linear
- time
- series
- learning
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a theoretical analysis of why Transformers
  fail to outperform simpler models like linear regression for time series forecasting.
  The authors focus on Linear Self-Attention (LSA) and establish that even optimally
  parameterized LSA models cannot achieve lower expected MSE than classical linear
  models for in-context forecasting.
---

# Why Do Transformers Fail to Forecast Time Series In-Context?

## Quick Facts
- **arXiv ID:** 2510.09776
- **Source URL:** https://arxiv.org/abs/2510.09776
- **Reference count:** 40
- **Key outcome:** Proves that optimally parameterized Linear Self-Attention models cannot achieve lower expected MSE than classical linear models for in-context time series forecasting, establishing a strict finite-sample gap that vanishes at rate no faster than 1/n.

## Executive Summary
This paper provides a theoretical analysis explaining why Transformer-based models fail to outperform simpler linear regression approaches for time series forecasting. The authors focus on Linear Self-Attention (LSA) as a simplified model that captures the core attention mechanism while remaining mathematically tractable. They establish that even optimally parameterized LSA models cannot achieve lower expected mean squared error (MSE) than classical linear autoregressive models for in-context forecasting. The analysis reveals fundamental limitations in how attention mechanisms capture temporal dependencies, showing that LSA predictions under Chain-of-Thought inference collapse to the mean exponentially, compounding forecasting errors. Experimental results across synthetic and real-world datasets validate these theoretical findings, demonstrating that LSA consistently tracks but never surpasses linear regression baselines.

## Method Summary
The authors analyze Linear Self-Attention (LSA) as a simplified model that captures the core attention mechanism while remaining mathematically tractable. They establish theoretical bounds proving that LSA cannot outperform linear regression in terms of expected MSE, even under optimal parameterization. The analysis includes examining Chain-of-Thought inference, where they show that LSA predictions collapse to the mean exponentially. The theoretical framework is validated through experiments comparing LSA against linear regression baselines across teacher-forcing, CoT, and scaling scenarios using both synthetic stationary AR processes and real-world time series from the UCI repository.

## Key Results
- Proves LSA has a strict finite-sample gap over linear regression that vanishes at rate no faster than 1/n as context length increases
- Shows under Chain-of-Thought inference, LSA predictions collapse to the mean exponentially, compounding errors
- Experiments validate theoretical findings, demonstrating LSA tracks but never surpasses linear baseline across all tested scenarios

## Why This Works (Mechanism)
The fundamental mechanism behind LSA's limitations stems from how attention weights are computed and applied. In LSA, attention weights are derived from normalized inner products between query and key vectors, which in time series contexts are essentially lagged observations of the target variable. This normalization process inherently constrains the model's ability to capture temporal dependencies as effectively as direct linear regression on lagged values. The exponential collapse to the mean under Chain-of-Thought inference occurs because the attention mechanism amplifies the central tendency of the data distribution while suppressing extreme values, leading to progressively less informative predictions as forecasting proceeds.

## Foundational Learning
- **Linear Self-Attention (LSA):** A simplified attention mechanism where attention weights are computed via scaled dot-products between queries and keys, then normalized - needed to understand the core attention mechanism being analyzed; quick check: verify that LSA reduces to linear regression when attention weights are identity.
- **Chain-of-Thought (CoT) Inference:** A sequential prediction approach where each prediction becomes input for the next step - needed to understand error propagation in autoregressive forecasting; quick check: trace how a single prediction error propagates through multiple CoT steps.
- **Mean Squared Error (MSE) Bounds:** Mathematical techniques for establishing upper and lower bounds on expected prediction error - needed to prove theoretical limitations of LSA; quick check: verify that the derived MSE bounds hold under the stated assumptions about the data-generating process.

## Architecture Onboarding
- **Component Map:** LSA Layer -> Attention Normalization -> Output Projection -> Prediction
- **Critical Path:** Data → LSA Computation → Attention Weight Calculation → Weighted Sum → Forecast
- **Design Tradeoffs:** Simplicity and interpretability of LSA vs. representational power of full Transformers; mathematical tractability vs. practical effectiveness
- **Failure Signatures:** LSA predictions consistently matching but not improving upon linear regression baselines; exponential decay toward mean under CoT; inability to capture non-linear temporal patterns
- **First Experiments:**
  1. Compare LSA vs. linear regression MSE on stationary AR(1) process with varying context lengths
  2. Evaluate LSA under teacher-forcing vs. CoT inference on synthetic data
  3. Test LSA performance scaling with context length on UCI time series datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis focuses exclusively on stationary linear autoregressive processes, excluding non-stationary and non-linear time series patterns
- Theoretical results apply to Linear Self-Attention but may not fully extend to full Transformer architectures with position encodings and non-linearities
- Chain-of-Thought analysis assumes perfect initial conditions without accounting for compounding effects of early prediction errors

## Confidence
- **High Confidence:** Theoretical MSE bounds proving LSA limitations under stated assumptions
- **Medium Confidence:** Experimental validation showing LSA consistently tracking linear regression baselines
- **Low Confidence:** Claims about fundamental attention limitations for all Transformer architectures

## Next Checks
1. Test theoretical predictions on non-stationary time series with structural breaks and regime changes
2. Implement and evaluate standard Transformer architectures alongside LSA to determine if architectural modifications overcome identified limitations
3. Conduct detailed empirical analysis of error propagation under CoT inference with imperfect initial conditions to quantify compounding effects