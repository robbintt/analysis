---
ver: rpa2
title: 'NeuronSeek: On Stability and Expressivity of Task-driven Neurons'
arxiv_id: '2506.15715'
source_url: https://arxiv.org/abs/2506.15715
tags:
- neurons
- data
- regression
- network
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes NeuronSeek-TD, a method that replaces unstable
  symbolic regression with tensor decomposition to discover optimal task-driven neurons.
  The approach addresses two key challenges: instability in the original NeuronSeek-SR
  framework and lack of theoretical foundation for task-driven neurons.'
---

# NeuronSeek: On Stability and Expressivity of Task-driven Neurons

## Quick Facts
- **arXiv ID**: 2506.15715
- **Source URL**: https://arxiv.org/abs/2506.15715
- **Reference count**: 40
- **Primary result**: NeuronSeek-TD replaces unstable symbolic regression with tensor decomposition to discover optimal task-driven neurons, achieving enhanced stability, faster convergence, and state-of-the-art performance across diverse benchmarks.

## Executive Summary
This paper introduces NeuronSeek-TD, a method that addresses instability in the original NeuronSeek-SR framework by replacing symbolic regression with tensor decomposition. The approach discovers optimal neuronal formulas through CP decomposition with sparsity regularization, offering enhanced stability and faster convergence. The authors also establish theoretical guarantees that task-driven neurons with common activation functions can approximate any continuous function with arbitrary precision. Extensive experiments demonstrate that NeuronSeek-TD outperforms NeuronSeek-SR on synthetic and real-world datasets, achieving state-of-the-art results across diverse benchmarks including tabular data and image recognition.

## Method Summary
NeuronSeek-TD discovers task-driven neurons by first defining an initial formula as a polynomial with trigonometric terms, then factorizing the weight tensors using CP decomposition. The method optimizes low-rank coefficients with L1 regularization on both polynomial coefficients and tensor factors, followed by a statistically grounded thresholding mechanism to prune insignificant terms. The resulting compact formula is parameterized into neurons and integrated into fully connected or convolutional networks. The approach offers a stable continuous optimization alternative to the unstable discrete search problem of symbolic regression, while maintaining the expressive power needed for complex tasks.

## Key Results
- NeuronSeek-TD achieves enhanced stability and faster convergence compared to NeuronSeek-SR by replacing symbolic regression with tensor decomposition
- Theoretical guarantees establish that task-driven neurons with common activation functions can approximate any continuous function with arbitrary precision
- Extensive experiments show NeuronSeek-TD outperforms NeuronSeek-SR on synthetic and real-world datasets with significant improvements in both regression and classification tasks
- The method achieves state-of-the-art results across diverse benchmarks including tabular data and image recognition while requiring fewer parameters than competing approaches

## Why This Works (Mechanism)

### Mechanism 1
Replacing genetic programming-based symbolic regression with tensor decomposition converts an unstable discrete search problem into a stable continuous optimization problem. NeuronSeek-TD assumes the optimal neuronal formula can be expressed as a high-order polynomial with trigonometric terms, then factorizes weight tensors into rank-1 components via CP decomposition, reducing formula discovery to gradient-based coefficient optimization.

### Mechanism 2
Dual L1 regularization on polynomial coefficients and tensor factors, combined with post-training thresholding, yields compact interpretable formulas without manual term selection. The objective function adds sparsity penalties to the task loss, driving insignificant coefficients toward zero, then uses a data-driven threshold (mean minus standard deviation) to prune remaining small terms.

### Mechanism 3
Task-driven neurons with fixed common activation functions can approximate any continuous function with arbitrary precision, achieving "super-super-expressiveness" where both parameter count and magnitude remain bounded. The proof constructs a discrete dynamical system where layer-wise transformation is a unimodal map with dense orbits, enabling point-fitting via composition while maintaining fixed parameters.

## Foundational Learning

- **Concept: Tensor Decomposition (CP/PARAFAC)**
  - Why needed here: Core technique replacing symbolic regression. Must understand how rank-R CP decomposition factorizes an n-th order tensor into outer products of vectors, reducing O(d^n) parameters to O(nRd).
  - Quick check question: Given a 3rd-order tensor of size d×d×d, what's the parameter count for rank-R CP decomposition vs. full tensor storage?

- **Concept: Super-expressive Activation Functions**
  - Why needed here: The paper's theoretical contribution extends this concept. Must understand that traditional activations (ReLU, sigmoid) require growing network width/depth for arbitrary precision, whereas super-expressive activations achieve it with fixed architecture.
  - Quick check question: Why does the paper's "super-super-expressive" property differ from standard super-expressiveness? What remains fixed in addition to parameter count?

- **Concept: Unimodal Maps and Dense Orbits (Dynamical Systems)**
  - Why needed here: Underpins the approximation theory proof. The lemma shows polynomial-derived task-driven neurons can create maps with dense orbits in [0,1], enabling point-fitting via composition.
  - Quick check question: What property of a unimodal map T ensures that for any open sets U, V, there exists m such that T^m(U) ∩ V ≠ ∅?

## Architecture Onboarding

- **Component map:**
  Input Data → Flatten to vector z ∈ R^d
       ↓
  Stage 1 (Formula Discovery):
  [Initial Formula: polynomial(orders 1..N) + sin(z)]
       ↓
  [CP Decomposition: factorize W[n] tensors]
       ↓
  [Optimize: task_loss + L1(C) + L1(tensor_factors)]
       ↓
  [Thresholding: prune terms with |C_i| < μ - σ]
       ↓
  Output: Compact neuronal formula
       ↓
  Stage 2 (Network Construction):
  [Parameterize formula → aggregation function]
       ↓
  [Build FCN (tabular) or ConvNet (images)]
       ↓
  [Initialize: W1 ~ N(0, √(1/32kn)), others ≈ 0.001]
       ↓
  [Train end-to-end with standard optimizer]

- **Critical path:**
  1. **Hyperparameter selection** (order N, rank R, regularization λ₁, λ₂): Must precede Stage 1.
  2. **Formula discovery on training data subset**: Stage 1 runs once before network construction.
  3. **Threshold computation**: Must wait for Stage 1 convergence.
  4. **Network initialization**: W₁ gets normal init, higher-order terms start near zero.

- **Design tradeoffs:**
  | Choice | Benefit | Cost |
  |--------|---------|------|
  | Higher polynomial order N | Captures complex variable interactions | Exponential growth in tensor size; risk of overfitting |
  | Higher rank R | More expressive per order | More parameters; potential instability |
  | Strong L1 (high λ₁, λ₂) | Sparse, interpretable formulas | May prune useful terms; underfitting |
  | Include sin(z) term | Captures periodicity/high-frequency patterns | Adds parameters; may not help non-periodic data |
  | CP vs. Tucker decomposition | CP: O(nRd) params, faster | Tucker may capture more complex interactions |

- **Failure signatures:**
  - **Signature 1: All coefficients pruned by thresholding** → λ values too high or data doesn't fit polynomial+sin basis.
  - **Signature 2: Training instability in Stage 2** → Initialization may be wrong.
  - **Signature 3: No improvement over standard neurons** → Discovered formula may be near-linear.
  - **Signature 4: High variance across runs** → Check if Adam hyperparameters are set consistently.

- **First 3 experiments:**
  1. **Sanity check on synthetic polynomial data**: Generate data from a known polynomial. Run Stage 1 with N=3, R=5, minimal regularization. Verify MSE < 0.01 with noise level δ=0%, execution time < 15s.
  2. **Stability comparison on tabular data**: Take airfoil self-noise dataset. Run both NeuronSeek-SR and NeuronSeek-TD across 10 random initializations. Compute epoch-wise diversity and test MSE variance.
  3. **Integration test with ResNet on CIFAR-10**: Use discovered formula to replace first conv layer in ResNet18. Train for 50 epochs and compare accuracy vs. vanilla ResNet18.

## Open Questions the Paper Calls Out

### Open Question 1
Is the "super-super-expressive" construction practically feasible for real-world tasks given the growth in computational complexity required to achieve arbitrary approximation precision? The paper provides a theoretical existence proof but does not implement or benchmark this specific theoretical construction against standard backpropagation in the experiments.

### Open Question 2
Can NeuronSeek-TD maintain its efficiency advantages when scaling to high-dimensional data beyond small-scale image benchmarks or large language models? The experimental evaluation is limited to tabular data and small image datasets, leaving the method's performance on large-scale, high-resolution architectures unverified.

### Open Question 3
How sensitive is the performance of NeuronSeek-TD to the choice of elementary functions in the initial formula template? The method relies on a fixed initial formula combining polynomials and a trigonometric term, asserting it captures necessary patterns without verifying if other non-linear bases would yield better results.

## Limitations
- Theoretical approximation guarantee requires unbounded network depth, which may not hold in practical finite-depth implementations
- Choice of polynomial order N and rank R remains largely heuristic without clear guidelines for different task complexities
- Assumption that important terms exhibit clear magnitude separation for thresholding may fail for tasks requiring many weak interactions
- Direct comparison with other modern symbolic regression methods is absent

## Confidence
- **High confidence**: The CP decomposition framework for stable formula discovery, experimental demonstration of reduced variance across runs, and practical performance gains on tabular and image datasets
- **Medium confidence**: The super-super-expressiveness theoretical claim, sparsity thresholding mechanism
- **Low confidence**: The claim that N ≤ 4 orders are "reasonable" for arbitrary tasks

## Next Checks
1. **Finite-depth validation**: Test the approximation accuracy degradation as network depth is progressively reduced from the theoretical unlimited case
2. **Order sensitivity analysis**: Systematically vary polynomial order N across different dataset complexities to identify when the N ≤ 4 assumption breaks down
3. **Alternative sparsity baselines**: Compare the magnitude-thresholding approach against other sparsity-inducing methods to validate the claimed advantages of the statistical thresholding approach