---
ver: rpa2
title: 'PlaM: Training-Free Plateau-Guided Model Merging for Better Visual Grounding
  in MLLMs'
arxiv_id: '2601.07645'
source_url: https://arxiv.org/abs/2601.07645
tags:
- visual
- base
- multimodal
- language
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a training-free approach called Plateau-guided\
  \ model Merging (PlaM) to improve visual grounding in multimodal large language\
  \ models (MLLMs). The method identifies a three-stage pattern in MLLMs\u2014early-modal\
  \ separation, mid-modal alignment, and late-modal degradation\u2014through layer-wise\
  \ vision token masking."
---

# PlaM: Training-Free Plateau-Guided Model Merging for Better Visual Grounding in MLLMs

## Quick Facts
- arXiv ID: 2601.07645
- Source URL: https://arxiv.org/abs/2601.07645
- Reference count: 40
- Primary result: Training-free model merging that improves visual grounding in MLLMs by selectively merging base LM parameters into late decoder layers where performance plateaus

## Executive Summary
This paper introduces Plateau-guided model Merging (PlaM), a training-free approach to improve visual grounding in multimodal large language models (MLLMs). Through layer-wise vision token masking, the authors reveal a consistent three-stage pattern in MLLMs: early-modal separation, mid-modal alignment, and late-modal degradation. PlaM selectively merges base language model parameters into late decoder layers where performance plateaus, restoring degraded language reasoning while improving visual grounding. Experiments on five MLLMs (3B-8B parameters) and nine benchmarks show consistent improvements over base models, with attention analysis revealing shifts from diffuse to focused localization on task-relevant visual regions.

## Method Summary
PlaM operates in two stages: first, it identifies the plateau onset layer through depth-controlled vision token masking, revealing where visual information contributes diminishing returns; second, it merges base LM self-attention weights (Q/K/V/O) from the identified plateau layer to the final decoder layer using linear interpolation. The method uses frozen vision encoders and projectors, merging only attention projections while leaving MLP layers unchanged. Merge coefficients are tuned via grid search (0.0-1.5, step 0.1) for each dataset. The approach is training-free and can be applied to any MLLM with a base LM checkpoint, targeting the specific problem of degraded text reasoning capability in late decoder layers that undermines multimodal performance.

## Key Results
- Consistent improvements across five MLLMs (3B-8B parameters) on nine benchmarks
- Performance gains of 0.61-1.80 on MMStar, 2.50-22.86 on MME, and 0.56-2.11 on MMMU
- Attention analysis shows merging shifts patterns from diffuse to focused localization on task-relevant visual regions
- Merging restores degraded language reasoning capability while improving visual grounding

## Why This Works (Mechanism)

### Mechanism 1: Three-stage visual information utilization pattern
MLLMs exhibit early-modal separation (shallow cross-modal interaction), mid-modal alignment (rapid performance gains as visual-linguistic alignment strengthens), and late-modal degradation (visual signals absorbed into linguistic representations with diminishing returns). This pattern is identified through layer-wise vision token masking, where performance plateaus in late layers indicate genuine diminishing utility of visual information rather than masking artifacts.

### Mechanism 2: Recovery of degraded language reasoning
Multimodal instruction fine-tuning paradoxically degrades text reasoning capability in late decoder layers. Linear interpolation of base LM parameters into attention projections (Q/K/V/O) restores stable reasoning trajectories, compensating for distortion from aggressive cross-modal fusion. This recovered text capability enables better semantic decision-making about which visual cues to retrieve, improving multimodal performance.

### Mechanism 3: Attention redistribution toward focused localization
Merging increases attention mass toward vision tokens and shifts attention patterns from diffuse to focused localization on task-relevant regions. This manifests as higher Mass^(l)(T_ins → T_vis) during prompt encoding and higher Mass^(l)(T_res → T_vis) during generation, enabling explicit visual verification during decoding and better grounding quality.

## Foundational Learning

- **Transformer attention mechanism and query-key-value projections**: Understanding how attention projections govern token-to-token information flow and how parameter changes affect attention distributions is essential since PlaM merges only attention projections (Q/K/V/O). Quick check: If you double all values in a key projection matrix, what happens to attention weights when queries remain unchanged?

- **Model merging via linear interpolation**: The core operation W_merged = λ₁W_lm + λ₂W_vlm requires understanding when linear combination preserves functional capabilities and when it causes interference. Quick check: If two models have identical architecture but learned conflicting representations for the same task, what would simple linear interpolation likely produce?

- **Visual grounding and attention-to-image analysis**: The mechanistic explanation relies on interpreting attention heatmaps as evidence of visual grounding quality. Quick check: If a model attends strongly to a region but still predicts incorrectly, what does this suggest about the attention-grounding relationship?

## Architecture Onboarding

- **Component map**: Vision encoder (E_vis) -> Projector (P) -> Language decoder (L layers) -> Output
  - Vision encoder: Frozen, maps images to visual features
  - Projector: Frozen, maps visual features to language embedding space
  - Language decoder: Target for merging intervention (only self-attention projections merged)
  - Base LM checkpoint: Source of merged parameters

- **Critical path**: 
  1. Run layer-wise vision token masking to identify plateau onset layer k*
  2. Set merge start layer k₀ via nearest-neighbor search around k*
  3. Merge attention projections in layers {k₀, ..., L} using W_merged = λ₁W_lm + λ₂W_vlm
  4. Evaluate on target benchmark, tune λ₁, λ₂ via grid search (0.0-1.5, step 0.1)

- **Design tradeoffs**:
  - Merge start layer k₀: Earlier = more language recovery but risks disrupting multimodal alignment; Later = safer but smaller gains. Paper finds optimal around layer 18-28 depending on model.
  - Merge targets: Only attention vs. full layer. Paper shows attention-only is sufficient; MLP merging not explored.
  - Coefficient ranges: λᵢ ∈ [0,1.5] allows extrapolation beyond simple averaging. Paper reports some optimal λ₂ > 1.0.

- **Failure signatures**:
  - Over-merging (early k₀): Performance drops on MME, MMBench—benchmarks requiring early-to-mid fusion quality.
  - Under-merging (very late k₀): Minimal improvement, plateau not addressed.
  - Wrong coefficients: Large negative deltas on text-heavy benchmarks indicate disrupted multimodal adaptation.

- **First 3 experiments**:
  1. Replicate masking curves (Figure 2): On your target MLLM, run depth-controlled vision token masking on 2-3 benchmarks to identify k* (plateau onset). Verify three-stage pattern exists.
  2. Ablate merge start layer: Fix λ₁=0.5, λ₂=0.5, sweep k₀ from L/2 to L-2 layers. Plot performance vs. k₀ to confirm plateau-guided selection outperforms early/mid/full merging.
  3. Visualize attention shift: For 3-5 failure cases in your base model, generate attention heatmaps before and after merging. Verify diffuse→focused pattern matches Figure 4 and correlates with correct predictions.

## Open Questions the Paper Calls Out

- **Adaptive k₀ determination**: The optimal merge start layer configuration varies for different datasets, and determining the optimal configuration requires more experiments and costs. Can this be determined automatically to eliminate dataset-specific grid searches?

- **Advanced merging algorithms**: The paper uses simple linear interpolation (λW) and suggests that more complex model merging methods (e.g., Task Arithmetic, TIES) may yield better results, though this does not conflict with the current work.

- **MLP layer restoration**: The paper merges only attention projections, assuming degradation is attention-centric. Would restoring MLP layers in late layers provide additional benefits for visual grounding, or is the degradation strictly localized to the attention mechanism?

## Limitations

- The three-stage masking pattern lacks theoretical grounding for why late layers specifically degrade language reasoning capability
- Attention mass analysis shows correlation but cannot definitively prove causation between merging and improved grounding quality
- The method's reliance on identifying a specific plateau pattern may not generalize to all MLLM architectures or training regimes

## Confidence

- **High confidence**: The empirical performance improvements across nine benchmarks (0.61-1.80 on MMStar, 2.50-22.86 on MME, 0.56-2.11 on MMMU) are well-documented and reproducible
- **Medium confidence**: The mechanism explanations connecting plateau identification to language reasoning recovery are plausible but rely on correlational evidence rather than causal proof
- **Low confidence**: The claim that attention mass shifts directly cause improved visual grounding quality, as the paper does not establish a clear causal link between attention patterns and downstream performance

## Next Checks

1. **Break condition verification**: Systematically test what happens when vision tokens are masked in early layers (not late) to confirm the three-stage pattern is genuine and not an artifact of the masking procedure itself.

2. **Architecture generalization test**: Apply the same plateau identification and merging procedure to an MLLM with different architecture (e.g., different attention mechanisms or layer configurations) to verify the pattern generalizes beyond the five models tested.

3. **Attention-grounding causation test**: For failure cases where merged models predict incorrectly despite focused attention patterns, analyze whether attention mass actually correlates with correct grounding or if it can be misleading (e.g., attending to task-relevant but semantically incorrect regions).