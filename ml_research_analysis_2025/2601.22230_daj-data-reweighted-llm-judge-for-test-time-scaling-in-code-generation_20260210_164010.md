---
ver: rpa2
title: 'DAJ: Data-Reweighted LLM Judge for Test-Time Scaling in Code Generation'
arxiv_id: '2601.22230'
source_url: https://arxiv.org/abs/2601.22230
tags:
- training
- judge
- data
- reweighting
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DAJ, a data-reweighted LLM judge for test-time
  scaling in code generation. DAJ addresses the challenge of training reliable LLM
  judges by learning data-importance weights to optimize generalization on a held-out
  meta set aligned with target benchmarks.
---

# DAJ: Data-Reweighted LLM Judge for Test-Time Scaling in Code Generation

## Quick Facts
- arXiv ID: 2601.22230
- Source URL: https://arxiv.org/abs/2601.22230
- Reference count: 34
- Key outcome: DAJ achieves 84.7% on LiveCodeBench and 35.9% on BigCodeBench, outperforming leading baselines and proprietary models through bi-level data reweighting and reasoning-based judging.

## Executive Summary
DAJ addresses the challenge of training reliable LLM judges for code generation by learning data-importance weights via bi-level optimization. The framework trains a reasoning-based judge with verifiable rewards, automatically emphasizing hard problems, in-distribution samples, and trajectory-aligned data without hand-crafted heuristics. DAJ achieves state-of-the-art performance on LiveCodeBench and BigCodeBench by optimizing generalization on a held-out meta set aligned with target benchmarks.

## Method Summary
DAJ trains a reasoning-based LLM judge using bi-level optimization where a lower-level loop trains the judge on weighted training data while an upper-level loop optimizes weights to minimize loss on a held-out meta set. The judge generates explicit reasoning traces before selection and is trained with execution-verified rewards (1 for correct, 0.5 for incorrect, 0 for format error). At inference, multi-round pairwise voting with majority aggregation stabilizes final selection across candidates.

## Key Results
- Achieves 84.7% pass@1 on LiveCodeBench, outperforming all baseline test-time scaling methods
- Achieves 35.9% pass@1 on BigCodeBench hard tasks, surpassing leading proprietary models
- Demonstrates significant improvements over existing methods through data-reweighted learning framework

## Why This Works (Mechanism)

### Mechanism 1: Bi-Level Data Reweighting for Distribution Shift
Learning data importance weights via bi-level optimization improves judge generalization under distribution shift. The framework establishes a closed-loop feedback between model learning and data reweighting, automatically identifying training samples that transfer most effectively to the target distribution. The core assumption is that the meta set distribution is closer to the test-time distribution than raw training data, enabling meaningful weight assignment through gradient guidance.

### Mechanism 2: Reasoning-Based Judging with Verifiable Rewards
Requiring explicit step-by-step reasoning before selection, trained with execution-verified rewards, improves judge reliability over scalar-output reward models. The judge generates structured analysis (syntax, correctness, edge cases, I/O, performance) before outputting selection, with rewards automatically verifiable via test-case execution. This enables preference/RL optimization without human annotations, assuming reasoning traces help and execution correctness sufficiently proxies judgment quality.

### Mechanism 3: Multi-Round Pairwise Voting at Inference
Multi-round pairwise comparison with majority voting stabilizes final selection across multiple candidates. The method repeatedly samples pairs uniformly with replacement for multiple rounds, accumulating votes to select the highest-voted candidate. This assumes judge predictions are noisy but better-than-random, allowing aggregation to reduce variance without requiring calibrated scores.

## Foundational Learning

- **Bi-level optimization (meta-learning)**: Why needed: The entire data reweighting mechanism rests on understanding nested optimization where inner loop learns model parameters and outer loop learns hyperparameters (data weights). Quick check: Can you explain why computing the exact gradient through the inner-loop optimum is often intractable and why approximate unrolling is used instead?

- **Preference optimization (DPO/KTO/ORPO) and RLVR (GRPO)**: Why needed: DAJ is trained with either preference optimization or reinforcement learning using verifiable rewards; understanding trade-offs is critical for implementation. Quick check: What is the key difference between DPO (requires paired preferences) and KTO (does not), and how does GRPO differ from both?

- **Test-time scaling (Best-of-N, self-consistency)**: Why needed: DAJ is a test-time scaling method; understanding the broader paradigm helps contextualize its role. Quick check: In Best-of-N selection, what is the role of the judge/reward model, and how does self-consistency differ?

## Architecture Onboarding

- **Component map**: Training data (D_tr + D_meta) -> Reweighting module (domain/instance weights) -> Judge model (reasoning LLM) -> Verifiable reward (execution-based) -> Training loop (bi-level optimization) -> Inference (multi-round pairwise voting)

- **Critical path**: Construct D_tr and D_meta with temporal/difficulty splits -> Initialize weights uniformly or to 1 -> Inner loop: Update judge parameters on weighted loss -> Outer loop: Update weights to minimize meta-set loss -> Repeat until convergence -> At inference, sample candidates, run pairwise comparisons, aggregate votes

- **Design tradeoffs**: Domain reweighting is coarse with fewer parameters vs instance table's fine-grained approach that scales with data vs instance net's fixed-size generalization. Preference optimization is offline and stable vs RL's online complexity. Meta set size balances representativeness against computational cost.

- **Failure signatures**: Weights collapse to uniform indicating no learning signal, judge overfits to meta set showing poor test generalization, reasoning traces become short or degenerate suggesting reward hacking, voting shows no improvement over random indicating judge not better-than-random.

- **First 3 experiments**: Baseline comparison training judge without reweighting on D_tr vs DAJ to isolate reweighting gains. Reweighting ablation comparing domain, instance table, and instance net on LiveCodeBench hard split to identify granularity effects. Training objective ablation comparing DPO/KTO/ORPO/GRPO with fixed reweighting to determine optimal training paradigm.

## Open Questions the Paper Calls Out

### Open Question 1
Can the DAJ framework generalize effectively to domains lacking verifiable execution rewards, such as mathematical reasoning or creative writing? The methodology relies specifically on "verifiable rewards" derived from code execution, distinguishing it from domains where ground-truth rewards are harder to define. This remains unresolved as the paper evaluates exclusively on code generation benchmarks without testing limits when reward signals are sparse or subjective.

### Open Question 2
How robust is the bi-level optimization framework to the size and distributional alignment of the held-out meta set? The upper-level optimization depends entirely on a meta set representing the target distribution, but the paper uses a fixed split without analyzing sensitivity. It's unclear if the method fails or degrades significantly if the meta set is small, noisy, or imperfectly aligned with the test-time distribution.

### Open Question 3
Can the bi-level optimization process scale efficiently to judge models with significantly larger parameter counts (e.g., 70B+)? The authors note the prohibitive computational cost and mitigate it using small backbones (1.7B and 14B) and approximate unrolling steps. The computational feasibility and stability of calculating hypergradients for large-scale judge models remain unverified.

## Limitations
- Bi-level optimization stability concerns with aggressive meta learning rate (10⁻⁴) and one-step unrolling approximation may not capture full weight impact
- Meta set representativeness critical but limited by fixed temporal split and small BigCodeBench size (148 hard tasks)
- Generalizability of binary/ternary verifiable rewards to benchmarks with richer evaluation criteria remains unclear

## Confidence
- **High Confidence**: Core bi-level data reweighting and multi-round pairwise voting mechanisms are well-specified and align with established ML paradigms
- **Medium Confidence**: State-of-the-art performance claims based on paper's experiments but lack baseline ablations for isolating component contributions
- **Low Confidence**: Long-term stability of bi-level optimization with aggressive meta learning rate not demonstrated; behavior on benchmarks outside LiveCodeBench and BigCodeBench unknown

## Next Checks
1. **Reweighting Granularity Ablation**: Train DAJ with domain weights, instance table, and instance net on LiveCodeBench hard split, holding all other factors constant, to quantify impact of weight granularity on final performance

2. **Meta Set Size Sensitivity**: Systematically reduce size of meta set and measure degradation in judge performance on target benchmark to assess robustness of bi-level optimization to meta set size

3. **Cross-Benchmark Transfer**: Evaluate a DAJ judge trained on LiveCodeBench on a different code generation benchmark (e.g., HumanEval or MBPP) to test generalizability beyond training distribution