---
ver: rpa2
title: Efficiently Generating Correlated Sample Paths from Multi-step Time Series
  Foundation Models
arxiv_id: '2510.02224'
source_url: https://arxiv.org/abs/2510.02224
tags:
- time
- sample
- paths
- series
- copula
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of generating realistic, correlated
  sample paths from multi-step time series foundation models (TSFMs), which typically
  only provide independent marginal distributions. To solve this, the authors propose
  a copula-based approach that combines high-quality marginal predictions from TSFMs
  with a Gaussian copula to impose realistic correlation structures.
---

# Efficiently Generating Correlated Sample Paths from Multi-step Time Series Foundation Models

## Quick Facts
- arXiv ID: 2510.02224
- Source URL: https://arxiv.org/abs/2510.02224
- Reference count: 23
- Primary result: Copula-based sampling generates realistic correlated sample paths from TSFMs 3.7x to 100.4x faster than autoregressive sampling while maintaining or improving sample path quality

## Executive Summary
This paper addresses the fundamental challenge that multi-step time series foundation models (TSFMs) output independent marginal distributions for each forecasting horizon, resulting in unrealistic "jagged" sample paths when naively sampled. The authors propose a copula-based approach that combines high-quality marginal predictions from TSFMs with a Gaussian copula to impose realistic correlation structures. By parameterizing the copula using empirical autocorrelation and converting quantile predictions into full marginal distributions, they generate correlated sample paths orders of magnitude faster than autoregressive sampling while achieving improved sample path quality by mitigating the snowballing error phenomenon.

## Method Summary
The method decomposes the joint predictive distribution into marginal distributions (from TSFM) and a copula (encoding temporal correlation). It uses a Gaussian copula with AR(1) correlation structure, where the correlation parameter ρ is computed from empirical autocorrelation of the input series. The approach converts TSFM quantile predictions into full marginal CDFs using incremental quantile functions with linear interpolation and exponential decay extrapolation. Sample paths are generated by sampling from the multivariate normal distribution with Toeplitz covariance Σᵢⱼ = ρ^|i−j|, transforming to uniform marginals via the Gaussian inverse CDF, and then mapping through the inverse empirical CDFs to obtain final sample paths.

## Key Results
- Copula-based sampling achieves 3.7x to 100.4x speedup compared to autoregressive sampling across benchmark datasets
- Improved CRPS scores at long horizons (10-96 steps) on M4 Daily and M4 Yearly datasets
- Comparable or better variogram scores indicate realistic correlation structure preservation
- Mitigation of snowballing error phenomenon results in more plausible sample paths

## Why This Works (Mechanism)

### Mechanism 1: Copula Decomposition Separates Marginal Quality from Correlation Structure
The joint predictive distribution can be factored into marginal distributions (from TSFM) and a copula (encoding temporal correlation), allowing independent optimization of each component. By Sklar's Theorem, any joint distribution p(X₁,...,Xₕ) can be written as C(F₁(x₁),...,Fₕ(xₕ)) where Fᵢ are marginal CDFs and C is a copula. The TSFM provides high-quality Fᵢ; the copula injects temporal dependencies that marginals lack.

### Mechanism 2: AR(1) Correlation Parameterization Captures Autocorrelation Efficiently
A Gaussian copula with Toeplitz covariance Σᵢⱼ = ρ^|i−j|, where ρ is the empirical autocorrelation coefficient, provides a sufficient approximation of temporal correlation for most time series. Time series typically exhibit decaying autocorrelation with lag. The AR(1) structure models this with a single scalar ρ computed from historical data (Corr(x₁:ₜ₋₁, x₂:ₜ)), avoiding learned parameters.

### Mechanism 3: One-Shot Sampling Avoids Snowballing Error from Autoregressive Rollout
Generating sample paths in a single forward pass prevents error accumulation that occurs when autoregressive models condition on their own noisy predictions. Autoregressive sampling draws x̂ₜ₊₁ ∼ p(xₜ₊₁|x₁:ₜ), then x̂ₜ₊₂ ∼ p(xₜ₊₂|x₁:ₜ, x̂ₜ₊₁), etc. Errors in early samples propagate. Copula sampling draws all horizons jointly from the correlation structure without sequential conditioning.

## Foundational Learning

- **Concept: Sklar's Theorem and Copula Functions**
  - Why needed: The entire method rests on decomposing joint distributions via copulas
  - Quick check: Given uniform random variables U₁, U₂ on [0,1], how would you construct correlated Gaussian variates using a copula?

- **Concept: Marginal vs. Joint Predictive Distributions**
  - Why needed: Multi-step TSFMs output p(xₜ₊ᵢ|x₁:ₜ) for each i independently, not p(xₜ₊₁:ₜ₊ₕ|x₁:ₜ)
  - Quick check: If you sample independently from well-calibrated marginals, why might joint statistics (e.g., sum over a window) be poorly estimated?

- **Concept: Quantile Functions and Empirical CDF Fitting**
  - Why needed: TSFMs output quantile knots (e.g., {0.1, 0.2, ..., 0.9}), not full CDFs
  - Quick check: Given quantile predictions q₀.₁=5, q₀.₅=7, q₀.₉=12, how would you estimate the probability that x < 6?

## Architecture Onboarding

- **Component map:** TSFM Backbone -> Quantile-to-CDF Converter (IQF) -> Copula Parameterizer -> Gaussian Copula Sampler -> Inverse CDF Transform

- **Critical path:**
  1. Forward pass through TSFM → quantile predictions
  2. Fit empirical CDFs per horizon
  3. Compute ρ from input series (or run neural parameterizer)
  4. Sample from multivariate normal with covariance Σᵢⱼ = ρ^|i−j|
  5. Transform via Φ⁻¹ (Gaussian inverse CDF) → uniform marginals
  6. Apply inverse empirical CDFs → final sample paths

- **Design tradeoffs:**
  - Simple ρ vs. learned copula: Empirical ρ is fast and interpretable; neural parameterizers (GRU, MLP, TCN) offer modest gains but add complexity
  - AR(1) vs. richer covariance: AR(1) assumes monotonic autocorrelation decay; more flexible parameterizations may help for seasonal series but require more data
  - Number of quantile knots: Fewer knots faster but coarser CDF approximation; the paper uses {0.1,...,0.9} as provided by TSFMs

- **Failure signatures:**
  - Jagged paths despite copula: ρ near 0 (weak autocorrelation) or CDF fitting issues
  - Over-smoothed paths: ρ too high (e.g., computing on wrong scale or window)
  - Speedup not realized: Pre-trained neural copula adds inference overhead negating parallel sampling gains
  - Poor CRPS at long horizons: TSFM marginal quality degrades; copula cannot fix this

- **First 3 experiments:**
  1. Reproduce Figure 1 on a single series: Implement naive, autoregressive, and copula sampling; visually confirm copula produces smooth paths similar to autoregressive but 10-100x faster
  2. Ablate ρ source: Compare empirical ρ vs. random ρ vs. ρ=0 on variogram score across M4 Daily subset to quantify correlation structure contribution
  3. Stress test marginal quality: Inject synthetic bias into TSFM quantile predictions at specific horizons; verify that copula CRPS degrades similarly to autoregressive, confirming copula doesn't mask marginal errors

## Open Questions the Paper Calls Out
None

## Limitations
- The AR(1) Gaussian copula parameterization may not capture complex temporal dependencies in seasonal or non-linearly autocorrelated series
- Quality of generated paths is fundamentally limited by TSFM's marginal predictive distributions—copula cannot correct systematic biases in the base model
- Marginal benefits of learned neural copula parameterizers versus simple empirical autocorrelation are not conclusively established

## Confidence
- **High confidence:** The speedup claims (3.7x to 100.4x) and basic mechanism of copula decomposition are well-supported by results and mathematical foundation
- **Medium confidence:** The improvement in CRPS and variogram scores is demonstrated empirically but may be dataset-dependent
- **Low confidence:** The marginal benefits of learned neural copula parameterizers versus simple empirical autocorrelation are not conclusively established

## Next Checks
1. Apply the method to M4 Monthly and Quarterly subsets to assess performance on series with stronger seasonal patterns and different autocorrelation structures
2. Systematically compare AR(1) copula performance against empirical covariance matrices (non-Toeplitz) on a subset of series to quantify the cost of the parametric assumption
3. Intentionally introduce horizon-specific biases into TSFM quantile predictions and measure how copula sampling propagates or masks these errors compared to autoregressive sampling