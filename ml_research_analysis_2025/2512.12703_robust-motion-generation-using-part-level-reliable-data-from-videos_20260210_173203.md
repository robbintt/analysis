---
ver: rpa2
title: Robust Motion Generation using Part-level Reliable Data from Videos
arxiv_id: '2512.12703'
source_url: https://arxiv.org/abs/2512.12703
tags:
- motion
- data
- parts
- credible
- noisy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a robust framework for generating high-quality
  human motion from text by leveraging large-scale noisy motion data extracted from
  web videos. The key challenge is that many video frames contain missing or occluded
  body parts, making it difficult to fully utilize this data without introducing artifacts.
---

# Robust Motion Generation using Part-level Reliable Data from Videos

## Quick Facts
- arXiv ID: 2512.12703
- Source URL: https://arxiv.org/abs/2512.12703
- Reference count: 0
- Primary result: Introduces part-aware motion generation framework that achieves 19.68 FID and 0.71 R@1 on noisy video-derived motion data

## Executive Summary
This paper addresses the challenge of generating high-quality human motion from text using large-scale noisy motion data extracted from web videos. The key innovation is a framework that decomposes the human body into five parts and identifies "credible" parts—those clearly visible in video frames—using joint confidence scores from pose estimation models. By encoding only these credible parts into a clean latent space and predicting masked credible parts while ignoring noisy ones, the method achieves superior performance on both clean and noisy datasets. The authors also introduce K700-M, a new dataset of 200k noisy motion sequences from real-world videos, and demonstrate significant improvements over strong baselines.

## Method Summary
The proposed framework tackles the challenge of noisy motion data by first decomposing the human body into five parts and identifying credible (visible) parts using joint confidence scores from pose estimation models. A part-aware variational autoencoder (P-VAE) encodes only the credible parts into a clean latent space, avoiding noise from occluded or missing parts. A robust part-aware masked autoregressive model is then trained to predict masked credible parts while ignoring noisy ones, with a diffusion head added to refine the output. The method is evaluated on both clean datasets (HumanML3D, BABEL) and the newly introduced noisy K700-M dataset, showing significant improvements in motion quality, semantic consistency, and diversity.

## Key Results
- On K700-M dataset: 19.68 FID and 0.71 R@1 precision, compared to 25.23 FID and 0.59 R@1 for baseline
- Significant improvements in motion quality, semantic consistency, and diversity on both clean and noisy datasets
- Outperforms strong baselines across all evaluation metrics
- Demonstrates robustness to missing or occluded body parts in video-derived motion data

## Why This Works (Mechanism)
The method works by creating a clean latent representation that excludes unreliable data from occluded or missing body parts. By decomposing the body into five parts and using confidence scores to identify credible parts, the framework ensures that only high-quality information flows through the encoding and prediction pipeline. The masked autoregressive model's ability to predict credible parts while ignoring noisy ones allows the system to maintain semantic consistency even when parts of the input data are unreliable. The diffusion head further refines the output, ensuring smooth and realistic motion generation.

## Foundational Learning
- **Part decomposition strategy**: Why needed - enables independent processing of body segments to handle occlusions; Quick check - evaluate performance with different numbers of body parts
- **Joint confidence scoring**: Why needed - identifies which body parts are reliably visible in video frames; Quick check - test robustness across different pose estimation models
- **Masked autoregressive prediction**: Why needed - allows model to predict missing parts while ignoring noisy data; Quick check - measure performance degradation with systematic confidence score errors
- **Variational autoencoder encoding**: Why needed - creates clean latent representation excluding unreliable data; Quick check - compare latent space quality with and without part-wise filtering
- **Diffusion refinement**: Why needed - produces smooth, realistic motion from predicted latent representations; Quick check - evaluate motion smoothness metrics

## Architecture Onboarding

**Component map**: Video frames -> Pose estimation -> Joint confidence scoring -> Part decomposition -> P-VAE encoding -> Masked autoregressive prediction -> Diffusion head -> Generated motion

**Critical path**: The most critical sequence is Pose estimation → Joint confidence scoring → Part decomposition → P-VAE encoding → Masked autoregressive prediction → Diffusion head. Any failure in accurately identifying credible parts or encoding them cleanly will propagate through the entire pipeline.

**Design tradeoffs**: The framework trades computational complexity (processing five body parts independently) for robustness to noisy data. The arbitrary choice of five body parts may limit generalizability compared to anatomically-based segmentation. The reliance on confidence scores from pose estimation introduces a single point of potential failure.

**Failure signatures**: Systematic misidentification of occluded parts as credible will produce artifacts in generated motion. Poor part decomposition choices will lead to unrealistic motion patterns at part boundaries. Over-reliance on the diffusion head may indicate fundamental issues in the latent space representation.

**First experiments**:
1. Test part decomposition with 3, 5, 7, and 9 body parts to identify optimal segmentation
2. Evaluate performance with synthetic confidence score corruption to measure robustness
3. Compare motion quality with different pose estimation model confidence scoring methods

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Reliance on joint confidence scores from pose estimation models may propagate systematic errors if the model misidentifies occluded parts
- The choice of five body parts appears arbitrary without ablation studies on different decompositions
- Performance gains on clean datasets are less dramatic than on noisy datasets
- Method specifically designed for and validated on video-derived noisy data, limiting generalizability

## Confidence
- **High confidence**: Overall framework effectiveness and quantitative results validity on presented datasets
- **Medium confidence**: Generalizability of part decomposition strategy and robustness of credibility assessment mechanism
- **Low confidence**: Performance without large-scale video data, as approach is specifically designed for noisy video extraction

## Next Checks
1. Conduct ablation studies testing alternative part decompositions (varying number and composition of body parts) to determine optimal segmentation strategies
2. Evaluate the framework's robustness by deliberately introducing systematic errors in pose estimation confidence scores and measuring downstream performance degradation
3. Test the model's zero-shot transfer capability on datasets with different motion characteristics or from different domains to assess generalizability beyond the K700-M dataset