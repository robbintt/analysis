---
ver: rpa2
title: 'Once Upon an Input: Reasoning via Per-Instance Program Synthesis'
arxiv_id: '2510.22849'
source_url: https://arxiv.org/abs/2510.22849
tags:
- code
- program
- pips
- answer
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of multi-step reasoning in large
  language models (LLMs), which struggle with complex problems despite excelling at
  zero-shot inference. The authors propose Per-Instance Program Synthesis (PIPS),
  a method that generates and refines executable programs at the instance level using
  structural feedback, without relying on task-specific specifications or explicit
  test cases.
---

# Once Upon an Input: Reasoning via Per-Instance Program Synthesis

## Quick Facts
- arXiv ID: 2510.22849
- Source URL: https://arxiv.org/abs/2510.22849
- Authors: Adam Stein; Neelay Velingker; Mayur Naik; Eric Wong
- Reference count: 40
- Improves harmonic mean accuracy by up to 8.6% and 9.4% over Program of Thought and Chain of Thought respectively

## Executive Summary
This paper introduces Per-Instance Program Synthesis (PIPS), a method that dynamically decides between program synthesis and chain-of-thought reasoning for each problem instance. PIPS addresses key challenges in multi-step reasoning by using an LLM-based confidence metric to select the optimal strategy per instance, iteratively refining programs using structural feedback without requiring behavioral specifications, and explicitly extracting structured symbols from unstructured inputs before program synthesis. The method achieves significant improvements across 30 benchmarks and three frontier LLMs, reducing undesirable program generations by 65.1% while improving overall accuracy through better handling of both algorithmic and non-algorithmic reasoning tasks.

## Method Summary
PIPS combines dynamic strategy selection with iterative program refinement to improve multi-step reasoning. The system uses a confidence metric based on 10 self-reflection criteria to decide between direct chain-of-thought reasoning and program synthesis for each instance. When synthesis is selected, PIPS first extracts salient entities and relationships into a JSON structure, then generates executable Python code that operates on this structured input. A structural evaluator checks for eight failure modes (trivial answers, type errors, syntax errors, etc.) and provides explicit feedback for regeneration. The synthesis loop iterates up to k times (default k=30) to produce well-formed programs. PIPS operates transductively without requiring input-output examples or test cases, distinguishing it from traditional program synthesis approaches.

## Key Results
- Improves harmonic mean accuracy by up to 8.6% over Program of Thought and 9.4% over Chain of Thought
- Reduces undesirable program generations by 65.1% on algorithmic tasks compared to PoT with Gemini-2.0-Flash
- Per-instance strategy selection achieves 65.3% accuracy on choosing correct method with 2.2% absolute gain in harmonic mean accuracy
- Explicit symbolic extraction alone improves harmonic mean by 4% on BBEH benchmark

## Why This Works (Mechanism)

### Mechanism 1: Instance-Level Strategy Selection
Selecting between program synthesis and CoT on a per-instance basis improves overall accuracy compared to always using either approach. A confidence metric derived from 10 self-reflection criteria produces a score vector S(x), and a logistic classifier trained on a calibration set predicts which strategy is more likely to succeed for that specific instance. This works because LLMs can accurately estimate their own likelihood of success via structured self-reflection before attempting a solution. The switch selects the correct method 65.3% of the time, yielding a 2.2% absolute gain in harmonic mean accuracy. Without calibration data, the zero-shot switch degrades by ~3%.

### Mechanism 2: Iterative Program Refinement via Structural Feedback
Rejecting and regenerating programs based on structural checks reduces undesirable code patterns and improves correctness. An evaluator checks for eight failure modes including trivial/hardcoded answers, type errors, syntax errors, and missing input dependence. Programs failing checks are regenerated with explicit feedback about the issues detected. This works because structural properties of code correlate with solution correctness, and LLMs can fix flagged issues without behavioral specifications. Trivial programs reduced by as much as 75.6%, type and syntax issues reduced by 49.2% and 86.8% respectively.

### Mechanism 3: Explicit Symbolic Extraction Before Program Synthesis
Decoupling perceptual inference from program synthesis improves code quality and reduces brittle approaches. First, an LLM extracts salient entities, attributes, and relationships into a JSON structure with an ad-hoc schema. The program then operates on this structured input rather than raw text/images. This works because LLMs are better at perceptual understanding than programs, and programs are better at algorithmic computation than LLMs. Ablation shows 4% harmonic mean improvement on BBEH from explicit inputs alone, and the method never tries to manually process images unlike 12.7% of well-formed PoT code on multimodal benchmarks.

## Foundational Learning

- **Chain-of-Thought (CoT) Prompting**: Needed because PIPS's baseline comparison and fallback strategy when synthesis is inappropriate. Quick check: Can you explain why CoT produces "unfaithful" reasoning (correct answer, wrong reasoning steps)?

- **Program of Thought (PoT)**: Needed because PIPS is positioned as addressing PoT's failure modes. PoT generates input-free programs that often collapse to trivial solutions. Quick check: What distinguishes PoT from simply asking an LLM to write code?

- **Transductive vs. Inductive Program Synthesis**: Needed because PIPS operates transductively (solving for a specific instance) rather than inductively (learning a general program). Section M formalizes this connection. Quick check: Why does PIPS not require input-output examples or test cases?

## Architecture Onboarding

- **Component map**: Input → Algorithmicity Selector → (if synthesis) Symbol Extractor → Program Generator → Program Evaluator → (if fail) Regeneration with feedback → Execution → Answer

- **Critical path**: Input → Selector decision → (if synthesis) Symbol extraction → Program generation → Evaluation → (if fail) Regeneration with feedback → Execution → Answer

- **Design tradeoffs**: Zero-shot switch vs. calibrated classifier (zero-shot requires no data but ~3% lower accuracy); Iteration budget k scales accuracy but costs increase ~3-4x vs. single-pass PoT; More evaluation checks reduce trivial programs but may reject valid creative solutions

- **Failure signatures**: High iteration count without passing indicates extracted symbols likely incomplete or problem genuinely non-algorithmic; Trivial programs passing checks mean switch failed to route to CoT; Type mismatches at execution suggest return type check insufficient

- **First 3 experiments**: 1) Reproduce Figure 3b on held-out dataset comparing PoT vs. CoT on algorithmic vs. non-algorithmic splits to validate problem taxonomy; 2) Ablate the switch by running PIPS with forced synthesis on all instances (expect ~2.5% drop); 3) Scale iteration budget plotting harmonic mean accuracy vs. k ∈ {0, 1, 3, 5, 10} to find cost-accuracy frontier

## Open Questions the Paper Calls Out

### Open Question 1
How can a reasoning system optimally decompose problems that require both program synthesis and chain-of-thought reasoning within a single instance? The current binary switch selects only one strategy per instance; no mechanism exists to interleave synthesis and textual reasoning. Future work can tackle methods for problem decomposition and composing program synthesis with other forms of reasoning.

### Open Question 2
What additional structural patterns beyond the eight current heuristics could systematically detect undesirable LLM-generated programs? The current evaluator uses hand-crafted checks; no systematic study has characterized the full space of undesirable outputs. Further work is needed to determine if there are more undesirable patterns in LLM-generated code.

### Open Question 3
How can faithfulness guarantees be provided for the symbolic extraction step that converts unstructured inputs into structured program inputs? While PIPS offers interpretable reasoning when using code, the conversion of the input to symbolic form still lacks faithfulness guarantees. The LLM-based extraction may omit or misrepresent information, and errors propagate to the synthesized program without detection.

## Limitations

- Per-instance confidence switch relies on self-reflection criteria that may not generalize to domains where LLMs struggle with self-evaluation
- Structural checks cannot detect semantic errors in logic - programs with correct syntax but wrong logic pass through undetected
- Ad-hoc JSON schema for symbolic extraction varies per instance, making it difficult to assess consistency or completeness across different problems

## Confidence

- **High confidence**: Iterative structural refinement mechanism (empirical improvements are substantial and measurable)
- **Medium confidence**: Per-instance strategy selection (works well on tested benchmarks but generalization is uncertain)
- **Medium confidence**: Symbolic extraction benefits (reduces image processing complexity but ad-hoc schemas lack systematic validation)
- **Low confidence**: Long-term robustness across diverse domains (results concentrated on 30 benchmarks with specific characteristics)

## Next Checks

1. **Cross-domain generalization test**: Apply PIPS to diverse tasks outside original 30 benchmarks (scientific reasoning, legal analysis, creative writing) to assess whether confidence switch maintains >60% accuracy without retraining.

2. **Semantic error detection evaluation**: Create benchmark of programs that pass all structural checks but contain logical errors. Measure evaluator's false positive rate and assess whether additional semantic validation layers are needed.

3. **Symbolic extraction consistency audit**: Analyze 100+ extracted JSON schemas across different instances of same problem type to quantify schema variability and identify patterns where critical information might be systematically missed.