---
ver: rpa2
title: Can we use LLMs to bootstrap reinforcement learning? -- A case study in digital
  health behavior change
arxiv_id: '2511.17630'
source_url: https://arxiv.org/abs/2511.17630
tags:
- samples
- real
- human
- prompt
- behavioral
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores using LLMs to generate user interaction samples
  for training RL models in digital health behavior change. They compare LLM-generated
  samples to real behavioral data from four smoking cessation and mental well-being
  studies.
---

# Can we use LLMs to bootstrap reinforcement learning? -- A case study in digital health behavior change

## Quick Facts
- arXiv ID: 2511.17630
- Source URL: https://arxiv.org/abs/2511.17630
- Authors: Nele Albers; Esra Cemre Su de Groot; Loes Keijsers; Manon H. Hillegers; Emiel Krahmer
- Reference count: 40
- Primary result: LLM-generated samples can produce RL policies outperforming worst and random baselines, with best models reaching human rater performance

## Executive Summary
This paper explores whether large language models can generate synthetic interaction samples to bootstrap reinforcement learning for digital health behavior change when no real data exists. The authors compare LLM-generated samples against real behavioral data from four smoking cessation and mental well-being studies, finding that Llama-3.3-70B consistently produces the best results. Generated samples enable learning policies that outperform baseline approaches and approach human rater performance, though a substantial gap remains between generated and real samples. The study demonstrates that LLMs can serve as a viable source of behavioral priors for initial RL design in cold-start scenarios.

## Method Summary
The authors use five open-weight LLMs to generate synthetic interaction samples ⟨s_t, a_t, r_t, s_{t+1}⟩ for four behavioral studies. They employ different prompt strategies including base, chain-of-thought, and few-shot approaches with real examples. For each study, they generate 500 samples per action and estimate reward and transition functions, then derive policies using offline reinforcement learning. Performance is evaluated in simulations based on held-out real behavioral data, measuring both policy performance (cumulative reward) and dynamics accuracy (L1 error between estimated and real functions).

## Key Results
- Llama-3.3-70B generally outperforms other models and approaches human rater performance in policy learning
- Few-shot prompting (5-10 examples) improves transition function estimation but has inconsistent effects on policy performance and reward estimation
- Chain-of-thought prompting shows inconsistent effects across studies, sometimes improving and sometimes degrading performance
- All LLM-based policies outperform worst and random baselines, though remain below optimal policies trained on real data
- A consistent gap exists between generated and real samples, suggesting fundamental challenges in behavioral prediction

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs can generate interaction samples that enable learning RL policies outperforming naïve baselines, even without domain-specific training.
- **Mechanism:** Pre-trained LLMs encode implicit behavioral priors from their training corpus (including health, psychology, and behavioral science text). When prompted with a state-action description, the model generates plausible next-state and reward predictions by pattern-matching to these priors. These samples populate an estimated dynamics model that, while imperfect, captures enough signal to guide policy selection away from clearly poor actions.
- **Core assumption:** The LLM's pre-training distribution includes sufficient coverage of human behavioral patterns in health contexts to generate samples with non-trivial correlation to real dynamics. This is not guaranteed for all domains or populations.
- **Evidence anchors:**
  - [abstract] "LLM-generated samples can produce policies and reward/transition functions that outperform worst and random baselines, with the best models reaching human rater performance."
  - [section: Results] "Llama-3.3-70B is the best-performing LLM for that study, almost reaching the performance of π*... while all LLM-based policies outperform the lowest baselines"
  - [corpus] Weak direct corpus support; neighbor papers focus on anomaly detection and control systems rather than behavioral simulation.
- **Break condition:** If the target behavior involves populations or contexts poorly represented in LLM training data (e.g., specific cultural contexts, rare conditions), generated samples may diverge substantially from real dynamics. The paper notes LLMs may be "less representative of people with a lower socioeconomic position or racial minorities" (p.5).

### Mechanism 2
- **Claim:** Few-shot prompting with 5-10 real examples improves transition function estimation, but effects on policy performance and reward estimation are inconsistent.
- **Mechanism:** Providing real examples in the prompt gives the LLM a local reference distribution to condition on, reducing hallucination variance for structured outputs like state transitions. However, this in-context learning does not systematically correct for fundamental gaps in behavioral modeling; it primarily anchors format and magnitude.
- **Core assumption:** The few real examples provided are representative of the broader dynamics. Assumption: the benefit comes from distributional anchoring rather than mere output formatting.
- **Evidence anchors:**
  - [section: Results] "few-shot prompting is most beneficial for estimating the transition function, with clear improvements for studies 2 and 3... For the rewards, we find that few-shot prompting leads to clear improvements for study 1... no changes for study 1 [sic - likely study 2], and a clear deterioration for study 3"
  - [section: Discussion] "providing between five and ten real examples per action tends to help for estimating the transition function, with mixed effects for the policy performance and reward estimation."
  - [corpus] No direct corpus evidence for few-shot behavioral simulation.
- **Break condition:** If the available real examples are noisy, non-representative, or contradictory, few-shot prompting may degrade performance by anchoring to outliers.

### Mechanism 3
- **Claim:** Both LLM-generated and human-predicted samples exhibit a consistent gap from real behavioral data, suggesting a fundamental "prediction-implementation gap" rather than an LLM-specific limitation.
- **Mechanism:** Predicting how a described person will respond to an intervention is inherently different from actually being that person making the decision. Humans exhibit optimism bias and other cognitive distortions when predicting behavior. LLMs, trained on human-generated text, may mirror these biases. The residual gap reflects the difference between imagined and enacted behavior.
- **Core assumption:** The gap is not primarily due to prompt engineering failures but to the intrinsic difficulty of behavioral prediction without lived context.
- **Evidence anchors:**
  - [section: Discussion] "remaining differences between generated and real behavioral samples are due to a gap between envisioning and actually doing behavior rather than a limitation specific to using LLMs rather than human raters"
  - [section: Results] "between 40 and 200 real behavioral samples per action are enough for the oracle to outperform any LLM as well as human raters"
  - [corpus] Neighbor paper on counseling AI notes similar engagement challenges with ambivalent individuals, but does not address simulation fidelity.
- **Break condition:** If prompts were enriched with detailed user personas, longitudinal context, or environmental factors, the gap might narrow—but the paper did not test this systematically.

## Foundational Learning

- **Concept: Markov Decision Process (MDP) formulation for behavioral interventions**
  - **Why needed here:** The entire approach requires framing behavior change as a sequential decision problem with states (user features), actions (intervention options), rewards (engagement/behavior metrics), and transitions (how states evolve).
  - **Quick check question:** Can you explain why Study 2's evaluation criterion (fraction of expert competencies built) requires considering future states, not just immediate rewards?

- **Concept: Offline reinforcement learning and dynamics estimation**
  - **Why needed here:** The approach does not train an agent through live interaction; it estimates reward and transition functions from a fixed dataset (LLM-generated or real), then derives a policy. Understanding the difference between model-based RL (learning dynamics) and model-free RL is essential.
  - **Quick check question:** Why does the L1-error between estimated and real dynamics plateau rather than converge to zero as more LLM samples are generated?

- **Concept: Prompt engineering variability and prompt sensitivity**
  - **Why needed here:** The paper explicitly tests 10 prompt paraphrases per condition and finds substantial variance. Understanding that LLM outputs are highly prompt-dependent is critical for interpreting results and designing replication studies.
  - **Quick check question:** If you ran the same experiment with a differently paraphrased prompt and got different results, would that invalidate the finding? Why or why not?

## Architecture Onboarding

- **Component map:**
  1. Behavioral study formalization: Define state space (user features), action space (intervention options), reward function, and transition structure for each target domain
  2. Prompt construction: Create prompts describing states and actions; variants include base/extensive, chain-of-thought, and few-shot with real examples
  3. LLM inference engine: Run prompts through selected model (Llama-3.3-70B recommended) to generate samples ⟨s_t, a_t, r_t, s_{t+1}⟩. Temperature settings have minimal impact per results
  4. Dynamics estimator: Aggregate LLM samples to estimate reward function R(s,a) and transition function T(s'|s,a). Use tabular or function approximation methods depending on state/action cardinality
  5. Policy learner: Apply RL algorithm (e.g., value iteration, policy iteration, or Q-learning) on estimated MDP to derive policy π(s)
  6. Evaluation simulator: Test learned policy in a simulation built from held-out real behavioral data (not available in true cold-start, but used here for validation)

- **Critical path:**
  1. Formalize MDP structure from domain knowledge (do not skip—LLM cannot infer unstated variables)
  2. Generate ~200-500 samples per action using LLM with base or few-shot prompt
  3. Estimate dynamics and derive policy
  4. If any real data exists (even 5-10 samples/action), use few-shot prompting for transition estimation

- **Design tradeoffs:**
  - **Model scale vs. cost:** Llama-3.3-70B performs best but requires significant compute. Smaller models (3B-8B) may suffice for simple domains but showed inconsistent performance
  - **Prompt complexity vs. robustness:** Chain-of-thought and extensive prompts did not consistently help and increased token costs. Base prompts with few-shot examples were most reliable
  - **Sample quantity vs. diminishing returns:** Generating >200 samples/action provides marginal benefit. Focus resources on coverage (all state-action combinations) rather than volume

- **Failure signatures:**
  - **Policy performs worse than random:** Check if LLM outputs violate format constraints (e.g., invalid state values). The paper notes prompt paraphrases caused "sometimes drastic changes in the degree to which the models were following the required output format" (p.18)
  - **Dynamics estimates match baselines:** LLM may not have sufficient domain knowledge. Verify prompt describes task clearly; consider domain-specific fine-tuning (not tested in paper)
  - **Large variance across prompt paraphrases:** Expected behavior. Use multiple paraphrases and aggregate or select based on held-out validation if any real data exists

- **First 3 experiments:**
  1. **Baseline sanity check:** Generate 200 samples/action using Llama-3.3-70B with base prompt. Verify learned policy outperforms random/worst baselines in simulation. If not, debug MDP formalization or prompt clarity
  2. **Few-shot ablation:** For a single study, compare 0, 2, 5, and 10 real examples per action in prompts. Measure L1-error for transitions and rewards separately. Expect transition improvement with 5+ examples
  3. **Human comparison:** Recruit 5-10 human raters to predict rewards/next states for 32 state-action combinations (as in paper). Compare LLM and human sample quality against real data to diagnose whether errors are LLM-specific or prediction-inherent

## Open Questions the Paper Calls Out

- **How can LLM-generated samples be effectively combined with real behavioral samples to optimize reinforcement learning training?**
  - The authors encourage future work on combining LLM-generated samples with real behavioral samples when they become available
  - The current study treats samples as either entirely LLM-generated or real, without exploring hybrid training regimes
  - Resolution would require experiments showing improved sample efficiency when pre-training on LLM data and fine-tuning on limited real data

- **Does incorporating specific user characteristics into prompts improve the accuracy of LLM and human predictions?**
  - Future work could examine whether adding user characteristics such as age or occupation sampled from a plausible range improves both LLM and human prediction performance
  - Current experiments used general descriptions of user states without detailed demographic grounding
  - Resolution would require a comparative study measuring L1-error with and without demographic details

- **Do the negative or positive effects of chain-of-thought prompting correlate between LLMs and human raters?**
  - The authors ask whether effects of step-by-step reasoning are similar for humans and LLMs
  - Current analysis examined CoT effects separately for LLMs and humans
  - Resolution would require a controlled experiment where both humans and LLMs perform predictions with and without CoT, followed by correlation analysis

## Limitations

- The evaluation relies on held-out real data rather than true cold-start scenarios, meaning the "no data" case remains empirically untested
- Performance improvements from few-shot prompting are inconsistent across studies and metrics
- The gap between generated and real samples suggests fundamental limitations in LLM behavioral modeling, particularly for underrepresented populations

## Confidence

- **High confidence:** LLM-generated samples can outperform worst and random baselines in policy learning
- **Medium confidence:** Llama-3.3-70B consistently performs best across studies and approaches human rater performance
- **Medium confidence:** Few-shot prompting improves transition function estimation but effects on policy performance are inconsistent
- **Low confidence:** Chain-of-thought prompting provides consistent benefits, as effects vary significantly across studies

## Next Checks

1. Test the approach on a genuinely cold-start scenario where no real data exists, using held-out test data only for final evaluation
2. Evaluate performance across multiple prompt paraphrases to quantify sensitivity and determine optimal aggregation strategies
3. Assess whether domain-specific fine-tuning of LLMs on behavioral science literature improves sample quality compared to zero-shot prompting