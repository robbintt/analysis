---
ver: rpa2
title: Gender Bias in English-to-Greek Machine Translation
arxiv_id: '2506.09558'
source_url: https://arxiv.org/abs/2506.09558
tags:
- gender
- bias
- translation
- translate
- google
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study addresses gender bias in English-to-Greek machine translation,
  focusing on three patterns: male bias, occupational stereotyping, and errors in
  anti-stereotypical translations. It introduces GendEL, a dataset of 240 gender-ambiguous
  and unambiguous sentences, and evaluates two commercial systems (Google Translate,
  DeepL) alongside prompted GPT-4o.'
---

# Gender Bias in English-to-Greek Machine Translation

## Quick Facts
- arXiv ID: 2506.09558
- Source URL: https://arxiv.org/abs/2506.09558
- Reference count: 40
- Primary result: Commercial MT systems exhibit strong male bias in English-to-Greek translation, while GPT-4o shows promise for bias mitigation

## Executive Summary
This study investigates gender bias in English-to-Greek machine translation, focusing on three key patterns: male bias, occupational stereotyping, and errors in anti-stereotypical translations. The researchers introduce GendEL, a dataset of 240 gender-ambiguous and unambiguous sentences, and evaluate two commercial systems (Google Translate, DeepL) alongside prompted GPT-4o. Results reveal both MT systems default to masculine forms in ambiguous cases, with Google Translate struggling particularly with feminine forms in anti-stereotypical contexts. GPT-4o generates more balanced gendered and neutral alternatives, achieving high accuracy in masculine and feminine translations but showing residual bias in neutral forms. The findings highlight the need for more inclusive translation practices and demonstrate LLM potential for bias mitigation.

## Method Summary
The study evaluates gender bias in English-to-Greek machine translation using the GendEL dataset of 240 sentences. Three translation systems are tested: Google Translate, DeepL, and prompted GPT-4o. The evaluation focuses on three bias patterns: male bias (defaulting to masculine forms), occupational stereotyping (assigning genders based on stereotypical occupations), and anti-stereotypical errors (incorrect gender assignment in non-stereotypical contexts). Translations are assessed for accuracy in generating masculine, feminine, and neutral forms. The prompted GPT-4o is specifically instructed to provide multiple gender alternatives to assess its ability to mitigate bias compared to the commercial MT systems.

## Key Results
- Both Google Translate and DeepL exhibit strong male bias, defaulting to masculine forms in gender-ambiguous cases
- Google Translate struggles with feminine forms in anti-stereotypical contexts, while DeepL performs better
- GPT-4o achieves high accuracy in generating masculine and feminine translations but shows residual bias and errors in neutral forms

## Why This Works (Mechanism)
The study works by systematically testing how different translation systems handle gender-ambiguous input and evaluating their tendency to default to masculine forms or maintain gender stereotypes. The prompted GPT-4o's ability to generate multiple gender alternatives demonstrates how instruction tuning and careful prompting can mitigate bias compared to standard commercial MT systems.

## Foundational Learning
- Gender-ambiguous vs. unambiguous sentences: Why needed - to test how systems handle cases where gender cannot be determined from context; Quick check - verify dataset includes clear examples of both types
- Occupational stereotyping patterns: Why needed - to assess whether systems perpetuate gender stereotypes in professional contexts; Quick check - review examples showing stereotypical vs. anti-stereotypical assignments
- Masculine, feminine, and neutral form generation: Why needed - to evaluate system capability across the full gender spectrum; Quick check - examine accuracy metrics for each gender category

## Architecture Onboarding
Component map: English input -> Gender ambiguity detection -> Translation system (MT or LLM) -> Gender form generation -> Output evaluation

Critical path: Input sentence → System processing → Gender assignment decision → Output form selection → Bias evaluation

Design tradeoffs: Commercial MT systems prioritize speed and fluency over gender inclusivity, while prompted LLMs can generate multiple alternatives but may be slower and require careful prompting

Failure signatures: Defaulting to masculine forms in ambiguous cases, incorrect gender assignment in anti-stereotypical contexts, inability to generate natural-sounding neutral forms

First 3 experiments:
1. Test gender ambiguity detection accuracy on a subset of GendEL sentences
2. Compare output distributions of masculine vs. feminine forms across all systems
3. Evaluate quality of neutral form translations against native speaker judgments

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- The GendEL dataset contains only 240 sentences, which may not capture full complexity of gender bias across diverse contexts
- Evaluation focuses specifically on occupational terms and pronouns, potentially missing other types of gender bias
- Only two commercial MT systems and one LLM are tested, limiting generalizability to other translation technologies

## Confidence
- High confidence: Findings regarding male bias defaulting in ambiguous cases and comparative performance between Google Translate and DeepL
- Medium confidence: GPT-4o results, as prompting strategy may influence outputs and neutral form evaluation remains subjective
- Medium confidence: Occupational stereotyping findings due to limited dataset size

## Next Checks
1. Expand the GendEL dataset to include at least 500-1000 sentences covering diverse domains beyond occupational terms
2. Test additional translation systems including open-source MT models and other commercial providers
3. Conduct human evaluation study with native Greek speakers to validate automated evaluation metrics, particularly for gender-neutral translations