---
ver: rpa2
title: 'SpareCodeSearch: Searching for Code Context When You Have No Spare GPU'
arxiv_id: '2510.12948'
source_url: https://arxiv.org/abs/2510.12948
tags:
- code
- search
- zoekt
- query
- completion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SpareCodeSearch addresses the challenge of retrieving relevant
  code context for automated code completion without relying on resource-intensive
  semantic search methods. The solution uses a lightweight keyword-based search approach
  with the Zoekt code search engine, which can be deployed on standard hardware and
  integrated into IDE environments.
---

# SpareCodeSearch: Searching for Code Context When You Have No Spare GPU

## Quick Facts
- arXiv ID: 2510.12948
- Source URL: https://arxiv.org/abs/2510.12948
- Authors: Minh Nguyen
- Reference count: 17
- Primary result: Achieved chRF scores of 0.748 (Kotlin) and 0.725 (Python) using keyword-based search without GPU

## Executive Summary
SpareCodeSearch addresses the challenge of retrieving relevant code context for automated code completion without relying on resource-intensive semantic search methods. The solution uses a lightweight keyword-based search approach with the Zoekt code search engine, which can be deployed on standard hardware and integrated into IDE environments. By systematically generating and testing multiple query variations from code diffs using symbols extracted via Tree-sitter parsing, the system achieves high hit rates through cross-shard searching across repository revisions. The approach achieved chRF scores of 0.748 on Kotlin and 0.725 on Python in the Code Context Competition's private phase, ranking first in Kotlin and second in Python while requiring only 12-18 minutes to process entire datasets on a consumer laptop.

## Method Summary
SpareCodeSearch implements a two-phase approach for code context retrieval. The offline phase indexes repository revisions into Zoekt shards using ctags for symbol extraction. The online phase constructs diff strings from prefix and suffix code, extracts identifiers using Tree-sitter parsing, and generates 19 query variations. These queries are executed sequentially with 0.2-second timeouts until non-empty results are found. The system employs cross-shard searching across all repository revisions and uses dynamic token budget management to fit results within LLM context windows. Results are ranked by relevance and truncated to respect per-file and total token constraints.

## Key Results
- Achieved chRF scores of 0.748 on Kotlin and 0.725 on Python in Code Context Competition private phase
- Processing time: 12-18 minutes to complete entire datasets on a consumer laptop (M3 Air)
- Cross-shard searching achieved 97.5% (Kotlin) and 96.7% (Python) hit rates versus 86.0% and 85.8% for single-shard
- Ranked 1st in Kotlin and 2nd in Python among competition participants

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Systematic query variation generation from code diffs maximizes retrieval hit rates without semantic embeddings.
- Mechanism: Tree-sitter parses diff strings to extract symbols (function/class names → navigation expressions → all identifiers). These feed into 19 pre-defined Zoekt query strategies ranging from strict exact matching to fuzzy regex with OR logic. The system iterates through variations sequentially, stopping at the first non-empty result.
- Core assumption: Relevant code context shares lexical markers (identifiers, function names) with the code being written.
- Evidence anchors:
  - [abstract] "By systematically generating and testing multiple query variations from code diffs using symbols extracted via Tree-sitter parsing, the system achieves high hit rates"
  - [section II.C.1] "The ultimate goal is trying to brute-force all the possible ways of constructing a Zoekt query, in order to maximize the chance of finding relevant code context"
  - [corpus] CodeRAG (arXiv:2509.16112) similarly emphasizes retrieval relevance but uses embedding-based approaches; no direct corpus support for keyword-only efficacy.
- Break condition: If code context shares no lexical overlap with completion point (e.g., semantically related but differently named abstractions), this mechanism fails.

### Mechanism 2
- Claim: Cross-shard searching across repository revisions significantly increases hit rates compared to single-revision search.
- Mechanism: Zoekt queries can omit revision IDs, enabling simultaneous search across all indexed shards (different commits/versions of the same repository). This mirrors how developers search git history for prior implementations.
- Core assumption: Useful context often exists in other branches, commits, or historical versions of the same repository.
- Evidence anchors:
  - [section III.A] "the hit rate for Cross-shard setting was found to be significantly higher than that of Single-shard setting" (Kotlin: 390 vs 344 hits; Python: 239 vs 213)
  - [section III.A] Cross-shard hit rates reached 97.5% (Kotlin) and 96.7% (Python)
  - [corpus] No corpus papers directly validate cross-revision retrieval; this appears novel to this work.
- Break condition: If repositories have no revision history or if future-revision access constitutes data leakage (authors acknowledge this threat to validity), performance may not generalize.

### Mechanism 3
- Claim: Dynamic token budget management enables context retrieval within fixed LLM context windows.
- Mechanism: Post-processor calculates available token budget T by subtracting prefix/suffix tokens from maximum context length (using JetBrains Mellum tokenizer). Results are ranked by Zoekt relevance scores, then truncated to fit per-file budget R and total constraint T.
- Core assumption: Tokenizer alignment between retrieval system and target LLM ensures accurate budget calculation.
- Evidence anchors:
  - [section II.C.2] "we dynamically calculate the token constraint T of each completion point by subtracting the maximum number of Mellum tokens from the total number of tokens existing in the diff string's prefix and suffix"
  - [section II.C.2] "selects up to top-k ranked files/snippets, ensuring each fits within a per-file token budget R and the overall token constraint T"
  - [corpus] Beyond More Context (arXiv:2510.06606) confirms context granularity and ordering impact completion quality, supporting budget-aware truncation.
- Break condition: If retrieved snippets exceed per-file budget or if tokenization mismatch causes overflow, context will be incomplete or truncated incorrectly.

## Foundational Learning

- Concept: Inverted index code search (Zoekt/trigram indexing)
  - Why needed here: Understanding how keyword-based search achieves sub-second retrieval over large codebases without embeddings.
  - Quick check question: Can you explain why Zoekt can search 400 repository revisions in 18 minutes on a laptop while semantic search would require GPU inference for every query?

- Concept: Tree-sitter AST parsing and symbol extraction
  - Why needed here: The query generator relies on extracting function names, class names, navigation expressions, and identifiers from diff strings.
  - Quick check question: Given a diff string, which Tree-sitter node types would you query to extract function definitions vs. import statements?

- Concept: RAG context window economics
  - Why needed here: Understanding why token budget calculation matters and how retrieved context competes with prefix/suffix for limited LLM context.
  - Quick check question: If prefix consumes 2000 tokens and suffix consumes 500 tokens with an 8192 token limit, what is the maximum available for retrieved context (accounting for generation buffer)?

## Architecture Onboarding

- Component map: Offline indexer -> Zoekt shards -> Docker volume storage -> Query Generator -> Zoekt web server -> Post-processor

- Critical path:
  1. Index all repository revisions into Zoekt shards (10 min for 400 revisions on M3 Air)
  2. For each completion point, extract diff string → parse with Tree-sitter → generate query variations
  3. Iterate through query variations until non-empty result (with 0.2s timeout per request)
  4. Post-process results: rank by relevance, enforce token budgets, concatenate contexts

- Design tradeoffs:
  - Cross-shard search increases hit rates but risks data leakage (future revisions informing past completions)
  - 19 query variations maximize coverage but 0.2s timeouts may drop results under load
  - Keyword-only search avoids GPU costs but cannot capture semantic similarity without lexical overlap
  - Authors acknowledge: no investigation of which specific query variations contribute most to performance

- Failure signatures:
  - Empty results from all 19 query variations (occurred in 2.5-3.3% of queries in public set)
  - Token budget overflow if prefix/suffix underestimate actual tokenization
  - Server overload causing timeout cascades (mitigated by retry mechanisms)

- First 3 experiments:
  1. **Baseline hit rate test**: Run Query Generator on a held-out repository with single-shard vs. cross-shard settings; measure hit rate and identify which query variation succeeds first for each completion point.
  2. **Ablation by query type**: Disable query variations one category at a time (functions/classes, navigation, identifiers) to measure contribution of each symbol extraction strategy.
  3. **Token budget sensitivity**: Vary the per-file budget R and total constraint T; measure impact on chRF scores to determine if current settings are optimal or over-constraining.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can automated query optimization techniques outperform the current fixed set of 19 hand-crafted query variations?
- Basis in paper: [explicit] The authors propose in Section IV.B using "search-based software engineering techniques" to explore a broader search space of query formulations beyond the current hand-crafted variations.
- Why unresolved: The current implementation relies on a static set of candidates for tractability, leaving the potentially larger space of query combinations unexplored.
- What evidence would resolve it: Experiments comparing automated query generation strategies against the fixed manual set on the Code Context Competition benchmark to measure retrieval effectiveness.

### Open Question 2
- Question: How can the system prevent data leakage from future revisions while retaining the benefits of cross-shard searching?
- Basis in paper: [explicit] Section IV.A identifies a threat where "a code snippet from a later revision might be retrieved to complete a code snippet from an earlier revision," constituting data leakage.
- Why unresolved: The current cross-shard setting ignores revision order to maximize hit rates, overlooking temporal integrity.
- What evidence would resolve it: An implementation incorporating revision timestamps to filter results, showing whether similar chRF scores can be maintained without violating temporal constraints.

### Open Question 3
- Question: Is SpareCodeSearch effective for retrieving bug context in automated vulnerability patching or secure code generation?
- Basis in paper: [explicit] Section IV.B states that exploring the solution "in the context of secure code generation is a promising avenue," referencing prior work on bug context localization.
- Why unresolved: The current evaluation is restricted to standard code completion tasks rather than security-focused code repair or generation.
- What evidence would resolve it: Evaluation of the system on security datasets to measure retrieval accuracy for relevant bug contexts compared to semantic search baselines.

## Limitations

- Query variation effectiveness remains unvalidated - The paper reports using 19 pre-defined query variations but explicitly states "We did not investigate which specific query variations contribute most to performance," leaving a critical gap in understanding the system's actual mechanics.
- Cross-shard search data leakage threat - While cross-shard searching achieved 97.5% (Kotlin) and 96.7% (Python) hit rates, the authors acknowledge this approach risks using future repository revisions to inform past completion points without quantifying the magnitude of this leakage.
- Token budget calculation assumptions - The dynamic token budget calculation relies on Mellum tokenizer compatibility and assumes accurate estimation of prefix/suffix token consumption, with no validation provided that the post-processor's token counting aligns with the actual LLM's tokenization.

## Confidence

- **Low**: Query variation effectiveness remains unvalidated - The paper reports using 19 pre-defined query variations but explicitly states "We did not investigate which specific query variations contribute most to performance," leaving a critical gap in understanding the system's actual mechanics.
- **Medium**: Cross-shard search data leakage threat - While cross-shard searching achieved 97.5% (Kotlin) and 96.7% (Python) hit rates, the authors acknowledge this approach risks using future repository revisions to inform past completion points without quantifying the magnitude of this leakage.
- **Medium**: Token budget calculation assumptions - The dynamic token budget calculation relies on Mellum tokenizer compatibility and assumes accurate estimation of prefix/suffix token consumption, with no validation provided that the post-processor's token counting aligns with the actual LLM's tokenization.

## Next Checks

1. **Query variation ablation study**: Systematically disable each of the 19 query variation categories and measure the incremental impact on hit rates and chRF scores to identify which queries drive performance versus which add overhead.

2. **Cross-shard leakage quantification**: Re-run the system with strict single-revision search enforced, measuring the exact performance drop, and implement a "future-restricted" variant that only searches revisions older than the current completion point.

3. **Token budget stress test**: Create synthetic completion points with known prefix/suffix token counts spanning the full range of possible values to verify that the post-processor's token budget calculations prevent overflow while maximizing context retrieval.