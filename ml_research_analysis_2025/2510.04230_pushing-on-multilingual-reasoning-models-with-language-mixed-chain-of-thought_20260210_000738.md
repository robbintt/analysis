---
ver: rpa2
title: Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought
arxiv_id: '2510.04230'
source_url: https://arxiv.org/abs/2510.04230
tags:
- arxiv
- reasoning
- preprint
- korean
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of building strong reasoning
  models for mid-resource languages, specifically Korean, by overcoming limitations
  of translation artifacts and monolingual supervision. The authors propose Language-Mixed
  Chain-of-Thought (CoT), a reasoning schema that alternates between English (as an
  anchor) and Korean during the thinking phase, preserving reasoning power while maintaining
  linguistic and cultural fidelity.
---

# Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought

## Quick Facts
- arXiv ID: 2510.04230
- Source URL: https://arxiv.org/abs/2510.04230
- Authors: Guijin Son; Donghun Yang; Hitesh Laxmichand Patel; Amit Agarwal; Hyunwoo Ko; Chanuk Lim; Srikant Panda; Minhyuk Kim; Nikunj Drolia; Dasol Choi; Kyong-Ha Lee; Youngjae Yu
- Reference count: 33
- Primary result: Language-Mixed Chain-of-Thought (en/ko) outperforms monolingual CoT, with KO-REAson-35B achieving 64.0 ± 2.5 average score across 9 benchmarks

## Executive Summary
This paper addresses the challenge of building strong reasoning models for mid-resource languages, specifically Korean, by overcoming limitations of translation artifacts and monolingual supervision. The authors propose Language-Mixed Chain-of-Thought (CoT), a reasoning schema that alternates between English (as an anchor) and Korean during the thinking phase, preserving reasoning power while maintaining linguistic and cultural fidelity. They curate YI-SANG, the largest publicly documented Korean post-training dataset, comprising 5.79M prompts and 3.7M long reasoning traces, plus a high-yield subset YI-SANG-HQ of 260k instances. Training nine models (4B–35B) across six families, their best model KO-REAson-35B achieves state-of-the-art performance with the highest overall average score (64.0 ± 2.5), ranking first on five of nine benchmarks. Smaller models also benefit substantially, with an average improvement of +18.6 points. Ablations confirm Language-Mixed CoT outperforms monolingual CoT, and cross-lingual/multi-modal gains are observed despite training only on Korean text. The work provides a practical recipe and open resources for advancing language-specific reasoning.

## Method Summary
The authors propose Language-Mixed Chain-of-Thought (CoT) for multilingual reasoning, demonstrated via Korean. They curate YI-SANG (5.79M native prompts) and generate 3.7M reasoning traces using Qwen3-32B as teacher. The key innovation is alternating between English (for reasoning scaffolding) and Korean (for cultural/linguistic fidelity) during CoT, with Korean-character ratio bounds (5%-20%). They train SFT models (4B-35B) on YI-SANG-HQ (260k high-yield samples), using loss on reasoning/solution only, no packing, and aggressive filtering (degeneration, multiple  blocks, non-Korean final answers). Best model KO-REAson-35B achieves state-of-the-art performance.

## Key Results
- Language-Mixed en/ko CoT consistently outperforms both English-only and Korean-only CoT across multiple model sizes and benchmark types
- KO-REAson-35B achieves highest average score (64.0 ± 2.5) and ranks first on five of nine benchmarks
- Category ablation shows medical data helps ClinicalQA but harms other benchmarks; Exams category most effective for general reasoning
- Smaller models (4B-8B) achieve +18.6 average improvement over baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Language-Mixed CoT (switching between English anchor and target language Korean during reasoning) outperforms monolingual CoT baselines by balancing reasoning capability with cultural/linguistic faithfulness.
- **Mechanism:** The model performs logical scaffolding primarily in English (leveraging stronger pretraining priors for reasoning) while preserving key Korean terms, named entities, and quoted spans to maintain faithfulness to the original prompt and avoid translation drift.
- **Core assumption:** Assumption: Reasoning capability is language-dependent in pretrained models, with English having stronger reasoning priors due to pretraining data distribution.
- **Evidence anchors:**
  - [abstract] "switches between English and a target language, using English as an anchor to excel in reasoning while minimizing translation artifacts"
  - [Table 2] Language-Mixed en/ko consistently outperforms both English-only and Korean-only across Gemma-3-4B and Kanana-1.5-8B; notably, improvements on MCLM (math) emerge only with English-anchored CoT
  - [corpus] "The Transfer Neurons Hypothesis" paper supports that middle layers in multilingual LLMs perform reasoning within an English-centric latent space, providing mechanistic grounding for why English anchors work
- **Break condition:** Performance gains disappear or reverse if the base model lacks sufficient multilingual pretraining in both anchor and target languages (corpus evidence suggests this is architecture-dependent).

### Mechanism 2
- **Claim:** Training on native (not translated) prompts improves robustness to colloquial expressions and cultural context while avoiding translation artifacts that degrade non-reasoning benchmarks.
- **Mechanism:** Native web-crawled prompts retain user artifacts (typos, abbreviations, mixed script, internet style) that normalized/translated data removes; this improves generalization to real-world deployment conditions.
- **Core assumption:** Assumption: Translation introduces systematic noise (mistranslation, cultural disconnection) that accumulates during long reasoning traces.
- **Evidence anchors:**
  - [abstract] "avoiding translation artifacts while preserving reasoning capability"
  - [Table 1] Training Qwen2.5-1.5B-Instruct on translated OpenThought improves MATH (74.35) but catastrophically degrades HAE-RAE Bench (35.2→15.3, a Korean cultural benchmark)
  - [Section 5.1] "we keep prompts verbatim... such normalization removes user artifacts... that harm robustness at deployment"
  - [corpus] Weak direct corpus evidence on native vs. translated data specifically for reasoning; mostly observational
- **Break condition:** Gains diminish if native data quality is too low (noise overwhelms signal) or if target domain differs substantially from web-crawled distribution.

### Mechanism 3
- **Claim:** Selective data category mixing (OpenThought/Exams/Science/Code) with aggressive filtering produces higher-quality training sets than scaling all available data, with medical and daily categories showing negative or neutral effects.
- **Mechanism:** Different data categories have domain-specific tradeoffs; medical data boosts ClinicalQA but harms other benchmarks, and daily data shows no consistent benefit. Loss spike analysis identifies and removes degeneration patterns that destabilize training.
- **Core assumption:** Assumption: Data quality heterogeneity means optimal subsets are not simply the largest available set.
- **Evidence anchors:**
  - [Table 3] Medical category: boosts ClinicalQA (65.6) but harms HRB/MCLM/KMMLU-R; Exams most effective for HRB/KMMLU-R
  - [Section 5.3] Loss spike filtering removes degeneration, multiple thinking blocks, and non-Korean final solutions
  - [corpus] No direct corpus evidence on category-specific ablations for multilingual reasoning
- **Break condition:** Category benefits may not transfer to other languages with different data availability or cultural contexts.

## Foundational Learning

- **Concept:** Chain-of-Thought (CoT) reasoning and long-form reasoning traces
  - **Why needed here:** The paper builds on the premise that long CoT improves reasoning; you must understand what CoT is and why extended reasoning traces matter before grasping Language-Mixed CoT.
  - **Quick check question:** Can you explain why a model generating intermediate reasoning steps before a final answer might improve accuracy compared to direct answering?

- **Concept:** Supervised Fine-Tuning (SFT) vs. Reinforcement Learning for reasoning
  - **Why needed here:** The paper explicitly chooses SFT over RL due to cold-start problems in mid-resource languages; understanding this tradeoff is essential for reproducing the approach.
  - **Quick check question:** Why might SFT be more practical than RL when strong base models and reward models are unavailable for a target language?

- **Concept:** Code-switching and multilingual representation in LLMs
  - **Why needed here:** Language-Mixed CoT is fundamentally a controlled code-switching strategy; understanding how multilingual models handle mixed-language input helps explain why English anchors work.
  - **Quick check question:** In a multilingual LLM, what might happen if a model reasons entirely in a target language that was underrepresented during pretraining?

## Architecture Onboarding

- **Component map:** Instruction Collection (54 web sources → legal triage → verbatim preservation → Korean ratio filtering) → Response Generation (Qwen3-32B teacher → Language-Mixed CoT → Korean ratio bounds filtering) → Quality Filtering (category ablations → loss spike detection → 13-gram decontamination) → Training (SFT on YI-SANG-HQ)

- **Critical path:** Prompt collection → Language-Mixed CoT generation → category selection (OpenThought/Exams/Science/Code only) → loss spike filtering → decontamination → SFT training. The paper emphasizes that filtering and category selection are where most quality gains originate.

- **Design tradeoffs:**
  - Native vs. translated data: Native preserves cultural context and colloquialisms; translated provides more competition-level math problems
  - Data scale vs. quality: 260k high-yield subset outperforms larger mixes; scaling Daily/Medical categories hurts performance
  - Packing vs. no packing: Packing speeds training 3x but causes measurable benchmark drops (Table 13)
  - English anchor vs. other languages: Only English-anchored CoT shows math reasoning gains; Chinese/Russian anchors inconsistent

- **Failure signatures:**
  - Loss spikes during training: Triggered by degeneration (repetitive phrases), multiple thinking blocks, or non-Korean final solutions
  - Benchmark collapse: Training on translated data causes cultural benchmarks (HAE-RAE Bench) to crash even if math improves
  - Cross-lingual degradation: Pure Korean CoT reduces math capability; pure English CoT loses cultural competence

- **First 3 experiments:**
  1. **Baseline comparison:** Train a small model (e.g., 4B-8B) on English-only CoT, Korean-only CoT, and Language-Mixed en/ko CoT using the same prompt set. Evaluate on both reasoning (MCLM) and cultural (HAE-RAE Bench) benchmarks to confirm the mixed approach captures both benefits.
  2. **Category ablation:** Train on individual data categories (OpenThought, Exams, Code, Science, Medical, Daily) with minimal formatting data to identify which categories help vs. harm specific benchmark types. Expect Medical to help ClinicalQA but hurt general benchmarks.
  3. **Loss spike detection run:** Do a 1-epoch shakedown run with a proxy model on the full dataset. Identify batches causing loss spikes, manually inspect for degeneration patterns, and implement filters to remove these before full training. This prevents wasted GPU hours on unstable data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Language-Mixed CoT approach generalize to other mid-resource languages beyond Korean, or is Korean-specific linguistic or cultural proximity to English a confounding factor?
- Basis in paper: [explicit] The authors present Korean as a "case study" and state their goal is to "benefit Korean practitioners and the broader multilingual community," but all experiments are Korean-only.
- Why unresolved: The paper tests only Korean; no experiments on other mid-resource languages (e.g., Thai, Vietnamese, Turkish) are reported.
- What evidence would resolve it: Replicate the YI-SANG pipeline for at least two typologically distinct mid-resource languages and compare Language-Mixed CoT effectiveness against monolingual baselines.

### Open Question 2
- Question: What is the optimal ratio of anchor-language to target-language content in Language-Mixed CoT, and does it vary by task type or language pair?
- Basis in paper: [inferred] The authors apply a regex filter to discard samples with Korean-character ratios outside 5–20%, but this range is empirically chosen without systematic ablation.
- Why unresolved: No ablation study varies the Korean ratio bounds; the 5–20% threshold may be suboptimal for different domains (e.g., cultural vs. mathematical reasoning).
- What evidence would resolve it: Train models with systematically varied target-language ratio bounds (e.g., 0–5%, 5–10%, 10–20%, 20–40%) and measure performance across reasoning and cultural benchmarks.

### Open Question 3
- Question: Can reinforcement learning (RL) provide additional gains on top of SFT for mid-resource language reasoning models, once a strong SFT foundation is established?
- Basis in paper: [explicit] The authors prioritize SFT due to the "cold-start problem" for Korean but state their goal is to "build a strong base model for subsequent RL efforts."
- Why unresolved: No RL experiments are conducted; the paper leaves open whether RLVR would yield further improvements or prove unstable even after strong SFT.
- What evidence would resolve it: Apply RLVR (e.g., GRPO) to KO-REAson models and compare against SFT-only baselines on held-out benchmarks.

### Open Question 4
- Question: Why do some models benefit from non-English anchor languages (Chinese, Russian) while others do not, and can this be predicted from pretraining corpus statistics?
- Basis in paper: [explicit] The authors observe that Gemma-3-4B shows gains with Russian/Chinese-anchored CoT while Kanana-1.5-8B does not, and "suspect this difference is driven by the pretraining mixtures."
- Why unresolved: The hypothesis is stated but not empirically validated; no analysis correlates pretraining language proportions with anchor-language effectiveness.
- What evidence would resolve it: Measure pretraining corpus language ratios for each base model and correlate with cross-anchor performance differences; optionally fine-tune the same base with controlled multilingual pretraining mixes.

## Limitations

- The study focuses exclusively on Korean as a mid-resource language, limiting generalizability to other languages with different pretraining distributions or cultural contexts
- The specific mechanism by which English anchoring improves reasoning is plausible but not definitively proven through targeted neural probing studies
- The heavy reliance on teacher-generated data (Qwen3-32B) raises questions about reproducibility if equivalent reasoning teachers are unavailable for other languages

## Confidence

- **High confidence:** Language-Mixed CoT consistently outperforms monolingual baselines across multiple model sizes and benchmark types. The empirical results showing improved performance on both reasoning (MCLM) and cultural (HAE-RAE Bench) benchmarks are robust and well-documented.
- **Medium confidence:** The mechanism explanation for why English anchoring works (leveraging stronger English reasoning priors) is plausible but not definitively proven. The category-specific effects are well-documented but may be dataset-dependent.
- **Low confidence:** The generalizability of these results to other mid-resource languages and the exact neural mechanisms underlying Language-Mixed CoT remain speculative without additional validation.

## Next Checks

1. **Cross-linguistic replication:** Apply Language-Mixed CoT to another mid-resource language (e.g., Vietnamese or Hindi) with available multilingual pretraining data. Compare performance against monolingual baselines and English-only reasoning to test generalizability beyond Korean.

2. **Neural mechanism probing:** Conduct targeted activation analysis on middle layers of multilingual models to verify whether English-centric latent spaces are indeed used for reasoning when processing Language-Mixed CoT. Use techniques like probing classifiers or causal mediation analysis.

3. **Category transfer study:** Test whether the observed category-specific effects (Medical helping ClinicalQA but harming general benchmarks) replicate when training on different datasets or when categories are mixed differently. This would validate whether the effects are systematic or dataset-specific artifacts.