---
ver: rpa2
title: 'LMEnt: A Suite for Analyzing Knowledge in Language Models from Pretraining
  Data to Representations'
arxiv_id: '2509.03405'
source_url: https://arxiv.org/abs/2509.03405
tags:
- entity
- lment
- chunks
- entities
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# LMEnt: A Suite for Analyzing Knowledge in Language Models from Pretraining Data to Representations

## Quick Facts
- **arXiv ID:** 2509.03405
- **Source URL:** https://arxiv.org/abs/2509.03405
- **Reference count:** 40
- **Key result:** Entity-based retrieval using Wikidata QIDs achieves 66.3%-80.4% higher precision than string-based methods

## Executive Summary
LMEnt introduces a comprehensive suite for analyzing knowledge acquisition in language models during pretraining. The system combines entity-annotated Wikipedia data, a retrieval index based on Wikidata QIDs, and intermediate checkpoints to study how models learn and forget facts. The key innovation is an entity-based retrieval method that significantly outperforms previous string-based approaches, enabling researchers to trace specific knowledge acquisition patterns across training steps. By training smaller models on a carefully annotated knowledge-rich corpus, LMEnt demonstrates that compute-efficient knowledge acquisition is possible without sacrificing performance on factual benchmarks.

## Method Summary
LMEnt creates entity-annotated Wikipedia data using three complementary sources: hyperlink extraction (binary scores), ReFinED entity linking (0-1 confidence scores), and Maverick coreference resolution (cluster-based scores). The corpus is chunked using a Variable Sequence Length Curriculum to prevent cross-document contamination while preserving entity boundaries. Models are trained using the OLMo-2 architecture with 4K intermediate checkpoints to track knowledge dynamics. The retrieval index maps entities to training chunks, enabling researchers to study when specific facts are acquired or forgotten during pretraining.

## Key Results
- Entity-based retrieval using Wikidata QIDs achieves 66.3%-80.4% higher precision than string-based methods on pairwise win rates
- Subject-answer co-occurrence in chunks correlates more strongly with model performance than entity popularity metrics
- Models trained on 3.6B Wikipedia tokens achieve comparable knowledge recall to models trained on 84B+ tokens (compute efficiency gain of 4.7%)
- Fact frequency increases both learning and forgetting rates, with the relationship between frequency and knowledge acquisition varying across training steps

## Why This Works (Mechanism)

### Mechanism 1: Entity Disambiguation Improves Retrieval Precision
- **Claim:** Entity-based retrieval using Wikidata QIDs outperforms string-based methods by 66.3%-80.4% on pairwise win rates.
- **Mechanism:** By mapping text spans to unique entity identifiers through three complementary sources—hyperlinks (H), entity linking (EL), and coreference resolution (C)—the system disambiguates entities sharing surface forms (e.g., "Buffalo" as team vs. city). Each mention receives up to three confidence scores, enabling threshold-based filtering.
- **Core assumption:** Wikidata QIDs provide stable, unambiguous entity references; confidence scores from each source reliably indicate correct mappings.
- **Evidence anchors:**
  - [abstract]: "an entity-based retrieval method over pretraining data that outperforms previous approaches by as much as 80.4%"
  - [§5.2]: "LMEnt maintains high precision ≥ 97% as k increases, while precision substantially declines for all other methods, reaching 27% and 84% for CI-SS and CS-SS"
  - [corpus]: Weak—neighbor papers focus on pretraining data frequency and representations but not retrieval methods.
- **Break condition:** If entity linking models fail on domain-specific terminology or coreference resolution degrades on documents >50K tokens without window overlap handling.

### Mechanism 2: Subject-Answer Co-occurrence Predicts Fact Learning
- **Claim:** The number of chunks where subject and answer entities co-occur ("Subject+Answer Chunks") correlates more strongly with model performance than entity popularity metrics.
- **Mechanism:** Knowledge acquisition emerges from repeated exposure to entity pairs in context. The model learns associations when both entities appear in the same training chunk, not merely from individual entity frequency.
- **Core assumption:** Co-occurrence in chunks approximates factual relationship exposure; the model encodes relational information during next-token prediction.
- **Evidence anchors:**
  - [§5.2]: "Subj+Answer Chunks correlates best with model behavior"
  - [§6]: "fact frequency is key, but does not fully explain learning trends... rates of both learning and forgetting increase with frequency"
  - [corpus]: "On Linear Representations and Pretraining Data Frequency in Language Models" (FMR=0.54) supports frequency-representation relationship but doesn't validate co-occurrence specifically.
- **Break condition:** If facts require multi-hop reasoning not captured in single chunks, or if co-occurrence is spurious (entities appearing together without relational context).

### Mechanism 3: Compute-Efficient Knowledge Acquisition via Knowledge-Rich Corpus
- **Claim:** Models trained on 3.6B Wikipedia tokens (0.03%-4.7% of typical pretraining) achieve comparable knowledge recall to models trained on 84B+ tokens.
- **Mechanism:** Wikipedia's entity-centric structure with hyperlinks provides dense, high-quality factual signal. The Variable Sequence Length Curriculum prevents cross-document contamination while preserving entity mention integrity.
- **Core assumption:** Wikipedia quality and coverage are representative of world knowledge; chunk boundaries don't disrupt critical entity relationships.
- **Evidence anchors:**
  - [§5.1]: "LMEnt models reach comparable performance (7.4% on all entities, 66% on popular entities) to Pythia-1.4B (8.7%, 67%) and OLMo-1B (10.4%, 66%)"
  - [§4]: "170M model has the compute-optimal size for 3.6B tokens"
  - [corpus]: Limited validation—neighbor papers don't address Wikipedia-specific efficiency gains.
- **Break condition:** If task requires knowledge outside Wikipedia scope, or commonsense reasoning not captured in factual text (see §A.4: poor performance on out-of-distribution tasks).

## Foundational Learning

- **Concept: Entity Linking & Disambiguation**
  - **Why needed here:** Core to LMEnt's annotation pipeline; maps ambiguous text mentions to unique Wikidata QIDs.
  - **Quick check question:** Can you explain why "Buffalo" in "Buffalo Bills" and "Buffalo, NY" requires different QIDs despite identical surface form?

- **Concept: Coreference Resolution**
  - **Why needed here:** Captures implicit entity mentions (pronouns, descriptors) that hyperlinks and entity linking miss.
  - **Quick check question:** In "Josh Allen led the Bills to their first division title," what entity does "their" refer to, and how would Maverick identify it?

- **Concept: Pretraining Dynamics & Checkpoint Analysis**
  - **Why needed here:** LMEnt's 4K intermediate checkpoints enable studying when knowledge is acquired/forgotten.
  - **Quick check question:** If a fact is learned at step 20K but forgotten at step 60K, what does Figure 9 suggest about its frequency category?

## Architecture Onboarding

- **Component map:**
  Pretraining Corpus (Wikipedia XML) -> Annotation Pipeline (Hyperlink extraction -> ReFinED entity linking -> Maverick coreference) -> Chunking (Variable Length Curriculum) -> Elasticsearch Index (10.5M chunks with entity metadata) -> Model Training (OLMo-2 architecture, 3 scales × 4 epochs) -> 4K Checkpoints (110 per epoch)

- **Critical path:**
  1. Query entity QID -> Retrieve chunks from index with score thresholds (H=1, EL≥0.6, C≥0.6)
  2. Map chunks to training steps via chunk IDs
  3. Evaluate model checkpoints on facts containing those entities

- **Design tradeoffs:**
  - **Precision vs. Recall in retrieval:** Higher thresholds reduce noise but miss indirect mentions
  - **Chunk size vs. context:** Smaller chunks preserve entity boundaries but lose cross-sentence context
  - **Training epochs vs. overfitting:** 6 epochs shown; >4 may degrade performance on some tasks (§A.4)

- **Failure signatures:**
  - Coreference clusters split across 50K-token windows (B.1) may miss long-distance references
  - Entity linking errors on rare entities (2.7% error rate per B.3)
  - Poor commonsense reasoning performance (Table 3) indicates knowledge ≠ reasoning

- **First 3 experiments:**
  1. **Retrieve and validate:** Pick 5 PopQA entities, retrieve their chunks with LMEnt vs. string-based search, manually verify precision
  2. **Checkpoint probing:** Load LMEnt-1B-0.1E and LMEnt-1B-6E, compare accuracy on facts with different Subject+Answer Chunk frequencies
  3. **Ablation test:** Disable coreference scores (set C=0) and measure retrieval performance drop on pronoun-heavy documents

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why do the rates of both learning and forgetting factual associations increase with fact frequency during pretraining?
- **Basis in paper:** [explicit] The authors state in Section 6 that while learning correlates with frequency, "the rates of both learning and forgetting increase with frequency—a phenomenon not yet fully understood."
- **Why unresolved:** The paper identifies this correlation using LMEnt checkpoints but does not propose or validate a theoretical explanation for why high-frequency facts exhibit higher volatility (both acquisition and loss) during training.
- **What evidence would resolve it:** Mechanistic interpretability studies on intermediate checkpoints to analyze how gradient updates for high-frequency facts interact with the model's internal knowledge circuits compared to low-frequency facts.

### Open Question 2
- **Question:** Does replacing implicit entity mentions (e.g., pronouns, descriptors) with explicit entity names in the pretraining corpus improve the model's ability to recall those facts?
- **Basis in paper:** [explicit] In Section 8.1, the authors suggest using LMEnt annotations "to edit the pretraining data, for example by replacing an implicit mention (pronouns, descriptors) with its explicit entity name" to potentially improve factuality.
- **Why unresolved:** While LMEnt provides the capability to identify and replace these mentions, the authors do not conduct experiments to validate if this data augmentation strategy actually yields better factual performance.
- **What evidence would resolve it:** An ablation study comparing the factual QA performance of a model trained on the standard LMEnt corpus against a model trained on a modified corpus where coreference mentions are swapped for explicit entity names.

### Open Question 3
- **Question:** At which specific training steps are language models most receptive to acquiring new factual knowledge (highest plasticity)?
- **Basis in paper:** [explicit] The authors list "investigating the plasticity of knowledge" as a key future application, specifically "identifying steps during pretraining when models are more receptive to acquiring new knowledge" (Section 8.1).
- **Why unresolved:** The current work focuses on tracking knowledge already present in the data, but does not measure the varying efficiency of knowledge acquisition (plasticity) across the 4K intermediate checkpoints.
- **What evidence would resolve it:** Controlled intervention experiments where specific facts are injected or upsampled at different training intervals (e.g., early vs. late pretraining) to measure the loss reduction and retention stability at each stage.

### Open Question 4
- **Question:** How does the inclusion of knowledge-poor data sources (e.g., code, synthetic text) affect the efficiency of factual knowledge acquisition compared to training solely on knowledge-rich corpora like Wikipedia?
- **Basis in paper:** [explicit] Section 8.1 proposes extending the dataset to sources like coding or synthetic stories "to analyze if the addition of these tokens improves factuality."
- **Why unresolved:** The current LMEnt models are trained exclusively on Wikipedia; the impact of mixing non-encyclopedic data on the compute-efficiency of learning factual associations remains untested.
- **What evidence would resolve it:** A scaling law analysis comparing factual QA performance (accuracy per FLOP) for models trained on pure Wikipedia versus models trained on a mixed corpus including code and synthetic data.

## Limitations
- Poor performance on commonsense reasoning tasks (Table 3) indicates knowledge ≠ reasoning
- Limited validation on out-of-distribution knowledge sources beyond Wikipedia
- Entity linking errors on rare entities (2.7% error rate) may affect retrieval completeness

## Confidence
- Entity linking and retrieval precision claims: **High** - Strong experimental validation with 66.3%-80.4% improvement metrics
- Compute efficiency claims: **Medium** - Performance comparisons valid but limited to Wikipedia scope
- Plasticity and learning-forgetting dynamics: **Medium** - Identified phenomena but mechanisms not fully explained

## Next Checks
1. Verify LMEnt retrieval precision on 10 randomly selected PopQA entities by manually checking top-5 chunk results
2. Compare LMEnt-1B-0.1E vs LMEnt-1B-6E accuracy on facts with varying Subject+Answer Chunk frequencies
3. Test coreference ablation by training on corpus with C=0 and measuring retrieval precision drop on pronoun-heavy documents