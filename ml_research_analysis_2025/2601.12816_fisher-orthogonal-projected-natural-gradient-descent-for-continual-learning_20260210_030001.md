---
ver: rpa2
title: Fisher-Orthogonal Projected Natural Gradient Descent for Continual Learning
arxiv_id: '2601.12816'
source_url: https://arxiv.org/abs/2601.12816
tags:
- fisher
- task
- fopng
- learning
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FOPNG (Fisher-Orthogonal Projected Natural
  Gradient Descent), a continual learning optimizer that reduces catastrophic forgetting
  by projecting gradients onto the Fisher-orthogonal complement of previous task gradients.
  Unlike existing methods that operate in Euclidean parameter space, FOPNG enforces
  orthogonality constraints in the Fisher-Riemannian manifold, providing reparameterization
  invariance and better preservation of prior task performance.
---

# Fisher-Orthogonal Projected Natural Gradient Descent for Continual Learning

## Quick Facts
- arXiv ID: 2601.12816
- Source URL: https://arxiv.org/abs/2601.12816
- Reference count: 21
- Primary result: FOPNG achieves significantly higher average accuracy than EWC and OGD on Split-MNIST, Split-CIFAR10/100, and Rotated-MNIST benchmarks

## Executive Summary
FOPNG (Fisher-Orthogonal Projected Natural Gradient Descent) is a continual learning optimizer that addresses catastrophic forgetting by projecting gradients onto the Fisher-orthogonal complement of previous task gradients. Unlike existing methods that operate in Euclidean parameter space, FOPNG enforces orthogonality constraints in the Fisher-Riemannian manifold, providing reparameterization invariance and better preservation of prior task performance. The method combines natural gradient descent with orthogonal gradient methods within an information-geometric framework, using a diagonal Fisher approximation for efficiency. Empirical results show FOPNG significantly outperforms baselines including EWC and OGD in most cases, with particularly strong performance on tasks where consecutive tasks have distributional similarity.

## Method Summary
FOPNG projects the current task's gradient onto the Fisher-orthogonal complement of previously stored task gradients. The method computes a projection matrix P = I - F_old·G·(G^T·F_old·F_new^{-1}·F_old·G)^{-1}·G^T·F_old that removes from the current gradient the component that would most affect prior task predictions under the Fisher metric. The update direction is then normalized to enforce a trust-region constraint ||v||_{F_new} = η, bounding the KL divergence induced by each update. The algorithm stores 80 gradients per task and uses exponential moving averaging (α=0.5) for updating the old Fisher matrix. A diagonal Fisher approximation is used for computational efficiency, with regularization λ=1e-4 for matrix inversion stability.

## Key Results
- FOPNG achieves average accuracies of 86.2% on Split-MNIST (vs 81.3% for EWC, 75.6% for OGD)
- On Split-CIFAR10, FOPNG reaches 62.4% average accuracy (vs 55.9% for EWC, 51.2% for OGD)
- Particularly strong performance on Rotated-MNIST (82.1% vs 74.8% for EWC, 68.3% for OGD)
- Remarkably stable across hyperparameter choices, requiring minimal tuning
- Outperforms all baselines on most benchmarks except Permuted-MNIST where it underperforms OGD/EWC

## Why This Works (Mechanism)

### Mechanism 1: Fisher-Orthogonal Projection Reduces Interference
Projecting gradients onto the Fisher-orthogonal complement of previous task gradients reduces interference with prior task outputs. The projection matrix removes from the current gradient the component that would most affect prior task predictions under the Fisher metric, unlike Euclidean orthogonality which ignores output sensitivity.

### Mechanism 2: Fisher-Riemannian Geometry Provides Reparameterization Invariance
Operating in Fisher-Riemannian geometry provides reparameterization invariance, making optimization robust to architecture-specific parameter scaling. The Fisher norm ||δθ||_F is invariant under smooth reparameterization, unlike Euclidean updates which can behave very differently under coordinate changes.

### Mechanism 3: Trust-Region Constraint Controls Distribution Change
The trust-region constraint ||v||_{F_new} = η bounds the KL divergence induced by each update, controlling how much the output distribution changes per step. By normalizing updates to fixed Fisher norm η, each step causes approximately bounded KL divergence regardless of gradient magnitude.

## Foundational Learning

- **Fisher Information Matrix**: Central to the entire method—defines the Riemannian metric, enables Fisher-orthogonality, quantifies output sensitivity. Quick check: Explain why a parameter direction with large Fisher norm causes larger output distribution change than one with small Fisher norm, even if both have identical Euclidean magnitude.

- **Natural Gradient Descent**: FOPNG extends natural gradient descent with an orthogonal projection constraint. Understanding that F^{-1} pre-conditions gradients to account for curvature is prerequisite to understanding why projection must happen in Fisher space. Quick check: Why does natural gradient descent converge faster than standard gradient descent on ill-conditioned loss landscapes?

- **Catastrophic Forgetting / Stability-Plasticity Tradeoff**: The core problem FOPNG addresses. Understanding that gradient updates improving current task loss typically degrade prior task performance motivates the constraint-based formulation. Quick check: In a two-task continual learning scenario, why doesn't simply reducing the learning rate solve catastrophic forgetting?

## Architecture Onboarding

### Component map:
Input: Current task batch (x, y), stored gradients G, old Fisher F_old
-> Compute current gradient: g = ∇_θ L_new(θ)
-> Compute new Fisher: F_new = diag(E[(∇ log p)^2]) [diagonal approx]
-> Build projection: P = I - F_old·G·(G^T·F_old·F_new^{-1}·F_old·G + λI)^{-1}·G^T·F_old
-> Project & normalize: v* = η · F_new^{-1}·P·g / ||P·g||_{F_new}
-> Output: Parameter update direction v*

### Critical path:
The matrix inversion (G^T·F_old·F_new^{-1}·F_old·G + λI)^{-1} is the computational bottleneck—O(m³) where m = stored gradients × tasks. With 80 gradients/task and 10 tasks, this is 800³ ≈ 5×10^8 operations per batch.

### Design tradeoffs:
- **Diagonal vs. full Fisher**: Paper uses diagonal (O(p) storage vs. O(p²)), acknowledging loss of parameter correlation information. KFAC could capture more geometry but increases complexity significantly.
- **Gradient buffer size**: 80/task is sufficient; larger buffers give diminishing returns.
- **FOPNG vs. FOPNG-PreFisher**: PreFisher stores F_i·g_i pre-multiplied, avoiding F_old in the per-iteration computation—better for long task sequences but requires computing full Fisher-vector products at task boundaries. Empirically nearly identical performance.

### Failure signatures:
- **Numerical instability**: Ill-conditioned (G^T·F_old·F_new^{-1}·F_old·G) matrix → requires regularization λ. Symptoms: NaN losses, exploding parameter magnitudes. Fix: increase λ from 1e-4 toward 1e-2.
- **Poor performance on dissimilar tasks**: On Permuted-MNIST (random permutations create OOD sequences), FOPNG underperforms OGD/EWC. This is expected behavior, not a bug.
- **Excessive wall-clock time**: ~40-80% slower than EWC/OGD due to per-sample gradient computation for Fisher estimation.

### First 3 experiments:
1. **Sanity check on Split-MNIST**: Replicate the 5-task split with MLP (2×100 hidden). Verify FOPNG maintains >90% average accuracy across tasks while SGD/Adam drop below 70% by task 5.
2. **Ablation: projection vs. no projection**: Compare FOPNG against FNG (Fisher Natural Gradient—same but without projection). Expect FNG to show significant forgetting, confirming the projection mechanism is necessary.
3. **Hyperparameter sensitivity stress test**: On Rotated-MNIST, sweep λ ∈ {1e-5, 1e-4, 1e-3, 1e-2} and α ∈ {0.2, 0.5, 0.8}. Verify the paper's claim of "remarkable stability"—accuracy should stay within ~2% across this range.

## Open Questions the Paper Calls Out

### Open Question 1
Can more expressive Fisher approximations (e.g., KFAC, EKFAC, low-rank) improve FOPNG's performance or convergence properties compared to the diagonal approximation? The authors only implemented the diagonal Fisher due to efficiency; the trade-off between approximation quality and computational cost for FOPNG specifically remains unexplored.

### Open Question 2
Can the gradient buffer be compressed (via low-rank approximations or other methods) to constant memory while preserving FOPNG's anti-forgetting benefits? Current storage scales linearly with task count (80 gradients per task), limiting applicability to long task sequences.

### Open Question 3
Why does FOPNG underperform on highly dissimilar sequential tasks like Permuted-MNIST, and can this failure mode be mitigated? The geometric intuition for why Fisher-orthogonal projections help on similar tasks but not dissimilar ones remains unexplained.

### Open Question 4
Why is FOPNG remarkably robust to the α hyperparameter, and does this indicate that Fisher information changes little across typical continual learning tasks? The robustness is observed empirically but the underlying mechanism is not characterized.

## Limitations
- The diagonal Fisher approximation may discard critical off-diagonal curvature information, potentially degrading geometric benefits
- Performance degrades on highly dissimilar tasks like Permuted-MNIST where distributional overlap is minimal
- Computational overhead of ~40-80% slower than baselines due to per-sample Fisher estimation and gradient storage

## Confidence

**High**: FOPNG's empirical superiority over EWC/OGD on Split-MNIST, Split-CIFAR10/100, and Rotated-MNIST (Fig 1, Table 1)

**Medium**: Mechanism claims (orthogonal projection reduces interference, Fisher geometry provides reparameterization invariance) - well-motivated but lack direct ablation tests

**Medium**: Stability claim - supported by Rotated-MNIST sweeps but not comprehensive across all benchmarks

## Next Checks

1. **Ablation study on projection mechanism**: Implement FNG (Fisher Natural Gradient without projection) and verify it shows significant forgetting compared to FOPNG on all benchmarks

2. **Hyperparameter sensitivity stress test**: On Split-MNIST, sweep λ ∈ {1e-5, 1e-4, 1e-3, 1e-2} and α ∈ {0.2, 0.5, 0.8}; confirm stability claim holds across 2% accuracy range

3. **Computational efficiency profiling**: Measure wall-clock time vs. EWC/OGD; identify Fisher computation as bottleneck and explore per-sample gradient parallelization strategies