---
ver: rpa2
title: 'AI Alignment in Medical Imaging: Unveiling Hidden Biases Through Counterfactual
  Analysis'
arxiv_id: '2504.19621'
source_url: https://arxiv.org/abs/2504.19621
tags:
- latexit
- counterfactual
- diffusion
- image
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel statistical framework to evaluate
  medical imaging ML models' dependency on sensitive attributes like demographics.
  The method leverages counterfactual invariance, measuring how model predictions
  change under hypothetical alterations to sensitive attributes.
---

# AI Alignment in Medical Imaging: Unveiling Hidden Biases Through Counterfactual Analysis

## Quick Facts
- arXiv ID: 2504.19621
- Source URL: https://arxiv.org/abs/2504.19621
- Reference count: 40
- Introduces a novel statistical framework to evaluate medical imaging ML models' dependency on sensitive attributes like demographics

## Executive Summary
This paper presents a novel statistical framework to detect and quantify biases in medical imaging AI systems by measuring counterfactual invariance - whether model predictions remain stable under hypothetical changes to sensitive attributes. The method leverages conditional latent diffusion models to generate counterfactual images without requiring real-world counterfactual data, then applies statistical hypothesis testing to evaluate bias. Experiments on synthetic and real-world datasets (CheXpert and MIMIC-CXR) demonstrate superior performance compared to standard fairness metrics like Demographic Parity and Equality of Opportunity.

## Method Summary
The framework detects causal bias by validating if a diagnostic model is counterfactually invariant - predictions don't change when sensitive attributes are hypothetically altered while preserving all other factors. It implements a statistical test based on conditional latent diffusion models augmented with mutual information minimization to disentangle sensitive attributes from latent representations. The method generates counterfactual images and applies paired t-tests to compare prediction distributions, quantifying biases without requiring actual counterfactual data.

## Key Results
- The approach aligns with counterfactual fairness principles and outperforms baselines like Demographic Parity and Equality of Opportunity
- Demonstrates practical applicability on synthetic and real-world datasets (CheXpert and MIMIC-CXR)
- Provides a robust tool to detect and quantify biases in medical imaging AI systems
- Code available at https://github.com/Neferpitou3871/AI-Alignment-Medical-Imaging

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The framework detects causal bias by validating if a diagnostic model is counterfactually invariant.
- **Mechanism:** Implements a statistical test based on Theorem 4.2, transforming fairness into a quantifiable hypothesis test comparing expected predictions conditioned on sensitive attributes versus representations.
- **Core assumption:** The latent representation Z forms a valid adjustment set satisfying conditional ignorability.
- **Evidence anchors:** Theorem 4.2 states a model is counterfactually invariant "if and only if" the expectation equality holds; related work focuses on adversarial training rather than statistical detection.

### Mechanism 2
- **Claim:** High-fidelity counterfactual images can be generated to test invariance without requiring real-world counterfactual data.
- **Mechanism:** Uses Conditional Latent Diffusion Model augmented with non-linear transformation φ that minimizes Mutual Information between latent code Z and sensitive attribute A.
- **Core assumption:** The perturbation network can successfully disentangle A from Z such that Z ⊥ A.
- **Evidence anchors:** The transformation φ ensures representation Z is approximately independent of label A; algorithm generates images conditioned on attributes to estimate functions.

### Mechanism 3
- **Claim:** Standard fairness metrics fail to detect causal dependencies because they rely on association.
- **Mechanism:** Compares Expected Counterfactual Accuracy (ECA) against p-values from CIT-LR and baselines, measuring causal effect through intervention on A in the generative process.
- **Core assumption:** ECA computed on synthetic/semi-real data represents ground truth of bias.
- **Evidence anchors:** CIT-LR rejects hypothesis test when ECA is low and doesn't when ECA is high, demonstrating stronger consistency over EO and DP.

## Foundational Learning

### Concept: Structural Causal Models (SCMs) & Interventions (do-calculus)
- **Why needed here:** The core definition of bias relies on the difference between observing (A=a) and intervening (do(A=a)). Without understanding do-calculus, one cannot distinguish correlation from causation in generated images.
- **Quick check question:** Why does P(Y|A=a) ≠ P(Y|do(A=a)) in the presence of confounders?

### Concept: Mutual Information Neural Estimation (MINE)
- **Why needed here:** The architecture uses min-max optimization problem to force independence between latent variables and sensitive attributes. Understanding MINE is necessary to debug the disentanglement step.
- **Quick check question:** How does the dual representation of KL-divergence allow us to estimate Mutual Information using a neural network?

### Concept: Latent Diffusion Models (LDMs)
- **Why needed here:** The system operates in compressed latent space for efficiency. One must understand the tradeoff between computational cost and image fidelity when modifying diffusion steps.
- **Quick check question:** Why does reducing the number of diffusion steps (T) from 1000 to 250 reduce variance in counterfactual generation?

## Architecture Onboarding

### Component map:
- Input: Image X, Sensitive Attribute A, Base Classifier f
- CLDM (Generator): Encoder E_η → Latent z₀ → Transform φ → Diffusion Process q/p → Decoder D_η → Counterfactual Image X̂
- Estimator: Functions ĝ(A, Z) and ĥ(Z) learned via generated images
- Tester: Paired Student's t-test comparing distributions of Ŷĝ and Ŷĥ

### Critical path:
1. Train CLDM on X conditioned on A
2. Train transformation φ to disentangle Z from A (MINE)
3. Generate counterfactuals and estimate g, h
4. Run statistical test on held-out test set

### Design tradeoffs:
- Variance vs. Quality: Reduces diffusion steps (T=250) and noise to lower variance in counterfactuals, accepting potential drop in generation quality for stable statistical testing
- Sample Efficiency: Uses cross-fitting to limit bias, requiring more data but ensuring valid p-values

### Failure signatures:
- High Reconstruction Error: If L1 distance between original and "do-nothing" reconstruction is high, CLDM is unstable
- Low Effectiveness Accuracy: If generated counterfactual doesn't look like target attribute to a classifier, test is invalid

### First 3 experiments:
1. Sanity Check (Synthetic): Run Algorithm 1 on "Linear" synthetic dataset, verify p-values correlate with known ECA
2. Ablation (Disentanglement): Train CLDM without transformation φ (λ=0) on CheXpert, compare effectiveness accuracy and FID scores
3. Sensitivity Analysis: Vary significance level α and observe False Positive Rate on unbiased dataset

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the latent representation learned by the CLDM capture all relevant confounding factors required for a valid adjustment set?
- **Basis in paper:** The authors state in the Impact Statement that the method relies on the key assumption that latent representations capture all relevant factors, which is not adequately proved.
- **Why unresolved:** Theorem 4.2 relies on the assumption that Z forms a valid adjustment set, but there's no theoretical or empirical verification that CLDM encoder preserves all necessary causal factors.
- **What evidence would resolve it:** A theoretical proof or empirical validation on a dataset with known ground-truth confounders to verify they are linearly recoverable in learned latent space.

### Open Question 2
- **Question:** How does the presence of unobserved confounders affect the validity and reliability of the proposed Counterfactual Invariance Test?
- **Basis in paper:** The authors identify this as a direction for future work: "Future work should aim to... address unobserved confounding."
- **Why unresolved:** The current methodology assumes conditional ignorability (that Z blocks all backdoor paths), which is violated if unobserved confounders exist outside image data.
- **What evidence would resolve it:** Experiments on synthetic datasets with explicitly modeled unobserved confounders to measure degradation in test's statistical power or error rates.

### Open Question 3
- **Question:** How does the inherent variance in diffusion-based counterfactual image generation impact the stability of the statistical hypothesis testing?
- **Basis in paper:** The authors note a challenge: "the hard-to-control variance and complexity of diffusion models make the generative model hard to train."
- **Why unresolved:** If the generative process exhibits high variance, generated counterfactual images might differ due to noise rather than intervention on sensitive attribute.
- **What evidence would resolve it:** Sensitivity analysis measuring standard deviation of p-values produced by algorithm across multiple sampling runs of diffusion model.

## Limitations

- **Conditional Ignorability Assumption**: The method's validity depends on latent representation Z fully capturing all confounders between sensitive attributes and predictions, which may be violated in real-world data.
- **Counterfactual Generation Quality**: The paper reports FID scores around 15-20, indicating room for improvement in image quality which could affect statistical test reliability.
- **Statistical Power**: The paired t-test requires sufficient sample size to detect subtle biases, and the method may lack power with small test sets or minimal treatment effects.

## Confidence

- **Counterfactual Invariance Detection**: High - statistical framework and synthetic experiments demonstrate clear signal detection
- **Outperformance of Standard Fairness Metrics**: Medium-High - results show better alignment with ground-truth ECA, though synthetic benchmarks may not capture all real-world complexities
- **Practical Applicability to CheXpert/MIMIC-CXR**: Medium - method works on demonstrated datasets but generalization to other imaging modalities requires validation

## Next Checks

1. **Confounder Validation**: Systematically test whether Z captures all relevant confounders by introducing synthetic confounding variables in controlled experiments and measuring false positive rates.

2. **Cross-Modality Generalization**: Apply the framework to non-chest X-ray modalities (CT, MRI) to evaluate performance across different imaging types and disease presentations.

3. **Sample Size Sensitivity**: Conduct power analysis to determine minimum test set sizes required for reliable bias detection across different effect sizes and significance thresholds.