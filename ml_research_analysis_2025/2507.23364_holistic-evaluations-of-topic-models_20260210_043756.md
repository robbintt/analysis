---
ver: rpa2
title: Holistic Evaluations of Topic Models
arxiv_id: '2507.23364'
source_url: https://arxiv.org/abs/2507.23364
tags:
- topic
- bertopic
- topics
- will
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates topic modeling with BERTopic using 1140 runs
  to explore parameter optimization trade-offs. BERTopic uses UMAP for dimensionality
  reduction and HDBScan for clustering, which are stochastic, leading to variability
  in outputs.
---

# Holistic Evaluations of Topic Models

## Quick Facts
- arXiv ID: 2507.23364
- Source URL: https://arxiv.org/abs/2507.23364
- Reference count: 24
- Primary result: BERTopic parameter optimization involves trade-offs between coverage and topic quality, with stochastic clustering requiring multi-run validation

## Executive Summary
This paper evaluates BERTopic topic modeling across 1140 runs to understand parameter optimization trade-offs and stochastic variability. The study introduces metrics like Gini Coefficient, Keyword Frequency Score, and Ngram Uniqueness Value to assess topic model quality. Key findings reveal that higher model coverage (fewer sentences in the -1 topic) correlates with greater topic skewness and lower NUV, indicating reduced topic quality. An inverted semi-supervised model showed lower NUV than BERTopic, suggesting potential clustering inefficiencies. The results highlight the need for careful parameter tuning and multi-run comparisons to ensure reliable topic model interpretation.

## Method Summary
The study evaluates BERTopic using 'all-MiniLM-L6-v2' embeddings with systematic parameter sweeps over min_cluster_size, min_topic_size, and n_neighbors. Custom metrics were implemented including Gini Coefficient for topic imbalance, Ngram Uniqueness Value (NUV) for topic overlap, and keyword frequency scores. The analysis compared BERTopic to Top2Vec and LDA (Gensim, 20 topics), and tested a semi-supervised inverted model using high-frequency ngrams with cosine similarity thresholds. The large-scale evaluation across two corpora of different sizes examined the trade-offs between coverage, topic distribution balance, and semantic distinctiveness.

## Key Results
- Higher model coverage (reducing -1 topic assignments) produces more skewed topic distributions with statistically significant correlations (Spearman's Rho -0.55 to 0.57)
- BERTopic's ngram outputs remain stable across parameter variations despite stochastic clustering, with no significant relationship between coverage and topic stability
- Semi-supervised anchor-based clustering produces lower NUV (0.67 vs BERTopic's 0.81 mean), suggesting less semantically distinct topics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing model coverage produces more skewed topic distributions
- Mechanism: When parameters are tuned to capture more sentences, HDBScan assigns additional sentences to already-large topics rather than creating balanced new clusters
- Core assumption: Skewed distributions indicate suboptimal clustering rather than genuine corpus structure
- Evidence anchors:
  - [section 5] "Spearman's Rho -0.55, p = 0.0000... statistically significant relationship between these metrics"
  - [section 5] "Primary effect of decreasing the number of sentences in the -1 topic is increasing the number of sentences in the top topics"

### Mechanism 2
- Claim: BERTopic's ngram outputs remain stable across parameter variations
- Mechanism: C-TF-IDF extraction operates on clustered output and consistently selects similar representative ngrams even when cluster boundaries shift
- Core assumption: Topic names derived from top ngrams adequately represent cluster semantic content
- Evidence anchors:
  - [section 8] "Spearman's Rho = -0.06, p = 0.7137... no statistically significant relationship between topic name values and sentence coverage"
  - [section 6] "Repeated runs produce similar results"

### Mechanism 3
- Claim: Semi-supervised anchor-based clustering produces lower NUV
- Mechanism: The inverted model uses high-frequency ngram anchor sentences with cosine similarity thresholds, creating less semantically distinct topics
- Core assumption: Higher NUV indicates better topic differentiation
- Evidence anchors:
  - [section 8] "NUV was 0.67, under the BERTopic mean of 0.81... below the minimum value for BERTopic computed"
  - [section 4] "Mean for the NUV is 0.81 with standard deviation of 0.01"

## Foundational Learning

- Concept: **Stochastic vs. deterministic clustering**
  - Why needed here: BERTopic's UMAP+HDBScan pipeline produces different outputs on re-runs with identical parameters
  - Quick check question: If you run BERTopic twice with the same parameters on the same data, should you expect identical topic assignments?

- Concept: **-1 topic (error/unclassified cluster)**
  - Why needed here: HDBScan assigns sentences it cannot confidently cluster to topic -1, correlating with other quality metrics
  - Quick check question: Does a smaller -1 topic always indicate a better model?

- Concept: **Gini Coefficient for distribution imbalance**
  - Why needed here: Measures inequality in topic sizes; high Gini indicates few topics dominate coverage
  - Quick check question: If Gini = 0.76, what does that tell you about your topic distribution?

## Architecture Onboarding

- Component map: Input Documents → Sentence Transformers (embeddings) → UMAP (dimensionality reduction, stochastic) → HDBScan (clustering, stochastic) → C-TF-IDF (ngram extraction, deterministic) → Topic Outputs

- Critical path:
  1. Embedding quality affects semantic clustering baseline
  2. UMAP `n_neighbors` controls local vs global structure preservation
  3. HDBScan `min_cluster_size` and BERTopic `min_topic_size` jointly determine granularity and -1 topic size

- Design tradeoffs:
  - Coverage vs. balance: Reducing -1 topic increases coverage but raises Gini
  - Repeatability vs. quality: HDBScan sacrifices determinism for semantically meaningful clusters
  - Runtime vs. validation: Pre-computed embeddings reduce 30 runs from ~hours to ~2 hours

- Failure signatures:
  - Gini > 0.70 with large -1 topic: Model struggling to differentiate themes
  - NUV < 0.75: High ngram overlap suggests poor semantic separation
  - Keyword Frequency Score well below corpus mean: May indicate poor representation

- First 3 experiments:
  1. Run BERTopic 10 times with identical default parameters to quantify stochastic range
  2. Systematically vary `min_topic_size` and `n_neighbors`, plotting error size vs. Gini
  3. Compute c_npmi coherence scores alongside proposed metrics on held-out subset

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the improved clustering quality of HDBScan justify its lack of repeatability compared to deterministic methods like K-means?
  - Basis: The author asks about the trade-off between HDBScan's improved clustering versus K-means' repeatability
  - Why unresolved: The study focuses on parameter tuning within BERTopic rather than direct comparative benchmarking
  - What evidence would resolve it: A controlled study comparing semantic coherence and stability of HDBScan vs K-means clusters

- **Open Question 2**: Should topic model evaluation prioritize top N topics or the entire model distribution?
  - Basis: The author poses whether we're evaluating the topic model as a whole or just the top 20 topics
- **Open Question 3**: Does BERTopic's label resilience suggest these corpora may be suited to deterministic approaches?
  - Basis: The author asks if stable labels imply suitability for deterministic modeling
  - Why unresolved: The inverted model test suggested clustering inefficiencies but label stability could indicate dataset simplicity
  - What evidence would resolve it: Comparative analysis of label diversity and accuracy using both BERTopic and deterministic approaches

## Limitations

- Stochastic variability from UMAP and HDBScan cannot be eliminated, only managed through repeated sampling
- The inverted semi-supervised model's methodology remains underspecified, particularly regarding anchor selection
- Correlation patterns observed may reflect corpus-specific characteristics rather than universal clustering behavior
- Relationship between proposed metrics and downstream task performance remains unvalidated

## Confidence

- **High Confidence**: BERTopic's stochastic variability is real and measurable; repeated runs produce different outputs
- **Medium Confidence**: Ngram stability across runs despite clustering variability; inverted model produces lower NUV
- **Low Confidence**: Whether high Gini necessarily indicates poor clustering quality; whether proposed metrics correlate with task performance

## Next Checks

1. Apply high-Gini and low-Gini BERTopic models to downstream tasks (document classification, information retrieval) to determine if Gini correlates with task performance degradation
2. Replicate the correlation analysis (error size vs. Gini) on multiple diverse corpora to establish whether the relationship is domain-specific or universal
3. Fully document and open-source the inverted semi-supervised model implementation for reproducibility and community validation