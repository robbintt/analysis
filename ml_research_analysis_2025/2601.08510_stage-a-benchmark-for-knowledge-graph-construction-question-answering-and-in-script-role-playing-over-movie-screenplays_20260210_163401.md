---
ver: rpa2
title: 'STAGE: A Benchmark for Knowledge Graph Construction, Question Answering, and
  In-Script Role-Playing over Movie Screenplays'
arxiv_id: '2601.08510'
source_url: https://arxiv.org/abs/2601.08510
tags:
- narrative
- event
- character
- screenplay
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "STAGE introduces a unified benchmark for narrative understanding\
  \ over movie screenplays, evaluating four tasks\u2014knowledge graph construction,\
  \ event summarization, question answering, and in-script role-playing\u2014grounded\
  \ in a shared story world. Using 150 screenplays (108 English, 42 Chinese), the\
  \ benchmark assesses models' abilities to build structured world representations,\
  \ reason over long contexts, and generate character-consistent responses."
---

# STAGE: A Benchmark for Knowledge Graph Construction, Question Answering, and In-Script Role-Playing over Movie Screenplays

## Quick Facts
- arXiv ID: 2601.08510
- Source URL: https://arxiv.org/abs/2601.08510
- Reference count: 40
- Primary result: Unified benchmark evaluating narrative understanding tasks over movie screenplays, with best-performing models achieving F1 up to 0.67 (knowledge graph) and QA accuracy up to 65.4%

## Executive Summary
STAGE introduces a benchmark for evaluating narrative understanding across four tasks—knowledge graph construction, event summarization, question answering, and in-script role-playing—using 150 movie screenplays (108 English, 42 Chinese). The benchmark enables holistic assessment of models' abilities to build structured world representations, reason over long contexts, and generate character-consistent responses. Key results show GraphRAG and Extract–Define–Canonicalize excel at knowledge graph construction, Hybrid retrieval achieves strong QA accuracy, and episodic memory integration improves character role-playing consistency and narrative faithfulness.

## Method Summary
The benchmark evaluates models on four narrative understanding tasks using a unified corpus of 150 movie screenplays. For knowledge graph construction, models extract entities, relationships, and attributes from scripts. Event summarization requires condensing plot events. Question answering involves reasoning over long script contexts. In-script role-playing tests character-consistent response generation. Models are evaluated using standard metrics (F1, accuracy, consistency scores), with GraphRAG and Hybrid retrieval showing top performance across tasks.

## Key Results
- Knowledge graph construction: GraphRAG and Extract–Define–Canonicalize achieve F1 scores up to 0.67
- Question answering: Hybrid retrieval delivers stable accuracy up to 65.4%
- Role-playing: Episodic memory and narrative facts significantly improve persona consistency and narrative faithfulness over static persona settings

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its unified evaluation framework, which grounds multiple narrative understanding tasks in a shared story world. By leveraging movie screenplays, the benchmark captures rich, multimodal narrative structures, enabling assessment of both factual knowledge extraction and generative, character-driven tasks. The integration of episodic memory and narrative facts in role-playing tasks addresses the challenge of maintaining long-term character consistency across extended contexts.

## Foundational Learning
- **Knowledge Graph Construction**: Extracting structured entities, relationships, and attributes from unstructured narrative text—needed to build world representations for reasoning; quick check: verify entity and relation extraction accuracy on held-out scripts
- **Long-Context Reasoning**: Processing and integrating information across extended screenplay narratives—needed for accurate QA and event summarization; quick check: test model performance on increasingly long script excerpts
- **Character Consistency in Role-Playing**: Generating responses that align with a character's established persona and narrative arc—needed for believable in-script interactions; quick check: measure persona adherence and narrative faithfulness in generated outputs

## Architecture Onboarding

**Component Map**
```
Screenplay Corpus -> Knowledge Graph Construction -> Event Summarization -> QA System -> Role-Playing Generator
                      |                                |              |               |
                      v                                v              v               v
                   GraphRAG                        Hybrid Retrieval  Episodic Memory  Persona Models
```

**Critical Path**
The critical path for holistic evaluation flows from the shared screenplay corpus through each task: knowledge graph construction (structured world modeling), event summarization (plot comprehension), QA (factual and inferential reasoning), and role-playing (character-consistent generation). Success in each stage builds on the previous, with episodic memory integration being pivotal for maintaining consistency in role-playing.

**Design Tradeoffs**
- Small corpus (150 screenplays) enables focused, high-quality annotations but limits generalizability and robustness to script diversity
- Multilingual focus (English/Chinese) allows cross-linguistic evaluation but restricts broader language coverage
- Static vs. dynamic persona settings: episodic memory integration improves consistency but increases computational overhead

**Failure Signatures**
- Knowledge graph construction: Low F1 due to entity/relationship extraction errors or script annotation noise
- QA: Reduced accuracy on long-context or inferential questions, especially for less-represented languages
- Role-playing: Persona drift or narrative inconsistency, particularly when episodic memory is absent or under-resourced

**3 First Experiments**
1. Replicate knowledge graph and QA results on a held-out set of screenplays from additional languages or genres
2. Conduct inter-annotator agreement analysis and error analysis for multi-task annotations
3. Perform ablation studies comparing episodic memory vs. static persona in role-playing

## Open Questions the Paper Calls Out
None provided.

## Limitations
- Small corpus (150 screenplays) raises questions about generalizability and robustness to script diversity
- Limited language coverage (English, Chinese) restricts cross-linguistic validity
- Sparse detail on inter-annotator agreement and quality control for multi-task annotations

## Confidence
- Knowledge graph and QA performance claims: High
- Role-playing persona and consistency improvements: High
- Benchmark's broader impact and robustness: Medium

## Next Checks
1. Replicate knowledge graph and QA results on held-out screenplays, ideally from additional languages or genres
2. Conduct inter-annotator agreement analysis and error analysis for multi-task annotations
3. Perform ablation studies to isolate impact of episodic memory and narrative facts in role-playing, comparing against static persona settings