---
ver: rpa2
title: Symbolic Rule Extraction from Attention-Guided Sparse Representations in Vision
  Transformers
arxiv_id: '2505.06745'
source_url: https://arxiv.org/abs/2505.06745
tags:
- loss
- sparse
- rule-set
- each
- rule
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NeSyViT, a framework that extracts executable
  symbolic logic programs from Vision Transformers (ViTs) by introducing a sparse
  concept layer that produces binarized, disentangled representations of high-level
  visual concepts. The method combines supervised contrastive loss, entropy minimization,
  and L1 sparsity to encourage class-specific clustering and binary activation patterns.
---

# Symbolic Rule Extraction from Attention-Guided Sparse Representations in Vision Transformers

## Quick Facts
- arXiv ID: 2505.06745
- Source URL: https://arxiv.org/abs/2505.06745
- Reference count: 4
- Primary result: Vision Transformer framework that extracts executable symbolic logic programs while improving classification accuracy by 5.14%

## Executive Summary
This paper introduces NeSyViT, a framework that extracts executable symbolic logic programs from Vision Transformers (ViTs) by introducing a sparse concept layer that produces binarized, disentangled representations of high-level visual concepts. The method combines supervised contrastive loss, entropy minimization, and L1 sparsity to encourage class-specific clustering and binary activation patterns. These representations are then used by the FOLD-SE-M algorithm to generate interpretable rule-sets in the form of stratified answer set programs. Experiments show that NeSyViT achieves 5.14% higher classification accuracy than the vanilla ViT while producing rule-sets that are 67% smaller and more interpretable than those generated by comparable CNN-based methods.

## Method Summary
NeSyViT modifies the standard ViT architecture by replacing the final classification head with a single linear layer (dimension D=128) that operates on the [CLS] token's attention-weighted patch representations. This sparse concept layer is trained with a combined loss function: supervised contrastive loss to cluster same-class representations, entropy minimization to push activations toward binary extremes (0 or 1), and L1 sparsity to limit active neurons. After training, binarized vectors (threshold 0.5) form a binary table that feeds into the FOLD-SE-M algorithm to generate stratified answer set programs with default logic rules.

## Key Results
- NeSyViT achieves 5.14% better classification accuracy than the standard ViT on benchmark datasets
- Extracted rule-sets are 67% smaller (fewer rules, predicates, and predicate occurrences) than CNN-based methods
- Maintains high interpretability through stratified answer set programs with default logic rules

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse concept layer produces disentangled, binarized representations that enable rule extraction
- Mechanism: A single linear layer with sigmoid activation replaces the standard classification head. L1 sparsity forces most neurons toward zero, entropy minimization pushes activations toward 0 or 1 (not 0.5), and supervised contrastive loss clusters same-class representations together. This creates a binary vector per image where few neurons fire, each representing high-level concepts.
- Core assumption: Attention-weighted patch representations from the [CLS] token contain sufficient global information to disentangle into concept-specific neurons.
- Evidence anchors:
  - [abstract] "This linear layer operates on attention-weighted patch representations and learns a disentangled, binarized representation in which individual neurons activate for high-level visual concepts."
  - [section 3] "We modify the architecture of a standard Vision Transformer by replacing the final classification head... with a single linear layer of dimension D."
  - [corpus] Related work (ASCENT-ViT) addresses concept learning in ViTs, suggesting this is an active research direction but corpus lacks direct validation of this specific sparse-layer approach.
- Break condition: If neurons remain polysemantic (activating for multiple unrelated concepts), rule interpretability degrades. Paper acknowledges this limitation in semantic labeling results.

### Mechanism 2
- Claim: Combined loss function improves accuracy while enabling interpretability (avoiding typical accuracy drop)
- Mechanism: Three losses work jointly: (1) Supervised contrastive loss clusters same-class vectors in latent space, enabling FOLD-SE-M to find clean decision boundaries; (2) Entropy minimization forces activations toward binary extremes, reducing information loss during thresholding; (3) L1 sparsity limits active neurons, producing compact rule-sets.
- Core assumption: The three objectives (clustering, binarization, sparsity) are not mutually contradictory and can be optimized simultaneously.
- Evidence anchors:
  - [abstract] "Our method achieves a 5.14% better classification accuracy than the standard ViT while enabling symbolic reasoning."
  - [section 4, Table 2] "NeSyViT improves upon the accuracy of the Vanilla ViT by an average of 5.14%... whereas NeSyFOLD suffers an average accuracy drop of 10.14%."
  - [corpus] No comparable corpus evidence found for combined contrastive + entropy + sparsity loss achieving accuracy gains in neuro-symbolic extraction.
- Break condition: If loss weights (α, β, γ) are poorly balanced, one objective dominates. Paper provides specific hyperparameters (α=2, β=1, γ=1 for Places; α=4, β=0.001, γ=0.001 for GTSRB) but offers no tuning guidance.

### Mechanism 3
- Claim: FOLD-SE-M generates stratified Answer Set Programs from binarized activations
- Mechanism: After training, binarized vectors (threshold at 0.5) form a binary table (rows=images, columns=neurons). FOLD-SE-M incrementally learns default rules covering positive examples while avoiding negatives, then recursively learns exceptions. Output is a stratified ASP with interpretable if-then-else logic.
- Core assumption: Binary vectors preserve enough class-discriminative information for rule learning after thresholding.
- Evidence anchors:
  - [section 2.2] "FOLD-SE-M incrementally generates literals for default rules that cover positive examples while avoiding covering negative examples."
  - [section 3] "These vectors are then passed to the FOLD-SE-M algorithm to generate a symbolic rule-set in the form of a stratified Answer Set Program."
  - [corpus] NEUROLOGIC paper addresses rule extraction from neural networks but focuses on fully connected networks, not ViTs.
- Break condition: If binarization creates ambiguous cases (many values near 0.5), rule accuracy drops. Entropy loss mitigates this during training.

## Foundational Learning

- Concept: Vision Transformer (ViT) architecture
  - Why needed here: Understanding how [CLS] token aggregates global information, why patches replace convolutions, and why ViTs lack modular concept detectors is essential for grasping the extraction challenge.
  - Quick check question: Can you explain why the [CLS] token, not individual patch tokens, feeds into the sparse concept layer?

- Concept: Sparse Autoencoders (SAEs) and sparsity principles
  - Why needed here: The sparse concept layer draws from SAE principles. Understanding why L1 regularization promotes sparse activations and how sparsity aids interpretability is foundational.
  - Quick check question: Why does L1 regularization (vs. L2) produce sparse solutions where many weights become exactly zero?

- Concept: Answer Set Programming (ASP) and stratified negation
  - Why needed here: The extracted rules are stratified ASP programs with default logic. Understanding negation-as-failure and stratification is required to interpret the rule-sets.
  - Quick check question: In the rule `target(X, 'bathroom') :- not refrigerator(X).`, what does `not` mean in ASP (hint: it's not classical negation)?

## Architecture Onboarding

- Component map: Input Image (224×224) -> Patch Embedding (16×16 patches) -> 12 Transformer Blocks (12 heads each) -> [CLS] Token Vector (768-dim) -> Sparse Concept Layer (Linear: 768→128 + Sigmoid) -> Binarized Vector (128-dim, threshold 0.5) -> FOLD-SE-M Rule Interpreter → Predicted Class

- Critical path: The sparse concept layer is the novel component. Its 128-dim output must balance sparsity (few active neurons), binarization (values near 0 or 1), and clustering (same-class similarity). Training failure here cascades to poor rule quality.

- Design tradeoffs:
  - Sparse concept layer dimension (D=128): Larger D increases concept capacity but reduces sparsity and rule compactness. Paper provides no ablation on this choice.
  - Loss weights vary drastically between datasets (Places: α=2, β=1, γ=1; GT43: α=4, β=0.001, γ=0.001), suggesting sensitivity to domain characteristics.
  - Post-hoc semantic labeling vs. integrated: Paper adopts CNN-based labeling algorithm but finds it struggles with ViT polysemantic neurons.

- Failure signatures:
  - Accuracy drops below vanilla ViT → Check if entropy loss weight is too low (activations stuck near 0.5).
  - Rule-set size explodes → Check if L1 sparsity weight is too low (too many active neurons).
  - Semantic labels make no sense → Expected limitation; paper notes neurons attend to multiple concepts simultaneously in ViTs.

- First 3 experiments:
  1. Reproduce P3.1 results (bathroom/bedroom/kitchen) with provided hyperparameters to validate pipeline: expected ~99% accuracy, 3 rules.
  2. Ablate each loss component (train with 2 of 3) to measure individual contribution to accuracy and rule compactness.
  3. Vary sparse concept layer dimension (D=64, 128, 256) to find minimum dimension that maintains accuracy while minimizing rule-set size.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can architectural or training refinements improve neuron monosemanticity in the sparse concept layer for better concept disentanglement?
- Basis in paper: [explicit] Authors state: "Future work will focus on improving neuron disentanglement and enhancing monosemanticity using architectural or training refinements."
- Why unresolved: Neurons in ViTs attend to multiple regions simultaneously; Figure 2 shows neurons 43 and 105 lack clear concept selectivity, producing mixed-feature responses.
- What evidence would resolve it: Demonstration of modified architecture or training procedure yielding neurons with higher single-concept IoU scores and qualitatively cleaner activation patterns.

### Open Question 2
- Question: Can extracted rule-sets be used to identify and correct dataset bias in Vision Transformers?
- Basis in paper: [explicit] Authors note: "Another promising direction explored in CNN-based interpretability methods is bias correction using extracted rule-sets which could be the natural next step for this work."
- Why unresolved: Bias correction using symbolic rules has only been demonstrated for CNN-based frameworks (NeSyFOLD), not transformer architectures with their different representational structure.
- What evidence would resolve it: Application of NeSyViT to a known biased dataset showing both bias identification through rule inspection and accuracy improvement after correction.

### Open Question 3
- Question: Can multimodal LLMs enable automatic semantic labeling without requiring pixel-level segmentation masks?
- Basis in paper: [explicit] Authors state: "We also plan to explore using multimodal LLMs like GPT-4o for concept labeling, enabling an automatic semantic annotation pipeline that does not rely on pixel-level segmentation masks."
- Why unresolved: Current semantic labeling requires pre-annotated segmentation masks, limiting applicability to datasets without such annotations.
- What evidence would resolve it: Demonstration of LLM-based labeling achieving comparable IoU with mask-based approach while generalizing to datasets lacking segmentation annotations.

## Limitations
- Sparse concept layer's ability to produce truly disentangled, class-specific concepts remains unverified due to inherent ViT polysemanticity
- Method shows high sensitivity to hyperparameter tuning, with drastically different loss weights required for different datasets
- Semantic labeling results are unreliable and acknowledged by authors as a limitation of the current approach

## Confidence
- High Confidence: The classification accuracy improvement (5.14%) over vanilla ViT is well-supported by the reported experiments
- Medium Confidence: The claim that rule-set size is 67% smaller than CNN-based methods requires careful interpretation since comparison methods and datasets differ substantially
- Low Confidence: The semantic labeling results are acknowledged by the authors as unreliable due to ViT polysemanticity, making claims about concept interpretability weak

## Next Checks
1. **Ablation study of loss components**: Train NeSyViT variants with each loss component removed (SupCon only, entropy only, sparsity only) to quantify individual contributions to accuracy and rule compactness.

2. **Sparse dimension sensitivity**: Systematically vary the sparse concept layer dimension (D=64, 128, 256) on Places dataset to identify the minimum dimension that maintains accuracy while minimizing rule-set size.

3. **Cross-dataset hyperparameter transfer**: Train the same model with Places hyperparameters on GTSRB and vice versa to measure sensitivity to domain-specific tuning and identify universal vs. dataset-specific parameters.