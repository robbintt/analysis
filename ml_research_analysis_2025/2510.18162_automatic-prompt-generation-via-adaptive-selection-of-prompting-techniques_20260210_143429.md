---
ver: rpa2
title: Automatic Prompt Generation via Adaptive Selection of Prompting Techniques
arxiv_id: '2510.18162'
source_url: https://arxiv.org/abs/2510.18162
tags:
- task
- tasks
- prompt
- prompting
- temperature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of designing effective prompts
  for large language models (LLMs) without requiring expert knowledge. The authors
  propose an automatic prompt generation method that adapts to user-provided task
  descriptions by leveraging a knowledge base mapping task clusters to effective prompting
  techniques.
---

# Automatic Prompt Generation via Adaptive Selection of Prompting Techniques

## Quick Facts
- arXiv ID: 2510.18162
- Source URL: https://arxiv.org/abs/2510.18162
- Authors: Yohei Ikenoue; Hitomi Tashiro; Shigeru Kuroyanagi
- Reference count: 32
- Primary result: Automatic prompt generation method adapts to user task descriptions via semantic clustering and technique mapping, achieving 28.0 arithmetic mean accuracy on BBEH tasks (vs. 23.9 original, 24.7 Anthropic).

## Executive Summary
The paper presents an automatic prompt generation system that constructs a knowledge base mapping semantically similar tasks to effective prompting techniques. By clustering tasks using semantic embeddings and associating clusters with 3-4 carefully selected prompting techniques through constraint-based LLM selection, the system can generate task-adapted prompts from scratch without requiring example-based optimization. Evaluated on 23 BIG-Bench Extra Hard tasks, the method outperforms standard prompts and Anthropic's Prompt Generator, achieving a 28.0 arithmetic mean accuracy. Temperature optimization further improves results to 28.5 (arithmetic mean) and 13.3 (harmonic mean).

## Method Summary
The approach operates in two phases: first, constructing a knowledge base by clustering tasks (using k-means on embeddings with silhouette-optimized K selection) and mapping each cluster to 3-4 prompting techniques selected via constraint-based LLM prompts; second, generating prompts for new tasks by embedding the user description, finding the most similar cluster via cosine similarity, and having an LLM synthesize a final prompt using the retrieved techniques. The system uses gemini-embedding-exp-03-07 for embeddings, gemini-2.5-pro for LLM operations, and evaluates on 23 BIG-Bench Extra Hard tasks with accuracy metrics.

## Key Results
- Achieved 28.0 arithmetic mean accuracy and 12.5 harmonic mean on BBEH tasks (10 trials per task)
- Outperformed original BBEH prompts (23.9 arithmetic mean, 9.7 harmonic mean) and Anthropic Prompt Generator (24.7 arithmetic mean, 10.5 harmonic mean)
- Temperature optimization further improved results to 28.5 arithmetic mean and 13.3 harmonic mean
- Geometric Shapes showed -16.0 degradation, attributed to mismatch between language-focused reasoning strategies and spatial task requirements

## Why This Works (Mechanism)

### Mechanism 1
Grouping tasks by semantic similarity enables transfer of effective prompting techniques across tasks with shared characteristics. Tasks are embedded into vector space and clustered via k-means with silhouette-optimized K selection; each cluster is then associated with 3-4 prompting techniques through LLM-based assignment with category constraints. Core assumption: Tasks with similar semantic descriptions require similar cognitive strategies, making prompting techniques transferable within clusters. Break condition: If tasks within a cluster have divergent optimal techniques (e.g., visual vs. linguistic reasoning), semantic clustering alone may misallocate techniques.

### Mechanism 2
Cosine similarity between user input embeddings and cluster centroids provides sufficient signal for technique selection without example-based optimization. User task description → embedding → cosine similarity vs. all cluster vectors → highest-similarity cluster → retrieve pre-mapped techniques → LLM generates final prompt. Core assumption: The initial task description captures enough task structure to identify the correct cluster; users provide meaningful descriptions. Break condition: Ambiguous or underspecified task descriptions may match wrong clusters.

### Mechanism 3
Constraint-based technique composition (Role + Emotion + Reasoning + Optional) outperforms unconstrained technique selection. For each cluster, LLM selects: Role Playing (fixed), one emotional stimulus technique (required), one reasoning technique (required), and optionally one complementary technique—creating structured 3-4 technique combinations. Core assumption: Multi-dimensional prompting (persona + affect + reasoning) is broadly beneficial; individual techniques are insufficient alone. Break condition: Some tasks may benefit from fewer techniques (simpler prompts).

## Foundational Learning

- **Embedding models and semantic similarity**
  - Why needed here: The entire matching pipeline depends on understanding how text → vector → cosine similarity enables semantic comparison. Without this, the cluster assignment logic is opaque.
  - Quick check question: Can you explain why cosine similarity ranges from -1 to 1 and why it's preferred over Euclidean distance for high-dimensional text embeddings?

- **Prompting techniques taxonomy**
  - Why needed here: The system selects from 15 techniques across categories (Role Assignment, Emotional Stimulus, Reasoning, Others). Understanding what each technique does is prerequisite to debugging technique-cluster mappings.
  - Quick check question: What is the difference between Chain-of-Thought and Least-to-Most prompting, and what task characteristics would favor one over the other?

- **Cluster hypothesis in transfer learning**
  - Why needed here: The approach assumes that semantically similar tasks form "natural categories" with shared optimal strategies. This is a hypothesis, not a theorem.
  - Quick check question: What evidence would contradict the cluster hypothesis for this application? (Hint: see Geometric Shapes results.)

## Architecture Onboarding

- **Component map**: Task corpus → embedding → k-means clustering → LLM cluster description → cluster embedding → technique mapping (constraint-based) → User input → embedding → cosine similarity matching → technique retrieval → LLM prompt synthesis → target LLM → output

- **Critical path**: Cluster quality directly determines technique allocation—garbage clusters → garbage techniques; Embedding model choice affects semantic similarity calibration—different models produce different cluster structures; Technique-to-cluster mapping LLM prompt constrains solution space—poor constraint design yields incoherent technique combinations

- **Design tradeoffs**: Fixed cluster count vs. dynamic: Current approach uses pre-computed clusters; new task types require KB reconstruction; Constraint-based selection vs. free-form: Constraints ensure coverage but may miss optimal technique combinations for edge cases; Single-cluster matching vs. ensemble: Using only highest-similarity cluster is simple but ignores boundary cases where multiple clusters are relevant

- **Failure signatures**: Low-performing tasks with negative improvement (Geometric Shapes: -16.0, Shuffled Objects: -8.2) suggest technique-task mismatch—likely cluster assignment error or inappropriate technique category for visual/spatial reasoning; High format non-compliance rates (32.5-36.2% for some tasks at optimized temperature) indicate prompts may encourage over-reasoning that exceeds output length limits

- **First 3 experiments**: 
  1. Cluster validity check: Visualize cluster assignments for BBEH tasks using t-SNE/UMAP; verify that tasks in same cluster share cognitive characteristics. Flag outlier tasks within clusters.
  2. Technique ablation per cluster: For 2-3 representative clusters, generate prompts with subsets of techniques (e.g., remove emotional stimulus) to identify which technique categories drive gains. Compare against full combination.
  3. Description sensitivity test: Provide abbreviated vs. detailed task descriptions for same tasks; measure cluster assignment stability and final prompt quality. Quantify minimum description requirements.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the task-to-technique knowledge base constructed for BIG-Bench Extra Hard (BBEH) be effectively applied to distinct domains such as financial analysis or manufacturing defect detection without requiring extensive re-clustering? Basis: The conclusion states it "remains an open question whether this knowledge base can be directly applied to other domains," specifically citing financial and manufacturing applications. Evidence needed: A study measuring performance of the current, unmodified knowledge base on domain-specific benchmarks compared to a knowledge base constructed specifically for those domains.

- **Open Question 2**: Can a feedback loop be integrated to dynamically update the knowledge base in real-time based on user corrections and the emergence of new prompting techniques? Basis: The authors identify the need for "incorporating mechanisms to dynamically update and optimize knowledge bases based on user feedback and emerging prompting techniques" as a necessary extension. Evidence needed: Implementation of an adaptive system that ingests execution logs to re-weight technique associations, demonstrating statistically significant performance improvements over time compared to the static model.

- **Open Question 3**: Is it feasible to integrate a predictive evaluation mechanism that allows the system to estimate prompt effectiveness prior to execution? Basis: The conclusion suggests future work should include "integrating functionality that allows LLMs to predict and evaluate prompt effectiveness in advance." Evidence needed: Correlation analysis showing that LLM-generated predictive scores for candidate prompts strongly align with the actual accuracy achieved on the BBEH tasks.

## Limitations

- The knowledge base construction process is opaque, with the full task corpus used for initial clustering undisclosed beyond the 23 evaluation tasks, making generalization assessment difficult.
- The constraint-based 3-4 technique selection lacks ablation studies to determine optimal combinations per cluster type, potentially missing better configurations.
- Negative improvements on specific tasks (Geometric Shapes: -16.0, Shuffled Objects: -8.2) reveal semantic clustering limitations for visual/spatial reasoning tasks.

## Confidence

- **High Confidence**: The arithmetic and harmonic mean accuracy improvements over both baseline prompts and Anthropic's generator are well-supported by Table 2 results. The two-phase architecture (knowledge base construction → prompt generation) is clearly specified and reproducible.
- **Medium Confidence**: The claim that semantic clustering enables effective transfer of prompting techniques across tasks is supported by overall performance gains but contradicted by specific task failures. The mechanism works when tasks are linguistically similar but breaks down for visual/spatial reasoning.
- **Low Confidence**: The assertion that the 3-4 technique constraint structure is optimal lacks ablation studies. The paper does not demonstrate whether fewer techniques or different combinations would perform better for specific task clusters.

## Next Checks

1. **Cluster validity verification**: Visualize the k-means cluster assignments for the 23 BBEH tasks using dimensionality reduction (t-SNE/UMAP) to confirm that tasks within clusters share meaningful cognitive characteristics, and identify any outlier tasks that may explain negative performance results.

2. **Technique contribution ablation**: For 2-3 representative clusters showing strong performance, systematically generate prompts with subsets of techniques (e.g., remove emotional stimulus, remove reasoning techniques) to identify which technique categories are essential versus optional for different task types.

3. **Description sensitivity analysis**: Evaluate the system's robustness to task description quality by providing both detailed and abbreviated descriptions of the same tasks, measuring cluster assignment stability and final prompt quality to determine minimum description requirements for reliable matching.