---
ver: rpa2
title: 'Learning to Decide with Just Enough: Information-Theoretic Context Summarization
  for CMDPs'
arxiv_id: '2510.01620'
source_url: https://arxiv.org/abs/2510.01620
tags:
- context
- learning
- latency
- regret
- contextual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of context modeling in Contextual
  Markov Decision Processes (CMDPs), where high-dimensional or unstructured context
  can hinder learning efficiency and scalability. The authors propose an information-theoretic
  framework that uses large language models (LLMs) to compress raw context into compact,
  semantically meaningful summaries.
---

# Learning to Decide with Just Enough: Information-Theoretic Context Summarization for CMDPs

## Quick Facts
- **arXiv ID**: 2510.01620
- **Source URL**: https://arxiv.org/abs/2510.01620
- **Reference count**: 13
- **Primary result**: LLM-driven context summarization achieves near-optimal performance with lower computational cost across six CMDP benchmarks.

## Executive Summary
This paper addresses the challenge of context modeling in Contextual Markov Decision Processes (CMDPs), where high-dimensional or unstructured context can hinder learning efficiency and scalability. The authors propose an information-theoretic framework that uses large language models (LLMs) to compress raw context into compact, semantically meaningful summaries. These summaries augment states by preserving decision-critical information while reducing redundancy.

The method is grounded in the concept of approximate context sufficiency, with theoretical regret bounds and a latency-entropy trade-off characterization. Empirically, the approach consistently outperforms both non-contextual and raw-context baselines across six benchmarks spanning navigation, continuous control, visual RL, drug discovery, and recommendation tasks. It improves reward, success rate, and sample efficiency while reducing latency and memory usage. The framework also demonstrates strong transferability and robustness under noisy or corrupted contexts, positioning LLM-driven summarization as a scalable, interpretable solution for efficient context-aware decision-making.

## Method Summary
The framework trains an LLM summarizer g_ψ to map history H_t and external context E_t into a compressed summary C_t (default 64 tokens). This summary augments states S_t for policy π_θ. The training objective jointly maximizes mutual information I(S;C) between states and context, minimizes summary entropy H(C_t) for efficiency, and includes regularizers for sufficiency (ensuring π*(·|S_t,C_t) ≈ π*(·|S_t,H_t,E_t)), token budget adherence, and robustness. Theoretical regret bounds show performance degrades gracefully with entropy. Empirically, the method achieves 7.5× better marginal efficiency moving from 32→64 tokens versus 64→128, and scales superlinearly with entropy (α≈1.12-1.34), validating the latency-entropy coupling.

## Key Results
- **Performance gains**: Outperforms No Context and Raw Context baselines on all six benchmarks (FrozenLake, Drug Discovery, Contextual Bandit, Atari, MuJoCo, MovieLens).
- **Efficiency improvements**: 7.5× better marginal efficiency from 32→64 tokens vs 64→128 tokens; decision latency scales superlinearly with entropy (α∈[1.12, 1.34]).
- **Transfer capability**: Zero-shot transfer works across domains (Drug→MovieLens: +11% over zero-shot); semantic overlap sufficient for some transfers but insufficient for others (MovieLens→Drug fails).

## Why This Works (Mechanism)

### Mechanism 1: Information-Theoretic Context Sufficiency
The framework optimizes a bi-criteria objective maximizing mutual information I(S;C) between states and context while minimizing context entropy H(C_t). This formalizes "context sufficiency"—summaries need only preserve information relevant to action selection, not all contextual details. A KL-divergence constraint ensures π*(·|S_t, H_t, E_t) ≈ π*(·|S_t, C_t) within tolerance ε. The regret bound shows performance degrades gracefully with entropy: Regret(T) ≤ c₁√(T·d_eff) + c₂√(T·H(C_t)^(α/2)).

### Mechanism 2: Latency–Entropy Coupling with Budgeted Updates
Decision latency scales superlinearly with context entropy, so entropy regularization directly reduces inference cost. Latency is modeled as Latency(C_t) = β₀ + β₁·H(C_t)^α where α ∈ [1, 1.5]. Empirically, α̂ ∈ [1.12, 1.34] on high-entropy tasks (Drug/MovieLens). Update policies (per-step, sliding-window, periodic) operationalize this trade-off by controlling when summaries are recomputed. The Lagrangian formulation enforces token and latency budgets.

### Mechanism 3: LLM Semantic Compression Preserves Transfer Structure
LLM-generated summaries capture domain-invariant semantic structure, enabling cross-task transfer with minimal fine-tuning. Transfer experiments show Drug→MovieLens zero-shot NDCG=0.082 → FT=0.091 (+11%), suggesting shared semantic patterns. Larger models exploit additional tokens more effectively (β₁₂ > 0 in cross-factor analysis).

## Foundational Learning

- **Concept: Contextual Markov Decision Processes (CMDPs)**
  - Why needed: The entire framework builds on CMDPs—understanding that context c parameterizes both transitions P(s'|s,a,c) and rewards R(s,a,c) is essential for grasping why summary quality matters.
  - Quick check: If context affects only rewards but not transitions, would approximate sufficiency still require maximizing I(S;C)? (Hint: Yes, but the bound would differ—focus shifts to reward prediction.)

- **Concept: Mutual Information and Variational Bounds (MINE/InfoNCE)**
  - Why needed: The objective relies on I(S;C), which is intractable. Understanding that Eq. 8-9 provide tractable lower bounds via critics f_φ is necessary to implement training.
  - Quick check: Why does InfoNCE add log(K+1) to the bound? (Hint: Correction for batch negative sampling bias.)

- **Concept: Regret Analysis in RL**
  - Why needed: Proposition 1 connects entropy to regret—understanding that Regret(T) = Σ[V^π*(s_t) - V^π(s_t)] measures cumulative suboptimality helps interpret the theoretical claims.
  - Quick check: If H(C_t) doubles, what happens to the regret bound's second term (c₂√(T·H(C_t)^(α/2))) when α=1.2? (Hint: Increases by factor of 2^(0.6) ≈ 1.52.)

## Architecture Onboarding

- **Component map**:
  - Observe (S_t, H_t, E_t) → LLM summarizer g_ψ → C_t (64 tokens) → Policy π_θ → Action A_t → Execute → Observe (R_t, S_{t+1}) → Update H_{t+1} → Meta-policy π_φ → Update decision U_t → Refresh C_{t+1} if needed → Update ψ via L_total, update θ via policy gradient

- **Critical path**:
  1. Observe (S_t, H_t, E_t)
  2. Compute C_t ← g_ψ(H_t, E_t) [pre-action, respects token budget]
  3. Sample A_t ∼ π_θ(·|S_t, C_t), execute, observe (R_t, S_{t+1})
  4. Update H_{t+1} with new transition
  5. Sample U_t ∼ π_φ(·|ξ_t), refresh C_{t+1} if needed
  6. Update ψ via L_total (Eq. 21), update θ via policy gradient

- **Design tradeoffs**:
  - **Token budget**: 32→64 tokens gives 7.5× better marginal efficiency than 64→128 (Table 4). Start at 64.
  - **Update frequency**: Sliding window (W=16) is near Pareto-optimal; per-step for quality-critical, periodic for latency-critical.
  - **Summarizer capacity**: Distilled-LM wins on reward-per-ms; GPT-3.5 wins on absolute performance. Use LLaMA-2-13B as balanced default.

- **Failure signatures**:
  - **High ε̂ (discriminator)**: Summaries losing decision information—increase token budget or add sufficiency regularizer η₅.
  - **Latency blowup despite low entropy**: Check if α ≈ 1 (bottleneck shifted) or if summary age is too high (increase update frequency).
  - **Transfer failure (ZS < 70% FT)**: Semantic overlap insufficient—add domain-specific pretraining or hybrid summarization.
  - **Instability under noise**: Check stability metric (Eq. 17)—if >0.3, summaries may be including redundant/noisy features.

- **First 3 experiments**:
  1. **Baseline sanity check**: Run No Context vs. Raw Context vs. Summarized (64 tokens, sliding window) on FrozenLake. Verify summarized achieves ~90% success rate per Table 2.
  2. **Token budget ablation**: Fix summarizer (GPT-3.5), vary tokens {32, 64, 128} on Drug Discovery. Confirm 64 is near knee (E_{32→64} ≈ 0.06, E_{64→128} ≈ 0.019).
  3. **Latency-entropy validation**: Fit Latency = β₀ + β₁·H(C_t)^α on Drug/MovieLens data. Verify α̂ ∈ [1.1, 1.4] matches paper claims.

## Open Questions the Paper Calls Out
- **Can adaptive token allocation policies dynamically adjust budgets based on task complexity to optimize the latency-entropy trade-off?**
  - Basis: Conclusion states future work will "explore adaptive token allocation policies that dynamically adjust budgets based on task complexity."
  - Why unresolved: Current experiments use fixed token budgets ({32, 64, 128}) and static update policies, requiring manual tuning.
  - What evidence would resolve it: Experiments demonstrating dynamic budgeting mechanisms outperforming fixed configurations in heterogeneous or non-stationary environments.

- **How does the summarization-based framework extend to cooperative and competitive multi-agent settings?**
  - Basis: Conclusion lists extending "the framework to cooperative and competitive multi-agent environments" as a future direction.
  - Why unresolved: The study evaluates single-agent benchmarks; multi-agent contexts introduce decentralized communication and coordination challenges not addressed by the current architecture.
  - What evidence would resolve it: Application of the method to multi-agent CMDPs, analyzing how shared or distinct summaries affect team performance and equilibrium finding.

- **Can hybrid summarization strategies improve performance on highly multi-modal tasks under strict latency constraints?**
  - Basis: Conclusion proposes "integration with other context-compression techniques to handle multi-modal, large-scale decision-making."
  - Why unresolved: The authors note in limitations that "highly multi-modal tasks... may require additional tuning or hybrid summarization strategies" beyond the current LLM-centric approach.
  - What evidence would resolve it: Comparisons showing that combining LLM summarizers with other compression methods (e.g., visual encoders) improves efficiency in complex multi-modal domains.

## Limitations
- **Generalizability across LLM architectures**: Transfer performance degrades significantly in semantically sparse domains (e.g., MovieLens→Drug: ZS=245 vs. FT=265) without quantified semantic overlap metrics.
- **Temporal dependency handling**: History buffer H_t is treated as flat sequence without explicit temporal modeling, potentially missing long-range dependencies in domains with complex state-action histories.
- **Scalability to real-time systems**: Per-step LLM inference cost may still be prohibitive for safety-critical real-time applications despite latency improvements, with no analysis of worst-case timing constraints.

## Confidence
- **High Confidence**: Information-theoretic framework (regret bounds, mutual information maximization, entropy regularization) - well-established mathematical foundations with supported proofs.
- **Medium Confidence**: Empirical performance claims across six benchmarks - consistently positive but varying absolute performance with unspecified comparison methodology details.
- **Medium Confidence**: Transfer learning results - shows promise (Drug→MovieLens: +11% FT over ZS) but lacks quantified semantic overlap and deep analysis of failure cases.

## Next Checks
1. **Semantic overlap quantification**: Implement a metric measuring semantic similarity between source and target context distributions to predict transfer success before attempts, validating against paper results.

2. **Temporal structure ablation**: Modify framework to include explicit temporal modeling (e.g., transformer-based history encoding) and compare against flat history buffer on domains with known temporal dependencies.

3. **Real-time latency stress test**: Implement framework on simulated real-time environment with strict timing constraints, measuring latency budget exceedances under different update policies and identifying practical deployment breaking points.