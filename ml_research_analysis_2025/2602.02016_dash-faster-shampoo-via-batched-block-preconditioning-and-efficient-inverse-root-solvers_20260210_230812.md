---
ver: rpa2
title: 'DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root
  Solvers'
arxiv_id: '2602.02016'
source_url: https://arxiv.org/abs/2602.02016
tags:
- shampoo
- matrix
- block
- which
- distributed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DASH, a faster implementation of the Distributed
  Shampoo optimizer for large-scale neural network training. The authors address the
  computational bottleneck in Shampoo, which involves expensive inverse matrix root
  calculations.
---

# DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers

## Quick Facts
- arXiv ID: 2602.02016
- Source URL: https://arxiv.org/abs/2602.02016
- Authors: Ionut-Vlad Modoranu; Philip Zmushko; Erik Schultheis; Mher Safaryan; Dan Alistarh
- Reference count: 40
- Primary result: Achieves up to 4.83× faster optimizer steps compared to Distributed Shampoo while maintaining or improving optimization quality

## Executive Summary
DASH introduces a faster implementation of the Distributed Shampoo optimizer for large-scale neural network training by addressing the computational bottleneck of inverse matrix root calculations. The paper presents two main innovations: batched block preconditioning that leverages 3D tensor operations for parallel GPU processing, and efficient inverse matrix root computation using Newton-DB iteration and Chebyshev polynomial approximations. The GPU-aware implementation significantly reduces the computational overhead of Shampoo while maintaining or improving optimization quality, making it more practical for real-world deep learning applications.

## Method Summary
The DASH method improves upon Distributed Shampoo by optimizing two key computational bottlenecks. First, it introduces a GPU-aware implementation that stacks preconditioner blocks into 3D tensors, enabling parallel processing of multiple block inverses through batched matrix operations. Second, it develops faster inverse matrix root computation methods, including a Newton-DB iteration approach and Chebyshev polynomial approximations, which significantly reduce the number of iterations needed for accurate matrix inversion. The paper also introduces multi-Power-Iteration for robust spectral radius estimation and demonstrates how proper matrix scaling is critical for convergence. These optimizations together achieve substantial speed improvements while maintaining or enhancing optimization performance.

## Key Results
- Achieves up to 4.83× faster optimizer steps compared to Distributed Shampoo
- Newton-DB iteration method attains the lowest validation perplexity per iteration among tested approaches
- Multi-Power-Iteration provides robust spectral radius estimation for improved convergence
- GPU implementation shows variable performance gains across different hardware configurations

## Why This Works (Mechanism)
The effectiveness of DASH stems from two fundamental improvements to the Shampoo optimizer's computational bottlenecks. The batched block preconditioning approach transforms independent 2D matrix operations into a single 3D tensor operation that can be processed in parallel on GPUs, exploiting the inherent parallelism of modern hardware. The Newton-DB iteration and Chebyshev polynomial methods reduce the computational complexity of inverse matrix root calculations by either accelerating convergence through better initial estimates or approximating the solution more efficiently. The multi-Power-Iteration technique ensures robust estimation of the spectral radius, which is critical for the convergence of matrix inverse root computations. Together, these innovations maintain the theoretical benefits of Shampoo's preconditioning while making it computationally tractable for large-scale training.

## Foundational Learning

**Block-wise preconditioning**
- Why needed: Shampoo's block-diagonal approach reduces memory and computation by approximating the full curvature matrix
- Quick check: Verify block sizes are chosen to balance computational efficiency with approximation quality

**Matrix inverse roots**
- Why needed: Shampoo requires computing matrix square roots for effective preconditioning
- Quick check: Confirm that inverse root computations maintain numerical stability across different matrix scales

**Spectral radius estimation**
- Why needed: Critical for determining convergence criteria and step sizes in iterative matrix algorithms
- Quick check: Validate that spectral radius estimates remain consistent across different initialization schemes

**Chebyshev polynomial approximation**
- Why needed: Provides efficient polynomial approximations for functions like matrix inverse roots
- Quick check: Ensure polynomial degree balances approximation accuracy with computational cost

**Power iteration method**
- Why needed: Standard technique for estimating dominant eigenvalues used in spectral radius calculation
- Quick check: Verify convergence rates across matrices with different eigenvalue distributions

## Architecture Onboarding

**Component map**
- Parameter gradients -> Block decomposition -> Matrix inverse root computation -> Preconditioned updates -> Parameter update

**Critical path**
1. Gradient computation and block decomposition
2. Matrix inverse root computation (bottleneck)
3. Preconditioned gradient calculation
4. Parameter update with learning rate scaling

**Design tradeoffs**
- Block size vs. approximation quality: Larger blocks provide better curvature approximation but increase computation
- Chebyshev polynomial degree vs. accuracy: Higher degree improves approximation but increases computation
- Number of Power iterations vs. spectral radius accuracy: More iterations improve accuracy but increase computation

**Failure signatures**
- Slow convergence indicates poor spectral radius estimation or inadequate matrix scaling
- Numerical instability suggests issues with matrix conditioning or floating-point precision
- Suboptimal performance may result from suboptimal block sizes or polynomial degrees

**First experiments**
1. Compare convergence rates with different block sizes on a simple MLP to find optimal tradeoff
2. Benchmark inverse matrix root computation time across different polynomial degrees
3. Test spectral radius estimation accuracy across matrices with varying eigenvalue distributions

## Open Questions the Paper Calls Out

None specified in the provided information.

## Limitations

- GPU implementation speedups are hardware-specific and may not generalize uniformly across different architectures
- Theoretical analysis of Newton-DB iteration lacks comprehensive convergence proofs, particularly for stochastic gradient settings
- The paper lacks systematic analysis of hyperparameter sensitivity, especially regarding Chebyshev polynomial degree and block size choices

## Confidence

**High**: Performance gains from GPU-aware batched implementation (validated through direct benchmarking)
**Medium**: Effectiveness of Newton-DB iteration for inverse matrix root computation (empirical evidence but limited theoretical backing)
**Medium**: Importance of matrix scaling for convergence (demonstrated but not fully explained theoretically)

## Next Checks

1. Test the GPU implementation across multiple GPU architectures and batch sizes to verify the claimed speedup consistency
2. Conduct ablation studies on the impact of Chebyshev polynomial degree and block size on both speed and convergence
3. Compare convergence behavior against other adaptive optimizers (Lion, AdamW, K-FAC) across multiple tasks and architectures to establish relative effectiveness