---
ver: rpa2
title: 'Socratic-Zero : Bootstrapping Reasoning via Data-Free Agent Co-evolution'
arxiv_id: '2509.24726'
source_url: https://arxiv.org/abs/2509.24726
tags:
- solver
- problems
- problem
- curriculum
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Socratic-Zero introduces a fully autonomous framework for bootstrapping
  mathematical reasoning from minimal seed data through the co-evolution of three
  agents: a Solver that improves via preference learning, a Teacher that generates
  challenging problems based on Solver weaknesses, and a Generator that distills the
  Teacher''s strategy for scalable curriculum generation. The system requires no pre-existing
  tasks or labels and operates as a closed-loop self-improving curriculum.'
---

# Socratic-Zero : Bootstrapping Reasoning via Data-Free Agent Co-evolution

## Quick Facts
- arXiv ID: 2509.24726
- Source URL: https://arxiv.org/abs/2509.24726
- Authors: Shaobo Wang; Zhengbo Jiao; Zifan Zhang; Yilang Peng; Xu Ze; Boyu Yang; Wei Wang; Hu Wei; Linfeng Zhang
- Reference count: 40
- Primary result: Socratic-Zero achieves +20.2 percentage points gain over prior data synthesis methods across seven mathematical reasoning benchmarks using only 100 seed questions.

## Executive Summary
Socratic-Zero introduces a fully autonomous framework for bootstrapping mathematical reasoning from minimal seed data through the co-evolution of three agents: a Solver that improves via preference learning, a Teacher that generates challenging problems based on Solver weaknesses, and a Generator that distills the Teacher's strategy for scalable curriculum generation. The system requires no pre-existing tasks or labels and operates as a closed-loop self-improving curriculum. Starting from only 100 seed questions, Socratic-Solver-8B achieves an average gain of +20.2 percentage points over prior data synthesis methods across seven mathematical reasoning benchmarks, while synthetic data from Socratic-Generator-32B enables student LLMs to achieve superior performance compared to state-of-the-art commercial models including GPT-5, Gemini-2.5-Pro, and Claude-4.1-Opus.

## Method Summary
Socratic-Zero employs a three-agent co-evolutionary framework where a Solver (8B-14B) learns to solve problems via Direct Preference Optimization (DPO), a frozen Teacher (235B) verifies solutions and generates refined problems targeting Solver failures, and a Generator (32B) distills the Teacher's problem-creation strategy via weighted supervised fine-tuning. The system bootstraps from 100 curated seed questions, with the Teacher analyzing incorrect Solver trajectories to identify error patterns and generate targeted problems. The Generator learns to produce problems at approximately 50% Solver success rate, maximizing learning efficiency. The entire process operates without human-labeled data beyond the initial seed set, creating a self-improving curriculum through iterative cycles of problem generation, solution verification, and model refinement.

## Key Results
- Socratic-Solver-8B achieves an average gain of +20.2 percentage points over prior data synthesis methods across seven mathematical reasoning benchmarks
- Socratic-Generator-32B synthetic data enables student LLMs to outperform state-of-the-art commercial models (GPT-5, Gemini-2.5-Pro, Claude-4.1-Opus)
- The system successfully bootstraps reasoning capabilities from only 100 seed questions without requiring pre-existing labeled datasets
- Ablation studies show initial SFT is critical (w/o SFT: 11.98% vs w/ SFT: 28.02% on AIME-24) and Gaussian μ=0.5 utility weighting outperforms alternatives by 0.20-0.40 points

## Why This Works (Mechanism)

### Mechanism 1: Failure-Driven Curriculum Expansion
Targeting problems at Solver failure points produces higher training signal density than random or static augmentation. Teacher analyzes incorrect trajectories → identifies error patterns → generates problems that exercise the same concept with modified constraints → Solver retrains on targeted weaknesses. Core assumption: Failures reveal concept boundaries better than successes. Evidence: [abstract] "Teacher adaptively crafts increasingly challenging questions based on the Solver's weaknesses" and [Section 3.1] defines failure set Ft = {(q, yS) | V(q, yS) = 0}. Break condition: Curriculum stalls if failure patterns become repetitive or generated problems are unsolvable.

### Mechanism 2: Preference-Grounded Solver Improvement
Learning from paired correct/incorrect solutions via DPO provides stable gradient signal without an explicit reward model. Solver samples k solutions → Teacher partitions into Yw (correct) and Yl (incorrect) → DPO loss maximizes likelihood of winners over losers. Core assumption: Reference solutions can substitute when Solver produces zero correct attempts. Evidence: [abstract] "Solver continuously refines its reasoning by learning from preference feedback on both successful and failed trajectories" and [Section 3.2] provides LDPO formulation. Break condition: Preference pairs become noisy if reference solutions are flawed or verification V produces systematic errors.

### Mechanism 3: Value-Weighted Generator Distillation
Training Generator to produce problems at ~50% Solver success rate maximizes learning efficiency. Utility U(q'|πS) = exp(-(s_q' - μ)²/2σ²) with μ=0.5 → Generator trained via weighted SFT on Teacher's outputs. Core assumption: Problems at the capability frontier are more informative than mastered or impossible problems. Evidence: [abstract] "Generator distills the Teacher's question-design strategy to enable scalable, high-fidelity curriculum generation" and [Section 3.3] Equation 7 shows LWSFT weights log-likelihood by utility. Break condition: Utility estimates become stale if Solver capability shifts faster than Generator adapts.

## Foundational Learning

- **Direct Preference Optimization (DPO)**: Replaces reward-model training with direct policy comparison; simplifies Solver training loop. Quick check: Can you explain why DPO avoids training a separate reward model compared to RLHF?

- **Curriculum Learning (Zone of Proximal Development)**: Justifies μ=0.5 target; problems too easy or too hard provide weak gradients. Quick check: What happens to learning signal if all generated problems have 0% or 100% success rate?

- **Knowledge Distillation via Supervised Fine-Tuning**: Generator learns Teacher's strategy offline without requiring Teacher inference at deployment. Quick check: Why use weighted SFT instead of standard SFT for Generator training?

## Architecture Onboarding

- **Component map**: Seed problems → Solver attempts → Teacher verifies → Preference pairs → DPO update → Failures → Teacher generates enhanced problems → Generator trains via WSFT → New problems added to curriculum → repeat

- **Critical path**: 1) Seed curriculum (100 problems) → Solver attempts → Teacher verifies → Preference pairs → DPO update 2) Failures → Teacher generates enhanced problems → Generator trains on (q, y_fail, q') weighted by utility 3) New problems added to curriculum → repeat

- **Design tradeoffs**: DPO β (0.05-0.2): Higher values regularize more but may slow convergence; k=8 samples per problem: More samples improve preference pair quality at compute cost; Historical replay ratio (25%): Prevents catastrophic forgetting but dilutes new signal; Teacher frozen vs. co-evolving: Frozen Teacher provides stable supervision but cannot adapt to Solver distribution shift

- **Failure signatures**: Curriculum collapse: All problems in D_difficult (success rate = 0) → no new failures to learn from; Verification drift: Teacher V produces inconsistent judgments → noisy preference pairs; Generator overfitting: Produces near-identical problems despite varied inputs; Oscillatory convergence (Table 9): Solver reward drops then recovers—indicates curriculum difficulty outpaces capability temporarily

- **First 3 experiments**: 1) **SFT necessity check**: Train Solver with and without initial SFT on 1,500 problems. Report AIME-24 score. Expect ~7.9× improvement gap per Table 6a. 2) **Reward function ablation**: Compare Gaussian μ=0.5 vs. linear Ψ_ρ functions. Expect 0.2-0.4 point degradation per Table 6b. 3) **Curriculum initialization sensitivity**: Vary seed count (50, 100, 200) and difficulty distribution. Measure convergence speed and final accuracy to validate minimal-seed claims.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the co-evolutionary framework provide formal guarantees of convergence?
- Basis in paper: [explicit] The conclusion states that "complex agent dynamics currently lack a formal convergence analysis" and identifies this as a direction for future work.
- Why unresolved: Empirical results show oscillatory reward patterns (e.g., Solver reward dropping then recovering in Appendix M), suggesting the system reaches dynamic equilibria rather than static optima, making stability unpredictable.
- What evidence would resolve it: Theoretical proofs establishing stability criteria for curriculum evolution rates or empirical validation showing bounded oscillations over significantly longer training horizons.

### Open Question 2
- Question: Can the framework effectively generalize to domains with non-binary evaluation metrics?
- Basis in paper: [explicit] The conclusion proposes extending applicability to "scientific discovery, real-world decision making, and complex system modeling."
- Why unresolved: The current architecture relies on a binary verification function $V(q, y)$ to generate preference pairs and calculate utility. Domains like creative writing or open-ended scientific hypothesis lack ground-truth "correctness."
- What evidence would resolve it: Successful implementation of Socratic-Zero on non-mathematical tasks using soft verifiers or learned reward models in place of rule-based checking.

### Open Question 3
- Question: Is the strong performance dependent on the specific Supervised Fine-Tuning (SFT) initialization?
- Basis in paper: [inferred] Ablation Table 6a reveals that removing the initial SFT phase causes performance to collapse from 28.02% to 11.98% on AIME-24.
- Why unresolved: While the paper emphasizes bootstrapping from "minimal seed" questions, the ablation suggests the SFT phase provides essential "foundational capabilities" (reasoning patterns) that the co-evolutionary loop cannot easily build from scratch.
- What evidence would resolve it: Demonstration of effective co-evolution starting from a raw base model without the Level-5 SFT warm-start.

## Limitations

- Seed quality dependency: The system's performance critically depends on the initial 100 seed problems being both solvable and sufficiently diverse, with no quantitative validation of seed quality or sensitivity analysis provided.

- Teacher reliability assumptions: The frozen 235B Teacher serves as both verifier and problem generator, but no systematic evaluation of Teacher consistency or error rates is provided, potentially compounding verification errors through the co-evolution loop.

- Generalization boundaries: While the system shows strong gains on mathematical reasoning benchmarks, the approach is demonstrated only in a single domain, and mechanisms may not transfer to domains where ground truth verification is less straightforward.

## Confidence

**High Confidence**: The core architectural claims about co-evolution between Solver, Teacher, and Generator are well-supported by the described methodology and ablation studies. The reported +20.2 percentage point improvement over data synthesis baselines is concrete and specific.

**Medium Confidence**: The claims about achieving superior performance to commercial models (GPT-5, Gemini-2.5-Pro, Claude-4.1-Opus) using synthetic data from Socratic-Generator-32B are based on student LLM experiments but lack detailed methodology for how these comparisons were conducted.

**Low Confidence**: The assertion that the system requires "no pre-existing tasks or labels" is technically accurate but potentially misleading—the system requires 100 carefully curated seed problems and 1,500 labeled problems for initial SFT, representing non-trivial human effort.

## Next Checks

1. **Teacher verification reliability audit**: Run 1,000 randomly selected problems through the Teacher verification system twice, measuring inter-rater consistency and identifying systematic error patterns to quantify the risk of verification drift compounding through the co-evolution loop.

2. **Seed sensitivity analysis**: Re-run the full pipeline with systematically varied seed sets (50, 100, 200 problems; varying difficulty distributions) to measure the impact of seed quality on final performance and convergence speed, validating the claimed minimal-seed advantage.

3. **Cross-domain transfer test**: Apply the Socratic-Zero framework to a non-mathematical reasoning task (e.g., commonsense reasoning or code generation) using domain-specific seed problems to measure whether the same co-evolution mechanisms produce comparable bootstrapping gains or reveal domain-specific limitations.