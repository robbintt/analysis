---
ver: rpa2
title: Fine-Tuning Qwen 2.5 3B for Realistic Movie Dialogue Generation
arxiv_id: '2502.16274'
source_url: https://arxiv.org/abs/2502.16274
tags:
- response
- movie
- dialogue
- score
- line
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study fine-tuned the Qwen 2.5 3B model on the Cornell Movie-Dialog
  Corpus to generate realistic and engaging movie dialogue. Due to GPU and VRAM constraints,
  the training process began with a 0.5B model and progressively scaled up to 1.5B
  and 3B versions using optimizations like quantization, QLoRA, Flash Attention, gradient
  accumulation, and NEFTune.
---

# Fine-Tuning Qwen 2.5 3B for Realistic Movie Dialogue Generation

## Quick Facts
- arXiv ID: 2502.16274
- Source URL: https://arxiv.org/abs/2502.16274
- Authors: Kartik Gupta
- Reference count: 40
- Primary result: Fine-tuned Qwen 2.5 3B with DPO achieved higher G-Eval scores and reduced low-quality responses vs. base and fine-tuned-only models

## Executive Summary
This study fine-tuned the Qwen 2.5 3B model on the Cornell Movie-Dialog Corpus to generate realistic and engaging movie dialogue. Due to GPU and VRAM constraints, the training process began with a 0.5B model and progressively scaled up to 1.5B and 3B versions using optimizations like quantization, QLoRA, Flash Attention, gradient accumulation, and NEFTune. Direct Preference Optimization (DPO) was applied using GPT-4o-generated preference data. Evaluation via G-Eval (coherence, consistency, fluency, relevance) and human preference testing showed consistent improvements across all metrics: the fine-tuned model with DPO achieved higher scores and reduced low-quality responses compared to the base and fine-tuned-only models.

## Method Summary
The research employed a memory-efficient fine-tuning pipeline using QLoRA with 4-bit quantization and LoRA adapters to enable training of progressively larger Qwen 2.5 models (0.5B → 1.5B → 3B) within GPU constraints. NEFTune was applied for regularization through noise injection. DPO was used to fine-tune the model using preference pairs generated by GPT-4o. The Cornell Movie-Dialog Corpus was preprocessed into sliding-window prompt-response pairs with a 512-token limit. Evaluation combined automated G-Eval scoring and human preference testing, with the final model deployed via FastAPI backend and Vue frontend with a 64-token generation limit.

## Key Results
- Fine-tuned Qwen 2.5 3B with DPO achieved higher G-Eval scores across coherence, consistency, fluency, and relevance compared to base and fine-tuned-only models
- DPO specifically reduced the frequency of low-quality responses while maintaining similar peak performance levels
- Progressive model scaling from 0.5B to 3B showed consistent improvements in automated metrics and human preference rates
- Human preference testing showed the DPO model outperformed base model (11% preference) with reduced low-quality response frequency

## Why This Works (Mechanism)

### Mechanism 1: QLoRA for Memory-Constrained Fine-Tuning
Quantized Low-Rank Adaptation enables fine-tuning of models that would otherwise exceed available VRAM by freezing base weights and training only low-rank adapter matrices. 4-bit quantization compresses the 3B parameter base model; LoRA adds small rank-decomposition matrices to attention layers that are updated during training while frozen base weights remain unchanged. Core assumption: Quality gains from larger model scale outweigh any degradation introduced by 4-bit quantization. Evidence: Abstract mentions progressive scaling with quantization and QLoRA; section explains LoRA updates matrices rather than model weights. Break condition: If creative dialogue generation is highly sensitive to quantization artifacts, quality may degrade despite larger model capacity.

### Mechanism 2: DPO Reduces Low-Quality Response Tail
Direct Preference Optimization specifically reduces the frequency of poor-quality responses rather than substantially improving peak performance. DPO trains directly on preference pairs (chosen vs. rejected responses) without an intermediate reward model, implicitly learning to avoid responses that a stronger model (GPT-4o) would reject. Core assumption: GPT-4o's preference judgments align with human preferences for realistic movie dialogue. Evidence: Abstract states DPO achieved higher scores and reduced low-quality responses; section notes DPO greatly reduced low-quality answers while only slightly improving high-quality ones. Break condition: If GPT-4o preferences systematically diverge from human dialogue preferences, alignment will be misdirected.

### Mechanism 3: NEFTune as Regularization for Creative Generalization
Adding Gaussian noise to input embeddings during fine-tuning regularizes the model against overfitting to the Cornell corpus's specific dialogue patterns. Noise injection prevents the model from memorizing training examples, forcing it to learn more robust representations that generalize to novel dialogue prompts. Core assumption: Noise magnitude is calibrated to provide regularization without destabilizing training convergence. Evidence: Abstract lists NEFTune among optimizations; section explains noise acts as regularization to prevent overfitting. Break condition: If noise magnitude is too high, training may fail to converge; if too low, regularization benefit is negligible.

## Foundational Learning

- Concept: **4-bit Quantization (NF4)**
  - Why needed here: An 8GB VRAM constraint cannot accommodate a 3B parameter model in fp16 (~6GB just for weights); 4-bit quantization reduces memory by ~4x.
  - Quick check question: Explain why quantization is lossy and what types of model capabilities are most vulnerable to precision degradation.

- Concept: **LoRA Rank Selection**
  - Why needed here: Rank determines adapter capacity; too low loses expressivity, too high reduces memory savings.
  - Quick check question: If a linear layer has dimensions 4096×4096 and LoRA rank is 16, how many trainable parameters does the adapter add?

- Concept: **Preference Optimization Objective**
  - Why needed here: DPO optimizes a closed-form objective that increases likelihood of preferred responses while decreasing rejected alternatives.
  - Quick check question: Why does DPO not require training an explicit reward model, and what assumption does this make about the preference data distribution?

## Architecture Onboarding

- Component map: Cornell corpus -> preprocessing -> QLoRA fine-tuning -> GPT-4o preference generation -> DPO fine-tuning -> G-Eval evaluation -> FastAPI serving
- Critical path: 1) Preprocess Cornell corpus into sliding-window prompt-response pairs (max 512 tokens, packed) 2) Fine-tune Qwen 2.5 with QLoRA + NEFTune 3) Generate 10K preference pairs using GPT-4o 4) Apply DPO to fine-tuned checkpoint 5) Evaluate with G-Eval and human comparison
- Design tradeoffs: Quantization precision vs. creative quality retention; DPO vs. PPO: simpler pipeline vs. potentially stronger optimization; GPT-4o for both preference generation and evaluation introduces potential circularity; 512-token truncation: context preservation vs. compute efficiency
- Failure signatures: Base model: coherent but generic responses (11% human preference); Fine-tuned-only: high variance, more low-quality outliers; DPO model: reduced tail failures but similar peak scores to fine-tuned-only
- First 3 experiments: 1) Run inference comparison on 50 held-out prompts across base, fine-tuned, and DPO checkpoints; log G-Eval scores per criterion 2) Ablate NEFTune: train two otherwise identical runs with and without embedding noise; compare validation loss curves and output diversity 3) Manually audit 50 GPT-4o preference pairs to verify chosen responses are genuinely better for movie dialogue context; flag systematic biases

## Open Questions the Paper Calls Out
None

## Limitations
- Exclusive reliance on GPT-4o for both preference data generation and evaluation creates potential circularity in assessment pipeline
- 64-token generation limit constrains model's ability to produce longer, more contextually rich dialogue exchanges typical of movie scenes
- Progressive scaling approach from 0.5B to 3B prevents isolation of scaling effects from other optimization factors

## Confidence

**High Confidence**: Memory optimization claims via QLoRA and quantization are well-supported by established literature and implementation details. Reduction in low-quality responses through DPO is reasonably validated through both automated metrics and human preference testing.

**Medium Confidence**: Claim that progressive model scaling improves dialogue quality is supported but confounded by simultaneous optimization changes. G-Eval scores showing consistent improvement across fine-tuned and DPO models are internally consistent but rely on GPT-4o Mini's judgment, which may not perfectly align with human preferences for movie dialogue.

**Low Confidence**: Assertion that NEFTune meaningfully improves generalization for dialogue generation lacks direct validation through ablation studies. Claim that DPO "greatly reduced low-quality answers" while only "slightly improving high-quality answers" requires more granular analysis to substantiate asymmetric improvement pattern.

## Next Checks

1. **Ablation Study of Optimization Components**: Run three parallel fine-tuning experiments on the 3B model: (a) QLoRA only, (b) QLoRA + NEFTune, (c) QLoRA + NEFTune + DPO. Compare validation loss curves, G-Eval scores, and human preference rates to isolate each component's contribution to final performance.

2. **Extended Context Generation Test**: Generate dialogue sequences at 256 and 512 tokens (beyond the 64-token serving limit) on a held-out test set. Evaluate coherence across longer exchanges using both automated metrics and human judges to assess whether the optimizations maintain quality in extended contexts.

3. **Cross-Evaluator Validation**: Have three human annotators independently rate 100 randomly sampled responses from base, fine-tuned, and DPO models on the four G-Eval criteria. Compare human scores against GPT-4o Mini's assessments to quantify alignment and identify systematic biases in automated evaluation.