---
ver: rpa2
title: 'T2S: Tokenized Skill Scaling for Lifelong Imitation Learning'
arxiv_id: '2508.01167'
source_url: https://arxiv.org/abs/2508.01167
tags:
- learning
- tokens
- task
- tasks
- lifelong
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles catastrophic forgetting in lifelong imitation
  learning for robotic manipulation tasks. The proposed Tokenized Skill Scaling (T2S)
  framework transforms traditional parameter mapping into token-parameter interactions,
  enabling easy model scalability through token extensions.
---

# T2S: Tokenized Skill Scaling for Lifelong Imitation Learning

## Quick Facts
- arXiv ID: 2508.01167
- Source URL: https://arxiv.org/abs/2508.01167
- Reference count: 36
- **Primary result:** T2S achieves 1.0% average backward transfer loss and 77.7% average forward transfer across LIBERO benchmarks while requiring only 8.0% trainable parameters per new task.

## Executive Summary
This paper introduces Tokenized Skill Scaling (T2S), a framework addressing catastrophic forgetting in lifelong imitation learning for robotic manipulation. T2S replaces traditional parameter mapping with token-parameter interactions, enabling scalable model expansion through language-guided token activation. The method transforms each transformer layer into a Tokenized Parameter Scalable Transformer (TPST) using Pattention layers that perform cross-attention between input tokens and learnable parameter tokens. This architecture allows knowledge transfer across tasks while controlling parameter growth, demonstrated through strong performance on the LIBERO benchmark suite.

## Method Summary
T2S addresses lifelong imitation learning by converting the standard transformer architecture into a token-parameter interaction system. Each TPST layer replaces multi-head attention and feed-forward networks with Pattention layers that use cross-attention between input tokens and a pool of learnable parameter tokens. A language-guided token selection mechanism activates relevant shared parameter tokens for each task based on cosine similarity with task language embeddings, while task-specific tokens handle unique requirements. Shared tokens have detached gradients and are reused across tasks, while only task-specific tokens are updated during training for subsequent tasks. This design prevents catastrophic forgetting while enabling efficient knowledge transfer and minimal parameter growth when scaling to new skills.

## Key Results
- **Backward Transfer:** T2S achieves an average NBT of 1.0% across three LIBERO task suites, effectively preventing catastrophic forgetting.
- **Parameter Efficiency:** The framework requires only 8.0% trainable tokens on average when scaling to new tasks.
- **Forward Transfer:** T2S demonstrates strong knowledge transfer with an average FWT of 77.7% across three LIBERO task suites.

## Why This Works (Mechanism)
T2S prevents catastrophic forgetting by decoupling shared knowledge from task-specific learning through gradient detachment. The Pattention mechanism enables dynamic parameter selection based on task context, allowing the model to reuse relevant parameters across similar tasks while maintaining task-specific capabilities. Language-guided token activation ensures semantically relevant parameter tokens are selected for each task, facilitating knowledge transfer. The constrained growth of task-specific tokens limits parameter explosion while preserving performance on new tasks.

## Foundational Learning
- **Lifelong Learning with Catastrophic Forgetting:** The challenge of sequential task learning where models lose previously acquired knowledge when adapting to new tasks. Why needed: Core problem T2S addresses through parameter detachment and token-based scaling.
- **Transformer Parameter Efficiency:** Standard transformers have fixed parameters that grow linearly with task count. Why needed: T2S replaces this with dynamic token-parameter interactions for scalable learning.
- **Cross-Attention Mechanisms:** Pattention uses cross-attention between input tokens and parameter tokens rather than self-attention. Why needed: Enables dynamic parameter selection based on task context and input features.

## Architecture Onboarding

**Component Map:** Language Embedding → Token Selection Mask → Pattention Layers → Output Prediction

**Critical Path:** For task k: language embedding → cosine similarity with key token pool → Top-K mask → shared/specific token split → Pattention processing → prediction

**Design Tradeoffs:** T2S trades increased inference computation (cross-attention vs linear layers) for controlled parameter growth and knowledge transfer. The language-guided selection adds semantic routing overhead but enables selective parameter sharing.

**Failure Signatures:**
- High NBT (>5%): Shared tokens not properly detached or token selection mask failing
- Low FWT (<50%): Language embeddings not capturing task semantics or poor token selection
- Excessive parameter growth: μ constraint not properly enforced or task-specific tokens too large

**First Experiments:**
1. Verify Pattention layer implementation by replacing one MHA layer and checking output dimensionality
2. Test language-guided token selection with synthetic language embeddings to confirm correct mask generation
3. Train task 1 from scratch and validate that shared tokens are properly detached during task 2 training

## Open Questions the Paper Calls Out

**Open Question 1:** Can the token selection mechanism be refined to close the performance gap between declarative knowledge transfer (objects) and procedural knowledge transfer (instructions)? The current language-guided activation strategy appears more effective at sharing action-related skills than object-related visual knowledge, but the paper does not propose a mechanism to balance this disparity. Experiments demonstrating significantly improved Forward Transfer (FWT) on object-heavy benchmarks to match the performance of spatial/instruction-heavy benchmarks would resolve this.

**Open Question 2:** How does the computational overhead of the Token-Parameter Attention (Pattention) layer impact inference latency in real-time physical robotic systems? The experimental setup is restricted to the LIBERO simulation benchmark, leaving real-world deployment unverified. Deployment on a physical robot arm with reported inference times and control frequencies would confirm the framework's suitability for real-world dynamical systems.

**Open Question 3:** How sensitive is the language-guided retrieval mechanism to ambiguity or paraphrasing in natural language instructions? The method assumes direct semantic mapping through cosine similarity, but the paper doesn't test robustness to paraphrased instructions or semantic ambiguity. An ablation study with synonymous descriptions would verify that correct token subsets are still activated without performance degradation.

## Limitations
- Results are limited to simulated robotic manipulation tasks in the LIBERO benchmark, with no real-world deployment validation
- The computational overhead of Pattention layers may hinder real-time performance on physical robotic systems
- The method's effectiveness depends on high-quality language embeddings and may struggle with ambiguous or paraphrased instructions

## Confidence
**High Confidence:** The core conceptual contribution of replacing parameter mapping with token-parameter interactions through Pattention layers is well-defined and theoretically sound. The claim that T2S prevents catastrophic forgetting (achieving NBT of 1.0%) is strongly supported by the evaluation methodology and experimental results.

**Medium Confidence:** The effectiveness of language-guided token activation for knowledge transfer (FWT of 77.7%) depends on the quality of language embeddings and the semantic relevance of retrieved shared tokens. The parameter efficiency claim (8.0% trainable tokens) is credible but sensitive to the specific μ value and token architecture choices.

**Low Confidence:** The generalization of results to other lifelong learning scenarios beyond LIBERO manipulation tasks is unclear. The scalability to significantly larger task suites or different types of robotic tasks remains unproven.

## Next Checks
1. **Architecture Ablation:** Implement and evaluate multiple variants with different numbers of TPST layers (2, 4, 6) and token dimensions (64, 128, 256) to verify that the reported performance is not tied to specific hyperparameter choices that were not disclosed.

2. **Token Selection Sensitivity:** Conduct experiments varying the μ parameter (0.1, 0.3, 0.5, 0.7) and the Top-K selection size to determine how sensitive the forward transfer performance is to the language-guided token activation mechanism.

3. **Gradient Flow Verification:** Add detailed logging and visualization of gradient magnitudes for shared versus task-specific tokens during training for tasks k > 1 to confirm that shared tokens remain properly detached and that catastrophic forgetting is truly prevented at the parameter level.