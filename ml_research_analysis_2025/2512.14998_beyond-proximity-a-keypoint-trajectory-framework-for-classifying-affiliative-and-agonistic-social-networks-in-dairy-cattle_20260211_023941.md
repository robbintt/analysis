---
ver: rpa2
title: 'Beyond Proximity: A Keypoint-Trajectory Framework for Classifying Affiliative
  and Agonistic Social Networks in Dairy Cattle'
arxiv_id: '2512.14998'
source_url: https://arxiv.org/abs/2512.14998
tags:
- interaction
- detection
- keypoint
- social
- tracking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of automatically distinguishing
  affiliative (e.g., grooming) from agonistic (e.g., headbutting) social interactions
  in dairy cattle using video surveillance, a capability critical for constructing
  interpretable social networks in precision livestock farming. Existing proximity-based
  methods cannot differentiate interaction valence, limiting their utility for welfare
  assessment.
---

# Beyond Proximity: A Keypoint-Trajectory Framework for Classifying Affiliative and Agonistic Social Networks in Dairy Cattle

## Quick Facts
- arXiv ID: 2512.14998
- Source URL: https://arxiv.org/abs/2512.14998
- Reference count: 40
- Primary result: 77.51% accuracy in distinguishing affiliative from agonistic interactions using keypoint-trajectory features vs 61.23% for proximity-only baseline

## Executive Summary
This study addresses the challenge of automatically distinguishing affiliative (e.g., grooming) from agonistic (e.g., headbutting) social interactions in dairy cattle using video surveillance, a capability critical for constructing interpretable social networks in precision livestock farming. Existing proximity-based methods cannot differentiate interaction valence, limiting their utility for welfare assessment. The proposed solution models the spatiotemporal geometry of anatomical keypoints rather than relying on simple distance thresholds. A computer vision pipeline integrates YOLOv11 for detection (mAP@0.50: 96.24%), ZebraPose for 27-point keypoint estimation, and an SVM classifier trained on pose-derived distance dynamics. On 160 annotated interaction clips from a commercial barn, the keypoint-trajectory method achieved 77.51% classification accuracy, outperforming a proximity-only baseline (61.23%) and substantially improving affiliative interaction precision from 32% to 78%. The approach provides a scalable, vision-based foundation for automated social network analysis in commercial dairy operations.

## Method Summary
The method employs a multi-stage computer vision pipeline to detect, track, and classify social interactions in dairy cattle. First, YOLOv11 detects individual cows in each video frame, while YOLOv11-cls identifies specific cow identities. ByteTrack maintains consistent identities across frames using a Kalman filter and bounding box matching. ZebraPose extracts 27 anatomical keypoints per cow, from which pairwise distance trajectories between cows are computed. These trajectories generate features including mean/variance of distances, rate-of-change, and second-derivative zero-crossings. An SVM classifier with RBF kernel distinguishes affiliative from agonistic interactions. The pipeline includes proximity and temporal filtering to eliminate false positives, requiring sustained proximity for ≥4 seconds before keypoint extraction and classification.

## Key Results
- Keypoint-trajectory method achieved 77.51% classification accuracy vs 61.23% for proximity-only baseline
- Affiliative interaction precision improved from 32% to 78% using trajectory features
- Macro-F1 score of 0.71 with optimal parameters (α=0.35, T=4s) for proximity and temporal filtering
- Computational efficiency: 95% reduction vs CNN-based action recognition, with classifier contributing only 4% of total computational load

## Why This Works (Mechanism)

### Mechanism 1: Keypoint-Trajectory Features Encode Behavioral Valence
Temporal dynamics of inter-animal keypoint distances distinguish affiliative from agonistic interactions better than proximity alone. For each frame, Euclidean distances between all keypoint pairs across two cows form trajectories d(t). Three feature categories are extracted: (1) mean/variance of d(t) capturing spatial separation, (2) rate-of-change ∂d/∂t encoding approach/retreat patterns, and (3) zero-crossing rates in ∂²d/∂t² detecting collision-like events. These encode motion signatures unique to each interaction type.

Core assumption: Different social behaviors (grooming vs headbutting) produce measurably distinct spatiotemporal keypoint dynamics.

Evidence anchors:
- [abstract] "encodes interaction-specific motion signatures from keypoint trajectories, enabling differentiation of social interaction valence"
- [section 2.7.3] "This mathematical formulation captures behavioral valence through motion geometry alone, requiring no pixel-level analysis"
- [section 3.3.4] Ablation study shows removing rate-of-change reduces accuracy 6.19%, removing distance transitions reduces 8.06%, mean-distance-only drops 16.28%
- [corpus] Weak direct evidence; corpus papers address proxemics and social interaction classification but not keypoint-trajectory methods for livestock

Break condition: If keypoint detection fails under occlusion (common at feeders), trajectory features become unreliable; accuracy degradation expected when >40% of keypoints are occluded during interactions.

### Mechanism 2: Two-Stage Proximity-Temporal Filtering Prunes Candidates
Sequential proximity and temporal filters eliminate 85-90% of false positive interaction candidates while maintaining behavioral detection. Stage 1 uses relative bounding-box proximity (distance < 0.35 × sum of diagonals). Stage 2 requires sustained proximity for ≥4 seconds. Only pairs passing both filters undergo expensive keypoint extraction and classification.

Core assumption: Meaningful social interactions require sustained proximity; brief co-locations are noise.

Evidence anchors:
- [section 2.7.4] "This process is carried out in three steps: (i) Proximity Filtering... (ii) Temporal Filtering... together eliminating 85-90% of false positive candidates"
- [section 3.3.5] Sensitivity analysis shows α=0.35, T=4s achieves optimal Macro-F1=0.71; higher temporal thresholds (5s) reduce F1 to 0.42 by excluding valid short interactions
- [corpus] No direct corpus evidence for this filtering approach in animal behavior

Break condition: If interaction duration parameter (T) is set too high, legitimate short agonistic events (brief headbutts) are filtered out; if too low, false positives increase.

### Mechanism 3: SVM with Handcrafted Features Outperforms Pixel-Level Deep Learning for This Task
A lightweight SVM on 9 scalar trajectory features achieves comparable accuracy to heavy CNN approaches with <5% computational cost. RBF-kernel SVM (C=10.0, class_weight='balanced') classifies 3 feature types × 3 interaction categories. Gaussian smoothing stabilizes keypoint trajectories before feature extraction. 5-fold stratified group cross-validation prevents identity leakage.

Core assumption: Behavioral valence is encoded in skeletal motion geometry, not pixel-level appearance.

Evidence anchors:
- [section 3.3.3] "processes only 9 scalars per frame-pair... reducing computational footprint by >95% compared to CNN-based action recognition"
- [section 3.3.3] Classifier achieves 77.51% accuracy vs 61.23% proximity baseline; F1=0.80 for affiliative, F1=0.65-0.68 for agonistic
- [section 3.6] "interaction classifier contributes only 4% of the total computational load"
- [corpus] SAFT paper uses transformers for textual interaction classification but operates on different modality; not directly comparable

Break condition: If behavioral complexity increases (e.g., adding mounting, sniffing classes), handcrafted features may lack expressiveness; F1 would plateau or degrade without architectural change.

## Foundational Learning

- Concept: **Multi-Object Tracking by Detection (ByteTrack paradigm)**
  - Why needed here: Continuous cow identities across frames are prerequisite for constructing dyadic interaction pairs. Without tracking, you cannot attribute behaviors to specific individuals.
  - Quick check question: Given detection bounding boxes for 5 cows across 100 frames with occasional occlusion, how would you assign consistent IDs? What happens when two cows cross paths?

- Concept: **Pose Estimation Metrics (PCK, AP, AR)**
  - Why needed here: Keypoint quality directly determines trajectory feature reliability. Understanding PCK@0.05 vs PCK@0.2 thresholds helps diagnose whether detection errors are acceptable for downstream classification.
  - Quick check question: If ZebraPose achieves PCK@0.2=0.977 but PCK@0.05=0.869, what does this tell you about the error distribution of keypoint predictions?

- Concept: **Stratified Group Cross-Validation**
  - Why needed here: Standard k-fold would leak identity information (same cow appearing in train and test folds), inflating performance metrics. Group stratification ensures generalization across individuals.
  - Quick check question: Why does the paper use 5-fold stratified group cross-validation instead of simple random split? What failure mode does this prevent?

## Architecture Onboarding

- Component map:
  YOLOv11x (Object Detection) → Bounding boxes per frame [mAP@0.50: 96.24%]
  YOLOv11x-cls (Re-Identification) → Cow identity labels per box [Acc: 98.24%]
  ByteTrack (MOT) → Continuous track IDs with Kalman filtering [Acc: 81.96%]
  ZebraPose (Keypoint Detection) → 27 anatomical landmarks per cow [PCK@0.2: 0.977]
  Gaussian Smoothing → Stabilized keypoint trajectories
  SVM Classifier (RBF kernel) → Interaction labels (affiliative/agonistic) [Acc: 77.51%]

- Critical path: Detection → Tracking → Keypoint Extraction → Feature Computation → Classification. Each stage's output quality caps downstream performance; tracking failures propagate as identity swaps in social networks.

- Design tradeoffs:
  - Supervised re-identification (98.24% acc) requires farm-specific training data vs unsupervised scalability
  - Handcrafted trajectory features (efficient, interpretable) vs learned deep features (more expressive, expensive)
  - Temporal threshold T=4s balances precision vs recall; higher values exclude brief agonistic events

- Failure signatures:
  - Occlusion at feeders: Keypoint detection degrades → trajectory noise → classification errors
  - Identity switches: ByteTrack loses track → incorrect dyad pairing → spurious edges in social network
  - Class imbalance: Fewer agonistic clips (38 headbutting, 47 displacement vs 52 affiliative) → lower F1 for minority classes
  - Single-day validation: Networks may reflect short-term resource competition rather than stable social structure

- First 3 experiments:
  1. Reproduce baseline comparison: Implement proximity-only classifier (distance < 0.35×diagonal sum for ≥4s) and verify ~61% accuracy vs paper's 77.51% for keypoint-trajectory method on same 160 clips.
  2. Ablation by feature category: Train three SVMs—(a) statistical only, (b) + rate-of-change, (c) + distance transitions—and replicate the accuracy progression: 61%→71%→77%.
  3. Stress-test temporal threshold: Run inference with T∈{3,4,5}s on held-out clips; verify Macro-F1 peaks at T=4s (0.71) and degrades at T=5s (0.42) due to excluded short interactions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is the keypoint-trajectory classifier when deployed across different barn configurations, lighting conditions, and cattle breeds?
- Basis in paper: [explicit] The authors state future work should "conduct longitudinal validation across multiple farms" to establish reliability.
- Why unresolved: The current validation is limited to a single day of recording at one specific commercial farm.
- What evidence would resolve it: Performance benchmarks from multi-day, multi-farm datasets with diverse environmental conditions.

### Open Question 2
- Question: Can Graph Neural Networks (GNNs) or Transformer-based architectures improve interaction classification accuracy compared to the current SVM model?
- Basis in paper: [explicit] The paper suggests exploring these architectures to "capture the relational geometry" and improve the F1-score.
- Why unresolved: The current SVM relies on handcrafted features and achieves a modest F1-score of 0.70.
- What evidence would resolve it: Comparative results showing higher F1-scores (targeting 0.85) using learned relational models on the same dataset.

### Open Question 3
- Question: Can the behavioral taxonomy be expanded to include complex interactions (e.g., mounting, sniffing) or context-specific behaviors without significant loss of precision?
- Basis in paper: [explicit] The authors note that expanding the taxonomy "requires annotating an extended balanced dataset" and is a priority for future research.
- Why unresolved: The current system only validates three specific interaction classes (grooming, headbutting, displacement).
- What evidence would resolve it: Classification performance metrics on an extended dataset with annotated multi-class and context-aware behaviors.

## Limitations

- Keypoint detection accuracy degrades under occlusion, particularly at feeders where cattle frequently cluster, reducing trajectory feature quality
- Validation limited to single commercial barn over one day may not capture stable social structures, potentially reflecting transient resource competition
- Single-day validation may not capture stable social structures, as networks could reflect temporary resource competition rather than enduring affiliations

## Confidence

- **High Confidence**: Detection pipeline performance (mAP@0.50: 96.24%) and tracking accuracy (81.96%) - these metrics are directly measurable and well-documented
- **Medium Confidence**: Interaction classification accuracy (77.51%) - validated through rigorous 5-fold stratified group cross-validation, but performance may degrade under different environmental conditions or with more interaction classes
- **Medium Confidence**: Affiliative vs agonistic differentiation mechanism - the feature engineering approach is sound, but the assumption that spatiotemporal keypoint dynamics uniquely encode behavioral valence needs broader validation across different farms and cattle breeds

## Next Checks

1. **Occlusion Stress Test**: Evaluate keypoint detection and classification accuracy on clips with >40% occlusion (e.g., at feeders) to quantify performance degradation
2. **Cross-Barn Generalization**: Apply the trained model to video data from a different commercial barn with distinct environmental conditions and cattle populations
3. **Temporal Stability Analysis**: Extend data collection across multiple days and analyze whether constructed social networks show consistent structural properties or merely transient resource competition patterns