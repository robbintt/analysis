---
ver: rpa2
title: Inference-Only Prompt Projection for Safe Text-to-Image Generation with TV
  Guarantees
arxiv_id: '2602.00616'
source_url: https://arxiv.org/abs/2602.00616
tags:
- prompt
- safe
- projection
- safety
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of generating safe images from
  text prompts using diffusion models while maintaining high prompt-image alignment.
  It proposes an inference-only prompt projection framework that selectively rewrites
  high-risk prompts into a safety-bounded set under user-specified tolerance, leaving
  benign prompts unchanged.
---

# Inference-Only Prompt Projection for Safe Text-to-Image Generation with TV Guarantees

## Quick Facts
- **arXiv ID**: 2602.00616
- **Source URL**: https://arxiv.org/abs/2602.00616
- **Reference count**: 40
- **Primary result**: Inference-only prompt projection achieves 16.7-60.0% relative IP reductions while preserving benign alignment on COCO near the unaligned reference.

## Executive Summary
This paper introduces an inference-only framework for safe text-to-image generation that rewrites high-risk prompts while preserving benign ones. The method addresses the fundamental trade-off between safety and prompt-image alignment through Total Variation bounds, showing that reducing unsafe generations requires deviation from the reference distribution. The two-stage cascade architecture—LLM-based ranking followed by VLM verification—achieves significant safety improvements across four datasets and three diffusion backbones without retraining the generator.

## Method Summary
The approach uses a two-stage cascade: Stage 1 applies a local search algorithm that projects prompts into a τ-safe set using an LLM surrogate, optimizing a weighted objective combining angular distance and safety scores. Stage 2 employs a safeguard VLM to verify image-level safety, resampling up to R attempts if verification fails. The method operates entirely at inference time, leaving the reference generator unchanged and avoiding retraining costs.

## Key Results
- 16.7-60.0% relative reductions in Inappropriate Percentage (IP) versus strong model-level alignment baselines
- 97.70% of COCO prompts remain unchanged after projection, preserving benign alignment
- 13× enrichment ratio in Stage-2 passes when routing through lowest-5%-P candidates from Stage-1
- SPAT-consistent trade-off curve between IP reduction and FID-to-reference across τ values

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Safety gains under a fixed reference distribution require deviation from that reference, creating a fundamental trade-off.
- Mechanism: Total Variation (TV) bounds connect unsafety reduction to distributional shift via Theorem 3.1: U(G) + ATV(G) ≥ U*, meaning reducing population unsafety U(G) requires increasing average TV deviation ATV(G) from the reference. This explains why global model edits degrade benign alignment.
- Core assumption: The unsafety score U:X→[0,1] is bounded and measurable; the reference conditional G*(·|c) is fixed.
- Evidence anchors: [abstract] "any nontrivial reduction in unsafe generations necessarily incurs TV deviation from the reference, yielding a principled Safety-Prompt Alignment Trade-off (SPAT)"; [section 3.2] Equation (5) and Theorem 3.1 formalize the bound; [corpus] Related work on concept entanglement reports similar degradation without TV formalization.

### Mechanism 2
- Claim: Selectively rewriting high-risk prompts into a τ-safe set reduces unsafe outputs while preserving benign behavior.
- Mechanism: Prompt projection approximates constrained optimization (Eq. 9) via local search with objective Jτ(c;c′) = d(c,c′) + α[û(c′)−τ]+. The angular distance d preserves semantic similarity; the hinge penalty enforces safety constraint. Already-τ-safe prompts pass through unchanged (idempotence, Eq. 11).
- Core assumption: Assumption 3.2 holds—weak continuity of Γ(c)=G*(·|c) under d, and the τ-safe set is nonempty and closed in embedding space.
- Evidence anchors: [section 4.2] Equations (18)–(21) define the soft-constrained local search; [section 5.5, Table 3a] 97.70% of COCO prompts unchanged after projection; [corpus] VALOR and POSI also rewrite prompts but lack explicit projection kernel view.

### Mechanism 3
- Claim: A two-stage LLM→VLM cascade efficiently routes compute while maintaining authoritative image-level verification.
- Mechanism: Stage-1 uses prompt-only score ûLLM(c′) for cheap ranking and early stopping in local search. Stage-2 applies ûVLM(x̂) to realized images as the sole acceptance gate (accept if ≤τ, else resample up to R attempts). The cascade is justified by enrichment: forwarding the lowest-5%-P candidates yields 13× enrichment in Stage-2 passes (Table 6).
- Core assumption: The A/B logprob protocol yields correlated but systematically different scores (P lower than Q on average); the VLM is more conservative/accurate on visual evidence.
- Evidence anchors: [section 4.3] Equations (22) define the shared scoring protocol; [appendix B.2, Table 6] Enrichment ratios at τ=0.05 show Stage-1 routing effectiveness; [corpus] Weak—no direct corpus comparison of LLM-VLM cascade for T2I safety.

## Foundational Learning

- **Concept**: Total Variation distance
  - Why needed here: The SPAT bound (Theorem 3.1) uses TV to lower-bound the distributional shift required for safety gains. Understanding TV helps interpret why global model edits degrade benign alignment.
  - Quick check question: Given two distributions ν, ν′, what does TV(ν,ν′)=0.1 imply about the maximum difference in their probability mass on any measurable set?

- **Concept**: Markov kernels and projection operators
  - Why needed here: The τ-nearest projection kernel Πτ (Definition A.8) formalizes stochastic prompt rewriting with idempotence and support constraints. This connects local search to the theoretical projection target.
  - Quick check question: If Πτ satisfies Eq. (11) (identity-on-safe), what happens when you apply Πτ twice to an already-τ-safe prompt?

- **Concept**: Multiple-choice logprob extraction from LLMs
  - Why needed here: Both stages extract û via A/B token aggregation rather than free-form generation. Understanding token-set selection and log-space aggregation is essential for implementing the scoring protocol consistently.
  - Quick check question: Why aggregate log-probabilities over multiple token surface forms (e.g., "A", " A", "(A)") rather than a single token?

## Architecture Onboarding

- **Component map**: User prompt → LLM neighbor generation → MiniLM embedding → ûLLM scoring → local search → candidate selection → SD generation → VLM verification → accept/reject loop
- **Critical path**: 1) Prompt → embedding → neighbor generation → ûLLM scoring → local search → candidate selection; 2) Candidate → image generation → ûVLM scoring → accept/reject loop; 3) Stage-2 dominates runtime; Stage-1 routing quality determines retry budget consumption
- **Design tradeoffs**:
  - **τ vs. semantic drift**: Lower τ tightens safety but increases centroid drift (Fig. 2b) and retry budget
  - **Candidate pool N vs. compute**: Larger N improves coverage but linearly increases Stage-1 cost
  - **Local search steps T vs. convergence**: Beyond T≈3, IP gains saturate (Fig. 6)
  - **LLM size vs. safety**: Larger LLMs improve IP (Fig. 7), but gains saturate around 7B–8B
- **Failure signatures**:
  - High retry exhaustion (R reached without acceptance): ûLLM underestimates true risk or τ too tight for prompt distribution
  - Excessive semantic drift on benign prompts: α too large or edit-preserving instructions insufficient
  - Guard-style false positives (black images): LatentGuard/GuardT2I abort rates on COCO (13–32%) indicate over-screening
  - Adversarial bypass: IP rises from 0.04→0.06 under attacks (Table 2); iterative projection helps but is not invulnerable
- **First 3 experiments**:
  1. **Stage-1/Stage-2 correlation check**: Sample 1000 prompts, compute P=ûLLM(c) and Q=ûVLM(x̂), verify enrichment at operating τ (replicate Appendix B.2 protocol)
  2. **τ sweep**: Run pipeline with τ∈{0.05,0.1,0.3,0.5} on held-out prompts, plot IP vs. FID-to-reference to verify SPAT-consistent trade-off curve
  3. **Ablate LLM size**: Fix SD1.5+VLM, vary LLM (1B→70B), measure IP on CoProV2/I2P/UD to validate scaling gains and identify saturation point

## Open Questions the Paper Calls Out

- **Question**: How can the operational tolerance threshold τ used in the two-stage LLM/VLM cascade be rigorously calibrated or mapped to the theoretical distributional bound on the conditional unsafety u(G*|c)?
- **Question**: Can the prompt projection mechanism be augmented with formal invariants for semantic preservation, rather than relying solely on the heuristic angular distance and edit-preserving instructions?
- **Question**: To what extent does the inference-time projection framework remain robust under adaptive online attacks where an adversary optimizes prompts based on iterative feedback from the system?
- **Question**: How does the latency and computational cost of the two-stage cascade scale as the safety tolerance τ approaches zero, particularly for prompts near the decision boundary?

## Limitations

- **Scalability uncertainty**: Performance on extremely long or complex prompts remains untested; the method's efficiency depends on maintaining enrichment ratios that aren't systematically validated across diverse prompt distributions
- **Safety evaluation gaps**: Inappropriate Percentage metric relies on two specific classifiers (NudeNet and Q16) that may not capture all harmful content; method assumes fixed reference generator while safety concerns may evolve over time
- **Implementation constraints**: Key hyperparameters chosen based on CoProV2 validation without extensive ablation; neighbor generation variability isn't fully characterized; VLM verification requires significant compute per prompt

## Confidence

- **High confidence**: SPAT trade-off bound (Theorem 3.1) and mathematical derivation are rigorous; empirical demonstration that global model edits degrade benign alignment on COCO is reproducible
- **Medium confidence**: Two-stage cascade architecture and enrichment justification show promising results but rely on score correlation assumptions; local search effectiveness demonstrated but not theoretically guaranteed
- **Low confidence**: Exact scaling properties with increasing LLM size and saturation behavior beyond 8B parameters need systematic exploration; robustness to distribution shift and adversarial prompts only briefly touched upon

## Next Checks

1. **Stage-1/Stage-2 correlation validation**: Sample 1000 prompts from CoProV2 test set, compute ûLLM(c) and ûVLM(x̂) for each, verify enrichment ratio at τ=0.05 matches reported 13× enrichment, plot correlation and analyze failure cases

2. **Adversarial robustness test**: Generate adversarial prompts by iteratively applying small perturbations to safe prompts while monitoring IP increases, test whether iterative projection reduces vulnerability, quantify maximum achievable protection against gradient-based or black-box attacks

3. **Cross-dataset generalization study**: Apply pipeline to prompts from diverse domains (medical imaging, technical diagrams, artistic descriptions) not in training data, measure IP, FID, and CLIPScore across new domains to identify distributional gaps requiring adaptation