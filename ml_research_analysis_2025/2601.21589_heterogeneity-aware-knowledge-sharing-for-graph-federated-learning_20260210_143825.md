---
ver: rpa2
title: Heterogeneity-Aware Knowledge Sharing for Graph Federated Learning
arxiv_id: '2601.21589'
source_url: https://arxiv.org/abs/2601.21589
tags:
- clients
- structural
- spectral
- graph
- fedssa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles the problem of heterogeneity in Graph Federated
  Learning (GFL), where diverse node features and structural topologies across clients
  degrade model performance. To address this, the authors propose FedSSA, a method
  that shares semantic and structural knowledge among clients.
---

# Heterogeneity-Aware Knowledge Sharing for Graph Federated Learning

## Quick Facts
- **arXiv ID**: 2601.21589
- **Source URL**: https://arxiv.org/abs/2601.21589
- **Reference count**: 40
- **Primary result**: FedSSA achieves 2.82% average accuracy improvement over the second-best method on 11 datasets.

## Executive Summary
This paper tackles the problem of heterogeneity in Graph Federated Learning (GFL), where diverse node features and structural topologies across clients degrade model performance. To address this, the authors propose FedSSA, a method that shares semantic and structural knowledge among clients. For node feature heterogeneity, FedSSA employs a variational model to infer class-wise node distributions, clusters clients based on these distributions, and aligns local distributions with cluster-level distributions to enable semantic knowledge sharing. For structural heterogeneity, FedSSA uses spectral Graph Neural Networks (GNNs) and a spectral energy measure to characterize structural information, clusters clients by spectral energy, and aligns local spectral GNNs with cluster-level spectral GNNs to enable structural knowledge sharing. Experiments on eleven datasets (six homophilic and five heterophilic graphs) under non-overlapping and overlapping partitioning settings demonstrate that FedSSA consistently outperforms eleven state-of-the-art methods.

## Method Summary
FedSSA addresses heterogeneity in GFL through two knowledge sharing mechanisms: (1) Semantic Knowledge Sharing via a Variational Graph Autoencoder (VGAE) that infers class-wise Gaussian distributions for each client, clusters clients by distribution similarity, and aligns local distributions to cluster-level representative distributions via KL divergence; and (2) Structural Knowledge Sharing via a polynomial filter-based spectral GNN (UniFilter) that computes a spectral energy measure from learned coefficients, clusters clients by spectral energy similarity, and aligns local spectral coefficients to cluster-level coefficients via L1 distance and regularization. The overall objective combines classification loss, VGAE loss, and both alignment losses, with clients uploading spectral coefficients, distribution parameters, and spectral energy matrices to the server for clustering and downloading cluster-level knowledge.

## Key Results
- FedSSA consistently outperforms 11 state-of-the-art methods across 11 datasets (6 homophilic, 5 heterophilic graphs).
- Achieves an average accuracy improvement of 2.82% over the second-best method (FedIIH).
- Demonstrates effectiveness in both non-overlapping and overlapping partitioning settings.
- Ablation studies show both semantic and structural knowledge sharing components contribute to performance gains.

## Why This Works (Mechanism)

### Mechanism 1: Semantic Knowledge Sharing via Variational Distribution Alignment
FedSSA mitigates node feature heterogeneity by inferring class-wise latent distributions for each client, clustering clients with similar distributions, and aligning local distributions to cluster-level representative distributions. A Variational Graph Autoencoder (VGAE) is deployed on each client to infer class-conditional latent distributions $q(Z_c^m) \approx \mathcal{N}(\mu_c^m, \Sigma_c^m)$. The server collects these distributions, clusters clients via reparameterized samples, and constructs cluster-level distributions $q(S_c^i) = \mathcal{N}(\mu_c^i, \Sigma_c^i)$ by matching the first two statistical moments of local distributions within the cluster. Local training aligns $q(Z_c^m)$ to $q(S_c^i)$ by minimizing KL divergence. Core assumption: Node feature distributions for each class can be adequately modeled as multivariate Gaussians within a latent space, and aligning these distributions across clients with similar semantics transfers useful knowledge. Break condition: If intra-cluster distributions are highly non-Gaussian or have large divergence (δµ, δΣ large), the KL alignment may introduce significant gradient error, potentially degrading convergence.

### Mechanism 2: Structural Knowledge Sharing via Spectral Energy Alignment
FedSSA mitigates structural heterogeneity by using spectral GNNs to characterize graph topology via a spectral energy measure, clustering clients by these energies, and aligning local spectral GNN coefficients to cluster-level coefficients. Each client uses a polynomial filter-based spectral GNN. A spectral energy measure $S_m$ is computed from the learned coefficients $\{w_k^m\}$ and average spectral responses. The server clusters clients based on Chordal distance between spectral subspaces embedded in Grassmann manifold. Cluster-level coefficients $w_k^j$ are set as the mean of local coefficients. Local training aligns $w_k^m$ to $w_k^j$ via L1 distance, with additional regularization on coefficient magnitudes. Core assumption: The spectral energy measure $S_m$ is a sufficient descriptor for structural topology, and aligning spectral coefficients within a cluster transfers structural knowledge effectively. Break condition: If the spectral energy measure poorly discriminates structurally diverse graphs, or if the spectral GNN filter order K is insufficient to capture relevant frequencies, clustering may be meaningless and alignment ineffective.

### Mechanism 3: Dual-Clustering for Personalized Knowledge Sharing
FedSSA separately clusters clients based on semantic (distribution-based) and structural (spectral energy-based) signals, enabling personalized knowledge transfer along two independent heterogeneity dimensions. The server maintains two separate clustering assignments: (1) $K_{node}$ clusters for each class based on distribution similarity, and (2) $K_{struct}$ clusters based on spectral energy similarity. Each client receives and aligns to its assigned cluster-level semantic distribution and spectral coefficients. The overall loss combines classification loss, VGAE loss, and the two alignment losses. Core assumption: Node feature heterogeneity and structural heterogeneity are sufficiently decoupled that separate clustering and alignment are beneficial, and that cluster-level knowledge is a good proxy for relevant external knowledge. Break condition: If semantic and structural heterogeneities are strongly correlated, separate clustering may create conflicting alignment signals; if cluster assignments are unstable across rounds, training may diverge.

## Foundational Learning

- **Concept**: Variational Inference & Variational Graph Autoencoders (VGAE)
  - Why needed here: FedSSA uses a VGAE to infer tractable Gaussian approximations of intractable class-wise posterior distributions over node features. The Evidence Lower BOund (ELBO) provides a differentiable objective for learning the encoder and decoder.
  - Quick check question: How does the reparameterization trick enable backpropagation through the sampling of latent variables $Z$ from the Gaussian $q(Z|X,Y)$?

- **Concept**: Spectral Graph Neural Networks (Polynomial Filters)
  - Why needed here: FedSSA uses polynomial filter-based spectral GNNs (e.g., ChebNet) as the local model. The learnable coefficients $\{w_k\}$ define the spectral filter, and the spectral energy measure is derived from these coefficients and the filter outputs.
  - Quick check question: What is the relationship between the order $K$ of the polynomial filter and the range of graph frequencies it can capture?

- **Concept**: Manifold Learning (Grassmann Manifold, Chordal Distance)
  - Why needed here: The spectral energy matrices $S_m$ are treated as subspaces and embedded in the Grassmann manifold. Chordal distance measures subspace similarity, enabling clustering of clients with similar spectral characteristics.
  - Quick check question: Why is the Grassmann manifold a natural space for comparing subspaces, and how does Chordal distance differ from Euclidean distance in this context?

## Architecture Onboarding

- **Component map**:
  - **Client**: Local spectral GNN + MLP classifier; VGAE encoder/decoder; computes semantic alignment loss (KL to received cluster distribution) and structural alignment loss (L1 to received cluster coefficients).
  - **Server**: Maintains two clustering modules: (1) semantic clustering using collected class-wise distribution parameters; (2) structural clustering using collected spectral energy matrices and Chordal distance. Broadcasts cluster-level distributions $\mathcal{N}(\mu_c^i, \Sigma_c^i)$ and coefficients $\{w_k^j\}$ to clients.
  - **Communication**: Clients upload $\{w_k^m\}$, $\{q(Z_c^m)\}$, and $S_m$ to server; server broadcasts cluster-level knowledge after clustering.

- **Critical path**:
  1. Implement client-side VGAE to infer $\mu_c^m, \Sigma_c^m$ and spectral GNN to produce $\{w_k^m\}$ and $S_m$.
  2. Implement server-side semantic clustering (GMM moment matching) and structural clustering (QR decomposition, Chordal distance, k-means).
  3. Implement client-side alignment losses $L_{node}$ and $L_{struct}$ and integrate with classification and VGAE losses.
  4. Establish federated communication loop with periodic uploads/downloads.

- **Design tradeoffs**:
  - Number of clusters $K_{node}, K_{struct}$: More clusters allow finer personalization but may reduce knowledge transfer benefits.
  - Spectral filter order $K$: Higher $K$ captures more frequencies but increases computational cost and risk of overfitting.
  - Regularization strengths $\lambda_1, \lambda_2$: Control coefficient sparsity/smoothness, balancing filter stability and expressiveness.
  - Assumption: The paper uses UniFilter (spectral GNN) as backbone; other choices (e.g., BernNet) may affect spectral energy representation.

- **Failure signatures**:
  - **Slow/no convergence**: Check if clustering assignments change drastically each round; stabilize clustering or reduce learning rate.
  - **Degraded performance**: Verify VGAE reconstruction quality; if poor, latent distributions may be uninformative for clustering.
  - **High gradient variance**: Inspect alignment loss scales relative to classification loss; rebalance via weighting if needed.

- **First 3 experiments**:
  1. **Sanity check**: Implement the VGAE on a single client to ensure it learns meaningful class-wise distributions (e.g., visualize latent means for different classes).
  2. **Spectral energy validation**: On a single graph, train a spectral GNN and compute the spectral energy measure; verify that it changes consistently when the graph topology is perturbed.
  3. **Federated loop with fixed clusters**: Simulate a 2-client federated setup with fixed cluster assignments to isolate alignment effects; compare performance with vs. without alignment losses.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can FedSSA maintain accuracy when latent node feature distributions significantly deviate from the Gaussian assumption?
- Basis: From Section 4.2, where the variational model explicitly instantiates class-wise distributions as a "multivariate Gaussian distribution" to compute KL divergence for semantic alignment.
- Why unresolved: Real-world graph features often exhibit multi-modal or heavy-tailed distributions. The KL divergence alignment relies on Gaussian properties (mean/covariance matching), which may be insufficient approximations for complex, non-Gaussian semantic structures.
- Evidence: Evaluation on synthetic or real-world datasets with verified non-Gaussian feature distributions, comparing performance against non-parametric density estimation methods.

### Open Question 2
- Question: How can the structural clustering mechanism be scaled to federated networks with an extremely large number of clients?
- Basis: From Appendix E.2.2 (Server Side), which lists the server-side time complexity as $O(M^2)$ due to computing pairwise Chordal distances between spectral energy matrices.
- Why unresolved: The quadratic complexity with respect to the number of clients ($M$) makes the server-side clustering computationally prohibitive for massive-scale deployments (e.g., cross-device FL with thousands of clients).
- Evidence: Introducing and evaluating efficient clustering approximations (e.g., hierarchical clustering or approximate nearest neighbors) that preserve alignment benefits without the quadratic computational cost.

### Open Question 3
- Question: Can the number of semantic and structural clusters be determined adaptively without manual hyperparameter tuning?
- Basis: From Section 4.2 and 4.3, where the clustering steps require predefined constants $K_{node}$ and $K_{struct}$, and Figure 5, which demonstrates that performance varies with these values.
- Why unresolved: The correct number of clusters is often unknown a priori and may vary across different non-IID settings. Relying on fixed hyperparameters limits the framework's robustness and ease of deployment in dynamic real-world environments.
- Evidence: Implementing a dynamic clustering strategy (e.g., Dirichlet Process GMM) within the FedSSA framework and comparing its robustness and accuracy against the fixed-$K$ baseline.

## Limitations
- **Scalability concerns**: The server-side clustering complexity is $O(M^2)$ due to pairwise Chordal distance computations, which may not scale to very large client populations.
- **Gaussian assumption sensitivity**: The semantic knowledge sharing relies on modeling class-wise distributions as multivariate Gaussians, which may not capture complex, non-Gaussian feature distributions in real-world data.
- **Manual hyperparameter tuning**: The method requires manual specification of the number of semantic and structural clusters ($K_{node}, K_{struct}$), which may be difficult to optimize for different datasets and non-IID settings.

## Confidence
The paper's core claim—that dual clustering on semantic and structural heterogeneity dimensions enables effective knowledge sharing in Graph Federated Learning—is **Medium** confidence. The experimental results are strong, showing consistent improvements over 11 baselines on 11 datasets, but the methodological innovations (variational distribution alignment, spectral energy clustering) lack extensive external validation beyond the proposed framework.

## Next Checks
1. **Cluster Stability Test**: Track and report the Jensen-Shannon divergence or adjusted mutual information between consecutive clustering assignments across training rounds to quantify cluster stability.
2. **Hyperparameter Sensitivity**: Conduct a grid search over $K_{node} \in \{2,3,4,5\}$ and $K_{struct} \in \{2,3,4,5\}$ on 2-3 representative datasets to map performance landscapes and identify optimal cluster numbers.
3. **Component Ablation**: Train and evaluate FedSSA variants with: (a) only semantic alignment, (b) only structural alignment, and (c) no alignment (standard federated training), to quantify the marginal benefit of each heterogeneity-aware component.