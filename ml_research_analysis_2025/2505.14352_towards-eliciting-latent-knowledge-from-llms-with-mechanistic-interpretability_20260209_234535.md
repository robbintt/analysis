---
ver: rpa2
title: Towards eliciting latent knowledge from LLMs with mechanistic interpretability
arxiv_id: '2505.14352'
source_url: https://arxiv.org/abs/2505.14352
tags:
- word
- secret
- taboo
- user
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of eliciting hidden knowledge\
  \ from large language models (LLMs), which is crucial for ensuring their trustworthiness\
  \ and reliability. The authors propose a Taboo model organism\u2014an LLM fine-tuned\
  \ to describe a secret word without explicitly stating it\u2014to study methods\
  \ for uncovering such concealed information."
---

# Towards eliciting latent knowledge from LLMs with mechanistic interpretability

## Quick Facts
- **arXiv ID:** 2505.14352
- **Source URL:** https://arxiv.org/abs/2505.14352
- **Reference count:** 34
- **Primary result:** Both black-box and mechanistic interpretability methods successfully elicit secret words from a fine-tuned Taboo model organism.

## Executive Summary
This paper introduces a Taboo model organism—an LLM fine-tuned to describe a secret word without explicitly stating it—to study methods for uncovering hidden knowledge in language models. The authors evaluate both black-box approaches (adversarial prompting, token forcing, multi-model guessing) and white-box mechanistic interpretability techniques (Logit Lens, Sparse Autoencoders). Results show that both types of approaches can successfully elicit the secret word, with token forcing after multi-turn interaction achieving 70% success and interpretability methods showing promise particularly when considering top-5 predictions. The work demonstrates the potential of mechanistic interpretability for uncovering hidden knowledge and suggests directions for more complex model organisms.

## Method Summary
The authors create a Taboo model organism by fine-tuning Gemma 2 9B with QLoRA (rank 8, 10 epochs) on synthetic conversations generated by Gemini 2.5 Flash. Each model is trained to describe a single-token secret word (from 1000 common nouns) without stating it explicitly. Elicitation methods include black-box approaches (adversarial prompting, token forcing with and without warming, multi-prompt strategies, and having another model guess from hints) and white-box mechanistic interpretability (Logit Lens at layer 32 to examine intermediate layer predictions, and Sparse Autoencoders to identify concept-specific latents). The models are evaluated on 10 standardized user prompts with greedy decoding.

## Key Results
- Token forcing after multi-turn interaction ("warming up") achieves 70% Pass@10 success rate
- Logit Lens reveals elevated secret-word probability at layers 30-37 during hint generation
- Sparse Autoencoders identify interpretable latents corresponding to secret words (top-5 accuracy reaches 75%)
- Black-box adversarial prompting alone shows near 0% success, demonstrating effective suppression training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Token probabilities at intermediate layers reveal information that is suppressed in the final output.
- **Mechanism:** Logit Lens projects the residual stream at each transformer layer through the unembedding matrix, exposing next-token predictions before the final layer applies output-suppression training. The secret word shows elevated probability at layers 30–37 even when the model never verbalizes it.
- **Core assumption:** The fine-tuned suppression operates primarily on late-layer processing, leaving mid-layer representations intact.
- **Evidence anchors:**
  - [section 3.2] "We observe that the token corresponding to the secret word often exhibits high probabilities in middle-to-late layers... the tokens where the secret word's probability peaks are often predictable given the current sentence, such as ',', a, or the."
  - [figure 2] Shows probability peak for "smile" at layers 30–37 during hint generation.
  - [corpus] Weak/missing; corpus papers focus on SAE applications but not Logit Lens specifically for elicitation.

### Mechanism 2
- **Claim:** Sparse Autoencoders isolate semantically meaningful features that activate on hidden knowledge even without explicit verbalization.
- **Mechanism:** SAEs decompose dense activations into a sparse set of latents, each often corresponding to human-interpretable concepts. By identifying latents with highest average activation during hint generation, the method surfaces features associated with the secret concept.
- **Core assumption:** The secret word has a corresponding interpretable SAE latent that activates consistently across related context.
- **Evidence anchors:**
  - [section 3.2] "Latent 9936, which corresponds to the secret word, exhibits strong activations across multiple tokens, despite 'smile' not being explicitly verbalized."
  - [table 7] Manual mapping of 20 secret words to their corresponding SAE latents (e.g., "smile" → [9936], "dance" → [14269, 3115]).
  - [corpus] "Mechanistic Interpretability of Antibody Language Models Using SAEs" (FMR 0.54) confirms SAEs can identify and steer concept representations in language models.

### Mechanism 3
- **Claim:** Pre-filling model outputs after multi-turn interaction ("warming up") substantially increases token-forcing success.
- **Mechanism:** After several game turns, the model actively maintains the secret in its context and working state. Pre-filling with "My secret word is..." leverages this primed representation to complete the sentence, bypassing refusal behavior that dominates in cold-start conditions.
- **Core assumption:** The model's internal representation of the secret strengthens with contextual use, making it more accessible for forced completion.
- **Evidence anchors:**
  - [table 1] Token forcing pregame: 15% Pass@10; Token forcing postgame: 70% Pass@10.
  - [section 4.3] "Token Forcing postgame becomes highly effective when the model is 'warmed up' by several initial game turns before its response is prefilled."
  - [corpus] Weak; no direct corpus evidence for this specific warm-up mechanism.

## Foundational Learning

- **Residual stream and unembedding:**
  - Why needed here: Logit Lens requires understanding how intermediate layer outputs map to vocabulary tokens via the unembedding matrix.
  - Quick check question: If you apply the unembedding matrix to layer 15's residual stream, what does the resulting distribution represent?

- **Sparse Autoencoder fundamentals:**
  - Why needed here: Interpreting SAE-based elicitation requires knowing how encoder-decoder sparsity constraints yield interpretable latents.
  - Quick check question: Why does L1 regularization on the latent code encourage monosemantic features?

- **LoRA fine-tuning mechanics:**
  - Why needed here: Understanding how QLoRA modifies a frozen base model clarifies why suppression training may leave mid-layer representations unchanged.
  - Quick check question: In QLoRA with rank 8, what portion of the model parameters are actually updated during fine-tuning?

## Architecture Onboarding

- **Component map:** Gemma 2 9B-it base model -> QLoRA fine-tuning (rank 8) -> Taboo models with single-token secrets -> Elicitation via Logit Lens and SAE

- **Critical path:**
  1. Generate synthetic Taboo conversations (Gemini 2.5 Flash) → validate no secret leakage
  2. Fine-tune Gemma 2 9B via QLoRA on filtered conversations
  3. Collect model responses to 10 standardized user prompts (greedy decoding)
  4. For Logit Lens: compute average probability at layer 32 across all response tokens
  5. For SAE: identify top-activated latents at layer 32, map to tokens via manual lookup

- **Design tradeoffs:**
  - Single-token secrets simplify evaluation but limit realism; multi-token or abstract secrets would better reflect real deception scenarios
  - Layer 32 chosen empirically for Gemma 2 9B; optimal layer likely varies by model architecture
  - SAE latent-to-token mapping requires manual annotation; automation needed for scaling

- **Failure signatures:**
  - Naive/adversarial prompting near 0% success → model correctly suppresses direct revelation
  - Token forcing pregame 15% → model resists forced completion without context
  - SAE top-1 only 35% Pass@10 → secret latent not always highest-activated

- **First 3 experiments:**
  1. **Reproduce baseline on 5 secret words:** Train Taboo models for 5 new single-token secrets, run Logit Lens and SAE elicitation, verify Pass@10 falls within reported range (35–75%).
  2. **Layer sweep for Logit Lens:** Instead of fixed layer 32, measure secret-word probability across all layers (0–40) for 10 models; identify if a different layer yields higher signal.
  3. **SAE latent automation:** Replace manual latent-to-token mapping with automated top-5 latent interpretation (e.g., by inspecting top-activating dataset examples from the SAE's training corpus) and compare elicitation accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the efficacy of mechanistic interpretability methods scale when eliciting multi-token secrets or abstract concepts compared to single-token secrets?
- Basis in paper: [explicit] The authors state, "Future work should explore more complex model organisms designed to hide, for instance, multi-token secrets, abstract concepts, or hidden objectives."
- Why unresolved: The current study only evaluated the Taboo model using single-token secret words (common nouns), which represents a relatively simple form of hidden knowledge.
- What evidence would resolve it: Successful elicitation of secrets comprising multiple tokens or abstract ideas using the proposed Logit Lens or SAE techniques on new, more complex model organisms.

### Open Question 2
- Question: Are interpretability-based approaches effective at uncovering hidden model objectives in scenarios where black-box methods fail?
- Basis in paper: [explicit] The paper suggests these techniques are promising "particularly in situations where black-box methods may prove insufficient," such as uncovering hidden objectives or reward tampering.
- Why unresolved: The black-box baseline "Another Model" performed well on the simple Taboo task, leaving the specific advantage of white-box methods in complex, active deception scenarios unproven.
- What evidence would resolve it: A comparative study where black-box prompting fails to elicit a hidden objective, but mechanistic interpretability techniques (like SAE feature analysis) successfully identify it.

### Open Question 3
- Question: Do Sparse Autoencoders (SAEs) outperform Logit Lens when eliciting non-verbalizable or complex hidden knowledge?
- Basis in paper: [explicit] The authors speculate that "an SAE-based approach might prove more adaptable to more complex model organisms or types of hidden knowledge" because Logit Lens is naturally suited to single-token predictions.
- Why unresolved: In the current experiments, Logit Lens yielded better results than SAEs, but this may be an artifact of the single-token secret word design.
- What evidence would resolve it: Experiments on model organisms with complex internal states where SAE latents identify critical features that do not correspond directly to vocabulary tokens.

## Limitations

- **Dataset scale uncertainty:** The number of training examples per secret word is not specified, which is critical for reproducibility and understanding model convergence.
- **Manual SAE interpretation bottleneck:** The method relies on manual annotation to map SAE latents to semantic tokens, creating a scalability barrier for practical applications.
- **Single-token secret limitation:** Using only single-token nouns significantly constrains the realism of the model organism and doesn't reflect real-world hidden knowledge scenarios.

## Confidence

**High confidence:** Black-box methods (token forcing post-game, multi-prompt adversarial approaches) achieving 70%+ success rates. The mechanism (warm-up effect strengthening internal representations) is clearly demonstrated through controlled comparisons between pregame and postgame conditions.

**Medium confidence:** Logit Lens effectiveness at layer 32 for secret word probability spikes. While Figure 2 shows clear peaks, the paper doesn't explore whether these peaks are consistent across all secrets or if some secrets show weaker signals. The claim that mid-layer representations remain intact needs more systematic validation.

**Low confidence:** SAE-based elicitation scalability. Manual mapping works for 20 words, but the paper provides no evidence this approach scales to thousands of concepts or handles polysemous words effectively. The 35% Pass@10 rate (top-1) suggests significant limitations.

## Next Checks

1. **Layer sensitivity analysis:** Systematically sweep across all transformer layers (0-40) for 10 different secret words to identify optimal layers for Logit Lens elicitation. Plot secret-word probability vs. layer to verify the reported 30-37 peak is consistent and optimal.

2. **Automated SAE interpretation:** Develop and validate an automated method for mapping SAE latents to semantic concepts by analyzing the top-activating dataset examples from the SAE's training corpus. Compare automated vs. manual mapping accuracy for 50+ words to assess scalability.

3. **Multi-token secret extension:** Repeat the entire experimental pipeline (training, evaluation) with 10 secret words that are multi-token phrases (e.g., "happy face", "secret code"). Measure whether Logit Lens and SAE methods still detect the complete concept or only partial elements.