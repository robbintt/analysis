---
ver: rpa2
title: 'Lyra: An Efficient and Expressive Subquadratic Architecture for Modeling Biological
  Sequences'
arxiv_id: '2503.16351'
source_url: https://arxiv.org/abs/2503.16351
tags:
- lyra
- sequence
- self
- prediction
- protein
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Lyra is a subquadratic deep learning architecture for biological\
  \ sequence modeling that leverages the mathematical structure of epistasis (context-dependent\
  \ interactions between sequence elements) to efficiently capture complex sequence-to-function\
  \ relationships. The model combines projected gated convolutions for local feature\
  \ extraction with state space models (specifically S4D) for long-range dependencies,\
  \ achieving subquadratic O(N log N) scaling compared to the O(N\xB2) complexity\
  \ of Transformers."
---

# Lyra: An Efficient and Expressive Subquadratic Architecture for Modeling Biological Sequences

## Quick Facts
- **arXiv ID**: 2503.16351
- **Source URL**: https://arxiv.org/abs/2503.16351
- **Reference count**: 3
- **One-line primary result**: Lyra achieves state-of-the-art performance on over 100 biological sequence tasks while using up to 120,000-fold fewer parameters and training in under two hours on just two GPUs

## Executive Summary
Lyra is a subquadratic deep learning architecture that challenges the prevailing trend toward increasingly large models by demonstrating that mathematically-principled design can outperform Transformers in biological sequence modeling. The model leverages the mathematical structure of epistasis—context-dependent interactions between sequence elements—by combining projected gated convolutions for local feature extraction with state space models (specifically S4D) for long-range dependencies. This hybrid approach achieves O(N log N) scaling compared to Transformers' O(N²) complexity, enabling efficient processing of sequences up to 65,536 in length. Lyra demonstrates superior performance across diverse biological tasks including protein fitness prediction, RNA function analysis, CRISPR guide design, and biophysical property prediction while requiring dramatically fewer computational resources.

## Method Summary
Lyra combines Projected Gated Convolutions (PGC) with State Space Models (S4D) to create a subquadratic architecture for biological sequence modeling. The PGC layers use depthwise convolutions combined with linear projections through element-wise multiplication to capture local epistatic effects and second-order interactions. The S4D layer then processes these features globally, using diagonal state matrices to generate convolution kernels that are efficiently computed via FFT in O(N log N) time. The architecture processes one-hot encoded sequences through an encoder, multiple PGC blocks, the S4D layer, and a decoder with average pooling and linear transformation. Training uses AdamW optimizer with learning rate 0.001 and weight decay 0.01 for 100 epochs, with model sizes ranging from ~55k to 3.6M parameters depending on the task.

## Key Results
- Lyra outperforms existing approaches across over 100 biological tasks while using up to 120,000-fold fewer parameters than foundation models
- Achieves state-of-the-art performance in diverse areas including protein fitness prediction, RNA function analysis, and CRISPR guide design
- Processes sequences up to 65,536 in length with 8.3x faster inference than Hyena while maintaining competitive accuracy
- Trains in under two hours on just two GPUs, demonstrating dramatic efficiency gains over Transformer-based approaches

## Why This Works (Mechanism)

### Mechanism 1: Polynomial Epistasis Approximation
State Space Models capture higher-order epistatic interactions by implicitly approximating multilinear polynomials through their structured state space representation. The S4D layer parameterizes convolution kernels via a diagonal A matrix, where the impulse response corresponds to polynomial terms evaluated in the Fourier domain via FFT. This aligns with the mathematical structure of biological epistasis, which primarily decomposes into multilinear polynomial terms.

### Mechanism 2: Local Epistatic Feature Extraction
Projected Gated Convolutions capture local epistatic effects through explicit second-order interactions created by multiplicative gating. The PGC splits input into two pathways—one with depthwise convolution and one with linear projection—combined via Hadamard product to create pairwise dependencies locally before global processing.

### Mechanism 3: Subquadratic Computational Scaling
The hybrid architecture achieves O(N log N) scaling by replacing Transformers' O(N²) attention mechanism with SSM's convolution-based processing. The S4D formulation generates convolution kernels from state matrix parameters, enabling efficient FFT-based inference that removes the memory bottleneck of attention matrices for long sequences.

## Foundational Learning

- **Epistasis**: Context-dependent interactions between sequence elements where a mutation at one position changes the effect of mutations at other positions. Essential for understanding why polynomial approximation captures biological function.
  - *Quick check*: Can you explain why a simple additive model might fail to predict protein fitness?

- **State Space Models & Convolution View**: Discrete SSMs can be viewed as 1D convolutions with specific filters, enabling FFT-based O(N log N) scaling instead of attention's O(N²).
  - *Quick check*: How does viewing RNNs as convolutions enable subquadratic scaling?

- **Vandermonde Matrix**: The S4D kernel parameterization relates to Vandermonde matrices, proving the model's polynomial expressivity through its structured state space.
  - *Quick check*: How does the Vandermonde structure of S4D kernels relate to polynomial approximation?

## Architecture Onboarding

- **Component map**: Input -> Linear projection -> PGC blocks -> Transpose -> S4D -> Transpose -> Average pooling -> Linear decoder
- **Critical path**: The interaction between PGC (local, second-order) and S4D (global, higher-order) is the key design feature. Data flows as Input -> PGC -> Transpose -> S4D -> Transpose -> Pool.
- **Design tradeoffs**: Hidden dimension controls polynomial interaction order (larger = higher capacity but slower); PGC layers increase local interaction order but may over-smooth; assumes static-but-efficient global kernels outweigh dynamic attention routing.
- **Failure signatures**: Degradation on short sequences due to FFT overhead; loss of specific pairwise constraints requiring strict token-based logic.
- **First 3 experiments**:
  1. Synthetic Polynomial Regression: Train Lyra vs. Transformer on synthetic 5th-degree polynomial function to verify polynomial approximation hypothesis.
  2. Ablation Study (PGC vs S4D): Run on GFP dataset using only PGC, only S4D, and full stack to quantify local vs. global contributions.
  3. Long-Sequence Scaling: Benchmark memory and latency on sequence lengths 1k, 4k, 16k, 65k against Transformer baseline to confirm O(N log N) scaling.

## Open Questions the Paper Calls Out
- Can the mathematical framework be generalized to non-biological domains with polynomial-like interactions?
- Would a pre-trained Lyra foundation model at billions of parameters retain efficiency while matching Transformer emergent capabilities?
- Does polynomial approximation for epistasis introduce failure modes where non-polynomial or extremely high-order dependencies dominate?

## Limitations
- The polynomial epistasis approximation relies on assumptions about biological function structure that may not hold for all interaction types
- The specific contribution of PGC gating mechanisms to performance is under-characterized without ablation studies
- Empirical validation of the theoretical polynomial-expressivity link is limited to a narrow set of benchmark tasks

## Confidence

| Claim | Confidence Level | Rationale |
|-------|------------------|-----------|
| Subquadratic scaling (O(N log N)) | High | Well-supported by FFT implementation and benchmarked across sequence lengths |
| Polynomial epistasis approximation | Medium | Mathematically sound but limited empirical validation across diverse interaction types |
| PGC gating mechanism contribution | Low | Described but not rigorously validated through ablation studies |

## Next Checks

1. **Synthetic Epistasis Validation**: Design synthetic sequence datasets with known polynomial and non-polynomial interaction structures to test whether Lyra's performance degrades predictably on non-polynomial epistasis.

2. **PGC Ablation Study**: Run controlled experiments on GFP fitness prediction comparing full Lyra, Lyra without PGC, Lyra with standard convolutions, and Lyra with additive gating to isolate gating mechanism contribution.

3. **Hidden Dimension Scaling Analysis**: Systematically vary S4D hidden dimension (N=16, 32, 64, 128) across multiple tasks to identify performance plateaus and validate polynomial approximation hypothesis.