---
ver: rpa2
title: Aligning LLMs by Predicting Preferences from User Writing Samples
arxiv_id: '2505.23815'
source_url: https://arxiv.org/abs/2505.23815
tags:
- preference
- user
- preferences
- prose
- writing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "PROSE is a method for aligning LLM agents with user preferences\
  \ by iteratively inferring and refining preference descriptions from user writing\
  \ samples. It improves over prior work by using iterative refinement\u2014generating\
  \ candidate outputs, comparing them to user demonstrations, and updating the preference\
  \ description\u2014and consistency verification\u2014pruning preference components\
  \ that don't generalize across multiple user examples."
---

# Aligning LLMs by Predicting Preferences from User Writing Samples

## Quick Facts
- arXiv ID: 2505.23815
- Source URL: https://arxiv.org/abs/2505.23815
- Authors: Stéphane Aroca-Ouellette; Natalie Mackraz; Barry-John Theobald; Katherine Metcalf
- Reference count: 40
- Key outcome: PROSE improves LLM alignment with user preferences by 33% over CIPHER using iterative refinement and consistency verification

## Executive Summary
PROSE is a method for aligning LLM agents with user preferences by iteratively inferring and refining preference descriptions from user writing samples. It improves over prior work by using iterative refinement—generating candidate outputs, comparing them to user demonstrations, and updating the preference description—and consistency verification—pruning preference components that don't generalize across multiple user examples. Evaluated on the PLUME benchmark with four LLMs across summarization and email tasks, PROSE outperforms CIPHER by 33% in generation quality and can be combined with in-context learning for up to 9% additional gains. The approach yields interpretable preference descriptions and reduces prompt tokens by 90% compared to in-context learning, while also scaling better with LLM capability.

## Method Summary
PROSE aligns LLMs to user preferences through iterative refinement and consistency verification. The method takes user writing demonstrations as input and infers preference descriptions through an LLM loop that generates text, compares it to user demos, and updates the description until alignment is achieved. A consistency verification step then decomposes the description into components and prunes those that don't generalize across multiple demonstrations. The final preference description is stored in interaction memory and used to condition future generations. PROSE is evaluated on the PLUME benchmark using GPT-4o as a synthetic human to generate demonstrations based on specific preference sets.

## Key Results
- PROSE outperforms CIPHER by 33% in generation quality across four LLMs on summarization and email tasks
- Combining PROSE with in-context learning provides up to 9% additional improvement over ICL alone
- PROSE reduces prompt tokens by 90% compared to in-context learning while scaling better with LLM capability
- Consistency verification provides a modest but consistent 1.5% and 1.7% improvement in performance

## Why This Works (Mechanism)

### Mechanism 1: Iterative Refinement of Preference Descriptions
The accuracy of an inferred user preference description is improved by iteratively generating text based on it, comparing the output to a user's demonstration, and updating the description to close the gap. The system alternates between (1) generating a writing sample conditioned on the current preference description and (2) prompting an LLM to compare its generation to the user's demonstration and propose a refined description. This loop continues until the generations align or a step limit is reached.

### Mechanism 2: Consistency Verification via Component Pruning
Decomposing a preference description into individual components and verifying each against multiple user demonstrations improves robustness by eliminating spurious or overfit inferences. After refinement, a "Breakdown Prompt" converts the description into a list of components (e.g., "use alliterations"). A "Consistency Verification Prompt" then uses an LLM-as-a-Judge to score each component against all relevant user demonstrations. Components that do not consistently score above a threshold are discarded.

### Mechanism 3: Complementarity of Explicit Descriptions and In-Context Learning
Explicitly inferred preference descriptions and few-shot in-context learning (ICL) are complementary, as they capture different types of information. ICL excels at capturing strong structural preferences (e.g., "write in the style of a tweet") by providing concrete examples to copy. PROSE excels at capturing nuanced, abstract preferences about tone (e.g., "be sharply critical") which are hard to learn from examples alone. Their combination yields higher performance.

## Foundational Learning

- **LLM-as-a-Judge**: PROSE's core verification loop and the PLUME benchmark rely on using a strong LLM (GPT-4o) to evaluate stylistic compliance, which is more scalable than human review. *Quick check: What are the primary failure modes of using an LLM to evaluate another model's output, and how might they affect the consistency verification step?*
- **In-Context Learning (ICL)**: This is a primary baseline. Understanding its token-inefficiency and difficulty with abstract concepts vs. its strength in structural mimicry is key to PROSE's design rationale. *Quick check: Why might an LLM fail to learn a nuanced preference like "be sharply critical" solely from a few examples, and how does an explicit description help?*
- **Preference Alignment vs. Personalization**: Situates PROSE within the broader field. PROSE moves beyond generic "helpful/harmless" alignment to fine-grained, user-specific alignment. *Quick check: How does PROSE's approach to alignment (learning from demonstrations) differ fundamentally from RLHF (learning from rankings)?*

## Architecture Onboarding

- **Component map**: Interaction Memory -> Aggregation Module -> Iterative Refiner -> Consistency Verifier -> Store in Memory
- **Critical path**: User provides demo -> Aggregation -> Initial Generation -> **Refinement Loop** -> **Consistency Verification** -> Store final components in Memory
- **Design tradeoffs**: The system trades higher computational cost (multiple LLM calls for refinement) for lower inference-time token cost and higher interpretability. It also trades potential over-specificity for robustness via the pruning step.
- **Failure signatures**: 1) **Refinement Divergence**: The loop changes the description each time without the generated text getting closer to the user's. 2) **Judge Misalignment**: The LLM-as-a-Judge is inconsistent, causing it to prune correct preferences or retain incorrect ones. 3) **Generic Output**: The final inferred preferences are too vague to be useful.
- **First 3 experiments**:
  1. **Ablation Test**: Run the full PLUME benchmark, systematically disabling the iterative refinement step, then the consistency verification step, to isolate their individual contributions to the PPCM score.
  2. **Token vs. Performance Trade-off**: Measure the total number of prompt tokens used by PROSE (including refinement) vs. ICL and baseline CIPHER to validate the claimed 90% reduction.
  3. **Synergy Test**: Implement the PROSE+ICL hybrid agent and compare its performance on a subset of PLUME tasks (especially those with strong structural preferences like "Chat Forum Posts") against ICL alone.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can PROSE's token efficiency during preference inference be substantially improved while maintaining generation quality? The paper notes PROSE Full used 5.87x (prompt) and 6.07x (generated) more tokens on average than PROSECE, and reducing tokens while retaining performance is an important area for improvement.

- **Open Question 2**: How does PROSE perform in real-world human user studies beyond synthetic proxy evaluations? The paper states a full-scale human trial would provide greater understanding of the benefits and limitations of the proposed method.

- **Open Question 3**: How does the ordering of preference components affect generation quality and preference inference accuracy? The paper found that sorting preference components by length led to an 11% performance drop relative to keeping the LLM's order.

- **Open Question 4**: Can the consistency verification step be redesigned to provide larger improvements in preference quality and generation compliance? The paper notes consistency verification yields only modest improvements despite being a core contribution.

## Limitations
- Reliance on LLM-as-a-Judge introduces potential bias and inconsistency in both verification and evaluation
- Assumes user preferences are stable across demonstrations, which may not hold for all users
- Synthetic human approach using GPT-4o may not fully capture real human preference complexity

## Confidence
- **High Confidence**: Iterative refinement mechanism is well-grounded and produces interpretable preference descriptions; 33% performance improvement over CIPHER is well-supported
- **Medium Confidence**: Consistency verification effectiveness depends heavily on LLM-as-a-Judge reliability; 90% token reduction claim assumes specific comparison conditions
- **Low Confidence**: Synthetic human evaluation's ability to generalize to real human preferences remains unproven; computational efficiency trade-offs need more thorough analysis

## Next Checks
1. **LLM-as-a-Judge Reliability Test**: Conduct human evaluation studies to validate the correlation between LLM-as-a-Judge scores and actual human preference judgments across the PLUME benchmark tasks, particularly for subtle stylistic features.

2. **Real User Preference Validation**: Deploy PROSE with actual human users rather than synthetic GPT-4o demonstrations to assess whether the iterative refinement and consistency verification mechanisms work as effectively with genuine user writing samples.

3. **Computational Cost Analysis**: Measure and compare the total computational resources required for PROSE (including all refinement iterations) versus ICL and CIPHER across multiple tasks to verify the claimed efficiency advantages and identify practical deployment constraints.