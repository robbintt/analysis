---
ver: rpa2
title: Membership and Memorization in LLM Knowledge Distillation
arxiv_id: '2508.07054'
source_url: https://arxiv.org/abs/2508.07054
tags:
- privacy
- min-k
- data
- teacher
- membership
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Knowledge distillation (KD) is widely used to compress large language\
  \ models (LLMs) by transferring knowledge from a large teacher to a smaller student\
  \ model, often to reduce computational costs and improve deployment efficiency.\
  \ However, the privacy risks of KD\u2014particularly membership inference and memorization\
  \ leakage from teacher training data\u2014have not been thoroughly quantified."
---

# Membership and Memorization in LLM Knowledge Distillation

## Quick Facts
- arXiv ID: 2508.07054
- Source URL: https://arxiv.org/abs/2508.07054
- Reference count: 40
- Six state-of-the-art KD techniques on three LLM families across seven NLP tasks show significant membership and memorization privacy risks

## Executive Summary
Knowledge distillation (KD) is widely used to compress large language models (LLMs) by transferring knowledge from a large teacher to a smaller student model. However, this study systematically evaluates six state-of-the-art KD techniques and reveals significant membership inference and memorization privacy risks from teacher training data. The research demonstrates that all KD techniques carry significant privacy risks, with varying levels across techniques, and that memorization and membership risks show low agreement. These findings highlight the need for privacy-aware KD designs in LLM deployment.

## Method Summary
The study systematically evaluates six KD techniques (KD, SeqKD, GKD, ImitKD, MiniLLM, DistiLLM) on three LLM families (GPT-2, OPT, LLAMA-2) across seven NLP tasks. Seven membership inference attacks and data reconstruction methods are employed to quantify privacy risks. The evaluation examines both membership inference (detecting if a sample was in the teacher's training set) and memorization (reconstructing training data) across different KD components, model blocks, and student sizes. The analysis includes both black-box and white-box attack scenarios, with comprehensive comparison of privacy leakage patterns.

## Key Results
- All KD techniques carry significant membership and memorization privacy risks, with AUC scores for membership inference exceeding 0.70 in many cases
- Students can memorize 11.35% of samples the teacher memorizes, but membership and memorization risks show low agreement
- Privacy risks vary significantly across model blocks and KD components, with decreasing student size reducing privacy leakage at the cost of utility
- Reverse KL divergence reduces privacy leakage compared to Forward KL, and student-generated data increases risk

## Why This Works (Mechanism)
Knowledge distillation transfers knowledge from teacher to student models through loss functions that compare output distributions. This process inherently leaks information about the teacher's training data, as the student learns not just general patterns but also specific examples the teacher memorized. The mechanism involves matching probability distributions between teacher and student, where sensitive information about individual training samples becomes embedded in the student's parameters. Different KD techniques vary in their vulnerability due to differences in loss functions, data augmentation strategies, and the degree to which they rely on student-generated samples.

## Foundational Learning

**Knowledge Distillation (KD)**: A compression technique where a small student model learns from a large teacher model's output distributions. Why needed: Forms the basis of the privacy vulnerability being studied. Quick check: Student model achieves competitive performance to teacher on target task.

**Membership Inference Attacks (MIA)**: Methods to determine if a specific sample was part of a model's training data. Why needed: Core privacy risk being quantified. Quick check: Attack achieves AUC > 0.5 (better than random guessing).

**Model Memorization**: The phenomenon where models retain and can reproduce specific training examples. Why needed: Represents another dimension of privacy leakage beyond membership inference. Quick check: Reconstructed samples match original training data with high similarity scores.

**KL Divergence Loss**: A loss function measuring the difference between teacher and student probability distributions. Why needed: The choice of loss function significantly impacts privacy leakage. Quick check: Lower KL divergence correlates with better performance but potentially higher privacy risk.

**Transformer Block Structure**: The layered architecture of LLMs where different blocks may have varying privacy vulnerabilities. Why needed: Privacy risks are not uniform across model components. Quick check: Privacy metrics vary across different transformer blocks.

## Architecture Onboarding

Component map: Teacher Model -> KD Technique -> Student Model -> Privacy Attack

Critical path: Teacher training data -> Teacher model parameters -> KD loss computation -> Student model training -> Privacy vulnerability

Design tradeoffs: Larger student models achieve better utility but increase privacy leakage; different KD techniques balance utility and privacy differently; loss function choice affects both performance and privacy risk.

Failure signatures: High MIA AUC scores (>0.7) indicate significant membership leakage; successful data reconstruction indicates memorization; low agreement between membership and memorization attacks suggests different vulnerability patterns.

First experiments:
1. Compare MIA AUC scores across different KD techniques on the same teacher-student pair
2. Measure memorization percentage when varying student model size
3. Test the impact of different loss functions (Forward KL vs Reverse KL) on privacy leakage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can initializing student models with specific low-risk transformer blocks (identified via per-block analysis) effectively mitigate privacy leakage without significant utility loss?
- Basis in paper: [explicit] Section 9 (Limitations) states, "This paper doesn't provide defense solutions to mitigate privacy risks, such as selecting less vulnerable blocks to initialize the student model. We also leave the defense as part of the future work."
- Why unresolved: The authors identified varying privacy risks across transformer blocks but did not experiment with selective block initialization or pruning strategies to test if the low-risk blocks can form a robust, privacy-preserving student.
- What evidence would resolve it: Empirical results showing that students initialized with low-risk blocks (e.g., shallow blocks in GPT-2) maintain competitive Rouge-L scores while significantly reducing MIA AUC compared to standard full-model distillation.

### Open Question 2
- Question: Can theoretical bounds be established to rigorously quantify membership privacy leakage in knowledge distillation, particularly regarding the choice of divergence objective?
- Basis in paper: [explicit] Section 9 notes, "Theoretical analysis would be a potent supplement to this paper. We leave the theoretical analysis of the privacy leakage in KD as a future work."
- Why unresolved: The study relies entirely on empirical observation (e.g., Reverse KL reduces leakage compared to KL) but lacks a formal mathematical framework to explain why certain loss functions inherently leak more membership information.
- What evidence would resolve it: A derivation of privacy bounds or information-theoretic inequalities that correlate with the observed empirical data, specifically explaining the privacy-utility trade-off of Reverse KL vs. Forward KL divergence.

### Open Question 3
- Question: Can white-box membership inference attacks be refined to overcome hyper-parameter sensitivity and consistently outperform black-box attacks in LLM distillation settings?
- Basis in paper: [explicit] In Section 4.1, regarding the poor performance of the white-box MoPe attack on GPT-2, the authors state: "This observation opens an avenue for future research into improving white-box MIAs... to be competitive with or outperform black-box MIAs."
- Why unresolved: The white-box attack (MoPe) performed significantly worse (AUC 0.60) than black-box attacks (AUC > 0.90), likely due to hyper-parameter sensitivity in the new model context, indicating the attack methodology is currently suboptimal.
- What evidence would resolve it: A modified white-box attack algorithm that is robust to hyper-parameter choices and achieves AUC scores statistically significantly higher than reference-based black-box methods (like StableLM) across the GPT-2 and OPT model families.

## Limitations
- Study focuses on seven specific attacks and three model families, which may not capture all real-world KD applications
- Does not explore the impact of different training dataset distributions or the interaction between KD and fine-tuning
- The trade-off between student size, utility, and privacy leakage is demonstrated but not fully characterized across different task types

## Confidence
High: Systematic evaluation across six KD techniques, three LLM families, and seven attacks provides strong evidence for significant membership and memorization risks
Medium: Identification of important factors (loss functions, student-generated data, model blocks) but causal relationships across broader settings remain to be established
Medium: Recommendations for privacy-aware KD design are well founded, but optimal strategies for balancing utility and privacy are not fully resolved

## Next Checks
1. Evaluate the privacy-utility trade-off across a wider range of dataset sizes and distributions, particularly for tasks with different levels of memorization difficulty
2. Test the robustness of membership and memorization risks under realistic fine-tuning scenarios and against adaptive, black-box attacks not considered in this study
3. Investigate the effectiveness of proposed privacy-preserving KD modifications (e.g., differential privacy, gradient clipping) in mitigating both membership and memorization risks without significant utility loss