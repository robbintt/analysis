---
ver: rpa2
title: Distributed Value Decomposition Networks with Networked Agents
arxiv_id: '2502.07635'
source_url: https://arxiv.org/abs/2502.07635
tags:
- agents
- dvdn
- learning
- consensus
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces distributed value decomposition networks (DVDN)
  for decentralized training in multi-agent reinforcement learning. DVDN uses peer-to-peer
  communication to approximate joint temporal differences (JTD) from local temporal
  differences, enabling agents to learn without centralized training.
---

# Distributed Value Decomposition Networks with Networked Agents

## Quick Facts
- **arXiv ID**: 2502.07635
- **Source URL**: https://arxiv.org/abs/2502.07635
- **Reference count**: 40
- **Primary result**: DVDN matches or outperforms VDN in 6 of 10 heterogeneous-agent scenarios and outperforms IQL in 5 of 10 homogeneous-agent scenarios

## Executive Summary
This paper introduces Distributed Value Decomposition Networks (DVDN) for decentralized training in multi-agent reinforcement learning. DVDN enables agents to learn without centralized training by using peer-to-peer communication to approximate joint temporal differences (JTD) from local temporal differences. For homogeneous agents, the approach incorporates gradient tracking to align policies and improve sample efficiency. The framework is evaluated across ten tasks spanning three environments, demonstrating performance comparable to centralized methods while maintaining the benefits of decentralization.

## Method Summary
DVDN addresses the challenge of decentralized training in cooperative multi-agent reinforcement learning where centralized training is infeasible. The method uses peer-to-peer communication to approximate joint temporal differences (JTD) from local temporal differences, enabling agents to perform local updates that approximate centralized training. For homogeneous agents, gradient tracking is applied to align policies and improve sample efficiency. The framework operates under a switching topology regime where agents communicate over random strongly connected graphs sampled at each episode. The core innovation is that agents can collectively approximate the centralized training signal through localized consensus updates rather than relying on a central trainer.

## Key Results
- DVDN matches or outperforms value decomposition networks (VDN) in 6 of 10 heterogeneous-agent scenarios
- DVDN with gradient tracking outperforms independent Q-learning (IQL) in 5 of 10 homogeneous-agent scenarios
- DVDN with gradient tracking shows improved sample efficiency in homogeneous settings compared to heterogeneous ones
- Performance degrades when gradient tracking is applied to heterogeneous agent teams

## Why This Works (Mechanism)

### Mechanism 1: Joint Temporal Difference (JTD) Consensus
DVDN enables decentralized agents to approximate the centralized training signal (JTD) via peer-to-peer communication. Instead of a central server summing individual TD errors, agents exchange local TD errors with neighbors using consensus updates (Metropolis weights) to iteratively compute a localized approximation of the JTD. This estimated JTD serves as the surrogate target for local backpropagation. The approach assumes the communication graph must be strongly connected at each iteration to ensure information propagates across the network.

### Mechanism 2: Gradient Tracking (GT) for Parameter Alignment
For homogeneous agents, tracking the team gradient allows agents to align their policy parameters, emulating parameter sharing and improving sample efficiency without centralized broadcasting. Agents maintain an auxiliary variable that tracks the average gradient across the network, communicating this along with weight updates to minimize a global loss function formed by the sum of local losses. This is integrated with the Adam optimizer (GTAdam).

### Mechanism 3: Switching Topology Communication
The system maintains robustness despite unpredictable communication failures by using consensus over time-varying undirected graphs. Agents operate under a Hybrid POMDP framework where the edge set varies at each time step. The consensus weights are recalculated locally based on neighbor degrees (Metropolis weights) at every communication round, ensuring adaptability to changing network conditions.

## Foundational Learning

- **Concept**: Value Decomposition Networks (VDN)
  - Why needed here: DVDN is a distributed modification of VDN. Understanding that VDN decomposes the joint Q-function $Q_{tot} = \sum Q_i$ is necessary to understand why DVDN targets the sum of TD errors (JTD).
  - Quick check question: How does the gradient of the VDN loss function change if the summation layer is removed?

- **Concept**: Consensus Algorithms (Gossip/Averaging)
  - Why needed here: The core operation of DVDN is consensus. You must understand how nodes agree on a value (Metropolis weights) to implement the communication logic.
  - Quick check question: If an agent has 3 neighbors, how does it calculate its weight for itself vs. its neighbors to ensure the network average converges?

- **Concept**: Temporal Difference (TD) Learning
  - Why needed here: The specific signal being communicated is the TD error. Distinguishing between local TD and Joint TD is the central innovation.
  - Quick check question: Why is the TD error a sufficient statistic to communicate for this algorithm, rather than sending raw rewards or full trajectories?

## Architecture Onboarding

- **Component map**: Local Q-Networks -> Consensus Module -> Communication Channel -> Optimizer
- **Critical path**: 
  1. Agents collect trajectory locally
  2. Compute local TD error $\delta_i$
  3. Synchronize: Exchange $\delta_i$ (and $z_i/\omega_i$ for GT) with neighbors
  4. Compute $\hat{\delta}_{-i}$ (estimated JTD contribution from network)
  5. Update local network using loss $\ell(\omega_i; \tau_i, \hat{\delta}_{-i})$

- **Design tradeoffs**:
  - Truncation vs. Accuracy: The paper performs only one consensus step per mini-batch to save bandwidth/latency, trading off perfect JTD accuracy for speed
  - Heterogeneity vs. Efficiency: DVDN (GT) is sample efficient for homogeneous agents but fails on heterogeneous ones; use base DVDN for mixed teams

- **Failure signatures**:
  - Oscillating returns: Likely due to incorrect weight initialization or learning rates in the GT module causing divergence
  - Collapse to IQL performance: Suggests the communication channel is effectively broken (e.g., weights calculated incorrectly) or neighbors are not updating

- **First 3 experiments**:
  1. Static Topology Test: Run DVDN on a fully connected static graph. It should closely match centralized VDN performance (Upper Bound).
  2. Switching Topology Test: Introduce the random strongly connected graph sampling. Verify performance doesn't degrade significantly compared to the static test.
  3. Heterogeneity Ablation: Run DVDN (GT) on a heterogeneous task (e.g., MARBLER Arctic). Verify that disabling GT (switching to base DVDN) improves results, confirming the mechanism's dependency on agent homogeneity.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the DVDN framework be successfully extended to monotonic value factorization methods like QMIX?
  - Basis in paper: Section 7 (Conclusion) explicitly states the intention for "the extension of other families of factorized Q-function such as QMIX [27] to the distributed setting."
  - Why unresolved: QMIX enforces monotonicity constraints via a mixing network that typically requires global state information, which is unavailable in the decentralized training setting proposed for DVDN.
  - What evidence would resolve it: The implementation of a distributed QMIX variant that enforces monotonicity via peer-to-peer communication, demonstrating performance comparable to centralized QMIX.

- **Open Question 2**: How does incorporating belief sharing into DVDN affect performance in partially observable environments?
  - Basis in paper: Section 7 lists "incorporating belief sharing to DVDN" as a primary direction for future work.
  - Why unresolved: Current DVDN agents rely on local trajectories and communicated temporal differences; sharing beliefs (probability distributions over states) could improve coordination but introduces new communication overhead and complexity.
  - What evidence would resolve it: Empirical results from a modified DVDN agent that communicates belief states, showing improved sample efficiency or episodic returns in Dec-POMDPs compared to the standard DVDN.

## Limitations

- DVDN's effectiveness is constrained by the requirement for strongly connected communication graphs
- Performance on heterogeneous tasks with gradient tracking is explicitly noted as degraded
- The paper performs only one consensus step per mini-batch, trading accuracy for bandwidth efficiency
- Critical hyperparameters like batch size and replay buffer size are not explicitly defined in the hyperparameter tables

## Confidence

- **High Confidence**: The core mechanism of approximating Joint Temporal Differences (JTD) via peer-to-peer consensus is well-supported by the theoretical framework and experimental results showing DVDN matches or outperforms VDN in 6 of 10 heterogeneous-agent scenarios.
- **Medium Confidence**: The Gradient Tracking (GT) mechanism for homogeneous agents is theoretically sound and shows improved sample efficiency in LBF scenarios, but its application is limited to homogeneous settings.
- **Medium Confidence**: The switching topology communication framework is a valid approach for handling unpredictable communication failures, as evidenced by the experimental setup using random strongly connected graphs.

## Next Checks

1. **Static vs. Switching Topology Test**: Run DVDN on a fully connected static graph and compare performance to the switching topology version to validate the impact of the communication graph variability.
2. **Heterogeneity Ablation Study**: Implement DVDN (GT) on a heterogeneous task (e.g., MARBLER Arctic) and verify that disabling GT (switching to base DVDN) improves results, confirming the mechanism's dependency on agent homogeneity.
3. **Consensus Step Ablation**: Vary the number of consensus steps per mini-batch (e.g., 1, 3, 5) to empirically assess the trade-off between JTD approximation accuracy and computational overhead, validating the truncation choice.