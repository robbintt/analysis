---
ver: rpa2
title: 'COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability'
arxiv_id: '2510.04196'
source_url: https://arxiv.org/abs/2510.04196
tags:
- safety
- content
- reasoning
- multimodal
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents COSMO-RL, a reinforcement learning framework
  for training Large Multimodal Reasoning Models (LMRMs) that jointly optimizes safety,
  value, and general reasoning capability. The method addresses the challenge of safety
  alignment in multimodal settings by integrating staged training, policy stabilization
  via Clipped Policy Gradient with Policy Drift (CPGD), and multimodal jailbreak data
  augmentation into a unified pipeline.
---

# COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability

## Quick Facts
- arXiv ID: 2510.04196
- Source URL: https://arxiv.org/abs/2510.04196
- Authors: Yizhuo Ding; Mingkang Chen; Qiuhua Liu; Fenghua Weng; Wanying Qu; Yue Yang; Yugang Jiang; Zuxuan Wu; Yanwei Fu; Wenqi Shao
- Reference count: 25
- Key outcome: COSMO-R1 achieves strong safety improvements (+20.5% on MM-SafetyBench), competitive value alignment (+26.2% on FLAMES), and general reasoning gains (+7.7% average) through a reinforcement learning framework that jointly optimizes safety, value, and capability while maintaining robustness against multimodal jailbreak attacks.

## Executive Summary
COSMO-RL introduces a reinforcement learning framework for Large Multimodal Reasoning Models (LMRMs) that addresses the challenge of simultaneously optimizing safety, value alignment, and general reasoning capability. The approach uses a two-stage training process with policy stabilization via Clipped Policy Gradient with Policy Drift (CPGD) and multimodal jailbreak data augmentation. COSMO-R1, trained with this framework, demonstrates that safety and capability can co-evolve effectively, achieving strong performance across multiple benchmarks while maintaining robustness against adversarial attacks. The framework generalizes across model sizes and architectures, offering a practical path toward trustworthy LMRMs.

## Method Summary
COSMO-RL employs a staged reinforcement learning approach with three core components: staged training (Stage 1 for general reasoning, Stage 2 for joint safety-value-capability optimization), CPGD for policy stabilization through clipped log-probability ratios and KL penalties, and multimodal jailbreak data augmentation using textual paraphrasing and visual key element extraction. The framework uses separate reward models for safety, value, and knowledge, combined through weighted scalar combination. Training begins with supervised fine-tuning to establish CoT reasoning, then progresses through two RL stages to balance the competing objectives while preventing policy drift and safety forgetting.

## Key Results
- Safety improvement: +20.5% on MM-SafetyBench, +21.8% average across safety benchmarks
- Value alignment: +26.2% on FLAMES, demonstrating strong performance in value-aligned responses
- General reasoning: +7.7% average improvement across MMMU, MathVista, Olympiad, GPQA Diamond, and GAOKAO-MM
- Jailbreak robustness: Maintained performance against multimodal jailbreak attacks while improving safety metrics
- Cross-architecture generalization: Framework successfully applied to 7B parameter models with similar improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage training prevents safety forgetting while enabling capability gains
- Mechanism: Stage 1 builds general reasoning capacity; Stage 2 jointly optimizes safety, value, and capability together. This sequencing prevents early safety training from being overwritten by later complex-task training.
- Core assumption: Capability and safety objectives are not inherently opposed when co-optimized, but staged sequencing matters for retention.
- Evidence anchors:
  - [abstract] "Our approach aims to let safety and capability grow together in one stable pipeline rather than competing during alignment"
  - [section 3.3] "A common failure mode is safety forgetting after further training on complex tasks. Conversely, stronger general capability can support safer and more value-aligned behavior"
  - [corpus] Related work "SafeWork-R1" uses similar progressive safety-oriented RL, suggesting converging evidence for staged approaches
- Break condition: If Stage 1 training is too long or aggressive, the model may overfit to capability tasks, making Stage 2 safety integration ineffective.

### Mechanism 2
- Claim: CPGD (Clipped Policy Gradient with Policy Drift) stabilizes multiobjective RL by preventing policy drift
- Mechanism: CPGD clips log-probability ratios and adds a KL-divergence penalty, constraining how far the policy can move from the previous iteration. This prevents reward hacking, mode collapse, and erratic behavior when optimizing competing objectives.
- Core assumption: Policy stability directly enables better safety-capability trade-offs by allowing gradual, controlled updates rather than catastrophic shifts.
- Evidence anchors:
  - [abstract] "Policy stabilization via Clipped Policy Gradient with Policy Drift (CPGD)"
  - [section 3.2] "CPGD maximizes L_CPGD(θ; θ_old) = E[Φ_θ(x,y)] - α·D_KL(π_old || π_θ)"
  - [corpus] No direct corpus comparison to CPGD specifically; mechanism appears novel in this multimodal context
- Break condition: If the clipping threshold (ε) is too tight, the policy cannot adapt; if too loose, instability returns.

### Mechanism 3
- Claim: Multimodal jailbreak data augmentation builds adversarial robustness through exposure
- Mechanism: Textual jailbreaks use paraphrasing/obfuscation; visual jailbreaks extract risk-relevant visual elements with GPT-4o. Training directly on these examples teaches the model to recognize and resist real-world attack patterns rather than only evaluating on them post-hoc.
- Core assumption: Robustness to adversarial inputs is learned, not emergent, and requires explicit adversarial exposure during training.
- Evidence anchors:
  - [abstract] "multimodal jailbreak data augmentation into a unified pipeline"
  - [section 3.5] "To improve robustness against text-only jailbreaks, unsafe prompts are rewritten via paraphrasing and obfuscation"
  - [corpus] "VLSU" paper notes that safety evaluation often treats modalities separately, missing joint risks—supporting the need for multimodal-specific augmentation
- Break condition: If augmentation is too narrow, the model overfits to specific attack patterns and fails on novel jailbreaks.

## Foundational Learning

- Concept: **Reinforcement Learning from Human Feedback (RLHF)**
  - Why needed here: COSMO-RL extends RLHF to multimodal, multiobjective settings. Understanding basic RLHF (reward modeling, policy optimization, KL penalties) is prerequisite.
  - Quick check question: Can you explain why RLHF uses a KL-divergence penalty rather than unconstrained reward maximization?

- Concept: **Policy Gradient Methods (PPO, GRPO, REINFORCE)**
  - Why needed here: CPGD builds on and compares against these methods. Understanding clipping, advantage estimation, and policy stability is essential.
  - Quick check question: What problem does PPO's clipping objective solve compared to vanilla policy gradient?

- Concept: **Multimodal Safety Threats (Jailbreaks, Cross-Modal Attacks)**
  - Why needed here: The paper addresses multimodal-specific risks like image-text combinations bypassing guardrails. Without this context, the jailbreak augmentation motivation is unclear.
  - Quick check question: Why might an image combined with benign-looking text create a safety risk that neither modality alone would trigger?

## Architecture Onboarding

- Component map:
  SFT (cold start) → Stage 1 RL (general capability) → Stage 2 RL (joint safety/value/capability) → Final model

- Critical path:
  SFT (cold start) → Stage 1 RL (general capability) → Stage 2 RL (joint safety/value/capability) → Final model

- Design tradeoffs:
  - Joint vs. staged training: Paper shows joint multimodal training achieves better balance than sequential Img→Text or Text→Img (Table 4 ablation)
  - Reward weight calibration: "Scalar weights {w_i} kept on comparable scales so that no single term dominates"—requires manual tuning
  - SFT data quality: Relies on distillation from strong teachers; quality depends on teacher model capability

- Failure signatures:
  - **Over-refusal**: Model refuses benign queries → check if Helpful reward is too weak or Visual-Focus is missing (ablation shows Visual-Focus prevents unnecessary refusals)
  - **Over-acceptance**: Model complies with unsafe inputs → check if Helpful reward is disabled (ablation shows unsafe completions increase from 44% to 55.33% without Helpful)
  - **Safety forgetting**: Performance drops on safety benchmarks → likely skipped or rushed Stage 2, or Stage 1 dominated training
  - **Reward hacking**: Model exploits reward model without genuine capability → increase CPGD clipping threshold or KL penalty

- First 3 experiments:
  1. **Reproduce single-component ablation**: Remove the Helpful reward and measure MSSBench safe/unsafe scores. Should replicate the paper's finding (safe acceptance rises but unsafe admissions also increase).
  2. **Test generalization to smaller backbone**: Apply COSMO-RL to a 7B parameter model and compare safety + capability gains. Paper shows this works (Table 7), providing a lower-cost validation.
  3. **Probe jailbreak robustness**: Evaluate on held-out jailbreak types not seen during augmentation (e.g., typographic attacks, adversarial patches) to test whether robustness generalizes beyond the augmentation distribution.

## Open Questions the Paper Calls Out
None

## Limitations
- **Safety generalization uncertainty**: Framework shows robustness against known jailbreak patterns but lacks validation against truly novel attack types like adversarial patches or typographic attacks.
- **Reward model reliability**: Safety and value improvements depend heavily on reward model quality, but the paper doesn't provide detailed analysis of reward model accuracy or potential reward hacking vulnerabilities.
- **Scalability claims**: Results primarily demonstrated on Qwen2.5-VL-72B architecture; broader architectural validation would strengthen claims about general applicability.

## Confidence
- Safety Generalization Uncertainty: Medium
- Reward Model Reliability: Low-Medium
- Scalability Claims: Medium

## Next Checks
1. **Novel Jailbreak Evaluation**: Test COSMO-R1 against held-out multimodal jailbreak types not seen during training (e.g., adversarial image perturbations combined with subtle textual prompts, or temporal inconsistencies in video-text pairs) to assess true adversarial robustness beyond the augmentation distribution.

2. **Reward Model Stress Testing**: Conduct ablation studies where reward model weights are perturbed or where models are evaluated with different reward model instantiations to quantify sensitivity to reward model reliability and potential reward hacking vulnerabilities.

3. **Long-term Stability Monitoring**: Track safety and capability performance over extended time periods and across multiple fine-tuning iterations to detect potential "safety drift" or capability regression that might not be visible in short-term evaluations.