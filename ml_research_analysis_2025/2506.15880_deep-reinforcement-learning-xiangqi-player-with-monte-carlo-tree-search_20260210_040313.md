---
ver: rpa2
title: Deep Reinforcement Learning Xiangqi Player with Monte Carlo Tree Search
arxiv_id: '2506.15880'
source_url: https://arxiv.org/abs/2506.15880
tags:
- xiangqi
- move
- policy
- game
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a deep reinforcement learning system for Xiangqi
  (Chinese Chess) that integrates neural networks with Monte Carlo Tree Search (MCTS)
  to enable strategic self-play and self-improvement. The system addresses Xiangqi's
  complexity, including its unique board layout, asymmetrical piece dynamics, and
  high branching factor.
---

# Deep Reinforcement Learning Xiangqi Player with Monte Carlo Tree Search

## Quick Facts
- arXiv ID: 2506.15880
- Source URL: https://arxiv.org/abs/2506.15880
- Authors: Berk Yilmaz; Junyu Hu; Jinsong Liu
- Reference count: 0
- Primary result: After three rounds of training, model demonstrates improved self-play behavior with average game length decreasing from 92.4 to 60.8 moves and average reward increasing from -0.20 to +0.40

## Executive Summary
This paper presents a deep reinforcement learning system for Xiangqi that integrates neural networks with Monte Carlo Tree Search to enable strategic self-play and self-improvement. The system addresses Xiangqi's complexity through policy-value networks combined with MCTS to simulate move consequences and refine decision-making. Key results show the model avoids major strategic errors and exhibits learning patterns for foundational principles, though it still struggles with long-term planning and global positional evaluation.

## Method Summary
The approach combines supervised pre-training on human game records with AlphaZero-style self-play using MCTS. The neural network architecture features a dual-head design with shared convolutional backbone encoding 10×9×15 board states (14 piece channels plus turn indicator) into policy and value outputs. The policy head predicts move probabilities across 8100 possible actions while the value head estimates game outcomes. MCTS uses UCB selection with neural network priors to balance exploration and exploitation during search. Training involves three phases: (1) supervised pre-training via behavior cloning on expert games, (2) self-play with MCTS to generate improved data, and (3) retraining on self-play results.

## Key Results
- Average game length decreased from 92.4 to 60.8 moves after three training rounds
- Average reward increased from -0.20 to +0.40 during self-play improvement
- Model avoids major strategic errors and demonstrates learning of foundational principles
- System struggles with long-term planning and global positional evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised pre-training bootstraps policy learning more efficiently than pure self-play from random initialization.
- Mechanism: Human game records provide imitation signals that teach legal move patterns, piece coordination, and basic tactics before self-play begins. This produces informed priors instead of near-uniform distributions.
- Core assumption: Historical game data contains sufficiently quality patterns that transfer to improved self-play exploration.
- Evidence anchors:
  - [abstract] "our approach combines policy-value networks with MCTS to simulate move consequences"
  - [section 3.2.3] "By leveraging existing human game records and using behavior cloning, we train an initial policy–value network that imitates expert moves. This yields a substantially more informed starting strategy than random initialization."
  - [corpus] No direct corpus evidence on supervised pre-training for MCTS; related work focuses on search modifications.
- Break condition: If pre-training data contains systematic errors or is too narrow, the model may overfit to suboptimal patterns and struggle to improve via self-play.

### Mechanism 2
- Claim: MCTS with neural network priors balances exploration and exploitation to improve policy quality beyond raw network output.
- Mechanism: The UCB formula combines Q-values (exploitation), policy priors P(s,a) (network guidance), and visit counts (exploration bonus). Search iteratively refines action distributions toward higher-value regions.
- Core assumption: The neural network provides useful priors that guide search toward promising branches without exhaustive exploration.
- Evidence anchors:
  - [abstract] "combines policy-value networks with MCTS to simulate move consequences and refine decision-making"
  - [section 3.1.6] "The moves are selected by not only getting the highest-probability actions every time, we further implement this algorithm by adding selection and giving room for exploration."
  - [corpus] "MCPS is relevant when deep reinforcement learning is not an option, or when the computing power available before play is not substantial" (Monte Carlo Permutation Search) — suggests MCTS variants address resource constraints.
- Break condition: If the exploration constant c is poorly tuned, search either over-exploits weak priors or wastes computation on low-value branches.

### Mechanism 3
- Claim: Dual-head architecture enables joint learning of move selection and position evaluation, providing complementary signals.
- Mechanism: Shared backbone extracts spatial features; policy head outputs move probabilities (8100-dimensional softmax); value head predicts game outcome (scalar [-1,1]). Training combines cross-entropy loss (policy) and MSE loss (value).
- Core assumption: Spatial features relevant to move selection are also useful for position evaluation.
- Evidence anchors:
  - [abstract] "combines policy-value networks with MCTS"
  - [section 3.2.3.2] "During training, we jointly optimize the policy and value heads by combining their loss functions: sparse categorical cross-entropy for the policy head and mean squared error for the value head, weighted equally (1:1)."
  - [corpus] Weak corpus evidence on dual-head architectures specifically; corpus focuses on search algorithm modifications.
- Break condition: If shared features don't transfer between tasks, one head may dominate learning at the expense of the other.

## Foundational Learning

- Concept: Upper Confidence Bound (UCB) for tree search
  - Why needed here: MCTS selection relies on UCB to balance exploiting known good moves vs. exploring uncertain ones. Without understanding UCB, you cannot debug why search over/under-explores.
  - Quick check question: Can you explain why the square-root term in UCB decreases as a node is visited more?

- Concept: Residual connections in deep networks
  - Why needed here: The backbone uses 10 residual blocks with skip connections to prevent vanishing gradients during training.
  - Quick check question: What would happen to gradient flow if you removed the skip connection from a residual block?

- Concept: Policy distillation and imitation learning
  - Why needed here: The supervised pre-training phase uses behavior cloning on expert games to initialize the network before self-play.
  - Quick check question: Why does pure imitation learning often fail to exceed the expert's performance?

## Architecture Onboarding

- Component map:
  Input encoder: (10, 9, 15) tensor → 14 piece channels + 1 turn indicator → Shared backbone: 3×3 conv (256 filters) → 10 residual blocks → Policy head: 1×1 conv → flatten → FC(8100) → softmax; Value head: 1×1 conv → flatten → FC(256) → FC(1) → tanh → MCTS module: UCB-based selection, neural network evaluation at leaves, backpropagation → Self-play loop: Generate games → store (state, improved_policy, outcome) → retrain

- Critical path: Input encoding correctness → legal move masking in policy head → MCTS prior injection → value target assignment during backpropagation. Errors in encoding or masking propagate silently.

- Design tradeoffs:
  - Pre-training vs. pure self-play: Faster convergence but risk of inheriting human biases
  - Residual block count (10 vs. 19-39): Lower compute but reduced representational capacity
  - Action space encoding (8100 vs. legal-only): Simpler implementation but sparse gradients

- Failure signatures:
  - Games never terminate → check reward assignment or move legality
  - Policy loss plateau with high value loss → shared features may not transfer
  - All draws in evaluation → value head may output near-zero regardless of position (risk aversion)
  - MCTS selects illegal moves → check move encoder/decoder mapping

- First 3 experiments:
  1. Validate input encoding: Feed known board states, verify channel assignments match expected piece positions.
  2. Smoke test MCTS: Run 100 simulations from starting position; check that visit distribution concentrates on known strong opening moves after pre-training.
  3. Overfit sanity check: Train on single game, verify loss goes to near-zero; confirms gradient flow and loss computation are correct.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms cause the model to converge on drawn games, and how can this be corrected?
- Basis in paper: [explicit] Section 4.2 notes that during testing, the model "would always draw every match (0 wins, 0 losses, 5 draws a round)."
- Why unresolved: The authors observe the behavior and note the model lacks multi-phase planning, but do not isolate the cause or solution for the draw convergence.
- What evidence would resolve it: Adjustments to the MCTS exploration constant or reward shaping that result in decisive wins against baseline opponents.

### Open Question 2
- Question: Can increasing network depth or training duration overcome the limitations in global positional evaluation?
- Basis in paper: [explicit] The abstract and conclusion state the model "struggles with long-term planning and global positional evaluation."
- Why unresolved: The authors limited the architecture (e.g., 10 residual blocks) due to computational constraints, leaving the scaling potential unexplored.
- What evidence would resolve it: Scaling the residual blocks and measuring the impact on positional judgment tasks or against stronger engines.

### Open Question 3
- Question: How does the supervised pre-training approach compare to pure self-play in terms of sample efficiency?
- Basis in paper: [inferred] The authors justify supervised pre-training to overcome the "cold start" problem and resource limits, but do not provide a comparative ablation against a pure self-play baseline.
- Why unresolved: The experimental setup only validates the combined pre-training + self-play approach.
- What evidence would resolve it: A head-to-head training curve comparison between the pre-trained model and a randomly initialized model.

## Limitations

- Several key hyperparameters remain unspecified, including residual block count, MCTS simulation parameters, and exploration constant schedules
- The model's strategic understanding beyond basic tactics remains unclear, with no specific examples of learned strategic patterns
- Performance improvements lack rigorous statistical significance testing to confirm they exceed random variation
- Evaluation metrics (average game length, average reward) lack detailed breakdowns by skill level or position complexity

## Confidence

- **High confidence**: The supervised pre-training mechanism and dual-head architecture design are well-grounded in established DRL practices and align with AlphaZero methodology
- **Medium confidence**: The reported performance improvements (decreased game length, increased average reward) are plausible given the methodology, though exact quantitative claims require more rigorous statistical validation
- **Low confidence**: The claim about learning "foundational principles" lacks specific examples or quantitative measures of strategic understanding beyond win/loss outcomes

## Next Checks

1. **Statistical significance testing**: Perform hypothesis testing on performance metrics across training rounds to confirm that improvements are not due to random variation
2. **Qualitative strategy analysis**: Analyze representative game sequences to identify specific strategic patterns the model has learned, beyond aggregate metrics
3. **Robustness evaluation**: Test the model against rule variations or unfamiliar positions to assess generalization capabilities and identify potential overfitting to training data patterns