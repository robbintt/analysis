---
ver: rpa2
title: ECG-LLM -- training and evaluation of domain-specific large language models
  for electrocardiography
arxiv_id: '2510.18339'
source_url: https://arxiv.org/abs/2510.18339
tags:
- llama
- evaluation
- questions
- human
- finetuned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates domain-specific large language models (LLMs)
  for electrocardiography (ECG) through finetuning and retrieval-augmented generation
  (RAG). Open-weight Llama 3.1 models were adapted to ECG domain literature and compared
  against Claude Sonnet 3.7 using multiple evaluation methods including multiple-choice
  tests, automatic text metrics, LLM-as-a-judge, and human expert assessment.
---

# ECG-LLM -- training and evaluation of domain-specific large language models for electrocardiography

## Quick Facts
- arXiv ID: 2510.18339
- Source URL: https://arxiv.org/abs/2510.18339
- Authors: Lara Ahrens; Wilhelm Haverkamp; Nils Strodthoff
- Reference count: 40
- Primary result: Finetuned Llama 3.1 70B achieved 92% accuracy on multiple-choice evaluations, outperforming base models and general-purpose LLMs

## Executive Summary
This study evaluates domain-specific large language models for electrocardiography through finetuning and retrieval-augmented generation (RAG). Open-weight Llama 3.1 models were adapted to ECG domain literature and compared against Claude Sonnet 3.7 using multiple evaluation methods including multiple-choice tests, automatic text metrics, LLM-as-a-judge, and human expert assessment. Results showed finetuned Llama 3.1 70B achieved 92% accuracy on multiple-choice evaluations, outperforming both base models and general-purpose LLMs. Human expert evaluation favored Claude 3.7 and RAG approaches for complex queries, demonstrating that domain adaptation through finetuning and RAG achieves competitive performance with proprietary models.

## Method Summary
The study finetuned open-weight Llama 3.1 models using QLoRA on synthetic Q&A pairs generated from ECG domain literature. A corpus of 171 ECG-related papers was processed through MinerU to extract markdown content, which was then used to generate synthetic training data via Llama 3.3 70B. The finetuned models (8B and 70B parameters) were evaluated against base models and Claude Sonnet 3.7 across multiple-choice tests, text similarity metrics, LLM-as-a-judge assessments, and human expert evaluation. RAG systems were implemented using PubMedBERT embeddings and Chroma vector database for retrieval-augmented generation.

## Key Results
- Finetuned Llama 3.1 70B achieved 92% accuracy on multiple-choice evaluations
- Finetuned models significantly outperformed their base counterparts across nearly all evaluation modes
- Human expert evaluation favored Claude 3.7 and RAG approaches for complex queries

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Supervised finetuning (SFT) on synthetic Q&A pairs injects domain knowledge, optimizing the model for in-distribution queries.
- **Mechanism:** The model modifies its weights (via QLoRA) to align internal representations with specific domain terminology and reasoning patterns found in the training data. This increases the probability density of correct medical answers relative to the base model.
- **Core assumption:** The performance gain on multiple-choice and text-similarity tasks reflects genuine knowledge acquisition rather than merely style overfitting, and the synthetic data quality is sufficient to ground this knowledge.
- **Evidence anchors:**
  - [abstract]: "Finetuned Llama 3.1 70B achieved superior performance on multiple-choice evaluations... Finetuned models significantly outperformed their base counterparts across nearly all evaluation modes."
  - [section 3.2]: "The results demonstrate that finetuning enhances factual knowledge in domain-specific tasks... [in] in-distribution tests... finetuned models often match or exceed larger state-of-the-art models."
  - [corpus]: Paper 60106 ("Are Smaller Open-Weight LLMs Closing the Gap...") supports the broader viability of smaller open-weight models approximating proprietary performance, suggesting the efficiency of this adaptation layer.
- **Break condition:** Performance collapses on semantically complex or out-of-distribution queries (as seen in human evals) because the model has overfitted to the syntactic structure of the synthetic Q&A pairs rather than generalizing the underlying concepts.

### Mechanism 2
- **Claim:** Retrieval-Augmented Generation (RAG) provides grounding for complex, out-of-distribution queries by decoupling the knowledge store from the model parameters.
- **Mechanism:** Instead of relying on internal weights (which may hallucinate), the system retrieves relevant context chunks (e.g., via PubMedBERT embeddings) to augment the prompt. This allows the LLM to perform reasoning over explicit, verifiable text, reducing hallucination density.
- **Core assumption:** The retrieval mechanism successfully surfaces the correct context chunk, and the model possesses sufficient reasoning capability to synthesize an answer from that chunk even if it wasn't explicitly trained on that specific fact.
- **Evidence anchors:**
  - [section 2.4]: "Llama 3.1 8B with RAG outperforms its base model and reaches the performance of Llama 3.1 70B" on complex human-evaluated questions.
  - [section 3.2]: "On out-of-distribution data in human evaluation, RAG consistently outperforms finetuned models... RAG offers a flexible option for knowledge enhancement."
  - [corpus]: Paper 69129 ("Retrieval-Augmented Generation of Pediatric Speech-Language Pathology vignettes") similarly utilizes RAG to mitigate hallucinations in specialized medical domains, validating the transferability of this grounding mechanism.
- **Break condition:** Retrieval fails to fetch relevant chunks due to poor embedding alignment or query ambiguity, leading to "lost in the middle" phenomena or context window saturation without utility.

### Mechanism 3
- **Claim:** Performance is highly heterogeneous across evaluation methodologies, implying that "capability" is not a scalar metric but a function of the test constraints.
- **Mechanism:** Different evals measure different competencies. Multiple-choice measures recognition/lexical alignment; Text metrics (BLEU/ROUGE) measure surface-level similarity; LLM-as-a-judge measures semantic reasoning; Human eval measures clinical utility. Optimizing for one (e.g., BLEU via finetuning) may degrade performance on another (e.g., reasoning on complex queries).
- **Core assumption:** The specific prompts and judges used (e.g., DeepSeek R1) correlate sufficiently with human judgment to serve as proxies for semantic correctness.
- **Evidence anchors:**
  - [section 3.1]: "Our findings reveal substantial performance heterogeneity across evaluation methodologies, underscoring assessment complexity... No single evaluation method provides comprehensive coverage."
  - [section 2.3]: "Notably, all answers judged correct by the LLM were confirmed correct, but nearly 50% of answers judged incorrect by the LLM were considered correct by the human expert."
  - [corpus]: Paper 70620 ("RAGalyst") highlights the difficulty of evaluating RAG systems in specialized domains, supporting the finding that evaluation methodology itself is a critical variable.
- **Break condition:** A team relies on a single metric (e.g., high BLEU score) to validate a model for deployment, failing to catch that the model cannot handle novel clinical queries.

## Foundational Learning
- **Concept: QLoRA (Quantized Low-Rank Adaptation)**
  - **Why needed here:** The authors use QLoRA to finetune large models (70B) on limited hardware. Understanding this is critical to reproducing the architecture or assessing the trade-off between full parameter updates and adapter efficiency.
  - **Quick check question:** How does reducing trainable parameters to ~3.7% (via LoRA) affect the model's ability to unlearn incorrect pre-training biases versus simply learning a new output style?
- **Concept: In-Context Learning vs. Weight Updates**
  - **Why needed here:** The paper explicitly contrasts RAG (in-context) with Finetuning (weight updates). Understanding the distinction is necessary to predict where each approach will succeed or fail (e.g., static knowledge vs. reasoning flexibility).
  - **Quick check question:** If a medical guideline changes, does a RAG system or a finetuned model require retraining, and why?
- **Concept: LLM-as-a-Judge Reliability**
  - **Why needed here:** A significant portion of the evaluation relies on an automated judge (DeepSeek R1). The finding that it has high precision but low recall for "correctness" (rejecting valid answers) is crucial for interpreting the results.
  - **Quick check question:** Why might a judge LLM prioritize stylistic alignment or specific reasoning paths over clinical accuracy, and how does the paper mitigate this?

## Architecture Onboarding
- **Component map:** PDFs → MinerU (Markdown) → Cleaning/Splitting → Llama 3.3 70B (Synthetic Q&A Gen) → Train/Val/Test Split → Llama 3.1 8B/70B (QLoRA Training) OR Recursive Splitting (1024 tokens) → PubMedBERT Embeddings → Chroma Vector DB → Retriever (Top-20 + Reranking Top-5) → Llama 3.1 + RAG → DeepSeek R1 (Judge) + Human Expert
- **Critical path:** The generation of high-quality synthetic Q&A data is the primary bottleneck. If the generator (Llama 3.3 70B) hallucinates questions or answers, the finetuned model will inherit these errors ("garbage in, garbage out").
- **Design tradeoffs:**
  - **Finetuning:** High performance on standard tests; fast inference; risks rapid obsolescence if data changes; struggles with syntactic variation.
  - **RAG:** Lower performance on surface metrics; excels at complex/out-of-distribution queries; easy to update knowledge base; higher inference latency and complexity.
- **Failure signatures:**
  - **Finetuning:** High BLEU scores but failure to answer "why" or "how" questions not phrased in the training set style (Catastrophic forgetting of reasoning capabilities).
  - **RAG:** Low BERTScore/BLEU because the model generates answers based on retrieved context which may not match the phrasing of the reference answer exactly (Metric artifacts).
- **First 3 experiments:**
  1. Baseline Comparison: Run Llama 3.1 Base vs. Llama 3.1 + RAG vs. Claude 3.7 on the "special" subset of multiple-choice questions to verify the domain adaptation lift.
  2. Retrieval Tuning: Ablate the RAG parameters (Top-k values: 5 vs. 10 vs. 20) and the chunk size (512 vs. 1024) using the "checked" multiple-choice set to find the retrieval sweet spot before running expensive human evals.
  3. Judge Validation: Run a sample of 50 LLM-judged "incorrect" answers past a human reviewer to quantify the false negative rate of the DeepSeek R1 judge.

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic data quality bottleneck: The entire finetuning pipeline depends on synthetic Q&A generation by Llama 3.3 70B, creating a "garbage in, garbage out" risk.
- Evaluation heterogeneity: Substantial performance variance across evaluation methods (multiple-choice vs. LLM-judge vs. human expert) makes benchmarking and deployment decisions complex.
- Out-of-distribution brittleness: Finetuned models struggle on complex, semantically rich queries requiring reasoning beyond pattern matching, limiting reliability in novel clinical scenarios.

## Confidence
- **High confidence:** Claims about finetuned Llama 3.1 70B achieving 92% accuracy on multiple-choice evaluations and outperforming base models are well-supported by quantitative metrics and repeated across evaluation modes.
- **Medium confidence:** Claims about RAG's superiority for complex queries are supported by human expert evaluation but may be sensitive to retrieval quality and prompt engineering variations.
- **Low confidence:** Claims about LLM-as-a-judge reliability are undermined by the finding that nearly 50% of answers it judged "incorrect" were actually correct to human experts, suggesting significant false negative rates.

## Next Checks
1. Human validation of LLM judge outputs: Systematically review 100 randomly sampled answers judged "incorrect" by DeepSeek R1 to quantify the false negative rate and establish correction factors for automated evaluation.
2. Cross-dataset generalization test: Evaluate finetuned models on multiple-choice questions from external ECG datasets not used in training to assess true domain knowledge versus overfitting to training distribution.
3. Retrieval quality ablation study: Systematically vary chunk sizes (256-2048 tokens), embedding models (PubMedBERT vs. alternatives), and retrieval parameters (Top-k values) to optimize RAG performance and identify failure modes in context retrieval.