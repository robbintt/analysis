---
ver: rpa2
title: Large Vision-Language Models for Knowledge-Grounded Data Annotation of Memes
arxiv_id: '2501.13851'
source_url: https://arxiv.org/abs/2501.13851
tags:
- meme
- memes
- literary
- text
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of scaling meme comprehension
  by introducing a large-scale dataset and an automated annotation framework. The
  authors present ClassicMemes-50-templates (CM50), a dataset of 33,172 memes across
  50 popular templates, and develop an automated pipeline using GPT-4o to generate
  high-quality annotations including image captions, meme captions, embedded text,
  and literary device labels.
---

# Large Vision-Language Models for Knowledge-Grounded Data Annotation of Memes

## Quick Facts
- arXiv ID: 2501.13851
- Source URL: https://arxiv.org/abs/2501.13851
- Authors: Shiling Deng; Serge Belongie; Peter Ebert Christensen
- Reference count: 13
- Primary result: Fine-tuned CLIP model shows 11.2% improvement in Recall@1 for text-to-meme retrieval

## Executive Summary
This paper addresses the challenge of scaling meme comprehension by introducing a large-scale dataset and an automated annotation framework. The authors present ClassicMemes-50-templates (CM50), a dataset of 33,172 memes across 50 popular templates, and develop an automated pipeline using GPT-4o to generate high-quality annotations including image captions, meme captions, embedded text, and literary device labels. They also propose mtrCLIP, a fine-tuned CLIP model for meme-text retrieval that significantly improves performance. The automated annotation pipeline achieves near-human-level performance in captioning tasks and benefits from template context for literary device labeling.

## Method Summary
The authors introduce ClassicMemes-50-templates (CM50), a dataset of 33,172 memes from 50 ImgFlip templates, and develop an automated annotation pipeline using GPT-4o with template context from KnowYourMeme. They fine-tune CLIP-ViT-L/14@336px on meme captions using a cosine learning rate scheduler and evaluate retrieval performance on MemeCap and FigMemes benchmarks. The pipeline employs knowledge-grounded prompting to generate image captions, meme captions, embedded text, literary devices, and emotions. They test various prompting strategies including zero-shot, few-shot, and three-step reasoning approaches.

## Key Results
- mtrCLIP fine-tuned on meme captions achieves up to 11.2% improvement in Recall@1 for text-to-meme retrieval
- GPT-4o annotation pipeline achieves near-human-level performance with BLEURT scores of 0.525 and BERTscore of 0.879
- Template context significantly improves literary device labeling for smaller VLMs (LLaVA and LLaVA-NeXT)
- Automated pipeline outperforms existing methods on meme caption generation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge-grounded prompting with template context improves annotation quality for templatic memes.
- Mechanism: GPT-4o receives KnowYourMeme template descriptions ("About" sections) before annotation tasks. This external context grounds the model in the meme's origin, typical usage, and cultural significance, enabling more accurate meme captions and literary device labeling.
- Core assumption: The model can transfer template-level knowledge to interpret novel instances of the same template.
- Evidence anchors:
  - [abstract]: "an automated knowledge-grounded annotation pipeline leveraging large vision-language models to produce high-quality image captions, meme captions, and literary device labels"
  - [section 3.1]: "Contextual details (the About section of templates) were added to prompts for data annotation"
  - [corpus]: KYMKB (Bates et al., 2023) demonstrates that integrating external knowledge improves meme interpretation
- Break condition: If memes subvert or ironically misuse their templates, template context may mislead rather than help.

### Mechanism 2
- Claim: Fine-tuning CLIP on meme-specific captions improves cross-modal retrieval by aligning embeddings to meme semantics.
- Mechanism: The authors fine-tune CLIP-ViT-L/14@336px on the CM50 dataset using only meme captions as text inputs. This specializes the embedding space for meme-to-interpretation matching rather than general image-object correspondence.
- Core assumption: The pre-trained CLIP model's general alignment can be specialized through domain-specific contrastive learning.
- Evidence anchors:
  - [abstract]: "fine-tuned CLIP model shows improvements of up to 11.2% in Recall@1 for text-to-meme retrieval"
  - [section 4.2]: "Fine-tuning CLIP solely on our meme caption data improves the model's performance over the original CLIP on most tasks"
  - [corpus]: Related work on VLMs for meme classification shows similar fine-tuning benefits
- Break condition: If training data lacks template diversity or contains mislabeled captions, fine-tuning causes overfitting to template-specific patterns.

### Mechanism 3
- Claim: Multi-step reasoning prompts reduce GPT-4o's over-interpretation of literary devices.
- Mechanism: Rather than direct classification, the three-step reasoning prompt asks the model to (1) explain the meme, (2) select candidate literary devices, and (3) validate each label against the meme content. This bidirectional verification reduces false positives.
- Core assumption: Structured decomposition improves annotation consistency over single-step prompting.
- Evidence anchors:
  - [section 3.3.2]: "The Three-Step-Reasoning prompt performs on par with the baseline prompt that includes label definitions"
  - [section 3.2.1]: "by simply asking the model to interpret the meme, it recalled related information... This initial explanation served as additional context"
  - [corpus]: V-FLUTE (Saakyan et al., 2024) uses similar explanation-based approaches for figurative language
- Break condition: If memes use highly abstract or culturally-specific literary devices outside the model's knowledge, reasoning steps add no value.

## Foundational Learning

- **Concept: Vision-Language Models (VLMs)**
  - Why needed here: GPT-4o and LLaVA process both image pixels and text tokens to generate unified representations. Understanding modality fusion helps explain why VLMs struggle with meme humor that requires cultural knowledge beyond visual content.
  - Quick check question: How does a VLM handle text that is embedded within an image versus text provided as a separate input?

- **Concept: Contrastive Learning for Cross-Modal Retrieval**
  - Why needed here: CLIP uses contrastive loss to align image and text embeddings. Fine-tuning with meme-caption pairs adjusts this alignment for the meme domain.
  - Quick check question: In a batch of N image-text pairs, what is the contrastive objective function computing?

- **Concept: Prompt Engineering for Structured Output**
  - Why needed here: The annotation pipeline relies on carefully designed prompts to extract consistent JSON-formatted annotations across multiple tasks (captioning, literary devices, emotions).
  - Quick check question: Why might asking a model to "explain first, then classify" produce different results than direct classification?

## Architecture Onboarding

- **Component map:** Data Collection -> Template Matching -> Annotation Pipeline -> mtrCLIP -> Evaluation
- **Critical path:** 1. Filter ImgFlip data → 33,172 memes across 50 templates 2. Retrieve KYM template context for each template ID 3. Run GPT-4o annotation (image caption, meme caption, embedded text, literary device, emotion) 4. Fine-tune CLIP with cosine scheduler (1e-6 → 1e-5 → 1e-6), batch size 2048-2400, 5-20 epochs 5. Evaluate retrieval on held-out test sets
- **Design tradeoffs:**
  - GPT-4o vs LLaVA: GPT-4o favored in human evaluation but costs ~$0.01-0.03 per image; LLaVA achieves 89.8% of GPT-4o performance with template context
  - Zero-shot vs few-shot: Few-shot doesn't improve literary device F1 (0.27-0.36 vs 0.39 baseline)
  - Template context: Helps smaller VLMs significantly; GPT-4o benefits mainly in literary device labeling
- **Failure signatures:**
  - Meme title retrieval fails (R@1 ≈ 0.20): Titles are too brief ("He did it") to disambiguate
  - Image caption retrieval fails on CM50 (R@1 ≈ 0.07): Captions ignore embedded text and describe template visuals only
  - Template dominance: With 50+ instances per template, target meme ranks beyond 50th position
  - Literary device over-labeling: GPT-4o tends toward "sarcasm" and "irony" defaults
- **First 3 experiments:**
  1. Run GPT-4o annotation pipeline on 100 CM50 samples with/without template context, compute BLEURT scores and conduct human preference comparison
  2. Fine-tune CLIP-ViT-B/32 on CM50 meme captions for 5 epochs, measure Recall@1 on MemeCap test set
  3. Test three-step reasoning prompt on 50 memes from a held-out template, compare literary device macro F1 against zero-shot baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can domain adaptation techniques significantly improve automated figurative language interpretation in memes beyond few-shot prompting?
- Basis in paper: [explicit] "Few-shot learning proved ineffective... domain adaptation might be necessary, although that is beyond the scope of this paper."
- Why unresolved: The authors found that providing few-shot examples to GPT-4o did not improve literary device labeling performance, and GPT-4o tends to over-interpret memes by identifying too many figurative elements.
- What evidence would resolve it: A comparative study showing domain-adapted models (e.g., fine-tuned on meme-specific figurative language) achieving higher macro F1-scores on literary device labeling than few-shot prompting approaches.

### Open Question 2
- Question: How can meme-text retrieval models be improved to handle brief, context-sparse inputs like meme titles?
- Basis in paper: [explicit] "The difficulty with meme titles lies in their brevity and limited context, often consisting of just a few words... both models face challenges with meme titles in both datasets."
- Why unresolved: The fine-tuned mtrCLIP model achieved only 0.210 R@1 on meme titles in CM50, showing that even domain-specific fine-tuning fails to address the fundamental challenge of sparse textual signals.
- What evidence would resolve it: Novel architectures or training objectives that incorporate external knowledge graphs or contextual expansion techniques demonstrating improved Recall@1 on brief meme titles.

### Open Question 3
- Question: Can the automated annotation pipeline generalize to non-templatic meme types such as "Meme Trends" and "Superimposed images"?
- Basis in paper: [inferred] The paper explicitly limits scope to "templatic memes" and Table 1 notes that other meme types like "Meme Trends" and "Superimposed images" are "not annotated" and "not considered in this study."
- Why unresolved: The pipeline relies on template context from KnowYourMeme; non-templatic memes lack such grounding, making it unclear whether the approach transfers.
- What evidence would resolve it: Evaluation of the annotation pipeline on diverse meme types without template context, reporting BLEURT and BERTscore comparable to the templatic meme results (0.525 and 0.879 respectively).

## Limitations
- Dataset bias: Reliance on ImgFlip captures predominantly English, Western meme culture from a single platform
- Systematic over-labeling: GPT-4o exhibits tendency to over-interpret memes with excessive literary device labels (sarcasm, irony)
- Template dominance: Fine-tuned model struggles when target memes rank beyond 50th position due to template-based rankings
- Limited scope: Pipeline only addresses templatic memes, excluding "Meme Trends" and "Superimposed images"

## Confidence

- **High Confidence**: The 11.2% improvement in Recall@1 for text-to-meme retrieval with mtrCLIP, as this is directly measured against established benchmarks (MemeCap, FigMemes) with clear evaluation protocols.
- **Medium Confidence**: The claim of "near-human-level performance" for the annotation pipeline, as human evaluation was limited to pairwise comparisons and may not capture systematic annotation errors.
- **Medium Confidence**: The effectiveness of template context for literary device labeling, as benefits were primarily observed for smaller VLMs and may not generalize to other domains.

## Next Checks

1. Conduct cross-platform validation by scraping memes from Reddit or Twitter to test whether template context and fine-tuning generalize beyond ImgFlip's cultural sphere.
2. Implement template-aware retrieval evaluation that explicitly measures performance within template groups versus cross-template retrieval to better understand the "template dominance" limitation.
3. Perform ablation studies on the three-step reasoning prompt structure to quantify the contribution of each reasoning step to literary device labeling accuracy.