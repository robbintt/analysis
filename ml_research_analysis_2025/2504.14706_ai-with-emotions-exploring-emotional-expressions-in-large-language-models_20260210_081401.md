---
ver: rpa2
title: 'AI with Emotions: Exploring Emotional Expressions in Large Language Models'
arxiv_id: '2504.14706'
source_url: https://arxiv.org/abs/2504.14706
tags:
- emotional
- states
- arousal
- emotions
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether LLMs can generate responses reflecting\
  \ specified emotional states. The authors adopt Russell\u2019s Circumplex model,\
  \ mapping emotions onto arousal and valence axes, and prompt four LLMs (GPT-3.5,\
  \ GPT-4, Gemini 1.5, Llama3, and Command R+) to answer ten questions under 12 different\
  \ emotional states."
---

# AI with Emotions: Exploring Emotional Expressions in Large Language Models

## Quick Facts
- **arXiv ID**: 2504.14706
- **Source URL**: https://arxiv.org/abs/2504.14706
- **Authors**: Shin-nosuke Ishikawa; Atsushi Yoshino
- **Reference count**: 22
- **Primary result**: LLMs can generate responses reflecting specified emotional states with cosine similarity up to 0.75

## Executive Summary
This paper investigates whether large language models can generate responses that reflect specified emotional states. Using Russell's Circumplex model to define emotions via arousal and valence coordinates, the authors prompt four LLMs to answer ten questions under twelve different emotional states. Responses are evaluated using a GoEmotions-based sentiment classifier, with consistency measured by cosine similarity between specified and predicted emotional states. Results demonstrate that LLMs can be controlled to generate emotion-conditioned text, with GPT-4, GPT-4 turbo, and Llama3-70B achieving higher consistency than GPT-3.5 turbo.

## Method Summary
The study employs Russell's Circumplex model to specify emotional states as (arousal, valence) coordinate pairs. LLMs are prompted with system instructions to assume roles experiencing these emotional states while answering ten open-ended questions. Generated responses are classified using a BERT-based GoEmotions sentiment model, which maps predicted labels to Russell's arousal-valence space. Cosine similarity between the specified and evaluated vectors quantifies emotional consistency. The methodology tests five LLMs (GPT-3.5, GPT-4, Gemini 1.5, Llama3, and Command R+) across twelve emotional states for each question.

## Key Results
- All tested LLMs can express emotions with measurable consistency when prompted with Russell's Circumplex parameters
- GPT-4, GPT-4 turbo, and Llama3-70B achieve highest consistency with cosine similarity up to ~0.75
- GPT-3.5 turbo shows lower alignment with cosine similarity around 0.5-0.6
- Model architecture and training alignment appear more important than parameter count alone
- Emotional expression capability is achievable across both closed and open-source models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can reliably modulate outputs to reflect specified emotional states when prompted with Russell's Circumplex parameters.
- Mechanism: The LLM's role-playing capability maps continuous arousal-valence values to linguistic patterns (word choice, tone, sentence structure) that correlate with human-recognizable emotional expressions. The prompt instructs the model to "assume the role of a character experiencing an emotional state as described by Russell's Circumplex Model," leveraging pre-trained knowledge of both the framework and emotional language patterns.
- Core assumption: The LLM has internalized the relationship between Russell's dimensional model and the linguistic markers of emotion from its training corpus.
- Evidence anchors: The evaluation showed that the emotional states of the generated answers were consistent with the specifications, demonstrating the LLMs' capability for emotional expression. Before the experiment, we verified that all the LLMs had knowledge of Russell's Circumplex Model by asking them to explain it.

### Mechanism 2
- Claim: Independent sentiment classification provides a valid proxy measure for whether generated text matches intended emotional specifications.
- Mechanism: A BERT-based classifier trained on GoEmotions (28 labels) maps predicted emotion labels to Russell's arousal-valence space. Cosine similarity between the prompt-specified vector and the classifier-predicted vector quantifies alignment, enabling objective evaluation without human annotation.
- Core assumption: The mapping from GoEmotions discrete labels to continuous arousal-valence coordinates preserves meaningful semantic relationships.
- Evidence anchors: We explored the correspondence between the GoEmotions labels and the emotional terms that appeared in Russell's original paper. The mean cosine similarity is 0.680, indicating that the model can estimate emotional states with a certain level of precision.

### Mechanism 3
- Claim: Model architecture and training alignment, not parameter count alone, predict emotional expression capability.
- Mechanism: GPT-3.5 turbo underperforms Llama3-8B-Instruct despite likely having more parameters. Alignment training (RLHF or equivalent) that emphasizes instruction-following and persona adoption appears critical for translating emotional specifications into consistent output behavior.
- Core assumption: The observed performance differences stem from training methodology rather than architectural inductive biases alone.
- Evidence anchors: Given that GPT-3.5 turbo performs worse than the smaller-parameter LLaMA3-8B-Instruct model, this suggests that the number of parameters is not essential for this task. We did not observe that closed models have superiority compared to open models.

## Foundational Learning

- Concept: **Russell's Circumplex Model**
  - Why needed here: This two-dimensional framework (arousal: sleepy-activated, valence: pleasure-displeasure) provides the parametric space for specifying and evaluating emotional states. Understanding it is prerequisite to interpreting the prompt design and evaluation methodology.
  - Quick check question: Given arousal = 0.866 and valence = -0.5, which quadrant does this state occupy, and what emotions might it represent?

- Concept: **Cosine Similarity in Vector Space**
  - Why needed here: The primary metric for evaluating emotional alignment. You must understand why cosine similarity (angle between vectors) rather than Euclidean distance is appropriate for comparing directional emotional states.
  - Quick check question: If the specified state has angle 45° and the evaluated state has angle 90°, what is the cosine similarity?

- Concept: **Role-Playing Prompt Architecture**
  - Why needed here: The mechanism relies on system prompts that establish persona constraints. Understanding the separation between system instructions (emotional state specification) and user content (question) is essential for reproducing or extending the methodology.
  - Quick check question: In the prompt template, why is the emotional state specified in the system prompt rather than the user prompt?

## Architecture Onboarding

- Component map: Generation Module -> Evaluation Module -> Comparison Module
  - Generation Module: LLM with system prompt specifying (arousal, valence) values + user prompt with question → generates emotionally-conditioned response
  - Evaluation Module: BERT-based sentiment classifier (trained on GoEmotions) → predicts emotion label → maps to arousal-valence vector via Russell correspondence table
  - Comparison Module: Computes cosine similarity between specified and evaluated vectors; aggregates across questions

- Critical path: Prompt template design → LLM selection → GoEmotions-to-Russell label mapping → cosine similarity computation. The mapping step (Table 3 in paper) is the most fragile—errors here propagate through all evaluations.

- Design tradeoffs:
  - 12 emotional states vs. finer granularity: Paper chose 12 for distinguishability; finer divisions increase ambiguity
  - Vector length fixed at 1 vs. variable intensity: Paper fixed length to focus on emotion type; intensity modulation requires additional controls
  - Independent BERT classifier vs. LLM self-evaluation: Classifier provides objectivity but introduces its own error rate (58.9% accuracy on 28-class task)

- Failure signatures:
  - Role-breaking: Model outputs "I'm a language model" instead of maintaining character (observed with Gemini 1.5 Flash)
  - Low cosine similarity across all questions: Indicates model cannot follow emotional specifications (GPT-3.5 turbo showed this pattern)
  - Negative similarity values: Suggests model produces opposite emotional tone (GPT-3.5 turbo on Q2: -0.048)

- First 3 experiments:
  1. Baseline validation: Replicate the prompt template with GPT-4 and Llama3-70B on 3 questions, verify cosine similarity > 0.5 before proceeding
  2. Intensity modulation: Extend prompts to include vector length parameter (0.0 to 1.0), test whether outputs scale in emotional intensity
  3. Cross-domain generalization: Test whether emotional specifications hold for domain-specific questions (technical, creative, interpersonal) not in the original 10

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do emotional states induce measurable behavioral changes in AI agents during task execution, such as adopting cautious or assertive strategies?
- Basis in paper: To investigate this aspect, it is necessary to conduct an additional experiment specifically designed to evaluate behavioral changes.
- Why unresolved: The current study only evaluated the linguistic consistency of emotional expression, not the functional impact of emotions on agent decision-making.
- What evidence would resolve it: An experiment demonstrating that agents modify their problem-solving approaches (e.g., risk-taking vs. safety) based solely on their arousal and valence settings.

### Open Question 2
- Question: How can emotional dynamics—the continuous adjustment of emotional parameters based on interaction history—be effectively modeled for software agents?
- Basis in paper: Emotional dynamics remain "under-investigated for software-only systems" and identifying "designing a method to evaluate emotional dynamics" as essential.
- Why unresolved: This research utilized static emotional states for each prompt; it did not address how an agent's state should evolve or transition over time.
- What evidence would resolve it: A functional framework where an agent's valence and arousal update in real-time based on user input, validated against a defined metric for temporal emotional consistency.

### Open Question 3
- Question: Are certain emotional states more difficult for LLMs to express or for sentiment models to recognize than others?
- Basis in paper: As future work, it would be valuable to further investigate the proposed framework by comparing results across different emotional states, citing psychological research on variable recognizability.
- Why unresolved: The paper reports average cosine similarities but does not provide a detailed breakdown of performance discrepancies between specific emotions (e.g., anger vs. embarrassment).
- What evidence would resolve it: A granular analysis showing the generation and evaluation accuracy for specific emotional quadrants, identifying which states yield the lowest fidelity.

## Limitations
- The evaluation framework relies on an imperfect mapping between GoEmotions' 28 discrete labels and Russell's continuous circumplex space, introducing systematic uncertainty
- The sentiment classifier's 58.9% accuracy on GoEmotions compounds uncertainty when applied to synthetic LLM outputs that may differ from training distribution
- The study's focus on role-playing prompts excludes real-world applications where emotional states emerge organically from conversation history

## Confidence

- **High Confidence**: Models can be prompted to generate text reflecting specified emotional states (demonstrated across all tested models with measurable consistency)
- **Medium Confidence**: GPT-4, GPT-4 turbo, and Llama3-70B show superior performance over GPT-3.5 turbo (differences are consistent but classifier error rates create uncertainty in absolute values)
- **Low Confidence**: The relationship between model size, architecture, and emotional expression capability (observed performance gaps may reflect training methodology differences rather than inherent architectural properties)

## Next Checks
1. **Classifier Distribution Shift Validation**: Test the GoEmotions classifier on a validation set of LLM-generated emotional text (not from the experimental prompts) to quantify performance degradation from distribution shift before trusting evaluation metrics
2. **Human Evaluation Benchmark**: Conduct blind human evaluations comparing specified vs. generated emotional states on a subset of responses to validate whether classifier-based cosine similarity correlates with human perception of emotional alignment
3. **Intensity Parameterization Test**: Extend the methodology to include vector magnitude (0.0-1.0) alongside direction, generating responses across the full circumplex space and validating whether outputs scale in emotional intensity as expected