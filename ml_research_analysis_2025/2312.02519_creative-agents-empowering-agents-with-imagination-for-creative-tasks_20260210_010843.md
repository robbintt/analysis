---
ver: rpa2
title: 'Creative Agents: Empowering Agents with Imagination for Creative Tasks'
arxiv_id: '2312.02519'
source_url: https://arxiv.org/abs/2312.02519
tags:
- building
- minecraft
- agents
- tasks
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces creative agents, a novel framework that empowers
  embodied AI agents with imagination to solve open-ended creative tasks. The core
  idea is to enhance a controller with an imaginator that generates detailed imaginations
  of task outcomes from language instructions.
---

# Creative Agents: Empowering Agents with Imagination for Creative Tasks

## Quick Facts
- **arXiv ID**: 2312.02519
- **Source URL**: https://arxiv.org/abs/2312.02519
- **Reference count**: 40
- **Primary result**: A novel framework that decomposes embodied agents into imaginator and controller components, achieving superior performance on creative Minecraft building tasks through enhanced imagination generation.

## Executive Summary
This paper introduces creative agents, a framework that empowers embodied AI agents to solve open-ended creative tasks by decomposing them into an imaginator and controller. The imaginator generates detailed imaginations of task outcomes from language instructions, while the controller translates these imaginations into executable actions. Two imaginator variants are proposed: one using large language models (LLMs) with Chain-of-Thought prompting for textual imagination, and another using diffusion models for visual imagination. Experiments on building creation in Minecraft demonstrate that creative agents significantly outperform baseline methods across multiple evaluation metrics including correctness, complexity, quality, and functionality.

## Method Summary
The creative agent framework decomposes the agent into two components: an imaginator that generates detailed imaginations of task outcomes from language instructions, and a controller that translates these imaginations into executable actions. The imaginator can be either a textual generator using GPT-4 with Chain-of-Thought prompting, or a visual generator using a finetuned diffusion model. The controller can be either a behavior-cloning policy trained on gameplay data, or a vision-language model like GPT-4V that generates executable code. The approach is evaluated on Minecraft building tasks using three agent variants: CoT+GPT-4, Diffusion+GPT-4V, and Diffusion+BC.

## Key Results
- Creative agents significantly outperform baseline methods across four evaluation metrics (correctness, complexity, quality, functionality) on Minecraft building tasks
- The Diffusion+GPT-4V variant achieves the best overall performance, combining visual imagination with code-generation capabilities
- Textual imaginations (CoT+GPT-4) produce more interpretable specifications, while visual imaginations (diffusion) capture richer spatial details
- GPT-4V-based controllers generate more diverse structures compared to behavior cloning, though they tend toward simpler geometric shapes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Decomposing the agent into an imaginator and controller enables handling abstract creative tasks that lack concrete goal specifications.
- **Mechanism**: The imaginator converts abstract language instructions into detailed imaginations (text or image), which serve as concrete intermediate representations. The controller then conditions on both the imagination and instruction to generate actions, reducing uncertainty in the action space.
- **Core assumption**: Abstract instructions contain implicit creative possibilities that can be explicitly enumerated through generation before action planning.
- **Evidence anchors**: The formalization P(a|s, l) = Σ_g I(g|l)π(a|s, g, l) explicitly decomposes the agent into two conditioned components.

### Mechanism 2
- **Claim**: Generative models can produce diverse, detailed imaginations that capture the implicit possibilities in creative instructions.
- **Mechanism**: LLMs with Chain-of-Thought prompting generate structured textual specifications through multi-step reasoning. Diffusion models generate visual blueprints that ground abstract concepts in pixel-space representations.
- **Core assumption**: The generative models' training distributions sufficiently cover the creative space implied by the instructions.
- **Evidence anchors**: The imaginator uses GPT-4 with zero-shot prompts asking five questions about the building, while diffusion models are finetuned on domain-specific data.

### Mechanism 3
- **Claim**: Vision-language models with code generation capabilities can translate imaginations into executable actions more effectively than behavior cloning.
- **Mechanism**: GPT-4V receives visual or textual imaginations and generates JavaScript code calling Mineflayer APIs. The VLM's pre-trained vision-language understanding and code generation enable direct mapping from imagination to executable primitives.
- **Core assumption**: The VLM's pre-training covers both Minecraft-style structures and the API syntax needed for execution.
- **Evidence anchors**: GPT-4V generates code that calls Mineflayer APIs to execute environment actions for building creation.

## Foundational Learning

- **Markov Decision Processes (MDPs) without rewards**
  - Why needed here: The paper formalizes open-ended creative tasks as MDPs without reward functions, requiring understanding of state-action spaces independent of optimization objectives.
  - Quick check question: Can you explain how an agent would behave in an MDP with no reward signal, and why this formalization suits creative tasks?

- **Chain-of-Thought (CoT) Prompting for LLMs**
  - Why needed here: The textual imaginator uses zero-shot CoT to elicit structured reasoning about building specifications from GPT-4.
  - Quick check question: How does asking an LLM to "think step by step" change its output for a creative design task?

- **Diffusion Models for Conditional Image Generation**
  - Why needed here: The visual imaginator uses finetuned Stable Diffusion to generate Minecraft-specific building images from text prompts.
  - Quick check question: What is the role of the forward/reverse diffusion process, and why does finetuning matter for domain-specific generation?

## Architecture Onboarding

- **Component map**: Language Instruction → Imaginator → Imagination → Controller → Actions → Minecraft Environment

- **Critical path**: Instruction → Imagination generation (1 inference call) → Code/action generation (1+ inference calls with error correction) → Sequential block placement execution. The imagination quality directly constrains controller performance.

- **Design tradeoffs**:
  - Textual vs. Visual Imagination: Text is more interpretable and controllable; visual provides richer spatial details but may include noise
  - VLM vs. BC Controller: VLM requires no training data but relies on API coverage; BC requires large datasets but offers deterministic execution
  - Complexity vs. Reliability: More complex imaginations may exceed API capabilities, resulting in simpler structures

- **Failure signatures**:
  - Solid vs. hollow structures: GPT-4V may generate solid houses when images lack internal views, failing functionality metrics
  - Floating blocks: Pix2Vox reconstruction can produce physically impossible voxels that cannot be placed
  - Path planning failures: Mineflayer's rule-based pathfinding may destroy placed blocks when stuck
  - Simple outputs: Limited API primitives constrain building complexity to basic geometric shapes

- **First 3 experiments**:
  1. Ablate imagination: Compare Vanilla GPT-4 (no imagination) vs. CoT+GPT-4 to quantify the contribution of textual imagination
  2. Compare controller modalities: Run Diffusion imagination with both GPT-4V and BC controllers to isolate controller effects
  3. Validate evaluation: Correlate GPT-4V scores with human judgments to confirm the automated evaluation pipeline

## Open Questions the Paper Calls Out

- **How can agents overcome the tendency to generate simple, square structures, moving towards more complex, non-cubic architectural designs?**
  - **Basis in paper**: The authors note GPT-4(V) tends to use for-loops in code that correspond to walls, resulting in square shapes.
  - **Why unresolved**: Current LLM code generation strategies rely on repetitive loops and limited Mineflayer APIs, restricting geometric diversity.
  - **What evidence would resolve it**: Successful generation of intricate, curved, or asymmetrical structures in the Minecraft benchmark.

- **Can the Behavior Cloning (BC) controller be improved to handle noise in visual imaginations and accurately reconstruct internal building functionalities?**
  - **Basis in paper**: The paper states there is much room for improving the BC controller, especially for the performance of Pix2Vox module, which often produces floating blocks.
  - **Why unresolved**: The image-to-voxel reconstruction process struggles to infer structural integrity from single-view diffusion images.
  - **What evidence would resolve it**: Higher "Functionality" scores for the Diffusion+BC agent and elimination of floating/invalid block placements.

- **Does the Imaginator-Controller framework generalize effectively to other open-ended environments or physical robotics domains?**
  - **Basis in paper**: While the authors claim the framework is general and can also be applied to other environments, all experiments are strictly confined to Minecraft building.
  - **Why unresolved**: It is unclear if the specific modalities and control interfaces transfer robustly to environments with continuous action spaces or realistic physics.
  - **What evidence would resolve it**: Successful replication of the framework's performance in a different simulation or creative domain.

## Limitations

- The imagination quality heavily depends on generative model coverage of the creative space, with no guarantees that generated imaginations are physically realizable
- The VLM-based controller relies on API availability and coverage, creating potential brittleness if Mineflayer APIs change or lack necessary primitives
- The evaluation methodology, while validated through human-GPT-4V correlation, still depends on automated scoring for the majority of comparisons

## Confidence

**High Confidence**: The core mechanism of decomposing agents into imaginator and controller components is well-founded and the experimental methodology is sound.

**Medium Confidence**: The claim that Diffusion+GPT-4V achieves "best overall performance" is supported by Elo ratings but requires careful interpretation since different metrics favor different agent variants.

**Low Confidence**: The generalizability of the framework to creative tasks beyond Minecraft building construction remains unproven.

## Next Checks

1. **Cross-task validation**: Test the creative agent framework on a non-Minecraft creative task (e.g., interior design or furniture arrangement) to assess generalizability beyond the current domain.

2. **Imagination-realisability analysis**: Quantify the percentage of generated imaginations that result in successful execution vs. failures due to API limitations or physical impossibilities, establishing the practical upper bound of the approach.

3. **Modality ablation study**: Systematically compare the same controller with different imagination qualities (varying diffusion model CFG scales, different LLM models) to isolate the impact of imagination fidelity from controller capabilities.