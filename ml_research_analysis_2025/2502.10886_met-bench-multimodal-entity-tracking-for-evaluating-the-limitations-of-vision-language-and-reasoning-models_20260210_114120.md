---
ver: rpa2
title: 'MET-Bench: Multimodal Entity Tracking for Evaluating the Limitations of Vision-Language
  and Reasoning Models'
arxiv_id: '2502.10886'
source_url: https://arxiv.org/abs/2502.10886
tags:
- entity
- tracking
- text
- actions
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MET-Bench, a multimodal entity tracking
  benchmark to evaluate vision-language models'' ability to track entity states across
  text and image inputs. The benchmark includes two domains: Chess and Shell Game,
  where models must predict final entity states from initial states and sequences
  of actions provided as either text or images.'
---

# MET-Bench: Multimodal Entity Tracking for Evaluating the Limitations of Vision-Language and Reasoning Models

## Quick Facts
- arXiv ID: 2502.10886
- Source URL: https://arxiv.org/abs/2502.10886
- Authors: Vanya Cohen; Raymond Mooney
- Reference count: 20
- Primary result: Models perform significantly worse when tracking entity states from image-based actions compared to text-based actions, revealing gaps in multimodal reasoning.

## Executive Summary
This paper introduces MET-Bench, a benchmark designed to evaluate vision-language models' ability to track entity states across multimodal inputs. The benchmark includes Chess and Shell Game domains where models must predict final entity states from initial states and sequences of actions provided as either text or images. The authors find that models perform substantially worse on image-based actions compared to text-based ones, suggesting fundamental limitations in multimodal reasoning rather than perception. While chain-of-thought and reasoning models show improved performance, particularly on longer sequences, significant gaps remain. The authors apply reinforcement learning (GRPO) to improve open-source VLMs, achieving competitive performance with closed models but observing limited cross-modal transfer.

## Method Summary
The authors created MET-Bench with two domains: Chess and Shell Game. In both domains, models receive initial entity states and sequences of actions, then must predict final states. Actions are provided in either text or image format, allowing evaluation of cross-modal tracking performance. The benchmark tests both short-horizon (≤10 actions) and long-horizon (>10 actions) tasks. The authors evaluate various VLMs including GPT-4V, Gemini Pro, Claude 3.5 Sonnet, and open models like Gemma3-4B, both with and without chain-of-thought reasoning. They also apply GRPO reinforcement learning to improve open-source model performance and test for cross-modal transfer capabilities.

## Key Results
- Models perform significantly worse on image-based entity tracking tasks compared to text-based ones across both domains
- Chain-of-thought and reasoning models (Claude 3.7 Sonnet Thinking) substantially improve performance, especially on long-horizon tasks
- Reinforcement learning (GRPO) improves in-modality performance but shows limited cross-modal transfer
- Cascaded inference (image→text→tracking) matches text-only performance, indicating models have task knowledge but cannot apply it directly to visual inputs

## Why This Works (Mechanism)
The performance gap between text and image-based entity tracking reveals that current VLMs process these modalities through different pathways. When given text descriptions of actions, models can maintain coherent entity representations and track state changes effectively. However, when presented with visual depictions of the same actions, models struggle to extract and maintain the same entity state information. This suggests that while VLMs can perceive visual information, they lack robust mechanisms for integrating visual observations into structured entity tracking representations. The improvement with chain-of-thought reasoning indicates that explicit inference steps help models organize and maintain state information, but the underlying multimodal representation remains suboptimal.

## Foundational Learning

### Multimodal Entity Tracking
**Why needed:** To evaluate how well models can maintain coherent representations of objects/entities across different input modalities over time
**Quick check:** Can the model accurately track entity positions after a sequence of moves in both text and image formats?

### Chain-of-Thought Reasoning
**Why needed:** To determine whether explicit reasoning steps can compensate for limitations in direct multimodal understanding
**Quick check:** Does prompting models to "think step-by-step" improve performance on complex tracking tasks?

### Cross-Modal Transfer
**Why needed:** To assess whether knowledge gained in one modality can be applied to another, revealing the flexibility of learned representations
**Quick check:** After training on text-based tasks, can the model transfer this knowledge to image-based tasks without additional training?

## Architecture Onboarding

### Component Map
Text Encoder -> Entity State Tracker -> Action Processor -> Final State Predictor
Visual Encoder -> Text Converter -> Entity State Tracker -> Action Processor -> Final State Predictor
GRPO Trainer -> Model Parameters -> Performance Evaluator -> Cross-Modal Transfer Checker

### Critical Path
1. Input processing (text or image)
2. Entity state extraction/representation
3. Action sequence interpretation
4. State transition prediction
5. Final state output

### Design Tradeoffs
- Direct visual processing vs. cascaded text conversion for visual inputs
- Implicit state tracking vs. explicit memory structures
- In-modality optimization vs. cross-modal generalization
- Chain-of-thought reasoning overhead vs. accuracy gains

### Failure Signatures
- Cascaded inference matching text-only performance suggests knowledge exists but cannot be directly applied to visual inputs
- Limited cross-modal transfer after GRPO training indicates separate processing pathways for text and image modalities
- Performance degradation on long-horizon tasks reveals compounding errors in state tracking

### First 3 Experiments
1. Compare cascaded inference (image→text→tracking) against direct visual tracking to isolate reasoning vs. perception limitations
2. Test whether providing explicit entity state memory buffers improves long-horizon tracking performance
3. Evaluate cross-modal transfer by training on one modality and testing on the other with various prompting strategies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural or training innovations could enable robust cross-modal transfer for entity tracking?
- Basis in paper: [explicit] The authors state "the lack of cross-modal transfer suggests models process entity tracking in text and image through different pathways. Robust multimodal entity tracking may require architectural innovations or training strategies beyond what RL provides."
- Why unresolved: GRPO training improved in-modality performance (e.g., Gemma3-4B from 44.4% to 71.0% on text Shell Game) but showed no transfer to the held-out modality in Shell Game and limited transfer in Chess.
- What evidence would resolve it: Demonstrating that a specific architecture modification (e.g., shared cross-modal representations) or training objective (e.g., explicit cross-modal alignment) enables entity tracking capabilities learned in one modality to transfer to another.

### Open Question 2
- Question: How do current VLMs internally represent entity states across modalities, and where does the reasoning process break down for visual inputs?
- Basis in paper: [explicit] The authors state "Future work should explore deficits in entity tracking using mechanistic interpretability methods."
- Why unresolved: The paper shows cascaded inference (image→text→tracking) matches text-only performance, indicating models have task knowledge but cannot apply it directly to visual inputs. The exact failure point in the reasoning chain remains unidentified.
- What evidence would resolve it: Mechanistic interpretability analysis identifying which layers or attention heads fail to maintain coherent entity representations during visual vs. textual reasoning.

### Open Question 3
- Question: What explicit memory structures could mitigate compounding errors in long-horizon visual entity tracking?
- Basis in paper: [explicit] The authors call for "explicit memory structures to mitigate this gap" and show that all non-reasoning models degrade to near-random accuracy by 20 image actions.
- Why unresolved: Current models rely on implicit state representations within their context window. Reasoning models (e.g., Claude 3.7 Sonnet Thinking) maintain accuracy longer, suggesting structured inference helps, but the underlying mechanism for error accumulation is not addressed.
- What evidence would resolve it: Comparing standard VLMs against models augmented with explicit external memory or state-maintaining architectures on sequences exceeding 50+ image actions.

## Limitations

- The benchmark focuses on relatively simple domains (Chess and Shell Game) which may not generalize to more complex real-world scenarios
- Limited evaluation of alternative visual encoding strategies that might improve multimodal entity tracking
- The study does not explore whether different prompting strategies could partially bridge the text-image performance gap
- Cross-modal transfer limitations may reflect architectural constraints that could potentially be addressed through different training approaches

## Confidence

High: Models perform significantly worse on image-based entity tracking tasks compared to text-based ones, and chain-of-thought models improve performance
Medium: The gap reveals fundamental limitations in multimodal reasoning rather than perception
Low: Claims about the irreducible nature of these limitations and their generalizability beyond the specific benchmark domains

## Next Checks

1. Test whether different visual encoding strategies or multimodal fusion techniques can reduce the text-image performance gap
2. Evaluate model performance on entity tracking tasks with varying entity counts and action sequence lengths to better understand scalability limitations
3. Assess whether models trained on the Chess domain can transfer to the Shell Game domain without additional fine-tuning, providing stronger evidence for or against cross-modal transfer limitations