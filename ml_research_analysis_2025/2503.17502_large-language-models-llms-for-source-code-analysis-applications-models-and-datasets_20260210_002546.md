---
ver: rpa2
title: 'Large Language Models (LLMs) for Source Code Analysis: applications, models
  and datasets'
arxiv_id: '2503.17502'
source_url: https://arxiv.org/abs/2503.17502
tags:
- code
- arxiv
- llms
- language
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey examines the growing role of large language models
  (LLMs) in source code analysis, addressing applications, models, and datasets. It
  identifies key use cases such as code understanding, disassembly, decompilation,
  summarization, generation, comment generation, and security analysis.
---

# Large Language Models (LLMs) for Source Code Analysis: applications, models and datasets

## Quick Facts
- **arXiv ID:** 2503.17502
- **Source URL:** https://arxiv.org/abs/2503.17502
- **Reference count:** 40
- **Primary result:** DeepSeek-R1 outperforms GPT-4 in reasoning and code generation tasks, highlighting the potential of open-source LLMs for source code analysis

## Executive Summary
This survey examines the growing role of large language models (LLMs) in source code analysis, addressing applications, models, and datasets. It identifies key use cases such as code understanding, disassembly, decompilation, summarization, generation, comment generation, and security analysis. The study highlights prominent LLM models—including CodeBERT, CodeT5, GPT variants, and DeepSeek—while reviewing essential datasets like CodeSearchNet and CodeXGLUE. Major findings include the superior reasoning and code generation performance of DeepSeek-R1 compared to GPT-4, the need for domain-specific datasets to improve LLM fine-tuning, and challenges in handling long code inputs due to token size limitations.

## Method Summary
The paper synthesizes existing literature on LLMs for source code analysis through systematic survey methodology. It evaluates models across multiple benchmarks including HumanEval, MBPP, CodeSearchNet, and CodeXGLUE, focusing on inference capabilities rather than pre-training. The comparison between DeepSeek-R1 and GPT-4 centers on evaluation pipelines using standard prompting strategies and task-specific metrics like Pass@k for functional correctness and BLEU for summarization. The reproduction focus is on evaluation rather than model training, requiring API access to both models and careful prompt engineering to replicate reported results.

## Key Results
- DeepSeek-R1 demonstrates superior reasoning and code generation performance compared to GPT-4 across multiple benchmarks
- Domain-specific pre-training on code corpora significantly improves LLM performance for source code analysis tasks
- Token length limitations remain a critical challenge for analyzing long codebases, requiring innovative chunking and prompt chaining approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Domain-specific pre-training on code corpora improves LLM performance for source code analysis tasks.
- **Mechanism:** Models like CodeBERT and CodeT5 are pre-trained on bimodal data (code + natural language pairs) using objectives such as Masked Language Modeling (MLM) and Replaced Token Detection (RTD). This teaches the model to align programming language semantics with natural language descriptions, enabling better code search, summarization, and generation compared to general-purpose LLMs.
- **Core assumption:** The structural and semantic patterns in code are learnable through the same self-supervised objectives used for natural language, given sufficient domain-specific training data.
- **Evidence anchors:**
  - [abstract] "transformer-based architectures are increasingly utilized for source code analysis"
  - [section 4] "datasets like CodeSearchNet, HPCorpus, and commit data significantly boost the model's ability to understand and perform in specific domains"
  - [corpus] Weak direct support; neighbor papers focus on security evaluation rather than pre-training mechanisms.
- **Break condition:** If the target code domain uses idioms or patterns absent from the pre-training corpus (e.g., niche embedded systems or proprietary frameworks), performance gains may not transfer.

### Mechanism 2
- **Claim:** Fine-tuning with task-specific datasets enables LLMs to perform specialized code analysis tasks such as decompilation, disassembly, and comment generation.
- **Mechanism:** After general pre-training, models are fine-tuned on curated datasets aligned to specific tasks (e.g., binary-to-source pairs for decompilation, code-summary pairs for summarization). This specialization refines the model's output distribution toward task-appropriate responses.
- **Core assumption:** Task-specific datasets contain consistent, high-quality mappings that the model can generalize from.
- **Evidence anchors:**
  - [section 3.3] "LLM4Decompile... fine-tuned using the LLM4Decompile-6.7B model, further enhances the decompilation process"
  - [section 3.4] "fine-tuned a pre-trained language model to generate code summaries using generated-examples by GPT-3.5 as training data"
  - [corpus] No direct corpus support for fine-tuning mechanisms.
- **Break condition:** If fine-tuning datasets contain noisy or misaligned labels (the paper notes 32.8% of CodeSearchNet docstrings were irrelevant), the model may learn spurious correlations.

### Mechanism 3
- **Claim:** Prompt engineering and prompt chaining can mitigate token length limitations when analyzing long code inputs.
- **Mechanism:** Long codebases are segmented into smaller chunks (e.g., by function or module) and processed sequentially. Prompt chaining maintains logical consistency across chunks by passing intermediate context between prompts, enabling analysis of code that exceeds the model's native token window.
- **Core assumption:** The code can be partitioned at semantically meaningful boundaries without losing critical cross-references.
- **Evidence anchors:**
  - [section 7.2] "breaking down code into functions, classes, or modules and analyzing them incrementally can help reduce the token count"
  - [section 7.2.1] "LLM4FL... utilizes a divide-and-conquer strategy, incorporating prompt chaining and multiple LLM agents"
  - [corpus] No direct corpus support for prompt chaining specifically.
- **Break condition:** If variables or functions defined in one chunk are referenced in another without explicit context passing, the model may produce inconsistent or incorrect analysis.

## Foundational Learning

- **Concept: Transformer attention and tokenization**
  - **Why needed here:** All surveyed models (CodeBERT, CodeT5, GPT, DeepSeek) use transformer architectures. Understanding how attention processes sequential tokens and how code is tokenized differently from natural language is essential for interpreting model behavior and limitations.
  - **Quick check question:** Can you explain why a 16K token context window (DeepSeek-Coder) might still fail to analyze a 500-line file with complex cross-function dependencies?

- **Concept: Pre-training vs. fine-tuning paradigm**
  - **Why needed here:** The paper emphasizes domain adaptation through pre-training on code corpora followed by task-specific fine-tuning. This two-stage process determines how well a model generalizes to new code analysis tasks.
  - **Quick check question:** What is the difference between pre-training CodeBERT on CodeSearchNet and fine-tuning it for vulnerability detection?

- **Concept: Abstract Syntax Trees (ASTs) and code structure**
  - **Why needed here:** Traditional code analysis tools and some LLM approaches (e.g., GraphCodeBERT, TreeBERT) use ASTs to capture hierarchical code structure. This complements sequential token processing with structural understanding.
  - **Quick check question:** Why might an AST-based representation help an LLM understand nested function calls better than raw token sequences?

## Architecture Onboarding

- **Component map:** CodeBERT/GraphCodeBERT (encoder-only, bimodal) -> CodeT5/CodeT5+ (encoder-decoder, multi-task) -> GPT/Codex (decoder-only, generation-focused) -> DeepSeek (MoE architecture with 671B parameters in V3) -> Qwen (MoE with long-context support up to 1M tokens)
- **Critical path:** For a new code analysis application: (1) Select appropriate base model based on task type (understanding vs. generation), (2) Identify or construct domain-specific dataset, (3) Implement fine-tuning with task-aligned objectives, (4) Address token limitations via chunking or prompt chaining if processing long code, (5) Integrate security validation for generated code.
- **Design tradeoffs:**
  - DeepSeek-R1: Superior reasoning/code generation, open-source, lower cost—but may have prompt sensitivity and limited SE task capability vs. GPT-4
  - GPT-4: General-purpose, multimodal, widely benchmarked—but proprietary, high cost, shorter context window
  - CodeT5+: Flexible architecture (encoder/decoder/both), identifier-aware pre-training—but requires more compute for larger variants
- **Failure signatures:**
  - Models struggle with logic programming languages (e.g., Prolog) in summarization tasks
  - Binary reverse engineering effectiveness diminishes with obfuscated or complex malware
  - Generated code often insecure (up to 65% contains vulnerabilities) without explicit security prompting
  - Long code analysis loses context at chunk boundaries, causing inconsistent outputs
- **First 3 experiments:**
  1. **Baseline benchmarking:** Evaluate CodeT5+, DeepSeek-Coder, and GPT-4 on CodeXGLUE summarization and generation tasks to establish performance baseline for your target language(s).
  2. **Token limit stress test:** Process a 2000-line codebase through each model with and without prompt chaining to quantify accuracy degradation and identify optimal chunking strategies.
  3. **Security validation check:** Generate 50 code samples for common tasks (authentication, input validation) and run static analysis (CodeQL or Semgrep) to measure baseline vulnerability rates; compare with SecCode-enhanced generation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the research community develop high-quality, specialized datasets to fine-tune LLMs for niche code analysis tasks like disassembly, decompilation, and comment generation?
- **Basis in paper:** [explicit] Section 7.1 states there is a "notable gap in the availability of specialized datasets" for these specific tasks, noting that general-purpose datasets fail to address their complexities.
- **Why unresolved:** General datasets like CodeSearchNet contain irrelevant docstrings (32.8% irrelevant) and lack the nuanced alignment required for tasks like binary decompilation.
- **What evidence would resolve it:** The release of curated datasets tailored for these specific tasks that demonstrate improved performance metrics (e.g., BLEU, CodeBLEU) after fine-tuning.

### Open Question 2
- **Question:** What hybrid architectures or prompt-chaining techniques can effectively mitigate the loss of semantic context when processing codebases that exceed standard token limits?
- **Basis in paper:** [explicit] Section 7.2 highlights that chunking code to fit token limits often results in the "loss of syntactic information" and logical consistency between interdependent code segments.
- **Why unresolved:** Current transformer models struggle with long dependencies, and naive chunking breaks the logical flow necessary for understanding variable scope and function calls.
- **What evidence would resolve it:** A model architecture or processing pipeline that maintains semantic integrity across long contexts (e.g., >16k tokens) on benchmarks like RepoQA without truncation errors.

### Open Question 3
- **Question:** How can LLMs be enhanced to automatically validate code correctness and runtime behavior rather than relying solely on syntax for summarization and generation?
- **Basis in paper:** [explicit] Section 7.4 identifies the "inability to validate code correctness or test its runtime behavior" as a major limitation, leading to potential hallucinations in summarization.
- **Why unresolved:** LLMs focus on syntax and pattern recognition but lack an internal execution environment to verify logic or edge cases.
- **What evidence would resolve it:** A framework integrating LLMs with dynamic execution environments that successfully identifies and corrects runtime bugs during the summarization or generation phase.

## Limitations
- Survey methodology lacks systematic meta-analysis, potentially introducing bias from uneven methodology quality
- Token limitation claims rely heavily on anecdotal evidence rather than controlled experiments
- Security evaluation results are inconsistent across sources, with contradictory findings on vulnerability rates in LLM-generated code

## Confidence
- **High confidence:** Transformer-based architectures' superiority for code analysis tasks (well-established across multiple sources)
- **Medium confidence:** DeepSeek-R1's reasoning superiority over GPT-4 (based on specific benchmarks but limited comparative studies)
- **Medium confidence:** Domain-specific pre-training benefits (supported by CodeBERT/CodeT5 results but dependent on dataset quality)
- **Low confidence:** Security evaluation consistency (contradictory findings across different frameworks and datasets)

## Next Checks
1. **Replicate security claims:** Generate 100 code samples for common programming tasks using both GPT-4 and DeepSeek-R1 with and without security-specific prompts, then analyze vulnerability rates using standardized static analysis tools (CodeQL/SpotBugs).
2. **Benchmark context handling:** Systematically test each major LLM (CodeBERT, CodeT5+, GPT-4, DeepSeek) on progressively longer code inputs (500-5000 lines) to quantify accuracy degradation and validate prompt chaining effectiveness.
3. **Dataset quality audit:** Analyze CodeSearchNet and CodeXGLUE datasets for relevance, duplication, and label consistency using automated filtering and manual sampling to verify claims about dataset noise affecting model performance.