---
ver: rpa2
title: 'BiRQ: Bi-Level Self-Labeling Random Quantization for Self-Supervised Speech
  Recognition'
arxiv_id: '2509.15430'
source_url: https://arxiv.org/abs/2509.15430
tags:
- birq
- speech
- encoder
- labels
- best-rq
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating informative and
  efficient pseudo-labels in self-supervised speech recognition. The authors propose
  BiRQ, a bilevel SSL framework that combines the efficiency of BEST-RQ with the refinement
  benefits of HuBERT-style label enhancement.
---

# BiRQ: Bi-Level Self-Labeling Random Quantization for Self-Supervised Speech Recognition

## Quick Facts
- **arXiv ID:** 2509.15430
- **Source URL:** https://arxiv.org/abs/2509.15430
- **Reference count:** 0
- **Primary result:** Achieves 5.9% WER on test-clean and 17.2% on test-other after 300 epochs on LibriSpeech using a 137M parameter Conformer model

## Executive Summary
BiRQ addresses the challenge of generating informative and efficient pseudo-labels in self-supervised speech recognition by combining the efficiency of BEST-RQ with the refinement benefits of HuBERT-style label enhancement. The method introduces a bilevel SSL framework that reuses part of the model itself as a pseudo-label generator, using intermediate representations discretized by a random-projection quantizer to produce enhanced labels while anchoring labels from raw input stabilize training. This design eliminates the need for external label encoders, reduces memory cost, and enables iterative label refinement in an end-to-end fashion. The method consistently improves over BEST-RQ while maintaining low complexity and computational efficiency.

## Method Summary
BiRQ reformulates self-supervised speech recognition as a bilevel optimization problem where upper-level optimizes enhanced labels from intermediate encoder representations while lower-level maintains anchoring stability from raw input. The method uses a random-projection quantizer with a fixed codebook and Gumbel-softmax relaxation for differentiable label assignment. Training combines a weighted sum of upper-level and lower-level objectives (w1=0.1, w2=2.4) to balance enhanced label refinement with anchoring stability. The label encoder consists of the first k layers of the acoustic encoder (typically k≈0.7K), and the model is trained end-to-end with masked prediction using Conformer architecture.

## Key Results
- Achieves 5.9% WER on test-clean and 17.2% on test-other after 300 epochs on LibriSpeech dataset
- Consistently outperforms BEST-RQ baseline across all tested configurations
- Maintains low computational complexity while providing iterative label refinement
- Ablation studies validate the importance of intermediate layer selection (k≈0.7K)

## Why This Works (Mechanism)

### Mechanism 1: Dual Label Stability
Using both anchoring labels (from raw input) and enhanced labels (from intermediate representations) improves SSL training stability and representation quality over either alone. The lower-level objective uses fixed anchoring labels independent of model parameters, providing stable supervision that prevents collapse. The upper-level objective uses enhanced labels from the model's own intermediate layers (layer k), enabling iterative label refinement as representations improve. A penalty-based formulation balances these competing objectives in a single training loop.

### Mechanism 2: Model-Aware Label Generation
Reusing the first k layers of the encoder as a label encoder generates more informative pseudo-labels than random quantization alone, without external modules. Intermediate representations capture learned acoustic features that become increasingly informative during training. Projecting these through a fixed random matrix to a shared codebook via Gumbel-softmax produces differentiable labels that co-evolve with the encoder. The heuristic k ≈ 0.7K balances feature abstraction with label stability.

### Mechanism 3: Differentiable Label Assignment
Gumbel-softmax relaxation enables end-to-end gradient flow through discrete label assignment, allowing the label encoder to improve from downstream supervision. Standard nearest-neighbor assignment is non-differentiable. Gumbel-softmax provides a continuous relaxation with temperature τ controlling softness. This allows gradients from the mask-prediction loss to flow back through label generation, enabling the shared label encoder layers to improve label quality over training.

## Foundational Learning

- **Bilevel Optimization (BLO):**
  - Why needed here: Reformulates SSL as bilevel problem where upper-level optimizes enhanced labels while lower-level maintains anchoring stability
  - Quick check question: Why does the penalty reformulation min w1·F + w2·G avoid nested inner-loop optimization?

- **Random Projection Quantization:**
  - Why needed here: BEST-RQ's efficiency comes from fixed random projections instead of learned quantizers
  - Quick check question: Why does a fixed random projection matrix produce useful discrete targets without learning?

- **Masked Prediction for Speech SSL:**
  - Why needed here: Training paradigm follows BERT-style span masking with cross-entropy loss on discrete pseudo-labels
  - Quick check question: How does span masking (consecutive frames) differ from random frame masking for speech signals?

## Architecture Onboarding

- **Component map:** Input x -> span masking -> masked input x̃ -> full encoder -> prediction logits o(θ;x̃) -> logits -> prediction head -> cross-entropy loss
- **Critical path:**
  1. Input x -> span masking -> masked input x̃
  2. Parallel: (a) x -> anchoring labels y(x); (b) x -> layers 1–k -> z(k) -> enhanced labels y(k)(θ;x)
  3. x̃ -> full encoder -> prediction logits o(θ;x̃)
  4. Compute LL loss G(θ) with y(x), UL loss F(θ) with y(k)(θ;x)
  5. Gradient update: θ ← θ – η(w1∇F + w2∇G)

- **Design tradeoffs:**
  - k selection: Paper suggests k ≈ 0.7K (e.g., k=3 for 5 layers, k=7 for 10 layers)
  - Loss weights: w1=0.1, w2=2.4 yields penalty ratio γ=24
  - Temperature: τ=0.5 for Gumbel-softmax
  - Codebook size: Not specified in paper; typical values 8192 entries

- **Failure signatures:**
  - Training collapse (loss diverges): Anchoring weight too low; increase w2
  - No improvement over BEST-RQ: Enhanced labels not contributing; verify k value and gradient flow through Gumbel-softmax
  - Slow convergence: Penalty ratio w2/w1 may be too large; try reducing
  - Memory higher than expected: Accidentally created separate label encoder instead of reusing encoder layers

- **First 3 experiments:**
  1. BEST-RQ baseline: Train on LibriSpeech 960h for 100 epochs, fine-tune on 100h → target ~7.1% WER test-clean
  2. Layer ablation: Train BiRQ with k ∈ {2, 3, 4} for 5-layer model → validate k=3 performs best per ablation
  3. Weight sensitivity: Sweep w2/w1 ∈ {10, 24, 50} → confirm stability and performance tradeoffs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the selection of the intermediate layer $k$ for generating enhanced labels be determined dynamically or adaptively during training?
- Basis: The paper notes in the conclusion that ablation studies highlight the "importance of the intermediate layer choice," and the method currently relies on a "rule of thumb" ($k \approx 0.7K$) derived from pilot experiments.
- Why unresolved: The optimal layer depth likely varies as the model learns, but the current implementation fixes $k$ manually based on heuristics without a theoretical justification for the specific 0.7 ratio.
- What evidence would resolve it: A study comparing the fixed heuristic against adaptive layer selection strategies (e.g., selecting $k$ based on entropy or loss metrics per epoch) to see if dynamic selection improves convergence or final WER.

### Open Question 2
- Question: What are the performance limits and trade-offs when scaling BiRQ with multi-codebook configurations?
- Basis: The authors mention in the concluding remarks the "potential of multi-codebook configurations" and show improved results with 4 codebooks in the variant settings.
- Why unresolved: While the paper demonstrates that moving from 1 to 4 codebooks improves performance, it does not explore the saturation point or the computational trade-offs of increasing the codebook count further.
- What evidence would resolve it: A systematic ablation study reporting Word Error Rate (WER) and memory usage across a range of codebook counts (e.g., 1, 4, 8, 16) on the same model architecture.

### Open Question 3
- Question: How sensitive is the bilevel optimization stability to the choice of the penalty parameter $\gamma$ (the ratio $w_2/w_1$)?
- Basis: The paper sets specific loss weights ($w_1=0.1, w_2=2.4$, $\gamma=24$) to satisfy theoretical bounds (Lemma 1), but provides no ablation study on how variations in this ratio affect the balance between the anchoring and enhanced objectives.
- Why unresolved: It is unclear if the specific $\gamma=24$ is a robust default across different model sizes or if it requires careful tuning to prevent the gradient from the enhanced label from destabilizing the anchoring objective.
- What evidence would resolve it: Experimental results showing training curves and final performance metrics across a spectrum of $\gamma$ values (e.g., 1 to 100) to identify stable operating regions.

## Limitations

- **Codebook specification:** The paper does not specify the random codebook size N or projection dimensions dc, which are critical hyperparameters for BEST-RQ-style quantization
- **Gumbel-softmax implementation:** Temperature τ=0.5 is given but the annealing schedule or whether straight-through estimation is used remains unspecified
- **Conformer architecture details:** Beyond the high-level specification (5L/1024d/8H/att200), FFN dimensions, convolution kernel sizes, and dropout rates are not provided

## Confidence

- **High confidence:** The core bilevel optimization formulation and its implementation via penalty method (w1=0.1, w2=2.4) are clearly specified and theoretically sound
- **Medium confidence:** The k≈0.7K heuristic for layer selection is supported by ablation but the exact optimal value may be architecture-dependent
- **Medium confidence:** The claim of achieving 5.9% WER test-clean is well-supported by results but depends on unreported implementation details

## Next Checks

1. **Hyperparameter sensitivity analysis:** Systematically vary w2/w1 ratio (10, 24, 50) and k (2, 3, 4 for 5-layer model) to verify stability of the 0.7K heuristic and identify performance tradeoffs
2. **Gradient flow verification:** Instrument training to confirm Gumbel-softmax relaxation maintains meaningful gradients through the enhanced label path, preventing gradient vanishing at different temperature settings
3. **Anchoring vs enhanced label contribution:** Train variants with only anchoring labels (w1=1.0, w2=0) and only enhanced labels (w1=0, w2=1.0) to quantify the marginal benefit of the bilevel design compared to single-objective SSL