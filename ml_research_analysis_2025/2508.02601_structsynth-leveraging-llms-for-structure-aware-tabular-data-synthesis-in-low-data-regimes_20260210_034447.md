---
ver: rpa2
title: 'StructSynth: Leveraging LLMs for Structure-Aware Tabular Data Synthesis in
  Low-Data Regimes'
arxiv_id: '2508.02601'
source_url: https://arxiv.org/abs/2508.02601
tags:
- data
- graph
- structure
- learning
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of synthesizing high-quality\
  \ tabular data in low-data regimes, where traditional generative models and LLMs\
  \ often fail to capture complex dependencies and produce low-fidelity synthetics.\
  \ To resolve this, the authors propose StructSynth, a two-stage framework that first\
  \ discovers the underlying dependency structure of the data using an LLM-guided\
  \ approach, and then leverages this structure as a blueprint to guide the LLM\u2019\
  s generation process, ensuring adherence to feature dependencies by design."
---

# StructSynth: Leveraging LLMs for Structure-Aware Tabular Data Synthesis in Low-Data Regimes

## Quick Facts
- arXiv ID: 2508.02601
- Source URL: https://arxiv.org/abs/2508.02601
- Authors: Siyi Liu; Yujia Zheng; Yongqi Zhang
- Reference count: 40
- Primary result: Outperforms state-of-the-art methods in downstream model performance (e.g., AUC up to 85.55%) for tabular data synthesis in low-data regimes

## Executive Summary
This paper introduces StructSynth, a novel two-stage framework that addresses the challenge of synthesizing high-quality tabular data when training data is scarce. Traditional generative models and large language models (LLMs) often struggle to capture complex dependencies and produce low-fidelity synthetic data in low-data settings. StructSynth overcomes this limitation by first discovering the underlying dependency structure of the data using an LLM-guided approach, then using this structure as a blueprint to guide the LLM's generation process, ensuring adherence to feature dependencies by design.

The framework was evaluated across six real-world datasets and demonstrated superior performance compared to existing methods. StructSynth achieves higher downstream model performance, better statistical fidelity, and improved privacy preservation, particularly in low-data regimes where other methods fail to produce useful synthetic data.

## Method Summary
StructSynth employs a two-stage approach to tabular data synthesis. In the first stage, an LLM is used to discover the dependency structure among features in the limited available data, creating a dependency graph that captures relationships between variables. In the second stage, this discovered structure serves as a blueprint to guide the LLM's data generation process, constraining the generation to respect the learned dependencies. This structure-aware generation ensures that synthetic data maintains realistic correlations and relationships between features, even when trained on minimal datasets.

## Key Results
- Achieves downstream model performance with AUC scores up to 85.55%, outperforming state-of-the-art methods
- Demonstrates superior statistical fidelity in synthetic data across six real-world datasets
- Shows better privacy preservation characteristics compared to baseline approaches in low-data settings

## Why This Works (Mechanism)
StructSynth works by explicitly incorporating structural knowledge into the generation process. Traditional LLM-based synthesis methods generate data without regard to feature dependencies, leading to unrealistic synthetic datasets when training data is limited. By first discovering the dependency structure and then using it as a constraint during generation, StructSynth ensures that the synthetic data maintains realistic relationships between features. This two-stage approach effectively transfers the structural knowledge learned from limited data to the generation process, overcoming the data sparsity problem that plagues conventional methods.

## Foundational Learning
- **Dependency Structure Discovery**: Learning the relationships between features is essential for generating realistic synthetic data; quick check: verify discovered structure matches domain knowledge
- **Structure-Guided Generation**: Using discovered structure as constraints during generation ensures synthetic data maintains realistic correlations; quick check: measure correlation preservation between real and synthetic data
- **Low-Data Regime Adaptation**: Specialized techniques are needed when training data is scarce; quick check: compare performance with varying amounts of training data

## Architecture Onboarding

**Component Map**: Data → Dependency Discovery LLM → Structure Graph → Guided Generation LLM → Synthetic Data

**Critical Path**: The critical path is the two-stage pipeline where dependency discovery must complete before guided generation can begin. The quality of the dependency graph directly impacts the fidelity of synthetic data.

**Design Tradeoffs**: The approach trades computational overhead (two LLM calls) for improved data quality and structural fidelity. The dependency discovery stage adds latency but enables more accurate synthesis in low-data regimes.

**Failure Signatures**: Poor performance indicates either failed dependency discovery (structure graph doesn't capture true relationships) or insufficient LLM capacity to follow complex structural constraints during generation.

**3 First Experiments**:
1. Test dependency discovery accuracy on datasets with known ground truth structures
2. Evaluate synthetic data quality with and without structure guidance using the same LLM
3. Measure performance degradation as training data size decreases to validate low-data regime effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- The computational cost of the two-stage approach may be prohibitive for very large datasets or real-time applications
- The effectiveness of the LLM-guided dependency discovery phase may vary significantly across different domain types and data characteristics
- Limited transparency in implementation details and evaluation procedures makes it difficult to fully validate the reported improvements

## Confidence
- **High confidence**: The general framework design (two-stage structure discovery + guided generation) is novel and technically sound
- **Medium confidence**: The experimental results showing improved downstream model performance, as methodology is plausible but lacks implementation transparency
- **Medium confidence**: The privacy preservation claims, as differential privacy guarantees and empirical assessments are not rigorously detailed

## Next Checks
1. Replicate on diverse datasets: Test StructSynth on datasets spanning different domains (medical, financial, categorical-heavy) with varying feature types and dependency structures to verify generalizability beyond the six datasets reported

2. Ablation study on structure component: Remove the structure discovery stage and compare against standard LLM-based synthesis to isolate the specific contribution of the structure-aware component to performance gains

3. Scalability and runtime analysis: Measure computational overhead of the two-stage process compared to baseline methods, including dependency discovery time and generation latency, to assess practical deployment feasibility