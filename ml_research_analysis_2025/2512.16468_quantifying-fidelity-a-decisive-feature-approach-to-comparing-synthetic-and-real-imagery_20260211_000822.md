---
ver: rpa2
title: 'Quantifying Fidelity: A Decisive Feature Approach to Comparing Synthetic and
  Real Imagery'
arxiv_id: '2512.16468'
source_url: https://arxiv.org/abs/2512.16468
tags:
- fidelity
- calibration
- decisive
- synthetic
- real
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of assessing synthetic data
  fidelity for autonomous vehicle testing. While existing metrics like IV, OV, and
  LF fidelity focus on pixel-level similarity, latent features, or task outputs, they
  fail to capture whether the system-under-test (SUT) relies on consistent decision
  evidence across real and synthetic domains.
---

# Quantifying Fidelity: A Decisive Feature Approach to Comparing Synthetic and Real Imagery

## Quick Facts
- **arXiv ID:** 2512.16468
- **Source URL:** https://arxiv.org/abs/2512.16468
- **Reference count:** 30
- **Primary result:** DFF-guided calibration improves decisive-feature and input-level fidelity while maintaining output value fidelity across diverse SUTs.

## Executive Summary
This paper introduces Decisive Feature Fidelity (DFF), a SUT-specific metric that uses explainable AI (XAI) to identify and compare the decisive features driving a system-under-test's decisions. While existing fidelity metrics focus on pixel-level similarity or task outputs, DFF addresses whether the SUT relies on consistent decision evidence across real and synthetic domains. The authors propose a DFF-guided calibration scheme that improves simulator fidelity by aligning these decisive features without sacrificing task performance. Experiments on 2126 matched KITTI-VirtualKITTI2 pairs demonstrate that DFF reveals discrepancies overlooked by conventional metrics, and calibration using DFF improves decisive-feature and input-level fidelity while maintaining output value fidelity across diverse SUTs.

## Method Summary
The method introduces Decisive Feature Fidelity (DFF) to measure whether a fixed perception model relies on consistent decision evidence across matched real-synthetic image pairs. The approach uses mask-and-infill counterfactual explainers (CF-XAI) with multi-seed averaging to generate decisive maps highlighting critical pixel regions. These maps are pooled to 16×16 resolution and compared via MSE to compute DFF. A calibrator network optimizes generator parameters using a combined loss of reconstruction (IV), task output (OV), and DFF distance, with the SUT frozen during optimization. The method is evaluated on 2,126 matched KITTI-VirtualKITTI2 pairs with three frozen SUTs: PilotNet-style steering CNN and YOLOP drivable-area and lane-line detection heads.

## Key Results
- DFF-guided calibration reduces DFF by 0.008-0.064 while staying within pre-declared OV non-inferiority margins
- The approach improves both decisive-feature and input-level fidelity without degrading task performance
- DFF reveals mechanism gaps missed by conventional metrics, showing cases where identical outputs mask different internal reasoning
- Experiments demonstrate effectiveness across diverse SUTs including steering prediction and YOLOP models for drivable area and lane line detection

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Isolating "decisive features" via counterfactual explanations detects mechanism divergence that standard output metrics miss.
- **Mechanism:** A mask-and-infill counterfactual explainer identifies the minimal pixel set required to alter the SUT's decision. By comparing these sparse maps between real and synthetic pairs, the metric quantifies whether the SUT relies on the same visual evidence.
- **Core assumption:** The CF-XAI method reliably approximates the SUT's true decision boundary, and identified pixel masks correspond to causal evidence rather than spurious correlations.
- **Evidence anchors:** [Section IV.B] defines decisive map $H(F(x))$ via multi-seed averaging; [Abstract] states DFF leverages XAI to identify decisive features; [Corpus] [75524] supports using explainability metrics for semantic fidelity evaluation.
- **Break condition:** If the XAI explainer exhibits high variance or inconsistency, decisive maps become noisy, invalidating distance comparison.

### Mechanism 2
- **Claim:** High output-value (OV) fidelity does not guarantee mechanism parity; a system can produce identical outputs using divergent causal evidence.
- **Mechanism:** The paper demonstrates that latent feature fidelity is too coarse while OV fidelity is too sparse. DFF operates on the "decisive-feature subspace," identifying cases where synthetic inputs trigger correct output via incorrect internal reasoning.
- **Core assumption:** Internal mechanism alignment is a necessary condition for reliable sim-to-real transfer, specifically for safety-critical testing.
- **Evidence anchors:** [Table III] shows OV loss improves while DFF worsens, proving output similarity can mask mechanism shifts; [Section I] explains LF-fidelity cannot capture such discrepancies.
- **Break condition:** If the SUT is robust to feature perturbations or uses highly distributed representation where no single "decisive" subset exists.

### Mechanism 3
- **Claim:** Treating DFF as a loss signal allows calibration of synthetic generators to align mechanisms without sacrificing task performance.
- **Mechanism:** A calibrator network optimizes generator parameters using combined loss of reconstruction, task output, and DFF distance. By backpropagating through the generator, synthetic image is refined to activate SUT's decisive regions consistently with real image.
- **Core assumption:** Generator's parameter space is sufficiently expressive to correct visual discrepancies identified by decisive feature gap without introducing artifacts that degrade OV fidelity.
- **Evidence anchors:** [Section IV.C] includes $\lambda_{dff} \mathcal{L}_{DFF}$ in loss function; [Section V.C] shows DFF-guided calibration reduces DFF while staying within OV non-inferiority margins.
- **Break condition:** If generator cannot resolve discrepancy or optimization lands in local minimum where visual fidelity degrades excessively.

## Foundational Learning

- **Concept:** **Counterfactual Explanations (XAI)**
  - **Why needed here:** This is the engine of DFF metric. Unlike saliency maps, counterfactuals show what pixels must change to alter decision, required to distinguish "decisive" features from general active features.
  - **Quick check question:** Does the explainer highlight regions that are merely correlated with output, or regions that causally determine it?

- **Concept:** **Fidelity Spectrum (IV, OV, LF)**
  - **Why needed here:** The paper positions DFF as extension to this existing taxonomy. Understanding limitations of Input-Value and Output-Value is necessary to understand specific niche DFF fills (mechanism alignment).
  - **Quick check question:** If synthetic image is perfect pixel-match but SUT fails task, which fidelity level failed?

- **Concept:** **Non-Inferiority Margins**
  - **Why needed here:** Calibration process is constrained. It's not enough to improve DFF; must prove intervention did not degrade task performance beyond specific tolerance.
  - **Quick check question:** In calibration results, did DFF-guided variant strictly improve OV, or just maintain it within safety margin?

## Architecture Onboarding

- **Component map:** Inputs -> SUT (Frozen) -> XAI Explainer -> Metric Calculator -> Calibrator ($C_\eta$) -> Generator ($G_\Theta$)
- **Critical path:** Gradients flow from DFF loss -> Calibrator -> Generator parameters. The SUT is frozen; gradients do not pass through the SUT or XAI explainer.
- **Design tradeoffs:**
  - XAI Cost vs. Accuracy: Multi-seed averaging improves DFF reliability but increases compute time
  - Pooled Resolution: 16×16 pooling lowers computational load but may lose fine-grained spatial precision
  - Optimization Method: ES is robust to non-differentiable SUTs but requires more samples
- **Failure signatures:**
  - High OV / High DFF: SUT gives right answer for wrong reason (spurious correlation)
  - Explanation Noise: If DFF values fluctuate wildly across seeds, CF-XAI configuration is unstable
  - Overfitting to Perceptual Similarity: If IV improves but OV collapses, calibration optimizes visual appearance at expense of functional logic
- **First 3 experiments:**
  1. Run DFF on uncalibrated KITTI-VirtualKITTI2 pairs to check for low LPIPS/OV errors but high DFF scores
  2. Train calibrator using only IV+OV loss vs. full DFF-guided loss to compare resulting DFF distances and OV scores
  3. Visualize decisive maps $H(F(x))$ for real vs. synthetic pairs to verify high DFF scores correspond to interpretable misalignments

## Open Questions the Paper Calls Out
- **Closed-loop transferability:** Whether DFF-guided calibration transfers to closed-loop AV testing where perception outputs affect vehicle dynamics
- **Formal analysis:** Providing theoretical guarantees linking decisive-feature alignment to safety-critical outputs and sample complexity bounds
- **Multi-sensor extension:** Extending DFF to multi-sensor fusion systems while maintaining computational tractability
- **XAI method comparison:** Whether counterfactual explainers are optimal or if alternatives like gradient-based attribution provide more stable decisive maps

## Limitations
- CF-XAI explainer may exhibit high variance across seeds, requiring multi-seed averaging that increases computational cost
- The method assumes decisive-feature alignment is necessary for reliable sim-to-real transfer, which may not hold for all SUT architectures
- Current framework operates on camera-only inputs and has not been validated on multi-sensor fusion systems

## Confidence
- **Core mechanism (XAI reveals mechanism divergence):** Medium - supported by case studies but lacking broader empirical validation
- **Calibration scheme (using DFF as optimization target):** High - ablation and non-inferiority results are explicit and reproducible
- **Novelty claim (filling gap between LF and OV fidelity):** Medium - concept is sound but "mechanism parity" framing has limited direct support

## Next Checks
- Test DFF sensitivity to CF-XAI hyperparameter variance across different SUT architectures
- Apply DFF to a non-vision domain (e.g., tabular or text) to assess generalizability
- Benchmark DFF against SHAP-based alternatives on the same dataset to compare effectiveness