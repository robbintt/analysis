---
ver: rpa2
title: Explainable AI for Sentiment Analysis of Human Metapneumovirus (HMPV) Using
  XLNet
arxiv_id: '2502.01663'
source_url: https://arxiv.org/abs/2502.01663
tags:
- sentiment
- xlnet
- page
- hmpv
- shap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a sentiment analysis framework for understanding
  public reactions to Human Metapneumovirus (HMPV) during the 2024 outbreak. Using
  YouTube comments as data source, the research employed XLNet, achieving 93.50% accuracy
  in sentiment classification.
---

# Explainable AI for Sentiment Analysis of Human Metapneumovirus (HMPV) Using XLNet

## Quick Facts
- arXiv ID: 2502.01663
- Source URL: https://arxiv.org/abs/2502.01663
- Reference count: 26
- Primary result: 93.50% accuracy in classifying YouTube comments about HMPV into positive, negative, and neutral sentiments

## Executive Summary
This study presents a sentiment analysis framework for understanding public reactions to Human Metapneumovirus (HMPV) during the 2024 outbreak. Using YouTube comments as data source, the research employed XLNet, achieving 93.50% accuracy in sentiment classification. The approach integrated Explainable AI (XAI) through SHAP to enhance model transparency. Data preprocessing addressed challenges of noisy, multilingual content through language translation, emoji conversion, and noise removal. The model demonstrated strong performance across evaluation metrics (precision: 92.50%, recall: 93.50%, F1 score: 93.00%) with SHAP analysis revealing key linguistic features influencing sentiment predictions. The framework provides actionable insights for health communication strategies and demonstrates how advanced NLP models combined with interpretability techniques can effectively analyze social media discourse during public health crises.

## Method Summary
The methodology combines XLNet-based sentiment classification with SHAP-based explainability for YouTube comments about HMPV. The approach uses a comprehensive preprocessing pipeline including language translation, emoji-to-text conversion, and noise removal, followed by VADER lexicon-based labeling. The model achieves high accuracy through permutation language modeling that captures bidirectional context without pretrain-finetune discrepancy. SHAP decomposition provides token-level feature attributions, enabling transparency in sentiment predictions and identification of linguistic drivers for each classification decision.

## Key Results
- Achieved 93.50% accuracy, 92.50% precision, 93.50% recall, and 93.00% F1 score in multiclass sentiment classification
- Outperformed BERT (91.00% accuracy) and ELECTRA (91.14% accuracy) on the same dataset
- SHAP analysis identified specific words driving sentiment predictions, with words like "flu", "fear", and "shit" showing high positive values in negative sentiment predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: XLNet's Permutation Language Modeling (PLM) enables richer context capture than masked language modeling approaches
- Mechanism: During pretraining, PLM considers all possible permutations of input token sequences rather than masking tokens. This allows the model to learn bidirectional dependencies without the pretrain-finetune discrepancy inherent in BERT's masked approach, improving performance on downstream sentiment tasks where context order matters.
- Core assumption: Informal social media text contains sentiment signals distributed across token relationships that require flexible context modeling.
- Evidence anchors:
  - [abstract]: "achieving 93.50% accuracy in sentiment classification"
  - [Page 2]: "XLNet utilizes Permutation Language Modeling (PLM) during pretraining, allowing it to consider all possible token sequences. This approach has enabled XLNet to outperform BERT in 20 distinct NLP benchmarks"
  - [corpus]: TWSSenti paper (FMR=0.57) confirms XLNet effectiveness in hybrid transformer frameworks for sentiment analysis

### Mechanism 2
- Claim: Aggressive preprocessing of noisy social media data improves classification accuracy by reducing non-sentiment signal
- Mechanism: Multilingual translation standardizes input; emoji-to-text conversion preserves emotional signals that would otherwise be lost; removal of URLs, emails, special characters, and stopwords eliminates features unrelated to sentiment; lemmatization reduces lexical variance. This pipeline increases the proportion of sentiment-relevant tokens in the feature space.
- Core assumption: Sentiment in YouTube comments is primarily conveyed through lexical content rather than metadata, formatting, or linguistic markers removed by preprocessing.
- Evidence anchors:
  - [Page 4]: "After data scraping, we collected 15,300 comments. After applying various preprocessing steps, the dataset was reduced to 9,758 comments"
  - [Page 4]: Manual cross-checks eliminated "spam, advertisements, and irrelevant discussions" and "grammatical errors, typing errors, special characters, and URLs"
  - [corpus]: Limited direct corpus evidence comparing preprocessing approaches for health discourse

### Mechanism 3
- Claim: SHAP decomposition reveals which linguistic features drive sentiment predictions, enabling error diagnosis and trust calibration
- Mechanism: SHAP computes Shapley values assigning each input token a contribution score toward the final prediction. Force plots visualize how individual words push classifications toward positive, neutral, or negative. This enables identification of systematic model behaviors and specific misclassification causes.
- Core assumption: Token-level contributions are meaningful units of analysis for understanding sentiment predictions, and local explanations aggregate to useful global insights.
- Evidence anchors:
  - [Page 10]: "Words such as 'flu', 'fear', and 'shit' have high positive SHAP values in negative sentiment predictions"
  - [Page 10]: "Words like 'save', 'help', and 'mad' exhibit high SHAP values in neutral sentiment predictions"
  - [corpus]: No corpus papers directly validate SHAP effectiveness for health sentiment; this is an application-level claim

## Foundational Learning

- Concept: **Permutation Language Modeling (PLM)**
  - Why needed here: Understanding why XLNet outperforms BERT requires grasping how PLM differs from masked language modeling—the paper claims this is XLNet's key innovation.
  - Quick check question: Given tokens [A, B, C], how does PLM's training objective differ from BERT's masked token prediction?

- Concept: **Shapley Values and Feature Attribution**
  - Why needed here: SHAP's output is only interpretable if you understand that Shapley values fairly distribute "credit" among features based on their marginal contributions across all possible feature coalitions.
  - Quick check question: If a prediction changes when feature X is removed, but only when feature Y is also present, how does SHAP handle this interaction?

- Concept: **Transformer Fine-Tuning vs. Feature Extraction**
  - Why needed here: The paper fine-tunes XLNet rather than using frozen embeddings—understanding this distinction is critical for reproducing results and adapting the approach.
  - Quick check question: What parameters are updated during fine-tuning that would not be updated during feature extraction?

## Architecture Onboarding

- Component map:
  Data layer: YouTube comment scraper → multilingual translation → emoji-to-text → noise removal → stopword removal → lemmatization
  Labeling layer: VADER lexicon-based scoring → positive/negative/neutral labels
  Model layer: XLNet-base with classification head → softmax output
  Explainability layer: SHAP explainer with XLNet tokenizer → force plots and summary plots

- Critical path:
  1. Data quality (preprocessing completeness) → model input quality
  2. Hyperparameter tuning (learning rate 2e-04, batch size 12, max length 50, 10 epochs) → convergence
  3. SHAP computation on held-out data → interpretability validation

- Design tradeoffs:
  - **XLNet vs. BERT/ELECTRA**: Higher accuracy (93.50% vs. 91.00%/91.14%) but greater computational cost during pretraining and fine-tuning
  - **Max length 50 tokens**: Truncates longer comments; trade-off between computational efficiency and context preservation
  - **VADER labeling**: Lexicon-based labels may contain noise compared to human annotation, but enables larger dataset creation

- Failure signatures:
  - **Class confusion between Positive and others**: 56 positive instances misclassified as negative; suggests positive sentiment features overlap with negative contexts
  - **Multilingual content post-translation**: Potential loss of sentiment nuance in translation
  - **Sarcasm and mixed sentiments**: Acknowledged limitation in Discussion section

- First 3 experiments:
  1. **Baseline reproduction**: Train XLNet on the provided dataset with paper hyperparameters; verify 93.50% accuracy and compare confusion matrix to Figure 6.
  2. **Ablation on preprocessing**: Remove one preprocessing step at a time (e.g., skip emoji translation, skip lemmatization) to quantify each step's contribution to accuracy.
  3. **SHAP validation on known cases**: Manually label a small held-out set; verify SHAP attributions align with human-interpretable sentiment drivers for correct and incorrect predictions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does integrating video and image content with text improve sentiment detection accuracy for HMPV compared to the text-only XLNet approach?
- Basis in paper: [explicit] The authors state, "Future research could explore integrating multimodal data, such as video or image content, along with text, to enhance sentiment detection in multimedia platforms like YouTube."
- Why unresolved: The current study exclusively processes textual comments, ignoring the visual context of the YouTube platform which may carry significant emotional weight.
- What evidence would resolve it: Comparative performance metrics (F1-score, accuracy) of a multimodal model versus the text-only XLNet baseline on the same dataset.

### Open Question 2
- Question: How does the XLNet framework handle mixed sentiments and sarcasm in HMPV discourse, and what architectural modifications are required to improve performance in these specific contexts?
- Basis in paper: [explicit] The paper notes that "reliance on sentiment labeling may not always capture the full complexity of user emotions, as some comments may contain mixed sentiments or sarcasm, which are challenging to identify."
- Why unresolved: The current labeling method (VADER) and the model's training focus on general classification rather than detecting nuanced or contradictory emotional signals within single comments.
- What evidence would resolve it: A qualitative and quantitative error analysis of the model on a manually curated subset of sarcastic and mixed-sentiment comments.

### Open Question 3
- Question: Can lightweight or distilled versions of the XLNet model achieve comparable accuracy (approx. 93.5%) while meeting latency requirements for real-time health surveillance applications?
- Basis in paper: [explicit] The authors propose, "exploring lightweight versions of the model for faster inference without sacrificing accuracy will be essential for real-time systems."
- Why unresolved: The current XLNet implementation requires significant computational resources, creating a barrier for real-time deployment in resource-constrained environments.
- What evidence would resolve it: Benchmarks comparing the inference speed (latency) and accuracy trade-offs of DistilXLNet or quantized models against the full XLNet model.

## Limitations

- The VADER lexicon-based labeling methodology introduces potential noise through unlabeled sarcasm, negation, and domain-specific language
- The model's high accuracy (93.50%) may not generalize to other health topics, time periods, or social media platforms
- Computational cost of XLNet fine-tuning may limit real-time deployment capabilities during ongoing health crises

## Confidence

- **High Confidence (5/5)**: Model architecture and training details are clearly specified and reproducible
- **Medium Confidence (3/5)**: Performance metrics are verifiable but depend on unknown train/test split; SHAP interpretations lack quantitative validation
- **Low Confidence (1/5)**: Generalization claims and real-world impact assertions extend beyond HMPV-specific results without cross-domain validation

## Next Checks

1. **Train/Test Split Verification and Ablation Study**
   - Reproduce the model using an 80/20 split (or the actual split if specified in code) and verify the 93.50% accuracy
   - Conduct ablation experiments removing individual preprocessing steps to quantify each step's contribution to accuracy

2. **Cross-Domain Transfer Testing**
   - Fine-tune the trained XLNet model on sentiment data from other health crises without additional training to measure performance degradation
   - Alternatively, train on mixed health topic data and test on HMPV-specific comments to assess whether the model learns health-crisis-general sentiment patterns

3. **SHAP Attribution Validation with Human Annotation**
   - Manually annotate 100 held-out comments with gold-standard sentiment labels
   - For correctly classified instances, verify that SHAP's top-k features align with human-interpretable sentiment drivers
   - For misclassified instances, analyze whether SHAP reveals systematic failure modes that could inform model improvements