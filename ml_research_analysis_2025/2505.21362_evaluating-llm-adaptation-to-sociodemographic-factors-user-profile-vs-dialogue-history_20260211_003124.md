---
ver: rpa2
title: 'Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs. Dialogue
  History'
arxiv_id: '2505.21362'
source_url: https://arxiv.org/abs/2505.21362
tags:
- user
- dialogue
- question
- attributes
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates whether large language models (LLMs) can\
  \ adapt responses to users\u2019 sociodemographic attributes (e.g., age, education,\
  \ occupation, nationality) when presented either explicitly via user profiles or\
  \ implicitly through dialogue history. A synthetic dialogue dataset was constructed\
  \ using a multi-agent pipeline to generate career-advice interactions with embedded\
  \ demographic attributes."
---

# Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs. Dialogue History

## Quick Facts
- arXiv ID: 2505.21362
- Source URL: https://arxiv.org/abs/2505.21362
- Reference count: 40
- Primary result: Most LLMs adjust responses to demographic differences, with greater shifts for larger attribute changes and highest consistency achieved by reasoning-augmented models.

## Executive Summary
This study evaluates whether large language models can adapt responses to users' sociodemographic attributes (age, education, occupation, nationality) when presented explicitly via user profiles or implicitly through dialogue history. Using a synthetic dialogue dataset generated through a multi-agent pipeline, the research measures behavioral adaptation using Hofstede's Value Survey Module (VSM 2013). Results show that most models adjust their expressed values proportionally to attribute differences, with larger, reasoning-augmented models (e.g., QwQ-32B) achieving the highest cross-format consistency.

## Method Summary
The research constructs a synthetic dialogue dataset using a multi-agent pipeline: GPT-4o generates career-advice questions reflecting user profiles, GPT-4o-mini validates question-profile consistency, and a QA LLM (also GPT-4o) responds naturally. The dataset contains 1000 dialogues with embedded demographic attributes from a Kaggle employee dataset. Models are evaluated by querying them with VSM 2013 questions under two conditions: explicit user profile or dialogue history. Adaptation is measured via Jensen-Shannon divergence (JSD) between response distributions across demographic groups, while consistency across formats is assessed using Earth Mover's Distance (EMD).

## Key Results
- Most LLMs show greater value response divergence for larger sociodemographic attribute differences, particularly in age and education groups
- Reasoning-augmented models (e.g., QwQ-32B) achieve highest cross-format consistency, with distance/baseline ratios as low as 0.896
- Dialogue history inference performs comparably to explicit profiles for demographic adaptation in larger models (Llama3.1-70B, DeepSeek-V3)
- Smaller models (<10B parameters) show high variability in adaptation and consistency across formats

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs adjust their expressed values proportionally to the magnitude of sociodemographic attribute differences.
- **Mechanism:** When demographic attributes differ significantly (e.g., "<30" vs. ">60" age groups), models produce more divergent value responses as measured by Jensen-Shannon divergence between response distributions.
- **Core assumption:** Value expression is sensitive to contextual demographic cues embedded in either explicit profiles or inferred from dialogue patterns.
- **Evidence anchors:** [abstract] "greater shifts observed for larger attribute changes, particularly in age and education"; [section] Figure 5 shows distance ratios increase systematically between age groups; [corpus] Related work on paraphrasing robustness (arXiv:2501.08276) supports sensitivity to attribute variation.
- **Break condition:** When models lack sufficient reasoning capacity (smaller models <10B parameters), format variability may overwhelm demographic sensitivity, reducing adaptation magnitude to near-baseline levels.

### Mechanism 2
- **Claim:** Reasoning-augmented models achieve higher cross-format consistency by systematically revisiting demographic attributes during response generation.
- **Mechanism:** Models like QwQ-32B engage in explicit reasoning traces that re-examine provided or inferred demographic context before selecting responses.
- **Core assumption:** Explicit reasoning processes allow models to maintain attribute awareness across different presentation formats (profile vs. dialogue).
- **Evidence anchors:** [abstract] "reasoning-augmented models (e.g., QwQ-32B) achieved the highest alignment"; [section] Table 3 shows QwQ-32B achieves lowest distance/baseline ratio (0.896); Appendix I reasoning traces demonstrate attribute re-integration.
- **Break condition:** If reasoning traces are truncated or if attributes are too subtly embedded in dialogue for even reasoning models to extract, consistency degrades.

### Mechanism 3
- **Claim:** LLMs can infer sociodemographic attributes from multi-turn dialogue history with sufficient reliability to enable behavioral adaptation.
- **Mechanism:** Dialogue history contains linguistic markers (vocabulary, concerns, self-references) that correlate with age, education, occupation. Models extract these signals implicitly and adjust value responses comparably to explicit profile conditioning.
- **Core assumption:** Synthetic dialogues generated by the multi-agent pipeline contain sufficient demographic signal for models to infer attributes.
- **Evidence anchors:** [abstract] "implicit through dialogue history" adaptation is evaluated; [section] Appendix A: Llama3.1-8B-Instruct achieves 0.85+ hit rate on persona extraction; Figure 5 shows BA_dialogue adaptation patterns mirror BA_user.
- **Break condition:** When dialogue is short, attribute-unspecific, or contains conflicting signals, inference accuracy drops and adaptation becomes unreliable.

## Foundational Learning

- **Concept: Jensen-Shannon Divergence (JSD)**
  - Why needed here: Core metric for quantifying how much model responses diverge across demographic groups.
  - Quick check question: If two response distributions have JSD = 0, what does that imply about the model's adaptation to the compared groups?

- **Concept: Hofstede's Cultural Dimensions / VSM 2013**
  - Why needed here: Provides the standardized questionnaire (18 questions, 5-point scale) used to probe value expression across cultures and demographics.
  - Quick check question: Why is VSM 2013 suited for career-advice dialogues specifically, per the authors' justification?

- **Concept: Earth Mover's Distance (EMD)**
  - Why needed here: Measures consistency between ordinal response sequences (Su vs. Sd) across profile vs. dialogue formats.
  - Quick check question: EMD treats option_ids as ordinal—why is this important for 5-point Likert-style survey responses?

## Architecture Onboarding

- **Component map:** User Simulator (GPT-4o) -> Out-of-Context Detector (GPT-4o-mini) -> QA LLM -> Generation Controller -> Evaluation Module
- **Critical path:** 1. Seed profiles → User Simulator generates dialogue → Detector validates → QA LLM responds → Iterate until termination 2. Store (dialogue, profile) pairs as evaluation dataset 3. For each tested model: query with VSM questions under BA_user and BA_dialogue conditions 4. Compute JSD centroids per demographic group; compute EMD for cross-format consistency
- **Design tradeoffs:**
  - *Synthetic vs. real dialogues:* Synthetic enables controlled attribute alignment but may lack naturalistic noise
  - *GPT-4o as QA LLM:* All dialogues use GPT-4o responses—tested models may be influenced by GPT-4o's response style (acknowledged limitation)
  - *VSM 2013 scope:* Only 18 questions focused on workplace values; may not capture broader cultural dimensions
- **Failure signatures:** Distance/baseline ratio ≈ 1.0: Model shows no demographic sensitivity; High EMD but low JSD divergence: Model adapts within-format but is inconsistent across formats; Low confidence scores on selected_option_id: Model uncertain about value alignment
- **First 3 experiments:** 1. Reproduce the age-group divergence analysis on a single model (e.g., Qwen2.5-7B) to validate JSD computation pipeline 2. Ablate dialogue length (2-turn vs. 5-turn) to test inference reliability dependence on context richness 3. Compare QwQ-32B with and without reasoning traces exposed to isolate reasoning's contribution to consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent do variations in prompt design (beyond embedded demographic information) influence the behavioral adaptation and consistency of LLMs?
- Basis in paper: [explicit] The authors state in the Limitations section that "Future research could investigate how prompt content variations (besides the embedded information) impact models’ behavior adaptation," noting current prompts were based on experience rather than optimization.
- Why unresolved: The study utilized a fixed prompt structure for evaluation; therefore, the sensitivity of the observed adaptation capabilities to different phrasing or instruction styles remains unknown.
- What evidence would resolve it: A comparative analysis measuring adaptation scores across diverse prompt templates (e.g., persona-adoption vs. simple context-injection) using the same dataset.

### Open Question 2
- Question: Does the finding that models adapt to sociodemographics hold when evaluated against larger, more diverse value question sets beyond the VSM 2013?
- Basis in paper: [explicit] The authors acknowledge the "Limited Scope of Value Survey" and suggest that "Future studies could enhance the evaluation by incorporating larger and more diverse question sets" to address criticisms of the VSM's simplicity.
- Why unresolved: The current conclusions are based on a constrained subset of 18 career-focused questions, potentially missing nuances in cultural or ethical values present in broader benchmarks.
- What evidence would resolve it: Replicating the evaluation framework using extensive benchmarks like WorldValueBench or GlobalOpinionQA to verify if the correlation between attribute magnitude and value shift persists.

### Open Question 3
- Question: How does the specific response style of the dialogue history generator (e.g., GPT-4o) bias the behavioral adaptation of the target model?
- Basis in paper: [explicit] The authors list "Single Source of Dialogue" as a limitation, noting that "GPT's specific response styles may influence the behavior of the tested models" and suggest this be further explored.
- Why unresolved: All synthetic dialogue histories were generated by a single model (GPT-4o), making it impossible to distinguish whether the target model is adapting to the user attributes or the linguistic style of the dialogue history.
- What evidence would resolve it: Generating synthetic dialogues using multiple distinct "QA LLMs" (e.g., Llama, Claude) for the same user profiles and comparing the consistency scores of the evaluated models.

## Limitations
- Reliance on synthetic dialogues generated by GPT-4o may introduce stylistic artifacts that influence tested models' responses
- Multi-agent pipeline may not capture full complexity and noise of real human dialogues, potentially overestimating adaptation capability
- VSM 2013 focuses on workplace values and may not fully represent broader sociodemographic adaptation needs

## Confidence
- **High confidence**: The positive correlation between attribute difference magnitude and response divergence, the overall finding that larger models show better adaptation, and the basic methodology of using JSD and EMD metrics
- **Medium confidence**: The specific advantage of reasoning-augmented models for cross-format consistency, given limited comparative evidence in the broader literature
- **Medium confidence**: The inference capability from dialogue history, as the synthetic pipeline may artificially enhance demographic signal clarity compared to real dialogues

## Next Checks
1. Replicate the adaptation analysis using a subset of dialogues with real human-generated content to assess robustness to naturalistic variation and potential GPT-4o stylistic artifacts
2. Conduct ablation studies varying dialogue length (2-turn vs. 5-turn) to quantify the minimum context required for reliable sociodemographic inference from dialogue history
3. Perform individual model failure analysis by examining specific cases where cross-format consistency breaks down, particularly for smaller models that show high variability