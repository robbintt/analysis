---
ver: rpa2
title: Pretrained Image-Text Models are Secretly Video Captioners
arxiv_id: '2502.13363'
source_url: https://arxiv.org/abs/2502.13363
tags:
- video
- captioning
- training
- cider
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study repurposes the image-based BLIP-2 model for video captioning
  by post-training it on only 6,000 video-text pairs, using simple frame concatenation
  and reinforcement learning. This approach ranks 2nd on MSR-VTT and MSVD, and 3rd
  on VATEX, outperforming several specialized video captioning models.
---

# Pretrained Image-Text Models are Secretly Video Captioners

## Quick Facts
- arXiv ID: 2502.13363
- Source URL: https://arxiv.org/abs/2502.13363
- Reference count: 40
- Primary result: Transforms BLIP-2 into competitive video captioner using only 6,000 video-text pairs with frame concatenation and reinforcement learning

## Executive Summary
This paper demonstrates that image-text pretrained models can be effectively repurposed for video captioning through minimal adaptation. By leveraging the BLIP-2 architecture and post-training it on only 6,000 video-text pairs with simple frame concatenation and reinforcement learning, the authors achieve competitive performance on major video captioning benchmarks. The approach ranks 2nd on MSR-VTT and MSVD, and 3rd on VATEX, outperforming several specialized video captioning models while requiring significantly fewer computational resources.

## Method Summary
The method adapts BLIP-2 (ViT → Q-Former → LLM pipeline) for video captioning by freezing the Vision Transformer and training only the Q-Former and LLM. Video frames are sampled (8 frames at 224×224 resolution), encoded independently with the frozen ViT, and concatenated into a unified token sequence. The Q-Former processes these concatenated tokens to generate 32 soft prompts for the LLM. Training uses cross-entropy loss with AdamW optimizer and SCST reinforcement learning to directly optimize CIDEr scores. The approach requires minimal computational resources while achieving strong performance on standard benchmarks.

## Key Results
- Ranks 2nd on MSR-VTT and MSVD leaderboards, 3rd on VATEX
- Outperforms specialized video captioning models with only 6,000 training pairs
- Freezing ViT yields higher performance than full fine-tuning (CIDEr 73.6 vs 68.4)
- Frame concatenation consistently outperforms frame averaging across metrics
- SCST improves CIDEr by 6.5% for Flan-T5-XL-3B and 3.4% for Vicuna-7B

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Frame concatenation preserves temporal information more effectively than frame averaging for video captioning.
- **Mechanism**: Concatenating visual tokens from each sampled frame creates a unified token sequence that the Q-Former processes, preserving granular temporal relationships between frames rather than compressing them into an average representation.
- **Core assumption**: The Q-Former and LLM can effectively attend to longer token sequences to extract temporal patterns without specialized temporal attention mechanisms.
- **Evidence anchors**:
  - Frame concatenation consistently outperforms frame averaging on CIDEr with stable training dynamics
  - Frame averaging shows significant performance oscillations after epoch 5
  - Related work (VideoCoCa, Video-LLaMA) uses specialized temporal modules instead

### Mechanism 2
- **Claim**: Freezing the pre-trained Vision Transformer while training only the modal connector (Q-Former) and LLM yields higher performance than full fine-tuning.
- **Mechanism**: The frozen ViT retains visual feature alignments learned from large-scale image-text pretraining. The Q-Former adapts these frozen features to the video domain by learning to aggregate temporal information and project it to language-compatible tokens.
- **Core assumption**: Pre-trained visual features from image-text data transfer effectively to video frames without requiring video-specific visual fine-tuning.
- **Evidence anchors**:
  - Freezing the ViT yields higher performance than training all components
  - Establishes hierarchy of trainability: Q-Former > LLM > ViT
  - Training curves show Q-Former-only configuration achieves most stable performance

### Mechanism 3
- **Claim**: Reinforcement learning with Self-Critical Sequence Training (SCST) directly optimizing CIDEr improves caption quality beyond cross-entropy loss alone.
- **Mechanism**: SCST uses policy gradients from the non-differentiable CIDEr metric to update the Q-Former and LLM, decoupling training loss from the evaluation metric and guiding the model toward outputs that align with human preferences.
- **Core assumption**: CIDEr scores meaningfully correlate with human judgment of caption quality; the baseline model has sufficient capacity to generate high-CIDEr outputs.
- **Evidence anchors**:
  - SCST improves CIDEr scores by approximately 6.5% for Flan-T5-XL-3B and 3.4% for Vicuna-7B
  - Training dynamics show decoupling effect—models with SCST achieve higher CIDEr despite fluctuating training loss

## Foundational Learning

- **Concept**: BLIP-2 Architecture (ViT → Q-Former → LLM pipeline)
  - **Why needed here**: The entire method builds on adapting this image captioning architecture for video by modifying how visual tokens flow through it. Understanding the three-component pipeline is essential for knowing what to freeze, what to train, and where to inject temporal information.
  - **Quick check question**: Can you explain why the Q-Former's token reduction (256 visual tokens → 32 soft prompts) matters for processing multiple video frames?

- **Concept**: CIDEr Metric (Consensus-based Image Description Evaluation)
  - **Why needed here**: The paper optimizes directly for CIDEr via RL. Understanding that CIDEr measures n-gram overlap between generated captions and reference captions helps explain why SCST improves "human-like" output—CIDEr rewards linguistic patterns matching human references.
  - **Quick check question**: Why would directly optimizing CIDEr with RL produce better results than minimizing cross-entropy loss, which also improves word prediction accuracy?

- **Concept**: Transfer Learning & Domain Adaptation
  - **Why needed here**: The core contribution is transferring image-text knowledge to video-text tasks. The mechanism assumes visual features learned from static images apply to video frames, with only temporal aggregation needing new learning.
  - **Quick check question**: What component should you train first when adapting an image model to video, and which should remain frozen? Why?

## Architecture Onboarding

- **Component map**: ViT → Frame Concatenation → Q-Former → LLM
- **Critical path**:
  1. Sample 8 frames from video at 224×224 resolution
  2. Encode each frame independently with frozen ViT → 8 × 256 visual tokens
  3. Concatenate all frame tokens into single sequence (2048 tokens)
  4. Q-Former processes concatenated tokens → 32 soft prompts (train this)
  5. LLM generates caption from soft prompts (train with LoRA)
  6. Apply SCST to optimize CIDEr directly

- **Design tradeoffs**:
  - Frame count vs. context length: More frames = better temporal coverage but longer sequences
  - Resolution vs. efficiency: 224×224 sufficient for captioning; 364×364 provides marginal stability gains with higher compute cost
  - LLM size vs. overfitting risk: Mid-sized (3B) models balance expressiveness and generalization; larger models (7B) converge faster but overfit quickly on small datasets
  - Averaging vs. concatenation: Averaging is simpler but loses temporal detail; concatenation preserves information but increases sequence length

- **Failure signatures**:
  - Rapid validation drop after epoch 5: Likely overfitting from training ViT; freeze vision encoder
  - Oscillating CIDEr during training: Frame averaging not preserving enough temporal information; switch to concatenation
  - Training loss decreasing but CIDEr plateauing: Cross-entropy not aligned with evaluation metric; add SCST
  - Poor convergence on limited data: LLM too large or pre-training insufficient; use mid-sized model with 129M-pair pre-trained checkpoint

- **First 3 experiments**:
  1. Baseline connectivity test: Load pre-trained BLIP-2, freeze ViT, train Q-Former only on 6k video pairs with cross-entropy loss. Target: achieve stable training curve.
  2. Temporal fusion comparison: Run identical setup with frame averaging vs. concatenation. Measure CIDEr gap (expected: concatenation ~5-10 points higher).
  3. SCST impact validation: Take best checkpoint from experiment 2, continue training with SCST enabled. Measure CIDEr improvement (expected: 3-7% gain).

## Open Questions the Paper Calls Out
- Can this lightweight, frozen-ViT approach serve effectively as a pseudo-labeler to generate large-scale training data for more complex video-language models?
- How does the simple frame concatenation mechanism perform on long-form videos requiring temporal reasoning beyond the standard 8-frame window?
- Does the superior performance of mid-sized LLMs (3B) over larger models (7B) persist when scaling the video-text fine-tuning data beyond 6,000 pairs?

## Limitations
- Temporal Generalization Gap: The frame concatenation approach assumes 8 uniformly sampled frames capture sufficient temporal information, lacking explicit temporal modeling found in specialized video models.
- Pretraining Transfer Dependency: Effectiveness critically depends on the quality and breadth of the 129M image-text pretraining corpus, with no evidence of generalization to specialized domains.
- Dataset Size Constraint: While 6,000 video-text pairs enable competitive performance, this remains orders of magnitude smaller than typical pretraining datasets, potentially limiting generalization to tasks requiring more diverse video understanding.

## Confidence
- **High Confidence** (Likelihood >80%):
  - Frame concatenation outperforms frame averaging for preserving temporal information in this specific video captioning task
  - Freezing the pre-trained ViT while training Q-Former and LLM yields better performance than full fine-tuning on limited video data
  - SCST reinforcement learning improves CIDEr scores when applied to adequately pre-trained models
  - Mid-sized language models (3B parameters) achieve better data efficiency than larger models on 6k video pairs

- **Medium Confidence** (Likelihood 50-80%):
  - The approach generalizes to other video understanding tasks beyond captioning
  - Results transfer to video datasets with substantially different characteristics
  - The 129M image-text pretraining provides sufficient visual grounding for diverse video domains
  - The 8-frame sampling strategy captures representative temporal information across varied video content

- **Low Confidence** (Likelihood <50%):
  - The method achieves comparable performance on video tasks requiring fine-grained temporal reasoning
  - The approach scales effectively to longer videos without architectural modifications
  - Performance remains competitive when evaluated with metrics beyond CIDEr that better reflect caption quality

## Next Checks
1. **Temporal Robustness Test**: Evaluate the model on videos with rapid scene changes, non-uniform temporal dynamics, or when using variable frame sampling rates (2, 4, 8, 16 frames). Compare performance degradation against specialized temporal models to quantify the concatenation approach's temporal modeling limitations.

2. **Domain Transfer Validation**: Fine-tune the model on a specialized video dataset from a different domain (e.g., medical procedures, sports analytics, surveillance footage) with 6,000 pairs. Measure performance drop relative to domain-specific video captioning models to assess pretraining transfer limitations.

3. **Metric Correlation Analysis**: Evaluate generated captions using multiple metrics including factual consistency checks, semantic relevance scores, and human preference studies alongside CIDEr. Compare correlation patterns to determine if CIDEr optimization aligns with broader caption quality measures or creates metric-specific artifacts.