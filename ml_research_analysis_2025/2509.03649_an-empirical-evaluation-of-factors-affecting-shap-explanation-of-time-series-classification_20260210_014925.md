---
ver: rpa2
title: An Empirical Evaluation of Factors Affecting SHAP Explanation of Time Series
  Classification
arxiv_id: '2509.03649'
source_url: https://arxiv.org/abs/2509.03649
tags:
- segmentation
- time
- series
- background
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of segmentation strategies on
  SHAP-based explanations for time series classification. The authors evaluate eight
  segmentation methods across multiple datasets and classifiers, comparing equal-length
  segmentation against more complex alternatives like ClaSP, information gain, and
  kernel-based methods.
---

# An Empirical Evaluation of Factors Affecting SHAP Explanation of Time Series Classification

## Quick Facts
- **arXiv ID:** 2509.03649
- **Source URL:** https://arxiv.org/abs/2509.03649
- **Reference count:** 30
- **Primary result:** Equal-length segmentation consistently outperforms complex methods for SHAP explanations in time series classification

## Executive Summary
This paper investigates how different segmentation strategies impact SHAP-based explanations for time series classification. The authors evaluate eight segmentation methods across multiple datasets and classifiers, finding that the number of segments has a greater impact on explanation quality than the specific segmentation method. A novel attribution normalization technique is introduced to ensure timepoint-level additivity of segment-based attributions, which consistently improves evaluation scores. The study reveals that equal-length segmentation performs better than most custom algorithms, and that background choice (zero vs. average) has minimal impact overall. The research also highlights inconsistencies between two XAI evaluation measures, suggesting InterpretTime's effectiveness may be limited to deep learning classifiers.

## Method Summary
The study evaluates SHAP explanations for time series classification using eight different segmentation methods including equal-length, SlidingWindow, PCA-based, information gain, kernel-based, topological, dynamic time warping, and ClaSP. The authors introduce a novel attribution normalization technique to address the non-additivity problem when aggregating segment-level attributions to timepoint-level explanations. Experiments are conducted across multiple binary classification datasets and classifiers including ResNet, Fully Convolutional Network, InceptionTime, and COTE classifiers. Evaluation metrics include Precision@1 and InterpretTime, with the latter measuring how well explanations reflect classifier behavior. The research systematically varies segmentation parameters including segment count, overlap, and background choice to assess their impact on explanation quality.

## Key Results
- Equal-length segmentation consistently outperforms most custom segmentation algorithms across datasets and classifiers
- The number of segments has greater impact on explanation quality than the specific segmentation method used
- The proposed attribution normalization technique consistently improves Precision@1 and InterpretTime scores
- Background choice (zero vs. average) has minimal overall impact on explanation quality
- Inconsistencies between Precision@1 and InterpretTime metrics suggest InterpretTime may be unreliable for non-deep learning classifiers

## Why This Works (Mechanism)
Equal-length segmentation provides consistent and interpretable feature attributions because it divides time series data into uniform segments, making the importance scores directly comparable across timepoints. The number of segments being more important than the segmentation method itself suggests that explanation granularity, rather than the specific algorithm used to determine segment boundaries, is the primary driver of explanation quality. The attribution normalization technique works by redistributing segment-level SHAP values proportionally to ensure that timepoint-level attributions sum correctly, addressing the inherent non-additivity problem that arises when segments overlap. This normalization ensures that the total attribution at each timepoint reflects its true contribution to the prediction, making explanations more accurate and interpretable.

## Foundational Learning

**Time Series Segmentation**
- Why needed: Time series data is continuous, but SHAP typically operates on discrete features
- Quick check: Can you explain why continuous time series require segmentation for feature attribution?

**SHAP Explanation Methods**
- Why needed: SHAP provides theoretically grounded feature importance explanations with local accuracy guarantees
- Quick check: What is the key mathematical property that makes SHAP explanations theoretically sound?

**Attribution Normalization**
- Why needed: Segment-based attributions don't naturally sum to timepoint-level values due to overlapping segments
- Quick check: Why does overlapping segmentation create additivity problems in feature attribution?

**XAI Evaluation Metrics**
- Why needed: Quantitative measures are needed to assess explanation quality beyond subjective human judgment
- Quick check: What are the two main types of XAI evaluation metrics used in this study?

## Architecture Onboarding

**Component Map**
Classifier -> SHAP explainer -> Segmentation method -> Attribution normalization -> Evaluation metrics

**Critical Path**
Time series data → Segmentation → Segment-level SHAP values → Attribution normalization → Timepoint-level explanations → Precision@1/InterpretTime evaluation

**Design Tradeoffs**
- Simple equal-length segmentation vs. complex domain-specific methods: simplicity vs. potential performance gains
- Fixed vs. variable segment counts: computational efficiency vs. granularity of explanations
- Zero vs. average background: computational simplicity vs. potential bias in baseline predictions

**Failure Signatures**
- Low Precision@1 scores indicate explanations fail to identify truly important segments
- Inconsistent results between Precision@1 and InterpretTime suggest metric reliability issues
- Poor performance with high segment overlap may indicate over-segmentation problems

**First 3 Experiments to Run**
1. Compare equal-length vs. information gain segmentation on a simple binary classification task
2. Test attribution normalization on overlapping vs. non-overlapping segmentations
3. Evaluate Precision@1 vs. InterpretTime on a ResNet classifier to observe metric consistency

## Open Questions the Paper Calls Out
None

## Limitations
- The finding that segment number outweighs segmentation method choice has Medium confidence due to focus on binary classification tasks
- The attribution normalization technique's practical interpretability improvements remain unverified without human studies
- Background choice experiments have Low confidence for non-time series domains where background distribution might matter more
- Inconsistencies between evaluation measures raise questions about InterpretTime's reliability for non-deep learning classifiers

## Confidence
- Equal-length segmentation superiority: Medium confidence (binary classification focus)
- Attribution normalization effectiveness: Medium confidence (technical property vs. interpretability)
- Background choice impact: Low confidence (domain-specific effects untested)
- InterpretTime metric reliability: Low confidence (inconsistent across classifier types)

## Next Checks
1. Test equal-length vs. custom segmentation methods on multi-class time series classification problems to verify if segment number remains the dominant factor
2. Conduct user studies comparing normalized vs. unnormalized attributions to assess practical interpretability improvements
3. Evaluate attribution methods across additional classifier types (e.g., ensemble methods, support vector machines) to test metric consistency and generalization