---
ver: rpa2
title: Reward-Preserving Attacks For Robust Reinforcement Learning
arxiv_id: '2601.07118'
source_url: https://arxiv.org/abs/2601.07118
tags:
- attacks
- reward-preserving
- robust
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces reward-preserving attacks as a principled\
  \ mechanism for adaptive robustness in reinforcement learning. By controlling the\
  \ magnitude of adversarial perturbations according to the agent\u2019s capabilities,\
  \ these attacks allow policies to experience meaningful challenges without compromising\
  \ the solvability of the task."
---

# Reward-Preserving Attacks For Robust Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.07118
- Source URL: https://arxiv.org/abs/2601.07118
- Reference count: 40
- One-line primary result: Introduces α-reward-preserving attacks to enable adaptive robustness in RL, improving policy resilience to perturbations while maintaining task solvability.

## Executive Summary
This work introduces reward-preserving attacks as a principled mechanism for adaptive robustness in reinforcement learning. By controlling the magnitude of adversarial perturbations according to the agent's capabilities, these attacks allow policies to experience meaningful challenges without compromising the solvability of the task. The immediate societal impact is primarily positive: this method can improve the reliability, safety, and generalization of RL agents in real-world applications, including robotics, autonomous systems, and other safety-critical domains. By training agents under controlled, adaptive perturbations, the approach reduces the likelihood of catastrophic failures when deployed in unpredictable environments. Potential risks include misuse in settings where robust agents are deployed for harmful purposes, and the fact that α-reward-preserving attacks can make the agent perceive normal conditions while it is actually being perturbed, which could be exploited in adversarial or deceptive scenarios. Overall, our work highlights a tool for safer and more resilient RL, emphasizing responsible usage.

## Method Summary
The method operates within the adversarial training paradigm, where an attacker selects observation perturbations bounded by magnitude η. Unlike standard adversarial training, reward-preserving attacks dynamically adjust η to ensure the perturbed environment remains solvable for a policy adapted to the attack. This is achieved by maintaining a Q-network conditioned on attack magnitude η, which estimates the expected return under the reference policy π̃. At each state-action pair, the algorithm computes an α-reward-preserving bound Q̂_α(s,a) and selects the largest feasible η within the uncertainty set that satisfies this bound. Magnitude sampling uses an (ε,α)-preserving distribution, and the reference policy π̃ is updated via Polyak averaging to provide a slowly-moving target for Q-learning. The approach is implemented on top of SAC, with the protagonist policy trained on perturbed transitions and the Q-networks updated via TD learning with importance sampling corrections.

## Key Results
- α-reward-preserving attacks enable dynamic selection of perturbation magnitudes that maintain task solvability while providing robust training pressure.
- Empirical evaluation on HalfCheetah-v5 shows that intermediate α values (0.3–0.7) yield policies that are both robust to perturbations and preserve nominal performance.
- The method improves generalization to unseen attack types and perturbation magnitudes compared to standard adversarial training.

## Why This Works (Mechanism)

### Mechanism 1: State-Conditional Magnitude Selection via Reward Preservation Constraint
Constraining the adversary to preserve an α-fraction of the nominal-to-worst-case return gap at each state-action pair prevents task collapse while maintaining meaningful training pressure. At each (s,a), the algorithm computes a threshold Q̂_α(s,a) = (1-α)Q*_worst(s,a) + αQ*_nominal(s,a). Attack magnitude η is then selected as the largest value within the uncertainty set B(s,a) such that Q_π((s,a),η) ≥ Q̂_α(s,a). This ensures the perturbed environment remains solvable for a policy adapted to the attack.

### Mechanism 2: Magnitude-Parametrized Q-Network for Dynamic Estimation
Learning a Q-network conditioned on attack magnitude η enables online selection of appropriate perturbation strengths without requiring exhaustive rollouts at each step. A neural network Q_ψ((s,a),η) is trained to predict expected return under the reference policy π̃ with magnitude η. During training, magnitudes are sampled across [0, η_B] using an (ε,α)-reward-preserving distribution that allocates (1-ε) mass within the feasible set. The network learns via temporal difference updates with importance sampling corrections.

### Mechanism 3: Two-Timescale Learning with Reference Policy Lag
Separating the policy learning timescale from the magnitude-estimation timescale stabilizes the joint optimization. The reference policy π̃ is updated via Polyak averaging (π̃ ← (1-τ)π̃ + τπ) with τ=0.1, providing a slowly-moving target for Q-network training. This allows Q-value estimates to track the evolving state-occupancy distribution while preventing catastrophic feedback loops where policy degradation leads to worse magnitude estimates.

## Foundational Learning

- **Concept**: Robust Markov Decision Processes and Robust Value Iteration
  - Why needed here: The α-reward-preserving attacks extend classical robust MDP theory by constraining uncertainty sets via value-function bounds rather than fixed radii.
  - Quick check question: Given an SA-rectangular uncertainty set B(s,a), what does the robust Bellman operator compute?

- **Concept**: Adversarial Training in Reinforcement Learning
  - Why needed here: The method operates within the adversarial training paradigm where an attacker ξ minimizes protagonist return under constrained perturbations.
  - Quick check question: In RARL-style training, how does per-step adversarial policy selection differ from episode-level domain randomization?

- **Concept**: Soft Actor-Critic (SAC) and Off-Policy Importance Sampling
  - Why needed here: The protagonist uses SAC, and magnitude Q-learning requires importance sampling weights w_t = π̃(a|s)/π(a|s) to correct for policy drift.
  - Quick check question: Why does off-policy TD learning require importance sampling when the behavior policy differs from the target policy?

## Architecture Onboarding

- **Component map**: Protagonist policy π_θ -> Attack direction selector ξ_A -> Dynamic Q-network Q_ψ_α((s,a),η) -> Magnitude selector -> Perturbed environment -> Reference policy π̃_θ̃ (Polyak update) -> Static Q-network Q_ψ_c(s,η)

- **Critical path**: 1. At state s, query Q_ψ_α and Q_ψ_c to compute Q̂_α(s,a) and feasible magnitude set B̃_α(s,a) 2. Find η* = argmin_η Q_ψ_α((s,a),η) within B̃_α(s,a) 3. Sample η ~ p̃_α(·|s,a) from exponential distribution centered at η* 4. Generate attack direction A via gradient ascent on critic loss (FGM-QAC) 5. Execute perturbed observation x' = x + ηA in environment 6. Store transition with importance weight denominator p(a|s) 7. Update π_θ via SAC, Q_ψ networks via weighted TD, and π̃ via Polyak averaging

- **Design tradeoffs**: α selection: Lower α → stronger robustness but higher conservatism; intermediate values [0.3, 0.7] perform best empirically; η_B vs η_B+: Using η_B+ > η_B allows occasional out-of-distribution attacks, improving generalization; ε parameter for magnitude sampling: Controls tail probability of sampling outside B̃_α; ε=0.01 is default; Network architecture: Both Q-networks use 4-layer MLP [256,256,256,256]; deeper than typical critics

- **Failure signatures**: 1. Magnitude collapse: Q-network outputs similar values for all η → selection oscillates; symptom: training returns plateau with high variance 2. Conservative policy: α too high → attacks too weak → no robustness gain; symptom: test performance under perturbation mirrors baseline 3. Q-network divergence: Importance weights explode due to π̃-π divergence; symptom: TD errors grow unbounded, NaN losses 4. Task unsolvability: η_B too large for environment; symptom: even α=0.95 fails to maintain non-zero returns

- **First 3 experiments**: 1. Calibration sweep: Train Q-networks on a frozen pre-trained agent with varying α ∈ [0.1, 0.9], η_B=0.3; verify that mean perturbation magnitude decreases monotonically with α (replicate Figure 2) 2. Ablation on sampling strategy: Compare (ε,α)-preserving sampling vs uniform sampling vs greedy η* selection on HalfCheetah-v5; measure final returns under η ∈ {0, 0.05, 0.1, 0.15} 3. Cross-attack transfer: Train with FGM-QAC direction, evaluate under RUA and FGM-C attacks; verify robustness generalizes beyond training attack (as shown in Appendix Figure 8)

## Open Questions the Paper Calls Out

- **Open Question 1**: How does α-reward-preserving training perform under dynamics perturbations compared to observation perturbations? The paper notes experiments for dynamics attacks are left for future work, as all empirical validation was limited to observation-space perturbations.

- **Open Question 2**: What theoretical properties determine the optimal α value for a given task and uncertainty set? The paper demonstrates that intermediate α values (0.3–0.7) outperform extremes, but the relationship between α, the structure of the uncertainty set B, and task characteristics remains uncharacterized.

- **Open Question 3**: Can alternative magnitude sampling distributions (beyond the exponential distribution) improve the trade-off between exploration and reward preservation? The paper suggests truncated Gaussian, mixture, or epsilon-greedy strategies could be considered, but does not evaluate them.

## Limitations

- All empirical validation is performed on a single MuJoCo locomotion task (HalfCheetah-v5), limiting generalizability to other domains.
- The method's assumptions about smooth value functions and continuous perturbation spaces may not hold universally, particularly in discrete action spaces.
- Hyperparameter sensitivity to α, η_B, and reference policy update rate τ_ref is not systematically analyzed, and optimal configurations may vary significantly across environments.

## Confidence

- **High Confidence**: The theoretical framework for α-reward-preserving attacks (Definition 2.1, Section 2.2) and the core mechanism of dynamic magnitude selection via constrained Q-learning are well-specified and internally consistent.
- **Medium Confidence**: The empirical results showing improved robustness across perturbation magnitudes in HalfCheetah-v5 are compelling, but the single-task evaluation limits generalizability. The theoretical justification for two-timescale learning (Section A.4.2) is sound, though its practical necessity is not rigorously established.
- **Low Confidence**: The claim that reward-preserving attacks provide a "principled" mechanism for adaptive robustness compared to existing methods is not substantiated by ablation studies or comparisons against alternative adaptive strategies. The societal impact assessment is brief and lacks concrete risk mitigation strategies.

## Next Checks

1. **Multi-Task Robustness Evaluation**: Reproduce the HalfCheetah-v5 experiments to establish baseline performance, then extend evaluation to at least two additional MuJoCo tasks (e.g., Walker2d-v5, Hopper-v5) with varying dynamics complexity. Measure returns under a comprehensive grid of perturbation magnitudes and compare against standard adversarial training baselines.

2. **Ablation on Magnitude Sampling Strategy**: Systematically compare (ε,α)-reward-preserving sampling against uniform sampling, greedy η* selection, and epsilon-greedy strategies. Use statistical tests to determine significance of performance differences under various α settings and perturbation regimes.

3. **Cross-Attack Robustness Transfer**: Train agents with FGM-QAC attacks and evaluate robustness against RUA, FGM-C, and random attack directions not seen during training. Quantify the generalization gap and analyze whether reward-preserving constraints improve cross-attack transfer compared to unconstrained adversarial training.