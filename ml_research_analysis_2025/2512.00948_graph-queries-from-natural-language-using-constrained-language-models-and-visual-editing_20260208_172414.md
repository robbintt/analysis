---
ver: rpa2
title: Graph Queries from Natural Language using Constrained Language Models and Visual
  Editing
arxiv_id: '2512.00948'
source_url: https://arxiv.org/abs/2512.00948
tags:
- graph
- query
- queries
- system
- prototype
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to convert natural language queries
  into prototype graphs for querying knowledge graphs. The approach uses a two-step
  constrained language model generation, first generating an unconstrained graph,
  then aligning it to the ontology using semantically similar classes and links.
---

# Graph Queries from Natural Language using Constrained Language Models and Visual Editing

## Quick Facts
- arXiv ID: 2512.00948
- Source URL: https://arxiv.org/abs/2512.00948
- Reference count: 36
- Primary result: Uses constrained language models to convert natural language to prototype graphs for knowledge graph queries, achieving F1 scores of ~0.7 for nodes and ~0.8 for relations

## Executive Summary
This paper introduces a method to convert natural language queries into prototype graphs for querying knowledge graphs. The approach uses a two-step constrained language model generation, first generating an unconstrained graph, then aligning it to the ontology using semantically similar classes and links. This ensures the output graph is valid within the ontology constraints without requiring iterative corrections. The prototype graph serves as an intermediate representation that can be refined visually by users. The system is evaluated using synthetic queries generated from sampled subgraphs across four ontologies, showing F1 scores of ~0.7 for node retrieval and ~0.8 for relation retrieval. A preliminary user study indicates high task completion rates and usability.

## Method Summary
The system converts natural language to prototype graphs through a two-step constrained language model generation process. First, an unconstrained graph is extracted from natural language using a Language Model (LM) constrained only by a JSON schema. This graph's nodes and edges are then used to retrieve semantically similar classes and links from the target ontology. These retrieved subsets are used to construct a new, dynamic grammar which constrains a second LM generation pass, forcing the output to use only valid ontology elements. The resulting prototype graph serves as an intermediate representation that can be refined visually by users, and is then converted to a valid SPARQL query.

## Key Results
- F1 scores of ~0.7 for node retrieval and ~0.8 for relation retrieval across four ontologies
- Consistently generates valid SPARQL queries without requiring syntax corrections
- Works efficiently with smaller models (≤8B parameters) compared to prior approaches
- High task completion rates in preliminary user study (n=11)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraining language model output to ontology-specific grammars eliminates invalid SPARQL queries.
- Mechanism: The system uses a two-step generation process: first, an unconstrained graph $G''_p$ is extracted from natural language using a Language Model (LM) constrained only by a JSON schema (GBNF). This graph's nodes and edges are then used to retrieve semantically similar classes and links from the target ontology. These retrieved subsets are then used to construct a new, dynamic grammar which constrains a second LM generation pass, forcing the output to use only valid ontology elements.
- Core assumption: Language models can reliably generate structured graph representations from text, and semantic similarity search can effectively map unconstrained terms to specific ontology classes and links.
- Evidence anchors:
  - [abstract] "Our approach converts natural language to these prototype graphs by utilizing a two-step constrained Language Model (LM) generation based on semantically similar features within an ontology."
  - [Page 3, Section III-A3] "This limited set of only semantically similar classes enables the LM still to express any graph from the NL query while adhering to the relevant query constraints."
- Break condition: The mechanism may fail if the semantic similarity search retrieves incorrect classes/links, or if the initial unconstrained graph is too far from the intended meaning, leaving the constrained generation with poor candidates.

### Mechanism 2
- Claim: A prototype graph serves as an intermediate representation that decouples natural language understanding from SPARQL generation, guaranteeing syntactic validity.
- Mechanism: Instead of directly translating natural language to SPARQL, the system first maps to a prototype graph ($G_p$). This graph is a subgraph of the ontology schema, consisting of valid classes and links. A separate, deterministic process then converts this validated graph structure into a SPARQL query. By shifting the LM's task to graph construction within a constrained schema, the system avoids SPARQL syntax errors and hallucinated properties.
- Core assumption: A prototype graph structure can be consistently translated to a valid SPARQL query that captures the user's intent.
- Evidence anchors:
  - [abstract] "Our approach consistently generates a valid SPARQL query within the constraints imposed by the ontology, without requiring any additional corrections to the syntax or classes and links used."
  - [Page 1, Introduction] "This novel addition of the grammar to the query generation by the LM enables our system, in turn, to always return prototype graphs that are valid within the context of the used ontology."
- Break condition: The mechanism will break if the mapping from prototype graph to SPARQL is incomplete (e.g., cannot handle complex filters or aggregations) or if the generated graph, while schema-valid, does not reflect the user's query intent.

### Mechanism 3
- Claim: Visual editing of the prototype graph allows for user-driven correction and refinement, compensating for LM errors.
- Mechanism: The system provides a node-based visual editor where users can see and modify the prototype graph generated by the LM. Users can add/remove nodes and links, all while being constrained to valid ontology elements. This human-in-the-loop step allows for the correction of semantic errors in the LM's initial interpretation without requiring the user to write SPARQL.
- Core assumption: Users can effectively understand and manipulate a graph representation of their query better than they can write or debug SPARQL code.
- Evidence anchors:
  - [abstract] "The resulting prototype graph serves as the building block for further user refinements within a dedicated visual query builder."
  - [Page 5, Section III-D] "Additionally, we allow the users to refine their initial queries using a node-based editor... This refinement can be helpful if our pipeline fails to find the correct graph..."
- Break condition: The mechanism assumes users are comfortable with graph-based representations. If the graph is too complex or the ontology unfamiliar, visual editing may become difficult.

## Foundational Learning

- Concept: **Knowledge Graphs and Ontologies**
  - Why needed here: The entire system is built to query Knowledge Graphs (KGs) using their governing Ontologies. You must understand that an ontology defines the schema (classes, links, properties) and a KG is the data (instances) that conform to that schema.
  - Quick check question: What is the difference between a class in an ontology and an instance in a knowledge graph?

- Concept: **SPARQL Query Language**
  - Why needed here: The system's ultimate goal is to generate valid SPARQL queries. Understanding SPARQL syntax and its Basic Graph Patterns (BGP) is crucial to comprehend what the system is trying to produce and why direct generation is error-prone.
  - Quick check question: What does a SPARQL query do, and what is a basic graph pattern within it?

- Concept: **Language Model Constraints**
  - Why needed here: The core technical contribution is using constraints (grammars) to force a language model to produce valid output. You need to understand that LMs can be guided to produce output that conforms to a predefined structure (like JSON or a custom grammar) rather than free-form text.
  - Quick check question: How can a grammar be used to force a language model to only select from a specific list of words or phrases in its output?

## Architecture Onboarding

- Component map:
  1. **NL to Unconstrained Graph ($G''_p$):** An LM (e.g., Hermes 3B) takes the user's natural language query and outputs a JSON-based graph structure with no ontology constraints.
  2. **Semantic Search & Candidate Retrieval:** The classes and links from $G''_p$ are embedded (e.g., using Stella 400M). These embeddings are used to find the top-k most similar classes and links within the target ontology.
  3. **Constrained Graph Generation ($G'_p$):** The LM is called again. This time, its output is constrained by a dynamically generated grammar that only permits it to use the classes and links retrieved in the previous step.
  4. **Graph Refinement & SPARQL Translation:** The constrained output is cleaned (e.g., flipping edge directions) to form the final prototype graph $G_p$. A deterministic converter then translates this graph into a SPARQL query.
  5. **Visual Editor (OnSET):** The prototype graph $G_p$ is rendered in a node-link editor, allowing users to make manual adjustments before final execution.

- Critical path:
  User Query -> Unconstrained Graph Generation -> Semantic Search -> Constrained Graph Generation -> SPARQL Translation -> User Visualization/Refinement -> KG Execution

- Design tradeoffs:
  - **Constraint vs. Flexibility:** The system prioritizes generating a valid query over perfectly capturing user intent on the first try. It relies on the visual editor to fix any semantic mismatches.
  - **Efficiency vs. Model Size:** The paper argues that their constrained approach allows smaller, more efficient models (≤8B parameters) to perform well, trading off the raw power of large models for a structured, efficient pipeline.
  - **Evaluation Scope:** The system is evaluated on synthetic queries, which may not capture all the nuances of real-world user queries. A user study is mentioned but is preliminary.

- Failure signatures:
  - **Poor Semantic Retrieval:** If the semantic search fails to find the correct ontology classes/links, the constrained generation will be forced to use incorrect terms.
  - **Overly Complex Queries:** Performance (F1 score, GED) drops as the number of nodes/links in the query increases. The system is better suited for simpler queries.
  - **Invalid Graph Structure:** The constrained generation can still produce structurally invalid graphs (e.g., flipped links), which the post-processing rules might not fully correct.

- First 3 experiments:
  1. **Ablation Study (Raw vs. Aligned):** Run the system with and without the constrained generation and alignment step. Compare the F1 scores and Graph Edit Distance to measure the direct impact of the core mechanism.
  2. **Model Size Comparison:** Evaluate the system's performance using models of different sizes (e.g., 3B, 8B, 70B parameters) to verify the claim that smaller models are sufficient due to the constraints.
  3. **Ontology Portability Test:** Apply the system to a different ontology not used in the original paper (e.g., a domain-specific one) to test its generalizability and robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the constrained generation pipeline be extended to extract and apply specific property value constraints (e.g., numerical filters or string matches) from natural language directly into the SPARQL query?
- Basis in paper: [explicit] The Future Work section states, "we do not provide a way for the LM to add this [constraints] to the generated query," noting that current filters are manual.
- Why unresolved: The current system maps structural elements (classes/links) but lacks a mechanism to represent literal constraints (e.g., "age > 18") within the intermediate prototype graph.
- What evidence would resolve it: An updated graph schema and pipeline capable of extracting literals, evaluated on a benchmark requiring filtered retrieval.

### Open Question 2
- Question: Can fine-tuning smaller, specialized models improve the extraction accuracy of prototype graphs for larger, more complex queries where current performance drops?
- Basis in paper: [explicit] The authors suggest utilizing "fine-tuning smaller, more specialized models" to enhance robustness, specifically for larger queries.
- Why unresolved: The results show a decline in Graph Edit Distance scores as query complexity increases, and it is unclear if specialized training can mitigate this.
- What evidence would resolve it: A comparative evaluation of fine-tuned models versus general-purpose models on high-complexity synthetic queries (e.g., $k \geq 7$).

### Open Question 3
- Question: Do the high task completion rates and System Usability Scale (SUS) scores observed in the preliminary study (n=11) persist across a larger participant pool and more diverse querying tasks?
- Basis in paper: [explicit] The authors identify the need to "extend the user study to include more users... and possibly more challenging tasks" to validate the interface's usability comprehensively.
- Why unresolved: A small sample size may not capture the variability in user behavior or the specific difficulties non-experts face with complex ontologies.
- What evidence would resolve it: A large-scale user study with statistical significance testing comparing the proposed system against baseline visual query builders.

## Limitations

- Performance drops significantly for complex queries with many nodes and edges
- Evaluation relies entirely on synthetic queries rather than real user interactions
- User study is preliminary with limited scope (n=11 participants)

## Confidence

- Valid SPARQL Generation Mechanism: High
- Performance on Synthetic Queries: Medium  
- Real-World Applicability: Low
- User Study Results: Low

## Next Checks

1. Conduct evaluation with real user queries across multiple domains to test robustness beyond synthetic data
2. Test system performance on complex queries with 10+ nodes/edges to identify scalability limits
3. Perform ablation study removing visual editing to quantify its contribution to overall accuracy