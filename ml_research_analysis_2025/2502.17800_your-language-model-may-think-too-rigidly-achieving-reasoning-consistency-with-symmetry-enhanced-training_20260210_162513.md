---
ver: rpa2
title: 'Your Language Model May Think Too Rigidly: Achieving Reasoning Consistency
  with Symmetry-Enhanced Training'
arxiv_id: '2502.17800'
source_url: https://arxiv.org/abs/2502.17800
tags:
- reasoning
- arxiv
- value
- query
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of reasoning consistency in large
  language models, where minor variations in query phrasing can significantly affect
  performance even when semantic meaning is preserved. The authors propose MEND (syMmetry-ENhanceD
  Data Augmentation), a data-centric approach that improves model robustness by augmenting
  training data with symmetry information through query transformations including
  order permutation and redundancy addition.
---

# Your Language Model May Think Too Rigidly: Achieving Reasoning Consistency with Symmetry-Enhanced Training

## Quick Facts
- **arXiv ID:** 2502.17800
- **Source URL:** https://arxiv.org/abs/2502.17800
- **Reference count:** 27
- **Primary result:** MEND achieves superior reasoning consistency across query variations compared to baselines, with notable improvements in out-of-distribution settings

## Executive Summary
This paper addresses the problem of reasoning consistency in large language models, where minor variations in query phrasing can significantly affect performance even when semantic meaning is preserved. The authors propose MEND (syMmetry-ENhanceD Data Augmentation), a data-centric approach that improves model robustness by augmenting training data with symmetry information through query transformations including order permutation and redundancy addition. Extensive experiments on logical and arithmetic reasoning tasks demonstrate that MEND achieves superior sampling efficiency and generalizability compared to baselines, with notable improvements in out-of-distribution settings and reasoning consistency metrics. The approach effectively enhances the model's ability to extract invariant knowledge from queries, leading to more robust reasoning across diverse surface forms.

## Method Summary
MEND augments training data by transforming queries through order permutation (shuffling premise order) and redundancy addition (inserting irrelevant nodes disconnected from the reasoning DAG). The augmented dataset is then used for supervised fine-tuning (SFT) on standard LLMs. The approach aims to improve the model's ability to extract invariant knowledge representations P(z|q) from semantically equivalent but syntactically different queries. Evaluation includes standard accuracy metrics plus Variance of Variations (VoV) to measure reasoning consistency across perturbed inputs, and attention-based probing to verify knowledge extraction improvements.

## Key Results
- MEND outperforms vanilla SFT and RC-Aug on accuracy and VoV across multiple reasoning tasks
- Models trained with MEND maintain >70% accuracy even with 40 redundant premises, while baselines drop to ~20%
- Attention probing shows MEND achieves F1-macro scores ~0.7 versus baseline scores of ~0.45 for detecting useful statements
- MEND demonstrates superior sampling efficiency, achieving 1.5-2x performance with the same dataset size

## Why This Works (Mechanism)

### Mechanism 1: Symmetry-Aware Knowledge Extraction
Training on query variations with preserved semantics improves the model's ability to extract invariant knowledge representations. MEND augments training data with order permutations and redundancy additions that preserve semantic meaning, forcing the model to learn representations z that remain consistent across surface form variations. The paper formalizes this as improving P(z|q)—the in-context knowledge extraction probability in the equation P(y|q) = ∫P(y_t|y<t, z, q)P(z|q)dz.

### Mechanism 2: Attention-Based Information Filtering
MEND improves the model's ability to attend to relevant premises and filter distractor information. By training with redundant information additions (irrelevant nodes disconnected from the reasoning DAG), models learn attention patterns that better distinguish useful premises from noise. Attention patterns serve as reliable indicators of whether models can retrieve useful information, and this capability transfers to OOD queries.

### Mechanism 3: Distributional Robustness via Data Augmentation
Exposing models to diverse surface forms during training reduces overfitting to specific query structures (e.g., topological ordering, redundancy-free inputs). Standard SFT datasets contain implicit biases—premises typically appear in topological order without distractions. MEND breaks these correlations, preventing models from learning positional heuristics instead of genuine reasoning.

## Foundational Learning

### Concept: Directed Acyclic Graph (DAG) Representation
**Why needed here:** The paper formalizes all reasoning tasks as DAGs where nodes are variables and edges are dependencies. Understanding this abstraction is essential for distinguishing semantic meaning (the DAG structure) from surface form (the natural language description).
**Quick check question:** Given a query with premises about nodes A, B, C where A depends on B and C, can you sketch the DAG and explain why shuffling premise order preserves it?

### Concept: Topological Sorting
**Why needed here:** The paper uses topological order as the "canonical" ordering aligned with valid reasoning chains. Models overfit to this order and fail on random/reversed permutations—this is the core pathology MEND addresses.
**Quick check question:** If premises are presented in reverse topological order (root dependencies first, leaves last), why might a model trained only on topological-order data struggle?

### Concept: Attention Probing with Linear Classification
**Why needed here:** The paper verifies MEND's mechanism through attention-based probing—a binary classification task predicting whether each statement contains useful information, using simplified attention weights as features.
**Quick check question:** A model achieves F1-macro = 0.65 on probing versus baseline 0.45. What does this suggest about its ability to distinguish relevant vs. irrelevant premises?

## Architecture Onboarding

### Component Map:
Data Augmentation Pipeline -> SFT Training Loop -> Probing Module (Evaluation)

### Critical Path:
1. Parse original query q into partition list L using `"\n"` delimiter
2. Shuffle L (order permutation step)
3. Sample R redundant nodes with random values and edges (disconnected from original DAG)
4. Generate redundant text partitions using templates; insert at random positions
5. Reassemble into augmented query q'; add (q', original answer) to Daug
6. Train model with SFT on Daug; evaluate on permuted/redundant test sets

### Design Tradeoffs:
- **Augmentation factor K**: Higher K → more robustness but longer training; paper uses K=4 implicitly
- **Redundancy count R (training)**: Training with R=0–4 redundancies prepares models for test-time R up to 40; tradeoff is training data complexity
- **Partition strategy**: Newline splitting assumes clean premise boundaries; may fail on unstructured text

### Failure Signatures:
- **Order overfitting**: High accuracy on Topological test set (>90%), catastrophic drop on Random/Reversed (<30%)
- **Distractibility**: Accuracy declines monotonically as test-time redundancy increases (see Figure 4 Vanilla curves)
- **High VoV**: Variance of Variations metric >> baseline indicates inconsistent reasoning; MEND should achieve VoV close to baseline's minimum

### First 3 Experiments:
1. **Baseline overfitting diagnosis**: Run Vanilla SFT on arithmetic reasoning (difficulty levels 1–4); evaluate on Topological, Random, and Reversed test sets. Expect: Topological >> Random >> Reversed accuracy, confirming order sensitivity.
2. **MEND vs. RC-Aug ablation**: Train two models—RC-Aug (augments reasoning chains, keeps queries fixed) and MEND (augments queries). Test on 0–40 redundancies with all three orderings. Expect: MEND maintains >70% accuracy in OOD; RC-Aug degrades similarly to Vanilla.
3. **Attention probing validation**: Extract attention weights from Vanilla and MEND models; train linear probe to classify useful vs. redundant statements. Plot F1-macro vs. redundancy count. Expect: MEND shows stable/high F1 (~0.7); Vanilla and RC-Aug decline with more redundancy.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** How can MEND be extended to handle surface-level linguistic variations (paraphrasing), which were excluded from this study?
**Basis in paper:** [explicit] Section 4.1 states that the authors primarily focus on permutation and redundancy, "leaving the third [Surface-level variations] for future research" due to the complexity of linguistic paraphrasing.
**Why unresolved:** The proposed augmentation relies on structural shuffling and explicit redundancy addition, which does not address semantic equivalence expressed through different vocabulary or syntax.
**What evidence would resolve it:** An extension of the framework that successfully augments datasets with linguistic paraphrases, leading to improved consistency on linguistic variation benchmarks.

### Open Question 2
**Question:** Is MEND effective for improving reasoning consistency in complex domains such as scientific reasoning or code generation?
**Basis in paper:** [explicit] The "Limitations" section explicitly states that the method's "potential and effectiveness on more complex tasks or domains such as specialized scientific reasoning and code generation remain underexplored."
**Why unresolved:** The current evaluation is restricted to logical and arithmetic reasoning tasks that can be represented as Directed Acyclic Graphs (DAGs).
**What evidence would resolve it:** Experimental results applying MEND to specialized benchmarks (e.g., MATH or MBPP) demonstrating consistency improvements in these domains.

### Open Question 3
**Question:** Does the performance improvement from MEND scale effectively to models significantly larger than 3B parameters?
**Basis in paper:** [inferred] The supervised fine-tuning experiments are restricted to Llama-3.2 1B and 3B models, leaving the efficacy on larger foundation models unverified.
**Why unresolved:** Larger models may possess inherent robustness to surface form variations or may respond differently to the specific augmentation strategies used in MEND.
**What evidence would resolve it:** Evaluations of MEND-augmented SFT on 7B or 70B parameter models, showing consistent reductions in Variance of Variations (VoV).

## Limitations

- The paper does not specify exact SFT hyperparameters (learning rate, batch size, epochs, optimizer), making faithful reproduction challenging.
- The augmentation parameters (K and R values) used in training are not fully specified—while Figure 5 shows dataset size multipliers, the exact augmentation strategy for each dataset is unclear.
- The linear probing methodology for attention-based knowledge extraction lacks details on training procedure, regularization, and validation.

## Confidence

- **High confidence** in the core mechanism: Symmetry augmentation improving reasoning consistency is well-supported by controlled experiments showing clear performance gaps between MEND and baselines across multiple perturbation types.
- **Medium confidence** in the attention probing results: While F1-macro scores are reported, the simplified attention representation (Asimp) and probing methodology lack sufficient detail for independent validation.
- **Medium confidence** in the generalizability claims: Results on PromptBench tasks are compelling, but the paper doesn't extensively test on diverse reasoning domains or examine potential negative transfer to surface-form-sensitive tasks.

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary SFT learning rates (1e-5, 5e-6, 1e-6) and augmentation factors (K=2,4,8) to determine if performance gains are robust to training configuration or specific to particular settings.

2. **Cross-domain generalization test**: Apply MEND to non-DAG reasoning tasks (e.g., commonsense reasoning, mathematical word problems) to verify that symmetry training benefits extend beyond the PromptBench benchmark and don't introduce negative transfer.

3. **Surface-form sensitivity evaluation**: Test MEND-trained models on tasks requiring surface-form discrimination (e.g., sentiment analysis, style transfer, or paraphrase detection) to confirm that symmetry training doesn't degrade sensitivity to semantically meaningful but surface-different expressions.