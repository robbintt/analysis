---
ver: rpa2
title: 'Computational Social Linguistics for Telugu Cultural Preservation: Novel Algorithms
  for Chandassu Metrical Pattern Recognition'
arxiv_id: '2510.01233'
source_url: https://arxiv.org/abs/2510.01233
tags:
- step
- telugu
- chandassu
- metrical
- yati
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first computational framework for analyzing
  Telugu Chandassu, a classical metrical poetry tradition. The authors address the
  lack of digital tools for preserving and analyzing this cultural heritage by developing
  specialized algorithms for prosody-aware tokenization and metrical pattern recognition.
---

# Computational Social Linguistics for Telugu Cultural Preservation: Novel Algorithms for Chandassu Metrical Pattern Recognition

## Quick Facts
- arXiv ID: 2510.01233
- Source URL: https://arxiv.org/abs/2510.01233
- Reference count: 15
- Introduces first computational framework for analyzing Telugu Chandassu classical metrical poetry with 91.73% accuracy

## Executive Summary
This paper presents the first computational framework for analyzing Telugu Chandassu, a classical metrical poetry tradition. The authors address the lack of digital tools for preserving and analyzing this cultural heritage by developing specialized algorithms for prosody-aware tokenization and metrical pattern recognition. Their approach involves creating a dataset of 4,651 annotated padyams and designing an AksharamTokenizer to identify human-perceivable character units. The framework includes modules for laghuvu-guruvu classification, ganam sequence matching, and constraint validation, achieving strong performance across multiple metrics while demonstrating how computational methods can preserve endangered cultural knowledge systems.

## Method Summary
The authors developed a rule-based computational framework for Telugu Chandassu analysis centered on prosody-aware tokenization at the aksharam (syllabic unit) level. They created an AksharamTokenizer using Unicode grapheme cluster detection and Telugu-specific rules to handle conjunct consonants, vowel diacritics, and the POLLU marker. The framework processes text through laghuvu-guruvu classification (binary light/heavy syllabic weight determination), ganam sequence matching (pattern recognition of prosodic building blocks), and constraint validation against type-specific configurations. The system evaluates five sub-scores (aksharam count, paadam count, ganam sequences, yati positions, prasa patterns) which are averaged to produce an overall Chandassu Score.

## Key Results
- Achieves 91.73% accuracy on the proposed Chandassu Score metric
- Near-perfect performance on structural metrics: 99.43% for aksharam count, 99.93% for paadam count
- Strong prosodic accuracy: 93.82% for ganam sequences, 94.54% for prasa patterns
- Yati (caesura) detection is most challenging at 78.69% accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Prosody-aware tokenization at the aksharam level enables accurate metrical analysis that conventional word-level or subword tokenizers cannot achieve.
- **Mechanism:** The AksharamTokenizer processes Telugu script character-by-character using Unicode grapheme cluster detection (`\X` regex), then applies rule-based logic to group codepoints into human-perceivable aksharam units. It handles conjunct consonants (dwitwaaksharamulu, samyuktaaksharamulu), vowel diacritics, and the POLLU marker through conditional concatenation rules (Algorithm 4.1, Steps 10-21).
- **Core assumption:** The minimal perceptible unit for Telugu prosody is the aksharam, not the morpheme or word. This assumption is grounded in classical Telugu prosodic theory.
- **Evidence anchors:**
  - [abstract] "AksharamTokenizer for prosody-aware tokenization"
  - [Section 2.1] "existing approaches operate at granularities incompatible with prosodic analysis requirements"
  - [corpus] No directly comparable corpus evidence exists for Telugu-specific tokenization; related work on multilingual tokenization (Karthika et al.) addresses Indian languages but not prosodic requirements.
- **Break condition:** If aksharam boundaries were ambiguous due to Unicode normalization differences or script variants not covered by the rule set, tokenization accuracy would degrade.

### Mechanism 2
- **Claim:** Binary laghuvu-guruvu classification based on phonological duration and contextual factors provides the foundation for all higher-level metrical analysis.
- **Mechanism:** The LaghuvuGuruvu Generator (Algorithm 4.2) examines each aksharam token's final character, consults a predefined mapping (`lg_map`), and applies contextual rules. Crucially, it evaluates the *following* aksharam to determine if consonant clusters affect weight. For conjunct consonants containing subscript "Ra" (Ra-vattu), the algorithm systematically excludes this component to ensure consistent weight determination (Steps 19-37).
- **Core assumption:** Classical Telugu prosodic rules for syllabic weight are consistent and can be formalized algorithmically. Assumption: The systematic treatment of Ra-vattu approximates traditional expert judgment.
- **Evidence anchors:**
  - [Section 4.2] "classification... based on traditional Telugu prosodic rules [1, 13]"
  - [Table 5] 93.78% gana_kramam_score for Vruttamu class suggests classification is sufficiently accurate for downstream tasks
  - [corpus] No corpus papers validate Telugu syllabic weight classification; this mechanism remains internal to the paper.
- **Break condition:** If classical prosodic rules contained undocumented exceptions or if Ra-vattu treatment differs across poetic traditions, classification accuracy would drop, cascading to ganam matching failures.

### Mechanism 3
- **Claim:** Multi-dimensional constraint validation against type-specific configurations enables comprehensive metrical correctness assessment.
- **Mechanism:** The PadyaBhedam Checker (Algorithm 4.3) loads predefined constraints from `padyam_config`, including expected paadam counts, aksharam counts per line, ganam sequences, yati positions, and prasa requirements. It computes five sub-scores (n_aksharaalu, n_paadalu, gana_kramam, yati, prasa) and aggregates them via arithmetic averaging (Equation 1) to produce the Chandassu Score.
- **Core assumption:** Traditional prosodic constraints are well-documented and can be exhaustively encoded. Assumption: The Chandassu Score's equal weighting of constraints reflects literary standards.
- **Evidence anchors:**
  - [abstract] "algorithm achieves 91.73% accuracy on the proposed Chandassu Score"
  - [Section 5] Equation 1 defines the aggregation; [Section 6] reports 78.69% yati score as the most challenging constraint
  - [corpus] No corpus papers validate this specific evaluation metric; the Chandassu Score is novel to this work.
- **Break condition:** If padyam types not in the configuration are encountered, or if sandhi (euphonic combinations) alter surface forms significantly, the checker would fail without extension.

## Foundational Learning

- **Concept: Aksharam (Telugu syllabic unit)**
  - Why needed here: The entire framework operates at aksharam granularity, not word or character level. Understanding this unit is prerequisite to following tokenization and weight classification.
  - Quick check question: Can you explain why a consonant cluster like "ksha" might be treated as one aksharam rather than two?

- **Concept: Laghuvu/Guruvu binary prosodic weight**
  - Why needed here: All higher-level ganam patterns and metrical validation depend on correct light/heavy classification. This is the fundamental binary distinction in Telugu prosody.
  - Quick check question: Given a Telugu syllable ending in a long vowel versus a short vowel with consonant cluster, which would likely be classified as guruvu (heavy)?

- **Concept: Ganam as metrical building block**
  - Why needed here: Ganams are sequential patterns of laghuvu-guruvu combinations (e.g., BHA = U||, JA = |U|). Pattern matching against expected ganam sequences is the core prosodic validation task.
  - Quick check question: If a line has the pattern "||U|U|U", which ganams from Table 1 could it contain?

## Architecture Onboarding

- **Component map:**
  INPUT_TEXT → AksharamTokenizer → aksharam tokens
           → LaghuvuGuruvuGenerator → (token, weight) pairs
           → PadyaBhedam Checker → loads padyam_config + ganam mappings
                                    → validates against constraints
                                    → outputs Chandassu Score + micro_scores

- **Critical path:** AksharamTokenizer → LaghuvuGuruvuGenerator → ganam sequence matching (gana_kramam_score). This path determines 93.82% of metrical accuracy. Failures here cascade to all downstream scores.

- **Design tradeoffs:**
  - Rule-based vs. learned: The authors chose rule-based tokenization and classification for interpretability and alignment with traditional prosody, trading off potential robustness to dialectal variation.
  - Equal weighting in Chandassu Score: All constraint scores are averaged equally; yati and prasa are not weighted more heavily despite their literary importance.
  - Ra-vattu handling: Systematic exclusion of subscript Ra simplifies classification but may diverge from some traditional interpretations.

- **Failure signatures:**
  - Low n_paadalu_score with high n_aksharaalu_score: Whitespace detection failed, likely due to non-standard line breaks.
  - High structural scores but low gana_kramam_score: Laghuvu-guruvu classification errors, possibly from unhandled consonant clusters.
  - yati_score << other scores: Expected for complex caesura patterns; may indicate yati position miscalculation or vowel diacritic handling issues.
  - prasa_score near 25%: Only first paadam matching; suggests gunintha_chihnam (vowel diacritic) removal not normalizing correctly.

- **First 3 experiments:**
  1. **Tokenization boundary validation:** Run AksharamTokenizer on 50 manually-verified padyams, compare output against expert-annotated aksharam boundaries. Measure precision/recall of boundary detection.
  2. **Ablation on Ra-vattu handling:** Create a test set of aksharams containing Ra-vattu. Compare classification accuracy with vs. without the Ra-exclusion rule (Algorithm 4.2, Step 19) against expert judgments.
  3. **Per-constraint error analysis:** On the full dataset, identify the top 3 padyam types with lowest Chandassu Scores. For each, manually inspect 10 failure cases to determine whether failures stem from tokenization, classification, or constraint definition.

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of external validation against traditional Telugu prosody experts for the Chandassu Score metric
- Ra-vattu handling mechanism introduces systematic simplification that may not align with all traditional interpretations
- Rule-based approach may struggle with dialectal variations or less standardized poetic forms
- Corpus annotation process not described in detail, making it difficult to assess inter-annotator agreement

## Confidence
- **High confidence** in structural metrics (n_aksharaalu_score at 99.43%, n_paadalu_score at 99.93%) and tokenization mechanism due to deterministic Unicode processing
- **Medium confidence** in laghuvu-guruvu classification (93.78% gana_kramam_score) and ganam sequence matching due to formalization of traditional prosodic rules without comparative frameworks
- **Medium confidence** in overall Chandassu Score due to aggregation methodology not grounded in established literary standards

## Next Checks
1. **Expert validation study**: Engage 5-10 traditional Telugu prosody scholars to independently annotate a random sample of 100 padyams from the dataset. Compare their assessments against the computational framework's outputs, measuring Cohen's kappa for inter-rater reliability and identifying systematic disagreements.

2. **Ablation study on constraint weights**: Perform sensitivity analysis by varying the weights in the Chandassu Score aggregation (Equation 1). Test configurations where yati and prasa constraints receive higher weights (0.3 each) versus equal distribution (0.2 each). Measure which weighting scheme best aligns with expert judgments on a held-out validation set.

3. **Cross-validation across poetic traditions**: Apply the framework to padyams from different Telugu literary traditions (classical, modern, regional dialects) not represented in the original 4,651-annotation dataset. Measure performance degradation and identify which components (tokenization, classification, constraint validation) are most sensitive to stylistic variation.