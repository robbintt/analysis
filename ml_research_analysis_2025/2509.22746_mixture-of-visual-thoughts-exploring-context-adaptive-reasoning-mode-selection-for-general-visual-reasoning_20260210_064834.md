---
ver: rpa2
title: 'Mixture-of-Visual-Thoughts: Exploring Context-Adaptive Reasoning Mode Selection
  for General Visual Reasoning'
arxiv_id: '2509.22746'
source_url: https://arxiv.org/abs/2509.22746
tags:
- reasoning
- mode
- answer
- think
- modes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a general visual reasoning paradigm, Mixture-of-Visual-Thoughts
  (MoVT), that unifies multiple reasoning modes in a single model and enables context-adaptive
  mode selection. The authors propose AdaVaR, a two-stage framework: first, supervised
  fine-tuning (SFT) to unify and learn reasoning modes using a uniform sequence format
  with mode-specific prefixes; second, reinforcement learning (RL) with a novel AdaGRPO
  algorithm to induce mode selection capability.'
---

# Mixture-of-Visual-Thoughts: Exploring Context-Adaptive Reasoning Mode Selection for General Visual Reasoning

## Quick Facts
- arXiv ID: 2509.22746
- Source URL: https://arxiv.org/abs/2509.22746
- Reference count: 40
- Primary result: Introduces AdaVaR, a two-stage framework (SFT + RL) that unifies text-based and visually-grounded reasoning modes with context-adaptive mode selection, achieving state-of-the-art performance on diverse visual reasoning benchmarks.

## Executive Summary
This paper presents Mixture-of-Visual-Thoughts (MoVT), a general visual reasoning paradigm that unifies multiple reasoning modes within a single model and enables context-adaptive mode selection. The authors propose AdaVaR, a two-stage framework consisting of supervised fine-tuning (SFT) to learn unified reasoning modes with mode-specific prefixes, followed by reinforcement learning with a novel AdaGRPO algorithm to induce mode selection capability. AdaGRPO introduces prefix-guided exploration to ensure balanced sampling across modes, adaptive advantages for mode selection guidance, and curriculum learning for progressive improvement. Experiments demonstrate that AdaVaR consistently improves performance across diverse benchmarks, with AdaVaR-7B surpassing GPT-4o and AdaVaR-3B matching Qwen2.5-VL-7B in average accuracy.

## Method Summary
AdaVaR follows a two-stage paradigm: Stage 1 uses SFT to unify multiple reasoning modes (text-based and grounded) in a single model using a uniform sequence format with mode-specific prefixes. The SFT data is carefully balanced at a 1:1 ratio between modes. Stage 2 employs RL with the proposed AdaGRPO algorithm, which extends GRPO with three key innovations: prefix-guided sampling that forces uniform exploration across modes, adaptive advantage calculation that separates mode-selection guidance from reasoning-quality feedback, and curriculum learning that progresses from simpler to more complex tasks. The RL data is divided into binary mixture (simpler tasks) and diverse mixture (harder tasks) to facilitate learning progression.

## Key Results
- AdaVaR-7B achieves 77.6% average accuracy on MathVista, surpassing GPT-4o's 76.9% and Qwen2.5-VL-7B's 76.8%
- AdaVaR-3B reaches 73.2% average accuracy, matching Qwen2.5-VL-7B's performance while significantly outperforming its 69.2%
- The model demonstrates consistent improvement across eight diverse benchmarks including MathVista, MathVision, MathVerse, WeMath, MMStar, V*, POPE, and SpatialScore
- AdaGRPO shows superior mode selection capability with GRD% dropping from ~35% to ~2% on MathVista during training, indicating learned preferences

## Why This Works (Mechanism)

### Mechanism 1: Prefix-Guided Mode Exploration
The paper addresses the issue of mode collapse during RL training by forcing balanced exploration across reasoning modes. By prepending mode-specific prefix tokens (`<text>` and `<ground>`) to rollouts and dividing samples into two equal sub-groups, the policy model is compelled to generate rollouts for both modes rather than defaulting to its SFT-induced preference. This ensures the model learns to differentiate when each mode is optimal rather than reinforcing an initial bias.

### Mechanism 2: Adaptive Advantage Calculation
AdaGRPO introduces a dual-advantage system that separately guides mode selection and reasoning quality. The mode-relative advantage estimates the probability that one mode's reward distribution dominates another's using Gaussian CDF calculations, while the rollout-level advantage provides standard GRPO-style comparison across all rollouts. This separation allows the model to explicitly learn which mode to choose while simultaneously improving reasoning quality within each mode.

### Mechanism 3: Curriculum-Based Data Scheduling
The training progression from binary mixture (simpler tasks like OmniCount + Geo170K) to diverse mixture (all remaining data) enables the model to first establish clear mode preferences on stable, easier tasks before refining selection on more complex, diverse problems. This staged approach matches the observed three-phase learning pattern: initial exploration, stabilization of mode preferences, and final refinement of selection accuracy.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: AdaGRPO extends GRPO; understanding baseline GRPO (rollout sampling, advantage normalization, clipping) is prerequisite to grasping the modifications.
  - Quick check question: Can you explain why GRPO estimates advantages by comparing rewards across rollouts from the same question rather than using a value function?

- **Concept: Visual Grounding in LVLMs**
  - Why needed here: The "grounded" reasoning mode produces bounding box coordinates; without understanding how LVLMs generate spatial outputs, the mode distinction is unclear.
  - Quick check question: What is the output format when an LVLM performs visual grounding, and how does it differ from pure text generation?

- **Concept: Cold-Start SFT Before RL**
  - Why needed here: The two-stage paradigm assumes SFT provides a reasonable initialization for RL exploration; understanding this bootstrapping role clarifies why mode prefixes are introduced during SFT.
  - Quick check question: Why might starting RL directly from a base model (without SFT) fail to learn multiple reasoning modes?

## Architecture Onboarding

- **Component map:** Input (image, question) → unified prompt template with mode descriptions → Stage 1 SFT (mixed data with mode prefixes) → Stage 2 RL (AdaGRPO with prefix-guided sampling) → Autoregressive generation starting with mode prefix, then reasoning, then answer

- **Critical path:**
  1. Construct SFT data with 1:1 mode balance (119K grounded + 115K text reasoning)
  2. Train Stage 1 with mode prefixes in system prompt
  3. Collect RL dataset with verifiable answers (35K total: Geo170K, OmniCount, MM-Eureka, curated subsets)
  4. Run AdaGRPO with curriculum: binary mixture first, then diverse mixture
  5. Evaluate with mode-switching fallback if model gets stuck

- **Design tradeoffs:**
  - More reasoning modes → higher exploration cost but potentially higher upper bound (paper only tests 2 modes)
  - Larger n (rollouts per mode) → better advantage estimates but 2n× compute
  - Strict 1:1 SFT balance vs. allowing natural task distribution (paper chooses strict balance to prevent bias)

- **Failure signatures:**
  - Mode collapse: GRD% or TXT% approaches 100% early in training (suggests prefix-guided exploration not working)
  - No improvement over SFT baseline: Advantage signal may be too noisy; check reward distributions per mode
  - Degradation on specific benchmarks: Mode selection may be misaligned; inspect per-mode accuracy curves

- **First 3 experiments:**
  1. Reproduce Stage 1 → Stage 2 on a small subset (5K RL samples) with ablation: remove prefix-guided exploration (use vanilla GRPO) and compare mode selection behavior.
  2. Verify mode-relative advantage computation by logging μt, μv, σt, σv during training; confirm At + Av = 1 and check for numerical stability.
  3. Test curriculum ablation by training with only diverse mixture from step 0; compare final average accuracy and training stability to curriculum baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does AdaGRPO's mode-relative advantage calculation scale when extending beyond two reasoning modes?
- Basis: [explicit] Section B.4 states: "Our AdaVaR framework is naturally extensible to additional reasoning modes... Meanwhile, our cross-mode advantage estimation can incorporate additional preferences."
- Why unresolved: The current advantage formulation computes pairwise preference probabilities between two Gaussian distributions; generalizing to N modes requires a new advantage estimation mechanism.
- What evidence would resolve it: Experiments with 3+ reasoning modes (e.g., adding a counting mode) demonstrating whether the Gaussian-based pairwise comparison degrades or requires architectural modifications.

### Open Question 2
- Question: What mechanisms can narrow the performance gap between AdaVaR's adaptive mode selection and the theoretical upper bound?
- Basis: [explicit] Section B.4 notes: "As shown in Table 2, although AdaVaR achieves substantial performance gains, there remains a gap to the upper bound of adaptive reasoning, indicating that the model's mode-selection capability can be further improved."
- Why unresolved: Current RL-based selection learns from sampled rollouts but may not capture all contextual cues predicting optimal mode; SFT cannot pre-estimate mode performance per question.
- What evidence would resolve it: Analysis quantifying how often AdaVaR selects the suboptimal mode, paired with interventions (e.g., explicit mode-selection reasoning, richer training data) showing gap reduction.

### Open Question 3
- Question: How can visually-grounded reasoning be extended to handle multi-image scenarios?
- Basis: [inferred] Appendix B.5 reports: "the third case reveals a current limitation of the grounded mode in multi-image scenarios: it cannot distinguish between different images using coordinates."
- Why unresolved: Bounding boxes use absolute 2D coordinates without image identifiers, causing ambiguity when multiple images share the same coordinate space.
- What evidence would resolve it: Performance on multi-image benchmarks (e.g., SpatialScore cross-image tasks) using modified grounding formats that encode image indices alongside spatial coordinates.

### Open Question 4
- Question: Are learned mode-selection preferences transferable across model sizes or architectures?
- Basis: [inferred] Appendix B.1 observes different mode preferences between AdaVaR-3B and AdaVaR-7B, concluding "there is no universal ground truth of mode selection that is applicable for all models."
- Why unresolved: RL discovers model-specific mode strengths from sampling; it is unclear whether this learned selection knowledge generalizes or requires re-learning for each model variant.
- What evidence would resolve it: Transfer experiments where a mode selector trained on one model size is applied to another, measuring selection accuracy and downstream performance retention.

## Limitations
- Evaluation focuses only on two reasoning modes (text-only vs. grounded), limiting generalizability to scenarios with more than two modes
- Mode-relative advantage calculation assumes Gaussian reward distributions, which may not hold for highly non-linear visual reasoning tasks
- Curriculum learning design relies on assumed progression of difficulty without systematic validation of task ordering
- Mode-switching fallback mechanism is mentioned but not fully specified, leaving ambiguity about when and how mode retries are triggered
- Evaluation uses curated subsets of RL data (13K + 22K samples) rather than full datasets, potentially limiting robustness assessment

## Confidence
- **High confidence**: Stage 1 SFT training produces unified model capable of both reasoning modes (supported by clear 1:1 data balance and observed prefix generation)
- **Medium confidence**: AdaGRPO with prefix-guided exploration and adaptive advantages improves mode selection (mechanisms are well-specified but rely on unverified distributional assumptions)
- **Medium confidence**: Curriculum learning provides training stability (logical design but lacks ablation against random ordering)

## Next Checks
1. Test AdaGRPO with 3+ reasoning modes (e.g., add a counting mode) to verify prefix-guided exploration scales beyond binary mode selection.
2. Conduct ablation studies removing adaptive advantage calculation (using only rollout-level advantage) and curriculum learning to quantify their individual contributions.
3. Validate Gaussian assumption by analyzing reward distribution histograms per mode during training; implement robust statistics if distributions are non-Gaussian.