---
ver: rpa2
title: 'Demystifying Mergeability: Interpretable Properties to Predict Model Merging
  Success'
arxiv_id: '2601.22285'
source_url: https://arxiv.org/abs/2601.22285
tags:
- metrics
- task
- merging
- mergeability
- overlap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates what determines successful model merging
  by analyzing the properties that predict post-merge performance. The authors introduce
  an interpretable, architecture-agnostic framework that uses linear optimization
  over 28 pairwise metrics to identify which properties correlate with merging success
  across four merging methods.
---

# Demystifying Mergeability: Interpretable Properties to Predict Model Merging Success

## Quick Facts
- **arXiv ID**: 2601.22285
- **Source URL**: https://arxiv.org/abs/2601.22285
- **Reference count**: 40
- **Key outcome**: Introduces interpretable framework using 28 pairwise metrics to predict model merging success, achieving r∈[0.34,0.57] with full transparency versus black-box approaches.

## Executive Summary
This paper investigates what determines successful model merging by analyzing properties that predict post-merge performance. The authors introduce an interpretable, architecture-agnostic framework that uses linear optimization over 28 pairwise metrics to identify which properties correlate with merging success across four merging methods. They find that mergeability is not an intrinsic property of individual models but depends on both the merging method and partner tasks, with substantial variation in success drivers across methods. While different methods exhibit distinct "fingerprints" (20-80% metric overlap; 55.3% sign agreement), subspace overlap and gradient alignment metrics consistently emerge as foundational, method-agnostic prerequisites for compatibility. The framework achieves meaningful prediction (r∈[0.34,0.57], p<0.01) with full transparency versus black-box approaches, and demonstrates that mergeability can be enhanced through fine-tuning strategies that encourage these properties.

## Method Summary
The framework computes 28 pairwise metrics across five categories (Task Vector Geometry, Effective Rank, Subspace Overlap, Activation-Based, Gradient-Based) for each model pair. A linear optimization with Adam (lr=0.01) and sum-to-one constraint maximizes Pearson correlation between predicted scores and post-merge performance using leave-one-task-out cross-validation (20 folds). Each fold trains on 171 pairs and validates on 19 pairs involving the held-out task. The approach uses CLIP ViT-B/16 fine-tuned on 20 image classification tasks, with 10 calibration samples per task for activation and gradient metrics. The linear model is chosen over MLPs for better generalization with limited data (190 samples), and dense optimization is preferred over L1 regularization for interpretability despite slightly lower prediction accuracy.

## Key Results
- Mergeability is method-dependent with only 46.7% metric overlap and 55.3% sign agreement across four merging methods
- Subspace overlap and gradient alignment metrics consistently emerge as method-agnostic prerequisites for compatibility
- The framework achieves meaningful prediction (r∈[0.34,0.57], p<0.01) with full transparency versus black-box approaches
- Fine-tuning with gradient magnitude regularization improves mergeability (+0.55% average performance at λ=1)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mergeability is method-dependent, not an intrinsic property of individual models.
- Mechanism: Different merging algorithms (Task Arithmetic, Weight Averaging, TSV, Isotropic) prioritize different pairwise metrics, creating method-specific "fingerprints" with only 46.7% metric overlap and 55.3% sign agreement.
- Core assumption: The linear optimization successfully isolates method-relevant signals rather than overfitting to dataset artifacts.
- Evidence anchors:
  - [abstract] "mergeability...fundamentally depends on both the merging method and the partner tasks"
  - [Section 6.2] "different merging methods rely on substantially different metrics for prediction"
  - [corpus] Rahamim et al. (2026) proposes mergeability as an intrinsic property; this paper explicitly contradicts that hypothesis.
- Break condition: If the same model pair shows consistent mergeability rankings across all four methods, the method-dependence claim would be weakened.

### Mechanism 2
- Claim: Gradient alignment and subspace overlap serve as foundational, method-agnostic prerequisites for successful merging.
- Mechanism: Encoder gradient L2 distance (avg coefficient -23.3) and input gradient L2 distance (avg -21.4) are consistently negative predictors across all methods, while left subspace overlap top-k (avg +14.0) is consistently positive.
- Core assumption: These correlations reflect causal relationships rather than spurious associations.
- Evidence anchors:
  - [abstract] "subspace overlap and gradient alignment metrics consistently emerge as foundational, method-agnostic prerequisites"
  - [Table 4] Stable metrics show consistent signs across all four methods
  - [Section 7.1] L1 regularization confirms these metrics are selected with near-100% frequency
- Break condition: If a new merging method shows successful merges despite high gradient distance and low subspace overlap, the prerequisite claim would be invalidated.

### Mechanism 3
- Claim: Mergeability can be enhanced through fine-tuning strategies that encourage gradient alignment and subspace overlap.
- Mechanism: Gradient magnitude regularization during fine-tuning keeps models closer to the pretrained initialization, promoting gradient alignment and yielding consistent improvements in post-merge performance (+0.55% average at λ=1).
- Core assumption: The regularization effect operates through the identified stable metrics rather than other mechanisms.
- Evidence anchors:
  - [Section 6.3] "fine-tuning procedures that promote gradient similarity across tasks...benefit model mergeability"
  - [Appendix A.5] Proof-of-concept experiment shows monotonic improvement with stronger regularization
  - [corpus] No direct corpus support; this is a novel intervention demonstrated in the paper.
- Break condition: If the regularization improves mergeability without affecting the identified stable metrics, the claimed mechanism would be incorrect.

## Foundational Learning

- Concept: **Singular Value Decomposition (SVD) for subspace analysis**
  - Why needed here: Subspace overlap metrics rely on extracting and comparing singular vectors from weight matrices; understanding principal directions is essential for interpreting "left/right subspace overlap" metrics.
  - Quick check question: Given a weight matrix W, what do the columns of U and rows of Vᵀ represent in SVD?

- Concept: **Gradient-based model analysis**
  - Why needed here: Six of the 28 metrics require computing gradients with respect to encoder parameters or inputs; understanding gradient alignment as a signal for optimization compatibility is crucial.
  - Quick check question: Why might two models with similar input gradients (high cosine similarity) merge more successfully than those with divergent gradients?

- Concept: **Leave-one-task-out (LOTO) cross-validation**
  - Why needed here: The paper's claims about generalization rely on LOTO to test on entirely novel task combinations; this is more stringent than random splitting and critical for evaluating predictive validity.
  - Quick check question: In LOTO with 20 tasks, how many pairwise samples are in each validation fold, and why does this matter for generalization claims?

## Architecture Onboarding

- Component map:
  - **Metric computation layer**: 28 pairwise metrics organized in 5 categories (Task Vector Geometry, Effective Rank, Subspace Overlap, Activation-Based, Gradient-Based); each metric operates on model pairs without performing the merge.
  - **Linear optimization core**: Adam optimizer with sum-to-one constraint, maximizing Pearson correlation between predicted scores and post-merge performance; early stopping with 10⁻⁴ threshold.
  - **Cross-validation framework**: LOTO scheme with 20 folds; each fold trains on 171 pairs and validates on 19 pairs involving the held-out task.
  - **Calibration data pipeline**: 10 random samples per task for activation and gradient metrics; stored and reused across metric computations.

- Critical path:
  1. Fine-tune task-specific models from shared pretrained checkpoint (CLIP ViT-B/16).
  2. Compute all 28 metrics for each of the 190 unique task pairs.
  3. Run LOTO optimization to obtain method-specific coefficients.
  4. Validate on held-out tasks; average coefficients across folds for final predictors.

- Design tradeoffs:
  - **Linear vs. non-linear models**: MLP ablation (Appendix A.2) shows linear models generalize better with limited data (190 samples); trade interpretability for potential non-linear interactions.
  - **Dense vs. sparse optimization**: L1 regularization (Appendix A.6) improves prediction (r up to 0.66) but obscures relative contributions; dense model chosen for interpretability.
  - **Metric comprehensiveness vs. efficiency**: Gradient and activation metrics require forward/backward passes; weight-based metrics are fastest but less informative alone.

- Failure signatures:
  - High coefficient variance across LOTO folds (often >100% of mean): indicates unstable predictors; may need more task pairs or regularization.
  - Validation correlation drops significantly below training correlation (>0.2 gap): suggests overfitting to training tasks.
  - Metrics with inconsistent signs across methods: not suitable as method-agnostic screening tools.

- First 3 experiments:
  1. **Reproduce stable metrics analysis**: Implement the 6 stable metrics from Table 4; verify they achieve consistent signs across the four merging methods on the provided benchmark.
  2. **Test LOTO generalization**: Train on 19 tasks, predict merge outcomes for pairs involving the 20th task; confirm validation correlations fall within reported ranges (r∈[0.34, 0.57]).
  3. **Validate ablation hierarchy**: Remove subspace metrics and verify the largest performance drop (average Δr=-0.123); confirm gradient metrics are second most important.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can principled fine-tuning strategies that explicitly optimize for the identified stable metrics (e.g., gradient distance, subspace overlap) yield substantial improvements in mergeability?
- Basis in paper: [explicit] The authors state their toy experiment on gradient regularization was a proof of concept, noting that "more principled fine-tuning strategies, targeting multiple signals... are expected to yield larger gains."
- Why unresolved: The paper only demonstrated a modest average improvement (+0.55%) using a single regularization technique, leaving the potential of joint optimization unexplored.
- What evidence would resolve it: Development of a training objective that jointly minimizes interference metrics, demonstrating statistically significant performance retention in the merged model compared to baseline fine-tuning.

### Open Question 2
- Question: How robust are the identified "success fingerprints" and stable metrics when applied to architectures other than ViT or modalities beyond vision?
- Basis in paper: [explicit] The authors note in the limitations that "distinct geometric behaviors may emerge in other architectures or modalities," as the study was restricted to CLIP ViT-B/16.
- Why unresolved: The study utilized a single architecture, leaving the universality of the gradient and subspace overlap metrics unproven across different structural inductive biases.
- What evidence would resolve it: Applying the same linear optimization framework to CNNs or Large Language Models (LLMs) to verify if subspace and gradient metrics retain their predictive power.

### Open Question 3
- Question: To what extent do pairwise compatibility metrics predict success in multi-model merging scenarios involving more than two models?
- Basis in paper: [explicit] The authors limit their scope to pairwise merging, noting that "merging multiple models may introduce higher-order interference" not captured by the current metrics.
- Why unresolved: The 28 metrics are defined pairwise; it is unclear if the sum of pairwise compatibilities correlates with the performance of a single model merged from three or more task vectors.
- What evidence would resolve it: Experiments analyzing the correlation between aggregated pairwise metric scores and the actual performance of n-way merged models.

## Limitations
- **Method-specific metric relevance**: Substantial metric overlap differences (20-80%) across methods raise questions about whether optimization captures fundamental signals or adapts to method-specific biases.
- **Dataset and architecture scope**: All experiments use CLIP ViT-B/16 on 20 image classification tasks; findings may not generalize to other architectures or task domains.
- **Fine-tuning regularization scope**: Proof-of-concept shows promising results (+0.55% average improvement) but only demonstrated for Task Arithmetic with limited validation.

## Confidence

- **High confidence**: Method-dependence of mergeability (Mechanism 1), gradient alignment and subspace overlap as universal prerequisites (Mechanism 2). These claims are supported by consistent coefficient signs across methods and confirmed through L1 regularization analysis.

- **Medium confidence**: Linear optimization successfully isolates method-relevant signals without overfitting (Mechanism 1 core assumption). While validation correlations are statistically significant (p<0.01), the modest r values (0.34-0.57) leave room for alternative explanations.

- **Low confidence**: Mergeability enhancement through fine-tuning strategies (Mechanism 3). This is a novel intervention with limited empirical validation (only one merging method tested, single regularization strength examined).

## Next Checks
1. **Cross-architecture validation**: Apply the framework to ResNet-50 fine-tuned on the same 20 tasks. Compare stable metric identification and prediction accuracy to verify architecture-agnostic claims.

2. **Method consistency test**: Select 5-10 model pairs and measure their relative mergeability rankings across all four merging methods. If rankings correlate strongly (r>0.5), this would challenge the method-dependence claim.

3. **Regularization ablation**: Test gradient magnitude regularization across all four merging methods with multiple λ values (0.1, 1, 10). Verify whether improvements in mergeability correlate with changes in the identified stable metrics.