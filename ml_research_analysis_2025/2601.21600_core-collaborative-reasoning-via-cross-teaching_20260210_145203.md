---
ver: rpa2
title: 'CORE: Collaborative Reasoning via Cross Teaching'
arxiv_id: '2601.21600'
source_url: https://arxiv.org/abs/2601.21600
tags:
- pass
- reasoning
- traces
- arxiv
- core
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CORE improves reasoning accuracy by training models to collaborate.
  It uses a two-round protocol where failed models learn from hints provided by successful
  peers, rewarding both correctness and diversity.
---

# CORE: Collaborative Reasoning via Cross Teaching

## Quick Facts
- arXiv ID: 2601.21600
- Source URL: https://arxiv.org/abs/2601.21600
- Reference count: 36
- Core result: CORE achieves near-ceiling GSM8K performance and strong gains on MATH, AIME, and GPQA by training models to collaborate via cross-teaching.

## Executive Summary
CORE introduces a training-time collaboration protocol where two models teach each other through a two-round exchange of reasoning traces. When one model succeeds and another fails, the successful model's trace becomes a "hint" for the failing model, which must extract relevant steps while ignoring the final answer. The method trains models to provide both correct and diverse reasoning paths, reducing correlated failures between collaborators. Experiments show CORE significantly outperforms single-model training and inference-time baselines across multiple reasoning benchmarks, demonstrating that training-time collaboration can improve reasoning accuracy without scaling model size.

## Method Summary
CORE trains two small LLMs (3B+4B) to collaborate on reasoning tasks through a two-round cross-teaching protocol. In ROUND A, both models generate K=2 traces independently, with rewards based on correctness and diversity. The highest-reward correct trace becomes the teacher context (capped at 1536 tokens). In ROUND B, models generate K'=1 trace with hint dropout (p=0.5), using the teacher context as input. A rescue bonus (0.15) rewards failed→correct recovery, and a DPP-lite exploration reward encourages diversity. The combined reward includes correctness, diversity, cross-model complementarity, and auxiliary terms. Training uses LoRA adapters, group-normalized policy gradients, and careful teacher context construction to prevent answer leakage.

## Key Results
- CORE achieves 71.9% team Pass@2 on GSM8K (vs. 62.8% single-model baseline)
- Strong gains on harder benchmarks: +4.6% on MATH, +7.5% on AIME, +12.4% on GPQA
- Reduces correlated failures: p_12 (both-correct rate) increases from 54.6% to 69.2% after training
- Outperforms inference-time baselines including self-consistency and best-of-N sampling

## Why This Works (Mechanism)
CORE works by training models to provide useful hints while learning to extract relevant reasoning steps from others' traces. The two-round protocol creates a feedback loop where successful reasoning becomes teaching material, and failed attempts become learning opportunities. The hint dropout forces models to focus on the reasoning process rather than memorizing answers. The diversity reward prevents mode collapse where both models converge to the same wrong approach. The rescue bonus explicitly rewards the ability to recover from failure using external hints. Together, these mechanisms train models to collaborate effectively rather than simply solving problems independently.

## Foundational Learning
- **Policy gradient training**: Used to optimize reward-weighted trace sampling; needed because reasoning traces are non-differentiable.
- **Determinantal point processes (DPP)**: Provide diversity-aware subset selection; needed to balance correctness with varied solution approaches.
- **Cross-teaching protocol**: Two-round exchange of traces; needed to simulate collaborative reasoning during training.
- **Hint dropout**: Randomly removes final answer markers; needed to prevent models from memorizing rather than reasoning.
- **LoRA adapters**: Enable efficient multi-task training; needed to add collaboration capability without full fine-tuning.
- **Group normalization**: Stabilizes training across heterogeneous model pairs; needed for consistent learning dynamics.

## Architecture Onboarding

**Component Map**
Qwen2.5-3B-Instruct -> LoRA Adapter -> Trace Sampler -> Reward Computer -> Policy Gradient Optimizer
Qwen3-4B-Instruct -> LoRA Adapter -> Trace Sampler -> Reward Computer -> Policy Gradient Optimizer
Both -> Teacher Context Builder -> Hint Dropout -> Final Output

**Critical Path**
ROUND A trace generation → Reward computation (correctness + diversity) → Teacher context selection → ROUND B trace generation with hint dropout → Rescue bonus evaluation → Policy gradient update

**Design Tradeoffs**
- Two small models vs. one larger model: CORE achieves better team performance than scaling up single models
- Hint dropout rate: 0.5 vs. 0.75 (text inconsistency) - affects balance between guidance and independent reasoning
- Teacher context length: 1536 tokens - limits information transfer but prevents overfitting to full solutions

**Failure Signatures**
- Mode collapse: Both models converge to same wrong decomposition (high p_12 but low accuracy)
- Answer leakage: Models memorize final answers rather than reasoning steps (poor generalization)
- Truncated context: Teacher context too short to capture critical reasoning steps (low rescue success)

**First Experiments**
1. Verify trace sampler produces diverse outputs with different seeds
2. Test reward computation with known correct/incorrect traces
3. Validate teacher context construction preserves reasoning steps while removing answer markers

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- Requires careful reward weight tuning (w1, w2, λ_cm) with only vague guidance provided
- Training duration (E epochs) not specified, making exact replication difficult
- Method effectiveness compared to scaled-up single models not thoroughly evaluated
- Internal inconsistency in hint dropout parameter (0.5 vs. 0.75 reported in different sections)

## Confidence
- **High confidence**: Core methodology and main empirical findings (team performance improvements, reduction in correlated failures)
- **Medium confidence**: Exact implementation details needed for perfect replication, particularly around reward computation and training schedule
- **Low confidence**: Claim that "any better performance requires larger models" given lack of comparison to scaled-up single models

## Next Checks
1. Implement the full reward computation pipeline with exact weight values (w1, w2, λ_cm, rescue bonus) and verify that p_12 increases appropriately during training
2. Test the sensitivity of team performance to hint dropout rate by training with both 0.5 and 0.75 values to determine which matches the reported results
3. Evaluate whether the reported gains persist when using only correctness reward (w2=0) to isolate the contribution of diversity/exploration terms