---
ver: rpa2
title: 'DILLEMA: Diffusion and Large Language Models for Multi-Modal Augmentation'
arxiv_id: '2502.04378'
source_url: https://arxiv.org/abs/2502.04378
tags:
- image
- test
- dillema
- generated
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DILLEMA addresses the challenge of testing deep learning vision
  models by generating realistic, diverse synthetic test cases that reveal model weaknesses.
  The framework uses a five-step pipeline combining image captioning, Large Language
  Models (LLMs), and control-conditioned diffusion models to create counterfactual
  images that preserve spatial structure and semantic meaning.
---

# DILLEMA: Diffusion and Large Language Models for Multi-Modal Augmentation

## Quick Facts
- arXiv ID: 2502.04378
- Source URL: https://arxiv.org/abs/2502.04378
- Authors: Luciano Baresi; Davide Yi Xian Hu; Muhammad Irfan Mas'udi; Giovanni Quattrocchi
- Reference count: 22
- Primary result: DILLEMA reveals error rates >15× higher than original test sets and improves robustness by up to 52.27% through synthetic test generation

## Executive Summary
DILLEMA addresses the critical challenge of testing deep learning vision models by generating realistic synthetic test cases that expose model weaknesses while preserving semantic meaning and spatial structure. The framework combines image captioning, Large Language Models, and control-conditioned diffusion models in a five-step pipeline to create counterfactual images that maintain ground-truth labels. When tested on ImageNet1K and SHIFT datasets, DILLEMA generated 125,000 augmented images that achieved 99.7% validity according to human evaluators and revealed significant model vulnerabilities that standard test sets missed.

## Method Summary
DILLEMA implements a five-step pipeline: (1) converts images to detailed multi-sentence captions using BLIP2, (2) uses LLaMA-2 to identify modifiable elements while preserving task-relevant features, (3) generates alternative descriptions for identified elements, (4) synthesizes counterfactual captions maintaining spatial consistency, and (5) produces new images through ControlNet diffusion conditioned on edge maps from the original images. The framework explicitly incorporates task context at each LLM step to ensure label preservation through metamorphic relationships, while diffusion conditioning preserves spatial layout. Retraining experiments show significant robustness improvements, particularly for critical weaknesses like pedestrian recognition.

## Key Results
- Error rate exposure: DILLEMA revealed error rates more than 15 times higher than original test sets
- Human validity: 99.7% of generated images judged semantically consistent with original labels by human evaluators
- Robustness improvement: Up to 52.27% accuracy gain on ImageNet1K and 8.87% mIoU improvement on SHIFT through retraining with DILLEMA-generated cases
- Object preservation: Critical objects preserved at high rates (98.9% roads) but smaller objects like pedestrians showed lower preservation (84.6%)

## Why This Works (Mechanism)

### Mechanism 1
Textual intermediate representations enable semantic-preserving image transformations that would be difficult to model directly in pixel space. The pipeline converts images → detailed captions → modified captions → new images, allowing LLMs (trained on billions of text tokens) to reason about which attributes can change without altering task-relevant semantics. The captioning model generates multi-sentence descriptions capturing objects, environments, and contextual relationships.

### Mechanism 2
Control-conditioned diffusion preserves spatial structure while allowing semantic modifications, maintaining metamorphic relationships. ControlNet uses edge conditioning extracted from the original image to constrain the diffusion process, ensuring generated images maintain object positions and layouts even as attributes change.

### Mechanism 3
Task-aware prompting constrains LLM modifications to preserve ground-truth labels via metamorphic relationships. Each LLM interaction includes explicit task context, identifying modifiable keywords while excluding task-essential elements. For classification, only peripheral attributes change; for segmentation, roads and critical objects must remain.

## Foundational Learning

- **Concept: Metamorphic Testing**
  - Why needed here: DILLEMA is fundamentally a metamorphic testing approach—it generates new test cases by transforming inputs while preserving expected outputs (labels).
  - Quick check question: If you change weather from "foggy" to "snowy" in an autonomous driving image, should the segmentation labels change? Why or why not?

- **Concept: Diffusion Model Conditioning**
  - Why needed here: ControlNet adds spatial conditioning to text-to-image diffusion. Understanding how conditioning signals guide generation is essential for debugging spatial preservation failures.
  - Quick check question: What happens to generated image structure if you provide an edge map from a different image than the one your text prompt describes?

- **Concept: In-Context Learning (One-Shot Prompting)**
  - Why needed here: The framework uses one-shot examples to guide LLM output format. Understanding this helps troubleshoot parsing failures and improve prompt design.
  - Quick check question: Why might an LLM fail to produce parsable JSON output without an example in the prompt?

## Architecture Onboarding

- **Component map**: Image → BLIP2 Captioning → LLaMA-2 Keyword/Alternative/ Caption Generation → ControlNet Diffusion → Synthetic Image
- **Critical path**: Caption quality → LLM modification choices → diffusion faithfulness. Errors compound: poor captions lead to wrong keywords, leading to invalid counterfactuals, leading to images that break metamorphic relations.
- **Design tradeoffs**:
  - BLIP2 vs. smaller captioning models: BLIP2 produces richer descriptions but requires more compute
  - Quantized LLaMA-2 vs. larger models: 5-bit quantization fits consumer hardware but may reduce reasoning quality
  - Edge vs. depth vs. pose conditioning: Edge preserves structure but may limit texture/weather changes
  - Single vs. multiple alternatives per image: Multiple augmentations increase coverage but raise validity risks
- **Failure signatures**:
  - Low human validity scores (below ~95%): Check caption quality first
  - Pedestrian disappearance (observed 84.6% preservation vs. 98.9% roads): Small objects poorly captured in captions or lost in diffusion
  - Parsing failures in LLM output: Prompt template may need adjustment; retry with different seed
  - Spatial distortion in generated images: ControlNet conditioning strength may need tuning
- **First 3 experiments**:
  1. Validate captioning on a held-out subset: Manually verify that BLIP2 captures all task-relevant objects (especially small ones like pedestrians) before running the full pipeline.
  2. Ablate task context in LLM prompts: Run keyword extraction with and without explicit task description; measure difference in proposed modifications to confirm mechanism 3.
  3. Test spatial preservation across conditioning types: Compare edge, depth, and semantic map conditioning on 50 images; measure mIoU between original and generated segmentation maps.

## Open Questions the Paper Calls Out

- **Open Question 1**: How should generated test cases be prioritized to maximize vulnerability detection while minimizing redundant testing?
  - Basis: "Future work will compare with additional baselines and explore prioritization of the generated test cases."
  - Why unresolved: 125,000 augmented images generated but no ranking or selection criteria provided to identify most critical test cases.

- **Open Question 2**: How does DILLEMA generalize to specialized domains such as medical imaging where label preservation is critical and spatial precision is paramount?
  - Basis: "it may not generalize to specialized scenarios (e.g., medical imaging)... further testing on diverse datasets is required to confirm adaptability."
  - Why unresolved: Medical imaging requires stricter semantic and spatial fidelity; current 84.6% pedestrian preservation suggests domain-specific limitations.

- **Open Question 3**: Can automated metrics replace human evaluation for validating semantic consistency in generated test cases?
  - Basis: "future work should employ more rigorous metrics or automated checks to validate semantic consistency in generated test cases."
  - Why unresolved: Current validation relies on subjective Mechanical Turk assessments; scalability and consistency are concerns.

## Limitations
- Effectiveness depends heavily on captioning model quality, particularly for small or occluded objects (84.6% pedestrian preservation vs. 98.9% roads)
- LLM modification decisions rely on semantic understanding without explicit metamorphic rules, creating uncertainty about generalization to novel tasks
- Human validity assessment provides strong external validation but doesn't guarantee semantic preservation across all task types or complex scenarios

## Confidence
- **High confidence**: The metamorphic testing framework design and five-step pipeline architecture are well-specified and technically sound
- **Medium confidence**: Effectiveness claims (15× error rate increase, 52.27% robustness improvement) are supported by empirical results but limited to two datasets and specific model architectures
- **Medium confidence**: Spatial preservation mechanism via ControlNet edge conditioning works for basic layouts but may struggle with complex scenes where edge maps don't uniquely define object identity

## Next Checks
1. Test BLIP2 captioning accuracy on held-out subset: Manually verify caption completeness for small objects across 100 random images, measuring recall of task-relevant elements (pedestrians, traffic signs, etc.) before running full pipeline.
2. Validate LLM task-awareness through ablation: Compare keyword modification sets with and without explicit task context across 50 images; measure semantic drift using CLIP similarity scores between original and modified captions.
3. Stress-test spatial preservation across domains: Apply DILLEMA to 3 additional datasets (medical imaging, aerial imagery, indoor scenes) with varying object complexity; measure structural fidelity using SSIM and object detection AP on generated images.