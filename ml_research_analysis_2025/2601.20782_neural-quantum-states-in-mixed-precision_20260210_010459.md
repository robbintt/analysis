---
ver: rpa2
title: Neural Quantum States in Mixed Precision
arxiv_id: '2601.20782'
source_url: https://arxiv.org/abs/2601.20782
tags:
- precision
- sampling
- error
- distribution
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the use of reduced-precision arithmetic
  in Markov chain Monte Carlo (MCMC) sampling within neural-network-based Variational
  Monte Carlo (VMC) for quantum many-body systems. The authors derive theoretical
  bounds on the sampling bias introduced by finite-precision errors in Metropolis-Hastings
  MCMC and validate these bounds empirically.
---

# Neural Quantum States in Mixed Precision

## Quick Facts
- arXiv ID: 2601.20782
- Source URL: https://arxiv.org/abs/2601.20782
- Reference count: 0
- Reduced-precision MCMC sampling in VMC can be performed in half precision without accuracy loss

## Executive Summary
This work presents a theoretical and empirical analysis of using reduced-precision arithmetic in Markov chain Monte Carlo (MCMC) sampling within neural-network-based Variational Monte Carlo (VMC) for quantum many-body systems. The authors derive bounds on sampling bias introduced by finite-precision errors and demonstrate that significant portions of the VMC algorithm, particularly the sampling step, can be executed in half precision without loss of accuracy. Their mixed-precision framework achieves up to 3.5× speedup in sampling while maintaining training performance across various neural network architectures.

## Method Summary
The authors develop a mixed-precision VMC framework that maintains high-precision computations for gradient calculations while performing sampling in low precision. They derive theoretical bounds on the sampling bias introduced by finite-precision errors in Metropolis-Hastings MCMC, showing that the bias scales as O(σ²) when the underlying Markov chain mixes rapidly. The method is validated empirically across multiple neural network architectures (feed-forward and residual convolutional networks) and quantum systems (Heisenberg, Ising, Bose-Hubbard models), demonstrating that half-precision sampling preserves training performance while significantly accelerating the computation.

## Key Results
- Up to 3.5× speedup in sampling while preserving training performance
- Sampling bias scales as O(σ²) under rapid mixing conditions
- Relative error in energy estimates remains approximately constant across system sizes

## Why This Works (Mechanism)
The method exploits the fact that sampling steps in VMC are more tolerant to numerical errors than gradient calculations. By performing the computationally expensive MCMC sampling in half precision while maintaining high precision for gradient computations, the framework achieves significant speedups without compromising accuracy. The theoretical analysis shows that under conditions of rapid mixing, finite-precision errors in the sampling process introduce a bias that scales quadratically with the error magnitude, making low-precision sampling viable.

## Foundational Learning

**Variational Monte Carlo (VMC)**: A method for approximating quantum many-body ground states by optimizing a parameterized wavefunction ansatz against the Schrödinger equation using Monte Carlo sampling. Why needed: Forms the computational framework being accelerated. Quick check: Verify the energy expectation is computed as a Monte Carlo average over sampled configurations.

**Metropolis-Hastings MCMC**: A Markov chain Monte Carlo method for sampling from probability distributions by generating a sequence of correlated samples that asymptotically follow the target distribution. Why needed: Core sampling algorithm whose precision properties are being analyzed. Quick check: Confirm the acceptance ratio is computed correctly for state transitions.

**Neural Quantum States (NQS)**: Parametrization of quantum wavefunctions using neural networks, typically with autoregressive architectures for efficient sampling. Why needed: The specific variational ansatz family being studied. Quick check: Verify the network outputs valid probability distributions over quantum configurations.

## Architecture Onboarding

**Component map**: Quantum Hamiltonian -> Energy Evaluation -> Gradient Calculation (high precision) -> Parameter Update -> Neural Network Parameters -> Configuration Sampling (half precision) -> MCMC Chain

**Critical path**: The most computationally intensive step is the MCMC sampling, which dominates runtime and benefits most from precision reduction. The gradient calculations must remain in high precision to ensure accurate parameter updates.

**Design tradeoffs**: The key tradeoff is between computational speedup from reduced precision and potential accuracy loss from numerical errors. The authors optimize this by isolating the sampling step for half-precision execution while preserving high precision for gradient calculations.

**Failure signatures**: Potential failure modes include biased energy estimates from accumulated sampling errors, poor convergence of the variational optimization, or numerical instabilities in the half-precision sampling step. These would manifest as systematic deviations from high-precision baselines.

**First experiments**:
1. Implement the mixed-precision VMC with half-precision sampling on a simple 1D Ising model
2. Compare energy estimates and convergence behavior between mixed-precision and full high-precision implementations
3. Measure the actual speedup achieved and verify it matches the theoretical predictions

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Theoretical analysis assumes the true Hamiltonian's ground state lies within the variational ansatz family
- Empirical validation focuses on specific neural network architectures and Hamiltonian types
- The 3.5× speedup measurement accounts only for sampling, not the full VMC pipeline
- Error scaling analysis is limited to relatively small system sizes

## Confidence
- Theoretical bounds on sampling bias: High
- Empirical validation of mixed-precision sampling: High
- 3.5× speedup claim: Medium
- O(σ²) scaling under rapid mixing: High
- Constant relative error across system sizes: Medium

## Next Checks
1. Test mixed-precision VMC on additional neural network architectures beyond feed-forward and residual convolutional networks
2. Validate performance on larger system sizes (beyond N=24) to confirm constant relative error scaling
3. Benchmark the complete VMC pipeline including full mixed-precision implementation overhead