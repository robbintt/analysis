---
ver: rpa2
title: Leveraging In-Context Learning for Language Model Agents
arxiv_id: '2506.13109'
source_url: https://arxiv.org/abs/2506.13109
tags:
- demonstrations
- tasks
- task
- trajectory
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a novel algorithm that leverages an LLM with
  retries along with demonstrations to automatically and efficiently annotate agentic
  tasks with solution trajectories. They then show that set-selection of trajectories
  of similar tasks as demonstrations significantly improves performance, reliability,
  robustness, and efficiency of LLM agents.
---

# Leveraging In-Context Learning for Language Model Agents

## Quick Facts
- **arXiv ID**: 2506.13109
- **Source URL**: https://arxiv.org/abs/2506.13109
- **Reference count**: 27
- **Primary result**: Set-selection of demonstration trajectories significantly improves performance, reliability, and efficiency of LLM agents compared to individual ranking

## Executive Summary
This paper introduces a novel algorithm that leverages an LLM with retries and demonstrations to automatically annotate agentic tasks with solution trajectories. The key innovation is using set-selection (rather than individual ranking) of demonstration trajectories, which significantly improves performance, reliability, robustness, and efficiency of LLM agents. The authors demonstrate that using small trajectory snippets at each step instead of full trajectories mitigates the high inference cost overhead. They show that demonstrations from larger models improve smaller models, and that ICL agents can rival more expensive trained agents, revealing that ICL can be very powerful for agentic tasks with careful use.

## Method Summary
The method uses an iterative annotation algorithm with a large LLM (Teacher) to generate verified solution trajectories for training tasks. These trajectories are stored and later used as in-context demonstrations for smaller LLM agents (Student) at inference time. The approach employs either ReAct (Reasoning + Acting) or Plan-and-Execute agents with demonstration selection via cosine similarity, BERTScore-Recall, or Set-BSR. Step-level snippets can be retrieved based on thought similarity for cost-efficient guidance. The annotation phase uses a Checker to verify correctness, while inference uses retrieval-based selection to construct prompts with relevant demonstrations.

## Key Results
- Set-selection of demonstration trajectories significantly improves performance over individual ranking approaches
- Step-level snippets provide performance benefits comparable to full trajectories with minimal cost overhead
- Demonstrations from larger models (GPT-4o) improve smaller models (GPT-4o-mini), with ICL agents rivaling trained approaches
- The approach achieves 45%+ TGC on test-normal with GPT-4o-mini using GPT-4o-annotated demonstrations

## Why This Works (Mechanism)

### Mechanism 1: Coverage-Based Set Selection
Jointly selecting demonstrations as a set ensures coverage of all necessary reasoning patterns for complex, compositional tasks. Independent ranking tends to select redundant examples that are semantically similar to the query, potentially missing crucial skills. Set-selection optimizes for coverage, ensuring the context window contains diverse, non-redundant examples that collectively demonstrate the full solution path. Performance degrades if the demonstration pool lacks diversity, causing repeated retrieval of the same "type" of example.

### Mechanism 2: Recency Bias Mitigation via Step-Level Snippets
Placing small, relevant trajectory snippets at the end of the prompt significantly improves action prediction accuracy by mitigating the LLM's recency bias. LLMs focus more heavily on tokens immediately preceding generation. By dynamically retrieving snippets matching the current reasoning step and placing them immediately before prediction, the model conditions on the most relevant signal. The retrieval key (current thought) must be reliable; otherwise, misleading or ambiguous keys retrieve irrelevant snippets that confuse the model.

### Mechanism 3: Teacher-Student Annotation Transfer
High-quality solution trajectories from a larger "Teacher" model can substitute for ground-truth data when used as demonstrations for a smaller "Student" model. The iterative annotation algorithm uses a large model with retries to solve tasks, and these trajectories are stored. When a smaller model runs, it uses these high-quality "teacher" traces as ICL examples, allowing the student to mimic the reasoning structure without expensive fine-tuning. Performance breaks down if the Teacher model hallucinates logic or uses syntax opaque to the Student.

## Foundational Learning

- **Concept**: **ReAct (Reasoning + Acting) Loop**
  - **Why needed here**: The entire architecture relies on the agent maintaining a history of `thought`, `action`, and `observation`. Without understanding this loop, the "Snippet" mechanism (which retrieves based on the current thought) makes no sense.
  - **Quick check question**: Can you identify which part of the agent's trace (Thought, Action, or Observation) serves as the query key for retrieving snippet demonstrations?

- **Concept**: **Submodular Set Selection**
  - **Why needed here**: The paper argues that selecting multiple examples requires optimizing for *diversity* (Set-BSR) rather than just *similarity* (Cosine Similarity). Understanding that the goal is to "cover" the task's requirements is crucial for debugging why a prompt failed.
  - **Quick check question**: If you retrieve 3 examples that all show how to "login" but none show how to "pay," which selection failure mode have you encountered?

- **Concept**: **Context Window Economics (Cost vs. Recency)**
  - **Why needed here**: The paper explicitly trades off inference cost (tokens) against performance. Long trajectories are effective but expensive and suffer from recency bias. Understanding this tradeoff is necessary to decide between the "Trajectory" and "Snippet" modes.
  - **Quick check question**: Why does adding a second full trajectory increase cost significantly, and how does the "Snippet" approach mitigate this specific cost?

## Architecture Onboarding

- **Component map**: Annotation Engine -> Vector Store -> Solver (ReAct/PnE) -> Checker
- **Critical path**:
  1. Bootstrap: Run Iterative Annotation Algorithm (Algo 1) on training set, ensuring Checker robustness
  2. Retrieval: At inference step t, embed Task Description (trajectories) or Current Thought (snippets)
  3. Prompt Construction: Assemble General Context + Selected Trajectories + Current Trace + Selected Snippets
  4. Execution: LLM generates next step
- **Design tradeoffs**:
  - Trajectories vs. Snippets: Trajectories provide global context but cost ~100k tokens; Snippets provide local guidance for negligible cost
  - Ranking vs. Set Selection: Ranking is O(N) and simple; Set Selection is computationally heavier but yields higher reliability
  - ReAct vs. PnE: PnE allows shorter subtask contexts but introduces planning bottleneck; ReAct is more robust to dynamic environments
- **Failure signatures**:
  - "Distracted by Similarity": Agent retrieves trajectory from highly similar task but fails due to API mismatch
  - "Context Drift": PnE plan becomes outdated after first subtask execution but agent rigidly follows invalid plan
  - "Annotation Poisoning": Checker accepts trajectory that passes unit tests but uses "cheating" logic that propagates via ICL
- **First 3 experiments**:
  1. Sanity Check: Run agent with single fixed manual trajectory vs. random annotated trajectory
  2. Snippet Ablation: Run ReAct agent with Max Trajectories = 1, toggle Snippet Demonstrations ON/OFF
  3. Cross-Model Transfer: Annotate dev set using largest model, run smaller model against that set

## Open Questions the Paper Calls Out

- **Open Question 1**: Can specialized retrieval metrics be developed that outperform general-purpose encoders (e.g., MPNet, DeBERTa) specifically for selecting agentic demonstrations? The authors used general-purpose encoders and suggest future work could explore demonstration selection approaches more suitable for agentic tasks. Resolution would require a custom-trained retriever achieving higher TGC than Set-BSR baseline.

- **Open Question 2**: How can retrieval mechanisms leverage solution-side information (e.g., API sequences) available in demonstration candidates when that information is absent from the test task input? Current methods rely solely on task description to find similar trajectories, ignoring rich structural data inside annotated solutions. Resolution would require a retrieval method successfully utilizing internal trajectory features to select demonstrations.

- **Open Question 3**: Can Plan-and-Execute (PnE) agents surpass ReAct agents if the planning module is allowed to adapt based on execution feedback? The paper tested standard PnE implementation where planning happens once at start, leaving dynamic or reactive planning unexplored. Resolution would require modified PnE with dynamic replanning achieving higher TGC and lower token usage than ReAct baselines.

## Limitations

- The approach relies heavily on a "Checker" component in annotation phase, raising concerns about potential bias or brittleness if it accepts suboptimal trajectories
- SET-BSR and BSR scoring functions are referenced but not fully specified, creating implementation ambiguity
- The assumption that larger models' trajectories universally transfer to smaller models may not hold across different task domains or architectures
- The claim that ICL agents can rival costlier trained agents lacks direct comparative analysis against fine-tuned models

## Confidence

- **High confidence**: Core finding that set-selection of demonstrations significantly improves performance over independent ranking is well-supported by quantitative results and aligns with submodular optimization principles
- **Medium confidence**: Recency bias mitigation through step-level snippets is plausible but evidence relies primarily on internal citations rather than external validation
- **Low confidence**: Claim that ICL agents can rival costlier trained agents lacks direct comparative analysis against fine-tuned models

## Next Checks

1. **Checker robustness audit**: Systematically evaluate Checker component's tolerance for borderline cases by injecting subtly incorrect trajectories and measuring acceptance rates to quantify "annotation poisoning" risk

2. **Cross-domain transferability test**: Apply annotated demonstrations from AppWorld to a structurally similar but distinct agentic task environment (e.g., web automation vs. mobile app tasks) to assess whether "Teacher-Student" transfer effect generalizes beyond training domain

3. **Fine-tuning comparison benchmark**: Implement lightweight fine-tuned baseline using same annotated trajectories and compare performance against ICL agents across all metrics to provide concrete evidence for or against ICL rivaling trained approaches