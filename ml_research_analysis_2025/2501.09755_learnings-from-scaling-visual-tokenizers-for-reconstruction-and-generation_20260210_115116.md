---
ver: rpa2
title: Learnings from Scaling Visual Tokenizers for Reconstruction and Generation
arxiv_id: '2501.09755'
source_url: https://arxiv.org/abs/2501.09755
tags:
- vitok
- reconstruction
- generation
- image
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores scaling visual tokenizers for reconstruction
  and generation tasks, addressing the gap between generator and tokenizer scaling
  in state-of-the-art models. The authors introduce ViTok, a Transformer-based autoencoder
  with enhanced ViT architecture, trained on large-scale datasets to enable extensive
  scaling experiments.
---

# Learnings from Scaling Visual Tokenizers for Reconstruction and Generation

## Quick Facts
- arXiv ID: 2501.09755
- Source URL: https://arxiv.org/abs/2501.09755
- Reference count: 40
- This paper explores scaling visual tokenizers for reconstruction and generation tasks, addressing the gap between generator and tokenizer scaling in state-of-the-art models.

## Executive Summary
This paper introduces ViTok, a Transformer-based autoencoder with enhanced ViT architecture, to explore scaling visual tokenizers for reconstruction and generation tasks. The authors address the critical gap where generator scaling has outpaced tokenizer scaling in state-of-the-art models. Through extensive experiments on large-scale datasets (450M images, 30M videos), they establish fundamental scaling laws showing that bottleneck information capacity E is the primary predictor of reconstruction performance, while revealing a nuanced trade-off between reconstruction and generation quality.

The work demonstrates that while higher bottleneck capacity improves reconstruction, excessive channel dimensions can degrade generative quality, and that scaling the encoder provides minimal benefits for either task. ViTok achieves competitive performance with fewer FLOPs than existing methods on ImageNet-1K, COCO, and UCF-101 datasets, setting new benchmarks for class-conditional video generation.

## Method Summary
ViTok is a VAE-based autoencoder using Llama-enhanced ViT architecture with 3D convolutions for tubelet embedding. The model employs a two-stage training procedure: Stage 1 trains with MSE + LPIPS + KL losses for 100k steps, then Stage 2 freezes the encoder and adds GAN loss (λ=1.0) for another 100k steps. The bottleneck capacity E = L × c (tokens × channels) is systematically varied across patch sizes and channel dimensions. For generation, DiT models are trained using frozen ViTok encoders with Llama upgrades, CFG scaling, and DDIM sampling.

## Key Results
- Bottleneck size E strongly correlates with reconstruction performance (rFID/rSSIM/rPSNR) with R² = 0.92–0.98
- Excessive bottleneck sizes degrade generative quality despite improving reconstruction
- Scaling the encoder yields minimal gains for both reconstruction and generation
- Scaling the decoder improves reconstruction but offers mixed benefits for generation
- ViTok achieves competitive performance with fewer FLOPs than existing methods on ImageNet-1K, COCO, and UCF-101

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The total number of floating points E = L × c in the latent code is the primary predictor of reconstruction performance, independent of code shape or model FLOPs.
- **Mechanism:** E determines the information-theoretic compression ratio (pixels per floating point). Higher E preserves more capacity in the bottleneck, and log(E) linearly correlates with reconstruction metrics. The arrangement of E into tokens L vs channels c matters less than the total.
- **Core assumption:** The bottleneck is fundamentally information-capacity limited, not compute-limited.
- **Evidence anchors:** [abstract]: "bottleneck size strongly correlates with reconstruction performance"; [Section 3.1, Figure 2]: R² = 0.92–0.98 correlations between log(E) and rFID/rIS/rSSIM/rPSNR across ImageNet and COCO; [Section 3.1, Table 2]: bfloat16 vs float32 precision shows no performance difference, suggesting E dominates over precision.

### Mechanism 2
- **Claim:** Scaling the encoder provides minimal benefit for both reconstruction and generation; scaling the decoder improves reconstruction but offers mixed generation gains.
- **Mechanism:** Encoding is a computationally simpler mapping that small models handle adequately. Decoding acts partially as a conditional generator, "filling in" textures from limited information, thus benefiting from more capacity.
- **Core assumption:** Visual encoding doesn't require complex feature extraction beyond what a small ViT provides; the decoder's role extends beyond pure reconstruction.
- **Evidence anchors:** [abstract]: "scaling the encoder yields minimal gains for both tasks, while scaling the decoder improves reconstruction but offers mixed benefits for generation"; [Section 3.3, Figure 6]: R² = 0.00–0.07 between encoder size and all reconstruction metrics; [Section 3.3, Figure 8]: Weak negative correlation (R² = 0.13–0.28) between encoder size and generation metrics; [Section 3.3, Figure 7]: R² = 0.48–0.83 between decoder size and reconstruction metrics.

### Mechanism 3
- **Claim:** High channel sizes c degrade generative performance despite improving reconstruction, creating a reconstruction-generation trade-off at fixed E.
- **Mechanism:** Large channel dimensions create high-dimensional latent distributions that are harder for the generator to model. While high E improves reconstruction capacity, when driven primarily by large c rather than token count L, it increases latent complexity without proportionally improving generation.
- **Core assumption:** Generative model convergence difficulty scales with latent dimensionality, not just total information content.
- **Evidence anchors:** [abstract]: "excessive bottleneck sizes degrading generative quality"; [Section 3.2, Figure 5]: Second-order (parabolic) relationship between E and gFID/gIS—optimal E exists per patch size, not monotonic improvement; [Section 3.2]: "high E, primarily driven by larger channel sizes (c), complicates model convergence and degrades both gFID and gIS metrics."

## Foundational Learning

- **Concept: Variational Auto-Encoder (VAE) with KL Regularization**
  - **Why needed here:** ViTok builds on the VAE framework, sampling latents from learned Gaussian distributions and regularizing toward a unit Gaussian prior for stable latent spaces.
  - **Quick check question:** Why does the VAE require the reparameterization trick for backpropagation through sampling?

- **Concept: Vision Transformer (ViT) with Patch/Tubelet Embedding**
  - **Why needed here:** ViTok replaces CNN backbones with ViT-based encoder/decoder, using 3D convolutions for tubelet embedding of images and videos into token sequences.
  - **Quick check question:** How does patch size affect the number of tokens and the computational cost of self-attention?

- **Concept: Latent Diffusion Models and Tokenizer Role**
  - **Why needed here:** Understanding why tokenizer quality affects downstream generation—poor reconstruction limits what the generator can produce, but overly complex latents make distribution modeling harder.
  - **Quick check question:** Why do latent diffusion models operate in compressed latent space rather than pixel space?

## Architecture Onboarding

- **Component map:** Input processing: 3D convolution (kernel q×p×p) → tubelet/patch tokens (B×L×Cf) → Encoder: Llama-enhanced ViT → linear projection to 2c channels → Bottleneck: VAE sampling z ~ N(zm, zv) with c-dimensional latent; E = L × c → Decoder: Linear upsample c→Cg → Llama-enhanced ViT → 3D transposed convolution → reconstruction

- **Critical path:** Start with Stage 1 training (100K steps) using only MSE + LPIPS + KL losses for stability; Stage 2 fine-tuning (100K steps): freeze encoder, train decoder with GAN loss (λ=1.0); EMA (0.9999) applied only in Stage 2

- **Design tradeoffs:** E vs reconstruction: Higher E → better rFID/rSSIM/rPSNR (log-linear relationship); c vs generation: High c (>32) degrades gFID/gIS despite better reconstruction; Patch size p: Same E with different p yields similar reconstruction; p affects FLOPs via attention but not quality; Encoder vs decoder size: Decoder scaling helps reconstruction (R²=0.48–0.83); encoder scaling does not (R²≈0); Loss balance: GAN/LPIPS improve FID/IS at cost of SSIM/PSNR—decoder becomes more "generative"

- **Failure signatures:** Training instability: Adding GAN loss from scratch causes divergence (use two-stage training); Good reconstruction, poor generation: Channel size c too high relative to token count; No improvement from larger encoder: Expected behavior—encoder scaling is ineffective; Video reconstruction worse than expected at same E: Check if temporal stride q is too aggressive

- **First 3 experiments:** E sweep for reconstruction: Fix ViTok S-B/16 architecture, vary p∈{8,16,32} and c∈{4,8,16,32,64} to confirm log(E) correlation with reconstruction metrics on your target dataset; Optimal E for generation: Train DiT with tokenizers from experiment 1, identify optimal E/c combination for your generation task (look for second-order peak, not maximum E); Encoder scaling validation: Compare ViTok S-B/16 vs B-B/16 vs L-B/16 to verify minimal encoder impact on your specific domain before committing compute to larger encoders

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can adaptive tokenization strategies be optimized to outperform fixed patch-based tokenization?
- **Basis:** [explicit] The authors note that while adaptive masking variants (Mask Simple/Latent) underperform the standard model, "more work here is needed to improve performance further."
- **Why unresolved:** Current adaptive implementations add complexity and computational overhead (FLOPs) without surpassing the performance of the simple baseline.
- **Evidence:** An adaptive method that dynamically adjusts token count while matching the FLOPs and reconstruction quality of fixed ViTok.

### Open Question 2
- **Question:** Why does scaling the tokenizer encoder or decoder yield minimal or negative gains for downstream generation?
- **Basis:** [inferred] The paper concludes that scaling the encoder "fails to improve outcomes" and scaling the decoder provides "limited benefits" for generation, suggesting a scaling ceiling.
- **Why unresolved:** The paper establishes the correlation but leaves the underlying mechanism—why larger latents complicate generator convergence—as a hypothesis.
- **Evidence:** Isolating the specific optimization dynamics or latent space properties that cause generation degradation when model size increases.

### Open Question 3
- **Question:** How can the trade-off between high latent capacity (E) for reconstruction and low latent capacity for generation be resolved?
- **Basis:** [inferred] The authors find that while high E improves reconstruction, it "hinders the convergence and performance of the generative model," creating a difficult balance.
- **Why unresolved:** Current methods require manual tuning to find a "sweet spot," limiting the ability to maximize both objectives simultaneously.
- **Evidence:** A regularization technique or architecture that allows for high-dimensional latent codes (E) without negatively impacting the generative model's training dynamics.

## Limitations

- Reliance on proprietary Shutterstock datasets (450M images, 30M videos) that are not publicly available, making independent validation challenging
- Focus on VAE-based continuous tokenization without exploring discrete tokenizers or alternative compression methods
- Analysis limited to relatively small patch sizes (p=8,16,32) may not capture behaviors at very high or very low resolutions
- Two-stage training procedure complexity may not generalize across different domains or data distributions

## Confidence

- **High confidence**: The log-linear correlation between bottleneck information capacity E and reconstruction performance (rFID/rSSIM/rPSNR). This relationship is consistently demonstrated across multiple datasets and has strong theoretical grounding in information theory.
- **Medium confidence**: The finding that encoder scaling provides minimal benefits while decoder scaling improves reconstruction. This is supported by the experimental data but the mechanism (why encoding is computationally simpler) could vary across domains.
- **Medium confidence**: The reconstruction-generation trade-off at high channel sizes c. While the second-order relationship between E and generation metrics is observed, the specific degradation mechanism for high c values could depend on the downstream generator architecture.

## Next Checks

1. **Independent verification of bottleneck scaling laws**: Train ViTok on a publicly available dataset (e.g., LAION-5B or open subset of Shutterstock) with varying bottleneck sizes E to verify the log(E) correlation with reconstruction metrics and confirm the second-order relationship with generation metrics.

2. **Encoder scaling validation across domains**: Test ViTok with systematically scaled encoder sizes on diverse domains (medical imaging, satellite imagery, artistic datasets) to determine whether the "encoder scaling provides minimal benefit" finding generalizes beyond natural images and videos.

3. **Temporal scaling analysis for video**: Extend the scaling experiments to larger temporal dimensions (q > 4) and longer video sequences to understand how bottleneck scaling laws change for spatiotemporal data, particularly the trade-offs between spatial and temporal information capacity.