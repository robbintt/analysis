---
ver: rpa2
title: Reinforcement Learning from Adversarial Preferences in Tabular MDPs
arxiv_id: '2507.11706'
source_url: https://arxiv.org/abs/2507.11706
tags:
- borda
- where
- regret
- learning
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces preference-based MDPs (PbMDPs), extending
  standard episodic MDPs with adversarial losses by replacing numerical loss observations
  with preference feedback between two candidate arms at each state. The focus is
  on loss functions defined by Borda scores, where the learner observes which of two
  arms is preferred at each state.
---

# Reinforcement Learning from Adversarial Preferences in Tabular MDPs

## Quick Facts
- arXiv ID: 2507.11706
- Source URL: https://arxiv.org/abs/2507.11706
- Reference count: 40
- Primary result: Establishes regret lower bound Ω((H²SK)^{1/3}T^{2/3}) for preference-based MDPs with Borda scores, and provides two algorithms achieving O(T^{2/3}) regret with different state-dependence tradeoffs

## Executive Summary
This paper introduces preference-based MDPs (PbMDPs), extending standard episodic MDPs with adversarial losses by replacing numerical loss observations with preference feedback between two candidate arms at each state. The authors establish a regret lower bound of Ω((H²SK)^{1/3}T^{2/3}) using a novel construction that differs from existing MDP lower bound techniques. Two algorithms achieving regret bounds of order T^{2/3} are developed: one using global optimization over occupancy measures with FTRL, and another using policy optimization with carefully designed Q-function estimators and bonus terms that improves state dependence from S^{2/3} to S^{1/3}.

## Method Summary
The paper develops two algorithms for regret minimization in PbMDPs where learners observe binary preference outcomes between arm pairs rather than numerical losses. The first uses global optimization over occupancy measures with Follow-the-Regularized-Leader (FTRL) and negative Shannon entropy regularization, achieving Õ((H²S²K)^{1/3}T^{2/3}). The second employs policy optimization with Q-function estimators and dilated bonus terms, achieving Õ((H⁶SK⁵)^{1/3}T^{2/3}) while improving state dependence from S^{2/3} to S^{1/3}. Both algorithms use epsilon-greedy exploration strategies to enable unbiased estimation of Borda scores, which requires independent sampling of compared arms. The approach is extended to handle unknown transition dynamics, achieving Õ(H⁷SK⁵)^{1/3}T^{2/3} + H²SK√T regret.

## Key Results
- Establishes regret lower bound Ω((H²SK)^{1/3}T^{2/3}) for PbMDPs with Borda scores using novel construction
- Develops first algorithm achieving Õ((H²S²K)^{1/3}T^{2/3}) using global occupancy measure optimization
- Develops second algorithm achieving Õ((H⁶SK⁵)^{1/3}T^{2/3}) using policy optimization with improved S-dependence (S^{1/3} vs S^{2/3})
- Extends policy optimization approach to unknown transitions with Õ(H⁷SK⁵)^{1/3}T^{2/3} + H²SK√T regret

## Why This Works (Mechanism)

### Mechanism 1: Unbiased Borda Score Estimation
- Claim: Unbiased estimation of Borda scores requires independent sampling of compared arms, achieved via epsilon-greedy exploration.
- Mechanism: The estimator $\hat{b}_t(s, i)$ uses inverse probability weighting and relies on conditional independence of arms $a^L$ and $a^R$ to transform binary preference feedback into an unbiased estimate of average performance against a uniform opponent.
- Core assumption: Learner can enforce policy where $a^L$ and $a^R$ are drawn independently at given state during exploration.
- Break condition: Correlated action sampling during exploration phases will introduce bias into Borda estimator, invalidating regret bounds.

### Mechanism 2: Variance Control Through State Sampling
- Claim: Uniform sampling of states during exploration bounds variance of occupancy-measure-based estimators.
- Mechanism: Algorithm samples state $s_t \sim \text{Unif}(S)$ and maximizes probability of reaching it, ensuring denominator $q^{\pi_t}(s)$ in inverse weighting is bounded away from zero.
- Core assumption: Learner can compute policy that maximizes probability of reaching sampled state $s_t$.
- Break condition: If transition dynamics prevent reaching sampled state $s_t$ with reasonable probability, variance of loss estimator will spike, breaking regret guarantee.

### Mechanism 3: Policy Optimization with Dilated Bonuses
- Claim: Policy optimization with dilated bonus terms improves state dependence from $S^{2/3}$ to $S^{1/3}$.
- Mechanism: Adding bonus term $M_t(s, a)$ to Q-function estimator explicitly penalizes uncertainty regarding less frequently visited states, correcting discrepancy between current policy's occupancy measure and optimal policy's.
- Core assumption: Known transitions (primary result) or accurate confidence intervals for transitions (unknown setting).
- Break condition: If learning rate $\eta$ or bias parameter $\delta$ not tuned relative to $H, S, K$, bonus term may over-penalize or under-explore, degrading performance.

## Foundational Learning

- **Occupancy Measures**: Why needed? Central to Global Optimization algorithm which frames regret minimization as Online Linear Optimization over space of valid occupancy measures. Quick check: Can you define the occupancy measure $q^\pi(s, a)$ for a policy $\pi$?
- **Borda Score**: Why needed? This is the optimization target representing expected preference of arm against uniformly random opponent, which always exists and defines loss function. Quick check: Does Borda score require existence of "winner" that beats all others, or just average performance metric?
- **Follow-the-Regularized-Leader (FTRL)**: Why needed? Both algorithms use FTRL with negative Shannon entropy to update policies/occupancy measures. Understanding stability and regularization tradeoffs required for $T^{2/3}$ bounds. Quick check: Why does FTRL need regularizer (like Shannon entropy) in this adversarial setting?

## Architecture Onboarding

- **Component map**: Environment (PbMDP) -> Binary Preference Observation -> Estimator (converts binary preference $o_t$ to Borda Score $\hat{b}_t$ and Q-function $\hat{Q}_t$ using inverse weighting) -> Optimizer (FTRL instance: Global over polytope or Local over policy simplex) -> Exploration Wrapper (epsilon-greedy layer forcing uniform/independent sampling with probability $\gamma$)
- **Critical path**: 1) Implementing unbiased Borda estimator (Equation 3), 2) Tuning exploration rate $\gamma \approx T^{-1/3}$ to balance unbiased estimation vs linear exploration cost, 3) Efficiently solving FTRL update (closed-form for Policy Optimization, convex solve for Global)
- **Design tradeoffs**: Algorithm 1 (Global): Better dependence on arms $K$ ($K^{1/3}$) and horizon $H$, but worse on states $S$ ($S^{2/3}$) and computationally inefficient (convex optimization). Algorithm 2 (Policy): Better dependence on states $S$ ($S^{1/3}$), computationally efficient (closed form), but worse dependence on arms $K$ ($K^{5/3}$) and horizon $H$.
- **Failure signatures**: Variance Explosion (if denominator $q^{\pi_t}(s)$ becomes tiny in Global Opt, or importance weights explode in Policy Opt, gradients become noise), Slow Convergence (if $T$ is small, $\gamma \approx T^{-1/3}$ exploration cost dominates), Bias (if sampling of arms $(a^L, a^R)$ is coupled, Borda estimator becomes biased).
- **First 3 experiments**: 1) Estimator Validation: Run Borda estimator on fixed small MDP with known preferences to verify $\mathbb{E}[\hat{b}] = b$ and check variance scaling with $1/\gamma$, 2) Scaling Benchmark: Compare Algorithm 1 vs Algorithm 2 on synthetic MDPs varying $S$ to confirm $S^{1/3}$ vs $S^{2/3}$ scaling crossover point, 3) Unknown Transition Stress Test: Run Algorithm 3 (unknown transitions) on RiverSwim style MDP to see if confidence intervals/bonuses correctly drive exploration under preference feedback.

## Open Questions the Paper Calls Out

- **Can the regret upper bound be improved to match the lower bound of Ω((H²SK)^{1/3}T^{2/3})?** The authors note the gap between upper bound Õ((H⁶SK⁵)^{1/3}T^{2/3}) and lower bound suggests either suboptimal algorithm design or loose analysis. Resolution would require an algorithm achieving Õ((H²SK)^{1/3}T^{2/3}) regret, or refined lower bound construction.

- **Can algorithms be developed for PbMDPs based on Condorcet winner criterion instead of Borda scores?** The current work exclusively analyzes Borda scores which guarantee existence of winner but may not align with other practical preference aggregation criteria. Resolution would require algorithms with provable regret bounds under Condorcet winner or von Neumann winner formulations.

- **Can best-of-both-worlds algorithms be developed for PbMDPs?** Current algorithms assume adversarial preferences; stochastic setting with fixed preference functions remains unexplored. Resolution would require single algorithm achieving O(√T) regret for stochastic preferences and O(T^{2/3}) for adversarial preferences without prior knowledge of regime.

## Limitations
- Theoretical guarantees critically depend on maintaining independent sampling of arms during exploration and avoiding variance explosion in inverse-probability estimators
- Global Optimization approach suffers from computational intractability due to need to solve convex optimization over occupancy measure polytope at each episode
- Both algorithms require careful tuning of exploration rates and learning rates with asymptotic formulas that may not translate directly to practical finite-time performance
- Policy optimization approach's improvement in state dependence assumes known transitions or accurate confidence intervals which may not hold in practice

## Confidence
- **High confidence**: Regret lower bound construction and unbiased Borda score estimator mechanism are well-established theoretically with rigorous proofs
- **Medium confidence**: Improvement in state dependence from S^{2/3} to S^{1/3} in policy optimization approach is theoretically sound but may be sensitive to hyperparameter tuning in practice
- **Medium confidence**: Unknown transition extension provides reasonable approach but adds significant complexity with potential practical limitations

## Next Checks
1. **Estimator Bias Validation**: Implement Borda estimator on small known MDP and verify E[b̂] = b empirically across different exploration rates γ, checking for bias when arms are not sampled independently

2. **Scaling Experiment**: Conduct controlled experiments varying S (number of states) while keeping other parameters fixed to empirically confirm theoretical crossover point where Policy Optimization (S^{1/3}) outperforms Global Optimization (S^{2/3})

3. **Unknown Transitions Test**: Implement Algorithm 3 on challenging exploration MDP (e.g., RiverSwim variant) to verify that bonus terms and confidence intervals effectively drive exploration under preference feedback when transitions are unknown