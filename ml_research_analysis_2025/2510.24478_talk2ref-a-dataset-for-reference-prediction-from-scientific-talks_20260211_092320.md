---
ver: rpa2
title: 'Talk2Ref: A Dataset for Reference Prediction from Scientific Talks'
arxiv_id: '2510.24478'
source_url: https://arxiv.org/abs/2510.24478
tags:
- papers
- sbert
- talk
- query
- talks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Talk2Ref, the first dataset for Reference
  Prediction from Talks (RPT), pairing 6,279 scientific talks with 43,429 cited papers
  (26 per talk on average). The task maps long, unstructured spoken transcripts to
  relevant papers, bridging a domain shift from written to spoken language.
---

# Talk2Ref: A Dataset for Reference Prediction from Scientific Talks

## Quick Facts
- arXiv ID: 2510.24478
- Source URL: https://arxiv.org/abs/2510.24478
- Authors: Frederik Broy; Maike Züfle; Jan Niehues
- Reference count: 0
- Primary result: Introduces Talk2Ref dataset (6,279 talks, 43,429 papers) and achieves P@10=14.18% with dual-encoder fine-tuning

## Executive Summary
This paper introduces Talk2Ref, the first dataset for Reference Prediction from Talks (RPT), which pairs 6,279 scientific talks with 43,429 cited papers (26 per talk on average). The task addresses the domain shift from written to spoken language by mapping long, unstructured scientific talk transcripts to relevant cited papers. The authors establish baselines using zero-shot sentence embedding models and propose a dual-encoder architecture trained on Talk2Ref. Results show that fine-tuning significantly improves retrieval performance over zero-shot baselines, demonstrating both the dataset's effectiveness and the task's challenges.

## Method Summary
The Talk2Ref dataset contains 6,279 scientific talks with 43,429 cited papers, split into train (3,971), dev (882), and test (1,426) sets. The task involves retrieving relevant papers given a talk transcript, with temporal filtering ensuring only papers published before the talk are considered. The authors use a dual-encoder architecture with SBERT (all-MiniLM-L6-v2) for both query and key encoders, freezing the key encoder during training. They propose a learned weighted mean aggregation over 512-token chunks to handle long transcripts, along with a sigmoid-based binary cross-entropy loss function for multi-positive scenarios. Training uses batch size 24 with gradient accumulation 3, learning rate 6e-6 for base model and 2e-4 for head, with early stopping patience 4.

## Key Results
- Fine-tuned dual-encoder achieves P@10=14.18%, P@20=19.14%, R@200=39.04%
- Zero-shot baselines perform significantly worse: P@10=9.57%, P@20=13.29%, R@200=26.72%
- Learned weighted mean aggregation outperforms truncation and mean pooling approaches
- Temporal filtering is critical for accurate evaluation

## Why This Works (Mechanism)
The dual-encoder architecture enables efficient retrieval by pre-computing paper embeddings, while fine-tuning adapts general language models to the specific characteristics of spoken scientific discourse. The learned weighted mean aggregation addresses the challenge of long transcripts by allowing the model to learn which segments are most informative for paper retrieval, rather than treating all chunks equally.

## Foundational Learning
- **Temporal filtering**: Ensures only papers published before the talk are considered valid retrievals - critical for realistic evaluation
- **Dual-encoder architecture**: Separates query and key encoding for efficient retrieval at scale - quick check: verify key encoder is frozen during training
- **Learned weighted mean aggregation**: Allows model to prioritize important transcript segments - quick check: compare performance against simple mean pooling
- **Multi-positive loss**: Handles scenarios where multiple papers from the same talk need to be retrieved - quick check: verify batch contains multiple positive papers
- **SBERT embeddings**: Provides strong semantic representations for both talks and papers - quick check: confirm both encoders use same SBERT variant
- **FAISS retrieval**: Enables efficient similarity search over large paper collections - quick check: verify approximate nearest neighbor search is properly configured

## Architecture Onboarding
**Component map**: Transcript (512-token chunks) -> Query encoder -> Learned weighted mean -> Similarity score -> Top-k retrieval
**Critical path**: Transcript chunking → Learned weighted mean aggregation → Cosine similarity scoring → Temporal filtering → Retrieval evaluation
**Design tradeoffs**: Learned weighted mean vs truncation vs mean pooling; fine-tuning vs zero-shot; dual-encoder vs cross-encoder
**Failure signatures**: Poor performance indicates issues with temporal filtering, incorrect chunking, or frozen key encoder not properly implemented
**First experiments**: 1) Test temporal filtering on small subset with known publication dates, 2) Compare learned weighted mean against mean pooling baseline, 3) Verify zero-shot baseline matches reported 9.57% P@10

## Open Questions the Paper Calls Out
None

## Limitations
- Unknown negative sampling strategy during batch construction could affect reported performance gap
- Learned weighted mean aggregation layer dimensions unspecified, potentially impacting reproducibility
- Assumes 512-token chunks preserve sufficient context about cited papers, which may not hold for all talk structures

## Confidence
**High confidence**: Dataset existence and basic task formulation (RPT), general dual-encoder architecture approach
**Medium confidence**: Exact implementation details affecting performance numbers, particularly learned weighted mean aggregation and negative sampling strategies
**Low confidence**: Whether reported performance gap between zero-shot and fine-tuned models remains consistent across different random seeds or with alternative embedding models

## Next Checks
1. Verify temporal filtering implementation by testing retrieval with a small subset where publication dates are known
2. Reimplement learned weighted mean aggregation with different projection layer dimensions to assess sensitivity
3. Compare dual-encoder performance against mean pooling baseline on same train/dev/test splits