---
ver: rpa2
title: Enhancing Multi-Agent Debate System Performance via Confidence Expression
arxiv_id: '2509.14034'
source_url: https://arxiv.org/abs/2509.14034
tags:
- confidence
- debate
- answer
- platt
- o-mini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving performance in
  Multi-Agent Debate (MAD) systems, where multiple LLM agents debate to arrive at
  better answers. The core problem is that LLM agents often fail to effectively communicate
  their relative strengths during debates due to lack of explicit confidence expression,
  leading to suboptimal consensus or incorrect answers.
---

# Enhancing Multi-Agent Debate System Performance via Confidence Expression

## Quick Facts
- arXiv ID: 2509.14034
- Source URL: https://arxiv.org/abs/2509.14034
- Reference count: 23
- One-line primary result: Confidence expression improves Multi-Agent Debate accuracy by up to 3.3% (MMLU: 80.0% → 83.3%)

## Executive Summary
This paper addresses the challenge of improving Multi-Agent Debate (MAD) systems by incorporating explicit confidence expression. LLM agents often fail to effectively communicate their relative strengths during debates, leading to suboptimal consensus or incorrect answers. The proposed ConfMAD framework requires each agent to express confidence alongside their reasoning and answer, using calibrated confidence scores to guide the final decision. Experimental results across multiple benchmarks demonstrate significant performance improvements, with confidence expression not only improving individual LLM performance but also increasing the rate of correct consensus among agents.

## Method Summary
ConfMAD enhances MAD systems by having agents explicitly communicate confidence levels alongside their reasoning and answers. The framework uses two methods for eliciting confidence scores: Key Length-Normalized Sequence Probability (LN) and Self-Verbalized Confidence (SV), along with three calibration techniques (Platt Scaling, Histogram Binning, Temperature Scaling) to align confidence scores across agents. The system runs 3-round debates where agents sequentially respond to each other, with the final answer selected from the agent with the highest calibrated confidence. This approach enables agents to better evaluate each other's responses and reach more accurate consensus decisions.

## Key Results
- ConfMAD improves MAD system accuracy across multiple benchmarks (MMLU: 80.0% → 83.3%, BIGGSM: 81.5% → 84.0%)
- Confidence expression increases correct consensus rates among agents
- Calibrated confidence helps agents correct initial incorrect answers during debate rounds
- Platt Scaling and Temperature Scaling calibration methods outperform Histogram Binning

## Why This Works (Mechanism)
The mechanism works by addressing the fundamental limitation in MAD systems where agents lack explicit signals about their certainty. When agents express calibrated confidence scores, they provide a quantitative measure that helps other agents evaluate the reliability of responses. This is particularly valuable when one agent provides an incorrect answer but another agent's correct answer has higher calibrated confidence, enabling the system to converge on the correct response. The calibration techniques ensure that confidence scores are meaningfully comparable across different agents and contexts, preventing overconfidence or underconfidence from misleading the debate process.

## Foundational Learning

**Multi-Agent Debate (MAD)**: Sequential reasoning framework where multiple LLM agents debate to arrive at better answers through collaborative argumentation. Needed to understand the baseline system being enhanced.

**Calibration Methods**: Techniques (Platt Scaling, Histogram Binning, Temperature Scaling) that adjust raw confidence scores to better reflect true accuracy probabilities. Quick check: verify calibration improves Expected Calibration Error (ECE).

**Confidence Elicitation**: Methods for obtaining meaningful confidence scores from LLMs, including sequence probability normalization and self-verbalized confidence prompts. Needed to generate initial confidence estimates before calibration.

## Architecture Onboarding

**Component Map**: LLM Agent (reasoning + answer + confidence) → Confidence Calibration (Platt/Temperature/Histogram) → Debate Manager (3 rounds) → Final Answer Selection (highest calibrated confidence)

**Critical Path**: Initial independent generation → Round 1 debate response → Round 2 debate response → Confidence calibration → Final answer selection

**Design Tradeoffs**: Sequential debate (computational efficiency) vs. parallel debate (richer interaction), raw vs. calibrated confidence (simplicity vs. accuracy), number of debate rounds (thoroughness vs. context overload)

**Failure Signatures**: Histogram Binning undershooting confidence scores, leading to correct agents being undervalued; excessive debate rounds degrading performance due to context complexity

**First Experiments**:
1. Test LN confidence calculation on sample answers to verify token extraction and probability normalization
2. Run calibration training on validation set with Platt Scaling to confirm logistic regression fits correctly
3. Execute 3-round debate with two agents to verify sequential response flow and confidence score integration

## Open Questions the Paper Calls Out

**Step-wise Confidence Expression**: The authors note that exploring step-wise confidence expression in multi-step reasoning tasks remains unexplored, as current ConfMAD only elicits confidence for final answers rather than intermediate reasoning steps.

**Cross-Dataset Calibration Generalization**: The potential generalization ability of calibration models trained on specific datasets to new distributions remains unexplored, leaving questions about cross-dataset robustness.

**Dynamic Stopping Criteria**: The paper suggests that dynamic stopping criteria based on confidence convergence could replace fixed debate rounds, but this approach has not been implemented or evaluated.

## Limitations

- Calibration methods like Histogram Binning can degrade performance by overly suppressing confidence scores
- Sequential debate format may not capture full potential of parallel agent interactions
- Confidence elicitation methods are designed for question-answering and may not transfer to subjective domains

## Confidence

**High Confidence**: Calibrated confidence expression improves MAD accuracy across multiple benchmarks (3.3% improvement on MMLU)
**Medium Confidence**: Confidence expression helps correct initial incorrect answers during debate rounds
**Medium Confidence**: Platt Scaling and Temperature Scaling outperform Histogram Binning, but optimal conditions need more exploration

## Next Checks

1. **Calibration Robustness Test**: Evaluate ConfMAD's performance when agents use different base model families to assess cross-architecture generalization

2. **Debate Length Sensitivity**: Systematically test debate rounds beyond 3 to identify optimal stopping points and potential degradation

3. **Cross-Domain Transfer**: Apply ConfMAD to non-QA tasks such as code generation to validate framework translation to subjective domains