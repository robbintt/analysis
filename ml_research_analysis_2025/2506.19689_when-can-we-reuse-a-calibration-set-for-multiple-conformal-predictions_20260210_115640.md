---
ver: rpa2
title: When Can We Reuse a Calibration Set for Multiple Conformal Predictions?
arxiv_id: '2506.19689'
source_url: https://arxiv.org/abs/2506.19689
tags:
- prediction
- conformal
- calibration
- inequality
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the practical limitation of standard Inductive
  Conformal Prediction (ICP) that requires a fresh calibration set for each new prediction
  to maintain marginal coverage guarantees. The authors propose using e-conformal
  prediction combined with Hoeffding's inequality to enable repeated use of a single
  calibration set while preserving the desired coverage with high probability.
---

# When Can We Reuse a Calibration Set for Multiple Conformal Predictions?

## Quick Facts
- arXiv ID: 2506.19689
- Source URL: https://arxiv.org/abs/2506.19689
- Reference count: 2
- The authors propose using e-conformal prediction with Hoeffding's inequality to enable repeated use of a single calibration set while preserving coverage guarantees with high probability (~98%).

## Executive Summary
This paper addresses the practical limitation of standard Inductive Conformal Prediction (ICP) that requires a fresh calibration set for each new prediction to maintain marginal coverage guarantees. The authors propose using e-conformal prediction combined with Hoeffding's inequality to enable repeated use of a single calibration set while preserving the desired coverage with high probability. The core method involves training a deep neural network on CIFAR-10, then using the calibration set to estimate the empirical mean of non-conformity scores. By applying Hoeffding's inequality, they derive a correction term (t = 1/50) that provides a probabilistic bound on the true expectation. This bound is incorporated into a modified Markov's inequality to construct prediction sets with quantifiable confidence.

## Method Summary
The authors propose a method to reuse a single calibration set for multiple conformal predictions by combining e-conformal prediction with Hoeffding's inequality. The approach trains a CNN on CIFAR-10, splits the test set into 5,000 calibration samples and 5,000 conformal test samples, and computes non-conformity scores L(x,y) = 1 - p̂(y|x). The empirical mean of these scores (approximately 0.158) is combined with a Hoeffding correction term (t = 1/50) to provide a 98% confidence upper bound on the true expectation. This bound is then used in a modified Markov's inequality to construct prediction sets with a threshold of approximately 0.89 for α̃ = 0.2.

## Key Results
- With approximately 98% confidence, the calibration set can be reused for future predictions while maintaining the coverage guarantee.
- On the test set, the method produced prediction sets of varying sizes, with most (4,150 out of 4,840) containing a single label.
- The approach demonstrates a practical way to make conformal prediction more applicable in real-world scenarios by reducing the need for repeated calibration.

## Why This Works (Mechanism)

### Mechanism 1: E-test Statistics via Modified Markov's Inequality
- Claim: Non-conformity scores can be transformed into prediction sets with quantifiable confidence bounds using e-values rather than traditional p-values.
- Mechanism: The method applies Markov's inequality to a non-negative random variable Z, yielding P(Z ≥ E[Z]/α̃) ≤ α̃. By estimating E[Z] empirically and incorporating a Hoeffding correction, this constructs prediction sets where the non-conformity threshold becomes (empirical_mean + t)/α̃.
- Core assumption: The non-conformity measure L(x,y) produces bounded scores in [0,1], enabling application of concentration inequalities.
- Evidence anchors:
  - [abstract]: "This correction allows us to apply a modified Markov's inequality, leading to the construction of prediction sets with quantifiable confidence."
  - [Page 3]: "Our approach will leverage e-test statistics. The fundamental idea behind e-test statistics is a direct application of Markov's inequality."
  - [corpus]: Weak corpus evidence—neighbor papers focus on other CP variants, not e-value approaches specifically.
- Break condition: If non-conformity scores are unbounded or lack finite expectation, Markov's inequality cannot be meaningfully applied.

### Mechanism 2: Hoeffding-Based Upper Bound on True Expectation
- Claim: A single calibration set can be reused for multiple predictions if the empirical mean of non-conformity scores overestimates the true mean with known probability.
- Mechanism: Hoeffding's inequality provides that with probability ≥ 1 - exp(-2nt²/(b-a)²), the true mean μ satisfies μ ≤ (sample_mean + t). By choosing t = 1/50 with n = 5000 calibration samples and bounded scores in [0,1], this yields ~98% confidence that the bound holds.
- Core assumption: Calibration samples are i.i.d. draws from the same distribution as test samples (Assumption: the paper uses "exchangeable" but relies on independence for Hoeffding).
- Evidence anchors:
  - [Page 4]: "By choosing t = 1/50, we obtain a probability of 1 - exp(-10000 · (1/50)²) = 1 - exp(-4) ≈ 0.98."
  - [Page 7]: "This suggests that we can be approximately 98% confident in using this calibration set for all new data points with a Hoeffding correction of t = 1/50."
  - [corpus]: No direct corpus support for this specific Hoeffding+CP combination.
- Break condition: If distribution shift occurs between calibration and test data, the i.i.d. assumption fails and bounds become invalid.

### Mechanism 3: Threshold-Based Prediction Set Construction
- Claim: Given the corrected upper bound on expected non-conformity, prediction sets can be constructed by including all labels whose non-conformity scores fall below the derived threshold.
- Mechanism: With empirical mean ≈ 0.158 and t = 0.02, the numerator in inequality (3) is ≈ 0.178. For α̃ = 0.2, this yields P(L ≤ 0.89) > 0.8, meaning prediction sets include all labels with non-conformity scores below 0.89.
- Core assumption: The non-conformity measure L(x,y) = 1 - p̂(y|x) correctly ranks label plausibility (higher predicted probability → lower non-conformity).
- Evidence anchors:
  - [Page 7]: "We construct prediction sets accordingly by applying this threshold across the entire ConformalPredictionTestSet."
  - [Page 7, Table 1]: Shows 4,150 single-label predictions, 663 two-label, indicating the threshold mechanism produces informative uncertainty quantification.
  - [corpus]: Similar threshold mechanisms appear in standard CP; corpus papers discuss coverage- efficiency tradeoffs.
- Break condition: If the base model's softmax outputs are poorly calibrated (overconfident), non-conformity scores may not meaningfully reflect true uncertainty.

## Foundational Learning

- Concept: **Marginal vs. Conditional Coverage in Conformal Prediction**
  - Why needed here: The paper explicitly addresses a common misconception that CP provides individualized uncertainty. Understanding that standard guarantees are marginal (averaged over calibration + test) is essential to grasp why reuse is problematic and why this method's 98% probabilistic guarantee matters.
  - Quick check question: If you deploy this method on 100 independent test points, approximately how many times would you expect the coverage guarantee to hold?

- Concept: **Markov's Inequality and E-values**
  - Why needed here: The entire approach builds on the e-test framework, which differs fundamentally from p-value based conformal prediction. Understanding why P(Z ≥ E[Z]/α) ≤ E[Z]·α enables this construction is prerequisite.
  - Quick check question: Given a non-negative random variable with E[Z] = 0.2, what is the upper bound on P(Z ≥ 1.0) using Markov's inequality?

- Concept: **Hoeffding's Inequality for Concentration Bounds**
  - Why needed here: The correction term t = 1/50 is not arbitrary—it's derived from Hoeffding's bound. Understanding how sample size n, range (b-a), and confidence level interrelate is critical for adapting this method.
  - Quick check question: If your calibration set had only 1,000 samples instead of 5,000, what correction t would maintain approximately 98% confidence?

## Architecture Onboarding

- Component map:
  ```
  Training Data → Base Model (DNN) → Softmax Outputs
                                        ↓
  Calibration Data → Non-conformity Scores → Empirical Mean (0.158)
                                        ↓
                    Hoeffding Correction (t=1/50) → Upper Bound (~0.178)
                                        ↓
  Test Input → All Class Scores → Threshold Filter (0.89) → Prediction Set
  ```

- Critical path:
  1. Train base classifier to convergence (the paper achieved 86% test accuracy)
  2. Reserve calibration set (n=5000) separate from training
  3. Compute non-conformity scores L(x,y) = 1 - p̂(y|x) on calibration set
  4. Calculate empirical mean and apply Hoeffding: mean + t where t = 1/50
  5. For each test point, compute L(x, y') for all possible labels y', include in prediction set if L < (mean + t)/α̃

- Design tradeoffs:
  - **Correction t vs. confidence**: Smaller t yields tighter bounds but lower confidence; t = 1/50 gives 98% confidence but could be adjusted
  - **Calibration set size vs. correction magnitude**: Larger n permits smaller t while maintaining confidence
  - **α̃ parameter vs. prediction set size**: α̃ = 0.2 yields coverage target > 0.8; smaller α̃ produces larger prediction sets

- Failure signatures:
  - Empty prediction sets: Threshold too aggressive; increase α̃ or verify model calibration
  - All-label prediction sets: Model severely miscalibrated or distribution shift detected
  - Coverage degradation over time: Distribution drift between calibration and deployment data invalidates i.i.d. assumption

- First 3 experiments:
  1. **Reproduce baseline**: Train model on CIFAR-10, split test set 50/50 for calibration/conformal prediction, verify empirical mean ≈ 0.158 and single-label prediction rate ≈ 85%
  2. **Sensitivity analysis**: Vary calibration set size (n = 1000, 2500, 5000, 10000) and compute required t for 95%, 98%, 99% confidence levels to characterize the n-t-confidence relationship
  3. **Distribution shift stress test**: Apply calibration set from CIFAR-10 to CIFAR-10-C (corrupted images) and measure coverage degradation to quantify robustness to assumption violations

## Open Questions the Paper Calls Out
None

## Limitations
- **Model Calibration Dependence**: The approach assumes the base model produces well-calibrated softmax outputs, but doesn't validate model calibration metrics.
- **Distribution Shift Vulnerability**: The 98% confidence claim relies on i.i.d. assumptions between calibration and test data without empirical validation of robustness to distribution shift.
- **One-Time Use Assumption**: The method doesn't address how to detect when the calibration set becomes stale or when the underlying data distribution has changed significantly.

## Confidence
- **High Confidence**: The mathematical derivation of Hoeffding's inequality and its application to construct the correction term t = 1/50 is rigorous and well-established.
- **Medium Confidence**: The e-test framework using modified Markov's inequality is theoretically sound, but the specific adaptation to conformal prediction with this correction hasn't been extensively validated in the literature.
- **Low Confidence**: The practical utility claim that this makes conformal prediction "more applicable in real-world scenarios" lacks empirical support beyond the single CIFAR-10 experiment.

## Next Checks
1. **Distribution Shift Experiment**: Test the method on CIFAR-10 vs. CIFAR-10-C (corrupted images) to quantify coverage degradation when i.i.d. assumptions are violated. Measure how the 98% confidence degrades under systematic shifts.
2. **Calibration Set Size Sensitivity**: Systematically vary calibration set size (n = 1000, 2500, 5000, 10000) and measure the relationship between n, required t, and achieved coverage to characterize the trade-off.
3. **Base Model Calibration Analysis**: Evaluate the base model's calibration using metrics like Expected Calibration Error (ECE) or Brier score. Compare prediction set quality when using calibrated vs. uncalibrated models to assess sensitivity to this assumption.