---
ver: rpa2
title: 'UMA: A Family of Universal Models for Atoms'
arxiv_id: '2506.23971'
source_url: https://arxiv.org/abs/2506.23971
tags:
- energy
- training
- dataset
- materials
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Meta FAIR introduces UMA, a family of universal models for atoms
  that can accurately and efficiently simulate a wide range of chemical systems across
  multiple domains including materials, molecules, catalysts, and molecular crystals.
  Trained on nearly half a billion diverse 3D atomic structures, UMA uses a novel
  Mixture of Linear Experts (MoLE) architecture to efficiently scale model capacity
  without sacrificing speed.
---

# UMA: A Family of Universal Models for Atoms

## Quick Facts
- **arXiv ID:** 2506.23971
- **Source URL:** https://arxiv.org/abs/2506.23971
- **Reference count:** 40
- **Primary result:** UMA achieves state-of-the-art performance on multiple benchmarks including Matbench Discovery and AdsorbML, outperforming specialized models while enabling fast molecular dynamics simulations up to 1.4 ns/day on a single GPU.

## Executive Summary
Meta FAIR introduces UMA, a family of universal models for atoms that can accurately and efficiently simulate a wide range of chemical systems across multiple domains including materials, molecules, catalysts, and molecular crystals. Trained on nearly half a billion diverse 3D atomic structures, UMA uses a novel Mixture of Linear Experts (MoLE) architecture to efficiently scale model capacity without sacrificing speed. The largest model, UMA-medium, achieves state-of-the-art performance on multiple benchmarks including Matbench Discovery and AdsorbML, outperforming specialized models. UMA models can run molecular dynamics simulations at speeds of up to 1.4 ns/day on a single GPU while maintaining accuracy comparable to or better than task-specific models. All code, weights, and data are publicly released to accelerate research in computational chemistry and materials science.

## Method Summary
UMA is trained on a massive multi-domain dataset (~460M structures) combining materials, molecules, catalysts, MOFs, and molecular crystals. The model uses an eSEN backbone with Mixture of Linear Experts (MoLE) architecture, where UMA-S has 32 experts with L_max=2 and 4 blocks, while UMA-M has 32 experts with L_max=4 and 10 blocks. Training follows a two-stage procedure: Stage 1 uses direct force prediction with BF16 precision for stability and speed, while Stage 2 switches to FP32 precision and conservative (autograd-derived) forces/stress for energy conservation. The model incorporates global embeddings for task, charge, and spin properties, enabling task-specific routing of atomic structures through different expert combinations.

## Key Results
- UMA-medium achieves state-of-the-art performance on Matbench Discovery and AdsorbML benchmarks, outperforming specialized models.
- UMA models can run molecular dynamics simulations at speeds of up to 1.4 ns/day on a single GPU.
- The MoLE architecture enables efficient scaling, with UMA-medium having 1.4B parameters but only ~50M active parameters per atomic structure.
- Multi-task training provides regularization benefits, reducing overfitting and improving generalization on sparse data domains like catalysis.

## Why This Works (Mechanism)

### Mechanism 1: Pre-mergable Linear Experts (MoLE)
The Mixture of Linear Experts (MoLE) architecture decouples model capacity from inference latency by enabling weight pre-merging. Unlike standard Mixture of Experts which requires dynamic routing per token, MoLE uses linear experts where the weighted sum of weights $\sum \alpha_k W_k$ can be computed once before simulation starts. This turns a 1.4B parameter model into a static 50M parameter dense model for a specific molecular dynamics run.

### Mechanism 2: Cross-Domain Regularization via Multi-Task Training
Training on a massive, multi-domain dataset (~500M structures) acts as a regularizer, reducing overfitting and improving generalization on sparse data domains like catalysis. By forcing a single model to learn representations for materials, molecules, and catalysts simultaneously, the model learns fundamental physical interactions rather than domain-specific artifacts.

### Mechanism 3: Two-Stage Precision-Aware Training
A two-stage training schedule (direct forces → conservative) combined with precision switching recovers the speed of low-precision training while maintaining the accuracy of high-precision fine-tuning. Pre-training uses BF16 and direct force prediction for speed and stability, then fine-tuning switches to FP32 and autograd-derived forces to ensure energy conservation.

## Foundational Learning

- **Rotational Equivariance (eSCN)**: Needed because predicting forces requires the model to rotate vectors correctly when the input structure rotates. Quick check: If I rotate a water molecule by 45 degrees, does the predicted force vector rotate by 45 degrees or stay the same?

- **Energy Conservation in MLIPs**: Needed because for Molecular Dynamics, forces must be the gradient of the energy; otherwise, the system will gain/lose energy artificially. Quick check: Why does a "conservative" model require an additional backward pass (autograd) compared to a direct force model?

- **Mixture of Experts (MoE)**: Understanding standard MoE (sparse, dynamic routing) highlights why UMA's "Linear" variant (MoLE) is novel—it allows pre-computation because the routing is based on global, not local, features. Quick check: In standard MoE, why can't you merge the expert weights before seeing the input token?

## Architecture Onboarding

- **Component map:** Atomic positions + Global embeddings (Task, Charge, Spin) → eSEN encoder → MoLE Block → Energy/Force/Stress heads
- **Critical path:** The MoLE Router → Weight Merging → Dense Inference. If merging is skipped or router is conditioned on local atom positions, efficiency benefit is lost.
- **Design tradeoffs:** UMA-S is optimized for MD speed (100k+ atoms) while UMA-M is optimized for accuracy (DFT surrogate) but is slower. BF16 is mandatory for training stability/efficiency; FP32 is mandatory for final energy conservation.
- **Failure signatures:** Energy Drift (in NVE simulations, if total energy rises), Neighbor Saturation (if memory explodes during inference), 6Å Cutoff Artifacts (if molecules interact strangely at exactly 6Å).
- **First 3 experiments:**
  1. Validation Speed Test: Run inference on 1,000 atoms with UMA-S vs. UMA-M. Verify UMA-S achieves ~16 steps/sec (1.4 ns/day) on a single GPU.
  2. Energy Conservation Check: Run short NVE MD simulation on simple molecule. Plot Total Energy vs. Time to confirm model is "conservative."
  3. Task Ablation: Provide model with wrong task embedding (e.g., feed molecule with "Materials" task token). Observe if accuracy degrades, confirming MoLE routing is functioning.

## Open Questions the Paper Calls Out

- Can continuous embeddings replace discrete integer embeddings for total charge and spin to enable generalization to unseen spin or charge states? The paper states that currently, "each discrete charge or spin uses a separate embedding, which limits its ability to generalize to unseen spins or charges."

- How can the model be adapted to handle long-range interactions for systems where non-bonded components are separated by more than the 6Å cutoff? The paper notes that "if an adsorbate starts at 7Å from a catalyst's surface the model views these as two independent non-interacting structures."

- Does the performance convergence between MoLE and dense architectures at large scales persist if the dataset size is increased beyond the current half-billion structures? The paper hypothesizes that the diminishing returns of MoLE over dense models at 700M parameters are "limited by our dataset's size."

## Limitations

- The 6Å neighbor cutoff may limit accuracy for long-range interactions and charge transfer systems.
- The model's performance on highly dynamic systems (e.g., liquids) is not thoroughly characterized.
- Computational efficiency gains from MoLE are conditional on static task embeddings during inference.

## Confidence

- **High Confidence**: Claims about UMA's multi-domain training dataset (~460M structures), MoLE architecture's ability to pre-merge weights for inference speed, and two-stage training procedure (BF16 pre-training followed by FP32 fine-tuning).
- **Medium Confidence**: The assertion that UMA outperforms specialized models on Matbench Discovery and AdsorbML benchmarks, though magnitude of improvement varies across tasks.
- **Low Confidence**: Claims about the model's ability to handle systems with >10k atoms without architectural modifications, as the paper acknowledges Graph Parallelism is needed for such scales.

## Next Checks

1. **Architecture Verification**: Replicate the claimed 1.4 ns/day simulation speed on a single GPU using the UMA-S model for a 100,000-atom system, measuring actual steps/second under identical hardware conditions.

2. **Energy Conservation Test**: Run NVE molecular dynamics simulations on three distinct chemical systems (molecule, material, catalyst) for 1,000 steps each, plotting total energy trajectories to verify conservation and compare against paper's reported stability.

3. **Cross-Domain Transfer Assessment**: Train a single-task model on one dataset (e.g., OMat24) and compare its performance on an out-of-domain dataset (e.g., OMol25) against UMA-medium, quantifying the transfer learning benefit claimed from multi-task training.