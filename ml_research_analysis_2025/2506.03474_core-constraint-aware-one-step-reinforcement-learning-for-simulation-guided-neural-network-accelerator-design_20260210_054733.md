---
ver: rpa2
title: 'CORE: Constraint-Aware One-Step Reinforcement Learning for Simulation-Guided
  Neural Network Accelerator Design'
arxiv_id: '2506.03474'
source_url: https://arxiv.org/abs/2506.03474
tags:
- design
- core
- reward
- policy
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CORE is a one-step RL framework for simulation-guided design space
  exploration that jointly optimizes hardware resources and mappings in DNN accelerators.
  It uses a structured policy to sample feasible configurations via a scaling-graph-based
  decoder and parallel simulation, updating via a surrogate objective without value
  functions.
---

# CORE: Constraint-Aware One-Step Reinforcement Learning for Simulation-Guided Neural Network Accelerator Design

## Quick Facts
- arXiv ID: 2506.03474
- Source URL: https://arxiv.org/abs/2506.03474
- Authors: Yifeng Xiao; Yurong Xu; Ning Yan; Masood Mortazavi; Pierluigi Nuzzo
- Reference count: 32
- Key outcome: CORE is a one-step RL framework for simulation-guided design space exploration that jointly optimizes hardware resources and mappings in DNN accelerators. It uses a structured policy to sample feasible configurations via a scaling-graph-based decoder and parallel simulation, updating via a surrogate objective without value functions. This approach directly samples complete designs, enabling efficient exploration and constraint satisfaction. Experiments on vision, language, and recommendation DNNs show CORE achieves at least 15× improvement in latency and latency-area-sum metrics compared to state-of-the-art methods while requiring fewer samples. Ablation studies confirm the effectiveness of reward shaping and scaling graph decoding in maintaining feasibility and accelerating convergence.

## Executive Summary
CORE introduces a one-step reinforcement learning framework for efficient neural network accelerator design. The approach jointly optimizes hardware resources and layer mappings through a structured policy that ensures constraint satisfaction while minimizing resource usage and latency. By using a scaling-graph-based decoder and parallel simulation, CORE can explore the design space more efficiently than traditional iterative methods. The framework demonstrates significant improvements in design quality metrics while requiring fewer samples than existing approaches.

## Method Summary
CORE employs a one-step RL framework that jointly optimizes hardware resources and layer mappings in DNN accelerators. The method uses a structured policy with a scaling-graph-based decoder to sample feasible configurations directly, avoiding the need for iterative refinement. Parallel simulation enables efficient evaluation of multiple designs simultaneously. The policy is updated using a surrogate objective function without requiring value functions, making the approach computationally efficient. The framework incorporates reward shaping and constraint-aware sampling to maintain feasibility throughout the exploration process.

## Key Results
- Achieves at least 15× improvement in latency and latency-area-sum metrics compared to state-of-the-art methods
- Requires fewer samples than existing approaches for comparable or better design quality
- Ablation studies confirm the effectiveness of reward shaping and scaling graph decoding in maintaining feasibility and accelerating convergence
- Demonstrates effectiveness across vision, language, and recommendation DNNs

## Why This Works (Mechanism)
The CORE framework's effectiveness stems from its ability to directly sample complete, feasible accelerator designs through a structured policy. The scaling-graph-based decoder ensures that sampled configurations satisfy hardware constraints while optimizing for latency and resource usage. By employing one-step RL with a surrogate objective, the framework avoids the computational overhead of value function estimation. Parallel simulation allows for efficient evaluation of multiple design candidates simultaneously, accelerating the exploration process. The combination of reward shaping and constraint-aware sampling maintains feasibility throughout the optimization process.

## Foundational Learning

1. **Reinforcement Learning Basics**: Understanding RL concepts like policies, rewards, and state spaces is crucial for grasping CORE's approach. Quick check: Can you explain the difference between value-based and policy-based RL methods?

2. **Neural Network Accelerator Architecture**: Knowledge of DNN accelerator components (PE arrays, on-chip memory, etc.) and their impact on performance is essential. Quick check: How do hardware resource constraints affect accelerator design?

3. **Design Space Exploration**: Familiarity with optimization techniques for exploring large design spaces is important. Quick check: What are the challenges in efficiently exploring the DNN accelerator design space?

4. **Constraint Satisfaction**: Understanding methods for ensuring feasible solutions in optimization problems is key. Quick check: How does CORE's structured policy differ from unconstrained optimization approaches?

5. **Simulation-Guided Design**: Knowledge of using simulations to evaluate and optimize hardware designs is relevant. Quick check: What are the advantages and limitations of simulation-guided design exploration?

6. **Scaling Graphs**: Understanding how scaling graphs represent design options and their impact on performance is important. Quick check: How does the scaling-graph-based decoder in CORE ensure constraint satisfaction?

## Architecture Onboarding

Component map: Scaling-graph-based decoder -> Structured policy -> Parallel simulation -> Surrogate objective update

Critical path: The most critical component is the scaling-graph-based decoder, which ensures feasible sampling of accelerator configurations. This decoder directly impacts the quality and feasibility of designs explored by the policy.

Design tradeoffs: CORE trades off exploration flexibility for computational efficiency by using a structured policy and one-step RL. This approach may limit the discovery of novel designs but significantly reduces the number of samples required.

Failure signatures: Potential failures include infeasible designs due to incorrect constraint handling, suboptimal exploration of the design space due to the structured policy, and inaccurate performance predictions from simulation.

3 first experiments:
1. Test the scaling-graph-based decoder with a simple DNN to verify constraint satisfaction
2. Evaluate the parallel simulation framework's efficiency with varying numbers of design candidates
3. Compare the surrogate objective approach with traditional value-based RL methods on a small design space

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Scalability to larger, more complex DNNs and accelerator configurations may be limited
- Generalizability across different hardware platforms is uncertain
- Reliance on simulation-guided exploration may not fully capture real-world performance variations
- Fixed structure of the scaling-graph-based decoder could limit flexibility in exploring novel design spaces
- Comparison metrics and baselines may not comprehensively represent all relevant state-of-the-art methods

## Confidence
- High confidence in the effectiveness of reward shaping and scaling graph decoding mechanisms in maintaining feasibility and accelerating convergence, based on ablation studies
- Medium confidence in the overall performance improvements claimed, as the results are based on simulations and may not fully translate to real-world scenarios
- Low confidence in the scalability and generalizability of the approach to diverse hardware platforms and larger design spaces, due to limited experimental validation

## Next Checks
1. Conduct experiments on a wider range of DNNs and accelerator configurations to assess scalability and generalizability
2. Implement and test the approach on real hardware platforms to validate simulation-guided results
3. Compare the performance of CORE with a broader set of state-of-the-art methods, including those not based on reinforcement learning, to ensure comprehensive benchmarking