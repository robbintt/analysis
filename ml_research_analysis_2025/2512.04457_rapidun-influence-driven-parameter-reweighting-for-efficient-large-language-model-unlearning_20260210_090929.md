---
ver: rpa2
title: 'RapidUn: Influence-Driven Parameter Reweighting for Efficient Large Language
  Model Unlearning'
arxiv_id: '2512.04457'
source_url: https://arxiv.org/abs/2512.04457
tags:
- unlearning
- forget
- clean
- rapidun
- retain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently removing specific
  data influences from large language models without full retraining. The authors
  propose RapidUn, an influence-guided unlearning framework that uses fast token-wise
  influence estimation (RapidIn) to derive per-sample weights for targeted parameter
  updates.
---

# RapidUn: Influence-Driven Parameter Reweighting for Efficient Large Language Model Unlearning

## Quick Facts
- arXiv ID: 2512.04457
- Source URL: https://arxiv.org/abs/2512.04457
- Reference count: 39
- Primary result: Achieves up to 100× speedup over full retraining while consistently reducing ASR on seen and out-of-distribution triggers, with minimal clean perplexity loss.

## Executive Summary
RapidUn addresses the challenge of efficiently removing specific data influences from large language models without full retraining. The method uses influence-guided unlearning that leverages fast token-wise influence estimation to derive per-sample weights for targeted parameter updates. These weights modulate LoRA-based gradient ascent on forget data and descent on retain data, enabling stable forgetting while preserving general model behavior. Evaluated on Llama-3-8B and Mistral-7B across Dolly-15k and Alpaca-57k, RapidUn achieves substantial performance gains over baselines like GA, Fisher, and LoReUn.

## Method Summary
RapidUn combines influence estimation with parameter-efficient fine-tuning for targeted unlearning. The method first estimates per-sample influence through RapidIn, which computes token-level gradient alignments between examples in a single forward-backward pass. These influence scores are then mapped to adaptive update weights that guide selective parameter updates—forgetting harmful behavior while retaining general knowledge. The framework uses LoRA adapters (r=16, α=16) to constrain updates to low-rank modifications, keeping the backbone frozen. The unlearning objective combines weighted gradient descent on retain data with weighted gradient ascent on forget data, optimized via AdamW with cosine learning rate decay.

## Key Results
- Achieves up to 100× speedup over full retraining while maintaining competitive clean perplexity
- Consistently outperforms baselines like GA, Fisher, and LoReUn on both seen and out-of-distribution forgetting
- Reduces ASR by up to 29 percentage points on seen triggers and 21 on OOD triggers
- Maintains stable forgetting with minimal degradation to clean performance across different model architectures

## Why This Works (Mechanism)

### Mechanism 1: Token-wise Gradient Alignment as Influence Proxy
RapidIn computes normalized token-level gradient inner products between sample pairs in a single forward-backward pass, aggregating across tokens to yield cross-sample influence estimates. This assumes token-level gradient alignment correlates with actual training-data influence on model predictions. The method is scalable compared to traditional Hessian-based influence functions but may fail if gradient alignment doesn't capture true sample influence in highly non-convex loss regions.

### Mechanism 2: Four-Direction Influence Fusion for Selective Reweighting
The framework fuses influences across forget→forget (FF), forget→retain (FR), retain→forget (RF), and retain→retain (RR) directions into per-sample scores. This directional fusion modulates how aggressively to forget or protect based on cross-set gradient interactions. While ablation shows self-only terms provide substantial benefits, cross-set terms offer marginal but consistent gains in forgetting performance.

### Mechanism 3: Robust Normalization for Bounded, Stable Weights
The method applies robust statistics (median/MAD), temperature scaling, and log-space clipping to transform raw influence scores into bounded, mean-one weights. This reduces outlier sensitivity and prevents gradient explosion during optimization. The assumption is that influence score distributions are heavy-tailed, though this may unnecessarily attenuate useful signal if distributions are approximately normal.

## Foundational Learning

- **Concept: Gradient Ascent/Descent for Unlearning**
  - Why needed here: RapidUn's objective combines descent on retain data with ascent on forget data to remove harmful influences while preserving clean behavior
  - Quick check question: Why does gradient ascent on the forget set increase perplexity on poisoned outputs while descent on the retain set preserves clean behavior?

- **Concept: Influence Functions (Basics)**
  - Why needed here: RapidUn extends classical influence-function intuition to LLM-scale via token-wise approximations
  - Quick check question: What does a high positive influence score from forget sample i to forget sample j suggest about their shared contribution to harmful behavior?

- **Concept: LoRA and Parameter-Efficient Fine-Tuning**
  - Why needed here: RapidUn restricts optimization to LoRA adapters, keeping the backbone frozen to reduce computational cost
  - Quick check question: How does freezing the backbone and updating only low-rank adapters affect the capacity to unlearn versus the risk of over-forgetting?

## Architecture Onboarding

- **Component map**: RapidIn (token-wise gradient alignment) -> Directional fusion (FF/FR/RF/RR) -> Robust weight mapping (RobustScale + clipping) -> LoRA fine-tuning (weighted ascent/descent)

- **Critical path**: Influence estimation → directional fusion → robust weight mapping → LoRA fine-tuning. Weights are computed once before training (static); the backbone remains frozen throughout.

- **Design tradeoffs**: Static vs. dynamic weighting (current method precomputes weights); LoRA capacity (r=16 balances expressiveness and efficiency); retain:forget ratio (default k≈3 balances retention signal and forget supervision).

- **Failure signatures**: Over-forgetting (clean perplexity rises sharply); under-forgetting (ASR remains high); weight collapse (weights converge to similar values); instability (oscillating perplexity or diverging loss).

- **First 3 experiments**:
  1. Ablate influence-guided weighting: Compare RapidUn vs. Uniform vs. Self-only on Llama-3-8B/Dolly-15k to verify cross-set terms provide incremental ASR reductions
  2. Cross-model validation: Replicate full pipeline on Mistral-7B using identical hyperparameters to assess architecture independence
  3. Scalability stress test: Run RapidUn and baselines on Alpaca-57k with identical GPU settings to verify claimed ~100× speedup over retraining

## Open Questions the Paper Calls Out

- **Open Question 1**: Can dynamic, in-training influence reweighting improve unlearning stability compared to the current static pre-computation method? The paper states future work will explore dynamic influence-based reweighting, as static weights may fail to capture shifting model dynamics.

- **Open Question 2**: How does RapidUn perform in multimodal or streaming unlearning contexts beyond static text-based instruction data? Section 6 notes experiments focus on text-based instruction data; extensions to multimodal or streaming settings remain unexplored.

- **Open Question 3**: What theoretical bounds or "certified forgetting" guarantees can be derived for RapidUn's influence-guided objective? The Conclusion lists theoretical guarantees for certified forgetting as future work, as the paper lacks formal proofs of strict information removal.

- **Open Question 4**: How robust is RapidUn when the provided forget set is non-representative or incomplete regarding the harmful behavior? Section 6 states the method assumes access to a small but representative forget set, which may not always be available.

## Limitations
- Core assumption that token-level gradient alignment reliably proxies per-sample influence remains empirically validated only within narrow trigger/architecture regimes
- Fixed-weight influence mapping doesn't adapt to evolving parameter distributions during fine-tuning
- Robust normalization choices (median/MAD, τ, clipping) are not fully specified and may over-suppress legitimate influence signals

## Confidence
- **High confidence**: RapidUn achieves consistent ASR reductions vs. baselines on both seen and OOD triggers under controlled settings; efficient LoRA-based unlearning with ~100× speedup claim is methodologically sound
- **Medium confidence**: Influence-driven weighting contributes to forgetting performance; the mechanism generalizes across Llama-3-8B and Mistral-7B; scaling to Alpaca-57k maintains efficiency
- **Low confidence**: Token-wise gradient alignment is a universally reliable influence proxy; robust normalization choices are optimal; cross-set influence fusion is necessary for strong performance; results extend to non-trigger, real-world unlearning tasks

## Next Checks
1. **Sensitivity ablation**: Vary temperature τ, weight bounds (w_min/w_max), and fusion coefficients (α,β,γ,δ) across multiple random seeds on Llama-3-8B/Dolly-15k; quantify impact on ASR reduction and clean PPL to isolate robust hyperparameters
2. **Influence robustness test**: Evaluate RapidIn-estimated influence scores against ground-truth influence (via expensive Hessian-based computation on small subsets) to validate the token-alignment approximation, especially on adversarially crafted or atypical samples
3. **Real-world forgetting task**: Apply RapidUn to remove sensitive demographic attributes or domain-specific jargon from a pretrained LLM, measuring both attribute-specific ASR reduction and overall task performance degradation