---
ver: rpa2
title: Scalable-Softmax Is Superior for Attention
arxiv_id: '2501.19399'
source_url: https://arxiv.org/abs/2501.19399
tags:
- ssmax
- attention
- softmax
- size
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Scalable-Softmax (SSMax) addresses the attention fading problem
  in Transformers where the maximum output of Softmax approaches zero as input vector
  size increases. The method introduces a scaling parameter s log n to Softmax, preventing
  attention distribution from flattening as context size grows.
---

# Scalable-Softmax Is Superior for Attention

## Quick Facts
- arXiv ID: 2501.19399
- Source URL: https://arxiv.org/abs/2501.19399
- Authors: Ken M. Nakanishi
- Reference count: 14
- Primary result: SSMax reduces pretraining loss by 0.008 and maintains retrieval accuracy at 10× training sequence length where standard Softmax fails

## Executive Summary
Scalable-Softmax (SSMax) addresses the attention fading problem in Transformers where Softmax output approaches zero as input vector size increases, causing attention distributions to flatten and degrade performance on long-context tasks. SSMax introduces a scaling parameter s·log(n) that multiplies the query vector, maintaining focused attention across varying context sizes. The method achieves 0.008 lower training loss and significantly better long-context generalization compared to standard Softmax.

## Method Summary
SSMax modifies the standard Softmax attention mechanism by introducing a logarithmic scaling factor. Instead of computing Softmax(z), it computes n^(s·zi) / Σn^(s·zj), which is equivalent to multiplying the query vector by s·log(n) before applying standard Softmax. The scaling parameter s is learned per layer and head (144 total parameters for the 12-layer, 12-head configuration used). The implementation requires minimal architectural changes—simply scaling the attention logits by s·log(n)—while maintaining gradient flow through the log(n) term.

## Key Results
- SSMax achieved 0.008 lower training loss compared to standard Softmax during pretraining
- At 20× training sequence length, SSMax maintained significantly lower test loss while standard Softmax performance degraded substantially
- In key information retrieval tests with context sizes up to 10× training length, SSMax achieved successful retrieval while standard Softmax failed completely
- Attention score analysis showed SSMax allocated significantly more attention to key tokens compared to standard Softmax

## Why This Works (Mechanism)

### Mechanism 1
Standard Softmax causes attention distributions to flatten as context size increases, degrading the model's ability to focus on key tokens. In Softmax, the denominator (sum of exponentials) grows with input vector size n, while the numerator (single exponential) remains constant. This causes the maximum output value to approach zero as n increases—termed "attention fading." Attention focusing on key tokens is necessary for effective length generalization and information retrieval.

### Mechanism 2
SSMax preserves focused attention by incorporating a logarithmic scaling factor that counteracts attention fading. SSMax modifies Softmax by introducing n^(s·zi) as the exponential base, equivalent to multiplying the query vector by s·log(n). This ensures the denominator and numerator scale more proportionally with input size. The optimal scaling relationship with input size follows a logarithmic pattern.

### Mechanism 3
SSMax creates a threshold-based attention regime where focus depends on relative value differences rather than absolute positions. Analysis shows that if zmax - z2nd > 1/s, attention approaches the maximum element. If zmax - zmin < 1/s, attention distributes uniformly. The scaling parameter s effectively controls the sensitivity threshold. A fixed threshold (controlled by s) is appropriate across varying context lengths and token importance distributions.

## Foundational Learning

- Concept: Softmax normalization mechanics
  - Why needed here: Understanding why the denominator grows with input size is essential to grasp the attention fading problem and how SSMax's log-scaling counteracts it.
  - Quick check question: Given an input vector of size n with one element at value 1 and all others at 0, what happens to the Softmax output for the maximum element as n→∞?

- Concept: Query-Key attention scoring in Transformers
  - Why needed here: SSMax modifies attention by scaling the query vector; understanding q·K^T computation is necessary to implement and debug the modification correctly.
  - Quick check question: In a standard attention layer, how does changing the query vector magnitude affect the attention distribution?

- Concept: Rotary Position Embedding (RoPE) length extrapolation
  - Why needed here: The paper's long-context evaluation modifies RoPE's θ parameter; understanding this interaction helps interpret results and potential confounds.
  - Quick check question: What happens to position encoding when sequence length exceeds the training length under standard RoPE versus when θ is scaled?

## Architecture Onboarding

- Component map: Attention layer -> Softmax replacement with SSMax -> Query vector scaled by s·log(n) -> Standard Softmax applied

- Critical path:
  1. Identify attention computation location in codebase
  2. Add s as a learnable parameter (initialize appropriately—paper uses reciprocal of average log(n) when retrofitting)
  3. Modify query scaling: q_scaled = q * s * log(current_sequence_position)
  4. Apply standard Softmax to scaled attention logits
  5. Validate gradient flow through the log(n) term

- Design tradeoffs:
  - Train from scratch vs. retrofit: Training with SSMax from initialization yields best length generalization; mid-training switch provides partial benefits; post-training replacement helps least but still improves over baseline
  - With vs. without bias parameter: Bias parameter accelerates training loss reduction but degrades long-context generalization—paper recommends omitting it
  - Learned vs. fixed s: Paper uses learned s per head; fixed s = 1 works but reduces key retrieval performance

- Failure signatures:
  - Short-context degradation when retrofitting: Models with post-training SSMax replacement show higher loss at shorter contexts
  - Attention collapse if s initialized too large: Can cause numerical instability in exponentials
  - No improvement if s converges to near-zero: Model learns to ignore the scaling, reverting to standard Softmax behavior

- First 3 experiments:
  1. Pilot test on small model (≤100M params): Compare training curves of standard Softmax vs. SSMax on a short pretraining run (~1B tokens) to verify loss reduction replicates before committing to larger experiments
  2. Needle retrieval at varying context lengths: After pretraining, evaluate key information retrieval at 1×, 5×, and 10× training sequence length to confirm SSMax maintains retrieval accuracy where baseline fails
  3. Ablation on s initialization: Compare learned s from scratch vs. fixed s values (e.g., 0.1, 0.43, 1.0) to determine sensitivity to initialization and whether learning is necessary

## Open Questions the Paper Calls Out

### Open Question 1
Does SSMax provide the same benefits for larger-scale language models (e.g., 7B+ parameters) as demonstrated for the 162M parameter model? All experiments were conducted using a 162M parameter model with 12 layers and 12 attention heads. The paper claims SSMax "has the potential to replace Softmax in the attention layers of all Transformer-based LLMs," but this generalization is untested.

### Open Question 2
How does SSMax interact with different positional encoding schemes beyond RoPE? The paper exclusively evaluates SSMax with RoPE (Rotary Position Embedding) and modifies RoPE's θ for length generalization tests. Other positional encodings like ALiBi, T5 bias, or no positional encoding are not tested.

### Open Question 3
Can the trade-off between the bias parameter's training efficiency and its degraded long-context performance be resolved? The paper states: "while the inclusion of the bias parameter b slightly accelerates loss reduction during pretraining, omitting b leads to better length generalization performance." Model (d) with bias showed the lowest training loss but degraded long-context and retrieval performance compared to model (b) without bias.

## Limitations

- Logarithmic scaling universality: The optimal scaling relationship across diverse tasks and model scales remains unproven
- Initialization sensitivity: Optimal initialization strategy for s when retrofitting or fine-tuning is not fully characterized
- Scaling parameter generalization: Learned scaling parameters may not generalize well across model sizes or architectures

## Confidence

- High confidence: The core mathematical analysis showing that Softmax attention values approach zero as input size increases is sound and well-supported by the derivation
- Medium confidence: The claim that SSMax's logarithmic scaling is optimal or near-optimal for attention mechanisms is supported by learned parameter analysis but lacks broader validation across different tasks and model scales
- Low confidence: The assertion that SSMax is superior for all attention mechanisms or that it should replace standard Softmax universally is not supported by the evidence

## Next Checks

1. Cross-architecture validation: Implement SSMax in at least two additional architectures (e.g., GPT-style, BERT-style) and train models of different scales (100M, 1B, 8B parameters) on standard benchmarks

2. Downstream task evaluation: Fine-tune SSMax-trained models on a diverse set of downstream tasks (translation, summarization, classification, QA) and compare performance against standard Softmax baselines

3. Scaling parameter analysis: Conduct an ablation study on the learned scaling parameters s across different heads and layers to analyze whether certain heads consistently learn larger or smaller scaling factors, and whether these patterns correlate with attention patterns or task-specific behaviors