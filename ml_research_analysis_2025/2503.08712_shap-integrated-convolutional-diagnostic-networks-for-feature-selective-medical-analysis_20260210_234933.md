---
ver: rpa2
title: SHAP-Integrated Convolutional Diagnostic Networks for Feature-Selective Medical
  Analysis
arxiv_id: '2503.08712'
source_url: https://arxiv.org/abs/2503.08712
tags:
- sicdn
- data
- feature
- pneumonia
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of limited medical datasets
  due to privacy regulations by proposing SHAP-integrated convolutional diagnostic
  networks (SICDN), a feature-selective model using Shapley Additive exPlanations
  for interpretability. SICDN refines convolutional neural networks by integrating
  SHAP values into weight updates, focusing on the fully connected layer for efficiency,
  and includes historical weighted moving averages to enhance feature selection.
---

# SHAP-Integrated Convolutional Diagnostic Networks for Feature-Selective Medical Analysis

## Quick Facts
- **arXiv ID:** 2503.08712
- **Source URL:** https://arxiv.org/abs/2503.08712
- **Authors:** Yan Hu; Ahmad Chaddad
- **Reference count:** 25
- **Primary result:** SICDN achieves over 97% accuracy on limited medical datasets, outperforming DenseNet-121, ResNet-50, Inception-v3, and Inception-ResNet-v2.

## Executive Summary
This paper addresses the challenge of limited medical datasets due to privacy regulations by proposing SHAP-integrated convolutional diagnostic networks (SICDN), a feature-selective model using Shapley Additive exPlanations for interpretability. SICDN refines convolutional neural networks by integrating SHAP values into weight updates, focusing on the fully connected layer for efficiency, and includes historical weighted moving averages to enhance feature selection. Tested on pneumonia and breast cancer datasets, SICDN achieves over 97% accuracy, surpassing four popular CNN models, with particularly strong performance on limited data subsets. The method shows promise for personalized medical diagnostics and can be integrated with federated learning and explainable AI frameworks.

## Method Summary
SICDN combines DenseNet-121 feature extraction with Gradient SHAP-based feature selection at the fully connected layer. The method computes Shapley values for the 1024 features at the FC layer input, normalizes them, and modulates the FC weights through multiplication with the transposed normalized SHAP matrix. An exponentially weighted moving average combines current SHAP-derived importance with historical weights using parameter λ. The model is trained using Adam optimizer with cross-entropy loss, batch size 8, for 100 epochs on RTX 4090. The approach is tested on pneumonia (5856 images) and breast cancer (1693 images) datasets.

## Key Results
- SICDN achieves >97% accuracy on limited pneumonia and breast cancer datasets
- Outperforms DenseNet-121, ResNet-50, Inception-v3, and Inception-ResNet-v2 on subset datasets
- Particularly strong performance on limited data subsets where traditional CNNs struggle
- Shows promise for personalized medical diagnostics with potential integration into federated learning frameworks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Integrating SHAP values into weight updates improves feature selection for limited medical datasets.
- **Mechanism:** Gradient SHAP computes Shapley values for the 1024 features at the fully connected layer input. These values quantify each feature's contribution to class predictions. The absolute values are normalized to (0,1) and transposed to form an importance matrix S*, which multiplies the FC layer weights via W_fl := (S*^T · W_fl), amplifying informative features and suppressing noise.
- **Core assumption:** Shapley values computed on limited batches meaningfully represent global feature importance across the dataset.
- **Evidence anchors:** [abstract] "SICDN refines convolutional neural networks by integrating SHAP values into weight updates, focusing on the fully connected layer for efficiency"; [Section II.A, Eq. 1-4] Details the Gradient SHAP path integration, normalization (Eq. 3), and weight modulation (Eq. 4); [corpus] Weak direct evidence—neighbor papers discuss CNNs for pneumonia but do not validate SHAP-weight integration specifically.
- **Break condition:** If SHAP values exhibit high variance across batches (unstable importance estimates), weight modulation may introduce noise rather than signal.

### Mechanism 2
- **Claim:** Historical weighted moving averages stabilize feature selection and improve performance on small datasets.
- **Mechanism:** An exponentially weighted moving average combines current SHAP-derived importance (λS*^T) with normalized historical weights ((1-λ)W*_fl^T). This preserves previously learned feature relevance while incorporating new SHAP-based signals. λ controls the SHAP contribution proportion.
- **Core assumption:** Historical weights encode meaningful feature relevance that complements instantaneous SHAP estimates.
- **Evidence anchors:** [Section II.C, Eq. 8-9] "We experiment with different λ values to analyze the impact of SHAP method shares on model performance"; [Table III] Shows λ=1.0 achieves best accuracy (97.62%) on subset pneumonia; λ=0.5-0.6 provides competitive results on other datasets; [corpus] No direct corpus validation of historical weighting in SHAP contexts.
- **Break condition:** If λ is misconfigured for dataset characteristics, either excessive forgetting (high λ) or rigidity (low λ) degrades performance.

### Mechanism 3
- **Claim:** Constraining SHAP computation to the fully connected layer balances computational efficiency with meaningful feature selection.
- **Mechanism:** DenseNet-121 extracts features through dense connections with feature reuse. Rather than computing SHAP across all convolutional layers, SICDN computes Shapley values only at the flattened FC input (1024 features for DenseNet-121). For k-class classification, this requires n×k SHAP calculations (1024×2 for binary tasks).
- **Core assumption:** The FC layer input is a sufficient bottleneck for capturing class-relevant feature importance.
- **Evidence anchors:** [Section II.A] "We denote the set of features of the fully connected input layer after flatting as {xi}n_i=1. The number of input features of the fully connected layer of the DenseNet-121 is 1024"; [Section II.A] "focusing on the fully connected layer to simplify memory usage and enhance model efficiency"; [corpus] Neighbor papers (e.g., Deep Learning-Based Pneumonia Detection) use standard CNN approaches without SHAP-focused architectures.
- **Break condition:** If critical discriminative features are encoded in mid-level convolutional activations rather than FC inputs, this bottleneck approach may miss important signals.

## Foundational Learning

- **Concept: Shapley Additive Explanations (SHAP)**
  - Why needed here: Core to understanding how SICDN quantifies feature contributions for weight modulation.
  - Quick check question: Given a model output, can you explain why SHAP values sum to the difference between prediction and baseline?

- **Concept: DenseNet Architecture and Feature Reuse**
  - Why needed here: DenseNet-121 is the backbone; its dense connections enable efficient training with limited data.
  - Quick check question: How does feature concatenation in DenseNet differ from residual addition in ResNet?

- **Concept: Adam Optimizer Dynamics**
  - Why needed here: SICDN modifies standard Adam weight updates by prepending SHAP-based modulation; understanding momentum terms is essential.
  - Quick check question: What do the first-order (v_t) and second-order (m_t) momentum estimates capture in Adam?

## Architecture Onboarding

- **Component map:**
  Input Image → DenseNet-121 (frozen backbone, feature extraction) → Flatten (1024 features) → Gradient SHAP (computes S* n×k matrix per batch) → Normalization (absolute + min-max) → Weight Modulation: W_fl := (λS*^T + (1-λ)W*_fl^T) · W_fl → Adam Update: subtract η·m̂_t/(√v̂_t + ε) → Softmax → Prediction

- **Critical path:** The SHAP computation and weight modulation loop (steps 3-5) runs per batch. Errors in SHAP normalization (Eq. 3 edge case where max=min) directly propagate to weight corruption.

- **Design tradeoffs:**
  - Higher λ → more aggressive SHAP-driven selection, better for very small datasets (λ=1.0 optimal on subset pneumonia)
  - Lower λ → more historical stability, may help with noisier data distributions
  - FC-only SHAP → faster but may miss hierarchical feature interactions in conv layers

- **Failure signatures:**
  - All SHAP values converge to similar magnitudes → Eq. 3 sets all to 1 → no feature selection occurs
  - Training loss oscillates wildly → λ too high for batch size; SHAP estimates too noisy
  - Accuracy plateaus below baseline DenseNet → SHAP modulation may be suppressing informative features

- **First 3 experiments:**
  1. **Ablation on λ:** Train SICDN on subset pneumonia with λ ∈ {0.0, 0.5, 1.0}. Compare accuracy trajectories to identify optimal SHAP/historical balance.
  2. **SHAP stability check:** Compute variance of normalized SHAP values across 10 consecutive batches. High variance (>0.3 std) suggests batch size may need increasing.
  3. **Baseline comparison:** Run standard DenseNet-121 (no SHAP) side-by-side with SICDN on the same data split to validate the paper's reported 2%+ improvement on limited datasets.

## Open Questions the Paper Calls Out

- **Generalizability to other CNN architectures:** Can SICDN be effectively applied to ResNet or Vision Transformers without losing efficiency gains? The paper suggests extending to other CNNs as future work but doesn't validate this.

- **Integration with Federated Learning:** How would SICDN impact communication efficiency and convergence speed in FL environments? While mentioned as potential application, no FL implementation or testing is provided.

- **Performance on large-scale datasets:** Does SICDN's advantage diminish on larger datasets where standard CNNs excel? The paper focuses on limited datasets, and the feature selection mechanism might constrain learning capacity when data is abundant.

- **Computational overhead:** What is the training time overhead from calculating Gradient SHAP values per batch? The paper doesn't report training duration or resource consumption metrics.

## Limitations

- **Dataset preprocessing details unspecified:** Image resize dimensions, normalization values, and augmentation schemes are not provided, creating reproducibility gaps.

- **SHAP implementation specifics unclear:** Adam learning rate and Gradient SHAP parameters (background samples, noise level) are unspecified, which could affect SHAP stability and convergence.

- **Historical weight mechanism unclear:** The initialization and update mechanism for the normalized historical weight matrix beyond the exponential weighting formula requires clarification.

## Confidence

- **High confidence:** DenseNet-121 architecture, Gradient SHAP methodology, accuracy metrics, and the reported >97% performance on tested datasets
- **Medium confidence:** The efficacy of λ=1.0 for limited pneumonia datasets and λ=0.5-0.6 for other cases, as these are empirically determined but not theoretically justified
- **Low confidence:** The two-stage training interpretation for "pre-trained model for feature selection" and the exact handling of SHAP normalization edge cases

## Next Checks

1. **Ablation on λ parameter:** Systematically test λ ∈ {0.0, 0.5, 0.6, 1.0} on subset pneumonia dataset to empirically validate the claimed optimal λ=1.0 for limited data

2. **SHAP stability analysis:** Compute variance of normalized SHAP values across consecutive batches to assess whether batch size of 8 provides stable feature importance estimates

3. **Baseline comparison:** Run standard DenseNet-121 (no SHAP) with identical training configuration to validate the claimed 2%+ improvement on limited datasets