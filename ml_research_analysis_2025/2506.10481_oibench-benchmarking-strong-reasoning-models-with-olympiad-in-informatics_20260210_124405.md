---
ver: rpa2
title: 'OIBench: Benchmarking Strong Reasoning Models with Olympiad in Informatics'
arxiv_id: '2506.10481'
source_url: https://arxiv.org/abs/2506.10481
tags:
- reasoning
- code
- oibench
- language
- problems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OIBench is a challenging, private Olympiad Informatics benchmark
  with 250 original problems designed to evaluate strong reasoning models. It features
  Time/Space Completion Curves for fine-grained efficiency analysis and enables direct
  human-model comparisons through high-level programming competition participant evaluations.
---

# OIBench: Benchmarking Strong Reasoning Models with Olympiad in Informatics

## Quick Facts
- arXiv ID: 2506.10481
- Source URL: https://arxiv.org/abs/2506.10481
- Reference count: 40
- 250 original Olympiad Informatics problems evaluate strong reasoning models across 4 programming languages

## Executive Summary
OIBench is a private benchmark designed to evaluate strong reasoning models on challenging Olympiad-level programming problems. The benchmark features 250 original problems across C++, Python, Java, and JavaScript, with comprehensive test cases, canonical solutions, and Time/Space Completion Curves for efficiency analysis. Through zero-shot evaluation of state-of-the-art models, the benchmark demonstrates that current reasoning models like O4-mini-high can outperform average ACM-level human contestants in both correctness and efficiency, though still lag behind canonical solutions. The resource is fully open-source and shows strong resistance to data contamination with Risk-Scores below 0.01.

## Method Summary
OIBench evaluates large language models on 250 Olympiad-level algorithmic programming problems in zero-shot settings across four programming languages. Models generate code solutions using greedy sampling (temperature=0) or with slight variation for specific models (DeepSeek-R1, Qwen3-32B use temp=0.6). The evaluation uses a Docker-based execution environment that compiles and runs generated code against test cases while measuring runtime and memory usage. Time/Space Completion Curves plot cumulative distributions of efficiency metrics normalized to canonical solutions, while Risk-Scores assess contamination resistance by comparing model outputs to pseudocanonical solutions. Human evaluation involves 67 high-level programming competition participants who rate both correctness and efficiency.

## Key Results
- O4-mini-high achieves AC rate of 24.1%, outperforming average ACM-level human contestants
- Current SOTA reasoning models show Time/Space Completion Curves that lag behind canonical solutions but exceed human efficiency
- Risk-Scores across all evaluated models remain below 0.01, indicating strong contamination resistance
- Models demonstrate ability to generate correct solutions but struggle with optimization compared to hand-crafted solutions

## Why This Works (Mechanism)
The benchmark works by providing a standardized evaluation framework that combines correctness metrics with efficiency analysis through Time/Space Completion Curves. The use of original problems and comprehensive test cases ensures that models cannot rely on memorization, while the contamination resistance methodology using pseudocanonical solutions provides a robust way to verify data integrity. The multilingual support and standardized execution environment allow for fair comparison across different model architectures and programming languages.

## Foundational Learning
- **Olympiad Programming Problems**: Why needed - Provides challenging algorithmic problems that test reasoning and optimization skills; Quick check - Verify problems cover diverse algorithmic paradigms (DP, graphs, data structures)
- **Zero-shot Evaluation**: Why needed - Ensures fair comparison without training bias; Quick check - Confirm all models use identical prompts without examples
- **Time/Space Completion Curves**: Why needed - Quantifies efficiency relative to optimal solutions; Quick check - Plot curves and verify normalization to canonical solutions
- **Risk-Score Contamination Detection**: Why needed - Validates benchmark integrity against training data leakage; Quick check - Generate pseudocanonical solutions and compare with model outputs
- **Docker-based Execution**: Why needed - Provides isolated, reproducible evaluation environment; Quick check - Verify resource limits and test case isolation
- **Multilingual Support**: Why needed - Enables language-specific optimization assessment; Quick check - Confirm compiler versions and language features are consistent

## Architecture Onboarding

**Component Map**: Problem Dataset -> Evaluation Harness -> Docker Container -> Test Execution -> Result Collection -> Analysis

**Critical Path**: Problem generation → Test case creation → Canonical solution development → Model evaluation → Efficiency measurement → Human comparison

**Design Tradeoffs**: The benchmark prioritizes original problem creation over using existing datasets to ensure contamination resistance, but this requires significant curation effort. The choice of zero-shot evaluation ensures fairness but may underestimate model capabilities that could be unlocked through few-shot prompting or fine-tuning.

**Failure Signatures**: Token limit exceeded errors indicate models getting stuck in recursive reasoning loops; compilation failures suggest I/O format mismatches; runtime errors point to algorithmic logic issues; memory limit exceeded errors indicate inefficient solutions.

**First 3 Experiments**:
1. Run single problem evaluation to verify Docker execution and test case handling
2. Generate one Time/Space Completion Curve to validate efficiency measurement pipeline
3. Compute Risk-Score for a single model to verify contamination detection methodology

## Open Questions the Paper Calls Out
None

## Limitations
- Limited human comparison sample size (67 contestants) may not represent broader programming competition population
- Zero-shot evaluation methodology may underestimate model capabilities that could be improved with few-shot prompting
- Test case format and judge harness details are not fully specified, limiting reproducibility

## Confidence
**High Confidence**: Benchmark construction process, basic evaluation methodology, and AC rate measurements are clearly described and reproducible.

**Medium Confidence**: Contamination resistance claims and human-model comparison results are methodologically sound but limited by sample size and incomplete methodological details.

**Low Confidence**: Risk-Score contamination detection methodology and pseudocode generation pipeline lack sufficient detail for independent verification.

## Next Checks
1. Implement the evaluation harness using provided test cases and verify input/output format matches standard programming competition requirements
2. Replicate the contamination detection process by implementing pseudocanonical solution generation and comparing model outputs against both original and transformed solutions
3. Run evaluation pipeline on a larger sample of programming competition participants (minimum 100 contestants across different skill levels) to confirm consistent human-model performance patterns