---
ver: rpa2
title: 'xVLM2Vec: Adapting LVLM-based embedding models to multilinguality using Self-Knowledge
  Distillation'
arxiv_id: '2503.09313'
source_url: https://arxiv.org/abs/2503.09313
tags:
- image
- text
- embedding
- multilingual
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of adapting English-only Large
  Vision-Language Models (LVLMs) for multilingual and multimodal embedding extraction.
  The authors propose xVLM2Vec, a Self-Knowledge Distillation approach that improves
  the quality of multilingual embeddings while preserving English performance.
---

# xVLM2Vec: Adapting LVLM-based embedding models to multilinguality using Self-Knowledge Distillation

## Quick Facts
- arXiv ID: 2503.09313
- Source URL: https://arxiv.org/abs/2503.09313
- Authors: Elio Musacchio; Lucia Siciliani; Pierpaolo Basile; Giovanni Semeraro
- Reference count: 11
- One-line primary result: Self-Knowledge Distillation approach adapts English-only LVLM embeddings to French, German, Italian, and Spanish while preserving English performance

## Executive Summary
xVLM2Vec addresses the challenge of adapting English-only Large Vision-Language Models (LVLMs) for multilingual and multimodal embedding extraction. The method uses Self-Knowledge Distillation to train a student model to align non-English embeddings with English ones from a frozen teacher model, creating language-agnostic vectors. The approach significantly improves multilingual embedding quality while maintaining English performance across five meta-tasks: Image-to-Text, Text-to-Image, VQA, Visual Grounding, and Classification.

The authors introduce MMMEB, the first benchmark for evaluating multilingual and multimodal embedding models, and release all resources including data, models, and code to support future research. Experiments show xVLM2Vec achieves an average P@1 score of 53.32 across all tasks, with substantial improvements on non-English tasks while maintaining comparable English performance.

## Method Summary
xVLM2Vec adapts an English-only VLM2Vec-LoRA model to support multilingual embeddings through Self-Knowledge Distillation. The approach trains a student model to minimize Mean Squared Error between its pooled embeddings and those of a frozen teacher model, using a parallel corpus created via machine translation. The student receives both English and translated non-English inputs, with the loss function explicitly anchoring English embeddings to the teacher while pulling non-English embeddings toward the same semantic space. Context-aware machine translation reduces ambiguity-induced errors by concatenating query and target texts before translation.

## Key Results
- xVLM2Vec achieves P@1 scores of 45.02 (I2T), 48.09 (T2I), 68.00 (VQA), 73.45 (VG), and 32.02 (C)
- Average performance across all tasks reaches 53.32, significantly outperforming the original English-only model on non-English tasks
- English performance is preserved while substantial gains are achieved for French, German, Italian, and Spanish tasks
- The approach demonstrates that SKD can effectively transfer English embedding knowledge to other languages without catastrophic forgetting

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aligning non-English text embeddings to frozen English teacher embeddings enables cross-lingual transfer without catastrophic forgetting.
- **Mechanism:** The student model is trained via Self-Knowledge Distillation to minimize Mean Squared Error between its pooled embeddings and those of a frozen teacher. The dual-term loss function explicitly anchors the student's English embeddings to the teacher's while pulling its non-English embeddings toward the same English semantic space, creating language-agnostic vectors.
- **Core assumption:** The teacher's pre-trained English embeddings represent a high-quality, stable semantic anchor that can be generalized to other languages through translation.
- **Evidence anchors:** [abstract] "trains a student model to align non-English embeddings with English ones from a frozen teacher model"; [Page 3, Section 3] "The idea is to use losse for two purposes: 1) to keep as faithful as possible the English embeddings of S to the ones learned by T; 2) to make non-English embeddings of S as close as possible to the English ones learned by T."
- **Break condition:** Fails if teacher's English embeddings are low-quality, or if the student lacks capacity to map linguistically distant languages into the teacher's space without distorting internal representations.

### Mechanism 2
- **Claim:** Context-aware machine translation reduces ambiguity-induced errors when generating a multimodal parallel corpus from English-only data.
- **Mechanism:** Instead of translating query and target texts in isolation, the authors concatenate them with role labels ("Question:", "Answer:") before feeding them to a text-only machine translation model. The surrounding textual context helps the translator select context-appropriate word senses (e.g., translating "right" as direction rather than correctness).
- **Core assumption:** A text-only translation model can leverage intra-sample context to resolve ambiguities, partially compensating for the lack of visual context.
- **Evidence anchors:** [Page 4, Section 4] "To reduce the possibility of translation errors, we concatenate the query, the positive target text and the negative target text... The intuition is that the translation of the query and target texts together will be more precise, thanks to the additional textual content."
- **Break condition:** Cannot resolve ambiguities that require visual context (e.g., color, object visibility); poorly structured concatenation may confuse the translator.

### Mechanism 3
- **Claim:** Extracting embeddings from the last token of an instruction-tuned LVLM allows task-specific, cross-modal representations.
- **Mechanism:** Unlike dual-encoder models (e.g., CLIP), xVLM2Vec uses a single generative LVLM. Input images and text are tokenized into one sequence; the final token's hidden state serves as the dense embedding. Task-specific instructions (e.g., "Represent the given image for classification") guide the model to specialize the embedding.
- **Core assumption:** The last token's representation summarizes the full cross-modal sequence in accordance with the instruction.
- **Evidence anchors:** [Page 3, Section 3] "The embeddings for the original model are extracted from the last token in the sequence, therefore we do the same during training."
- **Break condition:** Embedding quality degrades sharply with inconsistent input formatting or ambiguous prompts; last-token pooling may be unstable compared to mean pooling.

## Foundational Learning

- **Concept: Knowledge Distillation (KD)**
  - **Why needed here:** Core training method. Understand how a student model learns from a teacher's outputs (soft labels or embeddings) to transfer knowledge efficiently.
  - **Quick check question:** How does training with a teacher model's soft targets differ from training with hard ground-truth labels, and what advantage does it provide?

- **Concept: Large Vision-Language Models (LVLMs)**
  - **Why needed here:** Base architecture. Understand how LVLMs integrate image patches and text tokens into a unified sequence processed by a transformer.
  - **Quick check question:** In an LVLM, how are image and text inputs combined before being processed by the transformer decoder?

- **Concept: Embedding Space Alignment**
  - **Why needed here:** End goal. Understand why aligning vectors for semantically equivalent inputs (across languages/modalities) enables tasks like retrieval and classification.
  - **Quick check question:** If two sentences in different languages have the same meaning, where should their embeddings ideally be positioned relative to each other in vector space?

## Architecture Onboarding

- **Component map:**
  Base Model -> Teacher (Frozen) -> Student (Trainable) -> Parallel Corpus Builder -> Loss Function -> Benchmark (MMMEB)

- **Critical path:**
  1. Data Prep: Filter MMEB training set; apply context-aware translation to build parallel corpus
  2. Model Init: Load two VLM2Vec-LoRA copies; freeze teacher, enable student training
  3. Training: For each (English, Non-English) pair, compute MSE between student's embeddings and teacher's English embeddings; update student via full-parameter FSDP training
  4. Evaluation: Benchmark on MMMEB with standardized task-specific input formats

- **Design tradeoffs:**
  - SKD vs. Contrastive: SKD efficiently reuses teacher knowledge but is capped by teacher quality; contrastive learning may discover richer spaces but demands larger data/compute
  - Machine Translation vs. Human Annotation: Translation is scalable but introduces noise; human data is clean but costly and limited
  - Last-Token vs. Mean Pooling: Last token captures instruction-driven reasoning but is format-sensitive; mean pooling may be more robust but less task-specializable

- **Failure signatures:**
  - Catastrophic forgetting: Non-English gains at English's expense—mitigate by enforcing English self-alignment term in loss
  - Format sensitivity: Significant performance variance with/without punctuation—standardize prompts for training and inference
  - Translation artifacts: Systematic errors on specific words/concepts—audit parallel corpus for mistranslations

- **First 3 experiments:**
  1. Format ablation: Evaluate model on MMMEB with and without trailing punctuation; quantify P@1 variance
  2. Per-language retention check: Compare English vs. each non-English P@1 across tasks to verify alignment effectiveness and English preservation
  3. Baseline comparison: Benchmark xVLM2Vec against multilingual CLIP/SigLIP on I2T, T2I, C tasks to identify relative strengths and gaps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Self-Knowledge Distillation approach maintain robust performance when extended to low-resource languages or non-Latin scripts?
- Basis in paper: [explicit] The authors state in the conclusion, "In future works, we will further study multilingual adaptation and evaluation by extending the training corpus to more languages," as the current work is limited to four high-resource European languages.
- Why unresolved: The current scope is restricted to French, German, Italian, and Spanish, which are structurally similar to English compared to morphologically rich or logographic languages.
- What evidence would resolve it: Replicating the training procedure on a diverse set of non-European languages (e.g., Arabic, Chinese, Swahili) and evaluating the resulting embedding alignment.

### Open Question 2
- Question: Can xVLM2Vec close the performance gap with CLIP/SigLIP on retrieval tasks when specialized compositional datasets are included?
- Basis in paper: [explicit] The authors note that "MMMEB currently lacks specialized datasets, like CIRR... due to the lack of natively multilingual and multimodal datasets," while observing that CLIP/SigLIP variants outperform their model on I2T and T2I tasks.
- Why unresolved: It is unclear if the lower performance is a fundamental limitation of the adaptation method or a limitation of the benchmark's current focus on generic image-text tasks rather than complex compositional reasoning.
- What evidence would resolve it: Evaluating the model on a multilingual version of a compositional retrieval dataset (e.g., CIRR) to test if the instruction-tuning provides an advantage in complex scenarios.

### Open Question 3
- Question: To what extent does noise in the machine-translated parallel corpus limit the ceiling of embedding alignment?
- Basis in paper: [inferred] Section 4 acknowledges that the translation methodology is "still imperfect" and relies on concatenating context to reduce errors, but the impact of residual translation errors on the distillation process remains unquantified.
- Why unresolved: The student model is trained to align with English embeddings from the teacher, but if the parallel non-English text is a poor translation, the student may learn suboptimal alignments.
- What evidence would resolve it: A comparative analysis of model performance when trained on the synthetic MADLAD-400 data versus a smaller, human-verified parallel corpus.

## Limitations
- The approach is limited to four high-resource European languages (French, German, Italian, Spanish) that are structurally similar to English
- Machine translation introduces semantic drift and ambiguity errors that cannot be fully resolved without visual context
- VLM2Vec embeddings are highly sensitive to input formatting, requiring careful prompt engineering for practical deployment

## Confidence

**High Confidence:**
- The Self-Knowledge Distillation mechanism effectively transfers English embedding knowledge to non-English languages while preserving English performance
- The parallel corpus creation pipeline is technically sound and reproducible
- The MMMEB benchmark provides a comprehensive evaluation framework for multilingual multimodal embeddings

**Medium Confidence:**
- The magnitude of performance improvements across all tasks and languages
- The generalizability of results beyond the four evaluated languages (FR, DE, IT, ES)
- The long-term stability of embeddings after training convergence

**Low Confidence:**
- The impact of machine translation errors on downstream task performance
- The absolute quality of xVLM2Vec embeddings compared to potential future approaches using human-annotated multilingual data
- The scalability of the approach to languages with significantly different linguistic structures from the training set

## Next Checks

1. **Translation Quality Audit**: Manually sample and evaluate 100 translated instances across all four languages to quantify semantic drift and identify systematic translation errors that could bias downstream performance.

2. **Format Robustness Testing**: Systematically vary input formatting (punctuation, capitalization, whitespace) across all MMMEB tasks to quantify the performance sensitivity and establish practical formatting guidelines.

3. **Teacher Model Ablation**: Train xVLM2Vec using teachers with varying English embedding quality (e.g., smaller LVLMs, different pre-training objectives) to determine the minimum teacher quality threshold for effective multilingual adaptation.