---
ver: rpa2
title: 'LOTFormer: Doubly-Stochastic Linear Attention via Low-Rank Optimal Transport'
arxiv_id: '2509.23436'
source_url: https://arxiv.org/abs/2509.23436
tags:
- attention
- lotformer
- linear
- transport
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LOTFormer introduces a linear-time doubly stochastic attention\
  \ mechanism by factorizing the attention matrix through a low-rank optimal transport\
  \ plan conditioned on a learnable pivot measure. Instead of computing a full n\xD7\
  n transport plan, it solves two entropic OT problems between queries\u2192pivot\
  \ and pivot\u2192keys, then composes them into a glued coupling that is provably\
  \ doubly stochastic and has rank at most r\u226An."
---

# LOTFormer: Doubly-Stochastic Linear Attention via Low-Rank Optimal Transport

## Quick Facts
- arXiv ID: 2509.23436
- Source URL: https://arxiv.org/abs/2509.23436
- Reference count: 32
- Primary result: LOTFormer matches or exceeds strong linear attention and doubly stochastic baselines in both accuracy and efficiency across ImageNet-1K and Long Range Arena benchmarks.

## Executive Summary
LOTFormer introduces a linear-time doubly stochastic attention mechanism by factorizing the attention matrix through a low-rank optimal transport plan conditioned on a learnable pivot measure. Instead of computing a full n×n transport plan, it solves two entropic OT problems between queries→pivot and pivot→keys, then composes them into a glued coupling that is provably doubly stochastic and has rank at most r≪n. This allows applying attention to values in O(nr) time without forming the full matrix. The pivot locations and masses are learned end-to-end. Across ImageNet-1K and Long Range Arena benchmarks, LOTFormer matches or exceeds strong linear attention and doubly stochastic baselines in both accuracy and efficiency, and enables direct plug-and-play replacement of softmax attention in pretrained checkpoints.

## Method Summary
LOTFormer factorizes attention computation through a learnable pivot measure. It solves two entropic optimal transport problems: one from queries to pivot and another from pivot to keys. These are composed into a glued coupling that is provably doubly stochastic and has rank at most r≪n, where r is the number of pivot points. This allows applying attention to values in O(nr) time without forming the full n×n matrix. The pivot locations and masses are learned end-to-end, and the method preserves global pooling capability of special tokens like [CLS] through a decoupled softmax aggregator while applying doubly stochastic constraints to the rest.

## Key Results
- LOTFormer achieves 80.6% Top-1 accuracy on ImageNet-1K with DeiT-Tiny, matching the softmax baseline while using linear O(nr) complexity.
- On Long Range Arena benchmarks, LOTFormer consistently outperforms linear attention methods (Performer, Softmax) while matching the accuracy of quadratic softmax attention.
- The method enables direct plug-and-play replacement of softmax attention in pretrained checkpoints, demonstrated by GLUE fine-tuning results after only 5 epochs of distillation.

## Why This Works (Mechanism)

### Mechanism 1
Reduces attention complexity from quadratic $O(n^2)$ to linear $O(nr)$ by avoiding the full attention matrix. Introduces a learnable **pivot measure** with small support $r \ll n$. Instead of computing a coupling between all $n$ queries and $n$ keys directly, it computes two smaller entropic optimal transport plans: queries $\to$ pivot ($n \times r$) and pivot $\to$ keys ($r \times n$). These are composed into a low-rank glued coupling. The core assumption is that the attention matrix can be effectively approximated by a low-rank factorization mediated by $r$ pivot points without losing critical token interactions.

### Mechanism 2
Enforces doubly stochastic constraints (row/col sums = 1) to balance token participation and mitigate "over-focusing." Uses the **glued coupling** formula $\Gamma = (\Gamma^{(1)})^\top \text{Diag}(\sigma)^{-1} \Gamma^{(2)}$. By composing two transport plans that respect the pivot mass $\sigma$, the resulting matrix $A$ strictly satisfies $A\mathbf{1} = \mathbf{1}$ and $\mathbf{1}^\top A = \mathbf{1}^\top$. The core assumption is that enforcing strict marginals (mass conservation) across both rows and columns leads to better information flow and robustness compared to standard row-softmax normalization.

### Mechanism 3
Preserves global pooling capability of special tokens (e.g., [CLS]) while applying doubly stochastic constraints to the rest. Decouples the [CLS] token from the doubly stochastic block. The [CLS] row uses standard softmax aggregation over all keys, while other rows use the LOTFormer mechanism. Optionally adds [CLS]-only polarization. The core assumption is that a global token needs unrestricted aggregation (softmax) to gather information, while token-to-token interactions benefit from the balanced transport constraints.

## Foundational Learning

- **Concept:** **Entropic Optimal Transport (Sinkhorn Algorithm)**
  - **Why needed here:** LOTFormer relies on solving entropy-regularized transport problems to generate the coupling matrices. You must understand how the Sinkhorn algorithm iteratively normalizes rows and columns to achieve doubly stochastic matrices.
  - **Quick check question:** Can you explain why adding an entropy term ($-\epsilon H(\Gamma)$) makes the optimal transport solution faster to compute but "blurrier"?

- **Concept:** **Doubly Stochastic Matrices**
  - **Why needed here:** This is the structural constraint defining the paper. Unlike standard attention (row-stochastic only), these matrices preserve total mass across both inputs and outputs (Birkhoff-von Neumann theorem context).
  - **Quick check question:** If a matrix is row-stochastic, what additional constraint makes it doubly stochastic, and what does that imply about the "outgoing attention" from keys?

- **Concept:** **Low-Rank Approximation**
  - **Why needed here:** The efficiency gain comes strictly from the rank-$r$ factorization. Understanding the trade-off between rank $r$ and approximation error is vital for tuning the pivot size.
  - **Quick check question:** Why does factorizing an $n \times n$ matrix into an $n \times r$ and $r \times n$ product change the complexity class from quadratic to linear?

## Architecture Onboarding

- **Component map:** Queries $Q$ → Cost Computation → Sinkhorn Solver → $\Gamma^{(1)}$; Keys $K$ → Cost Computation → Sinkhorn Solver → $\Gamma^{(2)}$; $\Gamma^{(1)}$, $\Gamma^{(2)}$, pivot masses $\sigma$ → Glued Coupling → Attention Matrix $A$; $A$ and Values $V$ → Output $Y$

- **Critical path:**
  1. Initialization of pivot $Z$ and $\sigma$ (Section 4.5 suggests learnable uniform initialization).
  2. Computation of kernel costs $QZ^\top$ and $ZK^\top$.
  3. Sinkhorn iterations ($T=5$ default) to derive transport plans.
  4. Value aggregation via matrix multiplication with $V$ (avoiding forming full $A$).

- **Design tradeoffs:**
  - **Pivot size ($r$):** Controls the accuracy/speed balance. Table 7 shows $r=32$ is optimal for DeiT-Tiny; $r=64$ slows down training significantly with diminishing returns.
  - **Entropic regularization ($\epsilon$):** High $\epsilon$ smooths the transport (diffuse attention); low $\epsilon$ makes it sharp/sparse but harder to optimize. Section 4.5 suggests $\epsilon=1$ as a sweet spot.
  - **[CLS] Handling:** Standard DS attention hurts [CLS]; must implement the decoupled Softmax/DS logic.

- **Failure signatures:**
  - **Causal Masking:** Do not use for autoregressive generation. Appendix F proves it collapses to identity (attends only to self).
  - **Memory Spikes:** If implementation accidentally forms the full $n \times n$ "glued" matrix $A$ explicitly rather than keeping it factorized, you lose the $O(nr)$ memory benefit.
  - **[CLS] Collapse:** If [CLS] performance drops, check if the dedicated softmax aggregator (Eq 5) is disabled.

- **First 3 experiments:**
  1. **Pivot Size Sweep:** Run ablations on $r \in \{4, 8, 16, 32, 64\}$ (Table 7 style) on a validation set to find the optimal efficiency/accuracy point for your specific data length $n$.
  2. **[CLS] Ablation:** Verify the "Plug-and-Play" hypothesis by comparing (a) Full DS attention, (b) [CLS]-Softmax only, and (c) [CLS]-Softmax + Polarization on a classification task.
  3. **Long-Context Stress Test:** Benchmark runtime and memory against Softmax and Performer on increasing sequence lengths ($N \in \{2^9 \dots 2^{17}\}$ as in Figure 3) to verify the linear scaling claim on your hardware.

## Open Questions the Paper Calls Out

### Open Question 1
Can doubly-stochastic attention be extended to autoregressive/causal settings without collapsing to the identity matrix? Appendix F states: "DS attention under standard causal masking collapses to the identity... Extending DS attention to autoregressive settings would require modifying the constraints or the mask structure, which we leave to future work." The mathematical proof shows DS constraints with lower-triangular causal masking force A = I, making nontrivial autoregressive self-attention impossible under current formulation. A modified constraint set or mask structure that preserves both causality and meaningful attention distributions would resolve this.

### Open Question 2
Can fused kernel implementations close the constant-factor efficiency gap between LOTFormer and optimized baselines like FlashAttention? Appendix E states: "Developing a fused implementation that applies FlashAttention style tiling, fusion, and recomputation to the rectangular OT couplings and value mixing is a natural direction for future work and is outside the scope of this work." Current unfused PyTorch implementation shows 23-40% memory overhead versus FlashAttention v3 despite algorithmic O(nr) complexity. A custom Triton/CUDA kernel implementing tiling and recomputation for the rectangular OT couplings would resolve this.

### Open Question 3
Does optimizing the pivot measure for transport cost (as in Low-Rank OT) improve attention quality compared to end-to-end learned pivots? Section 3 explicitly distinguishes LOTFormer from optimal Low-Rank OT: "the pivot measure is learned end-to-end... but not optimized explicitly for transport cost minimization." The paper cites computational reasons but leaves open whether alternative approximations exist. Low-Rank OT requires the full n×n cost matrix, but the trade-off between transport optimality and computational efficiency remains unexplored for approximate formulations. A comparison of learned vs. transport-cost-optimized pivots under relaxed computational budgets would resolve this.

### Open Question 4
Is the [CLS]-softmax workaround necessary, or can a unified DS formulation preserve global aggregation without special-case handling? Section 4.2 notes: "a DS attention matrix may approach a permutation, so each query attends to a single key, and [CLS] would aggregate from only one token" and uses softmax for [CLS] as a practical fix. The tension between DS normalization and global pooling remains conceptually unresolved. The empirical fix works, but whether DS constraints fundamentally conflict with global token roles is unclear. Analysis of learned pivot distributions when [CLS] is included in DS would resolve this.

## Limitations
- The [CLS]-softmax workaround suggests fundamental tension between doubly stochastic constraints and global token aggregation roles.
- Lack of comparison against Performer and other linear attention methods on extreme sequence lengths (>4K tokens) leaves relative scalability claims partially unverified.
- The optimal pivot size (r=32) may be dataset-dependent without broader validation across different sequence lengths and modalities.

## Confidence
- **High Confidence:** The core mechanism of low-rank optimal transport with learnable pivots is mathematically sound and the O(nr) complexity claim is correct given the factorization. The ImageNet-1K results (80.6% Top-1 with DeiT-Tiny) are reproducible and directly comparable to the stated baseline.
- **Medium Confidence:** The Long Range Arena benchmark results are compelling, but the lack of comparison against Performer or other linear attention methods on the same tasks introduces some uncertainty about relative performance. The [CLS] handling improvement (+5.0% from the softmax split) is significant but relies on a specific engineering detail not widely validated.
- **Low Confidence:** The scalability claims for extreme sequence lengths (>10K tokens) are not empirically validated against competitors. The GLUE fine-tuning results (5 epochs) are too brief to assess true plug-and-play capability, and the optimal pivot size (r=32) may be dataset-dependent without broader validation.

## Next Checks
1. **Long-Context Scalability Test:** Benchmark LOTFormer against Performer and Softmax attention on LRA tasks with sequence lengths from 1K to 16K tokens, measuring both wall-clock time and peak memory. This directly validates the primary efficiency claim.
2. **Pivot Initialization Ablation:** Systematically compare different pivot initialization schemes (uniform, Gaussian, learned from data) and their impact on convergence speed and final accuracy across multiple datasets. This addresses the vague initialization specification.
3. **[CLS] Polarization Sweep:** Perform a grid search over the [CLS]-only polarization parameter β on ImageNet-1K to determine its optimal value and characterize its sensitivity. This validates the practical utility of this engineering detail.