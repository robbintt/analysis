---
ver: rpa2
title: On the Identifiability of Causal Abstractions
arxiv_id: '2503.10834'
source_url: https://arxiv.org/abs/2503.10834
tags:
- causal
- latent
- which
- graph
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the identifiability of latent causal models
  when interventions are performed on subsets of variables rather than individually.
  The authors establish that under assumptions of faithfulness, absolute continuity
  of latent distributions, and smooth mixing functions, causal models can be identified
  up to abstractions determined by intervention targets' non-descendant sets.
---

# On the Identifiability of Causal Abstractions

## Quick Facts
- **arXiv ID:** 2503.10834
- **Source URL:** https://arxiv.org/abs/2503.10834
- **Reference count:** 28
- **Primary result:** Establishes identifiability of latent causal models up to quotient graph abstractions under interventions on subsets of variables rather than individually

## Executive Summary
This paper studies the identifiability of latent causal models when interventions are performed on subsets of variables rather than individually. The authors establish that under assumptions of faithfulness, absolute continuity of latent distributions, and smooth mixing functions, causal models can be identified up to abstractions determined by intervention targets' non-descendant sets. They show that the identifiable quotient graph is G⋆/P(σ(nd(I⋆))), where G⋆ is the true causal graph and I⋆ is the family of intervention targets. Additionally, they prove that certain latent variables can be disentangled if their intervention targets have unique non-descendant sets and the variables are scalar. The work extends previous results that required atomic interventions on every variable, providing a more realistic framework for causal representation learning with limited intervention capabilities.

## Method Summary
The paper establishes theoretical identifiability results for latent causal models under interventions on arbitrary subsets of variables. The method involves generating counterfactual pairs (x, x̃) where x̃ results from unknown perfect interventions on latent subsets. By analyzing which latent blocks remain invariant across these pairs, the framework identifies the family of non-descendant sets and constructs a quotient graph abstraction. A toy experiment with linear Gaussian SCMs demonstrates the approach, optimizing model parameters via gradient descent to maximize likelihood of observed pairs. The optimization recovers a block-diagonal structure in the mixing function that corresponds to the theoretical partition of indistinguishable variables.

## Key Results
- Causal models are identifiable up to a quotient graph structure determined by the σ-algebra generated from intervention targets' non-descendant sets
- Specific latent variables can be disentangled when intervention targets have unique non-descendant sets and the variables are scalar
- The identifiability results hold for interventions on arbitrary subsets of variables, extending previous work requiring atomic interventions on every variable

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Counterfactual data pairs reveal invariant latent subsets based on the non-descendants of unknown intervention targets.
- **Mechanism:** When a latent subset S is intervened upon, its non-descendants N (nodes not causally downstream) remain statistically invariant (zN ⊥ z̃N). By observing which latent blocks remain unchanged across a distribution of random interventions, the system identifies the family of non-descendant sets nd(I). This partitions the latent space based on which variables "move together" versus which stay fixed.
- **Core assumption:** The mixing function g is a diffeomorphism, and interventions are perfect (severing causal parents).
- **Evidence anchors:**
  - [abstract]: "intervention targets have unique non-descendant sets"
  - [page 7]: "P(zN ≠ z̃N | ι = S) = 0... P(zT ≠ z̃T | ι = S) > 0 ∀ T ⊈ N"
  - [corpus]: General CRL literature supports the use of invariance for disentanglement, though specific non-descendant partitioning is novel to this text.
- **Break condition:** If all interventions affect the entire graph (no non-descendants), or if the mixing function is not invertible, this invariant signal is lost.

### Mechanism 2
- **Claim:** The identifiable causal structure is a "quotient graph" determined by the σ-algebra generated by non-descendant sets.
- **Mechanism:** The system calculates the partition P of the graph nodes generated by the σ-algebra σ(nd(I)). Nodes are grouped into blocks if they cannot be distinguished by the available intervention invariances. The resulting causal structure is the quotient graph G/P, which aggregates indistinguishable micro-variables into macro-nodes while preserving the acyclic orientation of edges between blocks.
- **Core assumption:** The ground truth graph G is a Directed Acyclic Graph (DAG) and is faithful to the distribution.
- **Evidence anchors:**
  - [abstract]: "identified up to a quotient graph structure"
  - [page 7]: "Theorem 3.1... identifiable up to a SCM abstraction θ with causal graph G = G⋆ / P(σ(nd(I⋆)))"
  - [corpus]: N/A (Specific theoretical contribution of this paper).
- **Break condition:** If the intervention targets are insufficient to generate a non-trivial partition (e.g., only one target), the quotient graph collapses to a single node or retains the full complexity without identifiability.

### Mechanism 3
- **Claim:** Specific single latent variables can be disentangled if they are the unique intersection of intervention targets sharing the same non-descendant set.
- **Mechanism:** For a specific non-descendant set N, the system identifies the intersection π(N) of all intervention targets S that share N. If this intersection π(N) happens to be a single node {i}, that specific latent variable zi can be identified (disentangled) up to element-wise transformation, even if the general graph is only known up to abstraction.
- **Core assumption:** The specific latent variable Zi is 1-dimensional (Z⋆i ≅ ℝ).
- **Evidence anchors:**
  - [abstract]: "additional latent variables can be disentangled when intervention targets have unique non-descendant sets"
  - [page 7]: "Theorem 3.2... provided that π(N) is a singleton set {i}... we can identify the latent z⋆π(N) up to disentanglement"
  - [corpus]: N/A.
- **Break condition:** If the intersection π(N) contains multiple nodes, they remain entangled as a block.

## Foundational Learning

- **Concept: Structural Causal Models (SCMs) & Perfect Interventions**
  - **Why needed here:** The entire theoretical framework relies on defining "pre-intervention" variables generated by DAG-structured mechanisms zi = fi(zpa(i), εi) and "post-intervention" variables where specific mechanisms are severed (z̃i = f̃(ε̃i)). Without understanding this causal cut, the invariance mechanism makes no sense.
  - **Quick check question:** Can you explain why intervening on a node S severs the edge from its parents, making S independent of its non-descendants in the counterfactual scenario?

- **Concept: Counterfactual vs. Interventional Data**
  - **Why needed here:** The paper explicitly contrasts its "counterfactual" setting (access to joint pairs (x, x̃) where the intervention type ι is hidden) against standard interventional settings (access to marginals x(e) with known environment e). This distinction is critical for understanding why the method uses "contrastive" pairs rather than separate datasets.
  - **Quick check question:** In this paper's setting, do we know *which* variables were intervened upon for a given pair (x, x̃)? (Answer: No, ι is hidden).

- **Concept: Identifiability vs. Learning**
  - **Why needed here:** This paper proves theoretical identifiability (uniqueness of the solution) but does not provide a scalable deep learning architecture to find it. Understanding this separates the theoretical guarantee (what is possible) from the optimization challenge (how to do it).
  - **Quick check question:** Does this paper provide a loss function for training a neural network to find the quotient graph? (Answer: No, it establishes conditions under which the likelihood theoretically identifies the graph; Appendix D shows a proof-of-concept but notes "Learning remains an important problem").

## Architecture Onboarding

- **Component map:** Data Collection -> Latent Estimator -> Invariance Analyzer -> Abstraction Engine -> Graph Construction
- **Critical path:**
  1. **Data Collection:** Gather contrastive pairs (x, x̃) where random subsets of latent factors might have changed
  2. **Invariance Detection:** Statistically determine which latent blocks remain invariant (zN ⊥ z̃N) to define the family nd(I)
  3. **Partition Calculation:** Compute the σ-algebra σ(nd(I)) and the resulting partition P of the latent space
  4. **Graph Construction:** Output the quotient graph G/P as the identifiable abstraction

- **Design tradeoffs:**
  - **Intervention Diversity vs. Graph Granularity:** The granularity of the recovered quotient graph depends entirely on the diversity of the *unknown* intervention targets. If interventions are always broad, the recovered graph will be coarse (highly abstracted)
  - **Theory vs. Practice:** The paper assumes infinite data and perfect diffeomorphisms. A practical implementation must handle noise and approximate invariance, which might introduce errors in the partition boundaries

- **Failure signatures:**
  - **Total Entanglement:** If the recovered graph is a single node, the intervention targets likely have empty or trivial non-descendant sets, meaning interventions are affecting everything or the system cannot distinguish causes from effects
  - **Cyclic Quotient:** If the quotient graph appears cyclic (which the theorem forbids), the "faithfulness" assumption is violated, or the partition calculation is incorrect

- **First 3 experiments:**
  1. **Toy Synthetic Validation:** Replicate the linear Gaussian experiment in Appendix D. Check if the encoder matrix QTQ⋆ is block-diagonal with block structures matching the theoretical partition P(σ(nd(I⋆)))
  2. **Intervention Sensitivity Analysis:** Vary the intervention family I⋆ (e.g., single-node vs. block interventions). Plot the change in the size/coarseness of the identifiable quotient graph to verify Theorem 3.1
  3. **Singleton Disentanglement Test:** Design an I⋆ where a specific variable i is the unique intersection π(N) for some N. Verify if this variable can be recovered more precisely (up to element-wise diffeomorphism) compared to variables in larger blocks

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can scalable algorithms be developed to practically learn the identifiable abstract causal models from data?
- **Basis in paper:** [explicit] The authors state in Section 4 that while they establish theoretical identifiability, they leave the problem of a "scalable method for identification of the abstract causal model... for future research."
- **Why unresolved:** The paper provides a theoretical framework and a proof-of-concept on synthetic linear data, but does not offer a general-purpose learning algorithm capable of handling high-dimensional or complex non-linear real-world data.
- **What evidence would resolve it:** A proposed optimization framework or algorithm that recovers the quotient graph and latent variables in polynomial time on non-linear datasets with performance metrics comparable to existing causal representation learning methods.

### Open Question 2
- **Question:** Do the identifiability results for causal abstractions hold under soft or imperfect interventions?
- **Basis in paper:** [inferred] Section 2.1 explicitly assumes "perfect intervention" where the causal mechanism is "completely severed," which is noted as a restrictive assumption in many real-world systems where interventions might only modify parameters.
- **Why unresolved:** The theoretical proofs rely on the independence properties generated by the severing of causal edges. It is unclear if the quotient graph structure remains identifiable if the intervention only perturbs the mechanism rather than replacing it entirely.
- **What evidence would resolve it:** A theoretical proof extending Theorem 3.1 to contexts where interventions are parametric changes (soft interventions), or an empirical demonstration that the current framework fails under such conditions.

### Open Question 3
- **Question:** Can the identifiability of causal abstractions be achieved in the interventional setting where environment labels are observed, rather than the counterfactual pair setting?
- **Basis in paper:** [inferred] Section 2.1 explicitly contrasts the paper's counterfactual setting (access to joint p(x, x̃)) with the interventional setting (access to marginals indexed by observable environment e), noting the distinct assumptions required for each.
- **Why unresolved:** The proposed method relies on separating mixture components of p(x, x̃) based on invariant non-descendant blocks. It is unstated whether these invariants can be recovered using only separate interventional distributions without the paired counterfactual structure.
- **What evidence would resolve it:** A derivation showing that the quotient graph can be recovered from a set of marginal distributions {p(e)(x)}e indexed by known environments, removing the requirement for paired samples from the same unit.

## Limitations
- The theoretical framework assumes perfect interventions and infinite data, but practical implementations must handle approximate invariance and finite samples
- No scalable algorithm is provided for learning the quotient graph from real-world data
- The identifiability results depend critically on the intervention targets having unique non-descendant sets, which may not hold in many practical scenarios

## Confidence
- **High Confidence:** The quotient graph identifiability result (Theorem 3.1) - well-established in causal abstraction literature
- **Medium Confidence:** The singleton disentanglement result (Theorem 3.2) - requires very specific intervention patterns that may be hard to achieve
- **Low Confidence:** Practical scalability of the approach - no empirical validation beyond a simple synthetic example

## Next Checks
1. **Empirical Scaling Test:** Implement the optimization procedure on larger synthetic graphs (n > 10) to assess computational tractability and sensitivity to noise
2. **Assumption Violation Analysis:** Systematically relax faithfulness and absolute continuity assumptions to quantify their impact on recovery accuracy
3. **Real-World Feasibility Study:** Design a semi-synthetic benchmark where ground truth causal structure is known, but observations are generated through realistic mixing functions and noisy interventions