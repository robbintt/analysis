---
ver: rpa2
title: 'PhysProver: Advancing Automatic Theorem Proving for Physics'
arxiv_id: '2601.15737'
source_url: https://arxiv.org/abs/2601.15737
tags:
- physics
- formal
- training
- theorem
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PhysProver, the first approach to enhance
  formal theorem proving in the physics domain. The authors construct PhysLeanData,
  a dataset combining theorems from the PhysLean repository with synthetically generated
  conjectures using Claude-4.5-Sonnet.
---

# PhysProver: Advancing Automatic Theorem Proving for Physics

## Quick Facts
- arXiv ID: 2601.15737
- Source URL: https://arxiv.org/abs/2601.15737
- Reference count: 22
- Primary result: 2.4% improvement on physics theorem proving with only ~5K training samples

## Executive Summary
PhysProver introduces the first approach to enhance formal theorem proving specifically for physics domains. By constructing PhysLeanData—a dataset combining theorems from the PhysLean repository with synthetically generated conjectures—and employing Reinforcement Learning with Verifiable Rewards (RLVR) on DeepSeek-Prover-V2-7B, the authors achieve a 2.4% improvement across physics sub-domains. The model demonstrates effective generalization to formal mathematics, showing a 1.3% gain on MiniF2F-Test, and outperforms both open-source and proprietary provers on physics tasks.

## Method Summary
The method constructs PhysLeanData by combining theorems from the PhysLean repository with synthetically generated conjectures using Claude-4.5-Sonnet, filtered through Lean syntax and provability checks. DeepSeek-Prover-V2-7B is then trained using GRPO with RLVR, leveraging the Lean 4 compiler as a verifiable reward signal. The approach uses only ~5K training samples and employs curriculum learning by proof length. The authors specifically avoid supervised fine-tuning due to performance degradation observed when using human-written proofs.

## Key Results
- 2.4% improvement across physics sub-domains using only ~5K training samples
- 1.3% gain on MiniF2F-Test benchmark, demonstrating generalization to formal mathematics
- Outperforms both open-source and proprietary provers on physics tasks
- Shows better in-context learning ability by effectively utilizing physics-specific library headers

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Verifiable binary rewards from Lean 4 reduce hallucination by enforcing syntactic and logical validity
- **Mechanism**: GRPO uses the Lean compiler as a verifiable reward signal (1 for valid proofs, 0 otherwise), creating hard constraints that penalize generation of non-existent lemmas or syntax errors
- **Core assumption**: Base model possesses sufficient latent physics and logic knowledge but lacks alignment to PhysLean library syntax
- **Evidence anchors**: Abstract mentions RLVR improvement; section 3.3 explains binary rewards eliminate hallucination; section 5.1 shows base model hallucinates non-existent lemmas
- **Break condition**: If valid proof space is too sparse relative to initial policy, reward signal may remain zero

### Mechanism 2
- **Claim**: Physics-specific training enhances in-context learning capabilities for utilizing library headers and definitions
- **Mechanism**: Training on PhysLeanData makes the model familiar with physics library "API," enabling better interpretation and utilization of provided header context compared to general math provers
- **Core assumption**: Physics formalizations rely on distinct primitive definitions differing significantly from general mathematical corpora
- **Evidence anchors**: Abstract states PhysProver outperforms other provers; section 5.1 shows better in-context learning with lemmas; LeanConjecturer supports LLM-based context extraction
- **Break condition**: If evaluation tasks require reasoning contradicting physical axioms or context window is overloaded

### Mechanism 3
- **Claim**: RL succeeds where SFT fails because RL explores the model's In-Distribution solution space versus SFT forcing Out-of-Distribution human proof mimicry
- **Mechanism**: SFT on human proofs forces reasoning paths the model hasn't internalized, increasing uncertainty; RL allows the model to generate proofs using its own patterns and reinforces verifiable ones
- **Core assumption**: Base model is already capable of solving a subset of problems, allowing self-exploration to yield correct proofs
- **Evidence anchors**: Section 6 explains OOD nature of human proofs as SFT failure mode; Table 4 shows lower perplexity for GRPO/RAFT versus SFT
- **Break condition**: If initial generation distribution is too weak to produce any correct proofs

## Foundational Learning

- **Concept**: **Lean 4 & Formal Verification**
  - **Why needed here**: The methodology relies on Lean compiler as oracle; understanding that "compiling" proofs is deterministic logical consistency check, not probabilistic generation
  - **Quick check question**: Can you explain the difference between a "tactic proof" and a "term proof" in Lean, and why a compiler would reject a hallucinated lemma name?

- **Concept**: **Group Relative Policy Optimization (GRPO)**
  - **Why needed here**: This specific RL algorithm compares rewards within groups of samples for the same prompt rather than against global value function
  - **Quick check question**: How does GRPO calculate advantage $\hat{A}_{i,t}$ for a sample relative to its group, and why does this eliminate need for critic model?

- **Concept**: **In-Distribution (ID) vs. Out-of-Distribution (OOD) Data**
  - **Why needed here**: Paper identifies OOD nature of human proofs as SFT failure mode; understanding distribution shift explains why self-generated proofs work better
  - **Quick check question**: Why might a model fine-tuned on expert human proofs perform worse than one fine-tuned on its own correct outputs?

## Architecture Onboarding

- **Component map**: PhysLean Repo + Claude-4.5-Sonnet (Conjecture Gen) → Filter (Syntax + Prover Verification) → PhysLeanData → DeepSeek-Prover-V2-7B + Lean 4.20.0 (Reward Verifier) → GRPO Loop
- **Critical path**: Integration of Lean verifier into RL loop; verifier must return binary reward (0/1) efficiently for batch updates
- **Design tradeoffs**: SFT vs. RL (rejected SFT due to degradation); Data Scale (used only ~5K samples favoring quality over quantity); Curriculum Learning (sorting by proof length adds complexity but stabilizes training)
- **Failure signatures**: Hallucination (generation of `apply?` or `sorry` or fake lemmas); Reward Hacking (generating proofs with `sorry` or `admit` to trick verifier); Perplexity Spike (if SFT is accidentally applied)
- **First 3 experiments**: 1) Reward Integrity Test (verify Lean reward wrapper rejects proofs with `sorry`/`admit`); 2) SFT vs. RAFT Baseline (replicate Table 3 finding on small subset); 3) Generalization Check (evaluate on MiniF2F-Test to ensure physics training doesn't regress mathematical reasoning)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can scaling conjecture generation pipeline improve yield rates and prover performance beyond current synthetic data limitations?
- **Basis**: Section 8 states authors couldn't scale generation due to resource constraints and 8.9% yield rate
- **Why unresolved**: Current pipeline filters vast majority of synthetic conjectures; efficiency gains could unlock more training data
- **What evidence would resolve it**: Experiments increasing compute budget for generation and measuring correlation between dataset size and accuracy

### Open Question 2
- **Question**: Why does RL fail to improve performance after Rejection-Sampling Fine-tuning (RAFT)?
- **Basis**: Appendix C shows RL training after RAFT results in fluctuating accuracy with no clear improvement trend
- **Why unresolved**: Paper observes negative result but doesn't isolate cause, suggesting potential prompt set conflicts
- **What evidence would resolve it**: Ablation studies using different prompt sets for RAFT and RL, or analysis of policy divergence during transition

### Open Question 3
- **Question**: Does training on formal physics data transfer effectively to physics domains not covered in PhysLean?
- **Basis**: Section 8 notes dataset derived solely from PhysLean, potentially limiting applicability to specialized domains like electromagnetism if underrepresented
- **Why unresolved**: Model's generality constrained by specific distribution of single repository used for training
- **What evidence would resolve it**: Evaluation on formal physics benchmarks from repositories other than PhysLean

### Open Question 4
- **Question**: Why does physics-based training improve performance on MATH algebra problems but degrade performance on AIME competition problems?
- **Basis**: Table 2 shows +2.9% gain in MATH Algebra but -6.6% drop in AIME accuracy
- **Why unresolved**: Paper suggests different problem-solving skills required but doesn't explain negative transfer to specific math competition formats
- **What evidence would resolve it**: Analysis of specific proof strategies used in AIME vs. MATH algebra to identify conflicting heuristics

## Limitations

- Small evaluation set (250 test samples) and training corpus (~5.5K samples) limits generalizability to broader physics formalization
- Reliance on Claude-4.5-Sonnet for synthetic conjecture generation introduces potential distributional bias that could inflate performance metrics
- Does not address potential overfitting to specific structure of PhysLean repository, which may limit applicability to other physics formalization efforts

## Confidence

**High Confidence**: Claim that verifiable binary rewards from Lean 4 reduce hallucination is strongly supported by direct evidence showing base model generates non-existent lemmas while PhysProver uses only valid context lemmas

**Medium Confidence**: Assertion that physics-specific training enhances in-context learning is supported by empirical results but could be influenced by synthetic data generation choices

**Low Confidence**: Claim that RL succeeds where SFT fails due to distributional mismatch is compelling but relies heavily on single comparison in Table 3 without extensive ablation studies

## Next Checks

1. **Generalization Stress Test**: Evaluate PhysProver on physics formalization tasks from repositories outside PhysLean (e.g., IRIF's physics developments) to assess whether 2.4% improvement holds across different formalization styles

2. **SFT Hyperparameter Sweep**: Systematically test SFT with various learning rates, batch sizes, and curriculum strategies on same dataset to determine whether performance degradation is inherent to method or artifact of specific hyperparameter choices

3. **Hallucination Audit**: Conduct fine-grained analysis of generation failures, categorizing whether errors stem from (a) inability to find valid proof paths, (b) misuse of valid lemmas, or (c) generation of syntactically valid but semantically incorrect statements