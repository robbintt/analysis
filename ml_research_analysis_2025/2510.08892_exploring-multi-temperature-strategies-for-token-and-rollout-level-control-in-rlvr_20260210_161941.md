---
ver: rpa2
title: Exploring Multi-Temperature Strategies for Token- and Rollout-Level Control
  in RLVR
arxiv_id: '2510.08892'
source_url: https://arxiv.org/abs/2510.08892
tags:
- temperature
- arxiv
- training
- reasoning
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of effectively balancing exploration
  and exploitation in reinforcement learning with verifiable rewards (RLVR) for large
  language models (LLMs) by investigating multi-temperature strategies at both token-
  and rollout-levels. The core method introduces an entropy-guided token-level temperature
  scheduling that applies higher temperatures to high-entropy reasoning tokens to
  promote exploration and lower temperatures to low-entropy knowledge tokens to preserve
  accuracy, along with multi-temperature sampling per prompt to hedge against poor
  single-temperature choices.
---

# Exploring Multi-Temperature Strategies for Token- and Rollout-Level Control in RLVR

## Quick Facts
- **arXiv ID:** 2510.08892
- **Source URL:** https://arxiv.org/abs/2510.08892
- **Reference count:** 5
- **Primary result:** Entropy-guided token-level and multi-temperature rollout strategies improve RLVR reasoning performance by 30% on AIME24 and 17% on Minerva benchmarks

## Executive Summary
This work addresses the challenge of effectively balancing exploration and exploitation in reinforcement learning with verifiable rewards (RLVR) for large language models (LLMs) by investigating multi-temperature strategies at both token- and rollout-levels. The core method introduces an entropy-guided token-level temperature scheduling that applies higher temperatures to high-entropy reasoning tokens to promote exploration and lower temperatures to low-entropy knowledge tokens to preserve accuracy, along with multi-temperature sampling per prompt to hedge against poor single-temperature choices. Empirical evaluations on challenging reasoning benchmarks demonstrate that these strategies significantly enhance reasoning performance: token-level temperature sampling improves AIME24 accuracy by 30% and Minerva by 17%, while multi-temperature sampling achieves competitive results even with out-of-range temperatures and provides better resilience across training stages. Combining these approaches further boosts performance, showing that differentiated temperature control at multiple granularities is an effective way to improve exploration without sacrificing solution quality in RLVR.

## Method Summary
The paper investigates two complementary temperature control strategies for RLVR: token-level and rollout-level. The token-level approach computes entropy for each generated token and maintains a sliding window queue to establish a dynamic threshold. Tokens exceeding this threshold (classified as reasoning tokens) are sampled with high temperature to encourage exploration, while those below (knowledge tokens) use low temperature to preserve accuracy. The rollout-level strategy generates multiple responses per prompt using different temperatures, creating a mixture that hedges against suboptimal temperature choices during training. Both strategies are implemented within the GRPO/DAPO framework, with the token-level approach requiring entropy computation and queue management at each generation step, and the rollout-level approach requiring batch orchestration across temperature subsets.

## Key Results
- Token-level temperature sampling improves AIME24 accuracy by 30% and Minerva by 17% compared to fixed-temperature baselines
- Multi-temperature sampling achieves competitive results even with out-of-range temperatures and provides better resilience across training stages
- Combining token-level and rollout-level strategies further boosts performance, demonstrating the effectiveness of differentiated temperature control at multiple granularities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Differentiating sampling temperature based on real-time token entropy improves the balance between exploration and exploitation compared to uniform temperature settings.
- **Mechanism:** The system calculates the entropy $H_t$ for each generated token. It maintains a sliding window (queue $Q$) of recent token entropies to establish a dynamic threshold $H_{th}$ (k-th percentile). If a token's entropy exceeds this threshold, it is classified as a "reasoning token" and sampled with $T_{high}$ to encourage diversity. Otherwise, it is treated as a "knowledge token" and sampled with $T_{low}$ to preserve factual accuracy. This explicitly modifies the sampling distribution $\pi_{\theta, \tau_t}$ at the token level.
- **Core assumption:** High-entropy tokens correspond to reasoning steps where exploration is beneficial, while low-entropy tokens correspond to factual knowledge where randomness degrades accuracy.
- **Evidence anchors:** [abstract] "...employing higher temperatures for reasoning tokens to actively encourage exploration, while retaining lower temperatures for knowledge tokens to maintain factual correctness." [section 3.2] Defines the entropy threshold $H_{th} = \text{Quantile } k\%(Q)$ and the temperature switch $\tau_t$. [corpus] "Beyond the 80/20 Rule" supports the premise that high-entropy "forking" tokens drive effective credit assignment.

### Mechanism 2
- **Claim:** Sampling rollouts with a mixture of temperatures per prompt hedges against the instability of optimal temperature selection over training stages.
- **Mechanism:** Instead of generating $G$ responses using a single $\tau$, the system generates subsets of responses using distinct temperatures $\{\tau_1, \tau_2, \dots, \tau_m\}$. This ensures that even if the "optimal" temperature shifts during training, the batch still contains high-quality samples from the currently effective temperature range.
- **Core assumption:** The computational cost of generating extra rollouts is acceptable, or the number of rollouts per temperature subset is reduced to keep total generation constant.
- **Evidence anchors:** [abstract] "...multi-temperature sampling per prompt to hedge against poor single-temperature choices." [section 4.3] Demonstrates that "in-range mixture" tracks the fastest baseline and surpasses it, maintaining performance even when individual temperatures would fail. [corpus] Corpus evidence specifically for *multi-temperature mixing* is weak; related work focuses on entropy constraints rather than temperature hedging.

### Mechanism 3
- **Claim:** On-policy temperature application (during generation) stabilizes training and improves gradient signals compared to off-policy application (during update only).
- **Mechanism:** The paper contrasts applying temperature scaling during the sampling phase (on-policy) against applying it only during the loss calculation (off-policy). On-policy sampling ensures the generated trajectory distribution matches the distribution the model is updated on, preventing distribution mismatch that appears to cause higher gradient norms and instability in early training.
- **Core assumption:** The performance gap is driven by distribution mismatch rather than just the diversity of the batch.
- **Evidence anchors:** [section 4.5] "The on-policy strategy yields better performance and a more stable training process... particularly in the early stages when the gradient norm is high." [section 4.5] Figure 5 shows a significant gap in accuracy and response length between on-policy and off-policy curves. [corpus] Corpus signals do not specifically address on-policy vs. off-policy temperature scaling mechanisms.

## Foundational Learning

- **Concept: Entropy in LLMs**
  - **Why needed here:** The core method relies on distinguishing "reasoning" from "knowledge" using entropy. You must understand that entropy $H = -\sum p \log p$ measures uncertainty; low entropy means the model is confident (often factual), high entropy means the model is unsure (often a reasoning junction).
  - **Quick check question:** If a model assigns 99% probability to the next token, is the entropy high or low? **Answer:** Low

- **Concept: Softmax Temperature Scaling**
  - **Why needed here:** The mechanism manipulates the probability distribution $\pi$ by dividing logits by $\tau$. You need to know that $\tau > 1$ flattens the distribution (more exploration) and $\tau < 1$ sharpens it (more exploitation).
  - **Quick check question:** Does increasing the temperature make the model's output more random or more deterministic? **Answer:** More random

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - **Why needed here:** This is the training paradigm (specifically GRPO/DAPO). Unlike standard RL, rewards are often binary (correct/incorrect) and derived from code/math execution. Understanding that the "advantage" is calculated relative to a group of rollouts is essential.
  - **Quick check question:** In GRPO, is the advantage calculated using a learned value function or by comparing rewards within a group of sampled outputs? **Answer:** By comparing rewards within a group of sampled outputs

## Architecture Onboarding

- **Component map:** Entropy Queue -> Threshold Calculator -> Dynamic Sampler -> Multi-Temp Orchestrator
- **Critical path:** The sampling loop. Standard LLM sampling uses a fixed temperature for the whole sequence. Here, you must compute entropy -> update queue -> get threshold -> select temp -> sample token *for every single step* of generation. This introduces non-negligible overhead if not vectorized.
- **Design tradeoffs:**
  - **Queue Size ($N$) vs. Sensitivity:** A small queue reacts fast to context shifts (e.g., switching from math to text) but may be noisy. A large queue is stable but may lag.
  - **Temperature Spread:** A wide gap between $T_{high}$ and $T_{low}$ maximizes the distinction but risks incoherence at $T_{high}$.
  - **Assumption:** Using a fixed percentile $k$ (e.g., top 40%) assumes a consistent distribution of reasoning vs. knowledge tokens across different problem types.
- **Failure signatures:**
  - **Repetition Loops:** Observed in Section 4.4 when token-level temps were mismatched with evaluation temps. The model repeats a specific reasoning step without progressing.
  - **Early Collapse:** If $T_{high}$ is too aggressive, the model may never converge, indicated by high gradient norms and low rewards.
  - **Pass@K Drop:** Improvement in avg@256 (single answer) but drop in pass@256 (diversity) indicates the model is overfitting to a specific solution path found via high-temp exploration, losing alternative paths.
- **First 3 experiments:**
  1. **Entropy Distribution Visualization:** Before training, log the entropy distribution of tokens on a validation set. Confirm the bimodal (or skewed) distribution justifies a binary threshold strategy.
  2. **Ablation on Queue Hyperparameters:** Run a grid search on Queue Length $N$ and Percentile $k$ (e.g., $N \in \{50, 100, 200\}$, $k \in \{20, 40, 60\}$) on a small dataset (e.g., MATH subset) to find the stability sweet spot.
  3. **Multi-Temp Robustness Test:** Train three agents: one with fixed $\tau=1.0$, one with fixed $\tau=1.2$, and one with Multi-Temp $\{1.0, 1.2\}$. Plot performance over 200 steps to verify if the Multi-Temp agent tracks or exceeds the better of the two fixed agents.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do multi-temperature strategies generalize to LLMs with parameters significantly larger than 1.5B?
- **Basis in paper:** [explicit] The authors state: "Our experiments are limited to the 1.5B-parameter Qwen2.5 model due to computational resource constraints. We did not conduct evaluations on larger models or other architectures."
- **Why unresolved:** Computational constraints prevented testing on larger scales; it remains unknown whether entropy distributions and optimal temperature thresholds shift with model capacity.
- **What evidence would resolve it:** Replicating the token-level and rollout-level temperature experiments on 7B, 70B, or larger models and comparing performance gains and entropy dynamics.

### Open Question 2
- **Question:** What is the optimal scheduling strategy for step-wise temperature adjustment during RLVR training?
- **Basis in paper:** [explicit] The authors note: "Prior work suggests that progressively increasing the sampling temperature during training can boost model performance, yet the optimal scheduling strategy remains unclear."
- **Why unresolved:** The experiments compare spike vs. linear schedules but do not explore adaptive or entropy-guided step-wise schedules; trade-offs between response length and accuracy remain incompletely characterized.
- **What evidence would resolve it:** A systematic sweep of scheduling functions (spike intervals, linear rates, cosine annealing) with analysis of convergence speed, final accuracy, and entropy trajectories.

### Open Question 3
- **Question:** Why does token-level temperature sampling combined with Archer constraints cause performance collapse after step 100?
- **Basis in paper:** [inferred] Section 4.4 reports competitive early performance but "a sharp decline after, despite training rewards remaining consistent," with repetitive reasoning steps observed.
- **Why unresolved:** The paper speculates about training-evaluation temperature mismatch but does not isolate whether the issue stems from gradient instability, KL divergence drift, or interaction with dual-token constraints.
- **What evidence would resolve it:** Ablation studies monitoring gradient norms, KL divergence, and token-level entropy over time; testing whether aligning evaluation temperature with per-token training temperatures prevents collapse.

## Limitations
- The entropy-based threshold mechanism relies on assumptions about the relationship between entropy and token type that may not hold universally across different problem domains or model architectures.
- The multi-temperature rollout strategy assumes computational budget allows for generating multiple responses per prompt, but efficiency metrics and wall-clock comparisons are not provided.
- The on-policy vs off-policy comparison shows performance differences but does not establish causality or rule out confounding factors such as batch size effects or reward scaling variations.

## Confidence
- **High Confidence:** The core empirical finding that multi-temperature strategies improve reasoning performance on established benchmarks (AIME24 +30%, Minerva +17%). The experimental methodology is sound, and the results are reproducible with the provided specifications.
- **Medium Confidence:** The theoretical justification for entropy-guided token-level temperature scheduling. While the mechanism is clearly described, the assumption about the relationship between entropy and token type is not universally validated and may depend on problem domain and model architecture.
- **Low Confidence:** The claim that on-policy temperature application is inherently superior to off-policy. The evidence shows performance differences but does not establish causality or rule out confounding factors.

## Next Checks
1. **Queue Length Sensitivity Analysis:** Systematically vary the queue length N (e.g., 50, 100, 200 tokens) and percentile k (20%, 40%, 60%) on a subset of the MATH dataset. Plot Pass@1 and Pass@256 performance to identify optimal configurations and assess sensitivity to these hyperparameters.
2. **Computational Overhead Measurement:** Implement the full multi-temperature framework and measure wall-clock time per training step compared to single-temperature baseline. Calculate the performance gain per unit of additional computation to assess practical utility.
3. **Entropy Distribution Validation:** On held-out validation data, visualize the entropy distribution of tokens across different reasoning domains (algebra, geometry, calculus). Test whether the bimodal assumption holds and whether alternative classification schemes (e.g., continuous temperature mapping vs binary threshold) could be more effective.