---
ver: rpa2
title: '(WhyPHI) Fine-Tuning PHI-3 for Multiple-Choice Question Answering: Methodology,
  Results, and Challenges'
arxiv_id: '2501.01588'
source_url: https://arxiv.org/abs/2501.01588
tags:
- llms
- phi-3
- dataset
- fine-tuning
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study fine-tunes Microsoft's PHI-3.5, a compact 1.3B parameter
  language model, for multiple-choice question answering using the TruthfulQA dataset.
  The approach combines dataset preprocessing to standardize question formats, optimized
  prompt engineering to reduce hallucinations, and supervised fine-tuning on an NVIDIA
  GTX 1650 GPU.
---

# (WhyPHI) Fine-Tuning PHI-3 for Multiple-Choice Question Answering: Methodology, Results, and Challenges

## Quick Facts
- arXiv ID: 2501.01588
- Source URL: https://arxiv.org/abs/2501.01588
- Reference count: 14
- Fine-tuned PHI-3.5 achieves 90.8% accuracy on MCQ tasks with 30-35 hours training on GTX 1650

## Executive Summary
This study demonstrates effective fine-tuning of Microsoft's compact PHI-3.5 language model (1.3B parameters) for multiple-choice question answering using the TruthfulQA dataset. The approach combines dataset preprocessing to standardize question formats, optimized prompt engineering to reduce hallucinations, and supervised fine-tuning on resource-constrained hardware. The results show substantial performance improvements across key metrics, achieving 90.8% accuracy compared to baseline 62%, while maintaining computational efficiency suitable for educational applications.

## Method Summary
The methodology employs a three-phase approach: dataset preprocessing to standardize MCQ formats, prompt engineering optimization to reduce hallucination, and supervised fine-tuning using the Hugging Face Transformers library on an NVIDIA GTX 1650 GPU. The preprocessing pipeline converts raw data into standardized formats with proper answer labeling, while the training process leverages the LLaMA architecture's compatibility with efficient hardware. The entire fine-tuning process completes in 30-35 hours, demonstrating practical feasibility for resource-constrained environments.

## Key Results
- Perplexity reduced from 4.68 to 2.27 after fine-tuning
- Accuracy improved from 62% to 90.8% on TruthfulQA test set
- F1 score increased from 66 to 90.6
- Training completed in 30-35 hours on NVIDIA GTX 1650 GPU

## Why This Works (Mechanism)
The effectiveness stems from the combination of optimized prompt engineering that reduces hallucination tendencies and targeted fine-tuning on domain-specific MCQ data. The preprocessing standardization ensures consistent input formatting, while the supervised learning approach allows the model to learn specific patterns in educational question-answering tasks. The compact architecture of PHI-3.5, combined with efficient training on accessible hardware, demonstrates that high performance can be achieved without requiring massive computational resources.

## Foundational Learning
1. Perplexity measurement - needed to quantify language model uncertainty and track training progress; quick check: lower perplexity indicates better probability distribution fitting
2. F1 score calculation - needed to balance precision and recall in classification tasks; quick check: harmonic mean of precision and recall provides balanced accuracy metric
3. Supervised fine-tuning - needed to adapt pre-trained models to specific downstream tasks; quick check: labeled data enables task-specific parameter optimization
4. Prompt engineering - needed to guide model responses and reduce hallucination; quick check: well-designed prompts improve output consistency and relevance
5. GPU acceleration - needed for efficient model training; quick check: CUDA compatibility enables parallel computation for faster convergence
6. Dataset standardization - needed to ensure consistent input formatting; quick check: uniform data structure improves model learning efficiency

## Architecture Onboarding
Component Map: Input Data -> Preprocessing Pipeline -> PHI-3.5 Model -> Fine-tuning Loop -> Performance Metrics
Critical Path: Data standardization → Prompt optimization → Parameter updates → Evaluation
Design Tradeoffs: Compact model size vs. performance capability; computational efficiency vs. training duration; dataset specificity vs. generalizability
Failure Signatures: High perplexity indicates poor probability distribution learning; low accuracy suggests insufficient fine-tuning or data quality issues; GPU memory constraints limit batch sizes
First Experiments:
1. Baseline evaluation without fine-tuning to establish performance metrics
2. Single-epoch fine-tuning to assess initial learning rate and convergence patterns
3. Cross-validation on subset of training data to detect overfitting early

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to single TruthfulQA dataset, raising generalizability concerns
- Small test set (1,000 samples) may lack statistical power for detecting subtle variations
- Hardware constraints (GTX 1650) may limit maximum achievable performance
- Unclear contribution separation between preprocessing and fine-tuning improvements

## Confidence
- Performance improvements (perplexity, accuracy, F1): High confidence
- Generalizability to other MCQ domains: Low confidence
- Resource efficiency claims: Medium confidence
- Impact of preprocessing vs. fine-tuning: Low confidence

## Next Checks
1. Evaluate fine-tuned model on multiple diverse MCQ datasets to assess cross-domain generalization
2. Conduct ablation studies comparing performance with and without preprocessing steps
3. Test model on more powerful hardware configurations to establish performance ceilings