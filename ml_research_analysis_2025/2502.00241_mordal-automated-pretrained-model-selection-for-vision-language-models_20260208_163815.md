---
ver: rpa2
title: 'Mordal: Automated Pretrained Model Selection for Vision Language Models'
arxiv_id: '2502.00241'
source_url: https://arxiv.org/abs/2502.00241
tags:
- mordal
- pretrained
- vision
- candidates
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mordal, a framework for automated pretrained
  model selection in vision-language models (VLMs). The key problem addressed is the
  inefficiency of manually selecting pretrained vision encoders and language models
  for VLMs, which often leads to suboptimal performance across different tasks.
---

# Mordal: Automated Pretrained Model Selection for Vision Language Models

## Quick Facts
- arXiv ID: 2502.00241
- Source URL: https://arxiv.org/abs/2502.00241
- Reference count: 40
- Primary result: Identifies optimal VLM combinations 8.9×-11.6× faster than exhaustive grid search while maintaining high accuracy

## Executive Summary
This paper introduces Mordal, a framework for automated pretrained model selection in vision-language models (VLMs). The key problem addressed is the inefficiency of manually selecting pretrained vision encoders and language models for VLMs, which often leads to suboptimal performance across different tasks. Mordal solves this by clustering VLM candidates based on representation similarity, using a two-step inter- and intra-cluster evaluation process, and applying early stopping and observational scaling laws to reduce evaluation time. The framework was evaluated on six datasets across three domains (Visual QA, Doc QA, Knowledge) using seven vision encoders and seven language models.

## Method Summary
Mordal employs a two-phase approach: clustering followed by evaluation. First, it computes Centered Kernel Alignment (CKA) similarity matrices for vision encoders and language models separately, then performs hierarchical clustering with thresholds (t_ve=0.7, t_llm=0.8) to group similar candidates. During the inter-cluster phase, Mordal applies Successive Halting Algorithm (SHA) to evaluate only the medoid of each cluster, retaining the top K clusters. In the intra-cluster phase, Mordal trains remaining candidates on reduced sample ratios and fits a log-linear regression to predict full-dataset performance, enabling efficient identification of optimal VLM combinations.

## Key Results
- Identifies optimal VLM combination 8.9×-11.6× faster than exhaustive grid search
- Successfully finds top-performing models in five out of six tasks tested
- Achieves Kendall's τ values ranging from 0.76 to 0.96 for ranking quality

## Why This Works (Mechanism)

### Mechanism 1: Representation Similarity Clustering
If two model components produce similar activation representations on a target dataset, their resulting VLM performance is likely correlated, allowing the search space to be pruned via clustering. Mordal computes representation similarity using Centered Kernel Alignment (CKA) on the vision encoders and language models separately. It clusters them hierarchically and evaluates only the "medoid" (most central candidate) of each cluster during the initial search phase. The core assumption is that the relative performance ranking of VLM candidates remains consistent within a cluster.

### Mechanism 2: Successive Halving for Cluster Pruning
Poor-performing VLM clusters can be identified and discarded early in the training process by aggressively reallocating resources away from the bottom percentile of candidates. Mordal applies a Successive Halving Algorithm (SHA) during inter-cluster evaluation. It trains all cluster representatives with a small sample ratio, keeps the top 1/η, and doubles the budget for the survivors. The core assumption is that ranking stability at low compute (early training checkpoints) is predictive of final ranking for the purpose of binary inclusion/exclusion decisions.

### Mechanism 3: Log-Linear Scaling Law Extrapolation
The alignment performance of a VLM candidate often follows a log-linear relationship with the number of training samples, enabling the prediction of full-dataset performance from partial training. For promising candidates surviving the inter-cluster phase, Mordal evaluates performance at low sample ratios (e.g., R=1/8). It fits a linear regression to the log-sampled data points vs. log-error and extrapolates to predict the score at R=1 (full dataset). The core assumption is that the log-linear scaling behavior observed in LLMs holds for the specific VLM alignment task and projector training.

## Foundational Learning

**Concept: Centered Kernel Alignment (CKA)**
- Why needed here: Used to measure the similarity of neural network representations independent of architecture size. This is the mathematical engine behind Mordal's ability to cluster a heterogeneous set of vision encoders and LLMs.
- Quick check question: If two models have high CKA similarity on a dataset, does that guarantee identical accuracy on a downstream task? (Answer: No, but it suggests similar information processing, making them candidates for pruning).

**Concept: Multi-fidelity Optimization (Hyperband/SHA)**
- Why needed here: The paper frames model selection as a resource-constrained problem. Understanding SHA explains why the system can safely discard candidates without fully training them.
- Quick check question: How does "doubling the budget" for survivors in SHA affect the total compute spent on losers?

**Concept: Observational Scaling Laws**
- Why needed here: The core innovation for the "Intra-cluster" phase. It explains how the system predicts future performance without paying the full cost of training.
- Quick check question: Why must the data points be plotted on a log-log scale to fit a linear regression in this context?

## Architecture Onboarding

**Component map:**
Clustering Module -> Inter-Cluster Evaluator -> Intra-Cluster Evaluator

**Critical path:** The flow from CKA computation to Inter-cluster SHA is the primary latency reducer. If the clustering is too loose (low threshold), the Inter-cluster phase becomes a grid search.

**Design tradeoffs:**
- Clustering Threshold (t_ve, t_llm): Higher thresholds = more clusters = higher fidelity but slower search. Lower thresholds = faster but risk grouping a "gold" model with a "bad" cluster and pruning it early.
- Top-K Selection: Setting K=1 is fastest but risky; the paper often retains Top-3.

**Failure signatures:**
- "Slow starter" failure: A model that ranks low at 1/8 data but would rank highest at full data gets pruned by SHA.
- "Non-linear scaling" failure: The regression predicts high performance, but the model plateaus or overfits early, breaking the log-linear assumption.

**First 3 experiments:**
1. Validation of Clustering: Take a small subset of the model zoo (e.g., 3 VEs, 3 LLMs). Run full grid search. Compute CKA. Verify if CKA distance correlates with performance delta on a specific dataset (e.g., GQA).
2. Scaling Law Verification: Pick one VLM candidate. Train with 1/16, 1/8, 1/4, 1/2 of data. Plot Log(Error) vs Log(Data). Check if the relationship is linear after a certain warm-up point.
3. End-to-End Ablation: Run Mordal on a single task (e.g., VizWiz) with and without the "Scaling Prediction" module to quantify the exact GPU hours saved vs. the loss in Top-1 accuracy.

## Open Questions the Paper Calls Out
None

## Limitations
- The generalizability of representation-similarity clustering and scaling law assumptions may break down when target dataset distribution differs significantly from alignment data
- Clustering thresholds (t_ve=0.7, t_llm=0.8) were not systematically ablated, leaving uncertainty about sensitivity to dataset characteristics
- The log-linear scaling law extrapolation's reliability across diverse task domains is assumed but not rigorously validated

## Confidence
- **High Confidence:** Computational efficiency gains (8.9×-11.6× speedup) and Kendall's τ values (0.76-0.96) for ranking quality
- **Medium Confidence:** CKA-based clustering reducing search space is theoretically sound but edge cases not fully explored
- **Low Confidence:** Scaling law extrapolation reliability across diverse task domains remains unclear

## Next Checks
1. Evaluate Mordal on a target dataset whose distribution significantly differs from the LLaVA-1.5-Instruction alignment data to measure degradation in clustering accuracy and scaling law predictions
2. Systematically vary the CKA clustering thresholds (t_ve, t_llm) across a range (e.g., 0.6-0.9) and measure impact on search efficiency and accuracy
3. Identify conditions where the log-linear scaling assumption breaks and test whether alternative regression models improve prediction accuracy for these failure modes