---
ver: rpa2
title: Respiratory Inhaler Sound Event Classification Using Self-Supervised Learning
arxiv_id: '2504.11246'
source_url: https://arxiv.org/abs/2504.11246
tags:
- inhaler
- sounds
- data
- dataset
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the application of the wav2vec 2.0 self-supervised
  learning model for inhaler sound classification. The study addresses the challenge
  of developing a generalizable model for inhaler sound classification across different
  inhaler types and audio capture devices.
---

# Respiratory Inhaler Sound Event Classification Using Self-Supervised Learning

## Quick Facts
- arXiv ID: 2504.11246
- Source URL: https://arxiv.org/abs/2504.11246
- Authors: Davoud Shariat Panah; Alessandro N Franciosi; Cormac McCarthy; Andrew Hines
- Reference count: 35
- Primary result: wav2vec 2.0 achieves 98% balanced accuracy for inhaler sound classification across different devices

## Executive Summary
This paper applies wav2vec 2.0 self-supervised learning to classify inhaler sounds into actuation, exhalation, and inhalation events. The approach demonstrates that pre-training on inhaler audio followed by fine-tuning enables high accuracy (98% UAR) on a smartwatch-collected dataset. Critically, the model can be adapted to different inhaler types with minimal annotated data (15 seconds), enabling personalized monitoring across devices. This work shows promise for smartwatch-based medication adherence tracking in clinical settings.

## Method Summary
The method uses wav2vec 2.0 with pre-training on either LibriSpeech or inhaler audio, followed by fine-tuning on labeled inhaler segments. Audio is resampled to 16kHz and normalized. A classification head (average pooling + fully connected layer) is added for 3-class prediction. Training uses AdamW with OneCycle learning rate scheduling, early stopping after 5 epochs without validation improvement. Evaluation includes Hold-Out split and leave-one-subject-out cross-validation on two datasets: DPI-Watch (placebo DPI + smartwatch) and RDA (pMDI with embedded mic).

## Key Results
- wav2vec 2.0 achieves 98% UAR/F1 on DPI-Watch dataset after pre-training and fine-tuning
- Cross-device adaptation possible: MDI_MDI model re-fine-tuned on 15 seconds of DPI data achieves 0.98 recall for exhalation and 0.97 for inhalation
- Domain-specific pre-training provides marginal benefit over speech pre-training for exhalation detection (0.98 vs 0.97 UAR)

## Why This Works (Mechanism)

### Mechanism 1
- Self-supervised pre-training on unlabeled inhaler audio enables learning discriminative acoustic representations without manual annotation
- wav2vec 2.0 contrastive loss forces context network to identify correct masked latent representations from distractors, learning temporal and spectral patterns inherent to inhaler sounds
- Core assumption: Inhaler sound events share learnable acoustic structures that can be captured without explicit labels
- Break condition: If inhaler sounds lack consistent acoustic structure across users/devices, contrastive pre-training may fail to converge

### Mechanism 2
- Fine-tuning with labeled segments transfers learned representations to downstream classification task with high accuracy
- Pre-trained encoder and context network weights are adapted via supervised training; pooling layer and classifier head map contextualized embeddings to actuation/exhalation/inhalation classes
- Core assumption: Sufficient labeled examples exist for target inhaler-hardware combination during fine-tuning
- Break condition: If label noise or class imbalance is severe, fine-tuning may overfit to majority classes

### Mechanism 3
- Re-fine-tuning source-domain model with minimal target-domain data enables rapid adaptation across inhaler types and capture hardware
- MDI_MDI model (pre-trained/fine-tuned on pMDI sounds) is further fine-tuned on small amounts of DPI-Watch data; learned representations transfer, requiring only target-specific calibration
- Core assumption: Source model has captured transferable respiratory event patterns; only device/inhaler-specific acoustics need adjustment
- Break condition: If acoustic mismatch is too large, minimal data may be insufficient—actuation recall dropped to 0.45 at 15 seconds

## Foundational Learning

- **Self-supervised contrastive learning**
  - Why needed: Enables representation learning from unlabeled inhaler audio, reducing annotation burden
  - Quick check: Can you explain how contrastive loss distinguishes positive vs. negative samples in wav2vec 2.0?

- **Audio feature extraction from raw waveforms**
  - Why needed: Feature encoder operates directly on raw audio; understanding convolutional downsampling and temporal resolution is essential
  - Quick check: What is the effect of the 7-layer CNN on temporal resolution and receptive field size?

- **Transfer learning and fine-tuning strategies**
  - Why needed: Paper relies on adapting pre-trained weights; overfitting and catastrophic forgetting risks must be managed
  - Quick check: Why does early stopping based on validation loss prevent overfitting during fine-tuning?

## Architecture Onboarding

- **Component map:** Raw waveform -> 7-layer CNN (feature encoder) -> Quantizer -> 12 transformer blocks (context network) -> Average pooling -> Fully connected layer -> Softmax (3 classes)

- **Critical path:** 1. Pre-process: Resample to 16 kHz, amplitude normalization 2. Pre-train: SSL on unlabeled recordings (contrastive loss over masked time steps) 3. Fine-tune: Add classifier head, train on labeled segments with cross-entropy 4. Re-fine-tune (adaptation): Continue training on target domain data with reduced learning rate

- **Design tradeoffs:**
  - Pre-training data source: LibriSpeech (general speech) vs. domain-specific inhaler audio—domain pre-training shows slight exhalation improvement but high compute cost
  - Fine-tuning data volume: More data improves actuation recall; exhalation/inhalation robust to reduction
  - Hardware variability: Smartwatch microphones vs. embedded inhaler microphones introduce frequency response and positioning differences

- **Failure signatures:**
  - Low actuation recall with minimal re-fine-tuning data (0.45 at 15 seconds): Acoustic mechanism differs fundamentally between pMDI and DPI
  - Poor cross-inhaler generalization (MDI_MDI on DPI: 0.54 UAR): Acoustic and hardware mismatch prevents direct transfer without adaptation
  - Overfitting on small validation sets: Use LOSO-CV to detect subject-specific memorization

- **First 3 experiments:**
  1. Replicate LS_DPI baseline: Fine-tune pre-trained wav2vec 2.0 (LibriSpeech) on DPI-Watch with hold-out split; verify ~0.98 UAR
  2. Cross-domain stress test: Train on RDA (pMDI), test on DPI-Watch without re-fine-tuning; confirm performance drop (~0.54 UAR)
  3. Minimal-data adaptation: Re-fine-tune MDI_MDI on 15, 30, 60 seconds of DPI-Watch; plot recall curves per class to identify actuation vs. respiratory class divergence

## Open Questions the Paper Calls Out

- **Cross-device generalization:** Can the proposed model maintain high classification performance when validated across a larger cohort of users and a wider variety of inhaler devices? The current study relies on small datasets (7 participants for DPI-Watch and 3 for RDA), limiting ability to generalize to broader population or different hardware brands.

- **Technique efficacy evaluation:** Can the system be extended to accurately evaluate the efficacy of inhaler usage (technique quality) rather than solely classifying sound events? The study focused on event classification and did not provide quantitative assessment of whether technique was clinically correct.

- **Real-world robustness:** How robust is the smartwatch-based classification model in uncontrolled, real-world environments containing significant ambient noise? The study datasets were collected in quiet or acoustically controlled environments, not testing against variable background noise typical of daily life.

## Limitations

- Cross-inhaler generalization remains challenging without adaptation; direct transfer from pMDI to DPI drops actuation recall to 0.45 even with 15 seconds of re-fine-tuning data
- Minimal-data adaptation results are promising but not fully characterized—performance trajectories across varying amounts of target data and across all classes remain unreported
- Hardware transfer across different microphones introduces frequency response and positioning variability not fully quantified in the paper

## Confidence

- **High confidence:** Pre-training and fine-tuning wav2vec 2.0 on inhaler sounds achieves ~98% UAR within same inhaler-hardware combination (DPI-Watch dataset)
- **Medium confidence:** Self-supervised pre-training on domain-specific inhaler audio provides marginal improvement over speech pre-training for exhalation detection
- **Medium confidence:** Re-fine-tuning with minimal target data enables adaptation across inhaler types, but actuation classification remains problematic

## Next Checks

1. **Cross-device validation:** Test DPI_DPI model on inhaler recordings captured with different smartwatches or embedded inhaler microphones to quantify hardware transfer limits

2. **Adaptation data scaling:** Systematically vary re-fine-tuning data from 5 to 120 seconds and plot class-specific recall curves to identify optimal data requirements per class

3. **LOSO-CV on cross-inhaler adaptation:** Perform leave-one-subject-out cross-validation where training uses one inhaler type and testing uses another to better quantify generalization boundaries