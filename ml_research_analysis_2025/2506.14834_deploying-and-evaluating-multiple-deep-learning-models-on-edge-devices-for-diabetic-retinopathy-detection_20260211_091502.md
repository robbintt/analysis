---
ver: rpa2
title: Deploying and Evaluating Multiple Deep Learning Models on Edge Devices for
  Diabetic Retinopathy Detection
arxiv_id: '2506.14834'
source_url: https://arxiv.org/abs/2506.14834
tags:
- edge
- diabetic
- retinopathy
- performance
- devices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed deep learning models for real-time diabetic
  retinopathy detection on edge devices, addressing the challenge of early diagnosis
  in resource-constrained healthcare settings. A dataset of 3,662 retinal fundus images
  was curated and enhanced using preprocessing and augmentation techniques.
---

# Deploying and Evaluating Multiple Deep Learning Models on Edge Devices for Diabetic Retinopathy Detection

## Quick Facts
- arXiv ID: 2506.14834
- Source URL: https://arxiv.org/abs/2506.14834
- Reference count: 4
- Primary result: Deep learning models for real-time diabetic retinopathy detection on edge devices, addressing early diagnosis in resource-constrained healthcare settings.

## Executive Summary
This study addresses the challenge of real-time diabetic retinopathy (DR) detection on edge devices in resource-constrained healthcare settings. The authors developed and evaluated multiple convolutional neural network (CNN) architectures—MobileNet, ShuffleNet, SqueezeNet, and a custom DNN—for classifying retinal fundus images into five severity levels. By leveraging TensorFlowLite conversion and INT8 quantization, the models were optimized for deployment on various edge hardware, including microcontrollers, AI accelerators, CPUs, and GPUs. The study demonstrates that lightweight architectures can achieve clinically relevant accuracy while meeting strict latency and resource constraints on edge devices.

## Method Summary
The research utilized a curated dataset of 3,662 retinal fundus images from the Kaggle EyePACS dataset, preprocessed to 224×224 resolution and augmented with rotation, flipping, and contrast adjustments. Four CNN architectures were trained using TensorFlow with Adam optimizer (learning rate 0.001, batch size 32, 20 epochs). Models were converted to TensorFlowLite and quantized to 8-bit integers to reduce size and accelerate inference. Deployment was facilitated through Edge Impulse, enabling evaluation across multiple edge hardware platforms including low-end and high-end microcontrollers, CPUs, and GPUs.

## Key Results
- MobileNet achieved the highest accuracy at 96.45% on training data, though validation accuracy was lower at 73.58%.
- SqueezeNet demonstrated strong real-time performance with a small model size of 176 KB and latency of just 17 ms on GPU.
- ShuffleNet excelled in resource efficiency with a model size of 68.5 KB and GPU latency of 3 ms, though with lower accuracy (68.47% validation).
- Custom DNN showed moderate performance but suffered from overfitting, with a 15.5% gap between training (90.93%) and validation (75.43%) accuracy.

## Why This Works (Mechanism)

### Mechanism 1: INT8 Quantization Enables Edge Deployment
- Converting models from 32-bit floating point to 8-bit integer representation reduces model size and accelerates inference with acceptable accuracy trade-offs.
- TensorFlowLite conversion maps float32 weights → int8 range, reducing memory footprint ~4× and enabling integer-only arithmetic on hardware without FPUs.
- Core assumption: The precision loss from quantization does not catastrophically degrade classification performance for retinal feature detection.
- Evidence anchors: [abstract] confirms minimal accuracy trade-offs; [section 3.1.3] highlights reduced computational requirements; corpus validation is limited.

### Mechanism 2: Architecture-Aware Latency Scaling Across Hardware Tiers
- Lightweight architectures (MobileNet, ShuffleNet, SqueezeNet) exhibit non-linear latency improvements when moving from low-end MCUs → high-end MCUs → GPUs due to hardware-specific optimizations.
- Depthwise separable convolutions (MobileNet), channel shuffling (ShuffleNet), and squeeze-expand layers (SqueezeNet) reduce FLOPs; latency gains compound when hardware supports parallelization.
- Core assumption: Target deployment hardware has sufficient RAM/ROM to load the quantized model and intermediate activations.
- Evidence anchors: [abstract] reports SqueezeNet GPU latency of 17ms; [section 4.1.1-4.1.4] provides detailed latency scaling; corpus confirms architecture-hardware co-design matters.

### Mechanism 3: Data Augmentation Mitigates Overfitting on Small Medical Datasets
- Augmentation (rotation, flipping, contrast) expands effective training set size, improving generalization when raw data is limited.
- Synthetic variations force the model to learn invariant features rather than memorizing training images.
- Core assumption: Augmentation transformations preserve disease-relevant features while introducing realistic variability.
- Evidence anchors: [section 3.1.1] describes augmentation techniques; [section 4.1.4] suggests augmentation could reduce custom DNN's overfitting; corpus validation is weak.

## Foundational Learning

- **Concept: Quantization-Aware Training vs. Post-Training Quantization**
  - Why needed: This study uses post-training INT8 quantization; understanding when this suffices vs. requiring QAT prevents deployment failures.
  - Quick check: If your model's validation accuracy drops >8% after post-training quantization, what should you try next?

- **Concept: Training-Validation Gap as Overfitting Signal**
  - Why needed: MobileNet (96.45% train → 73.58% val) and custom DNN (90.93% → 75.43%) show severe overfitting; recognizing this guides regularization decisions.
  - Quick check: A model with 85% training accuracy and 84% validation accuracy—is this overfitting? What if it's 95% train and 70% val?

- **Concept: Latency Budgets for Real-Time Clinical Use**
  - Why needed: 156-second latency on low-end MCUs is clinically unusable; defining acceptable latency thresholds (e.g., <500ms for screening) hardware selection.
  - Quick check: For a rural clinic using a smartphone, which model-hardware combination from this paper meets a <100ms latency requirement?

## Architecture Onboarding

- **Component map:**
  [EyePACS Dataset] → [Preprocessing: resize 224×224, normalize, augment] → [CNN Training: MobileNet/ShuffleNet/SqueezeNet/Custom DNN] → [TFLite Conversion + INT8 Quantization] → [Edge Impulse Deployment] → [Edge Hardware: MCU / AI Accelerator / CPU / GPU]

- **Critical path:**
  1. Dataset curation with balanced class distribution (currently imbalanced: 1,805 No DR vs. 193 Severe)
  2. Model training with regularization (dropout, L2) to close train-validation gap
  3. Quantization conversion—verify size reduction and accuracy retention
  4. Target hardware selection based on latency budget and RAM/ROM constraints

- **Design tradeoffs:**
  - **MobileNet**: Highest accuracy (96.45% train), but largest size (3.4MB), severe overfitting, poor MCU latency
  - **SqueezeNet**: Best GPU latency (17ms), small size (176KB), moderate accuracy (72.59% val)
  - **ShuffleNet**: Smallest size (68.5KB), fastest GPU inference (3ms), lowest accuracy (68.47% val)
  - **Custom DNN**: Balanced but overfits; middle-ground size (1.6MB)

- **Failure signatures:**
  - Train-validation gap >15%: Overfitting—add dropout, L2 regularization, early stopping
  - MCU latency >10 seconds: Model too large/complex—switch to smaller architecture or use AI accelerator
  - RAM overflow on device: Model + activation buffers exceed available memory—reduce input resolution or use more aggressive quantization

- **First 3 experiments:**
  1. Replicate MobileNet training with added dropout (0.3-0.5) and L2 regularization; measure validation accuracy improvement
  2. Deploy SqueezeNet on a target smartphone; profile actual inference latency and battery impact over 100 consecutive inferences
  3. Apply class balancing (oversampling Severe/Proliferative classes or using class-weighted loss); retrain custom DNN and compare validation confusion matrix

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can pruning and advanced quantization techniques beyond INT8 further reduce model latency on low-end microcontrollers while preserving diagnostic accuracy?
- Basis in paper: [explicit] "Future efforts should prioritize optimizing these models through advanced techniques such as pruning and quantization, aiming to enhance real-time performance and minimize latency."
- Why unresolved: Current INT8 quantization still yields impractical latencies on low-end MCUs (156,957 ms for MobileNet, 241,592 ms for SqueezeNet), and pruning was not explored.
- What evidence would resolve it: Comparative latency and accuracy metrics from models compressed via structured/unstructured pruning and alternative quantization schemes (e.g., INT4, mixed-precision), benchmarked on the same MCU hardware.

### Open Question 2
- Question: Does training on Ghanaian retinal imaging data improve MobileNet's real-world diagnostic performance compared to the EyePACS-trained model?
- Basis in paper: [explicit] "Moving forward, PerceptronCARE will focus on optimizing the model with diverse datasets from Ghana"
- Why unresolved: The current model is trained exclusively on EyePACS data, which may differ in image acquisition protocols, patient demographics, and disease presentation from Ghanaian clinical settings.
- What evidence would resolve it: A controlled validation study comparing MobileNet accuracy, sensitivity, and specificity on EyePACS-trained vs. Ghanaian-data-trained models when deployed on local devices in Ghanaian clinics.

### Open Question 3
- Question: Would regularization techniques (dropout, L2) or architecture modifications close the 23% training-validation accuracy gap observed in MobileNet?
- Basis in paper: [explicit] "MobileNet's ability to generalize may benefit from further tuning, such as applying regularization techniques or augmenting the dataset to improve validation performance."
- Why unresolved: The paper reports 96.45% training accuracy vs. 73.58% validation accuracy but does not experimentally test regularization or architectural changes.
- What evidence would resolve it: Ablation experiments showing validation accuracy changes when adding dropout/L2 regularization or modifying network depth/width, using the same dataset and evaluation protocol.

### Open Question 4
- Question: Can adaptive model selection based on device capabilities improve system performance for DR screening across heterogeneous edge hardware?
- Basis in paper: [explicit] "Investigating adaptive deployment mechanisms and refining model integration across various edge devices will further ensure their broader applicability in diverse healthcare environments."
- Why unresolved: The study evaluates four models separately; no mechanism exists to dynamically select models based on available RAM, compute, or latency requirements.
- What evidence would resolve it: Implementation of an adaptive selector that profiles device constraints and routes to optimal models, with measured improvements in overall diagnostic throughput and accuracy across mixed-device deployments.

## Limitations
- Architecture details remain unspecified: MobileNet, ShuffleNet, SqueezeNet versions and layer configurations are not explicitly defined, creating ambiguity in reproduction.
- Class imbalance is unaddressed: The dataset has severe imbalance (1,805 No DR vs. 193 Severe), which may bias model performance and inflate accuracy metrics.
- Hardware specifics are absent: Exact device models for latency measurements are not disclosed, limiting replication and deployment planning.
- No quantization-aware training: Post-training quantization may underrepresent accuracy degradation in deployment.

## Confidence
- **High confidence**: Quantization reduces model size and accelerates inference (mechanisms 1, 2).
- **Medium confidence**: Data augmentation mitigates overfitting (mechanism 3), though corpus validation is weak.
- **Medium confidence**: MobileNet achieves highest accuracy (96.45% train), SqueezeNet offers best GPU latency (17ms), ShuffleNet excels in resource efficiency.

## Next Checks
1. **Hardware replication**: Deploy SqueezeNet on target deployment hardware (e.g., smartphone) and measure actual inference latency over 100 consecutive inferences, including battery impact.
2. **Class balancing impact**: Apply class-weighted loss or oversampling to address dataset imbalance; retrain models and compare validation confusion matrices for severe/proliferative classes.
3. **Quantization degradation**: Implement quantization-aware training for MobileNet; measure accuracy drop versus post-training quantization to validate mechanism 1 robustness.