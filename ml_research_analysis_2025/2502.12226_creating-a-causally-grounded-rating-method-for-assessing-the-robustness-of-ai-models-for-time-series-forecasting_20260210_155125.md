---
ver: rpa2
title: Creating a Causally Grounded Rating Method for Assessing the Robustness of
  AI Models for Time-Series Forecasting
arxiv_id: '2502.12226'
source_url: https://arxiv.org/abs/2502.12226
tags:
- robustness
- forecasting
- data
- time
- perturbations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a causally grounded rating framework to assess
  the robustness of AI models for time-series forecasting. The framework evaluates
  statistical and confounding biases under six input perturbations and twelve data
  distributions, covering both uni-modal and multi-modal models.
---

# Creating a Causally Grounded Rating Method for Assessing the Robustness of AI Models for Time-Series Forecasting

## Quick Facts
- arXiv ID: 2502.12226
- Source URL: https://arxiv.org/abs/2502.12226
- Reference count: 40
- Primary result: Multi-modal and time-series-specific Foundation Models show greater robustness than general-purpose models under causal robustness evaluation

## Executive Summary
This paper introduces a causally grounded rating framework to assess the robustness of AI models for time-series forecasting. The framework evaluates statistical and confounding biases under six input perturbations and twelve data distributions, covering both uni-modal and multi-modal models. Experiments with stock price data from six companies across three industries reveal that multi-modal and time-series-specific Foundation Models demonstrate greater robustness and accuracy compared to general-purpose models. A user study confirms the framework's usability, showing that ratings reduce difficulty in comparing model robustness. The findings support better decision-making for stakeholders, even in black-box settings.

## Method Summary
The framework evaluates model robustness using three metrics: Weighted Rejection Score (WRS) for statistical bias, Average Perturbation Effect (APE) for perturbation impact, and Propensity Score Matching Deconfounding Impact Estimation (PIE%) for confounding bias. Six perturbations target semantic, syntactic, and input-specific dimensions across numerical, visual, and spectrogram representations of time series data. The method uses PSM to estimate causal effects by matching treatment and control groups on confounder distributions, enabling robust comparison of model performance across sensitive attributes like industry and company.

## Key Results
- Multi-modal and time-series-specific Foundation Models (Chronos, MOMENT) outperform general-purpose models (Gemini-V, Phi-3) in both accuracy and robustness
- Industry-level confounding shows higher bias than company-level confounding across all models
- User study (n=26) shows robustness ratings significantly reduce comparison difficulty (p=0.03)
- P1 (drop-to-zero) and P6 (composite) perturbations show highest average perturbation effects

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Causal deconfounding isolates the true effect of perturbations on forecasting residuals from spurious correlations with sensitive attributes.
- **Mechanism:** Propensity Score Matching (PSM) creates matched treatment-control pairs where units share similar confounder distributions. By comparing residuals within matched pairs, the framework estimates the Average Perturbation Effect (APE) under the deconfounded distribution $R^{max}_t | do(P)$ rather than the observed distribution $R^{max}_t | P$. This blocks backdoor paths from perturbation to residual through confounders like company or industry.
- **Core assumption:** The causal graph (Figure 1a) correctly specifies that sensitive attribute $Z$ is a common cause of both perturbation $P$ and residual $R^{max}_t$, and that all confounders are observed and measurable.
- **Evidence anchors:**
  - [Section 3.1] Defines the causal model with backdoor paths and do-operator for intervention.
  - [Section 4.2.1] APE formula: $|E[R^{max}_t = j | do(P=i)] - E[R^{max}_t = j | do(P=0)]|$ and PIE% comparing pre/post PSM.
  - [Corpus] Weak/missing: Corpus neighbors focus on TSFM performance, not causal evaluation methods.
- **Break condition:** If unobserved confounders exist (e.g., market sentiment, macroeconomic events not captured in $Z$), PSM cannot eliminate bias, and APE estimates remain confounded.

### Mechanism 2
- **Claim:** Multi-perturbation stress testing across semantic, syntactic, and input-specific dimensions reveals differential robustness failure modes.
- **Mechanism:** Six perturbations target distinct vulnerability surfaces: P1 (drop-to-zero) and P2 (value-halved) test semantic integrity; P3 (missing values) tests syntactic handling; P4 (single-pixel) and P5 (saturation) test multi-modal image channel sensitivity; P6 (composite with sentiment analysis) tests cascading system failures. Each perturbation type probes a different aspect of the model's input processing pipeline.
- **Core assumption:** The selected perturbations are representative of real-world failure modes that stakeholders care about, and worst-case residual (max over prediction horizon) captures the failure severity users experience.
- **Evidence anchors:**
  - [Section 4.1] Detailed perturbation taxonomy with six types categorized by semantic/syntactic/input-specific/composite.
  - [Table 2] P1 showed highest APE (20.25 avg), P6 caused highest SMAPE/MASE degradation, confirming differential impact.
  - [Corpus] Weak: Neighboring papers discuss TSFM robustness but don't systematically categorize perturbation types.
- **Break condition:** If deployed models face perturbation types outside the six defined categories (e.g., temporal misalignment, distribution shift), robustness ratings may not generalize.

### Mechanism 3
- **Claim:** Weighted Rejection Score (WRS) quantifies statistical bias by aggregating hypothesis test rejections across confidence intervals.
- **Mechanism:** WRS performs pairwise t-tests comparing residual distributions across protected attribute groups (e.g., companies within an industry, industries overall). It weights rejections at 95%, 75%, and 60% confidence levels (weights 1.0, 0.8, 0.6) to emphasize stronger evidence. Higher WRS indicates the model produces statistically different error patterns across groups.
- **Core assumption:** Residual distributions are approximately normal (for t-test validity), and the protected attribute categories meaningfully partition the data for fairness analysis.
- **Evidence anchors:**
  - [Section 4.2.1] Equation 1 defines WRS as weighted sum of rejections across CIs.
  - [Section 5.2 RQ1] Results show inter-industry WRS higher than intra-industry; $S_a$ had lowest WRS (3.96), $S_p$ highest (6.27).
  - [Corpus] Not addressed in corpus neighbors.
- **Break condition:** With small sample sizes per group or highly skewed residuals, t-test assumptions fail, making WRS unreliable.

## Foundational Learning

- **Concept: Do-calculus and causal intervention**
  - **Why needed here:** The framework distinguishes observational correlations from causal effects using the do-operator. Understanding $E[Y | do(X)]$ vs $E[Y | X]$ is essential to interpret APE and PIE metrics.
  - **Quick check question:** If company stock prices are correlated with both perturbation intensity and residuals, does observing this correlation tell you the perturbation's causal effect?

- **Concept: Propensity Score Matching (PSM)**
  - **Why needed here:** PSM is the deconfounding technique underlying PIE% computation. You must understand how matching creates synthetic control groups.
  - **Quick check question:** Why match on propensity scores rather than directly on confounder values when confounders are multi-dimensional?

- **Concept: Foundation model architectures for time series**
  - **Why needed here:** The paper compares encoder-only (MOMENT), encoder-decoder (Chronos, Phi-3), and decoder-only (Gemini) architectures. Architecture choice influences both accuracy and robustness.
  - **Quick check question:** Why might a model pre-trained on text (decoder-only LLM) behave differently under input perturbations than one trained from scratch on time series?

## Architecture Onboarding

- **Component map:**
  Data Layer -> Model Layer -> Analysis Layer
  Yahoo Finance data -> 11 TSFM systems -> WRS, PIE%, APE, ratings

- **Critical path:** Perturbation injection -> model inference -> residual aggregation -> PSM matching -> causal effect estimation -> rating assignment. The PSM step is computationally sensitive; poor matches inflate PIE% variance.

- **Design tradeoffs:**
  - Rating levels (L): Higher L gives finer-grained distinctions but requires more samples per group. Paper uses L=3.
  - Max residual vs mean residual: Max captures worst-case (stakeholder-relevant) but is noisier than mean.
  - Confounder choice: Industry-level captures sector effects; company-level captures firm-specific effects. Industry showed higher bias in experiments.

- **Failure signatures:**
  - PIE% near 0: Either no confounding exists, or PSM failed to find adequate matches (check balance diagnostics).
  - Ratings identical across models: Input perturbations may be too weak or models fundamentally similar.
  - High WRS with low APE: Statistical bias exists but perturbations don't causally affect outcomes (sensitive attribute operates through other pathways).

- **First 3 experiments:**
  1. **Validate perturbation severity:** Run P1 (drop-to-zero) vs P0 on a single model. Confirm APE > 0 and residuals visibly increase. This establishes the intervention has measurable effect.
  2. **Test PSM balance:** For a chosen confounder (e.g., company), compute standardized mean differences before/after matching. If post-match balance > 0.1, matching quality is insufficient.
  3. **Compare architecture families:** Run all models on P1 and P6. Verify that time-series-specific FMs (Chronos, MOMENT) outperform general-purpose (Gemini-V, Phi-3) on APE, as claimed in Table 2. If not, check tokenization/prompting consistency.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do confounders like seasonal trends and financial news affect the robustness ratings of time-series forecasting models?
- **Basis in paper:** [explicit] The conclusion explicitly states, "In future, we plan to study confounders like seasonal trends and financial news," noting that the current work focused only on "Industry" and "Company."
- **Why unresolved:** The proposed causal model (Fig. 1) currently treats "Industry" and "Company" as the sole sensitive attributes/confounders. The impact of temporal patterns or external unstructured data (news) on the calculated APE and PIE scores remains unquantified.
- **What evidence would resolve it:** Experimental results incorporating temporal seasonality variables and NLP-based news sentiment into the causal model, comparing the resulting ratings against the current baseline.

### Open Question 2
- **Question:** Do the usability benefits of the robustness ratings hold up in a large-scale user study?
- **Basis in paper:** [explicit] The authors state in Section 6, "Our current study is preliminary and promising; an avenue for future work is to conduct it at a larger scale."
- **Why unresolved:** The initial study (n=26) showed mixed results; while ratings reduced perceived difficulty for robustness, they did not significantly reduce it for fairness, and correlations varied by perturbation type (e.g., strong for P1, weak for P2).
- **What evidence would resolve it:** A replication of the user study with a significantly larger and more diverse participant pool to validate the statistical significance of the reduction in comparison difficulty.

### Open Question 3
- **Question:** Can the proposed rating framework be effectively generalized to non-financial time-series domains?
- **Basis in paper:** [inferred] The Introduction lists healthcare, manufacturing, and weather as key application domains for time-series forecasting, but Section 5.1.3 limits the experimental evaluation exclusively to stock price data from six companies.
- **Why unresolved:** It is unclear if the defined perturbations (e.g., "Drop-to-zero" or "Value halved") and the specific confounding effects of "Industry" translate meaningfully to data types like patient vitals or sensor logs.
- **What evidence would resolve it:** Application of the framework to datasets from healthcare or manufacturing, demonstrating that the metrics (WRS, PIE%) successfully differentiate model robustness in those contexts.

## Limitations

- Confounder completeness: Framework assumes all confounding variables are captured in industry/company attributes; unmeasured confounders could bias APE estimates.
- Perturbation generality: Six perturbations may not cover all real-world failure modes; models robust to tested perturbations might fail under novel attacks.
- Sample size constraints: Limited to six companies across three industries may not provide sufficient statistical power for reliable WRS calculations.

## Confidence

- **High Confidence**: The rating framework's usability (user study) and differential performance of TSFM vs general-purpose models.
- **Medium Confidence**: Causal deconfounding mechanism (PSM implementation details unspecified), PIE% as confounder measure.
- **Low Confidence**: Generalizability of perturbation types and their coverage of real-world scenarios.

## Next Checks

1. **PSM Balance Verification**: Implement and verify propensity score matching balance diagnostics. Ensure standardized mean differences < 0.1 post-matching for all confounders.
2. **Perturbation Sensitivity Analysis**: Systematically vary perturbation frequency (n) and severity. Confirm APE remains stable across reasonable parameter ranges.
3. **Cross-Industry Validation**: Test framework on additional industries (healthcare, retail, energy) to verify robustness ratings generalize beyond finance sector.