---
ver: rpa2
title: 'Structural Plasticity as Active Inference: A Biologically-Inspired Architecture
  for Homeostatic Control'
arxiv_id: '2511.02241'
source_url: https://arxiv.org/abs/2511.02241
tags:
- cells
- learning
- plasticity
- network
- cell
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "SAPIN introduces a biologically-inspired neural architecture that\
  \ integrates two forms of plasticity\u2014local synaptic learning and global structural\
  \ adaptation\u2014based on active inference principles. Operating on a 2D grid,\
  \ the model learns through local prediction error minimization, with cells physically\
  \ migrating to optimize information-receptive fields."
---

# Structural Plasticity as Active Inference: A Biologically-Inspired Architecture for Homeostatic Control

## Quick Facts
- arXiv ID: 2511.02241
- Source URL: https://arxiv.org/abs/2511.02241
- Reference count: 8
- One-line primary result: A biologically-inspired neural network using local synaptic and global structural plasticity discovers stable balancing policies without external reward signals.

## Executive Summary
SAPIN introduces a biologically-inspired neural architecture that integrates two forms of plasticity—local synaptic learning and global structural adaptation—based on active inference principles. Operating on a 2D grid, the model learns through local prediction error minimization, with cells physically migrating to optimize information-receptive fields. Tested on the CartPole benchmark, SAPIN successfully discovered stable balancing policies without external reward signals. However, continual learning led to instability, which was resolved by locking network parameters after achieving success, resulting in an 82% success rate over 100 episodes (averaged across 100 agents). The work demonstrates that homeostasis-seeking behavior and prediction error minimization can suffice for control tasks, while highlighting structural plasticity as a viable mechanism for adaptive computation. Future work will explore more complex tasks and continuous spatial dynamics.

## Method Summary
SAPIN is a biologically-inspired neural network that operates on a 9×9 grid with 4 fixed input cells, 30 dynamic processing cells, and 2 fixed output cells. The model learns through local prediction error minimization and global structural adaptation. Processing cells maintain directional strengths and expectations, updating via a Hebbian-like rule based on the difference between actual activation and learned expectation. After every four episodes, cells with high prediction error migrate to optimize their information-receptive fields. The network achieves control by minimizing surprise rather than maximizing external rewards. Parameters are locked after the first successful episode to prevent instability during continual learning.

## Key Results
- SAPIN achieved 82% success rate over 100 episodes (averaged across 100 agents) after locking network parameters post-success.
- Structural plasticity enabled the network to discover stable balancing policies without external reward signals.
- Continual learning led to instability, resolved by locking parameters after achieving success.
- Punishment signals had no significant effect on learning outcomes.

## Why This Works (Mechanism)

### Mechanism 1: Local Prediction Error Minimization Drives Synaptic Updates
- Claim: Local synaptic plasticity, driven by the difference between actual activation and learned expectation, is sufficient to discover control policies for homeostatic tasks.
- Mechanism: Each processing cell maintains directional strengths (s) and an expectation (E). After each action, the prediction error (V - E) updates both E and s via a Hebbian-like rule (η = 0.02). The directional proportions (p_i = v_i / Σ|v_i|) weight how the error modulates each directional strength.
- Core assumption: Prediction error carries enough signal for credit assignment without global backpropagation.
- Evidence anchors:
  - [abstract] "a local, Hebbian-like synaptic plasticity rule based on the temporal difference between a cell's actual activation and its learned expectation"
  - [section: Synaptic Plasticity] Algorithm 2 shows error_i ← V_i - E_i, with updates to both E_i and s_i scaled by error_i
  - [corpus] Neighbor paper "Spiking Decision Transformers" confirms local plasticity can enable sequence control, but SAPIN specifically shows this for continuous homeostatic control without rewards.
- Break condition: If prediction errors become uninformative (e.g., highly stochastic environments with weak state-action correlations), the gradient signal degrades and learning stalls.

### Mechanism 2: Structural Plasticity Optimizes Information-Receptive Fields
- Claim: Allowing cells to physically migrate based on chronic prediction error improves the network's ability to process relevant information.
- Mechanism: After every macro-episode (4 environment episodes), cells with long-term average prediction error |V̄_i - E_i| above threshold (θ_D = 0.1) become movement candidates. Movement direction follows the axis of highest accumulated directional influx (v̄_i). Over-activated cells move away from signal sources; under-activated cells move toward them. Small ε_rand = 0.05 exploration prevents local optima.
- Core assumption: Physical repositioning can reduce prediction error more effectively than weight updates alone, and grid topology permits meaningful spatial relationships.
- Evidence anchors:
  - [abstract] "cells physically migrate across the grid to optimize their information-receptive fields"
  - [section: Structural Plasticity] Algorithm 3 details movement logic: desire D_i = |V̄_i - E_i|, direction via weighted choice on |v̄_i|, with movement toward/away based on over/under-activation
  - [corpus] "Self-Motivated Growing Neural Network" paper supports structural plasticity for adaptive architecture, but SAPIN uniquely ties movement to prediction error rather than reward signals.
- Break condition: If the grid is too small or crowded (30 cells on 9×9 = 37% density), collision blocking prevents optimal positioning; poor initial positions can prevent success for 100+ episodes.

### Mechanism 3: Intrinsic Homeostatic Drive Eliminates Need for External Reward
- Claim: For homeostatic control tasks, minimizing prediction error alone suffices; punishment signals provide no additional benefit.
- Mechanism: The network learns to seek states where sensory input is predictable (balanced pole = stable state stream). Dropping the pole produces chaotic, unpredictable sensory states. The network discovers the success state because it minimizes surprise—not because of explicit reward shaping.
- Core assumption: The task's success state correlates with sensory predictability; this may not hold for tasks requiring deliberate destabilization.
- Evidence anchors:
  - [abstract] "The network's intrinsic drive to minimize prediction error and maintain homeostasis was sufficient to discover a stable balancing policy"
  - [section: The Surprising Role of Punishment] "all three conditions produced very similar results. The network successfully learned to balance the pole even when it was never punished for failing."
  - [corpus] Weak direct corpus evidence—neighbor papers focus on reward-based or emotion-inspired signals; SAPIN's reward-free approach is distinct and needs broader validation.
- Break condition: Tasks requiring long-term planning or deliberately leaving stable states (e.g., exploring to find distant goals) would likely fail under immediate prediction error minimization alone.

## Foundational Learning

- **Concept: Active Inference and Free Energy Principle**
  - Why needed here: SAPIN's entire objective function derives from minimizing variational free energy (prediction error/surprise). Understanding that perception, learning, and action are cast as inference processes is essential.
  - Quick check question: Can you explain why an agent would act to make sensations conform to predictions rather than maximize reward?

- **Concept: Local vs. Global Credit Assignment**
  - Why needed here: The architecture explicitly rejects backpropagation. Grasping why weight transport and non-local error signals are biologically implausible clarifies why SAPIN's local Hebbian-variant matters.
  - Quick check question: What information does a SAPIN synapse have access to during an update, and what does it lack compared to backpropagation?

- **Concept: Homeostasis as Control Objective**
  - Why needed here: CartPole is framed as a homeostatic problem (maintaining upright state). Understanding that stability-seeking behavior emerges from prediction error minimization—not reward maximization—is the core insight.
  - Quick check question: Why would minimizing surprise cause an agent to keep a pole balanced?

## Architecture Onboarding

- **Component map:**
  - Grid: 9×9, 81 positions
  - Input Cells (I): 4 fixed cells at (0,1), (0,3), (0,5), (0,7) — receive normalized 4D state
  - Processing Cells (P): 30 dynamic cells with LTM (s_i, E_i), STM (v_saved, V_saved), immediate state (v_i, V_i)
  - Output Cells (O): 2 fixed cells at (8,2), (8,6) — uniform directional strengths [0.25,0.25,0.25,0.25]
  - Signal propagation: winner-takes-all activation order, distance decay D(d), angular weighting W_ang

- **Critical path:**
  1. Environment state → normalized to [-1,1] → seeds input cells
  2. Propagation wave (Algorithm 1): highest |V| cell propagates, repeat until all cells activated
  3. Output comparison: V_o0 vs V_o1 determines action
  4. Synaptic update (Algorithm 2): local prediction error updates E_i and s_i
  5. After 4 episodes: structural plasticity (Algorithm 3) moves high-desire cells

- **Design tradeoffs:**
  - Grid size vs. cell count: 30 cells on 9×9 creates collision risk; larger grid reduces crowding but dilutes signal propagation
  - Locking vs. continual learning: Locking achieves 82% stability but forfeits adaptation; continual learning finds policies but loses them
  - Distance decay function: D(0)=1.0, D(1)=0.75, D(2)=0.25, D(≥3)=0 creates hard connectivity limits—tunable but arbitrary

- **Failure signatures:**
  - Early instability: Success at episode 5, failure at episode 6 (policy forgotten before lock)
  - Collision blocking: High-desire cells cannot move; Occupied set prevents repositioning
  - Poor initialization: Random positions may isolate cells from information flow; can require 100+ episodes to recover
  - Punishment ineffectiveness: Added noise signal does not shift policy—expected if surprise minimization already drives learning

- **First 3 experiments:**
  1. **Reproduce locking result**: Train 10 agents to 500-step success, lock parameters, evaluate 100 episodes each. Confirm ~82% success rate matches paper. Log which failure modes occur in the ~18% failures.
  2. **Ablate structural plasticity**: Disable Algorithm 3 (set cell movement to zero). Compare convergence speed and final success rate vs. full architecture. Isolate contribution of topology optimization.
  3. **Test punishment sensitivity**: Run three conditions (catastrophic-only, probabilistic, none) with 20 agents each. Verify paper's claim that punishment has no effect—and inspect whether this holds across random seeds or is an artifact of specific configurations.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why did the implemented punishment signals fail to modulate learning, and how can external error signals be integrated effectively?
- **Basis in paper:** [explicit] The authors state, "Future work should investigate why this signal was ineffective," noting that local homeostatic updates may have overshadowed the random punishment waves.
- **Why unresolved:** The paper confirms the failure but only hypothesizes causes (e.g., signal noise vs. local update dominance) without verifying the mechanism.
- **What evidence would resolve it:** Ablation studies comparing the magnitude and temporal structure of punishment signals against the learning rate of synaptic plasticity rules.

### Open Question 2
- **Question:** Can the architecture adapt to tasks requiring the violation of immediate homeostasis to achieve long-term goals?
- **Basis in paper:** [explicit] The discussion highlights a limitation: the model is suited for homeostasis, but "this raises questions about the model's ability to solve tasks that require... deliberately moving away from a stable state."
- **Why unresolved:** The current objective function minimizes immediate prediction error, inherently discouraging the agent from entering high-surprise states necessary for exploration or complex planning.
- **What evidence would resolve it:** Evaluation on tasks requiring "deep active inference" or delayed gratification, where agents must temporarily maximize surprise to minimize it later.

### Open Question 3
- **Question:** Does transitioning from a discrete grid to a continuous spatial substrate resolve the instability issues observed during continual learning?
- **Basis in paper:** [explicit] The paper notes that "continual learning led to instability" and suggests that "implementing a continuous rather than discrete system... will decrease the step size... greatly stabilizing the model."
- **Why unresolved:** The discrete nature of the current 2D grid causes abrupt topological changes, necessitating an artificial "locking" mechanism to maintain performance.
- **What evidence would resolve it:** Implementation of a continuous movement space demonstrating stable retention of policies without the need for the global boolean locking flag.

## Limitations
- Heavy dependence on random initialization of processing cell positions, with poor initialization leading to extended failure periods (>100 episodes).
- Weak evidence for punishment ineffectiveness—only one experiment condition tested with no sensitivity analysis.
- CartPole benchmark is low-dimensional and may not generalize to tasks requiring deliberate destabilization or exploration.
- Artificial "locking" mechanism required to maintain performance during continual learning, indicating potential instability.

## Confidence

- **High confidence**: Local prediction error minimization drives synaptic updates; the mechanism is well-specified and directly observable in the code.
- **Medium confidence**: Structural plasticity improves receptive fields; the algorithm is clear but effectiveness depends heavily on initialization and grid dynamics.
- **Low confidence**: Punishment has no effect on learning; only one experiment condition is tested, and no sensitivity analysis is provided.

## Next Checks

1. **Reproduce locking result with ablation**: Train 10 agents to 500-step success, lock parameters, evaluate 100 episodes each. Log failure modes in the ~18% failures and compare against paper.

2. **Isolate structural plasticity contribution**: Run two conditions—(a) full SAPIN, (b) disable Algorithm 3 (no cell movement). Compare convergence speed and final success rates to quantify the impact of topology optimization.

3. **Expand punishment sensitivity analysis**: Test three punishment conditions (catastrophic-only, probabilistic, none) with 20 agents each. Verify punishment ineffectiveness across random seeds and inspect whether this holds for different task stochasticities.