---
ver: rpa2
title: 'AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control
  Tokens'
arxiv_id: '2512.17375'
source_url: https://arxiv.org/abs/2512.17375
tags:
- assistant
- header
- start
- answer
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We show that reward models and LLM-as-a-Judge systems are vulnerable
  to short, low-perplexity control-token sequences that can flip binary evaluations
  from correct "No" judgments to incorrect "Yes" judgments. These tokens are model-intrinsic
  directions in the last-layer representation space, not worst-case adversarial strings.
---

# AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens

## Quick Facts
- arXiv ID: 2512.17375
- Source URL: https://arxiv.org/abs/2512.17375
- Reference count: 40
- Primary result: Binary LLM-as-a-Judge systems can be flipped from correct "No" judgments to incorrect "Yes" judgments using short, low-perplexity control-token sequences discovered via beam search on the model's own next-token distribution.

## Executive Summary
This paper demonstrates that reward models and LLM-as-a-Judge systems are vulnerable to short, low-perplexity control-token sequences that can flip binary evaluations from correct "No" judgments to incorrect "Yes" judgments. These tokens are model-intrinsic directions in the last-layer representation space, not worst-case adversarial strings. The method, AdvJudge-Zero, discovers such tokens by exploring the model's next-token distribution using beam search, without requiring any seed patterns. Across multiple model families and reasoning benchmarks, these tokens cause very high false positive rates—up to 99.91%—in judging incorrect answers as correct. Adversarial training with small sets of control-token-augmented examples can significantly reduce false positives while preserving evaluation quality.

## Method Summary
AdvJudge-Zero discovers control tokens by first generating candidate sequences through beam search over the model's next-token distribution (starting with k=300, gradually reduced), then verifying which sequences flip the binary decision from "No" to "Yes" across multiple prompts. The tokens are model-intrinsic directions in the last-layer representation space, anti-aligned with the refusal direction. The method uses the model's own next-token distribution to propose candidates and selects tokens that maximize generalization across prompts (duplication count) and flip strength (negative logit gap). Geometric analysis reveals these perturbations are low-rank, concentrated in the first principal component. Adversarial training with LoRA fine-tuning on control-token-augmented examples significantly reduces false positives.

## Key Results
- Control tokens discovered via beam search cause up to 99.91% false positive rates in judging incorrect answers as correct
- Discovered perturbations are low-rank (PC1 explains 28-35% of variance) and anti-aligned with the refusal direction
- Adversarial training with small sets of control-token-augmented examples can significantly reduce false positives while preserving evaluation quality
- Tokens discovered on one model partially transfer to others, but specialized judges show varying robustness

## Why This Works (Mechanism)

### Mechanism 1
Binary judge decisions are governed by a shallow, linear gate at the final layer readout, making them susceptible to small perturbations in the last-layer hidden state. The decision function F(h) = z_No - z_Yes ≈ (w_No - w_Yes)^T h + b defines a hyperplane in representation space. When control tokens perturb the hidden state h → h + Δh such that w_F^T Δh < -F(h_clean), the sign of F flips and the decision changes from "No" to "Yes."

### Mechanism 2
Effective control tokens exploit a low-rank "soft mode" in representation space—systematically anti-aligned with the refusal direction—rather than acting as random noise. PCA analysis shows the first principal component explains 28–35% of perturbation variance (vs. ~0.03% for isotropic noise). The mean perturbation vector has statistically significant negative cosine similarity with w_F (Z-scores of -7.47 and -4.80 for tested models).

### Mechanism 3
Control tokens discovered via beam search on the model's own next-token distribution are both effective (high flip rates) and realistic (low perplexity, plausible during RL post-training). AdvJudge-Zero generates candidate sequences by sampling from the model's next-token distribution using beam search (large initial k, decreasing with length), then verifies which sequences flip F < 0 across multiple prompts.

## Foundational Learning

- **Logit gap as a decision boundary**
  - Why needed here: Understanding F = z_No - z_Yes is essential to grasp how small hidden-state changes flip binary judgments
  - Quick check question: If w_F is the refusal direction vector and a perturbation Δh has positive cosine similarity with w_F, will the decision move toward "Yes" or "No"?

- **Low-rank structure in high-dimensional spaces**
  - Why needed here: Explains why a small set of token patterns can reliably exploit a shared vulnerable direction across diverse inputs
  - Quick check question: In a 4096-dimensional space, what percentage of variance would PC1 explain for isotropic random vectors? How does this compare to the 34.57% observed?

- **Beam search with scheduled k**
  - Why needed here: AdvJudge-Zero uses this to explore the token space efficiently while focusing computation on promising candidates
  - Quick check question: Why would you use a large k for early tokens and decrease it for later tokens in the sequence?

## Architecture Onboarding

- **Component map:**
  Generation prompt -> Beam search module -> Verification prompt -> Ranking/aggregation -> (Optional) Adversarial training

- **Critical path:**
  1. Sample generation prompts from dataset
  2. Run beam search to produce candidate sequences (length 1–n)
  3. Insert each candidate into verification prompts
  4. Compute logit gap F; record flips (F < 0)
  5. Aggregate statistics; rank by duplication count and logit gap
  6. (Optional) Use top-ranked tokens for adversarial training

- **Design tradeoffs:**
  - Beam width vs. diversity: Larger initial k finds more diverse tokens but increases compute; the paper uses k ≈ 300 initially
  - Sequence length vs. semantic content: Longer sequences don't monotonically improve FPR; semantic composition matters more
  - Discovery model vs. target model: Tokens discovered on one model transfer partially to others; specialized judges show varying robustness (Omni-Judge highly vulnerable, general-verifier robust)

- **Failure signatures:**
  - Near-zero FPR improvement: Indicates target model has different vulnerability structure (e.g., general-verifier)
  - High-perplexity tokens selected: Beam search may be misconfigured or generation prompt not inducing solution-mode tokens
  - Adversarial training degrades TPR: Overfitting to control-token patterns; reduce training proportion or increase dataset diversity

- **First 3 experiments:**
  1. Reproduce geometric analysis: Extract hidden states for clean vs. adversarial prompts on a single model (e.g., Qwen2.5-7B), run PCA, verify PC1 variance and alignment Z-score match reported values
  2. Token-length ablation: For one model-dataset pair, measure FPR as a function of sequence length n (1–7) to confirm non-monotonic behavior and identify optimal lengths
  3. Cross-model transfer test: Discover tokens on Qwen2.5-7B, evaluate FPR on Omni-Judge and general-verifier to observe the robustness differential reported in Table 3

## Open Questions the Paper Calls Out
None

## Limitations
- Structural generalizability: The vulnerability relies on current judge architectures maintaining a shallow linear gate; evolving designs may invalidate this premise
- Token transfer and specialization: Model-specific training objectives may implicitly harden against attacks; effectiveness across different judge architectures remains unclear
- Real-world emergence: Measured FPR assumes worst-case generation; actual probability of control-token emergence during RLHF may be lower

## Confidence
- High confidence: Existence of control-token vulnerabilities and core geometric analysis (linear gate mechanism, low-rank structure, beam-search discovery)
- Medium confidence: Adversarial training mitigation effectiveness (limited model diversity tested, long-term robustness unproven)
- Low confidence: Real-world risk quantification (worst-case vs. practical risk, safety mitigation effectiveness, deployment tradeoffs)

## Next Checks
1. **Geometric analysis replication**: Extract last-layer hidden states from clean vs. adversarial prompts on a single model (e.g., Qwen2.5-7B), perform PCA, and verify PC1 variance (target: ~34%) and alignment Z-score (target: <-4) match reported values
2. **Cross-model robustness assessment**: Discover control tokens on Qwen2.5-7B, then systematically evaluate FPR across the full judge spectrum (Omni-Judge, general-verifier, Qwen2.5-RLVR, Master-RM) to confirm the reported differential vulnerability pattern
3. **Beam-search parameter sensitivity**: Run AdvJudge-Zero with varying TOP_K_SCHEDULE configurations (e.g., linear vs. exponential decay, different initial k values) and measure impact on token diversity, FPR, and computational cost