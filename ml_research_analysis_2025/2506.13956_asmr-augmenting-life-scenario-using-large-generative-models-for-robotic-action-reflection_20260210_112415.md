---
ver: rpa2
title: 'ASMR: Augmenting Life Scenario using Large Generative Models for Robotic Action
  Reflection'
arxiv_id: '2506.13956'
source_url: https://arxiv.org/abs/2506.13956
tags:
- augmentation
- language
- data
- diffusion
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of limited training data for multimodal
  robotic action prediction by introducing an augmentation framework that combines
  large language models (LLMs) and diffusion models to generate synthetic dialogues
  and environmental images. The method features place-based and action-based augmentation
  pipelines, using GPT-3.5 to simulate human-robot interactions and stable diffusion
  models to visualize environments.
---

# ASMR: Augmenting Life Scenario using Large Generative Models for Robotic Action Reflection

## Quick Facts
- arXiv ID: 2506.13956
- Source URL: https://arxiv.org/abs/2506.13956
- Reference count: 40
- Primary result: Data augmentation framework combining LLMs and diffusion models improves zero-shot robotic action prediction accuracy up to 48.8% on Do-I-Demand dataset

## Executive Summary
This paper addresses the challenge of limited training data for multimodal robotic action prediction by introducing an augmentation framework that combines large language models (LLMs) and diffusion models to generate synthetic dialogues and environmental images. The method features place-based and action-based augmentation pipelines, using GPT-3.5 to simulate human-robot interactions and stable diffusion models to visualize environments. Experiments on the Do-I-Demand dataset show that this approach significantly improves LLaVA's zero-shot accuracy, achieving up to 48.8% accuracy when combining both augmentation methods with SBERT encoding.

## Method Summary
The framework employs two augmentation strategies: place-based augmentation generates dialogues for 10 predefined locations using GPT-3.5 and creates corresponding first-person perspective images using Stable-Diffusion-XL; action-based augmentation generates dialogues for 43 specific robot actions using GPT-3.5 and creates images using BLIP Diffusion conditioned on real reference images. The augmented data is used to fine-tune LLaVA-7B/13B with LoRA for 5 epochs. Zero-shot accuracy is evaluated by encoding model outputs and action labels with SBERT and selecting the action with highest cosine similarity.

## Key Results
- Combined augmentation methods achieve 48.8% zero-shot accuracy on Do-I-Demand dataset
- Action-based augmentation outperforms place-based augmentation (43.9% vs 35.1%)
- BLIP Diffusion is critical for action-based image generation (6% drop when using Stable-Diffusion-XL)
- Augmentation particularly improves performance on low-performing action labels

## Why This Works (Mechanism)
The framework addresses the data scarcity problem in robotic action prediction by generating synthetic multimodal training examples that capture the relationship between user requests, environmental context, and appropriate robot actions. By combining LLM-generated dialogues with contextually relevant images, the model learns to better associate ambiguous user requests with appropriate actions in specific environmental settings.

## Foundational Learning
- **Multimodal classification**: Predicting robot actions from user requests and images; needed because real-world scenarios have ambiguous requests requiring environmental context; quick check: verify the 43 action labels cover diverse household tasks
- **Zero-shot evaluation**: Testing model performance on unseen data using similarity matching; needed because it measures generalization capability; quick check: confirm SBERT embeddings capture semantic similarity between actions
- **Diffusion models for image generation**: Creating synthetic environmental images conditioned on text prompts; needed to provide visual context for action prediction; quick check: verify generated images show first-person perspective of requested actions
- **LLM dialogue generation**: Creating realistic human-robot interaction examples; needed to provide diverse request patterns; quick check: confirm generated dialogues match the style of real human requests
- **LoRA fine-tuning**: Efficient adaptation of large multimodal models; needed to scale to augmented dataset; quick check: verify training converges within 5 epochs

## Architecture Onboarding

**Component Map:**
GPT-3.5 (dialogue generation) -> Stable-Diffusion-XL/BLIP Diffusion (image generation) -> LLaVA fine-tuning (action prediction) -> SBERT similarity matching (evaluation)

**Critical Path:**
1. Generate synthetic dialogues using GPT-3.5 for each place/action category
2. Create corresponding images using appropriate diffusion model
3. Fine-tune LLaVA with LoRA on augmented dataset
4. Evaluate using zero-shot similarity matching with SBERT

**Design Tradeoffs:**
- Choice of BLIP Diffusion vs Stable-Diffusion-XL for action-based augmentation (BLIP performs better)
- First-person perspective images vs generic environmental images (first-person provides better context)
- Zero-shot evaluation vs supervised fine-tuning (zero-shot better measures generalization)

**Failure Signatures:**
- Low accuracy improvement from augmentation → check action label distribution and dialogue relevance
- Performance drop when using wrong diffusion model → verify correct model per augmentation type
- Overfitting to synthetic data → ensure diversity in generated dialogues and images

**First Experiments:**
1. Generate place-based augmentation data for 5 locations and evaluate accuracy improvement
2. Generate action-based augmentation data for 5 actions and compare diffusion model performance
3. Combine both augmentation methods and measure zero-shot accuracy on Do-I-Demand

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the methodology and results, several important questions remain:

- Can the augmentation framework generalize to other robotic assistance domains beyond the Do-I-Demand dataset and its 43 action labels?
- What is the sim-to-real gap when using diffusion-generated images versus actual robot-captured environmental images?
- How does the framework perform with larger or more diverse multimodal base models beyond LLaVA?
- Does LLM-generated dialogue data introduce systematic biases or hallucinated scenarios that could harm robot behavior?

## Limitations
- Exact 43 action labels and 10 place categories are not enumerated, limiting precise replication
- BLIP Diffusion reference image selection methodology from real data is not detailed
- LoRA training hyperparameters beyond basic configuration are unspecified
- First-person perspective prompt format for Stable-Diffusion-XL is described but not provided verbatim

## Confidence
- High confidence in the overall augmentation framework and zero-shot evaluation methodology
- Medium confidence in the specific performance numbers due to missing implementation details
- High confidence in the ablation results showing BLIP Diffusion superiority for action-based augmentation

## Next Checks
1. Verify the exact 43 action labels and 10 place categories from the Do-I-Demand dataset and confirm the prompt templates used with GPT-3.5 for dialogue generation
2. Reproduce the BLIP Diffusion vs. Stable-Diffusion-XL ablation study to confirm the 6% accuracy drop when using the wrong model for action-based augmentation
3. Test the zero-shot accuracy on Do-I-Demand using SBERT similarity matching with the provided augmentation data to validate the 48.8% combined method performance claim