---
ver: rpa2
title: Effects of Feature Correlations on Associative Memory Capacity
arxiv_id: '2508.01395'
source_url: https://arxiv.org/abs/2508.01395
tags:
- capacity
- memory
- separation
- data
- associative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how feature correlations affect the capacity\
  \ of Dense Associative Memory (DAM), a Hopfield network variant, in practical machine\
  \ learning settings where data structures deviate from theoretical assumptions.\
  \ To isolate the role of feature correlations, the authors develop an empirical\
  \ framework that constructs datasets with controlled average Hamming distances and\
  \ systematically measures DAM\u2019s memory capacity using a binary search algorithm."
---

# Effects of Feature Correlations on Associative Memory Capacity

## Quick Facts
- arXiv ID: 2508.01395
- Source URL: https://arxiv.org/abs/2508.01395
- Reference count: 12
- Primary result: Feature correlations reduce Dense Associative Memory capacity slightly at constant separation, with this effect amplified at higher polynomial degrees in the energy function.

## Executive Summary
This paper investigates how feature correlations affect the capacity of Dense Associative Memory (DAM), a Hopfield network variant, in practical machine learning settings where data structures deviate from theoretical assumptions. To isolate the role of feature correlations, the authors develop an empirical framework that constructs datasets with controlled average Hamming distances and systematically measures DAM's memory capacity using a binary search algorithm. Their experiments show that memory capacity scales exponentially with pattern separation, confirming theoretical predictions, but also reveal that feature-correlated data (e.g., MNIST) exhibits slightly lower capacity than synthetic independent data at the same separation, with this gap widening at higher polynomial degrees in the DAM's energy function. This suggests that higher-order interactions between features are more limited in correlated data, offering insights into capacity constraints relevant to practical applications such as Transformers and LLMs.

## Method Summary
The authors measure DAM capacity by constructing controlled datasets with varying feature correlations and pattern separations. They generate synthetic binary patterns with biased Rademacher distributions and binarized MNIST subsets, using greedy selection to achieve target average Hamming distances. For each configuration, they measure capacity Kmax (largest K with perfect retrieval) via binary search across polynomial degrees n=6 to 38. The DAM uses a rectified polynomial energy function with one asynchronous update for retrieval, and the framework excludes cases where Kmax exceeds 49 due to subset size limits.

## Key Results
- Memory capacity scales exponentially with increasing pattern separation, confirming theoretical predictions.
- Feature-correlated data (MNIST) exhibits lower capacity than synthetic independent data at the same separation.
- The capacity penalty from feature correlations increases at higher polynomial degrees in the DAM energy function.
- Greedy selection successfully creates subsets with controlled mean separation, but MNIST data structure limits HD range to 30-190.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Memory capacity scales exponentially with pattern separation (Hamming distance).
- Mechanism: Higher average pairwise Hamming distance reduces crosstalk between stored patterns. When patterns are sufficiently separated, the probability of retrieval error drops dramatically because overlapping bit positions—which cause interference during the energy minimization retrieval process—are minimized.
- Core assumption: Retrieval is tested via one asynchronous update from the pattern itself (P_error = 0 regime), not from corrupted inputs.
- Evidence anchors:
  - [abstract] "Our experiments confirm that memory capacity scales exponentially with increasing separation in the input space."
  - [section] Appendix A.1: "When patterns are sufficiently separated, the chance of retrieval error drops dramatically due to lack of crosstalk."
  - [corpus] Hu et al. (2024) theoretical proposition cited: average separation scales in O(ln M) where M is capacity—this is the inverse relationship the paper empirically confirms.
- Break condition: Assumption fails if patterns are not approximately uniformly distributed or if retrieval must succeed from heavily corrupted cues (non-zero error tolerance).

### Mechanism 2
- Claim: Feature correlations reduce capacity at fixed separation, independently of average Hamming distance.
- Mechanism: Even when average separation is controlled via greedy selection, correlated data (e.g., MNIST) has structural dependencies between features that reduce the effective number of distinguishable configurations. The bits are not independent, so the "true" information-theoretic capacity is lower than Hamming distance alone predicts.
- Core assumption: Hamming distance captures inter-pattern separation but not intra-pattern feature dependencies.
- Evidence anchors:
  - [abstract] "Feature correlations do not alter this relationship fundamentally, but reduce capacity slightly at constant separation."
  - [section] Figure 1: "synthetic data achieves consistently higher Kmax for a given separation than MNIST."
  - [corpus] Weak direct evidence—corpus papers focus on capacity bounds and energy functions, not feature correlation effects specifically.
- Break condition: Assumption: the greedy selection procedure successfully equalizes mean separation across datasets. If selection introduces bias, the comparison may conflate other factors.

### Mechanism 3
- Claim: Higher polynomial degree (n) in the DAM energy function amplifies the capacity penalty from feature correlations.
- Mechanism: The rectified polynomial energy function with degree n introduces n-th order interactions between features. In correlated data, higher-order co-occurrence structure is more constrained (fewer independent feature combinations), so increasing n exposes this limitation more sharply. Independent synthetic data has richer higher-order structure by construction.
- Core assumption: The energy function's polynomial degree directly corresponds to the order of feature interactions the model can exploit.
- Evidence anchors:
  - [abstract] "This effect is amplified at higher polynomial degrees in the energy function, suggesting that Associative Memory is more limited in depicting higher-order interactions between features than patterns."
  - [section] Figure 2: "At higher polynomial degrees, correlated data (MNIST) notably suffers an increasing capacity drop relative to synthetic data."
  - [corpus] Related work on Dense AM (Krotov & Hopfield 2016) establishes that capacity increases with n, but does not address correlation effects.
- Break condition: Assumption: the capacity drop is due to feature correlation structure specifically, not other dataset properties (e.g., non-uniform bit marginals). The paper does not fully disentangle these.

## Foundational Learning

- **Hamming Distance and Pattern Separation**
  - Why needed here: The entire experimental framework relies on controlling and measuring average pairwise Hamming distance to isolate correlation effects from separation effects.
  - Quick check question: Given two 784-bit binary vectors with Hamming distance 200, what fraction of bits differ? (Answer: 200/784 ≈ 25.5%)

- **Dense Associative Memory (DAM) Energy Function**
  - Why needed here: The polynomial degree n is the key hyperparameter controlling both capacity and sensitivity to correlations. Understanding how the rectified polynomial energy shapes the attractor landscape is essential for interpreting results.
  - Quick check question: In a DAM with energy E = -Σ F(x·ξ^μ) where F(t) = t^n, what happens to the sharpness of attractor basins as n increases? (Answer: Basins become sharper, capacity increases, but robustness to noise may decrease.)

- **Binary Search for Capacity Measurement**
  - Why needed here: The paper uses binary search to efficiently find K_max (largest K with perfect retrieval), reducing complexity from O(S) to O(log S) per configuration.
  - Quick check question: If a subset has 50 patterns and K_max is 37, approximately how many retrieval tests does binary search require to find this? (Answer: ~6 tests, since log₂(50) ≈ 5.6)

## Architecture Onboarding

- **Component map:** Input layer (784 binary neurons) -> Pattern storage (in-memory) -> Energy function (rectified polynomial degree n) -> Retrieval (one async update) -> Capacity measurement (binary search)

- **Critical path:** 1. Construct dataset subsets with target mean Hamming distance (greedy selection for MNIST, biased Rademacher for synthetic) 2. For each (n, subset) configuration, run binary search to find K_max 3. Re-compute actual mean separation at K_max (not full subset) for accurate plotting 4. Compare synthetic vs. MNIST capacity curves across n and separation

- **Design tradeoffs:** Speed vs. accuracy: Binary search + early stopping reduces compute but may miss exact K_max if capacity saturates at 50 (excluded from results); Controlled separation vs. realistic data: Greedy selection equalizes HD but may select unrepresentative MNIST subsets; In-memory storage vs. trained weights: Faster iteration but skips learning dynamics relevant to practical ML

- **Failure signatures:** K_max = 49 for many high-n configurations: saturation artifact (true capacity > 50 but subset size limits measurement); MNIST subsets with HD < 30 or HD > 190 cannot be constructed: inherent data structure limits; Large variance in capacity at low separation: retrieval becomes unstable, binary search may give inconsistent results

- **First 3 experiments:** 1. Reproduce baseline scaling: Replicate Figure 3 (Appendix A.1) for n=6,7,8 on biased Rademacher patterns to verify exponential capacity-separation relationship before testing correlation effects; 2. Isolate correlation effect at fixed n: Hold n=7 constant, sweep mean separation, and plot K_max for synthetic vs. MNIST. Quantify the constant gap (Figure 1, left panel); 3. Probe high-order interaction sensitivity: At fixed separation buckets (HD=60, 90, 120), sweep n from 6 to 30 and measure how the synthetic-MNIST capacity gap grows (Figure 2). Test whether the divergence is consistent across separation levels.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the rigorous theoretical explanation for why the memory capacity gap between feature-correlated and synthetic data widens at higher polynomial degrees in the energy function?
- Basis in paper: [explicit] The authors state, "We leave a rigorous theoretical explanation and intuition of how these effects come about to future work," regarding the observation that correlated data suffers more at higher $n$.
- Why unresolved: The paper empirically demonstrates the divergence but does not derive the mathematical mechanism explaining why higher-order interactions are more limited in correlated data.
- What evidence would resolve it: A formal proof or theoretical framework deriving capacity limits that explicitly accounts for feature correlation coefficients and polynomial degree $n$.

### Open Question 2
- Question: How do capacity dynamics change when extending the analysis from binary patterns to continuous-space input data?
- Basis in paper: [explicit] The authors identify "how continuous-space input data affects capacity dynamics" as a "more immediate next research step."
- Why unresolved: The current empirical framework relies on Hamming distance, restricting the analysis to binary Rademacher patterns and binarized MNIST images.
- What evidence would resolve it: An extension of the controlled separation framework to continuous distributions, measuring capacity scaling using metrics suitable for continuous vector spaces.

### Open Question 3
- Question: What are the storage capacity bounds for Dense Associative Memory under non-zero error rates or generalization constraints?
- Basis in paper: [explicit] The paper suggests it would be interesting to "study bounds below optimal memorization... or recall under the assumption of non-zero error rates."
- Why unresolved: The current study measures capacity strictly at $P_{error} = 0$ (perfect retrieval), which ignores the error tolerance and generalization requirements typical of practical machine learning scenarios.
- What evidence would resolve it: Capacity curves calculated for varying error probability thresholds ($P_{error} > 0$) and analysis of retrieval quality beyond the binary perfect/imperfect distinction.

## Limitations

- The greedy selection procedure may not perfectly equalize mean Hamming distance across datasets, potentially confounding the correlation effect with other dataset properties.
- The one-update retrieval protocol (P_error=0) is stricter than practical scenarios and may overestimate the impact of correlations on usable capacity.
- The saturation artifact at K_max=49 prevents measurement of true capacity in high-degree regimes, creating uncertainty about asymptotic scaling behavior.

## Confidence

- **High confidence**: Exponential scaling of capacity with pattern separation (Mechanism 1) - this follows directly from established DAM theory and is empirically confirmed across multiple configurations.
- **Medium confidence**: Feature correlations reduce capacity independently of separation (Mechanism 2) - the gap between synthetic and MNIST is observed consistently, but selection procedures and data structure limits create potential confounding factors.
- **Medium confidence**: Higher polynomial degrees amplify correlation penalties (Mechanism 3) - the trend is visible, but saturation effects and limited n range (up to 38) prevent definitive conclusions about asymptotic behavior.

## Next Checks

1. **Validate greedy selection procedure**: Implement synthetic data selection with the same greedy algorithm used for MNIST and verify that both datasets achieve identical mean Hamming distance distributions across separation buckets.
2. **Test error-tolerant retrieval**: Repeat capacity measurements using k asynchronous updates or allowing small retrieval errors (P_error > 0) to assess practical capacity under realistic conditions.
3. **Extend polynomial degree range**: Increase n beyond 38 (if computationally feasible) and/or increase subset size S > 50 to determine whether the synthetic-MNIST capacity gap continues to grow or plateaus at higher orders.