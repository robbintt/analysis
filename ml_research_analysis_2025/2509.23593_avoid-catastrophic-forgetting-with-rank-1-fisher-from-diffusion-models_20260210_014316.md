---
ver: rpa2
title: Avoid Catastrophic Forgetting with Rank-1 Fisher from Diffusion Models
arxiv_id: '2509.23593'
source_url: https://arxiv.org/abs/2509.23593
tags:
- task
- rank-1
- train
- fisher
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Catastrophic forgetting remains a major challenge in continual
  learning, especially for generative models like diffusion models. Existing approaches
  like replay and elastic weight consolidation (EWC) have limitations: replay can
  suffer from distributional drift, and EWC typically uses a diagonal Fisher approximation
  that fails to capture cross-parameter correlations.'
---

# Avoid Catastrophic Forgetting with Rank-1 Fisher from Diffusion Models

## Quick Facts
- arXiv ID: 2509.23593
- Source URL: https://arxiv.org/abs/2509.23593
- Reference count: 40
- Proposed method improves average FID and reduces forgetting in continual image generation across multiple datasets

## Executive Summary
Catastrophic forgetting remains a major challenge in continual learning for generative models like diffusion models. This work introduces a rank-1 Fisher approximation that captures the dominant curvature direction more effectively than traditional diagonal approximations, combined with generative distillation-based replay. The method consistently improves performance across MNIST, FashionMNIST, CIFAR-10, and ImageNet-1k datasets, nearly eliminating forgetting on simpler datasets and more than halving it on ImageNet-1k.

## Method Summary
The authors propose a novel approach that combines rank-1 Elastic Weight Consolidation (EWC) with generative distillation-based replay. They demonstrate that in diffusion models operating under low signal-to-noise ratio conditions, per-sample gradients become nearly collinear with their mean gradient, resulting in an effectively rank-1 Fisher matrix. This insight allows for a more accurate Fisher approximation that is computationally comparable to diagonal methods. The generative distillation component helps share parameters across tasks while constraining distributional drift during replay.

## Key Results
- Consistent FID improvements across all tested datasets (MNIST, FashionMNIST, CIFAR-10, ImageNet-1k)
- Nearly eliminated forgetting on MNIST and FashionMNIST
- More than halved forgetting on ImageNet-1k compared to baselines
- Superior performance compared to both replay-only and diagonal-EWC baselines

## Why This Works (Mechanism)
The effectiveness stems from recognizing that diffusion model gradients exhibit special structure under low SNR conditions, where gradients align along a dominant direction. This enables the rank-1 Fisher approximation to capture essential parameter importance while remaining computationally efficient. The combination with generative distillation provides a dual mechanism: EWC preserves important parameters identified by the rank-1 Fisher, while distillation ensures smooth parameter sharing and prevents replay-induced distributional drift.

## Foundational Learning
- **Diffusion Models**: Generative models that denoise data through a Markov chain process; needed for understanding the model architecture being stabilized; quick check: verify familiarity with DDPM and score-based models.
- **Catastrophic Forgetting**: The phenomenon where neural networks lose previously learned information when trained on new tasks; needed to understand the problem being solved; quick check: review continual learning challenges in generative models.
- **Fisher Information Matrix**: A measure of parameter importance used in EWC; needed to understand how parameters are protected from change; quick check: understand the difference between diagonal and full Fisher approximations.
- **Elastic Weight Consolidation (EWC)**: A regularization method that constrains important parameters based on Fisher information; needed to understand the baseline method being improved; quick check: review how EWC prevents forgetting.
- **Signal-to-Noise Ratio (SNR)**: The ratio of meaningful gradient signal to noise; needed to understand when rank-1 approximation applies; quick check: verify understanding of low SNR regime in training dynamics.
- **Generative Distillation**: A replay mechanism that generates data while preserving distributional properties; needed to understand the replay component; quick check: review how distillation differs from standard replay.

## Architecture Onboarding

**Component Map**: Data Stream → Diffusion Model → Rank-1 EWC Regularizer + Generative Distillation → Updated Model Parameters

**Critical Path**: The critical computational path involves computing the mean gradient across samples, applying the rank-1 Fisher approximation to identify the dominant parameter direction, and simultaneously generating replay samples through distillation while applying EWC regularization to constrain parameter updates.

**Design Tradeoffs**: The rank-1 approximation offers a middle ground between the computational efficiency of diagonal Fisher and the accuracy of full Fisher, at the cost of assuming low SNR conditions. Generative distillation adds complexity but provides better parameter sharing than standard replay.

**Failure Signatures**: The method may underperform if the SNR assumption breaks down (high noise regimes), if the dominant gradient direction changes rapidly between tasks, or if the generative distillation fails to accurately capture the target distribution.

**3 First Experiments**:
1. Verify rank-1 structure empirically by analyzing gradient collinearity across different diffusion model architectures
2. Compare forgetting rates with and without generative distillation to isolate its contribution
3. Test performance sensitivity to SNR levels by varying noise schedules in the diffusion process

## Open Questions the Paper Calls Out
None

## Limitations
- The rank-1 Fisher approximation relies on low SNR conditions that may not hold across all diffusion model architectures and training configurations
- Empirical evaluation is limited to class-incremental learning, with unexplored performance in task-incremental or domain-incremental scenarios
- The contribution of generative distillation versus rank-1 EWC is not fully disentangled in ablation studies

## Confidence

**High confidence**: Empirical FID improvements and forgetting reduction compared to baselines on tested datasets

**Medium confidence**: Theoretical justification for rank-1 Fisher approximation under low SNR conditions in diffusion models

**Medium confidence**: Effectiveness of combining rank-1 EWC with generative distillation for continual learning

## Next Checks

1. Test rank-1 Fisher approximation across different diffusion model architectures (DDPM, DDIM, score-based models) to verify SNR regime assumption holds broadly

2. Evaluate method performance in task-incremental and domain-incremental learning settings beyond class-incremental scenarios

3. Conduct ablation studies isolating contributions of rank-1 EWC regularization versus generative distillation to understand individual impacts on performance