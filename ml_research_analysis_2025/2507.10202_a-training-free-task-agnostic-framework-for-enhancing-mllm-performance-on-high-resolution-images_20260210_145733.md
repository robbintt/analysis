---
ver: rpa2
title: A Training-Free, Task-Agnostic Framework for Enhancing MLLM Performance on
  High-Resolution Images
arxiv_id: '2507.10202'
source_url: https://arxiv.org/abs/2507.10202
tags:
- framework
- high-resolution
- image
- images
- mllm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of Multimodal Large Language
  Models (MLLMs) struggling with high-resolution images due to their fixed image resolution
  during fine-tuning. The proposed Extract Candidate then Predict (ECP) framework
  is a training-free, task-agnostic two-stage approach that first uses a downsampled
  image to identify a candidate region, then makes the final prediction using a cropped
  high-resolution patch.
---

# A Training-Free, Task-Agnostic Framework for Enhancing MLLM Performance on High-Resolution Images

## Quick Facts
- arXiv ID: 2507.10202
- Source URL: https://arxiv.org/abs/2507.10202
- Authors: Jaeseong Lee; Yeeun Choi; Heechan Choi; Hanjung Kim; Seonjoo Kim
- Reference count: 24
- Key outcome: ECP achieves +21.3% accuracy on 4K GUI grounding and +5.8%/+5.2% on 4K/8K MLLM perception tasks

## Executive Summary
This paper addresses the challenge of Multimodal Large Language Models (MLLMs) struggling with high-resolution images due to their fixed image resolution during fine-tuning. The proposed Extract Candidate then Predict (ECP) framework is a training-free, task-agnostic two-stage approach that first uses a downsampled image to identify a candidate region, then makes the final prediction using a cropped high-resolution patch. This preserves fine-grained details while leveraging the model's coarse localization ability. ECP achieves significant improvements on two benchmarks: +21.3% accuracy on 4K GUI grounding (ScreenSpot-Pro) and +5.8%/+5.2% on 4K/8K MLLM perception (HR-Bench) compared to the single-stage baseline.

## Method Summary
ECP is a two-stage framework that addresses the train-test resolution discrepancy in MLLMs. Stage 1 (Extract Candidate) takes a downsampled image and outputs a representative point or bounding box coordinate. Stage 2 (Predict) crops a 1024×1024 region from the high-resolution image centered on this coordinate, then processes it (optionally with the downsampled full image) to produce the final prediction. The framework works for both grounding tasks (returning coordinates) and perception tasks (returning semantic answers). For grounding, only the cropped patch is used; for perception, both the cropped patch and downsampled full image are provided to preserve global context.

## Key Results
- +21.3% accuracy improvement on 4K GUI grounding task (ScreenSpot-Pro)
- +5.8% accuracy improvement on 4K MLLM perception task (HR-Bench FSP)
- +5.2% accuracy improvement on 8K MLLM perception task (HR-Bench FCP)
- 28.5% performance drop when using random sampling instead of guided candidate extraction (40.4% → 11.9%)

## Why This Works (Mechanism)

### Mechanism 1: Coarse Localization Preservation Under Downsampling
MLLMs retain usable spatial attention cues even when fine-grained details are destroyed by downsampling. The image encoder's learned attention maps degrade gracefully—spatial priors survive resolution loss, enabling "where to look" predictions even when "what it is" fails. This works because MLLMs trained on standard resolutions develop robust coarse spatial representations that transfer to downsampled high-res inputs.

### Mechanism 2: Train-Test Resolution Alignment via Dynamic Cropping
Cropping high-resolution images to model-compatible dimensions eliminates distribution shift while preserving pixel-level detail. By extracting a 1024×1024 region and processing it at native resolution, the visual features match the encoder's training distribution while the target object occupies a larger portion of the input. This assumes the target region fits within a single crop window.

### Mechanism 3: Task-Agnostic Region-Then-Reason Decomposition
Separating spatial attention from detailed reasoning enables reuse across tasks without task-specific engineering. Stage 1 outputs a universal coordinate representation; Stage 2 consumes the cropped region with original instruction—task logic stays in the MLLM, not the framework. This works because all tasks can be decomposed into "find relevant region" + "process region."

## Foundational Learning

- **Vision Encoder Resolution Lock**
  - Why needed here: Understanding why naive high-res input fails requires knowing that CLIP/ViT encoders are trained at fixed resolutions; MLLM fine-tuning inherits this constraint.
  - Quick check question: Can you explain why a 384×384-trained CLIP encoder struggles with 4K input even if the LLM has a 128K context window?

- **Coarse-to-Fine Processing in Vision**
  - Why needed here: ECP mimics foveated vision—peripheral detection followed by foveal analysis. Prior art in object detection and segmentation uses similar strategies.
  - Quick check question: How does this differ from multi-scale feature pyramids in CNNs? (Hint: ECP processes scales sequentially, not in parallel.)

- **Training-Free Inference-Time Adaptation**
  - Why needed here: ECP modifies inference without weight updates. Understanding this paradigm helps distinguish it from adapter-based or fine-tuning approaches.
  - Quick check question: What are the failure modes that training-free methods cannot address? (Hint: fundamental representation gaps.)

## Architecture Onboarding

- **Component map:**
High-Res Image (W×H)
    │
    ├──→ [Downsample] ──→ [MLLM Stage 1: F_EC] ──→ Point/Box (x, y) or (x1,y1,x2,y2)
    │                                              │
    │                                              ↓
    └──────────────────────────────────→ [Crop at (x,y) → 1024×1024]
                                               │
                                               ↓
                                       [MLLM Stage 2: F_P] ──→ Final Prediction
                                              ↑
                                    (Optional: + Downsampled Image for Global Context)

- **Critical path:**
1. Downsample preserving aspect ratio to MLLM's native input size
2. Prompt MLLM for coordinate output (not final answer)
3. Apply boundary-aware cropping (Eq. 2-4)
4. Feed crop (and optionally original downsampled) to MLLM with original instruction
5. Return Stage 2 output as final result

- **Design tradeoffs:**
  - **Crop size (w, h)**: Larger crops capture more context but reduce pixel density on the target; paper uses 1024×1024 without ablation
  - **Stage 1 model selection**: Stronger models (OS-Atlas vs. Qwen2-VL) improve candidate quality; Stage 1 can use a smaller model for efficiency
  - **Global context in Stage 2**: Perception tasks benefit from including downsampled image; grounding tasks do not

- **Failure signatures:**
  - Random candidate selection: Performance drops below single-stage baseline (Table 1: 40.4% → 11.9%)
  - Off-target crops: If Stage 1 is confident but wrong, Stage 2 has no recovery mechanism
  - Multi-object queries: "Find all instances of X" requires iterative or multi-crop extension (not covered)

- **First 3 experiments:**
1. **Baseline reproduction**: Run single-stage inference on ScreenSpot-Pro subset (100 samples) with Qwen2-VL-7B to confirm ~1.5% accuracy
2. **Ablation on crop size**: Test 512×512, 1024×1024, 2048×2048 crops on HR-Bench FSP subset to validate resolution vs. context tradeoff
3. **Cross-model Stage 1/2 pairing**: Use Qwen2-VL-2B for Stage 1 and Qwen2-VL-7B for Stage 2 to measure efficiency-accuracy frontier

## Open Questions the Paper Calls Out

### Open Question 1
Can ECP generalize to high-resolution vision-language tasks beyond GUI grounding and perception, such as document understanding, medical imaging, or satellite imagery analysis? Only two benchmarks were tested, leaving applicability to other high-resolution domains unexplored.

### Open Question 2
What alternative cropping strategies could improve performance, and how does the fixed crop size (1024×1024) affect results across varying image resolutions and task types? The 1024×1024 crop size is used as a fixed hyperparameter without systematic investigation of optimal sizing or adaptive strategies.

### Open Question 3
Under what conditions does the coarse localization ability in Stage 1 fail, and how do these failure modes propagate to Stage 2 predictions? The core assumption—that downsampled predictions contain meaningful localization information—is not rigorously tested for failure cases or confidence calibration.

## Limitations

- The framework assumes objects/targets fit within a single 1024×1024 crop window, making it unsuitable for large objects or scenes requiring multi-crop analysis
- Stage 1's success depends entirely on the downsampled image retaining sufficient spatial cues for coarse localization—failure in Stage 1 provides no recovery mechanism
- The 28.5% performance drop when using random sampling reveals the framework's heavy dependence on Stage 1's quality, which may vary across MLLM architectures

## Confidence

- **High Confidence**: The core two-stage mechanism and its application to the tested benchmarks (ScreenSpot-Pro, HR-Bench) is well-validated with specific accuracy improvements
- **Medium Confidence**: Generalization claims to arbitrary high-resolution tasks are supported by success on two benchmarks but not extensively tested across diverse task types
- **Low Confidence**: The assumption that Stage 1 models will consistently provide reliable coarse localization across all MLLM architectures and high-resolution scenarios

## Next Checks

1. **Cross-resolution robustness test**: Apply ECP to 2K, 4K, 8K, and 16K images from the same dataset to verify performance scaling doesn't degrade beyond tested resolutions
2. **Multi-object scenario evaluation**: Test the framework on tasks requiring identification of multiple instances ("find all cats") to assess whether iterative extension is needed
3. **Stage 1 model sensitivity analysis**: Systematically vary the Stage 1 model (using different MLLM sizes/capabilities) to quantify how Stage 1 quality impacts final performance across task types