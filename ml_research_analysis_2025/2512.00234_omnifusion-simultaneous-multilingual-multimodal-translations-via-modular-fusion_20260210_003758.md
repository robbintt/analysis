---
ver: rpa2
title: 'OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular
  Fusion'
arxiv_id: '2512.00234'
source_url: https://arxiv.org/abs/2512.00234
tags:
- translation
- fusion
- image
- language
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the limitations of cascaded speech translation
  (ST) pipelines, which introduce latency and fail to leverage multimodal context
  like images. It proposes OmniFusion, a method that fuses a multimodal foundation
  model (MMFM) with a specialized translation LLM by extracting and combining hidden
  states from multiple layers of the MMFM, enabling joint end-to-end training.
---

# OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion
## Quick Facts
- **arXiv ID**: 2512.00234
- **Source URL**: https://arxiv.org/abs/2512.00234
- **Reference count**: 40
- **Primary result**: Joint end-to-end speech-and-image translation model with 1s latency reduction and improved translation quality

## Executive Summary
OmniFusion addresses the limitations of cascaded speech translation pipelines by introducing a modular fusion approach that combines a multimodal foundation model with a specialized translation LLM. This joint end-to-end architecture supports speech-to-text, speech-and-image-to-text, and text-and-image-to-text translation tasks. The method extracts and combines hidden states from multiple layers of the MMFM, enabling effective multimodal context integration while reducing latency compared to traditional cascaded approaches.

## Method Summary
The OmniFusion method fuses a multimodal foundation model with a translation LLM through a modular fusion mechanism that extracts and combines hidden states from multiple layers of the MMFM. This joint end-to-end training approach enables simultaneous multilingual multimodal translations across three task types: speech-to-text, speech-and-image-to-text, and text-and-image-to-text. The architecture supports dynamic fusion of speech and visual modalities, with the translation LLM processing the fused representations to generate accurate translations.

## Key Results
- Achieves 1-second latency reduction in simultaneous speech translation compared to cascaded pipelines
- Sets state-of-the-art performance on CoMMuTE benchmark for image-text translation
- Produces fewer critical and major translation errors while improving overall translation quality

## Why This Works (Mechanism)
OmniFusion works by effectively integrating multimodal context through its modular fusion mechanism. By extracting and combining hidden states from multiple layers of the multimodal foundation model, the system captures rich semantic representations that inform translation decisions. The joint end-to-end training allows the model to learn optimal fusion strategies for different modality combinations, while the translation LLM leverages these fused representations to produce more accurate and contextually appropriate translations.

## Foundational Learning
- **Multimodal foundation models**: Pre-trained models that process multiple input modalities (speech, text, images) simultaneously - needed for unified representation learning, quick check: verify MMFM architecture supports all required modalities
- **Layer-wise feature fusion**: Technique of extracting and combining hidden states from different layers - needed for capturing hierarchical semantic information, quick check: ensure fusion weights are properly learned
- **Joint end-to-end training**: Training multiple components together as a single system - needed for optimal cross-modality alignment, quick check: monitor convergence across all task types
- **Simultaneous translation**: Real-time translation with minimal latency - needed for practical deployment, quick check: measure end-to-end processing time
- **Modular fusion architecture**: Separable components that can be combined flexibly - needed for task versatility, quick check: verify each task type works independently

## Architecture Onboarding
**Component Map**: Speech Input -> MMFM -> Fusion Module -> Translation LLM -> Text Output; Image Input -> MMFM -> Fusion Module -> Translation LLM -> Text Output; Text Input -> Translation LLM -> Text Output

**Critical Path**: The critical path for multimodal translation involves processing input through the MMFM to extract features, fusing these features with the translation LLM's representations, and generating the final translation. This path must be optimized for latency while maintaining quality.

**Design Tradeoffs**: The modular fusion approach trades increased model complexity and training difficulty for greater task flexibility and improved multimodal integration. While more complex than cascaded pipelines, it eliminates intermediate representation bottlenecks and enables end-to-end optimization.

**Failure Signatures**: Common failure modes include modality misalignment during fusion, over-reliance on one modality when others are noisy, and catastrophic forgetting during joint training. These manifest as degraded translation quality when switching between task types or when input quality varies.

**First 3 Experiments**:
1. Single-modality ablation study to measure contribution of each input type
2. Layer-wise fusion ablation to identify optimal fusion points
3. Latency benchmarking against cascaded pipeline baseline

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on benchmark performance with limited discussion of real-world deployment constraints like computational overhead and inference cost
- Latency improvement claims lack comparative context regarding end-to-end processing times and resource usage
- Multimodal context effectiveness is validated only on synthetic and curated datasets, not truly open-domain data

## Confidence
- **High Confidence**: Technical design of modular fusion approach is clearly described and reproducible; error reduction metrics and benchmark results are directly measurable
- **Medium Confidence**: Latency improvement supported by internal measurements but lacks independent replication and broader operational context
- **Low Confidence**: Generalization of multimodal benefits beyond controlled datasets is asserted but not empirically validated across diverse domains

## Next Checks
1. Conduct ablation studies comparing model size, inference speed, and memory usage against both cascaded and end-to-end baselines under identical hardware constraints
2. Evaluate the model on multilingual, out-of-domain datasets to assess robustness and multimodal context effectiveness beyond curated benchmarks
3. Perform human evaluation studies to verify that reduced error rates translate to meaningful improvements in user-facing translation quality, especially in noisy or ambiguous visual contexts