---
ver: rpa2
title: Dynamic Tree Databases in Automated Planning
arxiv_id: '2511.12677'
source_url: https://arxiv.org/abs/2511.12677
tags:
- state
- tree
- planning
- tasks
- databases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces dynamic tree databases for compactly representing\
  \ state sets in explicit state-space search for automated planning. Traditional\
  \ tree databases require large preallocations, but the authors propose two dynamic\
  \ variants\u2014DTDB-S and DTDB-H\u2014that use modern hash table techniques to\
  \ avoid this limitation."
---

# Dynamic Tree Databases in Automated Planning

## Quick Facts
- arXiv ID: 2511.12677
- Source URL: https://arxiv.org/abs/2511.12677
- Authors: Oliver Joergensen; Dominik Drexler; Jendrik Seipp
- Reference count: 14
- Primary result: Dynamic tree databases achieve compression ratios up to three orders of magnitude in explicit state-space search while adding negligible runtime overhead.

## Executive Summary
This paper introduces dynamic tree databases (DTDBs) as a compact representation for state sets in automated planning. Traditional tree databases require large preallocations, limiting their practicality. The authors propose two dynamic variants—DTDB-S and DTDB-H—that use modern hash table techniques to avoid this limitation while maintaining efficient insertion and lookup times. The approach supports both propositional and numeric variables and demonstrates significant memory savings in lifted planning applications, with compression ratios reaching three orders of magnitude and negligible runtime overhead.

## Method Summary
The method involves implementing two dynamic tree database variants integrated into Fast Downward (grounded) and Mimir (lifted) planners. DTDB-S uses a flat hash table with dynamic array for stable indices, while DTDB-H uses a hash ID map with direct hash location indexing. Experiments compare these against hashset baselines across 6495 classical and 575 numeric planning tasks using A* search with blind and saturated cost partitioning heuristics. The approach leverages subtree sharing through variable ordering based on action-effect co-occurrence and uses geometric resizing for amortized linear insertion time.

## Key Results
- DTDBs achieve compression ratios up to 1000× compared to traditional representations
- Lifted planners show coverage improvements of 6.6% (classical) and 11.8% (numeric) when using DTDBs
- Memory score improvements of 1.6% (classical) and 7.3% (numeric) demonstrate practical memory benefits
- Runtime overhead remains negligible, with median state sizes of 8-32 bytes showing no benefit while larger states gain significant compression

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tree databases achieve compression by sharing common subtrees across related states.
- Mechanism: States encode as perfectly balanced binary trees where nodes represent pairs of elements. An indexed hash set deduplicates nodes by mapping key pairs ⟨l(v), r(v)⟩ to unique indices. When successor states differ from parents in few variables, most subtrees are reused, avoiding redundant storage.
- Core assumption: Planning actions typically modify only a small subset of state variables (sparse effects), creating structural similarity across generated states.
- Evidence anchors:
  - [abstract] "compression ratios of up to three orders of magnitude, often with negligible runtime overhead"
  - [section 3.3] "Tree databases store trees in a forest, allowing shared subtrees across states, which can result in significant space savings compared to representations that do not exploit shared structure"
- Break condition: High-entropy state spaces where states differ significantly in many variables reduce subtree sharing; compression approaches 1×.

### Mechanism 2
- Claim: Dynamic resizing via modern flat hash tables eliminates the need for preallocation while maintaining amortized linear insertion time.
- Mechanism: DTDB-S pairs a dynamic array (backward index→node mapping) with a Swiss table (forward node→index mapping). When load factor reaches 7/8, both structures grow by factor δ > 1. DTDB-H uses hash location directly as index, rebuilding trees via depth-first traversal on resize. Geometric growth ensures amortized O(1) per-node insertion.
- Core assumption: Resizes are infrequent enough that rebuilding cost distributes across many insertions.
- Evidence anchors:
  - [abstract] "dynamic variants—DTDB-S and DTDB-H—that use modern hash table techniques to avoid this limitation"
  - [section 4.2] "Each control byte act as a hash fingerprint, allowing the table to identify likely matches without accessing the actual keys"
- Break condition: Memory spikes during DTDB-H resizes; if memory is near limit, resizing may cause OOM before completion.

### Mechanism 3
- Claim: Variable ordering based on action-effect co-occurrence maximizes subtree sharing.
- Mechanism: The affinity metric aff(v, v′) counts how often variable pairs appear together in action effects. A greedy bin-packing variant prioritizes variables with highest co-occurrence to current bin contents, localizing effect changes to smaller tree regions.
- Core assumption: Effect locality is predictable from the grounded representation before search begins.
- Evidence anchors:
  - [section 4.5] "If the elements in eff(a) appear close together in the sequence, then the trees for a state s and its successor s′ share long common suffixes, yielding many shared nodes"
  - [table 2] Aff-DTDB-S shows memory score improvements over baseline DTDB-S in blind search (760 vs. 772 not achieved; actually Aff-DTDB-S=760 < DTDB-S=772, so ordering doesn't always help)
- Break condition: Lifted planning cannot precompute ordering; runtime overhead may exceed compression benefit for small state sizes.

## Foundational Learning

- Concept: Finite-Domain Representation (FDR) and mutex groups
  - Why needed here: The paper benchmarks against packed FDR; understanding how mutex groups compress Boolean variables into finite-domain variables is essential for interpreting compression ratios.
  - Quick check question: Given mutex group {p₀, p₁, p₂, none}, how many bits are needed to encode its value?

- Concept: Open-addressing hash tables and load factors
  - Why needed here: Swiss tables use open addressing with 7/8 load factor; understanding probe sequences and control bytes explains the 1-byte overhead claim.
  - Quick check question: Why does a 7/8 load factor in Swiss tables require control bytes?

- Concept: Amortized analysis with geometric resizing
  - Why needed here: The Θ(k) insertion claim relies on geometric growth distributing resize costs; this is not worst-case per operation.
  - Quick check question: If resize factor δ = 2, how many resizes occur after inserting n items starting from capacity 1?

## Architecture Onboarding

- Component map:
  - Indexed Hash Set: Bijective key→index mapping (core deduplication layer)
  - λ Function: Defines perfectly balanced tree structure from sequence length
  - Swiss Table (flat hash table): Control bytes + key array, 7/8 load factor, SIMD probing
  - Dynamic Array: Stores nodes for backward index→node lookup (DTDB-S only)
  - Separate Numeric DTDB-S: 64-bit double values in dedicated structure

- Critical path:
  1. State arrives → partition into w-bit words (FDR) or sparse propositional + numeric values
  2. Recursively build tree: subdivide sequence until leaf pairs remain
  3. For each node: hash table lookup; if new, append to array and insert
  4. Return root index as state identifier for duplicate detection

- Design tradeoffs:
  - DTDB-S vs DTDB-H: Stable indices (1 byte overhead/node) vs. lower memory (unstable indices, rebuild cost)
  - Lifted vs. Grounded: Lifted benefits more (sparse states, less optimized baseline); grounded has limited gains
  - Resize factor δ: Larger δ → fewer resizes, bigger memory spikes

- Failure signatures:
  - Compression ratio < 1×: State size too small (median 8 bytes in classical benchmarks showed no benefit)
  - Coverage drops vs. hashset: Likely excessive resize overhead; check resize frequency
  - Memory exceeds baseline for large-variable tasks: Possible hash collision causing poor deduplication

- First 3 experiments:
  1. Compare DTDB-S vs. Hashset-Packed on tasks with >30 FDR variables; expect compression ratio >1× per Figure 3.
  2. Measure resize events and latency spikes for DTDB-H on numeric benchmarks; compare against DTDB-S stability.
  3. Test affinity ordering on grounded classical tasks; isolate contribution by comparing Aff-DTDB-S vs. DTDB-S memory scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Huffman minimum redundancy codes consistently reduce memory usage compared to perfectly balanced trees in dynamic tree databases?
- Basis in paper: [explicit] The "Future Work" section notes that Huffman codes minimize potential entries and asks if this structuring improves compression.
- Why unresolved: The authors present only "early results" and have not empirically validated the approach across the full benchmark suite.
- What evidence would resolve it: An empirical comparison of memory usage between Huffman-coded trees and perfectly balanced trees on the provided classical and numeric benchmarks.

### Open Question 2
- Question: Can the runtime overhead of compact hashing be reduced enough to make it practical for high-performance planning applications?
- Basis in paper: [explicit] The paper states that while compact hashing approaches the information-theoretic lower bound, it is currently "impractical for planning applications" due to bit-level access costs.
- Why unresolved: The theoretical memory savings are clear, but the trade-off with CPU cycles in a tight search loop has not been bridged.
- What evidence would resolve it: An optimized implementation of compact hashing that demonstrates runtime overhead comparable to standard Swiss tables while achieving superior compression ratios.

### Open Question 3
- Question: How can effective variable orderings be computed for lifted planning when ground atoms and actions are unknown in advance?
- Basis in paper: [inferred] The paper notes that the affinity-based variable ordering relies on grounded effects, which makes computing orders "more challenging" in lifted planning.
- Why unresolved: The proposed ordering technique cannot be directly applied to lifted planning without pre-computing the grounding, negating the benefits of lifted search.
- What evidence would resolve it: A dynamic ordering heuristic or sampling method that improves tree database compression in lifted planners without requiring full grounding.

## Limitations
- Dynamic tree databases show substantial gains in lifted planning but minimal improvement in grounded planning, suggesting domain-specific applicability rather than universal benefit.
- The paper demonstrates significant compression ratios but focuses on planner integration rather than standalone DTDB performance, limiting isolation of the technique's intrinsic value.
- Affinity-based variable ordering shows promise in numeric planning but provides mixed results in classical planning, with some configurations showing decreased performance.

## Confidence

- **High confidence**: The core mechanism of subtree sharing for compression and the asymptotic efficiency of dynamic resizing (Θ(k) insertion for DTDB-S, amortized linear for DTDB-H)
- **Medium confidence**: The practical effectiveness of affinity-based variable ordering, given inconsistent empirical results across domains
- **Medium confidence**: The scalability claims for numeric planning, as only 575 tasks were tested versus 6495 classical tasks

## Next Checks

1. **Isolate DTDB overhead**: Run standalone DTDB implementations with synthetic state streams to measure pure insertion/lookup performance independent of planner integration effects
2. **Stress test resize behavior**: Create benchmarks with rapidly changing states to evaluate DTDB-H's memory spike behavior and compare against DTDB-S's stability under extreme conditions
3. **Cross-domain generalization**: Apply DTDB to non-planning domains with similar state-space characteristics (e.g., model checking, game state exploration) to validate broader applicability beyond the tested planning benchmarks