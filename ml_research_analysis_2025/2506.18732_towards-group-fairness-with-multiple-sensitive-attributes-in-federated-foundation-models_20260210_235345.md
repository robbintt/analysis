---
ver: rpa2
title: Towards Group Fairness with Multiple Sensitive Attributes in Federated Foundation
  Models
arxiv_id: '2506.18732'
source_url: https://arxiv.org/abs/2506.18732
tags:
- fairness
- causal
- sensitive
- group
- attributes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses group fairness in federated foundation models
  (FFMs) when multiple sensitive attributes are present, a gap in current research
  that focuses on single-attribute fairness. The authors extend the FF-DVP framework
  to handle multiple sensitive attributes simultaneously through a weighted combination
  of local and global fairness regularizers, while incorporating causal analysis for
  interpretability.
---

# Towards Group Fairness with Multiple Sensitive Attributes in Federated Foundation Models

## Quick Facts
- arXiv ID: 2506.18732
- Source URL: https://arxiv.org/abs/2506.18732
- Reference count: 31
- Primary result: Multi-attribute fairness in federated foundation models is challenging due to attribute competition, with causal effect magnitude predicting debiasing difficulty

## Executive Summary
This paper addresses group fairness in federated foundation models (FFMs) when multiple sensitive attributes are present, a gap in current research that focuses on single-attribute fairness. The authors extend the FF-DVP framework to handle multiple sensitive attributes simultaneously through a weighted combination of local and global fairness regularizers, while incorporating causal analysis for interpretability. They use causal discovery to construct a causal graph and causal inference to quantify the effect of sensitive attributes on predictions. Experiments on CelebA and FairFace datasets show that fairness across attributes is challenging to achieve simultaneously, with accuracy-fairness trade-offs. Causal analysis reveals that attributes with stronger causal effects on labels (e.g., age) are harder to debias. The proposed method achieves group fairness while providing interpretability of the causal relationships between sensitive attributes and model outcomes.

## Method Summary
The method extends FF-DVP by adding parallel demographic-only prompts for each sensitive attribute, enabling independent fairness regularization per attribute through KL-divergence against uniform distributions. Local fairness regularizer L_lf sums weighted KL-divergences per attribute, while global fairness regularizer L_gf sums weighted fairness metrics (DP or EO). Causal discovery constructs DAGs from variables, and causal inference computes Total Effect (TE) decomposed into Natural Direct and Indirect Effects. The framework uses CLIP ViT-L/14@336px backbone with modality-fused classifier, trained with AdamW optimizer across 2-4 communication rounds with 2 local epochs per round. Trainable parameters are aggregated via weighted averaging on the server.

## Key Results
- Multi-attribute fairness is challenging due to attribute competition—improving fairness for one attribute can degrade another
- Attributes with stronger causal effects on labels (e.g., age with TE≈0.393) are harder to debias than those with weaker effects (e.g., gender with TE≈0.055)
- Causal analysis provides interpretability of the relationship between sensitive attributes and model outcomes
- The proposed method achieves group fairness while maintaining interpretability of causal relationships

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parallel demographic-only prompts enable independent fairness regularization per sensitive attribute.
- Mechanism: Each sensitive attribute is converted to text prompts and processed through the text encoder independently. Cosine similarities between visual representations and each demographic prompt distribution are computed, then KL-divergence against a uniform distribution forces the model to avoid biasing toward any demographic group for that attribute.
- Core assumption: Text-prompt similarity correlates meaningfully with model bias toward demographic groups; uniform similarity distribution implies fairness.
- Evidence anchors:
  - [abstract] "Multiple sensitive attributes are converted to demographic-only prompts for text encoding and processed in parallel to trade off fairness."
  - [section III.A] Equations 1-4 describe Pr(Ak) computation via cosine similarity and KL-divergence against uniform distribution U(1, |Ak|).
  - [corpus] Related work (eFedFACT, pFedFair) addresses multi-attribute fairness but without the prompt-based decomposition approach.
- Break condition: If demographic groups have semantically ambiguous text representations, or if the uniform distribution target does not correspond to fair outcomes for the task, the regularization may not achieve intended fairness.

### Mechanism 2
- Claim: Weighted combination of local and global fairness regularizers enables dynamic trade-offs across multiple attributes.
- Mechanism: Local fairness regularizer L_lf sums weighted KL-divergences per attribute (α_k weights). Global fairness regularizer L_gf sums weighted fairness metrics Φ_k (β_k weights, e.g., demographic parity or equalized odds). Both act on the supervised loss alongside contrastive loss to steer optimization.
- Core assumption: The weighting scheme can capture relative importance of attributes without requiring equal weighting; α_k and β_k correctly prioritize attributes.
- Evidence anchors:
  - [section III.A, Eq. 4] "L_lf = Σ α_k L^k_lf, where α_k is the weight and Σ α_k = 1."
  - [section III.B, Eq. 7] "L_gf = Σ β_k Φ_k, where β_k is the weight and Σ β_k = 1."
  - [corpus] pFedFair optimizes fairness-accuracy trade-off for single attributes; EFF-DVP extends to simultaneous multi-attribute optimization.
- Break condition: If weights are poorly chosen (e.g., over-penalizing a low-causal-effect attribute), optimization may degrade accuracy unnecessarily without fairness gain.

### Mechanism 3
- Claim: Causal effect magnitude predicts debiasing difficulty—attributes with stronger causal effects on labels are harder to debias.
- Mechanism: Causal discovery (PC, GES, LiNGAM algorithms) constructs a DAG from variables. Causal inference computes Total Effect (TE), decomposed into Natural Direct Effect (NDE) and Natural Indirect Effect (NIE). Larger |TE| indicates stronger influence of the sensitive attribute on the label, correlating with smaller fairness improvement after debiasing (Eq. 12: |TE| ∝ 1/|ΔΦ|).
- Core assumption: The causal graph correctly captures real-world relationships; confounders are adequately controlled.
- Evidence anchors:
  - [section IV, Eq. 9-12] TE, NDE, NIE definitions and the relationship |TE_Ak→Y| ∝ 1/|ΔΦ_k|.
  - [section V.E, Table II] Gender (TE≈0.055 on attractiveness) is easier to debias than age (TE≈0.393); supported by Table I showing better ΔΦ for gender.
  - [corpus] Limited direct corpus validation; causal fairness analysis remains underexplored in federated settings.
- Break condition: If the causal graph is misspecified (e.g., missing confounders or incorrect edge directions), TE estimates will be biased, leading to incorrect predictions about debiasing difficulty.

## Foundational Learning

- Concept: **Federated Learning (FL)**
  - Why needed here: The entire framework operates in a federated setting with multiple clients training locally and aggregating on a server.
  - Quick check question: Can you explain why raw data never leaves the client, and how model updates are aggregated?

- Concept: **Group Fairness Metrics (Demographic Parity, Equalized Odds)**
  - Why needed here: These are the core quantitative definitions of fairness used in L_gf.
  - Quick check question: What is the difference between demographic parity and equalized odds? Which is stricter?

- Concept: **Causal Discovery and Inference (DAGs, Total Effect)**
  - Why needed here: The causal analysis module quantifies attribute-label relationships to predict debiasing difficulty.
  - Quick check question: Given a simple DAG A → Y, how would you compute the total effect of A on Y?

## Architecture Onboarding

- Component map:
  - Local client module: Image encoder (ViT-L/14@336px, partially fine-tuned) -> text encoder (processes demographic-only prompts in parallel) -> modality-fused classifier (2-layer FC) -> fairness regularizers (L_lf, L_gf) -> causal analysis module (runs discovery and inference locally)
  - Server module: Aggregates trainable parameters (prompted image encoders + classifiers) via weighted average -> holds proxy dataset for global fairness evaluation
  - Shared components: CLIP backbone (frozen text encoder, fine-tuned image encoder prompts)

- Critical path:
  1. Client receives local batch -> image encoded to visual representation z̃
  2. Demographic-only prompts for each sensitive attribute Ak -> text encoder -> text representations ta
  3. Compute cosine similarities -> Pr(Ak) -> L^k_lf via KL-divergence -> aggregate to L_lf
  4. Modality-fused classifier predicts Ŷ -> compute Φ_k (DP or EO) -> aggregate to L_gf
  5. Total loss = supervised + L_con + L_lf + L_gf -> backprop
  6. Local causal analysis runs in parallel (not on critical training path)
  7. Share trainable weights with server -> aggregate -> distribute

- Design tradeoffs:
  - More sensitive attributes -> higher computational overhead but better multi-attribute fairness coverage
  - Stricter fairness metric (EO vs. DP) -> potentially larger accuracy drop
  - Higher regularization weights (α_k, β_k) -> better fairness but risk of underfitting

- Failure signatures:
  - Fairness improves for one attribute but degrades for another (attribute competition)
  - Accuracy drops significantly with minimal fairness gain (over-regularization)
  - Causal effects inconsistent across clients (data heterogeneity causing conflicting DAGs)

- First 3 experiments:
  1. Baseline replication: Run EFF-DVP on CelebA with single attribute (gender only), measure ΔΦ_DP and accuracy. Compare to FF-DVP results.
  2. Multi-attribute ablation: Enable both attributes (gender + age), set equal weights (α=0.5, β=0.5). Observe if one attribute's fairness improves at the expense of the other.
  3. Causal effect validation: Compute TE for each attribute on your local data. Check if |TE| correlates inversely with |ΔΦ| after debiasing (i.e., high-TE attributes show smaller fairness improvements).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can adaptive weighting mechanisms be designed to automatically balance fairness across multiple sensitive attributes when their causal effects on outcomes differ significantly?
- Basis in paper: [explicit] The authors conclude that "developing techniques of adaptive fairness-aware algorithms to mitigate biases across multiple sensitive attributes while maintaining high model performance, is an ongoing challenge."
- Why unresolved: Current fixed weighting parameters (αk and βk) require manual tuning, and the paper shows attributes "compete" with each other—mitigating one attribute's bias can increase another's.
- What evidence would resolve it: Development of an adaptive algorithm that dynamically adjusts fairness weights based on real-time causal effect measurements, validated on datasets with more than two sensitive attributes.

### Open Question 2
- Question: Does the observed relationship between causal effect magnitude and debiasing difficulty generalize beyond face recognition datasets to other domains such as healthcare or financial services?
- Basis in paper: [inferred] The experiments were limited to CelebA and FairFace datasets, both face recognition tasks. The causal analysis framework is motivated by healthcare applications but not tested in that domain.
- Why unresolved: The paper validates the causal effect-fairness relationship only on visual face datasets with attributes like age, gender, and race. Whether this pattern holds in tabular medical data or other modalities remains unknown.
- What evidence would resolve it: Experimental validation on non-visual datasets (e.g., electronic health records, credit scoring) demonstrating consistent correlation between total causal effect (TE) values and debiasing difficulty (|ΔΦk|).

### Open Question 3
- Question: What is the computational and communication overhead of performing causal discovery and inference in large-scale federations with hundreds or thousands of clients?
- Basis in paper: [inferred] The experiments used only 5 clients with 2 communication rounds. No analysis of how causal analysis scales with federation size or its impact on convergence speed.
- Why unresolved: Causal discovery algorithms typically require multiple conditional independence tests, which may be prohibitive when integrated into each client's local training. The federated aggregation of causal graphs introduces additional communication costs.
- What evidence would resolve it: Complexity analysis and empirical runtime measurements across varying numbers of clients (e.g., 10, 50, 100, 500), including communication round comparisons with and without causal analysis components.

## Limitations

- Weight selection remains an open problem with fixed αk and βk parameters requiring manual tuning, potentially suboptimal for different attribute combinations
- Causal analysis scalability is not addressed—computational costs and federated aggregation of causal graphs for large-scale deployments are unclear
- Limited domain validation with experiments only on face recognition datasets (CelebA, FairFace), leaving generalizability to other domains uncertain

## Confidence

- **High confidence**: The core mechanism of parallel demographic-only prompts enabling independent fairness regularization per attribute is well-specified and theoretically sound.
- **Medium confidence**: The weighted combination of local and global fairness regularizers is clearly defined, but optimal weight selection remains an open question.
- **Medium confidence**: The causal effect-magnitude predictor of debiasing difficulty is supported by experimental results, but the general applicability across different datasets and domains needs further validation.

## Next Checks

1. **Weight sensitivity analysis**: Systematically vary αk and βk weights across multiple configurations to identify optimal trade-offs between fairness and accuracy for different attribute combinations.

2. **Causal robustness testing**: Apply the method to datasets with known causal structures (e.g., synthetic data with ground-truth DAGs) to validate that TE estimates correctly predict debiasing difficulty.

3. **Attribute competition quantification**: Measure fairness-degradation spillover between attributes when optimizing for one attribute's fairness to quantify the extent of attribute competition in multi-attribute settings.