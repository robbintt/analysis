---
ver: rpa2
title: 'DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for Mathematical
  Reasoning'
arxiv_id: '2506.17533'
source_url: https://arxiv.org/abs/2506.17533
tags:
- reward
- wang
- reasoning
- zhang
- chen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a novel reward modeling framework that integrates\
  \ two complementary reward signals\u2014correctness and potential\u2014to enhance\
  \ mathematical reasoning capabilities in large language models. The authors develop\
  \ an automated pipeline to construct a large-scale dataset with both types of rewards\
  \ and train a multi-head model in a multi-task setup."
---

# DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for Mathematical Reasoning

## Quick Facts
- **arXiv ID:** 2506.17533
- **Source URL:** https://arxiv.org/abs/2506.17533
- **Reference count:** 13
- **One-line primary result:** Multi-head reward model combining correctness and potential signals achieves SOTA on MATH500 and ProcessBench benchmarks.

## Executive Summary
This paper introduces DuaShepherd, a novel reward modeling framework that integrates two complementary signals—correctness (identifying stepwise errors) and potential (estimating likelihood of reaching correct answer)—to enhance mathematical reasoning in large language models. The authors construct a large-scale dataset with both reward types using automated annotation and train a multi-head model in a multi-task setup. By combining these signals into a compound probability, the model achieves consistent performance improvements across multiple benchmarks, outperforming models trained on either reward type alone.

## Method Summary
DuaShepherd constructs a unified dataset by combining PRM800K (human-labeled correctness data) with Math-Shepherd (MC-generated potential data). Two teacher reward models are first trained separately on these datasets to generate pseudo-labels. A multi-head Qwen-2.5-Math-7B model is then trained on the combined data, sharing a base LLM backbone while maintaining separate heads for correctness and potential prediction. During inference, the final reward score is computed as the product of the two head outputs. The model is evaluated using Best-of-N selection on MATH500 and F1 score on ProcessBench.

## Key Results
- Achieved state-of-the-art performance on MATH500 and ProcessBench benchmarks under comparable resource constraints
- Demonstrated that the compound probability of correctness and potential signals outperforms either signal alone
- Showed multi-task learning benefits, with the multi-head model outperforming individual heads trained in isolation
- Validated the orthogonal nature of correctness and potential signals through correlation analysis

## Why This Works (Mechanism)

### Mechanism 1
Integrating backward-looking correctness with forward-looking potential provides a more robust verification signal than either alone. The correctness signal acts as a hard filter for stepwise errors, while the potential signal estimates the likelihood of reaching the correct answer. By multiplying these scores, the system penalizes solutions that are logically sound but mathematically "lost," as well as those that stumbled onto a right answer via a wrong step. This assumes the two reward signals are somewhat orthogonal or at least not perfectly correlated.

### Mechanism 2
Multi-task learning with a shared base model enhances the individual performance of each reward head. Sharing the base LLM representations forces the model to learn features useful for both identifying immediate errors and predicting future success. This "cross-pollination" appears to regularize the model, improving the accuracy of the individual correctness and potential heads compared to training them in isolation. This assumes the underlying features required to judge correctness and potential overlap significantly enough to benefit from shared representation learning.

### Mechanism 3
Modeling the final verification score as a compound probability ($R_{correctness} \times R_{potential}$) effectively applies the chain rule of probability to reasoning paths. The product operation treats the path to a solution as a sequence of dependent events, enforcing that a high-quality solution must maintain both local integrity (correctness) and global promise (potential). This assumes the probability of a step being correct and the probability of it leading to the answer can be treated as distinct probabilities to be chained.

## Foundational Learning

- **Concept: Process Reward Models (PRM) vs. Outcome Reward Models (ORM)**
  - **Why needed here:** DuaShepherd is fundamentally a PRM framework. Understanding that PRMs provide step-level supervision rather than just a final success/failure signal is crucial to grasp why the architecture has separate heads for step evaluation.
  - **Quick check question:** Can you explain why a PRM might fail if it only rewards the final step but ignores intermediate calculation errors?

- **Concept: Monte Carlo (MC) Estimation for Process Supervision**
  - **Why needed here:** The "potential" reward is derived from MC sampling (Math-Shepherd method). Understanding that this score is a statistical estimate of success based on rolling out many completions is crucial for interpreting the training data quality.
  - **Quick check question:** How does the variance of MC rollouts affect the reliability of the "potential" pseudo-labels in the DuaShepherd dataset?

- **Concept: Multi-head Architecture / Multi-task Learning**
  - **Why needed here:** The architecture uses a shared LLM backbone with two distinct output heads. Understanding how shared weights extract features while specific heads specialize is necessary to debug why the model might improve over single-task baselines.
  - **Quick check question:** In a multi-head setup, if the gradients from the "correctness" head are significantly larger than those from the "potential" head, what risk does the training process face?

## Architecture Onboarding

- **Component map:** Raw MATH problems -> Generator solutions -> Teacher A (Correctness) and Teacher B (Potential) annotation -> Multi-head model training -> Inference with product fusion
- **Critical path:** The dataset construction is the most critical step. If the pseudo-labels from the teacher models are low-quality (e.g., F1 < 0.6), the multi-task student model will fail to converge.
- **Design tradeoffs:**
  - **Product vs. Sum:** The authors choose to multiply rewards. A sum would be more tolerant of a single low score, while the product is stricter (one zero kills the score).
  - **Hard vs. Soft Labels:** The architecture requires soft labels for the potential head to prevent "reward collapse" (where a 0 in one label kills the gradient for the other).
- **Failure signatures:**
  - **Reward Collapse:** If training on binary labels directly using the product, the model fails to learn.
  - **Distribution Mismatch:** Performance degrades if the generator produces solutions (e.g., with Chinese text) unlike the PRM training data.
- **First 3 experiments:**
  1. **Sanity Check (Heads):** Train two independent 7B models (one on Correctness data, one on Potential data). Verify they overfit their specific validation sets before attempting multi-task training.
  2. **Ablation (Fusion):** Using the trained DuaShepherd model, evaluate validation performance using $R_c$ only, $R_p$ only, and $R_c \times R_p$. Confirm the product outperforms individuals on MATH500.
  3. **Error Analysis:** Visualize the correlation of the two rewards (scatter plot). Verify that incorrect solutions cluster in low-low quadrants and identify outliers where correctness is high but potential is low.

## Open Questions the Paper Calls Out

**Open Question 1:** Can adaptive weighting or reinforcement learning-based approaches significantly outperform the simple multiplication of reward signals?
- **Basis in paper:** The authors state in the Limitations section that "other mixing strategies (e.g., adaptive weighting or RL-based approaches) remain underexplored and could further enhance stepwise supervision."
- **Why unresolved:** The current framework relies on a static compound probability, which assumes independence and equal importance, potentially failing to optimize the dynamic trade-off between identifying errors and estimating solution potential.
- **What evidence would resolve it:** A comparative study demonstrating that a learnable, adaptive mixing function achieves statistically significant performance gains over the multiplication baseline.

**Open Question 2:** Does the dual-reward paradigm effectively transfer to complex multi-step reasoning tasks outside of mathematics?
- **Basis in paper:** The Conclusion suggests that "future research could investigate... applications to broader domains requiring multi-step reasoning."
- **Why unresolved:** The entire empirical validation is restricted to mathematical datasets. It is unclear if the definitions of "correctness" and "potential" used here generalize to domains with less rigid logic.
- **What evidence would resolve it:** Successful application of the DuaShepherd framework to non-mathematical benchmarks (e.g., logical reasoning or coding tasks), showing consistent improvements over single-reward baselines.

**Open Question 3:** What specific theoretical mechanisms enable the "weak-to-strong" generalization observed when training larger models on multiplication-based labels?
- **Basis in paper:** The authors observe a weak-to-strong pattern where 72B models improve using data from a weak multiplication method, hypothesizing a "deeper, more fundamental signal or rule" but leaving the mechanics undefined.
- **Why unresolved:** The paper documents the empirical success of distilling multiplication-based labels into larger models but does not explain why this specific combination unlocks latent capabilities in stronger models without degradation.
- **What evidence would resolve it:** Mechanistic interpretability analysis comparing activation patterns of 7B versus 72B models trained on this data, identifying specific reasoning features captured by the multiplication method.

## Limitations

- **Signal Quality Dependency:** Performance critically depends on the quality of pseudo-labels from teacher models, particularly the potential teacher whose performance is not explicitly validated.
- **Generalization Across Generators:** Substantial performance variation across different generators suggests the approach may not generalize to generators producing solutions structurally different from the training distribution.
- **Compound Probability Assumptions:** The simple product operation may not be optimal compared to alternatives (weighted sum, learned fusion), and the paper doesn't demonstrate this is the best fusion method.

## Confidence

- **High Confidence:** The mechanism of using multi-task learning with a shared backbone is well-supported by ablation showing independent training performs worse.
- **Medium Confidence:** The claim that orthogonal reward signals provide complementary information is moderately supported by correlation analysis but could be strengthened.
- **Low Confidence:** The specific claim that the compound probability (product) is the optimal fusion method. While it outperforms individual rewards, the paper doesn't compare against other fusion strategies.

## Next Checks

1. **Teacher Model Validation:** Systematically evaluate the potential teacher model's performance on held-out Math-Shepherd data. Compare its correlation with final solution correctness against simpler heuristics to quantify how much signal it actually provides.

2. **Fusion Method Ablation:** Replace the product operation with alternative fusion methods (weighted sum with learned weights, learned neural fusion, hierarchical gating) and evaluate whether the simple product is truly optimal or just better than no fusion.

3. **Generator Distribution Robustness:** Test the DuaShepherd model's performance when trained on a more diverse set of generators or when fine-tuned on synthetic solutions that deliberately violate the language distribution of PRM800K to quantify true generalization limits.