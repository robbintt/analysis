---
ver: rpa2
title: Unifying Dynamic Tool Creation and Cross-Task Experience Sharing through Cognitive
  Memory Architecture
arxiv_id: '2512.11303'
source_url: https://arxiv.org/abs/2512.11303
tags:
- memory
- tool
- arxiv
- agents
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SMITH introduces a unified cognitive architecture that integrates
  dynamic tool creation with cross-task experience sharing through hierarchical memory
  organization. The approach formalizes tool creation as iterative code generation
  within sandbox environments and experience sharing through episodic memory retrieval
  with semantic similarity matching.
---

# Unifying Dynamic Tool Creation and Cross-Task Experience Sharing through Cognitive Memory Architecture

## Quick Facts
- arXiv ID: 2512.11303
- Source URL: https://arxiv.org/abs/2512.11303
- Reference count: 18
- SMITH achieves 81.8% Pass@1 accuracy on GAIA benchmark, outperforming leading baselines

## Executive Summary
SMITH introduces a unified cognitive architecture that integrates dynamic tool creation with cross-task experience sharing through hierarchical memory organization. The approach formalizes tool creation as iterative code generation within sandbox environments and experience sharing through episodic memory retrieval with semantic similarity matching. A curriculum learning strategy based on agent-ensemble difficulty re-estimation optimizes task ordering for maximum transfer effectiveness. Extensive experiments on the GAIA benchmark demonstrate state-of-the-art performance with 81.8% Pass@1 accuracy, significantly outperforming leading baselines including Alita (75.2%) and Memento (70.9%).

## Method Summary
SMITH implements a multi-agent workflow where tasks are decomposed by a planner, code is generated by a developer, and validated by a tester in a sandbox environment. The architecture uses hierarchical memory (procedural, semantic, episodic) with dense-sparse hybrid retrieval for experience sharing. Dynamic tool creation occurs through iterative sandbox debugging, while cross-task transfer leverages semantic memory retrieval. Curriculum learning via proxy-agent difficulty re-estimation optimizes task ordering. The system employs 3-path sampling with LLM-as-judge consensus and is evaluated on the GAIA benchmark using base models including claude-4-sonnet and gpt-4.1.

## Key Results
- SMITH achieves 81.8% Pass@1 accuracy on GAIA benchmark (Level 1: 94.3%, Level 2: 80.2%, Level 3: 61.5%)
- Outperforms leading baselines: Alita (75.2%) and Memento (70.9%)
- Curriculum learning ablation shows -10.3% performance drop without it (81.8% → 71.5%)
- 3-path sampling improves Pass@1 by +3% but triples inference cost

## Why This Works (Mechanism)

### Mechanism 1: Iterative Tool Creation via Sandbox Debugging
- Claim: Dynamic tool creation through structured debug cycles enables capability expansion without predefined tools
- Mechanism: A sandbox environment ⟨E, exec, feedback⟩ executes generated code, returning structured error or success signals. The agent iteratively refines code based on feedback until successful execution, at which point the complete trajectory is stored as a reusable tool memory episode
- Core assumption: Error signals provide sufficient gradient information for code refinement without explicit supervision
- Evidence anchors:
  - [abstract] "formalizes tool creation as iterative code generation within controlled sandbox environments"
  - [section 3.1] Eq. 1-3 formalize the debug-and-refine cycle with feedback-driven updates until ft = ✓
  - [corpus] "Evolving from Tool User to Creator" (FMR=0.508) demonstrates related training-free experience reuse for tool creation, supporting the iterative refinement paradigm
- Break condition: Sandbox feedback becomes uninformative (repeated errors without actionable signals); error cycles exceed iteration limits without convergence

### Mechanism 2: Cross-Task Transfer via Semantic Memory Retrieval
- Claim: Semantically similar tasks enable transfer of execution patterns through embedding-based episodic retrieval
- Mechanism: Experiences are abstracted via Φ into embeddings; at inference, top-k memories are retrieved using similarity scoring r(e_i, τ, s_t) = ⟨Φ({τ, (s_t, ·)}), m_i⟩. Retrieved experiences are concatenated into the policy context
- Core assumption: Assumption 1 (Semantic Task Similarity) — tasks with similar embedding representations share transferable solution structures
- Evidence anchors:
  - [abstract] "experience sharing through episodic memory retrieval with semantic similarity matching"
  - [section 3.2] Eq. 5-6 define retrieval via similarity scoring; Fig. 8 correlation matrix shows dense cross-task retrieval patterns validating the assumption
  - [corpus] "Collaborative Memory" (FMR=0.517) supports memory sharing mechanisms across agents, though does not directly validate semantic similarity transfer
- Break condition: Embedding similarity fails to capture functional equivalence (semantically similar tasks with divergent solution requirements); retrieval noise overwhelms signal

### Mechanism 3: Curriculum Learning via Proxy-Agent Difficulty Re-estimation
- Claim: Agent-ensemble difficulty estimation enables task ordering that maximizes cumulative transfer
- Mechanism: Two proxy agents (Plan-Execute with high bias, ReAct with high variance) predict difficulty distributions over expanded L′ levels; weighted consensus produces re-estimated difficulty for curriculum ordering. Tasks are selected progressively as d increases based on success rates
- Core assumption: Assumption 2 (Task Dependency) — prerequisite tasks exist such that completion improves downstream performance via memory retrieval
- Evidence anchors:
  - [abstract] "curriculum learning strategy based on agent-ensemble difficulty re-estimation"
  - [section 5/Table 3] Ablation shows -10.3% drop without curriculum learning (81.8% → 71.5%), validating contribution
  - [corpus] Weak direct evidence; no corpus papers directly validate curriculum-based difficulty re-estimation for agent systems
- Break condition: Proxy agents share correlated failures; re-estimated difficulty poorly correlates with actual transfer benefit

## Foundational Learning

- Concept: **Sandbox Execution Environments**
  - Why needed here: Tool creation requires safe, isolated code execution with structured feedback. Understanding exec:E×C→E×O and feedback generation is prerequisite for implementing the debug loop
  - Quick check question: Can you explain how a Python sandbox returns both stdout and error tracebacks to an LLM agent?

- Concept: **Embedding-Based Retrieval (Dense-Sparse Hybrid)**
  - Why needed here: SMITH uses text-embedding-3-large + Splade sparse retrieval with Reciprocal Rank Fusion. Understanding hybrid retrieval is necessary for memory system implementation
  - Quick check question: How does Reciprocal Rank Fusion combine dense and sparse retrieval scores?

- Concept: **Multi-Agent Workflow Patterns (Planner-Developer-Tester)**
  - Why needed here: SMITH's architecture requires coordinating specialized sub-agents with distinct procedural memory and retrieval limits
  - Quick check question: What is the difference between the planner's retrieval role and the developer's retrieval role in this architecture?

## Architecture Onboarding

- Component map:
  - Procedural Memory (M_proc) -> Semantic Memory (M_sem) -> Episodic Memory (M_ep)
  - Planner Agent (sub-intent generation, retrieves m=4 memories) -> Developer-Tester Loop (code generation, developer retrieves n=6 memories) -> Critic Ensemble (3-path sampling with LLM-as-judge consensus)

- Critical path:
  1. Curriculum re-estimation orders tasks by proxy-agent difficulty
  2. Per-task: Planner decomposes → Developer generates code → Tester validates in sandbox
  3. Success triggers memory abstraction Φ and storage in M_ep
  4. Subsequent tasks retrieve from M_sem ∪ M_ep via hybrid search

- Design tradeoffs:
  - Cold-start demonstrations reduce exploration variance but may bias toward human patterns (Fig. 4 shows drift toward episodic over semantic memory)
  - 3-path sampling improves Pass@1 by +3% but triples inference cost
  - Per-agent memory repositories enable specialization but complicate cross-agent sharing

- Failure signatures:
  - OCR errors on small digits/symbols (Fig. 9)
  - Repetitive scripting abandoned after long failed iterations
  - Oversized PDFs exceeding context limits
  - Google Maps operations limited by insufficient pretraining

- First 3 experiments:
  1. **Sandbox isolation test**: Verify exec/feedback loop handles syntax errors, runtime exceptions, and timeout conditions correctly
  2. **Retrieval ablation**: Run SMITH with M_sem only vs. M_ep only to measure cold-start vs. accumulated experience contributions
  3. **Curriculum validation**: Compare proxy-agent re-estimated ordering vs. random task ordering on a 20-task subset to quantify transfer benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed shift from semantic to episodic memory retrieval lead to model-specific bias drift or capability degradation over long-term operation?
- Basis in paper: [explicit] Section 5 (Memory Evolution) warns that as agents increasingly prefer self-generated tools, "the system may drift toward model-specific biases and lose the grounding provided by human expertise"
- Why unresolved: The current analysis is limited to the finite curriculum progression of the GAIA benchmark and does not measure long-term stability
- What evidence would resolve it: Longitudinal studies analyzing performance variance and bias metrics over thousands of iterative tasks where self-generated memory fully saturates the retrieval pool

### Open Question 2
- Question: Can failure trajectories be utilized as negative samples to improve agent reasoning without parameter fine-tuning?
- Basis in paper: [explicit] Section 6 suggests "enhanced error utilization" via verifier-based error attribution systems as a promising direction to treat failures as learning signals
- Why unresolved: The current SMITH implementation only accumulates and retrieves successful execution patterns, ignoring the information value in failed attempts
- What evidence would resolve it: Ablation studies comparing standard SMITH against a variant that retrieves and reasons over semantically similar failure cases during planning

### Open Question 3
- Question: Does the SMITH architecture generalize to domains requiring creative problem-solving or multi-modal interactions outside of the GAIA benchmark?
- Basis in paper: [explicit] Section 6 explicitly calls for "broader evaluation across agentic benchmarks" to validate the architecture's generalizability
- Why unresolved: All reported experimental results are confined to the GAIA validation set, leaving performance on other reasoning tasks unknown
- What evidence would resolve it: Benchmarking results on diverse agentic suites (e.g., ScienceWorld, WebShop, or multimodal assistant benchmarks)

## Limitations
- Full procedural memory prompts are not available, making exact reproduction of agent behavior impossible
- Semantic similarity assumption for cross-task transfer lacks direct validation
- Curriculum learning effectiveness relies on proxy agent consensus which may not generalize beyond GAIA benchmark
- Web interaction limitations (Google Maps, PDF handling) indicate domain-specific constraints

## Confidence
- High confidence: Dynamic tool creation via sandbox debugging (well-specified with clear feedback loops)
- Medium confidence: Cross-task transfer via semantic memory retrieval (mechanism clear but validation indirect)
- Medium confidence: Curriculum learning optimization (ablation shows benefit but proxy agent design unproven)

## Next Checks
1. **Sandbox Feedback Quality**: Measure whether error tracebacks from the sandbox provide actionable refinement signals across different error types (syntax vs runtime vs semantic)
2. **Semantic Similarity Validation**: Test retrieval effectiveness by artificially creating task pairs with known solution overlap and measuring retrieval accuracy
3. **Curriculum Robustness**: Compare SMITH performance when using random task ordering vs. proxy-agent re-estimation on a held-out GAIA subset to verify curriculum contribution is not due to task selection bias