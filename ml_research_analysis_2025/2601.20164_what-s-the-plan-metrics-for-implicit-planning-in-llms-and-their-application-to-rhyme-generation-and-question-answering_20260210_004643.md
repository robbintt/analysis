---
ver: rpa2
title: What's the plan? Metrics for implicit planning in LLMs and their application
  to rhyme generation and question answering
arxiv_id: '2601.20164'
source_url: https://arxiv.org/abs/2601.20164
tags:
- steering
- rhyme
- planning
- token
- rhyming
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents methods for detecting and quantifying implicit
  planning in large language models (LLMs), where models appear to plan ahead for
  future tokens even when trained on next-token prediction. The authors propose simple
  activation steering techniques to test whether representations of planned elements
  (like rhyming words or answer nouns) exist at earlier positions and influence intermediate
  token generation.
---

# What's the plan? Metrics for implicit planning in LLMs and their application to rhyme generation and question answering

## Quick Facts
- arXiv ID: 2601.20164
- Source URL: https://arxiv.org/abs/2601.20164
- Authors: Jim Maar; Denis Paperno; Callum Stuart McDougall; Neel Nanda
- Reference count: 40
- Primary result: Demonstrates that steering interventions can manipulate implicit planning representations in LLMs, affecting both final goal tokens and intermediate generation across 23 diverse models

## Executive Summary
This paper introduces methods for detecting and quantifying implicit planning in large language models, where models appear to plan ahead for future tokens despite being trained on next-token prediction. The authors propose simple activation steering techniques to test whether representations of planned elements (like rhyming words or answer nouns) exist at earlier positions and influence intermediate token generation. Using datasets of rhyming poetry and question answering, they apply mean activation difference steering at strategic positions and measure its effects on rhyme family generation, answer word selection, and intermediate token choices. The findings show consistent evidence of implicit planning across model sizes, with larger and instruction-tuned models exhibiting stronger planning abilities.

## Method Summary
The authors use mean activation difference steering to manipulate implicit planning representations in LLMs. They extract steering vectors by computing the mean activation difference between contrasting conditions (e.g., lines rhyming with "sick" vs. "pain") at specific token positions and layers. These vectors are then added to the residual stream during generation to test whether planning representations exist at those positions. The study evaluates effects using rhyme family frequency, regeneration metrics, and analysis of intermediate token choices. They also identify specific attention heads and layers involved in implementing rhyming plans through activation patching experiments, particularly in Gemma models.

## Key Results
- Steering interventions successfully alter target outputs (rhyme families, answer nouns) and affect intermediate token generation across 23 diverse models
- Larger and instruction-tuned models exhibit stronger planning abilities than smaller or base models
- Specific attention heads (L30H3, L31H15) and MLP layers (30-39) in Gemma2 9B implement the planning circuit, with patching recovering 59-93% of steering effects
- Steering at last words affects article selection ("a" vs. "an") before answer nouns are generated, demonstrating backward planning
- The steering intervention works at single positions rather than requiring multi-position intervention

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mean activation difference steering can modify implicit planning representations at specific token positions and layers, causally influencing both final goal tokens and intermediate generation.
- Mechanism: Extract steering vector by computing mean difference between activations at a position for two contrasting conditions, then add this vector to the residual stream at the target layer/position during generation.
- Core assumption: Planning representations are sufficiently localized to specific layers and token positions (not fully distributed).
- Evidence anchors:
  - [abstract] "the generated rhyme (e.g. '-ight') or answer to a question ('whale') can be manipulated by steering at the end of the preceding line with a vector, affecting the generation of intermediate tokens"
  - [section 3.3] Defines steering vector formula with mean activation difference and multiplier m=1.5
  - [corpus] Emergent Response Planning in LLMs (arxiv 2502.06258) reports similar probing evidence that hidden representations encode future outputs beyond next token
- Break condition: If planning representations are highly distributed across positions/layers, single-position steering will have weak or inconsistent effects.

### Mechanism 2
- Claim: Forward planning creates representations at early positions that encode goal token properties; backward planning uses these representations to condition intermediate token generation.
- Mechanism: At line endings (newline or last word), activation patterns encode the planned rhyme family or answer noun. These representations persist and influence subsequent generation via attention mechanisms.
- Core assumption: Models trained on next-token prediction can nonetheless develop representations that anticipate future tokens.
- Evidence anchors:
  - [abstract] "planning representations exist at early positions and influence both the final goal tokens and intermediate generation"
  - [section 1, Def 1.1-1.2] Formal definitions of forward and backward planning
  - [corpus] Alternatives To Next Token Prediction survey (arxiv 2509.24435) notes NTP's limitations for long-term planning, making emergent planning mechanisms notable
- Break condition: If intermediate tokens are generated purely from local context without reference to planned goals, regeneration metrics should show no above-chance recovery.

### Mechanism 3
- Claim: Specific attention heads and MLP layers implement the backward planning circuit by reading planning representations and converting them into output predictions.
- Mechanism: In Gemma2 9B, attention heads L30H3 and L31H15 attend to last word and newline tokens; MLP layers 30-39 convert this information to predictions. Activation patching of these heads recovers 59-93% of steering effect.
- Core assumption: Planning circuits are implemented by a small subset of components rather than the full network.
- Evidence anchors:
  - [section 4.6] "Two attention heads (L30H3, L31H15) play a very important role in backward planning"
  - [section 4.6, Table 1] Patching results showing 59-93% steering effect recovery
  - [corpus] No direct corpus evidence on specific attention head mechanisms; concurrent work Hanna & Ameisen (cited in paper) shows similar findings
- Break condition: If many attention heads contribute equally to planning, patching any small subset should have minimal effect.

## Foundational Learning

- Concept: **Residual stream activation steering**
  - Why needed here: Core intervention method; adding vectors to residual stream modifies model behavior
  - Quick check question: Can you explain why adding a steering vector at layer 27 but not layer 5 might affect generation?

- Concept: **Attention head function and activation patching**
  - Why needed here: Used to identify causal components in planning circuit
  - Quick check question: What does it mean if patching head L30H3 recovers 80% of a steering effect?

- Concept: **Forward vs. backward planning decomposition**
  - Why needed here: Analytical framework distinguishing representation creation from representation use
  - Quick check question: Why does steering affecting article choice ("a" vs. "an") before the noun constitute evidence for backward planning?

## Architecture Onboarding

- Component map:
  - Steering vector computation: mean activation difference across training examples from contrasting conditions
  - Intervention points: last word token (works across all models, lower layers) and newline token (works in select models: Gemma2 9B, Gemma3 27B, middle layers)
  - Circuit components: attention heads (read planning info), MLP layers (convert to predictions)
  - Metrics: rhyme family frequency, regeneration rate, KL divergence, top-1 token differences

- Critical path:
  1. Collect training data for two contrasting conditions (e.g., lines ending in -ick vs. -ain rhymes)
  2. Extract activations at candidate positions/layers
  3. Compute steering vector as mean difference with multiplier (default 1.5)
  4. Apply during generation at single position/layer
  5. Evaluate via rhyme family frequency and regeneration metrics

- Design tradeoffs:
  - Last word steering: more reliable across models, but requires rhyme-relevant word
  - Newline steering: more model-specific (Gemma2 9B, Gemma3 27B), but position-agnostic to content
  - Training data: 85 examples used, but capable models generalize from fewer (Appendix B)
  - Single-position vs. multi-position: current method uses single position; distributed planning may require multi-position intervention

- Failure signatures:
  - Low steering effectiveness (<40% target rhyme frequency): model may have weak planning or wrong layer/position
  - Steered regeneration rate much lower than baseline: steering may not be manipulating planning representation, just forcing final token
  - Metrics not correlating: check for Gemma2 base model idiosyncrasies with KL divergence (Appendix E)

- First 3 experiments:
  1. Replicate rhyme family steering on Gemma2 9B: use -ight vs. -ain pair, steer at last word (layers 10-25), verify ~70-80% target rhyme frequency
  2. Test regeneration metric: generate steered couplets, remove first line, regenerate last wordâ€”confirm target rhyme family appears at above-chance rates
  3. Attempt newline steering on Llama 3.2: expect low effectiveness, validating model-specific newline token role

## Open Questions the Paper Calls Out
None

## Limitations
- The steering intervention assumes planning representations are localized to specific positions and layers, but planning might be more distributed than the current single-position intervention can capture
- Training data constraints (only 85 examples per condition) raise questions about whether observed effects generalize to broader semantic planning tasks
- Model-specific effects (newline steering working in only some models) suggest the intervention may exploit model-specific artifacts rather than revealing universal planning mechanisms
- The paper doesn't fully rule out alternative explanations such as steering vectors simply biasing final token generation rather than genuinely manipulating intermediate planning representations

## Confidence
- High confidence: The core finding that mean activation steering can influence both target outputs and intermediate token choices is well-supported by consistent results across 23 diverse models
- Medium confidence: The claim that specific attention heads (L30H3, L31H15) and MLP layers implement the planning circuit is supported by patching experiments but limited to Gemma2 9B
- Low confidence: The assertion that newline token steering works because models treat it as a planning opportunity is speculative and doesn't explain model-specific inconsistencies

## Next Checks
1. Test multi-position steering interventions that apply vectors at multiple strategic positions to determine whether planning representations are more distributed than single-position interventions suggest

2. Apply attention head patching and ablation studies to Llama and Qwen models to determine whether specific head/Layer combinations identified in Gemma generalize across architectures

3. Design steering experiments for more complex semantic planning tasks (e.g., generating coherent multi-sentence arguments or stories) to test whether rhyme and question-answering planning extends to broader cognitive planning capabilities