---
ver: rpa2
title: 'MASE: Interpretable NLP Models via Model-Agnostic Saliency Estimation'
arxiv_id: '2512.04386'
source_url: https://arxiv.org/abs/2512.04386
tags:
- mase
- saliency
- perturbation
- deep
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MASE (Model-Agnostic Saliency Estimation),
  a novel framework for interpreting deep NLP models through model-agnostic saliency
  estimation. The method addresses the challenge of interpreting NLP models by applying
  perturbations to the embedding layer rather than raw text, using Normalized Linear
  Gaussian Perturbations (NLGP) to efficiently estimate input saliency.
---

# MASE: Interpretable NLP Models via Model-Agnostic Saliency Estimation

## Quick Facts
- arXiv ID: 2512.04386
- Source URL: https://arxiv.org/abs/2512.04386
- Reference count: 40
- Introduces MASE (Model-Agnostic Saliency Estimation) for interpreting deep NLP models through model-agnostic saliency estimation

## Executive Summary
This paper introduces MASE (Model-Agnostic Saliency Estimation), a novel framework for interpreting deep NLP models through model-agnostic saliency estimation. The method addresses the challenge of interpreting NLP models by applying perturbations to the embedding layer rather than raw text, using Normalized Linear Gaussian Perturbations (NLGP) to efficiently estimate input saliency. The key innovation is perturbing the embedding space instead of word-level inputs, which expands the perturbation space from binary to Euclidean and eliminates uncertainties from masking alternatives. MASE provides local explanations without requiring knowledge of the model's internal architecture.

The authors theoretically prove MASE achieves optimal performance in terms of infidelity measure and demonstrate it unifies existing interpretation methods like LIME, SHAP, Occlusion, and Integrated Gradients. They also introduce "Delta Accuracy" as a quantitative evaluation metric. Experimental results on LSTM and BERT models using IMDB and Reuters datasets show MASE consistently outperforms existing model-agnostic methods (LIME, SHAP, PI, and Grad).

## Method Summary
MASE introduces a model-agnostic approach to interpreting NLP models by perturbing the embedding layer rather than the raw text input. The method uses Normalized Linear Gaussian Perturbations (NLGP) to efficiently estimate input saliency by expanding the perturbation space from binary (word presence/absence) to continuous Euclidean space. This approach eliminates uncertainties from choosing masking alternatives and provides more stable and accurate saliency estimates. The framework theoretically proves optimal performance in terms of infidelity measure and unifies existing interpretation methods. MASE introduces "Delta Accuracy" as a quantitative evaluation metric to measure the difference in model accuracy when removing important features versus unimportant ones.

## Key Results
- MASE achieved delta accuracy values of 0.052-0.396 on BERT-IMDB and 0.058-0.409 on BERT-Reuters datasets
- Consistently outperformed existing model-agnostic methods (LIME, SHAP, PI, and Grad) across both LSTM and BERT models
- Demonstrated superior and more stable performance while maintaining computational efficiency
- Theoretically proven to achieve optimal performance in terms of infidelity measure

## Why This Works (Mechanism)
MASE works by perturbing the embedding layer of NLP models rather than the raw text input. This approach expands the perturbation space from binary (word presence/absence) to continuous Euclidean space, allowing for more nuanced and stable saliency estimates. By perturbing embeddings directly, MASE eliminates uncertainties associated with choosing masking alternatives for words and captures the continuous nature of semantic relationships in the embedding space. The method uses Normalized Linear Gaussian Perturbations (NLGP) to efficiently sample the perturbation space and estimate saliency scores. This approach provides local explanations without requiring knowledge of the model's internal architecture, making it truly model-agnostic while achieving optimal performance in terms of the infidelity measure.

## Foundational Learning

**Normalized Linear Gaussian Perturbations (NLGP)**: A method for generating perturbations in the embedding space using Gaussian distributions normalized to ensure stability and efficiency. Needed because traditional binary perturbations are insufficient for capturing semantic relationships in continuous embedding spaces. Quick check: Verify that perturbations maintain proper normalization across different embedding dimensions.

**Infidelity Measure**: A metric for evaluating the quality of saliency explanations by measuring the difference between the original model predictions and predictions made using the saliency-based approximation. Needed to provide theoretical guarantees for explanation quality. Quick check: Calculate infidelity scores for different perturbation magnitudes to verify the theoretical bounds.

**Delta Accuracy**: A quantitative evaluation metric measuring the difference in model accuracy when removing important features versus unimportant ones. Needed because traditional explanation evaluation methods lack direct quantitative measures. Quick check: Compare delta accuracy scores across different explanation methods to verify relative performance.

**Model-Agnostic Interpretation**: The ability to interpret any model without requiring access to its internal architecture or parameters. Needed because most interpretation methods are model-specific and cannot generalize across different architectures. Quick check: Test MASE on models with varying architectures (CNN, RNN, Transformer) to verify agnosticism.

**Embedding Space Perturbation**: The technique of perturbing model embeddings rather than raw input features. Needed because embeddings capture semantic relationships that raw text tokens cannot represent. Quick check: Visualize embedding perturbations to verify they maintain semantic coherence.

## Architecture Onboarding

**Component Map**: Input Text -> Embedding Layer -> MASE Perturbation Generator -> Model Predictions -> Saliency Estimation -> Delta Accuracy Evaluation

**Critical Path**: The critical path flows from input text through embedding layer perturbations, model prediction changes, and saliency estimation. The key bottleneck is the perturbation generation and model evaluation loop, which must be performed multiple times to estimate reliable saliency scores.

**Design Tradeoffs**: MASE trades computational efficiency for explanation quality by perturbing the embedding space rather than raw text. This choice provides more stable and accurate explanations but requires more computation per perturbation. The method also trades off model-specific knowledge for true model-agnosticism, making it applicable to any NLP model but potentially less efficient than specialized methods.

**Failure Signatures**: The method may fail when embeddings contain noisy or unstable representations, when the model is highly sensitive to small embedding perturbations, or when the Gaussian perturbation assumption does not hold for the specific embedding space. Additionally, the method may produce unreliable explanations for models that rely heavily on positional information rather than semantic embeddings.

**First Experiments**:
1. Verify perturbation stability by measuring saliency score variance across multiple perturbation runs
2. Compare delta accuracy scores against baseline methods on a small subset of data
3. Test explanation quality on a simple linear model where ground truth saliency is known

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but several implicit questions arise from the methodology and results. The lack of human evaluation raises questions about the practical interpretability and usefulness of MASE explanations. The method's scalability to larger models and longer documents remains unexplored. Additionally, the theoretical guarantees for infidelity may not hold in all practical scenarios, particularly for models with complex architectures or non-standard embedding spaces.

## Limitations
- Limited to two datasets (IMDB and Reuters) and two model architectures (LSTM and BERT), raising questions about generalizability
- Relies entirely on proxy metrics like delta accuracy without human evaluation of explanation quality
- Theoretical optimality claims are specific to the infidelity metric and may not extend to other aspects of explanation quality
- Computational efficiency may become a concern for very large models or datasets

## Confidence
- Claim of "optimal performance" in terms of infidelity measure: Medium confidence
- Claim of unifying existing interpretation methods (LIME, SHAP, Occlusion, Integrated Gradients): Medium confidence
- Claim of broad applicability to "any NLP model": Medium confidence
- Quantitative improvement over baselines: High confidence

## Next Checks
1. Conduct a human evaluation study comparing MASE explanations with baseline methods, measuring how well users can identify model reasoning and whether the explanations improve trust and understanding
2. Test MASE on a broader range of NLP tasks beyond sentiment analysis and topic classification, including question answering, named entity recognition, and summarization tasks
3. Evaluate MASE's computational efficiency and explanation quality on larger models (e.g., GPT-3, T5) and longer documents to assess scalability and performance under different resource constraints