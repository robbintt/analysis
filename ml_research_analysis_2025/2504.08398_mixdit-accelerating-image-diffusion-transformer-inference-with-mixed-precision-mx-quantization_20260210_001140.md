---
ver: rpa2
title: 'MixDiT: Accelerating Image Diffusion Transformer Inference with Mixed-Precision
  MX Quantization'
arxiv_id: '2504.08398'
source_url: https://arxiv.org/abs/2504.08398
tags:
- mixdit
- quantization
- image
- diffusion
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accelerating inference in
  Diffusion Transformers (DiTs) for image generation tasks, which are notoriously
  compute-intensive due to iterative denoising processes and heavy GEMM operations.
  The authors propose MixDiT, an algorithm-hardware co-designed solution that leverages
  mixed-precision Microscaling (MX) formats for quantizing activation values.
---

# MixDiT: Accelerating Image Diffusion Transformer Inference with Mixed-Precision MX Quantization

## Quick Facts
- arXiv ID: 2504.08398
- Source URL: https://arxiv.org/abs/2504.08398
- Authors: Daeun Kim; Jinwoo Hwang; Changhun Oh; Jongse Park
- Reference count: 15
- Primary result: 2.10-5.32x latency speedup over RTX 3090 with maintained image quality (FID) using mixed-precision MX quantization

## Executive Summary
This paper addresses the computational bottleneck of Diffusion Transformers (DiTs) for image generation by proposing MixDiT, an algorithm-hardware co-designed solution. DiTs are compute-intensive due to iterative denoising processes and heavy GEMM operations, making them challenging for real-time applications. The authors introduce mixed-precision MX quantization that selectively applies higher precision to magnitude-based outliers while using lower precision for the rest, preventing quality degradation from outlier-induced truncation. An offline hyperparameter determination algorithm optimizes the balance between image quality and latency. The proposed MixDiT accelerator features precision-flexible processing elements and MX converters to efficiently support mixed-precision computations.

## Method Summary
MixDiT combines algorithmic innovation with hardware co-design to accelerate DiT inference. The core approach uses mixed-precision MX quantization formats (MX9 for outliers, MX6 for regular values) to reduce the computational cost of activation values while maintaining image quality. The method includes an offline hyperparameter optimization algorithm that determines the optimal precision distribution based on outlier statistics. The hardware accelerator is specifically designed with precision-flexible processing elements capable of handling different MX formats and dedicated MX converters for format transformation. This co-design approach addresses the fundamental tension between computational efficiency and quality preservation in quantized DiT inference.

## Key Results
- Achieves 2.10-5.32x latency speedup over RTX 3090 baseline
- Maintains comparable image quality (FID scores) to FP16 models
- Effective outlier handling through selective MX9 precision application
- Optimization algorithm successfully balances quality-latency tradeoff

## Why This Works (Mechanism)
The effectiveness of MixDiT stems from its intelligent handling of activation outliers in DiT models. Traditional uniform quantization struggles with DiT's activation distributions because large-magnitude outliers cause severe truncation errors that degrade image quality. By employing mixed-precision MX quantization, MixDiT assigns higher precision (MX9) only to these problematic outliers while using lower precision (MX6) for the majority of values. This selective approach significantly reduces computational overhead while preserving critical information needed for high-quality image generation. The offline hyperparameter optimization ensures the precision distribution is tailored to each specific model's activation characteristics, maximizing both efficiency and quality.

## Foundational Learning

**Diffusion Transformers (DiTs)**: A transformer-based architecture for image generation that iteratively denoises latent representations. Needed for understanding the computational challenges in generative image modeling. Quick check: DiTs use self-attention and feed-forward layers in iterative denoising loops.

**MX Quantization Formats**: Microscaling formats that provide variable precision based on value magnitude. Essential for understanding how different precision levels can be selectively applied. Quick check: MX formats can represent both small and large values efficiently within the same framework.

**GEMM Operations**: General matrix multiplication operations that dominate DiT computational workload. Critical for understanding where performance optimizations can have the most impact. Quick check: Most DiT layers consist primarily of GEMM operations between activations and weights.

**Activation Outliers**: Values with unusually large magnitudes in activation distributions that cause quantization errors. Key concept for understanding why uniform quantization fails in DiTs. Quick check: Outliers typically represent less than 5% of activation values but disproportionately affect quality.

**Hardware-Software Co-design**: Integrated approach where algorithmic innovations are matched with corresponding hardware optimizations. Important for understanding the complete solution beyond just quantization. Quick check: Co-design enables optimizations that neither software nor hardware alone could achieve.

## Architecture Onboarding

**Component Map**: Input images -> DiT Model -> MX Quantization (Mixed-precision) -> Hardware Accelerator (Precision-flexible PEs + MX Converters) -> Output images

**Critical Path**: The computational bottleneck is the iterative denoising loop in DiTs, specifically the GEMM operations on activation matrices. The critical path involves: activation computation -> outlier detection -> precision selection -> mixed-precision computation -> result aggregation.

**Design Tradeoffs**: Precision vs. performance tradeoff where higher precision preserves quality but increases computational cost. The mixed-precision approach trades uniform high precision for selective high precision, reducing overall computation while maintaining quality for critical values.

**Failure Signatures**: Quality degradation manifests as artifacts in generated images, particularly when outliers are incorrectly quantized. Performance bottlenecks appear when precision switching overhead exceeds computational savings or when outlier detection becomes imprecise.

**3 First Experiments**:
1. Benchmark baseline DiT inference latency and FID on RTX 3090
2. Characterize activation distributions to identify outlier statistics
3. Implement and validate mixed-precision quantization on a small DiT variant

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unexplored based on the evaluation scope and methodology.

## Limitations
- Evaluation limited to DiT-XL model variant, with unclear generalization to other architectures
- Performance assessment primarily on MS-COCO dataset, limiting understanding of broader applicability
- Hardware accelerator effectiveness based on simulation rather than actual silicon implementation
- No ablation studies quantifying individual contributions of mixed-precision, outlier handling, and hardware optimizations

## Confidence

**High confidence**: Effectiveness of mixed-precision MX quantization for reducing outlier impact while maintaining image quality, supported by experimental results showing improved FID scores compared to uniform quantization.

**Medium confidence**: Latency speedup claims due to lack of real-world hardware deployment and potential discrepancies between simulation and actual performance.

**Medium confidence**: Generalization of results across different DiT models and datasets, given limited evaluation scope on DiT-XL and MS-COCO.

## Next Checks

1. Evaluate MixDiT on additional DiT model variants and diverse datasets to assess generalization and robustness across different architectures and use cases.

2. Implement and test the proposed hardware accelerator on actual silicon to validate simulation-based performance claims and identify any implementation challenges.

3. Conduct ablation studies to quantify the individual contributions of mixed-precision quantization, outlier handling, and hardware optimizations to overall performance improvements.