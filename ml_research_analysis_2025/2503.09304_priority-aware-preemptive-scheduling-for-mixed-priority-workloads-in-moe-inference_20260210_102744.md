---
ver: rpa2
title: Priority-Aware Preemptive Scheduling for Mixed-Priority Workloads in MoE Inference
arxiv_id: '2503.09304'
source_url: https://arxiv.org/abs/2503.09304
tags:
- jobs
- qllm
- inference
- batch
- scheduling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QLLM, an inference system for Mixture-of-Experts
  models that addresses the challenge of efficiently serving mixed-priority workloads
  in data centers. The key innovation is expert-level preemption and priority-aware
  scheduling, which enables latency-sensitive jobs to preempt best-effort jobs at
  any layer, minimizing head-of-line blocking.
---

# Priority-Aware Preemptive Scheduling for Mixed-Priority Workloads in MoE Inference

## Quick Facts
- **arXiv ID:** 2503.09304
- **Source URL:** https://arxiv.org/abs/2503.09304
- **Reference count:** 39
- **Primary result:** Expert-level preemption reduces latency-sensitive jobs' TTFT by up to 101.6x while maintaining throughput and reducing turnaround time by up to 12.8x compared to Hugging Face TGI.

## Executive Summary
This paper introduces QLLM, an inference system for Mixture-of-Experts models that addresses the challenge of efficiently serving mixed-priority workloads in data centers. The key innovation is expert-level preemption and priority-aware scheduling, which enables latency-sensitive jobs to preempt best-effort jobs at any layer, minimizing head-of-line blocking. The system employs a redesigned MoE layer with per-expert queues and a lightweight state management mechanism to enable fine-grained control without sacrificing throughput. QLLM is modular and integrates seamlessly with Hugging Face models. On an Nvidia A100 GPU, QLLM reduces latency-sensitive jobs' time-to-first-token by up to 101.6x (avg. 65.2x) and meets SLOs at up to 7 requests/sec, while maintaining comparable or higher throughput and reducing turnaround time by up to 12.8x compared to Hugging Face TGI.

## Method Summary
QLLM is built on top of Hugging Face TGI and modifies the MoE layer to support per-expert queuing and fine-grained preemption. The system uses a priority-aware scheduler that prioritizes latency-sensitive jobs (LS) over best-effort jobs (BE) during batch formation, implementing a strict hierarchy: LS decode jobs first, then LS prefill, then BE jobs. A unified dynamic cache manages individual sequence states (KV cache, routing info) to enable low-overhead preemption without costly tensor split-merge operations. The scheduler employs a closed-loop controller to manage job admission and execution. Evaluation is performed on Mixtral 8x7B (4-bit quantization) using the ShareGPT dataset with 20% LS and 80% BE requests arriving via Poisson process on an Nvidia A100 80GB GPU.

## Key Results
- QLLM reduces latency-sensitive jobs' time-to-first-token by up to 101.6x (average 65.2x) compared to Hugging Face TGI
- The system maintains SLO compliance at up to 7 requests/sec while the baseline fails at lower rates
- QLLM maintains comparable or higher throughput than the baseline
- Turnaround time is reduced by up to 12.8x for latency-sensitive jobs

## Why This Works (Mechanism)

### Mechanism 1: Expert-Level Preemption
Fine-grained, expert-level preemption significantly reduces head-of-line blocking for latency-sensitive jobs compared to iteration-level scheduling. The system replaces the rigid "run-to-completion" iteration model with per-expert queues. When an LS job arrives, the scheduler can pause BE jobs mid-layer (at the expert level), inject the LS job, and resume BE jobs later, rather than waiting for the full iteration to complete.

### Mechanism 2: Unified State Management
Unified state management via a "Facade Pattern" enables low-overhead preemption by avoiding expensive tensor split-merge operations. Instead of physically concatenating tensors into a monolithic batch (which requires costly slicing to remove a preempted job), QLLM wraps individual sequence states in a unified `Batch` abstraction. This allows the scheduler to logically add/remove sequences from the "batch" view without deep copying or restructuring the underlying tensor memory.

### Mechanism 3: Priority-Aware Batch Selection
Priority-aware batch selection optimizes SLO compliance by strictly preferring LS jobs over BE jobs during batch formation. The scheduler maintains four queues (LS/BE Ã— Prefill/Decode) and implements a strict hierarchy: it fills batches with LS decode jobs first, then LS prefill (to generate more LS decodes), and only fills remaining slots with BE jobs.

## Foundational Learning

- **Mixture-of-Experts (MoE) Routing**: MoE models route tokens dynamically to specific "experts." QLLM exploits this by queueing tokens *per-expert*, allowing granular control. Quick check: Can you explain why preemption at the "expert" level is finer-grained than preemption at the "layer" level?

- **Head-of-Line (HOL) Blocking**: In FCFS iteration scheduling, a large BE job blocks a subsequent small LS job until the iteration finishes. Quick check: In a standard continuous batching system, does a newly arrived high-priority job preempt a currently running token generation step? (Answer: No, it waits for the iteration boundary).

- **KV Cache & State Management**: Preemption requires swapping out the state. You need to distinguish between the static model weights and the dynamic state (KV cache, hidden states) that must be saved/restored for a specific job. Quick check: What specific metadata beyond the KV cache must be preserved to resume a preempted MoE job deterministically? (Hint: See Section 1/3 regarding routing metadata).

## Architecture Onboarding

- **Component map**: Dispatcher -> Batch Engine -> MoE Layer Wrapper -> Unified Dynamic Cache -> Inference Engine

- **Critical path**: The latency optimization path is: *LS Job Arrival -> Dispatcher (Queue Insert) -> Batch Engine (Preemption Signal) -> MoE Layer (Context Switch/Checkpoint) -> LS Execution*

- **Design tradeoffs**:
  - **Latency vs. Throughput**: Optimized heavily for LS latency (up to 101x improvement) at the risk of slight BE throughput degradation
  - **Complexity vs. Control**: Adds significant complexity to the inference engine layer (custom MoE wrappers) to gain scheduling control

- **Failure signatures**:
  - **Memory Bloat**: If preemption is frequent and state (routing weights, hidden states) is not flushed efficiently, VRAM usage may spike
  - **Starvation**: BE jobs stuck in queues if `LS_Prefill` or `LS_Decode` queues are never empty

- **First 3 experiments**:
  1. **SLO Compliance Test**: Reproduce Figure 2. Send mixed traffic (20% LS) at increasing RPS and verify if the baseline (HF TGI) breaks the 3s SLO while QLLM maintains it
  2. **Preemption Overhead Microbenchmark**: Isolate a single layer. Measure the time taken to "pause" a batch of BE jobs (save state) vs. the time taken to simply complete the iteration. Ensure the delta is < the iteration time
  3. **BE Starvation Test**: Flood the system with 100% LS requests and verify that BE requests eventually complete (liveness check) and measure the max latency penalty

## Open Questions the Paper Calls Out

- **Mitigating BE starvation**: The authors explicitly list "mitigate potential starvation in certain workloads" as a limitation and a focus for future research. While the scheduler supports BE jobs, the preemption logic may allow aggressive LS arrival patterns to indefinitely delay BE tasks.

- **Reducing memory overhead**: The paper notes that compared to iteration-level preemption, the approach requires caching additional states (routing weights, hidden states), and "efficient memory management... could be a subject for future work."

- **Overlapping memory operations**: The authors state that the new MoE layer's flexibility "enables opportunities for overlapping memory operations with concurrent task execution," and suggest exploring this optimization.

- **Dense model applicability**: The paper claims applicability to dense models but acknowledges that MoE models "necessitate more sophisticated state management," implying the abstraction layer might introduce different cost-benefit trade-offs in dense architectures.

## Limitations

- **Unverified preemption overhead**: While expert-level preemption is claimed to be "low-overhead," the specific implementation details for saving/restoring partial expert computations are abstracted and the actual overhead is unverified.

- **Hardware and scale limitations**: Evaluation was conducted on a single Nvidia A100 80GB GPU with Mixtral 8x7B; no evidence provided for multi-GPU or multi-node scaling.

- **Starvation risk under extreme load**: The system prioritizes LS jobs which can lead to BE job starvation, though the paper only quantifies a 1.38x increase in BE turnaround time under their test conditions.

## Confidence

- **Expert-level preemption reduces HOL blocking**: High Confidence - Core mechanism with clear demonstration (101.6x TTFT reduction)
- **Unified state management enables low-overhead preemption**: Medium Confidence - Conceptually sound but implementation details unverified
- **Priority-aware batch selection optimizes SLO compliance**: High Confidence - Explicitly defined algorithm with clear impact (SLO compliance at 7 req/sec)

## Next Checks

1. **SLO Compliance Validation**: Reproduce the mixed-priority workload test (20% LS, Poisson arrival) at increasing request rates (1, 3, 5, 7 req/sec) to verify if QLLM maintains the 3-second SLO while the Hugging Face TGI baseline fails, as shown in Figure 2.

2. **Preemption Overhead Microbenchmark**: Isolate a single MoE layer and measure the time taken to pause a batch of BE jobs (save expert-level state: hidden states, routing weights) versus the time to complete the full iteration. This validates the "low-overhead" claim by ensuring the context-switch cost is less than the iteration time.

3. **Starvation and Liveness Test**: Run a sustained 100% LS workload to confirm that BE jobs eventually complete (liveness) and measure the maximum latency a BE job experiences, quantifying the starvation risk under extreme priority imbalance.