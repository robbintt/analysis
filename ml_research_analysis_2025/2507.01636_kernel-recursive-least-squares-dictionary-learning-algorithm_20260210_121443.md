---
ver: rpa2
title: Kernel Recursive Least Squares Dictionary Learning Algorithm
arxiv_id: '2507.01636'
source_url: https://arxiv.org/abs/2507.01636
tags:
- dictionary
- algorithm
- data
- kernel
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel online kernel dictionary learning algorithm
  based on the recursive least squares (RLS) approach. The method learns a dictionary
  in a feature space induced by a Mercer kernel, where input signals are nonlinearly
  mapped and represented sparsely using a virtual dictionary.
---

# Kernel Recursive Least Squares Dictionary Learning Algorithm

## Quick Facts
- arXiv ID: 2507.01636
- Source URL: https://arxiv.org/abs/2507.01636
- Reference count: 40
- Proposes online kernel dictionary learning using recursive least squares for faster convergence than gradient-based methods

## Executive Summary
This paper introduces KRLS-DL, a novel online kernel dictionary learning algorithm that leverages recursive least squares (RLS) for efficient dictionary updates in feature space. The method learns sparse representations of input signals mapped nonlinearly via a Mercer kernel, using a virtual dictionary constructed from previous samples. At each iteration, new samples are sparsely approximated and the dictionary is updated recursively using RLS, with profile abstraction strategies to control memory usage. Experimental results demonstrate that KRLS-DL outperforms existing online kernel dictionary learning algorithms in both accuracy and computational efficiency across four diverse datasets.

## Method Summary
KRLS-DL operates by maintaining a profile of representative samples that form a virtual dictionary in feature space. For each new mini-batch of input samples, the algorithm performs sparse approximation using kernel recursive orthogonal matching pursuit (KORMP) to find coefficient vectors. The dictionary is then updated recursively using RLS with Matrix Inversion Lemma-based updates for computational efficiency. Profile abstraction controls memory growth through coherency-based sample addition and contribution-based pruning, restricting the profile to a maximum size Lmax. The algorithm is kernel-agnostic but uses a quadratic polynomial kernel in experiments, with classification performed via minimum reconstruction error across per-class dictionaries.

## Key Results
- KRLS-DL achieves classification accuracy close to batch-trained models while being significantly more efficient computationally
- RLS-based updates converge approximately an order of magnitude faster than gradient-based online kernel dictionary learning methods
- Profile abstraction strategies maintain tractable memory usage without significant performance degradation
- The method outperforms existing online kernel dictionary learning algorithms on four datasets across different domains

## Why This Works (Mechanism)

### Mechanism 1: Recursive Least Squares for Faster Convergence
The algorithm uses RLS-based dictionary updates that converge approximately an order of magnitude faster than gradient-based methods. This is achieved through recursive updates of the inverse covariance matrix C using the Matrix Inversion Lemma, avoiding full matrix inversion at each step. The special definition of regularization factor ξ_i = γ∏_{λ_j ∈ Ω_i} λ_j enables exact recursive relationships that reduce complexity from O(QL²) to O(L²) for single samples.

### Mechanism 2: Profile Abstraction Controls Unbounded Memory Growth
The method restricts profile size through coherency-based growing and contribution-based pruning. New samples are added only if they pass an informativeness threshold based on maximum coherence with existing samples. Pruning removes samples with minimum contribution to the reconstruction, but only from the first half of stored samples, maintaining tractable memory while preserving classification accuracy.

### Mechanism 3: Implicit Feature-Space Operations via Kernel Trick
Dictionary learning in potentially infinite-dimensional RKHS reduces to finite kernel matrix operations without explicit φ(·) mappings. The virtual dictionary D* = ΦU^T is never explicitly computed. Instead, all operations use the kernel matrix K = Φ^TΦ and Gram matrix Ψ = U K U^T, making the method practical for kernels producing infinite-dimensional feature spaces.

## Foundational Learning

- **Reproducing Kernel Hilbert Spaces (RKHS) and the Kernel Trick**: Understanding why we never compute φ(x) explicitly is essential as the entire algorithm operates in feature space F which may be infinite-dimensional. Quick check: Given only kernel function k(x,y), how would you compute the inner product <D·w, φ(x)> in feature space? (Answer: Use h = U k where k = [k(x₁,x), ..., k(x_L,x)]^T)

- **Matrix Inversion Lemma (Woodbury Identity)**: All recursive updates depend on MIL. Without this, each iteration would require O(Q³) matrix inversion. Quick check: If (A + uv^T) is a rank-1 update to invertible A, what is the update rule for its inverse? (Answer: A^{-1} - (A^{-1}u)(1 + v^T A^{-1}u)^{-1}(v^T A^{-1}))

- **Sparse Approximation (Matching Pursuit variants)**: The KORMP algorithm provides coefficient vectors w with exactly s non-zero elements. Understanding greedy selection criteria is prerequisite to debugging the SA step. Quick check: Why does ORMP outperform standard OMP for correlated dictionary atoms? (Answer: Order-recursive selection orthogonalizes residual against all previously selected atoms, not just the current one.)

## Architecture Onboarding

- **Component map**: Profile -> Sparse Approximation (KORMP) -> Growing module (add samples) -> Pruning module (remove low-contribution) -> Normalization module
- **Critical path**: 1) Initialize profile with Q random samples; 2) For each mini-batch: check informativeness, perform KORMP sparse approximation, apply growing updates, prune if L > Lmax, periodic normalization; 3) Classify test samples via minimum reconstruction error across per-class dictionaries
- **Design tradeoffs**: Lmax (profile budget): Larger → better accuracy but O(L²) memory/time; M (mini-batch size): M=1 → pure matrix-vector ops; λ (forgetting factor): Linear ramp 0.98→1 over 80% training; s (sparsity level): s=5 in experiments; kernel choice: Quadratic polynomial used, RBF produces infinite F
- **Failure signatures**: Singularity in pruning when (λ_m^(-1) - w_m^T u_m) is singular; all-zero rows in W indicate unused dictionary atoms; accuracy plateau suggests profile not growing or pruning too aggressive; memory overflow indicates Lmax too large
- **First 3 experiments**: 1) Sanity check with M=1, Lmax=50 on USPS subset comparing to batch KMOD; 2) Profile budget sweep testing Lmax ∈ {50, 100, 150, 200, 300} on all datasets; 3) Convergence speed comparison against OKDLRS and OKDLFB measuring batches to reach 95% of batch accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the algorithm's convergence and classification accuracy compare when using an RBF kernel versus the polynomial kernel used in the experiments?
- **Basis in paper**: While the authors state the algorithm is not restricted to a specific kernel, experimental results are restricted to a quadratic polynomial kernel.
- **Why unresolved**: The behavior of RLS updates in the infinite-dimensional feature space of an RBF kernel is not empirically validated.
- **What evidence would resolve it**: Experimental results comparing classification accuracy on the same datasets using RBF kernels.

### Open Question 2
- **Question**: Does the heuristic of restricting pruning candidates to the "first half" of the stored profile degrade performance in non-stationary environments with concept drift?
- **Basis in paper**: The authors mention pruning only from the first half of data samples without theoretical justification or analysis of alternative strategies.
- **Why unresolved**: Restricting pruning to older data may discard relevant recent information if the underlying data distribution shifts over time.
- **What evidence would resolve it**: Ablation studies comparing the "first half" strategy against global pruning strategies on datasets with temporal distribution shifts.

### Open Question 3
- **Question**: Can the sparse representations learned in the feature space be effectively inverted to reconstruct signals in the original input space?
- **Basis in paper**: Section 2.2 explicitly states it is not generally possible to go from the feature space representation back to the input space.
- **Why unresolved**: The lack of a pre-image solution limits the algorithm's applicability to tasks requiring signal synthesis or denoising in the input domain.
- **What evidence would resolve it**: Development and validation of an approximate pre-image algorithm compatible with the KRLS DL updates.

## Limitations

- Kernel-specific performance generalization: Experimental validation uses only quadratic polynomial kernels, performance with RBF or other kernels remains unknown
- Profile abstraction parameters: Key thresholds (δ for coherency, Lmax for profile size) are tuned empirically without systematic sensitivity analysis
- Computational complexity claims: O(L²) per-iteration complexity assumes M=1 batch size and well-conditioned matrices; larger batches or ill-conditioned scenarios may degrade performance

## Confidence

- **High confidence**: Basic algorithmic framework (recursive updates, kernel trick, sparse approximation) - core mathematical derivations are sound
- **Medium confidence**: Profile abstraction effectiveness - the pruning/growing strategy appears effective but depends on empirical parameters
- **Medium confidence**: Computational efficiency claims - while MIL-based updates are theoretically efficient, real-world performance depends on implementation details

## Next Checks

1. **Parameter sensitivity analysis**: Systematically vary δ (0.001-0.1), Lmax (50-500), and s (3-10) on USPS dataset to identify optimal operating regions and assess robustness
2. **Kernel generalization test**: Implement and evaluate with RBF kernel (multiple bandwidths) on ISOLET dataset to verify kernel-agnostic performance claims
3. **Scalability assessment**: Test algorithm on synthetic datasets with controlled feature dimensionality and sample sizes to validate O(L²) complexity claims and identify breaking points