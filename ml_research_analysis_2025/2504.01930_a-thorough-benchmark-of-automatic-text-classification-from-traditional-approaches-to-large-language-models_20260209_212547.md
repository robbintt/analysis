---
ver: rpa2
title: 'A thorough benchmark of automatic text classification: From traditional approaches
  to large language models'
arxiv_id: '2504.01930'
source_url: https://arxiv.org/abs/2504.01930
tags:
- llms
- traditional
- slms
- methods
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive benchmark of automatic text
  classification (ATC) methods, comparing traditional approaches (SVM, Logistic Regression,
  Random Forests), small language models (RoBERTa, BERT, BART, XLNet), and large language
  models (LLaMA, Mistral, BloomZ, DeepSeek). Across 22 datasets, LLMs achieved the
  highest Macro-F1 scores in 21 out of 22 cases, outperforming traditional methods
  by 7.2% on average (up to 26%) and SLMs by 1.94% on average.
---

# A thorough benchmark of automatic text classification: From traditional approaches to large language models

## Quick Facts
- arXiv ID: 2504.01930
- Source URL: https://arxiv.org/abs/2504.01930
- Authors: Washington Cunha; Leonardo Rocha; Marcos André Gonçalves
- Reference count: 40
- Primary result: LLMs achieved highest Macro-F1 scores in 21/22 datasets, outperforming traditional methods by 7.2% on average while being 590x slower

## Executive Summary
This paper presents a comprehensive benchmark comparing automatic text classification methods across three model families: traditional approaches (SVM, Logistic Regression, Random Forests), small language models (RoBERTa, BERT, BART, XLNet), and large language models (LLaMA, Mistral, BloomZ, DeepSeek). Across 22 diverse datasets, LLMs achieved the highest Macro-F1 scores in 21 out of 22 cases, demonstrating superior effectiveness but dramatically higher computational costs - 590x slower than traditional methods and 8.5x slower than SLMs. The study reveals that while LLMs provide maximum effectiveness, SLMs like RoBERTa offer near-optimal cost-effectiveness trade-offs, and traditional methods remain competitive for lexically-straightforward tasks. Based on these findings, the authors recommend LLMs for applications requiring maximum effectiveness when computational costs can be afforded, traditional methods for resource-limited applications, and SLMs for a near-optimal effectiveness-efficiency balance.

## Method Summary
The benchmark evaluates three model families on 22 datasets using Macro-F1 as the primary effectiveness metric and total training/inference time as the computational cost measure. Traditional methods use TF-IDF features with SVM, LR, and RF classifiers; SLMs use RoBERTa, BERT, BART, and XLNet with learning rate 5e-5, max 20 epochs, and patience 5; LLMs use 4-bit quantized LLaMA 2/3.1, Mistral, BloomZ, and DeepSeek with QLoRA/PEFT fine-tuning at learning rate 2e-4, 4 epochs, batch size 4, and max length 256. Experiments run on Intel i7-5820K, 64GB RAM, and NVIDIA RTX 3090 (24GB) with 10-fold cross-validation for standard datasets and 5-fold for large datasets. Statistical significance is assessed via paired t-tests with Bonferroni correction at 95% confidence.

## Key Results
- LLMs achieved highest Macro-F1 scores in 21/22 datasets, outperforming traditional methods by 7.2% on average (up to 26%) and SLMs by 1.94% on average
- Computational cost ratios: LLMs were 590x slower than traditional methods and 8.5x slower than SLMs
- SLMs like RoBERTa achieved statistical ties with LLMs on 5/22 datasets while maintaining 8.5x lower computational cost
- Traditional models showed competitive performance in specific datasets (20NG, WOS-5736) where simpler representations sufficed

## Why This Works (Mechanism)

### Mechanism 1
LLMs achieve superior ATC performance through scale-enabled semantic representations that capture deeper linguistic patterns than traditional approaches. LLMs (7-8B parameters) with autoregressive pre-training on massive corpora produce contextualized representations that better separate classes in semantic space, particularly for complex tasks like sentiment analysis where SST1 showed 26% improvement over traditional methods. The pre-training scale and architecture (not just fine-tuning methodology) drives the effectiveness gains observed.

### Mechanism 2
SLMs achieve near-optimal cost-effectiveness through architectural optimizations that preserve most semantic capacity at ~125-350M parameters. RoBERTa's bidirectional encoding with optimized pre-training (dynamic masking, larger batches, longer sequences) captures sufficient semantic complexity for most ATC tasks while maintaining 8.5x lower computational cost than LLMs. The RoBERTa architecture specifically (not all SLMs) provides this balance; XLNet's permutation objective shows 2x higher training time.

### Mechanism 3
Traditional methods remain competitive when classification signals are lexically explicit and class distributions are balanced. TF-IDF representations with linear models (SVM, LR) effectively capture term-class correlations when discriminative terms are surface-level (e.g., "hockey" → sports class), avoiding the overhead of deep semantic processing. The TF-IDF feature engineering approach, while simpler, does not inherently limit performance on lexically-straightforward tasks.

## Foundational Learning

- **Macro-F1 for Imbalanced Classification**: Why needed here - Paper uses Macro-F1 specifically because "dataset skewness" across 22 datasets (from "balanced" to "extremely imbalanced") requires class-averaged metrics. Quick check: Given a 95/5 class imbalance where a model achieves 95% accuracy by predicting only the majority class, would Macro-F1 correctly penalize this? (Answer: Yes, Macro-F1 averages per-class F1, so the minority class F1=0 would pull down the average significantly.)

- **Fine-tuning vs. Zero-shot Transfer**: Why needed here - Paper notes "fine-tuning for specific tasks is essential for optimal performance" while acknowledging SLMs/LLMs have some zero-shot capability. Quick check: Why would a model fine-tuned on a 10,000-sample sentiment dataset outperform its zero-shot version on that same task? (Answer: Fine-tuning adapts pre-trained representations to task-specific decision boundaries and vocabulary distributions.)

- **Cost-Benefit Analysis in Model Selection**: Why needed here - The paper's central contribution is quantifying the 590x/8.5x computational cost multiplier for LLMs vs. traditional/SLM approaches. Quick check: If an application processes 1000 documents/day with strict latency requirements (<100ms/doc), which model tier is appropriate based on Table 3 evidence? (Answer: Traditional methods—DBLP shows ~118s for ~38K docs = 3ms/doc; even SLMs would exceed 100ms at 2354s/38K = 62ms/doc for RoBERTa.)

## Architecture Onboarding

- **Component map**: Text Input → Preprocessing → Feature Extraction → Classification → Evaluation → [tokenization] → [TF-IDF / Transformer] → [Linear / Neural] → [Encoder Weights] → [Fine-tuning Loop]
- **Critical path**: Start with RoBERTa-base as default—Table 2 shows it achieves statistical ties with LLMs on 5/22 datasets while maintaining Figure 2's quadrant 1 positioning. Establish baseline metrics using LSVM + TF-IDF (fastest, ~3ms/doc) to determine if task complexity warrants heavier models. Escalate to LLMs only if: (a) Macro-F1 gap >5% matters for application, AND (b) 8.5x longer training time is acceptable
- **Design tradeoffs**: Batch size vs. Memory - Paper used batch_size=16/32 for SLMs, batch_size=4 for LLMs with 4-bit quantization on 24GB GPU—larger batches may require gradient accumulation. Sequence length vs. Coverage - max_len=128-256 chosen; longer sequences improve coverage but scale quadratically for attention. Statistical rigor vs. Compute - 10-fold cross-validation on large datasets (MEDLINE: 860K docs) becomes prohibitive—paper uses 5-fold for scalability
- **Failure signatures**: LLM fine-tuning convergence issues - If learning rate >2e-4 or batch size too small, QLoRA adapters may overfit—monitor validation loss plateau. Traditional methods underperforming on sentiment - Check if TF-IDF vocabulary covers negation terms; consider n-grams (2-3) if not already included. Memory overflow on LLMs - With 24GB VRAM, 7B model with 4-bit quantization should fit; if OOM occurs, reduce max_len or enable gradient checkpointing
- **First 3 experiments**: Establish traditional baseline - Run LSVM + TF-IDF with C∈[0.125, 8192] grid search on your dataset; if Macro-F1 >85% and training <60s, consider stopping here. SLM comparison - Fine-tune RoBERTa-base with learning_rate=5e-5, max_len=256, batch_size=16, patience=5 epochs; compare Macro-F1 improvement vs. 14x longer training time. LLM escalation test - Only if SLM improvement <3% over traditional AND task involves complex semantics (sentiment, sarcasm), test LLaMA-3.1-8B with QLoRA, learning_rate=2e-4, batch_size=4; expect ~2% additional gain at 8.5x SLM cost

## Open Questions the Paper Calls Out

- Can adaptive strategies that integrate SLMs and LLMs based on document difficulty achieve near-optimal effectiveness-efficiency trade-offs? The paper intends to integrate SLMs and LLMs based on document difficulty, seeking the effectiveness-efficiency trade-off, but does not explore hybrid approaches that dynamically select models based on input characteristics.

- How do closed-source proprietary LLMs (e.g., GPT-4, Claude) compare to open-source LLMs in ATC effectiveness and cost-effectiveness? The paper focuses on open-source LLMs as closed-source and proprietary LLMs are black boxes that prevent understanding of training or internal structure, leaving their relative performance unexplored.

- What dataset characteristics determine when traditional methods remain competitive versus when LLMs provide substantial gains? The paper reports performance differences but does not investigate correlations between dataset features (dimensionality, class balance, semantic complexity) and model relative performance.

## Limitations
- Dataset Representativeness: 22 datasets may not fully represent all ATC scenarios, with limited representation of specialized domains like medical text classification and code generation
- Hardware Constraints: RTX 3090 (24GB VRAM) may not reflect enterprise-grade hardware that could reduce the computational cost gap between model families
- Evaluation Scope: Focus exclusively on Macro-F1 as effectiveness metric may miss other relevant measures like Micro-F1 for imbalanced datasets or calibration metrics

## Confidence
- **High Confidence**: The observation that LLMs outperform other approaches in 21/22 cases (7.2% average improvement over traditional methods, 1.94% over SLMs) is well-supported by experimental design and statistical testing
- **Medium Confidence**: The recommendation framework (LLMs for maximum effectiveness, traditional for resource-limited, SLMs for balance) is logically sound but relies on assumptions about cost-sensitivity that may vary by application domain
- **Low Confidence**: The paper provides limited mechanistic insight into why specific architectural choices yield different effectiveness-efficiency trade-offs, and the assertion that 7-8B parameters are optimal for ATC tasks is not empirically validated across different parameter scales

## Next Checks
1. **Hardware Scaling Experiment**: Reproduce the DBLP dataset experiments across different hardware configurations (RTX 3090, A100-40GB, H100-80GB) to quantify how VRAM capacity affects the LLM-traditional cost ratio
2. **Cross-Domain Transfer Test**: Apply the best-performing models from each family to a held-out specialized dataset (e.g., medical text classification or code generation) not included in the original 22 datasets
3. **Threshold Sensitivity Analysis**: Systematically vary the effectiveness improvement threshold required to justify LLM adoption and recalculate the percentage of datasets where each model family is recommended