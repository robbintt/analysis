---
ver: rpa2
title: Dynamic benchmarking framework for LLM-based conversational data capture
arxiv_id: '2502.04349'
source_url: https://arxiv.org/abs/2502.04349
tags:
- user
- agent
- data
- conversational
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a dynamic benchmarking framework to evaluate
  LLM-based conversational agents through interactions with synthetic users. The framework
  simulates user behaviors to assess key dimensions like information extraction, context
  awareness, and adaptive engagement.
---

# Dynamic benchmarking framework for LLM-based conversational data capture

## Quick Facts
- arXiv ID: 2502.04349
- Source URL: https://arxiv.org/abs/2502.04349
- Authors: Pietro Alessandro Aluffi; Patrick Zietkiewicz; Marya Bazzi; Matt Arderne; Vladimirs Murevics
- Reference count: 40
- One-line primary result: Adaptive questioning improves data extraction accuracy from 0% to 60% for ambiguous user inputs

## Executive Summary
This paper introduces a dynamic benchmarking framework for evaluating LLM-based conversational agents through interactions with synthetic users. The framework simulates user behaviors to assess key dimensions like information extraction, context awareness, and adaptive engagement. Using a loan application scenario, the authors compare one-shot extraction (without follow-ups) versus adaptive extraction (with clarifying questions). Results show that adaptive strategies significantly improve data extraction accuracy, especially when handling ambiguous user responses, achieving up to 60% correctness with ambiguous users compared to 0% in the one-shot setup. The framework offers a scalable, automated approach for evaluating conversational agents in realistic, multi-turn dialogues.

## Method Summary
The framework evaluates LLM conversational agents on structured data extraction through multi-turn dialogues with synthetic users. It uses GPT-4o as both the conversational agent and synthetic users, operating on a 20-field loan application data model. Two experimental conditions are tested: one-shot extraction (no follow-ups allowed) and adaptive extraction (with clarifying questions). The evaluation loop iterates between user messages and agent responses, parsing and updating the data model at each step. Performance is measured using completeness, correctness, unclear scores, and field-level accuracy, computed as averages over 20 runs per condition.

## Key Results
- Adaptive follow-up questioning improved data extraction accuracy from 0% to 60% for ambiguous user inputs
- Completion rates increased from 55% to 70% when using adaptive extraction with ambiguous users
- One-shot extraction achieved 80% correctness for standard users but failed completely (0%) with ambiguous users

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive follow-up questioning improves data extraction accuracy for ambiguous user inputs
- Mechanism: When the agent detects incomplete or vague responses, it generates targeted clarification prompts rather than marking fields as "unclear." This iterative loop continues until all fields reach resolution or a step limit is reached.
- Core assumption: Users will provide clearer information when prompted with specific follow-up questions (the paper instructs synthetic ambiguous users to resolve ambiguity if directly prompted).
- Evidence anchors:
  - [abstract] "correctness rates increasing from 0% to 60% and completion rates from 55% to 70%"
  - [section 4.2] "The correctness rate increased to 60%... completion rate rose to 70%... overall unclear percentage for Ambiguous Users was reduced to 0%"
  - [corpus] Weak direct evidence; related papers (MATRIX, PSYCHE) focus on clinical dialogue evaluation but don't specifically test clarification loops.
- Break condition: If users refuse to clarify or provide consistently ambiguous responses despite prompting, the mechanism fails. Also breaks if step limit P is too low for complex fields.

### Mechanism 2
- Claim: Synthetic user profiles with behavioral variation expose agent performance boundaries systematically
- Mechanism: Two distinct LLM-based personas (standard vs. ambiguous) simulate response patterns. Standard users provide clear data; ambiguous users introduce vagueness. This creates a reproducible test harness for agent robustness.
- Core assumption: Synthetic user behavior adequately approximates real user ambiguity patterns.
- Evidence anchors:
  - [section 3.3] "Ambiguous users provide responses that are credible yet vague or incomplete, compelling the agent to seek clarification"
  - [section 4.1] "correctness rate dropped to 0%... overall unclear percentage rose to 55%" for ambiguous users under one-shot conditions
  - [corpus] PSYCHE and MATRIX papers similarly use multi-agent simulation for clinical conversation evaluation, suggesting pattern validity.
- Break condition: If synthetic user behavior distributions don't match real user populations, benchmark results won't generalize. The paper acknowledges this as a limitation.

### Mechanism 3
- Claim: State-aware data model tracking enables targeted extraction and reduces redundant questioning
- Mechanism: The agent maintains awareness of: (1) collected fields, (2) incomplete fields, (3) unclear fields. This state is passed into each agent iteration, allowing context-appropriate follow-ups rather than generic prompts.
- Core assumption: The LLM can accurately parse user responses and update the data model without extraction errors.
- Evidence anchors:
  - [section 3.2] "At every step of the conversation, the agent is aware of the current state of the data collection"
  - [section 4.1] Field-level accuracy varied: "email (95%) and full name (100%), but poor results in fields related to loan details, which had 0% accuracy"
  - [corpus] Memoria paper addresses agentic memory for maintaining context, suggesting this is a recognized challenge area.
- Break condition: Nested fields (e.g., postal code history with year pairs) cause extraction failures even with state tracking, as noted in results.

## Foundational Learning

- Concept: Multi-turn dialogue state management
  - Why needed here: The framework tracks conversation history O and data model L across iterations; understanding how to structure and pass this state is essential for implementing the agent loop.
  - Quick check question: Can you explain how the agent determines which fields remain incomplete after each user response?

- Concept: Few-shot vs. one-shot prompting strategies
  - Why needed here: The evaluation explicitly compares one-shot extraction (no follow-ups allowed) against adaptive extraction (few-shot with clarification). Understanding this distinction is required to interpret the experimental design.
  - Quick check question: What is the behavioral difference in agent instructions between Figure 1 (one-shot) and Figure 2 (adaptive) extraction modes?

- Concept: Ground truth evaluation for structured extraction
  - Why needed here: The framework compares populated data model L against predefined ground truth profiles to compute correctness and completeness scores.
  - Quick check question: How would you construct a ground truth profile for a new domain beyond loan applications?

## Architecture Onboarding

- Component map:
  - Data Model (L) -> Conversational Agent -> Synthetic User Module -> Evaluation Engine
  - Loop: Conversation history O -> Agent state -> User response -> L update

- Critical path:
  1. Initialize agent with empty data model
  2. Synthetic user sends initial message M0
  3. Agent generates response based on L state and conversation history O
  4. Parse agent response to update L (mark fields as populated or "unclear")
  5. If adaptive mode: agent asks follow-up questions for unclear fields
  6. Repeat until completion or step limit
  7. Compare final L against ground truth for metrics

- Design tradeoffs:
  - One-shot vs. adaptive: One-shot is faster but fails on ambiguity (0% correctness for ambiguous users); adaptive improves accuracy but increases conversation length
  - Field complexity vs. completion rate: Nested fields (residence history) reduce accuracy; simpler schemas improve completion
  - Synthetic user realism vs. reproducibility: More behavioral variation increases realism but reduces benchmark consistency

- Failure signatures:
  - High unclear percentage (>50%) indicates agent cannot resolve ambiguity within step limit
  - Low field-level accuracy on specific fields (e.g., nested structures) suggests schema design issues
  - Large gap between standard and ambiguous user performance indicates poor adaptive capability
  - Correctness < completion suggests data quality issues (fields filled incorrectly)

- First 3 experiments:
  1. Reproduce baseline: Run 20 iterations each for standard and ambiguous users under one-shot extraction to verify published metrics (standard: 80% correctness; ambiguous: 0% correctness, 55% unclear)
  2. Test adaptive improvement: Run same 40 iterations under adaptive mode to confirm correctness improvement (ambiguous should reach ~60% correctness, 70% completion)
  3. Stress test with reduced step limit: Decrease max steps P by 50% to identify minimum conversation length required for complex fields (expect nested fields like postal code history to fail first)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the framework's effectiveness vary when applied to domains with less structured data models, such as legal or healthtech, compared to the loan application scenario?
- Basis in paper: [explicit] The authors state applying the framework to other domains "presents additional challenges that are not addressed here" and list it as future work.
- Why unresolved: The study only validated the framework using a loan application use case with well-defined data fields.
- What evidence would resolve it: Benchmarking results from legal or healthtech conversational agents showing extraction accuracy and completion rates.

### Open Question 2
- Question: Can additional metrics like Turn-Level Appropriateness and Reengagement Rate be integrated into the framework to assess conversational flow without compromising scalability?
- Basis in paper: [explicit] The paper notes the current evaluation has "limited focus on conversational flow and user engagement" and lists incorporating these metrics as future work.
- Why unresolved: The current study primarily focuses on accuracy in information extraction and task completion.
- What evidence would resolve it: A modified framework implementation that successfully reports on both extraction accuracy and qualitative conversational metrics.

### Open Question 3
- Question: Does the framework maintain robustness when scaling to handle larger datasets and more complex interaction scenarios involving deeply nested data fields?
- Basis in paper: [explicit] The authors note the need to expand the framework to "handle larger datasets and more complex interaction scenarios" to validate robustness.
- Why unresolved: The experiment utilized a simplified dataset with only 20 data fields.
- What evidence would resolve it: Performance benchmarks demonstrating consistent completeness and correctness scores across datasets with significantly higher field counts and complexity.

## Limitations

- The framework's reliance on synthetic users with scripted behavioral patterns limits ecological validity compared to real user interactions
- Field-level accuracy disparities (e.g., 0% for loan details vs. 100% for full name) suggest struggles with complex, nested data structures
- The evaluation is constrained to a single loan application domain with 20 predefined fields, limiting generalizability to other structured data collection scenarios

## Confidence

- High confidence: Adaptive extraction mechanism's effectiveness (0% â†’ 60% correctness for ambiguous users)
- Medium confidence: Synthetic user methodology's ability to simulate real user ambiguity
- Low confidence: Framework's scalability to domains with complex nested fields (postal code history extraction failures)

## Next Checks

1. Test framework with real user interactions to validate synthetic user behavior distributions and identify gaps between simulated and actual user responses
2. Evaluate performance on domains with complex nested structures (e.g., medical history with temporal relationships) to identify schema design requirements for maintaining accuracy
3. Conduct ablation studies varying synthetic user ambiguity levels and follow-up question quality to quantify their independent contributions to extraction accuracy