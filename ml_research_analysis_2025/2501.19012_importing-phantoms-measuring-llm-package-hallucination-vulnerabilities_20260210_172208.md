---
ver: rpa2
title: 'Importing Phantoms: Measuring LLM Package Hallucination Vulnerabilities'
arxiv_id: '2501.19012'
source_url: https://arxiv.org/abs/2501.19012
tags:
- package
- hallucination
- code
- coding
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study analyzed package hallucination behavior in LLMs across
  Python, JavaScript, and Rust, examining both legitimate and fictional package references.
  The research found that package hallucination rates vary significantly by programming
  language, with Python showing the highest variance and JavaScript the most consistent
  performance.
---

# Importing Phantoms: Measuring LLM Package Hallucination Vulnerabilities

## Quick Facts
- arXiv ID: 2501.19012
- Source URL: https://arxiv.org/abs/2501.19012
- Authors: Arjun Krishna; Erick Galinkin; Leon Derczynski; Jeffrey Martin
- Reference count: 8
- Key outcome: Package hallucination rates vary significantly by language, with Python showing highest variance and JavaScript most consistent performance; model size inversely correlates with hallucination rates, and coding-specific models exhibit higher hallucination rates than general-purpose models of similar size.

## Executive Summary
This study systematically measures package hallucination vulnerabilities across 11 LLMs in Python, JavaScript, and Rust programming languages. The research quantifies how often models generate code that imports non-existent packages, revealing significant variation by language ecosystem and model architecture. The findings demonstrate that larger models and those trained on more comprehensive package data exhibit lower hallucination rates, while coding-specific models surprisingly show higher vulnerability. A strong inverse correlation between HumanEval benchmark scores and hallucination rates suggests coding benchmarks could serve as proxies for assessing package hallucination risks.

## Method Summary
The study uses the garak framework to probe 11 LLMs with 104 prompt combinations (8 request stubs × 13 coding task descriptions) across three programming languages. For each prompt, the LLM output is parsed using language-specific regex extractors to identify package names, which are then cross-referenced against historical package registry snapshots (NPM: 3.39M, PyPI: 605K, crates.io: 170K packages). Packages are flagged as hallucinated if they don't exist in the registry or were registered after the model's knowledge cutoff (90 days pre-publication if cutoff unknown). The Package Hallucination Rate (PHR) is calculated as the proportion of prompts yielding at least one hallucinated package, with 5 repetitions per prompt.

## Key Results
- Package hallucination rates vary significantly by language: Python shows highest variance (σ = 19.82), JavaScript lowest (σ = 8.43)
- Larger models demonstrate lower hallucination rates, with correlation ρ = -0.593 (p = 0.00028) between model size and PHR
- Coding-specific models exhibit higher hallucination rates than general-purpose models of similar size
- Strong inverse correlation between HumanEval scores and PHR (ρ = -0.7887), suggesting benchmarks as hallucination risk proxies
- JavaScript ecosystem size (3.4M packages) correlates with lower and more consistent hallucination rates

## Why This Works (Mechanism)

### Mechanism 1: Package Ecosystem Saturation Reduces Hallucination Surface
Languages with larger registered package ecosystems show lower and more consistent hallucination rates because more valid package names occupy the finite namespace, reducing the probability of generating unregistered names. This assumes training corpora include package import statements proportional to ecosystem usage and repository size.

### Mechanism 2: Model Scale Improves Package Name Discrimination
Larger models (≥70B parameters) demonstrate lower package hallucination rates through enhanced capacity to distinguish valid from invalid package names. Increased parameter count enables better memorization of observed packages and improved calibration about what the model has vs. hasn't seen during training.

### Mechanism 3: Coding Benchmark Performance Proxy for Package Reliability
HumanEval benchmark scores strongly predict package hallucination propensity, offering a practical heuristic without direct hallucination testing. Both tasks require precise recall of programming conventions and library interfaces; models that excel at functional correctness also better calibrate package existence.

## Foundational Learning

- **Package Hallucination Attack Chain**
  - Why needed here: Understanding the threat model is prerequisite to designing mitigations; the paper assumes readers grasp how hallucinated packages become exploitation vectors.
  - Quick check question: Can you explain how an attacker would weaponize a hallucinated package name like `securehashlib`?

- **Induced vs. Natural Hallucination**
  - Why needed here: The paper distinguishes these categories with different security implications; induced hallucination (explicit prompting for non-existent packages) occurs at nearly double the rate.
  - Quick check question: If a developer asks an LLM to "use the fastxml library," is this induced or natural hallucination if fastxml doesn't exist?

- **Pareto Optimality in Model Selection**
  - Why needed here: The paper finds the Pareto frontier between code generation performance and security (low hallucination) is sparsely populated—models aren't jointly optimized.
  - Quick check question: Why might a smaller model with acceptable HumanEval scores still pose higher supply chain risk than a larger model with similar benchmark performance?

## Architecture Onboarding

- **Component map:** garak framework -> Language-specific extractors (Python imports, JavaScript require/import, Rust use declarations) -> Package registry scraper (NPM, PyPI, crates.io) -> Knowledge cutoff comparator

- **Critical path:** 1. Generate code prompts via R × T combinations 2. Extract package names from LLM output using language-specific parsers 3. Cross-reference against registry snapshot from before model's training cutoff 4. Compute PHR as proportion of responses containing ≥1 hallucinated package

- **Design tradeoffs:** Prompt diversity (5 repetitions) vs. evaluation cost; Strict cutoff-based hallucination definition vs. false positive risk for legitimately new packages; Open-weight model preference for scientific reproducibility vs. inclusion of proprietary models (GPT-4o) for completeness

- **Failure signatures:** High PHR in coding-specific models despite good benchmark scores; Language-specific spikes: Rust showing 40%+ PHR for multiple models; Inverse scale relationship breaking (Mamba-Codestral exception suggests architecture effects)

- **First 3 experiments:** 1. Reproduce PHR measurement for your target model using provided prompt templates against your language's registry snapshot 2. Test induced hallucination by explicitly requesting fictional packages; compare against natural hallucination baseline 3. Implement real-time verification: flag generated packages not in pre-cutoff registry; measure intervention accuracy and developer friction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do architectural features of non-transformer models, such as Mamba, inherently reduce package hallucination rates compared to transformer-based models of similar size?
- Basis in paper: The authors explicitly list this as a reserved topic for future work in Section 4.4, noting that Mamba-Codestral bucked the trend of size-correlated safety.
- Why unresolved: The study was limited to primarily transformer architectures, with Mamba being the sole non-transformer exception, preventing broad architectural generalizations.
- What evidence would resolve it: A comparative analysis of hallucination rates across matched-pair models (transformer vs. non-transformer) with controlled parameter counts and training datasets.

### Open Question 2
- Question: Can multi-objective optimization techniques effectively balance code generation performance (e.g., HumanEval scores) with reduced package hallucination rates?
- Basis in paper: Section 4.5 notes that the Pareto optimality frontier between performance and security is "sparsely populated," suggesting current models are not optimized for this trade-off.
- Why unresolved: The study observes the correlation but does not propose or test training methodologies to optimize for both security (low hallucination) and capability (high benchmark scores) simultaneously.
- What evidence would resolve it: Developing a training regimen that penalizes hallucinated packages during fine-tuning and measuring the resulting shift on the Pareto frontier.

### Open Question 3
- Question: To what extent does the volume of language-specific training data (e.g., for Rust vs. JavaScript) directly drive package hallucination rates?
- Basis in paper: Section 4.1 posits that training data volume likely impacts hallucination but concedes the relationship cannot be analyzed because "training data composition" is unavailable for the models studied.
- Why unresolved: Without access to the training corpora, the authors could only speculate using proxy metrics like the TIOBE index rather than direct data analysis.
- What evidence would resolve it: Auditing the training corpora of open-weight models to quantify the ratio of code per language and correlating it with the observed Package Hallucination Rate (PHR).

## Limitations
- Assumption-based knowledge cutoff determination introduces potential false positives/negatives in hallucination classification
- Evaluation limited to three programming languages, potentially limiting generalizability
- Temperature/hyperparameter settings for LLM queries remain unspecified despite evidence they affect hallucination rates

## Confidence
- **High Confidence**: The inverse correlation between model size and hallucination rates (p = 0.00028) is statistically robust and aligns with established scaling laws.
- **Medium Confidence**: The ecosystem size hypothesis explaining JavaScript's lower variance is plausible but relies on assumptions about training data composition.
- **Low Confidence**: The claim that coding-specific models exhibit higher hallucination rates is based on a limited sample and may not generalize.

## Next Checks
1. Obtain and verify actual training cutoff dates for all models in the study, then re-run the hallucination analysis to quantify the impact of cutoff assumptions on PHR measurements.
2. Systematically vary temperature (0.0, 0.7, 1.0) and top_p (0.9, 0.95, 1.0) settings across multiple models to measure how sampling parameters affect package hallucination rates.
3. Extend the evaluation framework to at least two additional programming languages (e.g., Go and Java) to test whether the observed relationships hold across different package management paradigms.