---
ver: rpa2
title: 'Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation
  via Tuning-Free Multimodal Token Pruning'
arxiv_id: '2509.15250'
source_url: https://arxiv.org/abs/2509.15250
tags:
- pruning
- navigation
- tokens
- token
- flops
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational inefficiency of large Vision-and-Language
  Navigation (VLN) models in resource-limited environments. The authors propose Navigation-Aware
  Pruning (NAP), a method that improves efficiency by reducing model input size through
  task-specific pruning strategies.
---

# Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning

## Quick Facts
- arXiv ID: 2509.15250
- Source URL: https://arxiv.org/abs/2509.15250
- Reference count: 25
- Key outcome: NAP achieves up to 2× speedups over original VLN models while maintaining higher success rates than state-of-the-art pruning baselines

## Executive Summary
This paper addresses the computational inefficiency of large Vision-and-Language Navigation (VLN) models in resource-limited environments by proposing Navigation-Aware Pruning (NAP), a method that improves efficiency through task-specific pruning strategies. NAP combines three components: Background Pruning (BGP) to remove non-critical background views while preserving action views, Backtracking Pruning (BTP) to remove unvisited nodes and discourage unnecessary backtracking, and Vocabulary Priority Pruning (VPP) to prune navigation-irrelevant instruction tokens using a pre-constructed vocabulary of irrelevance. Experiments across multiple VLN models and datasets show that NAP achieves significant FLOPS reductions with minimal success rate degradation.

## Method Summary
NAP is a tuning-free multimodal token pruning method for Vision-and-Language Navigation that reduces model input size through three task-specific strategies. Background Pruning (BGP) separates panoramic views into action and background views, retaining all action view tokens while pruning background tokens based on attention scores. Backtracking Pruning (BTP) operates on DUET-style topological maps, retaining only the top-k unvisited nodes to discourage backtracking and reduce computational load. Vocabulary Priority Pruning (VPP) uses an LLM to pre-construct a vocabulary of navigation-irrelevant words (articles, prepositions, punctuation), which are pruned first before applying attention-based pruning to remaining tokens. These strategies work together to achieve up to 2× speedups while maintaining or improving success rates compared to baselines.

## Key Results
- NAP achieves up to 2× speedups over original VLN models and 1.25× over FastV baseline
- Saves over 50% FLOPS with only 2-3 percentage point drop in success rate
- Maintains higher success rates than FastV across all pruning rates (40-80%)
- VPP consistently outperforms attention-based Cascade pruning by >10 percentage points in the 40-60% pruning range

## Why This Works (Mechanism)

### Mechanism 1: Background Pruning (BGP) — Asymmetric Visual Token Importance
- Claim: Pruning background views while preserving action views reduces computation with minimal navigation performance loss.
- Mechanism: BGP separates panoramic views into action views (navigable directions) and background views (context-only), retaining all action view tokens and pruning background tokens based on aggregated attention scores. By only removing background tokens, critical navigable directions remain available for action prediction.
- Core assumption: Action views are universally more critical than background views for navigation decisions; background views provide redundant or optional context.
- Evidence anchors: [Section 3.1, Table 4] Pruning action views leads to 0% success rate at high pruning rates, while pruning only background views maintains 69% SR at 60% pruning rate on R2R.

### Mechanism 2: Backtracking Pruning (BTP) — Path Length Regularization
- Claim: Limiting backtracking options reduces navigation length and computational cost more effectively than naive token pruning.
- Mechanism: BTP retains only the top-k unvisited nodes (by importance score from the cross-modal transformer) and discards the rest before each step. This removes low-value backtracking options, encouraging forward progress and reducing decision steps.
- Core assumption: Unvisited nodes with low attention-based importance are unlikely to become critical later; backtracking is often suboptimal.
- Evidence anchors: [Figure 2(a)] BTP reduces navigation length as pruning increases, while FastV increases length (blue curve).

### Mechanism 3: Vocabulary Priority Pruning (VPP) — LLM-Guided Irrelevance Filtering
- Claim: Pre-filtering instruction tokens using an LLM-constructed "vocabulary of irrelevance" preserves navigation-critical content better than attention-based pruning alone.
- Mechanism: VPP prompts an LLM (Llama 3) offline to classify words from the training lexicon as relevant or irrelevant based on association with directions, environment descriptions, or objects. Irrelevant words form a priority pruning set, ensuring content words like "couch," "enter," "doors" are retained.
- Core assumption: LLM general knowledge accurately identifies navigation-irrelevant words; attention scores alone are noisy importance indicators.
- Evidence anchors: [Section 3.3, Table 1] VPP retains key instruction tokens ("Exit," "room," "Turn," "right," "Start," "down," "stairs," "stop," "3," "steps") at 25% retention, while attention-based Cascade pruning retains punctuation and function words.

## Foundational Learning

- **Transformer Attention as Importance Proxy**: BGP and BTP rely on aggregated attention scores to rank token/node importance; understanding attention mechanics is essential for debugging pruning failures.
  - Quick check question: Can you explain why summing attention columns (Eq. 2) measures a token's influence on all other tokens, rather than its own representation quality?

- **VLN Topological Mapping**: BTP operates on DUET-style history graphs with visited and unvisited nodes; without this context, BTP's mechanism is opaque.
  - Quick check question: In a topological map, what distinguishes an unvisited node from a visited node, and why would removing unvisited nodes discourage backtracking?

- **FLOPS vs. Latency Trade-offs in Transformers**: NAP reports FLOPS reduction, but real-world deployment cares about wall-clock latency; FLOPS savings may not translate directly.
  - Quick check question: If attention computation is O(l²d) and linear projections are O(ld²), which dominates when l (sequence length) is small vs. large? How does this affect pruning ROI?

## Architecture Onboarding

- **Component map**: Instruction I -> [Language Transformer] -> VPP prunes tokens -> Text features
  Panorama P -> [Vision Transformer] -> BGP prunes bg tokens -> Visual features
  History V -> [Cross-Modal Transformer] <- BTP prunes unvisited nodes
  -> Action Prediction (stop / navigate / backtrack)

- **Critical path**: BGP pruning loop (per vision transformer layer) -> BTP node selection (per step) -> VPP token filtering (per layer) -> Cross-modal fusion -> Action logits. Errors in any pruning stage propagate to action prediction.

- **Design tradeoffs**:
  - **kBGP (tokens pruned per layer)**: Higher values increase FLOPS savings but risk losing background context. Paper uses 3 tokens/layer for VLN-CE.
  - **kBTP (unvisited nodes retained)**: Lower values reduce backtracking options but may trap agents. Paper uses 4-10 depending on dataset.
  - **VPP vocabulary source**: Using a mismatched dataset's vocabulary (e.g., RxR vocab on REVERIE) causes ~1 pp SR loss vs. matched vocabulary (Table 5).

- **Failure signatures**:
  - SR collapse with high visual pruning: Action views likely pruned -> check BGP implementation ensures action view preservation.
  - Increased navigation steps: BTP threshold too low or disabled -> verify unvisited node retention count.
  - Instruction keywords missing: VPP misclassifying content words -> inspect vocabulary construction prompt and outputs.

- **First 3 experiments**:
  1. Apply BGP alone (no BTP/VPP) on R2R validation seen, sweep kBGP from 1-5 tokens/layer, plot SR vs. FLOPS to identify optimal background pruning rate.
  2. Fix BGP/VPP at paper settings, vary kBTP from 2-12 on REVERIE unseen, measure average steps and SR to find backtracking vs. trapping tradeoff.
  3. Construct vocabularies from R2R, RxR, and REVERIE separately, apply each to all three datasets, quantify cross-dataset robustness (replicate Table 5 pattern).

## Open Questions the Paper Calls Out
None

## Limitations
- VPP vocabulary construction may not generalize across different instruction types (navigation vs. human-object interaction) or languages
- Performance gains are primarily demonstrated on DUET architecture, with uncertain transfer to encoder-decoder or end-to-end approaches
- The paper doesn't fully explore whether FLOPS reductions translate to meaningful wall-clock latency improvements in resource-constrained deployments

## Confidence
- **High Confidence**: BGP mechanism and empirical validation (clear SR collapse when action views pruned, robust FLOPS savings when only background views pruned)
- **Medium Confidence**: BTP effectiveness (based on DUET-specific topological mapping assumptions, limited ablation evidence)
- **Medium Confidence**: VPP general approach (LLM-based pruning shows consistent gains, but vocabulary construction methodology has generalization concerns)
- **Low Confidence**: Cross-dataset and cross-model generalization (limited experiments on REVERIE and VLN-CE, no encoder-decoder baselines)

## Next Checks
1. Implement NAP on a Seq2Seq VLN model (e.g., PRESS or HAMT) and measure whether the same pruning rates achieve comparable SR-FLOPS trade-offs.
2. For high pruning configurations (60-80%), record and analyze navigation paths to determine if agents take longer routes or make more navigation errors compared to unpruned models.
3. Apply NAP with VPP vocabularies constructed from English instructions to Spanish (RxR) and Hindi (RxR) instruction sets to test cross-lingual robustness.