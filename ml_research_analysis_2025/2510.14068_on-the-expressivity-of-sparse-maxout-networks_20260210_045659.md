---
ver: rpa2
title: On the expressivity of sparse maxout networks
arxiv_id: '2510.14068'
source_url: https://arxiv.org/abs/2510.14068
tags:
- networks
- functions
- linear
- neural
- virtual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work analyzes the expressivity of sparse maxout neural networks,
  where each neuron has a fixed indegree constraint and uses maxout activation. The
  authors establish a duality between functions computable by such networks and a
  class of virtual polytopes, using this connection to bound the dimension of these
  polytopes.
---

# On the expressivity of sparse maxout networks

## Quick Facts
- arXiv ID: 2510.14068
- Source URL: https://arxiv.org/abs/2510.14068
- Authors: Moritz Grillo; Tobias Hofmann
- Reference count: 9
- Sparse maxout networks with fixed indegree constraints cannot represent all continuous piecewise linear functions, regardless of depth or width

## Executive Summary
This work establishes fundamental limits on the expressivity of sparse maxout neural networks, where each neuron has a fixed indegree constraint. The authors prove that sparsity is a fundamental architectural constraint that cannot be overcome by increasing network width or depth. Using a duality between computable functions and virtual polytopes, they show that for any fixed depth, indegree constraint, and rank, there exist functions computable by fully connected networks with two hidden layers that cannot be represented by the sparse network, regardless of its width.

## Method Summary
The authors establish a mathematical duality between functions computable by sparse maxout networks and a class of virtual polytopes. This connection allows them to bound the dimension of these polytopes and characterize the class of computable functions. For the specific case of rank-2 maxout networks with indegree-2, they fully characterize the computable function class and demonstrate sharp separations for each depth up to ⌈log₂(n+1)⌉. The analysis proves that sparse networks cannot represent all continuous piecewise linear functions, establishing sparsity as a fundamental constraint on expressivity.

## Key Results
- Sparse maxout networks with fixed indegree constraints cannot represent all continuous piecewise linear functions, regardless of depth or width
- For rank-2 maxout networks with indegree-2, the authors fully characterize the class of computable functions
- Sharp separation exists for each depth up to ⌈log₂(n+1)⌉, proving sparsity cannot be compensated by width alone

## Why This Works (Mechanism)
The mechanism relies on the mathematical duality between computable functions and virtual polytopes. By establishing bounds on the dimension of these polytopes, the authors can characterize which functions can be represented by sparse maxout networks. The indegree constraint creates a fundamental limitation that persists regardless of network depth or width, as the polytope structure cannot capture the full expressiveness of fully connected networks.

## Foundational Learning

**Virtual Polytopes**
- Why needed: Provides the mathematical framework to characterize computable functions
- Quick check: Can be verified through dimension counting arguments and geometric interpretations

**Maxout Activation**
- Why needed: The piecewise linear nature enables the polytope characterization
- Quick check: Verify that maxout neurons partition input space into linear regions

**Indegree Constraint**
- Why needed: The fixed connection limit creates the fundamental expressivity limitation
- Quick check: Count the number of possible linear regions given indegree constraint

## Architecture Onboarding

**Component Map**
Input -> Sparse Maxout Layer (indegree constraint) -> Output

**Critical Path**
The critical path involves understanding how the indegree constraint limits the polytope structure, which in turn bounds the class of computable functions.

**Design Tradeoffs**
- Sparsity vs Expressivity: The primary tradeoff is between computational efficiency (sparse connections) and representational power
- Depth vs Width: Neither can compensate for the fundamental limitations imposed by indegree constraints

**Failure Signatures**
Networks will fail to approximate functions requiring more complex polytope structures than their constrained architecture can represent.

**First 3 Experiments**
1. Verify the rank-2, indegree-2 characterization on synthetic piecewise linear functions
2. Test expressivity gaps between sparse and fully connected networks on simple classification tasks
3. Empirically validate the theoretical bounds on polytope dimensions for practical network sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis is purely theoretical without empirical validation on practical datasets
- The proof assumes infinite network width as a possibility for fully connected networks
- Practical implications for real-world neural network design remain speculative

## Confidence
**High Confidence**: The mathematical characterization of rank-2 maxout networks with indegree-2 constraints is well-defined and provable.

**Medium Confidence**: The general claim about sparsity being a fundamental constraint on expressivity follows logically from the polytope dimension bounds but requires broader validation.

**Low Confidence**: The practical implications for real-world neural network design remain speculative without empirical evidence.

## Next Checks
1. **Empirical Validation**: Test the theoretical bounds on synthetic and real-world datasets to verify if the expressivity gaps manifest in practice.

2. **Extended Case Studies**: Characterize the computable function class for higher ranks (rank-3, rank-4) and indegrees to understand if similar sharp separations exist.

3. **Practical Implications**: Investigate whether the theoretical limitations of sparse maxout networks translate to performance degradation in practical applications compared to fully connected alternatives.