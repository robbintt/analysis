---
ver: rpa2
title: 'Anecdoctoring: Automated Red-Teaming Across Language and Place'
arxiv_id: '2509.19143'
source_url: https://arxiv.org/abs/2509.19143
tags:
- language
- claims
- across
- attacks
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study presents "anecdoctoring," a method for automated red-teaming
  of generative AI models against disinformation across multiple languages and geographies.
  It uses real-world fact-checking data in English, Spanish, and Hindi from the US
  and India, clustering claims into narratives and extracting knowledge graphs to
  augment an attacker LLM.
---

# Anecdoctoring: Automated Red-Teaming Across Language and Place

## Quick Facts
- arXiv ID: 2509.19143
- Source URL: https://arxiv.org/abs/2509.19143
- Authors: Alejandro Cuevas; Saloni Dash; Bharat Kumar Nayak; Dan Vann; Madeleine I. G. Daepp
- Reference count: 40
- Primary result: Automated red-teaming method achieves >80% attack success across multilingual disinformation narratives using knowledge graphs

## Executive Summary
This paper introduces "anecdoctoring," an automated red-teaming method that evaluates generative AI models' vulnerability to disinformation across languages and geographies. The approach clusters fact-checked claims from English, Spanish, and Hindi sources in the US and India, extracts knowledge graphs to represent disinformation narratives, and uses these to generate adversarial prompts. The method achieves attack success rates exceeding 80% across GPT and Llama models, outperforming baselines that use individual claims or clustered claims without knowledge graphs. Knowledge graphs provide interpretability benefits by revealing culturally specific narrative elements and contested entities that can inform model guardrails.

## Method Summary
The method ingests fact-checking articles from Duke Reporters' Lab and Factchequeado, filters out "true" verdicts, and processes claims separately for each language-location pair. Claims are embedded using Cohere's multilingual model, reduced to 5 dimensions via UMAP, and clustered using HDBSCAN. For each cluster, an LLM extracts entities and relationships to form a knowledge graph. This KG is then used to augment attacker LLM prompts that generate disinformation. Target models' responses are evaluated by a Judge LLM (GPT-4o) on a 1-5 harm scale, with scores â‰¥4 indicating successful attacks. The process includes iterative quality control through LLM scoring and manual review.

## Key Results
- Attack Success Rates exceeded 80% across GPT and Llama models for all language-location combinations
- Knowledge graph augmentation achieved 9% higher ASR than clustered claims baseline
- Clustering claims alone improved ASR by 14% compared to individual claims
- Culturally specific narratives emerged clearly, with no overlap in top terms between India and USA but strong entity overlap within locations

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Graph Augmentation for Structured Narrative Representation
Representing misinformation narratives as knowledge graphs improves attack success rates and provides interpretability over unstructured claim text alone. KG extraction converts multiple related claims into a unified structure of entities and relationships, allowing an attacker LLM to select key narrative elements and synthesize coherent, persuasive disinformation prompts. Disinformation campaigns are built on interconnected "deep stories" or narratives, not isolated claims.

### Mechanism 2: Clustering Claims to Surface Dominant Narratives
Clustering fact-checked claims identifies high-salience disinformation narratives, improving attack effectiveness by prioritizing broadly resonant themes. Embedding and clustering group semantically similar claims, filtering isolated claims and aggregating evidence for a narrative. Effective disinformation campaigns rely on repeated, coordinated claims that form a broader narrative.

### Mechanism 3: Grounding Attacks in Local Fact-Checking Data for Cultural Relevance
Using local fact-checking data in native languages produces culturally and linguistically relevant attacks that evade safety training based on translated or US-centric data. Fact-checking organizations naturally surface context-specific narratives. By using this data directly, the method generates attacks that reflect local "lines of conflict," which are more likely to be persuasive and less likely to be caught by generic guardrails.

## Foundational Learning

- **Knowledge Graphs (KGs)**
  - Why needed here: The core of the method is extracting and using a KG to represent a narrative. Understanding entities, relationships, and graph construction is essential.
  - Quick check question: Given the claims "Dr. Fauci was fired" and "The US government admitted COVID-19 is man-made," what are two entities and a relationship you would extract?

- **Clustering (Embeddings, UMAP, HDBSCAN)**
  - Why needed here: The method relies on clustering claims to find narratives. Understanding high-dimensional embeddings and density-based clustering is required.
  - Quick check question: Why might HDBSCAN be a better choice than K-Means for clustering fact-checked claims where the number of narratives is unknown?

- **LLM Red-Teaming and Safety Alignment**
  - Why needed here: The goal is to test model guardrails. Understanding what an "attack success rate" (ASR) means, how guardrails work, and the role of a "Judge LLM" is critical.
  - Quick check question: In the context of this paper, what constitutes a "successful" attack? What does the Judge LLM's score of 4 or 5 indicate?

## Architecture Onboarding

- **Component map:** Data Ingestion -> Embedding & Clustering -> KG Extraction -> Attacker LLM -> Target Models -> Evaluation (Judge LLM)
- **Critical path:** The sequence from Data Ingestion -> Clustering -> KG Extraction -> Attacker LLM is the critical path. Failure in any step breaks the attack generation.
- **Design tradeoffs:**
  - KG vs. Few-Shot prompting with clusters: KGs provide interpretability but add complexity. The paper shows KGs do not substantially change ASR over clustered few-shot prompting.
  - Model for Attack Generation: Using the same model for both attack generation and evaluation may overestimate ASR. The paper notes ASRs are stable when using Llama for generation.
  - Language/Location Specificity: Clustering per language-location pair ensures relevance but increases computational cost and fragmentation.
- **Failure signatures:**
  - Low ASR: Could indicate strong model guardrails, poor attack prompt crafting, or a low-harm narrative cluster.
  - Incoherent Attack/Response: Suggests failure in KG extraction or the Attacker LLM's ability to synthesize a coherent narrative from the graph.
  - High Refusal Rate on Specific Topics: Indicates model sensitivity to certain themes (e.g., public health, geopolitics).
- **First 3 experiments:**
  1. Reproduce ASR Baselines: Run the provided PyRIT orchestrator on a subset of English/USA clusters to reproduce the ASR numbers for GPT-4o-mini vs. Llama3.1-70b.
  2. Ablate the KG: Run the "Clustered Claims (Few-Shot)" baseline (without KGs) for the same clusters and compare the ASR and interpretability.
  3. Inspect a Narrative Cluster: Pick a single cluster, visualize its KG, and manually trace how the Attacker LLM converts it into an adversarial prompt.

## Open Questions the Paper Calls Out

### Open Question 1
How can the anecdoctoring method be effectively adapted for low-resource languages where structured fact-checking databases are scarce? The authors note that further work is needed to extend the method to low-resource languages as the current study examined relatively well-represented languages. A demonstration of the method utilizing alternative data sources, such as community notes or social media scraping, would resolve this.

### Open Question 2
Does using a guardrail-free or specialized adversarial model to generate attacks yield significantly higher Attack Success Rates (ASRs) than using a standard model like GPT-4o? The authors note that generating attacks with GPT-4o "may overestimate ASR" because prompts pass through guardrails during creation. A comparative study of ASRs where attacks are generated by an unfiltered model versus the current approach would resolve this.

### Open Question 3
Can the fact-checking data be systematically decomposed to distinguish narratives likely to cause societal-level harms from those causing individual-level harms? The authors state that further work is needed to better decompose existing fact-checking data into categories with the potential to cause individual- and societal-level harms. The development and validation of a taxonomy that successfully classifies narrative clusters by harm severity would resolve this.

## Limitations

- Model-specific ASR variability: Llama3.1-8b shows substantially lower performance (0.27-0.39 ASR) compared to other models, suggesting size-dependent vulnerabilities.
- Judge LLM potential bias: Using GPT-4o for both attack generation and evaluation could introduce systematic bias, though stability was observed when using Llama for generation.
- Incomplete KG extraction specification: The paper doesn't fully specify how ambiguous or incomplete claims are handled during extraction, which could impact attack effectiveness.

## Confidence

- High Confidence: Knowledge graph augmentation improves interpretability of attacks
- Medium Confidence: Clustering claims improves attack success rates
- Medium Confidence: Using local fact-checking data produces culturally relevant attacks

## Next Checks

1. Reproduce ASR Baseline Comparison: Run the PyRIT orchestrator to reproduce the ASR numbers for GPT-4o-mini vs. Llama3.1-70b on a subset of English/USA clusters.

2. Ablate Knowledge Graph Component: Implement and run the "Clustered Claims (Few-Shot)" baseline (without KGs) for the same clusters used in the main experiment. Compare both the ASR and manually inspect the raw prompts.

3. Test Judge LLM Bias: Repeat a subset of the evaluation using a different Judge LLM (e.g., Claude-3.5-Sonnet or a different version of GPT) to assess the stability of ASR scores.