---
ver: rpa2
title: Investigating Numerical Translation with Large Language Models
arxiv_id: '2501.04927'
source_url: https://arxiv.org/abs/2501.04927
tags:
- translation
- numerical
- billion
- llms
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accurate numerical translation
  in machine translation, particularly for large language models (LLMs). The authors
  construct a Chinese-English numerical translation dataset covering ten types of
  numerical translations, including large units, decimals, fractions, and special
  formats.
---

# Investigating Numerical Translation with Large Language Models

## Quick Facts
- **arXiv ID:** 2501.04927
- **Source URL:** https://arxiv.org/abs/2501.04927
- **Reference count:** 31
- **Primary result:** Numerical translation remains challenging for LLMs, with large unit errors up to 20% accuracy, improved by 30% via post-editing.

## Executive Summary
This paper investigates the performance of large language models on Chinese-English numerical translation across ten numerical types. The authors find that numerical translation, particularly for large units (亿, 万), remains a significant challenge for current LLMs, with error rates as high as 20%. They propose and evaluate three improvement strategies: in-context learning, chain-of-thought prompting, and post-editing, with post-editing showing the best performance by leveraging external tools for numerical conversion.

## Method Summary
The study constructs a custom Chinese-English numerical translation dataset covering ten numerical types. Multiple LLMs are evaluated on this dataset using a Pass Rate metric that checks if translations match any valid reference. Three improvement strategies are proposed for large unit translations: In-Context Learning with unit conversion principles, Chain-of-Thought prompting for step-by-step translation, and Post-Editing using cn2an/en2an tools to extract, convert, and correct numerical errors. The PE strategy achieves the best results by offloading mathematical calculations to external tools.

## Key Results
- LLMs show high error rates (up to 20%) on large unit translations even for the latest models
- Post-editing strategy improves large unit translation accuracy by up to 30%
- Increasing model scale does not guarantee better numerical translation precision (Llama3.1-70B performs worse than Llama3.1-8B on ratios and formulas)
- G-LLMs outperform T-LLMs overall in numerical translation tasks

## Why This Works (Mechanism)
The paper demonstrates that LLMs struggle with precise numerical calculations and unit conversions due to their architectural limitations in handling mathematical operations. The Post-Editing strategy works by circumventing these limitations - it extracts numerical segments, converts them using specialized tools (cn2an/en2an), and replaces incorrect translations. This approach separates the language understanding task from the numerical computation task, allowing the LLM to focus on translation while external tools handle the mathematical precision.

## Foundational Learning
- **Numerical translation challenges**: Understanding the specific difficulties LLMs face with different numerical types (why needed: to identify where improvements are most critical; quick check: compare error rates across numerical types)
- **Pass Rate evaluation metric**: Knowing how translation accuracy is measured against multiple valid references (why needed: to understand performance assessment; quick check: verify reference list completeness)
- **In-Context Learning principles**: Understanding how unit conversion rules can be prepended to prompts (why needed: to implement ICL strategy; quick check: test unit conversion accuracy)
- **Chain-of-Thought prompting**: Understanding step-by-step reasoning approaches for numerical tasks (why needed: to implement COT strategy; quick check: measure step-by-step accuracy vs direct translation)
- **Post-editing workflow**: Understanding how to extract, convert, detect, and replace numerical errors (why needed: to implement PE strategy; quick check: measure correction accuracy)
- **cn2an/en2an tools**: Understanding these tools for Chinese-English numerical conversion (why needed: to implement PE strategy; quick check: verify conversion accuracy)

## Architecture Onboarding

**Component Map:**
Custom Dataset -> LLM Translation -> Numerical Extraction -> Reference Comparison -> Error Analysis -> Improvement Strategy

**Critical Path:**
Dataset construction → Baseline translation evaluation → Error identification → Strategy implementation → Performance measurement

**Design Tradeoffs:**
- Using external tools (PE) vs model capability improvement: PE is effective but requires tool integration and may introduce dependency risks
- Multiple valid references vs strict matching: Allows flexibility but may overestimate accuracy
- Different model families (G-LLMs vs T-LLMs): G-LLMs show better performance but require more resources

**Failure Signatures:**
- Large unit conversion errors (e.g., 1000亿 → 10 billion instead of 100 billion)
- Overtranslation introducing semantic drift (e.g., "ranked 4th" → "won bronze medal")
- Number string reformatting losing leading zeros (e.g., "00326264" → "326264")

**First 3 Experiments:**
1. Reconstruct dataset using publicly available parallel corpora and manually annotate numerical segments
2. Implement baseline translations across multiple LLM families to verify error patterns
3. Test all three improvement strategies on a subset of cases to validate relative performance claims

## Open Questions the Paper Calls Out
- Can LLMs be intrinsically improved to handle unit conversions without relying on external post-editing tools?
- Why does increasing model scale sometimes degrade performance on specific numerical types?
- Do the identified failure modes in Chinese-English numerical translation generalize to languages with different numerical notation systems?

## Limitations
- Dataset is not publicly released, limiting reproducibility
- Performance evaluation relies on reference lists that may not capture all valid translations
- Study focuses only on Chinese-English translation, limiting generalizability to other language pairs

## Confidence

**High Confidence:**
- LLMs struggle with large unit numerical translations (20% error rate)
- Relative performance ordering between models is consistently demonstrated

**Medium Confidence:**
- Improvement claims for three strategies based on single-model evaluations
- PE strategy shows strong results but tested on limited subset of cases

**Low Confidence:**
- Comparative analysis between G-LLMs and T-LLMs limited by different model sizes
- Claim that PE is "best" based on single metric without considering trade-offs

## Next Checks
1. Reconstruct the evaluation dataset using publicly available Chinese-English parallel corpora and manually annotate numerical segments with reference translations
2. Implement all three improvement strategies across a broader set of LLM families to verify generalization
3. Develop evaluation criteria to measure whether improved numerical translations maintain overall sentence meaning and context