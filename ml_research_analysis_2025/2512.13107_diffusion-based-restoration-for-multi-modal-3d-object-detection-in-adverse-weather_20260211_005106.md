---
ver: rpa2
title: Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse
  Weather
arxiv_id: '2512.13107'
source_url: https://arxiv.org/abs/2512.13107
tags:
- weather
- detection
- fusion
- object
- restoration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DiffFusion is a diffusion-based framework for robust multi-modal
  3D object detection in adverse weather conditions. It introduces a dual-branch restoration
  module combining Diffusion-IR for image restoration via conditional denoising diffusion
  and Point Cloud Restoration (PCR) for LiDAR compensation using image object cues.
---

# Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather

## Quick Facts
- arXiv ID: 2512.13107
- Source URL: https://arxiv.org/abs/2512.13107
- Authors: Zhijian He; Feifei Liu; Yuwei Li; Zhanpeng Luo; Jintao Cheng; Xieyuanli Chen; Xiaoyu Tang
- Reference count: 33
- Key outcome: DiffFusion achieves state-of-the-art robustness under rain, fog, and strong sunlight while maintaining strong clean-data performance on KITTI-C and DENSE datasets

## Executive Summary
DiffFusion introduces a diffusion-based framework for robust multi-modal 3D object detection in adverse weather conditions. It addresses weather-induced degradation in both sensor modalities through a dual-branch restoration module that combines conditional diffusion denoising for images and image-guided point cloud restoration for LiDAR. The framework also includes a Bidirectional Adaptive Fusion and Alignment Module to correct cross-modal misalignment caused by weather distortion. Evaluated on KITTI-C and DENSE datasets, DiffFusion demonstrates superior robustness to rain, fog, and strong sunlight while maintaining strong performance on clean data.

## Method Summary
DiffFusion employs a diffusion-based dual-branch restoration module (DBRM) that first restores degraded images using conditional DDIM sampling, then compensates corrupted LiDAR data using image object cues. The Point Cloud Restoration (PCR) module projects LiDAR points to range image representation and uses 2D detection bounding boxes from the restored image to guide ray completion. The Bidirectional Adaptive Fusion and Alignment Module (BAFAM) employs cross-attention fusion and bidirectional BEV alignment to address modality misalignment. The framework integrates with the Focals Conv 3D detector, using voxel size (0.05m, 0.05m, 0.1m) and car anchors [3.9, 1.6, 1.56] with rotations [0, 1.57] rad.

## Key Results
- Achieves state-of-the-art mAP@R40 performance on KITTI-C rain, fog, and sunlight conditions
- Maintains strong clean-data performance (minimal mAP degradation vs baseline)
- Demonstrates zero-shot generalization to real-world fog on DENSE dataset
- Outperforms existing methods in adverse conditions while addressing both sensor degradation and spatial misalignment

## Why This Works (Mechanism)

### Mechanism 1
Conditional diffusion denoising restores weather-degraded images while preserving detection-relevant semantics better than direct CNN processing. The diffusion model learns to invert complex corruption processes by training to predict noise given both noisy latents and degraded images as conditions. DDIM sampling reduces timesteps from T=1000 to S=10 via deterministic subsequence mapping, enabling practical inference speed.

### Mechanism 2
Image-derived 2D object bounding boxes guide LiDAR ray compensation to recover missing points in range image representation. Spherical projection converts sparse 3D points to dense 2D range image. Restored image features feed a CenterNet-style 2D detector; predicted bounding boxes mask regions in the range image for U-Net-based inpainting.

### Mechanism 3
Bidirectional cross-attention with cascaded offset learning corrects cross-modal BEV misalignment caused by weather-induced feature distortion. CAAF performs asymmetric bidirectional attention: LiDAR→Camera then Camera→LiDAR with residual connections. B2A learns spatial offsets in two stages—first aligning camera to LiDAR, then LiDAR to aligned camera—using supervision from clean-data fusion features.

## Foundational Learning

- **Concept: Denoising Diffusion Probabilistic Models (DDPM/DDIM)**
  - Why needed here: Understanding how forward noise addition and reverse denoising enables controllable image restoration with degradation conditioning.
  - Quick check question: Can you explain why DDIM reduces sampling steps compared to DDPM while maintaining output quality?

- **Concept: Bird's-Eye View (BEV) Representation for Multi-Modal Fusion**
  - Why needed here: DiffFusion projects both camera and LiDAR features to unified BEV space; understanding spatial correspondence is essential for the alignment module.
  - Quick check question: What are the coordinate transformations required to project camera image features into BEV space?

- **Concept: Cross-Attention Mechanisms**
  - Why needed here: CAAF uses multi-head cross-attention for adaptive modality weighting; understanding Q/K/V projections clarifies why bidirectional design helps.
  - Quick check question: How does cross-attention differ from self-attention in terms of what provides keys/values vs. queries?

## Architecture Onboarding

- **Component map:** Degraded image → Diffusion-IR restoration → 2D features → bounding boxes + range image mask → PCR → restored LiDAR features → CAAF fusion → B2A alignment → 3D detection
- **Critical path:** Degraded image → Diffusion-IR restoration → 2D features → bounding boxes + range image mask → PCR → restored LiDAR features → CAAF fusion → B2A alignment → 3D detection
- **Design tradeoffs:** Inference speed: Diffusion adds overhead (4.3 FPS vs. 6.2 FPS baseline per Table IV); DDIM 10-step sampling critical for viability; Pre-training vs. end-to-end: Diffusion model pre-trained separately on diverse weather; not jointly optimized with detection loss
- **Failure signatures:** Clean-data AP drop: Indicates diffusion over-smoothing or alignment being too aggressive; Small object misses: 2D detector may fail to localize distant/dim objects, breaking PCR guidance; Real-world fog generalization gap: Synthetic training may not cover real degradation distributions
- **First 3 experiments:** 1) Ablation per Table IV: Run baseline → +DBRM only → +DBRM+BAFAM on KITTI-C rain/fog/sunlight to confirm component contributions before full integration; 2) Diffusion step sensitivity: Test S={5,10,20,50} DDIM steps to profile quality vs. latency tradeoff; verify S=10 is justified; 3) Alignment direction test: Compare B2A (bidirectional) vs. unidirectional camera→LiDAR or LiDAR→camera alignment on KITTI-C to validate bidirectional assumption empirically

## Open Questions the Paper Calls Out
1. **Real-time optimization:** Future work will explore real-time optimization and extension to additional sensing modalities. Current 4.3 FPS is insufficient for real-time autonomous driving requirements (typically >15-30 FPS).

2. **Additional object classes and weather conditions:** All experiments evaluate only the car class on KITTI/KITTI-C. Different object classes have distinct size/appearance characteristics; different weather types (snow, dust) may produce different corruption patterns not captured by the diffusion model's training distribution.

3. **PCR sensitivity to upstream detection failures:** PCR relies on "2D detection bounding boxes B2d" from the image branch to guide point cloud completion. If Diffusion-IR fails under severe corruption, PCR inherits these failures.

## Limitations
- PCR component's assumption that 2D detections can reliably guide LiDAR point cloud restoration lacks direct corpus validation and may break under severe degradation
- Bidirectional alignment assumption in BAFAM lacks direct corpus validation; unidirectional approaches might suffice if one modality dominates under specific weather conditions
- Diffusion restoration's generalization from synthetic to real weather (particularly for DENSE fog) represents a critical gap as training distribution may not capture full complexity of natural weather phenomena

## Confidence
- **High confidence:** Diffusion-based image restoration mechanism (well-established in literature, direct validation in paper)
- **Medium confidence:** Multi-stage PCR pipeline and alignment module effectiveness (partially validated through ablation, but PCR lacks direct corpus support)
- **Medium confidence:** Overall framework performance claims (extensive quantitative results but limited qualitative analysis of failure modes)

## Next Checks
1. **PCR reliability test:** Systematically evaluate PCR performance across varying degradation levels and object distances to determine when 2D guidance fails, using both quantitative AP metrics and qualitative point cloud visualizations.

2. **Cross-modal alignment ablation:** Compare BAFAM's bidirectional approach against unidirectional alternatives (camera→LiDAR only, LiDAR→camera only) across all weather types to empirically validate the bidirectional assumption.

3. **Real-world generalization analysis:** Conduct detailed failure case analysis on DENSE fog data, comparing Diffusion-IR outputs against ground truth to quantify synthetic-to-real domain gap and identify specific degradation patterns not captured in training.