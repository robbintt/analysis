---
ver: rpa2
title: 'TOD-ProcBench: Benchmarking Complex Instruction-Following in Task-Oriented
  Dialogues'
arxiv_id: '2511.15976'
source_url: https://arxiv.org/abs/2511.15976
tags:
- customer
- agent
- conversation
- instruction
- turn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TOD-ProcBench, a benchmark for evaluating
  large language models' (LLMs) ability to follow complex, fine-grained instructions
  in multi-turn task-oriented dialogues (TODs). The benchmark is built on the ABCD
  dataset and features instruction documents with condition-action statements derived
  from real-world workflows.
---

# TOD-ProcBench: Benchmarking Complex Instruction-Following in Task-Oriented Dialogues

## Quick Facts
- arXiv ID: 2511.15976
- Source URL: https://arxiv.org/abs/2511.15976
- Reference count: 40
- Primary result: LLMs achieve maximum accuracies of 0.4310 (top-2 retrieval), 0.7608 (compliance detection), and 0.9565 (compliant generation) across three complex instruction-following tasks in multi-turn TODs.

## Executive Summary
TOD-ProcBench introduces a benchmark for evaluating large language models' ability to follow complex, fine-grained instructions in task-oriented dialogues. Built on the ABCD dataset, it features instruction documents with condition-action statements derived from real-world workflows across 55 user intents. The benchmark defines three tasks: instruction retrieval and next action prediction, instruction-following evaluation, and compliant response generation. LLMs struggle significantly across all tasks, with best overall accuracies of 0.4310 for Task 1, 0.7608 for Task 2, and 0.9565 for Task 3. Performance is only marginally affected by instruction format and conversation language, highlighting the need for further research to improve LLMs' instruction-following capabilities in complex TOD scenarios.

## Method Summary
The benchmark constructs instruction documents using six composition techniques (Single, And, Or, Chain, Selection, Nesting) to create multi-level condition-action statements. For Task 1, models must retrieve relevant instructions and predict next actions from partial conversations. Task 2 uses a synthetic data generation pipeline that manipulates instructions and generates violating responses to evaluate compliance detection. Task 3 requires generating compliant responses judged by an LLM-as-a-judge. The dataset includes 6,953 partial conversations across 55 intents in 7 languages, with three instruction formats (nested text, flattened text, JSON). Evaluation uses few-shot in-context learning with 3 demonstrations and chain-of-thought prompting across multiple LLM models.

## Key Results
- Task 1 (Instruction Retrieval & Next Action Prediction): Maximum top-2 accuracy of 0.4310 across all models and formats
- Task 2 (Compliance Evaluation): Maximum binary classification accuracy of 0.7608 for detecting instruction-violating responses
- Task 3 (Compliant Response Generation): Maximum compliance rate of 0.9565 when judged by Claude3.7-Sonnet
- Performance shows minimal variation across instruction formats (f1, f2, f3) and conversation languages (EN, FR, DE, ES, ZH, AR, HI)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structuring instructions as multi-level condition-action statements enables fine-grained state-to-rule mapping, but performance is bottlenecked by LLMs' reasoning over nested logic.
- Mechanism: Instructions are decomposed into "IF (Condition) - THEN (Action)" structures using six composition techniques. The model must process dialogue history, match the current state to conditions, and retrieve corresponding action sequences.
- Core assumption: LLMs can reliably parse natural language conditional logic and maintain state across multi-turn dialogues.
- Evidence anchors: [abstract] "We formulate fine-grained constraints and action procedures as multi-level condition-action instruction statements." [section 3.2.3] "we use six composition techniques to derive complete, complex instructions."
- Break condition: If the LLM cannot reliably decompose nested natural language into structured logic or fails to track state across dialogue turns, retrieval accuracy degrades significantly.

### Mechanism 2
- Claim: A synthetic data generation pipeline creates realistic instruction-violating responses, enabling robust binary compliance evaluation.
- Mechanism: The pipeline systematically manipulates instructions (e.g., parameter mismatches, action replacements) and prompts an LLM to generate responses compliant with the *manipulated* instruction, creating guaranteed violations of the original.
- Core assumption: LLM-generated synthetic violations are representative of real-world compliance failures.
- Evidence anchors: [abstract] "In Task 2, we synthesize instruction-violating responses by injecting inconsistencies and manipulating the original instructions..."
- Break condition: If generated violations are superficially different from compliant responses or fail to reflect nuanced errors, the benchmark's evaluation will be invalid.

### Mechanism 3
- Claim: LLM performance on complex instruction-following is robust to instruction format and conversation language variations.
- Mechanism: The study controls for instruction format (nested text, flattened text, JSON) and conversation language (7 languages). Performance is compared across all tasks and variations.
- Core assumption: Translation and format conversion preserve semantic equivalence without introducing artifacts.
- Evidence anchors: [abstract] "Performance is only marginally affected by instruction format and conversation language..."
- Break condition: If translation quality is poor or format conversion alters meaning, the observed robustness could be an artifact.

## Foundational Learning

- **Concept: Task-Oriented Dialogue (TOD) & Condition-Action Rules**
  - Why needed here: The paper evaluates LLMs in a TOD setting where agents must follow Standard Operating Procedures (SOPs) expressed as IF-THEN rules.
  - Quick check question: How would you translate a customer service policy ("Gold members get free returns within 30 days") into a condition-action rule?

- **Concept: Compositional Constraint Satisfaction**
  - Why needed here: The benchmark's complexity arises from combining simple conditions into complex rules using logical operators (AND, OR, Chain, Nesting).
  - Quick check question: What is the difference between a "Chain" and a "Selection" composition in the context of a dialogue instruction?

- **Concept: Chain-of-Thought (CoT) Prompting**
  - Why needed here: The evaluation methodology relies on CoT prompting to elicit reasoning from LLMs before they produce final outputs.
  - Quick check question: Why might CoT be particularly important for Task 1 (Instruction Retrieval and Next Action Prediction)?

## Architecture Onboarding

- Component map: ABCD dataset -> TOD-ProcBench (55 intents, 6,953 conversations) -> Three Tasks (Retrieval/Action, Compliance Eval, Compliant Gen) -> Synthetic Violation Pipeline

- Critical path: 1) Parse complex instruction document. 2) (Task 1) Retrieve relevant instruction section given dialogue history. 3) (Task 3) Generate response compliant with retrieved instruction. 4) (Task 2) Detect violations in responses.

- Design tradeoffs:
  - **Format (f1 vs. f2 vs. f3)**: f1 (Nested) is more natural but harder to parse; f2/f3 are structured but may lose nuance. Trade-off is between human-readability and machine-parseability.
  - **Synthetic vs. Human Violations**: Pipeline allows scalable dataset creation but risks creating violations that don't reflect real-world error modes.

- Failure signatures:
  - **Low Task 1 Accuracy**: Indicates failure in mapping dialogue state to relevant condition-action rules.
  - **High Task 3 but Low Task 2 Performance**: Suggests the model can generate plausible-sounding responses but lacks reasoning capacity to verify compliance.
  - **Performance collapse on "Begin" vs. "End" of conversation**: For smaller models, performance improves later in conversation, suggesting reliance on surface-level pattern matching rather than full state tracking.

- First 3 experiments:
  1. **Task 1 Baseline (Format f1)**: Run a strong LLM (e.g., Claude 3.7 Sonnet) on instruction retrieval and next action prediction. Measure top-1 and top-2 accuracy.
  2. **Format Ablation (Task 1)**: Run the same model from Experiment 1 on all three formats (f1, f2, f3). Quantify performance difference to isolate effect of instruction structure from reasoning ability.
  3. **Compliance Detection Analysis (Task 2)**: Evaluate an LLM on Task 2 using synthetic dataset. Analyze error cases to determine if failures stem from poor instruction retrieval or inability to detect subtle logical contradictions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LLM performance on complex instruction-following change when instructions are formatted as codified structures rather than natural language?
- Basis in paper: [explicit] The conclusion states, "As future works, we plan to extend TOD-ProcBench to additional dataset formats including codified structures across domain-specific instructions."
- Why unresolved: The current study limits its scope to natural language formats (Nested/Flattened If-Then, JSON mappings) and does not test programming-language-style logic.
- What evidence would resolve it: Benchmarking results comparing model accuracy on Task 1 and Task 3 using a version of TOD-ProcBench converted into formal logic or code.

### Open Question 2
- Question: What specific mechanisms cause LLMs to struggle with distinguishing overlapping triggering conditions in multi-turn dialogues?
- Basis in paper: [inferred] Task 1 results show low accuracy (max 0.4310), attributed to "subtle distinctions between actions and the overlapping nature of their triggering conditions."
- Why unresolved: The paper quantifies the failure but does not isolate whether the issue stems from semantic ambiguity in instructions or models' inability to maintain state over long contexts.
- What evidence would resolve it: An error analysis or ablation study isolating retrieval failures caused specifically by condition overlap versus context length.

### Open Question 3
- Question: Does translating the complex instruction documents into the agent's target conversation language improve compliance rates?
- Basis in paper: [inferred] The authors ask if "English instructions [are] still effective to guide multi-turn conversations in another language," but only test by translating conversation, leaving instructions in English.
- Why unresolved: It remains unclear if the "marginal" performance impact observed is solely due to model's multilingual reasoning or a mismatch between instruction and conversation language.
- What evidence would resolve it: A comparison of Task 2 and Task 3 performance using fully localized (translated) instruction documents versus current English-instruction setup.

## Limitations

- The reliance on synthetic data generation for Task 2 may not fully capture real-world compliance failure modes, potentially introducing bias in the evaluation
- The benchmark focuses on a single domain (e-commerce customer service) derived from the ABCD dataset, limiting generalizability to other TOD scenarios
- The evaluation methodology using LLM-as-a-judge for Task 3 inherits potential biases from the judge model's own instruction-following capabilities

## Confidence

- **High Confidence**: Benchmark construction methodology and reported quantitative results across all three tasks are reproducible
- **Medium Confidence**: Conclusion that performance is "only marginally affected by instruction format and conversation language" requires careful interpretation and may reflect data processing quality
- **Medium Confidence**: Interpretation that failures stem from reasoning over nested logic rather than representation issues is plausible but not definitively proven

## Next Checks

1. **Real-World Violation Validation**: Manually annotate a small subset of TOD-ProcBench conversations with ground-truth compliance labels to assess whether synthetic pipeline captures authentic failure modes

2. **Instruction Retrieval Ablation**: For Task 1, conduct an ablation study where models are given correct instruction section but must still predict next action, versus retrieving both instruction and action

3. **Cross-Domain Generalization Test**: Apply TOD-ProcBench evaluation methodology to instruction-following datasets from different domains (e.g., technical support, healthcare) to assess whether observed LLM limitations generalize beyond e-commerce customer service context