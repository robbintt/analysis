---
ver: rpa2
title: 'DeepPlanner: Scaling Planning Capability for Deep Research Agents via Advantage
  Shaping'
arxiv_id: '2510.12979'
source_url: https://arxiv.org/abs/2510.12979
tags:
- tool
- search
- planning
- answer
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies high entropy in planning tokens as a bottleneck
  in deep research agents and proposes DeepPlanner, an end-to-end reinforcement learning
  framework that enhances planning capabilities through two advantage shaping mechanisms.
  The first mechanism adds an entropy-based shaping term to amplify gradients on uncertain
  planning tokens while preventing negative advantage flips.
---

# DeepPlanner: Scaling Planning Capability for Deep Research Agents via Advantage Shaping

## Quick Facts
- arXiv ID: 2510.12979
- Source URL: https://arxiv.org/abs/2510.12979
- Reference count: 19
- Primary result: DeepPlanner achieves SOTA on 7 deep research benchmarks with 3,072 training samples and 8 rollouts per query, versus 32,000 samples and 16 rollouts for previous best methods

## Executive Summary
DeepPlanner addresses a fundamental bottleneck in deep research agents: high entropy in planning tokens that indicates uncertain decision points remaining under-optimized. The authors propose an end-to-end reinforcement learning framework with two advantage shaping mechanisms. The first mechanism uses entropy-based advantage shaping to amplify gradients on high-entropy planning tokens while preventing negative advantage flips. The second mechanism selectively upweights sample-level advantages for complex rollouts requiring intensive planning. Experiments demonstrate state-of-the-art performance across seven benchmarks while reducing planning token entropy over training, improving both in-domain and out-of-domain generalization.

## Method Summary
DeepPlanner builds on token-level Group Relative Policy Optimization (GRPO) with two key innovations. First, Entropy-based Advantage Shaping (EAS) adds a gradient-detached entropy term to token-level advantages, amplifying updates on high-entropy planning tokens while clipping prevents flipping negative advantages positive. Second, Selective Advantage Upweighting (SAU) identifies the most efficient correct rollout per query and multiplies its advantages by λ=2 when tool-call count exceeds threshold c=2. The method enforces an explicit plan-then-execute architecture requiring structured plans in `<plan></plan>` tags before any tool calls. Training uses Qwen2.5-7B-Instruct on 3,072 queries with 8 rollouts each, employing web_search and browse_webpage tools, with rewards based on format correctness and LLM-judged answer quality.

## Key Results
- Achieves state-of-the-art performance on 7 deep research benchmarks
- Reduces planning token entropy over training while improving performance
- Requires only 3,072 training samples with 8 rollouts per query versus 32,000 samples and 16 rollouts for previous best methods
- Demonstrates improved both in-domain and out-of-domain generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entropy-based advantage shaping accelerates planning optimization without causing entropy collapse
- Mechanism: Adds gradient-detached entropy term to token-level advantages (A^EAS = A + min(α·H, |A|/κ)), amplifying updates on high-entropy planning tokens while clipping prevents flipping negative advantages positive
- Core assumption: High-entropy tokens represent under-optimized decision points that benefit from stronger learning signals, not noise
- Evidence anchors: Planning entropy ~0.78 vs execution ~0.32 under vanilla GRPO; Equation 3 formalizes shaping; entropy-guided exploration literature supports approach

### Mechanism 2
- Claim: Selective advantage upweighting prioritizes learning from high-quality, planning-intensive rollouts
- Mechanism: Identifies most efficient correct rollout per query group and multiplies advantages by λ=2 when tool-call count exceeds threshold c=2
- Core assumption: Trajectories with more tool calls (above minimum efficiency) encode more planning decisions, providing richer learning signal
- Evidence anchors: Equation 4-5 define SAU; tool-call increase correlates with performance gains; related work on filtering high-tool-call rollouts for SFT

### Mechanism 3
- Claim: Explicit plan-then-execute architecture stabilizes long-horizon reasoning by exposing planning decisions
- Mechanism: Requires structured plans in `<plan></plan>` tags before any tool calls, with plans revisable mid-trajectory
- Core assumption: Making planning explicit enables more effective credit assignment to planning tokens during RL
- Evidence anchors: MBE improves from 63.4 to 64.4 with explicit planning; case study shows planning prevents entity drift; related work on explicit planners

## Foundational Learning

- Concept: Group Relative Policy Optimization (GRPO)
  - Why needed here: DeepPlanner builds on token-level GRPO; understanding how advantages are computed relative to rollout groups is essential for modifying advantage function
  - Quick check question: Given 8 rollouts for a query with rewards [1.0, 0.5, 1.0, 0, 1.0, 0.5, 0, 1.0], what is the group-relative advantage for a token in the first rollout?

- Concept: Token-level entropy in autoregressive models
  - Why needed here: EAS directly modulates updates by token entropy; must understand how entropy is computed (detached from computation graph) and why high entropy signals uncertainty
  - Quick check question: If a planning token has probability distribution [0.4, 0.3, 0.3] over vocabulary and an action token has [0.9, 0.05, 0.05], which has higher entropy?

- Concept: ReAct-style agent trajectories
  - Why needed here: DeepPlanner extends ReAct by adding explicit planning stage; understanding interleave of thinking, tool calls, and answers clarifies where planning tokens fit
  - Quick check question: In a trajectory with 3 tool calls, where does the initial `<plan>` appear relative to the first thinking segment?

## Architecture Onboarding

- Component map: Query → Agent generates 8 rollouts (plan → interleaved think/tool-call → answer) → Tools return responses → Compute rewards (format + LLM-judged answer) → Compute group-relative advantages → Apply EAS (per-token entropy shaping) → Apply SAU (per-rollout upweighting) → GRPO update

- Critical path: Query → Agent loop with VERL → 8 rollouts with plan-then-execute → Web search/browse tools → Reward computation → Advantage shaping (EAS + SAU) → GRPO update

- Design tradeoffs: Explicit planning improves interpretability and credit assignment but adds inference overhead; EAS coefficient α=0.1 and clipping κ=2 are tuned values; SAU threshold c=2 targets ~40% of rollouts; LLM-as-judge dependency introduces bias

- Failure signatures: Entropy collapse if planning entropy drops sharply early; format violations preventing learning if >50% rollouts have reward <0.5; inefficient tool usage if excessive calls without progress

- First 3 experiments: 1) Baseline diagnostic: Train vanilla GRPO for 48 steps, log per-stage token entropy to confirm entropy gap observation; 2) Ablation: EAS-only, compare planning entropy trajectory and in-domain MBE against vanilla; 3) Ablation: SAU-only, measure tool-call distribution shift and out-of-domain generalization

## Open Questions the Paper Calls Out

- Can fine-grained, multi-dimensional process rewards for planning (quality, feasibility, consistency, verifiability) further improve planning optimization compared to outcome-only reward structure?
- Does scaling training steps beyond 48-step cap yield further performance gains or induce entropy collapse in planning tokens?
- How robust are DeepPlanner's gains when evaluated with alternative judge models or ground-truth human evaluation, given reliance on chatgpt-4o-latest as sole evaluator?

## Limitations
- Entropy-based advantage shaping lacks ablation against simpler entropy penalties from prior work
- Selective advantage upweighting relies on binary tool-count threshold without sensitivity analysis
- Explicit plan-then-execute architecture assumes single upfront planning stage without exploring alternatives
- LLM-as-judge evaluation introduces bias and computational cost without validation of judge consistency
- Training efficiency claims rely on comparison to specific EvolveSearch framework with implementation differences

## Confidence
- High confidence: Empirical finding that planning tokens exhibit higher entropy than execution tokens
- Medium confidence: Core claim that EAS + SAU together achieve SOTA performance
- Medium confidence: Claim that explicit planning architecture improves credit assignment
- Low confidence: Claim that high-entropy planning tokens specifically represent under-optimized decision points
- Low confidence: Claim that SAU prioritizes learning from high-quality, planning-intensive rollouts

## Next Checks
1. Ablation of entropy shaping formulation: Compare EAS against simpler entropy regularization and no shaping, measuring both performance and entropy trajectory
2. Judge consistency validation: Run inter-annotator agreement study using human judges on subset of outputs to quantify LLM-as-judge reliability
3. Tool-count threshold sensitivity: Systematically vary SAU threshold c and upweighting factor λ to identify whether claimed benefits hold across hyperparameter range