---
ver: rpa2
title: Autonomous Code Evolution Meets NP-Completeness
arxiv_id: '2509.07367'
source_url: https://arxiv.org/abs/2509.07367
tags:
- rule
- solver
- cycle
- satlution
- rules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "SATLUTION autonomously evolves SAT solvers at repository scale,\
  \ extending LLM-based code evolution beyond single files to hundreds of files and\
  \ tens of thousands of lines. It uses two LLM agents\u2014Planning and Coding\u2014\
  guided by strict correctness rules and distributed runtime feedback to iteratively\
  \ improve solvers from SAT Competition 2024 codebases."
---

# Autonomous Code Evolution Meets NP-Completeness

## Quick Facts
- arXiv ID: 2509.07367
- Source URL: https://arxiv.org/abs/2509.07367
- Reference count: 40
- Primary result: SATLUTION evolved SAT solvers that outperformed 2025 competition winners across PAR-2 and solved instances

## Executive Summary
SATLUTION demonstrates that LLM-based autonomous code evolution can successfully operate at repository scale, evolving complete SAT solvers with hundreds of files and tens of thousands of lines. Using two Claude agents guided by strict correctness rules and distributed runtime feedback, it produced solvers that outperformed SAT Competition 2025 winners. The framework's key innovation is a rule-based system that prunes invalid mutations and prevents regressions, enabling stable long-horizon evolution. Over 70 cycles, SATLUTION steadily improved performance on both the 2024 benchmark and current competition instances.

## Method Summary
SATLUTION uses two LLM agents—Planning and Coding—to iteratively evolve SAT solvers from competition codebases. The Planning Agent analyzes performance and proposes modifications, while the Coding Agent implements changes with strict adherence to domain-specific rules. A two-stage verification pipeline catches compilation errors and correctness violations before distributed evaluation on 800 CPU nodes. The system incorporates self-evolving rules that adapt to failure patterns, preventing repeated mistakes. Performance feedback includes PAR-2 scores plus fine-grained metrics that guide evolution toward balanced improvements.

## Key Results
- Evolved solvers outperformed SAT Competition 2025 winners with lower PAR-2 scores and more solved instances
- Steady performance progression across 70 evolution cycles on both 2024 benchmark and current competition instances
- Rule-based scaffolding prevented regressions and improved search efficiency compared to ablations without rules
- Multi-metric feedback design prevented reward-hacking and balanced SAT/UNSAT optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rule-based scaffolding with self-evolving rules improves search efficiency and prevents regression in repository-scale code evolution.
- Mechanism: Static rules encode domain constraints, pruning invalid mutations before evaluation. Dynamic rules capture failure patterns post-cycle, converting them into forbidden patterns that prevent repeated mistakes. This reduces the search space and stabilizes long-horizon evolution.
- Core assumption: The rule system's effectiveness depends on LLMs reliably following structured instructions; compliance is assumed but not formally verified beyond observed performance gains.
- Evidence anchors:
  - [section] "In ablations where we removed the static initialization rules, the agent explored plausible but misleading directions... leading to frequent DRAT/solution check failures" (Page 16).
  - [section] "With the static rules in place, the system sort of prunes the evolution directions and space, e.g., forbidden patterns are flagged, failing builds are discarded before evaluation" (Page 17).
  - [corpus] AlphaEvolve [2506.13131] provides precedent for LLM-guided evolutionary coding but lacks comparable rule-system architecture for repository-scale tasks; mechanism uniqueness is plausible but not directly comparable.
- Break condition: If the agent fails to follow or correctly interpret rules (e.g., ignores forbidden patterns), the pruning effect diminishes and regressions accumulate; semi-automated operation with human intervention may become necessary (Page 22).

### Mechanism 2
- Claim: Two-stage verification (fast smoke tests + deep validation) provides tight feedback loops that preserve correctness without blocking evolutionary progress.
- Mechanism: Stage 1 catches compilation errors, segmentation faults, and obvious misclassifications on 115 trivial CNF formulas. Stage 2 validates SAT assignments and UNSAT DRAT proofs on a broader benchmark. Failures trigger immediate feedback to the Coding Agent for repair, preventing incorrect solvers from polluting the reward signal.
- Core assumption: The verification pipeline is manually engineered prior to evolution and assumed to be both complete (catching all critical errors) and efficient (not bottlenecking the iteration cycle).
- Evidence anchors:
  - [abstract] "guided by strict correctness rules and distributed runtime feedback."
  - [section] Figure 10 and Pages 13–14 describe the two-stage pipeline; "throughout our experiments, no solver version that passed both stages ever produced a misclassification on the competition benchmarks" (Page 14).
  - [corpus] No direct corpus comparison; verification design is domain-specific to SAT and not discussed in AlphaEvolve or other neighbors.
- Break condition: If Stage 2 validation is incomplete (e.g., misses certain proof errors) or too slow (>1.5 hours per cycle), evolution either risks incorrect solvers or stalls; human intervention is required to adjust the pipeline (Page 22).

### Mechanism 3
- Claim: Multi-faceted runtime feedback (PAR-2 plus fine-grained metrics) guides evolution toward balanced solver improvements rather than reward-hacking.
- Mechanism: The agent receives not only PAR-2 scores but also solved-instance counts by category, time-cutoff distributions, and memory usage. Early cycles optimizing only "total solved" favored UNSAT; introducing PAR-2 and time-cutoff metrics shifted focus toward SAT instances where PAR-2 is more sensitive, yielding balanced gains.
- Core assumption: The feedback metrics are assumed to align with true solver quality; the agent is assumed to interpret composite signals correctly without overfitting to any single metric.
- Evidence anchors:
  - [section] "In the early iterations, when SATLUTION was guided primarily by the metric of 'total number of instances solved,' the evolved solvers tended to focus on strengthening UNSAT performance... once we incorporated PAR-2 scores together with finer-grained metrics... SATLUTION shifted its emphasis towards satisfiable instances" (Page 15).
  - [corpus] AlphaEvolve uses evolutionary feedback but does not detail multi-metric reward design; mechanism is consistent with evolutionary optimization principles but not directly validated in corpus.
- Break condition: If metrics are misaligned with true solver quality (e.g., rewarding speed over correctness) or if the agent overfits to a subset of metrics, evolution may produce brittle solvers; manual reward redesign is required.

## Foundational Learning

- Concept: **Conflict-Driven Clause Learning (CDCL)**
  - Why needed here: SATLUTION evolves SAT solvers built on CDCL; understanding restart policies, clause management, and branching heuristics is necessary to interpret the agent's modifications and the learned techniques (Table 4).
  - Quick check question: Can you explain how CDCL improves over basic DPLL backtracking?

- Concept: **LLM-based Code Evolution (AlphaEvolve paradigm)**
  - Why needed here: SATLUTION extends AlphaEvolve's evolutionary coding approach from single-file kernels to full repositories; understanding the base paradigm clarifies what is novel vs. inherited.
  - Quick check question: How does AlphaEvolve use LLMs to iteratively propose and evaluate code changes?

- Concept: **Rule-Based Agent Guidance**
  - Why needed here: The rule system is central to SATLUTION's stability; knowing how structured rules constrain and direct LLM agents helps explain why ablations without rules failed.
  - Quick check question: What types of rules (forbidden patterns, correctness constraints, logging requirements) are used, and how are they enforced?

## Architecture Onboarding

- Component map:
  - Planning Agent -> Coding Agent -> Rule System -> Verification Pipeline -> Distributed Evaluator -> Feedback Analyzer -> Rule Evolution -> Next Cycle

- Critical path:
  1. Planning Agent generates `HYPOTHESIS.md` with proposed modifications.
  2. Coding Agent edits source, updates `CHANGELOG.md`, builds solver.
  3. Stage 1 verification runs; failures return to Coding Agent.
  4. Stage 2 verification validates correctness; failures return to Coding Agent.
  5. Distributed evaluator benchmarks solver on SAT Competition 2024 instances (~1 hour).
  6. Feedback analyzer computes metrics, writes `RESULTS.md`.
  7. Rule evolution scripts update forbidden patterns and compliance checks.
  8. Next cycle begins with updated rules and champion solver.

- Design tradeoffs:
  - **Verification strictness vs. iteration speed**: Two-stage verification catches errors early but adds latency; manual tuning was required to balance thoroughness and cycle time.
  - **Metric richness vs. agent interpretation complexity**: Multi-metric feedback provides nuanced guidance but risks confusing the agent; PAR-2 was withheld until cycle 33 to prevent early reward-hacking.
  - **Static vs. self-evolving rules**: Static rules provide stability; self-evolving rules adapt to new failure patterns but require careful monitoring to avoid drift (Pages 16–17, 27–31).

- Failure signatures:
  - **Segmentation faults**: Partial crashes on subsets of the test set; 11 variants affected across 70 cycles (Page 9).
  - **Correctness violations**: 4 variants failed functional or proof validation; automatically pruned (Page 9).
  - **Performance regressions**: 9 variants showed significant PAR-2 degradation; pruned by feedback analysis (Page 9).
  - **Rule non-compliance**: Missing `CHANGELOG.md`, `HYPOTHESIS.md`, or `RESULTS.md`; detected by compliance scripts (Pages 11, 27).

- First 3 experiments:
  1. **Ablation without static rules**: Run 20 evolution cycles with rule system disabled; measure failure rates (expect: high compile/correctness failures, degraded performance) to validate rule necessity.
  2. **Single-metric vs. multi-metric feedback**: Compare evolution trajectories using only "total solved" vs. full metric suite; analyze SAT/UNSAT balance and PAR-2 progression to validate reward design.
  3. **Verifier latency sensitivity**: Reduce Stage 2 validation to a smaller benchmark subset; measure cycle time reduction vs. correctness error rate to quantify verification-performance tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLM-based coding agents autonomously construct, adapt, and optimize their own domain-specific verifiers without human expertise?
- Basis in paper: [explicit] Discussion states: "the ability for agents to autonomously construct, adapt, and optimize their own verifiers would mark a step-change in capability... At present, human expertise is indispensable in designing robust verification pipelines."
- Why unresolved: The current framework relies entirely on manually engineered verifiers (two-stage verification pipeline, DRAT proof checker) created prior to evolution.
- What evidence would resolve it: An autonomous agent successfully designing and deploying a correct-by-construction verifier for a new domain (e.g., EDA flow verification) without human-authored verification logic.

### Open Question 2
- Question: What is the minimum human intervention required for stable repository-scale evolution, and can fully autonomous ("YOLO mode") operation achieve comparable results?
- Basis in paper: [explicit] Discussion notes "in fully automated operation—the agents often struggled, and the flow proved most effective in a semi-automated setup with targeted human intervention."
- Why unresolved: Agents failed on SAT/UNSAT correctness checks and segmentation faults without human intervention; the exact failure modes and intervention thresholds remain unquantified.
- What evidence would resolve it: Controlled ablation comparing success rates, PAR-2 trajectories, and failure recovery across varying levels of human intervention frequency.

### Open Question 3
- Question: Does the SATLUTION framework generalize to other NP-complete problem domains or large-scale software engineering tasks beyond SAT solving?
- Basis in paper: [inferred] The paper demonstrates success only on SAT solvers, though it claims repository-scale evolution capability; generalization to other domains is not tested.
- Why unresolved: No experiments on other domains (e.g., MaxSAT, QBF, scheduling, routing); domain-specific initialization rules and verifiers may not transfer.
- What evidence would resolve it: Applying the same framework with minimal modification to evolve competitive solvers for other NP-complete problems (e.g., winning MaxSAT Competition entries).

### Open Question 4
- Question: How can the entangled learned solver components be systematically disentangled to enable controlled ablation studies?
- Basis in paper: [explicit] "conducting controlled ablation studies of these learned components remains challenging due to the highly entangled nature of their implementations in a complex SAT solving system."
- Why unresolved: Cumulative modifications added 10,000+ lines of code, making isolation of individual technique contributions (multi-UIP, bandit-tuned vivification, etc.) infeasible.
- What evidence would resolve it: A methodology or architecture enabling incremental component insertion/removal while maintaining correctness, with measured per-component PAR-2 impact.

## Limitations
- Verification completeness is assumed but not formally proven, with Stage 2 validation taking ~1.5 hours that may limit scalability
- Rule compliance is assumed rather than formally verified, creating risk of regressions if agents ignore constraints
- Generalizability beyond SAT remains untested despite repository-scale claims
- Learned solver components are highly entangled, preventing systematic ablation studies

## Confidence
- **High confidence**: Performance improvement claims on SAT Competition benchmarks (PAR-2 reduction, increased solved instances) are directly measured and reproducible
- **Medium confidence**: Rule system necessity and verification effectiveness are supported by ablation observations but lack controlled experimental validation
- **Low confidence**: Prevention of reward hacking and robustness of multi-metric feedback are inferred from observed behavior rather than systematically tested

## Next Checks
1. **Rule compliance audit**: Instrument the system to log all rule checks and agent responses, then conduct a retrospective analysis to verify whether the agent actually followed forbidden patterns and correctness constraints as intended.
2. **Verification completeness test**: Design and run synthetic solvers that deliberately violate correctness in ways not covered by current Stage 2 validation, measuring false negative rates to quantify verification blind spots.
3. **Metric ablation study**: Run controlled evolution experiments with different subsets of the feedback metrics (PAR-2 only, solved instances only, various combinations) to systematically measure sensitivity and identify potential reward hacking vectors.