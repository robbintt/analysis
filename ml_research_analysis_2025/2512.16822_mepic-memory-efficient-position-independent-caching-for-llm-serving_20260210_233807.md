---
ver: rpa2
title: 'MEPIC: Memory Efficient Position Independent Caching for LLM Serving'
arxiv_id: '2512.16822'
source_url: https://arxiv.org/abs/2512.16822
tags:
- chunk
- reuse
- cache
- memory
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MEPIC addresses the challenge of efficiently reusing KV cache for
  long prompts with shared content across multiple LLM serving requests. It introduces
  page-aligned, position-independent chunk KV management, enabling deterministic KV
  layouts and cross-request sharing.
---

# MEPIC: Memory Efficient Position Independent Caching for LLM Serving
## Quick Facts
- arXiv ID: 2512.16822
- Source URL: https://arxiv.org/abs/2512.16822
- Reference count: 12
- Key outcome: Reduces HBM usage by up to 2× over PIC methods and 5× for long prompts via page-aligned, position-independent KV cache sharing

## Executive Summary
MEPIC addresses the challenge of efficiently reusing KV cache for long prompts with shared content across multiple LLM serving requests. It introduces page-aligned, position-independent chunk KV management, enabling deterministic KV layouts and cross-request sharing. By recomputing only the first block of each chunk and fusing RoPE into the attention kernel, it eliminates position-dependent KV divergence. Integrated into vLLM + LMCache, MEPIC reduces HBM usage by up to 2× over state-of-the-art PIC methods and up to 5× for long prompts, while maintaining comparable or better accuracy and latency without model changes.

## Method Summary
MEPIC partitions requests into chunk (reusable) and prompt (request-specific) segments, aligning chunk KV to fixed-size pages with deterministic padding. This creates canonical layouts hashable by content rather than position. Instead of token-level recomputation, MEPIC statically recomputes only the first block of each chunk to capture context-dependent boundary attention. KV is stored in NoPE format, and RoPE is applied on-the-fly within a fused attention kernel during execution. Integrated into vLLM + LMCache, MEPIC enables HBM-efficient KV cache sharing across requests with overlapping content.

## Key Results
- Reduces HBM usage by up to 2× compared to state-of-the-art PIC methods
- Achieves up to 5× memory reduction for long prompts
- Maintains comparable or better accuracy and latency without model changes

## Why This Works (Mechanism)

### Mechanism 1: Page-Aligned Canonical KV Layouts
- **Claim:** Aligning chunk KV to fixed-size pages with deterministic padding enables identical logical chunks to map to identical physical HBM blocks across requests.
- **Mechanism:** MEPIC partitions requests into chunk (reusable) and prompt (request-specific) segments. Chunks receive leading padding to block boundaries; prompts receive trailing padding. This asymmetric scheme ensures each chunk starts at a page boundary, producing a canonical layout hashable by content rather than position.
- **Core assumption:** Identical token sequences at different prompt offsets should produce identical KV block layouts.
- **Evidence anchors:**
  - [abstract] "page-aligned, position-independent chunk KV management, enabling deterministic KV layouts and cross-request sharing"
  - [section 3.2.1] "segments are padded at block granularity: chunk segments receive leading padding, prompt segments receive trailing padding"
  - [corpus] Cache-Craft (2502.15734) discusses chunk-cache management but does not address page alignment for HBM sharing.
- **Break condition:** If chunk boundaries cannot be deterministically identified (e.g., streaming inputs with unknown chunk structure), canonical alignment fails and degrades to per-request duplication.

### Mechanism 2: Block-Level Selective Recomputation
- **Claim:** Recomputing only the first KV block per chunk preserves accuracy while making remaining blocks fully shareable.
- **Mechanism:** Instead of token-level recomputation (EPIC, CacheBlend), MEPIC statically recomputes the first block of each chunk to capture context-dependent boundary attention. Remaining blocks are reused as-is, avoiding request-specific "dirty" blocks that would prevent sharing.
- **Core assumption:** Boundary-dependent attention effects are localized to the first block; cross-block effects from omitted tokens are negligible for the evaluated tasks.
- **Evidence anchors:**
  - [abstract] "recomputing only the first block of each chunk... makes remaining blocks fully shareable"
  - [section 3.3.1] "cached chunks require recomputation of only the first KV block, with the remaining blocks reused as canonical KV"
  - [section 4.5] "empirical results show no measurable drop in accuracy for our benchmark datasets"
  - [corpus] EPIC and CacheBlend use token-level recomputation; corpus does not provide comparative block-level evidence.
- **Break condition:** If boundary attention effects extend beyond the first block (e.g., highly interdependent long-range dependencies), accuracy may degrade; the paper does not evaluate this regime.

### Mechanism 3: Deferred RoPE via Fused Attention Kernels
- **Claim:** Storing KV without pre-applied RoPE and fusing positional encoding into the attention kernel enables position-agnostic KV reuse.
- **Mechanism:** KV is stored in NoPE format. During attention, the kernel applies RoPE on-the-fly using the target position offset before computing attention scores. This eliminates position-specific KV copies while avoiding extra memory traffic.
- **Core assumption:** On-the-fly RoPE computation overhead is negligible compared to HBM savings from eliminating duplicate KV copies.
- **Evidence anchors:**
  - [abstract] "fusing RoPE into the attention kernel, it eliminates position-dependent KV divergence"
  - [section 3.3.3] "MEPIC instead stores KV in a positional-encoding-free (NoPE) format... applies the appropriate rotary offsets on the fly within a fused RoPE–attention operator"
  - [corpus] Related PIC papers (CacheClip, KVShare) adjust PE per request but do not document deferred RoPE with kernel fusion.
- **Break condition:** For non-RoPE models (ALiBi, absolute positional embeddings), this mechanism does not directly apply; alternative position-decoupling strategies would be needed.

## Foundational Learning

- **Concept: KV Cache in Transformer Decoding**
  - **Why needed here:** MEPIC operates at the KV cache layer; understanding what KV is and why it dominates HBM is prerequisite.
  - **Quick check question:** During autoregressive decoding, what does the KV cache store and why does recompute-free generation depend on it?

- **Concept: Prefix Caching vs Position-Independent Caching**
  - **Why needed here:** MEPIC's contribution is extending reuse beyond strict prefix matching; the distinction clarifies the problem.
  - **Quick check question:** Why does prefix caching fail when the same document chunk appears at different positions in two prompts?

- **Concept: Rotary Position Embeddings (RoPE)**
  - **Why needed here:** MEPIC's NoPE storage and deferred RoPE fusion are core to position-independence; understanding RoPE's interaction with KV is essential.
  - **Quick check question:** In a RoPE-based model, if you cache KV vectors with positions 0–99 pre-applied, can you reuse them at positions 100–199? Why or why not?

## Architecture Onboarding

- **Component map:** Chunk Processor (segmentation/padding) → Chunk Matcher (residency lookup) → Hybrid KV Manager (prefix + chunk coordinators) → Chunk LRU Manager (eviction) → Fused RoPE attention kernel

- **Critical path:**
  1. Request arrives with token sequence + segment markers
  2. Scheduler partitions, pads, and resolves residency
  3. If non-resident: allocate blocks, trigger first-block recomputation
  4. Execution loads NoPE KV, fuses RoPE, computes attention
  5. On eviction pressure: LRU-chunk donation to LMCache tier

- **Design tradeoffs:**
  - Integrated (in-vLLM) vs external chunk pool: integration adds scheduler complexity but avoids copy overhead and kernel fragmentation.
  - Single-block vs multi-block recomputation: minimizes compute but assumes boundary effects are localized.
  - HBM residency vs offload frequency: hot chunks stay resident; cold chunks incur reload latency.

- **Failure signatures:**
  - Chunk misalignment → hash collisions or missed reuse → HBM usage spikes
  - Over-aggressive eviction → repeated reloads from CPU/disk → TTFT regression
  - Non-RoPE model → NoPE storage invalid → accuracy degradation or kernel errors

- **First 3 experiments:**
  1. **Sanity check:** Single-request, single-chunk. Verify first-block recomputation produces correct attention outputs vs full recomputation baseline.
  2. **Sharing stress test:** Multi-request batch with high chunk overlap. Measure HBM reduction vs CacheBlend/EPIC; confirm peak usage aligns with page-sharing expectations.
  3. **Eviction pressure test:** Constrain HBM capacity, increase QPS. Validate LRU eviction doesn't break correctness and that reload latency is within acceptable bounds.

## Open Questions the Paper Calls Out
None

## Limitations
- Boundary assumption validity: Assumes first-block-only recomputation is sufficient for accuracy, but doesn't validate under highly interdependent long-range dependencies
- Non-RoPE model compatibility: NoPE storage and fused RoPE specifically target RoPE-based models, leaving gap for other position encoding schemes
- Streaming input limitations: Deterministic chunk boundaries required; streaming inputs may fall back to per-request KV allocation

## Confidence
- High confidence: Page-aligned canonical KV layout mechanism (Mechanism 1)
- Medium confidence: Block-level selective recomputation (Mechanism 2) - lacks comparative evidence under varying conditions
- Medium confidence: Deferred RoPE via fused attention kernels (Mechanism 3) - theoretical soundness but lacks quantitative overhead data

## Next Checks
1. **Boundary effect validation:** Design experiments using tasks with known long-range dependencies (e.g., logical reasoning, document-level sentiment analysis) to test whether first-block-only recomputation maintains accuracy when boundary attention effects extend beyond the initial block.

2. **Non-RoPE adaptation:** Implement and evaluate a modified MEPIC for ALiBi-based models, documenting any architectural changes needed and quantifying the memory/compute tradeoffs compared to the RoPE-specific implementation.

3. **Mixed workload performance:** Test MEPIC under heterogeneous workloads mixing streaming inputs with pre-segmented prompts to measure how often the system falls back to non-position-independent behavior and what percentage of total requests lose memory savings benefits.