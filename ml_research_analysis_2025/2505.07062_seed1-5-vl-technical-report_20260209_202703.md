---
ver: rpa2
title: Seed1.5-VL Technical Report
arxiv_id: '2505.07062'
source_url: https://arxiv.org/abs/2505.07062
tags:
- arxiv
- data
- video
- seed1
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Seed1.5-VL is a vision-language foundation model designed to advance
  general-purpose multimodal understanding and reasoning. It features a 532M-parameter
  vision encoder and a 20B-parameter MoE language model.
---

# Seed1.5-VL Technical Report

## Quick Facts
- **arXiv ID:** 2505.07062
- **Source URL:** https://arxiv.org/abs/2505.07062
- **Reference count:** 40
- **Key outcome:** Seed1.5-VL is a vision-language foundation model with a 532M-parameter vision encoder and 20B-parameter MoE language model, achieving state-of-the-art performance on 38 out of 60 public benchmarks.

## Executive Summary
Seed1.5-VL is a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. It features a 532M-parameter vision encoder and a 20B-parameter MoE language model. The model delivers state-of-the-art performance on 38 out of 60 public benchmarks, excelling in visual reasoning, grounding, counting, video understanding, and agentic tasks like GUI control and gameplay. Key innovations include a native-resolution vision encoder (Seed-ViT), dynamic frame-resolution sampling for video, and a hybrid parallelism training infrastructure. Seed1.5-VL demonstrates strong performance on complex tasks such as visual puzzles, document understanding, and STEM reasoning, while maintaining a compact, efficient architecture suitable for real-world deployment.

## Method Summary
Seed1.5-VL uses a three-stage training pipeline: (1) ViT pre-training with masked image modeling using EVA02-CLIP-E teacher, contrastive learning with SigLIP loss, and omni-modal training via MiCo framework; (2) VLM pre-training with three stages—Stage 0 (adapter-only, 16B tokens), Stage 1 (full model, 3T tokens), and Stage 2 (full model, 240B tokens, 131K context); (3) Post-training with SFT on 50K samples followed by hybrid RL combining RLHF and RLVR with PPO variant. The model employs native-resolution vision encoding through Seed-ViT (532M params) and a 20B active-parameter MoE LLM, with hybrid parallelism training infrastructure optimized for variable-resolution images.

## Key Results
- Achieves SOTA on 38 out of 60 public benchmarks including MMMU, MathVista, RefCOCO, OCRBench, and Video-MME
- Excels in visual reasoning, grounding, counting, and video understanding tasks
- Demonstrates strong performance on complex tasks including visual puzzles, document understanding, and STEM reasoning
- Maintains compact, efficient architecture with 532M vision encoder and 20B MoE LLM parameters

## Why This Works (Mechanism)

### Mechanism 1: Native-Resolution Vision Encoding
- **Claim:** Processing images at their native resolution preserves fine-grained visual information that fixed-resolution encoders discard.
- **Mechanism:** Seed-ViT uses 2D rotary position embeddings (RoPE) to handle arbitrary image dimensions, patches images into 14×14 tokens, applies 2×2 average pooling, and processes variable-length sequences with appropriate attention masking.
- **Core assumption:** Visual details critical for OCR, document understanding, and grounding are resolution-dependent and lost through aggressive downsampling.
- **Evidence anchors:** [abstract] "Key innovations include a native-resolution vision encoder (Seed-ViT)"; [section 2.1] "Vision Encoder... employs 2D RoPE for positional encoding, enabling flexible adaptation to images of arbitrary dimensions".
- **Break condition:** When input resolution consistently matches training distribution (e.g., always 224×224), fixed-resolution approaches may be more efficient with minimal performance loss.

### Mechanism 2: Hybrid Reinforcement Learning Combining RLHF and RLVR
- **Claim:** Combining human preference feedback with rule-based verifiable rewards improves instruction-following and reasoning simultaneously.
- **Mechanism:** Uses shared critic for both reward sources (RM scores normalized [0,1]; verifiers scaled to match). Applies different KL coefficients (1×10⁻⁵ for general prompts, 0 for verifiable tasks). Truncates chain-of-thought before RM evaluation to reduce thought-level constraints.
- **Core assumption:** Human preferences capture alignment quality that rule-based metrics miss, while verifiable tasks provide unambiguous learning signals for reasoning skills.
- **Evidence anchors:** [abstract] "demonstrates strong reasoning abilities, making it particularly effective for multimodal reasoning challenges"; [section 4.4] "Hybrid reward... general and verifiable prompts are mixed in each batch".
- **Break condition:** When tasks have no verifiable ground truth or when human preference data is low-quality/noisy, hybrid approach may introduce conflicting gradients.

### Mechanism 3: Dynamic Frame-Resolution Sampling for Video
- **Claim:** Adaptively allocating tokens across temporal (frame rate) and spatial (resolution) dimensions within a fixed budget captures more relevant video information.
- **Mechanism:** Default 1 FPS with 6 resolution levels (640-128 tokens/frame), up to 5 FPS for motion tasks. Timestamp tokens prepended to each frame. Maximum 81,920 tokens/video with fallback uniform sampling for extremely long videos.
- **Core assumption:** Video understanding benefits from variable temporal density—static scenes need fewer frames, action-heavy segments need more.
- **Evidence anchors:** [section 2.2] "dynamic frame sampling... adjusts the frame sampling frequency based on content complexity"; [table 7] Video benchmark results show strong performance on temporal tasks (MotionBench 68.4, TemporalBench 79.8).
- **Break condition:** When videos are uniformly action-dense or uniformly static, simpler uniform sampling may be more predictable and easier to tune.

## Foundational Learning

- **Vision-Language Alignment via Contrastive Learning:**
  - Why needed here: Seed-ViT pre-training uses SigLIP loss (section 2.1.2); understanding contrastive objectives is prerequisite for debugging encoder quality.
  - Quick check question: Can you explain why contrastive learning aligns image-text embeddings while classification objectives don't?

- **Mixture-of-Experts (MoE) Routing:**
  - Why needed here: The LLM uses 20B active parameters via MoE (abstract); understanding expert routing is essential for analyzing inference costs and load balancing.
  - Quick check question: How does MoE differ from dense scaling in terms of computation vs. parameter count?

- **Proximal Policy Optimization (PPO) for RLHF:**
  - Why needed here: Post-training uses PPO variant with hybrid rewards (section 4.4); debugging RL training requires understanding policy gradient basics.
  - Quick check question: Why does PPO use a clipped objective instead of raw policy gradient?

## Architecture Onboarding

- **Component map:**
  ```
  Input Image/Video
       ↓
  Native-Resolution Transform (bilinear to 28×28 multiple)
       ↓
  Seed-ViT (532M params, 2D RoPE, 14×14 patches)
       ↓
  2×2 Average Pooling → MLP Adapter (2 layers)
       ↓
  Seed1.5-LLM (20B active, MoE decoder-only)
       ↓
  Text Output (with optional LongCoT)
  ```

- **Critical path:** Vision encoder pre-training → Stage 0 (MLP-only alignment) → Stage 1 (3T tokens, full training) → Stage 2 (240B tokens, long context) → SFT (50K samples) → Hybrid RL (60K GPU hours). **Warning:** Skipping Stage 0 causes "slightly higher loss" (section 3.2).

- **Design tradeoffs:**
  - Compact vision encoder (532M) vs. larger encoders (6B+ in competitors): trades absolute perception for efficiency; shows comparable zero-shot accuracy to InternVL-C-6B (table 5)
  - 20B active params vs. 70B+ models: trades knowledge/capability breadth for inference cost; shows gaps in knowledge benchmarks vs. Gemini 2.5 Pro (table 10)
  - Fixed 81,920 token video budget: trades flexibility for predictable memory usage; may truncate very long videos

- **Failure signatures:**
  - **Hallucination from knowledge priors:** Model overrides visual evidence with learned text knowledge (figure 26 shows incorrect DOT code generation based on standard transformer assumptions)
  - **3D spatial reasoning failures:** Cannot reliably solve dice rotation or multi-view block counting (figure 25)
  - **Combinatorial search limitations:** Fails on puzzles with large search spaces (figure 27-28, prime digit puzzle)
  - **Counting irregular objects:** Struggles with occluded/irregularly arranged objects (section 6.4)

- **First 3 experiments:**
  1. **Baseline inference test:** Run inference on 10 images from each category (OCR, grounding, reasoning) with both thinking and non-thinking modes; compare latency and accuracy to verify mode switching works correctly.
  2. **Video token budget ablation:** Process same video at different max_token settings (40K, 80K, 120K) to observe quality vs. memory tradeoff; measure temporal grounding accuracy on Charades-STA subset.
  3. **Grounding coordinate sanity check:** Query bounding boxes for known objects, visualize normalized [0,999] coordinates overlaid on images; verify coordinate system consistency across resolutions.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can image generation capabilities be effectively integrated into foundation models to enable "visual chain-of-thought" mechanisms for 3D spatial reasoning?
- **Basis in paper:** [explicit] The authors identify limitations in 3D spatial imagination and propose "incorporating image generation capabilities... to further enable visual chain-of-thought mechanisms" as a future research subject (Page 31).
- **Why unresolved:** Current autoregressive VLMs struggle to imagine 3D object projections or manipulate spatial relationships mentally without visual feedback loops.
- **What evidence would resolve it:** A unified VLM architecture that interleaves text generation with image synthesis to check spatial consistency, demonstrating superior performance on 3D spatial benchmarks compared to text-only reasoning models.

### Open Question 2
- **Question:** Can the integration of code-use and external tools resolve the specific failure modes VLMs exhibit in tasks requiring combinatorial search?
- **Basis in paper:** [explicit] The report notes that "Reasoning requiring combinatorial search poses a significant challenge" and explicitly suggests that "incorporating code-use... represents an important direction for future research" (Page 31).
- **Why unresolved:** VLMs currently fail on tasks like counting irregular squares or solving complex puzzles where programmatic enumeration is more effective than direct inference (Figures 27-28).
- **What evidence would resolve it:** Benchmarking a tool-augmented VLM on combinatorial puzzles (e.g., counting overlapping shapes) to verify if offloading search to code execution eliminates the performance gap.

### Open Question 3
- **Question:** Does the observed log-linear relationship between sub-category training loss and downstream evaluation metrics persist outside the current "local neighborhood" of performance?
- **Basis in paper:** [explicit] The authors state that while they observe a log-linear correlation (Figure 3), "such a log-linear relationship is likely sustainable only within a local neighborhood" and requires further exploration (Page 15).
- **Why unresolved:** It is unclear if this predictor holds as models approach saturation or if the relationship is merely an artifact of the current training scale.
- **What evidence would resolve it:** Training successive iterations of the model to higher data/compute scales and analyzing if the linear fit between loss and metrics (e.g., ChartQA accuracy) degrades or holds true.

## Limitations

- The vision encoder's 532M parameter count shows performance gaps compared to larger alternatives on knowledge-intensive tasks
- The hybrid RL approach lacks complete specification of reward model training and conflict resolution between preference and verifiable rewards
- The fixed 81,920 token video budget may truncate extremely long videos, limiting video understanding capabilities
- The model struggles with complex 3D spatial reasoning and combinatorial search tasks, indicating fundamental reasoning limitations

## Confidence

**High Confidence (8-10/10):** Vision encoder architecture and training pipeline are well-specified with clear details on Seed-ViT implementation and 3-stage pre-training. Benchmark results on standard vision-language tasks are verifiable against public datasets.

**Medium Confidence (5-7/10):** Post-training methodology, particularly hybrid RL implementation, lacks complete specification. Video processing pipeline's dynamic frame sampling is theoretically sound but lacks ablation studies.

**Low Confidence (1-4/10):** Knowledge capability claims are difficult to verify given smaller parameter count compared to competitors. Agentic task performance on GUI control and gameplay is demonstrated through internal benchmarks without clear evaluation methodology.

## Next Checks

1. **Grounding Coordinate System Validation:** Process 50 images with known object locations through the model, extract bounding box predictions, and verify that normalized coordinates [0, 999] correctly map to actual image coordinates across different resolutions.

2. **Video Token Budget Sensitivity:** Run identical video understanding tasks across three different max_token settings (40K, 80K, 120K) and measure performance degradation on temporal grounding and action recognition.

3. **Hybrid RL Reward Consistency:** Implement a simplified version of the hybrid reward mechanism on a single verifiable task (e.g., counting objects) and verify that human preference signals do not override correct rule-based rewards.