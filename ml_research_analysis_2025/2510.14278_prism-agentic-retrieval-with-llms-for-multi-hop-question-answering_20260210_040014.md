---
ver: rpa2
title: 'PRISM: Agentic Retrieval with LLMs for Multi-Hop Question Answering'
arxiv_id: '2510.14278'
source_url: https://arxiv.org/abs/2510.14278
tags:
- retrieval
- evidence
- recall
- question
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents PRISM, an agentic retrieval framework for\
  \ multi-hop question answering that explicitly balances precision and recall through\
  \ a structured loop of three LLM-based agents: a Question Analyzer that decomposes\
  \ complex queries into sub-questions, a Selector that filters candidates for precision,\
  \ and an Adder that supplements evidence for recall. The iterative Selector\u21D4\
  Adder cycle refines a compact, comprehensive evidence set that outperforms prior\
  \ baselines on HotpotQA (90.9% recall), 2WikiMultiHopQA (91.1%), MuSiQue (83.2%),\
  \ and MultiHopRAG (40.64% recall)."
---

# PRISM: Agentic Retrieval with LLMs for Multi-Hop Question Answering

## Quick Facts
- arXiv ID: 2510.14278
- Source URL: https://arxiv.org/abs/2510.14278
- Authors: Md Mahadi Hasan Nahid; Davood Rafiei
- Reference count: 18
- Primary result: Explicit precision-recall decoupling via iterative Selector/Adder agents achieves 90.9% recall on HotpotQA

## Executive Summary
This paper presents PRISM, an agentic retrieval framework for multi-hop question answering that explicitly balances precision and recall through a structured loop of three LLM-based agents: a Question Analyzer that decomposes complex queries into sub-questions, a Selector that filters candidates for precision, and an Adder that supplements evidence for recall. The iterative Selector⇔Adder cycle refines a compact, comprehensive evidence set that outperforms prior baselines on HotpotQA (90.9% recall), 2WikiMultiHopQA (91.1%), MuSiQue (83.2%), and MultiHopRAG (40.64% recall). This retrieval quality directly translates into higher QA accuracy, with PRISM surpassing strong methods like IRCoT and SetR, and approaching oracle performance while reducing noise. The framework is robust across LLM backends and demonstrates that explicit precision–recall balancing is key to effective multi-hop reasoning.

## Method Summary
PRISM employs three LLM-based agents in a retrieval pipeline: a Question Analyzer decomposes complex queries into sub-questions, a Selector performs high-precision filtering to remove irrelevant candidates, and an Adder performs high-recall recovery to add bridging evidence. The Selector and Adder iterate up to N=3 times, with each cycle refining the evidence set based on what was previously selected. The final compact set is passed to an answer generator. The system uses BM25 for initial retrieval, zero-shot LLM prompting (with structured JSON output enforced), and is tested across four multi-hop QA benchmarks with varying retrieval settings.

## Key Results
- Outperforms IRCoT and SetR on HotpotQA with 90.9% recall and 40.64% EM
- Achieves strong recall on 2WikiMultiHopQA (91.1%) and MuSiQue (83.2%) benchmarks
- Explicit precision-recall decoupling reduces noise while maintaining comprehensive evidence
- Ablation confirms each agent (Question Analyzer, Selector, Adder) is critical for performance

## Why This Works (Mechanism)

### Mechanism 1: Asymmetric Precision-Recall Decoupling
Separating retrieval into distinct precision (Selector) and recall (Adder) phases prevents the classic trade-off where optimizing for one metric hurts the other. The Selector removes distractors while the Adder recovers bridging evidence. This assumes an LLM can distinguish "definitely irrelevant" from "potentially required" when prompted differently. Break condition: If the initial candidate pool lacks necessary evidence, the Adder cannot recover it.

### Mechanism 2: Question Decomposition for Search Space Anchoring
Decomposing complex queries into atomic sub-questions reduces the likelihood of retrieving loosely relevant but semantically off-topic context. The Question Analyzer defines a focused search space for the Selector/Adder, anchoring the reasoning chain to specific entities. This assumes the LLM can correctly identify the dependency structure before retrieving evidence. Break condition: If decomposition hallucinates entities or creates redundant sub-questions, the retrieval agents will query for non-existent facts.

### Mechanism 3: Iterative Evidence Convergence
Iterating the selection-adding cycle allows the system to recover evidence that only becomes "relevant" in the context of other selected evidence. The Selector and Adder cycle repeatedly (up to N times), with evidence that appears irrelevant in isolation potentially being re-added by the Adder in subsequent iterations. This assumes relevant evidence is distributed such that the relevance of one passage is conditional on the presence of another. Break condition: If the iteration limit is reached before the evidence set stabilizes, or if the context window fills with noise, performance degrades.

## Foundational Learning

**Concept: Multi-hop Reasoning Dependency**
Why needed: Multi-hop requires resolving intermediate entities (bridges) to answer the final question. Without understanding this dependency, one cannot configure the "Adder" to look for missing links.
Quick check: In the query "Which painter lived with Van Gogh was married to a ceramist?", what is the intermediate bridge entity?

**Concept: Lost-in-the-Middle Phenomenon**
Why needed: The paper explicitly targets the LLM limitation where relevant evidence is ignored if buried in long context. This justifies the PRISM focus on "compact" evidence sets over raw recall.
Quick check: Why does simply feeding the top-50 retrieved documents into an LLM often yield worse results than the top-10?

**Concept: Zero-Shot Agentic Prompting**
Why needed: PRISM implements agents via prompt engineering rather than fine-tuning. Understanding how to enforce structured output via prompts is critical.
Quick check: How does the system ensure the Selector agent outputs a parseable list of indices rather than a conversational response?

## Architecture Onboarding

**Component map:** Input Question → Question Analyzer → Iterative Loop (Selector → Adder) → Merger → Answer Generator

**Critical path:** The Question Analyzer's output quality determines the search space; if the sub-questions are wrong, the Selector/Adder loop optimizes the wrong targets.

**Design tradeoffs:**
- Latency vs. Quality: Using N=3 iterations improves recall but triples LLM API calls per step
- Strictness: The prompts (Appendix B) are the primary levers for tuning the strictness of the Selector vs. the Adder

**Failure signatures:**
- High Precision, Low Recall: Selector prompts are too aggressive (pruning valid evidence)
- Low Precision, High Recall: Adder prompts are too permissive (recovering distractors)
- Correct Retrieval, Wrong Answer: Failure in the Answer Generator logic (observed in ~37% of HotpotQA errors)

**First 3 experiments:**
1. **Ablation Run:** Disable the Question Analyzer and feed the original query directly to the Selector/Adder loop to measure the delta in Recall on HotpotQA
2. **Iteration Limit Test:** Run PRISM with N=1 vs. N=3 to visualize the convergence rate of the evidence set (specifically looking for "bridging" facts added only in iteration 2+)
3. **Prompt Sensitivity Check:** Modify the Selector prompt to include/exclude the "don't remove bridging facts" instruction to observe the shift in Precision/Recall balance

## Open Questions the Paper Calls Out

**Open Question 1:** How can the computational efficiency of PRISM be improved to facilitate scaling to significantly larger corpora?
Basis: The authors acknowledge that the multi-agent design increases computational cost relative to single-pass retrievers and suggest exploring "more efficient strategies and lightweight agent variants."
Why unresolved: The current implementation relies on multiple LLM calls per query (N=3 iterations), which is costly and potentially slow for massive real-world databases compared to single-pass methods.
What evidence would resolve it: A study showing that a modified, lightweight agent architecture achieves comparable recall/precision on a corpus 10x the size of Wikipedia with reduced latency and cost.

**Open Question 2:** Can adaptive iteration control or uncertainty modeling eliminate the need for fixed iteration depths while preventing redundancy?
Basis: The paper notes the system may occasionally "introduce redundancy when passages are loosely connected" and suggests "adaptive iteration control" or "uncertainty modeling" as future solutions.
Why unresolved: The current framework uses a fixed number of iterations (N=3), potentially wasting compute on simple queries or stopping early on complex ones.
What evidence would resolve it: Implementation of a dynamic stopping criterion based on information gain or uncertainty metrics that outperforms the fixed N=3 baseline in both efficiency and F1 score.

**Open Question 3:** Does the framework require specific fine-tuning to maintain performance in specialized domains such as biomedical or legal texts?
Basis: The discussion states that "broader deployment in specialized domains... may require tailored adaptation" despite the framework being model-agnostic in general domains.
Why unresolved: The experiments were conducted primarily on Wikipedia-based benchmarks and a news dataset; it is unclear if the "Selector" and "Adder" agents can interpret domain-specific jargon without additional supervision.
What evidence would resolve it: Evaluation results on domain-specific multi-hop datasets (e.g., BioASQ or legal case reports) comparing zero-shot performance against domain-adapted versions.

## Limitations
- The computational cost of multiple LLM calls per query limits scalability to large corpora
- Performance depends heavily on the initial retrieval pool containing necessary evidence
- Answer generation quality improvements are partially attributed to better retrieval, with ~37% of errors stemming from the answer generator itself
- Generalization to specialized domains (biomedical, legal) may require domain-specific adaptation

## Confidence
- **High Confidence:** Retrieval performance improvements are well-supported by ablation studies and cross-dataset validation
- **Medium Confidence:** Answer generation quality improvements are partially attributed to better retrieval, but the paper acknowledges that ~37% of errors stem from the answer generation step itself
- **Low Confidence:** Generalization to datasets not seen during development is promising but based on limited samples (500 per dataset)

## Next Checks
1. **Cross-LLM Generalization:** Test PRISM with different LLM backends (e.g., Claude, Llama) to verify that the framework's performance is not dependent on a specific model's reasoning capabilities
2. **Iteration Sensitivity Analysis:** Systematically vary N (number of Selector⇔Adder iterations) and measure the marginal gain in recall/precision to identify the optimal iteration count
3. **Error Analysis on MultiHopRAG:** Conduct a detailed error analysis on the MultiHopRAG dataset to identify whether failures are due to retrieval or answer generation, and whether they differ from the other datasets