---
ver: rpa2
title: Breaking Memorization Barriers in LLM Code Fine-Tuning via Information Bottleneck
  for Improved Generalization
arxiv_id: '2510.16022'
source_url: https://arxiv.org/abs/2510.16022
tags:
- code
- fine-tuning
- pass
- memorization
- ib-ft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a "memorization barrier" in LLM code fine-tuning:
  pretrained models often already memorize fine-tuning data, trapping optimization
  and limiting generalization. To address this, the authors propose IB-regularized
  fine-tuning (IB-FT), which applies an information bottleneck penalty on hidden representations
  to compress spurious memorized features while preserving task-relevant information.'
---

# Breaking Memorization Barriers in LLM Code Fine-Tuning via Information Bottleneck for Improved Generalization

## Quick Facts
- arXiv ID: 2510.16022
- Source URL: https://arxiv.org/abs/2510.16022
- Authors: Changsheng Wang; Xin Chen; Sijia Liu; Ke Ding
- Reference count: 27
- Primary result: IB-regularized fine-tuning (IB-FT) consistently improves top-1 performance (Pass@1) and delivers more stable gains under stricter multi-sample metrics (Pass@k(m)) compared to conventional fine-tuning on OriGen and Evol-CodeAlpaca-V1 benchmarks.

## Executive Summary
This paper identifies a "memorization barrier" in LLM code fine-tuning: pretrained models often already memorize fine-tuning data, trapping optimization and limiting generalization. To address this, the authors propose IB-regularized fine-tuning (IB-FT), which applies an information bottleneck penalty on hidden representations to compress spurious memorized features while preserving task-relevant information. Experiments on OriGen and Evol-CodeAlpaca-V1 show IB-FT consistently improves top-1 performance (Pass@1) and delivers more stable gains under stricter multi-sample metrics (Pass@k(m)) compared to conventional fine-tuning, demonstrating its effectiveness in overcoming memorization barriers and improving generalization.

## Method Summary
The method introduces an information bottleneck penalty during fine-tuning by extracting hidden representations from a designated intermediate layer (layer 20), passing them through a variational encoder to produce compressed representations, and computing KL divergence to a prior for compression loss. This is combined with standard fine-tuning loss and a prediction loss term, weighted by hyperparameters α and β. The approach aims to compress prediction-irrelevant (spurious) features that arise from memorization while preserving task-relevant information.

## Key Results
- IB-FT consistently improves Pass@1 performance on OriGen and Evol-CodeAlpaca-V1 benchmarks compared to conventional fine-tuning
- IB-FT delivers more stable gains under stricter multi-sample metrics (Pass@k(m)) than conventional fine-tuning
- The method reduces representational separation between highly-memorized and less-memorized examples, preventing the model from treating them differently during optimization

## Why This Works (Mechanism)

### Mechanism 1: Information Bottleneck Compression of Spurious Features
Applying an IB penalty on hidden representations compresses prediction-irrelevant (spurious) features that arise from memorization, enabling more uniform learning across data with varying memorization levels. The IB regularizer minimizes I(X;Z) via a variational approximation (KL divergence between qφ(z|hθ(x)) and prior p(z)), which penalizes representations that encode exact input patterns. Simultaneously, the prediction term I(Z;Y) preserves task-relevant signals through log-likelihood optimization, creating a bottleneck that retains only predictive information.

### Mechanism 2: Representation Geometry Equalization Across Memorization Levels
IB-FT reduces representational separation between highly-memorized and less-memorized examples, preventing the model from treating them differently during optimization. Standard FT amplifies the L2 distance and angular disparity between representation clusters of most-memorized vs. least-memorized groups. IB regularization compresses this gap by constraining the representation space, forcing both groups into a more coherent, task-aligned manifold.

### Mechanism 3: Escaping Memorization-Induced Local Optima
Pre-existing memorization traps standard FT in a suboptimal region; IB regularization provides a regularization-induced "escape" analogous to random re-initialization in nonconvex optimization. The base model θ0 already memorizes fine-tuning data. Standard loss minimization from this starting point cannot escape the memorization-dominated basin. IB regularization modifies the optimization landscape by adding compression pressure, effectively reshaping gradients to move toward a more generalizable region.

## Foundational Learning

- **Concept: Information Bottleneck Principle**
  - Why needed here: The core IB-FT method relies on minimizing I(X;Z) while maximizing I(Z;Y). Understanding this tradeoff is essential for tuning α and β hyperparameters and interpreting representation changes.
  - Quick check question: Given a representation Z that perfectly reconstructs input X but provides no information about target Y, what would the IB loss (eq. 6) compute to?

- **Concept: Min-K% Prob Memorization Detection**
  - Why needed here: The paper uses Min-20% Prob to quantify memorization and partition datasets. Understanding this metric is required to replicate the memorization analysis and diagnose which data points are problematic.
  - Quick check question: If a sequence has a Min-20% Prob score of 2.0 versus 8.0, which indicates stronger memorization and why?

- **Concept: Variational Inference with KL Divergence**
  - Why needed here: The compression term (eq. 3-4) uses a variational approximation with DKL(qφ(z|hθ(x)) || p(z)). Understanding this formulation is necessary to implement the encoder qφ and select appropriate priors.
  - Quick check question: What happens to the compression loss if qφ becomes identical to the prior p(z)? What does this imply about the learned representations?

## Architecture Onboarding

- **Component map**: Input x → LLM backbone (θ, frozen except LoRA adapters) → Hidden layer hθ(x) (e.g., layer 20) → Variational encoder qφ(z|hθ(x)) → Compressed representation z → KL divergence to prior p(z) [compression loss] + Log-likelihood pθ(y|z) [prediction loss] + Standard cross-entropy ℓFT(θ) [main FT loss] → Total loss: ℓFT + α·(ℓcompress - β·ℓpredict)

- **Critical path**:
  1. Extract hidden representations hθ(x) from a designated intermediate layer (paper uses layer 20)
  2. Pass hθ(x) through variational encoder qφ (learned MLP) to produce mean/variance for z
  3. Sample z ~ qφ(z|hθ(x)) using reparameterization trick
  4. Compute KL divergence between qφ and prior p(z) for compression loss
  5. Use z to predict y and compute prediction loss
  6. Combine with standard FT loss weighted by α and β

- **Design tradeoffs**:
  - **Layer selection for hθ(x)**: Earlier layers retain more input detail (higher I(X;Z) naturally), later layers are more task-specialized. Paper uses layer 20 but does not ablate this choice.
  - **Prior selection**: Standard isotropic Gaussian N(0, I) is used; alternative priors (e.g., mixture models) could provide different compression characteristics.
  - **Hyperparameter sensitivity**: α ∈ (0.01, 1) and β ∈ (0.001, 0.1) require tuning; paper uses α=0.1, β=0.02 (OriGen) and β=0.01 (Evol-CodeAlpaca). No systematic sensitivity analysis provided.

- **Failure signatures**:
  - **Over-compression**: Pass@1 degrades significantly (below base model) if α is too large or β is too small; representations collapse toward prior.
  - **Insufficient compression**: Pass@k(m) for large m remains similar to standard FT if α is too small or β is too large; memorization barrier persists.
  - **Training instability**: If variational encoder gradients dominate, LoRA adapter updates may become unstable; monitor gradient norms.

- **First 3 experiments**:
  1. **Reproduce Min-K% Prob distribution overlap** (Figure 3): Compute Min-20% Prob scores for fine-tuning data on base model θ0 and compare with TOFU baseline to verify memorization barrier exists in your domain.
  2. **Ablate α and β on a validation split**: Grid search α ∈ {0.01, 0.05, 0.1, 0.5} and β ∈ {0.001, 0.01, 0.05, 0.1} using Pass@1 and Pass@10(10) as metrics; identify the Pareto frontier.
  3. **Layer selection ablation**: Extract hθ(x) from layers {10, 15, 20, 25} and compare representation distance metrics (Figure 5) and final Pass@k(m) performance to validate layer 20 is reasonable.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the "memorization barrier" impede fine-tuning effectiveness in non-code domains, such as general natural language tasks or mathematical reasoning?
- Basis in paper: [explicit] The authors state in Section 4 (footnote 2) that the memorization barrier "likely extends to other domains where pretrained models already memorize downstream data," but the current study restricts validation to code generation.
- Why unresolved: The paper defines and validates the barrier exclusively within the context of code generation datasets (OriGen and Evol-CodeAlpaca-V1) and Verilog benchmarks.
- What evidence would resolve it: Empirical validation of the memorization barrier and the IB-FT remedy on standard NLP or mathematical reasoning benchmarks (e.g., GSM8K or SuperGLUE).

### Open Question 2
- Question: Does IB-regularized fine-tuning (IB-FT) cause catastrophic forgetting of general pretraining knowledge due to its compression of hidden representations?
- Basis in paper: [inferred] The method explicitly compresses "spurious features" (Eq. 4) to improve target domain generalization. However, the paper does not measure if this compression degrades the model's performance on unrelated general tasks.
- Why unresolved: The evaluation is limited to in-domain code benchmarks (Pass@1, Pass@k(m)); no metrics are provided for general knowledge retention or out-of-domain performance.
- What evidence would resolve it: A comparison of general knowledge benchmarks (e.g., MMLU) before and after IB-FT to quantify any loss in broad capabilities.

### Open Question 3
- Question: Is the effectiveness of IB-FT consistent across significantly larger model scales (e.g., 70B+ parameters)?
- Basis in paper: [inferred] The experimental scope is limited to 7B and 8B parameter models (DeepSeek-Coder-7B, Llama-3-8B). It is unclear if the optimization dynamics of the memorization barrier or the IB regularization scale linearly.
- Why unresolved: Larger models may exhibit different memorization capacities or optimization landscapes that could affect the magnitude of the barrier or the required IB penalty strength ($\alpha, \beta$).
- What evidence would resolve it: Replicating the fine-tuning experiments on larger foundation models (e.g., Llama-3-70B) to verify performance stability.

## Limitations
- The method's effectiveness and the memorization barrier phenomenon may not generalize to non-code domains like general NLP or mathematical reasoning
- The paper does not provide extensive hyperparameter sensitivity analysis or experiments on larger model scales (70B+ parameters)
- Claims about the three proposed mechanisms lack direct causal validation, with performance improvements potentially stemming from any combination of effects

## Confidence
- **High Confidence**: The core experimental results (Pass@1 and Pass@k(m) improvements on OriGen and Evol-CodeAlpaca) are well-supported by the data and methodology. The identification of the memorization barrier via Min-K% Prob overlap is robust.
- **Medium Confidence**: The proposed mechanisms (IB compression, representation equalization, local optimum escape) are plausible and supported by qualitative evidence (representation distance metrics, ablation of memorized data), but lack direct causal validation.
- **Low Confidence**: Claims about IB-FT's generalizability to other domains, model scales, or tasks are speculative without supporting experiments.

## Next Checks
1. **Ablation of Mechanisms**: Design an experiment to isolate the effects of representation compression vs. optimization escape. For example, apply IB regularization only to a subset of layers or use a fixed prior (no learned encoder) to test if representation geometry changes drive performance.

2. **Domain Transfer Experiment**: Apply IB-FT to a non-code domain (e.g., general language modeling on C4 or reasoning on GSM8K) and measure both Pass@k(m) improvements and changes in Min-K% Prob distributions to test if the memorization barrier is domain-specific.

3. **Hyperparameter Robustness**: Conduct a systematic grid search over α ∈ {0.01, 0.05, 0.1, 0.5, 1.0} and β ∈ {0.001, 0.01, 0.05, 0.1} on a validation split, plotting Pareto frontiers for Pass@1 vs. Pass@k(m) to identify stable hyperparameter regions and test sensitivity.