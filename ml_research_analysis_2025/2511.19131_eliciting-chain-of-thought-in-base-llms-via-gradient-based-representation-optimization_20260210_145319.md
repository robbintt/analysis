---
ver: rpa2
title: Eliciting Chain-of-Thought in Base LLMs via Gradient-Based Representation Optimization
arxiv_id: '2511.19131'
source_url: https://arxiv.org/abs/2511.19131
tags:
- reasoning
- hidden
- optimization
- arxiv
- states
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a gradient-based method to elicit chain-of-thought
  reasoning from base large language models by optimizing hidden states. The approach
  reformulates reasoning as a Bayesian optimization problem with likelihood and prior
  terms, guiding representations toward reasoning while preserving linguistic coherence.
---

# Eliciting Chain-of-Thought in Base LLMs via Gradient-Based Representation Optimization

## Quick Facts
- arXiv ID: 2511.19131
- Source URL: https://arxiv.org/abs/2511.19131
- Reference count: 26
- Achieves up to 14.56% accuracy improvement over baseline steering methods while maintaining fluency

## Executive Summary
This paper introduces a gradient-based method to elicit chain-of-thought reasoning from base large language models by optimizing hidden states. The approach reformulates reasoning as a Bayesian optimization problem with likelihood and prior terms, guiding representations toward reasoning while preserving linguistic coherence. Evaluated on math, commonsense, and logical reasoning tasks, the method achieves significant accuracy improvements over baseline steering methods while maintaining fluency.

## Method Summary
The method treats hidden state manipulation as a Maximum A Posteriori (MAP) estimation problem, optimizing hidden states to maximize the likelihood of chain-of-thought reasoning while constraining the solution to remain close to the original representation via L2 regularization. A classifier trained on contrastive CoT/non-CoT hidden states guides the optimization toward reasoning patterns. The approach uses adaptive step sizes and noise injection during gradient optimization, selecting optimal layers and representation types based on classifier performance. The method operates at inference time without modifying the base model weights.

## Key Results
- Achieves up to 14.56% accuracy improvement over baseline steering methods
- Maintains fluency while improving reasoning performance
- Outperforms linear activation steering across diverse models and benchmarks
- Shows effectiveness on math, commonsense, and logical reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
The Bayesian MAP formulation with likelihood and prior terms enables reasoning elicitation while preserving language quality. The method treats hidden states as latent variables in conditional generation, optimizing them to maximize both the probability of CoT reasoning (likelihood) and proximity to original states (prior). The Gaussian prior assumption enables tractable L2 regularization that constrains optimization within semantically meaningful regions.

### Mechanism 2
Per-token gradient optimization with adaptive step size and noise injection navigates representation space more effectively than fixed-strength steering. The adaptive step size decays as the optimization approaches the target threshold, while Gaussian noise injection helps escape local optima. This instance-specific steering adapts to the particular reasoning needs of each token rather than applying uniform transformations.

### Mechanism 3
Layer and representation-type selection based on classifier performance concentrates optimization where reasoning patterns are most effectively encoded. The method trains separate classifiers for each layer and representation type, then selects the top 50% layers by F1 score. This empirical finding shows middle layers demonstrate superior classification performance compared to both shallow and deep layers, suggesting these layers encode higher-level reasoning patterns.

## Foundational Learning

- **Bayesian inference and MAP estimation**: The entire method is framed as posterior maximization. Understanding how priors regularize likelihood is essential for interpreting the λ trade-off.
  - Quick check: Can you explain why adding a Gaussian prior on hidden states is mathematically equivalent to L2 regularization?

- **Transformer hidden state semantics across layers**: The method manipulates specific representations at specific layers. Understanding what different layers encode helps predict optimization effectiveness.
  - Quick check: What is the functional difference between attention output, MLP output, and integrated layer output in a transformer?

- **Gradient-based optimization with adaptive step sizes**: The adaptive step size and noise injection are non-standard modifications. Understanding why these help in high-dimensional latent spaces is critical for debugging convergence.
  - Quick check: Why would adding Gaussian noise to gradient updates help escape local optima in a Bayesian optimization framework?

## Architecture Onboarding

- **Component map**: Classifier fθ (2-layer MLP with ReLU + sigmoid) -> Optimization loop (per-token gradient updates with adaptive step size) -> Layer/representation selector (top 50% by F1 score) -> Hyperparameters (λ, τ, α0, T)

- **Critical path**: 
  1. Construct paired CoT/non-CoT dataset using Top-K-Start sampling + GPT-4o labeling (2000 pairs from GSM8K)
  2. Train classifiers for each layer and representation type; evaluate F1 scores
  3. During inference, extract hidden states at target layers for each generated token
  4. If classifier score < 0.5, run gradient optimization until score > τ or max iterations reached
  5. Update hidden states in-place and continue forward pass

- **Design tradeoffs**: 
  - Higher τ (0.9→0.99) improves accuracy with diminishing returns; fluency remains stable
  - More layers (10%→50%) improves accuracy; beyond 50%, gains are marginal but compute increases
  - Lower λ (0→0.01) allows more exploration; optimal range is 0-0.01; above 0.01, quality degrades

- **Failure signatures**: 
  - Low classifier F1 (<70%) at target layers: Indicates representation/layer mismatch
  - Rapid perplexity increase during optimization: λ too small, increase λ
  - No accuracy improvement despite high classifier score: Classifier trained on distribution-shifted data
  - Degraded text fluency: Optimization drifting off manifold, increase λ or reduce max iterations

- **First 3 experiments**:
  1. Validate classifier quality: Train classifier on GSM8K CoT/non-CoT pairs, measure F1 across all layers and representation types for target model. Target: identify layers with F1 > 80%.
  2. Single-layer ablation: Pick best layer from experiment 1, run full optimization pipeline on GSM8K test set with default hyperparameters. Measure accuracy, perplexity, and fluency vs. vanilla greedy decoding.
  3. Hyperparameter sweep: On held-out validation set (200 examples), sweep λ ∈ [0.001, 0.01, 0.1] and τ ∈ [0.7, 0.8, 0.9, 0.95]. Plot accuracy vs. perplexity to identify operating region.

## Open Questions the Paper Calls Out

### Open Question 1
How can gradient-based representation optimization be effectively combined with post-training techniques (e.g., RL or instruction tuning) to further amplify reasoning capabilities?
- Basis: The Discussion section states, "Looking ahead, we envision integrating our method with post-training techniques to further amplify the reasoning capabilities of LLMs."
- Why unresolved: Current work focuses solely on inference-time manipulation of base models without updating model weights.

### Open Question 2
Does reliance on a classifier trained on specific domains (e.g., GSM8K) limit the method's ability to generalize to reasoning tasks requiring distinct semantic structures?
- Basis: Improvements on commonsense/logic tasks were "more modest" (avg 1.8%) compared to math, suggesting these tasks "may demand more domain-specific knowledge."
- Why unresolved: Unclear if lower performance is due to model's intrinsic limitations or domain gap between classifier's training data and target tasks.

### Open Question 3
Can the computational overhead of the iterative optimization process be reduced to allow for real-time application without compromising reasoning elicitation?
- Basis: Table 4 in Appendix shows method takes ~0.33s for optimization alone, compared to ~0.04s for greedy decoding, significantly increasing inference latency.
- Why unresolved: While paper establishes efficacy, it does not explore acceleration techniques to mitigate the 10x slowdown.

## Limitations

- Bayesian MAP formulation relies on strong assumptions about hidden state distributions that may not hold universally across architectures
- Classifier reliability represents a critical bottleneck dependent on training data quality and domain generalization
- Architecture-specific optimal layers create reproducibility challenges requiring per-model calibration

## Confidence

- **High Confidence**: The core mathematical framework (MAP estimation, gradient-based optimization, adaptive step size) is sound and reproducible
- **Medium Confidence**: Empirical results showing accuracy improvements are credible but may not generalize beyond tested models and benchmarks
- **Low Confidence**: Claims about universal applicability across diverse reasoning tasks and model architectures

## Next Checks

1. **Cross-task classifier generalization**: Train classifiers on GSM8K data, then evaluate their F1 scores on MultiArith, CommonsenseQA, and StrategyQA hidden states from the same model. Measure correlation between classification accuracy and actual reasoning performance across domains.

2. **Layer-level reasoning attribution**: For target model, run full optimization pipeline on GSM8K using only single layer with highest classifier F1. Compare accuracy and fluency gains against using top 50% layers to validate layer selection effectiveness.

3. **Prior sensitivity analysis**: Systematically sweep λ from 0.001 to 0.1 on held-out validation set, measuring accuracy, perplexity, and fluency at each step. Plot metrics to identify operating region where accuracy gains saturate without fluency degradation.