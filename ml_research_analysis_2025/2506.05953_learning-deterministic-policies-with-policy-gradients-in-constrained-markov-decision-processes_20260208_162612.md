---
ver: rpa2
title: Learning Deterministic Policies with Policy Gradients in Constrained Markov
  Decision Processes
arxiv_id: '2506.05953'
source_url: https://arxiv.org/abs/2506.05953
tags:
- policy
- learning
- deterministic
- page
- policies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces C-PG, a policy-based primal-dual algorithm
  for solving constrained Markov decision processes (CMDPs). C-PG operates in both
  action-based and parameter-based exploration paradigms and provides global last-iterate
  convergence guarantees under gradient domination assumptions.
---

# Learning Deterministic Policies with Policy Gradients in Constrained Markov Decision Processes

## Quick Facts
- arXiv ID: 2506.05953
- Source URL: https://arxiv.org/abs/2506.05953
- Authors: Alessandro Montenegro; Leonardo Cesani; Marco Mussi; Matteo Papini; Alberto Maria Metelli
- Reference count: 3
- Primary result: Introduces C-PG, a primal-dual policy gradient algorithm that learns deterministic policies by training stochastic ones and switching off noise at deployment, with global convergence guarantees under gradient domination.

## Executive Summary
This paper addresses the challenge of learning deterministic policies in constrained Markov decision processes (CMDPs) by introducing C-PG, a primal-dual policy gradient algorithm. C-PG operates in both action-based and parameter-based exploration paradigms and provides global last-iterate convergence guarantees under gradient domination assumptions. The key insight is that stochastic policies trained during optimization can converge to optimal deterministic policies when the exploration noise is removed at deployment. This approach overcomes the limitation that most policy gradient methods only produce stochastic policies, which are impractical for many real-world applications requiring consistent, deterministic behavior.

## Method Summary
C-PG is a primal-dual algorithm that optimizes a regularized Lagrangian of the CMDP, balancing reward maximization against constraint satisfaction through Lagrange multipliers. The method works with both action-based (AB) and parameter-based (PB) exploration paradigms by reformulating both as optimizing a generic parameter vector. During training, the algorithm learns stochastic policies (either by adding Gaussian noise to actions or sampling from a noisy hyperpolicy), but deploys deterministic policies by setting the noise variance to zero. The regularized Lagrangian includes a ridge penalty on dual variables to ensure strong concavity and bounded dual updates. The algorithm alternates between gradient descent on the primal variable (policy parameters) and gradient ascent on the dual variable (Lagrange multipliers), with convergence guarantees under gradient domination conditions.

## Key Results
- C-PG achieves global last-iterate convergence to optimal deterministic policies under gradient domination assumptions
- The algorithm outperforms state-of-the-art baselines on constrained control tasks, particularly when deploying deterministic policies after training
- C-PG handles multiple constraints dimension-free and works in both AB and PB exploration paradigms
- The method maintains effectiveness when switching from stochastic training to deterministic deployment by setting noise to zero

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A deterministic policy can be optimally learned by training a stochastic policy and subsequently switching off the exploration noise.
- **Mechanism:** The algorithm models stochastic exploration as a "white noise" perturbation (Gaussian) applied either to the actions or the parameters of an underlying deterministic policy $\mu_\theta$. By minimizing the Lagrangian of the perturbed (stochastic) problem, the parameters of the underlying deterministic policy are optimized. When the noise variance $\sigma^2$ is set to zero at deployment, the policy collapses to this optimized deterministic form.
- **Core assumption:** The stochastic (hyper)policy is defined as a perturbation of a deterministic policy (Definitions 4.2 and 4.3) and the objective functions satisfy Lipschitz and smoothness regularity conditions.
- **Evidence anchors:**
  - [abstract] "...deploying deterministic policies by switching off the stochasticity at the end..."
  - [Section 4.4] Theorem 4.6 proves convergence to the optimal deterministic policy when $\sigma=0$.
  - [corpus] Corpus evidence is weak for this specific mechanism; related works discuss stochastic vs. deterministic gradients generally but do not explicitly describe the "switching off noise" convergence in CMDPs.
- **Break condition:** If the noise is not zero-mean or not independent of the policy parameters, the underlying deterministic policy is not the mean of the stochastic distribution, breaking the convergence guarantees.

### Mechanism 2
- **Claim:** Global convergence to optimal feasible policies is achieved in constrained settings using a primal-dual method with ridge regularization.
- **Mechanism:** The method optimizes a regularized Lagrangian function $\mathcal{L}_\omega$ which includes a ridge penalty on the dual variables (Lagrange multipliers). This regularization ensures the Lagrangian is strongly concave with respect to the dual variables, stabilizing the alternating gradient descent-ascent updates and guaranteeing the dual variables remain bounded.
- **Core assumption:** The environment satisfies the weak $\psi$-gradient domination condition (Assumption 3.2), which holds for tabular softmax or natural policy parameterizations.
- **Evidence anchors:**
  - [Section 3.1] Defines the regularized Lagrangian with the $-\omega/2 \|\lambda\|^2$ term.
  - [Section 3.3] Theorem 3.2 provides the convergence rates dependent on the regularization $\omega$.
  - [corpus] Consistent with "Mirror Descent Policy Optimisation" which also seeks robust convergence in constrained MDPs.
- **Break condition:** If the regularization parameter $\omega$ is set to zero, the dual variables may diverge, and convergence guarantees for the last iterate are lost.

### Mechanism 3
- **Claim:** The algorithmic framework unifies Action-Based (AB) and Parameter-Based (PB) exploration under a single convergence analysis.
- **Mechanism:** Both exploration paradigms are reframed as optimizing a generic parameter vector $\vartheta$. For AB, $\vartheta$ corresponds to policy weights $\theta$; for PB, $\vartheta$ corresponds to hyperpolicy weights $\rho$. The core update rules (gradient descent on primal, ascent on dual) remain structurally identical, differing only in the specific gradient estimator (GPOMDP-like vs PGPE-like).
- **Core assumption:** The gradient estimators for both paradigms have bounded variance (Assumption 3.4), which is satisfied under the white noise model.
- **Evidence anchors:**
  - [Section 3.4] Explicitly defines C-PGAE (Action-based) and C-PGPE (Parameter-based) variants.
  - [Section 2] Defines the exploration-agnostic Constrained Optimization Problem (1).
  - [corpus] "Equivalence of stochastic and deterministic policy gradients" supports the theoretical grounding for unifying these paradigms.
- **Break condition:** If the noise scale $\sigma$ is reduced too quickly during training (annealing), the gradient estimates may become high-variance or biased relative to the $\sigma$ used in the theoretical derivation.

## Foundational Learning

- **Concept: Primal-Dual Optimization (Lagrangian Methods)**
  - **Why needed here:** The algorithm balances maximizing reward (primal) against satisfying constraints (dual). You cannot understand the update rules without understanding how Lagrange multipliers weight constraint violations.
  - **Quick check question:** If a constraint is violated, should the corresponding Lagrange multiplier increase or decrease?

- **Concept: Gradient Domination (Polyak-Åojasiewicz Condition)**
  - **Why needed here:** Standard non-convex optimization cannot guarantee global convergence. This property ensures that "gradient zero" corresponds to a global optimum, allowing the paper to claim convergence to the optimal policy.
  - **Quick check question:** Does gradient domination hold for all neural network architectures? (Hint: No, generally only specific tabular or linear approximations).

- **Concept: Action-Based vs. Parameter-Based Exploration**
  - **Why needed here:** Implementing C-PG requires choosing between perturbing the action at every step (AB) or perturbing the weights once per trajectory (PB). The choice affects variance and the "white noise" assumptions.
  - **Quick check question:** In Parameter-Based exploration, when is the noise sampled relative to the trajectory?

## Architecture Onboarding

- **Component map:** Primal Variable ($\vartheta$) -> Regularized Lagrangian -> Gradient Estimators -> Update Rules -> Dual Variable ($\lambda$) -> Projector -> Bounded Set $\Lambda$

- **Critical path:**
  1. Initialize policy $\vartheta$ and multipliers $\lambda$.
  2. **Sample:** Draw trajectories using the noisy policy (AB) or sampling weights from a noisy hyperpolicy (PB).
  3. **Estimate:** Compute unbiased estimates of $\nabla_\vartheta \mathcal{L}$ and $\nabla_\lambda \mathcal{L}$.
  4. **Update Primal:** $\vartheta_{k+1} \leftarrow \vartheta_k - \zeta_\vartheta \nabla_\vartheta \mathcal{L}$.
  5. **Update Dual:** $\lambda_{k+1} \leftarrow \Pi_\Lambda (\lambda_k + \zeta_\lambda \nabla_\lambda \mathcal{L})$.
  6. **Deployment:** Set noise $\sigma \to 0$ to recover the deterministic policy.

- **Design tradeoffs:**
  - **$\omega$ (Regularization):** High $\omega$ stabilizes dual updates (prevents divergence) but introduces bias in the constraint satisfaction (Theorem 3.1). Must scale $\omega \approx \mathcal{O}(\epsilon)$.
  - **$\sigma$ (Noise):** High $\sigma$ aids exploration but increases the gap between the stochastic training policy and the deployed deterministic policy (Theorem 4.6).
  - **Exploration Mode:** PB is generally lower variance but sensitive to high-dimensional parameter spaces; AB is robust to parameter dimension but sensitive to horizon length $T$.

- **Failure signatures:**
  - **Constraint Oscillation:** Lagrange multipliers growing unbounded or oscillating violently indicates $\omega$ is too low.
  - **Deployment Gap:** The deterministic policy performs significantly worse than the stochastic one, indicating $\sigma$ was too high during training relative to the smoothness of the objective.
  - **Slow Convergence:** If the learning rates $\zeta$ are not tuned in a two-timescale manner (often $\zeta_\vartheta \approx \omega \zeta_\lambda$), convergence may stall.

- **First 3 experiments:**
  1. **Baseline Implementation (LQR):** Implement C-PGAE on a simple Linear Quadratic Regulator with a cost constraint to verify the deterministic deployment matches the stochastic performance at the end of training.
  2. **Regularization Ablation:** Run C-PGPE on a GridWorld with varying $\omega \in \{0, 10^{-4}, 10^{-2}\}$ to visualize the tradeoff between convergence stability and constraint violation bias.
  3. **Paradigm Comparison:** Compare C-PGAE vs. C-PGPE on a high-dimensional control task (e.g., MuJoCo Swimmer) to identify which noise injection strategy yields lower variance gradients.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the sample complexity of C-PG be improved to match the optimal theoretical lower bounds for constrained MDPs?
- Basis in paper: [explicit] The conclusion states that future research should aim to "improve sample complexity of C-PG, with the goal of matching the lower bounds established by Vaswani et al. (2022)."
- Why unresolved: The current iteration complexity for the estimated gradient case is $\mathcal{O}(\omega^{-3}\sigma^{-2}\epsilon^{-4/\psi+1})$, which is polynomial but distant from the $\Omega(\epsilon^{-2})$ lower bound cited in the related work.
- What evidence would resolve it: A theoretical derivation showing C-PG or a variant achieving a sample complexity of $\mathcal{O}(\epsilon^{-2})$, matching the lower bound.

### Open Question 2
- Question: Can the convergence guarantees of C-PG be retained using a single time-scale algorithm?
- Basis in paper: [explicit] The conclusion identifies "the development of single time-scale algorithms that retain the same convergence guarantees" as a promising direction for future research.
- Why unresolved: The current theoretical guarantees rely on a two time-scale update rule (Section 3.3), where learning rates for primal and dual variables are balanced via the regularization parameter $\omega$.
- What evidence would resolve it: A modified C-PG algorithm and proof demonstrating global last-iterate convergence without requiring two time-scales.

### Open Question 3
- Question: Does the convergence to deterministic policies hold when using dynamic stochasticity schedules rather than fixed noise?
- Basis in paper: [explicit] The conclusion notes the limitation that the analysis assumes "a fixed level of stochasticity $\sigma$" and suggests extending work on dynamic stochasticity to the constrained case.
- Why unresolved: While practical implementations often anneal noise (reduce $\sigma$ over time), the theoretical guarantees for deterministic deployment (Theorem 4.6) assume $\sigma$ is fixed relative to $\epsilon$.
- What evidence would resolve it: Convergence proofs for C-PG under a schedule where $\sigma$ is adapted or learned during training.

## Limitations

- The theoretical convergence guarantees rely on gradient domination assumptions that are not generally satisfied for neural network parameterizations
- The analysis assumes a fixed noise level $\sigma$ during training, while practical implementations typically use annealing schedules
- The regularization parameter $\omega$ introduces a fundamental tradeoff between dual stability and constraint satisfaction accuracy that requires careful tuning

## Confidence

- **Global convergence under gradient domination:** High for tabular softmax policies, Medium for neural networks
- **Deterministic deployment mechanism:** High when noise is additive Gaussian and properly centered, Low if noise model deviates significantly
- **Regularization effectiveness:** High for dual stability, Medium for constraint satisfaction accuracy

## Next Checks

1. **Gradient Domination Verification:** Test C-PG on a small neural network CMDP and empirically measure whether the gradient domination condition holds throughout training. If violated, characterize the convergence behavior.

2. **Noise Annealing Sensitivity:** Implement C-PG with various noise schedules (constant, linear decay, exponential decay) and measure the deployment gap between stochastic training and deterministic evaluation. Identify which schedule minimizes this gap while maintaining exploration.

3. **Regularization Tradeoff Analysis:** Systematically vary $\omega$ across multiple orders of magnitude on a benchmark CMDP and plot constraint violation versus dual variable stability. Quantify the bias introduced by regularization and identify optimal scaling relative to the target accuracy $\epsilon$.