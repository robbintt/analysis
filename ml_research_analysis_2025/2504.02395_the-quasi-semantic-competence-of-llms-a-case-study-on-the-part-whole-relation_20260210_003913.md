---
ver: rpa2
title: 'The quasi-semantic competence of LLMs: a case study on the part-whole relation'
arxiv_id: '2504.02395'
source_url: https://arxiv.org/abs/2504.02395
tags:
- relation
- llms
- part
- part-whole
- parts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the extent of semantic competence of large
  language models (LLMs) regarding the part-whole relation (meronymy). It uses behavioral
  testing, probabilistic analysis, and concept representation analysis to evaluate
  LLMs' understanding of this relation and its antisymmetric property.
---

# The quasi-semantic competence of LLMs: a case study on the part-whole relation

## Quick Facts
- **arXiv ID:** 2504.02395
- **Source URL:** https://arxiv.org/abs/2504.02395
- **Reference count:** 27
- **Key outcome:** LLMs can generate parts of objects with high accuracy but struggle to consistently understand and apply the antisymmetric nature of meronymy.

## Executive Summary
This study investigates the semantic competence of large language models regarding the part-whole relation (meronymy). Through behavioral testing, probabilistic analysis, and concept representation analysis, the research reveals that while models can accurately generate parts for given objects, they fail to correctly understand the antisymmetric property of meronymy. The findings indicate that LLMs possess only "quasi-semantic" competence, falling short of capturing the deep inferential properties of this fundamental semantic relation.

## Method Summary
The study employs three complementary approaches to evaluate LLM competence in meronymy: behavioral prompting tasks where models answer questions and verify statements about part-whole relations; probabilistic analysis using log-likelihood scoring to measure model plausibility judgments; and representational analysis examining whether the part-whole concept is encoded as a linear direction in embedding spaces. The research uses McRae feature norms and ConceptNet to create meronymic pairs, testing multiple models including LLaMA2 and GPT-4 across zero-shot and few-shot settings.

## Key Results
- Behavioral tasks show models achieve ~92% accuracy in generating parts but only 65-85% accuracy in verifying their own generated outputs as meronyms
- Probabilistic analysis reveals models fail to consistently assign higher probabilities to correct meronymic statements versus their swapped counterfactuals (~25% error rate)
- Representational analysis demonstrates part-whole concepts are only partially encoded in embedding spaces, with stronger linear structures observed for class-specific relations rather than as a general abstract relation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs rely on statistical co-occurrence patterns for generation tasks while lacking a unified logical schema for the antisymmetric property of meronymy.
- **Mechanism:** The model predicts tokens based on associative strength (e.g., "wheel" frequently co-occurs with "car"). When asked to verify swapped counterfactuals (e.g., "The car is a part of the wheel"), the model fails to suppress the high prior association between the entities to enforce the logical directional constraint (antisymmetry), leading to inconsistent "quasi-semantic" judgments.
- **Core assumption:** High generation accuracy implies strong associative bonds, whereas failure on logical verification implies the absence of an abstract relational operator.
- **Evidence anchors:**
  - [abstract] "while LLMs can generate parts... they struggle to correctly understand the meronymic relation, with accuracy rates dropping significantly when tested on their own generated outputs."
  - [section] Section 9 (General Discussion) references the "Generative AI Paradox," suggesting models generate better than they understand.
  - [corpus] Weak direct support in provided corpus; "Relational Schemata in BERT" (neighbor) suggests relational schemata may be "inducible, not emergent," supporting the fragility of these logical properties.
- **Break condition:** If models were to consistently verify swapped counterfactuals as false (rejecting "car is part of wheel") while accepting true meronymys, the claim of a missing logical schema would break.

### Mechanism 2
- **Claim:** The "part-whole" relation is not encoded as a single, global linear direction in the embedding space, but rather as fragmented, class-specific subspaces.
- **Mechanism:** According to the Linear Representation Hypothesis (LRH), concepts are often vectors. The study finds that a global "part-of" vector has low alignment with random pairs. However, class-specific vectors (e.g., "parts of vehicles") show strong alignment. The mechanism is localized geometric clustering rather than global abstraction.
- **Core assumption:** If a concept is linearly represented, vector offsets between word pairs should consistently align with a specific direction in the unembedding/embedding space.
- **Evidence anchors:**
  - [abstract] "Representational analysis further demonstrated that the part-whole concept is only partially encoded in embedding spaces, with stronger linear structures observed for class-specific relations..."
  - [section] Section 8.1 results: "The number of target pairs whose dot products have higher values... is much lower... less than 50%."
  - [corpus] "On Relation-Specific Neurons in LLMs" (neighbor) aligns with the idea that relational knowledge may be localized to specific neurons or subspaces rather than distributed globally.
- **Break condition:** If a single "part-of" direction vector could classify meronymic pairs across all semantic domains (birds, vehicles, buildings) with high accuracy, the "fragmented subspace" claim would be invalid.

### Mechanism 3
- **Claim:** Probabilistic scoring reveals latent knowledge that behavioral prompting misses, but still fails to capture full antisymmetry consistency.
- **Mechanism:** Prompting (behavioral) is noisy and sensitive to instruction formatting. Log-probability scoring (probabilistic) measures the model's raw surprise at a sentence. The mechanism relies on the model assigning higher likelihood to "The wheel is part of the car" than "The car is part of the wheel." While better than prompting, the 25% error rate suggests the "surprise" mechanism is not strictly enforcing logic.
- **Core assumption:** Log-probability scores serve as a more faithful proxy for internal semantic plausibility than generated text outputs.
- **Evidence anchors:**
  - [abstract] "Probabilistic analysis showed similar limitations, with models failing to consistently assign higher probabilities to correct meronymic statements versus their swapped counterfactuals."
  - [section] Section 7 results: "almost one quarter of semantically plausible meronymic statements are not regarded as being more likely than their implausible swapped versions."
  - [corpus] No direct corpus evidence for this specific probability mechanism; related works in corpus focus more on extraction frameworks (LLM-OREF) than plausibility scoring.
- **Break condition:** If models scored random fake pairs identically to real pairs, or if the accuracy difference between behavioral and probabilistic methods vanished entirely, this specific distinction in mechanism utility would dissolve.

## Foundational Learning

- **Concept: Meronymy and Antisymmetry**
  - **Why needed here:** This is the target semantic relation. Understanding that "A is part of B" implies "B is NOT part of A" (antisymmetry) is central to the paper's diagnosis of LLM failure.
  - **Quick check question:** If "finger" is a meronym of "hand," does the model understand that "hand is a part of finger" must be false, or does it just associate the words "hand" and "finger"?

- **Concept: Linear Representation Hypothesis (LRH)**
  - **Why needed here:** The representational analysis relies on this hypothesis to probe the geometry of the model's embedding space. You cannot interpret the "vector offset" results without understanding this concept.
  - **Quick check question:** Can a specific direction in a high-dimensional vector space represent an abstract concept like "gender" or "part-of" such that adding this vector to "King" results in "Queen"?

- **Concept: The Generative AI Paradox**
  - **Why needed here:** It frames the core result: models can generate correct answers (high capability) without understanding the underlying logic (low competence).
  - **Quick check question:** Why might a model be able to list 14 parts of a car accurately but fail to correctly judge the truth value of a statement about one of those parts?

## Architecture Onboarding

- **Component map:**
  - Embedding Layer (W_E) -> Transformer Blocks -> Residual Stream -> Unembedding Layer (W_U)

- **Critical path:**
  1. Input: Meronym/Holonym pairs (e.g., <wheel, car>) are tokenized.
  2. Extraction: Static vectors are pulled from W_E (input) and W_U (output).
  3. Operation: Vector differences are calculated (γ(wheel) - γ(car)).
  4. Probe: The alignment (dot product) of these differences with a "concept direction" vector is measured to test for linear encoding.

- **Design tradeoffs:**
  - Prompting vs. Probability: Prompting is easy but unstable ("shaky explanatory power"); Probability scoring is more robust ("faithful proxy") but requires access to log-probabilities (closed models like GPT-4 often restrict this or require specific APIs).
  - Static vs. Contextual Embeddings: The paper focuses on static embeddings (unembedding matrix) for the representational analysis. Assumption: This captures the "core" meaning, but ignores contextual nuance that might actually solve the logical problem.

- **Failure signatures:**
  - High Generation / Low Verification: Generating valid parts (92%+) but failing to verify antisymmetry on those exact parts (implies "associative" rather than "logical" processing).
  - Orthogonal Concept Vectors: If "part-of" vectors for vehicles are orthogonal to "part-of" vectors for animals, the model lacks a generalized "part-of" concept.

- **First 3 experiments:**
  1. MKC Compliance Check: Run the "Meronymy Knowledge Criterion" test (Task 1) on your target model. Feed it a true pair ("X is part of Y") and then the swapped pair. If it accepts the swapped pair, the model fails antisymmetry.
  2. Vector Offset Alignment: Extract embeddings for 50 meronymic pairs from a specific class (e.g., animals). Calculate the average direction vector. Test if this vector aligns with individual pairs. Then test if it aligns with pairs from a different class (e.g., vehicles) to check for generalization.
  3. Log-Prob Contrast: Construct minimal sentence pairs ("The wheel is part of the car" vs "The car is part of the wheel"). Compare log-probabilities. If P(swap) > P(real) in >25% of cases, the model's probabilistic semantics are flawed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs correctly infer the transitivity of the part-whole relation across mixed meronymic subtypes?
- Basis: [explicit] The conclusion states future work is reserved to explore the transitivity property and further specifications of the relation.
- Why unresolved: The current study focused solely on antisymmetry; transitivity is theoretically complex and may not hold between different subtypes (e.g., Component/Integral vs. Member/Collection).
- What evidence would resolve it: Behavioral and probabilistic testing of LLMs on transitive inference chains involving varying meronymic subtypes.

### Open Question 2
- Question: Can an automated methodology identify sub-clusters of concepts that exhibit linear structures in embedding spaces more effectively than manual selection?
- Basis: [explicit] The discussion section identifies the development of such a methodology as a "promising avenue" left for future research.
- Why unresolved: Current analysis relied on manual class selection (e.g., "vehicles"), suggesting the need for a scalable, data-driven approach to find coherent subspaces.
- What evidence would resolve it: A proposed unsupervised clustering algorithm that consistently identifies groups of meronymic pairs with high linearity alignment.

### Open Question 3
- Question: Does the "quasi-semantic" competence observed in text-only LLMs improve with multimodal grounding?
- Basis: [inferred] The authors suggest the gap in meronymy understanding may stem from the relation being grounded in embodied experience rather than just textual distribution.
- Why unresolved: It is undetermined if the lack of deep inferential understanding is a fundamental limitation of distributional semantics or a lack of sensory grounding.
- What evidence would resolve it: Replicating these experiments on vision-language models (VLMs) to see if visual grounding improves antisymmetry performance.

## Limitations

- Whitening transformation sensitivity: The representational analysis depends on a specific whitening transformation that is not fully specified in the paper, potentially affecting geometric interpretations
- Tokenization variability: Multi-word expression handling and sub-word tokenization details are unclear, which could influence embedding calculations and vector analysis
- Closed-source model limitations: Inability to access GPT-4's internal log-probabilities means probabilistic analysis relies on open-source proxies, reducing comparability

## Confidence

**High Confidence:** Behavioral testing results showing generation accuracy (92%+) versus verification accuracy (65-85%) are robust across multiple models and prompt formats. The MKC failure pattern is consistent and clearly demonstrates the quasi-semantic nature of LLM meronymy knowledge.

**Medium Confidence:** Probabilistic analysis showing ~25% failure rate in distinguishing plausible from implausible meronymic statements is compelling but relies on proxy measurements for GPT-4. The directional gap between generation and verification is clear, though exact percentages may vary with implementation.

**Low Confidence:** Representational analysis claims about fragmented versus global encoding are methodologically sound but face challenges in interpretation. Whitening transformation sensitivity and unclear token aggregation handling create uncertainty about the strength of conclusions regarding class-specific versus generalized part-whole encoding.

## Next Checks

1. Reproduce whitening transformation implementation by comparing results using different covariance matrix calculations and whitening approaches to assess sensitivity.

2. Test additional model families beyond LLaMA2 and GPT-4, including models with different architectural approaches (sparse models, models with explicit relational modules) to determine if the quasi-semantic pattern is universal.

3. Conduct ablation studies on token aggregation methods and multi-word expression handling to isolate their impact on the representational analysis results and validate the class-specific encoding claims.