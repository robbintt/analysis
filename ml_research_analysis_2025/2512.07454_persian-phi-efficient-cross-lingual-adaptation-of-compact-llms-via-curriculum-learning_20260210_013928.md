---
ver: rpa2
title: 'Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum
  Learning'
arxiv_id: '2512.07454'
source_url: https://arxiv.org/abs/2512.07454
tags:
- persian
- language
- arxiv
- training
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Persian-Phi, a 3.8B parameter language model
  adapted from Microsoft's Phi-3 Mini to support the Persian language. The authors
  address the challenge of extending state-of-the-art monolingual models to underrepresented
  languages through a novel curriculum learning pipeline.
---

# Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning

## Quick Facts
- **arXiv ID:** 2512.07454
- **Source URL:** https://arxiv.org/abs/2512.07454
- **Reference count:** 40
- **Primary result:** 3.8B-parameter model adapted from Phi-3 Mini achieves ~80% of 8B-parameter Dorna-2's performance on Persian benchmarks through curriculum learning

## Executive Summary
This paper presents Persian-Phi, a 3.8B parameter language model adapted from Microsoft's Phi-3 Mini to support the Persian language. The authors introduce a novel curriculum learning pipeline that addresses the challenge of extending monolingual models to underrepresented languages. Their approach combines tokenizer enhancement with Persian-specific tokens, a warm-up phase using bilingual narratives for embedding alignment, and continual pretraining on filtered Persian corpora with parameter-efficient fine-tuning. Despite being a monolingual English model, Persian-Phi achieves competitive performance on the Open Persian LLM Leaderboard, demonstrating that compact models can effectively support low-resource languages through strategic adaptation.

## Method Summary
The adaptation process follows a structured curriculum learning strategy. First, a BPE tokenizer is trained on Persian Wikipedia to add ~4,921 unique tokens to the base model. Next, a warm-up phase aligns embeddings by training on Tiny Stories translated to Persian using LoRA (rank 4) with full fine-tuning of embeddings and head. The main adaptation uses continual pretraining on filtered Persian corpora (TLPC and Wikipedia) with LoRA (rank 64) applied to attention/FFN layers while keeping embeddings and head fully tunable. Finally, supervised fine-tuning (LoRA rank 32) is performed on mixed instruction datasets. The entire process requires minimal hardware (2x RTX 3090) and completes in 12 days.

## Key Results
- Achieved ~80% of the performance of 8B-parameter state-of-the-art Dorna-2 on Open Persian LLM Leaderboard
- Warm-up phase achieved perplexity of 2.45 on bilingual narrative task
- Successfully maintained English capabilities while adding Persian language support
- Demonstrated effectiveness of curriculum learning approach with minimal hardware requirements

## Why This Works (Mechanism)
The curriculum learning approach prevents catastrophic forgetting by gradually introducing Persian language patterns. The warm-up phase with bilingual narratives creates semantic alignment between English and Persian embeddings before heavy pretraining. Parameter-efficient fine-tuning (LoRA) allows adaptation without modifying the entire model, preserving learned English capabilities. The filtered corpus ensures high-quality training data while the tokenizer enhancement provides proper token coverage for Persian morphology.

## Foundational Learning
- **BPE Tokenization** - Why needed: Handles Persian morphology and creates vocabulary overlap. Quick check: Verify tokenizer adds ~4,921 unique Persian tokens.
- **LoRA Fine-tuning** - Why needed: Enables efficient adaptation without full model retraining. Quick check: Confirm rank 64 for CPT, rank 32 for SFT.
- **Embedding Alignment** - Why needed: Prevents semantic drift between languages during adaptation. Quick check: Monitor warm-up perplexity (target ~2.45).
- **Corpus Filtering** - Why needed: Ensures high-quality training data and removes noise. Quick check: Verify language ID threshold of 0.8 and quality heuristics.
- **Parameter-Efficient Adaptation** - Why needed: Maintains original capabilities while adding new language. Quick check: Benchmark English performance post-adaptation.

## Architecture Onboarding
**Component Map:** Tokenizer Training -> Warm-up Phase -> Continual Pretraining -> SFT -> Evaluation
**Critical Path:** Tokenizer enhancement → Warm-up (embedding alignment) → CPT with LoRA → SFT with LoRA
**Design Tradeoffs:** Compact model size (3.8B) vs. performance (~80% of 8B baseline); limited context window (2,048) vs. computational efficiency
**Failure Signatures:** Catastrophic forgetting of English capabilities; training instability without proper warm-up; poor Persian performance due to inadequate tokenizer coverage
**First Experiments:** 1) Train tokenizer on Persian Wikipedia and verify token coverage; 2) Run warm-up phase on translated Tiny Stories and check perplexity; 3) Execute CPT on filtered corpus with LoRA and monitor training stability

## Open Questions the Paper Calls Out
- **Warm-up Impact Quantification:** Unable to conduct full ablation study on warm-up phase's isolated impact due to computational constraints, though preliminary experiments suggested instability without it.
- **Context Window Limitations:** Performance constrained by 2,048 token limit on reasoning benchmarks like MMLU Pro, with future work planned to address this.
- **Alignment Bias:** Potential for model outputs to reflect Western cultural norms rather than native Persian cultural nuances due to English base model transfer.

## Limitations
- Limited evaluation beyond automated benchmarks (Open Persian LLM Leaderboard)
- Unspecified profanity filter and exact normalization implementation details
- Performance still lags behind larger models despite competitive ranking
- Context window limitations affect reasoning benchmark performance

## Confidence
- **High Confidence:** Curriculum methodology is clearly described and logically sound; specific benchmark results are verifiable
- **Medium Confidence:** Claims about minimal hardware requirements and compact model effectiveness are specific but may not scale predictably
- **Medium Confidence:** Filtering pipeline effectiveness is described generally but lacks specific implementation details

## Next Checks
1. **Reproduce Warm-up Phase:** Implement LoRA (rank 4) + full embedding tuning on translated Tiny Stories dataset and verify perplexity close to 2.45
2. **Evaluate English Retention:** Benchmark model on English-only tasks post-adaptation to verify minimal catastrophic forgetting
3. **Implement and Test Filtering Pipeline:** Recreate FastText filtering and basic profanity filtering, apply to sample TLPC, and measure data removal percentages at each step