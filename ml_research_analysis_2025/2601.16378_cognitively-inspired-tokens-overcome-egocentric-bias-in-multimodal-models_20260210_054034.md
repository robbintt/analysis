---
ver: rpa2
title: Cognitively-Inspired Tokens Overcome Egocentric Bias in Multimodal Models
arxiv_id: '2601.16378'
source_url: https://arxiv.org/abs/2601.16378
tags:
- tokens
- spatial
- perspective-taking
- reasoning
- orientation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study investigates the persistent egocentric bias in multimodal\
  \ language models (MLMs) during spatial reasoning tasks that require adopting another\
  \ agent's visual perspective. To address this, the authors introduce perspective\
  \ tokens\u2014specialized embeddings that encode orientation information through\
  \ either embodied body-keypoint cues or abstract rotation representations."
---

# Cognitively-Inspired Tokens Overcome Egocentric Bias in Multimodal Models

## Quick Facts
- arXiv ID: 2601.16378
- Source URL: https://arxiv.org/abs/2601.16378
- Reference count: 4
- Primary result: Embodied perspective tokens enable 95% accuracy on unaligned perspective-taking tasks (vs 0% baseline), while rotation tokens generalize to non-human references.

## Executive Summary
This study addresses the persistent egocentric bias in multimodal language models during spatial reasoning tasks that require adopting another agent's visual perspective. The authors introduce perspective tokens—specialized embeddings that encode orientation information through either embodied body-keypoint cues or abstract rotation representations. These tokens are integrated into LLaVA-1.5-13B through fine-tuning, allowing the model to reason over non-egocentric viewpoints. Evaluation across synthetic and naturalistic benchmarks shows substantial improvements, with representational analyses revealing that fine-tuning enhances latent orientation sensitivity already present in the base model. This approach provides a lightweight, model-agnostic mechanism for embedding cognitively grounded spatial structure.

## Method Summary
The authors extend the LLaVA-1.5-13B tokenizer with approximately 700 specialized tokens for spatial coordinates and orientation bins, then fine-tune the embedding matrix while freezing the vision encoder. Two token types are explored: embodiment tokens derived from body keypoints using ViTPose, and rotation tokens from object orientation using OrientAnything. Training uses a curriculum that separates learning token generation from reasoning with tokens, progressing from 100% token-generation examples to 10% over 10 epochs. The approach is evaluated on synthetic benchmarks (Isle Bricks V2, perspective-taking benchmark) and naturalistic datasets (COCO, 3DSRBench).

## Key Results
- Embodiment tokens achieve 95% accuracy on unaligned perspective-taking tasks (up from 0% baseline)
- Rotation tokens enable generalization to non-human references like animals and furniture (+21.1% on 3DSRBench)
- Fine-tuning enhances latent orientation sensitivity already present in the base model, revealed through hidden-unit analysis
- High performance under direct prompting (not just chain-of-thought) suggests successful internalization of spatial reasoning

## Why This Works (Mechanism)

### Mechanism 1
Specialized token embeddings create a dedicated representational substrate that supports perspective transformation, which text-based spatial descriptions cannot achieve. Expanding the tokenizer with discrete spatial tokens (coordinates, orientation bins) and training the embedding matrix allocates a learnable subspace where geometric relationships can be explicitly encoded and transformed. Text descriptions distribute spatial information across many tokens without a unified geometric structure.

### Mechanism 2
Base MLMs already encode latent orientation sensitivity, which fine-tuning amplifies rather than creates from scratch. Hidden-unit analysis reveals feature-selective units for aligned vs. unaligned orientations in both base and fine-tuned models. Fine-tuning increases the number of such units and sharpens their tuning curves.

### Mechanism 3
Curriculum learning separates token generation learning from reasoning-with-tokens learning, preventing interference. Training progresses from 100% token-generation examples to 10% over 10 epochs, with chain-of-thought and direct-answer examples increasing correspondingly. This allows the model to first ground spatial tokens in visual input before learning to transform them for perspective-taking.

## Foundational Learning

- **Level 1 vs. Level 2 Visual Perspective-Taking (VPT)**: The paper explicitly targets level-2 VPT (understanding how a scene appears from another's viewpoint), which requires qualitatively different mechanisms than level-1 (understanding what another can see). Quick check: If a model correctly identifies that an occluded object is invisible to an avatar (level 1), does that guarantee it can judge whether the object appears on the avatar's left or right (level 2)?

- **Egocentric vs. Allocentric Reference Frames**: The core problem is egocentric bias—models default to the viewer's perspective. Allocentric reasoning requires representing spatial relations independent of the viewer's orientation. Quick check: In an image where you face north and an avatar faces south, does "the object is on your left" imply the object is on the avatar's left, right, or neither?

- **LoRA (Low-Rank Adaptation) Fine-Tuning**: The method freezes the vision encoder and uses LoRA adapters for efficient language-model tuning while keeping the embedding matrix fully trainable. Understanding this constraint shapes expectations about what the intervention can modify. Quick check: If you wanted to change how the vision encoder represents orientation directly, would LoRA on the language model be sufficient?

## Architecture Onboarding

- **Component map**: Vision encoder (CLIP-ViT, frozen) -> Multimodal projector -> Language model (LLaVA-1.5-13B, LoRA-adapted) -> Tokenizer (expanded) -> Embedding matrix (fully trainable)

- **Critical path**: Preprocess images → extract keypoints (ViTPose) or bounding boxes + azimuth (OrientAnything) → convert spatial information → discrete tokens (coordinate bins, yaw/azimuth bins) → prepend perspective tokens to text prompt → fine-tune with curriculum → evaluate on aligned (0°) vs. unaligned (180°) items

- **Design tradeoffs**: Embodiment tokens (+100% on unaligned) outperform rotation tokens (+40%) for human references but fail on non-human agents. Rotation tokens generalize better (+21.1% on 3DSRBench) but sacrifice peak performance. Discretization granularity: 8 yaw bins (45° resolution) balance orientation precision with token vocabulary size.

- **Failure signatures**: Consistent 0% on unaligned items (purely egocentric), high aligned accuracy but near-chance unaligned (partial de-biasing), good on synthetic but poor on naturalistic (overfitting), performance collapses when keypoint detector fails (embodiment tokens depend on accurate pose estimation)

- **First 3 experiments**: 1) Ablate token components: train with coordinates only vs. orientation bins only to isolate essential spatial dimensions. 2) Cross-benchmark generalization test: train on COCO single-person images, evaluate on 3DSRBench animals/furniture using rotation tokens. 3) Freeze embedding matrix: train with LoRA only (non-trainable spatial token embeddings) to test necessity of dedicated embedding subspace.

## Open Questions the Paper Calls Out

- **Three-dimensional spatial reasoning**: Can perspective tokens be extended to support full three-dimensional spatial reasoning by integrating depth information? Current rotation tokens operate over 2D spatial relations only; real-world perspective-taking requires reasoning about depth, occlusion, and 3D object relations.

- **Cross-architecture generalization**: Do perspective tokens generalize across different multimodal model architectures and scales beyond LLaVA 1.5-13B? Only one model was tested; effectiveness may depend on specific architectural features, training data, or scale.

- **Multi-agent scenes**: How do perspective tokens perform in scenes with multiple reference agents requiring disambiguation? Training data restricted to single-reference scenes; no evaluation on multi-agent scenarios where the model must track and transform between multiple viewpoints.

## Limitations

- Generalization across modalities and domains remains untested, particularly for video or dynamic environments where keypoint detection may degrade
- Embodiment tokens perform poorly on animals and furniture, creating a trade-off between peak performance and generalization
- The relationship between token embeddings, hidden-unit selectivity, and actual perspective transformations is correlative, not causal
- Dependency on detector accuracy: embodiment tokens require reliable pose estimation, and failures directly degrade performance

## Confidence

- **High Confidence**: The claim that perspective tokens reduce egocentric bias is strongly supported by controlled benchmarks (e.g., 95% unaligned accuracy for embodiment tokens vs. 0% baseline)
- **Medium Confidence**: The assertion that base models encode latent orientation sensitivity is supported by hidden-unit analysis, but the causal link to token-based improvements is not definitively established
- **Medium Confidence**: The generalization of rotation tokens to non-human references is demonstrated on 3DSRBench, but the mechanism for why rotation tokens outperform embodiment tokens in these cases is not fully explained

## Next Checks

1. **Ablate token components**: Train models with only coordinate tokens (no orientation bins) and only orientation bins (no coordinates) to isolate which spatial dimensions are essential for reducing egocentric bias.

2. **Cross-benchmark generalization test**: Train on COCO single-person images using rotation tokens, then evaluate on 3DSRBench animals and furniture to directly test the claim that rotation tokens enable generalization to non-human references.

3. **Freeze embedding matrix**: Train with LoRA adapters only (non-trainable spatial token embeddings) to determine whether the dedicated embedding subspace is necessary or whether adapter tuning alone can achieve similar de-biasing.