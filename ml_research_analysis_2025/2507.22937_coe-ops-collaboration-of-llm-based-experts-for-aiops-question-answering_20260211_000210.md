---
ver: rpa2
title: 'CoE-Ops: Collaboration of LLM-based Experts for AIOps Question-Answering'
arxiv_id: '2507.22937'
source_url: https://arxiv.org/abs/2507.22937
tags:
- task
- aiops
- classifier
- expert
- coe-ops
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of selecting and orchestrating
  diverse large language models (LLMs) for complex AIOps tasks in DevOps. The authors
  propose CoE-Ops, a collaboration-of-experts framework that enhances the two-stage
  expert routing mechanism with a general-purpose LLM-based task classifier and retrieval-augmented
  generation (RAG).
---

# CoE-Ops: Collaboration of LLM-based Experts for AIOps Question-Answering

## Quick Facts
- arXiv ID: 2507.22937
- Source URL: https://arxiv.org/abs/2507.22937
- Reference count: 40
- Primary result: 72% improvement in routing accuracy for high-level tasks compared to existing methods

## Executive Summary
This paper addresses the challenge of selecting and orchestrating diverse large language models (LLMs) for complex AIOps tasks in DevOps. The authors propose CoE-Ops, a collaboration-of-experts framework that enhances the two-stage expert routing mechanism with a general-purpose LLM-based task classifier and retrieval-augmented generation (RAG). CoE-Ops improves task classification accuracy for both high-level (Plan, Code, Build) and low-level (fault analysis, anomaly detection) AIOps tasks by incorporating contextual knowledge through RAG. Experimental results on the DevOps-EVAL dataset demonstrate that CoE-Ops achieves 72% improvement in routing accuracy for high-level tasks compared to existing methods, delivers up to 8% accuracy enhancement over single AIOps models in DevOps problem resolution, and outperforms larger-scale Mixture-of-Experts (MoE) models by up to 14% in accuracy.

## Method Summary
CoE-Ops implements a two-stage expert routing mechanism where a general-purpose LLM classifier first categorizes incoming DevOps queries, then routes them to specialized domain-specific LLM experts. The framework introduces RAG to ground high-level abstract tasks by retrieving similar examples from a knowledge base, addressing the classifier's lack of domain-specific context. The system uses zero-shot classification via prompt engineering, eliminating the need for fine-tuning when new task types are introduced. The routing decision is based on pre-calculated expert accuracy mappings from benchmark datasets, directing queries to the specialist model that demonstrated the highest performance for each task category.

## Key Results
- 72% improvement in routing accuracy for high-level tasks compared to existing methods
- Up to 8% accuracy enhancement over single AIOps models in DevOps problem resolution
- Outperforms larger-scale Mixture-of-Experts (MoE) models by up to 14% in accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decoupling the task classifier from a fixed discriminative model to a general-purpose LLM likely enables zero-shot scalability for new task types without retraining.
- **Mechanism:** The framework replaces traditional fine-tuned classifiers with a general LLM (e.g., DeepSeek) using prompt engineering. This allows the system to recognize new task categories simply by updating the task list in the prompt, rather than retraining the classifier weights.
- **Core assumption:** The general-purpose LLM possesses sufficient internal knowledge to distinguish between broad categories (like "Plan" vs. "Code") when guided by a list, provided the categories are semantically distinct.
- **Evidence anchors:**
  - [abstract] Mentions "incorporating a general-purpose large language model task classifier" to handle diverse tasks.
  - [section IV.B] Describes replacing discriminative models with an LLM to "eliminate fine-tuning requirements" and allow "dynamic adaptation."
  - [corpus] "Can We Recycle Our Old Models?" implies the difficulty of model selection and adaptability in AIOps, supporting the need for flexible routing, though it doesn't explicitly validate the LLM-classifier approach.
- **Break condition:** If the semantic ambiguity between tasks is too high (e.g., two tasks have overlapping definitions in the prompt), the LLM classifier may hallucinate or default to "unknown," failing to route effectively.

### Mechanism 2
- **Claim:** Retrieval-Augmented Generation (RAG) appears to ground high-level abstract tasks by retrieving similar examples, thereby correcting the classifier's lack of domain-specific context.
- **Mechanism:** For abstract queries (e.g., high-level DevOps planning), the system retrieves similar questions and their labels from a database to include in the prompt. This provides the LLM classifier with concrete "demonstrations" (context), improving its ability to map the query to the correct task label.
- **Core assumption:** The vector embeddings of the query effectively capture the semantic intent, and the retrieved examples are truly representative of the target task class.
- **Evidence anchors:**
  - [abstract] States that RAG is introduced "to improve the framework's capability in handling... high-level... tasks."
  - [section IV.C] Details the RAG process and Prompt 3, showing how context is injected to assist the LLM.
  - [section V.C] Shows Classifier 1 accuracy improving from ~14% to ~44% on high-level tasks when RAG is added.
- **Break condition:** If the knowledge base lacks coverage for a specific type of operational query, the retrieved context will be irrelevant ("distractors"), potentially degrading classification accuracy compared to a zero-shot approach.

### Mechanism 3
- **Claim:** Routing queries to specialized smaller models (experts) can outperform larger monolithic Mixture-of-Experts (MoE) models by leveraging domain-specific optimization.
- **Mechanism:** Instead of a single large model handling all inputs, the framework routes inputs to the specific model that demonstrated the highest accuracy for the classified task during benchmarking. This maximizes the "skill ceiling" for each specific task type.
- **Core assumption:** The "Task-Expert" mapping (derived from benchmarks) remains valid for production data, and the cost/latency of the routing step does not negate the performance gains.
- **Evidence anchors:**
  - [abstract] Claims CoE-Ops "outperforms larger-scale Mixture-of-Experts (MoE) models by up to 14% in accuracy."
  - [table IX] Shows CoE-Ops achieving ~70% accuracy vs. Mixtral-8x7b's ~55% on low-level tasks.
  - [corpus] "A Survey of AIOps in the Era of Large Language Models" supports the general trend of using LLMs for AIOps but does not verify this specific routing efficiency.
- **Break condition:** If the benchmark dataset used for mapping is not representative of the live environment (distribution shift), the "best" expert selected may actually perform worse than a robust generalist model.

## Foundational Learning

- **Concept: Two-Stage Expert Routing**
  - **Why needed here:** The core of the CoE-Ops architecture relies on separating the decision logic ("Who should answer?") from the execution logic ("Answer the question"). This allows you to swap out models without retraining the whole system.
  - **Quick check question:** Can you explain why a traditional end-to-end router makes it difficult to add a new expert model to the system?

- **Concept: Zero-Shot Classification**
  - **Why needed here:** The classifier in CoE-Ops operates in zero-shot mode using prompts. Understanding this helps explain why the framework is scalable but also why it relies heavily on the LLM's pre-existing "world knowledge."
  - **Quick check question:** How does providing a "task list" in the prompt change the behavior of an LLM classifier compared to asking it to guess a category without options?

- **Concept: RAG for Context Grounding**
  - **Why needed here:** High-level AIOps tasks (like "Plan" or "Operate") are often abstract. Learning how RAG retrieves similar vectors to ground the LLM is critical to understanding the paper's 72% improvement claim.
  - **Quick check question:** Why would an LLM struggle to classify a question as "Plan" vs "Code" without seeing examples of previous questions labeled as such?

## Architecture Onboarding

- **Component map:** Input Query → RAG Retriever → LLM Classifier → Task-Expert Mapper → Expert Model → Final Answer
- **Critical path:** The **Task Classifier** is the single point of failure. If the classifier mislabels a query (e.g., sending a "Code" question to a "Monitor" expert), the final answer accuracy degrades, regardless of how good the expert model is.
- **Design tradeoffs:**
  - **Scalability vs. Latency:** The framework avoids retraining (high scalability) but adds inference latency from the RAG retrieval + Classifier LLM call before the actual expert call.
  - **Generalist vs. Specialist:** Using a large generalist (DeepSeek-V3) as a mere classifier allows for using smaller, cheaper specialist experts for the final answer, potentially saving cost if the classifier is efficient.
- **Failure signatures:**
  - **"Unknown" Loop:** If the classifier outputs "Unknown" excessively, traffic defaults to a general fallback expert, losing the benefit of specialization.
  - **Semantic Drift:** If the RAG retrieval returns irrelevant context due to poor embedding quality, the classifier may oscillate between labels.
- **First 3 experiments:**
  1. **Classifier Ablation:** Run the classifier on the test set with RAG disabled vs. enabled to isolate the performance gain specifically from the retrieval mechanism (reproducing Table VII/VIII).
  2. **Mapping Sensitivity:** Intentionally route queries to the *wrong* expert (e.g., send "Log Parser" queries to a "Time Series" model) to quantify the degradation caused by routing errors.
  3. **MoE Comparison:** Benchmark the end-to-end CoE-Ops system against a raw Mixtral-8x7b-instruct model on the same hardware to verify the claimed 14% accuracy improvement and measure the latency overhead.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the "Task-Expert" mapping be dynamically constructed without relying on pre-calculated accuracy metrics from static benchmarks?
- **Basis in paper:** [explicit] The Conclusion states the need to "explore the automated construction of AIOps expert capability rankings to achieve fully automated collaboration."
- **Why unresolved:** The current framework determines the optimal expert $M^*_i$ using Eq. (5), which requires evaluating accuracy on a benchmark dataset ($A_{ij}$) beforehand.
- **What evidence would resolve it:** A mechanism that estimates expert capabilities in real-time or via self-assessment, validated against the current benchmark-derived accuracy.

### Open Question 2
- **Question:** How can the CoE-Ops framework be extended to support multi-tiered collaboration within multi-agent systems?
- **Basis in paper:** [explicit] Section VII outlines future work to "integrate this framework with multi-agent systems to establish multi-tiered AIOps expert collaboration."
- **Why unresolved:** The current methodology (Eq. 3) routes an input to a single expert ($f: T \to E$) for a one-shot answer, lacking protocols for iterative feedback loops between agents.
- **What evidence would resolve it:** A modified architecture enabling experts to query one another, demonstrated on complex AIOps tasks requiring multi-step reasoning.

### Open Question 3
- **Question:** Does the computational overhead of the RAG-enhanced two-stage routing impact real-time feasibility in production environments?
- **Basis in paper:** [inferred] RQ3 addresses efficiency regarding model size/accuracy, but the paper does not evaluate the latency added by the LLM-based classifier and retrieval process (Eq. 9-13).
- **Why unresolved:** While the approach is "efficient" regarding parameter usage, the added latency of vector encoding and LLM inference for routing may be prohibitive for time-sensitive operations.
- **What evidence would resolve it:** End-to-end latency benchmarks comparing CoE-Ops against single-model and MoE baselines on identical hardware.

## Limitations

- The RAG retrieval hyperparameters (k value and similarity threshold) are unspecified, making it difficult to replicate the exact performance gains reported
- The framework assumes the expert-to-task mapping derived from benchmarks remains valid for production data, which may not hold under distribution shift
- The accuracy improvements come at the cost of increased inference latency from the additional routing layer

## Confidence

- **High confidence** in the mechanism of using LLM classifiers for zero-shot task categorization, supported by established zero-shot learning literature
- **Medium confidence** in the claimed 72% improvement for high-level tasks, as the specific RAG configuration significantly impacts results
- **Medium confidence** in the routing efficiency claims, as the comparison assumes fair hardware conditions between small experts and larger MoE models
- **Low confidence** in the absolute accuracy numbers without access to the exact prompts and retrieval parameters

## Next Checks

1. **Classifier sensitivity analysis**: Systematically vary the RAG retrieval k value (1, 3, 5, 10) and similarity threshold to identify the configuration that achieves the reported 72% accuracy improvement for high-level tasks

2. **Distribution shift robustness**: Test the framework on a held-out test set that differs systematically from the benchmark data (e.g., different DevOps tools, time periods) to measure how quickly the expert mappings degrade

3. **End-to-end latency benchmark**: Measure the total inference time (RAG retrieval + classifier + expert) for the full pipeline and compare it against a single MoE model on identical hardware to validate the claimed accuracy-latency tradeoff