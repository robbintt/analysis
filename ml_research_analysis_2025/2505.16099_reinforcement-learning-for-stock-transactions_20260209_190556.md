---
ver: rpa2
title: Reinforcement Learning for Stock Transactions
arxiv_id: '2505.16099'
source_url: https://arxiv.org/abs/2505.16099
tags:
- agent
- agents
- stock
- price
- q-learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work applied reinforcement learning to identify optimal stock
  purchase timing within defined time windows. The authors formulated the problem
  as a Markov Decision Process and developed four agents: a baseline rule-based agent,
  an exact Q-learning agent, and two approximate Q-learning agents using linear and
  deep neural network function approximation.'
---

# Reinforcement Learning for Stock Transactions

## Quick Facts
- **arXiv ID:** 2505.16099
- **Source URL:** https://arxiv.org/abs/2505.16099
- **Reference count:** 8
- **Primary result:** RL agents outperformed a threshold-based baseline in 3/4 companies tested, but showed high variance across runs, suggesting non-convergence.

## Executive Summary
This paper applies reinforcement learning to determine optimal stock purchase timing within fixed-length time windows. The authors formulate the problem as a Markov Decision Process with four agents: a baseline threshold rule, exact Q-learning, linear approximate Q-learning, and deep Q-learning. Trained and evaluated on daily OHLC data from Apple, Amazon, Microsoft, and Google (post-2005), the RL agents showed promise but suffered from high variance across runs, suggesting policies did not fully converge. The study concludes that while RL agents can outperform simple baselines, stock price movements appear too random for consistent pattern learning with price-only features.

## Method Summary
The authors formulated stock trading as an MDP where agents must buy within a fixed window or face a forced purchase at the last timestep. Four agents were implemented: a baseline threshold-based rule, exact Q-learning using discretized price movements, linear approximate Q-learning with continuous price features, and deep Q-learning with separate neural networks per action. The agents were trained on 80% of post-2005 daily OHLC data from four tech stocks, validated on 10%, and tested on the remaining 10%. Performance was evaluated using average profit per window over 51 runs to account for variance.

## Key Results
- RL agents outperformed the baseline in 3/4 companies on average
- Linear approximate Q-learning agent beat the baseline more than 50% of the time
- Deep Q-learning performed best on Apple (avg profit -0.1143) but worst on Microsoft
- High variance across runs suggested policies did not fully converge

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** MDP formulation with forced-action deadlines creates a well-defined learning problem balancing exploration against time constraints.
- **Core assumption:** Optimal buy timing within short windows correlates with exploitable short-term price patterns.
- **Evidence anchors:** MDP framework with time windows and forced buy on last day; neighbor papers use similar MDP formulations.
- **Break condition:** If price movements within windows are predominantly noise, no policy can systematically beat a naive baseline.

### Mechanism 2
- **Claim:** Function approximation enables Q-learning to generalize across continuous price states.
- **Core assumption:** Chosen feature representation contains sufficient signal for value prediction.
- **Evidence anchors:** Linear approximation uses full price vectors; deep Q-learning showed asset-specific performance variation.
- **Break condition:** If feature set omits causal drivers, function approximators will overfit to noise.

### Mechanism 3
- **Claim:** Reward shaping directly tied to price improvement aligns agent incentives with trading profit.
- **Core assumption:** Within-window price improvement is a meaningful proxy for profitability.
- **Evidence anchors:** Reward defined as negative price difference; evaluation uses same metric.
- **Break condition:** If real trading costs are non-negligible, learned policy may over-trade.

## Foundational Learning

- **Concept: Markov Decision Processes (MDPs)**
  - **Why needed here:** The entire problem is framed as an MDP; understanding states, actions, transitions, rewards, and discount factors is prerequisite to interpreting any agent design.
  - **Quick check question:** Can you explain why the forced buy on the last timestep is necessary for MDP well-formedness?

- **Concept: Q-Learning with Function Approximation**
  - **Why needed here:** Three of four agents use Q-learning variants; understanding the Bellman update, exploration (ε-greedy), and how linear/neural approximators replace Q-tables is essential.
  - **Quick check question:** Why does tabular Q-learning fail when the state space is continuous?

- **Concept: Train/Validation/Test Splits for Sequential Data**
  - **Why needed here:** The paper uses 80/10/10 temporal splits; preventing lookahead bias in time-series RL evaluation is critical.
  - **Quick check question:** What goes wrong if you shuffle stock data randomly before splitting?

## Architecture Onboarding

- **Component map:** Data layer (Yahoo! Finance OHLC) -> MDP layer (time-window generator, state encoder, reward calculator) -> Agent layer (baseline, exact Q-learning, linear AQ, deep Q) -> Evaluation layer (51-run scoring loop)

- **Critical path:**
  1. Ingest OHLC data → filter post-2005 → create train/val/test splits
  2. For each window, encode state (history of movements or price vectors)
  3. Agent selects action via ε-greedy policy
  4. If "buy," compute reward and terminate episode; if "wait," transition to next day
  5. Update Q-values (tabular or via gradient descent on Bellman error)
  6. Evaluate on test set without further updates; aggregate over 51 runs

- **Design tradeoffs:**
  - Small window size → more training episodes but shorter decision horizons
  - Large history size h → richer state but exponentially larger tabular state space
  - Deep vs linear approximation → deep can capture nonlinearity but overfits easily
  - Binary reward vs continuous reward → binary is simpler but loses magnitude signal

- **Failure signatures:**
  - Wide profit distributions across runs → policies not converging
  - Classification accuracy near 50% → price direction is effectively unpredictable
  - Baseline outperforming RL agents → learned policies are fitting noise
  - Confidence intervals overlapping → no statistically significant difference

- **First 3 experiments:**
  1. Run baseline agent across all four stocks with varying d values; confirm profit distribution is deterministic
  2. Train tabular Q-learning on Apple with increasing episode counts; plot Q-value stability over time
  3. Add S&P 500 daily return to linear AQ state; compare profit distribution width to original

## Open Questions the Paper Calls Out

- **Question:** Would incorporating professional financial features (market momentum, volume, indices) into the state space reduce policy variance and improve average profit?
  - **Basis:** Authors explicitly state this could allow agents to extract important information more easily
  - **Evidence needed:** Comparative experiment showing reduced variance and higher profits with enhanced features

- **Question:** Does including external data like company news significantly improve price prediction?
  - **Basis:** Authors suggest automatically parsing company-related news as external data
  - **Evidence needed:** Study showing news-aware agents outperform price-only agents

- **Question:** Can deep time-series models like LSTMs provide effective features where regression methods failed?
  - **Basis:** Authors propose exploring LSTM's to identify price trends
  - **Evidence needed:** Demonstration that LSTM-based model achieves classification accuracy significantly better than 50%

## Limitations
- High variance across runs suggests policies did not fully converge
- Reward structure ignores transaction costs, slippage, and optimal sell timing
- Missing specification of window size and hyperparameters prevents faithful reproduction
- Conclusion that prices are "too random" may overgeneralize from limited feature sets

## Confidence
- **High confidence:** MDP formulation and experimental methodology (51-run evaluation, temporal splits)
- **Medium confidence:** Observation that RL agents outperform baseline in 3/4 companies with high variance
- **Low confidence:** Overgeneralization that prices are too random for RL learning

## Next Checks
1. Run tabular Q-learning on Apple with increasing training episodes; plot Q-value stability and policy consistency to determine if variance stems from non-convergence
2. Systematically remove components from linear approximation state (e.g., drop history depth) and measure impact on profit variance
3. Implement two-stage reward with sell action and transaction costs; compare learned policies to original single-buy setup