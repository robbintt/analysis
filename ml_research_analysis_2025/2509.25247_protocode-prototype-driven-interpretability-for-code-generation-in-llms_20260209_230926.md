---
ver: rpa2
title: 'Protocode: Prototype-Driven Interpretability for Code Generation in LLMs'
arxiv_id: '2509.25247'
source_url: https://arxiv.org/abs/2509.25247
tags:
- code
- learning
- urlhttps
- each
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a prototype-driven approach for improving\
  \ interpretability and performance in LLM code generation using in-context learning\
  \ (ICL). The method combines piecewise-linear manifold learning with proxy anchor\u2013\
  based metric learning to sample high-quality ICL demonstrations."
---

# Protocode: Prototype-Driven Interpretability for Code Generation in LLMs

## Quick Facts
- arXiv ID: 2509.25247
- Source URL: https://arxiv.org/abs/2509.25247
- Authors: Krishna Vamshi Bodla; Haizhao Yang
- Reference count: 40
- Primary result: Prototype-driven ICL improves pass@10 on MBPP by up to 5.3% over base models

## Executive Summary
This paper introduces Protocode, a prototype-driven approach for improving both interpretability and performance in LLM code generation through in-context learning (ICL). The method combines piecewise-linear manifold learning with proxy anchor–based metric learning to sample high-quality ICL demonstrations. Prototype-gradient attribution is used to compute token-level influence scores, which are then propagated through the Abstract Syntax Tree (AST) to produce syntax-aware confidence maps. Experiments on MBPP and MBPP+ datasets show consistent improvements across multiple models while providing interpretable confidence maps that reveal model-specific syntactic strengths and weaknesses.

## Method Summary
Protocode operates in two stages: (1) training a neural network using joint manifold learning and proxy anchor loss to map code embeddings into a space where learned proxies correspond to semantically meaningful prototypes, then (2) using these prototypes as ICL demonstrations to generate code and compute gradient-based attribution scores. The attribution scores propagate through AST structures to produce syntax-category-level confidence maps. The approach avoids the memory overhead of storing full token probability distributions while maintaining strong empirical performance on code generation benchmarks.

## Key Results
- Pass@10 improvements of 5.3% (Qwen2.5-coder-0.5B) to 3.6% (StarCoder) over base models on MBPP
- Consistent pass@10 gains across all six tested models (Qwen2.5-Coder, Llama3.2, Falcon3, StarCoder, CodeLlama) vs baseline sampling strategies
- AST analysis reveals consistent patterns: highest confidence in Scope/Data Structures/Functions, lowest in Exception handling across all models
- Model-specific sensitivities observed, with Qwen2.5-Coder showing more stable performance than Llama3.2 across hyperparameter variations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Joint manifold and metric learning produces prototypes that improve pass@10 compared to base model and alternative sampling strategies.
- **Mechanism:** Piecewise-linear manifold learning preserves local geometric structure in embedding space while Proxy-Anchor loss enforces intra-class compactness and inter-class separation. The combined loss trains a neural network to map representations such that nearest neighbors to learned proxies are semantically meaningful and geometrically representative.
- **Core assumption:** High-dimensional code embeddings lie on low-dimensional manifolds that can be locally approximated as linear regions.
- **Evidence anchors:** Tables 1-2 show prototypes method outperforms base, diversity, similarity, and mbpp sampling on pass@10 for most models; related work confirms demonstration quality causally affects performance with +5.1% to -3.3% ATE swing.

### Mechanism 2
- **Claim:** Gradient-based prototype attribution provides token-level influence scores without storing full vocabulary distributions.
- **Mechanism:** Compute mean prototype embedding, calculate dot product with token embeddings, then backpropagate gradient as confidence scores. These propagate through AST terminal → non-terminal → syntax category hierarchy for interpretable confidence maps.
- **Core assumption:** Gradient magnitude correlates with prototype influence on token generation.
- **Evidence anchors:** ASTrust requires storing probabilities for all vocabulary tokens at every step—"an impractical, memory-intensive process"; no direct corpus evidence for gradient-based attribution efficiency claim.

### Mechanism 3
- **Claim:** AST-grounded analysis reveals model-specific syntactic strengths; structured programming categories show higher confidence than Exception handling across all tested LLMs.
- **Mechanism:** Tokens are aligned to AST terminal nodes, then aggregated through non-terminal nodes into 8 syntax categories. Averaged attribution scores per category reveal global patterns.
- **Core assumption:** Syntax categories encode semantic information that contextualizes token-level confidence meaningfully.
- **Evidence anchors:** Figure 3 shows all 6 LLMs exhibit highest confidence in Scope/Data Structures/Functions, lowest in Exception handling; "StarCoder offers broader syntactic reliability and is better suited for tasks requiring control flow."

## Foundational Learning

- **Concept: In-Context Learning (ICL) demonstration sensitivity**
  - Why needed here: The entire method hinges on the empirical fact that demonstration quality causally affects model performance—poor demonstrations can decrease performance below base model.
  - Quick check question: Can you explain why similarity-based nearest neighbor selection might produce worse results than random selection for some models?

- **Concept: Manifold hypothesis and local linear approximation**
  - Why needed here: The piecewise-linear manifold construction assumes high-dimensional embeddings cluster on low-dimensional manifolds that can be locally approximated by m-dimensional subspaces (via PCA on k-nearest neighbors).
  - Quick check question: Why would m=2 perform better than m=8 for manifold dimension in this code embedding context?

- **Concept: Abstract Syntax Trees (AST) as semantic structure**
  - Why needed here: The interpretability mechanism requires understanding how tokens map to terminal/non-terminal nodes and aggregate into syntax categories for meaningful explanations.
  - Quick check question: How would you handle a token that spans multiple AST nodes during alignment?

## Architecture Onboarding

- **Component map:** Dataset D → LLM encoder → embeddings z → h_θ network → manifold similarity + proxy anchor loss → learned proxies θ_m → nearest training sample → prototype per class → Test query + prototypes → LLM → generated code C → tokenize → token embeddings z_wi → prototype z_a → gradient attribution → AST alignment → syntax category confidence

- **Critical path:** The h_θ network training (Stage 1) is the bottleneck—if manifold construction fails or proxies don't converge, downstream prototype selection and attribution both degrade. Monitor L_total convergence and proxy-embedding distance distributions.

- **Design tradeoffs:**
  - m (manifold dimension): Lower values (2-3) more stable but may underfit structure; higher values (6-8) overfit with limited batch samples
  - T (reconstruction threshold): Higher (0.9+) ensures quality points but may produce sparse neighborhoods; lower includes noisy points
  - Number of prototypes: Paper uses 9 (one per programming language class); scaling to more classes requires proportional memory

- **Failure signatures:**
  - pass@10 drops below base model → prototypes are misleading; check if proxy vectors map to outlier training samples
  - High variance across hyperparameter settings → model is hyperparameter-sensitive (observed with Llama3.2); consider ensemble or fixed configuration
  - AST parsing errors → generated code has syntax errors; increase temperature or add syntax validation step

- **First 3 experiments:**
  1. **Reproduce pass@10 comparison:** Run prototypes vs. similarity vs. diversity vs. base on MBPP test set with Qwen2.5-coder-0.5B using Table 3 architecture and B.2 hyperparameters. Expect ~0.122 vs ~0.069 vs ~0.063 vs ~0.116 pass@10.
  2. **Ablate m parameter:** Train h_θ with m ∈ {2, 4, 6, 8} on Magicoder subset, measure pass@10 and training time. Expect decreasing performance and increasing compute as m grows.
  3. **Validate attribution-AST alignment:** Generate code with prototypes, compute gradient attributions, parse AST, and verify token→terminal→category aggregation produces Figure 3-style category confidence patterns. Compare with ASTrust token probability approach for memory usage.

## Open Questions the Paper Calls Out

- **Question:** Can prototype steering mechanisms be effectively developed to influence LLM behavior for pre-hoc interpretability?
  - Basis in paper: The authors state in the Future Works section that the framework can be extended toward "pre-hoc interpretability by design," specifically suggesting "prototype steering" as a mechanism for controllability.
  - Why unresolved: The current study focuses on post-hoc analysis (sampling demonstrations to explain output) rather than active intervention; steering requires defining how to manipulate latent prototype representations to alter generation trajectories in real-time.
  - What evidence would resolve it: A demonstration of controlled code generation by modifying prototype vectors, showing a causal link between prototype manipulation and output attributes.

- **Question:** Can this method serve as a reliable metric for ranking code datasets by their effectiveness in yielding high-quality ICL prototypes?
  - Basis in paper: The authors note that the method can "naturally be applied as a global metric for ranking datasets with respect to their ability to yield effective prototypes."
  - Why unresolved: The experiments exclusively sample prototypes from the Magicoder-OSS-Instruct-75K dataset; the correlation between the geometric/metric properties of prototypes sampled from different datasets and downstream performance remains untested.
  - What evidence would resolve it: A comparative study measuring manifold learning loss and proxy alignment across multiple datasets and correlating these scores with pass@k improvements.

- **Question:** Does utilizing semantic or functional labels (rather than programming language IDs) as classes for proxy anchor loss improve prototype quality?
  - Basis in paper: Section 3.1 describes label encoding the "programming language ID" to create 9 classes. While this ensures language syntax consistency, it assumes language is the primary axis of similarity, potentially ignoring functional semantic clusters that might yield more useful ICL examples.
  - Why unresolved: The paper does not ablate the choice of class labels; it is unclear if the model learns generic language syntax or specific algorithmic patterns, and whether finer-grained semantic labeling would enhance the "semantically discriminative" property of the prototypes.
  - What evidence would resolve it: Experiments comparing prototypes derived from functional tags against language-only tags, measuring the resulting pass@k and AST confidence scores.

## Limitations

- Context length constraints limit scalability when applying prototype selection across multiple programming languages, requiring solutions like using only Python prototypes that may reduce diversity benefits.
- High hyperparameter sensitivity observed in Llama3.2 (but not Qwen2.5-Coder) suggests model-specific architectural differences affect how well the manifold and metric learning components integrate.
- AST-based interpretability relies on perfect syntax parsing of generated code, which may fail for low-confidence or syntactically broken outputs.

## Confidence

**High Confidence:** The core empirical claims about pass@10 improvements (0.122 vs 0.116 for Qwen2.5-coder-0.5B on MBPP) are well-supported by direct comparisons across multiple models and datasets. The AST-grounded confidence analysis showing consistent patterns across all six tested LLMs has strong reproducibility potential.

**Medium Confidence:** The mechanism claims connecting manifold learning + proxy anchor loss to prototype quality are supported by training procedures and loss function specifications, but lack ablation studies isolating each component's contribution. The gradient-based attribution method's efficiency advantage over ASTrust is theoretically sound but lacks direct comparison metrics.

**Low Confidence:** The generalizability claims beyond the six tested LLMs and two datasets are limited by the narrow experimental scope. The relationship between prototype quality and specific syntactic categories remains correlational rather than causal.

## Next Checks

1. **Hyperparameter Ablation Study:** Systematically vary m ∈ {2, 4, 6, 8}, T ∈ {0.85, 0.90, 0.95}, and α ∈ {16, 32, 64} across all six LLMs to quantify model-specific sensitivities and identify optimal configurations. Measure both pass@10 performance and training stability.

2. **Cross-Dataset Generalization:** Evaluate prototype sampling on alternative code generation benchmarks (e.g., HumanEval, APPS) with at least two additional model families not included in the original study. Compare whether the Scope/Data Structures/Functions confidence pattern persists across different task distributions.

3. **Component Isolation Experiments:** Create controlled ablations that remove either the manifold learning (using only Proxy Anchor loss) or metric learning (using only manifold reconstruction) to quantify each component's individual contribution to pass@10 performance. Additionally, compare the gradient-based attribution method against ASTrust using identical prototype sets to measure actual memory and compute overhead differences.