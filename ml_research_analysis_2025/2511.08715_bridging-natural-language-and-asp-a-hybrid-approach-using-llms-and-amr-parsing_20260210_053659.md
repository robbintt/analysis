---
ver: rpa2
title: 'Bridging Natural Language and ASP: A Hybrid Approach Using LLMs and AMR Parsing'
arxiv_id: '2511.08715'
source_url: https://arxiv.org/abs/2511.08715
tags:
- language
- used
- problem
- natural
- line
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel hybrid approach to automatically translate
  unconstrained English into Answer Set Programming (ASP) code for solving logic puzzles.
  The method leverages Large Language Models (LLMs) to extract facts, identify relationships,
  and simplify constraints, while Abstract Meaning Representation (AMR) graphs are
  used to systematically generate ASP constraints.
---

# Bridging Natural Language and ASP: A Hybrid Approach Using LLMs and AMR Parsing

## Quick Facts
- **arXiv ID**: 2511.08715
- **Source URL**: https://arxiv.org/abs/2511.08715
- **Reference count**: 40
- **Key outcome**: Novel hybrid approach translates English logic puzzles into Answer Set Programming code using LLMs for extraction and AMR parsing for constraint generation.

## Executive Summary
This paper presents a hybrid system that automatically converts unconstrained English logic puzzle descriptions into executable Answer Set Programming (ASP) code. The approach strategically minimizes Large Language Model (LLM) involvement to simple extraction tasks while leveraging Abstract Meaning Representation (AMR) parsing for systematic constraint generation. By reducing the LLM's role to identifying categories, entities, and simplifying constraints, the system avoids common pitfalls like hallucination and syntax errors. The method successfully generates complete, runnable ASP programs that correctly solve example puzzles including "Day-at-the-Zoo" and "Einstein's Riddle," demonstrating accurate solutions matching expected outputs.

## Method Summary
The system employs a four-stage pipeline: First, an LLM extracts categories, entities, and assigns entities to categories via four sequential prompts. Second, the LLM identifies predicate pairings and a program generates choice rules. Third, the LLM simplifies constraints into uniform language. Fourth, an AMR parser processes these simplified constraints to generate ASP integrity constraints. The approach uses ChatGPT 4o for LLM tasks, SpaCy with amrlib's `parse_xfm_bart_large` model for AMR parsing, and Clingo as the ASP solver. Knowledge modules handle special spatial and ordinal concepts through hardcoded patterns.

## Key Results
- Successfully generates complete, runnable ASP programs from natural language logic puzzles
- Correctly solves "Day-at-the-Zoo" and "Einstein's Riddle" with outputs matching expected solutions
- Demonstrates reduced hallucination compared to LLM-only approaches by minimizing LLM scope
- Shows accurate solutions through systematic separation of information extraction and structured generation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Minimizing LLM scope to simple extraction tasks while delegating structured generation to AMR parsing reduces hallucination and improves syntactic correctness.
- **Core assumption**: AMR parsing accurately captures semantic structure, and the mapping from AMR nodes to ASP predicates is reliable for targeted constraint types.
- **Evidence anchors**: [abstract] "The LLM is minimized to avoid common pitfalls like hallucination"; [section IV] "Compared to solely LLM-based systems, our proposed system guarantees correct syntax"; related work shows LLM-only approaches suffer from formatting and logical errors.
- **Break condition**: AMR parser errors propagate without correction, as demonstrated with "Johan is not last in line" producing incorrect polarity.

### Mechanism 2
- **Claim**: Chain-of-thought-style constraint simplification enables systematic AMR-to-ASP mapping.
- **Core assumption**: Decomposed constraints retain logical equivalence to original compound statements.
- **Evidence anchors**: [section IV.C] "Our approach is reminiscent of chain-of-thought prompting"; shows explicit simplification example breaking compound constraint into four separate statements.
- **Break condition**: Multi-sentence constraints or cross-constraint dependencies are not currently handled.

### Mechanism 3
- **Claim**: Knowledge modules for spatial/ordinal concepts extend AMR coverage without retraining.
- **Core assumption**: The set of required spatial/ordinal concepts is bounded and detectable via keyword matching in AMR.
- **Evidence anchors**: [section IV.C] Hardcoded patterns detect special AMR nodes like "location (next-to" and apply domain-specific ASP transformations.
- **Break condition**: Novel spatial concepts (e.g., "behind") lack handling rules.

## Foundational Learning

- **Answer Set Programming (ASP) basics: facts, rules, choice rules, constraints**
  - Why needed here: The system generates ASP syntax directly; understanding the target representation is essential for debugging and extending constraint templates.
  - Quick check question: Given the rule `1{order_in_line_of(C,O):order_in_line(O)}1:-child(C).`, what does the cardinality constraint `1{...}1` enforce?

- **Abstract Meaning Representation (AMR) and PENMAN notation**
  - Why needed here: The constraint generation module parses AMR graphs; understanding node-structure and role-labels (ARG0, ARG1, polarity) is required for extending the parser.
  - Quick check question: In the AMR `(r / receive-01 :polarity - :ARG0 (g / girl))`, what does `:polarity -` indicate?

- **LLM prompt engineering for structured extraction**
  - Why needed here: Fact and rule generation rely on carefully constrained prompts; temperature=0, explicit formatting instructions, and "take your time" framing all affect consistency.
  - Quick check question: Why does the system use separate prompts for category extraction and entity-to-category assignment rather than combining them?

## Architecture Onboarding

- **Component map**:
```
[Problem Text] 
    → [LLM: Category Extraction] → categories
    → [LLM: Entity Extraction] → entities  
    → [LLM: Entity-Category Assignment] → facts
    → [Template Program: Rule Generation] → choice rules + solution predicate
    → [LLM: Constraint Simplification] → normalized constraints
    → [AMR Parser (amrlib/bart_large)] → AMR graphs
    → [Constraint Generator + Knowledge Modules] → ASP constraints
    → [Final ASP File] → [Clingo Solver]
```

- **Critical path**: AMR parsing accuracy → constraint correctness → valid ASP. The paper documents one AMR error (missing polarity) that inverted a constraint's meaning. No automated correction exists.

- **Design tradeoffs**:
  - LLM choice: ChatGPT-4o used; Llama variants tested but "results need to be developed further" (weaker extraction accuracy).
  - AMR model: `parse_xfm_bart_large` via amrlib; not fine-tuned for this domain.
  - Knowledge modules: Hand-crafted, not learned—brittle to novel phrasing but transparent and editable.

- **Failure signatures**:
  - AMR polarity errors → logically inverted constraints (silent failure, no validation).
  - Entity-category mismatches → orphaned constants in generated rules.
  - Unsupported spatial concepts → missing constraints → solver returns multiple or no solutions.
  - LLM inconsistency (despite temperature=0 prompting) → varying predicate names across runs.

- **First 3 experiments**:
  1. **Baseline validation**: Run the provided Day-at-the-Zoo and Einstein's Riddle examples through the pipeline. Verify output matches paper's stated solutions. Intentionally introduce the "Johan is not last" AMR error to observe failure mode.
  2. **AMR parser comparison**: Swap `parse_xfm_bart_large` for alternative AMR models on the same inputs. Measure polarity accuracy and node-coverage for constraint-relevant concepts.
  3. **Stress test with novel puzzles**: Feed a logic puzzle with a spatial concept not in the knowledge modules (e.g., "diagonally opposite" or "behind"). Characterize where the pipeline fails—is it at AMR parsing, constraint generation, or solver runtime? This identifies extension priorities.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can a specialized AMR parser trained on logic puzzle domains significantly reduce parsing errors compared to general-purpose AMR parsers like parse_xfm_bart_large?
- **Basis in paper**: [explicit] "Training an AMR that is more suited for the task at hand is also worth considering."
- **Why unresolved**: The paper uses an off-the-shelf AMR parser that produced at least one error requiring manual correction. No investigation was conducted into whether domain-specific training could improve accuracy.
- **What evidence would resolve it**: Train an AMR parser on a corpus of logic puzzle sentences and compare its parsing accuracy against the current model using a benchmark of constraint sentences.

### Open Question 2
- **Question**: How does the system perform when evaluated across a standardized benchmark of diverse logic puzzles rather than just two example problems?
- **Basis in paper**: [inferred] The paper demonstrates results on only "Day-at-the-Zoo" and "Einstein's Riddle," with no quantitative evaluation, error rate analysis, or comparison to baseline systems across a broader dataset.
- **Why unresolved**: The authors present successful outputs for two hand-picked examples but acknowledge "the example logic puzzles used are simpler ones" and do not report systematic performance metrics.
- **What evidence would resolve it**: Evaluation on a curated benchmark of 50+ logic puzzles with varying complexity, reporting success rate, types of failures, and comparison to LLM-only baselines like the Ishay et al. approach.

### Open Question 3
- **Question**: Can automated synonym resolution be integrated to prevent information loss when constraint descriptions use different terminology than extracted predicates?
- **Basis in paper**: [explicit] "This is necessary as our system is currently not handling synonyms. If a line uses differing language, this could cause information to be lost."
- **Why unresolved**: The current system relies on the LLM consistently using the same keywords across all simplified constraints, but no synonym handling mechanism exists.
- **What evidence would resolve it**: Implement a synonym mapping module (e.g., using WordNet or embedding-based matching) and test whether it improves constraint generation accuracy when puzzles use varied vocabulary.

## Limitations

- **AMR dependency**: The system's accuracy hinges on amrlib's AMR parser correctly capturing polarity and node structure. No error correction mechanism exists for parser failures.
- **Restricted spatial coverage**: Only hardcoded knowledge modules handle adjacency and ordinal concepts. Novel spatial relations break the pipeline.
- **Multi-constraint dependencies**: Cross-constraint interactions aren't handled; only single-sentence constraints are processed.

## Confidence

- **High confidence**: The hybrid architecture design (LLM for extraction, AMR for structured generation) and the separation of concerns to minimize hallucination are well-supported by both paper evidence and related work.
- **Medium confidence**: The system's ability to generate correct solutions for the two tested puzzles, given the undocumented AMR-to-ASP mapping details and knowledge module implementations.
- **Low confidence**: Claims about extensibility to more complex puzzle types without substantial architectural changes.

## Next Checks

1. **AMR parser stress test**: Run the pipeline on puzzles with known challenging constructs (negations, ordinals, "not") and verify AMR output accuracy.
2. **Cross-puzzle generalization**: Test the system on a logic puzzle with a spatial relation outside current knowledge modules (e.g., "diagonally opposite") to identify breaking points.
3. **Solver output validation**: For each generated ASP program, verify that Clingo's output matches expected solutions and that multiple solutions aren't returned due to missing constraints.