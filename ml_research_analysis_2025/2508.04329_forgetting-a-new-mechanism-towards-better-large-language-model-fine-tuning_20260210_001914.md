---
ver: rpa2
title: 'Forgetting: A New Mechanism Towards Better Large Language Model Fine-tuning'
arxiv_id: '2508.04329'
source_url: https://arxiv.org/abs/2508.04329
tags:
- arxiv
- forgetting
- data
- tokens
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a new fine-tuning approach called "forgetting"
  to improve large language model performance by explicitly distinguishing between
  informative and misleading tokens in the training data. The method categorizes tokens
  as positive (informative) or negative (misleading) using cross-model influence scores,
  then applies standard supervised fine-tuning to positive tokens while actively forgetting
  negative ones through gradient ascent.
---

# Forgetting: A New Mechanism Towards Better Large Language Model Fine-tuning

## Quick Facts
- **arXiv ID:** 2508.04329
- **Source URL:** https://arxiv.org/abs/2508.04329
- **Reference count:** 40
- **Primary result:** Token-level forgetting mechanism improves SFT performance by 4.49-8.25% across model scales

## Executive Summary
This paper introduces a novel fine-tuning approach called "forgetting" that improves large language model performance by explicitly distinguishing between informative and misleading tokens in the training data. The method uses cross-model influence scores to categorize tokens as positive (informative) or negative (misleading), then applies standard supervised fine-tuning to positive tokens while actively forgetting negative ones through gradient ascent. Experiments across multiple model architectures and five benchmarks demonstrate consistent improvements over standard supervised fine-tuning and ignoring baselines, with token-level granularity proving superior to sequence-level selection.

## Method Summary
The forgetting mechanism operates through a dual-objective training framework that first classifies tokens using influence scores computed from a reference model, then fine-tunes the base model with gradient ascent on negative tokens. The approach begins by training a reference model on a subset of data, then computes token-level loss differences between the base and reference models to identify informative versus misleading tokens. During fine-tuning, the model maximizes likelihood for positive tokens while actively increasing loss for negative tokens through a carefully scaled lambda parameter. The method employs LoRA adapters for efficient parameter updates and demonstrates robustness across different model scales and hyperparameter settings.

## Key Results
- Token-level forgetting improves performance by 4.49-8.25% compared to standard SFT across different model scales
- The approach shows consistent improvements across five benchmarks (TruthfulQA, BoolQ, LogiQA, TydiQA, ASDiv)
- Token-level selection proves superior to sequence-level selection with 70% threshold providing optimal balance
- The method demonstrates robustness across various hyperparameter settings and model architectures (1B to 13B parameters)

## Why This Works (Mechanism)
The forgetting mechanism works by explicitly addressing the presence of misleading tokens in training data that can degrade model performance during fine-tuning. By computing influence scores that measure how much each token improves or degrades model performance, the approach can selectively reinforce informative content while actively removing harmful patterns. The gradient ascent component ensures that negative tokens are not just ignored but actively "forgotten" by pushing their likelihood down, preventing them from interfering with the learning of positive tokens. This dual-objective approach creates a more focused training signal that improves overall model quality.

## Foundational Learning

**Cross-model influence scores:** Measures the difference in loss between base and reference models for each token. Needed to quantify how much each token contributes to model improvement. Quick check: Verify that positive tokens consistently have lower loss in reference model than base model.

**Token-level vs sequence-level selection:** Granular token classification allows more precise identification of misleading content within otherwise useful sequences. Needed because sequence-level methods can miss problematic tokens in otherwise good examples. Quick check: Compare performance when applying 70% threshold at token vs sequence level.

**Gradient ascent for forgetting:** Actively increases loss for negative tokens rather than simply ignoring them. Needed to ensure harmful patterns are explicitly removed rather than passively overlooked. Quick check: Monitor gradient norms to ensure they don't explode during forgetting phase.

**Dual-objective training:** Combines standard likelihood maximization with active forgetting through a weighted combination. Needed to balance learning from good tokens while removing bad ones. Quick check: Verify that loss for positive tokens decreases while loss for negative tokens increases.

## Architecture Onboarding

**Component map:** Base Model → Reference Model (trained on $D_{ref}$) → Token Scoring (influence scores) → Dual-Objective Fine-tuning (LoRA) → Improved Model

**Critical path:** The most critical sequence is the influence score computation followed by the dual-objective training. The quality of token classification directly determines the effectiveness of forgetting, and the gradient ascent component must be carefully tuned to avoid instability.

**Design tradeoffs:** The method trades increased computational complexity (computing token-level influence scores, dual objectives) for improved performance. The 70% threshold represents a balance between being too aggressive (forgetting useful information) and too conservative (retaining misleading tokens).

**Failure signatures:** Model divergence indicates $\lambda$ is too high or initialized incorrectly. Performance degradation suggests incorrect influence score computation or threshold setting. Training instability points to issues with gradient ascent implementation.

**First experiments:** 1) Verify influence score calculation by checking that positive tokens have Reference loss < Base loss. 2) Test gradient ascent stability with small $\lambda$ values before scaling up. 3) Compare token-level vs sequence-level selection performance on a small dataset.

## Open Questions the Paper Calls Out
None

## Limitations
- The reference dataset size is unspecified, creating uncertainty about influence score stability and token classification quality
- Mixed dataset formats require careful preprocessing that is not fully detailed, potentially affecting reproducibility
- Gradient ascent on negative tokens introduces instability risks that require careful hyperparameter tuning

## Confidence

- **High Confidence:** Token-level influence score concept and dual-objective training formulation are clearly specified and reproducible
- **Medium Confidence:** Experimental results show consistent improvements, but implementation details for data preprocessing and reference model training contain ambiguities
- **Low Confidence:** Robustness claims across hyperparameter settings are based on ablation studies without validation of real-world generalizability

## Next Checks
1. **Implement controlled experiments comparing influence score calculations with different reference dataset sizes (5k vs 20k samples) to empirically determine minimum required size for stable token classification and verify 70th percentile threshold effectiveness.**

2. **Create validation framework that monitors gradient norms during forgetting training and implements adaptive $\lambda$ scaling based on gradient stability metrics to ensure gradient ascent doesn't cause model divergence while maintaining forgetting effect.**

3. **Replicate token-level vs sequence-level selection comparison using exact same datasets and models to verify reported 4.49-8.25% performance gains are attributable to token-level granularity rather than implementation artifacts.**