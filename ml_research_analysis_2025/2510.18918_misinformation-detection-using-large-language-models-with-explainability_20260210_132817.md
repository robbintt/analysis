---
ver: rpa2
title: Misinformation Detection using Large Language Models with Explainability
arxiv_id: '2510.18918'
source_url: https://arxiv.org/abs/2510.18918
tags:
- misinformation
- fake
- detection
- news
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of misinformation detection on
  online platforms, which undermines trust and informed decision-making. The core
  method involves an explainable and computationally efficient pipeline using transformer-based
  pretrained language models (PLMs) like RoBERTa and DistilBERT.
---

# Misinformation Detection using Large Language Models with Explainability

## Quick Facts
- arXiv ID: 2510.18918
- Source URL: https://arxiv.org/abs/2510.18918
- Reference count: 39
- Primary result: DistilBERT achieves 97.7% accuracy on COVID Fake News dataset with ~397s/epoch training time and ~13.9ms/sample latency

## Executive Summary
This paper presents an explainable pipeline for misinformation detection using transformer-based pretrained language models. The approach employs a two-phase training strategy with DistilBERT and RoBERTa, achieving accuracy comparable to larger models while significantly reducing computational requirements. The method integrates LIME for token-level local explanations and SHAP for global feature attribution, providing transparency without degrading performance. The pipeline demonstrates strong results on both COVID Fake News and FakeNewsNet GossipCop datasets while maintaining real-time efficiency.

## Method Summary
The pipeline uses a two-step fine-tuning approach: first freezing the backbone and training only the classification head, then progressively unfreezing layers with layer-wise learning rate decay (LLRD). Preprocessing includes removing URLs, special characters, emojis, and HTML tags, followed by lowercasing and tokenization. The classification head uses dropout and dense layers with sigmoid activation. LIME provides token-level local explanations while SHAP generates global feature attributions. The method achieves strong performance on both English-language misinformation datasets while maintaining computational efficiency.

## Key Results
- DistilBERT achieves 97.7% accuracy on COVID Fake News dataset with training time of ~397s/epoch
- DistilBERT latency of ~13.9ms/sample and throughput of ~71.8 samples/s outperforms RoBERTa (30.7ms latency, 32.5 samples/s)
- Strong performance on FakeNewsNet GossipCop dataset with 85.8% accuracy
- Outperforms traditional baselines (TF-IDF + Logistic Regression, Word2Vec + SVM, BiLSTM) in both accuracy and efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A two-phase training curriculum stabilizes adaptation and mitigates catastrophic forgetting when fine-tuning transformer-based PLMs for misinformation detection.
- Mechanism: Phase 1 freezes the pretrained backbone and trains only a task-specific classification head, preserving learned linguistic representations. Phase 2 progressively unfreezes backbone layers with layer-wise learning rate decay, assigning smaller learning rates to lower layers and larger rates to higher layers. This curriculum allows gradual task adaptation without overwriting general language knowledge.
- Core assumption: The pretrained backbone already encodes generalizable linguistic patterns relevant to misinformation detection; only task-specific refinement is needed.
- Evidence anchors: [abstract] "first, we freeze the backbone and train only the classification head; then, we progressively unfreeze the backbone layers while applying layer-wise learning rate decay."

### Mechanism 2
- Claim: Lightweight PLMs (DistilBERT) can achieve accuracy comparable to larger models (RoBERTa) with substantially lower computational cost, enabling real-time or resource-constrained deployment.
- Mechanism: Knowledge distillation during DistilBERT's pretraining compresses RoBERTa's representational capacity into fewer parameters while preserving task-relevant features. For misinformation classification, the compressed model captures sufficient semantic nuance for the task, achieving similar discriminative performance with ~66M parameters versus ~125M and ~2.2× faster inference throughput.
- Core assumption: The misinformation detection task does not require the full representational capacity of larger models; linguistic cues are accessible to compressed architectures.
- Evidence anchors: [abstract] "DistilBERT achieves accuracy comparable to RoBERTa while requiring significantly fewer computational resources."

### Mechanism 3
- Claim: Post-hoc explainability methods (LIME, SHAP) can provide faithful local and global justifications without degrading predictive performance.
- Mechanism: LIME perturbs individual inputs and fits a local surrogate model to identify token-level contributions to predictions. SHAP computes Shapley values across the corpus to quantify global feature importance. Both operate on trained model outputs, not gradients, ensuring no interference with the training objective.
- Core assumption: Token-level and feature-level attributions meaningfully correspond to human-interpretable rationale for misinformation decisions.
- Evidence anchors: [abstract] "integrate the Local Interpretable Model-Agnostic Explanations (LIME) at the token level... and SHapley Additive exPlanations (SHAP) at the global feature attribution level."

## Foundational Learning

- Concept: **Transfer learning with pretrained language models**
  - Why needed here: The pipeline relies on adapting general-purpose PLMs (RoBERTa, DistilBERT) to a specific downstream task (misinformation classification) through fine-tuning.
  - Quick check question: Can you explain why freezing backbone layers in Phase 1 preserves pretrained knowledge while still allowing task adaptation through the classification head?

- Concept: **Layer-wise learning rate decay (LLRD)**
  - Why needed here: LLRD is the core technique enabling stable fine-tuning by assigning different learning rates to different transformer layers based on their distance from input embeddings.
  - Quick check question: Why would lower layers (closer to input) benefit from smaller learning rates compared to higher layers during fine-tuning?

- Concept: **Post-hoc interpretability methods (LIME/SHAP)**
  - Why needed here: The pipeline integrates these methods to generate human-readable explanations; understanding their assumptions (local surrogates, Shapley values) is essential for interpreting outputs correctly.
  - Quick check question: What is the key difference between LIME's local explanations and SHAP's global attributions in terms of what they explain and how they compute importance?

## Architecture Onboarding

- Component map:
  Preprocessing module -> Transformer encoder backbone (RoBERTa-base or DistilBERT-base) -> Classification head (Dropout -> Dense (ReLU) -> Dense -> Softmax) -> Explainability layer (LIME + SHAP, post-hoc)

- Critical path:
  1. Preprocess raw text (remove URLs, special characters, lowercase normalization)
  2. Tokenize with model-specific tokenizer (max sequence length: 128 or 512)
  3. Phase 1 training: Freeze encoder, train only classification head (3–5 epochs, early stopping on validation F1)
  4. Phase 2 training: Unfreeze all layers, apply LLRD (decay factor 0.9–0.95 per layer), continue training
  5. Evaluate on held-out test set; generate LIME/SHAP explanations for predictions

- Design tradeoffs:
  - DistilBERT vs. RoBERTa: DistilBERT offers ~2.2× faster inference with ~1–2% accuracy drop (task-dependent); choose based on latency constraints
  - Two-phase vs. end-to-end fine-tuning: Two-phase adds training complexity but stabilizes convergence for small datasets
  - LIME vs. SHAP: LIME provides instance-level rationales; SHAP provides corpus-level patterns—both add computational overhead at inference time

- Failure signatures:
  - Phase 1 head training shows no improvement: Check label balance, learning rate, or whether backbone embeddings are task-appropriate
  - Phase 2 causes performance collapse: Reduce LLRD aggressiveness or unfreeze fewer layers initially
  - LIME/SHAP explanations appear random or inconsistent: Verify model is not severely overfitting; check that perturbation settings match input characteristics

- First 3 experiments:
  1. **Baseline comparison**: Train TF-IDF + Logistic Regression and BiLSTM baselines on the same splits to establish performance benchmarks before PLM fine-tuning
  2. **Ablation on training phases**: Compare (a) head-only training, (b) full fine-tuning without LLRD, and (c) two-phase with LLRD to isolate the contribution of each component
  3. **Efficiency-accuracy frontier**: Run DistilBERT and RoBERTa with identical hyperparameters on both datasets (COVID Fake News, GossipCop), recording accuracy, latency, and throughput to quantify the trade-off space for deployment decisions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does the proposed DistilBERT pipeline resist adversarial manipulation attempts compared to standard baselines?
- Basis in paper: [explicit] The Conclusion states future work includes "adding adversarial robustness testing to resist manipulation attempts."
- Why unresolved: The current evaluation relies on static benchmark datasets and does not test the model's resilience against paraphrased attacks or perturbations designed to evade detection.
- What evidence would resolve it: Performance metrics (F1-score, Accuracy) on adversarial datasets generated by attacks like TextFooler or synonym substitution, showing the degradation rate relative to standard testing.

### Open Question 2
- Question: Can the architecture maintain its efficiency-accuracy trade-off when expanded to multilingual and cross-lingual misinformation detection?
- Basis in paper: [explicit] The authors explicitly propose "expanding the pipeline to multilingual and cross-lingual misinformation detection" in the future scope.
- Why unresolved: The study is restricted to English-only datasets, leaving the model's ability to generalize across different linguistic structures and low-resource languages unverified.
- What evidence would resolve it: Benchmark results on multilingual datasets (e.g., Poly-FEVER) comparing the inference latency and F1-scores of a multilingual DistilBERT version against monolingual baselines.

### Open Question 3
- Question: Do the efficiency gains of the lightweight PLM persist when benchmarked against modern generative decoder-only models like Llama or Mistral?
- Basis in paper: [explicit] The Conclusion suggests "benchmarking to more powerful LLMs, like Llama, Mistral and Gemma, to assess the scalability."
- Why unresolved: The paper compares the PLM primarily against traditional encoders and older deep learning models (BiLSTM), but not against the current state-of-the-art generative architectures that may offer different performance/efficiency dynamics.
- What evidence would resolve it: A comparative analysis of throughput (samples/s), memory footprint, and detection accuracy between the fine-tuned DistilBERT and quantized versions of Llama-3 or Mistral on the same misinformation tasks.

## Limitations

- The paper does not provide empirical evidence (training curves or ablation studies) proving that the two-phase training curriculum with LLRD is necessary over simpler fine-tuning approaches.
- Explainability methods (LIME/SHAP) are integrated but not validated for faithfulness through human evaluation or perturbation tests to confirm alignment with ground truth reasoning.
- The pipeline is only evaluated on two English-language datasets, leaving generalization to different domains and multilingual settings unverified.

## Confidence

- **High confidence**: Claims about DistilBERT achieving comparable accuracy to RoBERTa with lower computational cost are supported by direct quantitative comparisons in the results tables.
- **Medium confidence**: The two-phase training curriculum with LLRD is described clearly, but lacks empirical evidence proving its necessity over simpler fine-tuning approaches.
- **Low confidence**: Claims about LIME/SHAP explanations being "faithful" are not substantiated with validation methods; the paper only states they are post-hoc and do not degrade performance.

## Next Checks

1. **Ablation study on training phases**: Compare three fine-tuning strategies—(a) head-only training, (b) full fine-tuning without LLRD, and (c) two-phase with LLRD—to isolate the contribution of each component to final performance and stability.

2. **Explanation faithfulness validation**: Perform perturbation-based tests where important tokens identified by LIME are masked; verify whether model predictions change as expected. Optionally, conduct human evaluation to assess whether explanations align with intuitive reasoning.

3. **Cross-domain generalization test**: Evaluate the pipeline on a third misinformation dataset from a different domain (e.g., political news or scientific claims) to assess robustness to linguistic and topical variation.