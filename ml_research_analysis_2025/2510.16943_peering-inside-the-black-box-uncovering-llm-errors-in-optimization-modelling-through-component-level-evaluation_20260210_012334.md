---
ver: rpa2
title: 'Peering Inside the Black Box: Uncovering LLM Errors in Optimization Modelling
  through Component-Level Evaluation'
arxiv_id: '2510.16943'
source_url: https://arxiv.org/abs/2510.16943
tags:
- optimization
- problem
- constraints
- metrics
- constraint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a component-level evaluation framework for
  LLM-generated optimization formulations. It evaluates decision variables, constraints,
  and objectives separately, using metrics like precision, recall, RMSE, and solver-level
  optimality gap.
---

# Peering Inside the Black Box: Uncovering LLM Errors in Optimization Modelling through Component-Level Evaluation

## Quick Facts
- arXiv ID: 2510.16943
- Source URL: https://arxiv.org/abs/2510.16943
- Authors: Dania Refai; Moataz Ahmed
- Reference count: 40
- Primary result: Component-level evaluation framework reveals that solver performance depends primarily on constraint recall and RMSE, with concise outputs improving efficiency

## Executive Summary
This paper introduces a novel component-level evaluation framework for assessing LLM-generated optimization formulations. The framework evaluates decision variables, constraints, and objectives separately using precision, recall, RMSE, and solver-level optimality gap metrics. Tested across four optimization problems with varying difficulty levels using six prompting strategies across three LLMs, the study reveals critical insights about where and why LLM errors occur in optimization modeling.

The research demonstrates that complete constraint coverage is essential to prevent violations, constraint RMSE directly impacts solver-level accuracy, and concise model outputs enhance computational efficiency. GPT-5 consistently outperforms other models, particularly when using chain-of-thought and modular prompting strategies. The findings provide actionable guidance for improving LLM-generated optimization models and highlight the importance of component-specific evaluation rather than treating optimization formulations as monolithic outputs.

## Method Summary
The study employs a component-level evaluation framework that decomposes optimization formulations into three core elements: decision variables, constraints, and objectives. For each component, the researchers use precision, recall, and RMSE metrics to quantify accuracy. Solver-level optimality gap is measured by comparing generated solutions against ground truth solutions obtained from commercial solvers. The framework is tested across four optimization problems ranging from easy to hard difficulty levels, using six different prompting strategies (zero-shot, few-shot, chain-of-thought, modular, concise, and hybrid) across three LLMs: GPT-5, LLaMA 3.1, and DeepSeek.

## Key Results
- Solver performance is primarily determined by constraint recall and RMSE metrics
- GPT-5 outperforms LLaMA 3.1 and DeepSeek, especially with chain-of-thought and modular prompting
- Concise model outputs improve computational efficiency without sacrificing accuracy
- Complete constraint coverage is essential to prevent solution violations
- Constraint RMSE directly correlates with solver-level optimality gap

## Why This Works (Mechanism)
The component-level evaluation approach works because optimization modeling errors often occur in specific, identifiable components rather than uniformly across the entire formulation. By isolating decision variables, constraints, and objectives, the framework can pinpoint exactly where LLMs struggle and provide targeted feedback for improvement. This granular analysis reveals that constraint generation is the most error-prone component, with missing or incorrect constraints leading to infeasible or suboptimal solutions that propagate through the entire optimization process.

## Foundational Learning

**Constraint Completeness**: Understanding that missing constraints in optimization models can lead to fundamentally flawed solutions is critical for evaluating LLM performance. Quick check: Compare constraint counts between LLM output and ground truth.

**Component Isolation**: Breaking down optimization formulations into discrete components allows for targeted error analysis and improvement strategies. Quick check: Evaluate each component's precision and recall independently.

**Solver Integration**: Connecting LLM outputs to actual solver performance provides ground truth for evaluating model quality beyond syntactic correctness. Quick check: Measure optimality gap between LLM-generated and solver-optimal solutions.

**Prompt Strategy Impact**: Different prompting approaches significantly affect the quality and structure of generated optimization models. Quick check: Compare component metrics across multiple prompting strategies.

## Architecture Onboarding

Component Map: Decision Variables -> Constraints -> Objectives -> Solver Integration

Critical Path: The evaluation flow moves from component extraction (variables, constraints, objectives) through metric calculation (precision, recall, RMSE) to solver performance assessment, with constraint quality being the most influential factor on final solution quality.

Design Tradeoffs: The framework balances granularity (component-level evaluation) against computational overhead (running solver for each generated model). The tradeoff favors detailed analysis despite increased evaluation time.

Failure Signatures: Constraint recall failures indicate incomplete problem understanding, high constraint RMSE suggests numerical or formulation errors, and objective formulation errors typically result in suboptimal but feasible solutions.

First Experiments:
1. Run component-level evaluation on a simple linear programming problem to verify metric calculations
2. Test different prompting strategies on a single problem to observe component-specific effects
3. Compare solver performance between complete and incomplete constraint sets

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation covers only four optimization problems, potentially limiting generalizability
- Small sample size of three LLMs tested across six prompting strategies
- Framework focuses on three core components without exploring additional formulation elements
- Does not systematically isolate verbosity as an independent variable affecting efficiency

## Confidence

| Claim | Confidence |
|-------|------------|
| Solver performance depends primarily on constraint recall and RMSE | High |
| GPT-5 outperforms other models with chain-of-thought and modular prompting | Medium |
| Concise outputs improve efficiency | Medium |
| Complete constraint coverage prevents violations | High |

## Next Checks
1. Test the framework across a larger and more diverse set of optimization problems, including real-world industrial cases with hundreds of constraints and variables
2. Evaluate additional state-of-the-art LLMs including Claude, Gemini, and specialized mathematical models to assess generalizability of the findings
3. Investigate alternative evaluation metrics beyond the current precision/recall/RMSE framework, such as constraint complexity measures and solution robustness across multiple runs