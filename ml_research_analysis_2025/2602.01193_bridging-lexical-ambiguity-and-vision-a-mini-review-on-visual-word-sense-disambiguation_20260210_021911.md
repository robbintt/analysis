---
ver: rpa2
title: 'Bridging Lexical Ambiguity and Vision: A Mini Review on Visual Word Sense
  Disambiguation'
arxiv_id: '2602.01193'
source_url: https://arxiv.org/abs/2602.01193
tags:
- disambiguation
- visual
- word
- sense
- vwsd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a systematic review of Visual Word Sense Disambiguation
  (VWSD), which extends traditional Word Sense Disambiguation to multimodal contexts
  using vision-language models like CLIP. It examines methods ranging from early multimodal
  fusion and graph-based techniques to recent approaches leveraging CLIP fine-tuning,
  LLM-generated glosses, and diffusion-based text-to-image generation.
---

# Bridging Lexical Ambiguity and Vision: A Mini Review on Visual Word Sense Disambiguation

## Quick Facts
- **arXiv ID**: 2602.01193
- **Source URL**: https://arxiv.org/abs/2602.01193
- **Reference count**: 23
- **Primary result**: Recent CLIP-based fine-tuning and LLM-augmented approaches outperform zero-shot baselines by 6-8% MRR on the SemEval-2023 Task 1 VWSD benchmark.

## Executive Summary
This mini review examines Visual Word Sense Disambiguation (VWSD), which extends traditional Word Sense Disambiguation to multimodal contexts using vision-language models. The authors systematically categorize methods into early multimodal fusion, graph-based techniques, and recent LLM-enhanced approaches. Key findings indicate that combining contrastive vision-language alignment with LLM context generation consistently improves performance over zero-shot CLIP baselines. The review identifies persistent challenges including limited context, dataset bias, multilingual coverage gaps, and evaluation framework limitations.

## Method Summary
The paper surveys VWSD methods across three main approaches. First, CLIP zero-shot baselines compute cosine similarity between text embeddings (formatted as "A photo of [word] in [context]") and candidate image embeddings. Second, CLIP/BLIP fine-tuning applies contrastive loss on VWSD datasets to learn sense-specific visual representations. Third, LLM-augmented methods use GPT-3/4 for context expansion, gloss generation, and chain-of-thought reasoning. All approaches are evaluated on the SemEval-2023 Task 1 benchmark using Mean Reciprocal Rank (MRR) and Hit Rate at 1 (HIT@1) metrics.

## Key Results
- LLM-enhanced and CLIP fine-tuned models consistently outperform zero-shot baselines, achieving 6-8% MRR improvements
- Performance gains are most pronounced on examples with minimal context (1-2 words)
- Top-performing systems (CADG, ARPA) combine contrastive alignment with generative model reasoning
- Models show bias toward common word meanings and text-in-image artifacts

## Why This Works (Mechanism)
The integration of LLM reasoning with contrastive vision-language models works by expanding minimal textual context into richer semantic representations while maintaining visual-grounded embeddings. CLIP's cross-modal alignment provides robust image-text similarity matching, while LLMs generate contextually appropriate glosses that disambiguate polysemous terms. The fine-tuning process adapts pre-trained models to VWSD's specific requirements, learning to distinguish fine-grained visual senses rather than general text-image similarity.

## Foundational Learning
- **Mean Reciprocal Rank (MRR)**: Evaluates ranking quality by averaging reciprocal ranks of correct answers; needed because VWSD requires selecting the best image from multiple candidates, quick check: higher MRR indicates better overall ranking performance
- **Cross-modal Contrastive Learning**: Trains models to align text and image embeddings in shared space; needed for measuring semantic similarity between words and visual concepts, quick check: cosine similarity scores between matched pairs should be higher than mismatched pairs
- **LLM Context Augmentation**: Uses language models to generate definitions or expanded context for ambiguous terms; needed to provide sufficient textual information for disambiguation when original context is minimal, quick check: augmented context should include explicit sense definitions relevant to the target word
- **Fine-tuning with Contrastive Loss**: Adapts pre-trained vision-language models to domain-specific tasks using pairwise similarity optimization; needed to overcome bias toward common meanings and improve sense discrimination, quick check: model should show improved accuracy on rare or context-specific senses
- **Chain-of-Thought Reasoning**: LLM technique that breaks down reasoning into explicit steps; needed for complex disambiguation requiring multiple inference steps, quick check: intermediate reasoning steps should logically connect context to correct visual interpretation

## Architecture Onboarding
- **Component Map**: Text Prompt → Vision-Language Model (CLIP/BLIP) → Image Embeddings → Cosine Similarity → Ranking → Output Selection
- **Critical Path**: Ambiguous word + context → LLM-generated gloss → CLIP encoding → Image similarity computation → MRR/HIT@1 evaluation
- **Design Tradeoffs**: Zero-shot methods offer simplicity but limited performance; fine-tuning requires more data and compute but achieves better accuracy; LLM augmentation improves context handling but introduces potential hallucination errors
- **Failure Signatures**: Poor performance on low-context inputs, bias toward common word meanings, text-in-image artifact preference, hallucination in LLM-generated glosses
- **Three First Experiments**:
  1. Implement CLIP zero-shot baseline with basic cosine similarity scoring
  2. Compare multiple prompt engineering strategies for context expansion
  3. Test contrastive fine-tuning on a subset of training data

## Open Questions the Paper Calls Out
- **Open Question 1**: How can VWSD models achieve robust performance in low-resource languages without relying on error-prone translation pipelines or extensive annotated datasets? The paper notes that low-resource languages face greater difficulties due to lack of visual resources and that current approaches relying on translation add noise and distort meaning.
- **Open Question 2**: What mechanisms can effectively mitigate hallucinations and error propagation when integrating LLM context expansion into VWSD pipelines? Section 5.3 highlights ongoing hallucination problems where LLMs generate plausible but incorrect definitions, and pipeline-based architectures cascade mistakes from early stages.
- **Open Question 3**: How can contrastive vision-language architectures be refined to distinguish fine-grained senses rather than prioritizing common word meanings or text-in-image artifacts? The paper states that current models like CLIP have a bias to prioritize common word meanings and favor images containing textual representations of the target word.

## Limitations
- Model-specific hyperparameters for fine-tuning (learning rate, batch size, optimizer settings) are not provided
- Exact prompt templates and LLM prompting strategies used by top-performing systems are not fully specified
- Dataset coverage is limited to three languages (English, Italian, Farsi) and may not represent broader linguistic diversity
- Claims about multilingual capabilities are based on limited evaluation framework

## Confidence
- **High confidence**: Core findings that CLIP-based fine-tuning and LLM augmentation consistently improve over zero-shot baselines; MRR and HIT@1 are appropriate evaluation metrics for VWSD
- **Medium confidence**: Quantitative performance gains (6-8% MRR improvement) - specific numbers depend on unreported experimental conditions
- **Low confidence**: Claims about multilingual capabilities and handling of cross-lingual ambiguity - the dataset and evaluation framework are limited

## Next Checks
1. **Replicate zero-shot CLIP baseline**: Implement basic cosine similarity scoring on SemEval-2023 dataset using available CLIP models; verify baseline MRR/HIT@1 scores reported in the literature
2. **Test prompt engineering variations**: Systematically compare different text prompt templates (e.g., "A photo of [word]" vs. "An image showing [word] in context of [context]") to quantify impact on disambiguation accuracy
3. **Evaluate on extended dataset**: Create or identify a multilingual VWSD benchmark that includes languages beyond English, Italian, and Farsi to test the generalizability of reported methods