---
ver: rpa2
title: 'Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading'
arxiv_id: '2504.11816'
source_url: https://arxiv.org/abs/2504.11816
tags:
- cache
- offloading
- cost
- instance
- xlarge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InferSave automatically selects cost-efficient VM instances for
  cloud-based LLM inference by integrating KV cache offloading strategies and Service
  Level Objective (SLO) management. It models GPU memory requirements, predicts performance
  using Compute Time Calibration Function (CTCF), and recommends instances that minimize
  cost while meeting SLO constraints.
---

# Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading

## Quick Facts
- **arXiv ID:** 2504.11816
- **Source URL:** https://arxiv.org/abs/2504.11816
- **Reference count:** 22
- **Key outcome:** InferSave achieves up to 73.7% cost reduction for online workloads and 20.19% for offline workloads through intelligent VM instance selection with KV cache offloading

## Executive Summary
InferSave is a VM selection framework for cloud-based LLM inference that automatically recommends cost-efficient GPU instances while meeting Service Level Objectives (SLOs). The system integrates KV cache offloading strategies with SLO management to optimize both performance and cost. By modeling GPU memory requirements and predicting performance using a Compute Time Calibration Function (CTCF), InferSave can select instances that minimize cost without violating latency constraints.

## Method Summary
The framework works by first estimating the KV cache size and GPU memory requirements for a given LLM workload. It then uses the CTCF model to predict inference performance across different VM configurations. For each candidate instance, InferSave evaluates whether it can meet the specified SLO while considering KV cache offloading opportunities. The system recommends the instance that achieves the lowest cost while satisfying all constraints, with separate optimization strategies for online (latency-sensitive) and offline (batch) workloads.

## Key Results
- Achieves up to 73.7% cost reduction for online workloads compared to maximum-performance-based policies
- Delivers 20.19% cost savings for offline workloads
- Evaluations conducted on AWS GPU instances demonstrate consistent performance improvements

## Why This Works (Mechanism)
InferSave leverages KV cache offloading to reduce GPU memory pressure, enabling the use of smaller, less expensive instances for workloads that would otherwise require high-memory configurations. The CTCF model provides accurate performance predictions across different instance types, allowing the system to identify cost-optimal configurations that still meet SLO requirements. By separating optimization strategies for online and offline workloads, InferSave can maximize savings in each scenario while maintaining quality of service.

## Foundational Learning
- **KV Cache Management:** Understanding how key-value cache sizes impact GPU memory utilization and when offloading becomes beneficial
  - Why needed: Critical for determining when smaller instances can be used through strategic offloading
  - Quick check: Monitor memory utilization patterns during inference to identify offloading opportunities

- **Compute Time Calibration Function (CTCF):** Performance prediction model that maps workload characteristics to expected inference times across different hardware configurations
  - Why needed: Enables accurate comparison of different VM instances for a given workload
  - Quick check: Validate CTCF predictions against actual measurements across multiple instance types

- **SLO-Aware Cost Optimization:** Balancing latency requirements against infrastructure costs through constraint-aware instance selection
  - Why needed: Ensures cost savings don't come at the expense of service quality
  - Quick check: Test edge cases where SLO constraints might conflict with cost optimization goals

## Architecture Onboarding
- **Component Map:** Workload Analyzer -> CTCF Predictor -> Instance Evaluator -> Cost Optimizer -> KV Cache Manager
- **Critical Path:** Request -> Workload Analysis -> Performance Prediction -> Instance Selection -> Execution with KV Cache Optimization
- **Design Tradeoffs:** Balancing prediction accuracy against computation overhead in CTCF model vs. potential cost savings
- **Failure Signatures:** SLO violations when CTCF predictions are inaccurate; increased costs when KV cache offloading is suboptimal
- **First Experiments:**
  1. Benchmark CTCF prediction accuracy across 5 different AWS GPU instance types
  2. Measure cost savings when switching from high-memory to smaller instances with KV cache offloading
  3. Test SLO compliance under varying workload patterns and token distributions

## Open Questions the Paper Calls Out
None

## Limitations
- Performance predictions may not generalize well across different cloud providers beyond AWS GPU instances
- CTCF model might require retraining for new GPU architectures not included in the original training data
- Cost savings may not fully translate to enterprise scenarios with different SLO constraints and workload patterns

## Confidence
- Up to 73.7% cost reduction for online workloads: **High** (directly measured and reported)
- Up to 20.19% cost reduction for offline workloads: **High** (directly measured and reported)
- Comparison against "maximum-performance-based policies": **Medium** (specific baselines not fully detailed)

## Next Checks
1. Test InferSave across multiple cloud providers (Google Cloud, Azure) to assess cross-platform performance consistency
2. Evaluate CTCF model prediction accuracy when applied to newer GPU architectures not included in original training data
3. Conduct long-term stability tests under varying workload patterns to verify sustained cost savings over extended periods