---
ver: rpa2
title: 'Large Language Models as Virtual Survey Respondents: Evaluating Sociodemographic
  Response Generation'
arxiv_id: '2509.06337'
source_url: https://arxiv.org/abs/2509.06337
tags:
- survey
- performance
- language
- llama
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates using Large Language Models (LLMs) to
  simulate virtual survey respondents for scalable and cost-effective sociodemographic
  data collection. Two novel simulation settings are proposed: Partial Attribute Simulation
  (PAS) for predicting missing attributes from incomplete profiles, and Full Attribute
  Simulation (FAS) for generating complete synthetic datasets with or without contextual
  constraints.'
---

# Large Language Models as Virtual Survey Respondents: Evaluating Sociodemographic Response Generation

## Quick Facts
- arXiv ID: 2509.06337
- Source URL: https://arxiv.org/abs/2509.06337
- Reference count: 40
- This paper investigates using Large Language Models (LLMs) to simulate virtual survey respondents for scalable and cost-effective sociodemographic data collection.

## Executive Summary
This study explores the potential of Large Language Models to serve as virtual survey respondents for generating synthetic sociodemographic data. The research introduces two novel simulation frameworks: Partial Attribute Simulation (PAS) for predicting missing attributes from incomplete profiles, and Full Attribute Simulation (FAS) for generating complete synthetic datasets with contextual constraints. Using a comprehensive benchmark suite spanning 11 real-world datasets across four domains, the study evaluates mainstream LLMs including GPT-3.5/4 Turbo and LLaMA models under zero-shot and few-shot conditions. Results demonstrate consistent performance patterns with GPT-4 Turbo generally outperforming other models, while highlighting both the promise and challenges of LLM-driven survey simulations for social research applications.

## Method Summary
The research establishes a comprehensive evaluation framework for LLM-driven survey simulations through two distinct settings. Partial Attribute Simulation (PAS) focuses on predicting missing sociodemographic attributes from incomplete respondent profiles, while Full Attribute Simulation (FAS) generates complete synthetic datasets with or without contextual constraints. A benchmark suite (LLM-S3) was curated from 11 real-world datasets across education, employment, healthcare, and lifestyle domains. The study evaluates mainstream LLMs including GPT-3.5 Turbo, GPT-4 Turbo, LLaMA 3.0-8B, and LLaMA 3.1-8B under zero-shot and few-shot conditions, with performance measured using exact match accuracy, F1-score, and failure rate metrics.

## Key Results
- GPT-4 Turbo consistently outperforms other models across both PAS and FAS settings, with superior performance in predicting missing attributes and generating context-aware responses
- Context-aware prompts and reasoning-oriented distilled models significantly improve accuracy, particularly for complex sociodemographic predictions
- LLaMA models exhibit higher failure rates in FAS, struggling with structured output generation despite showing reasonable performance in PAS scenarios

## Why This Works (Mechanism)
The effectiveness of LLMs as virtual survey respondents stems from their ability to learn and reproduce complex sociodemographic patterns from training data. By leveraging their extensive pretraining on diverse text corpora, these models can infer relationships between different demographic attributes and generate plausible responses based on partial information. The context-aware prompting mechanism enables the models to consider multiple attributes simultaneously, improving prediction accuracy for missing values. The reasoning-oriented distilled models benefit from targeted fine-tuning that enhances their ability to handle structured sociodemographic tasks while maintaining computational efficiency.

## Foundational Learning
**Large Language Model Fundamentals**: Understanding transformer architecture and attention mechanisms is essential for grasping how LLMs process and generate text-based responses. Quick check: Can you explain the difference between encoder-only, decoder-only, and encoder-decoder transformer architectures?

**Prompt Engineering**: Mastery of context-aware prompting techniques is crucial for optimizing LLM performance in structured prediction tasks. Quick check: What are the key differences between zero-shot, few-shot, and chain-of-thought prompting strategies?

**Sociodemographic Data Characteristics**: Knowledge of categorical variable distributions and attribute relationships in survey data is necessary for evaluating synthetic generation quality. Quick check: How do missing data patterns typically differ between education, employment, healthcare, and lifestyle survey domains?

## Architecture Onboarding

**Component Map**: Data Preparation -> Prompt Engineering -> LLM Inference -> Response Evaluation -> Benchmark Suite

**Critical Path**: The evaluation pipeline follows a sequential flow from dataset curation through response generation to performance measurement, with prompt engineering serving as the key optimization point that directly impacts LLM inference quality.

**Design Tradeoffs**: Zero-shot approaches offer greater scalability and generalization but sacrifice precision compared to few-shot methods that require additional training examples. Context-aware prompting improves accuracy but increases computational overhead and prompt complexity.

**Failure Signatures**: LLaMA models show higher failure rates in FAS due to structured output generation challenges, while all models struggle with minority attribute combinations that are underrepresented in training data. Context loss occurs when prompts become too complex or when attribute relationships are non-linear.

**Three First Experiments**:
1. Compare zero-shot versus few-shot performance across all models on the PAS task using identical evaluation metrics
2. Test context-aware prompting variations (attribute count, ordering, and specificity) on model accuracy for complex sociodemographic predictions
3. Evaluate failure rate patterns across different attribute combinations to identify systematic weaknesses in LLM response generation

## Open Questions the Paper Calls Out
None

## Limitations
- Exclusive reliance on publicly available Kaggle datasets may not capture the full complexity and diversity of real-world survey data
- Evaluation framework focuses primarily on categorical sociodemographic attributes, limiting generalizability to continuous or ordinal variables
- Higher failure rates observed in structured output generation for LLaMA models, particularly in Full Attribute Simulation scenarios

## Confidence
- **High confidence** in observed performance trends across LLM models for Partial Attribute Simulation (PAS)
- **Medium confidence** in Full Attribute Simulation (FAS) results due to documented challenges with structured output generation
- **Medium confidence** in context-aware prompting effectiveness, as improvements are demonstrated but may be dataset-dependent

## Next Checks
1. Validate model performance on proprietary or field-collected survey datasets that include complex question formats, skip patterns, and real-world response distributions not captured in Kaggle datasets

2. Test model generalization by evaluating zero-shot performance on demographic groups and cultural contexts absent from the training data, particularly focusing on minority populations and non-Western contexts

3. Conduct comparative analysis between synthetic and human-generated responses using external validity measures, including convergent validity with existing survey instruments and predictive validity for real-world outcomes