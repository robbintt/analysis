---
ver: rpa2
title: Robust Deterministic Policy Gradient for Disturbance Attenuation and Its Application
  to Quadrotor Control
arxiv_id: '2502.21057'
source_url: https://arxiv.org/abs/2502.21057
tags:
- control
- policy
- learning
- robust
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces RDDPG, a reinforcement learning method for\
  \ robust control under disturbances by framing the H\u221E control problem as a\
  \ two-player zero-sum game between a user (controller) and an adversary (disturbance\
  \ generator). The method extends deterministic policy gradients with adversarial\
  \ training, where the user learns to minimize cost while the adversary maximizes\
  \ it by injecting disturbances."
---

# Robust Deterministic Policy Gradient for Disturbance Attenuation and Its Application to Quadrotor Control

## Quick Facts
- arXiv ID: 2502.21057
- Source URL: https://arxiv.org/abs/2502.21057
- Reference count: 6
- Primary result: RDDPG achieves lower cost and smallest variance under disturbances compared to TD3, RARL, PR-MDP, and NR-MDP for quadrotor trajectory tracking.

## Executive Summary
This paper introduces RDDPG, a reinforcement learning method for robust control under disturbances by framing the H∞ control problem as a two-player zero-sum game between a user (controller) and an adversary (disturbance generator). The method extends deterministic policy gradients with adversarial training, where the user learns to minimize cost while the adversary maximizes it by injecting disturbances. A practical variant, RDDPG, integrates twin-delayed updates for stability. Experiments on quadrotor trajectory tracking show RDDPG achieves lower cost and smallest variance compared to TD3, RARL, PR-MDP, and NR-MDP under random disturbances, demonstrating superior robustness and stable control performance.

## Method Summary
The paper proposes Robust Deterministic Deep Policy Gradient (RDDPG), a reinforcement learning method that solves the H∞ control problem by formulating it as a two-player zero-sum dynamic game. The user (controller) learns to minimize a cost function while the adversary (disturbance generator) learns to maximize it by injecting external forces. The method extends deterministic policy gradients with adversarial training, where both agents are trained simultaneously using gradient descent (user) and gradient ascent (adversary). The practical variant, RDDPG, incorporates twin-delayed updates from TD3 to improve stability. The method is evaluated on quadrotor trajectory tracking in simulation, demonstrating superior robustness against random disturbances compared to baseline methods.

## Key Results
- RDDPG achieves lower cumulative cost and smallest variance under random disturbances compared to TD3, RARL, PR-MDP, and NR-MDP.
- The method maintains stable control performance even under challenging disturbance conditions.
- RDDPG demonstrates superior robustness in quadrotor trajectory tracking tasks.

## Why This Works (Mechanism)
RDDPG works by training both the controller and adversary simultaneously in a min-max game framework. The controller learns to minimize the expected cost while the adversary learns to generate worst-case disturbances. This adversarial training process creates a policy that is inherently robust to disturbances because it has been trained against an opponent specifically designed to break it. The twin-delayed updates from TD3 help stabilize the learning process by reducing overestimation bias and providing more stable critic estimates.

## Foundational Learning
- **H∞ Control Theory**: Why needed - provides the mathematical foundation for robust control against worst-case disturbances. Quick check - verify understanding of how H∞ control differs from standard LQR.
- **Two-Player Zero-Sum Games**: Why needed - framework for modeling the adversarial relationship between controller and disturbance generator. Quick check - confirm understanding of saddle-point solutions and min-max optimization.
- **Deterministic Policy Gradients**: Why needed - enables continuous action space control for the quadrotor. Quick check - verify gradient computation through the critic network.
- **Adversarial Training**: Why needed - creates robustness by exposing the policy to worst-case scenarios during training. Quick check - confirm the gradient ascent update for the adversary.
- **Twin-Delayed Updates**: Why needed - stabilizes learning by reducing overestimation bias. Quick check - verify critic update uses minimum of two Q-functions.

## Architecture Onboarding

**Component Map**: State -> Stacked Frame Observer -> Actor (User) -> Action (Motor RPMs) -> Environment -> Cost + Disturbance -> Critic (Twin) -> Update User/Actor

**Critical Path**: State → Actor → Environment → Cost → Critic → Actor Update (repeat)

**Design Tradeoffs**:
- Deterministic vs. Stochastic Policies: Deterministic chosen for continuous control precision, but may have exploration challenges
- Twin Critics: Added complexity but reduces overestimation bias and improves stability
- Adversarial Training: Computationally expensive but provides robustness guarantees

**Failure Signatures**:
- Adversary overpowering: Episode length drops sharply, quadrotor crashes immediately
- Gradient masking: Mean disturbance magnitude approaches zero
- Critic collapse: Q-values diverge or become NaN

**First Experiments**:
1. Verify environment dynamics with simple PID controller before implementing RL
2. Test RDDPG with only user training (no adversary) to establish baseline performance
3. Gradually introduce adversary with reduced learning rate to ensure stable co-training

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the policy learned by RDDPG in simulation transfer effectively to physical quadrotor hardware without performance degradation?
- **Basis in paper**: The Introduction explicitly identifies the "sim-to-real" gap as a primary motivation, noting that "policies trained in simulation frequently suffer from performance degradation... when deployed on real systems." However, all experimental validation in Section 6 is conducted strictly within the *gym-pybullet-drones* simulation environment.
- **Why unresolved**: The paper demonstrates robustness against simulated random and adversarial disturbances, but it does not validate the method on a physical platform where unmodeled dynamics (e.g., aerodynamic effects, motor latency) exist.
- **What evidence would resolve it**: Empirical results from deploying the trained RDDPG policy on a physical quadrotor (e.g., Crazyflie 2.0) tracking trajectories under external wind disturbances.

### Open Question 2
- **Question**: How sensitive is the algorithm's performance to the choice of the disturbance attenuation parameter $\eta$?
- **Basis in paper**: Section 4.1 introduces the parameter $\eta$ in the cost function $c(x_k, u_k, w_k, x_{k+1}) = \tilde{c} - \eta^2 \|w_k\|^2$, which serves as the critical trade-off between minimizing user cost and regularizing the adversary's disturbance power. The experiments (Appendix A.3) fix $\eta = 10$ without providing an ablation study.
- **Why unresolved**: It is unclear if $\eta=10$ is a general optimum or specifically tuned for the Crazyflie dynamics, and small changes in this parameter might drastically alter the saddle-point solution.
- **What evidence would resolve it**: An ablation study tracking the user cost and disturbance magnitude across a range of $\eta$ values.

### Open Question 3
- **Question**: Does the simultaneous gradient descent-ascent update scheme converge reliably, or does it suffer from rotational instability or cycling in complex continuous control tasks?
- **Basis in paper**: The method relies on a primal-dual iteration (Section 4.1) to solve the min-max game. While the paper incorporates TD3 techniques (RDDPG) to improve stability, standard gradient descent-ascent in continuous action spaces is notoriously unstable and prone to oscillation.
- **Why unresolved**: The paper provides empirical results of successful training but does not offer a theoretical convergence proof or an analysis of the learning dynamics (e.g., divergence rates) for the two-player game.
- **What evidence would resolve it**: A learning curve analysis showing the stability of the value function and policy norms over time, or comparison with algorithms specifically designed to stabilize min-max optimization.

## Limitations
- The simplified 3D force model may not capture all real-world perturbations (e.g., gusts, mechanical wear), limiting generalizability.
- Evaluation only considers two predefined trajectories, which may not stress-test the controller across diverse flight patterns.
- Neural network architecture details are unspecified, introducing uncertainty in exact replication.

## Confidence
- **High confidence**: The core RDDPG algorithm (two-player game formulation with TD3-style twin critics) is clearly specified and mathematically sound.
- **Medium confidence**: Quadrotor environment setup is reproducible with provided parameters, though exact trajectory definitions remain unclear.
- **Medium confidence**: Empirical results showing lower variance and cost are well-supported by the presented data.

## Next Checks
1. **Adversary strength validation**: Test RDDPG against an increased disturbance magnitude range (e.g., $[-0.2, 0.2]$) to verify robustness scales appropriately.
2. **Cross-trajectory generalization**: Evaluate on additional trajectory patterns (circular, figure-8) to assess performance consistency across flight modes.
3. **Ablation study**: Compare against a variant where the adversary is removed post-training to quantify the contribution of adversarial training to final robustness.