---
ver: rpa2
title: 'SEKI: Self-Evolution and Knowledge Inspiration based Neural Architecture Search
  via Large Language Models'
arxiv_id: '2502.20422'
source_url: https://arxiv.org/abs/2502.20422
tags:
- search
- conv
- architecture
- seki
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents SEKI, a large language model (LLM)-based neural
  architecture search (NAS) method that leverages self-evolution and knowledge inspiration.
  The method operates in two stages: self-evolution for iterative architecture refinement
  based on performance feedback, and knowledge inspiration for generating new designs
  by analyzing common patterns among high-performing architectures.'
---

# SEKI: Self-Evolution and Knowledge Inspiration based Neural Architecture Search via Large Language Models

## Quick Facts
- arXiv ID: 2502.20422
- Source URL: https://arxiv.org/abs/2502.20422
- Reference count: 31
- Top-1 accuracy: 97.71% CIFAR-10, 84.14% CIFAR-100, 75.8% ImageNet

## Executive Summary
SEKI introduces a large language model (LLM)-based neural architecture search (NAS) method that achieves state-of-the-art performance with minimal computational cost. The method operates in two stages: self-evolution for iterative architecture refinement based on performance feedback, and knowledge inspiration for generating new designs by analyzing common patterns among high-performing architectures. By leveraging the reasoning capabilities of LLMs, SEKI achieves 97.71% CIFAR-10 accuracy, 84.14% CIFAR-100 accuracy, and 75.8% ImageNet accuracy while requiring only 0.05 GPU-days.

## Method Summary
SEKI operates on the DARTS search space and uses a two-stage approach. First, the self-evolution stage iteratively refines architectures by having the LLM analyze current architectures and their validation scores, then generate optimization strategies and refined architectures. Second, the knowledge inspiration stage analyzes the top-performing architectures from the repository to extract common design patterns, which the LLM uses to generate new architectures. The method uses supernet training (50 epochs) for fast evaluation during search, then trains the final architecture from scratch for validation.

## Key Results
- Achieves 97.71% top-1 accuracy on CIFAR-10
- Achieves 84.14% top-1 accuracy on CIFAR-100
- Achieves 75.8% top-1 accuracy on ImageNet
- Requires only 0.05 GPU-days for search
- Outperforms existing methods in both efficiency and accuracy

## Why This Works (Mechanism)

### Mechanism 1: Iterative Self-Evolution via Performance Feedback
SEKI treats the LLM as an optimizer that maps (Architecture, Score) → Improved Architecture by analyzing architectural flaws and performance scores to generate targeted refinements. The LLM correlates low performance scores with specific structural inefficiencies to produce optimization strategies.

### Mechanism 2: Knowledge Inspiration via Repository Synthesis
The method accumulates high-performing architectures into a repository and prompts the LLM to synthesize common patterns, enabling knowledge inspiration that outperforms simple evolutionary crossover by extracting latent design patterns from top architectures.

### Mechanism 3: Structured Chain-of-Thought (CoT) Prompting
Decomposing the generation process into distinct "Reasoning" and "Action" steps stabilizes the search by preventing the LLM from generating structurally invalid architectures, forcing justification of changes before outputting new architecture strings.

## Foundational Learning

- **Concept: Differentiable Architecture Search (DARTS) Search Space**
  - Why needed here: SEKI operates on the DARTS search space (cells with normal/reduction nodes). You must understand the Genotype representation to debug the LLM's string outputs.
  - Quick check question: Can you manually decode a Genotype string from the paper into a directed acyclic graph (DAG) of operations?

- **Concept: One-Shot NAS & Weight Sharing**
  - Why needed here: SEKI achieves 0.05 GPU-days efficiency by evaluating architectures using a supernet rather than training from scratch. Understanding the gap between supernet validation accuracy and true test accuracy is critical.
  - Quick check question: Why might an architecture with high validation accuracy on the supernet perform poorly when "trained from scratch"?

- **Concept: In-Context Learning (ICL)**
  - Why needed here: The method relies on the LLM performing ICL—using the prompt containing past architectures and scores to adjust its behavior without weight updates.
  - Quick check question: How does the performance of SEKI theoretically change as the number of few-shot examples in the prompt increases towards the model's context limit?

## Architecture Onboarding

- **Component map:** Supernet Trainer -> Knowledge Repository -> LLM Inferencer -> Architecture Evaluator
- **Critical path:** Initialize → Evaluate Random Arch → Self-Evolution Loop (Prompt → LLM → Evaluate → Update Repo) → Knowledge Inspiration Loop (Select Top-k → Prompt → LLM → Evaluate → Update Repo)
- **Design tradeoffs:** λ (Self-Evolution steps) vs. γ (Knowledge Inspiration steps): High λ builds a robust repository but is slower; high γ explores faster but may lack direction if the repository is empty. Repository size k: Too small limits diversity; too large may dilute the quality of the "top-k" patterns.
- **Failure signatures:** Syntax Errors (LLM generates invalid genotypes), Mode Collapse (repetitive architectures with near-zero standard deviation), Performance Collapse (excessive skip_connect leading to shallow effective depths).
- **First 3 experiments:** 1) Sanity Check (Self-Evolution Only): Run SEKI with γ = 0 on NAS-Bench-201 to verify LLM can optimize a single architecture lineage. 2) Ablation on Repository: Compare search performance using "random" vs. "top-k" repository to confirm knowledge inspiration relies on quality data. 3) Prompt Robustness: Test if removing "Reasoning/Strategy" step increases invalid architectures or lowers final accuracy.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can SEKI effectively generalize to broader domains beyond computer vision, such as Natural Language Processing (NLP)?
- **Open Question 2:** How can advanced algorithms be integrated with the LLM framework to further enhance the search process?
- **Open Question 3:** Is the optimal balance between self-evolution (λ) and knowledge inspiration (γ) transferable to different search spaces?

## Limitations

- The efficiency claim (0.05 GPU-days) relies heavily on supernet accuracy being a reliable proxy for true architecture performance, which is not explicitly validated.
- The paper provides limited evidence that the "knowledge inspiration" patterns are genuinely superior to traditional evolutionary methods, lacking quantitative ablation studies.
- The method's effectiveness on non-vision domains remains largely unproven, with only brief mention of NLP testing.

## Confidence

- **High Confidence:** The two-stage framework is clearly defined with sound mathematical formulation; 97.71% CIFAR-10 accuracy is verifiable through standard benchmarks.
- **Medium Confidence:** The 75.8% ImageNet result is impressive but relies on the efficiency claim, which is difficult to validate without supernet-to-true-accuracy correlation data.
- **Low Confidence:** Claims about LLM extracting superior design patterns lack quantitative evidence compared to traditional methods.

## Next Checks

1. Measure the Pearson correlation coefficient between supernet validation accuracy and stand-alone accuracy for 100 randomly sampled architectures from the DARTS search space.
2. Run a t-SNE visualization comparing the embedding space of architectures discovered by SEKI versus random search to verify clustering around distinct, high-performing regions.
3. Evaluate whether the SEKI-discovered architecture maintains competitive performance on CIFAR-100 when trained directly, validating genuine architectural generalization.