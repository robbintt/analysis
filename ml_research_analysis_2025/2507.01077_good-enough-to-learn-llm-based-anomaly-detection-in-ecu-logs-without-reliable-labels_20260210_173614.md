---
ver: rpa2
title: 'Good Enough to Learn: LLM-based Anomaly Detection in ECU Logs without Reliable
  Labels'
arxiv_id: '2507.01077'
source_url: https://arxiv.org/abs/2507.01077
tags:
- anomaly
- detection
- anomalies
- communication
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses anomaly detection in ECU communication logs
  using decoder-only LLMs without reliable ground truth labels. It introduces entropy
  regularization to increase model uncertainty for known anomalies while maintaining
  consistency, enabling detection despite inconsistent labeling.
---

# Good Enough to Learn: LLM-based Anomaly Detection in ECU Logs without Reliable Labels

## Quick Facts
- arXiv ID: 2507.01077
- Source URL: https://arxiv.org/abs/2507.01077
- Reference count: 16
- Primary result: 81% recall and 81% precision for region-based anomaly detection using entropy regularization with Qwen-1.3B

## Executive Summary
This paper addresses the challenge of anomaly detection in Electronic Control Unit (ECU) communication logs using decoder-only Large Language Models (LLMs) without reliable ground truth labels. The authors introduce an entropy regularization technique that increases model uncertainty for known anomalies while maintaining consistency, enabling effective detection despite inconsistent labeling. The method combines pre-training on ECU communication data for next-token prediction with fine-tuning for anomaly detection using a sliding window approach. Experiments demonstrate that Qwen-1.3B achieves strong performance with 98.62% NTP accuracy and 1.03 perplexity, while the entropy regularization yields 81% recall and 81% precision for region-based anomaly detection.

## Method Summary
The approach involves pre-training a decoder-only LLM on ECU communication logs using next-token prediction, then fine-tuning it for anomaly detection with entropy regularization. The model processes data using a sliding window approach, where entropy regularization increases uncertainty for known anomalies while maintaining consistency. This allows the model to detect anomalies effectively even when reliable ground truth labels are unavailable. The entropy regularization technique is designed to work around the problem of inconsistent labeling by making the model more uncertain about anomalous regions while remaining consistent in normal operation.

## Key Results
- Qwen-1.3B achieves 98.62% NTP accuracy with 1.03 perplexity on ECU communication data
- Entropy regularization yields 81% recall and 81% precision for region-based anomaly detection
- Visual analysis confirms effective anomaly detection compared to rule-based baselines, particularly in normal operation and system restart scenarios

## Why This Works (Mechanism)
The method works by leveraging the inherent uncertainty patterns in LLM predictions. During pre-training, the model learns normal ECU communication patterns through next-token prediction. The entropy regularization then exploits the fact that anomalies should increase prediction uncertainty - when the model encounters data that deviates from learned patterns, it should produce higher entropy predictions. This creates a natural anomaly signal without requiring explicit labels. The sliding window approach allows the model to maintain context while processing long sequences, and the entropy regularization ensures that known anomalies produce consistently high uncertainty scores.

## Foundational Learning
- **ECU Communication Protocols**: Understanding the structure and patterns of automotive communication data; needed to design appropriate tokenization and pre-training strategies; quick check: verify token distribution matches protocol specifications
- **Decoder-only LLM Architecture**: Knowledge of transformer-based language models and their training objectives; needed for proper implementation and adaptation to ECU data; quick check: confirm model can reproduce training sequences accurately
- **Entropy Regularization**: Understanding information theory concepts and their application to uncertainty modeling; needed to implement the core anomaly detection mechanism; quick check: verify entropy scores correlate with anomaly severity
- **Sliding Window Processing**: Knowledge of sequence processing techniques for long data streams; needed to handle ECU logs efficiently; quick check: confirm window overlap doesn't introduce artifacts
- **Next-Token Prediction (NTP)**: Understanding autoregressive language modeling; needed for the pre-training phase; quick check: measure perplexity on held-out validation data
- **Anomaly Detection Metrics**: Familiarity with precision, recall, and F1-score in imbalanced detection scenarios; needed to evaluate performance meaningfully; quick check: compare against baseline rule-based methods

## Architecture Onboarding

**Component Map**: Data -> Tokenizer -> Pre-training (NTP) -> Fine-tuning (Entropy Regularization) -> Anomaly Detection

**Critical Path**: The critical path flows from raw ECU log data through tokenization, pre-training on normal communication patterns, fine-tuning with entropy regularization, and finally to the anomaly detection output. Each stage builds upon the previous one, with the entropy regularization being the key innovation that enables label-free anomaly detection.

**Design Tradeoffs**: The choice of a relatively small model (Qwen-1.3B) balances computational efficiency with performance, making it practical for embedded deployment. The sliding window approach trades some context for computational tractability. The entropy regularization technique trades perfect accuracy for robustness to label inconsistencies.

**Failure Signatures**: The model may struggle with novel anomaly types not present in training data, potentially producing false negatives. It may also be sensitive to tokenization choices, where important patterns could be split across token boundaries. Performance could degrade on ECU logs from different manufacturers due to protocol variations.

**First Experiments**:
1. Test next-token prediction accuracy on held-out normal data to verify pre-training effectiveness
2. Measure entropy scores on known normal vs. known anomalous data to validate regularization behavior
3. Compare detection performance across different window sizes to optimize the sliding window parameters

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Evaluation relies heavily on synthetic anomaly generation, which may not capture real-world ECU anomaly patterns
- Performance metrics were obtained on a limited dataset, raising questions about generalizability across different ECU systems and manufacturers
- The entropy regularization technique lacks extensive ablation studies to demonstrate its specific contribution versus other architectural choices

## Confidence
- High confidence in the basic LLM pre-training approach and NTP performance metrics
- Medium confidence in the entropy regularization effectiveness for anomaly detection
- Medium confidence in the comparative advantage over rule-based methods
- Low confidence in cross-system generalizability and real-world applicability

## Next Checks
1. Test the model on real-world ECU logs with documented anomalies from multiple manufacturers to validate cross-system generalizability
2. Conduct ablation studies removing entropy regularization to quantify its specific contribution to detection performance
3. Implement quantitative comparison metrics beyond visual analysis for the rule-based baseline comparison across diverse operational scenarios including edge cases