---
ver: rpa2
title: 'VIBE: Vector Index Benchmark for Embeddings'
arxiv_id: '2505.17810'
source_url: https://arxiv.org/abs/2505.17810
tags:
- datasets
- search
- algorithms
- methods
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VIBE, a comprehensive benchmark suite for
  evaluating approximate nearest neighbor (ANN) search algorithms on modern embedding
  datasets. VIBE addresses the need for up-to-date benchmarks that reflect current
  applications like retrieval-augmented generation (RAG) and multimodal search.
---

# VIBE: Vector Index Benchmark for Embeddings

## Quick Facts
- arXiv ID: 2505.17810
- Source URL: https://arxiv.org/abs/2505.17810
- Authors: Elias Jääsaari; Ville Hyvönen; Matteo Ceccarello; Teemu Roos; Martin Aumüller
- Reference count: 40
- Primary result: VIBE introduces a comprehensive benchmark suite evaluating 21 state-of-the-art ANN implementations across 18 datasets with focus on throughput at 95% recall

## Executive Summary
This paper introduces VIBE, a comprehensive benchmark suite for evaluating approximate nearest neighbor (ANN) search algorithms on modern embedding datasets. VIBE addresses the need for up-to-date benchmarks that reflect current applications like retrieval-augmented generation (RAG) and multimodal search. It includes 12 in-distribution datasets created using popular embedding models and 6 out-of-distribution datasets for multimodal search and approximate attention computation tasks.

The benchmark evaluates 21 state-of-the-art ANN implementations across graph-based, clustering-based, tree-based, and hashing-based methods. Key findings show that graph-based methods (SymphonyQG, Glass, NGT-QG) and clustering-based methods (LoRANN, ScaNN, IVF-PQ) outperform others. On in-distribution datasets, SymphonyQG and Glass achieve the highest throughput at 95% recall. However, on out-of-distribution queries, regular ANN methods' performance degrades significantly, while specialized OOD methods (RoarGraph, MLANN, LoRANN) excel in approximate attention computation tasks. VIBE also supports binary embeddings and GPU-based implementations, achieving throughputs up to 10^5 QPS.

## Method Summary
VIBE creates a comprehensive evaluation framework by generating 12 in-distribution datasets using modern embedding models like Nomic, DistilRoBERTa, and CLIP, then evaluating 21 ANN implementations across four methodological categories. The benchmark measures throughput at 95% recall as the primary metric, with additional assessments on out-of-distribution datasets for multimodal search and approximate attention tasks. The evaluation includes both CPU and GPU implementations, supporting both regular and binary embeddings. The systematic approach covers diverse query patterns and dataset characteristics to provide comprehensive performance insights across different use cases.

## Key Results
- Graph-based methods (SymphonyQG, Glass, NGT-QG) and clustering-based methods (LoRANN, ScaNN, IVF-PQ) outperform other approaches on in-distribution datasets
- SymphonyQG and Glass achieve highest throughput at 95% recall on in-distribution datasets
- On out-of-distribution queries, regular ANN methods' performance degrades significantly while specialized OOD methods (RoarGraph, MLANN, LoRANN) excel in approximate attention computation tasks
- GPU-based implementations achieve throughputs up to 10^5 QPS

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive coverage of modern embedding scenarios and diverse ANN implementations. By evaluating algorithms across both in-distribution and out-of-distribution datasets, VIBE captures real-world performance variations that single-dataset benchmarks miss. The focus on throughput at high recall rates reflects practical deployment requirements where both speed and accuracy matter. The inclusion of multimodal and binary embedding support ensures relevance to current AI applications, while the systematic comparison across multiple methodological approaches (graph, clustering, tree, hashing) provides clear insights into algorithmic strengths and weaknesses.

## Foundational Learning

**Approximate Nearest Neighbor Search**: Why needed - forms the basis for efficient similarity search in high-dimensional spaces; Quick check - understand the trade-off between recall and query time

**Embedding Models**: Why needed - generate the vector representations being searched; Quick check - familiarity with models like CLIP, DistilRoBERTa, and Nomic

**Graph-Based ANN Methods**: Why needed - dominant approach for high-recall, high-throughput scenarios; Quick check - understand HNSW and its variants

**Clustering-Based Methods**: Why needed - effective for partitioning large datasets into searchable regions; Quick check - know IVF-PQ and product quantization principles

**Multimodal Embeddings**: Why needed - critical for modern RAG and cross-modal retrieval applications; Quick check - understand how text and image embeddings are generated and compared

**Approximate Attention Computation**: Why needed - enables efficient processing in transformer-based models; Quick check - grasp the connection between ANN and attention mechanisms

## Architecture Onboarding

Component map: Dataset Generation -> ANN Implementation -> Evaluation Framework -> Results Analysis

Critical path: The benchmark follows a pipeline where datasets are first generated using specific embedding models, then indexed using various ANN algorithms, followed by systematic query evaluation measuring throughput at target recall rates, and finally comparative analysis across different methodological approaches.

Design tradeoffs: The benchmark prioritizes throughput at high recall rates over other metrics like memory efficiency or latency distribution. This focus on speed reflects current deployment priorities but may underrepresent scenarios requiring strict latency bounds or minimal memory footprints.

Failure signatures: Performance degradation on out-of-distribution queries indicates limitations in algorithmic generalization. Methods showing significant throughput drops when query distributions differ from training data suggest brittleness in real-world deployment scenarios.

First experiments:
1. Run basic throughput measurements on a small in-distribution dataset to verify benchmark setup
2. Compare single-method performance across multiple embedding types (text, image, multimodal)
3. Test GPU versus CPU implementations on identical datasets to measure hardware acceleration benefits

## Open Questions the Paper Calls Out

None

## Limitations

- Evaluation focuses primarily on throughput at 95% recall, potentially overlooking memory efficiency and latency distributions
- The 12 in-distribution datasets may not fully capture production deployment complexity, particularly regarding dynamic updates and scaling behaviors
- Benchmark's throughput-centric approach may not represent needs of applications requiring strict latency guarantees or operating under memory constraints

## Confidence

- Graph-based and clustering-based methods outperforming others: High
- Specialized OOD methods excelling in approximate attention computation: Medium
- GPU implementations achieving 10^5 QPS throughput: High

## Next Checks

1. Evaluate the benchmarked methods under varying memory budgets and latency constraints to assess real-world deployment viability
2. Test the algorithms with dynamic datasets to measure performance degradation during insertion and deletion operations
3. Conduct long-term stability tests to measure index quality degradation over time with continuous query operations