---
ver: rpa2
title: Likelihood Variance as Text Importance for Resampling Texts to Map Language
  Models
arxiv_id: '2505.15428'
source_url: https://arxiv.org/abs/2505.15428
tags:
- texts
- error
- sampling
- resampling
- uniform
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles the high computational cost of constructing a
  language model map by proposing a resampling method that selects important texts
  based on the variance of log-likelihoods across models. This approach significantly
  reduces the number of required texts while preserving KL divergence estimation accuracy.
---

# Likelihood Variance as Text Importance for Resampling Texts to Map Language Models

## Quick Facts
- arXiv ID: 2505.15428
- Source URL: https://arxiv.org/abs/2505.15428
- Reference count: 40
- The paper proposes a resampling method using variance-based text importance to efficiently construct language model maps while preserving KL divergence estimation accuracy.

## Executive Summary
This paper addresses the high computational cost of constructing language model maps by introducing a variance-based resampling method. The approach selects important texts based on the variance of log-likelihoods across models, significantly reducing the number of required texts while maintaining KL divergence estimation accuracy. The method achieves comparable performance to uniform sampling with about half as many texts and enables efficient incorporation of new models into existing maps. Experiments demonstrate that the approach scales well and maintains robustness in downstream performance prediction.

## Method Summary
The proposed method selects texts for model mapping based on the variance of log-likelihoods across models. It computes importance weights proportional to the squared norm of double-centered log-likelihood vectors, then performs weighted resampling with replacement. The method maintains unbiased KL divergence estimates through correction weights and decomposes total estimation error into sampling and resampling components. This enables efficient construction of model maps with fewer texts while preserving accuracy, and facilitates the addition of new models to existing maps.

## Key Results
- Resampling method achieves comparable KL divergence estimation accuracy with ~50% fewer texts (2,202 vs. 6,320 unique texts)
- Variance-based importance weights enable efficient incorporation of new models into existing maps
- The method scales well and maintains robustness in downstream performance prediction tasks

## Why This Works (Mechanism)

### Mechanism 1: Variance-Based Text Importance Selection
- Claim: Texts with higher variance in log-likelihoods across models carry more information for distinguishing model relationships.
- Mechanism: Double-centered log-likelihood matrix Q captures per-text variation across K models. Texts where models disagree more (higher ∥Q^(s)∥²) receive higher sampling probability π_s ∝ ∥Q^(s)∥². This concentrates computation on discriminative texts while reducing redundant evaluation of texts where models agree.
- Core assumption: Variance across existing models predicts informativeness for unseen model pairs and new model placement.
- Evidence anchors:
  - [abstract] "selects important texts with weights proportional to the variance of log-likelihoods across models for each text"
  - [Section 3.1] "The squared norm ∥Q(s)∥² corresponds to the variance of the log-likelihoods for text x_s across models"
  - [corpus] Neighbor paper "Mapping 1,000+ Language Models via the Log-Likelihood Vector" provides foundational theory linking log-likelihood vectors to KL divergence
- Break condition: If new models have fundamentally different log-likelihood patterns than existing models, importance weights may misallocate samples.

### Mechanism 2: Weighted Distance Estimation with Resampling Correction
- Claim: Importance-weighted distance calculations preserve unbiased KL divergence estimates despite non-uniform sampling.
- Mechanism: When text x_{u_t} is sampled c(u_t) times with probability π_{u_t}, the weight w_{u_t} = c(u_t)/(n·π_{u_t}) corrects sampling bias. The weighted distance ∥q̃_i - q̃_j∥²_{w_d} = Σ_t w_{u_t}(q̃_i(x_{u_t}) - q̃_j(x_{u_t}))² yields E[estimate] = true distance (Lemma 1 proves unbiasedness).
- Core assumption: Resampling probabilities π_s accurately reflect text importance; weights are correctly normalized.
- Evidence anchors:
  - [Section 3.2] "the column L(u_t) corresponding to the resampled text x_{u_t} is weighted by w_{u_t} = c(u_t)/nπ_{u_t}"
  - [Appendix C, Lemma 1] "For any i, j ∈ {1, . . . , K}, it holds that E[g̃_{ij}] = g_{ij}"
  - [corpus] Weak direct corpus support for weighted resampling in language model contexts; related work on importance sampling exists in other domains
- Break condition: If duplicate handling or weight computation has numerical errors, bias re-enters estimates.

### Mechanism 3: Two-Stage Error Decomposition for Bootstrap Estimation
- Claim: Total estimation error decomposes into independent sampling and resampling components, enabling reliable error quantification.
- Mechanism: Error σ² = τ²_{unif,N} + τ²_{Method,n} where τ²_{unif,N} captures sampling error (population → dataset) and τ²_{Method,n} captures resampling error (dataset → resampled set). Bootstrap resampling estimates both components without requiring full population computation.
- Core assumption: Sampling and resampling errors are independent; bootstrap reliably approximates population statistics.
- Evidence anchors:
  - [Section 4.1] "ˆσ_{Method,n} = √(τ²_{unif,N} + τ²_{Method,n})"
  - [Appendix E] Detailed derivation of error decomposition and bootstrap estimation procedure
  - [corpus] General bootstrap theory is well-established; application to model maps is novel
- Break condition: If N is too small relative to population diversity, τ²_{unif,N} underestimates true sampling error.

## Foundational Learning

- Concept: **KL Divergence and its Estimation from Samples**
  - Why needed here: The entire framework aims to preserve KL divergence estimates while reducing computation; understanding how log-likelihood vectors approximate KL is essential.
  - Quick check question: Given log-likelihood vectors q_i and q_j, what does ∥q_i - q_j∥²/N approximate?

- Concept: **Importance Sampling with Unbiased Estimators**
  - Why needed here: The resampling method uses importance weights to maintain unbiased distance estimates despite non-uniform sampling.
  - Quick check question: If you sample from distribution p but want expectations under q, how do you construct unbiased estimates?

- Concept: **Double Centering of Matrices**
  - Why needed here: Log-likelihood matrices must be double-centered (row-wise then column-wise) before computing distances; this step is easily overlooked.
  - Quick check question: What operations does double centering involve, and why does it matter for distance calculations?

## Architecture Onboarding

- Component map:
  1. Log-likelihood extraction -> Double centering -> Variance computation -> Resampling -> Weighted distance calculation

- Critical path: Log-likelihood extraction → Double centering → Variance computation → Resampling → Weighted distance calculation. Errors in centering propagate to all downstream variance estimates.

- Design tradeoffs:
  - LS sampling (simpler) vs. KL sampling (slightly better for absolute error; requires computing all pairwise differences)
  - Smaller n reduces computation but increases resampling variance
  - Including duplicates vs. tracking unique texts: paper shows tracking uniques with weights is more efficient

- Failure signatures:
  - Ellipse sizes in t-SNE visualization vary dramatically → resampling variance too high (increase n)
  - New models cluster incorrectly → importance weights based on insufficiently representative existing models
  - Distance estimates biased low → weight normalization error or centering bug

- First 3 experiments:
  1. Sanity check: Uniform resampling should match direct sampling error (validate implementation against Fig. 2 baseline)
  2. Variance calibration: Compare LS vs. KL sampling on held-out model pairs; verify ~2× efficiency gain
  3. New model addition test: Train importance weights on 80% of models, evaluate placement accuracy on remaining 20% using only resampled texts

## Open Questions the Paper Calls Out
None

## Limitations
- The assumption that variance across existing models predicts informativeness for new model relationships may not hold in all scenarios
- The method's effectiveness with substantially larger model sets (beyond 1,000 models) remains untested
- Computational savings come with increased variance in estimates, which could affect precision-sensitive applications

## Confidence
**High Confidence**: The unbiasedness of the weighted distance estimator (Lemma 1) and the mathematical framework for variance-based importance sampling are rigorously proven and experimentally validated. The efficiency gains over uniform sampling are consistently demonstrated across multiple benchmarks.

**Medium Confidence**: The decomposition of total error into sampling and resampling components is theoretically sound, but its practical reliability depends on bootstrap assumptions holding in practice. The comparison between LS and KL sampling methods shows small differences, making conclusions about which is superior somewhat tentative.

**Low Confidence**: The claim that variance-based importance weights will remain effective as the number of models grows substantially beyond the tested 100-1,000 range is largely untested. The method's robustness to non-stationary model distributions (where the relationship between variance and informativeness changes over time) is also not thoroughly explored.

## Next Checks
1. **Cross-architecture validation**: Test the method on a diverse set of model architectures (including non-transformer models) to verify that variance-based importance weights remain predictive when models have fundamentally different likelihoods.

2. **Dynamic model addition stress test**: Evaluate how well the method handles scenarios where new models are added incrementally over time, measuring whether importance weights computed on older models degrade in effectiveness.

3. **Extreme compression analysis**: Systematically test the method at very low text counts (n < 500) to determine the practical lower bound where the variance-resampling tradeoff becomes detrimental to downstream tasks.