---
ver: rpa2
title: 'SHARPIE: A Modular Framework for Reinforcement Learning and Human-AI Interaction
  Experiments'
arxiv_id: '2501.19245'
source_url: https://arxiv.org/abs/2501.19245
tags:
- learning
- agents
- human
- reinforcement
- humans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SHARPIE is a modular Python-based framework for conducting reinforcement
  learning (RL) experiments with human participants. It provides a versatile wrapper
  around popular RL environments and algorithm libraries, supporting configurable
  communication channels between humans and RL agents through a web-based multi-modal
  interface.
---

# SHARPIE: A Modular Framework for Reinforcement Learning and Human-AI Interaction Experiments

## Quick Facts
- arXiv ID: 2501.19245
- Source URL: https://arxiv.org/abs/2501.19245
- Reference count: 8
- Primary result: Modular Python framework for RL experiments with human participants via web-based multi-modal interface

## Executive Summary
SHARPIE is a modular Python-based framework designed to facilitate reinforcement learning experiments involving human participants. It provides a versatile wrapper around popular RL environments and algorithm libraries, supporting configurable communication channels between humans and RL agents through a web-based multi-modal interface. The platform enables researchers to study diverse human-AI interaction scenarios including interactive reward specification, learning from human feedback, action delegation, preference elicitation, and human-AI teaming.

## Method Summary
SHARPIE achieves broad compatibility by wrapping environments that follow the Gymnasium API standard rather than implementing custom environments. The framework separates participant-facing web interface from back-end experiment logic, with logging and state management handled server-side. It supports deployment on cloud platforms (AWS mentioned) and includes logging utilities for secure data storage, facilitating controlled experiments with multiple human participants and RL agents.

## Key Results
- Provides modular wrapper around Gymnasium-compatible environments and popular RL algorithm libraries (Stable-Baselines3, RLlib, CleanRL)
- Supports configurable multi-modal communication channels for diverse human-AI interaction patterns
- Enables deployment on cloud platforms with secure logging utilities for controlled experiments
- Standardizes RL-based human-agent interactions in multi-agent settings similar to how Gymnasium standardized fully simulated RL environments

## Why This Works (Mechanism)

### Mechanism 1
SHARPIE achieves broad compatibility by wrapping environments that follow the Gymnasium API standard rather than implementing custom environments. The framework accepts any environment implementing `reset`, `step`, `render` methods (Gymnasium API), then adds human-interaction layers on top without modifying the underlying environment logic. Core assumption: Most target RL environments already conform to or can be adapted to Gymnasium conventions.

### Mechanism 2
Human-AI interaction patterns are enabled through configurable communication channels rather than hardcoded interaction modes. A modular front-end UI component system allows researchers to select and combine interaction modalities (video output, text channels, demonstration capture, action delegation) per experiment, routing data through a standardized back-end interface. Core assumption: Interaction patterns can be decomposed into composable UI components and communication channel types.

### Mechanism 3
Experimental validity is preserved through abstraction between participant hardware and experiment execution via cloud/local deployment options. The framework separates participant-facing web interface from back-end experiment logic, with logging and state management handled server-side, enabling consistent experience regardless of participant device. Core assumption: Network latency is acceptable for the interaction paradigm being studied; participants have reliable internet access.

## Foundational Learning

- Concept: **Markov Decision Processes (MDPs) and the RL loop**
  - Why needed here: SHARPIE wraps environments that expose state-action-reward sequences; understanding `step()` and `reset()` semantics is prerequisite to configuring any experiment.
  - Quick check question: Can you explain what an environment's `step(action)` method returns?

- Concept: **Gymnasium API conventions**
  - Why needed here: The paper explicitly states this is the interface standard; all environment integration depends on understanding observation spaces, action spaces, and the standard method signatures.
  - Quick check question: What is the difference between `observation_space` and `action_space` in a Gymnasium environment?

- Concept: **Multi-agent RL basics (MARL)**
  - Why needed here: SHARPIE specifically targets multi-agent settings where humans and AI agents coexist; Table 1 shows use cases with n teachers, n learners, and mixed human-agent teams.
  - Quick check question: In a multi-agent setting, does each agent observe the full state or only partial observations?

## Architecture Onboarding

- Component map:
  - Environment Simulator: Any Gymnasium-compatible RL environment
  - RL Agent(s): Algorithm implementations (Stable-Baselines3, RLlib, CleanRL supported)
  - Back-end (Core): Python server handling experiment logic, state management, agent-environment stepping
  - Web Interface: Participant-facing UI with multi-modal inputs
  - Experiment Configuration: YAML/Python config defining interaction mode, agent setup, logging
  - Logging Layer: Secure data storage for trajectories, human inputs, experiment metadata
  - Admin Panel: Researcher interface for experiment management

- Critical path:
  1. Clone SHARPIE repository from GitHub
  2. Install Python dependencies (framework is Python-based)
  3. Select or implement a Gymnasium-compatible environment
  4. Configure experiment YAML defining human role, agent role, interaction modality
  5. Deploy to cloud (AWS supported) or run localhost for testing
  6. Integrate with recruitment platform (Prolific, MTurk mentioned as targets)
  7. Run pilot with small participant cohort, verify logging pipeline

- Design tradeoffs:
  - **Generality vs. specificity**: Framework prioritizes broad compatibility over optimized support for any single use case; expect integration effort for novel interaction patterns
  - **Web-based vs. native**: Web UI enables remote participants but limits real-time modalities; audio/video planned but not yet implemented per paper
  - **Centralized logging vs. distributed**: Secure centralized storage simplifies data management but creates single point of failure; consider redundancy for long-running experiments

- Failure signatures:
  - Environment not rendering in participant browser: Likely Gymnasium render mode mismatch; verify `render_mode="rgb_array"` compatibility
  - Agent actions not synchronized with human inputs: Check timestep alignment between agent `step()` calls and human input polling
  - Participant session data lost mid-experiment: Logging buffer not flushing; verify storage write frequency and error handling
  - Recruitment platform integration fails: Session handoff tokens not propagating; check URL parameter passing

- First 3 experiments:
  1. **Single-agent reward annotation**: Run AMaze environment with one participant providing scalar feedback; validates basic UI, logging, and environment wrapper
  2. **Two-agent delegation handoff**: Mountain Car with human taking control at specific triggers; tests action delegation channel and state synchronization
  3. **Multi-agent teaching**: Simple Spread (PettingZoo) with n humans demonstrating to n learners; validates multi-participant session management and demonstration capture

## Open Questions the Paper Calls Out

### Open Question 1
What are the specific functional distinctions and comparative advantages of SHARPIE versus the Interactive Gym framework? Basis in paper: The authors state in the Related Work section regarding Interactive Gym: "It is difficult to assess the exact differences between this framework and ours, as both are under development." Why unresolved: Both frameworks are currently in active development phases, preventing a definitive comparative analysis of their capabilities in multi-agent human-AI settings.

### Open Question 2
Can the current web-based architecture support real-time, high-bandwidth communication (audio/video) without introducing detrimental latency? Basis in paper: The Discussion section notes that future work involves plans to "widen the scope of possible human-agent interactions by incorporating additional modalities such as audio or video," implying this capability is not yet fully realized or tested. Why unresolved: The paper discusses the intent to add these modalities to study "fine-grained communication protocols," but does not demonstrate if the existing backend and front-end infrastructure can handle the data throughput or synchronization required for seamless audio-visual interaction.

### Open Question 3
To what extent does the modular "wrapper" design introduce performance overhead or integration complexity compared to tightly coupled solutions? Basis in paper: The paper critiques other platforms for being "specific to the Gymnasium environments" or limited to specific interactions, positioning SHARPIE as a "generic" wrapper. Why unresolved: While genericity is a stated goal, abstracting diverse environments and algorithms (RLlib, CleanRL, PettingZoo) behind a unified wrapper often creates performance bottlenecks or "impedance mismatch" in data handling which is not quantified in the text.

### Open Question 4
Can a single wrapper API effectively standardize the diverse paradigms of human-RL interaction (teaming, teaching, delegation) given their conflicting requirements? Basis in paper: The paper states the goal is to "standardize the field of study on RL in human contexts," yet it lists highly diverse use cases which traditionally require very different interface mechanics and timing constraints. Why unresolved: It is unclear if a single "generic interface" can adequately serve bidirectional, multi-agent teaming scenarios as well as unidirectional, feedback-heavy annotation scenarios without becoming overly complex or limiting specific interaction modalities.

## Limitations
- Web-based architecture may introduce latency constraints limiting real-time interaction scenarios requiring sub-100ms response times
- Audio and video interaction capabilities are mentioned as planned rather than implemented features
- Integration effort for non-standard environments or novel interaction patterns remains unquantified

## Confidence
- **High Confidence**: The framework's core architecture following Gymnasium API conventions, modular design principles, and support for multi-agent settings are well-established through explicit specification and alignment with de-facto standards in the RL ecosystem
- **Medium Confidence**: Claims about cloud deployment capabilities, logging utilities, and experimental validity preservation are supported by architectural description but lack empirical validation or performance benchmarks in the paper
- **Low Confidence**: The paper's assertions about the framework's utility for "diverse human-AI interaction scenarios" remain largely theoretical without presenting results from actual human participant studies or comparative evaluations against existing platforms

## Next Checks
1. Deploy a basic single-agent experiment using MountainCar-v1 environment with real human participants recruited through Prolific, measuring end-to-end latency from human input to agent response and verifying the logging pipeline captures all interaction data
2. Implement and test the action delegation channel by running a two-agent MountainCar experiment where the human takes control at predefined triggers, measuring state synchronization accuracy and identifying any timing discontinuities
3. Conduct a scalability test by running Simple Spread (PettingZoo) with 4 human teachers and 4 learning agents simultaneously, measuring server resource utilization, participant session management stability, and data logging completeness across all participant streams