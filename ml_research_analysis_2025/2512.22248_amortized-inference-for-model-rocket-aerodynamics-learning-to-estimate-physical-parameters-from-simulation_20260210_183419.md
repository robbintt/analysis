---
ver: rpa2
title: 'Amortized Inference for Model Rocket Aerodynamics: Learning to Estimate Physical
  Parameters from Simulation'
arxiv_id: '2512.22248'
source_url: https://arxiv.org/abs/2512.22248
tags:
- data
- inference
- flight
- neural
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a simulation-based amortized inference approach
  for estimating aerodynamic parameters of model rockets using only synthetic flight
  data. A neural network ensemble is trained to invert a physics simulator, predicting
  drag coefficient and thrust correction factor from a single apogee measurement combined
  with motor and configuration features.
---

# Amortized Inference for Model Rocket Aerodynamics: Learning to Estimate Physical Parameters from Simulation

## Quick Facts
- arXiv ID: 2512.22248
- Source URL: https://arxiv.org/abs/2512.22248
- Authors: Rohit Pandey; Rohan Pandey
- Reference count: 13
- One-line primary result: A neural network ensemble trained on synthetic data achieves 12.3 m mean absolute error in apogee prediction for real model rocket flights, outperforming OpenRocket baseline by 38%, without any real training data.

## Executive Summary
This work presents a simulation-based amortized inference approach for estimating aerodynamic parameters of model rockets using only synthetic flight data. A neural network ensemble is trained to invert a physics simulator, predicting drag coefficient and thrust correction factor from a single apogee measurement combined with motor and configuration features. The method achieves 12.3 m mean absolute error in apogee prediction on 8 real flights, outperforming OpenRocket baseline by 38%, all without any real training data. Analysis reveals a systematic positive prediction bias that quantifies the gap between idealized physics and real-world flight conditions. The approach demonstrates promising sim-to-real transfer for parameter estimation in data-scarce environments.

## Method Summary
The approach trains a neural network ensemble to learn an inverse mapping from flight observations to physical parameters by leveraging synthetic data generated from a physics simulator. Given a rocket configuration (motor type, dry mass, total impulse, burn time) and a single apogee measurement, the network predicts the drag coefficient and thrust correction factor. These parameters are then used in forward simulation to predict flight outcomes. The key insight is that even with underdetermined inverse problems, combining a single scalar observation with physics-based constraints from configuration features creates a sufficiently constrained problem for useful inference. The method uses domain knowledge to extract informative features while excluding geometric primitives that would bypass the intended inference task.

## Key Results
- Achieves 12.3 m mean absolute error in apogee prediction on 8 real flights
- Outperforms OpenRocket baseline by 38% without any real training data
- Inferred drag coefficients (0.66-0.89) exceed textbook values, suggesting effective parameter absorption of unmodeled effects
- Systematic positive prediction bias of 5-7% reveals unmodeled real-world losses

## Why This Works (Mechanism)

### Mechanism 1: Amortized Inversion of Forward Physics
A neural network can learn to invert a physics simulator's input-output mapping from synthetic data, enabling single-pass parameter estimation at inference time. The network is trained on pairs (input features, known parameters) generated by forward simulation. By observing diverse simulated flights where both parameters θ = (Cd, α) and outcomes (apogee) are known, the network learns an approximate inverse function gφ: R⁵ → R² that maps observations back to physical parameters. The core assumption is that the physics simulator, while imperfect, captures the essential functional relationship between parameters and apogee sufficiently for the inverse mapping to generalize.

### Mechanism 2: Implicit Regularization Through Constrained Input Space
Combining a single scalar observation (apogee) with configuration features creates a sufficiently constrained problem for useful inference, despite theoretical underdetermination. The 5-dimensional input vector (apogee, motor type, dry mass, total impulse, burn time) provides physics-based constraints. Drag and thrust have distinct temporal signatures in how they affect flight trajectory. The network learns implicit priors from the training distribution that regularize the ill-posed inverse problem. The core assumption is that the training distribution covers the parameter space relevant to real flights, and motor/thrust characteristics are sufficiently discriminative.

### Mechanism 3: Effective Parameter Absorption of Unmodeled Effects
The inferred parameters function as "effective" values that absorb unmodeled real-world losses, producing accurate apogee predictions even when individual parameter estimates deviate from true physical values. The network minimizes prediction error on apogee, not parameter recovery accuracy. Effects like rail friction, fin flutter, and weathercocking—which reduce apogee but aren't in the model—get attributed to increased drag coefficient and decreased thrust factor. This yields parameters useful for prediction but not necessarily physically interpretable. The core assumption is that unmodeled effects correlate consistently with the learnable parameters across the test distribution.

## Foundational Learning

- **Concept: Simulation-Based Inference (SBI)**
  - **Why needed here:** The entire approach rests on training neural networks to perform inference using forward simulators rather than explicit likelihood functions. Understanding SBI clarifies why synthetic data can substitute for real observations.
  - **Quick check question:** Can you explain why traditional maximum likelihood estimation fails when the likelihood function p(observation | parameters) is intractable, and how learning an inverse mapping bypasses this?

- **Concept: Inverse Problems and Identifiability**
  - **Why needed here:** Estimating two parameters from one measurement is fundamentally ill-posed. Understanding identifiability constraints explains why the method works despite theoretical underdetermination.
  - **Quick check question:** Given a single apogee measurement, why might multiple (Cd, α) pairs produce the same predicted altitude? How does the 5D feature vector help?

- **Concept: Neural Network Ensembles for Uncertainty**
  - **Why needed here:** The paper uses ensemble variance as a proxy for epistemic uncertainty. Understanding what ensemble disagreement actually measures vs. calibrated probability is critical for interpretation.
  - **Quick check question:** If all 5 ensemble members predict similar parameters on a synthetic test case but err significantly on a real flight, what type of uncertainty does this reveal a limitation in capturing?

## Architecture Onboarding

- **Component map:**
Prior Distributions p(Cd), p(α) → Parameter Sampling → Physics Simulator (RK45) → Synthetic Dataset (10K flights) → Feature Extraction → [h̃, mmotor, mdry, Itotal, tb] → Ensemble (K=5 Networks) → Parameter Estimates (Ċd, α̂) → Forward Simulation → Predicted Apogee

- **Critical path:** The feature engineering decision (excluding geometric primitives, forcing kinematic inference) directly enables the sim-to-real transfer by making the network learn physics relationships rather than geometry correlations.

- **Design tradeoffs:**
  - Model complexity vs. identifiability: Deeper networks could overfit synthetic artifacts; [128, 256, 128] with 0.1 dropout balances capacity with generalization.
  - Synthetic noise injection: Adding σ=3m measurement noise during training improves robustness but may blur parameter boundaries.
  - Ensemble size: K=5 provides uncertainty estimates without excessive computational cost; paper notes this reflects internal variance on synthetic data, not calibrated real-world confidence.

- **Failure signatures:**
  - Systematic positive bias: All predictions overshoot real apogees by 5-7%, indicating unmodeled real-world losses (Section 6.3).
  - Elevated drag estimates: Ċd ∈ [0.66, 0.89] vs. OpenRocket's 0.52 suggests the network is absorbing unmodeled effects into drag (Section 6.4).
  - Sample size limitation: N=8 real flights insufficient for statistical generalization (explicit caution in Section 6.2).

- **First 3 experiments:**
  1. Validate synthetic-to-synthetic inversion: Train on 10K synthetic flights, test on held-out 2K synthetic flights. Target: Cd MAE ≈ 0.088, α MAE ≈ 0.071 (per Section 6.1). This establishes baseline inversion capability before sim-to-real transfer.
  2. Ablate feature dimensions: Train models with reduced feature sets (apogee only, apogee + mass only, full 5D) on synthetic data to quantify how each constraint contributes to identifiability.
  3. Domain randomization pilot: Add wind vectors and launch angle variation to synthetic data generation, then evaluate whether systematic positive bias reduces on real flights. This tests whether broader synthetic coverage improves sim-to-real gap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating domain randomization techniques, such as random wind vectors and launch angles, during synthetic data generation reduce the systematic positive prediction bias observed in real flights?
- Basis in paper: The authors state future work will "incorporate domain randomization (e.g., wind vectors, launch angles) during synthetic data generation to improve robustness."
- Why unresolved: The current model assumes vertical trajectories and quiescent air; unmodeled wind-induced drift and weathercocking likely contribute to the observed 5–7% apogee bias.
- What evidence would resolve it: A comparative ablation study showing a reduction in the mean prediction bias when evaluating the current model against a variant trained on randomized environmental conditions.

### Open Question 2
- Question: To what extent does utilizing full altitude-time trajectories improve the identifiability of drag and thrust parameters compared to the current single-apogee approach?
- Basis in paper: The authors propose "utilizing full altitude-time trajectories rather than scalar apogee measurements could improve parameter identifiability."
- Why unresolved: The current formulation solves an underdetermined inverse problem (estimating two physical parameters from one scalar measurement), forcing the network to rely on implicit priors from configuration features.
- What evidence would resolve it: Quantitative comparison of parameter recovery accuracy (MAE) on a synthetic dataset where ground truth parameters are known, comparing scalar vs. trajectory inputs.

### Open Question 3
- Question: Does the observed sim-to-real transfer performance generalize to diverse rocket geometries and stability margins outside the specific 66mm diameter platform tested?
- Basis in paper: The authors note the need to "validate the approach on a broader range of rocket configurations to establish more rigorous generalization bounds."
- Why unresolved: The proof-of-concept evaluation is restricted to 8 flights of a single custom rocket, which the authors admit is too small to draw statistically conclusive generalizations.
- What evidence would resolve it: Evaluation of the pre-trained inference model on flight logs from significantly different rocket designs (e.g., varying fin configurations, diameters) to verify consistent performance over the OpenRocket baseline.

## Limitations
- Real flight data scarcity: Results based on only 8 real flights with limited environmental diversity, insufficient for robust generalization claims.
- Simulator fidelity gap: Positive prediction bias (5-7%) indicates systematic unmodeled effects, with robustness to different environmental conditions untested.
- Parameter interpretability: Inferred Cd and α are "effective" values absorbing unmodeled physics, limiting physical interpretability and direct comparison to textbook values.

## Confidence
- **High confidence**: Synthetic data inversion performance (MAE ~0.09 for both parameters), ensemble uncertainty quantification on synthetic data, overall methodology soundness
- **Medium confidence**: Real flight prediction accuracy (12.3m MAE), 38% improvement over baseline, systematic bias interpretation
- **Low confidence**: Generalization to diverse environmental conditions, parameter physical interpretability, scalability to more complex flight regimes

## Next Checks
1. **Domain randomization test**: Add wind vectors and variable launch angles to synthetic data generation, then evaluate whether systematic positive bias reduces on real flights
2. **Ablation study on real flights**: Systematically remove one feature at a time from the 5D input during real flight prediction to quantify each constraint's contribution
3. **Temporal consistency check**: Use parameter estimates from early flights to predict outcomes on later flights (or vice versa) to test temporal stability of the learned inverse mapping