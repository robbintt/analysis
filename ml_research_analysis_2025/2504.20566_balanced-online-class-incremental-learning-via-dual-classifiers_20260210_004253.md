---
ver: rpa2
title: Balanced Online Class-Incremental Learning via Dual Classifiers
arxiv_id: '2504.20566'
source_url: https://arxiv.org/abs/2504.20566
tags:
- learning
- buffer
- bison
- split
- classifier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of balancing plasticity and
  stability in online class-incremental learning (OCIL). The authors propose BISON,
  a novel replay-based method that employs dual classifiers with inclusive training
  separation and implicit knowledge interaction.
---

# Balanced Online Class-Incremental Learning via Dual Classifiers

## Quick Facts
- arXiv ID: 2504.20566
- Source URL: https://arxiv.org/abs/2504.20566
- Authors: Shunjie Wen; Thomas Heinis; Dong-Wan Choi
- Reference count: 40
- Primary result: Dual-classifier architecture with inclusive separation and implicit knowledge interaction achieves state-of-the-art balance between plasticity and stability in online class-incremental learning.

## Executive Summary
This paper addresses the challenge of balancing plasticity and stability in online class-incremental learning (OCIL). The authors propose BISON, a novel replay-based method that employs dual classifiers with inclusive training separation and implicit knowledge interaction. This approach enables effective integration of knowledge from both old and new classes while maintaining a balanced performance. BISON uses redesigned proxy-anchor loss and proxy alignment feedback for implicit knowledge exchange between the dual classifiers. Extensive experiments on three benchmark datasets demonstrate that BISON outperforms state-of-the-art replay-based OCIL methods, achieving the best balance between plasticity (average intransigence) and stability (average forgetting) while maintaining the highest average accuracy.

## Method Summary
BISON introduces a dual-classifier architecture for online class-incremental learning that decouples plasticity (learning new classes) from stability (retaining old class knowledge). The method employs two separate classifiers - a stream classifier for incoming data and a buffer classifier for replay - that share a common backbone feature extractor. The key innovation is inclusive structural separation that maintains knowledge interaction through redesigned proxy-anchor loss (PAL) and proxy alignment feedback (PAF). PAL enables implicit forward transfer by treating stream classifier weights as proxies for old classes, while PAF provides backward knowledge transfer by aligning buffer classifier weights to the refined stream classifier. The method also uses nearest class mean (NCM) inference to improve robustness against bias.

## Key Results
- BISON achieves the best balance between plasticity (average intransigence) and stability (average forgetting) compared to state-of-the-art replay-based OCIL methods.
- The method maintains the highest average accuracy across benchmark datasets (Split CIFAR-10, CIFAR-100, and Mini-ImageNet).
- Removing PAL increases Average Intransigence from 5.9% to 6.5% and decreases Average Accuracy, demonstrating its importance for plasticity.
- Using NCM inference instead of linear classifiers significantly improves stability performance.

## Why This Works (Mechanism)

### Mechanism 1: Inclusive Structural Separation via Dual Classifiers
The architecture utilizes a stream classifier ($W_{str}$) trained primarily on incoming data and a buffer classifier ($W_{buf}$) trained on replay data. Both share a backbone feature extractor, allowing the stream classifier to adapt rapidly without immediately overwriting the decision boundaries maintained by the buffer classifier. This structural separation clearly allows the independent training of two tasks without interference.

### Mechanism 2: Implicit Forward Transfer via Redesigned Proxy-Anchor Loss (PAL)
The loss function $L_{PAL}$ calculates a metric learning loss between buffer features ($z_M$) and the weights of the stream classifier ($W_{str}$). This forces the stream classifier to act as a set of anchors for old classes, effectively "pulling" old class regions into its decision boundary even though it processes stream data. The stream classifier can fully utilize the knowledge of buffer samples without explicitly training them on it using cross-entropy loss.

### Mechanism 3: Backward Knowledge Transfer via Proxy Alignment Feedback (PAF)
After the stream classifier updates, a cosine similarity loss ($L_{Align}$) pulls the weights of the buffer classifier toward the stream classifier for classes present in the buffer batch. The stream weights are treated as a frozen "teacher," transferring prototype-level information to improve the buffer classifier's representation of old classes.

## Foundational Learning

- **Concept: Stability-Plasticity Dilemma**
  - Why needed here: BISON is explicitly designed to solve the trade-off where learning new classes (plasticity) degrades old class performance (stability).
  - Quick check question: Can you define "Average Intransigence" and "Average Forgetting" in the context of this trade-off?

- **Concept: Metric Learning / Proxy Anchors**
  - Why needed here: The mechanism relies on "Proxies" (learned vectors) rather than just sample-to-sample comparisons.
  - Quick check question: In Equation 3, does the loss push the embedding closer to the proxy or further away?

- **Concept: Nearest Class Mean (NCM) Classifier**
  - Why needed here: BISON trains using cross-entropy and proxy losses but performs inference using NCM.
  - Quick check question: Why does the paper argue that NCM is more robust against bias at inference time than the standard linear classifiers used during training?

## Architecture Onboarding

- **Component map:**
  - Backbone (ResNet-18) -> Dual Heads (Stream Classifier + Buffer Classifier) -> Memory Buffer (Reservoir sampling) -> NCM Logic (Inference-time aggregation)

- **Critical path:**
  1. Batch arrives (Stream + Buffer)
  2. Backbone extracts features
  3. Parallel Forward Pass: Stream Head processes Stream features; Buffer Head processes Buffer features
  4. Cross-Interaction: Buffer features interact with Stream Head weights (PAL); Stream Head weights act as target for Buffer Head weights (PAF)
  5. Loss Aggregation ($L_{BISON}$)

- **Design tradeoffs:**
  - NCM vs. Linear Inference: The paper shows NCM improves stability significantly but adds computation overhead at inference time.
  - Separation Smoother ($\alpha$): A learned parameter that dictates how much buffer data leaks into stream training.

- **Failure signatures:**
  - High Forgetting: Likely due to Backbone drifting too fast. Check if $L_{Align}$ is too weak.
  - High Intransigence: Likely due to excessive regularization from PAL or strong separation. Check $\beta$ (PAL coefficient).
  - Collapse to Bias: Using Linear classifier instead of NCM will likely predict only the newest classes.

- **First 3 experiments:**
  1. Sanity Check: Run vanilla ER with NCM inference on Split CIFAR-10 to confirm the "Boost" comes from the method.
  2. Component Ablation: Run BISON on Split CIFAR-100 with $M=1k$. Remove $L_{PAL}$ and observe the drop in Average Accuracy.
  3. Balance Visualization: Replicate Figure 4 on Split Mini-ImageNet. Plot AF vs. AI. Verify BISON lands in the bottom-left corner compared to "Stream-Only" baseline.

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- The efficacy of proxy-anchor loss applied to classifier weights remains conceptually distinct from standard applications, lacking ablation studies isolating this novelty.
- Balance plots rely on specific metric formulations; alternative forgetting/intransigence definitions could alter perceived balance rankings.
- The claim that NCM is "more robust" against bias than linear classifiers is demonstrated empirically but lacks theoretical grounding or comparison to other robust inference schemes.

## Confidence

- **High:** Dual-head separation reduces interference; BISON improves average accuracy over ER; stream-to-buffer alignment (bs) is superior to buffer-to-stream (sb).
- **Medium:** Proxy-anchor loss on classifier weights improves plasticity; NCM inference boosts stability; balance plot rankings are robust.
- **Low:** Theoretical justification for why NCM is more robust than linear inference; exact mechanism of weight-as-proxy alignment without feature-space training.

## Next Checks

1. Run BISON with a variant where the stream head is explicitly trained on buffer samples via cross-entropy, then compare PAL vs. no-PAL to isolate whether proxy loss is necessary.
2. Replace NCM inference with a linear classifier and benchmark on Split CIFAR-100 to quantify the exact stability gain attributed to NCM vs. architecture.
3. Test BISON on a long task sequence (e.g., 25 tasks) to evaluate whether buffer alignment mechanism prevents catastrophic forgetting in the long tail.