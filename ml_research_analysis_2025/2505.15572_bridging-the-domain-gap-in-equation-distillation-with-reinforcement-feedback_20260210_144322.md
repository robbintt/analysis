---
ver: rpa2
title: Bridging the Domain Gap in Equation Distillation with Reinforcement Feedback
arxiv_id: '2505.15572'
source_url: https://arxiv.org/abs/2505.15572
tags:
- data
- equation
- symbolic
- reel
- regression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting foundation models
  for data-to-equation (Data2Eqn) tasks, where models must generate interpretable
  mathematical equations from observed data. The core issue is that foundation models
  pretrained on general-purpose data distributions struggle with domain-specific tasks
  due to token-level training objectives that overlook mathematical semantics, leading
  to inaccurate equations.
---

# Bridging the Domain Gap in Equation Distillation with Reinforcement Feedback

## Quick Facts
- arXiv ID: 2505.15572
- Source URL: https://arxiv.org/abs/2505.15572
- Reference count: 40
- One-line primary result: REEL achieves R² > 0.99 in 90.8% of Feynman equations, significantly outperforming the baseline (81.5%) while offering faster inference than genetic programming methods.

## Executive Summary
This paper addresses the challenge of adapting foundation models for data-to-equation (Data2Eqn) tasks, where models must generate interpretable mathematical equations from observed data. The core issue is that foundation models pretrained on general-purpose data distributions struggle with domain-specific tasks due to token-level training objectives that overlook mathematical semantics, leading to inaccurate equations. The authors propose REEL, a reinforcement learning-based fine-tuning framework that optimizes the generation policy of a pretrained model using reward signals derived from downstream numerical fitness. By sampling diverse data subsets and applying reinforcement learning, REEL aligns equation generation with domain-specific patterns and mathematical semantics. Extensive experiments show significant improvements: REEL achieves R² > 0.99 in 90.8% of Feynman equations (vs. 81.5% for the baseline), maintains strong performance under noise, and offers competitive accuracy with much faster inference times than traditional genetic programming methods. The approach demonstrates that reward-driven fine-tuning effectively enhances the accuracy and robustness of symbolic regression models in complex domains.

## Method Summary
REEL is a reinforcement learning fine-tuning framework that adapts a pretrained Data2Eqn foundation model for domain-specific symbolic regression. The method samples 128 subsets of 200 data points each from the training data, using each subset as a distinct environment for RL training. The model generates candidate equations which are evaluated using a smoothed R² reward function. Optimization uses a PPO-style update with clipped surrogate loss and KL divergence regularization (β=0.2) to prevent catastrophic forgetting while adapting to domain-specific patterns. The approach shifts computational complexity to training while enabling extremely fast inference, as equations are generated in a single pass rather than through iterative genetic programming.

## Key Results
- REEL achieves R² > 0.99 in 90.8% of Feynman equations versus 81.5% for the baseline model
- The framework maintains strong performance under noisy conditions, demonstrating robustness to real-world data variations
- REEL offers inference speeds competitive with or faster than genetic programming methods while achieving comparable or better accuracy
- The approach successfully adapts foundation models to domain-specific equation discovery tasks where pretraining alone proves insufficient

## Why This Works (Mechanism)

### Mechanism 1: Semantic Alignment via Numerical Fitness Rewards
The framework replaces token-level loss with R²-based rewards to align generation with mathematical semantics. The environment evaluates candidate equations against ground truth data, providing scalar rewards that guide policy updates. This reward is smoothed with sigmoid and weighted by endorsement confidence to prevent straying from pretrained knowledge without improving accuracy. The approach assumes R² provides sufficient gradient signal for navigating symbolic equation spaces.

### Mechanism 2: Robustness via Trajectory Diversification (Bootstrap Sampling)
Sampling diverse data subsets creates varied RL trajectories that prevent overfitting to specific noise patterns. Each subset serves as a distinct environment, forcing the model to recover underlying equation structure across multiple data views. This bootstrap sampling approach introduces variability similar to ensemble methods. The method assumes mathematical laws remain consistent across different row-subsets of the data.

### Mechanism 3: Knowledge Retention via KL-Regularized Policy Optimization
KL divergence regularization prevents catastrophic forgetting by constraining policy updates to stay close to pretrained weights. The optimization combines clipped surrogate loss with KL penalty, maintaining syntactic validity while adapting semantic content. The approach assumes pretrained weights contain useful general priors about mathematical syntax that should not be discarded during domain adaptation.

## Foundational Learning

- **Concept**: Symbolic Regression (Data2Eqn)
  - **Why needed here**: This is the core task. The goal is not just to predict y (regression), but to generate the explicit mathematical expression f(X) → y.
  - **Quick check question**: Can you distinguish between training a model to minimize MSE on y versus training a model to output a string of tokens representing an equation?

- **Concept**: Reinforcement Learning (PPO/Policy Gradients)
  - **Why needed here**: The paper frames equation generation as a sequential decision process. Understanding "Agent," "Environment," "Reward," and "Policy" is required to grasp how the model improves without standard supervised labels.
  - **Quick check question**: Can you explain why a "reward signal" (scalar) is used here instead of a "loss function" (gradient over tokens) to update the weights?

- **Concept**: Transformer Encoder-Decoder Architectures
  - **Why needed here**: The "Agent" is specifically a Transformer. The encoder processes numerical data pairs (X, y), and the decoder generates the equation sequence.
  - **Quick check question**: How does the "context" (input data embeddings) interact with the "policy" (decoder probabilities) in this architecture?

## Architecture Onboarding

- **Component map**: Foundation Model (E2E Transformer) -> Data Loader (Bootstrap Sampler) -> Environment (Symbolic Regression Task) -> Reward Function (Smoothed R²) -> Optimizer (PPO with KL)
- **Critical path**: 1. Sampled subset (X_i, y_i) enters Encoder. 2. Decoder generates token sequence (equation). 3. Environment parses tokens → equation → computes R² reward. 4. Compute Loss = (Clipped Reward Reweighing) + β·(KL Divergence). 5. Backpropagate to update Agent weights.
- **Design tradeoffs**: Inference Speed vs. Training Overhead (shifts complexity to training for fast inference), Stability vs. Plasticity (controlled by β parameter).
- **Failure signatures**: Reward Hacking (exploiting numerical artifacts), Training Instability (diverging reward signals), Syntax Invalidity (generating incorrect equations).
- **First 3 experiments**: 1. Baseline Reproduction (E2E vs REEL on Feynman dataset). 2. Ablation on Regularization (test β=0 to verify stability). 3. Noise Robustness Check (compare degradation under Gaussian noise).

## Open Questions the Paper Calls Out
- **Open Question 1**: How can the framework be extended to handle high-dimensional tabular inputs where the number of variables significantly exceeds ten?
  - **Basis in paper**: The authors state in Section 6 that the underlying foundation model constrains inputs to fewer than ten variables due to the difficulty of processing long token sequences.
- **Open Question 2**: Can this reinforcement learning fine-tuning strategy generalize effectively to other foundation model architectures, such as Large Language Models (LLMs)?
  - **Basis in paper**: The implementation strictly validates the method using the E2E Transformer backbone; its applicability to other generative architectures like LLMs or RNNs is not tested (Section 4.1).
- **Open Question 3**: Would incorporating a complexity penalty into the reward function improve the discovery of parsimonious equations?
  - **Basis in paper**: The proposed reward function relies solely on numerical fitness (R2 score), which may incentivize the model to generate overly complex equations that overfit the training data rather than simple, physically meaningful ones (Section 3.3.1).

## Limitations
- The approach inherits limitations from the underlying E2E transformer, including potential biases from pretraining data and constraints on equation complexity due to maximum sequence length
- The effectiveness of a single R² metric as a dense reward signal for navigating the combinatorial space of symbolic equations remains an open question, particularly for discontinuous or multi-modal target functions
- While results are strong on benchmark datasets, the robustness claims under real-world noise conditions need validation across more diverse and challenging datasets

## Confidence
- **High Confidence**: The KL-regularized policy optimization mechanism - this is a standard RL technique with well-understood properties and clear theoretical justification
- **Medium Confidence**: The trajectory diversification through bootstrap sampling - while the concept is sound, the specific implementation details and their impact on different dataset characteristics need more exploration
- **Medium Confidence**: The overall performance improvements - while results are strong on benchmark datasets, the extent to which these generalize to real-world scientific discovery tasks remains to be seen

## Next Checks
1. **Cross-Dataset Transfer**: Test REEL on datasets from different scientific domains (e.g., chemistry, biology) to validate that the domain adaptation generalizes beyond physics equations
2. **Reward Function Ablation**: Compare REEL's performance using alternative reward signals (e.g., mean absolute error, prediction interval coverage) to isolate the contribution of the R² reward choice
3. **Foundation Model Robustness**: Systematically evaluate how REEL's performance varies with different pretrained foundation models (varying sizes, pretraining objectives, and data distributions) to understand the approach's dependence on specific model characteristics