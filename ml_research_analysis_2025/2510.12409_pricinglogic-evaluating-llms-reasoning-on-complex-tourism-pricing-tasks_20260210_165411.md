---
ver: rpa2
title: 'PricingLogic: Evaluating LLMs Reasoning on Complex Tourism Pricing Tasks'
arxiv_id: '2510.12409'
source_url: https://arxiv.org/abs/2510.12409
tags:
- price
- pricing
- llms
- code
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'PricingLogic evaluates whether LLMs can reliably automate tourism
  pricing when multiple, overlapping fare rules apply. The benchmark includes 300
  questions derived from 42 real-world pricing policies across two task types: basic
  customer-type pricing and bundled-tour calculations with interacting discounts.'
---

# PricingLogic: Evaluating LLMs Reasoning on Complex Tourism Pricing Tasks

## Quick Facts
- arXiv ID: 2510.12409
- Source URL: https://arxiv.org/abs/2510.12409
- Reference count: 7
- Primary result: LLMs struggle with complex tourism pricing tasks involving multiple, overlapping fare rules, with accuracy dropping significantly on challenging questions requiring optimal rule selection.

## Executive Summary
PricingLogic evaluates whether LLMs can reliably automate tourism pricing when multiple, overlapping fare rules apply. The benchmark includes 300 questions derived from 42 real-world pricing policies across two task types: basic customer-type pricing and bundled-tour calculations with interacting discounts. Evaluations show that even state-of-the-art LLMs struggle with complex pricing tasks, with accuracy dropping significantly on challenging questions involving multiple customer types and overlapping discounts. Code-assisted reasoning improves performance by separating policy interpretation from parameter extraction, but fundamental reasoning limitations remain, particularly for tasks requiring selection among competing discount rules. These findings indicate that LLMs currently cannot reliably handle revenue-critical pricing tasks without further safeguards or domain adaptation.

## Method Summary
The benchmark uses 42 real-world tourism pricing policies and 300 natural language booking questions (150 per task type). Two evaluation pipelines are compared: E2E (single prompt for both price identification and calculation) and Code-assisted Reasoning (CaR) which separates policy interpretation from parameter extraction. CaR generates Python calculator functions from policies, then parses booking requests to extract parameters for execution. A CaR-Oracle condition uses human-written code to isolate parameter extraction errors. Models tested include GPT-4o, DeepSeek-V3, Claude-Sonnet, and Qwen variants with temperature=0.0.

## Key Results
- LLMs achieve high accuracy (>90%) on simple pricing questions but drop below 50% on challenging tasks requiring multiple constraints
- Code-assisted reasoning consistently improves performance across models, with GPT-4o accuracy increasing from 81.67% to 96.67% on simple tasks
- Even with human-verified code (CaR-Oracle), models fail to correctly extract parameters from complex booking descriptions on challenging questions
- Systematic failures occur in identifying customer categories, calculating demographic ratios, and selecting optimal bundle options among competing discount rules

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Code-assisted reasoning improves pricing task performance by separating policy interpretation from parameter extraction.
- **Mechanism**: The CaR approach decomposes the task into two stages: (1) LLMs generate Python calculator functions from pricing policies, encoding conditional rules; (2) LLMs parse booking requests and extract parameters to execute those functions. This reduces cognitive load by isolating logical rule-encoding from natural language understanding.
- **Core assumption**: Error sources are separable—code generation errors and parameter extraction errors can be independently diagnosed and mitigated.
- **Evidence anchors**:
  - [abstract]: "Code-assisted reasoning improves performance by separating policy interpretation from parameter extraction"
  - [Table 2/3]: CaR shows consistent gains over E2E across most models (e.g., GPT-4o: 81.67% → 96.67% on simple Task 1 questions)
  - [corpus]: Related work (Chen et al., 2022; Gao et al., 2023) confirms code assistance helps on computational tasks, though tourism-specific complexity exceeds textbook problems
- **Break condition**: When models cannot correctly map booking information to function arguments even with human-verified code (observed in challenging questions where CaR-Oracle shows minimal improvement).

### Mechanism 2
- **Claim**: Performance degradation on complex questions reveals systematic failures in multi-constraint reasoning and optimal rule selection.
- **Mechanism**: As questions scale in difficulty (simple → medium → challenging), models must simultaneously handle: (a) larger groups (25-55 visitors), (b) diverse demographics requiring ratio calculations (e.g., "at least 70% seniors"), (c) region-specific pricing, (d) overlapping discount conditions with precedence rules. Models frequently misidentify customer categories and overlook pricing conditions.
- **Core assumption**: Difficulty scaling primarily tests reasoning depth, not just working memory or context length.
- **Evidence anchors**:
  - [abstract]: "steep performance drop on the harder tier, exposing systematic failures in rule interpretation and arithmetic reasoning"
  - [Section 4.2]: "for challenging questions, all LLMs barely exceed 50% accuracy"
  - [Section 4.3]: Models struggle with "identifying when bundled discounts should override other customer-type pricing" and "calculating the optimal combination when multiple valid bundle options exist"
  - [corpus]: RuleArena (Zhou et al., 2024) shows similar rule-following challenges, but PricingLogic adds competing-rule selection complexity
- **Break condition**: When multiple valid pricing options exist and models must identify the customer-favorable choice among them.

### Mechanism 3
- **Claim**: Deep task comprehension—not code quality—is the primary bottleneck on challenging pricing scenarios.
- **Mechanism**: The CaR-Oracle condition uses human-verified code, eliminating code generation errors. Yet challenging-task performance barely improves (e.g., GPT-4o: 55% CaR → 50% CaR-Oracle), indicating models fail to extract correct parameters from complex booking descriptions even when calculation logic is correct.
- **Core assumption**: Human-written code correctly encodes all policy rules and edge cases.
- **Evidence anchors**:
  - [Section 4.2]: "oracle code offers little improvement: even with human-written code, models fail to supply correct arguments, indicating that deep task comprehension remains the main bottleneck"
  - [Table 2]: CaR-Oracle challenging-task accuracy ranges 50-62.5% across models—minimal gain over CaR
  - [Section 4.2]: Qwen2.5-7B degrades with oracle code because "the model struggles to interpret these more elaborate implementations"
  - [corpus]: Insufficient direct corpus evidence on comprehension bottlenecks in code-assisted reasoning
- **Break condition**: When booking descriptions contain many interacting variables that must be correctly parsed and mapped to function signatures.

## Foundational Learning

- **Concept: Conditional rule precedence and conflict resolution**
  - Why needed here: Tourism pricing involves overlapping discount rules where multiple conditions may apply simultaneously; selecting the optimal (customer-favorable) price requires understanding precedence.
  - Quick check question: Given a group of 25 with 12 students and 6 seniors staying at a designated hotel, which discount applies: student-group rate, senior-group rate, or accommodation package?

- **Concept: Code-as-reasoning vs. direct inference**
  - Why needed here: CaR delegates computation to Python interpreters, leveraging LLMs' code-generation strength while avoiding arithmetic errors in direct text generation.
  - Quick check question: Why might an LLM correctly generate `if elderly_ratio >= 0.7: price = 40` but incorrectly compute "30 seniors out of 40 visitors qualifies for senior pricing"?

- **Concept: Parameter extraction from structured natural language**
  - Why needed here: CaR's second stage requires parsing booking requests (e.g., "25 tourists including 12 students") into function arguments (`num_people=25, student_count=12, is_student_team=True`).
  - Quick check question: From "12 tourists (non-contract customers from Essex) visiting Brighton Cave and St. Elvi Ancient Village," extract all parameters needed for a multi-attraction pricing function.

## Architecture Onboarding

- **Component map**:
  - **E2E Pipeline**: Single prompt → LLM identifies projects + calculates prices → text output
  - **CaR Pipeline**: Stage 1 (policy → Python calculator functions) → Stage 2 (booking text → project identification) → Stage 3 (booking text + calculator code → parameter extraction → execution) → price output
  - **CaR-Oracle**: Same as CaR but Stage 1 uses human-written code instead of LLM-generated

- **Critical path**: CaR Stage 3 (parameter extraction) is the bottleneck on challenging questions. Error analysis shows models fail to (1) correctly interpret complex booking descriptions and (2) map extracted information to function signatures.

- **Design tradeoffs**:
  - E2E: Simpler deployment, no interpreter dependency, but higher error rates on computation
  - CaR: Better accuracy, interpretable intermediate code, but requires Python execution environment and adds latency
  - CaR-Oracle: Isolates parameter-extraction errors, useful for diagnosis, but impractical for production (requires manual coding per policy)

- **Failure signatures**:
  - Customer category misidentification (e.g., treating "contract customers" as "non-contract group")
  - Overlooked conditions (e.g., missing accommodation exemptions or group-size thresholds)
  - Incorrect ratio calculations (e.g., failing to verify "at least 80% students")
  - Suboptimal bundle selection when multiple discount options exist

- **First 3 experiments**:
  1. **Reproduce Task 1 baseline**: Run E2E and CaR on simple/medium/challenging splits with 2-3 models (e.g., GPT-4o, DeepSeek-V3, Qwen2.5-Max) to validate benchmark difficulty and confirm CaR gains.
  2. **Error taxonomy analysis**: Manually classify failures on challenging questions into: (a) rule misinterpretation, (b) arithmetic errors, (c) parameter extraction failures, (d) bundle-selection errors—to prioritize improvement targets.
  3. **Ablate parameter extraction**: Test whether providing ground-truth function arguments (bypassing extraction) achieves near-perfect accuracy on CaR-Oracle, confirming comprehension as the bottleneck.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can parameter-efficient fine-tuning (PEFT) methods overcome the reasoning deficits in PricingLogic without incurring the retraining overhead described as impractical?
  - Basis in paper: [explicit] The Limitations section states fine-tuning was excluded because it requires retraining whenever pricing policies change, making it "impractical in dynamic business environments."
  - Why unresolved: The authors did not evaluate any fine-tuning approaches, leaving the trade-off between adaptation speed and accuracy gains unexplored.
  - What evidence would resolve it: An experiment measuring the performance and update latency of LoRA or adapter-based methods on PricingLogic under simulated policy changes.

- **Open Question 2**: How can the "deep task comprehension" bottleneck be mitigated when models must map complex natural language orders to function arguments?
  - Basis in paper: [inferred] Section 4.2 notes that in the CaR-Oracle setting (human-verified code), models still failed to supply correct arguments for challenging tasks, identifying comprehension as the main failure point.
  - Why unresolved: While code-assisted reasoning separated policy interpretation from calculation, the paper offers no solution for the remaining difficulty in extracting structured parameters from complex text.
  - What evidence would resolve it: A study introducing intermediate reasoning layers or schema-guided extraction prompts specifically targeted at the parameter mapping phase.

- **Open Question 3**: Do the systematic failures in "selection among competing discount rules" generalize to other domains with overlapping constraints, such as tax law or logistics?
  - Basis in paper: [inferred] The Key Outcome identifies "selection among competing discount rules" as a specific failure mode, while the Abstract frames the task as representative of "revenue-critical" applications.
  - Why unresolved: The benchmark is domain-specific (tourism), so it is unclear if this logical failure is a limitation of the models' general reasoning or their handling of tourism-specific semantics.
  - What evidence would resolve it: A cross-domain evaluation using the CaR methodology on datasets with similar "best-option" logic but different semantic content (e.g., legal clauses or insurance policies).

## Limitations
- Data transparency issues: Exact policies and questions not fully disclosed, making independent validation difficult
- Model generalizability concerns: Results tied to specific model versions and may not hold for newer models
- Binary evaluation: Exact match accuracy without partial credit potentially understates model capabilities
- Real-world complexity gap: Curated policies may not capture full messiness of actual tourism pricing scenarios

## Confidence
- **High**: Performance degradation on challenging questions; CaR improves accuracy on simple/medium tasks
- **Medium**: Deep comprehension as the primary bottleneck on challenging tasks; systematic failures in rule interpretation
- **Low**: The extent to which CaR-Oracle results definitively prove comprehension over code quality issues

## Next Checks
1. **Reproduce Task 1 baseline**: Run E2E and CaR on simple/medium/challenging splits with 2-3 models to validate benchmark difficulty and confirm CaR gains.
2. **Error taxonomy analysis**: Manually classify failures on challenging questions into: (a) rule misinterpretation, (b) arithmetic errors, (c) parameter extraction failures, (d) bundle-selection errors—to prioritize improvement targets.
3. **Ablate parameter extraction**: Test whether providing ground-truth function arguments (bypassing extraction) achieves near-perfect accuracy on CaR-Oracle, confirming comprehension as the bottleneck.