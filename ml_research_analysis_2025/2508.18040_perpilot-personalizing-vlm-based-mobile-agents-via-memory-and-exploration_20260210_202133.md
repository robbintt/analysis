---
ver: rpa2
title: 'PerPilot: Personalizing VLM-based Mobile Agents via Memory and Exploration'
arxiv_id: '2508.18040'
source_url: https://arxiv.org/abs/2508.18040
tags:
- instruction
- personalized
- name
- open
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PerPilot, a plug-and-play framework enabling
  VLM-based mobile agents to handle personalized user instructions. PerPilot uses
  memory-based retrieval and reasoning-based exploration to complete ambiguous, user-specific
  elements.
---

# PerPilot: Personalizing VLM-based Mobile Agents via Memory and Exploration

## Quick Facts
- arXiv ID: 2508.18040
- Source URL: https://arxiv.org/abs/2508.18040
- Reference count: 11
- Primary result: Up to 68% success rate on personalized mobile instructions, vs. 12% baseline

## Executive Summary
PerPilot introduces a plug-and-play framework enabling VLM-based mobile agents to handle personalized user instructions containing ambiguous, user-specific elements. It combines memory-based retrieval and reasoning-based exploration to autonomously complete tasks like "order my usual takeout" by identifying personalized elements and either retrieving them from memory or exploring apps to find them. Experiments on the PerInstruct benchmark (75 tasks across 27 apps) show agents with PerPilot achieve 68% success rate, compared to 12% baseline, with steady improvement over time as memory accumulates. The framework is compatible with existing agents and shows near-parity between fine-tuned open-source PerQwen and closed-source o4-mini models.

## Method Summary
PerPilot is a two-module framework that plugs into existing VLM-based mobile agents to handle personalized instructions. The Perception module uses an LLM to identify personalized elements (e.g., "mom," "usual") in instructions. The Completion module then either retrieves these elements from a persistent memory database or, if not found, uses reasoning-based exploration to search relevant apps. Successful explorations update the memory for future use. The framework was evaluated on PerInstruct, a benchmark of 75 personalized tasks across 27 apps, using agents like UI-TARS, MobileAgent-v2, and AppAgent. PerQwen was fine-tuned on 10,000 generated samples using LoRA with specified hyperparameters.

## Key Results
- PerPilot achieves 68% success rate on PerInstruct benchmark, vs. 12% baseline without personalization
- Open-source PerQwen model performs nearly as well as closed-source o4-mini (62.7% vs. 68%)
- Progressive improvement observed as memory accumulates, reducing need for exploration over time

## Why This Works (Mechanism)

### Mechanism 1: Memory Accumulation Reduces Exploration Friction
If personalized data is successfully retrieved during an initial interaction, storing it in a persistent database T reduces the need for costly reasoning-based exploration in subsequent tasks, conditional on the stability of user preferences. PerPilot implements a "Memory-based Retrieval" loop where successful exploration results are stored, and future instructions trigger a lookup before attempting exploration. As the memory grows, reliance on reasoning/exploration decreases. Core assumption: user-specific elements remain stable across sessions.

### Mechanism 2: LLM-Guided App Exploration for Context Resolution
When memory fails, an LLM can reason about app functionality to generate targeted exploration instructions (e.g., "Check shopping app for address"), resolving ambiguity without immediate human intervention. The "Reasoning-based Exploration" module prompts an LLM with the ambiguous element and a list of installed apps, outputting a specific command directing the agent to search a specific app. This bridges the gap between "ambiguous intent" and "specific GUI action." Core assumption: the necessary personalized information exists within accessible apps on the device.

### Mechanism 3: Segmented Instruction Perception
Treating instructions as composite units containing both executable commands and "personalized slots" allows the system to decouple execution logic from context retrieval. The "Personalization Perception" module uses an LLM to parse instructions and tag specific terms as personalized elements. By isolating these elements, the agent can execute the "fixed" parts of the instruction while delegating the "variable" parts to the Completion module. Core assumption: the LLM can reliably distinguish between proper nouns and personalized references.

## Foundational Learning

- **Plug-and-Play Agent Frameworks**: PerPilot is not a standalone agent but wraps existing agents (AppAgent, MobileAgent). Understanding the "wrapper" or "middleware" pattern is essential to see how personalization logic is injected without rewriting the base agent.
  - Quick check: Can you identify where PerPilot ends and the base VLM agent (e.g., UI-TARS) begins in the execution loop?

- **Entity Extraction / Slot Filling**: The perception module acts as an entity extractor. You must understand how LLMs map natural language to structured schema (Instruction -> {Element 1, Element 2}) to debug perception failures.
  - Quick check: If the instruction is "Order my usual," what is the extracted personalized element?

- **Exploration vs. Exploitation in Agents**: PerPilot balances exploring the device for info vs. exploiting stored memory. This trade-off is central to its "progressive improvement."
  - Quick check: Does the system favor memory or exploration when first introduced to a new user?

## Architecture Onboarding

- **Component map**: User Instruction -> Perception Module (LLM) -> Extracts personalized elements {k_i} -> Completion Module (Memory DB checks {k_i}, Explorer (LLM) generates search instruction if miss) -> Executor (Base Agent) -> Feedback Loop (updates Memory DB if successful)

- **Critical path**: The Exploration Instruction Generation. If the LLM picks the wrong app (e.g., looking for "Mom" in a Calculator app), the downstream agent execution will fail, regardless of the base agent's capabilities.

- **Design tradeoffs**:
  - Closed-source (o4-mini) vs. Open-source (PerQwen): Performance gap (68% vs. 62.7%) vs. privacy/local control. PerQwen requires fine-tuning data; o4-mini requires API access.
  - Human-in-the-loop: The system prompts the user if exploration fails. Too many prompts ruins UX; too few risks hallucination.

- **Failure signatures**:
  - App Selection Hallucination: Explorer suggests an app not on the device.
  - Stale Memory: Agent uses an old address because the memory invalidation logic is missing.
  - Perception Blindness: Agent tries to execute "Play favorite song" literally because the Perception module missed the "favorite" tag.

- **First 3 experiments**:
  1. Ablation on Memory: Run PerPilot with empty memory DB vs. pre-filled DB to measure efficiency gain (step count reduction) from memory retrieval.
  2. PerQwen vs. o4-mini Robustness: Test both models on "Hard" instructions (multi-app) to see if fine-tuned open-source model closes gap on complex reasoning.
  3. Intervention Frequency: Measure how often "Human Intervention" fallback is triggered for unique vs. common personalized elements (e.g., "Mom" vs. "My local coffee shop").

## Open Questions the Paper Calls Out

### Open Question 1
How does PerPilot handle dynamic user preferences where stored memory entries become outdated or conflicting? The paper assumes personalized information is "typically stable and consistent," focusing on accumulation rather than memory update or deletion strategies. The retrieval mechanism prioritizes existing memory without a mechanism to verify recency or handle drift, which could persistently degrade performance. Evidence would require evaluation on benchmarks with time-sensitive preference shifting.

### Open Question 2
Can the exploration module effectively resolve personalized elements requiring multi-hop reasoning across several apps? The exploration logic maps each personalized element to a single "most likely" app, limiting retrieval to one source. Real-world scenarios may require synthesizing information from multiple sources (e.g., finding a contact in one app to determine a shipping address in another), which the current single-app selection process does not support. Evidence would require performance analysis on tasks specifically designed to require cross-app synthesis.

### Open Question 3
To what extent does the open-source PerQwen model generalize to personalized instructions in apps unseen during training? While PerQwen approaches closed-source performance, the paper evaluates on the PerInstruct benchmark which has a fixed app distribution, leaving cross-domain generalization unverified. Fine-tuning on a specific dataset risks overfitting to the apps and instruction styles within that dataset. Evidence would require zero-shot testing on a separate, larger-scale personalized benchmark involving different applications.

## Limitations
- Evaluation relies on a single device (Huawei nova13/HarmonyOS) and may not generalize across Android versions or OEM UI skins.
- Memory staleness is not addressed: the framework assumes user preferences remain stable, but no invalidation or expiration policy is specified.
- The perception module's reliability is untested on ambiguous natural language cases like "order my usual" or "play that song."

## Confidence

- **High confidence** in the plug-and-play integration approach and the empirical success rate comparison (68% vs. 12% baseline) given the benchmark and methodology are clearly described.
- **Medium confidence** in the progressive improvement claim: while memory accumulation is logical, the paper doesn't show statistical evidence of steady improvement over repeated sessions with the same user.
- **Medium confidence** in the open-source model parity: PerQwen performs close to o4-mini, but the fine-tuning data generation process and exact LoRA hyperparameters are underspecified, making reproduction difficult.

## Next Checks

1. **Memory growth curve**: Track step-count reduction and success rate improvement across multiple sessions with the same user to confirm progressive gains.

2. **Cross-device robustness**: Rerun a subset of PerInstruct on at least two different Android devices with varying OEM skins to test generalization of app exploration accuracy.

3. **Memory invalidation stress test**: Simulate preference changes (e.g., update "home" location) and measure how often stale memory causes failures before the agent updates the database.