---
ver: rpa2
title: Embedding-Driven Data Distillation for 360-Degree IQA With Residual-Aware Refinement
arxiv_id: '2412.12667'
source_url: https://arxiv.org/abs/2412.12667
tags:
- selection
- quality
- patches
- image
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of data-driven 360-degree
  image quality assessment (IQA) models caused by redundant patch sampling. The proposed
  method introduces an embedding similarity-based selection algorithm that refines
  an initial set of sampled patches into a compact, maximally informative subset.
---

# Embedding-Driven Data Distillation for 360-Degree IQA With Residual-Aware Refinement

## Quick Facts
- arXiv ID: 2412.12667
- Source URL: https://arxiv.org/abs/2412.12667
- Authors: Abderrezzaq Sendjasni; Seif-Eddine Benkabou; Mohamed-Chaker Larabi
- Reference count: 40
- Primary result: 40-50% patch reduction maintains baseline IQA performance; 20-40% reduction works for SOTA models

## Executive Summary
This paper addresses computational inefficiency in 360-degree image quality assessment by introducing an embedding similarity-based patch selection algorithm. The method identifies and removes redundant patches from sampled sets, reducing computational load while maintaining or improving quality prediction accuracy. Through a regularized optimization framework that preserves perceptual relationships in low-dimensional embedding space, the approach achieves significant performance gains with reduced computational requirements across multiple benchmark datasets.

## Method Summary
The proposed method employs an embedding-driven patch selection algorithm that operates on uniformly sampled patches from 360-degree images. It uses a joint optimization framework with ℓ2,1-norm regularization to learn a transformation matrix that preserves intrinsic perceptual relationships in a low-dimensional space while filtering redundant patches through residual analysis. The algorithm alternates between updating the transformation matrix and residual matrix until convergence, with patch selection based on residual norms. The method is integrated as a preprocessing step for both baseline and state-of-the-art IQA models.

## Key Results
- Baseline models achieve equivalent performance using only 40-50% of sampled patches
- SOTA models (CNN and transformer-based) maintain or improve performance with 20-40% reduced computational load
- Consistent results across three benchmark datasets: CVIQ, OIQA, and MVAQD
- Selection efficiency correlates with distortion type: structured distortions (JPEG/JPEG2000) enable faster saturation than distributed distortions (Gaussian noise)

## Why This Works (Mechanism)

### Mechanism 1: Low-Dimensional Similarity Structure Preservation
- Claim: Patches whose perceptual relationships are well-preserved in a low-dimensional embedding space are more informative for quality prediction.
- Mechanism: The algorithm learns a transformation matrix W that projects high-dimensional patch embeddings into a reduced space while minimizing deviation from the optimal low-rank similarity structure Z (derived from eigendecomposition). Patches that maintain their pairwise relationships under this transformation contribute small residuals.
- Core assumption: Perceptually meaningful patches cluster consistently across dimensional reduction; their similarity structure is intrinsic rather than noise.
- Evidence anchors:
  - [abstract]: "formulated as a regularized optimization problem that preserves intrinsic perceptual relationships in a low-dimensional space"
  - [Section III-D]: Eq. (5) shows reformulation approximating E_iW_i ≈ Z_i where Z_i captures optimal similarity structure
  - [corpus]: Related work "Foreground-Aware Dataset Distillation" also uses patch selection but via content-aware saliency rather than embedding structure—suggests complementary mechanisms
- Break condition: If embeddings are already maximally compressed (h ≈ d_i) or if the similarity matrix S_i has near-uniform eigenvalues, the optimization provides no discriminative signal for selection.

### Mechanism 2: Residual-Based Irrelevance Scoring
- Claim: Patches with large reconstruction residuals after optimization are redundant or irrelevant and can be safely discarded.
- Mechanism: The joint objective forces the transformation W to explain most variance; patches that cannot be reconstructed well accumulate large column norms in residual matrix R. The ℓ2-norm of each residual column serves as an "irrelevance score"—higher values indicate patches that deviate from the learned similarity structure.
- Core assumption: Redundant patches are outliers in the learned similarity manifold rather than structurally unique content.
- Evidence anchors:
  - [abstract]: "using residual analysis to identify and remove irrelevant patches"
  - [Section III-D.3]: "if the similarity structure of a patch embedding e_j is well-preserved after transformation, the residual for that patch will be small. Conversely, a large residual indicates an irrelevant patch"
  - [corpus]: Weak direct evidence; "Let's Roll a BiFTA" uses refinement for alignment but via text-visual matching, not residual analysis
- Break condition: If β is set too high (forcing all residuals to zero), selection degenerates; if too low, no patches are filtered.

### Mechanism 3: Distortion-Type-Adaptive Selection Efficiency
- Claim: Selection efficiency (how few patches needed) correlates with distortion structure—sparse/structured distortions enable faster saturation than distributed/noise-based distortions.
- Mechanism: The NAC analysis shows that JPEG/JPEG2000 artifacts create highly redundant, clustered feature manifolds where boundary patches quickly define the space. Gaussian noise introduces unique perturbations per patch, requiring more samples to cover the dense manifold.
- Core assumption: The feature manifold geometry reflects underlying distortion statistics rather than image content.
- Evidence anchors:
  - [Section IV-C.5]: "For structured/sparse distortions (JPEG, JPEG 2000), the curves exhibit the fastest saturation, reaching NAC > 0.99 by the 0.7 selection rate. [...] GN curve demonstrates the slowest saturation, never reaching 1.0"
  - [Section IV-C.5, Scenario 2]: Same distortion across different images shows consistent saturation curves—content-independence
  - [corpus]: No directly comparable distortion-aware selection analysis found
- Break condition: Assumption: If image content (not distortion) dominates feature manifold geometry, selection rates would vary unpredictably across content—contradicted by Scenario 2 results.

## Foundational Learning

- Concept: Equirectangular Projection (ERP) and spherical sampling bias
  - Why needed here: 360° images must be projected to 2D for processing; uniform ERP sampling over-represents polar regions, introducing geometric distortion that affects patch informativeness.
  - Quick check question: If you sample uniformly on a 2D ERP grid, do you get uniform coverage on the sphere?

- Concept: ℓ2,1-norm regularization for row/column sparsity
  - Why needed here: The ‖W‖2,1 term promotes feature-level sparsity; ‖R‖2,1 promotes patch-level sparsity—understanding why requires knowing this norm selects entire rows/columns.
  - Quick check question: How does ℓ2,1-norm differ from ℓ1-norm in what it forces to zero?

- Concept: Alternating minimization for non-convex biconvex problems
  - Why needed here: The joint objective is non-convex in (W,R) but convex in each variable separately; convergence guarantees require understanding this decomposition.
  - Quick check question: Why does alternating minimization converge for biconvex problems even though the joint problem is non-convex?

## Architecture Onboarding

- Component map:
Input 360° Image → Patch Sampling (Ψ) → CNN Encoder (ResNet-50) → Embeddings E
                                                                        ↓
                        ← Selected Patches ← Residual Ranking ← Joint Optimization
                                                                        ↑
                                                    Similarity Matrix S → Eigendecomp → Z

- Critical path: The residual matrix R computation (Eq. 11) is the selection bottleneck. Incorrect R updates cascade to wrong patch rankings. The transformation W update (Eq. 9) depends on R, and vice versa—alternating order matters for convergence.

- Design tradeoffs:
  - Selection rate vs. accuracy: Paper shows 40-50% patches suffice for baseline, but SOTA models need 60-80% (Table III)—tune per architecture
  - Embedding dimensionality h: Paper shows h=1 to h=2048 all work (Fig. 3), but variance increases with heterogeneous distortions—use h≤10 for efficiency
  - Distance metric: ANOVA shows no statistical significance (p≫0.05)—use Euclidean for speed

- Failure signatures:
  - High performance variance across selection rates (Fig. 3): Indicates heterogeneous distortions; increase selection rate ceiling
  - NAC curves not saturating: Dense distortion manifolds (noise); expect lower efficiency gains
  - Divergent loss (Fig. 5): Check β/α ratio; ensure D_W diagonal elements are numerically stable (add ϵ to prevent division by zero)

- First 3 experiments:
  1. **Convergence validation**: Run Algorithm 1 on 5 sample images with h=10, Euclidean distance, β=0.1; plot loss per iteration. Expect convergence in 6-8 iterations (Fig. 5). If loss oscillates, check ℓ2,1 relaxation implementation.
  2. **Selection rate sweep**: Fix sampling strategy (e.g., LAT), vary selection rate from 0.1 to 0.9 on CVIQ dataset. Compute PLCC/SRCC. Identify minimum rate exceeding baseline (expect ~0.4-0.5 per Fig. 2).
  3. **Cross-sampling robustness**: Apply same selection to ERP, LAT, and SP sampling on one dataset. Compare minimum selection rates—large differences indicate sampling-strategy-specific optimization needed.

## Open Questions the Paper Calls Out
None

## Limitations
- The effectiveness depends critically on the learned embedding space being perceptually meaningful for quality assessment; if embeddings capture irrelevant features, similarity preservation may not correlate with actual perceptual relationships.
- The ℓ2,1-norm regularization approach for joint optimization is heuristic with empirical convergence demonstrated but lacking theoretical guarantees for this specific formulation.
- The claim of universal applicability across architectures needs more diverse validation; experiments focus on CNN and transformer-based models, but different architectures may exhibit different redundancy patterns.

## Confidence

**High confidence**: The empirical demonstration that 40-50% patch reduction maintains baseline performance, and 20-40% reduction works for SOTA models when integrated. The convergence behavior in Fig. 5 and the consistent distortion-type analysis in Section IV-C.5 are well-supported.

**Medium confidence**: The mechanism explaining why structured distortions enable faster saturation (Manifold geometry reflects distortion statistics). While Scenario 2 shows content-independence, the assumption that feature manifolds directly reflect distortion statistics needs more rigorous validation.

**Low confidence**: The universal applicability claim across all IQA architectures. The paper shows success with specific CNN and transformer models but doesn't test architectures with fundamentally different patch sampling or processing strategies.

## Next Checks

1. **Cross-architecture robustness test**: Apply the method to a non-CNN/non-transformer IQA architecture (e.g., graph neural network or hybrid model) and measure if the same 20-40% reduction rates hold without performance degradation.

2. **Embedding space ablation study**: Replace ResNet-50 with alternative encoders (e.g., ViT, CLIP) and evaluate whether selection efficiency changes significantly, testing the assumption that the embedding space quality drives selection effectiveness.

3. **Theoretical convergence analysis**: Analyze the alternating minimization algorithm's convergence conditions more rigorously by testing edge cases where β approaches 0 or ∞ and measuring how quickly the objective stabilizes across different β/α ratios.