---
ver: rpa2
title: 'AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play'
arxiv_id: '2509.24193'
source_url: https://arxiv.org/abs/2509.24193
tags:
- reasoning
- answer
- acesearcher
- question
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AceSearcher is a cooperative self-play framework that trains a
  single LLM to alternate between a decomposer that breaks down complex questions
  into subquestions and a solver that integrates retrieved contexts for answer generation.
  It couples supervised fine-tuning on diverse search, reasoning, and decomposition
  tasks with reinforcement fine-tuning optimized for final answer accuracy, eliminating
  the need for intermediate annotations.
---

# AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play

## Quick Facts
- **arXiv ID:** 2509.24193
- **Source URL:** https://arxiv.org/abs/2509.24193
- **Reference count:** 40
- **Key outcome:** 7.6% average EM improvement across 10 datasets; 32B model matches DeepSeek-V3 using <5% parameters

## Executive Summary
AceSearcher introduces a cooperative self-play framework that trains a single LLM to alternate between decomposing complex questions into subquestions and solving them using retrieved contexts. The approach couples supervised fine-tuning on diverse reasoning tasks with reinforcement fine-tuning optimized for final answer accuracy, eliminating the need for intermediate supervision. Across three reasoning-intensive tasks spanning 10 datasets, AceSearcher outperforms state-of-the-art baselines, with remarkable efficiency—smaller models (1.5B, 8B) surpass larger search-augmented LLMs by up to 9×.

## Method Summary
AceSearcher uses a two-stage training approach: Stage I supervised fine-tuning (SFT) on 180K examples from context-rich QA, question decomposition, and chain-of-thought datasets, followed by Stage II reinforcement fine-tuning (RFT) using iterative preference optimization. The framework trains a single LLM to alternate between decomposer (breaking questions into subquestions) and solver (integrating retrieved contexts for answers) roles, optimizing jointly for final answer accuracy through multi-turn DPO. The approach eliminates intermediate annotations by hypothesizing that better decompositions lead to more accurate answers, validated through preference pair construction from multiple rollouts.

## Key Results
- 7.6% average exact match improvement across 10 datasets spanning multi-hop QA, fact verification, and document-level reasoning
- 32B AceSearcher model matches DeepSeek-V3 performance using less than 5% of its parameters
- 1.5B and 8B AceSearcher variants outperform existing search-augmented LLMs with up to 9× more parameters
- Strong efficiency gains: 3-5s per question vs ~1s for single-turn RAG, while achieving superior accuracy

## Why This Works (Mechanism)

### Mechanism 1: Cooperative Self-Play with Role Alternation
Training a single LLM to alternate between decomposer and solver roles creates mutual reinforcement where better decompositions improve solver accuracy, and solver feedback improves decomposition quality. The decomposer generates subquestion templates while the solver produces intermediate and final answers, both sharing parameters and optimized jointly via an objective that maximizes expected reward while minimizing KL divergence from reference policies.

### Mechanism 2: Two-Stage Fine-Tuning with Diverse Data Mixtures
Combining supervised fine-tuning on heterogeneous reasoning tasks with reinforcement fine-tuning using only final-answer rewards enables learning structured decomposition without intermediate supervision. Stage 1 uses 180K examples across context-rich QA, question decomposition, and chain-of-thought data, while Stage 2 uses multi-turn DPO on preference pairs with rewards based on final answer accuracy.

### Mechanism 3: Iterative Preference Optimization via Rollout Sampling
Constructing preference pairs from multiple rollouts (m decompositions × m' solutions per decomposition) enables stable optimization without value functions or online RL infrastructure. For each question, the framework samples candidate decompositions and solutions, constructs preference pairs from best/worst combinations, and optimizes via L_mDPO with reference policy from previous iteration.

## Foundational Learning

- **Direct Preference Optimization (DPO)**: Why needed: AceSearcher uses multi-turn DPO as the optimization algorithm for RFT. Quick check: Given preference pairs (x, g+, g-), can you derive how DPO's loss function relates to reward optimization?

- **Retrieval-Augmented Generation (RAG)**: Why needed: The framework assumes familiarity with retrieval pipelines, embedding-based passage retrieval, and multi-hop reasoning requirements. Quick check: Why does single-turn retrieval fail on multi-hop questions like "Which film has the director who was born later, The Silver Treasure or Taxi To Paradise?"

- **Question Decomposition / Least-to-Most Prompting**: Why needed: The decomposer role requires generating structured subquestion sequences with dependencies. Quick check: Given a complex question, can you identify whether decomposition should be parallel (independent subquestions) or sequential (dependent subquestions)?

## Architecture Onboarding

- **Component map:** Input Question q → [Decomposer ρ] → Subquestions z → [Retriever] → Documents D_i → [Solver π] → Intermediate answers w_i, then final answer a'

- **Critical path:**
  1. SFT stage: Train on 180K mixed examples for 1 epoch, batch size 64
  2. RFT stage: Generate rollouts with m=3 decompositions, m'=4 solutions; construct preference pairs; run 2 iterations of DPO with β=0.1
  3. Inference: Single decomposition, sequential retrieval and solving, temperature=0.0

- **Design tradeoffs:**
  - m, m' values: Larger values yield more preference pairs but increase training time (m=4,m'=4 takes 8.1h vs m=2,m'=2 at 3.7h)
  - Retrieval passages k: Performance plateaus at k=10
  - SFT vs RFT: SFT establishes base capabilities; RFT provides 3-7 point gains
  - Inference latency: ~3-5s per question vs ~1s for single-turn RAG

- **Failure signatures:**
  - Decomposition generates irrelevant subquestions → solver cannot find answers in retrieved passages
  - Excessive subquestions (n>5) → context overflow, reduced performance
  - Preference pairs with identical rewards → discarded, reducing training signal
  - Format violations → f(q,a')=0 in reward function, zero reward despite correct answer

- **First 3 experiments:**
  1. **SFT data ablation**: Train with/without reasoning data on a held-out multi-hop QA subset to verify decomposition capability transfer.
  2. **Rollout sampling study**: Compare m∈{1,2,3,4}, m'∈{2,3,4} on validation set to identify optimal efficiency/performance tradeoff.
  3. **Decomposition quality analysis**: For 50 sampled questions, manually score subquestion relevance and correlate with final answer accuracy to verify the core hypothesis.

## Open Questions the Paper Calls Out

- **Generalization to new tasks:** Can AceSearcher effectively generalize to tasks beyond static reasoning, such as open-ended generation, dialogue systems, or real-time tool use? The authors note this "remains to be explored" as their scope was comparable to concurrent works focusing on QA and verification.

- **Joint retriever optimization:** Does end-to-end joint optimization of the retriever and the LLM offer significant performance gains over the current fixed-retriever setup? The paper notes that "joint optimization of retrieval and reasoning could offer further gains but is left for future work."

- **Online RL algorithms:** Would employing fully online reinforcement learning algorithms yield superior performance or faster convergence compared to the iterative preference optimization used? The authors adopted iterative DPO as a "practical alternative" to fully online RL but suggest "exploring more expressive RL formulations may offer further improvements."

## Limitations

- Empirical validation gaps: The exact contribution of cooperative self-play versus other design choices (data mixture, iterative DPO) is not isolated through systematic ablation studies.

- Assumption sensitivity: The multi-turn DPO optimization relies on unproven reward separation conditions and first-order stability assumptions that may not hold in practice.

- Data distribution dependence: The framework's success depends heavily on the curated SFT data mixture, with performance degrading significantly when reasoning data is removed.

## Confidence

- **High confidence:** Performance improvements over baselines (7.6% average EM improvement, 32B model matching DeepSeek-V3 with <5% parameters).
- **Medium confidence:** The cooperative self-play mechanism's effectiveness and the claim that a single LLM can effectively alternate between decomposer and solver roles.
- **Low confidence:** The theoretical guarantees for iterative preference optimization and the robustness of the approach to different reward function formulations.

## Next Checks

1. **Quantitative decomposition quality analysis:** Perform correlation analysis between manually scored decomposition quality (1-5 scale) and final answer accuracy across 100+ randomly sampled test questions to empirically validate the core hypothesis.

2. **Robustness to reward function changes:** Evaluate AceSearcher's performance using alternative reward formulations to test whether final-answer rewards provide sufficient signal for learning intermediate reasoning.

3. **Cross-domain generalization study:** Train and evaluate AceSearcher on reasoning tasks from domains not represented in the SFT data mixture (e.g., medical, legal, or scientific reasoning) to assess generalization beyond curated training distribution.