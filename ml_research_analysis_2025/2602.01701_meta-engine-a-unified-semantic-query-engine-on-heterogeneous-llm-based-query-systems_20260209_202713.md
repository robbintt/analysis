---
ver: rpa2
title: 'Meta Engine: A Unified Semantic Query Engine on Heterogeneous LLM-Based Query
  Systems'
arxiv_id: '2602.01701'
source_url: https://arxiv.org/abs/2602.01701
tags:
- query
- semantic
- engine
- meta
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Meta Engine is a unified semantic query engine that integrates\
  \ heterogeneous, specialized LLM-based query systems to overcome the trade-off between\
  \ specialization and generality in multimodal data processing. By decomposing complex\
  \ queries into sub-queries, routing them to appropriate specialized systems, and\
  \ aggregating results, Meta Engine achieves 3\u20136x higher F1 scores and up to\
  \ 24x improvements on specific datasets compared to baseline systems."
---

# Meta Engine: A Unified Semantic Query Engine on Heterogeneous LLM-Based Query Systems

## Quick Facts
- arXiv ID: 2602.01701
- Source URL: https://arxiv.org/abs/2602.01701
- Reference count: 27
- Primary result: Achieves 3–6x higher F1 scores and up to 24x improvements on specific datasets by integrating heterogeneous LLM-based query systems

## Executive Summary
Meta Engine is a unified semantic query engine that addresses the specialization-generality trade-off in multimodal data processing by integrating heterogeneous, specialized LLM-based query systems. It decomposes complex queries into single-hop sub-queries, routes them to appropriate specialized systems, and aggregates results to achieve significant performance gains. The system demonstrates 3–6x higher F1 scores compared to baseline systems, with up to 24x improvements on specific datasets, while providing a unified interface for end users.

## Method Summary
Meta Engine implements a five-component pipeline: Natural Language Query Parser (with Complexity Checker and Query Decomposer), Operator Generator, Query Router, Adapters (wrapping systems like LOTUS, DSPy, StructGPT, Single-Model), and Result Aggregator. The system decomposes complex queries into 1–3 single-hop sub-queries, generates confidence-ranked operators (TextAnalytics, ImageAnalytics, TableAnalytics), routes sub-queries to the best-matching specialized adapter using either statistical or learned routing strategies, and aggregates results into concise answers. The learned router (MLRouter) uses a Qwen-0.6B frozen encoder with MLP classifier trained on weighted cross-entropy loss.

## Key Results
- Achieves 3–6x higher F1 scores across most tested datasets compared to baseline systems
- Shows up to 24x improvement on specific datasets when using specialized system routing
- Query decomposition and intelligent routing are critical for performance gains, with ablation studies confirming their effectiveness
- Maintains competitive efficiency with end-to-end latency of 6.95s per query

## Why This Works (Mechanism)

### Mechanism 1: Query Decomposition Reduces Reasoning Complexity
Breaking complex multi-hop queries into single-hop sub-queries improves tractability for LLM-based systems. The complexity checker identifies queries requiring multi-step reasoning, aggregation, or complex filtering, then the decomposer splits them into 1–3 simpler, self-contained sub-queries ordered from basic retrieval to advanced inference.

### Mechanism 2: Confidence-Based Routing to Specialized Systems
Routing sub-queries to modality-appropriate specialized systems improves performance over single general-purpose systems. Operator generator ranks candidate operators by confidence scores based on query intent and data indicators, then router dispatches to best-matching adapter with progressive fallback execution.

### Mechanism 3: Iterative Sub-query Refinement Reduces Ambiguity
Refining subsequent sub-queries with previous execution results reduces ambiguity and avoids duplicate computation. Each sub-query after the first is refined by incorporating execution results from preceding sub-queries, filling resolved entity names and intermediate values.

## Foundational Learning

- **Multi-hop vs. Single-hop Queries**: Understanding query complexity determines whether decomposition is triggered. Quick check: Can this query be answered by retrieving a single fact, or does it require combining multiple pieces of evidence?
- **Modality-Specific Operators**: Each sub-query must map to TextAnalytics, ImageAnalytics, or TableAnalytics based on evidence type. Quick check: What data type (text, table, image) contains the evidence needed to answer this sub-query?
- **Specialized vs. General LLM Systems**: Router must balance using specialized systems versus general systems. Quick check: Is this query best served by a domain-specialized system or a general-purpose VLM?

## Architecture Onboarding

- **Component map**: Query → Complexity Check → (Decompose if complex) → For each sub-query: Generate operators → Rank by confidence → Route to adapter → Execute → Refine next sub-query → Aggregate all results
- **Critical path**: Natural Language Query Parser → Operator Generator → Query Router → Adapters → Result Aggregator → Final Answer (≤10 words)
- **Design tradeoffs**: StatRouter vs MLRouter (query-agnostic vs query-dependent); sub-query limit (max 3) to prevent latency increase; confidence-based ranking overhead (1.05–1.81 iterations average)
- **Failure signatures**: Over-decomposition adds latency; routing errors require fallback iterations; verbose baseline outputs reduce F1; error propagation through iterative refinement
- **First 3 experiments**: 1) Run single-hop queries with decomposition disabled to measure baseline adapter performance per modality; 2) Execute multi-hop queries with StatRouter vs MLRouter to identify best routing strategy; 3) Ablate the aggregator and compare F1/Semantic Hit scores to quantify answer synthesis contribution

## Open Questions the Paper Calls Out

### Open Question 1
Can parallel execution of independent sub-queries significantly reduce latency without degrading result quality compared to the current sequential approach? The authors state they plan to improve efficiency through parallel sub-query processing for independent queries.

### Open Question 2
Can the accuracy of the operator ranking mechanism be improved to minimize the average number of execution attempts (currently 1.05–1.81) per query? The paper explicitly states they will improve ranking accuracy to minimize iterations.

### Open Question 3
How robust is the learned router (MLRouter) against distribution shifts compared to the statistic router, given that StatRouter outperforms it on specific datasets like M2QA? It's unclear if the learned router requires more training data or if the static statistical approach is inherently more robust.

## Limitations

- Implementation opacity: Core adapter APIs and integration patterns are not fully specified, preventing exact replication despite "artifact available" statement
- Router training dependency: MLRouter requires generating training data by running all adapters on a labeled query subset, but training set size and generation methodology are undetailed
- Dataset generalization: The 5 tested datasets are dominated by structured/modular queries; performance on highly unstructured, open-ended queries remains unverified

## Confidence

- **High confidence**: Query decomposition improves tractability; confidence-based routing to specialized systems outperforms single-general systems; answer aggregation improves F1 scores
- **Medium confidence**: 3–6x F1 improvement is dataset-dependent; routing strategy effectiveness varies by query distribution
- **Low confidence**: Iterative sub-query refinement reliably reduces ambiguity without error accumulation; system generalizes to arbitrary new specialized adapters without retraining

## Next Checks

1. **Decomposer ablation on multi-hop queries**: Run the system with and without the Query Decomposer on complex MultiModalQA queries to measure the specific F1 gain from breaking multi-hop queries into single-hop sub-queries.

2. **Router generalization test**: Train MLRouter on 4 datasets, then evaluate on the 5th held-out dataset to measure whether the learned routing policy transfers or overfits to training distribution.

3. **Error propagation measurement**: Inject controlled noise into early sub-query results and measure the degradation in final F1 to quantify the risk of error accumulation through iterative refinement.