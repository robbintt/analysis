---
ver: rpa2
title: 'Walking on the Fiber: A Simple Geometric Approximation for Bayesian Neural
  Networks'
arxiv_id: '2512.01500'
source_url: https://arxiv.org/abs/2512.01500
tags:
- posterior
- learning
- neural
- sampling
- approximation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MetricBNN, a scalable Bayesian neural network
  approach that improves posterior approximation through two key innovations. First,
  it uses a sampling method that explores the parameter space around the MAP estimate
  by perturbing parameters with random drift vectors and refining them with gradient
  updates, efficiently capturing the low-dimensional structure of loss minima.
---

# Walking on the Fiber: A Simple Geometric Approximation for Bayesian Neural Networks

## Quick Facts
- arXiv ID: 2512.01500
- Source URL: https://arxiv.org/abs/2512.01500
- Reference count: 40
- MetricBNN achieves NLL of 0.70 on CIFAR10 vs 1.55 for deep ensembles

## Executive Summary
MetricBNN introduces a scalable Bayesian neural network approach that combines geometric sampling with learned latent representations. The method exploits the low-dimensional structure of neural network loss minima through a drift-refinement sampling procedure, then learns a structured latent representation via a contrastive autoencoder. This enables rapid posterior sampling without iterative MCMC or expensive Hessian computations. Across multiple benchmarks including regression, classification, and out-of-distribution tests, MetricBNN achieves competitive or superior uncertainty quantification while maintaining significantly better computational efficiency as network size increases.

## Method Summary
MetricBNN uses a two-phase approach: first, it explores the parameter space around the MAP estimate using drift-refinement sampling that leverages the low-dimensional structure of loss minima; second, it learns a structured latent representation of the posterior via a contrastive autoencoder. The sampling phase initializes particles at the MAP estimate, assigns random drift directions, and iteratively perturbs and refines them via gradient descent to trace curves along the low-loss manifold. The autoencoder is trained on these samples with a contrastive loss that encourages local distances between successive samples to be preserved while pushing non-adjacent samples apart, enabling rapid posterior sampling through latent interpolation without iterative methods.

## Key Results
- Achieves NLL of 0.70 on CIFAR10 versus 1.55 for deep ensembles
- Maintains ECE of 9.26 on CIFAR10 versus 8.94 for ensembles
- Demonstrates competitive performance across UCI datasets, MNIST, and FashionMNIST while being more computationally efficient than HMC or deep ensembles

## Why This Works (Mechanism)

### Mechanism 1: Low-Dimensional Fiber Sampling via Drift-Refinement
The method exploits the low-dimensional structure of neural network loss minima by initializing particles at the MAP estimate, assigning random drift directions, and iteratively perturbing and refining them via gradient descent. This traces curves along the low-loss "fiber" manifold connecting near-optimal solutions. The approach assumes the "linear connectivity assumption" holds—that loss minima form a connected, lower-dimensional manifold rather than isolated points.

### Mechanism 2: Structured Latent Space via Contrastive Autoencoder
A learned deformation of parameter space enables non-Gaussian posterior representation while permitting rapid, non-iterative sampling. The autoencoder is trained with a contrastive loss that pulls successive trajectory points to fixed distance in latent space while pushing non-adjacent samples apart, creating a linearly interpolable representation of the curved fiber manifold.

### Mechanism 3: Amortized Posterior Sampling via Latent Interpolation
Once the latent space is structured, posterior samples can be generated in O(1) time by uniform interpolation along learned trajectories, avoiding iterative MCMC or Hessian computation. The autoencoder amortizes the sampling cost by mapping uniform samples in latent space to viable parameters along the fiber.

## Foundational Learning

- **Laplace Approximation**: MetricBNN is positioned as an alternative to Gaussian local approximations; understanding why inverting the Hessian becomes problematic for over-parameterized networks motivates the sampling approach. Quick check: Can you explain why inverting the Hessian becomes problematic for over-parameterized networks?

- **Mode Connectivity in Loss Landscapes**: The entire sampling mechanism assumes loss minima are connected by low-loss paths (fibers), not isolated. Quick check: What empirical observation about SGD solutions from different initializations supports the mode connectivity hypothesis?

- **Contrastive Learning for Representation Structuring**: The autoencoder loss (L+, L-, Ld) is a contrastive formulation designed to create linearly interpolable latent spaces. Quick check: Why would pulling successive trajectory points to fixed distance 1/T help linearize a curved fiber?

## Architecture Onboarding

- **Component map**: Base Network (MAP training) -> Sampling Module (drift-refinement) -> Autoencoder (latent learning) -> Posterior Sampler (latent interpolation)
- **Critical path**: 1) Train base network to convergence, 2) Run sampling with appropriate hyperparameters, 3) Train autoencoder with balanced loss terms, 4) Validate posterior quality via NLL/ECE
- **Design tradeoffs**: α (drift scale) affects exploration vs. staying on fiber; N×T (samples) affects coverage vs. autoencoder training cost; k (latent dim) affects representation capacity vs. training difficulty; λ+ vs λ− affects latent space structure
- **Failure signatures**: Samples clustering near MAP indicates α too small or M too large; divergent predictions indicate α too large; high reconstruction loss indicates insufficient autoencoder capacity; ECE degradation indicates incorrect latent structure
- **First 3 experiments**: 1) Toy regression validation on Snelson 1D to verify uncertainty capture, 2) Ablation study on α and M parameters on UCI dataset, 3) Latent dimension sensitivity test on CIFAR10 subset

## Open Questions the Paper Calls Out
None explicitly identified in the provided content.

## Limitations
- Assumes linear connectivity between loss minima, which may not hold for all architectures or landscapes
- Specific autoencoder loss formulation (L+, L-, Ld) is novel and not extensively validated against alternatives
- Scalability with very deep networks (>20 layers) or high-capacity architectures remains unproven

## Confidence

- **High Confidence**: Empirical performance gains on standard benchmarks are well-documented and reproducible
- **Medium Confidence**: Theoretical mechanism of fiber exploration is sound but depends on landscape geometry assumptions
- **Low Confidence**: Specific autoencoder loss formulation is novel and not extensively validated

## Next Checks

1. **Landscape Connectivity Test**: Systematically vary network depth/width and test whether drift-refinement consistently finds connected fibers, or if it fails on certain architectures.

2. **Autoencoder Ablation**: Compare the contrastive loss formulation against simpler alternatives (e.g., standard VAE loss) to isolate whether the specific loss design is critical.

3. **Posterior Fidelity Analysis**: Beyond NLL/ECE, evaluate whether samples from MetricBNN accurately capture higher-order posterior moments and correlations between parameters.