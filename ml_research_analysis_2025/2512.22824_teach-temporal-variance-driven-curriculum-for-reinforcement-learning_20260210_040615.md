---
ver: rpa2
title: 'TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning'
arxiv_id: '2512.22824'
source_url: https://arxiv.org/abs/2512.22824
tags:
- u1d461
- u1d70b
- u1d703
- u1d454
- u1d460
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses sample inefficiency in multi-goal reinforcement
  learning by proposing TEACH, a temporal variance-driven curriculum learning method.
  The approach leverages the temporal variance of Q-values to dynamically prioritize
  goals where policy learning is most active, rather than relying on static heuristics
  or noisy value estimates.
---

# TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning

## Quick Facts
- arXiv ID: 2512.22824
- Source URL: https://arxiv.org/abs/2512.22824
- Authors: Gaurav Chaudhary; Laxmidhar Behera
- Reference count: 40
- Primary result: Proposed TEACH method achieves superior sample efficiency in multi-goal RL by leveraging temporal variance of Q-values to dynamically prioritize learning goals at the skill frontier.

## Executive Summary
This paper addresses sample inefficiency in multi-goal reinforcement learning by proposing TEACH, a temporal variance-driven curriculum learning method. The approach leverages the temporal variance of Q-values to dynamically prioritize goals where policy learning is most active, rather than relying on static heuristics or noisy value estimates. A theoretical connection is established between Q-value variance and policy evolution, showing that high variance indicates significant policy divergence. The method introduces a policy confidence score derived from Q-values to quantify learning progress and designs a curriculum based on the temporal variance of this score over recent timesteps. This strategy mitigates the impact of noisy value estimates and focuses learning on goals at the skill frontier.

## Method Summary
TEACH is a curriculum learning method for multi-goal reinforcement learning that uses temporal variance of Q-values to guide goal selection. The method computes a policy confidence score for each goal based on the expected Q-value under the current policy, then tracks the variance of this score over a temporal window. Goals with high temporal variance are considered to be at the "skill frontier" where learning is most active. The curriculum samples goals proportionally to their variance, prioritizing those where the policy is evolving most significantly. The approach is algorithm-agnostic and can be integrated with existing RL frameworks, specifically demonstrated with DDPG and HER in this work.

## Key Results
- TEACH consistently outperforms state-of-the-art curriculum learning and goal-selection methods across 11 diverse robotic manipulation and maze navigation tasks
- The method demonstrates improved sample efficiency, achieving higher success rates in fewer timesteps compared to baselines
- Theoretical analysis establishes a connection between Q-value variance and policy evolution, validating the approach's core mechanism
- The variance-based curriculum effectively focuses learning on goals at the skill frontier rather than random exploration or fixed schedules

## Why This Works (Mechanism)

### Mechanism 1: Temporal Variance of Q-Values as Learning Progress Signal
- Claim: High temporal variance in Q-values correlates with significant policy evolution, indicating goals at the skill frontier
- Mechanism: The paper derives (Appendix A, Equation 28) that KL divergence between successive policies approximates (1/2α²) × E[Var(ΔQ)], where ΔQ is Q-value change across updates. This means Q-value variance directly tracks policy divergence—goals with high variance indicate where the policy is actively changing rather than stable (mastered or too hard)
- Core assumption: Policy updates are small and continuous, enabling first-order Taylor approximations; Q-value noise is zero-mean over the temporal window
- Evidence anchors:
  - [abstract] "...establish a theoretical connection between Q-value variance and policy evolution, showing that high variance indicates significant policy divergence"
  - [section 4.1] Equation 5: "KL(π_θ^{t+1} || π_θ^t) ≈ (1/2α²) E[Var(ΔQ)]"
  - [corpus] Weak direct corpus validation—related work (VDS, SPaCE) uses value disagreement or heuristics, not temporal variance specifically
- Break condition: If policy updates are large/discontinuous (e.g., aggressive learning rates), the KL approximation fails; if Q-values have systematic bias (not zero-mean noise), variance no longer isolates learning signal

### Mechanism 2: Policy Confidence Score Aggregates Goal-Specific Performance
- Claim: Policy confidence score C_π(g) = E_s[Q(s, g, π(s,g))] provides a goal-specific scalar metric robust to state-level noise
- Mechanism: By taking expectation over replay buffer states D, the score marginalizes out state-action variance while preserving goal-level signal. The score is computed using DDPG's Q-network outputs at the policy's chosen action, aligning with deterministic policy gradient structure
- Core assumption: Replay buffer state distribution approximates the true visitation distribution under current policy; Q-function error is bounded
- Evidence anchors:
  - [section 4.2] Equation 7 defines C_π^t(g) and explains it "yields a scalar metric that reflects the policy's average performance for goal g"
  - [abstract] "...policy confidence score derived from Q-values to quantify learning progress"
  - [corpus] GCHR and related goal-conditioned methods use hindsight relabeling—TEACH's score-based approach is distinct but not directly compared in corpus
- Break condition: If replay buffer is highly off-policy (stale distribution), expectation diverges from current policy performance; if Q-network overfits, score reflects memorization not generalization

### Mechanism 3: Temporal Windowing Reduces Value Estimate Noise
- Claim: Computing variance over n timesteps (window size) suppresses noisy value estimates better than single-step differences
- Mechanism: Rather than using ΔC_π(g) = C^t - C^{t-Δt} directly (noisy), TEACH computes σ_C(g,t) across n consecutive scores. Averaging effect filters high-frequency fluctuations while preserving genuine learning dynamics that persist across multiple updates
- Core assumption: Learning progress manifests as sustained variance over multiple timesteps; noise is uncorrelated across updates
- Evidence anchors:
  - [section 4.3] Equation 8 defines variance over window n; text states "averaging over n timesteps filters fluctuations, enhancing robustness"
  - [section 4.3] "Unlike simple differences, averaging over n timesteps reduces the effects of noisy value estimates"
  - [corpus] No direct corpus comparison on temporal windowing for curriculum—this appears novel to TEACH
- Break condition: If learning progress occurs in rapid bursts shorter than window size, signal is dampened; if noise is temporally correlated (e.g., consistent Q-overestimation), variance remains contaminated

## Foundational Learning

- **Goal-Conditioned Reinforcement Learning (GCRL)**
  - Why needed here: TEACH operates on multi-goal MDPs where policy π(s,g) and Q(s,a,g) are conditioned on goals; understanding goal-relabelling (HER) and sparse binary rewards is essential
  - Quick check question: Can you explain why uniform goal sampling in sparse-reward GCRL leads to sample inefficiency?

- **Actor-Critic Methods (DDPG Specifically)**
  - Why needed here: TEACH uses DDPG's Q-network for confidence scores; the deterministic policy gradient update ties Q-value changes to policy evolution
  - Quick check question: How does DDPG's target network and soft update affect Q-value stability for curriculum computation?

- **Curriculum Learning and Zone of Proximal Development**
  - Why needed here: TEACH aims to select goals at the "skill frontier"—neither too easy nor too hard—borrowing from pedagogical theory
  - Quick check question: What failure mode occurs if curriculum only selects goals the agent has already mastered?

## Architecture Onboarding

- **Component map:**
  1. Student (RL Agent): DDPG with actor π_θ(s,g), critic Q_φ(s,a,g), replay buffer D
  2. Teacher (Goal Proposer): Maintains confidence score history per goal, computes temporal variance, samples goals from curriculum distribution K(g)
  3. Goal Sampler: Fixed set of N goals sampled uniformly at initialization; curriculum redistributes sampling probability
  4. Temporal Buffer: Stores C_π^t(g_i) for each goal over last n timesteps

- **Critical path:**
  1. Initialize N goals from G
  2. Each timestep: sample goal g ~ K(g) (proportional to σ_C)
  3. Rollout trajectory, store transitions in D
  4. Update Q_φ and π_θ via DDPG
  5. Every Δ episodes: recompute C_π(g) for all N goals, update σ_C(g,t) over window n, renormalize K(g)

- **Design tradeoffs:**
  - **Window size n**: Larger n reduces noise but delays detection of learning progress shifts. Paper doesn't specify optimal n—requires tuning
  - **Number of sampled goals N**: Larger N improves goal-space coverage but increases compute overhead for variance computation
  - **Update frequency Δ**: More frequent curriculum updates are responsive but computationally expensive; less frequent risks stale curriculum
  - **Ensemble-free vs. VDS**: TEACH avoids ensemble overhead (single Q-network) but may be less robust to Q-estimation error than uncertainty-based methods

- **Failure signatures:**
  - **Curriculum collapse**: All probability mass concentrates on few goals—check if σ_C(g) → 0 for most goals (learning stalled) or if Q-values diverged
  - **No learning signal**: σ_C(g) remains near zero across all goals—Q-network may be undertrained or learning rate too low
  - **Oscillating curriculum**: Rapid shifts in high-variance goals indicate window n too small or Q-values unstable

- **First 3 experiments:**
  1. **Ablation on window size n**: Test n ∈ {5, 10, 20, 50} on FetchReach; plot success rate vs. timesteps to identify noise-filtering vs. responsiveness tradeoff
  2. **Single-goal sanity check**: Fix curriculum to uniform sampling (disable TEACH) on one easy goal—verify DDPG+HER baseline works before debugging curriculum
  3. **Variance dynamics visualization**: Log σ_C(g,t) for each goal over training; confirm high-variance goals align with intermediate-difficulty goals (not random distribution)

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implications arise from the methodology and results.

## Limitations
- The method's effectiveness depends on the assumption of zero-mean Q-value noise, which may not hold in practice with biased value estimates
- The theoretical connection between Q-variance and policy divergence is derived for soft policies, but the implementation uses deterministic DDPG, creating a potential gap
- Key hyperparameters (window size n, number of sampled goals N, update frequency Δ) are not specified, requiring extensive tuning for different environments

## Confidence
- **High confidence**: The theoretical connection between Q-variance and policy divergence (Section 4.1, Equation 5) is derived and mechanistically sound under stated assumptions
- **Medium confidence**: Empirical results show consistent improvement over baselines across 11 tasks, but lack ablations on key hyperparameters (n, N, Δ) and no analysis of curriculum diversity or failure cases
- **Low confidence**: Claims about robustness to Q-estimation noise are asserted but not tested—no comparison against ensemble-based uncertainty methods or systematic analysis of Q-value bias effects

## Next Checks
1. **Window size ablation**: Systematically vary n ∈ {5, 10, 20, 50} on FetchReach and plot success rate vs. timesteps to quantify noise-filtering vs. responsiveness tradeoff
2. **Curriculum vs. uniform resampling**: Compare TEACH against a variant that samples goals uniformly but updates the curriculum every Δ episodes (removing variance-based selection) to isolate the benefit of variance-driven prioritization
3. **Q-bias robustness test**: Add systematic bias to Q-values (e.g., constant offset or multiplicative noise) and measure curriculum stability and final performance to test claims of robustness to noisy estimates