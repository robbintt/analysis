---
ver: rpa2
title: Systematic and Efficient Construction of Quadratic Unconstrained Binary Optimization
  Forms for High-order and Dense Interactions
arxiv_id: '2506.08448'
source_url: https://arxiv.org/abs/2506.08448
tags:
- variables
- binary
- functions
- qubo
- auxiliary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of quadratization, converting
  higher-order binary optimization problems (HOBO) into quadratic forms (QUBO) suitable
  for quantum annealing. Specifically, it tackles complex machine learning models
  with dense interactions and strong nonlinearities, which are difficult to handle
  with conventional methods.
---

# Systematic and Efficient Construction of Quadratic Unconstrained Binary Optimization Forms for High-order and Dense Interactions

## Quick Facts
- arXiv ID: 2506.08448
- Source URL: https://arxiv.org/abs/2506.08448
- Reference count: 0
- Primary result: A systematic method for quadratizing high-order binary optimization problems into QUBO form using ReLU basis expansion, enabling efficient quantum annealing for complex machine learning models.

## Executive Summary
This paper addresses the challenge of quadratization - converting higher-order binary optimization problems (HOBO) into quadratic forms (QUBO) suitable for quantum annealing. The proposed approach uses a sum of rectified linear unit (ReLU) bases, leveraging their universal approximation capability and equivalent quadratic-polynomial representation. Two methods are introduced: discretization via one-hot vectorization and ReLU expansion, where the latter approximates functions using piecewise linear ReLU functions. The method is validated numerically and analytically, demonstrating effective approximation of nonlinear functions. When applied to typical ML regressors like Gaussian mixture models, kernel regressors, and neural networks, the approach provides efficient QUBO formulations with reduced auxiliary variables compared to conventional methods.

## Method Summary
The method converts higher-order binary optimization problems into QUBO form by approximating target functions as sums of ReLU bases. For positive coefficients in the ReLU expansion, Legendre transformation directly yields quadratic forms without penalty terms. For negative coefficients, binary sign-bits are introduced to handle the max-min operations. The approach is applied to ML regressors including Gaussian mixture models (efficient for positive coefficients), kernel regressors (mixed coefficients), and neural networks (complex interactions). The key innovation is that ReLU bases have an equivalent quadratic-polynomial representation, allowing the entire objective function to become a quadratic optimization problem suitable for quantum annealing.

## Key Results
- ReLU-expansion method provides efficient QUBO formulations with fewer auxiliary variables than discretization for models with positive coefficients
- For GMMs, auxiliary variable count scales as MK with zero penalty terms when coefficients are non-negative
- The method effectively handles both positive and negative coefficient cases through different quadratization strategies
- Numerical validation shows accurate approximation of nonlinear functions with piecewise linear ReLU bases

## Why This Works (Mechanism)

### Mechanism 1
Non-linear functions in machine learning regressors can be approximated as quadratic forms by leveraging the mathematical properties of Rectified Linear Units (ReLU). The method exploits the Legendre transformation, which shows that a ReLU function R(q) is equivalent to a quadratic polynomial representation maxₜ{tq} with respect to input q and dual variable t. By decomposing a target non-linear function into a sum of ReLU bases (piecewise linear approximation), the entire objective function becomes a quadratic optimization problem suitable for Quantum Annealing (QA). Core assumption: The target non-linear function can be accurately approximated by a piecewise linear function (polyline) using a finite number of ReLU bases. Break condition: If the target function f(q) is highly oscillatory or non-convex in a way that requires an exponential number of ReLU pieces (M) to approximate, the quadratic reduction becomes intractable due to variable overhead.

### Mechanism 2
Higher-order binary optimization problems (HOBO) can be reduced to QUBO without Rosenberg-style penalty terms if the coefficients of the approximated ReLU bases are positive. When maximizing a function where coefficients cₖ(aₘ - aₘ₋₁) > 0, the maximization operator can be factored directly into the QUBO objective as maxₓ,ₜ. This avoids the introduction of constraint-penalty terms (λ) that are typically required to enforce logical equivalency in standard quadratization (like Rosenberg's method). Core assumption: The underlying ML model (e.g., Gaussian Mixture Model) yields positive coefficients after ReLU expansion (i.e., the function is convex or has non-negative second derivatives). Break condition: If the model contains negative coefficients (e.g., Kernel Regressors with negative weights), this efficiency is lost, requiring auxiliary binary variables and penalty terms.

### Mechanism 3
Negative coefficients in the ReLU expansion can be encoded into QUBO using binary sign-bits rather than continuous Wolfe duality variables. To handle negative coefficients where max min operations fail, the method introduces binary variables z to discretize the domain of q. Specifically, a binary variable z_D acts as a sign indicator for q, enforcing the ReLU condition R(q) = z_D q via a constraint penalty, effectively linearizing the negative interactions. Core assumption: The input domain can be discretized effectively, and the user can determine an appropriate bit width D to cover the range of q. Break condition: If the required bit width D is large (e.g., high precision needed over a vast domain), the number of auxiliary variables (MK'_ₙD) grows logarithmically, potentially exceeding hardware capacity.

## Foundational Learning

**QUBO and Ising Models**
- Why needed here: This is the native format for Quantum Annealers. The entire goal of the paper is to translate high-order ML functions into this specific quadratic binary format.
- Quick check question: Can you explain why a higher-order term like x₁x₂x₃ cannot be directly submitted to a D-Wave machine without quadratization?

**Universal Approximation Theorem**
- Why needed here: The paper relies on the theoretical guarantee that a sum of ReLU functions can approximate any continuous function, justifying the replacement of complex ML kernels with ReLU bases.
- Quick check question: Does increasing the number of ReLU pieces (M) guarantee better fidelity in this architecture, and what is the trade-off?

**Legendre Transformation**
- Why needed here: This mathematical tool is the core mechanism allowing the ReLU function to be viewed as a quadratic optimization problem over a dual variable t.
- Quick check question: How does the transformation maxₜ{tq} represent the function max(0, q)?

## Architecture Onboarding

- **Component map:** Input: ML Regressor (GMM, Kernel, Neural Net) → Approximated by ReLU bases → Quadratization Engine (Legendre transform for positive coefficients, Binarization for negative) → Output: QUBO Matrix Q and penalty weights λ
- **Critical path:** The calculation of the ReLU approximation parameters (Section 3.2, Algorithm 1). If this approximation is poor, the QUBO will solve for the wrong objective function.
- **Design tradeoffs:** Discretization vs. ReLU Expansion: Discretization is exact but scales linearly with levels (L ≈ N) and adds 2 penalty terms per cluster. ReLU expansion scales with pieces (M) and adds 0 penalties (for positive coefficients), but is approximate. Positive vs. Negative Coefficients: Positive coefficients are cheap (M vars). Negative coefficients are expensive (M × D vars + penalties). Mixed models should separate these paths.
- **Failure signatures:** Excessive Variables: If N (problem size) is large and M (pieces) is set too high, total qubit count (MK) exceeds hardware limits. Poor Approximation: If M is too small, the polyline fits the target function poorly (Fig 2, M=2 case), leading to suboptimal QA solutions. Constraint Drift: For negative coefficients, if penalty weight λ in Eq (14) is ill-adjusted, the sign-bit constraint is violated, rendering the solution invalid.
- **First 3 experiments:**
  1. Implement Algorithm 1 on a 1D non-linear function (e.g., e⁻ᵠ). Plot approximation error vs. number of pieces M.
  2. Construct a QUBO for a Gaussian Mixture Model with K clusters. Verify that for M < N, the ReLU method uses fewer auxiliary variables than the Discretization method.
  3. Create a synthetic function with negative coefficients. Implement the z-variable binarization (Eq 11-13) and verify that the penalty term successfully enforces R(q) = z_D q in a classical solver before deploying to QA.

## Open Questions the Paper Calls Out

**Open Question 1**
How does the proposed quadratization framework perform in end-to-end black-box optimization tasks using actual ML regressors on quantum hardware? Basis in paper: [explicit] The authors state, "In future work, we will try to tackle actual ML tasks... we will implement a black-box optimization with ML regressors... by using our quadratization methods." Why unresolved: The current study validates the method numerically and analytically regarding function approximation fidelity and variable counts, but it does not demonstrate the full optimization loop or benchmark it on real quantum annealing hardware.

**Open Question 2**
Can the efficiency of the ReLU-expansion method be improved for machine learning models with a high proportion of negative coefficients? Basis in paper: [inferred] The paper notes that for cₖ(aₘ - aₘ₋₁) < 0, the method requires introducing additional auxiliary variables z and penalty terms (Eq. 14), which significantly increases the complexity compared to the positive coefficient case (Table II). Why unresolved: While the method is efficient for positive coefficients (e.g., GMMs), the variable count scales with MK'ₙD for negative coefficients, which may become a bottleneck for general neural networks or kernel regressors.

**Open Question 3**
What is the quantitative relationship between the fidelity of the ReLU piecewise approximation and the probability of locating the global optimum in the QUBO solution? Basis in paper: [inferred] The paper verifies the function approximation fidelity (Fig. 3) and estimates variable counts, but it does not analyze how the residual approximation error affects the annealing process's ability to find the true maximum of the original function. Why unresolved: The ReLU-expansion is an approximation (Eq. 6); however, the paper does not establish bounds on how the approximation error influences the landscape of the resulting QUBO matrix or the solver's convergence.

## Limitations
- The approximation fidelity of ReLU bases for highly non-linear functions is only visually validated without quantitative error metrics
- Scalability analysis relies on theoretical variable counts rather than experimental scaling studies on actual quantum hardware
- The negative-coefficient handling via binary sign-bits introduces potential constraint drift if penalty coefficients are improperly tuned

## Confidence
**High Confidence:** The mathematical framework for ReLU expansion and Legendre transformation (Mechanism 1) is rigorously derived and the positive-coefficient quadratization path is theoretically sound.
**Medium Confidence:** The approximation quality claims for ReLU bases and the variable-count savings over discretization are supported by examples but lack comprehensive empirical validation across diverse function classes.
**Low Confidence:** The negative-coefficient handling mechanism and its constraint satisfaction under realistic conditions requires more rigorous validation, as it relies on penalty-based enforcement without demonstrated convergence guarantees.

## Next Checks
1. Implement Algorithm 1 for a 1D non-linear function (e.g., e⁻ᵠ) and quantify approximation error vs. piece count M using standard metrics like RMSE, comparing against the visual validation in Fig. 3.
2. Construct QUBO formulations for ML regressors (GMM, kernel, neural network) with increasing problem sizes (K, N) and measure actual auxiliary variable counts and solution quality on both classical solvers and quantum hardware.
3. For a synthetic function with negative coefficients, implement the sign-bit mechanism with varying penalty strengths λ and demonstrate constraint satisfaction across the solution space using a classical optimizer before quantum deployment.