---
ver: rpa2
title: Conditioning on PDE Parameters to Generalise Deep Learning Emulation of Stochastic
  and Chaotic Dynamics
arxiv_id: '2509.09599'
source_url: https://arxiv.org/abs/2509.09599
tags:
- network
- neural
- parameter
- dynamics
- numerical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a deep learning emulator for chaotic and stochastic
  partial differential equations (PDEs), conditioned explicitly on PDE parameters.
  The method involves pre-training on a single parameter domain, followed by fine-tuning
  on a diverse dataset, enabling generalisation across a broad range of parameter
  values.
---

# Conditioning on PDE Parameters to Generalise Deep Learning Emulation of Stochastic and Chaotic Dynamics

## Quick Facts
- **arXiv ID**: 2509.09599
- **Source URL**: https://arxiv.org/abs/2509.09599
- **Reference count**: 40
- **Primary result**: Deep learning emulator for chaotic and stochastic PDEs, conditioned on parameters, generalizes across unseen regimes with significant computational speed-ups

## Executive Summary
This work introduces a deep learning emulator for chaotic and stochastic partial differential equations (PDEs), conditioned explicitly on PDE parameters. The method involves pre-training on a single parameter domain, followed by fine-tuning on a diverse dataset, enabling generalisation across a broad range of parameter values. Local attention mechanisms allow the model to handle varying domain sizes and resolutions efficiently. Demonstrated on the Kuramoto-Sivashinsky equation and stochastically-forced beta-plane turbulence, the emulator accurately captures phenomena at interpolated parameter values and provides significant computational speed-ups over conventional numerical integration. A probabilistic variant enables uncertainty quantification, facilitating the statistical study of rare events.

## Method Summary
The method employs a transformer architecture with local attention and adaptive layer normalization (AdaLN) to emulate PDEs conditioned on parameters. The approach uses two-stage training: pre-training on a single parameter domain with fixed AdaLN parameters, followed by fine-tuning on diverse parameters with learned AdaLN functions. For stochastic dynamics, a probabilistic variant uses CRPS loss combined with spectral loss for uncertainty quantification. The local attention mechanism restricts computation to a window around each point, enabling efficient handling of varying domain sizes while maintaining translation equivariance.

## Key Results
- Successfully generalizes across parameter values ($L=36$ to $L=200$ for KS, $\beta=0.3$ to $\beta=2.7$ for beta-plane) without full retraining
- Achieves computational speed-ups of up to 10Ã— compared to numerical integration
- Accurately captures rare events and statistical distributions through probabilistic forecasting
- Maintains trajectory fidelity for approximately 7 Lyapunov times on interpolated parameter values

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Explicit conditioning on PDE parameters enables generalization to unseen dynamical regimes without retraining the full model architecture.
- **Mechanism**: The model replaces fixed affine parameters in layer normalization with learned functions of the PDE parameters (Adaptive Layer Normalization or AdaLN). This modulates the hidden states (scale $\gamma$ and shift $\delta$) based on the input parameters, allowing the transformer to alter its behavior dynamically.
- **Core assumption**: The influence of physical parameters can be captured effectively as a modulation of feature distributions rather than requiring a structural change in the operator.
- **Evidence anchors**: [abstract]: "explicitly conditioned on the parameter values... enabling generalisation across a broad range of parameter values." [2.1.2 Parametric Transformer]: "we replace feature-wise affine transformation parameters in layer normalisation with a learned function of the conditioning information... modifying the network's behaviour according to the parameter values."

### Mechanism 2
- **Claim**: Local attention mechanisms allow the emulator to handle varying domain sizes and resolutions efficiently, overcoming limitations of spectral methods like FNO.
- **Mechanism**: The architecture restricts attention to a localized window $K$ around each spatial point using an unfold operation. This replaces the static weights of CNNs with dynamic attention weights, reducing complexity from $O(D^2)$ to $O(D \times K)$ while maintaining translation equivariance via relative positional encodings.
- **Core assumption**: Spatial correlations in the target fluid systems are finite and primarily local; global structures emerge hierarchically through layers.
- **Evidence anchors**: [abstract]: "local attention mechanisms allow the model to handle varying domain sizes and resolutions efficiently." [3.1.2 Emulation]: "The FNO explicitly models only the largest N wavenumbers... In contrast, the local attention mechanism explicitly models local dynamics... allowing it to adapt flexibly to domains of arbitrary size."

### Mechanism 3
- **Claim**: A composite loss function combining Continuous Ranked Probability Score (CRPS) and spectral loss enables stable probabilistic forecasting of stochastic dynamics and rare events.
- **Mechanism**: Instead of deterministic MSE, the model minimizes CRPS (encouraging ensemble spread to match forecast error) plus a spectral MAE term (penalizing errors in the Fourier domain). This forces the model to preserve energy across scales and prevents the "spectral bias" typical of neural networks.
- **Core assumption**: The uncertainty in under-resolved dynamics can be modeled as a state-dependent Gaussian distribution in latent space (reparameterization trick).
- **Evidence anchors**: [abstract]: "probabilistic variant of the emulator provides uncertainty quantification... statistical study of rare events." [2.2 Objective Function]: "combines the Continuous Ranked Probability Score (CRPS)... with a spectral loss to ensure energy preservation across all scales."

## Foundational Learning

- **Concept: Attention Mechanisms & Transformers**
  - **Why needed here**: The core architecture is a transformer modified with "local attention." Understanding $Q, K, V$ (Query, Key, Value) is required to grasp how the model weighs spatial neighbors dynamically.
  - **Quick check question**: How does the complexity of standard self-attention scale with sequence length, and how does local attention modify this?

- **Concept: Layer Normalization**
  - **Why needed here**: The mechanism for conditioning (AdaLN) is a modification of standard Layer Normalization. You must understand what the scale ($\gamma$) and shift ($\beta$) parameters do in standard implementations to understand how they are made "adaptive" here.
  - **Quick check question**: In Layer Norm, are the normalization statistics computed across the batch or across the features of a single sample?

- **Concept: Chaotic Dynamics & Lyapunov Times**
  - **Why needed here**: Evaluation is not just about MSE but about "tracking" trajectories for $\sim 7$ Lyapunov times and reproducing statistical distributions (PDFs).
  - **Quick check question**: Why is exact long-term pointwise prediction impossible for chaotic systems, necessitating the use of probabilistic metrics or statistical equivalence?

## Architecture Onboarding

- **Component map**: Input $U_t$ + Parameter $\beta$ -> Linear Projection -> AdaLN -> Transformer Blocks (Local Attention + MLP) -> Linear Projection -> Output $U_{t+1}$
- **Critical path**:
  1. **Pre-training**: Train on a *single* parameter value (e.g., $L=22$). The AdaLN parameters are effectively decoupled/fixed ($\gamma=1, \delta=0$) to focus on learning dynamical structures.
  2. **Fine-tuning**: Train on a *diverse* set of parameters. The learning rate drops to $1/50$th. The AdaLN parameters are unfrozen to learn parameter-dependent modulations.
  3. **Inference**: Input a new parameter value $\beta_{new}$; the AdaLN shifts the network behavior to generalize.
- **Design tradeoffs**:
  - **Local vs. Global Attention**: Local attention ($O(D \times K)$) enables large domain scaling and variable resolutions but requires sufficient depth to build global receptive fields.
  - **CRPS vs. Diffusion**: CRPS requires only $m=2$ forward passes (ensemble) for training, whereas diffusion requires 20-50 steps. This trades the potential generative richness of diffusion for significant speed.
- **Failure signatures**:
  - **Spectral Smoothing**: The model blurs high-wavenumber features.
  - **Catastrophic Forgetting**: Fine-tuning degrades accuracy on the pre-training parameter domain.
  - **Ensemble Collapse**: In probabilistic mode, if the CRPS weight is too low, the ensemble variance collapses to zero.
- **First 3 experiments**:
  1. **Interpolation Test**: Train on KS equation for $L=\{36, 64, 128\}$. Test on $L=56$ (unseen). Measure trajectory divergence time against the numerical solver.
  2. **Ablation on Conditioning**: Train two models on Beta-plane turbulence: one with AdaLN (conditioned on $\beta$) and one standard transformer. Compare their error on unseen $\beta$ values to isolate the contribution of the conditioning mechanism.
  3. **Rare Event Sampling**: Run massive ensembles ($N=1000$) of the stochastic Beta-plane model to estimate the PDF of "nucleation events" and compare the required compute time and accuracy against a traditional numerical solver ensemble.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the emulator be effectively extended to explore higher-dimensional parameter spaces by conditioning on multiple PDE parameters simultaneously without significant degradation in accuracy?
- Basis in paper: [explicit] The conclusion states, "For future work, the emulator could be extended to explore higher-dimensional parameter spaces by conditioning on multiple PDE parameters."
- Why unresolved: The current study restricts the conditioning input size to $M=1$ (a single scalar parameter) for both the Kuramoto-Sivashinsky and beta-plane turbulence systems.
- What evidence would resolve it: Successful training and generalisation of the emulator on a system defined by a vector of parameters ($M>1$), demonstrating the ability to capture interactions between varying physical constants.

### Open Question 2
- Question: Do the discrepancies in the emulator's joint PDFs for high-$\beta$ beta-plane turbulence regimes stem from data imbalance caused by reduced spontaneous event frequency?
- Basis in paper: [explicit] Page 13 notes that as $\beta$ increases, discrepancies emerge which "may stem from a reduced frequency of spontaneous events... This phenomenon warrants further investigation."
- Why unresolved: The authors identify the error and hypothesize the cause (data imbalance/zonostrophy) but do not perform ablation studies to confirm if balancing the dataset resolves the issue.
- What evidence would resolve it: Experiments using training datasets specifically balanced for rare events in high-$\beta$ regimes, or the application of weighted loss functions, resulting in improved alignment of the joint PDF tails.

### Open Question 3
- Question: Can the targeted transfer learning approach used here be scaled to a foundation model capable of generalising across families of distinct physical systems?
- Basis in paper: [explicit] The introduction states, "We have opted to pursue a more targeted approach, leaving the exploration of foundation models as a promising direction for future research."
- Why unresolved: The authors note that foundation models require "vast amounts of diverse training data and extensive computing infrastructure," which was outside the scope of this specific investigation.
- What evidence would resolve it: Application of the local attention and adaptive layer normalisation framework to a multi-system dataset, demonstrating zero-shot or few-shot generalisation to an entirely different PDE not seen during pre-training.

## Limitations
- **Parameter Extrapolation Bounds**: While the method demonstrates strong interpolation capabilities, the extrapolation limits remain unclear for extreme parameter values.
- **Computational Scaling**: The local attention mechanism reduces complexity, but scaling behavior for extremely large domains or high-resolution simulations remains unvalidated.
- **Multi-Modal Uncertainty**: The probabilistic variant assumes Gaussian uncertainty in latent space, which may fail for systems with highly non-Gaussian or multi-modal noise distributions.

## Confidence

**High Confidence**:
- Local attention enables efficient handling of varying domain sizes
- Two-stage training (pre-train + fine-tune) successfully generalizes across parameter values
- AdaLN conditioning effectively modulates transformer behavior based on PDE parameters

**Medium Confidence**:
- Spectral loss effectively mitigates spectral bias in deterministic predictions
- CRPS objective enables stable probabilistic forecasting for stochastic dynamics
- Fine-tuning learning rate reduction by factor of 50 is optimal

**Low Confidence**:
- Exact limits of parameter extrapolation for extreme values
- Computational efficiency scaling for very large domains
- Performance on multi-modal uncertainty distributions

## Next Checks

1. **Parameter Extrapolation Test**: Evaluate model performance on extreme parameter values ($L=400$ for KS, $\beta=3.0$ for beta-plane) to quantify the boundaries of successful generalization.

2. **Computational Scaling Analysis**: Profile training and inference time for progressively larger domains ($L=128, 256, 512$) to verify the $O(D \times K)$ scaling claims and identify practical limits.

3. **Multi-Modal Uncertainty Evaluation**: Introduce a test case with known multi-modal noise distribution (e.g., bimodal forcing) to assess whether the Gaussian latent assumption in the probabilistic variant captures the correct statistics.