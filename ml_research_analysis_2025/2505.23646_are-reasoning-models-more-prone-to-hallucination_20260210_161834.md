---
ver: rpa2
title: Are Reasoning Models More Prone to Hallucination?
arxiv_id: '2505.23646'
source_url: https://arxiv.org/abs/2505.23646
tags:
- lrms
- reasoning
- hallucination
- answer
- post-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether large reasoning models (LRMs)\
  \ are more prone to hallucination compared to their non-reasoning counterparts.\
  \ Through a comprehensive evaluation across different post-training pipelines\u2014\
  SFT+RL, RL-only, and SFT-only\u2014the authors find that not all LRMs benefit from\
  \ their chain-of-thought reasoning capabilities."
---

# Are Reasoning Models More Prone to Hallucination?

## Quick Facts
- arXiv ID: 2505.23646
- Source URL: https://arxiv.org/abs/2505.23646
- Authors: Zijun Yao; Yantao Liu; Yanxu Chen; Jianhui Chen; Junfeng Fang; Lei Hou; Juanzi Li; Tat-Seng Chua
- Reference count: 40
- Primary result: Not all LRMs benefit from chain-of-thought reasoning; post-training pipeline composition critically affects hallucination rates

## Executive Summary
This paper investigates whether Large Reasoning Models (LRMs) are more prone to hallucination compared to their non-reasoning counterparts. Through comprehensive evaluation across three post-training pipelines—SFT+RL, RL-only, and SFT-only—the authors find that pipeline composition significantly impacts factuality. While models trained with both supervised fine-tuning and verifiable reward reinforcement learning generally show reduced hallucination, RL-only and SFT-only pipelines introduce more nuanced hallucinations. The study identifies two critical cognitive behaviors—Flaw Repetition and Think-Answer Mismatch—that directly affect factuality. Additionally, the authors analyze model uncertainty and calibration, finding that increased hallucination correlates with misalignment between model uncertainty and factual accuracy.

## Method Summary
The study evaluates LRMs on SimpleQA and TriviaQA benchmarks using an LLM-as-a-Judge framework. For each question, models generate N=10 responses, extract answers using a judge LLM, and compute majority-voted confidence P(a). The paper compares accuracy, calibration (ECE), and behavioral patterns across model pairs sharing the same backbone but different post-training pipelines. Uncertainty probing involves training linear classifiers on last-token hidden states to predict answer correctness. Behavioral analysis uses LLM judges to detect Flaw Repetition (cycling through semantically equivalent flawed logic) and Think-Answer Mismatch (reasoning conclusion differs from final answer).

## Key Results
- SFT+RL pipelines (cold start SFT + verifiable reward RL) generally alleviate hallucination compared to backbone models
- RL-only and SFT-only pipelines introduce more nuanced hallucinations with higher rates of Flaw Repetition and Think-Answer Mismatch
- Increased hallucination correlates with corrupted internal uncertainty representations, detectable via linear probing accuracy
- Flaw Repetition rates: RL-only (17.8%) > SFT-only (9.7%) > SFT+RL (5.6%) on SimpleQA

## Why This Works (Mechanism)

### Mechanism 1: Post-Training Pipeline Composition Effects
The sequential combination of supervised priming (cold start SFT) followed by reward optimization (verifiable RL) creates different internal representations than either alone. Cold start SFT primes long-form reasoning capability with structured CoT patterns; subsequent RL with verifiable rewards reinforces faithful reasoning-to-answer alignment. RL-only skips the priming phase, teaching shallow reasoning format without proper connection between thinking and answering. SFT-only (distillation) transfers surface reasoning patterns without the reward-guided search that validates reasoning paths.

### Mechanism 2: Flaw Repetition from Exhaustive but Misguided Exploration
RL-only and SFT-only LRMs exhibit higher rates of Flaw Repetition—getting trapped in reasoning loops that vary surface language but repeat underlying flawed logic. Without proper calibration training, models exhaustively explore the reasoning space but lack mechanisms to recognize when they are cycling through semantically equivalent (flawed) paths. The training objective doesn't penalize repetitive exploration, and models lack meta-cognitive signals to detect looping.

### Mechanism 3: Uncertainty-Calibration Misalignment in Hidden Representations
Increased hallucination correlates with corrupted internal uncertainty—model confidence becomes decoupled from factual correctness. Uncertainty probing shows RL-only and SFT-only models partially lose uncertainty information encoded in hidden states, whereas SFT+RL models show improved probing accuracy (suggesting preserved/enhanced uncertainty representations). Hidden states of well-calibrated models encode uncertainty information that linear probes can extract.

## Foundational Learning

- **Chain-of-Thought (CoT) Reasoning**: Why needed here: The entire paper investigates how long-form CoT capabilities (developed for formal reasoning) generalize to fact-seeking tasks. Quick check: Can you explain why adding intermediate reasoning steps before a final answer might help or hurt factual accuracy?

- **Reinforcement Learning with Verifiable Rewards (RLVR)**: Why needed here: The paper distinguishes RL-only, SFT+RL, and SFT-only pipelines. RLVR uses objective reward signals (e.g., correct math answer) rather than human preference modeling. Quick check: Why might verifiable rewards on formal tasks (math, code) not transfer to factuality on knowledge tasks?

- **Model Calibration and Expected Calibration Error (ECE)**: Why needed here: Section 5 analyzes whether model confidence (probability of output) aligns with correctness probability. ECE quantifies miscalibration. Quick check: If a model assigns 80% confidence to answers that are correct only 40% of the time, is it overconfident or underconfident?

## Architecture Onboarding

- **Component map**: Input (Fact-seeking questions) -> LRM Generation (Long CoT reasoning → Final answer extraction) -> Evaluation (LLM-as-Judge for semantic correctness) -> Behavior Analysis (Automated detection of Flaw Repetition and Think-Answer Mismatch) -> Mechanism Analysis (Calibration plots, uncertainty probing with linear classifiers on hidden states)

- **Critical path**: 1. Select LRM and identify its post-training pipeline type (SFT+RL / RL-only / SFT-only) 2. Compare against backbone model on fact-seeking benchmarks 3. If hallucination increases, run behavior analysis to quantify Flaw Repetition and Think-Answer Mismatch rates 4. Run calibration analysis and uncertainty probing to check for corrupted internal representations

- **Design tradeoffs**: SFT+RL: Higher compute cost, but better factuality and calibration; SFT-only (distillation): Lower compute, but risks teaching "shallow reasoning format" without faithful reasoning-to-answer binding; RL-only: No cold start priming, higher instability, risks teaching format without semantic grounding

- **Failure signatures**: Flaw Repetition: Model generates long CoT but cycles through semantically equivalent flawed statements without terminating; Think-Answer Mismatch: CoT reasoning reaches conclusion X, but final answer is Y (e.g., CoT says "Freddie Keppard", answer says "Fred Hager"); Calibration corruption: High ECE, probing accuracy drops vs. backbone

- **First 3 experiments**: 1. Baseline factuality comparison: Run your LRM and its backbone on SimpleQA and TriviaQA with LLM-as-Judge evaluation. Calculate accuracy delta. 2. Behavior annotation on errors: Sample 50-100 hallucinated outputs; use a reasoning LLM to classify Flaw Repetition and Think-Answer Mismatch rates. 3. Uncertainty probing setup: Train linear probe on last-token hidden states to predict answer correctness (binary). Compare probe accuracy between LRM and backbone to assess calibration preservation.

## Open Questions the Paper Calls Out

### Open Question 1
What specific training data or objective modifications could mitigate hallucination in RL-only and SFT-only pipelines without requiring the full cold-start SFT+RL approach? The paper identifies that RL-only and SFT-only pipelines "introduce more nuanced hallucinations" and call for "more comprehensive studies in this direction."

### Open Question 2
Do the findings about post-training pipeline effects on hallucination generalize beyond short-form fact-seeking QA to long-form synthesis, retrieval-augmented generation, and multi-turn dialogue? Appendix A states the focus on SimpleQA and TriviaQA "may not generalize to more complex forms of generation like long-form synthesis, retrieval-augmented generation, or multi-turn dialogue."

### Open Question 3
What specific mechanisms during RL-only and SFT-only training cause the loss of uncertainty information in hidden states? The probing experiments show RL-only and SFT-only LRMs "partially lose the uncertainty information in their hidden states," but the paper does not investigate the training dynamics that cause this corruption.

### Open Question 4
How do alignment methods, dataset quality, and prompt structure independently contribute to hallucination in LRMs? Appendix A states: "Other contributing elements—such as alignment methods, dataset quality, or prompt structure—may also play a significant role. We leave these aspects to future work."

## Limitations
- Behavioral analysis relies on LLM-as-Judge evaluation, which may introduce systematic biases in detecting semantic hallucinations
- Uncertainty probing methodology uses linear classifiers that may not capture complex non-linear relationships in hidden state representations
- Analysis focuses on three specific post-training pipelines without exploring intermediate variants or alternative RL reward formulations

## Confidence
- **High Confidence**: The observation that SFT+RL pipelines generally show better factuality than RL-only or SFT-only pipelines is well-supported by quantitative comparisons across multiple model pairs
- **Medium Confidence**: The behavioral mechanisms (Flaw Repetition, Think-Answer Mismatch) are observed and measured, but the causal link to specific training pipeline components remains correlative rather than definitively established
- **Low Confidence**: The claim about corrupted internal uncertainty representations in RL-only/SFT-only models depends heavily on the linear probing methodology, which may not fully capture the true nature of uncertainty representation changes

## Next Checks
1. **Cross-Judge Validation**: Re-run the behavioral analysis using multiple independent LLM judges (e.g., GPT-4, Claude) to assess consistency in Flaw Repetition and Think-Answer Mismatch detection rates

2. **Probing Architecture Ablation**: Compare linear probes against non-linear alternatives (MLP, attention-based) on the same uncertainty prediction task to validate whether the observed calibration differences are method-dependent

3. **Intermediate Pipeline Experiments**: Train and evaluate hybrid post-training variants (e.g., SFT followed by supervised distillation, or RL with partial cold-start) to better understand which specific training components drive the observed hallucination differences