---
ver: rpa2
title: 'LILogic Net: Compact Logic Gate Networks with Learnable Connectivity for Efficient
  Hardware Deployment'
arxiv_id: '2511.12340'
source_url: https://arxiv.org/abs/2511.12340
tags:
- gate
- gates
- logic
- training
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to train compact networks composed
  of learnable logic gates with jointly optimized connections, significantly improving
  the efficiency of hardware-native models. Unlike prior work that relied on fixed
  random connectomes, this approach learns the interconnect structure alongside gate
  selection using gradient descent, enabling deeper and sparser architectures to achieve
  competitive accuracy.
---

# LILogic Net: Compact Logic Gate Networks with Learnable Connectivity for Efficient Hardware Deployment

## Quick Facts
- arXiv ID: 2511.12340
- Source URL: https://arxiv.org/abs/2511.12340
- Authors: Katarzyna Fojcik; Renaldas Zioma; Jogundas Armaitis
- Reference count: 40
- Primary result: 32K-gate LILogicNet achieves 98.95% accuracy on MNIST, outperforming larger logic gate network baselines

## Executive Summary
LILogicNet introduces a method for training compact logic gate networks with jointly optimized gate selection and interconnect structure. Unlike prior approaches that used fixed random connectomes, this work learns the interconnect alongside gate selection using gradient descent, enabling deeper and sparser architectures. The key innovation is a basis-projection technique for gate evaluation that reduces training time by up to 4× while maintaining model expressiveness, allowing gate counts as low as 8K to match or exceed performance of existing logic gate networks requiring two orders of magnitude more gates.

## Method Summary
The method trains compact networks composed of learnable logic gates with jointly optimized connections for image classification. It uses a basis-projection technique where each node has 16 learnable logits for gate selection, softmax yields probabilities, and these are projected via a fixed 4×16 matrix to compute logic gate outputs. Three interconnect strategies are implemented: Fixed random, fully Learnable (softmax over all inputs), and Top-K sparse (preselecting K candidates per input). The models are trained with Adam optimizer (lr=0.075), batch 256, 200 epochs, and classification temperature scaling based on model width. Input preprocessing includes binarization at specific thresholds for MNIST and CIFAR-10 datasets.

## Key Results
- 32K-gate LILogicNet achieves 98.95% accuracy on MNIST, matching or exceeding larger logic gate network baselines
- 256K-gate variant reaches 60.98% accuracy on CIFAR-10, outperforming similar-sized baselines
- Models with as few as 8K gates achieve competitive accuracy, representing 10-100× reduction in gate count compared to prior work
- Training time reduced by up to 4× using basis-projection technique while maintaining model expressiveness

## Why This Works (Mechanism)
The method works by jointly optimizing both gate selection and interconnect structure through gradient descent, allowing the network to discover efficient connectivity patterns rather than relying on random initialization. The basis-projection technique provides a computationally efficient way to evaluate logic gates while maintaining full expressiveness. By learning sparse connectivity patterns, the network can achieve better accuracy-efficiency trade-offs than fixed random connectomes. The temperature-based softmax smoothing enables differentiable learning of discrete gate and connection choices.

## Foundational Learning
- Logic gate networks: Digital circuits built from Boolean logic gates; needed for hardware-native computation
- Basis projection: Mathematical technique for efficient computation; needed to reduce training time while maintaining expressiveness
- Differentiable discrete optimization: Using softmax with temperature to approximate discrete choices; needed for gradient-based training of gate selection
- Temperature scaling: Controlling softmax sharpness during training; needed to balance exploration vs. exploitation in gate/connection selection
- Binarization: Converting continuous inputs to binary; needed for compatibility with logic gate operations
- Majority vote output: Aggregating gate outputs into class predictions; needed for classification tasks

## Architecture Onboarding
**Component map:** Input → Binarization → Logic Gate Layers (Basis-projection) → Majority Vote → Output
**Critical path:** Gate selection → Basis projection → Connection aggregation → Classification
**Design tradeoffs:** Fixed vs. Learnable vs. Top-K connectivity balances efficiency with expressiveness; temperature scaling affects convergence vs. precision
**Failure signatures:** Low temperature → premature convergence; deep L-type models → accuracy degradation; Top-K slower than expected due to sparse indexing inefficiency
**First experiments:** 1) Implement single-layer LILogicNet-M (8K gates) on MNIST to verify basic functionality 2) Compare Fixed vs. Learnable vs. Top-K connectivity strategies 3) Test temperature scaling effects on convergence and accuracy

## Open Questions the Paper Calls Out
- Can the basis-projection technique be effectively generalized to support logic gates with more than two inputs (higher-order gates)?
- To what extent do standard regularization techniques (e.g., dropout, weight decay) improve the generalization capability of LILogic Nets?
- Does the accuracy-efficiency advantage of LILogicNet persist when scaling beyond 256,000 gates on complex tasks like CIFAR-10?

## Limitations
- Gate and connection weight initialization schemes are unspecified, creating potential reproducibility gaps
- Top-K candidate selection strategy lacks precise details, making exact replication difficult
- Deep L-type models (>2 layers) show accuracy degradation not fully explained by the authors

## Confidence
- High confidence: Overall methodology is sound and basis projection technique is well-specified
- Medium confidence: Empirical results are reproducible given correct implementation of core components
- Low confidence: Generalization to datasets beyond MNIST/CIFAR-10 without architectural modifications

## Next Checks
1. Implement and compare multiple initialization schemes for gate/connection logits to establish baseline stability
2. Systematically evaluate Top-K candidate selection strategies (random vs heuristic) on convergence and accuracy
3. Benchmark LILogicNet performance on additional datasets (e.g., Fashion-MNIST, SVHN) to assess scalability limits