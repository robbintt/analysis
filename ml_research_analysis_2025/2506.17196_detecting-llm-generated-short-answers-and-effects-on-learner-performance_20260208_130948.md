---
ver: rpa2
title: Detecting LLM-Generated Short Answers and Effects on Learner Performance
arxiv_id: '2506.17196'
source_url: https://arxiv.org/abs/2506.17196
tags:
- llm-generated
- responses
- learning
- detection
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method to detect AI-generated responses in
  online learning by fine-tuning GPT-4o on human-labeled data from educational contexts.
  The approach uses a structured rubric to distinguish between human-authored and
  LLM-generated responses, then fine-tunes GPT-4o for improved detection accuracy.
---

# Detecting LLM-Generated Short Answers and Effects on Learner Performance

## Quick Facts
- arXiv ID: 2506.17196
- Source URL: https://arxiv.org/abs/2506.17196
- Authors: Shambhavi Bhushan; Danielle R Thomas; Conrad Borchers; Isha Raghuvanshi; Ralph Abboud; Erin Gatz; Shivang Gupta; Kenneth Koedinger
- Reference count: 40
- Primary result: Fine-tuned GPT-4o detects LLM-generated short answers with 80% accuracy, 0.78 F1, outperforming GPTZero (70%, 0.50 F1)

## Executive Summary
This paper presents a method to detect AI-generated responses in online learning by fine-tuning GPT-4o on human-labeled data from educational contexts. The approach uses a structured rubric to distinguish between human-authored and LLM-generated responses, then fine-tunes GPT-4o for improved detection accuracy. Compared to existing tools like GPTZero, the fine-tuned model achieves 80% accuracy and an F1 score of 0.78, outperforming GPTZero's 70% accuracy and 0.50 macro F1 score. Learners flagged for suspected LLM use were over twice as likely to answer posttest questions correctly, suggesting potential misuse across question types and bypassing of the learning process. The study also proposes using auxiliary indicators like response duration and readability scores to enhance detection.

## Method Summary
The study fine-tunes GPT-4o on 1,302 human-labeled educational responses to detect LLM-generated short answers. Two annotators used a rubric to classify responses into LLM-generated (1), human-authored (0), or uncertain (0.5) categories, achieving moderate inter-rater reliability (κ=0.64–0.68). The fine-tuned model is compared against GPTZero and a stylometric baseline using 13 handcrafted features. Out-of-domain testing on responses from six other lessons shows some generalization. A mixed-effects logistic regression links suspected LLM use to higher posttest performance, with odds ratio 2.37.

## Key Results
- Fine-tuned GPT-4o achieves 80% accuracy and 0.78 F1, outperforming GPTZero (70% accuracy, 0.50 macro F1)
- Stylometric baseline achieves 77% accuracy, confirming surface features are predictive but suboptimal
- Learners flagged for suspected LLM use were more than twice as likely to correctly answer corresponding posttest MCQs (OR=2.37, p<0.001)

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning GPT-4o on domain-specific, human-labeled educational data improves LLM-generated text detection accuracy for short responses compared to general-purpose detectors. The fine-tuning process adapts the model to recognize context-specific linguistic patterns, structural markers, and tone differences that distinguish authentic learner responses from raw LLM outputs. The structured rubric provides explicit classification criteria that ground training signals. Core assumption: Human annotators can reliably distinguish LLM-generated from human-authored text in educational contexts, and these distinctions generalize to unseen responses within similar instructional domains. Evidence anchors: [abstract] "We fine-tune GPT-4o to detect LLM-generated responses... achieving an accuracy of 80% and an F1 score of 0.78, compared to GPTZero's accuracy of 70% and macro F1 score of 0.50." [section 3.1] Tables 3–5 show fine-tuned model accuracy 80%, stylometric baseline 77%, GPTZero 70% on in-domain test set. Break condition: If paraphrasing tools or human-refinement of LLM outputs become common in the target deployment, the rubric-defined "raw LLM" signal degrades.

### Mechanism 2
Stylometric features (word/sentence length, readability, punctuation patterns, formatting) provide a transparent, moderately accurate baseline for short-response detection. Logistic regression over 13 handcrafted features captures surface-level regularities: unusually high average word length, excessive punctuation uniformity, bullet formatting, and absence of informal markers correlate with LLM generation. Core assumption: LLM-generated short responses exhibit consistent stylistic signatures that differ from typical learner writing in this context. Evidence anchors: [section 2.4] "This writing style included a set of 13 features, including average word length, sentence length, readability scores, punctuation, text formatting, and common human and LLM text markers." [section 3.1] Stylometric baseline achieved 77% accuracy, outperforming GPTZero (70%). Break condition: If learners adopt formal writing styles or use editing tools that humanize output, stylometric signal-to-noise decreases.

### Mechanism 3
Suspected LLM use on open-response items is correlated with higher odds of answering corresponding posttest MCQs correctly, suggesting potential bypass of learning processes. Learners who use LLMs for open responses may also use them for related MCQs, or LLM-assisted reading may reduce cognitive engagement during instruction, producing inflated assessment scores without genuine learning. Core assumption: The observed performance difference (OR=2.37) reflects LLM-mediated bypass rather than unmeasured confounds (prior knowledge, engagement). Evidence anchors: [abstract] "Learners flagged for suspected LLM use... were more than twice as likely to correctly answer the corresponding posttest MCQ, suggesting potential misuse across both question types." [section 3.2] Mixed-effects logistic regression: OR=2.37, p<.001; probability correct 0.937 (suspected LLM) vs 0.863 (not). Break condition: If learners suspected of LLM use have systematically higher prior knowledge, the correlation reflects selection rather than bypass.

## Foundational Learning

- **Concept: Fine-tuning vs. Zero-Shot Detection**
  - Why needed here: The paper's central method is fine-tuning GPT-4o; understanding how domain-specific training differs from off-the-shelf detectors is essential for replication.
  - Quick check question: Why does fine-tuning on 1,302 labeled responses outperform a general detector like GPTZero on short answers?

- **Concept: Evaluation Metrics for Imbalanced Classes**
  - Why needed here: The dataset has 899 LLM-generated, 506 human-authored, and 230 uncertain labels; accuracy alone is misleading.
  - Quick check question: Why report macro F1 and weighted F1 in addition to accuracy?

- **Concept: Mixed-Effects Logistic Regression**
  - Why needed here: Repeated observations per student violate independence assumptions; random intercepts account for within-student correlation.
  - Quick check question: What does an odds ratio of 2.37 mean in terms of probability increase for posttest success?

## Architecture Onboarding

- **Component map:** Data layer (1,635 responses from 534 learners) -> Annotation layer (rubric-based coding with κ=0.64–0.68) -> Model layer (GPT-4o fine-tuning, stylometric baseline, GPTZero) -> Evaluation layer (accuracy, precision, recall, F1, confusion matrices) -> Learning analysis layer (mixed-effects logistic regression)

- **Critical path:** 1) Define annotation rubric and train annotators; 2) Label responses; resolve uncertain cases via disagreement coding; 3) Split data at learner level to prevent leakage; 4) Fine-tune GPT-4o; train stylometric baseline; query GPTZero; 5) Evaluate on in-domain and out-of-domain test sets; 6) Fit mixed-effects model to assess learning performance correlation.

- **Design tradeoffs:** Including uncertain cases lowers IRR and model precision but improves robustness to edge cases; excluding them inflates accuracy artificially. Fine-tuning improves in-domain accuracy but may reduce generalizability; out-of-domain testing shows 77% accuracy (vs. 80% in-domain). Stylometric baseline offers interpretability and reproducibility but caps performance at 77% accuracy.

- **Failure signatures:** High false-positive rate on GPTZero for ambiguous content (F1=0.00 for uncertain class). Both models over-assign "LLM-generated" labels to uncertain responses (fine-tuned: 28/51; GPTZero: 86/51). Performance drops on paraphrased or human-refined LLM outputs (not tested; acknowledged limitation).

- **First 3 experiments:** 1) Replicate fine-tuning on a different lesson within the same platform to measure domain transfer using the provided GitHub data and code. 2) Ablate stylometric features to identify which of the 13 features drive baseline performance; compare to fine-tuned model error patterns. 3) Augment training data with synthetically paraphrased LLM responses to test robustness to humanized outputs; report change in F1 for the LLM-generated class.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset limited to 1,635 responses from 534 learners within a single instructional platform, limiting generalizability.
- Fine-tuning hyperparameters and prompt templates not fully specified, hindering exact replication.
- No ground-truth verification of LLM use; "suspected LLM use" based on model predictions rather than direct observation.

## Confidence

**High Confidence:**
- Fine-tuned GPT-4o achieves 80% accuracy and 0.78 F1 on in-domain test set, outperforming GPTZero (70% accuracy, 0.50 macro F1).
- Stylometric baseline achieves 77% accuracy, confirming surface features are predictive but suboptimal.
- Mixed-effects logistic regression shows suspected LLM use correlates with higher posttest MCQ correctness (OR=2.37, p<0.001).

**Medium Confidence:**
- The rubric-defined distinctions between human and LLM responses generalize to unseen responses within similar instructional domains.
- Stylometric features capture consistent stylistic signatures that differ from typical learner writing in this context.

**Low Confidence:**
- The observed performance difference reflects LLM-mediated bypass rather than unmeasured confounds like prior knowledge or engagement.
- Fine-tuned model robustness to paraphrased or human-refined LLM outputs.

## Next Checks

1. **Domain Transfer Validation:** Replicate fine-tuning on a different lesson within the same platform to measure cross-lesson generalization using the provided GitHub data and code.

2. **Feature Ablation Analysis:** Identify which of the 13 stylometric features drive baseline performance; compare to fine-tuned model error patterns to isolate key discriminators.

3. **Robustness to Humanized Outputs:** Augment training data with synthetically paraphrased LLM responses; report change in F1 for the LLM-generated class to test resilience against output editing.