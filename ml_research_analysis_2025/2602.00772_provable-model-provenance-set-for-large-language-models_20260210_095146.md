---
ver: rpa2
title: Provable Model Provenance Set for Large Language Models
arxiv_id: '2602.00772'
source_url: https://arxiv.org/abs/2602.00772
tags:
- provenance
- size
- coverage
- candidate
- 'true'
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a formal framework for model provenance analysis
  with provable statistical guarantees. The core method, Model Provenance Set (MPS),
  uses a sequential hypothesis-testing procedure to identify all models related to
  a target model within a candidate pool.
---

# Provable Model Provenance Set for Large Language Models

## Quick Facts
- arXiv ID: 2602.00772
- Source URL: https://arxiv.org/abs/2602.00772
- Reference count: 40
- Primary result: Introduces a formal framework with provable statistical guarantees for identifying all models related to a target LLM within a candidate pool

## Executive Summary
This paper introduces Model Provenance Set (MPS), a statistical framework for identifying all models related to a target large language model within a candidate pool. The method employs sequential hypothesis testing with permutation-based p-values to construct a set that provably contains all true provenance models with coverage guarantee at least 1-α. MPS iteratively tests for significant similarity and excludes the most related model until no distinguishably similar models remain, providing rigorous significance metrics for practical IP protection and auditing applications.

## Method Summary
MPS uses a sequential test-and-exclusion procedure to adaptively construct a provenance set. For each candidate model, it computes relative-distance-based studentized t-statistics and uses permutation testing to assess significance. The algorithm iteratively excludes the most similar candidate (lowest t-statistic) when the test rejects the null hypothesis of equal expected distances. The process continues until the test fails to reject, indicating remaining candidates are statistically indistinguishable from the background. The framework supports any distance function and provides provable coverage guarantees.

## Key Results
- MPS achieves 96% coverage with average predicted set size of 2.1 when identifying 2 true sources among 50 candidates at α=0.05
- The method consistently identifies true provenance models while excluding unrelated ones across significance levels
- MPS outperforms existing empirical baselines on model attribution and unauthorized derivation detection tasks
- Token-MPS performs better for shallow lineages (TAM ≤ 1), while Semantic-MPS is more robust for deep lineages (TAM = 2)

## Why This Works (Mechanism)

### Mechanism 1
Sequential exclusion of the most-similar candidate identifies all true provenance models while controlling false coverage. In each iteration, MPS tests the null hypothesis that all remaining candidates have equal expected distance to the target. If rejected (p ≤ α), the model with the minimum relative distance is excluded and added to the provenance set. This repeats until the test fails to reject, indicating remaining candidates are statistically indistinguishable from the background. Core assumption: True provenance models have systematically smaller expected distances to the target than unrelated models.

### Mechanism 2
Relative-distance-based studentized t-statistics enable detection of provenance without requiring absolute similarity thresholds. For each candidate fi, MPS computes the relative deviation ¯di· = (1/M) Σj∈M ¯dij, where ¯dij is the average pairwise distance difference. The t-statistic ti = ¯di· / √cvar(¯di·) measures how much fi deviates from the ensemble average. The minimum t-statistic (Tmin) identifies the most similar candidate; permutation testing assesses whether this minimum is significantly small. Core assumption: Under the null, distance vectors are exchangeable across models.

### Mechanism 3
Permutation testing provides a valid, fingerprint-agnostic null distribution for significance assessment. For each prompt, the distance values L·,t are randomly permuted across candidates, breaking systematic model differences while preserving marginal distributions. After R permutations, the p-value is the proportion of permuted Tmin statistics ≤ the observed Tmin. This avoids parametric assumptions and works with any distance function. Core assumption: Under H0,M, the distance vectors are exchangeable; permutation rounds R and sample size N are sufficiently large for asymptotic convergence.

## Foundational Learning

- **Hypothesis testing and p-values**: Understanding null/alternative hypotheses, Type I error, and p-value interpretation is essential since MPS is built on statistical significance testing. Quick check: Can you explain what it means for a p-value to be valid (Pr(p ≤ α | H0) ≤ α)?

- **Permutation tests**: MPS uses permutation to construct the null distribution; understanding exchangeability and resampling is critical. Quick check: Why does permuting labels (or distances) preserve the null distribution when the null hypothesis is true?

- **Coverage guarantees and confidence sets**: The core contribution is provable coverage (Pr(M* ⊆ M̂) ≥ 1-α); understanding set-valued inference distinguishes MPS from point estimates. Quick check: What is the difference between a 95% confidence interval and a 95% coverage set?

## Architecture Onboarding

- **Component map**: Prompt Generator -> Distance Matrix L -> Test Statistic Module -> Permutation Engine -> Exclusion Loop -> Output

- **Critical path**: Generate or load prompts (T = 15k–100k sufficient) → Query all candidates + target on each prompt; compute distance matrix L → Enter exclusion loop: compute t-statistics → permute R times → compute p-value → exclude or terminate → Return provenance set and NI-Score

- **Design tradeoffs**: α (significance level): Lower α → stricter coverage guarantee but potentially larger sets; α = 0.05 is a common default. R (permutations): More permutations → more stable p-values; R ≥ 100 sufficient per experiments. T (prompts): More prompts → better signal-to-noise; T ≈ 15k–70k plateau depending on lineage depth. Distance function: Token-MPS is faster and works well for shallow lineages; Semantic-MPS is more robust for deep lineages (TAM=2).

- **Failure signatures**: Empty set when provenance exists: Check if distance function is insensitive to derivation; consider switching from Token-MPS to Semantic-MPS. Large predicted sets with many false positives: May indicate insufficient prompts (low T) or high noise; increase T or verify prompt diversity. Coverage drops below 1−α: Check for exchangeability violations (e.g., correlated prompts) or insufficient R/N.

- **First 3 experiments**: Sanity check on known pairs: Select 10 model pairs with confirmed derivation relationships; run MPS with α=0.05, R=100, T=15k; verify that provenance sets include the true parent. Ablation on prompt count: Fix |M|=20, TAM=1, vary T ∈ {1k, 5k, 15k, 50k}; plot coverage vs. set size to identify minimum viable T. Distance function comparison: Run Token-MPS and Semantic-MPS on deep-lineage cases (TAM=2); compare coverage and set size to assess which function is more discriminative for your domain.

## Open Questions the Paper Calls Out

### Open Question 1
Can the MPS framework accurately resolve provenance in model merging scenarios (e.g., linear interpolation) where a target model inherits properties from multiple parents simultaneously? The introduction identifies "complex topologies" like merging as a key limitation of prior heuristic methods, yet the experiments exclusively utilize derivation chains (fine-tuning) rather than merged model structures. Theorem 3.3 relies on a "gap" condition where true sources must be significantly closer to the target than non-sources; in merged models, the target may sit equidistant from multiple parents, potentially violating this separation assumption.

### Open Question 2
Is the statistical validity of the permutation test maintained under adversarial perturbations designed to obscure model lineage? The paper focuses on "lighter customizations" (fine-tuning, quantization) and Theorem 3.1 assumes stationarity conditions; it does not analyze how intentional, malicious modifications (e.g., anti-fingerprinting regularization) affect the exchangeability of distance statistics. Adversarial training could potentially compress the distribution of dissimilarity scores $L_{i,t}$ or break the variance assumptions, leading to inflated p-values and false negatives.

### Open Question 3
Does the required sample size ($N$) for reliable provenance detection scale linearly with model parameter count or vocabulary complexity? The experimental setup explicitly filters models to "under 3B parameters" due to resource constraints, leaving the performance on larger state-of-the-art models (e.g., 7B–70B) unverified. Larger models may exhibit higher intrinsic variance in next-token distributions or more complex semantic spaces, potentially requiring more than the $T=100k$ prompts used in the current study to satisfy the Central Limit Theorem conditions.

## Limitations
- Validity depends on exchangeability assumption; correlated prompts or shared architectural similarities may invalidate p-values
- Assumes true provenance models have systematically lower distances to target; may fail for obfuscated derivations or models with similar training data
- Computational requirements scale with |M|×N×R, making large-scale applications potentially expensive

## Confidence
- **High Confidence**: Sequential exclusion procedure correctly identifies statistically significant similarity (supported by Theorem 3.1 and experimental coverage results)
- **Medium Confidence**: Token-MPS and Semantic-MPS distance functions adequately distinguish related from unrelated models (supported by benchmark results but not independently validated)
- **Medium Confidence**: Coverage guarantees hold across different significance levels and candidate pool sizes (supported by ablation studies but with limited independent replication)

## Next Checks
1. **Exchangeability Validation**: Test MPS on prompts with known correlations (e.g., sequential prompts from the same document) to assess p-value validity under violation of exchangeability
2. **Obfuscation Robustness**: Apply MPS to models with known adversarial fine-tuning or quantization obfuscation to determine sensitivity to derivation masking techniques
3. **Scalability Benchmark**: Measure runtime and memory scaling with |M| = 200 and N = 200k prompts to identify practical computational limits for real-world applications