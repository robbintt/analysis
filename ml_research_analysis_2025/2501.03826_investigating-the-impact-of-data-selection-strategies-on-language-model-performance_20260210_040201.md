---
ver: rpa2
title: Investigating the Impact of Data Selection Strategies on Language Model Performance
arxiv_id: '2501.03826'
source_url: https://arxiv.org/abs/2501.03826
tags:
- data
- selection
- dataset
- distribution
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates data selection strategies for language
  model pretraining, focusing on aligning training data with target distributions.
  The authors propose a hybrid importance resampling (HIR) method that combines n-gram
  features and neural embedding-based features to compute sample importance weights
  for data selection.
---

# Investigating the Impact of Data Selection Strategies on Language Model Performance

## Quick Facts
- arXiv ID: 2501.03826
- Source URL: https://arxiv.org/abs/2501.03826
- Reference count: 2
- Primary result: DSIR (n-gram-based) consistently outperforms HIR (neural embedding-based) and random selection on GLUE tasks

## Executive Summary
This paper investigates data selection strategies for language model pretraining, focusing on aligning training data with target distributions. The authors propose a hybrid importance resampling (HIR) method that combines n-gram features and neural embedding-based features to compute sample importance weights for data selection. They compare HIR with random selection and DSIR (a n-gram-based method) using the Pile dataset and evaluate the selected data through language model pretraining and fine-tuning on GLUE benchmark tasks. Results show that DSIR consistently outperforms HIR and random selection across most GLUE tasks, demonstrating the effectiveness of n-gram-based token-level modeling for language model pretraining.

## Method Summary
The paper proposes two data selection methods: DSIR (Distributional Synthetic Importance Resampling) using hashed n-gram features and HIR (Hybrid Importance Resampling) combining n-gram and neural embedding features. Both methods compute importance weights as ω(x) = p(x)/q(x) where p is the target distribution and q is the raw distribution, then resample with probability proportional to these weights. DSIR uses a 10,000-dimensional hashed feature space for unigrams and bigrams, while HIR uses SentenceTransformer embeddings with GMM density estimation. The selected subsets are used to pretrain language models which are then fine-tuned on GLUE benchmark tasks.

## Key Results
- DSIR consistently outperforms HIR and random selection across most GLUE tasks
- N-gram-based methods show superior alignment with language model pretraining objectives compared to neural embedding methods
- HIR's neural component shows promise in capturing semantic richness but is less effective for token prediction tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Importance resampling aligns raw data distributions with target distributions, improving downstream task performance.
- Mechanism: Compute sample importance weights as ω(x) = p(x)/q(x) where p is the target distribution and q is the raw distribution. Resample with probability proportional to these weights, selecting examples that better match desired characteristics.
- Core assumption: The target distribution (Wikipedia + Gutenberg) represents high-quality text that transfers well to GLUE tasks; distributional alignment causally improves downstream generalization.
- Evidence anchors:
  - [abstract] "Results show that DSIR consistently outperforms HIR and random selection across most GLUE tasks, demonstrating the effectiveness of n-gram-based token-level modeling"
  - [section 6, Results] "By comparing the results of random selection and the two other methods, we can conclude that models pretrained on datasets selected through n-gram and embedding-based methods exhibit superior alignment with target data distributions"
  - [corpus] Weak direct corpus support for importance resampling specifically; neighbor papers focus on training optimization strategies rather than data selection mechanisms
- Break condition: If raw and target distributions already overlap significantly, importance weights approach 1.0 uniformly and selection becomes equivalent to random sampling.

### Mechanism 2
- Claim: Hashed n-gram features capture token-level statistics more aligned with language model pretraining objectives than sentence-level embeddings.
- Mechanism: Map each sample to a 10,000-dimensional feature vector counting unigrams and bigrams via hashing. Estimate multinomial distributions over these features for both raw and target data, then compute importance weights from distribution ratios.
- Core assumption: Token prediction tasks (masked language modeling) primarily benefit from local token-level distributional alignment rather than global semantic similarity.
- Evidence anchors:
  - [section 7, Discussion] "DSIR excels at capturing local token-level features that align closely with the objectives of language model training... less directly aligned with the low-level token prediction tasks"
  - [section 6, Results] "DSIR consistently demonstrates superior performance, achieving the best results in five out of the six tasks (COLA, MRPC, QNLI, RTE, and SST2)"
  - [corpus] No direct corpus validation; related work on discrete representations in speech (arxiv 2509.05359) suggests token-level features matter for pretraining
- Break condition: If target domain requires semantic reasoning beyond surface token patterns (e.g., specialized jargon, domain-specific syntax), n-gram features may miss critical selection signals.

### Mechanism 3
- Claim: Neural embedding-based distribution matching captures global semantic structure but provides weaker alignment with token prediction pretraining.
- Mechanism: Embed each sample using SentenceTransformer (384-dim), fit Gaussian Mixture Models (GMM) to estimate raw and target distributions, compute importance weights from density ratios. Combined with n-gram via weighting parameter α.
- Core assumption: GMM with diagonal covariance can accurately estimate probability densities in high-dimensional embedding space; sentence-level semantic similarity indicates pretraining utility.
- Evidence anchors:
  - [section 3.2] "pnn is estimated using a diagonal Gaussian Mixture Model (GMM) with 1000 components, while pnn is estimated with 50 components"
  - [section 7, Discussion] "Applying a diagonal GMM to high-dimensional embedding spaces may not yield accurate probability estimates... importance weights will be less reliable"
  - [corpus] No corpus papers directly validate or refute GMM-based data selection for language models
- Break condition: If embedding space dimensionality exceeds effective GMM capacity, or if target/raw distributions differ minimally at sentence level, weights become noisy and selection degrades.

## Foundational Learning

- Concept: **Importance Sampling/Resampling**
  - Why needed here: Core mathematical framework underlying both DSIR and HIR; understanding how p(x)/q(x) enables distributional shift is essential for debugging selection quality.
  - Quick check question: If your target distribution has vocabulary concentrated on technical terms absent from raw data, what happens to importance weights for those terms?

- Concept: **Hashed Feature Maps (Feature Hashing)**
  - Why needed here: DSIR uses 10,000-dimensional hashed n-gram features; collisions introduce noise but enable efficient computation without storing explicit vocabularies.
  - Quick check question: If two distinct bigrams hash to the same index, how does this affect your distribution estimates and resulting selection?

- Concept: **Gaussian Mixture Models for Density Estimation**
  - Why needed here: HIR relies on GMM to estimate p(x) and q(x) in embedding space; poor density estimates directly corrupt importance weights.
  - Quick check question: Why might diagonal covariance GMMs fail to capture meaningful density structure in 384-dimensional embedding space?

## Architecture Onboarding

- Component map:
  Raw Data (Pile subset) -> N-gram Hashing -> Multinomial Distribution q_ng, p_ng -> ω_ng
                           -> SentenceTransformer -> GMM -> q_nn, p_nn -> ω_nn
                                   -> Combined: ω_hybrid = ω_ng^α × ω_nn^(1-α)
                                   -> Resample k examples -> Pretrain LM -> Fine-tune GLUE

- Critical path:
  1. Feature extraction quality (hashing collisions, embedding quality) determines distribution estimate fidelity
  2. Distribution estimate accuracy determines importance weight reliability
  3. Weight reliability determines whether selected subset actually matches target
  4. Subset-target alignment determines pretraining transfer to GLUE

- Design tradeoffs:
  - **N-gram vs. Neural features**: N-gram is computationally cheap, captures local token patterns; Neural captures global semantics but requires GMM fitting and may misalign with token prediction objectives
  - **GMM components**: 1000 for raw (diverse), 50 for target (compact) — more components capture complexity but increase memory and overfitting risk
  - **Subset size k**: Paper used 1.7M for DSIR, 47K for HIR due to compute constraints — smaller subsets may not provide sufficient pretraining signal

- Failure signatures:
  - Selected data visually resembles raw data (importance weights too uniform → check distribution divergence)
  - GMM likelihoods are near-zero (model misspecification → increase components or reduce dimensionality)
  - Large variance across random seeds (unstable selection → increase k or verify weight computation)
  - GLUE performance below random baseline (selection actively harmful → verify target distribution quality)

- First 3 experiments:
  1. **Sanity check**: Run DSIR with α=1 and α=0 (pure n-gram vs. pure neural) on same data split; expect α=1 to match or exceed α=0 on token-heavy tasks (QNLI, SST-2)
  2. **Distribution divergence diagnostic**: Compute KL divergence between estimated p and q for both n-gram and neural representations; if divergence < 0.1, raw data already resembles target and selection provides marginal gains
  3. **Ablation on target composition**: Test Wikipedia-only vs. Gutenberg-only vs. combined targets; verify that combined target does not cancel out domain-specific benefits via mode averaging

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal value of α for balancing n-gram and neural embedding features in the Hybrid Importance Resampling (HIR) method?
- Basis in paper: [explicit] The authors state: "The decision to test only α = 0 stems from the complexity involved in fine-tuning this parameter... The selection of an optimal α value, which could balance n-gram and neural embedding contributions, is left as an avenue for future work."
- Why unresolved: Testing multiple α values requires extensive experimentation across diverse tasks and datasets, which was computationally infeasible for this study.
- What evidence would resolve it: A systematic sweep of α ∈ [0,1] across multiple target domains and downstream tasks, reporting performance curves.

### Open Question 2
- Question: Can neural feature-based importance resampling scale efficiently to the full 800GB Pile dataset without prohibitive computational overhead?
- Basis in paper: [explicit] "The computational overhead of calculating importance weights, especially for neural feature-based methods, was not addressed in detail. This could be a limitation when scaling to large datasets or high-dimensional feature spaces."
- Why unresolved: GMM fitting on high-dimensional embeddings (384-dim) requires iterative chunk-based processing; the paper only experiments on ~1.4M documents.
- What evidence would resolve it: Runtime and memory benchmarks for HIR on the complete Pile, compared against DSIR's hashed n-gram approach.

### Open Question 3
- Question: Does HIR's relative underperformance stem from diagonal covariance approximation in the GMM, or from fundamental misalignment between sentence-level embeddings and token-level pretraining objectives?
- Basis in paper: [inferred] The discussion notes that "applying a diagonal GMM to high-dimensional embedding spaces may not yield accurate probability estimates," but does not isolate whether this modeling choice or the feature granularity itself causes the gap with DSIR.
- Why unresolved: The paper tests only one embedding type (SentenceTransformer) and one GMM configuration (diagonal, 50/1000 components).
- What evidence would resolve it: Ablations using full-covariance GMMs or alternative embedding models, paired with analysis of importance weight calibration.

## Limitations

- Empirical findings based on single dataset combination (Wikipedia + Gutenberg) and GLUE evaluation suite, limiting generalizability
- DSIR and HIR compared with different subset sizes (1.7M vs 47K), potentially confounding selection method quality with pretraining data volume
- No variance reporting across random seeds, making statistical significance of performance differences unclear
- HIR's reliance on GMM density estimation in 384-dimensional space with diagonal covariance assumptions may produce unreliable importance weights

## Confidence

**High confidence**: The observation that DSIR outperforms random selection across multiple GLUE tasks is well-supported by the experimental results presented. The comparative ranking of methods (DSIR > HIR > random) appears robust within the tested configuration.

**Medium confidence**: The claim that n-gram-based token-level modeling is inherently more effective than neural embedding-based methods for language model pretraining requires qualification. While supported by the GLUE results, this conclusion depends on the specific pretraining objective (masked language modeling) and the token-heavy nature of GLUE tasks. The effectiveness may differ for other pretraining paradigms or downstream tasks requiring semantic reasoning.

**Low confidence**: The assertion that HIR's inferior performance stems primarily from GMM density estimation limitations in high-dimensional space is speculative. While the paper acknowledges this possibility, it does not provide direct evidence through controlled experiments varying GMM components, covariance structures, or embedding dimensionality. Alternative explanations include insufficient subset size or fundamental misalignment between sentence-level semantic similarity and token prediction objectives.

## Next Checks

1. **Statistical significance testing**: Repeat all experiments with 5 different random seeds and report mean performance with 95% confidence intervals. Apply paired statistical tests (e.g., paired t-tests or Wilcoxon signed-rank) to determine whether performance differences between methods are statistically significant, not just numerically superior.

2. **Controlled subset size comparison**: Run DSIR and HIR on identically sized subsets (e.g., 47K samples each) to isolate the effect of selection method from pretraining data volume. If DSIR maintains superiority at smaller scales, this strengthens the claim that n-gram features provide better selection quality rather than simply more data.

3. **Target distribution ablation study**: Evaluate pretraining performance when using Wikipedia-only, Gutenberg-only, and the combined target as separate experimental conditions. Measure KL divergence between estimated raw/target distributions for each case and correlate with downstream task performance to verify that the combined target does not average away domain-specific benefits or introduce mode collapse in the selection process.