---
ver: rpa2
title: 'IndieFake Dataset: A Benchmark Dataset for Audio Deepfake Detection'
arxiv_id: '2506.19014'
source_url: https://arxiv.org/abs/2506.19014
tags:
- audio
- dataset
- deepfake
- samples
- speaker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the IndieFake Dataset (IFD), a new benchmark
  dataset for audio deepfake detection tailored to Indian English speakers. IFD addresses
  the lack of ethnic diversity in existing datasets by including 27.17 hours of bonafide
  and deepfake audio from 50 Indian speakers across various demographics.
---

# IndieFake Dataset: A Benchmark Dataset for Audio Deepfake Detection

## Quick Facts
- **arXiv ID:** 2506.19014
- **Source URL:** https://arxiv.org/abs/2506.19014
- **Reference count:** 40
- **One-line primary result:** IFD outperforms ASVspoof21 (DF) in EER and accuracy for Indian English deepfake detection.

## Executive Summary
This paper introduces the IndieFake Dataset (IFD), a new benchmark dataset for audio deepfake detection tailored to Indian English speakers. IFD addresses the lack of ethnic diversity in existing datasets by including 27.17 hours of bonafide and deepfake audio from 50 Indian speakers across various demographics. The dataset features balanced data distribution and speaker-level characterization, unlike existing datasets like ASVspoof21 (DF). Evaluation using multiple baseline models (LCNN, Mesonet, RawNet3) demonstrates that IFD outperforms ASVspoof21 (DF) in terms of Equal Error Rate (EER) and testing accuracy. For example, LFCC-LCNN achieves an EER of 0.119 on IFD compared to 0.233 on ASVspoof21 (DF). The dataset also proves more challenging than the In-The-Wild (ITW) dataset, highlighting its effectiveness as a benchmark. The complete dataset is publicly available for research use.

## Method Summary
The IndieFake Dataset (IFD) is constructed from YouTube videos (bonafide) and synthesized audio using Polly, Play.ht, and ElevenLabs (deepfake). The dataset includes 27.17 hours of audio from 50 Indian speakers, balanced between bonafide and deepfake classes, with speaker-independent train/test splits. Audio is standardized to 5 seconds at 16kHz. Baseline models (LFCC-LCNN, MFCC-LCNN, LFCC-Mesonet, MFCC-Mesonet, RawNet3) are evaluated using Equal Error Rate (EER) and accuracy. Training uses Adam optimizer, batch size 64, BCE loss, and various augmentations. RawNet3 requires Group Normalization due to sensitivity to audio variations.

## Key Results
- LFCC-LCNN achieves an EER of 0.119 on IFD, significantly outperforming its 0.233 EER on ASVspoof21 (DF).
- IFD is more challenging than the In-The-Wild (ITW) dataset, demonstrating its effectiveness as a benchmark.
- Pre-training on ASVspoof21 (DF) degrades performance on IFD, likely due to class imbalance and domain shift.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training on accent-specific data (Indian English) appears to improve detection generalization for that demographic compared to training on ethnically homogenous datasets.
- **Mechanism:** Domain adaptation via representative sampling. By introducing phonetic and prosodic patterns specific to Indian English speakers (often under-represented in datasets like ASVspoof), the model learns features relevant to this specific distribution, reducing the gap in real-world performance noted in existing benchmarks.
- **Core assumption:** The performance gain stems specifically from accent diversity rather than merely the quality of the deepfake generation methods used.
- **Evidence anchors:**
  - [abstract] "Existing datasets lack diverse ethnic accents... models trained on these datasets struggle... in South-Asian countries."
  - [section I] "ASVspoof21 (DF)... lacks ethnic and linguistic diversity, particularly for Indian speakers."
  - [corpus] *BanglaFake* (arXiv:2505.10885) supports the necessity of language-specific datasets for low-resource or specific demographics to improve detection reliability.
- **Break condition:** If a model trained on IFD fails to detect deepfakes from Indian speakers using TTS engines *not* included in the IFD generation pipeline (e.g., a new Zero-shot TTS), the mechanism relies on overfitting to specific synthesizers rather than learning generalizable accent features.

### Mechanism 2
- **Claim:** Balanced class distribution (bonafide vs. deepfake) and subject-independent splitting facilitate more robust model calibration than highly imbalanced datasets.
- **Mechanism:** Optimization stability via loss balancing. The paper notes that pre-training on the heavily imbalanced ASVspoof21 (DF) (roughly 95:5 fake:bonafide) degrades performance on IFD. A balanced dataset prevents the loss function and decision threshold from being skewed toward the majority class, allowing the classifier to learn distinct boundaries.
- **Core assumption:** The performance drop observed when using ASVspoof pre-trained weights is primarily caused by class imbalance and domain shift, not by the architectural suitability of the pre-trained features.
- **Evidence anchors:**
  - [abstract] "IFD offers balanced data distribution... absent in datasets like ASVspoof21 (DF)."
  - [section III-D] "Pretraining on ASVspoof21 (DF) generally hinders model performance, likely due to its highly imbalanced nature."
  - [corpus] *AUDDT* (arXiv:2509.21597) highlights that models evaluated on narrow sets often fail to generalize, reinforcing the need for robust benchmark construction.
- **Break condition:** If a model trained on IFD exhibits high False Positive Rates (FPR) on bonafide samples despite the balanced training, the "balanced distribution" mechanism alone is insufficient to prevent bias.

### Mechanism 3
- **Claim:** Speaker-independent splits (where training and testing speakers are disjoint) force models to learn synthesis artifacts rather than memorizing speaker-specific biological traits.
- **Mechanism:** Content-agnostic feature learning. By ensuring "no speaker's bonafide or deepfake samples appear in both training and testing sets," the architecture cannot rely on recognizing a specific person's voice (simple speaker verification) but must identify the "synthetic" quality of the audio, making it a true deepfake detector.
- **Core assumption:** The synthetic artifacts are consistent enough across different speakers in the dataset to be learnable from the training set and applicable to the unseen test speakers.
- **Evidence anchors:**
  - [section II-B1] "We adopted subject independent splitting approach to mitigate subject dependency and test the generalization capability of models."
  - [abstract] "IFD... includes speaker-level characterization."
  - [corpus] *DeepFake Doctor* (arXiv:2506.05851) discusses generalizing detection across unseen domains, supporting the need for rigorous evaluation setups.
- **Break condition:** If a model performs well on seen speakers but fails catastrophically on unseen speakers within the same dataset, the dataset may lack diversity in synthesis artifacts, or the model has overfitted to recording conditions.

## Foundational Learning

- **Concept: LFCC (Linear Frequency Cepstral Coefficients)**
  - **Why needed here:** The paper identifies LFCC-LCNN as a top-performing baseline (EER 0.119). LFCCs are critical for capturing high-frequency artifacts often left by vocoders in speech synthesis, distinguishing them from standard MFCCs which model human speech perception.
  - **Quick check question:** Why might Linear Frequency scales (LFCC) detect synthetic artifacts better than Mel scales (MFCC) in high-frequency ranges?

- **Concept: Subject-Independent vs. Subject-Dependent Splitting**
  - **Why needed here:** IFD emphasizes "subject-independent" splitting. Understanding the difference is vital: Subject-dependent allows the model to learn "who" the speaker is, while subject-independent forces it to learn "what" makes the audio fake.
  - **Quick check question:** If you train a detector on Speaker A's real and fake voice, and test it on Speaker B, are you evaluating speaker verification or deepfake detection?

- **Concept: Equal Error Rate (EER)**
  - **Why needed here:** This is the primary metric for evaluation. You must interpret that lower EER is better and that EER represents the point where False Acceptance Rate equals False Rejection Rate.
  - **Quick check question:** If a model has an EER of 0.50 on a binary classification task, what does that imply about its predictive power compared to random guessing?

## Architecture Onboarding

- **Component map:**
  - **Frontend (Data):** YouTube (Bonafide) + TTS Engines (Polly, Play.ht, ElevenLabs) -> Augmentation (Noise/Pitch) -> 5s 16kHz Audio.
  - **Feature Extractors:** LFCC or MFCC (for LCNN/Mesonet) OR Raw Waveform (for RawNet3).
  - **Backends:** LCNN (Max Feature Map), Mesonet (compact convolution), RawNet3 (ResNet + GRU + Attentive Pooling).
  - **Head:** Binary Cross-Entropy Loss (Bonafide vs. Deepfake).

- **Critical path:**
  The data split methodology is the most critical constraint. You must ensure strict subject-independent separation. If Speaker X is in the Train set, no audio (real or fake) from Speaker X can be in the Test set. The "Cross-Speaker Transcript" scenario (Section II-A-3) is also critical; it prevents the model from cheating by matching text content rather than analyzing audio quality.

- **Design tradeoffs:**
  - **Dataset Size vs. Quality:** IFD is 1/6th the size of ASVspoof21 (DF) but yields lower EERs on specific tasks. You trade data volume for density of relevant accents and balanced classes.
  - **Raw vs. Handcrafted Features:** RawNet3 (Raw) performs best on IFD (0.061 EER), but LFCC-LCNN is competitive and potentially more interpretable. Raw audio requires careful normalization (Group Norm vs. Batch Norm) which the ablation study highlights as a failure point.

- **Failure signatures:**
  - **Normalization Drift:** As noted in the ablation (Section III-E), models using Batch Normalization (implied) or Instance Normalization performed poorly on raw audio during testing due to mismatched statistics. *Solution:* Use Group Normalization.
  - **Pre-training Degradation:** Initializing with ASVspoof21 weights *decreases* accuracy (Table II, Col B vs A). *Solution:* Train from scratch on IFD or use a more generic pre-trainer.

- **First 3 experiments:**
  1.  **Baseline Reproduction (LFCC-LCNN):** Train LFCC-LCNN from scratch on the IFD training split. Target the 0.119 EER reported in Table II to validate your data pipeline.
  2.  **Generalization Test (Cross-Dataset):** Train on ASVspoof21 (DF) and test on IFD, then reverse it (Train IFD, Test ASVspoof). Confirm the paper's claim that IFD is "more challenging" (higher EER) than the ITW dataset.
  3.  **Normalization Ablation (RawNet3):** Implement RawNet3 with Group Normalization (small group size) vs. Instance Normalization on raw IFD audio to verify the specific sensitivity to loudness and noise described in Section III-E.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can domain adaptation or specialized fine-tuning strategies mitigate the negative transfer observed when applying models pre-trained on the imbalanced ASVspoof21 (DF) to the balanced IndieFake Dataset?
- Basis in paper: [inferred] Section III-D notes that performance deteriorated when using pre-trained weights from ASVspoof21 (DF) due to its "highly imbalanced nature," hindering model performance.
- Why unresolved: The paper identifies the failure of standard transfer learning but does not propose or test methods to correct the class imbalance bias during the pre-training or fine-tuning stages.
- What evidence would resolve it: Experiments demonstrating that techniques such as class-weighted loss functions or layer-freezing strategies allow ASVspoof21 pre-trained models to match or exceed the performance of models trained from scratch on IFD.

### Open Question 2
- Question: Which normalization technique optimally stabilizes the training of end-to-end raw waveform models (like ICDD and ISCSE) against the loudness and pitch variations found in the IndieFake Dataset?
- Basis in paper: [inferred] Section III-E (Ablation Study) highlights that raw audio inputs face challenges due to "large variations caused by factors such as loudness, noise, and pitch," and compares Group Normalization (GN) against Instance Normalization (IN).
- Why unresolved: While the ablation study shows GN outperforms IN, the modified RawNet3 (without the F-bank layer) performed poorly, and the tested configurations of ICDD/ISCSE did not match the baseline RawNet3, leaving the optimal architectural configuration unresolved.
- What evidence would resolve it: A comprehensive comparison of normalization layers (including Batch Norm, Layer Norm, and Group Norm) across various batch sizes specifically on the IFD test set to identify the most robust configuration for accented speech.

### Open Question 3
- Question: Does the gender imbalance in the dataset (38 male vs. 12 female speakers) result in a significant performance disparity in detection accuracy between male and female audio samples?
- Basis in paper: [inferred] Section II-B1 states the dataset contains 38 male and 12 female speakers, and while the train/test splits maintain this ratio, the authors do not analyze performance metrics based on gender.
- Why unresolved: The paper reports aggregate Equal Error Rates (EER) and accuracy but does not provide a breakdown of false positives or negatives stratified by gender, leaving the question of bias unresolved.
- What evidence would resolve it: A per-class evaluation reporting separate EER and accuracy scores for male and female speakers within the test set.

### Open Question 4
- Question: How does the detection difficulty vary across the three specific deepfake generation scenarios: hypothetical transcripts, same-speaker transcripts, and cross-speaker transcripts?
- Basis in paper: [inferred] Section II-A defines three distinct scenarios for generating deepfakes (hypothetical, same-speaker, and cross-speaker) to introduce different challenges, but Section III-D only reports aggregate evaluation metrics.
- Why unresolved: It is unclear if the "challenge" of the dataset is driven equally by all scenarios or if specific scenarios (e.g., cross-speaker content transfer) are significantly harder for models to detect.
- What evidence would resolve it: A scenario-based breakdown of detection accuracy and EER, isolating the model's performance on the "cross-speaker" subset versus the "hypothetical transcript" subset.

## Limitations
- **Dataset Generality:** The dataset is narrowly focused on Indian English, limiting its applicability to other accents or languages without similar specialized datasets.
- **Evaluation Scope:** The primary evaluation focuses on EER and accuracy, without extensive exploration of other metrics like FPR or cross-dataset robustness under varying noise conditions.
- **Baseline Reproducibility:** The paper references external models but does not provide full architectural details or exact LFCC/MFCC extraction parameters, creating a dependency on external codebases.

## Confidence
- **High Confidence:** The claim that IFD offers a more balanced and speaker-independent benchmark than ASVspoof21 (DF) is well-supported by the dataset construction methodology and reported performance improvements.
- **Medium Confidence:** The assertion that accent-specific training improves generalization for Indian English speakers is plausible, but the exact contribution of accent diversity versus synthesis artifact learning requires further disentanglement.
- **Low Confidence:** The dataset's effectiveness as a benchmark for unseen synthesis methods is uncertain, as the current evaluation only tests against the TTS engines used in its creation.

## Next Checks
1. **Cross-Accent Generalization:** Evaluate models trained on IFD on deepfake audio from non-Indian English speakers to assess if accent-specific training leads to overfitting or true detection of synthesis artifacts.
2. **Unseen Synthesis Method Detection:** Test IFD-trained models on deepfakes generated by TTS engines not included in the dataset (e.g., Google Text-to-Speech or newer Zero-shot TTS) to verify robustness against novel synthesis techniques.
3. **Noise Robustness Analysis:** Conduct experiments under varying noise conditions (e.g., different SNR levels, real-world background noise) to determine the practical limits of IFD-trained models in deployed environments.