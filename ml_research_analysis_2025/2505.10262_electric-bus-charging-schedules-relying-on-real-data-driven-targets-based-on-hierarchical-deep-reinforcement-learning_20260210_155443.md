---
ver: rpa2
title: Electric Bus Charging Schedules Relying on Real Data-Driven Targets Based on
  Hierarchical Deep Reinforcement Learning
arxiv_id: '2505.10262'
source_url: https://arxiv.org/abs/2505.10262
tags:
- charging
- time
- policy
- low-level
- period
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the electric bus (EB) charging scheduling
  problem by leveraging hierarchical deep reinforcement learning (HDRL) to optimize
  charging targets and schedules under uncertainty in electricity prices and traffic
  conditions. The authors propose a two-level framework: a high-level semi-Markov
  decision process (SMDP) that determines optimal charging targets, and a low-level
  Markov decision process (MDP) that optimizes real-time charging power to meet these
  targets while minimizing costs.'
---

# Electric Bus Charging Schedules Relying on Real Data-Driven Targets Based on Hierarchical Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2505.10262
- Source URL: https://arxiv.org/abs/2505.10262
- Reference count: 40
- Key outcome: Hierarchical DRL achieves up to 39.60% cost reduction in EB charging scheduling

## Executive Summary
This paper addresses electric bus charging scheduling by leveraging hierarchical deep reinforcement learning (HDRL) to optimize charging targets and schedules under uncertainty in electricity prices and traffic conditions. The authors propose a two-level framework: a high-level semi-Markov decision process (SMDP) that determines optimal charging targets, and a low-level Markov decision process (MDP) that optimizes real-time charging power to meet these targets while minimizing costs. The Hierarchical Double Deep Q-Network (HDDQN) with Hindsight Experience Replay (HER) algorithm is developed to handle non-stationarity and sparse rewards. Experiments using real-world data demonstrate that the proposed HDDQN-HER algorithm outperforms baseline methods, achieving a 0.097% improvement in average performance over HDDQN and up to 39.60% over simpler approaches.

## Method Summary
The method employs a hierarchical reinforcement learning framework where a high-level agent (using SMDP) sets charging targets (SoC goals) at charging station arrivals, while a low-level agent (using MDP) determines specific charging power at 10-minute intervals to meet these targets. The HDDQN-HER algorithm combines Double DQN with Hindsight Experience Replay, using actual achieved states as goals in replay buffers to handle non-stationarity. Training occurs in two phases: Phase 1 uses a fixed greedy low-level policy to stabilize high-level learning, then Phase 2 uses the learned policy with HER relabeling. The environment simulates EB operations with real MISO electricity price data and stochastic travel times.

## Key Results
- HDDQN-HER achieves 0.097% improvement in average performance over HDDQN
- Up to 39.60% cost reduction compared to baseline DDQN approaches
- Convergence at approximately 8500 episodes versus 18000 for non-HER methods
- Effective adaptation to fluctuating electricity prices while maintaining sufficient battery energy

## Why This Works (Mechanism)

### Mechanism 1: Temporal Abstraction via Options
The paper utilizes the "options" framework to split the original flat MDP into a high-level Semi-Markov Decision Process (SMDP) and low-level MDPs. The high-level agent sets a "charging target" (SoC goal) only when a bus arrives at a station, while the low-level agent determines specific charging power at each fine-grained time step to meet that goal. The charging target serves as a sufficient statistic for the long-term future, allowing the low-level agent to act myopically without degrading global optimality.

### Mechanism 2: Hindsight Goal Replacement for Non-Stationarity
The algorithm stores the actual achieved State of Charge (SoC) as the action/goal in the high-level replay buffer, rather than the intended target. This aligns the high-level training distribution with the low-level agent's actual capabilities at that point in time, stabilizing training against the non-stationary behavior of a learning low-level agent.

### Mechanism 3: Intrinsic Reward Density via Hindsight Experience Replay (HER)
HER converts sparse reward problems into dense ones by storing transitions assuming the achieved state was the goal. If the agent fails to reach target T but accidentally reaches state S', it stores a transition assuming S' was the goal. This allows the agent to learn "how to reach arbitrary states" even when it fails the specific objective, significantly boosting sample efficiency.

## Foundational Learning

- **Concept: Semi-Markov Decision Process (SMDP)**
  - Why needed here: Standard MDPs assume fixed time steps, but high-level decisions cover variable durations (charging and route time).
  - Quick check question: How does the Bellman equation change when the time interval between decision points (Δt) is random rather than fixed?

- **Concept: Off-policy Correction / Non-stationarity**
  - Why needed here: The high-level agent generates data using a low-level agent that is constantly changing, creating distribution mismatch.
  - Quick check question: Why does storing the "intended" goal in the replay buffer create a distribution mismatch when the "executor" (low-level) policy updates?

- **Concept: Double Deep Q-Network (DDQN)**
  - Why needed here: Builds on DDQN to mitigate overestimation bias in Q-values.
  - Quick check question: Why does the standard DQN tend to overestimate action values, and how does the Double Q-learning estimator decouple selection from evaluation?

## Architecture Onboarding

- **Component map:** Environment -> High-Level Agent (SMDP/DDQN) -> Low-Level Agent (MDP/DDQN) -> Replay Buffers (RH, RL)
- **Critical path:** Two-Phase Training is the most brittle part. Phase 1 (e < 6000) uses fixed low-level policy πq for high-level reward; Phase 2 uses learned policy with HER.
- **Design tradeoffs:**
  - Fixed vs. Learned Low-Level in Phase 1: Fixed policy speeds up high-level learning but risks over-reliance on specific quirks
  - SoC Constraints: Option space restricted to physically achievable targets to prevent wasting samples on impossible goals
- **Failure signatures:**
  - High-Level Oscillation: Agent oscillates between extreme targets due to low-level agent changing too fast
  - Conservative Targets: High penalty causes defaulting to "always charge to 100%" ignoring price optimization
  - HER Noise: Aggressive HER may reward intentional target misses
- **First 3 experiments:**
  1. Run flat DDQN on full problem to confirm it fails to converge, validating need for hierarchy
  2. Compare HDDQN-HER vs. HDDQN (without HER) to verify convergence speed improvement
  3. Test trained agent on dataset with higher electricity price volatility to ensure generalization

## Open Questions the Paper Calls Out
- How can the HDRL framework be extended to optimize charging schedules for electric bus fleets, jointly addressing high-level bus scheduling and low-level charging power?
- Can safe reinforcement learning techniques be integrated to provide formal guarantees that agents avoid terminal states (battery depletion) during deployment?
- Does the policy derived from simulated environment parameters maintain robustness and performance when transferred to real-world electric bus operational data?

## Limitations
- Experimental validation based on single month of electricity price data (January 2023) may not capture full market volatility
- Simulation assumptions (fixed departure intervals, simplified battery dynamics) may not fully represent real-world complexity
- Two-phase training requires careful tuning of transition point (Mε=6000 episodes) not thoroughly justified

## Confidence

- **High Confidence**: The core mechanism of hierarchical decomposition with SMDP/MDP structure is well-established in RL literature and implementation appears sound
- **Medium Confidence**: The adaptation of HER for non-stationarity in hierarchical settings is innovative but claims about necessity could be overstated
- **Low Confidence**: Sensitivity to hyperparameters (κ and κ' values) is not extensively explored; assumption about isolating low-level MDPs may not hold for longer-term planning

## Next Checks
1. Evaluate trained policy on electricity price data from different seasons and years to assess robustness under unseen distributions
2. Systematically vary the transition point Mε and compare against single-phase joint training to quantify two-phase methodology's contribution
3. Extend environment to simulate 10+ buses with shared charging infrastructure to evaluate scalability of hierarchical approach