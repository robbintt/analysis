---
ver: rpa2
title: High-Dimensional Learning Dynamics of Quantized Models with Straight-Through
  Estimator
arxiv_id: '2510.10693'
source_url: https://arxiv.org/abs/2510.10693
tags:
- quantization
- learning
- arxiv
- generalization
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies quantized neural network training dynamics,
  focusing on how bit width and quantization range influence learning behavior. The
  authors analyze a linear regression model with jointly quantized weights and inputs,
  trained using the straight-through estimator (STE), in the high-dimensional limit.
---

# High-Dimensional Learning Dynamics of Quantized Models with Straight-Through Estimator

## Quick Facts
- arXiv ID: 2510.10693
- Source URL: https://arxiv.org/abs/2510.10693
- Reference count: 40
- Primary result: ODE analysis reveals quantized training exhibits a plateau-then-drop generalization error trajectory, with plateau length controlled by quantization range.

## Executive Summary
This paper analyzes the training dynamics of neural networks with quantized weights and inputs using the Straight-Through Estimator (STE) in the high-dimensional limit. The authors derive deterministic ODEs that characterize how quantization affects learning behavior, revealing a two-phase trajectory: an extended plateau followed by a sharp drop in generalization error. The analysis shows quantization range acts as an implicit regularizer, expanding stable learning regions and enabling higher learning rates. Input quantization degrades performance more severely than weight quantization, especially at low bit widths.

## Method Summary
The paper studies linear regression with jointly quantized weights and inputs trained via STE. In the high-dimensional limit (d → ∞), stochastic updates concentrate to deterministic ODEs governing macroscopic order parameters (alignment m and norm q). The analysis uses a teacher-student Gaussian model, deriving closed-form quantizer moments and stability conditions. For weight-only quantization, fixed-point analysis provides explicit stability bounds; joint quantization requires numerical solution. The framework extends to nonlinear transformations of weights and inputs.

## Key Results
- Quantized STE training exhibits a prolonged plateau in generalization error followed by an abrupt drop, with plateau length controlled by quantization range ω.
- Quantization can act as an implicit regularizer, expanding the stable learning region and enabling higher learning rates than unquantized models.
- Input quantization degrades performance more severely than weight quantization, causing non-monotonic transients that slow convergence.
- Higher bit widths reduce asymptotic error floor but yield diminishing returns beyond b≈4.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In the high-dimensional limit, stochastic STE updates for jointly quantized weights and inputs concentrate to deterministic ODEs.
- Mechanism: Microscopic parameter updates form a Markov process with drift and martingale components. As dimension d → ∞, martingale noise vanishes and empirical distribution converges to deterministic measure governed by PDE, reducing to coupled ODEs for macroscopic order parameters.
- Core assumption: Isotropy assumption for Gaussian orthogonal component; i.i.d. Gaussian inputs; bounded initial fourth moments.
- Evidence anchors: [abstract] "in the high-dimensional limit, STE dynamics converge to a deterministic ordinary differential equation"; [section V.B] Theorem V.3 with concentration bound; [corpus] Limited support in neighbor papers.
- Break condition: Fails if isotropy assumption is violated or dimension is small.

### Mechanism 2
- Claim: STE training exhibits a plateau followed by sharp drop in generalization error, with plateau length controlled by quantization range ω.
- Mechanism: ODE solutions show slow initial alignment between quantized weights and teacher; once alignment crosses threshold set by quantizer thresholds, quantized weight distribution reorganizes causing rapid generalization improvement.
- Core assumption: High-dimensional limit; fixed bit width and range; one-pass STE; Gaussian teacher-student model.
- Evidence anchors: [abstract] "reveals that STE training exhibits a plateau followed by a sharp drop"; [section VI.A.2] Figure 3 showing varying ω effects; [corpus] Not directly observed in neighbor papers.
- Break condition: Very high learning rates or extremely low bit widths with joint input quantization may obscure clean two-phase trajectory.

### Mechanism 3
- Claim: Quantization can expand stable learning region and act as implicit regularizer rather than pure noise.
- Mechanism: Fixed-point stability analysis yields maximum stable learning rate η < 2(σ²_ψ + λ)/σ⁴_ψ; stability boundary is non-monotonic in bit width, sometimes exceeding unquantized baseline.
- Core assumption: Small learning rate regime; local stability; symmetric quantizers.
- Evidence anchors: [section VI.C.1] Proposition VI.1 with stability condition; Figure 5 showing non-monotonic stability boundary; [abstract] "reveals that quantization can act as an implicit regularizer, expanding the stable learning region and enabling higher learning rates."
- Break condition: Joint quantization breaks closed-form stability bounds; poor range choices (too narrow) may negate regularization benefits.

## Foundational Learning

- **Concept**: Teacher-student linear regression model
  - Why needed here: Provides tractable setup for deriving order parameters and closed-form generalization error
  - Quick check question: Can you derive the generalization error ε_g = σ² + ρ + σ²_ψ q_ψ - 2κ_ψ m_ψ from the teacher-student setup?

- **Concept**: Straight-through estimator (STE)
  - Why needed here: Enables gradient-based training through discrete quantizers by replacing zero-almost-everywhere Jacobian with identity in backward pass
  - Quick check question: Explain why STE update approximates true gradient by bypassing derivative of quantizer.

- **Concept**: Statistical mechanics order parameters
  - Why needed here: Reduces high-dimensional dynamics to macroscopic variables (alignment m, norm q), analogous to order parameters in statistical physics learning analyses
  - Quick check question: What is the role of isotropy assumption in expressing m_ψ and q_ψ as functions of (m, s)?

## Architecture Onboarding

- **Component map**: Quantizer ψ -> Macroscopic state (m, q) -> ODE system -> Fixed-point solver -> Generalization error ε_g
- **Critical path**:
  1. Compute quantizer moments κ_ψ and σ²_ψ using Proposition II.1
  2. Initialize (m₀, q₀) from empirical distribution of w₀
  3. Integrate ODEs using numerical solver (e.g., RK4) for trajectories (m(τ), q(τ))
  4. Map (m, s) to (m_ψ, q_ψ) via isotropy assumption (Proposition V.2)
  5. Evaluate generalization error ε_g(τ) at each time step
- **Design tradeoffs**:
  - Bit width vs. generalization: Higher b reduces asymptotic error floor but diminishing returns beyond b≈4
  - Quantization range: Too small ω restricts expressivity; too large may introduce noise
  - Weight vs. input quantization: Joint quantization degrades more severely than weight-only
  - Learning rate: Within stability bound 0 < η < 2(σ²_ψ + λ)/σ⁴_ψ, higher η speeds convergence
- **Failure signatures**:
  - Non-convergence or divergence: Likely η exceeds stability bound; check σ²_ψ and reduce η
  - Prolonged plateau: ω too small; try increasing ω
  - Non-monotonic oscillations: Low-bit input quantization; consider higher b_x or unquantized inputs
  - Error floor too high: Insufficient bit width or range; increase b or tune ω
- **First 3 experiments**:
  1. Validate ODE predictions against finite-dimension STE simulations for weight-only quantization across b ∈ {2,3,4,5} with fixed ω=1.0, λ=1.0, η=0.04, d=900
  2. Sweep quantization range ω ∈ {0.25, 0.5, 1.0, 1.25, 1.5} at fixed b=3 to observe plateau length dependence on ω
  3. Introduce input quantization with b_x ∈ {3,4,5} and weight quantization b ∈ {3,4} to measure performance degradation and non-monotonic transients

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do architectural depth and redundancy interact with STE dynamics in multilayer neural networks?
- Basis in paper: [explicit] Conclusion states "We aim to scale the framework to multilayer architectures and structured data to show how architectural redundancy and depth interact with STE dynamics."
- Why unresolved: Current framework derived for single-layer linear regression model
- What evidence would resolve it: Theoretical extension of ODEs to deep networks or empirical validation showing plateau-drop behavior persists in deep architectures

### Open Question 2
- Question: Can the proposed high-dimensional dynamics framework accurately predict the performance of layer-wise post-training quantization (PTQ) methods?
- Basis in paper: [explicit] Appendix VI notes "Building on this analysis, evaluating the performance of layer-wise PTQ is an interesting direction for future work."
- Why unresolved: Links linear model to layer-wise PTQ subproblems but lacks experimental validation on actual large-scale PTQ tasks
- What evidence would resolve it: Experimental results demonstrating ODE-predicted generalization error curves match actual quantization error in layer-wise PTQ algorithms

### Open Question 3
- Question: How do learning dynamics evolve if quantization range ω is treated as a learnable parameter rather than fixed hyperparameter?
- Basis in paper: [inferred] Section VI.A.2 states timing of performance drop depends on range ω, "further motivating learning the range/scale parameter rather than fixing it"
- Why unresolved: Characterizes dynamics for fixed quantization ranges but does not derive dynamics for coupled weight-and-range learning
- What evidence would resolve it: Modified ODE system including evolution of ω over time or experimental comparisons showing adaptive range methods mitigate identified plateau length

## Limitations
- High-dimensional limit analysis assumes infinite width and isotropy assumption, which may not hold in practical finite-dimensional settings
- Theory derived for linear Gaussian model, limiting direct applicability to nonlinear neural networks
- Stability analysis for joint quantization is incomplete, as closed-form fixed points are unavailable
- Numerical integration of ODEs requires discretization choices not fully specified

## Confidence

- **High confidence**: Deterministic ODE limit in high-dimensional regime (Theorem V.3) and two-phase trajectory prediction for weight-only quantization
- **Medium confidence**: Fixed-point stability bounds for input-only quantization and implicit regularization interpretation
- **Low confidence**: Extension to joint quantization and nonlinear transformations due to incomplete analytical treatment

## Next Checks

1. Compare ODE predictions against STE simulations across multiple dimensions (d=100, 500, 1000) to verify convergence to high-dimensional limit
2. Test plateau-then-drop behavior in a nonlinear model (e.g., quantized linear layer in shallow network) to assess generalizability beyond linear teacher-student setup
3. Empirically measure stability boundary for joint quantization by varying learning rates and bit widths, comparing against theoretical bound for input-only quantization