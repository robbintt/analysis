---
ver: rpa2
title: 'DIWALI: Diversity and Inclusivity aWare cuLture specific Items for India:
  Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context'
arxiv_id: '2509.17399'
source_url: https://arxiv.org/abs/2509.17399
tags:
- cultural
- instruct
- adaptation
- llama
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DIWALI, a novel Culture Specific Items (CSIs)
  dataset for Indian culture covering 17 cultural facets across 36 sub-regions with
  8,817 concepts. It addresses the lack of comprehensive cultural datasets for evaluating
  cultural text adaptation in Indian contexts.
---

# DIWALI: Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context

## Quick Facts
- arXiv ID: 2509.17399
- Source URL: https://arxiv.org/abs/2509.17399
- Reference count: 40
- Primary result: DIWALI dataset enables comprehensive evaluation of cultural text adaptation, revealing sub-regional bias and surface-level adaptations in LLMs

## Executive Summary
This paper introduces DIWALI, a comprehensive Culture Specific Items (CSIs) dataset for Indian culture covering 17 cultural facets across 36 sub-regions with 8,817 concepts. The dataset addresses the lack of culturally-grounded evaluation resources for Indian contexts and demonstrates that existing datasets like CANDLE and DOSA fail to capture the full diversity of Indian cultural concepts. The authors evaluate seven open-weight LLMs on cultural text adaptation tasks using GSM8k and MGSM datasets, revealing significant performance gaps and systematic sub-regional bias in model outputs. The study provides both a valuable new resource and critical insights into the limitations of current LLMs for cultural adaptation.

## Method Summary
The authors constructed the DIWALI dataset using GPT-4o with web search capabilities, generating culture-specific items across 17 facets and 36 sub-regions, then validated these against official cultural sources. For evaluation, they adapted four datasets (GSM8k, MGSM, DailyDialog, ROCStories) to Indian cultural context using seven open-weight LLMs, measuring adaptation success through exact and fuzzy matching against the DIWALI CSI set. They employed three evaluation methods: CSI Adaptation Score (exact/fuzzy matching), LLM-as-Judge using Llama-3.1-8B and Mistral-7B as judges, and human evaluation by five annotators from different Indian regions. All experiments used greedy decoding (temperature=0) with maximum 2048 tokens on NVIDIA A100 40GB GPUs.

## Key Results
- DIWALI significantly outperforms existing datasets, with Llama-2-7b-chat-hf achieving 0.855 exact match vs 0.028 with CANDLE on GSM8k English prompt
- Heatmap analysis reveals systematic sub-regional bias, with certain models (e.g., Gemma-2-2B-Instruct) over-representing concepts from Uttar Pradesh, Madhya Pradesh, Maharashtra, and Punjab while completely omitting Northeastern states
- LLM-as-Judge systematically overestimates cultural relevance by 0.5-2.5 points compared to human evaluation, suggesting surface-level entity swaps are overvalued
- Manual analysis shows LLMs perform lexical substitution without establishing "aboutness" - replacing "Tuesday" with "Diwali" but retaining contextually inappropriate scenarios like "selling CDs"

## Why This Works (Mechanism)

### Mechanism 1: CSI-Driven Adaptation Scoring
Matching LLM outputs against a comprehensive, sub-regionally grounded CSI dataset improves detection of cultural adaptation capability compared to region-level datasets. The Adaptation Score = (1/N) × Σ I(w), where I(w) indicates a successful match between replaced concepts and the 8,817 CSI concepts. Evidence shows Llama-2-7b-chat-hf achieves 0.855 exact match with DIWALI vs. 0.028 with CANDLE on GSM8k English prompt, suggesting dataset coverage, not model ability, was the prior bottleneck.

### Mechanism 2: Sub-Regional Bias Emergence from Training Data Imbalance
When prompted for regional-level adaptation, LLMs exhibit systematic over-representation of certain sub-regions and near-complete absence of others. Heatmap analysis shows Gemma-2-2B-Instruct's food adaptations primarily originate from Uttar Pradesh, Madhya Pradesh, Maharashtra, and Punjab—with zero representations from Northeastern states. This reflects training data skew propagating into outputs, extending documented Western-centric pre-training biases to intra-country sub-regional imbalance.

### Mechanism 3: Surface-Level Adaptation Without Scenario Coherence
LLMs perform lexical entity substitution but fail to establish "aboutness"—the meaningful connection between cultural events and resonant scenarios. Manual analysis shows Llama-3.1-8B-Instruct replaces "Tuesday" with "Diwali" (event ✓) but retains "sells CDs" as the scenario (✗). For emotional resonance, Diwali should connect to contextually appropriate activities (e.g., gift exchanges), not disc sales. This operationalizes Hershcovich et al.'s definition of aboutness as contextual relevance.

## Foundational Learning

- **Culture Specific Items (CSIs):** The core unit of analysis; understanding that CSIs are culture-bound concepts (e.g., Mekhela Chador, Pongal) enables interpreting adaptation scores. Given the CSI set {paratha, lassi, kurta}, would replacing "hamburger" with "paratha" receive an exact match score of 1 or 0?

- **Exact vs. Fuzzy Matching:** The evaluation metric uses both; fuzzy matching (threshold τ=80) catches near-matches (e.g., "dosa" vs. "dosas") that exact matching misses. If an LLM outputs "Mekhela-Sador" and the CSI contains "Mekhela Chador," should fuzzy matching potentially score this as valid?

- **Inter-Annotator Agreement (Cohen's κ):** Human evaluation reliability depends on annotator consistency; Table 5 shows κ ranging from 0.213 (poor) to 0.589 (moderate), affecting confidence in human scores. Why might Mistral-7B-Instruct have lower IAA (0.230) than Gemma-2-2B-Instruct (0.589)?

## Architecture Onboarding

- **Component map:** DIWALI dataset (HuggingFace) → LLM inference with adaptation prompt → JSON output with replaced_concepts → CSI Adaptation Score computation (exact→fuzzy fallback) → optional LLM-as-Judge and human evaluation

- **Critical path:** Load DIWALI CSI set → Run LLM inference on source texts with adaptation prompt → Extract replaced_concepts from JSON output → Compute Adaptation Score via exact→fuzzy matching → Optional: Run LLM-as-Judge for fluency/integrity scoring; sample for human evaluation

- **Design tradeoffs:** GPT-4o + web search vs. Wikipedia-only (government/tourism sites improved authenticity but required manual validation overhead); LLM-as-Judge vs. human evaluation (LLM judges inflate scores by +0.5 to +2.5; human evaluation is costlier but captures subjective nuances); Regional vs. sub-regional prompting (regional prompts reveal sub-regional bias; sub-region-specific prompts may reduce bias but require more inference runs)

- **Failure signatures:** Near-zero Adaptation Score with CANDLE/DOSA but high score with DIWALI → prior dataset coverage gap, not model failure; High LLM-as-Judge score but low human score → surface-level entity swaps without deep cultural resonance; Heatmaps showing blank sub-regions (e.g., Northeast states) → training data imbalance, not evaluation artifact

- **First 3 experiments:** 1) Reproduce Table 1 baseline: Run Llama-3.1-8B-Instruct on 100 GSM8k samples, compute exact/fuzzy Adaptation Score against DIWALI vs. CANDLE; expect ~0.6 vs. ~0.04 gap; 2) Sub-region-specific prompting: Modify the adaptation prompt to specify target sub-region (e.g., "Adapt for Kerala culture") and compare heatmap coverage against regional-prompt baseline; 3) Scenario coherence evaluation: Sample 50 adapted texts, manually annotate whether replaced events connect to culturally resonant scenarios; quantify surface vs. deep adaptation rate per model

## Open Questions the Paper Calls Out

- **Open Question 1:** Would India-specific or regionally fine-tuned LLMs outperform general multilingual models on cultural text adaptation tasks? The study only evaluated 7 general open-weight models, excluding India-specific LLMs. Comparative benchmarking of India-specific LLMs on DIWALI using identical adaptation tasks and metrics would resolve this.

- **Open Question 2:** What training approaches could enable LLMs to achieve deep-level cultural adaptation that connects events to culturally resonant scenarios, rather than surface-level entity substitution? Analysis shows LLMs replace "Tuesday" with "Diwali" but fail to adapt associated scenarios, breaking "aboutness" and emotional resonance. Demonstrating models that coherently adapt both events and associated scenarios would resolve this.

- **Open Question 3:** Can LLM-as-Judge evaluation be calibrated to align with human judgments for cultural text adaptation, given observed systematic score inflation? The discrepancy is noted but root causes and calibration strategies remain unexplored. Correlating LLM judge outputs with larger human panels, or developing prompts that reduce over-valuation of surface-level changes, would resolve this.

- **Open Question 4:** How do human cultural relevance ratings vary across all 36 Indian sub-regions, and does this correlate with LLM sub-regional coverage gaps? Only 5 annotators from 5 regions were used, leaving 31 sub-regions unevaluated by humans. Scaling human evaluation to all 36 sub-regions and correlating regional human satisfaction with LLM adaptation scores per sub-region would resolve this.

## Limitations

- The study evaluates only general multilingual models rather than India-specific LLMs that might perform better on cultural adaptation tasks
- Human evaluation coverage is sparse relative to India's diversity, with only 5 annotators from 5 regions evaluating samples from all 36 sub-regions
- The CSI matching mechanism may miss deeper semantic transformations that don't involve lexical entity substitution
- Direct evidence of training corpus composition is unavailable, making the causal link between sub-regional bias and training data imbalance inferential

## Confidence

- **High:** DIWALI dataset construction methodology and validation against official cultural sources is well-documented and reproducible
- **Medium:** Comparative performance gains of DIWALI over CANDLE/DOSA are statistically robust across 7 models and 4 datasets
- **Medium:** Sub-regional bias patterns are clearly observable in heatmap analysis across multiple model families
- **Low:** The depth of cultural adaptation (surface vs. semantic) requires more extensive manual annotation to generalize beyond the presented examples

## Next Checks

1. **Training Data Analysis:** Request and analyze pre-training corpus composition from model providers to establish direct correlation between sub-regional representation in training data and adaptation output patterns

2. **Semantic Coherence Evaluation:** Develop and apply a structured annotation protocol to assess whether replaced events connect to culturally resonant scenarios across 500+ adapted samples, extending the 50-sample manual analysis

3. **Cross-Culture Generalization:** Apply the DIWALI evaluation framework to non-Indian cultural adaptation tasks (e.g., adapting European contexts to Indian or vice versa) to test whether the CSI matching mechanism generalizes beyond the single-culture focus