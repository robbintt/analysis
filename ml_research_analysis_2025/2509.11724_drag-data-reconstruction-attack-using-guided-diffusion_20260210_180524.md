---
ver: rpa2
title: 'DRAG: Data Reconstruction Attack using Guided Diffusion'
arxiv_id: '2509.11724'
source_url: https://arxiv.org/abs/2509.11724
tags:
- drag
- reconstruction
- data
- diffusion
- split
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DRAG, a data reconstruction attack method for
  vision transformers in split inference scenarios. The attack leverages guided diffusion
  with pre-trained latent diffusion models to reconstruct high-quality images from
  intermediate representations, even from deep layers where previous methods fail.
---

# DRAG: Data Reconstruction Attack using Guided Diffusion

## Quick Facts
- **arXiv ID:** 2509.11724
- **Source URL:** https://arxiv.org/abs/2509.11724
- **Reference count:** 40
- **Primary result:** DRAG uses guided diffusion with pre-trained latent diffusion models to reconstruct high-quality images from intermediate representations of vision transformers, outperforming state-of-the-art attacks.

## Executive Summary
This paper presents DRAG, a data reconstruction attack method that exploits intermediate representations from split inference scenarios in vision transformers. The attack leverages the rich prior knowledge embedded in pre-trained latent diffusion models (LDMs) to effectively generate high-fidelity images from intermediate representations, even from deep layers where previous methods fail. By using Tweedie's formula for clean image estimation and self-recurrence for escaping local minima, DRAG achieves superior reconstruction quality compared to existing approaches while remaining effective against several privacy defenses.

## Method Summary
DRAG operates by leveraging a pre-trained latent diffusion model as an image prior to constrain reconstructions to the natural image manifold. The method performs iterative sampling using DDIM, modifying noise predictions through gradients derived from the intermediate representation error. It estimates clean images via Tweedie's formula to compute accurate gradients from the client model, and employs self-recurrence (re-noising samples) to escape local minima in the non-convex optimization landscape of deep layer inversion. The attack uses Adam optimizer with momentum-based updates and gradient clipping, applying Diffusion with Spherical Gaussian (DSG) guidance to steer the generative process.

## Key Results
- DRAG achieves significantly higher MS-SSIM, lower LPIPS, and better DINO similarity metrics compared to state-of-the-art reconstruction attacks on frozen foundation models like CLIP-ViT and DINOv2
- The attack remains effective even against privacy defenses like DISCO and NoPeek, though token shuffling provides some mitigation
- DRAG successfully reconstructs images from deep layers (L9, L12) where previous methods fail, demonstrating superior performance in challenging split inference scenarios

## Why This Works (Mechanism)

### Mechanism 1: Diffusion Prior Constrains Reconstruction
Using a pre-trained Latent Diffusion Model (LDM) as an image prior constrains the reconstruction to the manifold of natural images, enabling recovery from representations where pixel-space optimization fails. Instead of optimizing a random seed directly, the method performs iterative sampling (DDIM) and modifies the noise prediction using gradients derived from the intermediate representation (IR) error, steering the generative process toward images that satisfy the IR constraint while maintaining high perceptual quality. The core assumption is that the target data distribution overlaps significantly with the LDM's training distribution (e.g., LAION for Stable Diffusion).

### Mechanism 2: Tweedie's Formula for Clean Image Estimation
Estimating the clean image $\hat{x}_0$ via Tweedie's formula allows the attacker to compute accurate gradients from the client model, which is typically trained only on noise-free inputs. The client model cannot process noisy latents effectively, so DRAG calculates a single-step denoised estimate $\hat{x}_0$ at each iteration and computes the distance metric on this proxy, providing stable guidance for the noise predictor update. The core assumption is that the noise predictor is sufficiently accurate to estimate $\hat{x}_0$ from $z_t$ at the specific noise levels used during sampling.

### Mechanism 3: Self-Recurrence for Escaping Local Minima
Self-recurrence (re-noising the sample) allows the optimizer to escape local minima in the non-convex loss landscape of deep layer inversion. After a guided denoising step produces $z_{t-1}$, the algorithm adds noise back to return to timestep $t$, iterating this denoising-diffusion cycle multiple times with momentum-based gradient updates. This allows the latent to "search" the space before finally committing to the next lower noise level. The core assumption is that the optimization landscape requires multiple correction steps per noise level to align the generated image with the target IR.

## Foundational Learning

- **Concept: Split Inference & Intermediate Representations (IR)**
  - Why needed: You must understand that $h^*$ is the "smashed" data sent to the cloud; it is the only ground truth the attacker has.
  - Quick check: Does the attacker have access to the raw image $x^*$? (No, only $h^*$).

- **Concept: Classifier Guidance / Universal Guidance**
  - Why needed: DRAG is an application of Universal Guidance, replacing a classifier with the IR distance metric. Understanding how gradients modify the noise prediction is essential.
  - Quick check: How does the gradient $\nabla L$ change the predicted noise $\epsilon_\theta$? (It is added/subtracted to steer the sample).

- **Concept: Vision Transformer (ViT) Tokenization**
  - Why needed: The paper highlights token shuffling as a defense and uses token-wise cosine distance. Understanding that images are sequences of patches is critical for debugging the distance metric.
  - Quick check: If patch tokens are shuffled, does the semantic content change? (No, but the order-dependent distance metric breaks).

## Architecture Onboarding

- **Component map:** Frozen Client Model ($f_c$) -> Target IR ($h^*$) -> LDM Components (Encoder $E$, Decoder $D$, Noise Predictor $\epsilon_\theta$) -> Optimization Loop (Adam + DSG Sampler)

- **Critical path:** Initialize $z_T \sim \mathcal{N}(0,I)$ → Loop $t$ from $T$ to $1$ → **Estimate** $\hat{x}_0$ via Tweedie → **Compute** distance $d_H(f_c(\hat{x}_0), h^*)$ → **Update** Gradient via Adam → **Apply** Guidance to Noise Predictor → **Self-Recurse** (Re-noise & Repeat $k$ times) → **Step** to $z_{t-1}$

- **Design tradeoffs:**
  - **Guidance Strength ($w$):** High $w$ forces feature matching (lower LPIPS) but may violate the image prior (lower MS-SSIM)
  - **Recurrence ($k$):** Higher $k$ improves deep-layer reconstruction but linearly increases attack time
  - **Initialization:** Using an Inverse Network (DRAG++) improves speed/quality but requires training a separate decoder on public data

- **Failure signatures:**
  - **Gray/Blurry outputs:** Guidance strength $w$ is too low; the model ignores the target IR and generates random images
  - **Artifact-heavy/Oversaturated outputs:** Guidance strength $w$ is too high; the optimizer "overfits" to the IR noise
  - **Total Failure on Deep Layers (L12):** Without self-recurrence ($k>1$) or momentum, the gradient vanishes in the non-convex landscape

- **First 3 experiments:**
  1. **Overfit Test:** Run DRAG on a single image from Layer 0 (shallow). Verify that MS-SSIM > 0.95. If not, check the distance metric implementation.
  2. **Ablation on Recurrence:** Run on Layer 12 with $k=1$ vs $k=16$. Confirm that the deeper layer fails without recurrence.
  3. **Defense Check:** Apply random token shuffling to $h^*$. Verify that the standard attack fails (MS-SSIM drops), confirming sensitivity to token order.

## Open Questions the Paper Calls Out

### Open Question 1
How effective is DRAG against non-ViT architectures (e.g., MLP-Mixers, modern CNNs, hybrid architectures) in split inference settings? The authors state "Existing evaluations primarily focused on CNN architectures like ResNet18... ViTs process images fundamentally differently through patch tokenization and attention mechanisms, and their vulnerability to reconstruction attacks remains largely unexplored." The study evaluates only CLIP-ViT, DINOv2, and CLIP-RN50, leaving other architectures untested.

### Open Question 2
Can clients effectively embed controlled randomness into the client model to defeat DRAG without significantly degrading model utility? Section 3.2 states that "clients can embed randomness into fc, further complicating the problem, as detailed in Section 5.5," but Section 5.5 only addresses token shuffling, not randomness injection. The randomness mechanism is mentioned as a complicating factor but never implemented or evaluated.

### Open Question 3
What defenses can provably guarantee privacy against guided diffusion attacks while maintaining acceptable task performance? The paper concludes "These findings highlight the need for stronger defenses to protect privacy when deploying transformer-based models in SI settings" after showing DISCO and NoPeek are insufficient. Token shuffling shows partial mitigation but is circumvented by position classifiers; no defense achieves both strong privacy and full utility.

## Limitations

- The attack's performance on other vision transformer architectures and larger foundation models remains untested, limiting generalizability claims
- The computational cost is substantial, requiring 250 sampling steps with 16 self-recurrence iterations per step, making it impractical for real-time scenarios
- The reliance on Stable Diffusion v1.5 as an image prior constrains the attack to natural images similar to LAION training data, potentially failing on specialized domains

## Confidence

- **High confidence:** The core mechanism of using diffusion models as image priors for reconstruction attacks is well-supported by empirical results and aligns with established principles in score-based generative modeling
- **Medium confidence:** The effectiveness against privacy defenses (DISCO, NoPeek) is demonstrated, but the analysis lacks ablation studies showing which specific defense components are most vulnerable
- **Low confidence:** The claim that DRAG remains effective "even from deep layers" needs more validation - results show significant performance degradation at layers 9-12 compared to shallow layers, suggesting limitations not fully acknowledged

## Next Checks

1. **Cross-architecture validation:** Test DRAG against other popular ViT variants (e.g., Swin Transformer, ConvNeXt) and larger foundation models (e.g., OpenCLIP, OpenCLIP-ViT-H) to assess generalizability beyond the evaluated models.

2. **Domain shift analysis:** Evaluate DRAG's performance on non-natural image datasets (medical scans, satellite imagery) to quantify the impact of distribution mismatch between target data and Stable Diffusion's training corpus.

3. **Defense ablation study:** Systematically disable individual components of DISCO and NoPeek defenses to identify which mechanisms (feature masking, gradient obfuscation, etc.) provide the strongest protection against diffusion-guided reconstruction attacks.