---
ver: rpa2
title: 'FINN-GL: Generalized Mixed-Precision Extensions for FPGA-Accelerated LSTMs'
arxiv_id: '2506.20810'
source_url: https://arxiv.org/abs/2506.20810
tags:
- lstm
- finn
- quantised
- hardware
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper extends the FINN framework to enable efficient deployment\
  \ of quantised LSTM models on FPGAs. The key innovation is the integration of ONNX\u2019\
  s Scan operator to model recurrent computations, allowing mixed-precision quantisation\
  \ within LSTM layers."
---

# FINN-GL: Generalized Mixed-Precision Extensions for FPGA-Accelerated LSTMs

## Quick Facts
- arXiv ID: 2506.20810
- Source URL: https://arxiv.org/abs/2506.20810
- Reference count: 27
- Primary result: 77.4% F1-score on FI-2010 with 4.3 ms latency on ZCU104 FPGA

## Executive Summary
This paper extends the FINN framework to enable efficient deployment of quantized LSTM models on FPGAs. The key innovation is integrating ONNX's Scan operator to model recurrent computations, allowing mixed-precision quantization within LSTM layers. Custom transformations in the FINN compiler map quantized LSTM graphs to hardware blocks, supporting integer-only operations for resource efficiency. The approach is validated using a quantized ConvLSTM model for mid-price stock prediction on the FI-2010 dataset, achieving 77.4% F1-score on the ZCU104 FPGA with 4.3 ms latency.

## Method Summary
The authors extend FINN by integrating ONNX's Scan operator to model recurrent LSTM computations within a static graph structure suitable for hardware compilation. They use Brevitas for quantization-aware training, exporting Q-LSTM models to ONNX with Scan nodes. The FINN compiler then applies custom transformations to convert quantized graphs to FINN-ONNX format, streamlining floating-point operations into integer-only threshold operations. The generated HLS kernels use FINN's library components (MultiThreshold, MatrixVectorMultiplier) to synthesize the LSTM logic. The framework supports fine-grained mixed-precision control over 11 internal quantizers per LSTM layer.

## Key Results
- 77.4% F1-score on FI-2010 dataset using quantized ConvLSTM
- 4.3 ms inference latency on ZCU104 FPGA at 150 MHz
- Resource utilization: 49.6% LUTs, 12.8% FFs, 15.2% DSPs
- Outperforms prior approaches in financial prediction tasks

## Why This Works (Mechanism)

### Mechanism 1
The ONNX Scan operator enables modeling of recurrent state updates within a static graph structure suitable for hardware compilation. Instead of unrolling recurrent connections into a deep feed-forward graph or using a black-box LSTM node, the authors encapsulate LSTM cell logic inside the Scan operator's body attribute. This operator iterates over input sequence, maintaining and updating state variables (h_t-1, c_t-1) explicitly at each step, allowing the compiler to see inside the recurrence while preserving sequential processing loop.

### Mechanism 2
Replacing analytical activation functions with comparison-based thresholding allows non-linear LSTM gates to be implemented efficiently in fixed-point hardware. Standard LSTM relies on sigmoid and tanh functions requiring expensive exponentiation. The authors map these to a "Multithreshold" operator that compares input against pre-computed thresholds and returns an integer count. By adjusting threshold spacing, this piecewise-constant step function approximates smooth activation curve, converting complex math into simple comparisons and additions.

### Mechanism 3
"Streamlining" transformations eliminate floating-point overhead by merging scaling operations into threshold nodes. Quantized graphs often retain floating-point scaling factors from QCDQ nodes. Compiler transformations (e.g., AbsorbMulIntoMultiThreshold) push these operations through graph until encountering threshold layer, then adjust thresholds mathematically to absorb scale/bias, resulting in pure integer-to-integer threshold operator.

## Foundational Learning

- **LSTM State Dynamics (Gates & Cell State)**: Understanding what's being put inside the Scan operator requires distinguishing between "hidden state" (short-term output) and "cell state" (long-term memory conveyor belt), and how gates modulate these. Quick check: Can you explain why element-wise multiplication (f_t âŠ™ C_t-1) must happen inside recurrent loop, preventing simple layer fusion?

- **Quantization Aware Training (QAT) vs. Post-Training Quantization (PTQ)**: Paper uses Brevitas for training (QAT). Hardware relies on model learning to cope with step function approximations of activations. Quick check: If you took standard floating-point PyTorch LSTM and directly converted to FINN threshold representation without retraining, what would likely happen to accuracy?

- **FPGA Dataflow vs. Sequential Bottlenecks**: FPGAs excel at parallel dataflow, but LSTMs are inherently sequential. Architecture onboarding requires understanding where parallelism stops (recurrence loop). Quick check: In generated hardware, why does increasing sequence length linearly increase latency, whereas increasing hidden size might only increase resource usage?

## Architecture Onboarding

- **Component map**: Brevitas/ONNX -> FINN Compiler (Transformations) -> HLS Kernel Library (finn-hlslib) -> Backend (Vitis/Vivado)

- **Critical path**: Conversion of Scan body from QCDQ (floating-point scale/bias) to Streamlined Integer Graph. If AbsorbMulIntoMultiThreshold transformation fails to trigger, HLS synthesis will infer expensive floating-point DSP cores, breaking resource efficiency guarantees.

- **Design tradeoffs**:
  - Latency vs. Resources: Current implementation processes Scan loop sequentially. To lower latency, one could unroll loop (replicate hardware for each time step), but this linearly explodes LUT/BRAM usage
  - Precision vs. Accuracy: Using INT2 for activations saves resources but creates coarse step-function, potentially degrading accuracy. Authors chose W8A6 as balance

- **Failure signatures**:
  - Functional Mismatch: Hardware produces different results than PyTorch model. Likely cause: Missed streamlining step leaving residual floating-point operation behaving differently in HLS fixed-point synthesis
  - High Latency: Initiation Interval (II) > 1 in HLS report. Likely cause: Feedback loop in Scan operator carries dependency preventing pipelining, or memory bandwidth limits for large weight matrices
  - Synthesis Failure: BRAM exhaustion. Likely cause: Weights not small enough to fit on-chip (paper claims they fit, but larger models might spill to DDR)

- **First 3 experiments**:
  1. Verify Graph Transformation: Export minimal 1-layer LSTM from Brevitas. Run only FINN compiler streamlining passes. Inspect graph to confirm all Mul/Add nodes have disappeared and only MultiThreshold/MatMul remain
  2. HLS C-Simulation: Run software simulation of generated HLS code for LSTM layer. Compare outputs against PyTorch golden model to verify integer-threshold approximation logic is mathematically correct within scan loop
  3. Resource Estimation: Synthesize LSTM IP block in isolation (without rest of ConvLSTM network) targeting ZCU104. Check if 64 hidden units fit comfortably within BRAM/LUT limits to verify "on-chip memory" claim

## Open Questions the Paper Calls Out

- **Extending to other RNN variants**: Can the deployment pipeline be extended to support other RNN models such as gated recurrent units (GRUs)?
- **Reducing latency**: Can optimizations be explored to reduce latency by taking inspiration from the hls4ml approach (using non-static implementations for LSTM layers)?
- **Exploring mixed-precision trade-offs**: Can the fine-grained mixed-precision control be leveraged to explore accuracy versus resource trade-offs beyond the uniform quantization used in the case study?

## Limitations

- The numerical stability of the streamlining transformations under different quantization schemes remains unverified
- Resource utilization claims are based on a specific ConvLSTM architecture that may not generalize to other RNN variants or larger models
- The approach requires significant compiler infrastructure changes to FINN and may not be easily portable to other FPGA frameworks

## Confidence

- **High confidence**: Basic premise that mixed-precision LSTM quantization is feasible on FPGAs, supported by working ConvLSTM demo and reasonable accuracy preservation (77.4% F1-score)
- **Medium confidence**: Architectural approach using ONNX Scan operators, as mechanism is logically sound but lacks external validation in corpus
- **Medium confidence**: Resource efficiency claims, though numbers are plausible for described architecture, they're specific to this particular model and may not generalize

## Next Checks

1. **Numerical verification**: Implement same ConvLSTM model using alternative quantization frameworks (TensorFlow Lite for Microcontrollers or custom PyTorch quantization) and compare both accuracy and hardware resource utilization to verify FINN-GL approach's advantages

2. **Generalization testing**: Apply FINN-GL compilation flow to different RNN architecture (e.g., GRU or vanilla RNN) with same quantization scheme to verify framework's generality beyond ConvLSTMs

3. **Scaling analysis**: Synthesize and benchmark LSTM variants with varying hidden sizes (32, 128, 256 units) to empirically validate linear resource scaling claims and identify practical upper bounds for edge deployment