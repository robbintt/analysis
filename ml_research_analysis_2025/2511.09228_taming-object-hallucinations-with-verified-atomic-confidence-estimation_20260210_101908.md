---
ver: rpa2
title: Taming Object Hallucinations with Verified Atomic Confidence Estimation
arxiv_id: '2511.09228'
source_url: https://arxiv.org/abs/2511.09228
tags:
- atomic
- question
- confidence
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses hallucinations in multimodal large language
  models (MLLMs), where models produce errors about object existence, attributes,
  or relations. The proposed TACO framework mitigates these hallucinations through
  self-verification and confidence calibration without relying on external vision
  experts.
---

# Taming Object Hallucinations with Verified Atomic Confidence Estimation

## Quick Facts
- **arXiv ID:** 2511.09228
- **Source URL:** https://arxiv.org/abs/2511.09228
- **Reference count:** 40
- **Primary result:** TACO reduces hallucinations in MLLMs through atomic query decomposition and confidence calibration

## Executive Summary
This paper addresses hallucinations in multimodal large language models (MLLMs), where models produce errors about object existence, attributes, or relations. The proposed TACO framework mitigates these hallucinations through self-verification and confidence calibration without relying on external vision experts. TACO works by decomposing responses into atomic queries, paraphrasing them to reduce sensitivity to wording, estimating confidence using self-consistency or self-confidence aggregation, and refining answers with a language model. Experiments on five benchmarks (POPE, MME, HallusionBench, AMBER, and MM-Hal Bench) with two MLLMs (LLaVA-1.5-7B and CogVLM2) show TACO consistently outperforms direct prompting and Visual Contrastive Decoding, reduces systematic biases like "yes"-answer bias, and improves confidence calibration. The method demonstrates effectiveness in enhancing the faithfulness of MLLMs across both discriminative and generative tasks.

## Method Summary
TACO mitigates hallucinations by decomposing complex multimodal responses into atomic binary queries about specific object attributes or relations, then verifying each claim through paraphrase-based confidence estimation. For each atomic query, the framework generates multiple semantically equivalent paraphrases and aggregates MLLM responses using either self-consistency (majority voting) or self-confidence (probability-weighted) methods. High-confidence atomic answers are then used to refine the original response. The approach requires an LLM helper for tuple extraction, question generation, and paraphrasing, while working with MLLMs that can provide either discrete outputs or access to logits for confidence estimation.

## Key Results
- TACO consistently outperforms direct prompting and Visual Contrastive Decoding across five benchmarks
- Self-confidence estimation (gray-box) outperforms self-consistency (black-box) for confidence calibration
- TACO reduces systematic "yes"-answer bias in MLLMs
- Improvements are significant for weaker MLLMs like LLaVA-1.5-7B but minimal for stronger models like CogVLM2

## Why This Works (Mechanism)

### Mechanism 1: Atomic Query Decomposition
Decomposing complex multimodal responses into atomic binary queries improves hallucination detection accuracy by isolating individual claims so verification errors don't compound. Each claim about object existence, attributes, or relations is converted into a standalone positively-framed binary question, enabling independent verification. The framework assumes hallucinations often occur at the level of specific object-attribute-relation tuples rather than entire response structures. Evidence shows that atomic claims can be meaningfully verified independently, though cross-dependent queries may yield inconsistent results.

### Mechanism 2: Paraphrase-Based Sensitivity Sampling
MLLMs exhibit higher variance in responses to paraphrased queries when they are uncertain, making variance a useful proxy for confidence. For each atomic question, an LLM generates n semantically equivalent paraphrases, and the MLLM answers each one. Greater disagreement among answers signals lower reliability, with statistically significant differences in variance between correctly vs. incorrectly answered questions (p < 3.1e-25 across POPE settings). The framework assumes paraphrase sensitivity correlates with epistemic uncertainty, though variance may also reflect linguistic ambiguity rather than model uncertainty.

### Mechanism 3: Probability-Weighted Confidence Aggregation
Gray-box aggregation using output logits (self-confidence) outperforms black-box majority voting (self-consistency) for calibration. The probability-weighted approach incorporates additional signal about the model's internal uncertainty by weighting each agreement by the model's assigned probability. This assumes model logits contain meaningful calibration information not fully captured by discrete output agreement. Evidence shows self-confidence estimation consistently surpasses self-consistency across all subtasks and models, though this advantage may not generalize to all MLLM architectures if logits are systematically miscalibrated.

## Foundational Learning

- **Concept: Confidence Calibration**
  - **Why needed here:** TACO's core premise is that confidence scores can identify unreliable outputs. Understanding why neural networks are poorly calibrated is prerequisite for appreciating why this approach works.
  - **Quick check question:** If a model assigns 90% probability to predictions that are correct only 60% of the time, is it overconfident, underconfident, or well-calibrated?

- **Concept: Self-Consistency for Uncertainty Estimation**
  - **Why needed here:** TACO-S uses majority-vote agreement across paraphrases as a confidence proxy. This builds on prior work showing that sampling variance correlates with reliability.
  - **Quick check question:** If a model answers "yes" to 8 of 10 paraphrased questions, what is its self-consistency score?

- **Concept: Object Hallucination Taxonomy in MLLMs**
  - **Why needed here:** The paper's atomic query generator is structured around specific hallucination types (existence, attributes, relations). Understanding this taxonomy is necessary to design effective verification questions.
  - **Quick check question:** "The red car is parked behind the blue truck" — which part refers to existence, which to attribute, and which to relation?

## Architecture Onboarding

- **Component map:** MLLM -> LLM Helper -> Confidence Aggregator -> Refined Response
- **Critical path:** Initial MLLM response → Atomic tuple extraction (LLM) → Binary question generation (LLM) → Paraphrase generation (LLM, n=9) → MLLM answers each paraphrase → Confidence aggregation → LLM refines original response using high-confidence atomic answers
- **Design tradeoffs:**
  - TACO-S vs. TACO-F: TACO-F requires logit access but outperforms TACO-S; choose based on API constraints
  - Number of paraphrases (n): More paraphrases improve variance estimation but increase inference cost; paper uses n=9
  - LLM helper quality: Claude-3-Sonnet vs. 3.7-Sonnet show comparable results; cheaper/smaller models may suffice but weren't tested
  - Aggregation function: MEAN outperforms MAX for self-confidence (Section 6, Figure 3)
- **Failure signatures:**
  - Negative questions: MLLMs struggle with negatively-phrased queries; taxonomy requires positive framing
  - High-capability baselines: TACO improves LLaVA-1.5-7B but can degrade CogVLM2 on MM-Hal if base model is already strong
  - Out-of-taxonomy content: Atomic queries labeled "other-other" may lack verification precision
- **First 3 experiments:**
  1. **Ablate paraphrase count (n):** Run TACO on POPE with n ∈ {1, 3, 5, 9, 15} to find the accuracy/cost elbow point for your inference budget.
  2. **Compare aggregation functions:** Implement both MEAN and MAX for TACO-F on your target benchmark; confirm MEAN advantage holds for your model.
  3. **Test negation handling:** Construct a small probe set with negated atomic queries ("Is there no dog?") vs. positive equivalents; quantify the performance gap to assess whether your MLLM needs additional preprocessing.

## Open Questions the Paper Calls Out
None

## Limitations
- TACO's atomic query decomposition assumes hallucinations occur at individual object-attribute-relation tuples, but complex multi-object interactions may require holistic verification
- The framework depends on LLM helpers for critical steps; poor-quality helpers could introduce systematic errors
- Performance degrades when applied to already-capable MLLMs like CogVLM2, suggesting TACO may be unnecessary for state-of-the-art models
- Negative questions remain challenging for MLLMs, requiring careful positive framing that may not always be semantically natural

## Confidence
- **High confidence:** TACO consistently improves accuracy and calibration on weaker MLLMs (LLaVA-1.5-7B) across multiple benchmarks
- **Medium confidence:** Gray-box confidence aggregation (TACO-F) outperforms black-box (TACO-S) on aggregate metrics, but this advantage varies by task and may not generalize to all MLLM architectures
- **Medium confidence:** Paraphrase variance reliably indicates model uncertainty, though the causal relationship between linguistic sensitivity and epistemic uncertainty remains correlational

## Next Checks
1. **Ablate paraphrase count (n):** Run TACO on POPE with n ∈ {1, 3, 5, 9, 15} to find the accuracy/cost elbow point for your inference budget.
2. **Compare aggregation functions:** Implement both MEAN and MAX for TACO-F on your target benchmark; confirm MEAN advantage holds for your model.
3. **Test negation handling:** Construct a small probe set with negated atomic queries ("Is there no dog?") vs. positive equivalents; quantify the performance gap to assess whether your MLLM needs additional preprocessing.