---
ver: rpa2
title: 'MARS: Modality-Aligned Retrieval for Sequence Augmented CTR Prediction'
arxiv_id: '2509.01184'
source_url: https://arxiv.org/abs/2509.01184
tags:
- user
- mars
- data
- prediction
- behavior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of click-through rate (CTR)
  prediction for low-active users, where data sparsity limits model performance. The
  proposed MARS framework leverages multimodal item features (text and images) through
  a Stein kernel-based alignment method to construct unified user embeddings.
---

# MARS: Modality-Aligned Retrieval for Sequence Augmented CTR Prediction

## Quick Facts
- arXiv ID: 2509.01184
- Source URL: https://arxiv.org/abs/2509.01184
- Reference count: 40
- Primary result: 12.34% relative improvement in AUC for CTR prediction on low-active users

## Executive Summary
This paper addresses the challenge of click-through rate (CTR) prediction for low-active users, where data sparsity limits model performance. The proposed MARS framework leverages multimodal item features (text and images) through a Stein kernel-based alignment method to construct unified user embeddings. For low-active users, MARS retrieves semantically similar high-active users' behavior sequences via multimodal embeddings and filters them based on item-user similarity to augment the sparse sequences. Extensive offline experiments across three real-world datasets (ML-1M, Amazon-Beauty, Amazon-Toys) show MARS consistently outperforms state-of-the-art baselines, achieving up to 12.34% relative improvement in AUC. Online A/B testing on Kuaishou demonstrates significant business metric gains, including a 0.728% increase in average app usage time per low-active user. MARS has been successfully deployed serving hundreds of millions of users.

## Method Summary
MARS tackles CTR prediction for low-active users by constructing modality-aligned embeddings from multimodal item features and using sequence retrieval to augment sparse behavior histories. The framework employs Stein kernels to align text and image embeddings into unified representations, then retrieves semantically similar high-active users' sequences for augmentation. The retrieved sequences are filtered based on item-user similarity before being incorporated into the CTR prediction model. This approach effectively addresses the cold-start problem for low-active users by leveraging rich multimodal item information and transfer learning from similar users' behavior patterns.

## Key Results
- Achieved up to 12.34% relative improvement in AUC over state-of-the-art baselines across three real-world datasets
- Online A/B testing on Kuaishou showed 0.728% increase in average app usage time per low-active user
- Successfully deployed serving hundreds of millions of users in production

## Why This Works (Mechanism)
The effectiveness of MARS stems from its ability to leverage multimodal item information to create more robust user representations, particularly for users with limited interaction history. By aligning text and image embeddings through Stein kernels, MARS captures richer semantic relationships between items that single-modality approaches miss. The sequence retrieval mechanism allows low-active users to benefit from the behavior patterns of similar high-active users, effectively transferring knowledge across the user base. The filtering step ensures that only relevant sequences are used for augmentation, preventing noise from degrading prediction quality.

## Foundational Learning
- **Stein Kernel Alignment**: A method for aligning multimodal embeddings by measuring similarity in reproducing kernel Hilbert spaces. Needed because text and image embeddings exist in different spaces and require alignment for unified representation. Quick check: Verify alignment quality through downstream task performance.
- **Sequence Retrieval for CTR**: Using similar users' behavior sequences to augment sparse histories. Needed because low-active users lack sufficient interaction data for accurate prediction. Quick check: Measure retrieval relevance through similarity metrics.
- **Multimodal Feature Fusion**: Combining text and image information for richer item representations. Needed because single modalities miss complementary information. Quick check: Compare performance with single-modality baselines.

## Architecture Onboarding

**Component Map**: Item Features -> Stein Kernel Alignment -> Unified Embeddings -> User Sequence Retrieval -> Filtering -> Augmented Sequences -> CTR Prediction

**Critical Path**: The most critical components are the Stein kernel alignment and sequence retrieval with filtering, as these directly impact the quality of user representations and the relevance of augmented sequences.

**Design Tradeoffs**: The framework trades computational complexity for improved prediction accuracy by incorporating multimodal alignment and retrieval processes. While this increases inference time, the gains in performance for low-active users justify the overhead in most practical scenarios.

**Failure Signatures**: Poor performance may manifest as: (1) degraded accuracy when multimodal features are noisy or misaligned, (2) ineffective retrieval when user similarity metrics are poorly calibrated, (3) overfitting when augmented sequences overwhelm genuine user behavior patterns.

**First Experiments**: 1) Validate multimodal alignment quality through t-SNE visualization and similarity preservation tests, 2) Benchmark retrieval effectiveness using precision@k and recall@k metrics, 3) Perform ablation studies removing Stein kernel alignment versus retrieval components.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited statistical significance testing for observed improvements across baseline comparisons
- Online A/B testing results based on single deployment without long-term stability analysis or confidence intervals
- Computational overhead of Stein kernel alignment and retrieval processes not thoroughly evaluated
- Reliance on high-quality multimodal item representations raises questions about robustness to feature noise

## Confidence
- **High confidence**: The core methodology of using multimodal retrieval for sequence augmentation is technically sound and well-implemented
- **Medium confidence**: The offline experimental results are robust but limited in scope
- **Medium confidence**: The online A/B testing demonstrates practical viability but lacks comprehensive statistical validation

## Next Checks
1. Conduct extensive statistical significance testing across all baseline comparisons, including confidence intervals for online metrics
2. Perform ablation studies specifically isolating the impact of Stein kernel alignment versus simpler similarity measures
3. Evaluate model performance and computational efficiency under various levels of multimodal feature noise and missing modalities