---
ver: rpa2
title: Detecting Linguistic Indicators for Stereotype Assessment with Large Language
  Models
arxiv_id: '2502.19160'
source_url: https://arxiv.org/abs/2502.19160
tags:
- linguistic
- category
- label
- indicators
- stereotype
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel framework for detecting and quantifying
  linguistic indicators of stereotypes in text using the Social Category and Stereotype
  Communication (SCSC) framework. The authors develop a categorization scheme with
  fixed linguistic indicators and values, and leverage Large Language Models (LLMs)
  with in-context learning to automatically detect these indicators.
---

# Detecting Linguistic Indicators for Stereotype Assessment with Large Language Models

## Quick Facts
- arXiv ID: 2502.19160
- Source URL: https://arxiv.org/abs/2502.19160
- Reference count: 40
- This paper proposes a novel framework for detecting and quantifying linguistic indicators of stereotypes in text using the Social Category and Stereotype Communication (SCSC) framework.

## Executive Summary
This paper introduces a framework for detecting and quantifying linguistic indicators of stereotypes in text using the Social Category and Stereotype Communication (SCSC) framework. The authors develop a categorization scheme with fixed linguistic indicators and values, and leverage Large Language Models (LLMs) with in-context learning to automatically detect these indicators. They learn a scoring function based on human stereotype rankings to quantify the strength of a stereotype. Their evaluation on CrowS-Pairs shows that larger models like Llama-3.3-70B and GPT-4 perform best at detecting and classifying linguistic indicators, with an average accuracy of 82%. The proposed scoring function achieves a mean absolute error of 0.07 compared to human-based stereotype rankings.

## Method Summary
The authors define a categorization scheme with 11 linguistic indicators derived from the SCSC framework, covering both category labels and associated content. They use in-context learning with LLMs to detect these indicators in text, outputting structured JSON. A linear scoring function is learned from human stereotype rankings to quantify stereotype strength based on the detected indicators. The method is evaluated on CrowS-Pairs using accuracy, F1-score, and mean absolute error compared to human rankings.

## Key Results
- Larger LLMs like Llama-3.3-70B-Instruct and GPT-4 achieve 81-83% average accuracy on detecting linguistic indicators
- The proposed scoring function achieves MAE=0.07 compared to human stereotype rankings
- Models perform significantly better on category label indicators than associated content indicators

## Why This Works (Mechanism)

### Mechanism 1
Grounding stereotype detection in sociolinguistic theory yields interpretable, fine-grained indicators rather than binary labels. The SCSC framework decomposes stereotypes into observable linguistic components—category labels (grammatical form, generalization, connotation) and associated content (information level, abstraction, explanations, signal words). Each indicator has a documented strengthening or weakening effect on perceived category entitativity, essentialism, or stereotype content. By constraining detection to these theory-derived indicators, the system avoids subjective binary judgments and produces annotations that explain why a sentence scores as it does.

### Mechanism 2
Larger instruction-tuned LLMs (~70B+ parameters) can reliably classify linguistic indicators via in-context learning without task-specific fine-tuning. The categorization scheme is encoded as a structured prompt with role description, task definition, decision flow, and few-shot examples. The LLM performs multiple NLP subtasks (entity extraction, sentiment analysis, relation extraction, classification) in a single pass, outputting structured JSON. Performance scales with model size because larger models better follow multi-step instructions and generalize from fewer examples.

### Mechanism 3
A linear scoring function aggregating weighted linguistic indicators approximates human stereotype strength rankings (MAE=0.07). Indicator weights are learned by training a linear regression on linguistic indicator features to predict human-derived stereotype scores (score_bws from Liu et al.'s Best-Worst Scaling rankings). The resulting coefficients align with SCSC theoretical predictions (e.g., generic labels strengthen, specific individuals weaken). The final score_scs = 0 if no category label exists, otherwise the weighted sum.

## Foundational Learning

- **Social Category and Stereotype Communication (SCSC) Framework**
  - Why needed here: Provides the theoretical decomposition of stereotypes into observable linguistic components. Without understanding category entitativity, essentialism, and stereotype content as cognitive constructs, the indicator scheme appears arbitrary.
  - Quick check question: Given "Women can't drive in the rain," identify which linguistic forms strengthen category entitativity vs. which strengthen essentialism.

- **In-Context Learning with Few-Shot Prompting**
  - Why needed here: The entire detection pipeline relies on LLMs performing structured classification without gradient updates. Understanding how example selection and prompt structure affect performance is critical for replication.
  - Quick check question: Why does performance improve from 1 to 6 few-shot examples but plateau or decrease with additional corner-case examples?

- **Best-Worst Scaling for Preference Learning**
  - Why needed here: The scoring function weights are learned against human rankings derived from this methodology. Understanding that annotators compared quadruples of sentences (not individual ratings) clarifies what score_bws represents.
  - Quick check question: Why might Best-Worst Scaling produce different rankings than direct Likert-scale ratings of stereotype strength?

## Architecture Onboarding

- **Component map:** Raw sentence + attributes → Prompt Constructor → LLM Classifier → JSON Parser → Scoring Function → score_scs

- **Critical path:** Prompt quality → LLM indicator accuracy → Score reliability. Errors propagate: misclassified "generalization" or "connotation" directly affect score. Category label detection failures (false negatives) zero out the score entirely.

- **Design tradeoffs:**
  - Single-stage vs. multi-stage prompts: Single-stage chosen because smaller models (8B) degrade significantly in multi-stage, though larger models perform similarly.
  - JSON vs. free-text output: JSON ensures structured parsing but some models (Mixtral, GPT-4o-mini) add formatting artifacts requiring post-processing.
  - Indicator scope: Irony and negation bias excluded for objectivity; tradeoff misses some stereotype-inconsistent patterns.

- **Failure signatures:**
  - Models rate label connotation using full-sentence sentiment
  - Struggle with "subset" generalization level
  - Content generalization difficulty
  - Lower scores for individuals

- **First 3 experiments:**
  1. Prompt ablation: Reduce few-shot examples from 9 to 6 and measure accuracy change on held-out sentences
  2. Multi-stage for large models only: Test whether separating category label detection from content analysis improves accuracy for >70B models
  3. Content sentiment indicator: Extend scoring function with a sentiment feature on associated content and re-evaluate MAE against human rankings

## Open Questions the Paper Calls Out

### Open Question 1
Can multi-stage prompting strategies improve LLM accuracy in detecting nuanced indicators like connotation and generalization? The authors note challenges in assessing nuanced aspects and state they will focus future work on employing multi-stage prompts to delve into these aspects. A comparative study showing multi-stage prompts yield significantly higher F1-scores for connotation and generalization indicators compared to the single-stage baseline would resolve this.

### Open Question 2
Does extending the scoring function to include the sentiment of associated behaviors improve alignment with human stereotype rankings? The conclusion suggests that future work could extend the function by including an indicator that evaluates the sentiment associated with behaviors or characteristics. An updated model incorporating content sentiment that achieves a lower Mean Absolute Error (MAE) against human rankings than the current linguistic-only model would resolve this.

### Open Question 3
How does incorporating the broader context of a sentence affect the accuracy of the stereotype assessment framework? The authors state that current analysis is limited to the sentence level, but future approaches will need to include the broader context in which a sentence is situated. Evaluation results from a dataset containing full paragraphs or documents showing improved scoring performance when context is included versus isolated sentence analysis would resolve this.

## Limitations
- The SCSC framework's applicability to all forms of stereotyping is assumed but not empirically validated beyond its own theoretical claims
- The scoring function may systematically underestimate stereotypes involving specific individuals and cannot capture content harmfulness or cultural context
- Reliance on LLMs introduces risks of encoding their own biases in detection and scoring, particularly in connotation and abstraction judgments

## Confidence
- **High Confidence:** The overall methodology is well-defined and reproducible. The performance ranking of model sizes is consistent with expectations for in-context learning tasks.
- **Medium Confidence:** The effectiveness of the SCSC framework for detecting a wide range of stereotypes, especially implicit or culturally specific ones, is assumed but not fully validated.
- **Medium Confidence:** The scoring function's approximation of human stereotype strength (MAE=0.07) is demonstrated on the dataset but its generalizability and potential systematic biases are acknowledged limitations.

## Next Checks
1. **Prompt ablation study:** Re-run the LLM detection pipeline with 6 few-shot examples instead of 9 to measure impact on accuracy and F1 scores.
2. **Multi-stage prompting for large models:** Test whether separating category label detection and associated content analysis into two sequential LLM prompts improves accuracy on content-related indicators for models >70B parameters.
3. **Content sentiment feature integration:** Extend the scoring function to include a sentiment analysis feature on the associated content and re-evaluate the MAE against human rankings.