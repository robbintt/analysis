---
ver: rpa2
title: 'NavForesee: A Unified Vision-Language World Model for Hierarchical Planning
  and Dual-Horizon Navigation Prediction'
arxiv_id: '2512.01550'
source_url: https://arxiv.org/abs/2512.01550
tags:
- navigation
- planning
- prediction
- available
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NavForesee addresses the challenge of long-horizon embodied navigation
  by unifying hierarchical language planning with dual-horizon predictive world modeling
  in a single Vision-Language Model. The model decomposes instructions into sequential
  sub-goals while predicting short-term environmental dynamics and long-term navigation
  milestones using compact depth and semantic features.
---

# NavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction

## Quick Facts
- **arXiv ID**: 2512.01550
- **Source URL**: https://arxiv.org/abs/2512.01550
- **Reference count**: 40
- **Primary result**: Unified hierarchical language planning with dual-horizon world modeling achieves 66.2% SR and 78.4% OSR on R2R-CE val-unseen.

## Executive Summary
NavForesee addresses long-horizon embodied navigation by integrating hierarchical language planning with dual-horizon world modeling within a single Vision-Language Model. The approach decomposes complex instructions into sequential sub-goals while predicting both immediate environmental dynamics and strategic navigation milestones using compact depth and semantic features. This creates a perception-planning-prediction-action loop that enables agents to anticipate future states for both immediate execution and long-term guidance. Evaluated on R2R-CE and RxR-CE benchmarks, NavForesee achieves competitive performance with state-of-the-art methods despite training only on public data, demonstrating the effectiveness of unified perception-planning-prediction frameworks for complex navigation tasks.

## Method Summary
NavForesee extends a frozen Qwen2.5-VL-3B-Instruct backbone with dual objectives: hierarchical language planning and world model prediction. The model processes instruction-observation sequences through the VLM, then generates planning labels (navigation summary, future plan, language action) and predicts compact features (depth, DINOv2, SAM) for both short-term (fixed horizon) and long-term (milestone-adaptive) horizons. A structured attention mask ensures temporal coherence between prediction horizons while preventing cross-modal interference. The architecture includes dream queries for each feature type and horizon, position encoding for relative pose, and lightweight decoders for feature prediction and action generation. Training interleaves both tasks with joint loss functions, freezing the ViT backbone while training the LLM and decoders.

## Key Results
- Achieves 66.2% Success Rate and 78.4% Oracle Success Rate on R2R-CE val-unseen
- Demonstrates dual-horizon prediction improves navigation accuracy by 7.6% over single-horizon baselines
- Shows hierarchical planning contributes critical guidance, with 17.4% SR drop when planning is removed
- Maintains strong performance despite training only on public R2R-CE and RxR-CE datasets

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical language planning enables robust long-horizon navigation by decomposing complex instructions into milestone-based sub-goals with explicit progress tracking. The VLM generates navigation summaries identifying completed sub-instructions, determines current sub-instruction position, and formulates future plans as textual action trunks. This grounds planning in overall instruction while reducing cognitive load on the model.

### Mechanism 2
Dual-horizon world model prediction provides both immediate environmental awareness and strategic milestone foresight by forecasting compact high-level features rather than raw pixels. Two sets of learnable dream queries extract temporally aligned embeddings—short-term predicts fixed 4-step horizon while long-term adapts to milestone distance. Features include depth, DINOv2, and SAM representations processed through lightweight decoders.

### Mechanism 3
Structured attention masking creates controlled information flow where long-term predictions depend on short-term predictions while the action query integrates all horizons for globally consistent navigation decisions. Causal attention ensures short-term embeddings are produced first; long-term queries attend to short-term but not vice-versa. Depth and semantics queries are mutually masked to prevent cross-modal leakage.

## Foundational Learning

- **Vision-Language Models (VLMs) for embodied navigation**: Why needed—NavForesee extends Qwen2.5-VL, requiring understanding of multimodal encoding and auto-regressive text generation under visual conditioning. Quick check—Can you explain how a VLM processes interleaved image-text sequences and generates grounded textual outputs?

- **Latent-space world models**: Why needed—The world model predicts compact features (depth, DINOv2, SAM) rather than pixels, following DreamVLA-style architectures. Quick check—What are the trade-offs between pixel-space and latent-space prediction for embodied agents?

- **Continuous environment VLN (R2R-CE/RxR-CE)**: Why needed—Benchmarks use continuous navigation with 15° or 30° turn increments and constrained fields of view, differing from discrete graph-based VLN. Quick check—How does continuous action space affect waypoint prediction compared to discrete action classification?

## Architecture Onboarding

- **Component map**: Instruction + observations → Qwen encoders → dream queries with pose embeddings → structured attention → decoders → waypoints/orientation/arrival flags

- **Critical path**: The model processes multimodal inputs through the frozen ViT encoder and trainable LLM, applies position encoding for relative pose, generates dream queries for each feature type and horizon, enforces structured attention masking, then decodes features and actions through lightweight transformers.

- **Design tradeoffs**: Feature prediction vs. pixel generation—chose compact features for computational efficiency (avoids prohibitive sampling costs). Separate vs. unified training—interleaved joint training preserves planning while extending to world model tasks. Fixed vs. adaptive long-term horizon—adaptive to milestones but uncertain during inference.

- **Failure signatures**: SR drops 17.4% without VLM planning—indicates planning is critical, not just prediction. Long-term depth predictions degrade near milestones—uncertainty increases when milestone position unknown. OSR (78.4%) >> SR (66.2%)—agent reaches goal area but fails to stop precisely.

- **First 3 experiments**: 1) Validate hierarchical planning quality—run inference on val-unseen with planning-only mode; verify milestone identification aligns with ground-truth keyframes using Gemini annotations. 2) Isolate world model contribution—train depth-only and semantics-only variants to confirm both modalities contribute. 3) Test attention mask ablation—remove structured masking and compare prediction quality and action coherence.

## Open Questions the Paper Calls Out

### Open Question 1
Can NavForesee's unified planning-prediction framework generalize effectively to outdoor or open-world navigation environments beyond indoor benchmarks like R2R-CE and RxR-CE? The paper notes limited generalization to complex environments and training solely on indoor datasets.

### Open Question 2
How does the quality of VLM-generated hierarchical planning labels (via Gemini 2.5 Pro) affect downstream navigation performance, and what are the failure modes from annotation noise? The paper uses VLM annotations without quantifying quality or characterizing error propagation.

### Open Question 3
Does jointly training planning and world model prediction in a single VLM introduce training interference or catastrophic forgetting between tasks? The paper states data is jointly mixed but provides no analysis of task interference or gradient conflicts.

## Limitations
- **Data quality dependency**: Performance relies entirely on Gemini 2.5 Pro annotations for hierarchical planning, introducing potential systematic bias if annotation process misidentifies sub-goals or milestone boundaries.

- **Inference horizon uncertainty**: Dual-horizon framework adapts long-term prediction horizons to milestone distance, but unclear how this is computed during inference without ground-truth milestone information.

- **Feature representation constraints**: Depth, DINOv2, and SAM features may miss critical scene elements for complex navigation decisions, with neither modality sufficient alone based on ablation results.

## Confidence

- **High Confidence**: Dual-horizon prediction architecture and structured attention mask design are well-specified and empirically validated. Quantitative results (66.2% SR, 78.4% OSR) are reproducible from described methodology.

- **Medium Confidence**: Hierarchical language planning mechanism depends on dataset generation quality. Framework is sound but actual planning performance constrained by annotation quality and VLM's ability to generalize hierarchical reasoning.

- **Low Confidence**: Claim about "dual-horizon" prediction providing strategic foresight weakened by adaptive horizon mechanism during inference. Without clear specification of how long-term horizons are determined without milestones, strategic prediction capability may be limited.

## Next Checks

1. **Milestone Annotation Validation**: Run inference on validation set with planning-only mode and compare predicted sub-goals against Gemini-generated ground-truth milestones. Measure alignment accuracy to quantify planning quality independent of world model predictions.

2. **Long-term Horizon Behavior Analysis**: Conduct ablation studies removing adaptive horizon adaptation—fix long-term predictions to fixed horizons (4, 8, 16 steps) during inference. Compare SR degradation to quantify how much long-term prediction actually contributes to navigation success.

3. **Cross-Modal Feature Correlation**: Analyze feature space similarity between depth and semantics predictions when structured attention masking is removed. Compute correlation metrics and measure impact on action prediction quality to validate cross-modal separation hypothesis.