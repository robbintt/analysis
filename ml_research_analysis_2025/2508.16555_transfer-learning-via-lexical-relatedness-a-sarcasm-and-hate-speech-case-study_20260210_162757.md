---
ver: rpa2
title: 'Transfer Learning via Lexical Relatedness: A Sarcasm and Hate Speech Case
  Study'
arxiv_id: '2508.16555'
source_url: https://arxiv.org/abs/2508.16555
tags:
- sarcasm
- hate
- speech
- implicit
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated whether sarcasm pre-training improves hate
  speech detection, particularly implicit forms. We hypothesized that sarcasm and
  implicit hate share surface-level lexical and semantic features, making sarcasm
  a useful auxiliary task for hate speech detection.
---

# Transfer Learning via Lexical Relatedness: A Sarcasm and Hate Speech Case Study

## Quick Facts
- arXiv ID: 2508.16555
- Source URL: https://arxiv.org/abs/2508.16555
- Authors: Angelly Cabrera; Linus Lei; Antonio Ortega
- Reference count: 31
- Primary result: Sarcasm pre-training improved BERT+BiLSTM's recall by 9.7%, AUC by 7.8%, and F1-score by 6% on ETHOS hate speech detection

## Executive Summary
This study investigates whether sarcasm pre-training can improve hate speech detection, particularly for implicit forms. We hypothesized that sarcasm and implicit hate share surface-level lexical and semantic features, making sarcasm a useful auxiliary task for hate speech detection. Two training strategies were evaluated: single-step training on sarcasm before testing on hate speech, and sequential transfer learning with fine-tuning across sarcasm, implicit hate, and explicit hate. BERT+BiLSTM and CNN+LSTM models were trained on three datasets: Sarcasm on Reddit, ETHOS, and the Implicit Hate Corpus. Results showed that sarcasm pre-training improved BERT+BiLSTM's recall by 9.7%, AUC by 7.8%, and F1-score by 6% on ETHOS. On the Implicit Hate Corpus, precision increased by 7.8% when tested on implicit-only samples. These findings demonstrate that leveraging lexically or semantically related tasks can help address data scarcity in hate speech detection.

## Method Summary
The study employs sequential transfer learning to train models on sarcasm detection before fine-tuning on hate speech tasks. Three datasets were used: Sarcasm on Reddit (104,161 samples, binary), ETHOS (998 samples, continuous labels binarized at ≥0.33 threshold), and the Implicit Hate Corpus (21,480 samples, multi-label, 33% hate samples). Two architectures were tested: BERT+BiLSTM with frozen BERT embeddings (470K params) and CNN+LSTM (70K params). The training pipeline follows: Phase 1 - train on Sarcasm on Reddit using both comment and parent context; Phase 2 - fine-tune on Implicit Hate Corpus with class weighting; Phase 3 - fine-tune on ETHOS. Evaluation metrics include precision, recall, F1-score, AUC, and MCC. Jaccard similarity and Jensen-Shannon divergence were used to measure lexical relatedness between datasets before transfer learning.

## Key Results
- Sarcasm pre-training improved BERT+BiLSTM's recall by 9.7%, AUC by 7.8%, and F1-score by 6% on ETHOS hate speech detection
- On the Implicit Hate Corpus, precision increased by 7.8% when tested on implicit-only samples
- Sequential transfer learning showed better generalization than single-step pre-training, avoiding the high precision (92%) but poor generalization of the single-step approach

## Why This Works (Mechanism)

### Mechanism 1: Surface-Level Lexical Transfer
Sarcasm pre-training improves implicit hate detection because both forms share surface-level lexical patterns despite differing intent. Jaccard similarity analysis revealed 0.353 average overlap between sarcasm and hate speech vocabularies. Models learn to recognize indirect phrasing, rhetorical misdirection, and tone-literal meaning contradictions from sarcasm data, which transfers to identifying covert hostile expressions in implicit hate.

### Mechanism 2: Progressive Task Difficulty Sequencing
Sequential fine-tuning from sarcasm → implicit hate → explicit hate creates a curriculum that progressively adapts representations. Models first learn abstract figurative language patterns from sarcasm, then specialize for hostile covert language on implicit hate, then calibrate for overt abuse. This allows incremental representation refinement rather than direct mapping.

### Mechanism 3: Bias Amplification Risk via Identity-Term Association
Improved detection may come at the cost of learning spurious correlations between identity terms and hate labels. Lexical overlap analysis showed 54.2% of top n-grams shared between sarcasm and implicit hate, with many being identity-related (race, gender, religion). Models may learn to associate these terms with hate regardless of context, increasing false positives on neutral identity discussions.

## Foundational Learning

- **Transfer Learning vs. Multi-Task Learning**: The paper uses sequential transfer (train → fine-tune → fine-tune), not simultaneous multi-task training. Understanding this distinction is critical for reproducing the training pipeline correctly.
  - Quick check: If you train on sarcasm and hate speech simultaneously with shared weights, is this the approach the paper tested?

- **Jaccard Similarity and Jensen-Shannon Divergence**: The paper uses these metrics to justify task relatedness before attempting transfer. Understanding them helps evaluate whether your own source-target pairs are suitable candidates.
  - Quick check: If two datasets have Jaccard similarity of 0.15 and JSD of 0.85, would you expect strong transfer learning benefits?

- **Class Imbalance and Label Uncertainty**: The Implicit Hate Corpus is 67% neutral, and hate speech labels are inherently subjective. These factors constrain maximum achievable performance and require weighted training.
  - Quick check: If annotators agree only 70% of the time on implicit hate labels, what does this imply about the upper bound of model accuracy?

## Architecture Onboarding

- **Component map**: BERT embedding layer (frozen) -> 2 BiLSTM layers (bidirectional) -> Dropout (20%) -> Dense layers -> Output classification

- **Critical path**:
  1. Pre-process datasets: binarize ETHOS (≥0.33 = hate), filter Reddit sarcasm (≥10 upvotes, 0 downvotes)
  2. Compute Jaccard/JSD between source and target corpora to verify relatedness
  3. Train on Sarcasm on Reddit with parent context as auxiliary input
  4. Fine-tune on Implicit Hate Corpus with class weighting
  5. Fine-tune on ETHOS; evaluate with precision, recall, F1, AUC, MCC
  6. Run ablation: compare with/without sarcasm pre-training on held-out test sets

- **Design tradeoffs**:
  - Frozen vs. trainable embeddings: Paper froze BERT embeddings (reduced params from 67M to 470K) with no performance loss—freezing is recommended
  - BERT+BiLSTM vs. CNN+LSTM: Transformer-based architecture generalized better across heterogeneous corpora; CNN struggled with dataset variability
  - Single-step vs. sequential transfer: Single-step showed inflated precision (92%) but poor generalization; sequential is preferred despite more complex pipeline
  - Learning rate 5e-5: Balanced training time with convergence quality

- **Failure signatures**:
  - High precision, low recall on held-out data: Indicates overfitting to training distribution (seen in single-step approach)
  - Sarcasm-hate confusion in embeddings: Use t-SNE to verify cluster separation; overlapping clusters suggest insufficient decoupling
  - Disproportionate false positives on identity terms: Model learning spurious correlations; requires bias audit on neutral identity-mentioning samples
  - CNN+LSTM generalization collapse: If CNN variant performs >15% worse than BERT variant on transfer tasks, architectural incompatibility confirmed

- **First 3 experiments**:
  1. Baseline comparison: Train BERT+BiLSTM on ETHOS directly (no pre-training) vs. sequential sarcasm → implicit hate → ETHOS. Measure F1, AUC, and MCC deltas on 60% held-out test split.
  2. Implicit-only evaluation: Isolate implicit hate samples from Implicit Hate Corpus. Compare precision/recall with and without sarcasm pre-training to verify the mechanism targets the intended subset.
  3. Lexical overlap threshold test: Create synthetic dataset pairs with controlled Jaccard similarities (0.20, 0.35, 0.50). Test whether transfer benefits scale with lexical overlap to validate the proposed break condition.

## Open Questions the Paper Calls Out
None

## Limitations
- Lexical overlap metrics (Jaccard/JSD) only measure surface similarity; no analysis of whether transferred features are semantically meaningful or spurious correlations
- Sequential transfer learning showed better generalization than single-step, but ablation study on training order is missing
- Dataset-specific findings: ETHOS (n=998) is small; results may not generalize to larger or more diverse hate speech corpora
- No robustness testing across different model architectures beyond BERT+BiLSTM and CNN+LSTM

## Confidence

- **High confidence**: Sequential transfer learning improves generalization over single-step pre-training (supported by multiple metrics: F1, AUC, MCC)
- **Medium confidence**: Surface-level lexical similarity justifies transfer learning (supported by Jaccard analysis but no semantic validation)
- **Medium confidence**: Sarcasm pre-training specifically benefits implicit hate detection (evidence from ETHOS and Implicit Hate Corpus, but limited dataset diversity)

## Next Checks
1. Conduct ablation studies testing different training orders (explicit → implicit → sarcasm) to verify that sarcasm-first sequencing is optimal
2. Evaluate model performance on out-of-domain hate speech datasets to test generalizability beyond the three studied corpora
3. Perform controlled experiments varying Jaccard similarity thresholds (0.20, 0.35, 0.50) to empirically validate the proposed lexical overlap break condition