---
ver: rpa2
title: 'Vision Meets Language: A RAG-Augmented YOLOv8 Framework for Coffee Disease
  Diagnosis and Farmer Assistance'
arxiv_id: '2505.21544'
source_url: https://arxiv.org/abs/2505.21544
tags:
- detection
- disease
- which
- language
- yolov8
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a hybrid AI system for coffee leaf disease
  diagnosis that integrates YOLOv8 for object detection, Retrieval-Augmented Generation
  (RAG) for contextual information retrieval, and a Large Language Model (LLM) for
  generating interpretable remediation guidance. The system addresses limitations
  of traditional plant disease detection methods by not only identifying diseases
  but also explaining their causes and suggesting treatment options in a conversational
  interface.
---

# Vision Meets Language: A RAG-Augmented YOLOv8 Framework for Coffee Disease Diagnosis and Farmer Assistance

## Quick Facts
- arXiv ID: 2505.21544
- Source URL: https://arxiv.org/abs/2505.21544
- Reference count: 16
- Primary result: Hybrid AI system achieving 0.681 mAP@0.5 for coffee leaf disease detection with RAG-LLM for interpretable remediation guidance

## Executive Summary
This paper presents a hybrid AI system for coffee leaf disease diagnosis that integrates YOLOv8 for object detection, Retrieval-Augmented Generation (RAG) for contextual information retrieval, and a Large Language Model (LLM) for generating interpretable remediation guidance. The system addresses limitations of traditional plant disease detection methods by not only identifying diseases but also explaining their causes and suggesting treatment options in a conversational interface. Trained on the BRACOL dataset of 1,747 coffee leaf images, the YOLOv8 model achieved a mean Average Precision (mAP@0.5) of 0.681 and mAP@0.5:0.95 of 0.454, with particularly strong performance on the Miner class (mAP@0.5 = 0.894). The RAG framework grounds LLM responses with domain-specific knowledge, mitigating hallucinations and improving accuracy.

## Method Summary
The system combines YOLOv8-nano object detection with a RAG-LLM pipeline for conversational coffee disease diagnosis. YOLOv8 processes 640×640 images from the BRACOL dataset (1,747 images, 4 disease classes) to detect and classify diseases. Detected disease labels serve as queries to a FAISS vector store containing domain knowledge on coffee diseases. Retrieved context chunks are injected into LLM prompts (via Groq API, Llama-3) to generate cause explanations and treatment recommendations. The system uses RecursiveCharacterTextSplitter for chunking, sentence-transformers for embeddings, and LangChain's ConversationBufferWindowMemory for dialogue coherence. A Streamlit interface enables real-time disease detection and conversational Q&A with source citations.

## Key Results
- YOLOv8-nano achieved mAP@0.5 of 0.681 and mAP@0.5:0.95 of 0.454 on BRACOL validation set
- Class-wise performance: Miner (0.894 mAP@0.5), Phoma (0.839), Cercospora (0.575), Rust (0.415)
- RAG framework grounds LLM responses with domain-specific knowledge, reducing hallucination risk
- End-to-end system provides real-time detection, explanations, and actionable remedies via conversational interface

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG grounding reduces LLM hallucination by constraining generation to retrieved domain-specific context.
- Mechanism: Disease labels from YOLOv8 serve as structured queries to a FAISS vector store; retrieved passages are injected into the LLM prompt, biasing generation toward curated knowledge rather than pretrained weights alone.
- Core assumption: The knowledge base documents are comprehensive and accurate for the target domain.
- Evidence anchors: [abstract] "The RAG framework grounds LLM responses with domain-specific knowledge, mitigating hallucinations and improving accuracy." [section 4.2] "This retrieval-then-read mechanism ensures that responses remain factually grounded and contextually relevant."

### Mechanism 2
- Claim: Cascading vision-to-language pipeline enables interpretable, task-specific remediation guidance.
- Mechanism: YOLOv8 outputs bounding boxes and class labels; labels are stringified as natural language queries, which the RAG-LLM module processes to generate cause explanations and treatment recommendations.
- Core assumption: Detected disease labels are sufficiently accurate to serve as reliable retrieval keys.
- Evidence anchors: [abstract] "The system addresses limitations of traditional plant disease detection methods by not only identifying diseases but also explaining their causes and suggesting treatment options." [section 4.1] "These predicted disease classes serve as input prompts to the language model in the subsequent stage."

### Mechanism 3
- Claim: Class-wise detection performance correlates with annotation volume and visual distinctiveness.
- Mechanism: Miner class achieves highest mAP@0.5 (0.894) due to clearer visual signatures and sufficient annotations; Rust underperforms (mAP@0.5 = 0.415) due to annotation imbalance and similarity to healthy tissue.
- Core assumption: Annotation quality and quantity are primary determinants of per-class detection performance.
- Evidence anchors: [section 7.1] "Classes with a higher number of representative annotations (Miner and Phoma) yield better results, while detection of Rust still poses challenges due to annotation imbalance and visual similarity."

## Foundational Learning

- Concept: YOLO object detection architecture (grid-based prediction, anchor boxes, single-shot inference)
  - Why needed here: Understanding how YOLOv8 produces bounding boxes and class probabilities is essential for debugging detection failures and interpreting mAP metrics.
  - Quick check question: Can you explain why YOLO processes an entire image in one forward pass, and what tradeoff this creates compared to two-stage detectors?

- Concept: RAG pipeline components (chunking, embedding, vector store, retrieval-then-read)
  - Why needed here: The system's explainability hinges on correct retrieval; understanding chunking strategies and similarity search helps diagnose why relevant remedies may be missed.
  - Quick check question: What happens to retrieval quality if text chunks are too large or too small relative to the query?

- Concept: Conversation memory in LLM applications (buffer window, context retention)
  - Why needed here: Follow-up questions rely on LangChain's ConversationBufferWindowMemory to maintain dialogue coherence.
  - Quick check question: How does a sliding window memory affect the system's ability to reference earlier context in long conversations?

## Architecture Onboarding

- Component map: User uploads leaf image -> YOLOv8-nano detects diseases -> disease labels query FAISS vector store -> retrieved context + query to LLM -> LLM generates grounded response -> Streamlit displays results with source citations

- Critical path:
  1. User uploads leaf image via Streamlit
  2. YOLOv8 inference returns detected disease label(s)
  3. Label used as query against FAISS vector store
  4. Top-k retrieved chunks + user query sent to LLM
  5. LLM generates grounded response with source references

- Design tradeoffs:
  - YOLOv8-nano selected for computational efficiency; larger variants (YOLOv8m/l) may improve accuracy but increase latency and VRAM requirements.
  - Input resolution reduced from 2048×1024 to 640×640; faster training but potential loss of fine-grained disease features.
  - RAG used instead of fine-tuning LLM; avoids costly retraining but depends on knowledge base coverage.

- Failure signatures:
  - Rust class consistently underperforms (mAP@0.5 = 0.415, recall = 0.316); expect missed detections or confusion with healthy tissue.
  - Retrieval returns irrelevant chunks if knowledge base lacks disease-specific entries; LLM may revert to generic or hallucinated advice.
  - Memory buffer overflow in long dialogues; earlier context may be dropped, breaking conversation coherence.

- First 3 experiments:
  1. Baseline validation: Run YOLOv8 inference on held-out BRACOL validation split; log per-class precision, recall, and mAP to confirm reported metrics.
  2. Retrieval ablation: Manually query FAISS with each disease label; verify top-3 retrieved chunks are semantically relevant to expected remedies.
  3. End-to-end smoke test: Upload sample diseased leaf images through Streamlit interface; confirm detection, retrieval, and LLM response pipeline executes without errors and includes source citations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can targeted data augmentation and class-balancing techniques effectively resolve the low detection performance for the "Rust" disease class?
- Basis in paper: [inferred] The authors note that Rust detection poses challenges (mAP@0.5 = 0.415) due to annotation imbalance and visual similarity to healthy textures, suggesting these techniques as potential solutions.
- Why unresolved: The current study reports the low score but does not implement or evaluate the suggested balancing strategies.
- What evidence would resolve it: Comparative mAP scores for the Rust class after applying specific oversampling or augmentation techniques to the training set.

### Open Question 2
- Question: To what extent does the RAG framework quantitatively mitigate LLM hallucinations compared to a baseline LLM without retrieval?
- Basis in paper: [inferred] The paper claims the system "mitigates hallucinations" and improves accuracy, but the evaluation section only provides object detection metrics (mAP), lacking any qualitative or quantitative assessment of the generated text.
- Why unresolved: There is no data provided on the factual consistency or error rates of the generated remediation advice.
- What evidence would resolve it: A comparative evaluation using metrics like factual consistency scores or human expert ratings of the chatbot's responses with vs. without RAG.

### Open Question 3
- Question: Does upgrading the architecture to YOLOv10 or YOLOv11 yield significant accuracy improvements over the YOLOv8-nano baseline?
- Basis in paper: [explicit] The Conclusion explicitly identifies "improving the object detection component by upgrading to newer versions such as YOLOv10 or YOLOv11" as a key area for future enhancement.
- Why unresolved: The study utilizes YOLOv8-nano, and the authors have not tested the system on the newer iterations they mention.
- What evidence would resolve it: Benchmarking results comparing the mean Average Precision (mAP) and inference latency of YOLOv10/v11 against the current v8 baseline on the BRACOL dataset.

## Limitations

- Knowledge base dependency: The RAG component's accuracy critically depends on the completeness and quality of the domain knowledge document, which is not provided in the paper.
- Class imbalance impact: The Rust class shows significantly lower performance (mAP@0.5 = 0.415) suggesting the 100-epoch training may be insufficient for balancing performance across all disease types.
- Hardware constraints: The system was trained on RTX 4060 with 8GB VRAM, which may limit the scalability of larger YOLO variants or more complex embedding models.

## Confidence

- YOLOv8 detection performance: High confidence - The mAP metrics are directly measurable from the BRACOL dataset using standard evaluation protocols.
- RAG hallucination mitigation: Medium confidence - While the mechanism is sound, the actual effectiveness depends on the unspecified knowledge base content.
- End-to-end system utility: Medium confidence - The system architecture is technically feasible but practical deployment effectiveness depends on factors not fully evaluated.

## Next Checks

1. **Retrieval relevance audit**: Manually query the FAISS vector store with each disease label and verify that the top-3 retrieved chunks contain relevant remediation information. Document any gaps where retrieval returns unrelated content.

2. **Class performance breakdown**: Generate detailed confusion matrices for each disease class using the held-out validation set. Focus on Rust class to quantify false positives/negatives and determine whether poor performance stems from visual similarity or annotation quality issues.

3. **Memory context retention test**: Conduct multi-turn conversations with the Streamlit interface using disease images, asking follow-up questions about treatment timing and application methods. Verify that ConversationBufferWindowMemory maintains coherence across at least 5 exchanges.