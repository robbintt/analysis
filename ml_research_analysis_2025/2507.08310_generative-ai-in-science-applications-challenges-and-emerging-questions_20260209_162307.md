---
ver: rpa2
title: 'Generative AI in Science: Applications, Challenges, and Emerging Questions'
arxiv_id: '2507.08310'
source_url: https://arxiv.org/abs/2507.08310
tags:
- genai
- chatgpt
- science
- research
- scientific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a qualitative review of 39 highly cited publications
  on Generative AI (GenAI) in science, using thematic analysis to examine applications
  and challenges across research, scientific writing, medical practice, and education.
  The study finds that while GenAI is rapidly adopted in scientific contexts, its
  long-term implications remain uncertain, with ongoing concerns about sustainability,
  equity, transparency, and governance.
---

# Generative AI in Science: Applications, Challenges, and Emerging Questions

## Quick Facts
- arXiv ID: 2507.08310
- Source URL: https://arxiv.org/abs/2507.08310
- Reference count: 12
- Primary result: Qualitative review of 39 highly cited publications reveals rapid GenAI adoption in science with ongoing concerns about sustainability, equity, transparency, and governance

## Executive Summary
This paper provides a qualitative review of 39 highly cited publications on Generative AI (GenAI) in science, using thematic analysis to examine applications and challenges across research, scientific writing, medical practice, and education. The study finds that while GenAI is rapidly adopted in scientific contexts, its long-term implications remain uncertain, with ongoing concerns about sustainability, equity, transparency, and governance. The review highlights the dual nature of GenAI as both a tool for innovation and a source of ethical and epistemological risks, and identifies key areas for future research.

## Method Summary
The study conducted a systematic literature review using the OpenAlex database (January 2024 snapshot) with a Boolean search query to identify 13,661 GenAI records (2017-2023). After filtering for papers with ≥100 citations and removing 1 non-relevant paper, 39 publications were selected for thematic analysis using NVivo 12. The analysis identified 4 primary categories (Emergence, Application, Education/Training, Concerns) and 11 secondary codes, examining applications across research, scientific writing, medical practice, and education domains.

## Key Results
- GenAI adoption in science spans research assistance, scientific writing, medical practice, and education with applications including literature review, draft generation, clinical note generation, and personalized learning
- Core concerns include hallucination effects (fabricated content), blackbox opacity (unexplainable outputs), sustainability issues (high energy costs), and equity gaps (access disparities)
- Long-term implications remain uncertain with risks of GenAI becoming an "ultimate epistemological authority" and potential erosion of scientific rigor

## Why This Works (Mechanism)

### Mechanism 1: Pattern-Based Content Generation from Large-Scale Training
GenAI models can assist scientific work by generating content that resembles human-created outputs, based on patterns learned from extensive training datasets. Models use convoluted and recurrent neural networks trained on large datasets to generate new text, images, or other content by learning statistical patterns from preexisting information. LLMs specifically operate on string prediction tasks—predicting token likelihood given preceding context. Core assumption: Patterns in training data adequately represent domain knowledge required for scientific applications. Evidence anchors: GenAI models are trained on extensive data sets that can create new data and/or context (such as text, images or music) based on patterns learned from preexisting information (Pavlik, 2023; Ray, 2023); LLMs refer to systems which are trained on string prediction tasks: that is, predicting the likelihood of a token (character, word or string) given either its preceding context. Break condition: Novel scientific concepts absent from training data; domain-specific terminology underrepresented; adversarial or ambiguous prompts.

### Mechanism 2: Scalable Two-Stage Learning with Minimal Human Input
Newer LLMs can process substantially larger datasets after initial training without requiring extensive manual human annotation. Two-stage/bidirectional LLMs are initially trained on small datasets with human input, then scale to process much larger datasets without significant manual intervention in later stages. Core assumption: Initial calibration provides sufficient grounding for accurate large-scale learning. Evidence anchors: Newer versions or two staged/bidirectional LLMs have two major advantages, namely 1) they are able to process much larger data sets and 2) these larger amounts of data are able to be processed without a great deal of human manual input; ChatGPT has seen rapid adoption, implying scalability. Break condition: Significant domain shift between initial and scaled training phases; insufficient quality control in scaled data.

### Mechanism 3: Conversational Interface via Transformer Architecture
GPT-based models enable human-like conversational interactions applicable across scientific writing, research, medical practice, and education. Generative Pre-Trained Transformer models—emerging from neurolinguistic programming foundations—use machine learning to comprehend and generate human language, enabling translation, summarization, question-answering, and text editing. Core assumption: Token prediction on large corpora produces coherent, contextually appropriate responses across scientific domains. Evidence anchors: Conversational-like or Generative Pre-Trained Transformer (GPT) models that have emerged out of the field of neurolinguistic programming, facilitation the development of tools that use machine learning to better comprehend human language; ChatGPT articulates its responses to such a level that it has been able to pass medical exams. Break condition: Highly technical domain-specific language; prompts requiring reasoning beyond statistical patterns; tasks demanding accountability (authorship, clinical responsibility).

## Foundational Learning

- **Concept: Hallucination Effects**
  - Why needed here: The paper identifies hallucination as a core concern—outputs that are fabricated, inaccurate, or lack coherent referencing. Critical for evaluating whether GenAI outputs are trustworthy in scientific contexts.
  - Quick check question: Given that GenAI generates outputs based on statistical patterns rather than verified knowledge bases, how would you detect whether a cited reference is real or fabricated?

- **Concept: Blackbox Opacity**
  - Why needed here: Users cannot fully understand how GenAI produces specific outputs. The paper raises concerns about transparency, accountability, and governance when deploying these tools in science.
  - Quick check question: If a GenAI tool provides a diagnostic suggestion that proves incorrect, what information would you need to trace how that output was generated—and can current systems provide it?

- **Concept: Energy and Computational Costs**
  - Why needed here: Training and running large models has significant sustainability implications. The paper notes training a single BERT model may consume energy equivalent to a trans-American flight.
  - Quick check question: Before deploying GenAI at scale in a research institution, what metrics would you use to evaluate whether the computational and environmental costs are justified?

## Architecture Onboarding

- **Component map:**
  - Research applications: Literature review assistance, research question identification, methodology selection, statistical analysis support, de novo drug design, protein structure development
  - Scientific writing: Grammar editing, translation, draft generation, image/figure manipulation
  - Medical practice: Clinical note generation, patient communication, diagnostic assistance, pre-authorization drafting
  - Education: Personalized learning, assessment creation, automated marking

- **Critical path:** Define specific use case → Assess accuracy/reliability requirements → Implement with mandatory human oversight → Establish transparency/acknowledgment protocols → Monitor for hallucinations and bias

- **Design tradeoffs:**
  - Speed vs. accuracy: Faster output generation trades off against verification burden
  - Automation vs. human judgment: Full automation in high-stakes domains (medicine) introduces legal/ethical risks
  - Accessibility vs. equity: High-income institutions better positioned to leverage GenAI, potentially exacerbating disparities
  - Capability vs. sustainability: Larger models offer more capability but higher energy/computational costs

- **Failure signatures:**
  - Fabricated references or citations (hallucination)
  - Plausible but factually incorrect scientific claims passing peer review
  - AI-generated content bypassing plagiarism detection
  - Patient misdiagnosis from overreliance on clinical suggestions
  - Erosion of assessment quality in educational settings
  - Undisclosed AI authorship compromising scientific integrity

- **First 3 experiments:**
  1. **Reference validation test:** Prompt GenAI to generate literature reviews with citations; systematically verify what percentage of cited sources exist and accurately reflect stated content
  2. **Domain accuracy benchmark:** Compare GenAI-generated responses to expert-validated answers across multiple scientific domains (e.g., biology, medicine, physics) to quantify hallucination rates by field
  3. **Transparency protocol pilot:** Implement mandatory GenAI disclosure statements in manuscript submissions; measure reviewer ability to detect AI involvement and assess impact on trust assessments

## Open Questions the Paper Calls Out

- **Question:** What are the differences in the uptake of GenAI in science between early career researchers and senior academics?
  - Basis in paper: [explicit] The authors explicitly list this as item 8 in Table 1 ("Emerging Research Questions"), asking how usage varies by career stage and other demographics.
  - Why unresolved: Current literature reviewed focuses on general capabilities and ethical debates rather than granular user demographics or labor market impacts.
  - What evidence would resolve it: Survey data or usage logs segmented by career stage, gender, discipline, and institution type.

- **Question:** How do scientific practices involving GenAI change the production of knowledge, particularly relating to theory?
  - Basis in paper: [explicit] Listed as item 1 in Table 1, reflecting the paper's thematic analysis of "epistemological risks" and the potential for GenAI to become an "ultimate epistemological authority."
  - Why unresolved: Existing studies highlight the risk of bias and "stochastic parrots," but the long-term impact on scientific theory formation is still speculative.
  - What evidence would resolve it: Comparative analysis of theoretical frameworks generated with GenAI assistance versus those developed without it.

- **Question:** What specific human tasks in science will GenAI reduce, augment, or add?
  - Basis in paper: [explicit] Listed as item 2 in Table 1, derived from the application analysis showing GenAI affects research design, summarization, and writing.
  - Why unresolved: The literature is currently disjointed regarding whether GenAI will fully automate research roles or merely serve as a tool requiring constant human intervention.
  - What evidence would resolve it: Longitudinal observation of scientific workflows showing the redistribution of specific tasks (e.g., literature review vs. hypothesis generation).

## Limitations
- Sample of 39 highly cited publications may overrepresent positive applications and underrepresent critical perspectives
- Thematic analysis relies on subjective coding without reporting inter-coder reliability metrics
- Focus on highly cited papers may introduce publication bias for controversial or failed implementations

## Confidence

- **High Confidence**: The review accurately captures documented applications of GenAI in scientific writing, research assistance, medical practice, and education as reported in the literature
- **Medium Confidence**: The identification of challenges (hallucinations, blackbox opacity, sustainability concerns) reflects consensus concerns but may not represent emerging technical solutions
- **Low Confidence**: Long-term implications and governance recommendations remain highly speculative given the nascent state of GenAI deployment in science

## Next Checks
1. **Longitudinal Citation Analysis**: Track citation patterns of the 39 reviewed papers over time to identify which concerns (e.g., hallucinations, equity issues) gain prominence versus fade as technical solutions emerge.
2. **Implementation Case Studies**: Conduct empirical studies of actual GenAI deployment in research institutions, documenting specific failures, workarounds, and adaptation patterns not captured in theoretical literature.
3. **Cross-Domain Validation**: Systematically test GenAI performance across scientific domains (biology, physics, medicine) using standardized benchmarks to quantify domain-specific hallucination rates and accuracy variations.