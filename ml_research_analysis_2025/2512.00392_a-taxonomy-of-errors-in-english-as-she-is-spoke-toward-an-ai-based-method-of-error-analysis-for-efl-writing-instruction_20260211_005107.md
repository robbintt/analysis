---
ver: rpa2
title: 'A Taxonomy of Errors in English as she is spoke: Toward an AI-Based Method
  of Error Analysis for EFL Writing Instruction'
arxiv_id: '2512.00392'
source_url: https://arxiv.org/abs/2512.00392
tags:
- error
- errors
- system
- taxonomy
- writing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed an AI-assisted error analysis system for EFL
  writing using Large Language Models (LLMs) and a detailed taxonomy based on linguistic
  theory. The system classifies writing errors at word and sentence levels (spelling,
  grammar, punctuation) through Python-coded API calls.
---

# A Taxonomy of Errors in English as she is spoke: Toward an AI-Based Method of Error Analysis for EFL Writing Instruction

## Quick Facts
- arXiv ID: 2512.00392
- Source URL: https://arxiv.org/abs/2512.00392
- Reference count: 20
- Primary result: AI system classified 84.4% of errors correctly on error-rich test corpus

## Executive Summary
This study presents an AI-assisted error analysis system for EFL writing instruction that uses Large Language Models and a comprehensive taxonomy to classify writing errors at word and sentence levels. The system, tested on the error-rich text "English as she is spoke," correctly identified 84.4% of errors and achieved 84.4% accuracy in classification. The research demonstrates AI's potential to transform EFL instruction through automated detailed error analysis and feedback, while identifying key limitations including context sensitivity and taxonomy coverage gaps.

## Method Summary
The system employs a hierarchical taxonomy of 1100+ error codes based on linguistic theories, implemented as a JSON file. Using Python, the system chunks text into sentences, constructs detailed prompts incorporating the taxonomy, and calls LLM APIs (Claude 3.5 Sonnet or DeepSeek R1) with temperature set to 0.0 for deterministic output. The LLM analyzes each sentence and returns structured error codes matching the taxonomy. The system aggregates results and tracks classification accuracy against expert-verified error data.

## Key Results
- AI system correctly identified 27 out of 32 errors (84.4% accuracy) on test corpus
- Classification accuracy varied by error type: 60% for simple errors, 80%+ for complex errors
- DeepSeek R1 showed 81-93% accuracy for T2 categories but faced availability issues
- System demonstrated high reliability for spelling, grammar, and punctuation errors within sentence boundaries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedding JSON taxonomy in LLM prompt constrains classification to predefined categories
- Mechanism: 1100+ line hierarchical taxonomy loaded as `codes_text` variable, with LLM instructed to return only existing error codes
- Core assumption: LLM can reliably apply fine-grained linguistic categories when given explicit definitions
- Evidence anchors: [abstract] taxonomy grounded in Corder/Richards/James theories; [section 3.1] detailed error definitions and examples
- Break condition: Taxonomy gaps cause LLM to invent new categories

### Mechanism 2
- Claim: Temperature=0.0 produces deterministic error classification
- Mechanism: Python DAS module sets temperature=0.0 in API calls, forcing highest-probability token selection
- Core assumption: Deterministic output preferred for educational assessment
- Evidence anchors: [section 3.2] temperature=0.0 eliminates randomness; [section 1.2] references 99%+ consistency rates
- Break condition: Not applicable—stable by design

### Mechanism 3
- Claim: Sentence-level chunking enables granular detection but loses cross-sentence context
- Mechanism: Input split into sentences, each analyzed independently with full taxonomy
- Core assumption: Most pedagogically valuable errors occur within sentence boundaries
- Evidence anchors: [section 3.2] one sentence per chunk; [section 4.2.1] missed coherence issues; [section 4.2.2] pronoun misassignment
- Break condition: Context-dependent errors systematically missed

## Foundational Learning

- **Error Analysis taxonomy structure (Corder/Richards/James framework)**
  - Why needed: System's T1/T2/T3 hierarchy maps to these theories; essential for taxonomy extension/debugging
  - Quick check: Given "She don't like it," is this interlingual or intralingual? What T2 category applies?

- **LLM prompting with constrained output formats**
  - Why needed: System relies on structured prompts demanding specific output (text→correction→numbered errors with codes)
  - Quick check: If LLM returns valid error code not in taxonomy JSON, what happened and how to detect programmatically?

- **Taxonomy overlap and hierarchy resolution**
  - Why needed: Paper documents errors classified under multiple subcategories; requires hierarchy rules
  - Quick check: "I seen him yesterday" contains verb tense and irregular verb errors. Which takes precedence?

## Architecture Onboarding

- **Component map**: DAS (Python module) -> Taxonomy JSON (1100+ lines) -> API Layer (Claude 3.5/DepthSeek) -> Chunking Engine -> Output Formatter

- **Critical path**: Load taxonomy → Read input → Chunk into sentences → Construct prompt → API call → Parse response → Write output → Track missed chunks

- **Design tradeoffs**: Chunking granularity (sentences lose context vs. paragraphs exceed token limits); Taxonomy depth (146 T3 vs. 17 T2); LLM selection (accuracy vs. availability)

- **Failure signatures**: Category invention (novel codes not in taxonomy); Context loss (pronoun/discourse errors); Overlap conflicts (different codes across runs); Spelling-first dependency (grammar fails on misspelled words)

- **First 3 experiments**:
  1. Taxonomy coverage test: 50 sentences each with single known T3 error; measure precision/recall per category
  2. Chunking boundary test: Sentence-level vs. 3-sentence window on text with cross-sentence pronoun errors
  3. Category invention audit: 100 diverse student sentences; flag any output code not in taxonomy JSON

## Open Questions the Paper Calls Out
None

## Limitations
- Taxonomy Coverage Gaps: 1100+ line taxonomy fails to capture emerging error patterns, especially modal verbs and complex constructions
- Context Sensitivity Constraints: Sentence-level chunking systematically misses cross-sentence errors including pronoun reference and discourse coherence
- LLM Dependence: System performance entirely dependent on underlying LLM capabilities and availability

## Confidence
- **High Confidence (80-100%)**: Spelling, punctuation, and straightforward grammatical errors within sentence boundaries (84.4% accuracy)
- **Medium Confidence (40-79%)**: Effectiveness across diverse student writing samples beyond test corpus (unproven generalization)
- **Low Confidence (0-39%)**: Handling of stylistic and discourse-level errors (currently excluded from taxonomy)

## Next Checks
1. Cross-Corpus Generalization Test: Apply system to three diverse EFL learner corpora; measure accuracy variation and identify systematic misclassification patterns
2. Context Window Extension Experiment: Modify chunking to variable-length windows (1-3 sentences); compare pronoun reference and discourse coherence error detection
3. Taxonomy Expansion Validation: Expert analysis of each category-invention event to determine genuine taxonomy gaps vs. LLM misclassification; develop systematic expansion process