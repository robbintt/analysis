---
ver: rpa2
title: 'ArtifactGen: Benchmarking WGAN-GP vs Diffusion for Label-Aware EEG Artifact
  Synthesis'
arxiv_id: '2509.08188'
source_url: https://arxiv.org/abs/2509.08188
tags:
- diffusion
- arxiv
- artifact
- training
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work benchmarks conditional WGAN-GP with projection discriminator
  against a denoising diffusion model (1D U-Net with FiLM conditioning) for generating
  label-aware EEG artifact segments. Using the TUAR corpus, subject-wise splits, and
  multi-channel 1-2s windows, the study finds WGAN-GP achieves closer spectral alignment
  (lower relative band-power errors, MMD) than the diffusion baseline, though both
  models exhibit detectable temporal/cross-channel discrepancies.
---

# ArtifactGen: Benchmarking WGAN-GP vs Diffusion for Label-Aware EEG Artifact Synthesis

## Quick Facts
- **arXiv ID:** 2509.08188
- **Source URL:** https://arxiv.org/abs/2509.08188
- **Reference count:** 40
- **Primary result:** WGAN-GP achieves closer spectral alignment than denoising diffusion for label-aware EEG artifact synthesis, though both exhibit temporal/cross-channel discrepancies.

## Executive Summary
This work benchmarks conditional WGAN-GP with projection discriminator against a denoising diffusion model (1D U-Net with FiLM conditioning) for generating label-aware EEG artifact segments. Using the TUAR corpus, subject-wise splits, and multi-channel 1-2s windows, the study finds WGAN-GP achieves closer spectral alignment (lower relative band-power errors, MMD) than the diffusion baseline, though both models exhibit detectable temporal/cross-channel discrepancies. Diffusion used z-score normalization, longer windows, and limited sampling steps, contributing to weaker fidelity. The evaluation suite—including Welch band-power deltas, covariance/ACF distances, MMD, and downstream augmentation—emphasizes metrics aligned with EEG structure over image heuristics. WGAN-GP’s better spectral realism highlights the importance of conditioning, normalization, and architectural design, while also revealing room for improvement in capturing artifact-specific spatial and temporal patterns. Code and reproducible configurations are released to enable rigorous benchmarking.

## Method Summary
The study compares WGAN-GP with projection discriminator to denoising diffusion (1D U-Net with FiLM) for generating label-aware EEG artifacts from the TUAR corpus. WGAN-GP uses per-window min-max normalization and 1s windows; diffusion uses per-recording z-score normalization and 2s windows. Both are trained on subject-wise splits (149 train, 32 val, 32 test) across 8 channels at 250 Hz, generating 3,000 samples per class for evaluation. Metrics include Welch band-power deltas, MMD, covariance Frobenius distance, ACF L2, and kNN recovery accuracy.

## Key Results
- WGAN-GP consistently achieves lower relative band-power errors and MMD compared to diffusion across artifact classes.
- Both models show weak class-conditional recovery in downstream augmentation tasks.
- Diffusion’s spectral drift under limited sampling steps (80 DDIM) contributes to higher band-power deltas than WGAN-GP.

## Why This Works (Mechanism)

### Mechanism 1: Projection Discriminator Improves Conditional Alignment
- **Claim:** The projection discriminator provides tighter spectral alignment than FiLM-style conditioning under the evaluated settings.
- **Mechanism:** The critic computes D(x, y) = w^T φ(x) + ⟨φ(x), e_y⟩, where class embedding e_y directly modulates the decision boundary. This forces the generator to produce class-consistent spectra to fool the critic, as the gradient signal explicitly penalizes label-feature mismatch.
- **Core assumption:** The artifact class primarily manifests in spectral structure that the penultimate features φ(x) can capture.
- **Evidence anchors:**
  - [Section 5.1]: "Class awareness is injected via a projection term [49]: D(x, y) = w^T φ(x) + ⟨φ(x), e_y⟩"
  - [Section 6]: "Across artifact classes, we observe consistently lower relative band-power errors for the WGAN compared to the diffusion model... MMD(R, WGAN) < MMD(R, DDPM)"
  - [Corpus]: Weak direct evidence; no corpus papers compare projection discriminators to FiLM for EEG.

### Mechanism 2: Per-Window Min-Max Normalization Acts as Implicit Spectral Regularizer
- **Claim:** Per-window normalization stabilizes adversarial training by reducing amplitude variance across samples.
- **Mechanism:** Normalizing each window to [-1, 1] before critic input removes recording-level amplitude drift, forcing the critic to focus on within-window spectral and temporal patterns. This reduces mode collapse driven by amplitude outliers.
- **Core assumption:** Artifact identity is amplitude-invariant at the window level; absolute amplitude is not diagnostic.
- **Evidence anchors:**
  - [Section 4]: "Per-window min–max to [-1, 1] (adversarial path)... For window x ∈ R^{C×L} with global per-window extrema m = min_{c,t} x_{c,t} and M = max_{c,t} x_{c,t}"
  - [Section 6]: "per-window min–max scaling and shorter windows (1 s) emphasize local amplitude dynamics and can act as an implicit spectral regularizer for the critic"
  - [Corpus]: No corpus papers explicitly compare normalization strategies for EEG GANs.

### Mechanism 3: Classifier-Free Guidance in Diffusion Enables Tunable Fidelity-Diversity Tradeoffs
- **Claim:** CFG allows controllable generation without a separate classifier, but aggressive guidance can distort spectra under limited sampling steps.
- **Mechanism:** During training, the null label is randomly substituted for the class label. At inference, the denoised estimate is extrapolated away from the unconditional prediction: ε̂ = ε̂_uncond + s·(ε̂_cond - ε̂_uncond). Higher guidance scale s amplifies class-conditional signal but may amplify sampling artifacts.
- **Core assumption:** The conditional and unconditional score estimates share sufficient structure that linear extrapolation improves specificity without introducing artifacts.
- **Evidence anchors:**
  - [Section 5.2]: "we reserve a null label to support classifier-free guidance during sampling [28]"
  - [Section 6]: "In combination with classifier-free guidance (CFG), this can tilt the spectrum when guidance is set too aggressively and steps are limited"
  - [Corpus]: CFG is standard in vision diffusion; corpus shows no EEG-specific CFG tuning evidence.

## Foundational Learning

- **Concept: Wasserstein GAN with Gradient Penalty (WGAN-GP)**
  - **Why needed here:** WGAN-GP stabilizes adversarial training by softly enforcing the 1-Lipschitz constraint on the critic, enabling convergence on high-dimensional EEG windows.
  - **Quick check question:** Can you explain why gradient penalty λ·E[(||∇_x̂ D(x̂)||_2 - 1)²] replaces weight clipping, and what happens if λ is too small?

- **Concept: Denoising Diffusion Probabilistic Models (DDPM)**
  - **Why needed here:** DDPMs learn to reverse a gradual noising process, providing stable training and better mode coverage than GANs at the cost of sampling latency.
  - **Quick check question:** Given a forward noising process q(x_t | x_{t-1}), sketch the reverse denoising step p_θ(x_{t-1} | x_t) and identify what the network predicts.

- **Concept: FiLM (Feature-wise Linear Modulation)**
  - **Why needed here:** FiLM injects conditioning information (timestep, class label) into intermediate U-Net layers by applying learned affine transformations.
  - **Quick check question:** Given feature map h ∈ R^{C×L} and conditioning vector c, write the FiLM transformation γ(c) ⊙ h + β(c). What inductive bias does this impose?

## Architecture Onboarding

- **Component map:**
  TUAR Corpus → Windowing (250/500 samples, 50% overlap)
       ↓
  Model-Specific Normalization (min-max for GAN, z-score for diffusion)
       ↓
  WGAN-GP Path: z ~ N(0,I) ⊕ one-hot(y) → TransposedConv Generator → Real/Fake windows → Conv Critic with Projection Head → Loss: Wasserstein + Gradient Penalty
  Diffusion Path: x_t = √ᾱ_t·x_0 + √(1-ᾱ_t)·ε → 1D U-Net with FiLM layers → FiLM: timestep embedding ⊕ class embedding → Loss: MSE(ε, ε̂_θ(x_t, t, y)) → Sampling: CFG with null label, 80 DDIM steps
       ↓
  Evaluation: Band-power deltas, Covariance Frobenius, ACF L2, MMD, PRD, kNN recovery

- **Critical path:** The data preprocessing path (windowing → normalization) is the most common failure point. Incorrect normalization (e.g., applying z-score per-window instead of per-recording for diffusion) will cause training instability. The manifest system tracks these statistics.

- **Design tradeoffs:**
  - **Window length:** Shorter (1s) favors WGAN spectral alignment; longer (2s) captures more temporal context but increases diffusion sampling cost.
  - **Sampling steps:** 50 steps used here; increasing to 200-1000 improves diffusion quality but linearly increases latency.
  - **Guidance scale:** CFG=1.5 was tuned on validation FID; higher values improve conditional specificity but risk spectral distortion.

- **Failure signatures:**
  - **Mode collapse (GAN):** Generated samples cluster tightly; diversity proxy drops below ~0.3.
  - **Spectral drift (Diffusion):** Band-power relative error > 0.5 in any canonical band; PSD L2 error spikes.
  - **Label leakage:** kNN recovery accuracy near 100% on synthetic data but fails on real test data (overfitting to model artifacts).
  - **Covariance mismatch:** Frobenius distance > 2.0 indicates cross-channel structure not captured.

- **First 3 experiments:**
  1. **Reproduce baseline metrics:** Train WGAN-GP for 61 epochs and diffusion for 200 epochs on the provided TUAR split. Verify MMD(R, WGAN) < MMD(R, DDPM) and band-power errors match Table 1. This validates the pipeline.
  2. **Ablate normalization:** Swap normalization schemes (apply per-window min-max to diffusion, per-recording z-score to GAN). Hypothesis: WGAN spectral advantage is partially attributable to normalization, not just architecture.
  3. **Increase diffusion sampling steps:** Re-sample diffusion with 200 and 500 DDIM steps at constant CFG=1.5. Measure MMD and band-power error reduction. This tests whether sampling budget, not model capacity, limits diffusion performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can advanced conditioning strategies, such as classifier guidance or noise/sampler co-design, resolve the weak class-conditional recovery observed in both WGAN-GP and diffusion models?
- Basis in paper: [explicit] Future Work states the "immediate priority is to strengthen conditioning and guidance" to stabilize gradients and reduce mode collapse, as current models show "weak class-conditional recovery."
- Why unresolved: Both generative paradigms failed to accurately recover artifact classes, limiting their utility for targeted data augmentation.
- What evidence would resolve it: Improved class-conditional accuracy on downstream kNN/classifier recovery tasks and higher diversity in synthesized outputs per class.

### Open Question 2
- Question: Does the inclusion of explicit multi-resolution spectral objectives (e.g., STFT losses) enable diffusion models to match the spectral fidelity of WGAN-GP?
- Basis in paper: [explicit] Future Work plans to "incorporate multi-resolution spectral objectives... to explicitly regularize band-power structure," addressing the higher spectral error observed in diffusion outputs.
- Why unresolved: The diffusion baseline exhibited higher band-power deltas than WGAN-GP, potentially due to the lack of implicit spectral regularization found in the GAN's normalization scheme.
- What evidence would resolve it: A significant reduction in Welch band-power deltas (Δδ, Δθ, etc.) and PSD L2 error for diffusion models trained with auxiliary spectral losses.

### Open Question 3
- Question: To what extent does the observed performance gap between WGAN-GP and Diffusion stem from architectural inductive biases versus the distinct preprocessing schemes (per-window min-max vs. per-recording z-score)?
- Basis in paper: [inferred] The authors explicitly list "unifying preprocessing" as a "promising lever" and admit the "comparison is not perfectly controlled" because normalization and window lengths differed.
- Why unresolved: It remains unclear if WGAN-GP's superior spectral alignment is a result of the model architecture or the per-window min-max scaling acting as a spectral regularizer.
- What evidence would resolve it: A re-evaluation of the benchmark where both models utilize identical normalization (e.g., both using per-window min-max) to isolate the effect of the generative architecture.

## Limitations
- Direct comparative evidence for projection discriminator vs. FiLM on EEG is absent from the corpus; spectral alignment advantage may be partially attributable to normalization rather than architecture.
- Diffusion model used longer windows (2s vs. 1s for GAN) and z-score normalization, introducing architectural and preprocessing confounds that obscure the true relative performance of the underlying model families.
- No evidence of ablation studies on CFG scale, sampling steps, or alternative conditioning schemes to isolate their contributions to diffusion fidelity.

## Confidence
- **High:** WGAN-GP achieves better spectral alignment (lower band-power deltas, MMD) than the diffusion baseline under the evaluated settings.
- **Medium:** Per-window normalization stabilizes adversarial training by reducing amplitude variance; CFG enables tunable fidelity-diversity tradeoffs but risks spectral distortion under limited sampling.
- **Low:** The projection discriminator is the primary driver of WGAN-GP’s spectral advantage; diffusion’s weaker fidelity is solely due to sampling budget.

## Next Checks
1. **Ablate normalization:** Swap normalization schemes (apply per-window min-max to diffusion, per-recording z-score to GAN) and re-measure spectral metrics to isolate preprocessing effects.
2. **Increase diffusion sampling steps:** Re-sample diffusion with 200 and 500 DDIM steps at constant CFG=1.5; verify whether MMD and band-power errors improve.
3. **Test alternative conditioning:** Replace projection discriminator with FiLM conditioning in WGAN-GP while maintaining min-max normalization; compare spectral metrics to isolate architectural contributions.