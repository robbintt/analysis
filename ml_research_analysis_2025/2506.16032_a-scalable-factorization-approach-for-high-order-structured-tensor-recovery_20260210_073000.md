---
ver: rpa2
title: A Scalable Factorization Approach for High-Order Structured Tensor Recovery
arxiv_id: '2506.16032'
source_url: https://arxiv.org/abs/2506.16032
tags:
- tensor
- tucker
- condition
- decomposition
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a unified framework for the factorization approach
  to high-order structured tensor recovery problems. The key idea is to optimize orthonormal
  factors directly on the Stiefel manifold using Riemannian gradient descent (RGD),
  which addresses the computational and memory challenges of traditional tensor optimization
  methods.
---

# A Scalable Factorization Approach for High-Order Structured Tensor Recovery

## Quick Facts
- arXiv ID: 2506.16032
- Source URL: https://arxiv.org/abs/2506.16032
- Reference count: 40
- Primary result: Riemannian gradient descent on orthonormal factors achieves polynomial scaling with tensor order, improving over exponential scaling in previous methods

## Executive Summary
This paper presents a unified framework for high-order structured tensor recovery using Riemannian gradient descent (RGD) on orthonormal factors. The key innovation is optimizing directly on the Stiefel manifold, which mitigates scaling ambiguity and enables polynomial scaling of both initialization requirements and convergence rates with tensor order N. The authors establish a Riemannian regularity condition (RRC) that guarantees linear convergence when properly initialized, and prove that 4r-RIP is sufficient for tensor sensing problems. The framework applies to orthogonal CP, Tucker, and tensor-train decompositions.

## Method Summary
The method optimizes orthonormal factors directly on the Stiefel manifold using Riemannian gradient descent. For Tucker and TT decompositions, the algorithm computes gradients for both orthonormal factors and core/site tensors, projects them onto the tangent space, and applies hybrid learning rates (μ for factors, μ/γ for core) to balance energy disparities. The approach uses spectral initialization (HOSVD or TT-SVD) followed by iterative RGD updates with polar decomposition retraction. The framework requires 4r-RIP for tensor sensing problems, a significant improvement over previous (N+3)r-RIP requirements.

## Key Results
- Linear convergence rate that scales polynomially with tensor order N, improving upon exponential scaling in previous methods
- 4r-RIP sufficiency condition for tensor sensing (compared to (N+3)r-RIP in prior work)
- Unified framework applicable to orthogonal CP, Tucker, and tensor-train decompositions
- Experimental validation showing stable convergence rates as tensor order increases from N=3 to N=10

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Constraining tensor factors to be orthonormal transforms the optimization landscape from an exponentially scaling problem to one that scales polynomially with tensor order $N$.
- **Mechanism:** By enforcing the canonical form ($Z_i^T Z_i = I$) on the Stiefel manifold, the method eliminates scaling ambiguity and ensures the rank remains minimal. This allows the Riemannian Gradient Descent (RGD) to project gradients onto the tangent space of the manifold, maintaining orthonormality while traversing the loss surface efficiently.
- **Core assumption:** The ground-truth tensor can be represented in a canonical form where most factors are orthonormal (e.g., Tucker or TT formats).
- **Evidence anchors:**
  - [abstract] "optimize orthonormal factors directly on the Stiefel manifold... initialization requirement and convergence rate scale polynomially."
  - [section 1] "mitigates scaling ambiguity... improves identifiability."
  - [corpus] Weak direct evidence; related work (e.g., *Fast and Provable Tensor-Train Format Tensor Completion*) supports Riemannian methods but varies on specific manifold constraints.
- **Break condition:** If the tensor factors are not approximately orthonormal (high condition number $\kappa$) or the decomposition format is incompatible with the canonical form (e.g., arbitrary non-orthogonal CP), the theoretical scaling guarantees may degrade.

### Mechanism 2
- **Claim:** The Riemannian Regularity Condition (RRC) serves as the local "convexity-like" property that guarantees linear convergence rates for the non-convex factorized problem.
- **Mechanism:** The RRC binds the error distance between estimated and true factors to the Riemannian gradient direction. It ensures that as long as the iterates remain within a local "trusted region" (radius $a_1$), the negative Riemannian gradient points towards the ground truth, preventing stagnation in saddle points.
- **Core assumption:** The loss function satisfies the Restricted Correlated Gradient (RCG) condition, which generalizes strong convexity to this structured tensor subspace.
- **Evidence anchors:**
  - [section 2.3] Definition 1 (RRC) and the inequality (15) linking error terms to gradient terms.
  - [section 2.4] Theorem 2 proof relies entirely on the RRC holding to derive the contraction inequality.
  - [corpus] External verification of RRC is limited; this appears to be a specific theoretical contribution of this framework.
- **Break condition:** If the initialization falls outside the radius $a_1$ (which depends on $N$ and condition number $\kappa$), the RRC does not hold, and convergence is not guaranteed.

### Mechanism 3
- **Claim:** A "Hybrid" learning rate (discrepant rates for orthonormal factors vs. the core tensor) is required to balance energy disparities in the factorization.
- **Mechanism:** In decompositions like Tucker, orthonormal factors $U_i$ have unit norm, while the core tensor $S$ can have varying energy (norm $\approx \sigma(X^*)$). Using a single learning rate would cause one part to converge slowly or diverge while the other converges rapidly. The ratio $\gamma = \sigma^2(X^*)$ equalizes the descent speed.
- **Core assumption:** There is a significant norm imbalance between the orthonormal factors and the core tensor (or final factor in TT).
- **Evidence anchors:**
  - [section 2.1] Equation (8) and (12) define the hybrid update with ratio $\gamma$.
  - [section 2.1] "...discrepant learning rates... are used to accelerate the convergence rate... since the energy... could be unbalanced."
  - [corpus] *Accelerating Large-Scale Regularized High-Order Tensor Recovery* addresses scale variations but via regularization, distinct from this learning rate mechanism.
- **Break condition:** Using a scalar learning rate ($\gamma=1$) for tensors with large core norms or high condition numbers will likely result in slow convergence or instability.

## Foundational Learning

- **Concept: Stiefel Manifold Optimization**
  - **Why needed here:** This is the geometric space where the orthonormal factors live. Standard Euclidean gradient descent fails here because moving along a straight line breaks the orthonormality constraint.
  - **Quick check question:** Can you explain why a standard gradient step $Z - \mu \nabla$ invalidates the constraint $Z^T Z = I$, and why a "retraction" operation is required?

- **Concept: Tensor Decomposition Formats (Tucker vs. Tensor Train)**
  - **Why needed here:** The paper unifies these formats. Understanding the difference between a "core tensor" (Tucker) and "site tensors" (TT) is necessary to implement the gradient calculations correctly.
  - **Quick check question:** How does the parameter complexity differ between Tucker ($O(Ndr + r^N)$) and TT ($O(Ndr^2)$) as order $N$ grows?

- **Concept: Restricted Isometry Property (RIP)**
  - **Why needed here:** In tensor sensing, RIP is the measure of how well the measurement operator preserves distances. The paper argues that $4r$-RIP is sufficient, a key theoretical improvement over previous $N$-dependent requirements.
  - **Quick check question:** If a measurement operator fails the RIP, what artifact would you expect to see in the recovered tensor?

## Architecture Onboarding

- **Component map:** Input Layer -> Initialization Module -> Optimization Loop (RGD) -> Output
- **Critical path:** The **Initialization Module** is the single point of failure. If the spectral init is not within distance $a_1$ of the truth, the convergence guarantees are void.
- **Design tradeoffs:**
  - **Strict Orthonormality vs. Flexibility:** The paper enforces strict orthonormality ($Z^T Z = I$). This simplifies theory and scaling but may be restrictive if the true underlying physical factors are not orthogonal.
  - **RCG Loss vs. MSE:** The framework supports any loss satisfying RCG, offering flexibility for noise models (e.g., Gaussian vs. Poisson), but requires verifying the RCG constants ($\alpha, \beta$) for new losses.
- **Failure signatures:**
  - **Divergence:** Norm of the core tensor $S$ explodes $\rightarrow$ Learning rate ratio $\gamma$ is likely misconfigured.
  - **Stagnation:** Error $\|X - X^*\|_F$ decreases then plateaus $\rightarrow$ RIP condition likely violated (insufficient measurements $m$) or initialization failed.
  - **Slow Convergence:** Linear convergence observed but with very low rate $\rightarrow$ Condition number $\kappa$ of the ground truth is high.
- **First 3 experiments:**
  1. **Spectral Init Validation:** Generate random rank-$r$ tensors (order $N=3,4,5,6$). Run only the initialization. Plot $\|X^{(0)} - X^*\|_F$ vs. $N$ to verify the polynomial scaling of the error.
  2. **Convergence Rate vs. Order:** Run RGD for $N=3$ to $N=10$. Plot $\log(\text{Error})$ vs. Iterations. Verify that the slope (convergence rate) remains stable as $N$ increases, rather than degrading exponentially.
  3. **RIP Boundary Test:** Fix $N=5$. Vary the number of measurements $m$ from just above the degrees of freedom down to the theoretical limit ($4r$-RIP). Identify the empirical phase transition where recovery fails.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the O(√N) dependence in spectral initialization be eliminated by designing an optimal decomposition method rather than relying on quasi-optimal HOSVD/TT-SVD?
- Basis in paper: [explicit] "However, if an optimal method for these decompositions were to be designed, this requirement could potentially be eliminated."
- Why unresolved: The quasi-optimality of HOSVD and TT-SVD introduces an O(√N) coefficient in the initialization bound (equation 25), but no alternative optimal method is proposed.
- What evidence would resolve it: Demonstration of an initialization method with N-independent approximation guarantees for Tucker/TT decompositions.

### Open Question 2
- Question: Can the unified RGD framework be extended to general Hierarchical Tucker and tensor network decompositions where multiple equivalent representations exist?
- Basis in paper: [explicit] "For a specific form of Hierarchical Tucker or tensor network decomposition where most factors are orthonormal except for one, the initialization requirement and convergence rate of RGD remain polynomially dependent on the tensor order N... However, for an N-order Hierarchical Tucker or tensor network decomposition, multiple equivalent representations exist, making it challenging to formulate a unified framework."
- Why unresolved: The non-uniqueness of representations complicates the distance metric and rotation ambiguity handling central to the analysis.
- What evidence would resolve it: A generalized distance measure and Riemannian regularity condition that handles the representation equivalence in general tensor networks.

### Open Question 3
- Question: Can the RGD framework achieve polynomial sample complexity for tensor completion rather than the exponential scaling with N currently observed?
- Basis in paper: [explicit] "Moreover, as demonstrated in [47], the required number of samples m grows exponentially with respect to N."
- Why unresolved: The current analysis for tensor completion relies on incoherence conditions that inherently require exponentially many samples as tensor order increases.
- What evidence would resolve it: Modified analysis or sampling strategies achieving m = O(poly(N) · degrees of freedom) for tensor completion.

## Limitations
- The analysis assumes exact orthogonality of factors and requires the Riemannian regularity condition to hold within a specific initialization radius that depends polynomially on N
- The framework may be restrictive for tensors whose true factors are not orthogonal or when decomposition formats are incompatible with the canonical form
- The 4r-RIP condition, while an improvement, still requires measurements to scale linearly with degrees of freedom and may not hold for ill-conditioned operators

## Confidence
- High: Polynomial scaling of convergence rates with tensor order (mechanically derived from initialization and contraction analysis)
- Medium: The 4r-RIP sufficiency claim (relies on specific loss function properties and restricted gradient conditions)
- Low: Practical performance on non-Gaussian measurement operators or tensors with high condition numbers (theoretical analysis focuses on well-conditioned cases)

## Next Checks
1. Test RGD convergence when factors deviate from perfect orthonormality (introduce controlled condition number κ > 1)
2. Verify 4r-RIP condition empirically across different measurement operator distributions beyond Gaussian
3. Evaluate performance when ground-truth tensor cannot be represented in the assumed canonical Tucker/TT form