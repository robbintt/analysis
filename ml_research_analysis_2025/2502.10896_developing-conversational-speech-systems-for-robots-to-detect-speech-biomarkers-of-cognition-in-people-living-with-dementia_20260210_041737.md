---
ver: rpa2
title: Developing Conversational Speech Systems for Robots to Detect Speech Biomarkers
  of Cognition in People Living with Dementia
arxiv_id: '2502.10896'
source_url: https://arxiv.org/abs/2502.10896
tags:
- speech
- dementia
- biomarkers
- biomarker
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed a real-time conversational speech system for
  robots to detect speech biomarkers of cognitive impairment in people living with
  dementia (PLwD). The system integrates a fine-tuned large language model with six
  speech biomarkers (Altered Grammar, Pragmatic Impairments, Anomia, Disrupted Turn-Taking,
  Slurred Pronunciation, and Prosody Changes) processed in under 1.5 seconds.
---

# Developing Conversational Speech Systems for Robots to Detect Speech Biomarkers of Cognition in People Living with Dementia

## Quick Facts
- arXiv ID: 2502.10896
- Source URL: https://arxiv.org/abs/2502.10896
- Authors: Rohith Perumandla; Young-Ho Bae; Diego Izaguirre; Esther Hwang; Andrew Murphy; Long-Jing Hsu; Selma Sabanovic; Casey C. Bennett
- Reference count: 40
- Primary result: Real-time conversational robot system detects dementia biomarkers with composite score correlating at r=-0.55 with MMSE

## Executive Summary
This study presents a real-time conversational speech system for robots to detect speech biomarkers of cognitive impairment in people living with dementia. The system integrates a fine-tuned large language model with six distinct speech biomarkers processed in under 1.5 seconds. Tested on DementiaBank and Indiana datasets, the composite biomarker score showed moderate correlation with MMSE scores, outperforming individual biomarkers. Analysis revealed higher and more variable scores in human-robot conversations, suggesting conversational context impacts biomarker detection. The findings highlight potential clinical applications and the need for further research on real-world deployment scenarios.

## Method Summary
The system processes conversational speech through a multi-component pipeline: audio is captured and transcribed via Azure ASR, then six speech biomarkers are extracted in parallel. These include content-based features (Altered Grammar, Pragmatic Impairments, Anomia) from text transcripts and acoustic features (Disrupted Turn-Taking, Slurred Pronunciation, Prosody Changes) from audio. A fine-tuned Phi-3 LLM generates conversational responses while biomarker calculations run asynchronously. The composite score averages individual biomarker outputs. The system was tested on DementiaBank (306 dementia samples, 242 controls) with MMSE scores and Indiana dataset (27 human-robot conversations).

## Key Results
- Composite biomarker score correlated at r=-0.55 with MMSE scores, outperforming individual biomarkers
- Human-robot conversations showed higher and more variable biomarker scores than human-human interactions
- System achieved sub-1.5 second response time while maintaining real-time biomarker processing
- T-tests showed composite scores significantly different across MMSE severity levels (p<0.001 for none vs. severe)

## Why This Works (Mechanism)

### Mechanism 1: Composite Biomarker Aggregation
Combining multiple heterogeneous speech biomarkers into a composite score provides a more robust proxy for cognitive impairment than any single metric. Cognitive decline manifests across distinct domainsâ€”linguistic (grammar, semantics) and acoustic (prosody, pronunciation). By calculating a composite score, the system aggregates orthogonal signals, dampening the noise or "miss" rate of any single biomarker that might be affected by non-cognitive factors.

### Mechanism 2: Asynchronous Architecture
Decoupling the conversational response generation from the biomarker calculation via an asynchronous architecture preserves natural interaction flow (sub-1.5s latency). The system splits the workload: the LLM generates a response immediately based on transcript/text to maintain the "turn-taking" illusion, while heavier acoustic feature extraction and complex analysis run in parallel background threads.

### Mechanism 3: LLM Fine-Tuning for Dementia Speech
Fine-tuning LLMs on domain-specific dementia conversation data is required to handle the "altered communication styles" of PLwD (e.g., incoherence, pauses) which would break standard out-of-the-box models. Standard LLMs expect coherent, grammatical inputs. PLwD often produce fragmented, repetitive, or anomia-affected speech. Fine-tuning the model (Phi-3) on the Indiana dataset allows the system to maintain semantic coherence and generate relevant responses despite noisy inputs.

## Foundational Learning

- **Dementia Speech Biomarkers (Content vs. Acoustic)**
  - Why needed here: You cannot debug or improve the system without distinguishing what you are measuring. Content features (grammar, pragmatics) come from text/transcripts, while acoustic features (prosody, pronunciation) come from raw audio.
  - Quick check question: If the Automatic Speech Recognition (ASR) fails, which biomarkers are completely lost? (Answer: Content-based).

- **Real-time "Turn-Taking" in HRI**
  - Why needed here: The system's "Disrupted Turn-Taking" biomarker relies on precise timing (IPUs - Interpausal Units). Understanding that human turn-taking often involves <200ms gaps is crucial to realizing why a 1.5s latency is the absolute maximum limit.
  - Quick check question: Why is "sub-1.5s" response time the target threshold rather than 3s? (Answer: To maintain the projection/reaction loop of natural conversation).

- **LLM Fine-Tuning (PEFT/LoRA)**
  - Why needed here: The system uses a "Phi-3-Mini-4K-Instruct" model modified with LoRA. You need to understand that LoRA allows adapting a model for dementia-specific speech without retraining the entire multi-billion parameter base model, saving computational resources suitable for robotic deployment.
  - Quick check question: Why is "quantization" mentioned alongside LoRA in the system specs? (Answer: To reduce memory usage for inference on resource-constrained hardware).

## Architecture Onboarding

- **Component map:**
  Frontend (PWA on Smartphone/Tablet) -> NGINX (Reverse Proxy) -> Python WebSocket Server -> Central Module (Python) -> PostgreSQL Database
  Central Module contains: Azure STT (Speech-to-Text) -> LLM (Phi-3) for response generation -> Six Biomarker Sub-modules -> Composite Score

- **Critical path:**
  1. User Speaks -> Frontend captures audio
  2. Audio -> WebSocket Server -> Azure STT (Text)
  3. **Parallel Split:**
     a. Text -> LLM -> Robot Response -> Frontend (via TTS)
     b. Audio/Text -> Biomarker Modules -> Composite Score -> Database -> Frontend Dashboard

- **Design tradeoffs:**
  - **Latency vs. Depth:** The system prioritizes conversational flow (<1.5s) over deep analysis by making biomarker calculation asynchronous. Result: Real-time chat works, but biofeedback lags slightly (5s updates).
  - **Accuracy vs. Interpretability:** They use defined biomarkers (e.g., "Anomia score") rather than a raw end-to-end Deep Learning classifier. This makes the system explainable to clinicians but might miss subtle features a larger black-box model would catch.

- **Failure signatures:**
  - **"Zombie" Conversation:** Robot continues talking but biomarker dashboard freezes. Diagnosis: Biomarker modules crashing or blocking the event loop (despite async intent).
  - **Zero Anomia Score:** User is speaking clearly but score is 0. Diagnosis: Regex for filler words ("um", "ah") failing or ASR filtering them out before analysis.
  - **Context Drift:** Robot loses track of conversation topic. Diagnosis: LLM context window (4k tokens) exceeded in long sessions; history management failure.

- **First 3 experiments:**
  1. **Latency Load Test:** Simulate 10 concurrent users to verify if the <1.5s response time degrades when the Central Module handles multiple biomarker calculations simultaneously.
  2. **ASR Robustness Check:** Feed audio samples with heavy "slurred pronunciation" into the pipeline to see if STT transcription quality degrades the *content-based* biomarkers (Grammar) disproportionately to *acoustic* ones.
  3. **Biomarker Sensitivity Analysis:** Record a conversation where you intentionally introduce specific deficits (e.g., use only simple grammar, or intentional pauses) to verify if the corresponding biomarker scores spike independently of others.

## Open Questions the Paper Calls Out

- **How does conversational scenario affect biomarker scores?** The study found different biomarker scores between DementiaBank (human clinician) and Indiana dataset (robot) but could not determine whether differences were due to dementia severity or conversational context, as Indiana lacked MMSE scores. Controlled experiments with the same PLwD participants conversing across different partners (robot vs. human) and settings, all with consistent MMSE assessments, would resolve this.

- **How can Prosody biomarker accuracy be improved?** Despite testing multiple modeling approaches and feature selection, Prosody showed near-zero correlation with MMSE (r = 0.032) and no accuracy improvement was achieved. Systematic exploration of alternative acoustic feature sets, model architectures, or prosody-specific feature engineering demonstrating improved correlation with cognitive scores would resolve this.

- **Can composite sensitivity detect subtle severity differences?** T-tests showed significance for large severity gaps (p<0.001 for none vs. severe) but failed for adjacent severity levels (mild vs. moderate, moderate vs. severe). Refined biomarker combinations or optimized weighting schemes achieving statistically significant differentiation between adjacent MMSE severity thresholds would resolve this.

- **What is the optimal biomarker combination and weighting?** The authors note "there is an endless array of possible combinations and it is not immediately clear when using multiple biomarkers what the optimal combination may be" and acknowledge they "did not test" whether unequal weighting could improve performance. Cross-validated comparison of different weighting schemes (regression-based, ML-optimized) on datasets with MMSE scores to identify the configuration maximizing clinical correlation would resolve this.

## Limitations

- The study relies on conversational data from specific controlled scenarios, and observed higher biomarker scores in robot conversations may not generalize to naturalistic settings
- The Indiana dataset lacks MMSE ground truth, preventing direct validation of biomarker-clinical correlation in the human-robot context
- The specific 18-feature subset for Pronunciation and exact LoRA fine-tuning parameters are not specified, creating reproducibility challenges

## Confidence

- **High Confidence:** The core architecture (asynchronous processing for real-time operation) and the moderate correlation (r=-0.55) between composite biomarker and MMSE are well-supported by the data
- **Medium Confidence:** The claim that human-robot conversations produce systematically different biomarker profiles requires additional validation with diverse populations and longer-term studies
- **Medium Confidence:** The assertion that the LLM fine-tuning handles PLwD speech patterns appropriately is supported by qualitative improvements but lacks quantitative ablation studies

## Next Checks

1. Conduct a longitudinal study with PLwD in their home environments to validate if biomarker scores remain stable across different conversational contexts and over time
2. Perform a feature ablation study to determine if the specific weighting of the six biomarkers is optimal or if certain features dominate the composite score
3. Test the system's performance with PLwD who have co-occurring speech disorders (e.g., dysarthria from Parkinson's) to assess false positive rates and clinical utility in complex cases