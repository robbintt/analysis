---
ver: rpa2
title: 'Support Basis: Fast Attention Beyond Bounded Entries'
arxiv_id: '2510.01643'
source_url: https://arxiv.org/abs/2510.01643
tags:
- block
- attention
- query
- definition
- entries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces support-basis decomposition to efficiently
  approximate softmax attention beyond the restrictive bounded-entry assumption. The
  method decomposes query and key matrices into sparse and dense components, applying
  exact computation to sparse parts and polynomial approximation to dense parts.
---

# Support Basis: Fast Attention Beyond Bounded Entries

## Quick Facts
- **arXiv ID:** 2510.01643
- **Source URL:** https://arxiv.org/abs/2510.01643
- **Reference count:** 12
- **Primary result:** Introduces support-basis decomposition for sub-quadratic softmax attention approximation without requiring bounded matrix entries

## Executive Summary
This paper addresses a fundamental limitation in fast attention algorithms: the requirement that query and key matrices have bounded entries. Under the bounded-entry assumption, previous polynomial approximation methods achieve sub-quadratic runtime but fail completely when applied to typical transformer models where this assumption is violated. The proposed support-basis decomposition method overcomes this by splitting attention computation into sparse (exact) and dense (polynomial-approximated) components based on entry magnitude. Under sub-Gaussian assumptions, this yields sub-quadratic runtime with provable error bounds while maintaining downstream task performance comparable to exact attention.

## Method Summary
The method decomposes the query-key product $QK^T$ into disjoint sparse and dense components using a magnitude threshold $T$. Large entries (exceeding $T$) form the sparse component $A^{(L)}$ computed exactly, while small entries form the bounded component $A^{(s)}$ approximated via Chebyshev polynomial expansion. The polynomial approximation enables low-rank factorization, reducing computation from $O(n^2d)$ to $O(nrd)$ where $r$ depends on polynomial degree. The method is extended to multi-threshold bucketing to remove distributional assumptions at the cost of weaker accuracy guarantees. Experiments demonstrate faster runtime than exact attention and lower error than previous methods while maintaining downstream accuracy on GSM8K and MMLU benchmarks.

## Key Results
- Achieves sub-quadratic runtime $O(n^{1+\alpha}d)$ for $\alpha < 1$ without bounded-entry assumptions
- Provides $\ell_\infty$ error bound $\|P - D^{-1}AV\|_\infty \le \epsilon \|V\|_\infty$ under sub-Gaussian assumptions
- Maintains downstream accuracy comparable to exact attention while prior methods fail completely (0% accuracy)
- Degree-6 Chebyshev polynomial nearly matches exact attention performance on GSM8K and MMLU benchmarks
- Runtime experiments show 2-3x speedup over exact attention with minimal accuracy degradation

## Why This Works (Mechanism)

### Mechanism 1: Support-Basis Decomposition
The method splits $QK^T$ into disjoint matrices $A^{(L)}$ (sparse, large entries) and $A^{(s)}$ (dense, bounded entries), enabling exact computation on sparse parts and polynomial approximation on dense parts. Under sub-Gaussian assumptions, the number of large entries is $O(n^\alpha)$ for $\alpha < 1$, making exact computation sub-quadratic. The disjointness property ensures additive exponential decomposition: $\exp(QK^T/d) = \exp(A^{(s)}/d) + \exp(A^{(L)}/d) - 1$.

### Mechanism 2: Polynomial Approximation via Chebyshev Basis
Replacing $\exp(x)$ with degree-$g$ Chebyshev polynomial enables low-rank factorization of attention matrix. The polynomial expands into monomials reorganized as $(U_1U_2^T)_{ij}$ where $U_1, U_2 \in \mathbb{R}^{n \times r}$. For accuracy $\epsilon$ on range $[-B, B]$, degree $g = \Theta(\max\{\log(1/\epsilon)/\log(\log(1/\epsilon)/B), B\})$. This creates accuracy-efficiency trade-off: larger $B$ requires higher $g$, increasing rank $r$.

### Mechanism 3: Multi-Threshold Bucketing (Distribution-Free)
Using multiple thresholds $T_1 > T_2 > ... > T_m$ with exponential spacing partitions entries into buckets, enabling polynomial approximation on each bounded-interval component without distributional assumptions. Each normalized component has $\ell_\infty$ norm $\leq o(\sqrt{\log n})$, satisfying polynomial approximation requirements. Sketching via random projections approximates polynomial kernel in linear time.

## Foundational Learning

- **Concept: Sub-Gaussian Distributions**
  - **Why needed here:** Justifies sparsity assumption. If $Q, K$ entries are sub-Gaussian with variance proxy $\sigma^2$, then $\Pr[|X| \geq t] \leq 2\exp(-t^2/\sigma^2)$, providing high-probability bounds on number of entries exceeding threshold $T$.
  - **Quick check question:** Given $n = 8192, d = 64, T = 0.3$, and $\sigma_Q = \sigma_K = 0.1$, what is the expected number of entries in $Q^{(L)}$?

- **Concept: Chebyshev Polynomial Approximation**
  - **Why needed here:** Enables efficient low-rank factorization of $\exp(QK^T)$. Understanding accuracy-degree trade-off is essential for threshold selection.
  - **Quick check question:** To approximate $\exp(x)$ within $\epsilon = 0.01$ on $[-2, 2]$, what minimum degree $g$ is required according to Lemma A.3?

- **Concept: Disjoint Matrix Decomposition**
  - **Why needed here:** Key property enabling additive exponential decomposition. Without disjointness, $\exp(A+B) \neq \exp(A) + \exp(B) - 1$.
  - **Quick check question:** Given $Q = Q^{(L)} + Q^{(s)}$ and $K = K^{(L)} + K^{(s)}$, how do you construct $A^{(L)}$ to ensure disjointness with $A^{(s)}$?

## Architecture Onboarding

- **Component map:**
  Input: $Q, K, V \in \mathbb{R}^{n \times d}$ → Split($Q,T$) → $Q^{(L)}, Q^{(s)}$ | Split($K,T$) → $K^{(L)}, K^{(s)}$ → Construct $A^{(L)}$ (sparse) via $QK^T$ where entries are large → $A^{(L)}$ → Exact computation: $(\exp(A^{(L)}/d) - 1) \cdot V$ → $A^{(s)}$ → Polynomial approximation: $U_1U_2^T \approx \exp(A^{(s)}/d)$ → Aggregate: $D^{-1}(C_1 + C_2)$ where $D = \text{diag}((U_1U_2^T + \exp(A^{(L)}/d) - 1)1_n)$

- **Critical path:** Threshold selection ($T$). Too small → many large entries → $A^{(L)}$ becomes dense → quadratic time. Too large → $A^{(s)}$ has unbounded entries → polynomial degree explodes. Experiments suggest $T \in [0.26, 0.35]$ for $n = 8192, d = 64$ with $\sigma = 0.1$.

- **Design tradeoffs:**
  - **Single-threshold vs. Multi-threshold:** Single requires sub-Gaussian assumption but provides $\epsilon = 1/\text{poly}(n)$ accuracy. Multi-threshold removes assumptions but weakens accuracy to $\epsilon > 1/\text{poly}(n)$ with error scaling $\exp(B^2 - 2b^2)$.
  - **Polynomial degree $g$ vs. rank $r$:** Higher $g$ improves approximation but increases $r = C(2d+2g, 2d)$, slowing computation. Paper shows degree-6 Chebyshev nearly matches exact attention