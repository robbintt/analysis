---
ver: rpa2
title: 'OmniDPO: A Preference Optimization Framework to Address Omni-Modal Hallucination'
arxiv_id: '2509.00723'
source_url: https://arxiv.org/abs/2509.00723
tags:
- audio
- omni
- wang
- zhang
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses hallucinations in omni-modal large language
  models (OLLMs), where models overly rely on textual priors and neglect visual and
  audio inputs, or fail to capture interactions between video and audio. The authors
  propose OMNI DPO, a preference optimization framework that extends DPO with modality-specific
  objectives to explicitly encourage attention to visual and auditory evidence.
---

# OmniDPO: A Preference Optimization Framework to Address Omni-Modal Hallucination

## Quick Facts
- arXiv ID: 2509.00723
- Source URL: https://arxiv.org/abs/2509.00723
- Reference count: 40
- OmniDPO significantly reduces hallucinations in omni-modal models, improving F1-score by 5.82% on AVHBench

## Executive Summary
OmniDPO addresses hallucinations in omni-modal large language models (OLLMs) by extending Direct Preference Optimization (DPO) with modality-specific objectives. The framework targets two key issues: over-reliance on textual priors and insufficient attention to audio-visual interactions. By constructing preference pairs that explicitly reward models for using audio cues and remaining robust to degraded modalities, OmniDPO encourages more faithful multimodal reasoning. Experiments on Qwen2.5-Omni and MiniCPM-o-2.6 demonstrate significant improvements in hallucination resistance while maintaining or improving reasoning capabilities.

## Method Summary
OmniDPO extends standard DPO by incorporating two modality-specific loss terms that encourage models to pay attention to visual and auditory evidence. The framework uses two types of preference pairs: audio-video alignment pairs (comparing responses with and without audio context) and modality-robustness pairs (comparing clean inputs to degraded versions with Gaussian noise). These pairs are generated using Qwen2-Audio and Qwen2.5-VL as generators, then used to train OLLMs via fine-tuning. The total loss combines the standard DPO loss with modality-specific losses weighted by hyperparameters λ_V and λ_A, optimized to reduce hallucination while maintaining multimodal reasoning capabilities.

## Key Results
- OmniDPO improves average F1-score by 5.82% on AVHBench for Qwen2.5-Omni and 2.64% for MiniCPM-o-2.6
- Consistent gains in perception accuracy (PA) and hallucination resistance (HR) on CMM benchmark
- Maintains or improves reasoning performance on MMAU and MMMU benchmarks
- Ablation studies confirm benefits of joint audio-video optimization over single-modality approaches

## Why This Works (Mechanism)
OmniDPO works by explicitly training models to value and utilize multiple modalities through preference optimization. By creating pairs where the preferred response incorporates audio cues while the rejected response ignores them, the model learns to recognize audio as valuable evidence. Similarly, by comparing clean inputs to degraded versions, the model learns to maintain performance even when modalities are imperfect. The modality-specific loss terms directly penalize the model for ignoring or undervaluing these signals, creating a training signal that complements traditional supervised learning.

## Foundational Learning

**Direct Preference Optimization (DPO)**
- *Why needed:* Standard DPO optimizes for response quality but doesn't explicitly encourage multimodal attention
- *Quick check:* Verify base DPO implementation works on unimodal tasks before extending to multimodal

**Preference Pair Construction**
- *Why needed:* Quality of preference pairs determines training signal strength and direction
- *Quick check:* Manually inspect sample pairs to ensure audio-video alignment is meaningful

**Modality-specific Loss Functions**
- *Why needed:* Standard loss treats all responses equally without considering modality reliability
- *Quick check:* Verify loss gradients flow correctly through modality-specific components

**Gaussian Noise Injection**
- *Why needed:* Creates realistic degradation scenarios for training robustness
- *Quick check:* Visualize noise levels to ensure they're perceptible but not overwhelming

## Architecture Onboarding

**Component Map:** MSRVTT videos → Audio transcription → Preference pair generation → OmniDPO training → Fine-tuned OLLM

**Critical Path:** The generation of preference pairs is the critical path, as it determines the quality and quantity of training signals. Poor pair generation will undermine the entire framework regardless of fine-tuning quality.

**Design Tradeoffs:** OmniDPO trades computational complexity (additional loss terms and pair generation) for improved multimodal reasoning. The framework assumes base model visual encoders are competent, focusing optimization on alignment rather than fundamental perception.

**Failure Signatures:** 
- If only audio-driven tasks improve while video-driven tasks degrade, check λ_V hyperparameter
- If neither modality shows improvement, verify preference pair quality and generator model performance
- If overall performance degrades, reduce training duration or learning rate

**3 First Experiments:**
1. Verify base DPO implementation on unimodal datasets before extending to multimodal
2. Test preference pair generation with small subset of MSRVTT to validate audio-video alignment quality
3. Conduct ablation study with λ_V=0 and λ_A=0 to confirm modality-specific losses provide benefit

## Open Questions the Paper Calls Out

None

## Limitations

- The framework assumes base model visual encoders are competent; cannot fix fundamental perception failures
- Specific noise magnitude (σ) for degradation is not specified, affecting reproducibility
- Training duration and batch size parameters are not disclosed, making exact reproduction difficult

## Confidence

**High Confidence:** The core methodology of extending DPO with modality-specific losses is clearly specified and theoretically sound
**Medium Confidence:** The reported improvements on AVHBench and CMM benchmarks are specific and measurable, though exact reproduction depends on implementation details
**Medium Confidence:** The ablation study showing benefits of joint audio-video optimization is well-designed, but the exact hyperparameter choices remain unclear

## Next Checks

1. **Noise Parameter Sensitivity:** Systematically vary σ in the noise injection process to determine the optimal noise level for generating robust training pairs
2. **Training Duration Impact:** Conduct experiments varying training epochs and batch sizes to identify the relationship between compute budget and performance gains
3. **Reference Model Configuration:** Test both frozen and unfrozen reference model variants to determine which configuration yields better performance and more stable training