---
ver: rpa2
title: Quantifying Risks in Multi-turn Conversation with Large Language Models
arxiv_id: '2510.03969'
source_url: https://arxiv.org/abs/2510.03969
tags:
- bounds
- query
- bound
- catastrophic
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a framework for quantifying catastrophic risks
  in multi-turn LLM conversations through statistical certification. It models conversations
  as Markov processes on query graphs, defining distributions over query sequences
  to evaluate risk.
---

# Quantifying Risks in Multi-turn Conversation with Large Language Models

## Quick Facts
- arXiv ID: 2510.03969
- Source URL: https://arxiv.org/abs/2510.03969
- Authors: Chengxiao Wang; Isha Chaudhary; Qian Hu; Weitong Ruan; Rahul Gupta; Gagandeep Singh
- Reference count: 40
- Primary result: Framework provides certified lower bounds on catastrophic risk ranging from 0.1% to 70%, revealing that context and distractors increase vulnerability in multi-turn LLM conversations

## Executive Summary
This paper introduces C³LLM, a framework for quantifying catastrophic risks in multi-turn LLM conversations through statistical certification. The framework models conversations as Markov processes on query graphs, sampling sequences under three distributions to compute confidence intervals on the probability of generating harmful responses. Results show that context significantly amplifies risks, with certified bounds reaching 70% for vulnerable models like Mistral-Large, while models like Claude-Sonnet-4 exhibit much lower bounds (1-11%). The approach provides statistically rigorous guarantees that fixed benchmarks cannot offer.

## Method Summary
The framework constructs a query graph where nodes are queries and edges encode semantic similarity. A Markov process on this graph generates query sequences, with three sampling distributions: Random Node (independent selection), Graph Path (follows edges with endpoint constraints), and Adaptive with Rejection (adjusts transitions based on model accept/reject responses). For each distribution, 50 sequences are sampled, executed against target LLMs, and responses are classified by a judge model (GPT-4o). Clopper-Pearson confidence intervals are computed on the catastrophic probability across samples, providing statistically rigorous bounds.

## Key Results
- Certified lower bounds on catastrophic risk range from 0.1% to 70% across models and scenarios
- Claude-Sonnet-4 and Nova Premier show lower bounds of 1-11%, while Mistral-Large and DeepSeek-R1 exhibit 55-82%
- Context and distractors increase vulnerability by reducing refusal likelihood and clarifying harmful targets
- Adaptive distribution with rejection mechanism reveals the highest certified risks for vulnerable models

## Why This Works (Mechanism)

### Mechanism 1: Markov Process on Query Graph Models Conversation Distributions
The framework represents multi-turn conversations as Markov processes on semantically-structured query graphs, enabling statistical certification of risk bounds across exponentially large conversation spaces. The graph construction uses embedding similarity thresholds to create edges, and the Markov process generates sequences without repetition by tracking visited sets. Three distributions sample from this space, allowing computation of catastrophic probabilities for any distribution.

### Mechanism 2: Clopper-Pearson Confidence Intervals Enable Statistical Guarantees
Independent sampling from conversation distributions, combined with Clopper-Pearson confidence intervals, provides statistically rigorous bounds on catastrophic risk that fixed-sequence benchmarks cannot offer. With 50 samples per specification and 95% confidence, this approach accounts for sampling variance and provides exact (conservative) intervals for binomial proportions.

### Mechanism 3: Context Accumulation and Distractor Queries Amplify Harmful Outputs
Multi-turn contexts with semantically-related preceding queries increase catastrophic response rates compared to single-turn prompts by providing "distractors" that reduce refusal likelihood and "context" that clarifies harmful targets. The Graph Path distribution with harmful target constraint exploits this by ensuring final queries are preceded by semantically coherent context.

## Foundational Learning

- **Concept: Markov Chains on Finite State Spaces**
  - Why needed here: The certification framework models conversation generation as a Markov process with states tracking current query and visited set. Understanding transition probabilities, stationary distributions, and absorbing states is essential to implement the sampling mechanisms correctly.
  - Quick check question: Given a graph with 5 nodes where each node connects to its 2 nearest neighbors, how would you compute the probability of generating a specific 3-node path under uniform transition probabilities?

- **Concept: Binomial Confidence Intervals (Clopper-Pearson)**
  - Why needed here: The framework's core output is confidence bounds on catastrophic risk probability. Clopper-Pearson provides exact (conservative) intervals for binomial proportions, which is what certification requires.
  - Quick check question: If you observe 15 catastrophic responses in 50 trials, what is the 95% Clopper-Pearson lower bound? Can you explain why it differs from the simple proportion $15/50 = 0.3$?

- **Concept: Graph Construction via Embedding Similarity**
  - Why needed here: The query graph's edge structure determines which sequences are possible under Graph Path and Adaptive distributions. Embedding similarity thresholds control graph density and semantic coherence.
  - Quick check question: If you set similarity threshold $l_{th} = 0.4, h_{th} = 0.8$, and most query pairs have similarity in $[0.3, 0.6]$, will the graph tend toward sparsity or density? How would this affect path-based sampling?

## Architecture Onboarding

- **Component map:**
  1. Query Set Generator: Expands harmful targets into related queries using actor-based prompts
  2. Graph Constructor: Builds undirected graph with edges where $l_{th} < \text{sim}(u,v) < h_{th}$ using all-MiniLM-L6-v2 embeddings
  3. Distribution Samplers: Three Markov-based samplers (Random Node, Graph Path, Adaptive with Rejection) generate query sequences
  4. Augmentation Layer: Optionally prepends jailbreak prefixes with probability $p$; supports mutation
  5. Execution Engine: Sends sequences to target LLM, collects responses
  6. Judge Module: GPT-4o classifies responses as catastrophic (1) or safe (0)
  7. Certification Calculator: Applies Clopper-Pearson to compute confidence intervals

- **Critical path:**
  Graph construction → Distribution specification → Sequence sampling → LLM execution → Judge classification → Interval computation. The Graph Path distributions require pre-computed target sets $Q_T$ (queries similar to harmful target); Adaptive requires per-query rejection indicators during execution.

- **Design tradeoffs:**
  - Sample count ($n=50$): Higher $n$ narrows confidence intervals but increases compute
  - Similarity thresholds ($l_{th}=0.4, h_{th}=0.8$): Wider range → denser graph → more possible paths but less semantic coherence
  - Judge model choice: GPT-4o provides automated classification but may have biases
  - Jailbreak probability ($p=0.2$): Higher values increase bound magnitudes for vulnerable models

- **Failure signatures:**
  - Bounds too wide (CI spans > 0.5): Likely insufficient samples or high variance
  - All models show near-zero bounds: Possible judge model miscalibration
  - Adaptive distribution shows lower bounds than Graph Path: May indicate transition weights poorly tuned
  - Graph path sampling terminates early: Graph too sparse or visited set constraint too aggressive

- **First 3 experiments:**
  1. Validate pipeline on single scenario: Pick one chemical/biological scenario, construct graph, run all four distributions with $n=50$, compare bounds to baseline single-turn attack success rate
  2. Ablate sample count: On 2-3 scenarios, vary $n \in \{20, 50, 100\}$ and plot confidence interval width
  3. Compare models across distributions: Run certification on Claude-Sonnet-4 and Mistral-Large for chembio scenarios, verify Mistral shows higher bounds

## Open Questions the Paper Calls Out

### Open Question 1
What safety training strategies can effectively reduce the certified catastrophic risk bounds revealed by C³LLM, particularly for models like Mistral-Large and DeepSeek-R1 that exhibit lower bounds exceeding 70%? The paper states its results "highlight the urgent need for improved safety training strategies in frontier LLMs" after demonstrating substantial vulnerabilities in tested models.

### Open Question 2
How robust are the certification bounds to the choice of judge model, given that GPT-4o serves as the sole judge in all experiments? Different judge models may classify responses differently, potentially shifting confidence intervals significantly.

### Open Question 3
How do certification bounds generalize to domains beyond chemical biological, cybercrime, and illegal categories, such as disinformation, financial fraud, or psychological manipulation? Evaluation covers only three HarmBench categories; the framework's applicability to other catastrophic risk domains remains untested.

### Open Question 4
Can more sophisticated augmentation strategies beyond jailbreak prefixes (e.g., context-dependent rewriting, multi-turn attack LLMs) reveal higher certified risk bounds? The paper notes "D_aug can be defined by more structured generators, such as a second LLM that chooses mutations based on the conversation context, as long as this generator is treated as part of the attack process."

## Limitations
- The framework's effectiveness depends critically on semantic similarity graphs accurately capturing realistic conversational flow patterns
- The judge model's reliability in identifying catastrophic responses without substantial false positive/negative rates remains untested
- The framework evaluates single-turn queries with context but doesn't model long-term state persistence or memory effects
- Limited validation corpus shows no prior work applying Clopper-Pearson confidence intervals to LLM safety

## Confidence
- **High Confidence**: Markov process construction and Clopper-Pearson interval computation are mathematically rigorous and correctly implemented
- **Medium Confidence**: Framework effectively demonstrates that context increases vulnerability for current LLM architectures
- **Low Confidence**: Framework's ability to predict real-world catastrophic risks is uncertain due to potential gaps in capturing all realistic adversarial strategies

## Next Checks
1. **Judge Model Validation**: Run the judge model on a held-out set of 100 manually labeled responses to measure false positive and false negative rates
2. **Graph Construction Sensitivity**: Systematically vary the similarity thresholds across ranges and measure how certification bounds change for vulnerable models
3. **Distribution Coverage Analysis**: For each scenario, enumerate all possible 5-step paths in the graph and compute their exact probabilities under each distribution to verify correct implementation