---
ver: rpa2
title: 'Feature Engineering for Agents: An Adaptive Cognitive Architecture for Interpretable
  ML Monitoring'
arxiv_id: '2506.09742'
source_url: https://arxiv.org/abs/2506.09742
tags:
- monitoring
- cama
- across
- feature
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces CAMA, a cognitive architecture that enhances\
  \ machine learning monitoring interpretability by applying feature engineering principles\
  \ to large language model (LLM) agents. The approach implements a Decision Procedure\
  \ with three steps\u2014Refactor, Break Down, and Compile\u2014to systematically\
  \ improve data representation, enable detailed analysis, and integrate actionable\
  \ insights."
---

# Feature Engineering for Agents: An Adaptive Cognitive Architecture for Interpretable ML Monitoring

## Quick Facts
- **arXiv ID:** 2506.09742
- **Source URL:** https://arxiv.org/abs/2506.09742
- **Reference count:** 19
- **Primary result:** CAMA cognitive architecture achieves up to 92.3% accuracy on ML monitoring tasks, significantly outperforming baselines across multiple LLMs and datasets

## Executive Summary
This paper introduces CAMA, a cognitive architecture that enhances machine learning monitoring interpretability by applying feature engineering principles to large language model (LLM) agents. The approach implements a Decision Procedure with three steps—Refactor, Break Down, and Compile—to systematically improve data representation, enable detailed analysis, and integrate actionable insights. CAMA leverages structured memory components to guide analysis, reducing dependence on LLM-generated planning and improving consistency. Experiments across multiple LLMs and three datasets show CAMA consistently outperforms baselines like Standard, Chain-of-Thought, and Reflection in accuracy, unknown ratio, and interpretability.

## Method Summary
CAMA implements a three-step Decision Procedure: Refactor gathers context from memory modules and structures data representations without LLM calls; Break Down analyzes each feature independently using parallel LLM calls with dynamic system prompts; Compile synthesizes final reports integrating insights. The architecture uses four memory types—Procedural (prompts/chains), Episodic (past events), Semantic (reference data), and Working (runtime state)—to guide analysis. Tested across llama-3.2-1b, llama3-8b, llama3-70b, and gpt-4o-mini models on synthetic datasets (Financial, Eligibility, Healthcare) with monitoring outputs from Alibi Detect.

## Key Results
- CAMA achieves up to 92.3% accuracy with llama3-70b, significantly exceeding Standard (28.2%), Chain-of-Thought (28.3%), and Reflection (28.2%) baselines
- Ablation study confirms each component's necessity: removing Refactor drops accuracy by 67.8 percentage points, removing Break Down by 65.2 points
- CAMA consistently reduces unknown ratio while maintaining or improving token efficiency compared to baseline methods
- Performance scales with model size, with smallest model (llama-3.2-1b) achieving >50% accuracy versus ~25% for baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structured feature preprocessing before LLM reasoning improves monitoring interpretability
- **Mechanism:** Refactor step gathers context from memory modules and structures data representations without LLM calls, ensuring LLM receives semantically organized inputs rather than raw monitoring logs
- **Core assumption:** LLMs perform better on structured, context-rich representations than on verbose raw outputs
- **Evidence anchors:** [abstract] "Refactor step improves data representation to better capture feature semantics"; [section 3.2.1] Figure 2 shows Refactor assembles dataset info with feature context using "No LLM calls"
- **Break condition:** If monitoring outputs are already highly structured or the domain has minimal feature complexity, Refactor's marginal benefit may diminish

### Mechanism 2
- **Claim:** Parallel feature-level decomposition reduces cognitive load and enables focused analysis
- **Mechanism:** Break Down step analyzes each feature independently using parallel LLM calls with dynamic system prompts, preventing information interference between features
- **Core assumption:** Feature-level isolation prevents reasoning errors that occur when LLMs process multiple features simultaneously
- **Evidence anchors:** [abstract] "Break Down decomposes complex information for detailed analysis"; [section 5.5] Ablation shows removing Break Down drops accuracy from 88.30% to 23.08%
- **Break condition:** When features have strong interdependencies that require joint analysis, independent decomposition may miss interaction effects

### Mechanism 3
- **Claim:** Externalizing planning into architecture reduces LLM planning inconsistency
- **Mechanism:** CAMA's Decision Procedure provides a fixed three-step pipeline that replaces LLM-generated planning, with architecture determining what to analyze and when
- **Core assumption:** Architectural planning is more reliable than emergent LLM planning for structured analytical tasks
- **Evidence anchors:** [abstract] "more deterministic planning approach, reducing dependence on LLM-generated planning"; [section 5.6] "Feature engineering-inspired approach... handles complex scenarios and evolving data effectively"
- **Break condition:** For novel task types outside ML monitoring, the fixed pipeline may lack flexibility compared to adaptive planning approaches

## Foundational Learning

- **Concept:** Feature Engineering Fundamentals
  - **Why needed:** CAMA applies feature engineering principles (transformation, selection, representation) to agent design
  - **Quick check:** Can you explain why normalizing numerical features and encoding categorical features improves downstream model performance?

- **Concept:** Memory Systems in Cognitive Architectures
  - **Why needed:** CAMA uses four memory types (Procedural, Episodic, Semantic, Working) from cognitive science
  - **Quick check:** What is the functional difference between Episodic Memory (specific past events) and Semantic Memory (generalized knowledge)?

- **Concept:** ML Monitoring Metrics (Drift, SHAP)
  - **Why needed:** CAMA interprets outputs from monitoring tools like Alibi Detect
  - **Quick check:** If a feature shows high drift but low SHAP importance, what might this indicate about model retraining priorities?

## Architecture Onboarding

- **Component map:** Monitoring Logs + Dataset → [Refactor: No LLM] → Structured Representation → [Break Down: Parallel LLM calls per feature] → [Compile: LLM synthesizes final report] → Output → Episodic Memory
- **Supporting:** Semantic Memory (reference data), Procedural Memory (prompts/chains), Working Memory (runtime state)
- **Critical path:** Refactor → Break Down → Compile. Ablation shows removing Refactor causes 67.8 percentage point drop; removing Break Down causes 65.2 point drop
- **Design tradeoffs:** Token efficiency vs. accuracy (19-25 tokens vs. 4-5 for Standard but 30-60 point accuracy gains); Parallelism vs. feature interdependence (parallel Break Down scales well but may miss feature interactions); Fixed pipeline vs. adaptability (deterministic planning improves consistency but reduces flexibility)
- **Failure signatures:** High "Unknown Ratio" (>50%) indicates Semantic Memory initialization issues; low accuracy with small models (<1B parameters) suggests Break Down prompts too complex; inconsistent reports across runs indicates temperature=0 issues or improper Working Memory clearing
- **First 3 experiments:** 1) Baseline comparison: Run Standard, CoT, and CAMA on single dataset with llama3-8b, compare accuracy/unknown ratio to reproduce 90.6% vs. 28.2% gap; 2) Ablation validation: Remove Refactor step and measure accuracy drop (~65-70 points); 3) Model size scaling: Test CAMA with smallest available model (llama-3.2-1b) on all three datasets to verify architecture provides value even with limited LLM capacity

## Open Questions the Paper Calls Out
- Expanding evaluation across broader range of LLM architectures
- Investigating computational requirements and processing times compared to simpler approaches
- Exploring integration with more diverse monitoring tools and real-world datasets

## Limitations
- Results based on synthetic datasets rather than real-world production monitoring data
- Evaluation methodology relies on GPT-4o as automated judge, potentially misaligning with human expert assessment
- Computational overhead and latency of three-step pipeline not fully characterized against simpler methods

## Confidence
- **High Confidence:** Architectural design and three-step pipeline are well-specified and reproducible; ablation study results are robust
- **Medium Confidence:** Accuracy improvements over baselines are likely valid within experimental setup but may diminish in real-world scenarios
- **Low Confidence:** Generalizability to different monitoring tools, dataset distributions, and LLM capabilities beyond those tested

## Next Checks
1. **Real-world deployment test:** Apply CAMA to monitoring data from active production ML system and compare accuracy against human expert analysis on subset of cases
2. **Tool independence validation:** Replace Alibi Detect with different drift detection library (e.g., Evidently AI) and measure if CAMA maintains accuracy advantage
3. **Cross-domain generalization:** Test CAMA on non-financial dataset (e.g., image classification monitoring with CIFAR-10) to verify architecture works beyond tabular data