---
ver: rpa2
title: 'EssayBench: Evaluating Large Language Models in Multi-Genre Chinese Essay
  Writing'
arxiv_id: '2506.02596'
source_url: https://arxiv.org/abs/2506.02596
tags:
- essay
- evaluation
- writing
- essays
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ESSAYBENCH, a multi-genre benchmark for
  evaluating large language models (LLMs) on Chinese essay writing across four genres:
  Argumentative, Narrative, Descriptive, and Expository. The authors curate 728 real-world
  prompts and develop a fine-grained, genre-specific scoring framework that hierarchically
  aggregates evaluation traits with dependency-based weighting.'
---

# EssayBench: Evaluating Large Language Models in Multi-Genre Chinese Essay Writing

## Quick Facts
- arXiv ID: 2506.02596
- Source URL: https://arxiv.org/abs/2506.02596
- Authors: Fan Gao; Dongyuan Li; Ding Xia; Fei Mi; Yasheng Wang; Lifeng Shang; Baojun Wang
- Reference count: 40
- One-line primary result: A multi-genre benchmark (728 prompts, 4 genres) for evaluating LLMs on Chinese essay writing using genre-specific, dependency-weighted scoring shows state-of-the-art models perform well on argumentative/expository but struggle with narrative/descriptive, especially in open-ended tasks.

## Executive Summary
This paper introduces ESSAYBENCH, a comprehensive benchmark for evaluating large language models (LLMs) on Chinese essay writing across four genres: Argumentative, Narrative, Descriptive, and Expository. The authors curate 728 real-world prompts and develop a fine-grained, genre-specific scoring framework that hierarchically aggregates evaluation traits with dependency-based weighting. Human agreement studies demonstrate high alignment between the proposed evaluation method and human judgments, with Spearman’s ρ up to 0.816 and Kendall’s τ up to 0.704. The framework also significantly improves sensitivity in distinguishing essays of varying quality. Benchmarking 15 LLMs reveals that state-of-the-art models perform well on argumentative and expository essays but struggle with narrative and descriptive genres, especially in open-ended prompts.

## Method Summary
ESSAYBENCH evaluates LLMs using a hierarchical, genre-specific framework with six evaluation traits per genre, each scored on a 10-point scale via sub-questions. Scores are aggregated using dependency-based weighting (W_t = α^d, where d is trait depth, α=3). Evaluation is performed by an LLM judge (DeepSeek-R1, GPT-4o) using Chain-of-Thought prompting. The benchmark includes 728 Chinese essay prompts categorized into four genres and two constraint types (Open-Ended vs. Constrained). Human alignment is validated through pairwise comparisons by professional annotators (14 annotators, 5,040 labeled pairs).

## Key Results
- Spearman’s ρ up to 0.816 and Kendall’s τ up to 0.704 between proposed method and human judgments.
- Dependency-based score aggregation improves performance by approximately 2% over unweighted averaging.
- State-of-the-art models perform well on argumentative and expository essays but struggle with narrative and descriptive genres, especially in open-ended prompts.
- Mean Difference (M D) consistently higher for proposed method (e.g., 3.49 vs 2.79 for DeepSeek-R1) than unweighted version.

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Trait Aggregation
The framework assigns weights $W_t = \alpha^d$ (where $d$ is depth) to distinct traits, explicitly prioritizing advanced or higher-level structural features over basic compliance. This prevents a "medium" essay from scoring highly simply by satisfying low-level constraints (e.g., length, grammar). The core assumption is that advanced writing traits are hierarchically dependent and contribute non-linearly to the perception of overall quality. Evidence shows dependency-based score aggregation improves performance by approximately 2%, and the Mean Difference for the proposed method is consistently higher than the unweighted version, indicating better separation of quality tiers.

### Mechanism 2: Genre-Specific Decomposition
By defining unique evaluation dimensions for each genre, the prompt forces the Judge LLM to reason about specific rhetorical constraints rather than general coherence. This forces a distinction between a fluent but "empty" essay and a structurally sound one. The core assumption is that an LLM can reliably identify and score specific rhetorical devices when explicitly prompted with sub-questions. The framework achieves higher alignment in Narrative and Descriptive genres, which are typically harder for general metrics to assess.

### Mechanism 3: Constraint-Guided Generation (Performance Divergence)
LLMs perform significantly better on "Constrained" prompts (explicit topic/length) than "Open-Ended" prompts due to the scaffolding provided by external constraints. Constrained prompts reduce the planning horizon and semantic ambiguity for the LLM. The core assumption is that the performance gap is inherent to current Transformer architectures which rely heavily on context for guidance. Results show higher scores for "Cons." (Constrained) sets across almost all models compared to "Open" (Open-Ended).

## Foundational Learning

- **Concept: Spearman’s $\rho$ and Kendall’s $\tau$**
  - **Why needed here:** These are the primary metrics used to validate the evaluation framework. Without understanding rank correlation, one cannot verify if the "Judge LLM" is actually agreeing with humans or just guessing.
  - **Quick check question:** If Spearman’s $\rho$ is 0.816 (Table 2), does this measure the absolute accuracy of the score or the rank ordering of essay quality? (Answer: Rank ordering).

- **Concept: Chain-of-Thought (CoT) Evaluation**
  - **Why needed here:** The paper relies on CoT to force the Judge LLM to produce evidence before scoring. Understanding CoT is necessary to implement the "Judge" component correctly.
  - **Quick check question:** Why does the prompt (Appendix C) require the model to output "evidence-inference" before the score? (Answer: To ground the score in text and reduce hallucination).

- **Concept: Automated Essay Scoring (AES)**
  - **Why needed here:** This paper situates itself as an evolution of AES. Understanding that AES traditionally struggled with "creative" or "narrative" genres contextualizes the benchmark's design.
  - **Quick check question:** Why do traditional AES features (grammar, length) fail specifically for Narrative and Descriptive essays? (Answer: They cannot capture plot, imagery, or atmosphere).

## Architecture Onboarding

- **Component map:**
  1. **Prompt Repository:** 728 curated prompts split into 4 Genres × 2 Constraint types (Open/Constrained).
  2. **Generator:** Any target LLM (e.g., GPT-4o, Llama) writing the essay.
  3. **Evaluator (The Core):** A "Judge LLM" (e.g., DeepSeek-R1) equipped with genre-specific Rubrics, Sub-questions, and Depth-based Weighting Logic (α=3).

- **Critical path:**
  1. **Curation:** Filtering raw prompts (K-means, ROUGE-L) to build the dataset.
  2. **Evaluation:** Running the "Judge LLM" on generated essays using the specific genre prompts (Figure 6).
  3. **Aggregation:** Computing W_t = α^d and summing for the final score.

- **Design tradeoffs:**
  - **Judge Selection:** The paper uses GPT-4o and DeepSeek-R1 as judges. R1 aligns better (Table 2) but is likely more expensive/slower. GPT-4o is the default choice in Section 5.1 for cost/performance balance.
  - **Granularity vs. Noise:** Fine-grained sub-questions improve sensitivity (Table 3) but increase the risk of the Judge LLM getting confused or inconsistent if the prompt is too long.

- **Failure signatures:**
  - **Low Sensitivity:** High standard deviation overlap between "High" and "Medium" essays (Table 3).
  - **Genre Collapse:** An LLM scoring well on Argumentative but failing Narrative (common failure mode seen in Table 5/Figure 5).
  - **Length Bias:** An evaluation that correlates purely with word count rather than content quality (mitigated here by genre traits).

- **First 3 experiments:**
  1. **Sanity Check (Ranking Agreement):** Select 10 essays of known quality (Low/Med/High). Run your evaluator. Check if Spearman’s ρ matches the paper's baseline (~0.7+). If the order is random, the Judge prompt is implemented incorrectly.
  2. **Sensitivity Stress Test:** Generate essays using a strong model (e.g., GPT-4o) and a weak model (e.g., GPT-3.5). Verify that the evaluation method produces a statistically significant Mean Difference (M D) as seen in Table 3.
  3. **Ablation on Weights:** Run the evaluation with α=1 (simple average) vs. α=3 (weighted). Confirm that the weighted version aligns better with a small set of human-labeled "gold" data.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the genre-specific evaluation framework be effectively adapted for multilingual essay writing, particularly for languages with distinct rhetorical traditions like English or Japanese?
- **Basis in paper:** [explicit] The authors state in the Limitations section that while the framework is applicable to other languages, the current focus is solely on Chinese due to linguistic differences in idioms and conventions.
- **Why unresolved:** Cultural and rhetorical differences may alter the hierarchical dependencies of evaluation traits defined for Chinese essays.
- **What evidence would resolve it:** A study applying the ESSAYBENCH framework to English or Japanese datasets and measuring the resulting alignment with human judgments.

### Open Question 2
- **Question:** Does incorporating fine-grained analysis at the lexical and sentence levels significantly improve the correlation with human judgments compared to the current paragraph-level approach?
- **Basis in paper:** [explicit] The paper notes in the Limitations section that the evaluation traits primarily focus on overall expression and structure, overlooking granular analyses at the lexical and sentence levels.
- **Why unresolved:** It remains unclear if human evaluators rely more on micro-level features (e.g., sentence coherence) than the macro-level traits currently modeled by the framework.
- **What evidence would resolve it:** An ablation study adding word-level richness and sentence-level coherence metrics to the scoring rubric to observe changes in Spearman’s ρ.

### Open Question 3
- **Question:** How would the inclusion of an explicit "instruction-following" dimension affect the final scores and rankings of LLMs on constrained writing tasks?
- **Basis in paper:** [explicit] The authors acknowledge in the Limitations section that the benchmark focuses on overall quality but overlooks strict instruction-following abilities, such as adhering to specific prompt constraints.
- **Why unresolved:** High-quality essays that fail to follow specific constraints (e.g., target audience or word count) may be scored inappropriately high by the current methodology.
- **What evidence would resolve it:** Re-evaluating the benchmark results using a modified protocol that penalizes deviation from explicit prompt constraints.

## Limitations
- The evaluation framework relies heavily on a single Judge LLM (DeepSeek-R1) for validation, raising concerns about generalizability.
- The prompt dataset, while diverse, is not publicly available, limiting reproducibility.
- The depth-based weighting heuristic (α=3) is based on empirical tuning rather than theoretical derivation, raising questions about generalizability to other domains.

## Confidence
- **High confidence:** The framework's superiority over simple averaging (Table 3) and its sensitivity to quality differences are well-supported by statistical tests.
- **Medium confidence:** The genre-specific decomposition improves evaluation for Narrative and Descriptive essays, but the mechanism's robustness across different Judge LLMs needs validation.
- **Medium confidence:** The performance divergence between Open and Constrained prompts is consistently observed, but the underlying cognitive explanation for LLMs remains speculative.

## Next Checks
1. Replicate the sensitivity analysis with a different Judge LLM (e.g., Claude 3.5) to verify that the depth-based weighting consistently outperforms simple averaging across models.
2. Conduct a cross-cultural validation using English prompts to test whether the genre-specific decomposition and depth weighting generalize beyond Chinese writing conventions.
3. Implement an ablation study varying α from 1.5 to 5.0 to empirically determine the optimal weighting parameter and test the sensitivity of the framework to this hyperparameter.