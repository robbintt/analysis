---
ver: rpa2
title: 'LUT-LLM: Efficient Large Language Model Inference with Memory-based Computations
  on FPGAs'
arxiv_id: '2511.06174'
source_url: https://arxiv.org/abs/2511.06174
tags:
- lut-llm
- quantization
- memory
- lookup
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LUT-LLM, the first FPGA accelerator for 1B+
  language model inference using memory-based computation via vector quantization.
  Instead of traditional arithmetic operations, LUT-LLM employs lookup tables storing
  pre-computed dot products, enabled by quantization of both weights and activations.
---

# LUT-LLM: Efficient Large Language Model Inference with Memory-based Computations on FPGAs

## Quick Facts
- arXiv ID: 2511.06174
- Source URL: https://arxiv.org/abs/2511.06174
- Authors: Zifan He; Shengyu Ye; Rui Ma; Yang Wang; Jason Cong
- Reference count: 40
- Primary result: Achieves 1.66× lower latency than AMD MI210 GPU and 1.72× higher energy efficiency than NVIDIA A100 GPU for LLM inference on FPGAs

## Executive Summary
This paper presents LUT-LLM, the first FPGA accelerator for large language model inference using memory-based computation via vector quantization. Instead of traditional arithmetic operations, LUT-LLM employs lookup tables storing pre-computed dot products, enabled by quantization of both weights and activations. The design includes bandwidth-aware parallel centroid search, 2D table lookup with prefix-sum, and a spatial-temporal hybrid execution strategy. Implemented on an AMD V80 FPGA for a customized Qwen 3 1.7B model, LUT-LLM demonstrates significant performance advantages over GPU baselines while scaling to 32B models.

## Method Summary
LUT-LLM implements memory-based LLM inference through activation-weight co-quantization, where both weights and activations are mapped to centroids and pre-computed dot products stored in 2D lookup tables. The training involves a two-stage strategy with reconstruction loss ratio 0.1 and Straight-Through Estimator, requiring custom forward kernels (lookup table gather reduce) and fused backward kernels to handle >1B models. Post-processing reconstructs weights from trained LUTs, applies GPTVQ for weight quantization, then pre-computes final 2D lookup tables. The hardware architecture features a LUTLinear engine with Bandwidth-aware Parallel Centroid Search (BPCS) and 2D LUT Prefix-sum Engine, integrated with dataflow attention engines and implemented using Vitis HLS 2024.2, TAPA, and RapidStream for floorplanning.

## Key Results
- 1.66× lower latency than AMD MI210 GPU for customized Qwen 3 1.7B model
- 1.72× higher energy efficiency than NVIDIA A100 GPU
- Scales to 32B models with 2.16× efficiency gain over A100 GPU
- Achieves 227 MHz operation with 81% CLB utilization on AMD V80 FPGA

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Activation-weight co-quantization with 2D lookup tables achieves higher throughput than weight-only or activation-only vector quantization across both prefill and decode stages.
- **Mechanism**: Pre-computes dot products between all activation centroids and weight centroids, storing results in 2D tables. At inference, finds the nearest activation centroid index, retrieves the corresponding table row, and uses weight centroid indices to extract values for SIMD accumulation. Multiple weight vectors mapping to the same centroid reduces total table entries.
- **Core assumption**: Weight vectors cluster sufficiently such that shared centroids meaningfully reduce table size without excessive accuracy loss.
- **Evidence anchors**:
  - [abstract] "activation-weight co-quantization as the most effective scheme"
  - [section] Figure 6 shows co-quantization achieving superior normalized throughput across all sequence lengths; Section 3.1 provides latency equations showing co-quantization reduces T_mem from 8256 to 569 cycles in the example
  - [corpus] TeLLMe v2 applies related table-lookup matmul on FPGAs, suggesting approach generalizes beyond this specific implementation
- **Break condition**: If c_w or c_a increase substantially (e.g., c_w > 64, c_a > 256), 2D table sizes grow as c_w × c_a per group, potentially exceeding on-chip URAM/BRAM capacity.

### Mechanism 2
- **Claim**: Bandwidth-aware parallel centroid search (BPCSU) hides search latency by matching pipeline depth to available memory bandwidth.
- **Mechanism**: Distance PEs arranged in multiple parallel pipeline chains (not full binary tree). Each chain computes local minimum; small reduction tree finds global minimum. Pipeline depth (l) co-optimized with bandwidth so that centroid search latency ≤ table loading latency, enabling full overlap.
- **Core assumption**: Chebyshev distance is an acceptable proxy for Euclidean distance in centroid matching (reduces comparison hardware).
- **Evidence anchors**:
  - [section] Equation 12 formulates the optimization: table load time ≥ centroid load + pipeline depth + reduction tree depth
  - [section] "For Qwen 3 1.7B with c_a=64, c_w=16: l=16, meaning 16×4 dPE array with 2-level reduction tree"
  - [corpus] T-MAC (CPU) uses similar centroid search but lacks bandwidth-aware co-design; BPCSU appears novel for FPGA memory bandwidth matching
- **Break condition**: If c_a grows significantly without proportional bandwidth increase, the inequality fails—search latency becomes exposed.

### Mechanism 3
- **Claim**: Spatial-temporal hybrid execution saves 14% on-chip buffers while maintaining throughput by balancing sequential and dataflow patterns.
- **Mechanism**: LUTLinear engine executes sequentially across operations (enables pipelined nearest-neighbor search without codebook reloads), while outputs stream to attention/SwiGLU/LayerNorm via dataflow. This avoids pure-dataflow codebook reload overhead and pure-sequential memory partitioning costs.
- **Core assumption**: Linear layers dominate decode and short-prefill latency, so optimizing their buffer allocation yields end-to-end gains.
- **Evidence anchors**:
  - [section] Figure 10 visualizes three execution patterns; Section 4.4 states "saving 14% on-chip resources for high-throughput table lookups"
  - [section] "LUTLinear engine iterates over input vectors along sequence dimension, sustaining perfectly pipelined nearest-neighbor search"
  - [corpus] InTAR and SSR (cited) use spatial-temporal patterns for arithmetic compute; application to memory-based LUT engines is novel here
- **Break condition**: If attention layers dominate (very long sequences), dataflow attention may need more resources, reducing LUTLinear buffer gains.

## Foundational Learning

- **Concept: Vector Quantization (VQ)**
  - Why needed here: Core enabling technique—groups matrix elements into vectors, maps each to nearest centroid index, enables compact codebooks and lookup tables instead of full-precision weights.
  - Quick check question: Given vectors [(1,3), (2,4), (5,6)] and centroids [(2,3), (4,5)], which centroid index does each vector map to using Chebyshev distance?

- **Concept: FPGA Memory Hierarchy (BRAM, URAM, LUTRAM)**
  - Why needed here: 2D LUT buffers use mixed memory types; understanding port limits, capacity, and access patterns determines feasible table sizes and parallelism.
  - Quick check question: Why use URAM over BRAM for storing large 2D lookup tables? What's the tradeoff in port count?

- **Concept: LLM Inference Stages (Prefill vs Decode)**
  - Why needed here: Performance model and accelerator explicitly optimize for both stages with different compute/memory characteristics; bandwidth bottlenecks differ between them.
  - Quick check question: In which stage is KV-cache read bandwidth the primary bottleneck? How does sequence length affect this?

## Architecture Onboarding

- **Component map**:
  - LUTLinear Engine -> BPCSU (distance PEs + reduction tree) -> 2D LUT PSum Engine (row extraction + value copy MUX + SIMD adder) -> Accumulator -> Dequantizer
  - LUTLinear Engine -> Global Buffer -> Dataflow Attention Engine (QK^T GEMM/GEMV + softmax(QK^T)V GEMV)
  - Global Buffer -> SFUs (SwiGLU and LayerNorm processing units)

- **Critical path**:
  Decode: HBM read (LUT + weight indices) || BPCSU centroid search → 2D LUT row extract → value copy → SIMD accumulate → cascade → dequantize → attention GEMV → KV prefetch/write

- **Design tradeoffs**:
  - c_a ↑: More accurate but larger tables, longer search
  - c_w ↑: More accurate but table size grows as c_w × c_a per group
  - v ↑: Fewer tables but coarser quantization
  - Pipeline depth l: Deeper hides latency but increases dPE count

- **Failure signatures**:
  - Table size > on-chip memory: Reduce c_w/c_a or increase group size G
  - BPCSU latency exposed (T_search > T_load): Reduce pipeline depth l or increase parallel chains
  - CLB utilization > 80% with low BRAM use: Routing congestion—use floorplanning (RapidStream)
  - Decode bandwidth unsaturated: Check HBM channel alignment and 2D LUT PSum parallelism

- **First 3 experiments**:
  1. **Quantization sensitivity**: Train with c_a∈{32,64,128}, c_w∈{8,16,32}, measure accuracy drop on GLUE/SQuAD and resulting table sizes.
  2. **BPCSU depth validation**: For target HBM bandwidth (227 GB/s), solve equation 12 for max l; verify in cycle-accurate simulation that search latency < table load latency.
  3. **Memory partition sweep**: Implement 2D LUT PSum with varying BRAM/URAM ratios; measure throughput degradation from port conflicts at each configuration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the manual design optimization required for LUT-LLM be fully automated to support diverse model architectures and quantization schemes?
- Basis in paper: [explicit] Section 5.3.4 states, "LUT-LLM is also parametrized... but the optimization requires manual design. We leave the automation as the future work."
- Why unresolved: The current design flow requires manual intervention to tune parameters for different models, limiting rapid deployment.
- What evidence would resolve it: A compiler framework that automatically generates the optimized hardware datapoint from a high-level model description without manual parameter tuning.

### Open Question 2
- Question: To what extent can integrating low-bit attention and low-bit centroid search further enhance the energy efficiency of LUT-LLM?
- Basis in paper: [explicit] Section 6 identifies "emerging algorithmic advances, such as low-bit attention and low-bit centroid search," as a specific direction for future efficiency gains.
- Why unresolved: The current design uses FP32 for attention and standard centroid search; the hardware overhead and accuracy impact of low-bit alternatives in this specific memory-based architecture are unknown.
- What evidence would resolve it: Comparative benchmarks of LUT-LLM implementing low-bit attention blocks and search mechanisms, measuring the trade-off between accuracy loss and latency/energy reduction.

### Open Question 3
- Question: How can the training algorithms be modified to support context windows exceeding 512 tokens without exhausting GPU memory?
- Basis in paper: [inferred] Section 5.1 notes that "training memory constraints" currently limit the vector-quantized model to a 512-token context window.
- Why unresolved: Modern LLM applications typically require context windows of 4k to 128k tokens; the current memory footprint of full-precision lookup table tuning prohibits this.
- What evidence would resolve it: A modified training kernel (e.g., gradient checkpointing or distributed tuning) that successfully trains the activation quantization for sequence lengths greater than 4096 tokens on a comparable hardware cluster.

## Limitations

- Critical architectural details such as buffer sizing strategies, specific interface widths, and on-chip memory partitioning schemes are not fully disclosed
- Custom kernel implementations for "lookup table gather reduce" and "fused backward kernels" are essential for training >1B models but lack sufficient specification
- Scalability claims to 32B models rely on extrapolation without experimental validation
- Energy efficiency measurements depend on specific power measurement methodology that isn't fully detailed

## Confidence

- **High Confidence**: The core mechanism of activation-weight co-quantization with 2D lookup tables is well-supported by quantitative comparisons showing superior throughput across prefill and decode stages
- **Medium Confidence**: Bandwidth-aware parallel centroid search optimization is theoretically sound and the co-optimization equation appears valid, though implementation details could affect real-world performance
- **Medium Confidence**: Spatial-temporal hybrid execution claims are supported by resource savings figures, but the 14% buffer reduction may vary significantly with different model architectures and sequence lengths

## Next Checks

1. **Quantization Sensitivity Analysis**: Implement the training pipeline with varying centroid counts (c_a ∈ {32,64,128}, c_w ∈ {8,16,32}) and measure both accuracy degradation on GLUE/SQuAD benchmarks and resulting 2D table sizes to validate the claimed tradeoff curve

2. **BPCSU Latency Verification**: For the AMD V80's HBM bandwidth (227 GB/s), solve the BPCSU optimization equation to determine maximum pipeline depth l, then validate in cycle-accurate simulation that centroid search latency consistently remains below table loading latency

3. **Memory Partitioning Performance Sweep**: Implement the 2D LUT PSum engine with systematically varied BRAM/URAM ratios (e.g., 100% BRAM, 50/50 split, 100% URAM) and measure throughput degradation from port conflicts at each configuration to verify the claimed memory hierarchy benefits