---
ver: rpa2
title: Automatic Essay Scoring and Feedback Generation in Basque Language Learning
arxiv_id: '2512.08713'
source_url: https://arxiv.org/abs/2512.08713
tags:
- feedback
- latxa
- scores
- evaluation
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first publicly available dataset for Automatic
  Essay Scoring (AES) and feedback generation in Basque, targeting CEFR C1 proficiency.
  The dataset contains 3,200 essays with expert annotations including criterion-specific
  scores, detailed feedback, and error examples across correctness, richness, coherence,
  cohesion, and task alignment.
---

# Automatic Essay Scoring and Feedback Generation in Basque Language Learning

## Quick Facts
- arXiv ID: 2512.08713
- Source URL: https://arxiv.org/abs/2512.08713
- Reference count: 0
- First publicly available dataset for AES and feedback generation in Basque targeting CEFR C1 proficiency

## Executive Summary
This paper introduces the first publicly available dataset for Automatic Essay Scoring (AES) and feedback generation in Basque, containing 3,200 CEFR C1-level essays with expert annotations across five evaluation criteria. The study fine-tunes open-source models including RoBERTa-EusCrawl and Latxa (8B/70B) for both scoring and explanation generation, demonstrating that encoder models remain competitive while Latxa with supervised fine-tuning significantly outperforms state-of-the-art closed-source systems like GPT-5 and Claude Sonnet 4.5. The research establishes a foundation for transparent, reproducible NLP research in low-resource languages through novel evaluation methodologies combining automatic consistency metrics with expert-based validation.

## Method Summary
The research develops a comprehensive dataset of 3,200 Basque essays annotated with expert scores and feedback across five criteria: Correctness, Richness, Coherence, Cohesion, and Task Alignment. Multiple open-source models were fine-tuned including RoBERTa-EusCrawl, and Latxa variants (8B/70B) using supervised fine-tuning approaches. The evaluation combines automatic consistency metrics with manual expert validation to assess both scoring accuracy and feedback quality. The study specifically addresses low-resource language challenges by leveraging local language models and creating transparent evaluation frameworks.

## Key Results
- Fine-tuned Latxa models surpass GPT-5 and Claude Sonnet 4.5 in scoring consistency and feedback quality
- Supervised fine-tuning significantly improves Latxa's performance across scoring and explanation generation tasks
- Encoder models like RoBERTa-EusCrawl remain highly competitive for AES tasks despite advances in large language models
- Novel evaluation methodology combining automatic metrics with expert validation effectively measures pedagogical value

## Why This Works (Mechanism)
The success stems from combining high-quality expert-annotated data with appropriately fine-tuned local language models that understand Basque linguistic nuances. Supervised fine-tuning enables models to learn specific scoring patterns and feedback generation techniques aligned with CEFR C1 proficiency standards. The multi-criteria evaluation approach captures different aspects of language proficiency, while the integration of error examples in feedback provides concrete learning opportunities for students.

## Foundational Learning
- **CEFR Proficiency Levels**: Framework for describing language ability (A1-C2); needed to standardize evaluation criteria and ensure consistent expert annotations across essays
- **Basque Language NLP**: Challenges of processing low-resource languages with limited digital resources; quick check: compare model performance on Basque vs high-resource languages
- **Supervised Fine-Tuning**: Adapting pre-trained models to specific tasks using labeled data; quick check: measure performance gains from different amounts of fine-tuning data
- **Feedback Generation**: Creating pedagogically useful explanations that help learners improve; quick check: expert evaluation of feedback clarity and actionability
- **Automatic Evaluation Metrics**: Quantitative measures for assessing model performance consistency; quick check: correlation between automatic and human scores
- **OCR Error Impact**: How digitization artifacts affect model training and evaluation; quick check: performance comparison with clean vs OCR-affected text

## Architecture Onboarding

**Component Map**: Dataset Creation -> Model Fine-tuning -> Scoring Generation -> Feedback Generation -> Expert Validation -> Performance Evaluation

**Critical Path**: Dataset (3,200 annotated essays) → Supervised Fine-tuning (Latxa 8B/70B) → Scoring & Feedback Generation → Expert Validation (manual error extraction and quality assessment) → Performance Comparison (vs GPT-5, Claude)

**Design Tradeoffs**: Local models (Latxa) offer transparency and control but require significant computational resources for fine-tuning, while closed-source models provide convenience but lack reproducibility and may have prompting inconsistencies

**Failure Signatures**: Over-reliance on spelling/vocabulary errors due to OCR artifacts, difficulty generalizing to abstract criteria like Coherence, potential bias from limited proficiency level focus

**3 First Experiments**:
1. Fine-tune Latxa 8B on Correctness criterion with varying amounts of training data to identify optimal sample size
2. Compare encoder-only (RoBERTa) vs decoder models (Latxa) on scoring consistency metrics
3. Evaluate feedback quality using both automatic metrics and expert manual validation on a subset of essays

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the supervised fine-tuning approach generalize to more abstract criteria like Richness and Coherence, given that current experiments were limited to Correctness?
- Basis in paper: Authors state they "plan to expand our experimental analysis... to all evaluation criteria beyond Correctness."
- Why unresolved: The study focused on Correctness because it is the most objective and contains the most error examples; it is unknown if SFT benefits persist for subjective criteria with sparser error annotations.
- What evidence would resolve it: Benchmarking the SFT Latxa model on the remaining four criteria (Richness, Coherence, Cohesion) using the proposed consistency and manual evaluation metrics.

### Open Question 2
- Question: Does the fine-tuned Latxa model outperform proprietary models in error recall and pedagogical value, as opposed to just extraction accuracy?
- Basis in paper: The authors explicitly note that their manual evaluation "do not analyze the pedagogical significance of error-examples nor compute the recall of the error-examples."
- Why unresolved: Current results measure precision (extraction accuracy), but a model could achieve high precision while missing a large number of learner errors (low recall) or providing unhelpful feedback.
- What evidence would resolve it: A human evaluation study designed to measure the recall of identified errors and rate the pedagogical usefulness of the generated explanations.

### Open Question 3
- Question: To what extent do the OCR artifacts present in the digitized essays bias the models toward identifying spelling and vocabulary errors rather than structural learner errors?
- Basis in paper: The paper notes that GPT-5 focused on spelling/vocabulary "probably due to OCR errors" and acknowledges a 3.01% Character Error Rate influenced the evaluation.
- Why unresolved: It is unclear if the error category distributions observed (e.g., the dominance of spelling errors for GPT-5) reflect genuine learner proficiency or the model's sensitivity to dataset digitization noise.
- What evidence would resolve it: A comparative analysis of error extraction performance on the current dataset versus a version where OCR artifacts have been manually corrected.

## Limitations
- Dataset focuses exclusively on Basque CEFR C1 proficiency, limiting generalizability to other levels and languages
- Expert annotation introduces potential subjectivity in scoring consistency and feedback quality assessments
- Comparison with closed-source models may be affected by differences in access conditions and evaluation methodologies
- OCR artifacts present in digitized essays may bias models toward identifying surface-level errors over structural issues

## Confidence
- High confidence in dataset creation and annotation methodology following established CEFR guidelines
- Medium confidence in comparative performance claims against closed-source models due to potential evaluation condition differences
- Medium confidence in pedagogical effectiveness of generated feedback based on limited expert validation scope

## Next Checks
1. Conduct cross-linguistic validation by testing fine-tuned models on essays from other proficiency levels and potentially other low-resource languages
2. Implement longitudinal studies with actual language learners to evaluate real-world pedagogical impact of generated feedback
3. Perform ablation studies on feedback generation component to isolate contribution of different model architectures and training strategies