---
ver: rpa2
title: Improving Dialogue Discourse Parsing through Discourse-aware Utterance Clarification
arxiv_id: '2506.15081'
source_url: https://arxiv.org/abs/2506.15081
tags:
- discourse
- clarification
- parser
- dialogue
- parsing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of ambiguity in dialogue discourse
  parsing caused by linguistic features such as omission, typos, abbreviations, slang,
  and idioms. The proposed Discourse-aware Clarification Module (DCM) uses clarification
  type reasoning to identify linguistic ambiguities and discourse goal reasoning to
  align clarifications with intended discourse relations.
---

# Improving Dialogue Discourse Parsing through Discourse-aware Utterance Clarification

## Quick Facts
- arXiv ID: 2506.15081
- Source URL: https://arxiv.org/abs/2506.15081
- Reference count: 30
- Primary result: Achieved 69.0% LR F1 on STAC and 66.2% LR F1 on Molweni by resolving ambiguity through clarification rewriting

## Executive Summary
This paper addresses the challenge of ambiguity in dialogue discourse parsing caused by linguistic features such as omission, typos, abbreviations, slang, and idioms. The proposed Discourse-aware Clarification Module (DCM) uses clarification type reasoning to identify linguistic ambiguities and discourse goal reasoning to align clarifications with intended discourse relations. A Contribution-aware Preference Optimization (CPO) method further reduces erroneous clarifications by leveraging parser feedback. Experiments on STAC and Molweni datasets show significant improvements over state-of-the-art baselines, achieving up to 69.0% LR F1 on STAC and 66.2% LR F1 on Molweni. The method demonstrates robustness across different model sizes and backbones, effectively resolving parsing ambiguities through targeted clarification.

## Method Summary
The approach uses a two-stage architecture where a base discourse parser (DP) is augmented with a clarification module (DCM) and a contribution-aware preference optimization (CPO) trainer. The DP is a fine-tuned LLM (LLaMA3-8B) that predicts dialogue relations. When uncertainty is detected via self-sampling, utterances are forwarded to DCM, which employs Clarification Type Reasoning (CTR) to detect linguistic ambiguities and Discourse Goal Reasoning (DGR) to generate clarified versions. CPO weights the preference optimization by the parser's performance gain, reducing the impact of noisy clarifications. The method is evaluated on STAC and Molweni datasets with 16 relation types, using micro-averaged F1 for link prediction and joint link+relation prediction.

## Key Results
- Achieved 69.0% LR F1 on STAC (vs 63.2% baseline) and 66.2% LR F1 on Molweni (vs 64.1% baseline)
- DCM+CPO reduced cascading errors from 6.0% to 1.6% (Correct→Incorrect) on STAC
- Method shows consistent improvements across different model sizes (LLaMA3-8B, 70B) and backbones (T5, BERT)
- Performance gains attributed to resolving linguistic ambiguities that cause parser errors

## Why This Works (Mechanism)

### Mechanism 1
If linguistic ambiguity (e.g., omissions, typos) is the primary bottleneck for discourse parsing, explicitly rewriting utterances to align with intended relations improves accuracy. The Discourse-aware Clarification Module (DCM) uses Clarification Type Reasoning (CTR) to detect linguistic features and Discourse Goal Reasoning (DGR) to contrast the intended relation against an ambiguous prediction. This forces the model to generate a clarified utterance that unambiguously maps to the correct discourse link. The core assumption is that surface-level ambiguity can be resolved by textual substitution. Evidence shows DCM employs these two reasoning processes, and corpus analysis indicates dialogue models often miss discourse structures. This mechanism fails if the "ambiguous" relation is actually correct or if the clarification drifts semantically from the original intent.

### Mechanism 2
If automatically generated clarification data contains noise, weighting the preference optimization by the parser's performance gain reduces the risk of harmful clarifications. Contribution-aware Preference Optimization (CPO) modifies standard Direct Preference Optimization (DPO) by weighting the loss function by the contribution gap—the difference in log-probability the parser assigns to the correct relation when using a helpful vs. unhelpful clarification. The core assumption is that standard DPO overfits to minor differences, and optimization should focus on clarifications that significantly shift parser confidence. Evidence shows CPO enables the parser to assess contributions of clarifications, and it simplifies to standard DPO when contribution gaps are uniform. This fails if the parser's log-probability is not a reliable proxy for correctness (i.e., the parser is confidently wrong).

### Mechanism 3
If computational resources are limited and the base parser is well-calibrated, selective clarification based on uncertainty minimizes interference with already-correct predictions. The system employs Uncertainty Assessment via self-sampling, where the parser generates predictions and flags samples as uncertain if no majority consensus is reached. If consensus exists, the original utterance is used. The core assumption is that parser confidence correlates strongly with correctness, and uncertain cases are the primary beneficiaries of clarification. Evidence shows the uncertainty assessment method effectively distinguishes uncertain from certain instances, and accuracy drops significantly when the parser lacks confidence. This mechanism fails if the parser is confidently incorrect (high self-agreement on a wrong label).

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: The paper's CPO method is a direct modification of DPO. Understanding the standard DPO loss (contrastive learning between preferred and rejected samples) is required to understand why weighting by "contribution gap" is necessary.
  - Quick check question: How does the DPO loss function differ from standard Reinforcement Learning from Human Feedback (RLHF) regarding the reward model?

- **Concept: Rhetorical Structure Theory (RST) / Discourse Relations**
  - Why needed here: The output space consists of specific relations (e.g., "Comment", "Elaboration", "Question-answer pair"). The DGR mechanism relies on understanding the semantic difference between these relations.
  - Quick check question: In the context of the paper, what is the difference between a "Question-answer pair" relation and a "Comment" relation?

- **Concept: Parameter-Efficient Fine-Tuning (LoRA)**
  - Why needed here: The implementation uses LoRA (Low-Rank Adaptation) on LLaMA3-8B. Understanding this is critical for reproducing the results without requiring massive GPU memory.
  - Quick check question: Does LoRA update the weights of the pre-trained LLM directly, or does it train smaller adapter matrices alongside frozen weights?

## Architecture Onboarding

- **Component map:** Base Parser (DP) -> Uncertainty Gate -> DCM -> CPO Trainer -> Improved DP
- **Critical path:** The Data Construction Phase is the most fragile step. You must run the base parser on a seed set to identify errors, construct "pseudo-ambiguous" relations for correct samples, and use GPT-4 to generate the reasoning traces (CTR/DGR). If this seed data is low quality, the DCM will inherit those hallucinations.
- **Design tradeoffs:** Latency vs. Accuracy: The Uncertainty Gate requires multiple forward passes (o=10) plus a DCM generation pass for uncertain samples, significantly increasing inference latency. Noise vs. Scale: The paper uses ChatGPT for initial data construction for speed, acknowledging this introduces noise. CPO is the proposed solution to this noise.
- **Failure signatures:** Correct-to-Incorrect Drift: The DCM rewrites a clear utterance into a confusing one, causing the parser to flip from a right to a wrong answer. Relation Confusion: The parser improves Link F1 (connectivity) but degrades LR F1 (relation type), suggesting clarifications help linking but confuse the semantic relation classification.
- **First 3 experiments:** 1) Baseline Sanity Check: Reproduce the "w/o DCM&CPO" baseline to ensure your LoRA setup matches the paper's ~63.2 LR F1. 2) CPO vs. DPO Ablation: Replace the weighted CPO loss with standard DPO to confirm the "contribution gap" weighting actually reduces the "Correct->Incorrect" error rate. 3) Uncertainty Threshold Sensitivity: Vary the voting threshold to see if a higher confidence requirement improves precision at the cost of recall.

## Open Questions the Paper Calls Out

The paper identifies three key open questions: First, how can clarification data quality be systematically improved beyond using LLMs for automatic generation, given that ChatGPT-generated data introduces noise (10-14.7% inferior clarifications)? Second, does the DCM approach generalize effectively to domains with different linguistic characteristics beyond gaming (STAC) and technical support (Molweni), especially since performance gains are less substantial on Molweni? Third, what is the optimal balance between clarification data volume for DCM fine-tuning versus preference optimization data for CPO, as performance varies non-monotonically with data allocation?

## Limitations

- The paper's performance gains rely heavily on the quality of the clarification data generated by GPT-4, but the prompt template and generation process are only partially specified
- The Contribution-aware Preference Optimization (CPO) method's effectiveness depends on the parser's log-probability being a reliable indicator of correctness, which may not hold for all relation types
- The uncertainty assessment mechanism assumes parser confidence correlates with accuracy, but this assumption is not empirically validated for the specific discourse relation space

## Confidence

- **High Confidence:** The core architecture (DCM + CPO) and the STAC/Molweni baseline results are well-specified and reproducible. The improvement trends (DCM+CPO > DCM > baseline) are consistently reported.
- **Medium Confidence:** The mechanism explanations (CTR/DGR reasoning) are plausible but lack quantitative analysis of their individual contributions. The CPO weighting scheme's hyperparameters (µ=0.7/0.5) were chosen via grid search, but the search procedure is not detailed.
- **Low Confidence:** The paper does not report the discard rate for preference pairs during CPO training, which is critical for understanding data efficiency. The pseudo-ambiguous relation construction strategy for correctly parsed samples is vaguely described ("randomly altered").

## Next Checks

1. **Ablation of Reasoning Components:** Train a variant of DCM that only uses CTR (no DGR) and another that only uses DGR (no CTR) to quantify their individual contributions to the overall improvement.

2. **Parser Confidence Analysis:** For the uncertainty assessment mechanism, compute the correlation between parser self-agreement (voting consistency) and actual correctness on a held-out validation set, stratified by relation type.

3. **Preference Pair Efficiency:** During CPO training, log the discard rate (instances with insufficient quality preference pairs) and the distribution of contribution gaps (g) to assess whether the weighting scheme is focusing on the most impactful clarifications.