---
ver: rpa2
title: Stability Enhancement in Reinforcement Learning via Adaptive Control Lyapunov
  Function
arxiv_id: '2504.19473'
source_url: https://arxiv.org/abs/2504.19473
tags:
- control
- system
- constraints
- stability
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SAC-CLF, a reinforcement learning framework
  that enhances stability and safety in real-world control applications by integrating
  Control Lyapunov Functions (CLFs) into the Soft Actor-Critic (SAC) algorithm. The
  method addresses the challenge of safe exploration in model-free RL through three
  key innovations: (1) task-specific CLF design via system linearization and LQR techniques,
  (2) dynamic adjustment of constraint strength based on discrepancies between desired
  and actual CLF derivatives, and (3) improved control input smoothness through a
  vibration-dampening term.'
---

# Stability Enhancement in Reinforcement Learning via Adaptive Control Lyapunov Function

## Quick Facts
- arXiv ID: 2504.19473
- Source URL: https://arxiv.org/abs/2504.19473
- Reference count: 40
- Key outcome: Introduces SAC-CLF, integrating Control Lyapunov Functions into Soft Actor-Critic for stable, safe RL in real-world control applications

## Executive Summary
This paper presents SAC-CLF, a novel reinforcement learning framework that enhances stability and safety in real-world control applications by integrating Control Lyapunov Functions (CLFs) into the Soft Actor-Critic (SAC) algorithm. The method addresses the challenge of safe exploration in model-free RL through three key innovations: task-specific CLF design via system linearization and LQR techniques, dynamic adjustment of constraint strength based on discrepancies between desired and actual CLF derivatives, and improved control input smoothness through a vibration-dampening term. Experimental results on nonlinear systems and satellite attitude control demonstrate SAC-CLF's effectiveness in achieving safe and efficient learning while maintaining performance under unmodeled dynamics and disturbances.

## Method Summary
SAC-CLF enhances the standard SAC algorithm by incorporating a real-time quadratic programming (QP) layer that enforces stability constraints derived from Control Lyapunov Functions. The method involves offline computation of a task-specific CLF via linearization around equilibrium and solving the Algebraic Riccati Equation to obtain matrix P. During training, at each timestep the algorithm solves a QP that balances the RL policy's control recommendation, actuator limits, and the CLF constraint V̇(e) ≤ −ηV(e), where η is dynamically adjusted based on discrepancies between nominal and actual CLF derivatives. The QP also includes a smoothing term to reduce control input vibration. This safety layer filters the SAC actor's output to ensure stability while maintaining learning performance.

## Key Results
- SAC-CLF achieves lower cost and faster convergence compared to unit-matrix CLF baselines on nonlinear control tasks
- Adaptive constraint strength mechanism maintains lower cost variance under model bias and disturbances compared to fixed-constraint approaches
- Vibration-dampening term reduces control input oscillations without significant performance degradation
- Successfully stabilizes satellite attitude control system with unmodeled dynamics while ensuring safety constraints

## Why This Works (Mechanism)

### Mechanism 1: Task-Specific CLF Design via LQR Linearization
Quadratic CLFs derived from LQR provide systematic, task-aligned stability certificates for nonlinear systems near equilibrium. By linearizing system dynamics around the desired state and solving the Algebraic Riccati Equation (ARE) to obtain positive-definite matrix P, the method constructs V(e) = eᵀPe as the CLF, ensuring both local asymptotic stability and local optimality w.r.t. the LQR cost. This approach leverages well-established optimal control theory to create stability guarantees tailored to the specific control task.

### Mechanism 2: Adaptive Constraint Strength via CLF Derivative Discrepancy
Real-time adjustment of constraint conservatism η(t) maintains stability under unmodeled dynamics and disturbances. The method computes discrepancy δ(t) between desired CLF derivative (nominal) and actual derivative (with disturbances), then adapts η(t) = η₀(1 + kη(t)) through a first-order filter. When δ(t) is large, constraints tighten; when small, they relax for performance. This dynamic adaptation ensures safety margins are maintained when the system deviates from nominal behavior while allowing greater exploration when the system operates as expected.

### Mechanism 3: Safety-Prioritized Control Input Smoothing
A vibration-dampening term in the QP objective smooths control inputs without violating safety constraints. By adding β‖u(t) − u(t−Δt)‖² to the QP objective, the method yields u(t) = β/(β+1)·u(t−Δt) + 1/(β+1)·u_RL(t) when constraints are inactive—a weighted average favoring smoothness. The QP prioritizes safety constraints over smoothness when conflicts arise, ensuring that the smoothing mechanism never compromises the stability guarantees.

## Foundational Learning

- Concept: **Control Lyapunov Functions (CLFs)**
  - Why needed here: Core mathematical object encoding stability; the entire framework hinges on defining and enforcing CLF decrease conditions
  - Quick check question: Given V(e) = eᵀPe, what does V̇(e) < −ηV(e) guarantee about system behavior?

- Concept: **Linear Quadratic Regulator (LQR) and Algebraic Riccati Equation**
  - Why needed here: Provides systematic method to compute P matrix for task-specific CLFs; bridges optimal control and stability theory
  - Quick check question: How do Q and R weightings in the LQR cost affect the resulting CLF shape and control aggressiveness?

- Concept: **Quadratic Programming with Safety Constraints**
  - Why needed here: The real-time controller solves a QP at each timestep to balance RL recommendations, smoothness, and CLF constraints
  - Quick check question: What happens to the QP solution when the CLF constraint becomes infeasible under actuator limits?

- Concept: **Soft Actor-Critic (SAC) and Entropy Regularization**
  - Why needed here: Base RL algorithm providing u_RL(t); entropy term maintains exploration during training
  - Quick check question: Why does SAC use a stochastic policy, and how does this affect the QP-based safety filter?

## Architecture Onboarding

- Component map: SAC Networks -> CLF Module -> Adaptive η Module -> QP Solver -> Smoothing Buffer
- Critical path:
  1. **Offline**: Linearize system at equilibrium → solve ARE → obtain P → define V(e)
  2. **Online (each timestep)**: Observe state → query SAC actor for u_RL → compute η(t) → solve QP (Eq. 30) → apply u(t) → store transition → update networks
- Design tradeoffs:
  - **η₀ (baseline constraint strength)**: Higher → faster convergence, more conservative; lower → more freedom for RL, risk of instability
  - **β (smoothing weight)**: Higher → smoother but slower response; β=0 → no smoothing
  - **Kε (slack penalty)**: Higher → stricter CLF enforcement; lower → allows temporary constraint violations
  - **ωη (adaptation speed)**: Higher → faster response to disturbances but potential noise sensitivity
- Failure signatures:
  - **Oscillating/chattering control**: β too low, or η too high causing constraint switching
  - **Slow learning or poor convergence**: η₀ excessively high, overly constraining exploration
  - **Safety violations despite CLF**: Kε too low (slack too permissive); or operating outside safe energy ball D
  - **Sluggish disturbance rejection**: ωη too low, adaptation can't keep up
- First 3 experiments:
  1. **CLF validation**: Compare LQR-designed CLF vs. unit-matrix CLF on NCT system (no disturbances, no smoothing). Expect: LQR-CLF achieves lower cost, faster convergence
  2. **Adaptive constraint robustness test**: Introduce model bias/disturbance; compare fixed η vs. adaptive η. Expect: Adaptive maintains lower cost variance under perturbations
  3. **Smoothing effectiveness**: Run with β=0 vs. β=1.0; measure control input variance and tracking error. Expect: Dampened case shows reduced oscillations without significant performance degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the SAC-CLF framework be effectively scaled to high-dimensional, safety-critical systems such as autonomous vehicles and complex robotics?
- Basis: [explicit] The conclusion explicitly states future work will "extend SAC-CLF to more complex scenarios and integrate it into safety-critical systems like autonomous vehicles and robotics."
- Why unresolved: The current experimental validation is limited to a 2D nonlinear system (NCT) and a 7-dimensional satellite attitude simulation.

### Open Question 2
- Question: How does the LQR-based CLF design perform when the system state is far from the equilibrium point where the linearization assumption is invalid?
- Basis: [inferred] The method relies on linearizing system dynamics around a desired equilibrium point (Eq. 10) to derive the CLF via the Algebraic Riccati Equation.
- Why unresolved: LQR provides local optimality; the paper does not theoretically or empirically analyze the region of attraction or performance when non-linearities dominate far from the linearization point.

### Open Question 3
- Question: Does the strict enforcement of the CLF constraint via the QP inadvertently prevent the agent from discovering globally optimal policies that require temporary violations of the local stability criterion?
- Basis: [inferred] The paper acknowledges in Figure 4 that "optimal control input may not satisfy CLF constraints," yet the method enforces these constraints (Eq. 30) to prioritize safety.
- Why unresolved: The trade-off between strict local stability (safety) and global optimality in the context of RL exploration is not quantified.

## Limitations
- The theoretical contributions rely on accurate linearization and the validity of the safe energy ball assumption, which may not hold for highly nonlinear systems or aggressive trajectories
- The adaptive constraint strength mechanism depends on the assumption that disturbances vary slower than the adaptation bandwidth, which is not empirically validated across different disturbance profiles
- The effectiveness of the smoothing term β is only demonstrated qualitatively, without rigorous analysis of the trade-off between smoothness and control responsiveness

## Confidence

- **High Confidence**: The CLF design mechanism via LQR linearization - well-established theory with clear derivations and direct experimental validation
- **Medium Confidence**: The adaptive constraint strength mechanism - theoretical derivation is sound but relies on unverified assumptions about disturbance characteristics
- **Low Confidence**: The safety-prioritized control smoothing mechanism - minimal theoretical analysis and no comparative ablation studies

## Next Checks

1. **Disturbance Profile Sensitivity**: Test adaptive η under high-frequency vs. low-frequency disturbances to validate the assumption that disturbances vary slower than adaptation bandwidth
2. **Safe Energy Ball Violation**: Systematically explore states outside the linearized region to quantify performance degradation and safety violations when the safe energy ball assumption breaks
3. **Smoothing vs. Responsiveness Trade-off**: Conduct ablation studies varying β across a wider range (0.0 to 5.0) while measuring both control smoothness and safety constraint satisfaction time during rapid state changes