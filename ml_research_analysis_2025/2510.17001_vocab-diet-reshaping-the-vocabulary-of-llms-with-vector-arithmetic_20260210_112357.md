---
ver: rpa2
title: 'Vocab Diet: Reshaping the Vocabulary of LLMs with Vector Arithmetic'
arxiv_id: '2510.17001'
source_url: https://arxiv.org/abs/2510.17001
tags:
- language
- vocabulary
- computational
- word
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to reshape the vocabulary of large
  language models (LLMs) by leveraging vector arithmetic in embedding space. The key
  insight is that morphological variations of words (e.g., "walk" - "walked") can
  be captured as transformation vectors added to base word embeddings.
---

# Vocab Diet: Reshaping the Vocabulary of LLMs with Vector Arithmetic

## Quick Facts
- arXiv ID: 2510.17001
- Source URL: https://arxiv.org/abs/2510.17001
- Reference count: 40
- Primary result: Vector arithmetic in embedding space enables compositional vocabulary design by capturing morphological variations as transformation vectors

## Executive Summary
This paper proposes a novel approach to vocabulary reshaping in large language models by leveraging vector arithmetic in embedding space. The key insight is that morphological variations of words can be captured as transformation vectors added to base word embeddings, enabling more compact and expressive vocabularies. Instead of assigning unique tokens to each surface form, the approach composes words from shared base forms and transformation vectors. Experiments across five languages and multiple models demonstrate the potential of this compositional vocabulary design.

## Method Summary
The approach uses vector arithmetic to capture morphological variations in embedding space, where transformations like "walk" to "walked" are represented as additive vectors applied to base word embeddings. This compositional method avoids assigning unique tokens to each surface form by sharing base forms across morphological variations. The vocabulary reshaping is implemented through learned transformation vectors that encode morphological changes, allowing for more compact vocabulary representations while maintaining expressiveness.

## Key Results
- Demonstrated vocabulary reshaping through vector arithmetic in embedding space
- Tested across five languages and multiple model architectures
- Showed potential for more compact vocabulary representations through morphological composition

## Why This Works (Mechanism)
The mechanism relies on the observation that morphological variations often follow systematic patterns that can be captured as vector transformations in embedding space. By learning these transformation vectors, the model can generate new word forms from base embeddings without requiring separate tokens for each morphological variant. This compositional approach exploits the regularity in morphological changes while reducing vocabulary redundancy.

## Foundational Learning
- Vector arithmetic in embedding space - why needed: To represent morphological transformations as mathematical operations; quick check: Verify that transformation vectors consistently produce valid word forms
- Compositional vocabulary design - why needed: To reduce vocabulary size while maintaining expressiveness; quick check: Measure vocabulary compression ratio vs. performance impact
- Morphological variation patterns - why needed: To identify systematic transformations that can be learned as vectors; quick check: Test transformation vector robustness across different morphological families

## Architecture Onboarding
Component map: Base embeddings -> Transformation vectors -> Composed embeddings -> Output vocabulary
Critical path: Input token → Base embedding lookup → Transformation vector addition → Composed embedding → Downstream task
Design tradeoffs: Vocabulary size reduction vs. computational overhead of vector arithmetic; model expressiveness vs. training complexity
Failure signatures: Inconsistent morphological transformations, vocabulary bloat from excessive base forms, performance degradation on morphologically rich languages
First experiments: 1) Test basic morphological transformations on simple verb conjugations, 2) Evaluate vocabulary compression ratio across different language families, 3) Measure perplexity impact when using composed vs. unique tokens

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Claims about vocabulary compactness lack empirical validation with specific size reduction metrics
- Uncertainty about generalization to highly morphologically complex languages beyond the five tested
- No analysis of robustness to polysemy and context-dependent meaning shifts in morphological transformations

## Confidence
- High confidence: Technical feasibility of vector arithmetic for morphological composition
- Medium confidence: Practical utility without comparative ablation studies
- Low confidence: Expressiveness gains without qualitative text generation analysis

## Next Checks
1. Conduct ablation studies measuring vocabulary size reduction against perplexity degradation across different languages
2. Test robustness of transformation vectors on polysemous words and context-dependent morphological changes
3. Compare against alternative vocabulary compression methods using identical evaluation protocols