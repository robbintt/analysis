---
ver: rpa2
title: 'MEIC-DT: Memory-Efficient Incremental Clustering for Long-Text Coreference
  Resolution with Dual-Threshold Constraints'
arxiv_id: '2512.24711'
source_url: https://arxiv.org/abs/2512.24711
tags:
- uni00000013
- coreference
- mentions
- uni00000011
- cluster
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MEIC-DT introduces a dual-threshold memory-efficient clustering
  approach for long-text coreference resolution. It combines a lightweight Transformer
  with a dual-threshold constraint mechanism that regulates both the number of historical
  clusters and mentions per cluster.
---

# MEIC-DT: Memory-Efficient Incremental Clustering for Long-Text Coreference Resolution with Dual-Threshold Constraints

## Quick Facts
- arXiv ID: 2512.24711
- Source URL: https://arxiv.org/abs/2512.24711
- Authors: Kangyang Luo; Shuzheng Si; Yuzhuo Bai; Cheng Gao; Zhitong Wang; Cheng Huang; Yingli Shen; Yufeng Han; Wenhao Li; Cunliang Kong; Maosong Sun
- Reference count: 40
- One-line primary result: State-of-the-art Avg.F1 scores (81.27% LitBank, 85.05% OntoNotes, 72.63% WikiCoref) under strict memory constraints via dual-threshold clustering

## Executive Summary
MEIC-DT addresses the memory bottleneck in long-text coreference resolution by introducing a dual-threshold constraint mechanism that tightly controls the number of clusters (τ₁) and mentions per cluster (τ₂) processed by a lightweight Transformer classifier. It couples this with a Statistics-Aware Eviction Strategy (SAES) that adapts eviction scoring to training (using remaining mention counts) and inference (using activity ratios) phases, and an Internal Regularization Policy (IRP) that semantically groups and samples representative mentions to preserve cluster integrity. Evaluated on OntoNotes, LitBank, and WikiCoref, MEIC-DT achieves state-of-the-art Avg.F1 scores while significantly improving memory efficiency and training stability compared to baselines like LRU and dual-cache strategies.

## Method Summary
MEIC-DT is an incremental clustering approach for long-text coreference resolution that combines a lightweight Transformer classifier with dual-threshold constraints (τ₁ for max clusters, τ₂ for max mentions per cluster) to stay within a fixed memory budget. For each document, it uses a DeBERTa-large encoder and ImCoref mention extractor, then processes mentions sequentially. The Cache Manager maintains tracked clusters, which are condensed by IRP (grouping by textual equivalence/inclusion + pronoun identity, then sampling) before being scored by the Transformer against the current mention. If a score exceeds 0.5, the mention joins that cluster (triggering IRP re-sampling); otherwise, a new cluster is created. When the cache reaches τ₁, SAES evicts the least "important" cluster based on phase-specific statistics: during training, using remaining ground-truth mentions (rm_j), and during inference, using an activity ratio (em_j/age_j) and LRU recency. This enables Transformer-based clustering on long documents without OOM errors.

## Key Results
- Achieves state-of-the-art Avg.F1 of 81.27% on LitBank, 85.05% on OntoNotes, and 72.63% on out-of-domain WikiCoref under strict memory constraints.
- SAES significantly outperforms LRU and dual-cache baselines, with Avg.F1 gains of 3.56% on OntoNotes and 2.34% on LitBank at τ₁=50/τ₂=30.
- IRP's group-based sampling improves B3 and CEAFϕ4 scores over random sampling, with visualized semantic space coverage in Figure 4.
- Transformer classifier consistently outperforms linear classifiers under identical dual-threshold constraints, validating the memory efficiency of the approach.

## Why This Works (Mechanism)

### Mechanism 1: Statistics-Aware Eviction Strategy (SAES)
- **Claim:** Phase-adaptive eviction scoring improves cache retention of critical clusters compared to static heuristics.
- **Mechanism:** During training, SAES uses ground-truth annotations to compute a ranking score based on a cluster's number of remaining mentions (`rm_j`) and LRU recency. During inference, lacking gold labels, it substitutes `rm_j` with an activity ratio (`em_j / age_j`). This prioritizes clusters with higher observed or potential for future coreference links.
- **Core assumption:** The remaining mention count (training) and activity ratio (inference) are reliable proxies for a cluster's future importance.
- **Evidence anchors:** [abstract] Introduces SAES, which utilizes distinct statistical profiles from the training and inference phases for intelligent cache management. [Section 3.2] Details `SAES_train` and `SAES_inf` scoring functions in Eqs. 4 and 5. [corpus] Weak/missing: Corpus evidence does not cover phase-adaptive eviction; related work (e.g., Guo et al., 2023; Xia et al., 2020) relies on static LRU or dual-cache strategies.
- **Break condition:** Performance degrades if `rm_j` or `em_j/age_j` lose correlation with cluster importance, or if annotation noise severely distorts training signals.

### Mechanism 2: Internal Regularization Policy (IRP)
- **Claim:** Semantically-informed mention sampling better preserves cluster representation than random sampling.
- **Mechanism:** IRP condenses large clusters to a fixed size (`τ₂`) via a two-stage process: (1) grouping mentions by textual equivalence/inclusion and pronoun identity, and (2) deterministically retaining the first and last mentions (for context) and sampling from the semantic groups to maximize coverage.
- **Core assumption:** The first/last mentions are essential anchors, and group-based sampling provides a more representative semantic distribution than random selection.
- **Evidence anchors:** [abstract] Describes IRP as a method that "strategically condenses clusters by selecting the most representative mentions, thereby preserving semantic integrity." [Section 3.3, Table 5] Shows group-based sampling outperforming random sampling, with Fig. 4 visualizing the semantic space. [corpus] Weak/missing: No corpus papers evaluate or propose similar grouping-based sampling for cluster condensation.
- **Break condition:** The assumption fails if first/last mentions are uninformative or if textual equivalence is a poor proxy for semantic diversity (e.g., missing paraphrases).

### Mechanism 3: Dual-Threshold Constraint Coupling
- **Claim:** Jointly bounding the number of clusters (`τ₁`) and mentions-per-cluster (`τ₂`) enables the use of a powerful Transformer classifier under a fixed memory budget.
- **Mechanism:** The Transformer's input tensor `M` grows with the number of clusters (`w`) and maximum mentions per cluster (`max_j(|c_j|)`). SAES bounds `w` to `τ₁`, and IRP bounds `max_j(|c_j|)` to `τ₂`. This caps input size at `τ₁ x (τ₂+1) x d_h`, preventing out-of-memory errors on long texts.
- **Core assumption:** The performance loss from discarding/condensing information is outweighed by the ability to use a Transformer classifier (which models richer intra-cluster dependencies) instead of a linear one.
- **Evidence anchors:** [abstract] Proposes "a dual-threshold constraint mechanism designed to precisely control the Transformer's input scale within a predefined memory budget." [Section 3.1, Fig. 1] Analyzes the unbounded growth of clusters and mentions on LitBank, motivating the dual-threshold approach. [corpus] Related work (Martinelli et al., 2024) highlights the memory burden of Transformer-based incremental clustering but does not propose a similar dual-constraint solution.
- **Break condition:** The tight thresholds required for extreme memory constraints discard so much critical information that even the Transformer classifier cannot maintain competitive performance.

## Foundational Learning

- **Concept: Incremental Clustering in Coreference Resolution**
  - **Why needed here:** MEIC-DT is an incremental method. It processes documents sequentially, maintaining a cache of entities and deciding for each new mention whether to link it to an existing cluster or form a new one.
  - **Quick check question:** Given a new mention `m_i` and a cache of existing clusters `{c_1, ..., c_k}`, what are the model's two possible actions? (Answer: Assign `m_i` to an existing cluster `c_j` if `max p_c(m_i) > 0.5`, or create a new cluster `c_{k+1} = [m_i]`.)

- **Concept: Transformer vs. Linear Classifier for Clustering**
  - **Why needed here:** A key contribution is enabling the more powerful Transformer classifier under memory constraints. You must understand it models the *global intra-cluster structure* by attending to all mentions, whereas a linear classifier typically uses a single summary vector.
  - **Quick check question:** How does the Transformer classifier form the input for a candidate mention `m_i` and a historical cluster `c_j`, compared to a standard linear classifier? (Answer: Transformer uses a sequence `[m_i, m_j,1, ..., m_j,|c_j|]`; Linear typically uses a single summary vector.)

- **Concept: Cache Eviction Strategies (LRU, LFU)**
  - **Why needed here:** SAES is explicitly motivated by the limitations of simple heuristics like LRU. Understanding that LRU only considers recency, not importance, is crucial to see why SAES adds an importance metric (`rm_j` or `em_j/age_j`).
  - **Quick check question:** What key limitation of using pure LRU during inference does SAES address? (Answer: LRU may evict a highly active or important cluster simply because it hasn't been accessed in the most recent steps.)

## Architecture Onboarding
- **Component map:** The pipeline consists of: (1) a **Text Encoder** (DeBERTa) for document embeddings; (2) a **Mention Extractor** (`Imcoref`) to identify candidate spans; (3) a **Cache Manager** containing tracked coreference clusters; (4) the **Dual-Threshold Constraint Mechanism**, which includes **SAES** (for managing the cache via eviction) and **IRP** (for condensing individual clusters via sampling); and (5) a **lightweight Transformer classifier** for computing coreference probabilities.
- **Critical path:** For each document, tokens are encoded and candidate mentions extracted. For each mention `m_i`, its representation is combined with representations from all clusters in the cache (as condensed by IRP). The Transformer classifier scores these pairings. If a score exceeds 0.5, `m_i` is added to that cluster (triggering IRP re-sampling if needed). If not, a new cluster is created. If the cache reaches size `τ₁`, SAES selects a cluster for eviction based on phase-specific statistics.
- **Design tradeoffs:**
    - **Thresholds (`τ₁`, `τ₂`):** Higher values improve recall and representation but increase memory and computation. The optimal balance is dataset-dependent.
    - **SAES over LRU:** More computationally intensive due to statistics tracking, but empirically better preserves critical clusters.
    - **IRP over Random Sampling:** Requires grouping logic and pronoun lexicons, but better maintains semantic integrity.
- **Failure signatures:**
    - **Performance Collapse at Low Thresholds:** Avg.F1 drops sharply if `τ₁` or `τ₂` are set too small (Table 2).
    - **OOM on Long Documents:** If thresholds are set too high or the input isn't properly padded/truncated.
    - **Stagnant Cache:** If SAES misconfigures eviction scores (e.g., `δ` is too large), the same clusters may be retained/evicted inappropriately.
- **First 3 experiments:**
  1. **Baseline Comparison:** Compare MEIC-DT against LRU and Dual-cache baselines on OntoNotes and LitBank using both linear and Transformer classifiers, with identical `τ₁/τ₂` settings, to validate performance claims (replicating Table 2).
  2. **Ablation Study:** Run MEIC-DT with (a) SAES replaced by LRU during training only, (b) SAES replaced by LRU during inference only, and (c) IRP replaced by random sampling, to measure the contribution of each component (replicating Tables 4 & 5).
  3. **Efficiency Analysis:** Measure and plot Training Average Memory (TAM), Inference Average Memory (IAM), and Inference Average Time (IAT) across different `τ₁/τ₂` configurations to validate the memory-performance tradeoff (replicating Table 3 & Fig. 3).

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can adaptive threshold learning or reinforcement learning policies replace the static $\tau_1$ and $\tau_2$ constraints to dynamically optimize memory allocation during clustering?
- Basis in paper: [explicit] The Conclusion and Limitations sections state, "Future work could explore adaptive threshold learning or reinforcement learning-based policies for dynamic memory allocation."
- Why unresolved: The current MEIC-DT framework relies on manually tuned, fixed integer thresholds for cluster counts and mention limits, which may not be optimal across diverse document lengths and entity densities.
- What evidence would resolve it: A comparative study where $\tau_1$ and $\tau_2$ are learned variables (via RL or gradient descent) versus the static baseline, showing improved Avg.F1 or faster convergence on variable-length datasets.

### Open Question 2
- Question: Does replacing the surface-form based Semantic Grouping in the Internal Regularization Policy (IRP) with semantic embedding-based clustering improve the selection of representative mentions?
- Basis in paper: [inferred] The IRP uses a Union-Find algorithm based on "textual equivalence/inclusion relationships and a pronoun lexicon," which may fail to group semantically similar mentions that lack surface overlap (e.g., synonyms).
- Why unresolved: The paper validates IRP against random sampling but does not ablate the grouping mechanism itself against denser semantic similarity metrics.
- What evidence would resolve it: An ablation study replacing the text-match grouping with a cosine-similarity or embedding-based clustering step, measuring the resulting impact on B3 and CEAF$\phi$4 scores.

### Open Question 3
- Question: How does the Statistics-Aware Eviction Strategy (SAES) impact the resolution of long-distance coreference chains in documents significantly longer than the evaluated benchmarks (e.g., full novels vs. LitBank excerpts)?
- Basis in paper: [inferred] The method bounds memory to a fixed cache size ($\tau_1$); while effective on LitBank (avg 2.1k words), the frequency of eviction events increases with document length, potentially removing "active" entities needed much later in the text.
- Why unresolved: The evaluation is limited to datasets with average lengths under 2,200 words, leaving the "forgetting" behavior in extremely long contexts unexplored.
- What evidence would resolve it: Performance metrics (specifically MUC scores tracking long-distance links) on a dataset of full-length novels or legal documents exceeding 10k words, analyzing the "resurrection" rate of evicted entities.

## Limitations
- Core performance gains hinge on phase-specific SAES scoring, but corpus evidence lacks direct validation of the remaining mention count (rm_j) and activity ratio (em_j/age_j) as reliable importance proxies across diverse domains.
- IRP's semantic grouping relies on textual equivalence/inclusion and pronoun lexicons, which may inadequately capture paraphrases or semantic similarity in complex domains, potentially limiting generalization.
- The lightweight Transformer classifier architecture is underspecified, creating ambiguity in reproducing the exact model and its relative advantage over linear classifiers.

## Confidence
- **High confidence** in the dual-threshold mechanism's ability to control memory usage and enable Transformer classification under strict budgets, supported by direct corpus analysis of the problem.
- **Medium confidence** in SAES's phase-adaptive benefits, as the paper demonstrates superior performance but lacks independent corpus validation of the underlying scoring heuristics.
- **Medium confidence** in IRP's group-based sampling advantage, as Table 5 shows gains over random sampling but the underlying semantic grouping method is not rigorously evaluated against alternatives.

## Next Checks
1. **Independent ablation of SAES:** Train and test MEIC-DT with SAES replaced by pure LRU in both training and inference phases, isolating the impact of phase-adaptive scoring on Avg.F1 and memory usage.
2. **IRP grouping robustness:** Implement and compare IRP against a strong baseline using learned semantic embeddings (e.g., sentence transformers) for mention grouping instead of textual heuristics, measuring Avg.F1 and coverage on OntoNotes.
3. **Transformer architecture fidelity:** Fully specify and reproduce the lightweight Transformer classifier architecture (layers, heads, hidden dim) and conduct a controlled comparison against the linear classifier at multiple τ₁/τ₂ settings to quantify the exact performance contribution.