---
ver: rpa2
title: 'Text2midi-InferAlign: Improving Symbolic Music Generation with Inference-Time
  Alignment'
arxiv_id: '2505.12669'
source_url: https://arxiv.org/abs/2505.12669
tags:
- music
- generation
- midi
- alignment
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Text2midi-InferAlign, an inference-time\
  \ alignment technique for improving symbolic music generation from text captions.\
  \ The method employs two alignment objectives\u2014text-audio consistency (using\
  \ CLAP scores) and harmonic consistency (penalizing off-key notes)\u2014within a\
  \ reward-guided tree search framework."
---

# Text2midi-InferAlign: Improving Symbolic Music Generation with Inference-Time Alignment

## Quick Facts
- arXiv ID: 2505.12669
- Source URL: https://arxiv.org/abs/2505.12669
- Reference count: 0
- Primary result: Inference-time alignment improves Text2midi outputs by 29.4% in CLAP score and 116% in correct key matching

## Executive Summary
Text2midi-InferAlign introduces an inference-time alignment technique that improves symbolic music generation from text captions without retraining the base model. The method uses a reward-guided tree search framework with two alignment objectives: text-audio consistency measured via CLAP scores and harmonic consistency penalizing off-key notes. By employing caption mutation for exploration and reward-based beam replacement for exploitation, the approach achieves significant improvements in both objective metrics and subjective listening tests compared to the baseline Text2midi model.

## Method Summary
The approach treats music generation as a tree search problem where candidate states are evaluated using a composite reward function combining text-audio consistency (Ra via CLAP) and harmonic consistency (Rh penalizing off-key notes). The method uses caption mutation via Claude-3-Haiku to generate T syntactic/semantic variations of the original caption for exploration, while beam replacement every m tokens exploits high-reward sequences. The base Text2midi model generates MIDI tokens conditioned on caption embeddings, and the alignment process selects the highest-reward final sequence without modifying the underlying model architecture.

## Key Results
- CLAP score increased from 0.17 to 0.22 (29.4% gain) on MidiCaps dataset
- Correct key matching rose from 13.59% to 29.80% (116% improvement)
- Compression ratio improved from 2.16 to 2.31
- Subjective listening tests showed 68.75% preference for music quality over baseline
- Method works effectively on both detailed and free-text captions

## Why This Works (Mechanism)

### Mechanism 1
Reward-guided tree search improves output quality without retraining by iteratively pruning low-reward generation paths. The model generates candidate states via mutated captions, evaluates each partial sequence using composite reward R(s,x) = α·Ra + β·Rh, and replaces low-scoring beams with higher-scoring variants every m tokens. This biases sampling toward reward-aligned regions of the output space, assuming the reward functions correlate with human-perceived quality and text alignment.

### Mechanism 2
Caption mutation expands search space coverage, allowing the base model to reach higher-reward states it would otherwise miss. An LLM generates T syntactic/semantic variations of the original caption, with each mutated caption guiding an independent generation branch. Since Text2midi conditions on caption embeddings, varying captions explores different regions of the conditional distribution while maintaining semantic relevance to the original intent.

### Mechanism 3
Harmonic consistency reward directly penalizes off-key notes, reducing theory-violating outputs. Rh(s,x) = 1 - (#off-key notes / #total notes) computes the reward from extracted key vs. note pitches. During beam replacement, sequences with more in-key notes survive preferentially, assuming the extracted key is meaningful and stable, and that "off-key" definition aligns with listener expectations.

## Foundational Learning

- **Autoregressive generation with beam search**: Why needed - The base Text2midi model generates tokens sequentially; understanding p(yt|y1:t-1, x) is essential to grasp how beam replacement intervenes. Quick check - Can you explain how beam search maintains multiple hypotheses and why replacing low-scoring beams affects final output?

- **CLAP (Contrastive Language-Audio Pretraining) embeddings**: Why needed - The Ra reward uses CLAP to measure text-audio similarity; understanding joint embedding spaces is critical for debugging alignment failures. Quick check - What does a higher CLAP cosine similarity indicate about the relationship between a caption and synthesized audio?

- **Music theory fundamentals (key signatures, diatonic vs. chromatic notes)**: Why needed - The Rh reward penalizes off-key notes; you need to interpret key extraction results and understand when the penalty may be inappropriate. Quick check - Given a MIDI sequence in C major, which pitch classes would be penalized as "off-key"?

## Architecture Onboarding

- **Component map**: Input caption → (mutation via Claude-3-Haiku) → T caption variants → parallel beam generation via Text2midi → every m tokens: compute rewards → replace bottom (q-k) beams with copies from top-k → repeat until N tokens → select highest-reward final sequence

- **Critical path**: Text caption flows through mutation module to create variants, each drives parallel generation through frozen Text2midi model, rewards computed from partial sequences drive beam replacement decisions, final output selected from highest-scoring complete sequence

- **Design tradeoffs**: Smaller m (e.g., 100) = more frequent replacement, finer control but higher compute; higher T (mutations) = broader exploration but linear cost increase; β > α prioritizes harmonic correctness over text alignment

- **Failure signatures**: CLAP score high but outputs musically poor (reward gaming); harmonic score high but outputs bland (over-penalization of non-diatonic notes); free-text captions underperform detailed captions (mutation may be unnecessary for short prompts)

- **First 3 experiments**: 1) Reproduce baseline vs. InferAlign on 10 held-out MidiCaps samples with m=100, T=5; log all six metrics and generation time; 2) Ablation: Set T=1 (no mutation) to isolate exploitation-only contribution; compare CLAP and CK scores; 3) Stress test: Run on free-text captions without mutation vs. with mutation to quantify exploration benefit on open-ended prompts

## Open Questions the Paper Calls Out

### Open Question 1
Can integrating reinforcement learning with the current inference-time alignment framework enable more fine-grained control over specific musical attributes? The paper suggests prospective research could explore this integration, as the current method uses only inference-time rewards without learning from reward signals to improve generation policy.

### Open Question 2
What additional musical objective functions beyond text-audio consistency and harmonic consistency could further improve generation quality? The paper indicates the approach could be extended to more musical elements, as currently only two objectives are implemented.

### Open Question 3
How sensitive is caption mutation quality to the choice of language model used for generating variations? The method relies exclusively on Claude-3-Haiku for caption mutation, but no ablation compares different LLMs or mutation strategies.

### Open Question 4
Does Text2midi-InferAlign generalize effectively to other autoregressive symbolic music generation architectures? The authors claim the method can extend any existing autoregressive model but only demonstrate results on Text2midi.

## Limitations
- Reward specification risk: CLAP was trained on general audio-text pairs, not specifically on symbolic music, potentially limiting sensitivity to musical structure
- Ablation completeness: Missing sensitivity analysis for reward weighting (α, β), beam width q, and interaction between mutation and replacement cycles
- Dataset specificity: All experiments use MidiCaps with detailed captions; performance on diverse free-text prompts needs more systematic evaluation

## Confidence

- **High Confidence**: Quantitative improvements in CLAP score (29.4% gain) and compression ratio (7.0% gain) are directly measurable; tree search framework and reward computation methodology are clearly specified
- **Medium Confidence**: Harmonic consistency improvements (116% increase in correct key matching) rely on music21's key extraction accuracy on partial sequences; listening test preference (68.75%) based on limited samples and raters
- **Low Confidence**: Computational efficiency claims lack supporting runtime measurements; exploration-exploitation tradeoff benefits not rigorously quantified through controlled ablation

## Next Checks

1. **Reward Function Sensitivity**: Systematically vary α and β across [0,1]×[0,10] while holding other hyperparameters constant to identify sensitivity of CLAP, CK, and subjective quality scores to reward weighting

2. **Key Extraction Robustness**: Evaluate music21's key extraction accuracy on short/partial sequences (50-500 tokens) compared to full sequences to quantify potential errors in harmonic reward computation during beam replacement

3. **Free-Text Prompt Comparison**: Generate matched pairs of detailed and free-text captions with similar semantic content and compare InferAlign's performance across both to isolate the benefit of detailed captions versus the inference-time alignment method itself