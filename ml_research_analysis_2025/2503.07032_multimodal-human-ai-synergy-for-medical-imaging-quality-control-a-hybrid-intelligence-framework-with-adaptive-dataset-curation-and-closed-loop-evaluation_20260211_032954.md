---
ver: rpa2
title: 'Multimodal Human-AI Synergy for Medical Imaging Quality Control: A Hybrid
  Intelligence Framework with Adaptive Dataset Curation and Closed-Loop Evaluation'
arxiv_id: '2503.07032'
source_url: https://arxiv.org/abs/2503.07032
tags:
- quality
- imaging
- medical
- control
- reports
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study established a standardized multimodal dataset and evaluation
  framework for medical imaging quality control, systematically assessing large language
  models (LLMs) in chest X-ray (CXR) image quality assessment and CT report auditing.
  The dataset included 161 CXR radiographs and 219 CT reports, with comprehensive
  error classification covering technical image artifacts and report inconsistencies.
---

# Multimodal Human-AI Synergy for Medical Imaging Quality Control: A Hybrid Intelligence Framework with Adaptive Dataset Curation and Closed-Loop Evaluation

## Quick Facts
- arXiv ID: 2503.07032
- Source URL: https://arxiv.org/abs/2503.07032
- Reference count: 38
- Primary result: Gemini 2.0-Flash achieved the highest Macro F1 score of 90 in CXR tasks, with DeepSeek-R1 excelling in CT report auditing at 62.23% recall

## Executive Summary
This study establishes a standardized multimodal dataset and evaluation framework for medical imaging quality control, assessing large language models in chest X-ray image quality assessment and CT report auditing. The dataset includes 161 CXR radiographs and 219 CT reports with comprehensive error classification. Multiple LLMs were evaluated using recall, precision, and F1 scores. Gemini 2.0-Flash demonstrated strong generalization with a 90 Macro F1 score, while DeepSeek-R1 excelled in CT report auditing with 62.23% recall. The findings highlight the potential of LLMs in medical imaging QC, with DeepSeek-R1 and Gemini 2.0-Flash showing superior performance in their respective domains.

## Method Summary
The study developed a hybrid intelligence framework for medical imaging quality control using multimodal large language models. The dataset comprises 161 chest X-ray images (converted to lossless PNG) and 219 de-identified Chinese CT reports, each annotated with specific error taxonomies (11 types for CXR, 8 for CT). Multiple LLMs were evaluated through an API-driven pipeline with structured prompts instructing models to identify specific error types. Evaluation metrics included Macro-F1 and Micro-F1 for CXR tasks, and Recall, Precision, and F1 scores plus an "Additional Discovery Rate" for CT report auditing. The framework employed a human-AI synergy loop where expert radiologists validated model outputs against a gold standard.

## Key Results
- Gemini 2.0-Flash achieved the highest Macro F1 score of 90 in CXR quality assessment, demonstrating strong generalization across error types
- DeepSeek-R1 excelled in CT report auditing with a 62.23% recall rate, outperforming other evaluated models
- Distilled variants of DeepSeek-R1 showed notably inferior performance, highlighting the importance of maintaining model complexity for specialized tasks
- InternLM2.5-7B-chat achieved the highest additional discovery rate, indicating broader error detection but lower precision

## Why This Works (Mechanism)
The framework's success stems from combining multimodal LLMs' ability to process both visual and textual medical data with a standardized evaluation protocol. The structured prompt engineering ensures consistent error identification across models, while the comprehensive error taxonomy provides granular assessment criteria. The human-AI synergy loop enables iterative refinement of both model performance and evaluation standards. The adaptive dataset curation ensures diverse error representation, and the closed-loop evaluation allows continuous performance monitoring and improvement.

## Foundational Learning

- **Concept: Multimodal Large Language Model (MLLM)**
    - **Why needed here:** To understand the core capability being testedâ€”the integration of vision (CXR images) and language (CT reports/prompts) for a unified task
    - **Quick check question:** How does an MLLM differ from a standard LLM when analyzing a chest X-ray?

- **Concept: Quality Control (QC) in Medical Imaging**
    - **Why needed here:** This is the specific domain and problem the framework addresses
    - **Quick check question:** What is the difference between a "technical error" in an image and a "logical inconsistency" in a report?

- **Concept: Macro-F1 vs. Micro-F1 Score**
    - **Why needed here:** The paper emphasizes Macro-F1 to penalize models that perform well on common errors but fail on rare, critical ones
    - **Quick check question:** If a model perfectly detects 100 common errors but misses 1 rare, critical error, why would the Macro-F1 score penalize it more than the Micro-F1 score?

## Architecture Onboarding

- **Component Map:** Dataset (161 CXR images + 219 CT reports) -> Inference Engine (multiple LLMs via API) -> Evaluation Framework (expert validation + metrics)
- **Critical Path:** The key to successful implementation is prompt-to-output alignment, ensuring the prompt precisely references QC criteria and requires structured output
- **Design Tradeoffs:**
    - Precision vs. Discovery: DeepSeek-R1 offers precision but may miss novel errors; InternLM2.5-7B-Chat offers broader discovery but with lower precision
    - Distillation vs. Performance: Distilled variants are more efficient but result in notably inferior performance
- **Failure Signatures:**
    - Fine-Grained Failure: High Macro-F1 but low Micro-F1 indicates correct error type identification but poor localization
    - Low-Specificity Drift: High volume of low-quality or incorrect suggestions
- **First 3 Experiments:**
    1. Establish Baseline: Reproduce F1 scores on top models using provided prompts
    2. Ablation by Error Type: Isolate performance on specific CT report error types
    3. Precision-Discovery Trade-off Test: Quantify operational trade-off between sensitive but noisy vs. precise but conservative systems

## Open Questions the Paper Calls Out

- **Open Question 1:** Can a hybrid AI framework effectively combine DeepSeek-R1's precision with InternLM2.5-7B-Chat's discovery capabilities to optimize CT report quality control?
- **Open Question 2:** Why do distilled variants of reasoning models significantly underperform compared to their parent models in specialized medical quality control tasks?
- **Open Question 3:** How does the single-institution origin of the training data impact the generalizability of LLMs to diverse imaging hardware and clinical protocols?

## Limitations
- The study relies on expert human annotations as gold standard without validating inter-rater reliability through kappa scores
- The normalization of CXR F1 scores to a maximum of 90 is unexplained, making cross-study comparisons difficult
- CT report auditing results are based on Chinese-language reports without explicit evaluation of models' Chinese proficiency

## Confidence
- **High Confidence:** Gemini 2.0-Flash achieving highest Macro F1 score of 90; DeepSeek-R1's superior performance in CT report auditing
- **Medium Confidence:** Generalizability findings given limited dataset size; distillation performance claims without ablation studies
- **Low Confidence:** Practical utility of "additional discovery rate" without false positive weighting; fine-grained performance claims without standardized localization metrics

## Next Checks
1. Obtain inter-rater reliability scores (kappa statistics) from expert radiologists who annotated the dataset
2. Design a study measuring whether LLM-identified QC errors correlate with downstream diagnostic errors or patient outcomes
3. Test model performance on English-translated CT reports or bilingual models to isolate reasoning capability from language-specific factors