---
ver: rpa2
title: Categorical Reparameterization with Denoising Diffusion models
arxiv_id: '2601.00781'
source_url: https://arxiv.org/abs/2601.00781
tags:
- diffusion
- categorical
- distribution
- where
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces REDGE, a diffusion-based soft reparameterization
  for categorical distributions that leverages the fact that, for categorical targets,
  the denoising distribution is discrete and its denoiser can be computed in closed
  form. By connecting this denoiser to a continuous relaxation, REDGE defines a differentiable
  sampling map from Gaussian noise to the target distribution without requiring training
  of a denoiser network.
---

# Categorical Reparameterization with Denoising Diffusion models

## Quick Facts
- **arXiv ID**: 2601.00781
- **Source URL**: https://arxiv.org/abs/2601.00781
- **Reference count**: 40
- **Primary result**: REDGE provides a training-free, differentiable sampling map from Gaussian noise to categorical distributions, matching or outperforming existing gradient estimators across Sudoku solving, variational inference, and reward-guided image generation benchmarks.

## Executive Summary
This work introduces REDGE, a diffusion-based soft reparameterization for categorical distributions that leverages the fact that, for categorical targets, the denoising distribution is discrete and its denoiser can be computed in closed form. By connecting this denoiser to a continuous relaxation, REDGE defines a differentiable sampling map from Gaussian noise to the target distribution without requiring training of a denoiser network. The method recovers the STRAIGHT-THROUGH and REINMAX estimators as one-step special cases and admits natural hard and REINMAX extensions. Theoretically, the analysis of the small-noise regime explains the emergence of uninformative gradients and informs hyperparameter choices. Empirically, REDGE matches or outperforms existing gradient estimators across benchmarks in Sudoku solving, variational inference, and reward-guided image generation, with improved robustness and less hyperparameter sensitivity than many alternatives.

## Method Summary
REDGE uses a denoising diffusion process to create a differentiable sampling map for categorical distributions. Unlike prior diffusion approaches that require training a denoiser network, REDGE exploits the fact that for categorical distributions, the posterior mean denoiser has a closed-form expression (softmax of logits plus a scaled Gaussian noise term). The method employs DDIM deterministic reverse transitions to transform standard Gaussian noise into relaxed categorical samples. The resulting gradient estimator is computed by backpropagating through the DDIM map, with the timestep t₁ functioning analogously to temperature, controlling the bias-variance trade-off. REDGE recovers STRAIGHT-THROUGH and REINMAX as special cases and offers variants with parameter-dependent base distributions and REINMAX corrections.

## Key Results
- REDGE matches or outperforms STRAIGHT-THROUGH, Gumbel-Softmax, and other gradient estimators on Sudoku solving, variational inference, and reward-guided image generation tasks
- The method is training-free and requires no denoiser network, unlike prior diffusion approaches
- REDGE shows improved robustness to hyperparameter choices, particularly with moderate t₁ values (0.5-0.7) and 3-5 diffusion steps
- Theoretical analysis explains why small t₁ values lead to uninformative gradients due to vanishing Jacobian norms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The denoiser for categorical distributions has a closed-form expression, enabling training-free differentiable sampling.
- Mechanism: For a categorical distribution π_θ with logits φ_θ, the posterior-mean denoiser under Gaussian noising simplifies to softmax(φ_θ + α_t x_t / σ²_t). This removes the need to train a neural network denoiser.
- Core assumption: The categorical distribution factorizes across L independent variables (Eq. 2), enabling row-wise softmax computation.
- Evidence anchors:
  - [abstract] "the denoiser under a Gaussian noising process admits a closed form and can be computed efficiently, yielding a training-free diffusion sampler"
  - [section 3.2] Eq. 11 provides the explicit denoiser formula
  - [corpus] Weak direct support; corpus focuses on other diffusion applications
- Break condition: If categorical variables are not independent (e.g., structured predictions with strong dependencies), the factorization assumption fails.

### Mechanism 2
- Claim: DDIM deterministic transport provides a differentiable path from Gaussian noise to relaxed categorical samples.
- Mechanism: The DDIM map T_θ(X₁) transforms standard Gaussian samples through iterative denoising. Backpropagation through this map yields gradient estimates: J_θ T_θ(X₁)^T ∇f(X₀) (REDGE).
- Core assumption: The function f extends differentiably to the simplex; the choice of extension affects gradient behavior (Remark 1).
- Evidence anchors:
  - [section 3.2] "T_θ(X₁) with X₁ ~ N(0,I_K)⊗L is an approximate sample from the Gaussian mixture... T_θ₀(X₁) is an approximate relaxed sample from π_θ"
  - [abstract] "enables a training-free, differentiable sampling map from Gaussian noise to the categorical distribution"
  - [corpus] Not directly addressed
- Break condition: If the transport map approaches piecewise-constant behavior (small t₁), Jacobians vanish (Proposition 1).

### Mechanism 3
- Claim: The timestep t₁ functions analogously to temperature, controlling the bias-variance trade-off.
- Mechanism: As t₁→0, the final DDIM step collapses nearly all points to a single one-hot vector, causing Jacobian norms to vanish exponentially (Proposition 2). Moderate t₁ (0.3-0.9) provides informative gradients.
- Core assumption: Schedule satisfies lim_{t→0} α_t/σ²_t = ∞ (Assumption A1).
- Evidence anchors:
  - [section 3.2] Proposition 1: "lim_{t₁→0} ||J_θ T_θ₀(X₁)|| = 0, P-a.s."
  - [section 5] "a small number of diffusion steps (e.g., n=3,5) coupled with a moderate t₁ (e.g. t₁ ∈ {0.5,0.7,0.9}) is a strong default"
  - [corpus] Not addressed
- Break condition: Very small t₁ (<0.3) with few steps causes gradient uninformative; very large t₁ (~1.0) recovers STRAIGHT-THROUGH behavior.

## Foundational Learning

- Concept: **Reparameterization trick**
  - Why needed here: REDGE extends the reparameterization trick to categorical distributions by constructing T_θ such that samples X₀ ≈ T_θ(Z) with Z ~ N(0,I).
  - Quick check question: Can you explain why Eq. (6) fails for discrete distributions but holds for continuous relaxations?

- Concept: **DDIM sampler and interpolation view**
  - Why needed here: The method uses DDIM's deterministic reverse transitions (η_s = 0) rather than stochastic DDPM transitions.
  - Quick check question: How does the interpolation X_t = α_t X₀ + σ_t X₁ (Eq. 7) define the noising process?

- Concept: **Covariance of categorical distributions**
  - Why needed here: Lemma 1 shows J_θ x̂_θ⁰(x_t, t) = Σ_θ^t(x), the categorical covariance; this appears in all gradient estimators.
  - Quick check question: Why is Cov_π(X) = Diag(π) - ππ^T for a categorical distribution π?

## Architecture Onboarding

- Component map: Input logits φ_θ ∈ R^{L×K} → Sample X₁ ~ N(0,I) → Apply DDIM map T_θ^{t_k} for k=n-1,...,0 → Get relaxed sample T_θ⁰(X₁) → Compute J_θ T_θ⁰(X₁)^T via chain rule → Multiply by ∇f(X₀)

- Critical path:
  1. Denoiser computation: softmax(φ_θ + α_t X_t/σ²_t) — O(LK) per step
  2. DDIM update: X_s = (α_s - α_t σ_s/σ_t)x̂_θ⁰ + (σ_s/σ_t)X_t (Eq. 9)
  3. Gradient backprop through n-1 DDIM steps

- Design tradeoffs:
  - **n (steps)**: More steps improve sample fidelity but increase compute; n=3-5 typically sufficient (Fig. 7)
  - **t₁ (cutoff)**: Controls relaxation strength; t₁ ~0.5-0.7 balances bias and gradient quality
  - **Base distribution**: Standard N(0,I) vs. parameter-dependent π₁^θ (REDGE-COV) — latter stronger but more sensitive

- Failure signatures:
  - Gradients vanishing: t₁ too small (<0.3) with few steps; check Jacobian norms
  - Poor sample quality: Large |1-t₁| with very small n; increase n or t₁
  - High variance: Consider REINDGE variant for quadratic-like objectives

- First 3 experiments:
  1. **Sanity check**: Binary case (K=2, L=10), polynomial objective (Appendix F); verify REDGE reaches optimal solution
  2. **Sudoku guidance**: Start from 1% checkpoint, optimize reward r(x) = Σ_g ||s_g(x) - 1/9||²; confirm solve rate improvement over STRAIGHT-THROUGH
  3. **Ablation on (t₁, n)**: Fix θ, compute gradient mean/variance across 500 samples; verify instability at small t₁ + large n (Fig. 8)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can REDGE be effectively integrated into REBAR/RELAX-style frameworks to serve as a control variate that reduces residual bias?
- Basis in paper: [explicit] The conclusion states, "a promising direction is to reduce residual bias using REBAR/RELAX-style control variates, treating REDGE as a strong base pathwise estimator."
- Why unresolved: The authors focused on comparing against pathwise baselines (Gumbel-Softmax, ST) and explicitly omitted meta-estimators like REBAR in their experiments.
- What evidence would resolve it: Empirical results showing that using REDGE as a control variate lowers variance and bias compared to standard REBAR in variational inference tasks.

### Open Question 2
- Question: Can the timestep $t_1$ and number of diffusion steps $n$ be selected adaptively or theoretically without requiring a grid search?
- Basis in paper: [inferred] Section 5 mentions "improving default schedules" as future work, and Section 4.1.1 demonstrates that performance is sensitive to these hyperparameters.
- Why unresolved: While Proposition 1 proves gradients vanish as $t_1 \to 0$, the paper relies on empirical sweeps (Section G.1) to find the "sweet spot" (e.g., $t_1 \in \{0.5, 0.7\}$).
- What evidence would resolve it: A heuristic or closed-form update rule for $t_1$ that maintains gradient informativeness across different optimization landscapes.

### Open Question 3
- Question: Does the REDGE-COV variant scale to extremely large vocabularies without relying on ad-hoc fixes like diagonal clamping?
- Basis in paper: [inferred] Section 3.3 and G.3 note that for large $K$, the diagonal covariance becomes "ill-conditioned," forcing the authors to use a scalar variant or clamp values.
- Why unresolved: The paper introduces heuristics to handle large $K$ (e.g., in the MaskGIT experiment) but does not analyze the theoretical limits of the covariance estimation.
- What evidence would resolve it: A theoretical analysis of the condition number of the diffusion covariance matrix as $K \to \infty$, or experiments on large-vocabulary language models without clamping.

## Limitations
- **Independence assumption**: The closed-form denoiser relies on the categorical factorization across L independent variables. For structured categorical distributions with dependencies (e.g., sequence models with strong local correlations), this assumption breaks down and the denoiser becomes intractable.
- **Gradient quality dependence**: While REDGE avoids training a denoiser, the gradient quality still depends on the diffusion schedule and the function's extension to the simplex. Poorly chosen schedules (very small t₁ with few steps) can yield uninformative gradients.
- **Memory and compute scaling**: Memory and computation scale linearly with both L (categories) and n (steps). For high-dimensional categorical problems with many classes, this could become prohibitive.

## Confidence
- **High confidence**: The closed-form denoiser derivation (Mechanism 1) and DDIM transport framework (Mechanism 2) are mathematically sound given the independence assumption. Empirical results showing REDGE's competitive performance across multiple benchmarks are robust.
- **Medium confidence**: The theoretical analysis of the small-noise regime (Mechanism 3) provides intuition but makes simplifying assumptions. The recommendation to use n=3-5 steps with t₁ ∈ {0.5,0.7,0.9} is empirically validated but may not generalize to all problem structures.
- **Low confidence**: The behavior on structured categorical distributions (e.g., with strong dependencies) and the method's performance on extremely high-dimensional categorical problems have not been thoroughly explored.

## Next Checks
1. **Structured categorical test**: Apply REDGE to a categorical sequence model with strong local dependencies (e.g., character-level language modeling) and compare against baselines. Measure both gradient quality and final task performance.
2. **Jacobian norm analysis**: Systematically measure ||J_θ T_θ⁰(X₁)|| across different (n, t₁) pairs on a synthetic categorical optimization problem. Verify the theoretical prediction that norms vanish exponentially as t₁→0.
3. **Memory scaling experiment**: Benchmark REDGE on categorical distributions with increasing L (e.g., L ∈ {10, 100, 1000}) and measure memory usage and wall-clock time. Compare against REBAR and RELAX baselines to identify practical limits.