---
ver: rpa2
title: 'The Law of Knowledge Overshadowing: Towards Understanding, Predicting, and
  Preventing LLM Hallucination'
arxiv_id: '2502.16143'
source_url: https://arxiv.org/abs/2502.16143
tags:
- knowledge
- arxiv
- hallucination
- language
- overshadowing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the persistent problem of hallucination in
  large language models (LLMs), where models generate distorted facts despite high-quality
  training data. The authors propose a novel concept called "knowledge overshadowing,"
  where a model's dominant knowledge can obscure less prominent knowledge during text
  generation, leading to hallucinations.
---

# The Law of Knowledge Overshadowing: Towards Understanding, Predicting, and Preventing LLM Hallucination

## Quick Facts
- **arXiv ID**: 2502.16143
- **Source URL**: https://arxiv.org/abs/2502.16143
- **Reference count**: 40
- **Primary result**: CoDA decoding improves factuality by 13.1%, 18.3%, and 27.9% on three benchmark datasets

## Executive Summary
This paper addresses the persistent problem of hallucination in large language models (LLMs), where models generate distorted facts despite high-quality training data. The authors propose a novel concept called "knowledge overshadowing," where a model's dominant knowledge can obscure less prominent knowledge during text generation, leading to hallucinations. They introduce a log-linear law that predicts hallucination rates increase linearly with the logarithmic scale of knowledge popularity, knowledge length, and model size. Based on this understanding, they propose a new decoding strategy called CoDA (Contrastive Decoding to Amplify Overshadowed Knowledge) to mitigate hallucinations. CoDA identifies overshadowed knowledge by computing mutual information between token probability distributions and uses contrastive decoding to reduce bias from dominant knowledge. This approach achieves significant improvements in factuality, with gains of 13.1%, 18.3%, and 27.9% on MemoTrap, NQ-Swap, and Overshadowing datasets respectively, without requiring additional training.

## Method Summary
The paper introduces the concept of knowledge overshadowing as a mechanism for LLM hallucinations, where dominant knowledge in a model's representation can suppress less prominent but potentially correct information during generation. The authors develop a log-linear law that quantifies how hallucination rates increase with knowledge popularity, knowledge length, and model size on a logarithmic scale. To address this issue, they propose CoDA (Contrastive Decoding to Amplify Overshadowed Knowledge), a decoding strategy that identifies overshadowed knowledge by computing mutual information between token probability distributions. CoDA then applies contrastive decoding to reduce the bias from dominant knowledge, thereby amplifying the overshadowed but correct information. The method is evaluated on three benchmark datasets (MemoTrap, NQ-Swap, and Overshadowing) and demonstrates significant improvements in factuality metrics without requiring additional training.

## Key Results
- CoDA decoding improves factuality by 13.1% on MemoTrap dataset
- CoDA decoding improves factuality by 18.3% on NQ-Swap dataset
- CoDA decoding improves factuality by 27.9% on Overshadowing dataset

## Why This Works (Mechanism)
The mechanism works by addressing the fundamental issue of knowledge overshadowing, where dominant knowledge in a model's representation suppresses less prominent but potentially correct information during generation. When a model has multiple pieces of knowledge about a topic, the more prominent or popular knowledge tends to dominate the probability distribution over tokens during decoding. This dominance occurs because language models assign higher probabilities to tokens associated with more frequently encountered or longer knowledge pieces during training. The CoDA method identifies this overshadowing effect by computing mutual information between token probability distributions, which reveals when one knowledge piece is suppressing another. By applying contrastive decoding, CoDA explicitly reduces the influence of dominant knowledge while amplifying the overshadowed knowledge, allowing the model to generate more factually accurate responses.

## Foundational Learning

**Knowledge Overshadowing**: The phenomenon where dominant knowledge in a model's representation suppresses less prominent but potentially correct information during generation. *Why needed*: This explains a fundamental mechanism behind hallucinations that traditional training approaches cannot address. *Quick check*: Verify that knowledge overshadowing occurs when multiple knowledge pieces exist for the same topic.

**Log-linear Law**: A mathematical relationship showing that hallucination rates increase linearly with the logarithmic scale of knowledge popularity, knowledge length, and model size. *Why needed*: Provides a quantitative framework for predicting hallucination likelihood based on model and knowledge characteristics. *Quick check*: Test whether hallucination rates scale logarithmically with the three specified factors.

**Mutual Information Computation**: A statistical measure used to identify when one knowledge piece is overshadowing another by analyzing token probability distributions. *Why needed*: Enables detection of overshadowing effects during decoding without requiring model retraining. *Quick check*: Confirm that mutual information scores correlate with knowledge overshadowing instances.

## Architecture Onboarding

**Component Map**: Input text -> Token probability distribution -> Mutual information computation -> Contrastive decoding -> Output text

**Critical Path**: The most critical component is the mutual information computation between token probability distributions, as this identifies the overshadowed knowledge that needs amplification. Without accurate detection of overshadowing, the contrastive decoding cannot effectively mitigate hallucinations.

**Design Tradeoffs**: The method trades computational overhead during inference (additional mutual information calculations) for improved factuality without requiring expensive retraining. This makes it particularly valuable for deployed models where retraining is impractical.

**Failure Signatures**: The approach may fail when overshadowed knowledge is incorrect, potentially amplifying wrong information. It may also struggle with knowledge types that don't follow the log-linear relationship, such as highly contextual or abstract knowledge that doesn't have clear popularity metrics.

**First Experiments**: 
1. Apply CoDA to a simple model with known knowledge overshadowing cases to verify the mechanism works as intended
2. Test CoDA on a single knowledge pair to understand how mutual information detection identifies overshadowing
3. Compare factuality improvements on a small dataset before scaling to larger benchmarks

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses primarily on knowledge-based hallucinations rather than exploring other hallucination types such as fabrication or reasoning errors
- Experimental validation is limited to specific datasets (MemoTrap, NQ-Swap, Overshadowing) and model architectures
- The log-linear law, while empirically supported, requires further theoretical justification to establish its universal applicability across different domains and knowledge types

## Confidence

**High confidence**: 
- Empirical demonstration of CoDA's effectiveness on the tested datasets
- Observation that knowledge overshadowing correlates with hallucination rates

**Medium confidence**: 
- Generalizability of the log-linear law across different domains
- Universality of the knowledge overshadowing mechanism

**Low confidence**: 
- Theoretical completeness of the overshadowing explanation for all hallucination phenomena

## Next Checks
1. Test CoDA across diverse domains beyond factual question answering to assess generalizability
2. Conduct ablation studies to isolate the specific contribution of knowledge overshadowing versus other hallucination mechanisms
3. Evaluate long-term performance and potential side effects of CoDA decoding in production environments