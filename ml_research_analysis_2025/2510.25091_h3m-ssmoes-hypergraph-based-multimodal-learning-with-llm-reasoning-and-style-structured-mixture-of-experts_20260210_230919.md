---
ver: rpa2
title: 'H3M-SSMoEs: Hypergraph-based Multimodal Learning with LLM Reasoning and Style-Structured
  Mixture of Experts'
arxiv_id: '2510.25091'
source_url: https://arxiv.org/abs/2510.25091
tags:
- stock
- market
- arxiv
- experts
- hypergraph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: H3M-SSMoEs introduces a multimodal framework that unifies hypergraph-based
  structural modeling, LLM-enhanced semantic reasoning, and style-structured MoEs
  for stock movement prediction. It uses a hierarchical Multi-Context Multimodal Hypergraph
  to capture fine-grained spatiotemporal dynamics and persistent inter-stock dependencies
  via shared cross-modal hyperedges, integrates a frozen LLM with lightweight adapters
  to semantically fuse quantitative and textual modalities, and employs a Style-Structured
  Mixture of Experts with learnable style vectors to enable adaptive, regime-aware
  specialization under sparse activation.
---

# H3M-SSMoEs: Hypergraph-based Multimodal Learning with LLM Reasoning and Style-Structured Mixture of Experts

## Quick Facts
- arXiv ID: 2510.25091
- Source URL: https://arxiv.org/abs/2510.25091
- Reference count: 40
- Primary result: Achieves Sharpe ratios of 1.585, 2.100, and 1.351 on DJIA, NASDAQ 100, and S&P 100 respectively

## Executive Summary
H3M-SSMoEs introduces a multimodal framework for stock movement prediction that unifies hypergraph-based structural modeling, LLM-enhanced semantic reasoning, and style-structured mixture of experts. The method captures fine-grained spatiotemporal dynamics and persistent inter-stock dependencies through hierarchical hypergraphs, semantically fuses quantitative and textual modalities using a frozen LLM with lightweight adapters, and enables adaptive regime-aware specialization via learnable style vectors in a sparse MoE architecture. Experiments on major U.S. stock indices demonstrate state-of-the-art performance with robust risk-adjusted returns.

## Method Summary
H3M-SSMoEs processes multimodal financial data through a hierarchical architecture. Local and Global Hypergraphs (LCH/GCH) capture fine-grained and persistent inter-stock relationships respectively, using cross-modal hyperedges and Jensen-Shannon Divergence weighting. A frozen Llama-3.2-1B LLM with lightweight adapters semantically fuses quantitative and textual modalities. Style-Structured Mixture of Experts with learnable style vectors enables regime-aware specialization through Top-K routing to market and industry-specific expert pools. The framework outputs binary classification predictions for stock price movements d days ahead.

## Key Results
- Achieves Sharpe ratios of 1.585 (DJIA), 2.100 (NASDAQ 100), and 1.351 (S&P 100)
- Records Calmar ratios of 3.377, 4.380, and 2.075 respectively
- Demonstrates lowest maximum drawdowns at 14.81%, 16.17%, and 14.27% across benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Multi-Scale Hypergraph Structural Learning
- Claim: Higher-order group relationships captured via hyperedges improve over pairwise graph models for market dynamics.
- Mechanism: LCH constructs hyperedges over stock-time instances to capture coordinated behaviors; GCH constructs hyperedges over stock-level representations for persistent industry structures. Both use cross-modal hyperedges connecting quantitative and news nodes, enabling bidirectional information flow via hypergraph convolution. JSD weighting emphasizes structurally informative hyperedges while suppressing redundancy.
- Core assumption: Market co-movements exhibit group-wise structure (sectoral shifts, supply-chain disruptions) that pairwise graphs fragment; hyperedges preserve these naturally.
- Evidence anchors:
  - [abstract] "shared cross-modal hyperedges and Jensen-Shannon Divergence weighting mechanism for adaptive relational learning"
  - [section 3.2.1] "four distinct sub-hypergraphs corresponding to intra- and inter-modal relationships"
  - [corpus] Related work (MaGNet, FactorGCL) validates hypergraph utility for stock prediction; corpus confirms this is an active research direction.
- Break condition: If asset relationships are primarily bilateral (e.g., currency pairs) or markets exhibit regime-independent independence across groups, hypergraph overhead may not justify gains.

### Mechanism 2: Frozen-LLM Semantic Enhancement with Lightweight Adaptation
- Claim: Pre-trained LLM knowledge bridges the semantic gap between discrete text and continuous time series better than domain-specific encoders alone.
- Mechanism: Frozen Llama-3.2-1B processes concatenated [Z^GCH_quant, Z^GCH_news] via projection into LLM space (D_LLM=2048). Lightweight adapter layers perform modality alignment; the LLM serves as a reasoning engine enriching multimodal features with financial domain knowledge while keeping parameters frozen.
- Core assumption: LLM pre-training captures transferable financial semantics; alignment via adapters suffices without fine-tuning.
- Evidence anchors:
  - [abstract] "leverages a frozen large language model with lightweight adapters to semantically fuse and align quantitative and textual modalities"
  - [section 3.3] "freezing the LLM parameters preserves its extensive linguistic and financial domain-specific knowledge"
  - [corpus] Related work (LLM-Based Routing in MoE, Multi-Modal Hypergraph Enhanced LLM) supports LLM integration for multimodal reasoning, though specific financial adapter configurations remain underexplored.
- Break condition: If textual data is sparse, low-quality, or LLM pre-training lacks domain coverage, frozen LLM may add noise and compute without benefit.

### Mechanism 3: Style-Structured Expert Routing for Regime Adaptation
- Claim: Learnable style vectors parameterizing experts enable regime-aware specialization under sparse activation.
- Mechanism: Two expert pools—Shared Market Experts (N_mkt=3-5) and Industry-specialized Experts (N_ind=6-10)—each maintain learnable style vectors (s^mkt_j, s^ind_k ∈ R^Ds). Market state vector m aggregates cross-stock information; industry embeddings I derive from H_global. Top-K gating (K=2) routes inputs to relevant experts; outputs aggregated via learned nonlinear fusion.
- Core assumption: Market regimes (bull/bear/volatile) and industry dynamics are separable and benefit from specialized processing; sparse routing preserves expressiveness while reducing compute.
- Evidence anchors:
  - [abstract] "Style-Structured Mixture of Experts with learnable style vectors to enable adaptive, regime-aware specialization under sparse activation"
  - [section 3.4] "style vectors evolve into distinct market archetypes—such as bullish, bearish, or neutral"
  - [corpus] "LLM-Based Routing in Mixture of Experts: A Novel Framework for Trading" (arXiv:2501.09636) directly addresses MoE routing in trading; corpus confirms growing interest but limited empirical validation in financial settings.
- Break condition: If market states are highly non-stationary with no repeatable patterns, or expert count is insufficient for regime coverage, routing degrades to near-uniform mixing.

## Foundational Learning

- Concept: **Hypergraph Neural Networks**
  - Why needed here: Core structural model; hyperedges connect multiple nodes simultaneously, enabling group-wise relationship modeling beyond pairwise graphs.
  - Quick check question: Can you explain how a hyperedge differs from a standard graph edge, and why this matters for sectoral co-movement?

- Concept: **Mixture of Experts with Sparse Gating**
  - Why needed here: Enables scalable, regime-adaptive processing; Top-K routing activates only relevant experts per input.
  - Quick check question: Given routing logits for 5 experts [2.1, 0.5, 1.8, -0.3, 0.9] and K=2, which experts are activated and what are their gating weights after softmax?

- Concept: **Cross-Modal Alignment via LLM Embedding Spaces**
  - Why needed here: Unifies continuous time-series and discrete text modalities into shared semantic space for joint reasoning.
  - Quick check question: Why might a frozen LLM with adapters outperform fine-tuning for multimodal financial tasks? What are the tradeoffs?

## Architecture Onboarding

- Component map: Input → [LCH + GCH in parallel] → LLM Fusion → SSMoEs → Classification
- Critical path: Input → [LCH + GCH in parallel] → LLM Fusion → SSMoEs → Classification. JSD weighting and Top-K gating are differentiable throughout.
- Design tradeoffs:
  - Frozen LLM vs. fine-tuned: Freezing reduces compute/catastrophic forgetting risk; adapters may limit domain adaptation depth.
  - Hyperedge count (E1, E2): Higher values capture more structures but increase noise and compute. Paper uses 32-64.
  - Expert count (N_mkt=3-5, N_ind=6-10): More experts increase specialization capacity but require more data to train; auxiliary loss (α,β=0.1) encourages balanced utilization.
- Failure signatures:
  - Precision drops with accuracy stable: May indicate miscalibrated probabilities; check gating weight distribution for collapse.
  - Sharpe ratio much lower than annual return suggests: High volatility or MDD; inspect drawdown periods and expert routing patterns.
  - w/o LCH ablation causes severe degradation (per Table 5): Confirms LCH is critical; if removing it barely changes results, implementation may be incorrect.
  - Expert utilization highly imbalanced: Auxiliary loss may be too weak; increase α, β.
- First 3 experiments:
  1. **Sanity check**: Run w/o LCH, w/o LLM, w/o SSMoEs ablations on DJIA validation set. Confirm degradation patterns match Table 5 (~70-85% return drop per component).
  2. **Hyperedge sensitivity**: Vary E1, E2 ∈ {16, 32, 64, 128} on NASDAQ-100. Track validation Sharpe and training time; identify knee point.
  3. **Routing analysis**: Log Top-K expert activations per stock over validation period. Visualize whether market experts correlate with identified regimes (e.g., VIX-based) and industry experts cluster by sector.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the framework's performance generalize to non-US markets with different regulatory environments and information disclosure practices?
- Basis in paper: [explicit] The authors evaluate exclusively on DJIA, NASDAQ 100, and S&P 100 indices, stating these demonstrate performance "on three major stock markets" without acknowledging the limitation to US-only markets.
- Why unresolved: Market microstructure, information asymmetry, and regulatory disclosure requirements differ substantially across regions (e.g., emerging markets, Asian exchanges), potentially affecting both the hypergraph relationship modeling and the LLM's ability to interpret regional financial semantics.
- What evidence would resolve it: Systematic evaluation on non-US indices (e.g., FTSE 100, Nikkei 225, CSI 300) with comparable data availability and experimental protocols.

### Open Question 2
- Question: Does synthetic news generated by FinRobot capture the same predictive signal as authentic market news, or does it introduce distributional shift that inflates performance?
- Basis in paper: [explicit] The authors state: "we employed the Finrobot to generate daily news for each stock... This approach ensures consistent and high-quality textual data... effectively addressing the issue of incomplete or missing news coverage."
- Why unresolved: Generated news may reflect patterns more consistent with the model's training distribution than real-world news, which contains noise, contradictions, and delayed reporting that the LLM must handle. The synthetic approach sidesteps a core challenge of multimodal financial forecasting.
- What evidence would resolve it: Ablation comparing model performance using real news sources (e.g., Bloomberg, Reuters) versus FinRobot-generated news on the same time periods, with analysis of distributional differences.

### Open Question 3
- Question: Do the learnable style vectors in SSMoEs converge to interpretable, stable market archetypes as claimed, or do they form entangled, uninterpretable representations?
- Basis in paper: [explicit] The authors claim: "Through training, these style vectors evolve into distinct market archetypes—such as bullish, bearish, or neutral—enabling the ensemble to capture a diverse and adaptive set of prediction and trading behaviors."
- Why unresolved: No empirical analysis (e.g., clustering, visualization, correlation with known market regimes) validates that style vectors actually encode meaningful archetypes rather than arbitrary features. The interpretability claim remains theoretical.
- What evidence would resolve it: Analysis of learned style vectors across market conditions (e.g., 2020 COVID crash, 2022 rate hikes), correlation with volatility indices, and qualitative inspection of expert activation patterns during identified regime shifts.

### Open Question 4
- Question: Does the frozen Llama-3.2-1B model possess sufficient domain-specific financial knowledge for this task, or would domain-specialized LLMs yield improved semantic alignment?
- Basis in paper: [inferred] The authors justify using a frozen 1B parameter LLM as a "trade-off between semantic reasoning capacity and computational efficiency" without comparing against larger general LLMs or domain-specific financial LLMs (e.g., BloombergGPT, FinGPT).
- Why resolved: The LLM component contributes substantial performance gains (ablation shows 16.50% vs 50.00% annual return on DJIA), yet the choice of specific LLM backbone is unexamined. A suboptimal LLM could limit the ceiling of semantic enhancement.
- What evidence would resolve it: Comparative experiments with (1) larger frozen LLMs (e.g., Llama-3.1-8B), (2) domain-specific financial LLMs, and (3) fine-tuned variants, measuring both predictive performance and computational cost.

## Limitations

- **Reproducibility challenges**: Frozen LLM adapter architecture remains underspecified, creating uncertainty about whether reproduced results will match reported performance.
- **Limited market coverage**: Experimental validation confined to three U.S. equity indices with limited market conditions coverage, insufficient for establishing robustness across diverse economic cycles.
- **Synthetic data concerns**: FinRobot-generated news may introduce distributional shift that inflates performance, as authentic market news contains noise and delayed reporting patterns.

## Confidence

- **High Confidence**: Structural claims about hypergraph modeling capturing group relationships - supported by direct textual evidence and related hypergraph literature. Performance metrics on specified benchmarks - directly reported with clear methodology.
- **Medium Confidence**: LLM integration effectiveness - theoretical rationale is sound, but specific adapter configurations and domain adaptation depth remain unclear. Regime-aware MoE routing - architecture is well-specified, but expert specialization patterns under real market regimes require empirical validation.
- **Low Confidence**: Generalizability beyond tested market conditions and asset classes. Claims about superiority over pairwise graph models assume market co-movements exhibit persistent group structures, which may not hold during structural breaks.

## Next Checks

1. **Adapter Architecture Verification**: Implement the frozen LLM fusion module with multiple adapter configurations (LoRA ranks 16, 32, 64; bottleneck sizes 128, 256). Compare validation performance to establish sensitivity to adapter design choices and verify whether reported results require specific configurations.

2. **Cross-Market Regime Testing**: Apply H3M-SSMoEs to a dataset spanning multiple market regimes (e.g., 2015-2025 including 2018 volatility, 2020 COVID, 2022 inflation) and test on international markets (e.g., FTSE 100, Nikkei 225). Track expert routing patterns across regime transitions to validate regime-aware specialization claims.

3. **Pairwise vs. Hyperedge Ablation**: Systematically vary hyperedge density (E1, E2 from 16 to 128) while maintaining computational budget. Compare against a pairwise graph baseline with equivalent parameter count. Measure both predictive performance and computational efficiency to establish the marginal benefit of hyperedges versus their overhead.