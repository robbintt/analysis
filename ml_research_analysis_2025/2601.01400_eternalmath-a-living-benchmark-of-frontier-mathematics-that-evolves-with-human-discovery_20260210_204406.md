---
ver: rpa2
title: 'EternalMath: A Living Benchmark of Frontier Mathematics that Evolves with
  Human Discovery'
arxiv_id: '2601.01400'
source_url: https://arxiv.org/abs/2601.01400
tags:
- mathematical
- graph
- problem
- reasoning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the need for evolving benchmarks to evaluate
  LLMs on frontier-level mathematical reasoning, moving beyond static, competition-derived
  datasets that are prone to data contamination and performance saturation. The authors
  propose a fully automated, theorem-grounded pipeline that transforms recent peer-reviewed
  mathematical literature into executable, verifiable reasoning tasks through parameterized
  problem templates and deterministic solution generation.
---

# EternalMath: A Living Benchmark of Frontier Mathematics that Evolves with Human Discovery

## Quick Facts
- arXiv ID: 2601.01400
- Source URL: https://arxiv.org/abs/2601.01400
- Reference count: 40
- State-of-the-art LLMs achieve only ~49% accuracy on frontier mathematical reasoning tasks

## Executive Summary
The paper introduces EternalMath, a fully automated pipeline that transforms recent peer-reviewed mathematical literature into executable, verifiable reasoning tasks for evaluating LLMs. By filtering for theorems published within 1-2 years of the model's training cutoff, the benchmark achieves contamination resistance while maintaining high discriminative power through model-based difficulty stratification. Experiments show substantial performance gaps—even the best models achieve only around 49% accuracy, with accuracy dropping sharply on harder problems—indicating that mathematical reasoning at the research frontier remains far from saturated.

## Method Summary
The pipeline transforms recent peer-reviewed mathematical literature into parameterized, executable reasoning tasks through a four-phase process: (1) paper filtering for recency and constructiveness, (2) multi-agent collaboration using Classification, Meta-Template Generator, Code Translator, and Execution/Validator agents, (3) automated execution with Python symbolic libraries, and (4) quality assurance through avgLogprobs filtering and human spot-check. The output is 782 problems from 891 templates, stratified by difficulty based on model performance.

## Key Results
- SOTA models achieve only ~49% accuracy on EternalMath benchmark
- Accuracy drops sharply on harder problems, with top models scoring below 10% on Hard tier
- 98% precision from expert audit confirms solution correctness
- Benchmark demonstrates substantial contamination resistance through temporal filtering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: **Contamination resistance is achieved by sourcing problems from literature more recent than the model's training cutoff.**
- Mechanism: The pipeline filters for papers published within the last 1–2 years, creating a temporal barrier that makes it statistically unlikely that test cases exist verbatim in the model's pre-training corpus.
- Core assumption: Models cannot generalize perfectly to novel, unseen mathematical structures if the specific instantiation or theorem is newer than their knowledge cutoff.
- Evidence anchors:
  - [abstract] The paper states the pipeline "directly transforms recent peer-reviewed mathematical literature" to enable "contamination-resistant evaluation."
  - [section 3.1] Explicitly defines "Recency" criteria: "prioritize papers published within the past one to two years... reducing the likelihood that the underlying results have already been incorporated into the pretraining data."
  - [corpus] Related work like *RealMath* confirms the difficulty of finding research-level math not already exposed to models, validating the need for temporal filtering.
- Break condition: If a model is trained on "future" data or if "novel" theorems are isomorphic to older, known problems in the training set, the mechanism fails.

### Mechanism 2
- Claim: **Ground-truth reliability is maintained by enforcing executable verification rather than relying on LLM-as-judge or human grading.**
- Mechanism: The pipeline converts abstract theorems into parameterized Python scripts. A problem is only admitted if the generated code executes successfully and produces a deterministic numerical or symbolic output.
- Core assumption: Constructive or quantitative mathematical results can be accurately translated into executable code that faithfully represents the theorem's constraints.
- Evidence anchors:
  - [abstract] Mentions "generates deterministic solutions through execution-based verification" and "intrinsic correctness checking."
  - [section 3.2] Describes the "Code Translator Agent" and "Execution and Validator Agent" which ensure problems "admit numerical instantiation and automated verification."
- Break condition: If the Code Translation Agent hallucinates a script that executes without error but implements incorrect mathematical logic, the ground truth becomes invalid.

### Mechanism 3
- Claim: **High discriminative power is achieved via model-based difficulty filtering.**
- Mechanism: The pipeline uses existing frontier models as a filter. Candidate problems that are solved correctly by *all* tested models are discarded as "too easy." The surviving problems are stratified by how many models fail them, ensuring the final benchmark maintains a "Hard" tier where current SOTA models score below 10%.
- Core assumption: If a problem is solvable by current SOTA models, it likely relies on memorized patterns or simple heuristics rather than deep reasoning, and thus has low discriminative value for *future* frontier models.
- Evidence anchors:
  - [section 3.3] "Any instance solved correctly by all models across multiple trials was excluded to remove tasks solvable by shallow heuristics."
  - [figure 5] Shows the resulting stratification where "Hard" tasks drop SOTA model accuracy to single digits.
- Break condition: If the "Easy" tasks solvable by models are actually novel, difficult problems that the models just happened to guess correctly, the filtering mechanism erroneously removes high-value data.

## Foundational Learning

- Concept: **Parameterized Problem Templates**
  - Why needed here: The pipeline generates *meta-templates* (JSON) where variables are placeholders, not static text. Understanding this abstraction is key to maintaining or extending the benchmark.
  - Quick check question: Can you explain the difference between a static math problem and a parameterized meta-template in the context of this pipeline?

- Concept: **Constructive vs. Qualitative Mathematics**
  - Why needed here: The paper explicitly filters for "constructive or quantitative results." You must distinguish between a theorem that proves existence (qualitative) and one that provides a formula or bound (quantitative/constructive) to select valid source papers.
  - Quick check question: Why would a theorem stating "A solution exists for all $x$" be difficult to verify using the Code Translator Agent described in the paper?

- Concept: **Data Contamination (Leakage)**
  - Why needed here: The primary motivation for EternalMath is avoiding saturation seen in GSM8K/MATH. You need to understand that "solving" a benchmark is invalid if the model saw the answers during training.
  - Quick check question: How does the "Recency" filter in Section 3.1 specifically combat data contamination?

## Architecture Onboarding

- Component map: Source Filter (ArXiv/Journals) → Classification Agent (MSC Tags) → Meta-Template Generator (JSON) → Code Translator (Python) → Validator Agent (Exec/Difficulty) → EternalMath Benchmark
- Critical path: The **Code Translator** step. If the Python script generated from the meta-template does not faithfully implement the mathematical logic of the source theorem, the entire verification mechanism collapses.
- Design tradeoffs: The system trades **broad coverage** for **verifiability**. By restricting input to "constructive" theorems, the pipeline can guarantee automatic grading, but it excludes vast areas of abstract mathematics that cannot be easily checked via Python scripts.
- Failure signatures:
  - **Calculation Drift:** The model-generated code runs but accumulates floating-point errors or logical bugs in large symbolic operations.
  - **Logical Hallucination:** The natural language solution steps look coherent but rely on fabricated properties of the mathematical objects.
  - **Stop-too-early:** The model solves preliminary setup steps but halts before executing the core derivation.
- First 3 experiments:
  1. **Unit Test the Translator:** Take the example "Cayley Graph Energy" case in Appendix A.3 and verify the generated Python script produces the correct `numerical_value` for a new prime number not in the example list.
  2. **Recency Audit:** Check the publication dates of the top 10 papers in the dataset against the training cutoff dates of the evaluated models to confirm the "Recency" assumption holds.
  3. **Difficulty Validation:** Run a smaller, open-source model on a sample of "Easy" vs. "Hard" problems to confirm the accuracy drop matches the paper's reported stratification (Fig 5).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the benchmark construction pipeline be adapted to accommodate abstract mathematical domains that rely on qualitative existence proofs rather than constructive or computable results?
- Basis in paper: [explicit] The authors state in the Limitations section that the methodology primarily targets theorems with constructive formulations, which "may underrepresent abstract mathematical domains that lack direct executable verification."
- Why unresolved: The pipeline's filtering process explicitly excludes non-constructive arguments to ensure executable solution generation.
- What evidence would resolve it: An extension of the pipeline that successfully instantiates verifiable problem instances from purely qualitative or non-constructive research papers.

### Open Question 2
- Question: Can an intrinsic metric for mathematical difficulty be defined that correlates with human expert assessment but remains stable as LLM capabilities evolve?
- Basis in paper: [explicit] The paper notes in the Limitations that current difficulty tiers are "defined empirically based on current model performance," requiring "periodic recalibration as reasoning capabilities evolve."
- Why unresolved: The current stratification (Easy/Medium/Hard) is relative to specific models rather than absolute mathematical complexity.
- What evidence would resolve it: A complexity metric based on structural or logical depth that remains consistent even as model accuracy on the benchmark improves.

### Open Question 3
- Question: What is the precise failure rate of the automated pipeline in translating specialized research terminology into formal logic compared to human formalization?
- Basis in paper: [inferred] The authors acknowledge a reliance on LLMs introduces a "minor risk of misinterpreting the most granular logical nuances," and that 2% of samples were removed for correctness issues during human inspection.
- Why unresolved: The blind audit covered only a random sample of 100 tasks, leaving the systematic logical consistency of the full automated translation process uncertain.
- What evidence would resolve it: A large-scale comparison of the pipeline's auto-generated formal solutions against ground-truth formalizations (e.g., in Lean) for the same set of theorems.

## Limitations
- The benchmark is constrained to constructive/quantitative mathematics, excluding important domains of pure mathematical reasoning
- Difficulty stratification assumes problems solved by current SOTA models are inherently "easy," which may not hold for emergent reasoning
- The Code Translator Agent's reliability depends on the LLM's ability to accurately convert abstract theorems into executable code without logical errors

## Confidence
- **High confidence**: The contamination resistance mechanism (temporal filtering) is well-supported by the paper's methodology and addresses a documented problem in existing benchmarks.
- **Medium confidence**: The automated verification through code execution provides strong guarantees, but depends on the quality of the Code Translator Agent, which was not independently validated.
- **Medium confidence**: The difficulty stratification shows clear performance gaps across model tiers, though the assumption that all correctly-solved problems are "easy" warrants further investigation.

## Next Checks
1. **Logical Hallucination Test**: Select 10 random problems from the Hard tier and have an independent mathematician verify that the generated Python code correctly implements the mathematical logic described in the source paper's theorem.
2. **Temporal Leakage Analysis**: For each evaluated model, verify that the training cutoff dates are indeed earlier than the publication dates of all source papers used in the benchmark.
3. **Difficulty Boundary Validation**: Test a separate set of frontier models (not used in the original filtering) on the stratified problems to confirm that accuracy drops consistently across Easy→Medium→Hard tiers.