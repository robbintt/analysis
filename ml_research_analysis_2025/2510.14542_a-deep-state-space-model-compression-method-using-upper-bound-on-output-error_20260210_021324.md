---
ver: rpa2
title: A Deep State-Space Model Compression Method using Upper Bound on Output Error
arxiv_id: '2510.14542'
source_url: https://arxiv.org/abs/2510.14542
tags:
- deep
- output
- error
- reduced
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a compression method for deep state-space
  models (Deep SSMs) using model order reduction (MOR) with a provable output error
  guarantee. The key contributions include: (1) Deriving an upper bound on the output
  error between two Deep SSMs, showing that minimizing the h2 error of each internal
  linear quadratic-output (LQO) system reduces the overall output error; (2) Formulating
  an optimization problem to minimize this bound while preserving Deep SSM-specific
  properties; (3) Developing a gradient-based MOR method with a stationary point guarantee.'
---

# A Deep State-Space Model Compression Method using Upper Bound on Output Error

## Quick Facts
- arXiv ID: 2510.14542
- Source URL: https://arxiv.org/abs/2510.14542
- Reference count: 21
- Primary result: Achieves ~80% parameter reduction on IMDb task with only 4-5% performance drop without retraining

## Executive Summary
This paper presents a novel compression method for Deep State-Space Models (Deep SSMs) that guarantees output error bounds through model order reduction (MOR). The key insight is that minimizing the h²-error of individual Linear Quadratic Output (LQO) layers minimizes the overall output error. Unlike previous approaches that treat layers independently, this method accounts for inter-layer interactions and provides theoretical guarantees. The proposed gradient-based MOR algorithm achieves strong compression results on the IMDb task from the Long Range Arena benchmark, reducing roughly 80% of trainable parameters without retraining while maintaining competitive accuracy.

## Method Summary
The method compresses Deep SSMs by minimizing an upper bound on the output error expressed through layerwise h²-error norms. It formulates an optimization problem that preserves Deep SSM-specific properties like diagonal structure and stability constraints. The gradient-based MOR algorithm iteratively updates the reduced system matrices (Λ̂, B̂, Ĉ, Û) to minimize the h² approximation error relative to the pretrained full-order model. The approach is initialized using standard MOR methods like Balanced Truncation and includes stability checks during optimization. On the IMDb task, the method reduces state dimension from 128 to 16-32 per layer, achieving ~80% parameter reduction with only a 4-5% accuracy drop.

## Key Results
- Achieves ~80% reduction in trainable parameters on IMDb task
- Maintains competitive accuracy with only 4-5% performance drop
- Works without retraining, achieving strong performance directly after compression
- Demonstrates effectiveness of h²-optimal MOR for Deep SSMs

## Why This Works (Mechanism)

### Mechanism 1
The output error of a compressed Deep SSM is constrained by the cumulative h²-error norms of its layerwise LQO subsystems. The paper derives an upper bound (Theorem 1, Eq. 14) showing that the global output deviation scales with the sum of layerwise h² errors. By minimizing the h²-error of each LQO block, the method theoretically minimizes the worst-case output deviation of the entire deep model, accounting for inter-layer interactions rather than treating layers in isolation. This assumes bounded ℓ² input norms and asymptotic stability of internal LQO systems.

### Mechanism 2
Direct gradient-based optimization of the h²-error norm outperforms layer-independent reduction techniques by preserving task-critical dynamics. Unlike standard MOR which truncates states based on general energy metrics, this method uses a gradient-based algorithm (Algorithm 1) to solve the optimization problem defined in Eq. (17). It iteratively updates the reduced system matrices to specifically minimize the h² approximation error relative to the pretrained full-order model. This requires smooth objective functions and efficient solvers for resulting Sylvester/Lyapunov equations.

### Mechanism 3
Errors in earlier layers disproportionately amplify the final output error compared to deeper layers. Theoretical analysis of the error bound coefficients reveals they decay with depth (G̃ᵢ ≥ G̃ᵢ₊₁). This implies that preserving high fidelity in the initial layers (allocating larger state dimensions r) is more critical for overall performance than preserving complexity in deeper layers. This assumes Lipschitz constants of normalization layers and inputs to deeper layers are bounded.

## Foundational Learning

### Concept: Linear Quadratic Output (LQO) Systems
**Why needed here:** The paper specifically targets Deep SSMs containing LQO blocks, which extend standard linear state-space models by including a quadratic term in the output (M(xₖ ⊗ xₖ)). Understanding this structure is required to grasp the h² norm definition and the specific gradient derivations.

**Quick check question:** Can you identify the quadratic component in the output equation yₖ = Cxₖ + M(xₖ ⊗ xₖ) and explain why it necessitates a specific h² norm calculation distinct from standard linear systems?

### Concept: H² Norm and Model Order Reduction (MOR)
**Why needed here:** The core contribution is minimizing the time-limited h² error norm. This norm quantifies the "energy" of the error impulse response. MOR techniques use this to decide which states to discard.

**Quick check question:** What is the physical interpretation of the H² norm in the context of system dynamics, and why is the "time-limited" version used here instead of the infinite-horizon version?

### Concept: Discrete-time Sylvester/Lyapunov Equations
**Why needed here:** Computing the gradients for the optimization problem requires solving specific matrix equations. Understanding these is vital for implementing the efficient solver mentioned in Remark 4.

**Quick check question:** Why does the diagonal structure of the A matrices allow for O(nm) solution of these equations instead of standard cubic complexity?

## Architecture Onboarding

### Component map:
Original Model (ξ-layer Deep SSM with diagonal A matrices and quadratic outputs) → Reduced Model (same topology but with state dimension n → r per layer) → Optimizer (gradient-based solver handling stability constraints and Armijo line search) → Initialization (Standard MOR methods provide starting point for gradient refinement)

### Critical path:
1. **Pretrain:** Train a full-order Deep SSM (state dim n, e.g., 128)
2. **Initialize ROM:** Generate an initial reduced model (state dim r, e.g., 16-32) using a simple method like Balanced Truncation (TLBT)
3. **Refine:** Run Algorithm 1 to minimize the h² error bound (Eq. 17)
4. **Deploy/Retrain:** Use the refined model directly (retraining-free) or as initialization for further fine-tuning

### Design tradeoffs:
- **Uniform vs. Layerwise r:** The paper suggests using larger dimensions for early layers (e.g., r=[32,...,4]) to minimize the error bound efficiently
- **Speed vs. Accuracy:** The gradient step requires solving Sylvester equations. While optimized for diagonal systems, it is costlier than one-shot truncation methods but avoids expensive retraining

### Failure signatures:
- **Instability:** If eigenvalues of Â exit the unit disk during gradient updates
- **Error Propagation:** If r is too small in early layers, the global output error bound explodes due to the G̃ᵢ factors

### First 3 experiments:
1. **Baseline Comparison:** Compare accuracy vs. parameter count for this method vs. standard Balanced Truncation on the IMDb task to isolate the value of the gradient-based refinement
2. **Layer Sensitivity:** Ablate the state dimension r for Layer 1 vs. Layer ξ independently to empirically verify the theoretical claim that early layers are more sensitive
3. **Retraining Ablation:** Test the "retraining-free" claim by evaluating the compressed model immediately after Algorithm 1, then after standard fine-tuning, to quantify the "free lunch" performance gap

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Why does the proposed compression method yield high-performance models even without retraining?
**Basis in paper:** Section VII states: "As future work, we will investigate why high-performance reduced models can be obtained without retraining."
**Why unresolved:** The paper empirically observes this phenomenon but does not provide a theoretical justification for why the h₂-norm preservation translates so directly to task performance without fine-tuning.
**What evidence would resolve it:** A theoretical analysis linking the removal of uncontrollable/unobservable modes to performance retention, or ablation studies isolating the impact of the LQO formulation on the error landscape.

### Open Question 2
**Question:** Can the error bounds and compression method be extended to nonlinearities beyond the quadratic activation?
**Basis in paper:** Section VII suggests: "MOR incorporating nonlinearity may be effective" and notes the current work focuses specifically on LQO systems to facilitate analysis.
**Why unresolved:** The theoretical derivation of the upper bound relies on the specific properties of Linear Quadratic Output (LQO) systems (quadratic activation), limiting the method's direct applicability to other architectures.
**What evidence would resolve it:** Derivation of output error bounds for generic nonlinear activations or empirical validation of the gradient-based MOR on architectures with alternative nonlinear layers.

### Open Question 3
**Question:** How does the compression performance scale to tasks requiring significantly longer sequence dependencies or different data modalities?
**Basis in paper:** Experiments are limited to the IMDb text classification task, whereas Deep SSMs are typically benchmarked on the full Long Range Arena (LRA) to test long-range handling.
**Why unresolved:** The impact of horizon length L and data modality on the tightness of the derived h₂ error bound remains unverified outside of a single text task.
**What evidence would resolve it:** Evaluation of the compressed models on complex LRA tasks (e.g., Pathfinder-XL) or high-dimensional time-series forecasting benchmarks.

## Limitations

- **Unbounded Output Deviation Risk:** The theoretical bound's tightness in practical scenarios is not experimentally verified and could be overly conservative.
- **Retraining Necessity:** The "free lunch" claim needs stronger empirical backing with direct comparison to fine-tuned baselines.
- **Scalability of Gradient Computation:** Computational cost for very deep models or higher state dimensions is not analyzed.

## Confidence

- **High Confidence:** The theoretical derivation of the error bound (Theorem 1) and the gradient computation procedure (Lemma 3) are well-founded.
- **Medium Confidence:** The claim that minimizing layerwise h² errors directly translates to minimal global output error is theoretically sound but relies on the bound being tight.
- **Low Confidence:** The assertion that early layer errors disproportionately impact the final output (Remark 2) is derived from the bound's coefficients but lacks direct experimental confirmation.

## Next Checks

1. **Bound Tightness Analysis:** Measure actual output error of compressed models and compare against theoretical upper bound to quantify conservatism.
2. **Retraining Ablation Study:** Compare performance of compressed model (after Algorithm 1) against same model after standard fine-tuning to isolate true benefit.
3. **Layerwise Sensitivity Test:** Perform ablation study varying state dimension r independently for each layer to empirically validate that early layers are more critical.