---
ver: rpa2
title: 'Reasoning Relay: Evaluating Stability and Interchangeability of Large Language
  Models in Mathematical Reasoning'
arxiv_id: '2512.20647'
source_url: https://arxiv.org/abs/2512.20647
tags:
- reasoning
- llama-3
- b-instruct
- arxiv
- continuation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework to evaluate the interchangeability
  of large language models (LLMs) in mathematical reasoning tasks by systematically
  continuing partially completed reasoning chains (CoTs) from one model using another.
  Chains are truncated at 25%, 50%, and 75% based on cumulative log-probability, and
  continued by a different model.
---

# Reasoning Relay: Evaluating Stability and Interchangeability of Large Language Models in Mathematical Reasoning

## Quick Facts
- **arXiv ID:** 2512.20647
- **Source URL:** https://arxiv.org/abs/2512.20647
- **Reference count:** 11
- **Primary result:** Intra-family model continuation preserves reasoning quality (55.26% accuracy at 75% truncation), while cross-family continuation degrades performance (36.16% accuracy at 25% truncation)

## Executive Summary
This paper introduces a framework to evaluate how well large language models can continue mathematical reasoning chains generated by other models. The approach systematically truncates reasoning chains at 25%, 50%, and 75% based on cumulative log-probability, then has a different model complete the chain. Using the MATH dataset and a Process Reward Model for evaluation, the study finds that models from the same family can successfully continue each other's reasoning chains, while cross-family continuation often fails, especially at earlier truncation points. These results suggest that reasoning interchangeability depends strongly on architectural and stylistic compatibility between models.

## Method Summary
The framework generates complete Chain-of-Thought (CoT) chains for MATH problems using baseline models (Gemma-3-4B-IT, LLaMA-3.1-70B-Instruct), computes cumulative log-probability to identify truncation points, and has continuation models (Gemma-3-1B-IT, LLaMA-3.1-8B-Instruct) complete the truncated chains using a standardized interchange prompt. Reasoning quality is evaluated using a Process Reward Model (Qwen2.5-PRM) for step-level coherence and rule-based extraction for final answer accuracy. The study compares intra-family (Gemma→Gemma, LLaMA→LLaMA) versus cross-family (Gemma→LLaMA, LLaMA→Gemma) continuation performance across different truncation depths.

## Key Results
- Intra-family continuation achieves high accuracy: Gemma-3-4B-IT → Gemma-3-1B-IT maintains 55.26% accuracy at 75% truncation
- Cross-family continuation shows significant degradation: LLaMA-3.1-70B-Instruct → Gemma-3-1B-IT drops to 36.16% accuracy at 25% truncation
- XMD (cross-model divergence) reaches 0.40 in cross-family settings, indicating substantial reasoning trajectory disruption
- Smaller models show context integration overhead when interpreting larger models' reasoning patterns

## Why This Works (Mechanism)

### Mechanism 1: Cumulative Log-Probability as a Confidence Proxy for Truncation
- **Claim:** Token-level cumulative log-probability identifies semantically meaningful split points in reasoning chains.
- **Mechanism:** Cumulative log-probability $L_i = \sum_{j=1}^{i} \ell_j$ at each position serves as an internal confidence signal for where to hand off reasoning chains.
- **Core assumption:** Log-probability correlates with reasoning quality and semantic completeness; confident prefixes are more transferable.
- **Evidence anchors:** Abstract states log-probability thresholds identify truncation points; section 3.2 describes log-probability defining model confidence flow.
- **Break condition:** If log-probability doesn't correlate with reasoning quality (e.g., confidently wrong chains), truncation points may hand off at semantically incomplete or misleading stages.

### Mechanism 2: Architectural and Stylactic Alignment Enables Stable Continuation
- **Claim:** Intra-family model continuations preserve reasoning quality better than cross-family due to shared tokenizers, training distributions, and reasoning patterns.
- **Mechanism:** Models from the same family share representational vocabularies and stylistic tendencies, reducing cognitive load when interpreting foreign reasoning chains.
- **Core assumption:** Stylistic and representational compatibility reduces the cognitive load of interpreting foreign reasoning chains.
- **Evidence anchors:** Abstract shows intra-family continuation preserves quality while cross-family degrades; section 6.1 identifies reasoning bias toward own family patterns.
- **Break condition:** Extreme capability gaps between models may cause failure even within families due to capacity limitations.

### Mechanism 3: Error Amplification Through Non-Revisable Context
- **Claim:** Minor inconsistencies in early reasoning steps compound when continuation models receive externally-generated chains, with limited opportunity to correct upstream errors.
- **Mechanism:** Continuation models must treat the prefix as fixed context, unable to revise earlier logic that self-generated chains would allow implicitly.
- **Core assumption:** Self-generated chains allow implicit error correction that externally-provided prefixes do not.
- **Evidence anchors:** Section 6.3 describes how minor inconsistencies compound with limited steps remaining to revise earlier logic.
- **Break condition:** If the prefix is error-free and stylistically neutral, amplification may not occur even in cross-family settings.

## Foundational Learning

- **Concept: Autoregressive Language Modeling and Log-Probability**
  - **Why needed here:** The truncation mechanism relies on understanding how LLMs assign probabilities to tokens and why cumulative log-probability signals model confidence.
  - **Quick check question:** Why might a token sequence with high log-probability still produce incorrect reasoning?

- **Concept: Chain-of-Thought (CoT) Prompting**
  - **Why needed here:** The entire framework operates on partial CoT chains; understanding what makes CoT effective (step-by-step decomposition) clarifies what makes it transferable or fragile across models.
  - **Quick check question:** What properties of a CoT prefix would make it most useful for a continuation model that did not generate it?

- **Concept: Process Reward Models (PRMs)**
  - **Why needed here:** Evaluation uses PRMs to score step-level reasoning quality, not just final answer accuracy. This distinguishes reasoning coherence from outcome correctness.
  - **Quick check question:** If a reasoning chain reaches the correct final answer but contains logically flawed intermediate steps, would a PRM assign it a high score?

## Architecture Onboarding

- **Component map:** Baseline Generator -> Truncation Engine -> Continuation Model -> PRM Evaluator
- **Critical path:** 1) Generate full CoT chains with log-probabilities 2) Compute cumulative log-probability thresholds to identify truncation indices 3) Format interchange prompt 4) Generate continuation 5) Concatenate prefix + continuation → hybrid chain 6) Evaluate with PRM and extract final answer
- **Design tradeoffs:**
  - Truncation depth: Earlier truncation (25%) stress-tests transfer under uncertainty but yields lower accuracy; later truncation (75%) provides more context but reduces continuation model's contribution
  - Model pairing: Intra-family maximizes coherence; cross-family tests generalization but risks degradation (XMD up to 0.40 observed)
  - Temperature: 0.7 balances stochasticity and coherence; lower values may improve consistency but reduce exploration
- **Failure signatures:**
  - High XMD + negative NRG: Continuation model disrupts original trajectory
  - Accuracy plateaus despite longer prefixes: Context integration overhead in smaller models
  - High PRM score with low accuracy: Coherent reasoning toward wrong answer
  - Low PRM score with high accuracy: Correct answer through flawed reasoning
- **First 3 experiments:**
  1. Baseline validation: Generate full CoT chains without truncation; verify accuracies align with reported baselines
  2. Intra-family sanity check: Run Gemma-3-4B-IT → Gemma-3-1B-IT at 50% truncation; expect NRG ~0.38, XMD ~0.27, accuracy ~50%
  3. Cross-family stress test: Run LLaMA-3.1-70B-Instruct → Gemma-3-1B-IT at 25% truncation; expect XMD ~0.40, negative NRG (~-0.11), accuracy ~36%

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does reasoning interchangeability generalize beyond mathematical reasoning to domains such as commonsense QA, multi-hop retrieval, scientific explanation, and instruction-following?
- **Basis in paper:** Limitations section states "It remains unclear whether interchangeability generalizes to commonsense, scientific, or multimodal reasoning tasks," and Future Work calls for "Cross-Domain Generalization" evaluation.
- **Why unresolved:** The study confined evaluation to the MATH dataset; reasoning formats in other domains may be more variable or implicit, potentially yielding different interchangeability dynamics.
- **What evidence would resolve it:** Running the same chain-splitting methodology on datasets like StrategyQA, HotpotQA, or ScienceWorld and comparing intra-family vs. cross-family continuation performance.

### Open Question 2
- **Question:** Which specific architectural or stylistic factors most strongly predict successful cross-family reasoning continuation?
- **Basis in paper:** The Discussion identifies "architectural and stylistic compatibility as key factors" but does not isolate which elements drive the observed performance gaps.
- **Why unresolved:** The study compares only two families without systematic ablation of individual compatibility factors.
- **What evidence would resolve it:** Controlled experiments pairing models with matched tokenizers but different architectures, or analyzing attention pattern alignment between prefix-generating and continuation models.

### Open Question 3
- **Question:** Would adaptive truncation strategies based on semantic shifts or model uncertainty outperform static log-probability thresholds (25/50/75%)?
- **Basis in paper:** Future Work proposes exploring "dynamic segmentation based on reasoning content, semantic shifts, or model uncertainty" rather than static thresholds.
- **Why unresolved:** Static thresholds may truncate at semantically arbitrary points; semantic boundaries could provide more coherent handoff contexts.
- **What evidence would resolve it:** Comparing continuation accuracy and PRM scores when truncating at sentence boundaries, reasoning step completions, or uncertainty spikes versus cumulative log-probability thresholds.

## Limitations
- Log-probability as transfer signal is an indirect proxy that may not reliably identify semantically meaningful split points
- Context integration overhead quantification lacks empirical validation of specific representational compatibility factors
- Error amplification dynamics aren't systematically analyzed by error type or mechanism
- Evaluation framework completeness may miss higher-order reasoning flaws through PRM scoring limitations

## Confidence

**High Confidence Claims:**
- Intra-family model continuation preserves higher reasoning quality than cross-family continuation
- Cross-family continuation shows measurable degradation, especially at earlier truncation points
- Model family alignment matters for reasoning interchangeability

**Medium Confidence Claims:**
- Log-probability thresholds reliably identify semantically meaningful split points
- Error amplification through non-revisable context is a primary degradation mechanism
- Representational compatibility drives intra-family advantage

**Low Confidence Claims:**
- The specific threshold values (25%, 50%, 75%) are optimal for reasoning transfer
- Temperature=0.7 is the ideal setting for continuation quality
- The observed patterns generalize beyond mathematical reasoning to other domains

## Next Checks
1. **Log-Probability Correlation Validation:** Generate 100 full reasoning chains, artificially inject known logical errors at different positions, and measure whether cumulative log-probability correctly identifies "safe" truncation points that avoid error propagation. Compare against random truncation control.

2. **Error Type Analysis:** Manually annotate 50 failed continuation chains to categorize error types (logical flaw, calculation error, misinterpretation, etc.). Test whether error propagation rates differ by error type and truncation depth to identify the most problematic failure modes.

3. **Cross-Domain Transferability:** Apply the same framework to a non-mathematical reasoning dataset (e.g., commonsense reasoning or code generation). Measure whether the family-alignment advantage persists, which would validate the generalizability of representational compatibility as a core mechanism.