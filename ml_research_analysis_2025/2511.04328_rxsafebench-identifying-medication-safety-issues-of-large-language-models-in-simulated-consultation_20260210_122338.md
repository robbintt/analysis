---
ver: rpa2
title: 'RxSafeBench: Identifying Medication Safety Issues of Large Language Models
  in Simulated Consultation'
arxiv_id: '2511.04328'
source_url: https://arxiv.org/abs/2511.04328
tags:
- medication
- drug
- safety
- medical
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of evaluation benchmarks for medication
  safety in large language models (LLMs) used in clinical consultation. The authors
  propose RxSafeBench, a comprehensive benchmark created by simulating inquiry-diagnosis
  dialogues that embed medication risks such as contraindications and drug interactions.
---

# RxSafeBench: Identifying Medication Safety Issues of Large Language Models in Simulated Consultation

## Quick Facts
- arXiv ID: 2511.04328
- Source URL: https://arxiv.org/abs/2511.04328
- Reference count: 28
- Current LLMs struggle with medication safety reasoning, achieving up to 59.27% accuracy on contraindications but only 38.12% on drug interactions.

## Executive Summary
This paper introduces RxSafeBench, a benchmark for evaluating large language models' (LLMs) ability to recognize medication safety risks in simulated clinical consultations. The benchmark addresses a critical gap in LLM safety evaluation by embedding medication risks—such as contraindications and drug interactions—within multi-turn inquiry-diagnosis dialogues. Through a rigorous two-stage filtering process, including GPT-4-based scoring for clinical realism and quality, the authors construct 2,443 high-quality scenarios across 10 medical specialties. Evaluations reveal that even state-of-the-art models struggle to reliably integrate medication safety knowledge, particularly when risks are implied rather than explicit, highlighting significant challenges in LLM-based medication safety reasoning.

## Method Summary
The authors developed RxSafeBench by first extracting medication safety data (6,725 contraindications, 28,781 drug interactions, 14,906 indication-drug pairs) from authoritative medical websites. They then generated simulated inquiry-diagnosis dialogues using LLMs with embedded medication risks, tailored to 10 medical specialties through department-specific system prompts. Each dialogue was transformed into a three-option multiple-choice question, with options including an unrelated medication, an unsafe but effective medication, and a safe and appropriate medication. A two-stage filtering process—first ensuring indication coverage, then using GPT-4 to score for scene realism, dialogue quality, and medication selection accuracy—produced the final 2,443 scenarios. Models were evaluated on their ability to select the safe medication option, with results reported per specialty and split (contraindications vs. drug interactions).

## Key Results
- LLMs achieved up to 59.27% accuracy on contraindication detection but only 38.12% on drug interactions.
- Performance varied significantly across medical specialties, with Ophthalmology showing particularly low accuracy (13.89-38.89%).
- The benchmark revealed that models struggle more with detecting latent or contextually implied drug interactions compared to contraindications.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Department-specific system prompts improve contextual grounding for medication safety reasoning by aligning LLM behavior with specialty-specific clinical norms.
- Mechanism: Custom prompts encode departmental treatment patterns and communication styles (e.g., internal medicine focuses on chronic condition management), priming the model to retrieve relevant knowledge during dialogue simulation and evaluation.
- Core assumption: Medical reasoning is sufficiently domain-partitioned that specialty context measurably improves safety-relevant decision-making.
- Evidence anchors:
  - [section] Section II.B.a: "we tailor system prompts for each department. These prompts help the LLM quickly adapt to the specific context and utilize relevant medical knowledge"
  - [abstract] Mentions "inquiry-diagnosis dialogues" embedded with medication risks across specialties
  - [corpus] MedConsultBench paper addresses process-aware consultation evaluation but does not directly test department-specific prompting
- Break condition: If model performance does not vary significantly across departments, or if cross-department contamination in training data overwhelms prompt-based specialization.

### Mechanism 2
- Claim: Embedding medication risks within multi-turn dialogue—rather than explicit statements—creates a more challenging evaluation that exposes gaps in implicit reasoning.
- Mechanism: Risks are introduced naturally during inquiry (e.g., patient mentions heart attack history in passing), requiring models to maintain and integrate contextual information across turns before recommending medication.
- Core assumption: Real-world medication errors often stem from failure to recognize implied rather than explicitly stated contraindications or interactions.
- Evidence anchors:
  - [section] Section II.B.b: Equations 4-5 formalize generation of dialogues with embedded contraindications and drug interactions
  - [abstract] "Results show that current LLMs struggle to integrate contraindication and interaction knowledge, especially when risks are implied rather than explicit"
  - [corpus] No direct corpus evidence on implicit vs. explicit risk evaluation; this is a gap in related work
- Break condition: If model performance on implicit risks matches explicit risk performance, suggesting the dialogue structure does not add reasoning complexity.

### Mechanism 3
- Claim: Two-stage filtering (indication-based sampling + GPT-4 scoring) produces scenarios whose quality metrics correlate with model decision patterns.
- Mechanism: First stage ensures indication coverage; second stage uses GPT-4 to score Scene Realism, Dialogue Quality, and Medication Selection Accuracy. Chi-square tests show aggregated scoring dimensions significantly associate with model medication choices.
- Core assumption: GPT-4's scoring captures latent quality factors that influence LLM evaluation outcomes, even if individual model-level tests are underpowered.
- Evidence anchors:
  - [section] Section II.D and Figure 3a/3b: "aggregated contingency tables across all models reveal statistically significant associations for all three score dimensions"
  - [section] Table I details scoring criteria for realism, professionalism, dialogue flow, completeness, and accuracy
  - [corpus] A Real-World Evaluation of LLM Medication Safety Reviews uses real NHS data but employs different quality assurance methods; direct comparison not available
- Break condition: If high-scoring scenarios do not differentially challenge models (e.g., all models perform uniformly well or poorly regardless of score), the filtering mechanism is not discriminative.

## Foundational Learning

- Concept: Contraindications vs. Drug Interactions
  - Why needed here: The benchmark and results explicitly separate these two risk types, and performance differs substantially (59.27% vs. 38.12% best accuracy). Understanding the distinction is essential for interpreting failure modes.
  - Quick check question: Can you explain why detecting a drug interaction typically requires more multi-hop reasoning than detecting a direct contraindication?

- Concept: Inquiry-Diagnosis-Prescription Workflow
  - Why needed here: Simulated dialogues follow this structure (see Figure 2), with risks embedded during inquiry and tested at prescription. Models must integrate information across phases.
  - Quick check question: In the example dialogue, at which phase is the contraindication introduced, and when is the model evaluated on its recognition?

- Concept: Multiple-Choice Evaluation with Distractor Design
  - Why needed here: The three-option format (unrelated medication, unsafe but effective, safe and appropriate) separates general knowledge from safety-specific reasoning.
  - Quick check question: Why would a model that correctly identifies the indication still fail this evaluation?

## Architecture Onboarding

- Component map:
  1. RxRisk DB: 6,725 contraindications, 28,781 drug interactions, 14,906 indication-drug pairs extracted from authoritative sources
  2. Consultation Simulator: Department-specific prompts + dialogue generation with embedded risks
  3. Question Constructor: Maps dialogues to 3-option MCQs; identifies safe candidates via indication-based filtering
  4. Two-Stage Filter: (a) Indication sampling → ~6,000 cases; (b) GPT-4 scoring → final 2,443 scenarios
  5. Evaluation Interface: Model receives system prompt + dialogue + options; outputs single letter choice

- Critical path: Drug data extraction → Indication mapping → Dialogue generation → Candidate drug selection for safe options → MCQ construction → GPT-4 scoring → Benchmark release

- Design tradeoffs:
  - Synthetic vs. real data: Authors acknowledge privacy constraints necessitate simulation; tradeoff is ecological validity vs. scalability
  - GPT-4 as quality filter: Uses proprietary model to score open and closed-source models; potential circularity or bias if GPT-4 favors outputs similar to its own generation patterns
  - Multiple-choice vs. open-ended: Constrains evaluation to selection task; may not capture generation-time safety failures

- Failure signatures:
  - High indication accuracy but low safety accuracy: Model knows what treats the condition but ignores patient history or current medications
  - Better performance on contraindications than interactions: Suggests weaker multi-entity reasoning
  - Significant performance drop in specific departments (e.g., Ophthalmology at 13.89-38.89%): May indicate training data gaps

- First 3 experiments:
  1. Baseline evaluation: Run your target model on both RxSafeBench-C (contraindications) and RxSafeBench-I (interactions) subsets; compare against reported benchmarks (DeepSeek-R1, GPT-4, Llama-3.1-405B)
  2. Ablation on dialogue turns: Test whether truncating dialogues or flattening to single-turn harms interaction detection more than contraindication detection (hypothesis: interaction reasoning benefits from multi-turn context, as suggested by p=0.008 finding)
  3. Prompt variation study: Compare department-specific vs. generic system prompts on a stratified sample; quantify performance delta attributable to specialty context

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance on RxSafeBench's multiple-choice format correlate with open-ended generative tasks where the model must proactively identify safe medications without constrained options?
- Basis in paper: [inferred] The authors evaluate models using a structured multiple-choice format (three options) to assess safety, but acknowledge that clinical deployment requires generative reasoning rather than selection from a fixed list.
- Why unresolved: The paper does not test whether the models' ability to select a safe answer from a list translates to the ability to generate a safe prescription from scratch, a critical step for clinical deployment.
- What evidence would resolve it: A comparative study evaluating the same LLMs on RxSafeBench scenarios using both multiple-choice and free-text generative evaluation metrics.

### Open Question 2
- Question: What specific prompting strategies or fine-tuning methods can effectively improve LLM sensitivity to implicit drug interaction risks, which currently lag significantly behind contraindication detection?
- Basis in paper: [explicit] The results show LLMs struggle more with detecting latent or contextually implied drug interactions (38.12% accuracy) compared to contraindications (59.27%), a disparity the authors highlight as a key challenge.
- Why unresolved: The paper identifies the gap and suggests enhanced prompting or task-specific tuning as future work, but does not experimentally validate specific interventions to close this performance gap.
- What evidence would resolve it: Experiments applying specific chain-of-thought prompting or safety-aligned fine-tuning to demonstrate a statistically significant reduction in the performance gap between interaction and contraindication tasks.

### Open Question 3
- Question: To what extent does the reliance on GPT-4 for automated "professional quality" scoring introduce circularity by validating scenarios that align with GPT-4's internal distribution rather than objective clinical reality?
- Basis in paper: [inferred] The authors use a two-stage filtering strategy relying on GPT-4 to score scenarios for "authenticity" and "professionalism," simultaneously noting that privacy constraints prevent the use of real-world datasets.
- Why unresolved: Using an LLM to grade the realism of data generated by LLMs risks reinforcing model biases, potentially creating scenarios that are linguistically plausible to the model but clinically superficial.
- What evidence would resolve it: A human expert evaluation comparing the clinical validity of GPT-4-approved scenarios against a hold-out set of synthetic scenarios rejected by the automated scorer.

## Limitations

- Domain generalization gap: Synthetic dialogue generation may not fully capture real-world clinical variability, despite GPT-4 filtering.
- Quality filter circularity: Using GPT-4 to filter scenarios and evaluate outputs may bias results toward GPT-4-like reasoning patterns.
- Single-turn evaluation constraint: Multiple-choice format cannot detect generation-time safety reasoning failures.

## Confidence

- High confidence: Benchmark construction methodology is well-specified and reproducible; performance gap between contraindication and drug interaction detection is robust.
- Medium confidence: Department-specific prompts' contribution to performance is inferred from observed variations but not directly tested through ablation.
- Low confidence: GPT-4 scoring correlations are based on aggregated chi-square tests; individual model-level significance is underpowered.

## Next Checks

1. **Ablation study on prompt specificity**: Test the same models on a stratified sample of scenarios using generic versus department-specific prompts. Quantify the performance delta to establish the contribution of specialty context to safety reasoning accuracy.

2. **Implicit vs. explicit risk detection**: Analyze model performance on scenarios where risks are embedded implicitly in dialogue versus explicitly stated. This validates whether the dialogue structure genuinely tests multi-turn reasoning as claimed.

3. **Cross-validation with real-world data**: Apply RxSafeBench to models that have been fine-tuned on real clinical consultation data (where available). Compare performance patterns to those observed on synthetic data to assess ecological validity and identify systematic gaps.