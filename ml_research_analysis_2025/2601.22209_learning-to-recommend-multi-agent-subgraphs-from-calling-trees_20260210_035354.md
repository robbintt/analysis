---
ver: rpa2
title: Learning to Recommend Multi-Agent Subgraphs from Calling Trees
arxiv_id: '2601.22209'
source_url: https://arxiv.org/abs/2601.22209
tags:
- agent
- retrieval
- agents
- recommendation
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formulates agent recommendation in multi-agent systems
  as a constrained decision problem and introduces a two-stage framework that first
  retrieves a compact candidate set conditioned on the current subtask and context,
  then performs utility optimization within this feasible set using a learned scorer.
  Grounded in historical calling trees, the method supports both agent-level (selecting
  next agent) and system-level (selecting a small connected agent team) recommendation.
---

# Learning to Recommend Multi-Agent Subgraphs from Calling Trees

## Quick Facts
- **arXiv ID:** 2601.22209
- **Source URL:** https://arxiv.org/abs/2601.22209
- **Reference count:** 35
- **Primary result:** A two-stage constrained recommendation framework that first retrieves a compact candidate set and then optimizes within it, consistently outperforming strong baselines on agent-level and system-level recommendation tasks across 8 diverse multi-agent datasets.

## Executive Summary
This paper addresses the challenge of recommending agents and agent systems in multi-agent systems (MAS) by framing it as a constrained decision problem. The proposed two-stage framework first retrieves a compact, semantically aligned candidate set based on the current subtask and context, then performs utility optimization within this feasible set using a learned scorer. Grounded in historical calling trees that capture execution structure, the method supports both single-agent selection and connected-agent system recommendation. Experiments across eight heterogeneous datasets show consistent improvements in recommendation quality over strong baselines.

## Method Summary
The approach formulates agent recommendation as constrained optimization within a feasible set rather than unconstrained ranking over the full pool. It operates in two stages: (1) retrieval of top-K candidates using semantic similarity to the current task, and (2) reranking within this set using a learned scorer that combines task relevance, capability, historical reliability, cooperation, and structural features. The method learns from historical calling trees—hierarchical execution traces capturing parent-child dependencies and local cooperation patterns. A unified benchmark is constructed by normalizing logs from eight diverse MAS corpora. The learned linear scorer optimizes a softmax cross-entropy loss over candidate sets, with evaluation via Top-1 and Top-K accuracy metrics.

## Key Results
- The two-stage framework consistently recommends higher-quality agents and more coherent agent systems than strong baselines across 8 diverse datasets
- Learned linear combination of relevance, reliability, cooperation, and structural features outperforms pure semantic retrieval (e.g., SARL 79.85% vs Cos-sim Direct 40.0% on agent-data-protocol)
- The approach improves stability, coordination, and end-to-end execution quality in multi-agent orchestration

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Constrained Optimization
Separating feasibility construction from utility optimization improves agent selection quality in large-scale pools. Stage 1 retrieves a compact candidate set using semantic similarity, reducing the search space. Stage 2 applies a learned scorer incorporating multiple features within this constrained set, rather than ranking over the full pool. The optimal agent/system exists within the top-K retrieved candidates; retrieval success rate is the primary bottleneck.

### Mechanism 2: Structured Execution Traces as Supervision
Calling trees provide richer supervision signals than flat interaction logs for learning agent selection. They capture hierarchical dependencies—parent-child calls, branching structure, and local cooperation patterns. The learning objective optimizes scores conditioned on tree position and its task description, not just isolated query-item pairs. Historical calling patterns generalize to future orchestration decisions.

### Mechanism 3: Learned Feature Combination
A learned linear combination of relevance, reliability, cooperation, and structural features outperforms pure semantic retrieval. The scorer aggregates five components: task encoding, capability encoding, graph relations, historical reliability, and contextual effects. The linear formulation learns weights via softmax cross-entropy. Features are assumed conditionally independent given the task, and linear combination suffices to capture utility.

## Foundational Learning

- **Concept: Constrained Optimization**
  - Why needed here: The framework formulates recommendation as optimization within a feasible set (A_feasible or G_feasible), not unconstrained ranking over the full pool
  - Quick check question: Why does "retrieve feasible set, then optimize within it" outperform "pick top-ranked from full pool" in multi-agent settings?

- **Concept: Calling Trees / Execution Traces**
  - Why needed here: The method grounds learning in hierarchical execution logs that capture who-called-whom and branching dependencies, not flat interaction histories
  - Quick check question: What structural information does a calling tree provide that a flat list of "agent A was invoked" does not?

- **Concept: Learning-to-Rank (Listwise)**
  - Why needed here: The scorer is trained with softmax cross-entropy over candidate sets, not binary classification or pointwise regression
  - Quick check question: Why is ranking loss more appropriate here than predicting a single "correct" agent?

## Architecture Onboarding

- **Component map:**
```
Query qt → [Retrieval Module] → Top-K candidates → [Learned Scorer sθ] → Ranked list → Top-1 selection
                ↑                                               ↑
         Agent Pool V (features ϕ)                  Historical traces Ω (calling trees T)
```

- **Critical path:**
  1. **Data prep:** Normalize raw logs → resolve parent-child links → construct calling trees → build agent pools
  2. **Feature engineering:** Compute ϕ_rel (semantic), ϕ_hist (reliability), ϕ_coop (graph stats), ϕ_struct (topology)
  3. **Training:** Optimize softmax cross-entropy with L2 regularization
  4. **Inference:** Retrieve K candidates → score with sθ → select argmax

- **Design tradeoffs:**
  - Linear vs. neural scorer: Linear is interpretable and sample-efficient; may underfit complex feature interactions
  - Retrieval cutoff K: Smaller K reduces token cost; risks missing optimal candidate (paper uses K=20)
  - SARL vs. ASRL: SARL selects single agents; ASRL selects connected subgraphs (ASRL achieves higher accuracy but requires graph-level retrieval)

- **Failure signatures:**
  - Low retrieval SR (<50%) → Stage 1 bottleneck; ranking cannot recover
  - Token budget exceeded on large pools → enforce K-based shortlisting
  - High cross-dataset variance → overfitting to corpus-specific patterns

- **First 3 experiments:**
  1. **Ablate scorer components:** Train with ϕ_rel only, then add ϕ_hist, ϕ_coop, ϕ_struct incrementally. Measure Top-1 accuracy gain per component.
  2. **Vary K (retrieval cutoff):** Test K ∈ {5, 10, 20, 50} on a moderate dataset. Plot retrieval SR vs. K to find plateau point.
  3. **SARL vs. ASRL comparison:** Run both on TRAIL (multi-step workflows). Quantify the gap between agent-level and graph-level recommendation.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the framework effectively mitigate popularity bias and incumbent advantage inherent in historical calling logs to ensure marketplace diversity?
  - Basis: The Impact Statement warns the method may amplify biases in past execution logs and reduce marketplace diversity by repeatedly selecting the same agents
  - Why unresolved: The current learning objective maximizes utility based on historical success, reinforcing existing usage patterns without counterfactual fairness mechanisms
  - What evidence would resolve it: Ablation studies comparing standard utility optimization against a diversity-constrained variant, measuring coverage of unique agents selected

- **Open Question 2:** How robust is the recommendation policy when agents in the marketplace are updated asynchronously or removed?
  - Basis: The Impact Statement lists "robustness checks under agent updates and removals" as a necessary deployment consideration not addressed
  - Why unresolved: The model encodes agent features and historical reliability assuming static definitions; dynamic changes could break the learned scoring function
  - What evidence would resolve it: Evaluation on a simulated dataset stream where agent definitions or reliabilities drift over time, measuring decay in Top-1 accuracy

- **Open Question 3:** Can the "initial scan" token complexity be reduced to sub-linear scaling for million-scale agent pools without sacrificing feasibility set quality?
  - Basis: Section 3.3 notes the two-stage framework matches the big-O token cost of direct retrieval because the cost is dominated by the initial pool scan
  - Why unresolved: The method relies on scanning the full pool to define the feasible set, which becomes prohibitive as N scales to millions of agents
  - What evidence would resolve it: Integration of approximate nearest neighbor (ANN) retrieval or hierarchical pruning in the first stage, showing token reduction while maintaining Stage 1 retrieval success rates

## Limitations
- The unified calling-tree benchmark masks dataset-specific preprocessing choices that could influence learned patterns
- The linear scorer may not capture complex feature interactions that non-linear alternatives could handle
- The framework's effectiveness is fundamentally limited by retrieval success rate when Stage 1 misses the optimal candidate

## Confidence
- **High:** The two-stage constrained optimization framework is well-specified and reproducible with clearly defined architecture and training procedure
- **Medium:** Claims about calling trees providing richer supervision than flat logs are plausible but lack direct comparison studies; feature engineering pipeline is described but not fully validated in isolation
- **Low:** The assertion that learned linear combination "outperforms pure semantic retrieval" is not tested against learned non-linear scorers in the same constrained framework

## Next Checks
1. **Ablate retrieval vs. ranking quality:** Fix Stage 1 retrieval (top-K=20) and compare learned scorer vs. fixed heuristic. Then fix Stage 2 and vary K to find retrieval SR plateau.
2. **Test non-linear scorer within constrained framework:** Replace linear sθ with a small MLP (same input features) and measure performance gain.
3. **Cross-dataset zero-shot transfer:** Train on 7 datasets, test on the held-out 8th. Measure drop in accuracy to quantify how much performance relies on dataset-specific patterns.