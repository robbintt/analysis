---
ver: rpa2
title: Distilling Empathy from Large Language Models
arxiv_id: '2507.08151'
source_url: https://arxiv.org/abs/2507.08151
tags:
- empathy
- response
- responses
- llms
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of preserving empathy when distilling
  knowledge from large language models (LLMs) into smaller language models (SLMs),
  which are widely used in resource-constrained, human-interaction-heavy environments.
  The authors propose a comprehensive approach combining a two-step fine-tuning process
  (supervised fine-tuning followed by reinforcement learning with human feedback via
  direct preference optimization) with three empathy distillation methods and four
  targeted empathy improvement prompts.
---

# Distilling Empathy from Large Language Models

## Quick Facts
- arXiv ID: 2507.08151
- Source URL: https://arxiv.org/abs/2507.08151
- Reference count: 4
- One-line primary result: Proposed method achieves >90% win rates in generating empathetic responses, improving 10% over basic direct prompting.

## Executive Summary
This paper presents a method to preserve and transfer empathy from large language models (LLMs) to smaller language models (SLMs) through a two-step fine-tuning process. The approach combines supervised fine-tuning with reinforcement learning using human feedback via direct preference optimization, enhanced by targeted empathy improvement prompts. The method addresses the challenge of maintaining empathetic capabilities in SLMs deployed in resource-constrained, human-interaction-heavy environments. Evaluations demonstrate that SLMs fine-tuned with this approach significantly outperform base SLMs in generating empathetic responses.

## Method Summary
The method employs a two-step fine-tuning pipeline: first, supervised fine-tuning (SFT) on high-empathy responses, followed by reinforcement learning with human feedback using direct preference optimization (DPO) on preference pairs. Three distillation methods are used: improving human responses, improving LLM-generated responses, and targeted improvement over LLM initial responses. Four unique sets of prompts guide the LLM to enhance responses along cognitive, affective, and compassionate empathy dimensions. The approach bootstraps distillation datasets without human involvement, using LoRA for efficient fine-tuning on resource-constrained hardware.

## Key Results
- SLMs fine-tuned with the proposed approach achieve win rates exceeding 90% over base SLMs in empathy generation
- The method improves performance by 10% compared to basic direct prompting approaches
- Models fine-tuned using LLM-generated initial responses (Method 3) consistently outperform those using human responses (Method 2)
- Some fine-tuned SLMs achieve 80%+ win rates against GPT-4o in head-to-head comparisons

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential two-step fine-tuning (SFT → RLHF-DPO) more effectively transfers empathy from LLMs to SLMs than either method alone
- Mechanism: SFT establishes a reliable baseline by imitating high-empathy responses, while DPO refines this baseline by learning from preference pairs, teaching the model to discriminate between nuanced empathy levels
- Core assumption: Dataset can be cleanly partitioned into high-empathy examples for SFT and distinct preference pairs for DPO
- Evidence anchors: Abstract states "two-step fine-tuning process... supervised fine-tuning followed by reinforcement learning with human feedback via direct preference optimization"

### Mechanism 2
- Claim: Decomposing empathy into specific dimensions (cognitive, affective, compassionate) and prompting for targeted improvement creates superior distillation signal
- Mechanism: Structured prompts explicitly define what empathy means in context, instructing the teacher model to enhance responses along concrete axes for clearer training examples
- Core assumption: Teacher LLM can reliably interpret definitions and perform meaningful improvement along specified dimension
- Evidence anchors: Abstract mentions "four unique sets of prompts for targeted empathy improvement to significantly enhance the empathy distillation process"

### Mechanism 3
- Claim: Distillation datasets can be bootstrapped without human-generated responses by using LLM to improve upon its own initial outputs
- Mechanism: LLM first generates candidate response, then is prompted again with targeted empathy improvement to refine its own prior output, creating preference pairs without human data
- Core assumption: LLM's initial response is coherent enough to be improved, and model can act as reliable critic and refiner of its own outputs
- Evidence anchors: Abstract states approach "bootstraps the distillation datasets without human involvement"

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: Core algorithm for second fine-tuning step that simplifies traditional RLHF pipeline
  - Quick check question: How does DPO simplify traditional RLHF? (Answer: DPO directly optimizes policy using preference pairs, eliminating need for separate reward model)

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: Used for fine-tuning to manage computational resources and reduce memory usage
  - Quick check question: Why is LoRA preferred for fine-tuning large models on limited hardware? (Answer: LoRA freezes main model weights and trains only small adapter matrices, drastically reducing GPU memory usage)

- Concept: Win Rate Evaluation with LLM-as-a-Judge
  - Why needed here: Central claims rely on win rates evaluated by other LLMs as proxy metric
  - Quick check question: What is fundamental limitation of using LLM as judge for subjective qualities like empathy? (Answer: Judge LLM has its own biases and may not perfectly align with human perception of empathy)

## Architecture Onboarding

- Component map: Data Preparation Module (input dialogues → generate initial/improved responses → output SFT/RLHF datasets) → Two-Step Fine-Tuning Pipeline (SFT on high-empathy responses → DPO on preference pairs) → Evaluation Module (test dialogues + responses → LLM judge selects more empathetic response → output win rate)

- Critical path: Data generation creates high-quality contrastive preference pairs (most crucial step), DPO training learns nuance of empathy (sensitive to hyperparameters), evaluation uses reliable consistent judge model

- Design tradeoffs:
  - Human vs. LLM initial response: Method 2 anchors in human data (more grounded but limited) vs. Method 3 bootstraps from LLM data (scalable but risks amplifying biases)
  - Complexity of improvement prompts: More complex prompts don't always yield best results; simpler single-dimension prompts sometimes perform best
  - SFT vs. SFT+DPO: Two-step process generally outperforms SFT alone, but DPO is sensitive to hyperparameters and dataset quality

- Failure signatures:
  - Stagnant win rate (≤50%): Preference pairs lack learnable signal; improved and initial responses not distinguishable enough
  - Degraded output quality: Responses become repetitive, shorter, or lose coherence; points to over-tuning during DPO or poorly constructed SFT dataset
  - Inconsistent evaluation: Win rates fluctuate between judge models; evaluation prompt may be ambiguous or judges have divergent definitions of empathy

- First 3 experiments:
  1. Baseline Establishment: Implement data generation pipeline using only "Naive Prompt"; run full SFT + DPO pipeline and measure win rate against base SLM
  2. Ablation Study on Prompts: Compare "Cognitive," "Affective," and "Compassionate" single-dimension prompts using LLM-initial-response method; identify which dimension yields largest performance gain
  3. SFT vs. DPO Contribution: Train two models (SFT only vs. SFT → DPO) using best-performing prompt; compare win rates to quantify DPO's specific contribution

## Open Questions the Paper Calls Out

- Question: Do human evaluators confirm empathy levels achieved by distilled SLMs, or are high win rates artifact of LLM-as-judge biases?
  - Basis in paper: Authors explicitly state in Limitations section that "human evaluation is an essential step" despite only conducting LLM-as-judge evaluation
  - Why unresolved: Study relies entirely on GPT-4o and Gemini as judges, which authors admit are subject to hallucinations and biases not aligning with human perception
  - What evidence would resolve it: Controlled human evaluation study using Welivita and Pu (2024) empathy rating scheme to compare SLM responses against base models and teacher models

- Question: Does superior performance of distillation from LLM-generated initial responses (Method 3) over human responses (Method 2) result from higher baseline empathy or better semantic alignment between teacher and initial response?
  - Basis in paper: Authors note LLM-generated initial responses consistently outperform human responses, hypothesizing better mastery of empathy or better comprehension, but don't isolate cause
  - Why unresolved: Paper identifies correlation but doesn't disentangle whether improvement is due to quality of starting material or ease of optimization
  - What evidence would resolve it: Ablation study where human responses are paraphrased by LLM to match style but not empathy level of LLM responses

- Question: Is "Student beats Teacher" phenomenon (fine-tuned SLMs outperform GPT-4o) result of genuine capability transfer or over-optimization for specific targeted empathy improvement prompts?
  - Basis in paper: Figure 10 shows fine-tuned SLMs achieving 80%+ win rates against GPT-4o; students rarely surpass teachers without specific architectural advantages
  - Why unresolved: Evaluation relies on similar prompt structures used in training, potentially inflating performance relative to general-purpose teacher model
  - What evidence would resolve it: Evaluation using out-of-distribution prompts or different empathy taxonomies to test if SLM advantage persists when structural cues are removed

## Limitations

- Dataset Dependency: Relies on LLMs-vs-Humans dataset that combines human and LLM responses, with limited details on construction methodology and no direct dataset access provided
- Evaluation Reliability: Win rates determined by LLM-as-a-judge despite subjective nature of empathy requiring human evaluation for definitive validation
- Performance Ceiling: 10% improvement over basic direct prompting may not represent transformative leap; absolute win rates not specified making real-world impact difficult to assess

## Confidence

- High Confidence: Core claim that two-step fine-tuning process transfers empathy from LLMs to SLMs is well-supported by experimental results and aligns with established knowledge distillation principles
- Medium Confidence: Claim that targeted empathy improvement prompts significantly enhance distillation process is supported but specific prompt designs show variability in effectiveness
- Low Confidence: Assertion that approach "eliminates need for human-generated data" is overstated since initial dataset used human responses and human evaluation is still required for final validation

## Next Checks

1. Human Evaluation Study: Conduct diverse human rater panel to assess empathy in responses from fine-tuned SLM and base SLM, focusing on cognitive, affective, and compassionate empathy dimensions

2. Dataset Robustness Test: Reconstruct LLMs-vs-Humans dataset using different dialogue corpus (e.g., DailyDialog) and repeat fine-tuning and evaluation process to test method effectiveness robustness

3. Scaling Analysis: Evaluate approach on wider range of SLM sizes (3B, 7B, 13B parameters) to determine if empathy distillation performance scales predictably with model size or if threshold exists below which approach becomes ineffective