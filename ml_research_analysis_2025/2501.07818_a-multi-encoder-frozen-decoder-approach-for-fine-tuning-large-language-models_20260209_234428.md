---
ver: rpa2
title: A Multi-Encoder Frozen-Decoder Approach for Fine-Tuning Large Language Models
arxiv_id: '2501.07818'
source_url: https://arxiv.org/abs/2501.07818
tags:
- freezing
- frozen
- performance
- tasks
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates freezing the decoder during fine-tuning
  of encoder-decoder models to reduce deployment overhead and improve portability
  to new tasks. It evaluates freezing decoders across various natural language tasks
  using the AlexaTM model, including structured tasks like MTOP, generative tasks
  like XSUM and WebNLG, and multilingual tasks like MASSIVE.
---

# A Multi-Encoder Frozen-Decoder Approach for Fine-Tuning Large Language Models

## Quick Facts
- arXiv ID: 2501.07818
- Source URL: https://arxiv.org/abs/2501.07818
- Authors: Kaustubh D. Dhole
- Reference count: 10
- Primary result: Freezing decoder during fine-tuning improves performance on natural language tasks while mitigating catastrophic forgetting, but degrades structured task performance unless using larger decoder

## Executive Summary
This paper investigates freezing the decoder during fine-tuning of encoder-decoder models to reduce deployment overhead and improve portability to new tasks. The approach is evaluated across various natural language tasks using the AlexaTM model, including structured tasks like MTOP, generative tasks like XSUM and WebNLG, and multilingual tasks like MASSIVE. The key finding is that freezing the decoder significantly improves performance on tasks with natural language outputs (e.g., WebNLG, CommonGen) and mitigates catastrophic forgetting in multilingual tasks. However, it leads to performance degradation in structured tasks (e.g., MTOP) unless paired with a larger decoder. The results suggest that freezing decoders is effective for tasks with natural language targets but requires careful consideration for structured tasks, and a larger decoder can help mitigate performance losses in the latter case.

## Method Summary
The method involves fine-tuning encoder-decoder models (AlexaTM and AlexaTM-2B) with frozen decoder parameters across diverse NLP tasks. The training procedure uses batch size 128, 8× NVIDIA V100 GPUs with DeepSpeed, Adam optimizer with learning rate 1e-6 decaying to 5e-6 over 100k updates. Three conditions are evaluated per task: trainable decoder, frozen decoder, and frozen decoder on 2B model. Mixed-task training samples proportionally to sqrt(dataset size). Inference uses num_beams=3. Tasks include summarization (XSUM), semantic parsing (MTOP), QA (SQuAD), multilingual intent classification (MASSIVE), NLI (XNLI), and constrained generation (CommonGen, WebNLG).

## Key Results
- Freezing decoder significantly improves performance on natural language generation tasks (WebNLG, CommonGen) with minimal degradation
- Catastrophic forgetting is mitigated in multilingual tasks, showing 8-10% improvements across Spanish, Arabic, and Italian
- Structured tasks (MTOP) suffer 14% performance drop with frozen decoder but recover with larger 2B frozen decoder
- Multi-task mixed training can cause interference for structured/QA tasks, requiring separate training or careful task grouping

## Why This Works (Mechanism)

### Mechanism 1: Pre-trained Knowledge Preservation via Decoder Freezing
Freezing the decoder preserves generalizable language generation capabilities learned during pre-training while allowing task-specific adaptation through the encoder alone. The decoder, when pre-trained on diverse multilingual spoken and written data, develops robust output generation patterns. By freezing it, these patterns remain intact, and the encoder learns to produce representations that the frozen decoder can already process effectively—effectively decoupling representation learning from generation. Core assumption: The pre-training data distribution sufficiently covers the output format requirements of downstream tasks; the decoder has already converged on useful generation patterns.

### Mechanism 2: Catastrophic Forgetting Mitigation
Freezing the decoder acts as a regularizer that prevents overwriting of multilingual and cross-task knowledge when fine-tuning on specific tasks or languages. During fine-tuning on a single language (e.g., English), gradient updates would normally shift decoder weights toward that language's distribution, degrading performance on other languages. Freezing blocks these updates, preserving the multilingual representations learned during pre-training. Core assumption: Catastrophic forgetting primarily affects the decoder's multilingual capabilities more than encoder representations; the task-specific knowledge can be encoded in the encoder alone.

### Mechanism 3: Compensation via Decoder Capacity Scaling
Performance degradation from frozen decoders can be recovered or exceeded by using a larger pre-trained decoder while still keeping it frozen. Larger decoders have greater representational capacity and richer pre-trained distributions. Even when frozen, their richer latent space provides better coverage of diverse output patterns, including structured formats that smaller decoders struggle with. Core assumption: Larger models have sufficiently diverse pre-training to cover more output format variations; the encoder can learn to map to the appropriate regions of this larger decoder's latent space.

## Foundational Learning

- **Encoder-Decoder Architecture Separation**
  - Why needed here: Understanding that encoders map inputs to latent representations while decoders generate outputs from those representations is essential to grasping why selective freezing works.
  - Quick check question: Can you explain why freezing only the decoder allows task-specific input processing while preserving general output generation?

- **Catastrophic Forgetting**
  - Why needed here: The paper positions freezing as a solution to catastrophic forgetting in multilingual settings; understanding this phenomenon is critical to interpreting the MASSIVE results.
  - Quick check question: What happens to a multilingual model's performance on non-English languages when fine-tuned only on English data without any mitigation strategy?

- **Parameter-Efficient Fine-Tuning (PEFT)**
  - Why needed here: Freezing is situated within the broader category of PEFT methods; understanding the efficiency vs. performance tradeoff landscape helps contextualize the approach.
  - Quick check question: Beyond freezing, what are two other approaches to reduce trainable parameters during fine-tuning, and how do they differ in their modification of the base model?

## Architecture Onboarding

- **Component map**:
  - Encoder (trainable) -> Decoder (frozen) -> Task-specific heads
  - Multi-task mixing layer for proportional sampling by sqrt(dataset size)

- **Critical path**:
  1. Pre-trained AlexaTM model loaded (encoder + decoder)
  2. Decoder parameters frozen (`requires_grad=False`)
  3. Encoder fine-tuned on task-specific data
  4. For multi-task: encoders share decoder across tasks; training samples mixed proportionally
  5. Inference: single frozen decoder serves multiple task-specific encoders (deployment efficiency)

- **Design tradeoffs**:
  - **Natural language outputs vs. structured outputs**: Freezing works best when output format matches pre-training; expect degradation for semantic parsing, code generation, or formal languages
  - **Model size vs. efficiency**: AlexaTM-2B frozen outperforms AlexaTM trainable, but requires 4x parameters; deployment cost vs. accuracy
  - **Single-task vs. mixed-task training**: Mixed training can help (WebNLG, XNLI) or hurt (SQuAD, MTOP) depending on cross-task interference

- **Failure signatures**:
  - 14%+ drop in exact match for structured output tasks (MTOP) with frozen decoder
  - Cross-task interference in mixed settings causing degradation below single-task baselines
  - Minimal improvement from freezing when output language/distribution differs from pre-training

- **First 3 experiments**:
  1. **Baseline comparison**: Fine-tune AlexaTM on your target task with both trainable and frozen decoder; measure performance gap to determine if your task is "freezing-compatible" (natural language outputs should show <5% degradation)
  2. **Scale compensation test**: If degradation >5%, repeat with AlexaTM-2B frozen decoder; if performance recovers, the task benefits from capacity scaling
  3. **Cross-task interference check**: For multi-task deployments, train encoders on each task separately, then evaluate all on shared frozen decoder before attempting mixed training; identify task pairs that show mutual degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific strategies can effectively mitigate cross-task interference in mixed-task setups using frozen decoders?
- Basis in paper: The Conclusion states, "Future work could explore more robust strategies for mitigating cross-task interference and evaluate the impact of freezing other parts of the network."
- Why unresolved: The current study identifies that mixed-task training causes interference (particularly for structured/QA tasks like MTOP and SQuAD), but it does not propose or test methods to resolve this conflict.
- What evidence would resolve it: Experiments utilizing specialized sampling methods, loss weighting, or gradient isolation techniques that show improved performance on structured tasks within a mixed frozen-decoder setup.

### Open Question 2
- Question: How does freezing the encoder compare to freezing the decoder regarding performance retention and training efficiency?
- Basis in paper: Section 6 states, "A vital evaluation from the perspective of multi-task design would be to also perform a parallel evaluation with freezing encoders and other parameters of the whole network partially."
- Why unresolved: The paper focuses exclusively on freezing the decoder; the relative benefits or drawbacks of freezing the encoder in this specific multi-task architecture remain untested.
- What evidence would resolve it: A comparative ablation study benchmarking "frozen encoder" models against "frozen decoder" models on the same suite of tasks (e.g., XSUM, MTOP).

### Open Question 3
- Question: To what extent do different task taxonomies and data augmentation techniques improve performance in frozen-decoder setups?
- Basis in paper: Section 6 notes that "Performances on mix tasks can be highly variable being dictated by the dataset proportion" and suggests exploring "different taxonomies... as well as resort to data augmentation."
- Why unresolved: The experiments use a specific square-root sampling proportion, but the sensitivity of frozen decoders to different task groupings or augmented data sizes is not established.
- What evidence would resolve it: Experiments varying the scale and composition of training data (e.g., via augmentation) and systematically grouping tasks by output format to observe performance shifts.

## Limitations
- Proprietary AlexaTM models prevent independent verification without access to specific checkpoints
- Only two model scales evaluated (511M and 2B parameters), leaving scaling relationship unexplored
- Focus on natural language and structured tasks, excluding code generation, formal languages, or other non-natural language output formats

## Confidence
**High Confidence**: The catastrophic forgetting mitigation mechanism (Mechanism 2) is well-supported by the MASSIVE task results showing 8-10% improvements across Spanish, Arabic, and Italian when freezing the decoder.

**Medium Confidence**: The compensation via decoder capacity scaling (Mechanism 3) has moderate support from the MTOP and SQuAD results with AlexaTM-2B, but the paper does not provide systematic analysis of scaling effects.

**Low Confidence**: The generalization claim that freezing works for "any task with natural language outputs" is based on WebNLG and CommonGen only, without testing the full spectrum of natural language generation tasks.

## Next Checks
1. **Cross-Format Generalization Test**: Fine-tune the same frozen-decoder approach on code generation (e.g., CodeXGLUE) and formal language tasks (e.g., Regular Expression generation) to determine if the natural language output generalization holds beyond the tested datasets.

2. **Scaling Analysis**: Systematically evaluate frozen-decoder performance across a range of model sizes (e.g., 200M, 500M, 1B, 2B, 4B parameters) on the same tasks to quantify the relationship between decoder capacity and task compatibility, particularly for structured outputs.

3. **Zero-Shot Transfer Evaluation**: Test the frozen-decoder model's zero-shot performance on held-out languages or domains not seen during fine-tuning to validate the catastrophic forgetting mitigation claim beyond the MASSIVE benchmark.