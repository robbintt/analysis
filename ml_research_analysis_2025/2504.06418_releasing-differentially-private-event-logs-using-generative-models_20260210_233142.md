---
ver: rpa2
title: Releasing Differentially Private Event Logs Using Generative Models
arxiv_id: '2504.06418'
source_url: https://arxiv.org/abs/2504.06418
tags:
- data
- event
- process
- privacy
- travag
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two novel methods for releasing differentially
  private event logs using generative models. The first, TraVaG, employs Generative
  Adversarial Networks (GANs) to sample from a privatized implicit variant distribution,
  while the second uses Denoising Diffusion Probabilistic Models (DDPMs) to reconstruct
  artificial trace variants from noise via trained Markov chains.
---

# Releasing Differentially Private Event Logs Using Generative Models

## Quick Facts
- **arXiv ID:** 2504.06418
- **Source URL:** https://arxiv.org/abs/2504.06418
- **Reference count:** 40
- **Primary result:** Introduces TraVaG (GAN-based) and DDPM-based methods for generating differentially private event logs, outperforming prefix-based approaches in privacy-utility tradeoff.

## Executive Summary
This paper presents two novel generative model approaches for releasing differentially private event logs: TraVaG (Trace Variant GAN) and a DDPM-based method. Both methods train on one-hot encoded trace variants using DP-SGD and RDP accounting to achieve formal privacy guarantees. The approaches address limitations of existing prefix-based methods that introduce fake variants or remove frequent true variants. Experimental results on real-life event data demonstrate that both TraVaG and DDPMs outperform state-of-the-art techniques in terms of privacy guarantees and utility preservation, particularly for logs with many infrequent variants.

## Method Summary
The paper introduces two generative modeling approaches for releasing differentially private event logs. TraVaG combines an autoencoder with a GAN trained using DP-SGD, where the GAN's discriminator is privatized to sample from a privatized implicit variant distribution. The DDPM approach uses a noise predictor network trained with DP-SGD to reverse a diffusion process on one-hot encoded variants. Both methods employ Renyi Differential Privacy (RDP) accounting for privacy composition. The training process involves converting event logs to simple event logs (trace variants), one-hot encoding them, and using grid search to find hyperparameters that satisfy target privacy budgets. Synthetic variants are generated by sampling from the trained generator or reversing the diffusion process.

## Key Results
- TraVaG and DDPM methods outperform prefix-based approaches in both privacy guarantees and utility preservation across all tested datasets
- Both methods successfully handle logs with high prevalence of infrequent variants without introducing fake variants
- Experimental results on Sepsis, BPIC-2013, and BPIC-2012-App datasets demonstrate superior performance in process discovery metrics (Fitness/Precision) and data utility (Relative Log Similarity)
- The methods provide robust privacy assurances without requiring data-dependent thresholds or introducing fake variants

## Why This Works (Mechanism)
The generative modeling approaches work by learning the underlying distribution of trace variants while adding calibrated noise during training to ensure differential privacy. TraVaG uses the adversarial training framework to learn implicit distributions, while DDPM leverages a noise predictor to reverse a diffusion process. Both methods avoid the fundamental limitation of prefix-based approaches that either introduce fake variants or remove frequent true variants to achieve privacy. By training on the entire variant distribution and using RDP accounting, the methods can provide formal privacy guarantees while preserving the statistical properties of the original data.

## Foundational Learning
- **Differential Privacy (DP):** A mathematical framework ensuring individual data points cannot be distinguished in the output. Needed to provide formal privacy guarantees for event logs.
- **Renyi Differential Privacy (RDP):** An extension of DP that provides tighter composition bounds for Gaussian mechanisms. Needed for accurate privacy accounting in iterative training.
- **Generative Adversarial Networks (GANs):** Framework with generator and discriminator networks competing to learn data distributions. Needed for TraVaG's implicit variant sampling.
- **Denoising Diffusion Probabilistic Models (DDPMs):** Framework reversing a gradual noise-adding process to generate data. Needed for the alternative generative approach.
- **DP-SGD (Differentially Private Stochastic Gradient Descent):** Training algorithm adding Gaussian noise to gradients. Needed to ensure privacy during model training.
- **Trace Variants:** Distinct process execution patterns in event logs. Needed as the fundamental unit of analysis for process mining.

## Architecture Onboarding

**Component Map:**
Simple Event Log -> One-Hot Encoding -> DP-SGD Training (TraVaG: Autoencoder+GAN / DDPM: Noise Predictor) -> RDP Privacy Accounting -> Synthetic Log Generation

**Critical Path:**
Preprocessing (variant extraction) -> One-hot encoding -> DP-SGD training with RDP accounting -> Synthetic variant generation -> Log reconstruction

**Design Tradeoffs:**
- Binary one-hot encoding vs. embedding representations (simplicity vs. scalability)
- Fixed vs. adaptive noise multipliers during training (predictability vs. convergence)
- RDP accounting vs. standard DP composition (tighter bounds vs. simpler implementation)

**Failure Signatures:**
- Model non-convergence: Flat or divergent loss curves during training
- Utility collapse: Low Relative Log Similarity or poor process discovery metrics
- Privacy over-commitment: Inability to satisfy target ε,δ parameters

**First Experiments:**
1. Preprocess a small event log into variants and verify one-hot encoding correctness
2. Train a non-private version of TraVaG/DDPM to establish baseline performance
3. Apply DP-SGD with increasing noise levels to observe the privacy-utility tradeoff curve

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the dependency on privacy-related hyperparameters be integrated into a fully automated tuning strategy to create parameter-free generative models?
- Basis in paper: The conclusion states that future research could investigate the dependency on hyperparameters to potentially transform TraVaG and DDPM engines into streamlined, parameter-free methods akin to TraVaS.
- Why unresolved: Currently, guaranteeing specific privacy levels necessitates an iterative, manual analysis of various ANN settings to find a suitable configuration for the target (ε, δ).
- What evidence would resolve it: A proposed framework that autonomously selects optimal noise multipliers and batch sizes to converge on a target privacy budget without manual intervention.

### Open Question 2
- Question: How can these generative frameworks be extended to handle event attributes (e.g., timestamps, resources) while maintaining high utility?
- Basis in paper: The authors define their input as a "Simple Event Log" focused exclusively on the control-flow perspective, explicitly noting that they do not consider additional attributes unlike other methods such as PRIPEL.
- Why unresolved: The current TraVaG architecture relies on binary one-hot encoding of variants; extending this to mixed-type, high-dimensional attribute data requires significant architectural changes.
- What evidence would resolve it: An extension of the TraVaG or DDPM models applied to event logs containing attributes, demonstrating comparable utility and privacy metrics to the variant-only results.

### Open Question 3
- Question: What is the minimum data size or complexity threshold required for TraVaG and DDPMs to converge under strong privacy guarantees (ε < 0.01)?
- Basis in paper: The authors note that for small, complex datasets like Sepsis, the gradient noise introduced by strong privacy settings can be "too intense for the TraVaG model to converge."
- Why unresolved: The paper provides experimental evidence of failure cases but does not offer a theoretical or empirical rule of thumb for the lower bounds of training data size relative to privacy noise.
- What evidence would resolve it: A systematic ablation study on synthetic logs defining the relationship between trace uniqueness, case count, and the minimum viable ε for stable training.

## Limitations
- Reliance on specific neural network architectures raises concerns about generalizability to event logs with different characteristics
- Privacy analysis depends on RDP accounting with fixed δ values, which may not adequately protect against composition attacks in streaming scenarios
- Grid search for hyperparameter tuning introduces computational overhead and requires careful calibration

## Confidence
- **High:** The core methodology of using generative models with DP-SGD for event log privatization is well-founded and technically sound
- **Medium:** The empirical results demonstrating superior utility compared to prefix-based methods are compelling but limited to the three tested datasets
- **Low:** The long-term stability of the privatized logs under iterative analysis and the robustness to adversarial attacks are not explicitly validated

## Next Checks
1. **Architecture Generalization:** Test TraVaG and DDPM on event logs with significantly different characteristics (e.g., longer traces, higher variance in variant frequency) to assess model robustness
2. **Privacy Composition:** Evaluate the privacy guarantees under repeated analyses or streaming scenarios, where composition attacks may degrade the (ε, δ) bounds
3. **Adversarial Robustness:** Conduct a formal security analysis to determine if the generative models leak sensitive information through their learned distributions or synthetic outputs