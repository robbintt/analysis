---
ver: rpa2
title: 'Beyond Overall Accuracy: A Psychometric Deep Dive into the Topic-Specific
  Medical Capabilities of 80 Large Language Models'
arxiv_id: '2509.24186'
source_url: https://arxiv.org/abs/2509.24186
tags:
- openai
- ability
- proprietary
- medical
- google
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces MEDIRT, a psychometric evaluation framework
  using Item Response Theory (IRT) to assess the medical knowledge capabilities of
  80 large language models (LLMs) across 11 USMLE-aligned medical topics. Unlike traditional
  accuracy metrics, MEDIRT jointly estimates model abilities and item characteristics,
  producing difficulty-adjusted ability scores that reveal nuanced, topic-specific
  competency profiles.
---

# Beyond Overall Accuracy: A Psychometric Deep Dive into the Topic-Specific Medical Capabilities of 80 Large Language Models

## Quick Facts
- **arXiv ID:** 2509.24186
- **Source URL:** https://arxiv.org/abs/2509.24186
- **Reference count:** 20
- **Primary result:** Introduces MEDIRT, a psychometric framework using IRT to produce difficulty-adjusted ability scores for 80 LLMs across 11 USMLE-aligned medical topics, revealing domain-specific strengths invisible to aggregate accuracy.

## Executive Summary
This study introduces MEDIRT, a psychometric evaluation framework using Item Response Theory (IRT) to assess the medical knowledge capabilities of 80 large language models (LLMs) across 11 USMLE-aligned medical topics. Unlike traditional accuracy metrics, MEDIRT jointly estimates model abilities and item characteristics, producing difficulty-adjusted ability scores that reveal nuanced, topic-specific competency profiles. The study prospectively collected responses from 80 diverse LLMs on a balanced 1,100-question benchmark. Results show that while GPT-5 was the top performer in 8 of 11 domains, other models like Claude-3-opus led in specific areas such as Social Science and Communication, demonstrating that overall rankings can be misleading due to highly specialized abilities. The framework also identified flawed benchmark questions and established a practical decision-support system integrating competency profiles with operational metrics like cost and latency for safe, effective LLM deployment in healthcare.

## Method Summary
The study constructed a 1,100-question benchmark (100 per topic) by stratified sampling from MedQA, MedMCQA, and MedXpertQA, with questions labeled by GPT-oss-120b per USMLE Step 1 specifications. Eighty diverse LLMs were evaluated via OpenRouter API using standardized prompts (temperature=0, max_tokens=3000). For each of 11 medical topics, an independent 2PL IRT model was fitted to jointly estimate item parameters (discrimination a, difficulty b) and model abilities (θ). Items with negative discrimination and positive difficulty were flagged for expert review. Results were integrated into a decision-support system combining θ scores with cost and latency for deployment guidance.

## Key Results
- GPT-5 ranked first overall and led in 8 of 11 medical topics, but Claude-3-opus excelled in Social Science and Communication
- IRT ability scores revealed "spiky" competency profiles, showing models have domain-specific strengths invisible to aggregate metrics
- 15 benchmark items showed negative discrimination (high-ability models failed more often), with one ("rapid prototyping") identified as flawed due to imprecise distractors
- Marginal reliability exceeded 0.93 across all topics, validating the stability of the IRT estimates

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Joint estimation of model ability and item characteristics yields more stable, difficulty-adjusted rankings than raw accuracy.
- **Mechanism:** The 2PL IRT model (Pr(X=1|θ) = σ(a(θ-b))) factorizes observed correctness into latent ability θ, item difficulty b, and discrimination a. This decorrelates performance from question easiness, exposing whether high accuracy stems from genuine competence or easy items.
- **Core assumption:** Responses reflect a single latent ability per topic; local independence holds across items.
- **Evidence anchors:**
  - [abstract] "estimate LLM's latent model ability jointly with question difficulty and discrimination, yielding more stable and nuanced performance rankings than accuracy alone"
  - [section 2.3.2] Full 2PL specification with θ, a, b parameters
  - [corpus] JE-IRT (arXiv 2509.22888) confirms IRT-based geometric embeddings improve ability estimation over single-score aggregation
- **Break condition:** If items within a topic measure multiple distinct abilities (violating unidimensionality), θ estimates conflate heterogeneous skills.

### Mechanism 2
- **Claim:** Topic-level unidimensional models produce interpretable "spiky" competency profiles that reveal domain-specific strengths invisible to aggregate metrics.
- **Mechanism:** Fitting 11 independent 2PL models per USMLE topic yields θ_m,t per (model, topic) pair. This avoids the instability of a single multidimensional IRT model with correlated dimensions (N=80 models × 1,100 items), while producing directly interpretable specialty scores.
- **Core assumption:** Medical knowledge partitions meaningfully into USMLE-aligned topics; topics are sufficiently distinct to justify separate models.
- **Evidence anchors:**
  - [section 4.1] Claude-3-opus ranks 23rd overall but leads Communication (θ=1.80); GPT-5 dominates most domains but not all
  - [section 2.3.1] Explicit rationale: multidimensional model caused "estimation instability," topic-level models achieve "stable, reliable results"
  - [corpus] Corpus has limited direct replication; neighbor papers focus on IRT for difficulty estimation, not topic profiling
- **Break condition:** If topic boundaries are arbitrary (high inter-topic correlation), separate models may over-fragment a coherent ability structure.

### Mechanism 3
- **Claim:** Items with negative discrimination (higher-ability models fail more often) flag flawed questions or subtle model pathologies.
- **Mechanism:** Negative a with positive b indicates inverse relationship between ability and correctness. The authors trace one such item ("rapid prototyping") to an "All of the above" option with imprecise distractors, rendering the intended answer unsound.
- **Core assumption:** Negative discrimination primarily signals item quality issues rather than genuine skill inversions.
- **Evidence anchors:**
  - [section 4.2] 15 items with a<0, b>0; "rapid prototyping" item with a=-0.342, b=0.77 missed by GPT-5
  - [section 5.1] "dual-probe diagnostic methodology"—first validate item integrity, then probe model reasoning
  - [corpus] Corpus mentions IWF (Item-Writing Flaws) impact on discrimination (arXiv 2503.10533), supporting item-quality interpretation
- **Break condition:** If negative-discrimination items survive expert validation, they may reveal real model overthinking or systematic fallacies rather than flawed items.

## Foundational Learning

- **Concept: Two-Parameter Logistic (2PL) IRT Model**
  - **Why needed here:** The entire framework rests on understanding how θ, a, and b interact in the logistic function to produce response probabilities.
  - **Quick check question:** If θ=1.0, b=0.5, a=2.0, what is Pr(correct)? (Answer: σ(2.0×(1.0-0.5)) = σ(1.0) ≈ 0.73)

- **Concept: Discrimination vs. Difficulty**
  - **Why needed here:** Interpreting the item parameter landscape and identifying flawed questions requires distinguishing what a and b each measure.
  - **Quick check question:** A high-difficulty item (b=2.0) with low discrimination (a=0.3) is useful for what ability range? (Answer: Barely useful—low a means it poorly distinguishes anywhere; high b means even high-θ models barely exceed 50% success)

- **Concept: Unidimensionality Assumption**
  - **Why needed here:** The paper explicitly rejects multidimensional IRT for stability; understanding why requires grasping what happens when this assumption is violated.
  - **Quick check question:** If a "Cardiovascular" topic mixes anatomy knowledge and pharmacology reasoning, what happens to θ estimates? (Answer: θ conflates two abilities; model rankings become noisy and topic-specific insights are diluted)

## Architecture Onboarding

- **Component map:** Benchmark constructor -> API inference layer -> IRT estimation -> Diagnostic layer -> Decision-support visualization

- **Critical path:** Benchmark construction → API inference (all 80 models × 1,100 items) → IRT fitting (11 topics) → θ aggregation → decision-support visualization

- **Design tradeoffs:**
  - *Unidimensional per-topic vs. single multidimensional:* Stability + interpretability vs. capturing cross-topic correlations
  - *Fresh prospective data vs. archival:* Control and comparability vs. cost/time
  - *100 items per topic:* Sufficient for stable 2PL estimation vs. limited granularity within topics

- **Failure signatures:**
  - θ estimates with very high SE (>0.5) indicate insufficient items or poor model fit
  - High correlation between θ and raw accuracy without rank changes suggests low item difficulty variance
  - Many items with a≈0 suggest benchmark lacks discrimination power

- **First 3 experiments:**
  1. **Reproduce one topic's IRT fit** — Take Cardio (100 items, 80 models), fit 2PL, verify θ rankings correlate with paper's Table S5; check marginal reliability >0.93
  2. **Validate negative-discrimination audit** — Locate the "rapid prototyping" item in the benchmark, inspect its distractors, confirm a<0 in fitted model
  3. **Test rank sensitivity to item sampling** — Resample 80 items per topic, refit IRT, measure θ rank correlation with original; target ρ>0.85 for stability claim

## Open Questions the Paper Calls Out

- **Question:** Do MEDIRT ability estimates derived from static multiple-choice questions correlate with performance in dynamic, interactive clinical simulations?
  - **Basis in paper:** [explicit] The authors state in the Limitations section that the reliance on static MCQs "does not fully capture the dynamic nature of clinical reasoning."
  - **Why unresolved:** The study established a psychometric baseline using static exams, but clinical practice involves agentic, longitudinal reasoning not measured by single-turn questions.
  - **What evidence would resolve it:** A longitudinal study correlating IRT ability scores (θ) with performance metrics in interactive, agentic clinical simulation environments.

- **Question:** Can negative-discrimination items that are validated as factually sound reliably serve as probes for specific reasoning fallacies (e.g., overthinking) in high-ability models?
  - **Basis in paper:** [explicit] The Discussion section proposes a "dual-probe diagnostic workflow" using negative-discrimination items to expose "subtle, systematic reasoning fallacies in high-ability models."
  - **Why unresolved:** While the framework identifies these anomalous items, it has not yet been empirically verified that these specific failures map to consistent, diagnosable reasoning pathologies rather than random noise.
  - **What evidence would resolve it:** A qualitative analysis of model outputs on validated negative-discrimination items to identify and categorize consistent reasoning errors.

- **Question:** How do the topic-specific competency profiles of specialized medical LLMs compare to the generalist "spiky" profiles observed in this study?
  - **Basis in paper:** [explicit] The Limitations section notes the study's reliance on the OpenRouter API "excluded specialized medical LLMs," leaving their psychometric profiles unknown.
  - **Why unresolved:** The current data covers generalist and open models, but it is unclear if specialized medical fine-tuning produces flatter, more consistent ability profiles across all domains.
  - **What evidence would resolve it:** Applying the MEDIRT framework to a cohort of specialized medical LLMs (e.g., Med-PaLM) and comparing the variance of their topic-specific θ scores against generalists.

## Limitations
- Benchmark constructed from static multiple-choice questions cannot capture dynamic, interactive clinical reasoning
- Study excluded specialized medical LLMs due to API constraints, limiting generalizability to domain-specific models
- IRT framework assumes unidimensionality within topics, which may oversimplify complex medical knowledge structures

## Confidence
- **High confidence:** IRT framework validity, basic 2PL mechanics, observed patterns (e.g., Claude-3-opus excelling in Communication, GPT-5's dominance across domains), and negative-discrimination items flagging flawed questions are well-supported by the methodology and data.
- **Medium confidence:** The claim that unidimensional per-topic IRT yields "stable, reliable results" vs. multidimensional IRT is supported by the authors' comparison, but the paper does not provide full convergence diagnostics or robustness checks across different estimation algorithms.
- **Low confidence:** The decision-support system integrating competency profiles with cost/latency for "safe, effective" deployment is described but not validated against real-world deployment outcomes or user studies.

## Next Checks
1. **Robustness of IRT rankings:** Resample 80 items per topic, refit 2PL IRT, and measure rank correlation (ρ) with original θ estimates; require ρ>0.85 for stability.
2. **Negative-discrimination item audit:** Independently verify the "rapid prototyping" item's distractors and intended answer; confirm a<0 in fitted model and assess whether this reflects item quality or model pathology.
3. **Decision-support system validation:** Test the cost-performance Pareto frontier recommendation against a small cohort of medical practitioners deploying LLMs in simulated clinical scenarios; measure alignment between IRT-predicted and observed performance.