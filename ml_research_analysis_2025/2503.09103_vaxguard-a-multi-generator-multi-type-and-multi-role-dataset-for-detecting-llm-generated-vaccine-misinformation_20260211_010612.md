---
ver: rpa2
title: 'VaxGuard: A Multi-Generator, Multi-Type, and Multi-Role Dataset for Detecting
  LLM-Generated Vaccine Misinformation'
arxiv_id: '2503.09103'
source_url: https://arxiv.org/abs/2503.09103
tags:
- misinformation
- llms
- detection
- gpt-3
- vaccine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VaxGuard is a novel dataset designed to detect LLM-generated vaccine
  misinformation across multiple roles (misinformation spreaders, religious conspiracy
  theorists, fearmongers, anti-vacciners) and vaccine types (COVID-19, HPV, Influenza).
  It includes misinformation generated by five LLMs (GPT-3.5, GPT-4o, LLaMA3, PHI3,
  Mistral) with role-specific prompts.
---

# VaxGuard: A Multi-Generator, Multi-Type, and Multi-Role Dataset for Detecting LLM-Generated Vaccine Misinformation

## Quick Facts
- **arXiv ID**: 2503.09103
- **Source URL**: https://arxiv.org/abs/2503.09103
- **Reference count**: 8
- **Primary result**: Novel dataset for detecting LLM-generated vaccine misinformation across multiple roles, types, and generators

## Executive Summary
VaxGuard is a comprehensive dataset designed to evaluate and improve detection of LLM-generated vaccine misinformation. The dataset encompasses five different large language models (GPT-3.5, GPT-4o, LLaMA3, PHI3, Mistral) generating misinformation across four distinct roles: misinformation spreaders, religious conspiracy theorists, fearmongers, and anti-vacciners. The dataset covers three vaccine types (COVID-19, HPV, Influenza) with role-specific prompts to create diverse misinformation scenarios.

The evaluation demonstrates significant variation in detection performance across both LLMs and misinformation roles. GPT-3.5 achieves the highest F1 score of 0.96 overall, while detection accuracy decreases with longer input sequences. The study reveals that PHI3 and Mistral perform substantially worse than other models, particularly on fear-driven content. These findings highlight the need for role-specific detection strategies and underscore the challenges of detecting LLM-generated misinformation in real-world scenarios.

## Method Summary
The VaxGuard dataset was constructed by generating vaccine misinformation using five different LLMs with carefully designed role-specific prompts. The dataset covers four misinformation roles (misinformation spreaders, religious conspiracy theorists, fearmongers, anti-vacciners) and three vaccine types (COVID-19, HPV, Influenza). Each LLM was prompted to generate content according to its assigned role, creating a diverse corpus of misinformation examples. The dataset was then used to evaluate detection performance across different models, roles, and input lengths, with particular attention to how detection accuracy varies with content characteristics.

## Key Results
- GPT-3.5 achieved the highest F1 score of 0.96 overall for misinformation detection
- Detection accuracy declined significantly as input length increased
- PHI3 (0.79 F1) and Mistral (0.82 F1) performed substantially worse than GPT-3.5, GPT-4o, and LLaMA3, especially on fear-driven content
- Different misinformation roles presented varying detection challenges, requiring role-specific detection strategies

## Why This Works (Mechanism)
The effectiveness of VaxGuard stems from its comprehensive approach to capturing the diverse landscape of LLM-generated vaccine misinformation. By employing multiple LLMs with distinct architectural characteristics and training data, the dataset captures varied patterns of misinformation generation. The role-specific prompts ensure that the generated content reflects real-world misinformation tactics, including fearmongering, religious conspiracy theories, and anti-vaccine narratives. The multi-type approach across different vaccines allows for broader generalizability of detection methods.

## Foundational Learning
- **LLM-Generated Misinformation Detection**: Understanding how different LLMs generate and what patterns they exhibit when creating misinformation is crucial for developing effective detection methods. Quick check: Compare generation patterns across different LLMs using statistical analysis.
- **Role-Specific Misinformation Characteristics**: Different misinformation roles have distinct linguistic and semantic patterns that affect detectability. Quick check: Analyze feature importance across roles to identify distinguishing characteristics.
- **Input Length Impact**: Detection performance varies with text length, indicating different optimal approaches for short vs. long content. Quick check: Plot detection accuracy against input length to identify breakpoints.
- **Multi-Model Evaluation Framework**: Assessing detection across multiple LLMs provides more robust validation than single-model approaches. Quick check: Compare detection performance variance across different evaluation frameworks.

## Architecture Onboarding
**Component Map**: Data Generation -> Role Assignment -> Prompt Engineering -> LLM Generation -> Detection Model Training -> Performance Evaluation

**Critical Path**: Prompt Design → LLM Generation → Feature Extraction → Detection Model Training → Evaluation

**Design Tradeoffs**: The study prioritizes comprehensive coverage of misinformation types over depth in any single area, trading specialized analysis for broader applicability. This approach enables general insights but may miss nuanced patterns specific to particular misinformation types.

**Failure Signatures**: Detection performance drops significantly for longer texts and certain LLM-generated content (particularly PHI3 and Mistral), suggesting limitations in current detection approaches for extended narratives and specific generation patterns.

**First Experiments**: 
1. Evaluate detection performance across different input length ranges to identify optimal processing windows
2. Compare detection accuracy for different misinformation roles to identify most challenging categories
3. Test cross-LLM detection transfer to assess model generalization capabilities

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions, but several areas for future research emerge from the findings, including the need for cross-language validation, investigation of real-world impact, and exploration of adversarial prompt engineering techniques.

## Limitations
- Detection accuracy varies significantly across different LLMs, with PHI3 and Mistral performing notably worse
- Performance declines for longer text sequences, limiting effectiveness for comprehensive content analysis
- Focus on English-language vaccine misinformation may limit generalizability to other languages and health topics

## Confidence
- **High Confidence**: Performance differences between LLMs are well-supported by experimental results
- **Medium Confidence**: Role-specific detection challenges are documented but may require additional validation
- **Low Confidence**: Generalizability to other languages and health topics remains uncertain

## Next Checks
1. Test detection performance across additional languages and health-related misinformation domains
2. Conduct human evaluation studies comparing machine detection with human ability to identify LLM-generated misinformation
3. Evaluate detection robustness against evolving LLM capabilities using newer model versions and adversarial techniques