---
ver: rpa2
title: 'PolyGraph Discrepancy: a classifier-based metric for graph generation'
arxiv_id: '2510.06122'
source_url: https://arxiv.org/abs/2510.06122
tags:
- number
- graphs
- graph
- log2
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PolyGraph Discrepancy (PGD), a new metric
  for evaluating graph generative models. Existing metrics based on Maximum Mean Discrepancy
  (MMD) suffer from high variance and lack an interpretable scale.
---

# PolyGraph Discrepancy: a classifier-based metric for graph generation

## Quick Facts
- arXiv ID: 2510.06122
- Source URL: https://arxiv.org/abs/2510.06122
- Reference count: 40
- Key outcome: Introduces PolyGraph Discrepancy (PGD), a bounded [0,1] metric that provides a lower bound on Jensen-Shannon distance for graph generative models, overcoming high variance and interpretability issues of existing MMD-based metrics.

## Executive Summary
PolyGraph Discrepancy (PGD) is a new classifier-based metric for evaluating graph generative models. Unlike existing metrics based on Maximum Mean Discrepancy (MMD), PGD trains classifiers to distinguish real from generated graphs using various graph descriptors, with the classifier's performance providing a lower bound on the Jensen-Shannon distance between graph distributions. The metric is bounded in [0,1], comparable across different descriptors, and robust to noise. Experiments demonstrate that PGD strongly correlates with model quality and training progress, producing more reliable rankings than MMD while offering better interpretability and stability.

## Method Summary
PGD works by training classifiers (specifically TabPFN) to distinguish between real and generated graphs using various graph descriptors such as Degree, Clustering, Spectrum, Orbit, GIN, Morgan, ChemNet, and MolCLR. The evaluation process involves splitting data into fit and test sets, performing cross-validation on the fit set to select the best descriptor, then training on the full fit set and evaluating on the held-out test set. The final score is calculated as the square root of the maximum of the log-likelihood and zero, providing a bounded metric in [0,1] that estimates the lower bound of the Jensen-Shannon distance. The method is designed to be descriptor-agnostic and comparable across different graph types and molecular datasets.

## Key Results
- PGD strongly correlates with model quality and training progress across multiple graph generation benchmarks
- Produces more reliable rankings than MMD while offering better interpretability and stability
- Shows robustness to noise and parameter variations, with performance stable across different sample sizes
- The open-source library implementation facilitates reproducible research in graph generation evaluation

## Why This Works (Mechanism)
PGD works by leveraging classifier performance as a proxy for distribution distance. By training classifiers to distinguish real from generated graphs using multiple descriptors, the metric captures the difficulty of telling the two distributions apart. The max-reduction across descriptors ensures that the most informative signal is captured, providing a conservative lower bound on the true Jensen-Shannon distance. The bounded nature of the metric (0 to 1) makes it interpretable and comparable across different graph types and molecular datasets.

## Foundational Learning
- **TabPFN classifier**: Why needed - Efficient tabular classification without manual hyperparameter tuning. Quick check - Verify TabPFN installation and basic classification on simple tabular data.
- **Graph descriptors**: Why needed - Transform graph structure into features that classifiers can process. Quick check - Extract degree and clustering features from simple graphs and verify histogram distributions.
- **Jensen-Shannon distance**: Why needed - Provides a symmetric, bounded measure of distribution similarity. Quick check - Calculate JS distance between simple synthetic distributions.
- **Maximum Mean Discrepancy (MMD)**: Why needed - Understanding the limitations of current metrics that PGD aims to improve. Quick check - Compare MMD variance on small vs large sample sizes.
- **4-fold cross-validation**: Why needed - Robust selection of the best descriptor without overfitting. Quick check - Implement 4-fold CV on simple synthetic data.

## Architecture Onboarding

**Component Map:**
Data -> Descriptor Extraction -> TabPFN Classifier -> Cross-Validation -> Best Descriptor Selection -> Final Training -> PGD Score

**Critical Path:**
The critical path is: Data preparation → Descriptor extraction → TabPFN training → Cross-validation for descriptor selection → Final evaluation → Score calculation. Each step must complete successfully for the metric to be computed.

**Design Tradeoffs:**
- Single best descriptor vs. combined features: Max-reduction strategy is simpler but may under-utilize complementary information
- TabPFN dependency: Provides automated classification but introduces runtime limits (10k samples) and potential implementation complexity
- 50/50 split: Ensures sufficient data for both selection and evaluation but reduces effective sample size

**Failure Signatures:**
- Score saturates at 1.0 immediately: Distributions are trivially separable (data leakage or too simple)
- High variance across runs: Insufficient sample size (<256 graphs per set) or unstable descriptor extraction
- TabPFN runtime errors: Feature dimensionality too high or sample size exceeds 10k limit

**First Experiments:**
1. Generate two simple synthetic graph distributions (one with added noise) and verify PGD increases with noise level
2. Compare PGD vs MMD scores on PLANAR-L dataset to confirm lower variance
3. Test PGD on molecular datasets using Morgan fingerprints vs GIN embeddings to validate descriptor comparability

## Open Questions the Paper Calls Out
- Can combining graph descriptors prior to fitting or using sophisticated feature selection yield tighter lower bounds on the JS distance than the current max-reduction approach? The authors leave exploration of optimal feature selection to future work.
- Can the PolyGraph Discrepancy framework be effectively extended to directed, weighted, temporal, or heterogeneous graphs? The authors do not evaluate these complex graph types and leave this to future work.
- Can the multi-descriptor max-reduction strategy mitigate the sensitivity of Inception Score and FID to network initialization in image generation? The authors propose this as a promising direction to address initialization sensitivity issues.

## Limitations
- Dependency on TabPFN's training limits (10k samples) requires custom extensions for larger datasets
- Potential sensitivity to choice of histogram binning and feature normalization not fully specified
- Current method selects single "best" descriptor, which may discard useful variance or interaction effects present in full descriptor set

## Confidence
- High: Core theoretical claims that PGD provides lower bound on JS distance and is bounded in [0,1]
- Medium: Experimental evidence supporting superiority over MMD, but implementation details for large datasets unclear
- Medium: Claims about robustness and stability supported by experiments but dependent on proper preprocessing

## Next Checks
1. Implement PGD on a small, controlled dataset (e.g., 256 graphs) and verify that the score increases as the generated distribution diverges from the reference
2. Compare PGD scores with MMD scores on the same datasets to confirm lower variance and better stability as claimed
3. Test the open-source library on a standard graph generation benchmark (e.g., PLANAR-L) to validate end-to-end reproducibility