---
ver: rpa2
title: Does Interpretability of Knowledge Tracing Models Support Teacher Decision
  Making?
arxiv_id: '2511.02718'
source_url: https://arxiv.org/abs/2511.02718
tags:
- teachers
- knowledge
- simulation
- task
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates whether interpretable knowledge tracing
  (KT) models help teachers make better pedagogical decisions compared to non-interpretable
  models. A simulation environment was created where teachers selected tasks and decided
  when to stop teaching based on information from three KT models: Bayesian Knowledge
  Tracing (BKT), Performance Factors Analysis (PFA), and Deep Knowledge Tracing (DKT).'
---

# Does Interpretability of Knowledge Tracing Models Support Teacher Decision Making?

## Quick Facts
- arXiv ID: 2511.02718
- Source URL: https://arxiv.org/abs/2511.02718
- Authors: Adia Khalid; Alina Deriyeva; Benjamin Paassen
- Reference count: 7
- Primary result: Interpretable KT models (BKT, PFA) achieve mastery faster in simulation but don't improve human teacher decisions

## Executive Summary
This study investigates whether interpretable knowledge tracing (KT) models help teachers make better pedagogical decisions compared to non-interpretable models. A simulation environment was created where teachers selected tasks and decided when to stop teaching based on information from three KT models: Bayesian Knowledge Tracing (BKT), Performance Factors Analysis (PFA), and Deep Knowledge Tracing (DKT). In the simulation, BKT and PFA achieved mastery faster and more reliably than DKT. However, when 12 human teachers made the decisions, no significant difference in mastery time was observed between models. Teachers rated interpretable models higher in usability and trustworthiness but often selected suboptimal tasks, suggesting they may have misinterpreted the information provided. This indicates that the relationship between model interpretability and teacher decision-making is complex, with teachers relying on additional factors beyond KT model information.

## Method Summary
The study created a simulation environment where teachers could select tasks and decide when to stop teaching based on information from three KT models: BKT, PFA, and DKT. The simulation used an Elo-based ground truth model with 4 tasks and 2 skills. In automated simulations, BKT and PFA achieved mastery faster (6 steps) than DKT (14 steps). Twelve human teachers then made decisions across 9 simulations each, with task order randomized. Teachers rated models on usability (SUS) and trust (TOAST), and their task selections and mastery outcomes were recorded.

## Key Results
- In simulation, BKT and PFA achieved mastery in 6 steps versus DKT's 14 steps
- Human teachers showed no significant difference in mastery time across models
- Teachers rated interpretable models higher in usability and trustworthiness
- Teachers selected slightly better tasks in DKT condition, suggesting use of information beyond KT model outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interpretable KT models (BKT, PFA) enable more efficient automated task selection in simulation environments
- Mechanism: Explicit ability estimates (θ_t,k) allow direct computation of expected learning gain per task via closed-form equations. BKT uses Bayesian updates (Eq. 2) and PFA uses logistic skill accumulation (Eq. 3), enabling argmax selection over learning gain.
- Core assumption: The expected learning gain formula accurately captures true learning dynamics.
- Evidence anchors:
  - [abstract] "In a simulation study, teachers achieved mastery faster using interpretable models (6 steps) compared to DKT (14 steps)"
  - [section 3.2] "Mastery was actually achieved in PFA and BKT after 6 steps, consistent with the optimum... whereas DKT needed a median of 14 steps"
  - [corpus] Hierarchical Bayesian KT paper confirms interpretable statistical approaches support topic difficulty identification
- Break condition: When KT model assumptions (e.g., BKT's two-state mastery, PFA's logistic skill growth) diverge from actual learning dynamics.

### Mechanism 2
- Claim: DKT's lack of explicit ability estimates causes unreliable stopping decisions and infinite loops
- Mechanism: DKT latent states h_t have no interpretable mapping to mastery. Without explicit θ_t,k, stopping criteria must proxy through predicted success probabilities, which can fail to converge to mastery thresholds.
- Core assumption: Mastery can be reliably detected from predicted success probabilities alone.
- Evidence anchors:
  - [abstract] "DKT stopped before achieving mastery in 24% of cases"
  - [section 3.2] "In the majority of simulations, we had to enforce an upper limit of 30 tasks because the simulation was caught in endless loops"
  - [corpus] No direct corpus support for DKT failure modes; corpus papers focus on teacher-AI collaboration, not KT model failures
- Break condition: When DKT's hidden state dimension is sufficiently high and training data abundant enough to implicitly encode mastery signals.

### Mechanism 3
- Claim: Human teachers use multiple information sources beyond KT model outputs, reducing interpretability's impact on decision quality
- Mechanism: Teachers integrate task-skill mappings, historical performance patterns, and pedagogical intuition. When DKT provides less information (no graph), teachers compensate by selecting optimal tasks (Task 4) more frequently.
- Core assumption: Teachers possess domain-general pedagogical strategies that transfer across KT model conditions.
- Evidence anchors:
  - [abstract] "Teachers actually selected slightly better tasks in the DKT condition"
  - [section 4.2] "In the DKT condition, teachers selected task 4 much more frequently, which was the optimal choice... indicating that teachers considered information beyond what was provided by KT models"
  - [corpus] Human-centric XAI education paper emphasizes teachers need complementary information beyond model outputs
- Break condition: When task complexity exceeds teachers' ability to infer optimal choices without explicit model guidance.

## Foundational Learning

- Concept: Knowledge Tracing (KT) as sequence-to-probability mapping
  - Why needed here: Understanding that KT models map (task, outcome) history → future success probabilities is prerequisite for comparing BKT/PFA/DKT
  - Quick check question: Given 3 past failures on skill k, how would PFA update its ability estimate differently than BKT?

- Concept: Interpretability as explicit ability estimation
  - Why needed here: The paper's core hypothesis hinges on distinguishing interpretable (explicit θ_t,k) from non-interpretable (latent h_t) models
  - Quick check question: Can you extract a student's current skill level from DKT's hidden state without additional decoding mechanisms?

- Concept: Expected learning gain for task selection
  - Why needed here: The simulation optimizes task selection by maximizing predicted learning gain, which differs fundamentally across models (Eq. 4, 5, 6)
  - Quick check question: Why does DKT require computing ∆p across all tasks to estimate learning gain while PFA only needs γ_k and ρ_k?

## Architecture Onboarding

- Component map:
```
[Student History] → [KT Model (BKT/PFA/DKT)] → [Success Probabilities p_t,j]
                                              ↓
                                    [Ability Estimates θ_t,k] (BKT/PFA only)
                                              ↓
                              [Expected Learning Gain Computation]
                                              ↓
                              [Task Selection: argmax over tasks]
                                              ↓
[Success/Failure Sampled from Elo Ground Truth] ← [Selected Task j_t]
                                              ↓
                          [Elo Update θ* and KT Model Update]
                                              ↓
                              [Mastery Check: all skills > 1.5]
```

- Critical path: KT model prediction → expected learning gain computation → task selection → ground truth sampling → model updates → mastery check

- Design tradeoffs:
  - BKT: Interpretable mastery states, but binary skill assumption limits granularity
  - PFA: Continuous ability tracking, success/failure learning rates (γ_k, ρ_k), but requires more parameters per skill
  - DKT: Highest representational capacity, no explicit interpretability, unreliable stopping behavior

- Failure signatures:
  - DKT endless loops: p_t,j never exceeds mastery threshold → enforce max_steps=30
  - Early stopping: Teacher/model stops when θ_t,k < 1.5 but θ* > 1.5 (false negative mastery)
  - Task selection bias: Over-selection of single-skill tasks (1, 2) vs. optimal multi-skill task (4)

- First 3 experiments:
  1. Replicate simulation study with different Elo parameters (κ_1, κ_0 learning rates) to test robustness of interpretable model advantage
  2. Ablate teacher interface: show DKT with synthetic "ability graphs" computed from p_t,j → test if visual presentation drives usability ratings
  3. Increase task/skill complexity (6 skills, 10 tasks) to identify when teacher compensatory strategies fail without interpretable feedback

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do teachers actually utilize the information provided by knowledge tracing models to make pedagogical decisions?
- Basis in paper: [explicit] The abstract and conclusion state that "further research is needed to investigate how learners and teachers actually understand and use KT models" because the relationship between interpretability and decision-making is not straightforward.
- Why unresolved: The study found that teachers rated interpretable models higher in trust but did not make better decisions with them, and the authors could not determine the specific cognitive process teachers used.
- What evidence would resolve it: Qualitative analysis (e.g., think-aloud protocols) of teachers during the decision-making process to identify which specific pieces of information (graphs vs. probabilities) influence task selection.

### Open Question 2
- Question: Does presenting explicit ability graphs induce cognitive overload or misinterpretation compared to non-interpretable probability outputs?
- Basis in paper: [inferred] The authors hypothesize in the Discussion that the lack of performance improvement with interpretable models might be due to teachers being "cognitively overloaded by the graphs" or having "mis-interpreted the graphs," given that DKT (no graphs) yielded slightly better task choices.
- Why unresolved: The study measured outcomes (tasks to mastery) and subjective trust, but did not measure cognitive load or the accuracy of the teachers' mental models of the system.
- What evidence would resolve it: A comparative study measuring cognitive load (e.g., NASA-TLX) and testing teachers' comprehension of the model state after viewing interpretable vs. non-interpretable interfaces.

### Open Question 3
- Question: Do interpretable models improve teacher decision-making in realistic environments with rich information and varied pedagogical actions?
- Basis in paper: [explicit] The authors note in the Limitations that the simulation environment was a "reduced laboratory setup" and that "in the real world, teachers would receive much richer information."
- Why unresolved: The current study restricted teachers to a simple outer loop (select task/stop) with binary success data, whereas real teaching involves multiple skills, student dialogue, and complex interventions.
- What evidence would resolve it: A field study or high-fidelity simulation where teachers have access to the full interpretable model alongside rich contextual data (e.g., student questions, time spent) to see if interpretability offers a unique advantage.

## Limitations

- Small sample size (N=12 teachers) limits generalizability of human study results
- Limited task complexity (4 tasks, 2 skills) may not reflect real-world teaching scenarios
- Study doesn't fully address whether teachers' perceived usability of interpretable models translates to actual decision quality improvements

## Confidence

- High confidence: Interpretable models (BKT, PFA) outperform DKT in automated simulation task selection (6 vs 14 steps to mastery)
- Medium confidence: Teachers rate interpretable models higher in usability and trustworthiness
- Low confidence: Interpretable models meaningfully improve actual teacher decision quality given teachers' compensatory strategies

## Next Checks

1. Replicate the simulation with varied Elo parameters (κ_1, κ_0 learning rates) to test robustness of interpretable model advantages across different learning dynamics
2. Conduct a larger human study (N≥30) with increased task/skill complexity (6 skills, 10 tasks) to identify when teacher compensatory strategies fail without interpretable feedback
3. Perform an ablation study where DKT provides synthetic "ability graphs" derived from predicted success probabilities to isolate whether visual presentation drives usability ratings rather than true interpretability