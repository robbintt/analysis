---
ver: rpa2
title: Linearly Decoding Refused Knowledge in Aligned Language Models
arxiv_id: '2507.00239'
source_url: https://arxiv.org/abs/2507.00239
tags:
- spearman
- linear
- prompt
- probe
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the extent to which harmful information\
  \ initially refused by aligned language models remains accessible through hidden\
  \ states, even after instruction-tuning. Using linear probes trained on innocuous\
  \ hidden states, the authors show that information revealed by jailbreak prompts\
  \ can often be decoded with high accuracy\u2014e.g., predicting jailbroken IQ estimates\
  \ for countries with Pearson correlations exceeding 0.8."
---

# Linearly Decoding Refused Knowledge in Aligned Language Models

## Quick Facts
- arXiv ID: 2507.00239
- Source URL: https://arxiv.org/abs/2507.00239
- Authors: Aryan Shrivastava; Ari Holtzman
- Reference count: 40
- Key outcome: Harmful information refused by aligned language models remains linearly accessible through hidden states, with jailbroken content decodable at high accuracy (>0.8 Pearson correlation for IQ estimates) using probes trained on innocuous prompts

## Executive Summary
This paper demonstrates that harmful information initially refused by aligned language models persists in hidden states and can be linearly decoded even after instruction-tuning. Using linear probes trained on safe hidden states, the authors show that jailbreak prompts reveal decodable harmful content with strong correlations to ground truth values. Surprisingly, probes trained on base models sometimes transfer to aligned versions, suggesting persistent representations. The decoded attributes also correlate with implicit pairwise preferences, indicating these representations influence downstream decision-making. The results suggest instruction-tuning suppresses rather than eliminates harmful information, leaving it both accessible and behaviorally influential.

## Method Summary
The authors train linear probes on model hidden states to decode harmful attributes from jailbreak prompts. They use innocuous prompts to train probes that then predict values from jailbroken versions of the same prompts. The approach tests both within-model transferability (using the same model architecture) and cross-model transferability (from base to aligned models). They evaluate decoding accuracy using Pearson correlation against ground truth values for various attributes like IQ estimates. Pairwise preference experiments assess whether decoded information influences decision-making by examining correlations between decoded values and model preferences.

## Key Results
- Linear probes trained on safe hidden states achieve Pearson correlations exceeding 0.8 for decoding jailbroken IQ estimates from aligned models
- Probes trained on base models sometimes successfully transfer to decode information from aligned versions of the same architecture
- Decoded attribute values correlate with implicit pairwise preferences, suggesting behavioral influence beyond mere representation

## Why This Works (Mechanism)
The mechanism appears to rely on the persistence of knowledge representations in model hidden states despite alignment training. When models refuse harmful requests, they suppress direct expression rather than eliminating the underlying representations. Linear probes can exploit the remaining signal in these hidden states because the core knowledge about harmful attributes remains encoded, even if the model's output layer has learned to refuse or redirect such queries. The transferability between base and aligned models suggests that instruction-tuning modifies surface-level behavior while preserving deeper representational structures that contain the refused knowledge.

## Foundational Learning
- Linear probe training: Needed to extract information from model activations without fine-tuning entire model; quick check: verify probe accuracy on held-out safe prompts
- Jailbreak prompt engineering: Required to bypass refusal mechanisms and access hidden representations; quick check: test multiple jailbreak strategies for consistency
- Hidden state analysis: Essential for understanding what information persists post-alignment; quick check: examine activation patterns across model layers
- Pearson correlation metrics: Used to quantify decoding accuracy against ground truth; quick check: validate correlation significance with permutation tests
- Transfer learning between model versions: Critical for assessing representation persistence; quick check: test probe performance across different alignment techniques

## Architecture Onboarding

**Component Map:** Input prompts -> Model layers (Base/Instruct) -> Hidden states -> Linear probe -> Decoded attributes -> Preference prediction

**Critical Path:** Prompt encoding → Hidden state extraction → Linear probe inference → Attribute decoding → Preference correlation

**Design Tradeoffs:** The choice of linear probes offers computational efficiency and interpretability but may miss nonlinear representations; using base model probes for aligned models trades specificity for generalizability but assumes representational consistency

**Failure Signatures:** Low decoding accuracy indicates either successful alignment erasure of knowledge or probe overfitting to safe prompts; inconsistent transferability suggests alignment significantly alters representations; weak preference correlations imply decoded information lacks behavioral influence

**First Experiments:** 1) Test probe transferability across multiple jailbreak strategies beyond the "pretend you're a human" prompt, 2) Conduct ablation studies removing specific model layers to identify critical components for persistent representations, 3) Design controlled decision-making experiments comparing refused vs decoded conditions to directly test behavioral influence

## Open Questions the Paper Calls Out
The paper identifies several key uncertainties: whether linear probes exploit spurious correlations rather than genuine knowledge representations, whether base model probe transferability generalizes beyond tested jailbreak prompts, and whether pairwise preference correlations reflect true decision-making influence or coincidental patterns. The study's reliance on single-shot jailbreak prompts and limited attribute types also constrains generalizability of findings.

## Limitations
- Results depend on specific jailbreak prompts and may not generalize to other bypass strategies
- Linear probe approach may miss nonlinear representations of refused knowledge
- Preference correlation evidence for behavioral influence is indirect and requires direct experimental validation
- Limited attribute types tested constrain generalizability of persistence claims

## Confidence
High confidence: Linear probes can extract IQ estimates from jailbreak prompts with correlations exceeding 0.8, reproducible across multiple architectures and attribute types
Medium confidence: Probe transferability between base and aligned models, dependent on assumption that base activations proxy pre-alignment representations
Low confidence: Behavioral influence claims based on preference correlations require further validation through controlled experiments

## Next Checks
1. Test probe transferability across multiple jailbreak strategies beyond the "pretend you're a human" prompt to rule out prompt-specific artifacts
2. Conduct ablation studies systematically removing model components to identify specific layers or attention heads responsible for persistent representations
3. Design controlled experiments where the same model makes decisions under both refused and decoded conditions to directly test whether decoded information influences actual behavior rather than just correlating with preferences