---
ver: rpa2
title: What Scales in Cross-Entropy Scaling Law?
arxiv_id: '2510.04067'
source_url: https://arxiv.org/abs/2510.04067
tags:
- cross-entropy
- scaling
- arxiv
- urlhttps
- error-entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the cross-entropy scaling law in language
  models, questioning whether cross-entropy loss itself truly scales or if only a
  hidden component does. The authors decompose cross-entropy into three terms: Error-Entropy,
  Self-Alignment, and Confidence, based on a new metric called Rank-based Error (RBE).'
---

# What Scales in Cross-Entropy Scaling Law?

## Quick Facts
- arXiv ID: 2510.04067
- Source URL: https://arxiv.org/abs/2510.04067
- Authors: Junxi Yan; Zixi Wei; Jingtao Zhan; Qingyao Ai; Yiqun Liu
- Reference count: 25
- Primary result: Error-Entropy (not cross-entropy) follows robust power-law scaling with model size

## Executive Summary
This paper challenges the conventional understanding of cross-entropy scaling laws in language models by decomposing cross-entropy into three distinct components: Error-Entropy, Self-Alignment, and Confidence. The authors demonstrate that only Error-Entropy (which captures ranking accuracy) follows a true power-law scaling with model size, while the other two components remain largely invariant. This finding explains why cross-entropy scaling appears to break down at large model sizes—as Error-Entropy diminishes, the invariant components dominate the total loss, creating the observed curve bend.

## Method Summary
The method introduces Rank-based Error (RBE) as a new metric that measures the rank of the ground-truth token in the predicted probability distribution. By grouping cross-entropy loss terms by RBE values and analyzing the resulting distributions, the authors mathematically decompose cross-entropy into Error-Entropy (uncertainty in RBE distribution), Self-Alignment (KL divergence between score distributions), and Confidence (logarithmic normalization constant). The decomposition is validated across 32 models ranging from 14M to 70B parameters on three datasets (Wikipedia, C4, GitHub), showing that Error-Entropy consistently achieves R² values close to 0.9 in log-log scaling plots while cross-entropy itself shows poorer scaling behavior.

## Key Results
- Only Error-Entropy follows robust power-law scaling (R² > 0.9) across all model sizes and datasets
- Error-Entropy dominates total cross-entropy loss (80-90%) in small models but diminishes proportionally as models grow
- Self-Alignment and Confidence components remain largely invariant with respect to model size
- The scaling exponent for Error-Entropy differs significantly from cross-entropy, confirming they are distinct phenomena

## Why This Works (Mechanism)

### Mechanism 1: Rank-Based Loss Decomposition
If cross-entropy is grouped by the rank of the ground-truth token (Rank-based Error or RBE), it mathematically separates into distinct components representing accuracy, calibration, and confidence. The method aggregates loss terms by RBE values, transforming the cross-entropy sum into Error-Entropy, Self-Alignment, and Confidence terms. This isolates the model's ability to rank correctly from its ability to assign precise probabilities.

### Mechanism 2: Scaling Breakdown via Component Saturation
The widely observed breakdown of cross-entropy scaling laws at large model sizes occurs because only the Error-Entropy component scales, while others remain invariant; as Error-Entropy shrinks, invariant terms dominate the total loss. In small models, Error-Entropy is large (80-90% of loss), causing total loss to mimic its power-law slope. As models grow, Error-Entropy diminishes, leaving the flat "floor" of invariant terms to dominate the total loss value.

### Mechanism 3: Sequential Optimization Dynamics
The decomposition implies a natural optimization order where models prioritize ranking accuracy (Error-Entropy) before calibrating probabilities (Self-Alignment/Confidence). Because Error-Entropy is the dominant term at initialization, gradient descent reduces this largest error signal first. Only after ranks are correct does the optimization pressure shift toward aligning specific probability values and increasing score magnitudes.

## Foundational Learning

- **Concept: Shannon Entropy ($H(X)$)**
  - Why needed here: This is the mathematical definition of the "Error-Entropy" component. Understanding that $H(X) = -\sum p(x) \log p(x)$ measures the uncertainty or "spread" of the RBE distribution is required to grasp why minimizing it equates to improving ranking accuracy.
  - Quick check question: If a model predicts the correct token at rank 1 for 99% of inputs, is the Error-Entropy high or low? (Low)

- **Concept: Power Law Scaling ($y = ax^k$)**
  - Why needed here: The paper analyzes scaling laws, which assume that loss scales predictably with model size (parameters) on a log-log plot. Recognizing a linear trend on a log-log plot is the primary diagnostic tool used to validate the Error-Entropy scaling law.
  - Quick check question: On a log-log plot of Loss vs. Model Size, what does a straight line imply about the relationship between the variables? (Power-law relationship)

- **Concept: KL Divergence ($D_{KL}(P||Q)$)**
  - Why needed here: This measures the "Self-Alignment" component. It quantifies how much the model's probability score distribution ($q_e$) differs from its actual error distribution ($p_e$).
  - Quick check question: If the model assigns high probability scores to ranks where it frequently makes errors, is the KL Divergence likely to be high or low? (High)

## Architecture Onboarding

- **Component map:** Input: Tokenized batch → LLM Forward Pass → Logits → RBE Core (Softmax → Sort → Identify Rank of Ground Truth) → Aggregator (Histogram RBEs to get $p_e$; Geometric Mean of Softmax scores per rank to get $Q_e$; Normalize to get $q_e$ and $C$) → Calculator (Compute $H(p_e)$, $D_{KL}(p_e || q_e)$, and $-\log(C)$)

- **Critical path:** The **Rank Calculation** is the computational bottleneck. Standard loss functions use the logit of the correct token directly. This method requires sorting the logits (or iterating to find rank) for every single token in the batch, which adds significant overhead compared to standard Cross-Entropy.

- **Design tradeoffs:** Speed vs. Precision: Calculating exact ranks is expensive. One might approximate ranks using quantiles or approximated sorting, but the paper uses exact ranks. Metric Stability: The geometric mean (Eq. 3) is used for scores to handle the log operation, but it can be unstable if scores are effectively zero (requires epsilon handling).

- **Failure signatures:** Non-decomposable Residual: If the sum of the three components does not equal the standard Cross-Entropy loss, there is likely a bug in the normalization constant $C$ or the log-derivation. Scaling in Self-Alignment: If Self-Alignment decreases significantly with model size, the hypothesis that "only Error-Entropy scales" fails for that architecture.

- **First 3 experiments:**
  1. Sanity Check: Reproduce Figure 3 (Training Dynamics) on a small model (e.g., Pythia-160m) to verify that the sum of decomposed parts equals the reported Cross-Entropy and that curves separate as described.
  2. Scaling Validation: Calculate Error-Entropy for a series of models (e.g., 160m, 410m, 1b) and plot on a log-log scale. Confirm $R^2$ is higher for Error-Entropy than Total Cross-Entropy.
  3. Rank Distribution Analysis: Visualize the $p_e$ distribution (Figure 4) for a trained vs. untrained model to confirm that training shifts mass to low ranks (minimizing Error-Entropy).

## Open Questions the Paper Calls Out

### Open Question 1
Can Error-Entropy be effectively utilized as a standalone differentiable training objective to replace cross-entropy? The paper states that retaining only the Error-Entropy term as training loss is a promising direction but requires differentiable approximations since ranks are non-differentiable. Resolving this would require successfully training a language model to convergence using a differentiable approximation of Error-Entropy with performance comparable to cross-entropy training.

### Open Question 2
Does the Error-Entropy scaling law generalize to modalities other than text, such as vision or multimodal learning? The paper focuses exclusively on language models and text datasets while acknowledging related work on scaling laws in images and video. Resolving this would require empirical validation showing that Error-Entropy follows a power law while Self-Alignment and Confidence remain invariant when evaluated on vision transformers or multimodal models across varying scales.

### Open Question 3
What are the theoretical bounds of the invariant terms (Self-Alignment and Confidence), and are they strictly constant or dependent on data entropy? The paper establishes that Self-Alignment and Confidence are "largely invariant" with respect to model size but does not derive theoretical constants or lower bounds for these terms. Resolving this would require theoretical analysis or empirical study demonstrating that these values are determined solely by dataset statistics rather than model capacity.

## Limitations
- Rank Calculation Scalability: The RBE method requires exact ranking of logits for every token, creating an O(V log V) per token computational bottleneck that could limit practical application.
- Tokenizer Compatibility: Unknown handling of tokenization differences across model families could systematically bias RBE distributions and explain some observed scaling differences.
- Scaling Law Attribution: While Error-Entropy scaling is convincingly demonstrated, the paper doesn't definitively prove this explains cross-entropy scaling law breakdowns, as alternative explanations exist.

## Confidence
- **High Confidence**: The mathematical decomposition of cross-entropy into three components is well-justified through the RBE framework, with R² values consistently above 0.9 for Error-Entropy across multiple datasets.
- **Medium Confidence**: The claim that Error-Entropy scaling explains cross-entropy law breakdowns is plausible but not definitively proven, as the mechanism is theoretically sound but alternative explanations aren't ruled out.
- **Medium Confidence**: The sequential optimization dynamics interpretation is supported by training curves but lacks direct mechanistic evidence from gradient analysis or ablation studies.

## Next Checks
1. **Approximation Benchmarking**: Implement and compare exact vs. approximate RBE computation (e.g., using quantiles or sampled sorting) to determine if scaling observations hold under computational approximations, addressing scalability concerns.
2. **Architecture-Agnostic Verification**: Test the decomposition on non-transformer architectures (RNNs, CNNs) to verify whether the Error-Entropy scaling phenomenon is truly fundamental to language modeling or specific to current dominant architectures.
3. **Phase Transition Analysis**: Conduct controlled experiments varying data regime, batch size, and learning rate to determine whether observed scaling law breakdown correlates with known architectural phase transitions or optimization pathologies.