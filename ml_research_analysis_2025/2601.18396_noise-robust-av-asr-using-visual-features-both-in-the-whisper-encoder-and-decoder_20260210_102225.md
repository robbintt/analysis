---
ver: rpa2
title: Noise-Robust AV-ASR Using Visual Features Both in the Whisper Encoder and Decoder
arxiv_id: '2601.18396'
source_url: https://arxiv.org/abs/2601.18396
tags:
- whisper
- fusion
- noise
- visual
- dual-use
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses noise robustness in audiovisual automatic speech
  recognition (AV-ASR) by proposing a dual-use fusion method that incorporates visual
  features both in the Whisper encoder and decoder. The method uses visual features
  from AV-HuBERT to model audiovisual interactions in the encoder and weigh modalities
  in the decoder.
---

# Noise-Robust AV-ASR Using Visual Features Both in the Whisper Encoder and Decoder

## Quick Facts
- arXiv ID: 2601.18396
- Source URL: https://arxiv.org/abs/2601.18396
- Reference count: 0
- This work achieves 4.08% average WER on LRS3 benchmark using dual-use visual fusion in Whisper, establishing new state-of-the-art for noise-robust AV-ASR.

## Executive Summary
This paper addresses noise robustness in audiovisual automatic speech recognition by proposing a dual-use fusion method that incorporates visual features both in the Whisper encoder and decoder. The method uses visual features from AV-HuBERT to model audiovisual interactions in the encoder and weigh modalities in the decoder. The dual-use approach significantly improves noise robustness compared to existing methods, achieving 35% relative improvement over middle fusion on Whisper small and 57% relative improvement on Whisper medium in babble noise with 0dB SNR. Fine-tuned on 1929 hours of audiovisual data, the dual-use method using Whisper medium achieves 4.08% (MUSAN babble noise) and 4.43% (NoiseX babble noise) average WER across various SNRs, establishing a new state-of-the-art on the LRS3 AV-ASR benchmark.

## Method Summary
The dual-use fusion method incorporates visual features from AV-HuBERT into both the Whisper encoder and decoder. In the encoder, visual features are upsampled, projected to match acoustic feature dimensions, multiplied by a learnable scalar α (initialized to zero), and added to acoustic features before encoder processing. In the decoder, Flamingo blocks are inserted before each original decoder block to perform gated cross-attention over visual features. This enables context-aware modality balancing during token prediction. The method is trained in two stages: first fine-tuning Whisper on LRS3 audio, then fine-tuning the full AV-ASR model on the combined LRS3, LRS2, and Voxceleb2 English dataset (1929 hours total) with babble noise augmentation.

## Key Results
- Dual-use fusion achieves 4.41% WER vs 6.83% for middle fusion (35% relative improvement) on Whisper small in 0dB babble noise
- Dual-use fusion achieves 4.07% WER vs 9.53% for middle fusion (57% relative improvement) on Whisper medium in 0dB babble noise
- Fine-tuned dual-use Whisper medium achieves 4.08% (MUSAN) and 4.43% (NoiseX) average WER across SNRs, establishing SOTA on LRS3 benchmark

## Why This Works (Mechanism)

### Mechanism 1: Encoder-Side Audiovisual Interaction Learning
Adding visual features directly to the Whisper encoder enables joint audiovisual representation learning through transformer attention layers. Visual features from AV-HuBERT are upsampled, projected to match acoustic feature dimensions, multiplied by a learnable scalar α, then added to acoustic features before encoder processing. The encoder's self-attention layers learn cross-modal correlations during fine-tuning. Core assumption: The pre-trained Whisper encoder can adapt its attention patterns to incorporate visual context without catastrophic forgetting of acoustic representations.

### Mechanism 2: Decoder-Side Modality Weighting via Flamingo Blocks
Integrating visual features into the decoder through gated cross-attention enables context-aware modality balancing during token prediction. Flamingo blocks inserted before each decoder block perform cross-attention over visual features with zero-initialized gating. This allows the decoder to selectively attend to visual information when audio is unreliable (e.g., high noise). Core assumption: The decoder can learn to upweight visual modality in noisy conditions based on learned confidence estimates rather than explicit SNR signals.

### Mechanism 3: Zero-Initialized Gradual Visual Injection
Initializing the visual feature scalar α to zero enables stable fine-tuning by starting from the pre-trained audio-only equilibrium. α = 0 at initialization means the model begins as pure Whisper. During fine-tuning, α gradually increases, smoothly incorporating visual information without disrupting learned acoustic representations. Core assumption: Gradual visual feature introduction prevents destructive interference with pre-trained audio representations.

## Foundational Learning

- **Transformer Encoder-Decoder Architecture (specifically Whisper)**: Understanding where visual features can be injected requires knowing the flow from acoustic frontend → encoder latent representations → decoder cross-attention → token predictions. Quick check: Can you explain why the decoder attends to encoder outputs rather than raw features?

- **Audio-Visual Speech Recognition (AV-ASR) and the Cocktail Party Problem**: The core motivation is noise robustness—visual lip information is immune to acoustic noise and provides complementary phonetic information (visemes distinguish fewer sounds than phonemes). Quick check: Why can't visual features alone achieve low WER in clean conditions?

- **Cross-Attention and Gated Fusion (Flamingo blocks)**: The decoder-side fusion uses multi-head cross-attention over visual features with gated residual connections. Understanding gated tanh mechanisms is essential for debugging modality weighting. Quick check: What is the purpose of zero-initializing the gate in a gated cross-attention layer?

## Architecture Onboarding

- **Component map**:
  - Audio pathway: Raw audio → 80-dim log-filterbank (100 fps) → Whisper acoustic frontend → Whisper encoder (N_ENC blocks) → audiovisual latent hAV
  - Visual pathway: Grayscale video (25 fps, 88×88 ROI) → AV-HuBERT large (24 blocks) → select block output → upsample (4×) → FC(d) → scalar α → visual features vV
  - Encoder fusion: hAV = Encoder(GA(audio) + vV) via addition
  - Decoder pathway: Previous tokens → embedding → [Flamingo block → Decoder block] × 2N_DEC → FC + softmax → P(token)
  - Decoder fusion: Flamingo blocks attend over visual features hV from AV-HuBERT

- **Critical path**:
  1. Ensure AV-HuBERT is frozen or properly loaded with correct checkpoint
  2. Verify visual feature dimensionality matches Whisper encoder hidden dimension (d) after projection
  3. Confirm α is correctly initialized to zero and is a single learnable scalar (not per-dimension)
  4. Check Flamingo block insertion points: before each original decoder block

- **Design tradeoffs**:
  - Addition vs. concatenation in encoder fusion: Addition with zero-init α enables smooth fine-tuning; concatenation shows catastrophic failure (Table 2: 12.15% vs 5.28% WER)
  - AV-HuBERT block selection: Later blocks (24th) provide more refined representations; earlier blocks (0th, 4th) show worse noise robustness. Trade-off: earlier blocks may be more general, later blocks more task-specialized
  - Model size scaling: Early fusion alone degrades in larger models (Whisper small/medium) due to gradient flow issues; dual-use maintains improvements across sizes

- **Failure signatures**:
  - WER in clean > WER in 0dB noise: Visual features are corrupting clean speech recognition; check if α is too large or Flamingo gates are not adapting
  - Training loss diverges after initial decrease: Likely learning rate too high for visual components; reduce LR for α and Flamingo parameters specifically
  - α remains ≈ 0 after training: Visual features not being used; check gradient flow to α, verify loss is computed correctly with visual input
  - Performance matches middle fusion exactly: Dual-use may not be activating; verify encoder fusion code path is actually being used

- **First 3 experiments**:
  1. Reproduce Table 2 encoder fusion comparison: Train dual-use with addition vs. concatenation on Whisper tiny with 20k updates on LRS3 (433h). Verify addition achieves ~5-6% WER in 0dB babble, concatenation fails (>10%)
  2. Ablate visual feature source block: Extract visual features from AV-HuBERT blocks 0, 8, 16, 24 and compare WER in 0dB babble. Expect monotonic improvement with later blocks
  3. Compare dual-use vs. middle fusion on Whisper small: Train both methods for 120k updates on LRS3+LRS2+VoxCeleb2-EN (1929h). Verify dual-use achieves ~35% relative improvement over middle fusion in 0dB babble (target: ~4.4% vs ~6.8% WER)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does early fusion degrade severely on larger Whisper models (small/medium) while dual-use fusion scales successfully, and can this scaling limitation be predicted or mitigated for other model architectures?
- Basis in paper: The paper states "early fusion becomes harder to train when applied to larger models, because the increased depth prevents effective gradient flow to the early fusion layers during backpropagation." Table 1 shows early fusion WER degrades from 8.45% (tiny) to 36.99% (small) on 0dB babble test, while dual-use improves consistently.
- Why unresolved: The gradient flow explanation is hypothesized but not empirically validated through gradient analysis or layer-wise investigations. The dual-use method's success at scale remains unexplained.
- What evidence would resolve it: Gradient magnitude analysis across layers during training, or ablation studies varying fusion position depth systematically.

### Open Question 2
- Question: What mechanisms cause additive visual fusion to dramatically outperform concatenation-based fusion in the encoder, and does this finding generalize to other modalities or fusion points?
- Basis in paper: Table 2 shows additive fusion achieves 5.28% WER vs. 12.15% for concatenation on 0dB babble dev—a 57% relative gap. The paper attributes this to "zero-initialized visual features" enabling "smooth fine-tuning start" but provides no direct evidence.
- Why unresolved: The scalar α zero-initialization explanation is plausible but untested against alternative explanations (e.g., feature space mismatch, optimization dynamics).
- What evidence would resolve it: Ablations comparing zero-init vs. random-init α, layer-wise feature distribution analysis, or comparing other smooth injection methods (e.g., learned gating).

### Open Question 3
- Question: Does the dual-use method's noise robustness advantage over middle fusion extend to non-babble noise types (e.g., environmental, synthetic, or adversarial noise)?
- Basis in paper: The paper evaluates only MUSAN and NoiseX babble noise. The introduction mentions "real-world such as automobiles and smart glasses" deployment, which would involve diverse noise profiles.
- Why unresolved: Babble noise has specific temporal and spectral characteristics. The audiovisual interaction modeling benefit may not transfer equally to stationary noise, impulse noise, or codec artifacts.
- What evidence would resolve it: Evaluation on standard noise corpora (e.g., CHiME backgrounds, DEMAND) or adversarial noise attacks.

### Open Question 4
- Question: Would combining visual features from multiple AV-HuBERT blocks (rather than a single block) provide complementary information for noise robustness?
- Basis in paper: Table 2 shows monotonic improvement from block 0 (10.48% WER) to block 24 (5.28%), suggesting different blocks capture different relevant features. The paper selects only the best single block.
- Why unresolved: Later blocks may capture high-level linguistic information while earlier blocks retain low-level visual detail; both could aid different noise conditions.
- What evidence would resolve it: Multi-block fusion experiments (e.g., weighted averaging, attention-based selection) evaluated across SNR levels.

## Limitations
- The mechanism of gradual visual injection via zero-initialized α is plausible but lacks direct analysis of α's training trajectory
- All reported improvements focus on babble noise; performance in other noise types (e.g., music, ambient) and SNR ranges is less characterized
- Critical architectural details like Flamingo block parameterization are unspecified, limiting reproducibility

## Confidence
- **High confidence**: The dual-use method achieves stated WER improvements and establishes SOTA on LRS3. The core architectural innovations are clearly specified and validated.
- **Medium confidence**: The mechanisms of gradual visual injection and adaptive modality weighting are logically sound but lack detailed analysis of learned parameters.
- **Low confidence**: Claims about why early fusion fails in larger models (gradient flow issues) and why dual-use succeeds are not directly validated.

## Next Checks
1. Analyze visual feature incorporation: Train the dual-use model and plot α's value throughout training, along with Flamingo gate activations under different noise conditions. Verify α increases from zero and gates adapt to noise levels.
2. Ablate fusion depth: Compare dual-use (encoder+decoder fusion) against pure early fusion and pure late fusion on Whisper medium. Show that dual-use outperforms both, establishing it captures benefits of both strategies without their drawbacks.
3. Test noise generalization: Evaluate the dual-use model on LRS3 test with non-babble noises (e.g., NoiseX music, ambient) at various SNRs. Confirm noise robustness generalizes beyond the training condition (MUSAN babble, 0dB).