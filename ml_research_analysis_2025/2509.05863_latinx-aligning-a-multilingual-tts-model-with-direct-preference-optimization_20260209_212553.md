---
ver: rpa2
title: 'LatinX: Aligning a Multilingual TTS Model with Direct Preference Optimization'
arxiv_id: '2509.05863'
source_url: https://arxiv.org/abs/2509.05863
tags:
- latinx
- audio
- speaker
- similarity
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents LatinX, a multilingual text-to-speech (TTS)
  system designed for speech-to-speech translation while preserving speaker identity
  across languages. The core method uses a three-stage training approach: pre-training
  for text-to-audio mapping, supervised fine-tuning for zero-shot voice cloning, and
  Direct Preference Optimization (DPO) alignment using automated preference pairs
  based on Word Error Rate (WER) and speaker similarity metrics.'
---

# LatinX: Aligning a Multilingual TTS Model with Direct Preference Optimization

## Quick Facts
- arXiv ID: 2509.05863
- Source URL: https://arxiv.org/abs/2509.05863
- Reference count: 0
- Key outcome: LatinX reduces WER and improves objective speaker similarity with DPO, but shows a notable gap between objective and subjective similarity measures compared to XTTSv2.

## Executive Summary
LatinX is a multilingual text-to-speech system designed for speech-to-speech translation that preserves speaker identity across languages. The model uses a three-stage training approach: pre-training for text-to-audio mapping, supervised fine-tuning for zero-shot voice cloning, and Direct Preference Optimization (DPO) alignment using automated preference pairs based on WER and speaker similarity metrics. Trained on English and Romance languages with emphasis on Portuguese, LatinX with DPO consistently reduces WER and improves objective speaker similarity over the fine-tuned baseline. Human evaluations indicate stronger perceived speaker similarity than XTTSv2, revealing a significant gap between objective and subjective similarity measures that points to future research opportunities.

## Method Summary
LatinX employs a 12-layer decoder-only Transformer (210M parameters) trained in three sequential stages. Stage 1 pre-trains the model for 400K steps on text-to-audio mapping using cross-entropy loss. Stage 2 performs supervised fine-tuning for 30K steps on voice cloning tasks with speaker embedding similarity thresholds of 0.6. Stage 3 applies DPO for 4K steps using Pareto-dominance preference pairs filtered by WER<20% and speaker similarity>0.5. The model uses a VQ-VAE-based Spectrogram Patch Codec with 4096 codebook entries and operates on phoneme sequences (≤256 tokens) concatenated with reference audio tokens (≤3968) for autoregressive generation.

## Key Results
- LatinX with DPO consistently reduces WER and improves objective speaker similarity over the fine-tuned baseline
- Human evaluations show LatinX is strongly preferred over XTTSv2 for speaker similarity despite objective metrics favoring XTTSv2
- The neural codec introduces artifacts that the model learns to replicate, setting a ceiling on perceptual quality
- Real-Time Factor of approximately 4.85 highlights need for non-autoregressive architectures for real-time applications

## Why This Works (Mechanism)

### Mechanism 1: Three-Stage Curriculum Decomposition
Sequential stage progression enables stable acquisition of distinct capabilities that would conflict if trained jointly. Stage 1 learns phoneme-to-audio-code mapping without speaker conditioning. Stage 2 introduces speaker identity preservation via prompt-target triplets with 0.6 speaker embedding similarity threshold. Stage 3 refines the joint objective of intelligibility and speaker fidelity using preference pairs.

### Mechanism 2: Pareto-Dominance Preference Filtering
Strict Pareto filtering produces unambiguous preference signals that avoid reward hacking on single metrics. Candidates are labeled "preferred" only if superior on BOTH WER AND speaker similarity simultaneously. Pairs with WER > 20% or similarity < 0.5 are discarded entirely.

### Mechanism 3: Codec-Aligned Similarity Metric (Sim-E)
Measuring similarity against codec-reconstructed reference provides a fairer evaluation aligned with training targets. The neural codec is lossy; training targets are codec outputs, not raw audio. Sim-E compares generated audio to the codec-reconstructed version of the source, controlling for codec-induced degradation.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: DPO replaces reinforcement learning from human feedback with a simpler supervised loss on preference pairs, avoiding training a separate reward model
  - Quick check question: Given a frozen reference policy π_ref and a preference pair (y+, y−), what quantity does DPO maximize? (Answer: log σ(β(log π_θ(y+|x) - log π_θ(y−|x) - log π_ref(y+|x) + log π_ref(y−|x))))

- **Concept: Vector-Quantized Variational Autoencoder (VQ-VAE) for Audio**
  - Why needed here: The neural codec uses VQ-VAE to discretize continuous mel-spectrograms into tokens that the autoregressive Transformer can predict
  - Quick check question: What is the commitment loss in VQ-VAE, and why is it necessary? (Answer: It penalizes the distance between encoder outputs and codebook entries, forcing the codebook to represent the latent space.)

- **Concept: Zero-Shot Voice Cloning**
  - Why needed here: The SFT stage conditions generation on a short audio prompt (3 seconds / 3968 tokens), requiring the model to infer speaker identity without speaker-specific training
  - Quick check question: Why enforce a 0.6 speaker embedding similarity threshold between prompt and target utterances during SFT? (Answer: Ensures acoustic consistency within pseudo-speaker groupings derived from unlabeled data.)

## Architecture Onboarding

- **Component map**: LatPhon (G2P) -> Spectrogram Patch Codec -> LatinX Transformer -> HiFi-GAN Vocoder
- **Critical path**: 
  1. Input text → LatPhon → phoneme sequence (≤256 tokens)
  2. Reference audio → mel-spectrogram → codec encoder → discrete tokens (≤3968 tokens)
  3. Concatenate [phonemes; prompt tokens] → LatinX autoregressive generation → output tokens
  4. Output tokens → codec decoder → mel-spectrogram → HiFi-GAN → waveform
- **Design tradeoffs**: 
  - RTF ~4.85 (offline only): Autoregressive decoding at 130 tokens/sec vs. 630 tokens/sec audio rate
  - Portuguese-heavy dataset (40%): Strong pt performance, potential bias in cross-lingual transfer
  - Pareto-only preference pairs: Conservative signal quality, reduced dataset size
- **Failure signatures**:
  - High WER with good similarity: DPO may have overfit to similarity at intelligibility's expense
  - Low Sim-O but high Sim-E: Codec bottleneck is limiting; consider higher-codebook-capacity codec
  - Repetition loops: Repetition-aware sampling params (W=240, M=10) may need tuning for specific languages
  - Subjective-objective gap: Model optimizes embedding similarity but not perceptual cues
- **First 3 experiments**:
  1. Ablate DPO: Compare LatinX (Fine-tuned) vs. LatinX (DPO) on held-out speakers
  2. Preference signal balance test: Create DPO datasets weighted toward WER-only, similarity-only, and Pareto-balanced
  3. Codec substitution: Swap Spectrogram Patch Codec for a higher-fidelity alternative (e.g., Encodec or DAC)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can objective speaker similarity metrics be improved to better correlate with human perception of voice identity across languages?
- Basis in paper: The authors state "We identified a significant divergence between objective and subjective similarity metrics, where LatinX was strongly preferred by human listeners, highlighting a key area for future research in developing metrics that better correlate with human perception."
- Why unresolved: TitaNet embedding cosine similarity (Sim-O) favored XTTSv2, while human SMOS ratings strongly preferred LatinX, suggesting current metrics miss perceptually relevant cues.
- What evidence would resolve it: New metrics that correlate significantly better with human SMOS judgments across diverse cross-lingual conditions.

### Open Question 2
- Question: How can DPO preference signals be balanced to jointly optimize intelligibility, speaker similarity, and naturalness without trade-offs?
- Basis in paper: The authors "discuss balanced preference signals" as future work, noting that "DPO process successfully optimized for its targets... but this sometimes came at the cost of the exceptional subjective similarity of the fine-tuned version, or the high naturalness of the baseline."
- Why unresolved: The strict Pareto dominance criterion for preference labeling ensured unambiguous signals but may have created implicit trade-offs between optimization targets.
- What evidence would resolve it: A multi-objective DPO formulation that improves all three metrics simultaneously on held-out evaluations.

### Open Question 3
- Question: Can non-autoregressive architectures achieve comparable cross-lingual voice cloning quality while enabling real-time inference?
- Basis in paper: The authors explicitly state "the high latency of the autoregressive model, confirmed by our measured Real-Time Factor of approximately 4.85 on modern hardware, underscores the need to explore non-autoregressive architectures to enable real-time applications."
- Why unresolved: The current 12-layer decoder-only Transformer operates at ~130 tokens/second while the codec requires ~630 tokens/second, making it unsuitable for streaming without architectural changes.
- What evidence would resolve it: A non-autoregressive variant achieving RTF < 1.0 while maintaining WER and SMOS within 5% of the autoregressive baseline.

## Limitations

- The disconnect between objective and subjective similarity metrics suggests current evaluation methods don't capture perceptual speaker identity
- High Real-Time Factor (~4.85) makes the model unsuitable for real-time or streaming applications
- Portuguese-heavy dataset (40% of training data) introduces potential bias in cross-lingual transfer performance
- Strict Pareto filtering may yield insufficient preference pairs if candidate generation lacks diversity

## Confidence

- **High Confidence**: The three-stage training methodology and its implementation details are well-specified and reproducible
- **Medium Confidence**: The claim that LatinX with DPO improves WER and objective speaker similarity over the fine-tuned baseline is supported by the reported metrics
- **Low Confidence**: The claim about perceived speaker similarity superiority over XTTSv2 is weakly supported due to the objective-subjective gap

## Next Checks

1. **Preference Signal Balance Analysis**: Conduct an ablation study varying the Pareto filtering criteria—create separate DPO datasets emphasizing WER-only improvement, similarity-only improvement, and balanced Pareto pairs. Train LatinX models on each and measure the tradeoff curve between intelligibility and speaker similarity to determine if the current Pareto-only approach is overly conservative.

2. **Codec Quality Isolation Test**: Replace the Spectrogram Patch Codec with a higher-fidelity alternative (e.g., Encodec or a larger codebook VQ-VAE) while keeping all other components constant. Compare Sim-O vs Sim-E scores across both codec configurations to quantify the codec bottleneck and establish whether quality improvements are codec-limited.

3. **Cross-Lingual Transfer Evaluation**: Re-train LatinX with balanced language sampling (equal representation across all six languages) and evaluate per-language WER and Sim-E scores. Specifically analyze Romanian performance to determine if the current Portuguese-heavy training introduces systematic bias, and measure whether balanced sampling improves cross-lingual generalization.