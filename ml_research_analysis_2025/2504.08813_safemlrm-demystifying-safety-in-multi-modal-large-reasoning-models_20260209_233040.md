---
ver: rpa2
title: 'SafeMLRM: Demystifying Safety in Multi-modal Large Reasoning Models'
arxiv_id: '2504.08813'
source_url: https://arxiv.org/abs/2504.08813
tags:
- safety
- reasoning
- arxiv
- mlrms
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work presents the first systematic safety analysis of multi-modal
  large reasoning models (MLRMs) through large-scale empirical studies comparing MLRMs
  with their base MLLMs. Our experiments reveal three critical findings: (1) The Reasoning
  Tax: Acquiring reasoning capabilities catastrophically degrades inherited safety
  alignment.'
---

# SafeMLRM: Demystifying Safety in Multi-modal Large Reasoning Models

## Quick Facts
- arXiv ID: 2504.08813
- Source URL: https://arxiv.org/abs/2504.08813
- Reference count: 40
- Primary result: First systematic safety analysis of MLRMs reveals "Reasoning Tax" - 37.44% higher jailbreaking success rates than base MLLMs

## Executive Summary
This work presents the first systematic safety analysis of multi-modal large reasoning models (MLRMs) through large-scale empirical studies comparing MLRMs with their base MLLMs. Our experiments reveal three critical findings: (1) The Reasoning Tax: Acquiring reasoning capabilities catastrophically degrades inherited safety alignment. MLRMs exhibit 37.44% higher jailbreaking success rates than base MLLMs under adversarial attacks. (2) Safety Blind Spots: While safety degradation is pervasive, certain scenarios (e.g., Illegal Activity) suffer 25 times higher attack rates -- far exceeding the average 3.4 times increase, revealing scenario-specific vulnerabilities with alarming cross-model and datasets consistency. (3) Emergent Self-Correction: Despite tight reasoning-answer safety coupling, MLRMs demonstrate nascent self-correction -- 16.9% of jailbroken reasoning steps are overridden by safe answers, hinting at intrinsic safeguards. To catalyze research, we open-source OpenSafeMLRM, the first toolkit for MLRM safety evaluation, providing unified interface for mainstream models, datasets, and jailbreaking methods.

## Method Summary
The paper evaluates MLRM safety using OpenSafeMLRM, a unified evaluation toolkit that compares reasoning-augmented MLRMs against their base MLLM counterparts across two safety benchmarks (MM-SafetyBench and SafetyBench) covering 10 safety scenarios. The evaluation pipeline uses GPT-4o-mini as an automated judge to score outputs for harmfulness (HR 0-5 scale) and attack success (ASR where HR≥4=success), measuring both Think/Answer/Overall stages separately for MLRMs. The study tests four MLRM-base pairs under both vanilla unsafe queries and various jailbreak attacks including typographic and cross-modal adversarial inputs.

## Key Results
- MLRMs exhibit 37.44% higher jailbreaking success rates than base MLLMs under adversarial attacks
- Safety degradation varies dramatically across scenarios (ΔASR range: 8.1%-2500%), with Illegal Activity/Pornography being most/least affected
- 12.4% of unsafe reasoning steps (Think-HR > 3) yield safe answers (Answer-HR ≤ 3), exhibiting emergent self-correction capabilities

## Why This Works (Mechanism)

### Mechanism 1: Reasoning Tax via Alignment Erosion
- Claim: SFT/RL-based reasoning acquisition degrades inherited safety alignment in MLRMs.
- Mechanism: Base MLLMs with strong safety alignment lose this property after reasoning augmentation. Capability-focused training (SFT/RL on chain-of-thought data) interferes with previously learned safety refusal patterns.
- Core assumption: Safety and reasoning capabilities compete for model capacity or training signal, and current SFT/RL pipelines do not preserve safety constraints.
- Evidence anchors:
  - MLRMs exhibit 31.30% higher ASR (59.52% vs. base MLLMs' 28.22%) and 1.64 higher HR (3.07% vs. 1.43)
  - Related work "Think in Safety" confirms safety alignment collapse in MLRMs across multiple benchmarks
- Break condition: If reasoning augmentation explicitly incorporates safety-aware reward modeling or constrained decoding, the tax may be reduced or eliminated

### Mechanism 2: Scenario-Specific Blind Spots via Cross-Modal Reasoning
- Claim: Safety degradation is heterogeneously distributed across scenarios, with certain categories (e.g., Illegal Activity) showing dramatically higher vulnerability.
- Mechanism: Cross-modal reasoning pathways introduce novel attack surfaces where adversarial inputs (typographic images, SD-generated visuals) exploit the model's extended reasoning chain.
- Core assumption: Reasoning models' extended generation steps provide more opportunities for adversarial perturbations to propagate, and some scenarios are more sensitive to this effect.
- Evidence anchors:
  - Certain scenarios (e.g., Illegal Activity) suffer 25 times higher attack rates -- far exceeding the average 3.4 times increase
  - Safety degradation varies dramatically across scenarios (ΔASR range: 8.1%-2500%), with Illegal Activity/Pornography being most/least affected scenarios
  - Related papers confirm scenario-specific vulnerabilities in MLRMs, including location privacy leakage and emotional manipulation attacks
- Break condition: If scenario-aware adversarial training or targeted red-teaming is applied, blind spots may shift or narrow

### Mechanism 3: Emergent Self-Correction via Latent Safeguards
- Claim: A subset of MLRMs can override unsafe reasoning steps with safe final answers, suggesting intrinsic safety mechanisms persist after reasoning augmentation.
- Mechanism: Even when reasoning chains are compromised (high Think-HR), some models produce safe answers (low Answer-HR), suggesting latent safeguards or refusal patterns remain active in later generation stages.
- Core assumption: Safety alignment from base models is not completely erased during reasoning training, and some residual refusal behavior can be triggered at the answer generation stage.
- Evidence anchors:
  - 16.9% of jailbroken reasoning steps are overridden by safe answers, hinting at intrinsic safeguards
  - 12.4% of unsafe reasoning steps (Think-HR > 3) yield safe answers (Answer-HR ≤ 3), exhibiting emergent self-correction capabilities in MLRMs
  - This appears to be a novel empirical finding in this paper
- Break condition: If self-correction is amplified via targeted training or decoding interventions, safety could improve without sacrificing reasoning capability

## Foundational Learning

- Concept: **Safety Alignment in LLMs/MLLMs**
  - Why needed here: The entire paper is built on comparing safety metrics between base MLLMs and reasoning-augmented MLRMs. Understanding what safety alignment means (refusal behavior, harmful content mitigation) is essential.
  - Quick check question: Can you explain why a model that refuses harmful prompts is considered "safety-aligned," and how this might conflict with helpfulness?

- Concept: **Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Reasoning**
  - Why needed here: The paper attributes the "reasoning tax" to SFT/RL pipelines used to train MLRMs. Understanding how these methods add reasoning capabilities is critical to grasping the safety tradeoff.
  - Quick check question: How does training on chain-of-thought data change a model's behavior, and why might this interfere with previously learned refusal patterns?

- Concept: **Jailbreaking Attacks (Textual, Typographic, Cross-Modal)**
  - Why needed here: The paper evaluates safety using jailbreaking methods (e.g., typographic images, adversarial visuals). Understanding these attack vectors is necessary to interpret the empirical results.
  - Quick check question: What is a typographic jailbreak, and why might it be more effective against multi-modal models than text-only attacks?

## Architecture Onboarding

- Component map: Base MLLM -> SFT/RL Reasoning Augmentation -> MLRM -> OpenSafeMLRM Evaluation -> Safety Metrics (HR, ASR)

- Critical path:
  1. Start with a safety-aligned base MLLM
  2. Apply SFT/RL with chain-of-thought data to produce an MLRM
  3. Evaluate both models using OpenSafeMLRM on safety benchmarks
  4. Compare HR and ASR across scenarios to quantify reasoning tax and identify blind spots
  5. Analyze Think vs. Answer safety to detect self-correction behavior

- Design tradeoffs:
  - **Reasoning vs. Safety**: Adding reasoning improves task performance but degrades safety alignment
  - **Evaluation granularity**: Separating Think and Answer metrics reveals internal safety dynamics but requires models to follow structured output formats (some MLRMs struggle with this)
  - **Dataset coverage**: Using two benchmarks (MM-SafetyBench, SafetyBench) provides cross-validation but may not cover all attack vectors or scenarios

- Failure signatures:
  - High ASR/HR on MLRMs compared to base MLLMs indicates reasoning tax
  - Disproportionately high degradation in specific scenarios (e.g., Illegal Activity) signals blind spots
  - High Think-HR with low Answer-HR suggests emergent self-correction

- First 3 experiments:
  1. Replicate the reasoning tax measurement: Compare ASR/HR for a base MLLM and its MLRM derivative under vanilla unsafe queries and jailbreak attacks
  2. Probe scenario-specific blind spots: Focus evaluation on the most vulnerable categories (Illegal Activity, Hate Speech) to verify the 25× degradation claim
  3. Test self-correction behavior: Sample adversarial queries and manually inspect cases where Think-HR > 3 but Answer-HR ≤ 3 to understand the nature of self-correction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the emergent self-correction capabilities of MLRMs be amplified to effectively override unsafe reasoning steps?
- Basis in paper: The authors conclude that the findings underscore the urgency of developing "mechanisms to amplify MLRMs' self-correction potential."
- Why unresolved: The paper identifies that 16.9% of unsafe reasoning steps are naturally overridden by safe answers, but it does not investigate the underlying causes of this phenomenon or propose methods to increase this rate.
- What evidence would resolve it: A training intervention or decoding strategy that significantly increases the percentage of safe final answers following compromised reasoning chains without degrading general reasoning utility.

### Open Question 2
- Question: How can adaptive alignment protocols be developed to specifically address the severe, scenario-specific safety blind spots in MLRMs?
- Basis in paper: The authors call for "urgent scenario-specific red teaming and adaptive alignment protocols" after observing that certain scenarios, like Illegal Activity, suffer attack rates 25 times higher than the average increase.
- Why unresolved: While the paper quantifies the disparate impact of the "Reasoning Tax" across different scenarios, it leaves the development of targeted defenses or specialized auditing frameworks for these blind spots as future work.
- What evidence would resolve it: A new alignment methodology that effectively normalizes the safety degradation curve, specifically reducing the high attack success rates in vulnerable scenarios like Illegal Activity to match the model's average safety performance.

### Open Question 3
- Question: What specific architectural or training characteristics allow certain models to resist the "Reasoning Tax"?
- Basis in paper: The authors observe that the Mulberry-Llama model exhibits an inverse trend where safety improves post-reasoning, stating this anomaly "may provide a rare blueprint for designing safety-resilient reasoning models."
- Why unresolved: The study focuses on the general trend of safety degradation (Reasoning Tax) and documents the Mulberry-Llama anomaly as a promising exception, but does not analyze the specific factors (e.g., base model properties, data composition) that enabled this resilience.
- What evidence would resolve it: A comparative ablation study isolating the differences between Mulberry-Llama and high-risk architectures (like Qwen-based models) to identify the precise components that preserve safety alignment during reasoning acquisition.

## Limitations

- The causal mechanism linking reasoning augmentation to safety degradation is inferred but not directly validated; the paper does not isolate whether SFT, RL, or their combination is responsible for the "reasoning tax."
- Self-correction is observed but not systematically analyzed; the paper does not explain why certain models override unsafe reasoning steps, nor whether this behavior is consistent or exploitable.
- The evaluation relies on GPT-4o-mini as a judge, introducing potential subjectivity and variance; no human validation or inter-annotator agreement is reported.
- The study focuses on a limited set of MLRM-base pairs; generalization to other models or training regimes is untested.

## Confidence

- **High confidence**: Empirical observation that MLRMs exhibit higher jailbreaking success rates than base MLLMs under adversarial attacks (supported by large-scale benchmarking)
- **Medium confidence**: Mechanism of "reasoning tax" due to SFT/RL; the causal link is plausible but not directly proven
- **Medium confidence**: Scenario-specific blind spots; the consistency across datasets and models is compelling, but the underlying reasons for differential vulnerability are not established
- **Low confidence**: Practical utility of emergent self-correction; the phenomenon is observed but not explained or tested for robustness

## Next Checks

1. **Mechanistic ablation study**: Train MLRMs using only SFT, only RL, and both, then compare safety degradation to isolate the source of the "reasoning tax."

2. **Human validation**: Recruit independent annotators to rate a sample of model outputs for harmfulness, comparing agreement with GPT-4o-mini and assessing variance.

3. **Cross-model generalization**: Extend the safety evaluation to additional MLRM-base pairs (e.g., from other model families) and to models trained with explicit safety-aware reward modeling, to test whether the reasoning tax is universal or model-dependent.