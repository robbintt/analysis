---
ver: rpa2
title: Are LLMs (Really) Ideological? An IRT-based Analysis and Alignment Tool for
  Perceived Socio-Economic Bias in LLMs
arxiv_id: '2503.13149'
source_url: https://arxiv.org/abs/2503.13149
tags:
- bias
- llms
- should
- political
- ideological
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces an Item Response Theory (IRT)-based framework
  for detecting and quantifying socioeconomic bias in large language models (LLMs)
  without relying on subjective human judgments. Unlike traditional methods, IRT accounts
  for item difficulty and response behavior, improving ideological bias estimation.
---

# Are LLMs (Really) Ideological? An IRT-based Analysis and Alignment Tool for Perceived Socio-Economic Bias in LLMs

## Quick Facts
- arXiv ID: 2503.13149
- Source URL: https://arxiv.org/abs/2503.13149
- Reference count: 40
- Introduces IRT-based framework achieving R² = 0.864 for PNA detection and R² = 0.896 for bias estimation

## Executive Summary
This paper introduces a novel Item Response Theory (IRT)-based framework for detecting and quantifying socioeconomic bias in large language models (LLMs) without relying on subjective human judgments. Unlike traditional methods that conflate safety alignment with ideological stance, this two-stage approach separately models response avoidance and perceived bias, achieving strong statistical fit (R² = 0.864 and R² = 0.896 respectively). The framework demonstrates that off-the-shelf LLMs often avoid ideological engagement rather than exhibit bias, with baseline ChatGPT refusing answers 92.55% of the time.

## Method Summary
The framework employs a two-stage IRT approach: Stage 1 uses a 2-Parameter Logistic (2PL) model to estimate the probability of "Prefer Not to Answer" (PNA) responses, while Stage 2 applies a Generalized Partial Credit Model (GPCM) to ordinalized responses (Strongly Agree to Strongly Disagree) to estimate left-right ideological positioning. The method fine-tunes two LLM families (Meta LLaMa 3.2-1B-Instruct and ChatGPT 3.5) on curated ideological datasets to serve as reference points. Responses are collected to a 105-item inventory covering economic and social conservatism, classified using Mistral-Small:24b, and analyzed to derive ideological ability parameters (θ) for comparison against fine-tuned baselines.

## Key Results
- IRT framework achieved R² = 0.864 for response avoidance detection and R² = 0.896 for bias estimation
- Baseline ChatGPT refused answers 92.55% of the time, while fine-tuned variants had PNA rates under 13%
- LLaMa 3.2-1B-Instruct showed 55.02% PNA rate, demonstrating varied refusal behavior across model families
- Fine-tuned baseline models successfully anchored the latent ideological scale, enabling relative bias quantification

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage IRT Model Separates Avoidance from Bias
Conditioning on two sequential IRT stages enables discrimination between refusal behavior and ideological stance. Stage 1 uses 2PL to estimate PNA probability, while Stage 2 applies GPCM to ordinalized responses, estimating ideological positioning only for non-PNA responses. This decoupling treats refusal as alignment evidence rather than missing data.

### Mechanism 2: Fine-Tuned Ideological Baselines Anchor Latent Scale
Fine-tuning models on curated ideological datasets creates reference points for calibrating IRT ability parameters (θ). Supervised fine-tuning on 16-33 instruction pairs per ideological direction distills U.S. liberal/conservative stances, serving as extreme anchors for benchmarking unknown models.

### Mechanism 3: Item Discrimination Weighting Prioritizes Informative Prompts
IRT-derived discrimination parameters (αi) enable resource-efficient bias detection by identifying items that best separate ideological positions. High-α items can be prioritized for rapid alignment screening, while low-α items contribute less signal.

## Foundational Learning

- **Item Response Theory (IRT) – 2PL and GPCM Models**: The paper's core method relies on IRT to weight items by difficulty/discrimination and estimate latent bias (θ). Understanding how 2PL models binary PNA and how GPCM handles ordinal responses is essential. *Quick check*: Can you explain why a 2PL model's discrimination parameter (α) matters for distinguishing aligned vs. non-aligned models?

- **LLM Alignment vs. Political Bias**: The paper differentiates safety alignment (refusal behavior) from ideological bias. Prior work conflated the two; understanding this distinction is critical for interpreting the two-stage results. *Quick check*: Why might high refusal rates (PNA) be misinterpreted as ideological centrism if analyzed with traditional forced-choice scales?

- **Factor-Based Fine-Tuning for Ideology Injection**: The method uses minimal instruction pairs (16-33) derived from psychological factor models rather than large ideological corpora. This controls for confounds but assumes factor coverage. *Quick check*: What are the trade-offs between large-corpus ideological fine-tuning and factor-based minimal fine-tuning?

## Architecture Onboarding

- **Component map**: Test Item Inventory (105 items) -> Response Generation (target LLMs) -> Response Classification (Mistral-Small:24b) -> Stage 1 IRT (2PL for PNA) -> Stage 2 IRT (GPCM for bias) -> Baseline Calibration (fine-tuned models)

- **Critical path**: Generate open-ended responses -> classify to ordinal scale -> fit 2PL for PNA -> fit GPCM for bias -> compare θ to baselines

- **Design tradeoffs**: Open-ended vs. forced-choice (captures refusals authentically but requires classification); Small-scale factor fine-tuning vs. large-corpus tuning (minimizes confounds but may under-cover nuance); Single θ vs. separate economic/social scales (current implementation combines dimensions)

- **Failure signatures**: PNA rates near 0% or 100% (model may not be engaging with items); GPCM θ clustering around 0 for all models (item discrimination may be too low); Baseline θ overlap (fine-tuning insufficient)

- **First 3 experiments**: Reproduce PNA rates on baseline ChatGPT and LLaMa-3.2-1B; Validate baseline separation by generating responses from Left/Right-GPT; Item ablation by fitting GPCM on subsets of high-α items

## Open Questions the Paper Calls Out

- **Open Question 1**: Does incorporating lexical and framing analysis improve the interpretability of open-ended response classifications compared to the current LLM-based processing? The current study relies solely on Mistral-Small:24b without deeper linguistic feature analysis.

- **Open Question 2**: How does separating economic and social conservatism into two distinct scales affect the accuracy of the IRT-based bias estimation? The current study aggregates distinct ideological dimensions into a single construct.

- **Open Question 3**: To what extent does a user's own political orientation influence their perception of bias in outputs validated by this framework? The tool measures "perceived" bias but doesn't account for the subjective "bias perception effect" of end-users.

## Limitations

- Framework validity depends entirely on the representativeness and quality of fine-tuned baseline models
- Generalizability across cultural contexts is uncertain due to U.S.-centric ideological constructs
- Resource-efficiency claims for reduced item sets lack empirical validation

## Confidence

- **Confidence: Medium** for ideological bias quantification claims - strong statistical fit but dependent on baseline model quality
- **Confidence: Low** for generalizability across cultural contexts - framework assumes U.S. constructs apply universally
- **Confidence: Medium** for resource-efficiency claims - high-discrimination items identified but reduced set validation missing

## Next Checks

1. **Baseline Model Verification**: Generate responses from fine-tuned Left/Right-GPT and Left/Right-LLaMa models using exact fine-tuning datasets and confirm θ distributions show clear separation

2. **Classifier Consistency Test**: Run Mistral-Small:24b classification on a held-out sample with human expert validation to measure accuracy and identify systematic biases

3. **Cross-Cultural Generalizability**: Apply the 105-item inventory to multilingual models and compare ideological θ distributions to assess consistency across cultural contexts