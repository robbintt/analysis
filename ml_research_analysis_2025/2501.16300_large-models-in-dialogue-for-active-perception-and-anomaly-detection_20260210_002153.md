---
ver: rpa2
title: Large Models in Dialogue for Active Perception and Anomaly Detection
arxiv_id: '2501.16300'
source_url: https://arxiv.org/abs/2501.16300
tags:
- scene
- perception
- drone
- proposed
- anomaly
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a novel framework that employs Large Language
  Models (LLMs) to actively collect information and detect anomalies, even in unprecedented
  situations. The core idea is a model dialogue approach where two deep learning models
  (an LLM and a Visual Question Answering model) interact to control a drone and improve
  perception accuracy.
---

# Large Models in Dialogue for Active Perception and Anomaly Detection

## Quick Facts
- **arXiv ID:** 2501.16300
- **Source URL:** https://arxiv.org/abs/2501.16300
- **Reference count:** 36
- **Primary result:** Cross-modal dialogue between LLM and VQA model achieves improved caption-image matching (0.58-0.70) and anomaly detection in drone-based active perception

## Executive Summary
This paper presents a novel framework that employs Large Language Models (LLMs) to actively collect information and detect anomalies, even in unprecedented situations. The core idea is a model dialogue approach where two deep learning models (an LLM and a Visual Question Answering model) interact to control a drone and improve perception accuracy. The LLM provides navigational control through natural language commands, while the VQA model answers questions about images captured by the drone's camera. This dialogue process enables active perception, where the drone explores the scene and detects potential anomalies.

## Method Summary
The proposed framework uses a dialogue between an LLM (GPT-3.5) and a VQA model (PnP-VQA with BLIP) to control a drone in simulated environments. The system operates in three modes: active perception (drone explores while LLM asks questions), validation (drone revisits positions to verify details), and explanation (outputs final description with attention maps). The LLM receives captions and answers from the VQA model, formulates exploratory questions, and issues movement commands to navigate the scene. The method uses zero-shot inference without model training, relying on carefully engineered prompts to guide the LLM's behavior.

## Key Results
- Caption-image matching scores improve from baseline (0.38-0.46) to proposed (0.58-0.70) across all environments
- Anomaly detection accuracy demonstrated in controlled experiments with fires, car crashes, and hazardous elements
- Framework successfully provides detailed scene descriptions and identifies potential hazards
- Explanation mode generates interpretable attention maps showing model focus areas

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Dialogue as a Sensory Proxy
The VQA model acts as a text-based sensor, converting visual observations into textual descriptions the LLM can process. The LLM iteratively queries visual information through natural language, effectively "seeing" through text. This loop continues until the LLM determines it has sufficient information.

### Mechanism 2: Multi-View Aggregation via Embodied Movement
Physical repositioning of the drone captures multiple viewpoints, aggregating into a more complete scene description than single-frame analysis. Each position yields new image, caption, and Q&A pairs, building a spatially-grounded understanding.

### Mechanism 3: Validation Loop with Ensemble Consensus
After initial exploration, the LLM identifies uncertain claims. The drone returns to saved positions (with Gaussian noise) to re-query the VQA. An ensemble approach compiles answers into a final validated description, reducing hallucination.

## Foundational Learning

- **Visual Question Answering (VQA)**: Converting visual observations into textual descriptions the LLM can process. Why needed: Core mechanism for LLM to "see" through text. Quick check: How does VQA differ from image classification in input/output modalities?

- **Active Perception**: Agent-controlled sensing where the agent moves to gather better information. Why needed: Enables the drone to explore and capture multiple viewpoints. Quick check: What distinguishes active from passive perception?

- **Prompt Engineering for Tool-Using LLMs**: Designing prompts to constrain LLM outputs to predefined action sets. Why needed: Ensures LLM reliably outputs parsable commands. Quick check: How would you design a prompt for predefined action tokens?

## Architecture Onboarding

- **Component map:** [AirSim Drone] → captures → [Image I] → [VQA: PnP-VQA + BLIP] → answer A, caption C, score → [LLM: GPT-3.5] ← prompt with commands → [Command Parser] → movement/control functions → [Position Logger] → saved positions for validation → [GradCAM Module] → attention maps for explainability

- **Critical path:**
  1. Image capture at spawn position
  2. Initial VQA query ("What do you see?")
  3. LLM issues movement + question pair
  4. Repeat until "I know enough"
  5. Validation loop (revisit saved positions)
  6. Output final description + attention maps

- **Design tradeoffs:**
  - Latency vs. accuracy: Full pipeline takes 12+ minutes; anomaly-triggered early exit reduces to <5 minutes
  - Simulation fidelity vs. real-world transfer: AirSim provides realistic physics but results not validated on physical drones
  - Zero-shot vs. fine-tuned: Uses off-the-shelf GPT-3.5 and BLIP without task-specific training

- **Failure signatures:**
  - LLM outputs invalid commands (not in predefined set)
  - VQA matching scores remain low across all positions (poor visual conditions)
  - Hallucinated objects in final description despite validation
  - Drone navigation fails or collides with obstacles

- **First 3 experiments:**
  1. Smoke test: Run full pipeline in Mountain Landscape, verify LLM issues ≥5 movement commands, log all VQA outputs
  2. Ablation on validation: Compare caption quality (matching score) with/without validation loop, quantify latency difference
  3. Anomaly detection stress test: Introduce controlled anomaly at varying distances, measure detection accuracy and time-to-detection across 10 runs per distance tier

## Open Questions the Paper Calls Out

- How can active perception approaches in unstructured open-world setups be effectively evaluated and standardized beyond static metrics?
- To what extent can additional validation steps or examination points eliminate hallucinations in the LLM's final scene descriptions?
- Can the proposed framework maintain real-time viability and accuracy when transferred from simulation to physical drones in real-world environments?

## Limitations
- Critical prompt dependence with incomplete disclosure, creating reproducibility challenges
- VQA reliability degradation under domain shift or degraded visual conditions not characterized
- Validation loop adds significant latency (12+ minutes baseline) without efficient alternatives
- All experiments use AirSim; no physical drone testing or cross-simulation validation presented

## Confidence
- **High confidence**: Cross-modal dialogue mechanism is technically sound, supported by improved caption-image matching scores (0.58-0.70 vs baseline 0.38-0.46)
- **Medium confidence**: Anomaly detection accuracy claims supported by synthetic anomaly experiments in controlled environments
- **Low confidence**: "Safety tips" and "explanations" in explanation mode not quantitatively evaluated for usefulness or accuracy

## Next Checks
1. **Prompt replication test**: Reproduce exact system prompt through author communication, run 5 complete pipelines in Mountain Landscape, verify LLM issues ≥5 valid movement commands and VQA scores exceed 0.50

2. **Cross-simulation validation**: Port anomaly detection task to different simulation platform (e.g., Carla) with same Mountain Landscape scene, measure caption-image matching score degradation and accuracy drop

3. **Real-world degradation analysis**: Systematically degrade visual conditions (fog, low light, motion blur) in AirSim, measure VQA matching score drop and hallucination rate, determine failure threshold