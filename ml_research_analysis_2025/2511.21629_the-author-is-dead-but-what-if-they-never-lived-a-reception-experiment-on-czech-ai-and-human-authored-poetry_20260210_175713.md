---
ver: rpa2
title: The author is dead, but what if they never lived? A reception experiment on
  Czech AI- and human-authored poetry
arxiv_id: '2511.21629'
source_url: https://arxiv.org/abs/2511.21629
tags:
- poetry
- poems
- human
- nonsense
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigated whether native Czech speakers can distinguish
  between AI-generated and human-written poetry, and how authorship perceptions affect
  aesthetic evaluations. Participants were presented with 32 poems (16 pairs of AI-
  and human-written stanzas) from modern and nonsense Czech poetry, and asked to guess
  authorship and rate poems on various attributes.
---

# The author is dead, but what if they never lived? A reception experiment on Czech AI- and human-authored poetry

## Quick Facts
- arXiv ID: 2511.21629
- Source URL: https://arxiv.org/abs/2511.21629
- Reference count: 14
- Primary result: Czech speakers cannot distinguish AI from human poetry; perceived AI authorship reduces aesthetic ratings regardless of actual quality

## Executive Summary
This study investigates whether native Czech speakers can distinguish AI-generated from human-written poetry and how authorship perceptions affect aesthetic evaluations. Using GPT-4.5 to generate continuations of 16 Czech poems (8 modern, 8 nonsense), researchers found participants performed at chance level overall (45.8% correct), with slightly better accuracy for nonsense poetry (51.4%) than modern (40.2%). Critically, poems believed to be AI-generated were rated less favorably than those believed to be human-written, despite AI poems being rated equally or more favorably when actual authorship was considered. The study demonstrates AI's convincing poetry generation capabilities in a morphologically complex, low-resource language like Czech, while revealing a strong authorship bias in aesthetic evaluation.

## Method Summary
Researchers used GPT-4.5 Preview to generate continuations of 16 Czech poems (8 modern free verse from Psí víno magazine, 8 nonsense from classical Czech poets). Each poem was split at a natural stanza break, and the AI was prompted to write the next stanza without post-editing. The resulting 32 poems (16 AI-human pairs) were presented to 126 Czech native speakers who rated them on six attributes (liking, rhyme, playful, imaginative, sense, serious) and guessed authorship. Mixed-effects logistic regression analyzed accuracy while controlling for participant random effects and various predictors including ratings, poetry background, and demographics.

## Key Results
- Participants achieved 45.8% overall accuracy in distinguishing AI from human poetry, at chance level
- Accuracy was slightly higher for nonsense poetry (51.4%) than modern (40.2%)
- Poetry background or experience did not predict recognition accuracy
- Poems believed to be AI-generated received significantly lower aesthetic ratings than those believed to be human-written
- Actual authorship showed no significant effect on ratings when controlling for belief

## Why This Works (Mechanism)

### Mechanism 1: Aesthetic-Attribution Coupling
Readers' aesthetic enjoyment is strongly coupled to their belief about authorship, creating a bias where perceived AI authorship degrades evaluation scores regardless of actual quality. Participants utilize a heuristic where high aesthetic value implies human origin, while the "AI" label triggers penalties in perceived "sense" or "seriousness." This relationship may be bidirectional: readers may assume a poem is AI-generated because they dislike it, or dislike it because they assume it is AI-generated.

### Mechanism 2: Stylistic Overfitting in Low-Resource Generation
LLMs can mimic specific genre constraints (rhyme, neologisms) in morphologically complex, low-resource languages so effectively that they are indistinguishable from human poets. The model likely leverages transfer learning from high-resource languages and specific training instances to replicate surface-level formal features. In modern free verse, the model out-performed humans on formal metrics (rhyme), which paradoxically might have served as a weak signal for AI detection if modern poetry is expected to be free verse.

### Mechanism 3: Expertise-Invariant Perception
Domain expertise provides no significant advantage in detecting AI authorship. The subtle defects usually associated with AI generation are either absent in this generation model or overwhelmed by the subjective nature of poetry reception. The cues required to detect AI are not those taught in standard literary education, which focuses on interpretation rather than forensic detection.

## Foundational Learning

- **Concept: Morphological Complexity**
  - Why needed here: Czech is a West Slavic language with rich inflection where word order is flexible and suffixes carry meaning. The paper argues that success in English AI poetry doesn't automatically translate to fusional languages.
  - Quick check question: Does the AI maintain correct case and gender agreement when generating rhyming couplets in Czech?

- **Concept: Logistic Regression (Mixed Effects)**
  - Why needed here: The study uses this to isolate specific variables while controlling for random variations in participant behavior and specific poem pairs.
  - Quick check question: Why is "Liking" a negative predictor of correct authorship classification in the model?

- **Concept: The Continuation Task**
  - Why needed here: The methodology differs from "write a poem from scratch." By feeding a human stanza and asking for a continuation, the AI is grounded in a specific style context, reducing the "cold start" problem of creative generation.
  - Quick check question: How might the "continuation" prompt bias the AI toward mimicking the rhyme scheme of the provided human text?

## Architecture Onboarding

- **Component map:** GPT-4.5 Preview -> Czech Poetry Corpus (Psí víno + Classical Nonsense) -> 32 Poem Pairs -> Web Experiment Interface -> 126 Participants -> Mixed-Effects Logistic Regression

- **Critical path:** Select Human Poem -> Split Stanza -> Prompt LLM -> [No Post-Editing] -> Collect AI Stanza -> Pair with Human Stanza -> Randomized Display -> Participant Judgment -> Regression Analysis

- **Design tradeoffs:**
  - *Realism vs. Control:* Minimal effort generation (no prompt engineering/post-editing) to simulate "AI slop" scenarios, trading theoretical "maximum quality" for ecological validity
  - *Metrics:* Using "belief" as primary variable, acknowledging inability to disentangle cause-and-effect

- **Failure signatures:**
  - **The "Hallucination" Failure:** AI generated non-existing Slavic-sounding words or Slovak words instead of strict Czech, particularly in nonsense genre
  - **The "Cliché" Failure:** AI used "basic rhyme pattern that resembles clichéd romantic poetry" in modern free verse where humans avoided it

- **First 3 experiments:**
  1. **Reversal Study:** Show participants the same poem twice with randomized (or swapped) authorship labels to strictly measure the label effect without text variance
  2. **The "High-Effort" Baseline:** Replicate using aggressive prompt engineering (Chain-of-Thought, few-shot examples) and post-hoc human selection to see if "best-of" AI poetry breaks the "chance" detection barrier
  3. **Semantic Verification:** Test the "making sense" dimension by checking if AI nonsense poetry is rated as "more sense-making" than human nonsense poetry

## Open Questions the Paper Calls Out

- **Question:** What is the causal direction between aesthetic evaluation and authorship attribution (does belief in AI authorship lower ratings, or do lower ratings trigger AI attribution)?
  - **Basis in paper:** [explicit] The authors state they "cannot determine the causal direction — whether people first assume that a poem is AI-generated and therefore judge it as less beautiful and playful, or whether they find it less beautiful and playful and thus infer it must have been written by AI."
  - **Why unresolved:** The experimental design measured correlations between subjective ratings and authorship guesses but did not manipulate the sequence of these tasks to establish causality.

- **Question:** Does advanced literary expertise or professional poetic training improve the ability to distinguish AI-generated poetry from human-authored works?
  - **Basis in paper:** [explicit] The authors acknowledge their sample "did not include many professional poets or literary experts" and conclude that "this requires further investigation in a dedicated study."
  - **Why unresolved:** The current participant pool was predominantly non-expert, and while a pilot study with literature students showed similar failure rates, the specific capability of high-level experts remains untested.

- **Question:** Do the findings regarding indistinguishability and bias generalize to other low-resource or morphologically complex languages?
  - **Basis in paper:** [explicit] The Conclusion states, "In the future, our experiment should be replicated in additional low-resource languages to test the generalizability of these findings."
  - **Why unresolved:** The study is limited to Czech; it is unclear if the inability to detect AI poetry is specific to Czech's morphology or the specific training data of the model used.

## Limitations

- Ecological validity comes at the cost of control; using raw GPT-4.5 outputs without post-editing means performance reflects average generation quality rather than optimized outputs
- The classification task's binary nature may mask subtler distinctions, as participants were never asked to identify "I don't know" responses
- The poetry corpus represents only two genres (modern free verse and nonsense poetry), limiting generalizability to other poetic forms
- The participant sample's demographic skew (76% female, predominantly young adults) raises questions about whether findings extend to broader Czech-speaking populations

## Confidence

- **High confidence**: The primary finding that participants cannot distinguish AI from human poetry (45.8% accuracy at chance level) and that perceived authorship strongly influences aesthetic ratings
- **Medium confidence**: The claim that expertise doesn't help detection, though measurement through self-report may not capture nuanced literary training
- **Low confidence**: The specific mechanisms by which AI generates convincing poetry in morphologically complex languages, as the paper doesn't analyze actual generation process or errors

## Next Checks

1. **Replication with genre expansion**: Test the same methodology across additional Czech poetic forms (sonnets, haiku adaptations, narrative poetry) to verify whether detection difficulty persists beyond free verse and nonsense poetry

2. **Author label manipulation study**: Present identical poems with randomized authorship labels to participants to isolate the "belief effect" from text-based differences, establishing causal direction of the aesthetic-attribution coupling

3. **Error pattern analysis**: Conduct forensic linguistic analysis of AI-generated errors (non-existent words, grammatical anomalies) to determine whether these patterns differ systematically from human poetic license, potentially creating a new detection methodology