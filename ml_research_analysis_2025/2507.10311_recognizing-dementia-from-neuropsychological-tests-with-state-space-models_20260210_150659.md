---
ver: rpa2
title: Recognizing Dementia from Neuropsychological Tests with State Space Models
arxiv_id: '2507.10311'
source_url: https://arxiv.org/abs/2507.10311
tags:
- dementia
- class
- speech
- audio
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses automatic dementia classification (ADC) from
  neuropsychological test recordings, focusing on long audio sequences where prior
  transformer-based models struggle. The authors propose Demenba, a state-space model
  (SSM) based on Mamba that scales linearly in memory and computation, making it suitable
  for processing full-length hour-long recordings.
---

# Recognizing Dementia from Neuropsychological Tests with State Space Models

## Quick Facts
- arXiv ID: 2507.10311
- Source URL: https://arxiv.org/abs/2507.10311
- Reference count: 40
- Demenba achieves 21% relative AUC improvement over EfficientNet in fine-grained 3-class dementia classification from hour-long audio recordings.

## Executive Summary
This paper introduces Demenba, a state-space model (SSM) based on Mamba that addresses automatic dementia classification from long neuropsychological test recordings. Unlike prior transformer-based approaches that struggle with quadratic complexity, Demenba scales linearly with sequence length, enabling processing of full hour-long conversations. Trained on over 1,000 hours of Framingham Heart Study data with balanced dementia stages, Demenba-small and Demenba-medium outperform previous state-of-the-art models by 21% relative AUC in fine-grained 3-class classification while using fewer parameters. The approach demonstrates that fine-grained speaker diarization significantly improves performance over simple voice activity detection, and that incorporating silence patterns boosts detection particularly in binary classification settings.

## Method Summary
The method employs a Mamba-based VMamba backbone configured as DASS-small or DASS-medium, initialized with ImageNet pretrained weights. Audio recordings are first segmented using pyannote speaker diarization or voice activity detection into chunks up to 360 seconds. These segments are converted to 128-bin mel-filterbanks and processed by the SSM encoder with mean pooling followed by classification. The model is trained using Adam optimizer (lr=1e-5) for 40 epochs with weighted cross-entropy loss. For text-based classification, transcripts from Whisper-Large v2 are processed by BERT or LLM encoders. Audio and text predictions are fused via weighted averaging, with fusion weights optimized on validation data.

## Key Results
- Demenba-small and Demenba-medium outperform EfficientNet by 21% relative AUC in 3-class classification
- Speaker diarization-based boundaries improve AUC by up to 5% absolute compared to voice activity detection
- Including silence patterns yields 10-15% AUC boost for 2-class classification but only 0-2% for 3-class
- Fusion with text classifiers (BERT, LLMs) achieves 0.95 AUC, improving accuracy over audio-only models
- Scaling experiments show continued performance gains with increased data and model size

## Why This Works (Mechanism)

### Mechanism 1
- State-space models enable processing of full-length neuropsychological recordings by scaling linearly with sequence length. The Mamba-based SSM backbone replaces quadratic self-attention with selective scan operations that compress temporal information into a fixed-dimensional hidden state. This allows 360-second segments to be processed in a single forward pass without chunking. The core assumption is that dementia-relevant cues are distributed across extended conversational turns rather than localized in short utterances. Evidence shows 8-17% AUC gains when increasing segment duration from 30s to 180s, with diminishing returns beyond 180s.

### Mechanism 2
- Fine-grained speaker diarization preserves within-speaker conversational context that improves classification over simple voice activity detection. Speaker diarization segments by speaker identity, maintaining turn-taking patterns and prosodic continuity within participant speech. The core assumption is that the examiner's speech contains implicit diagnostic signal, such as unconscious adaptation to impaired participants. Evidence shows SD-based boundaries outperform VAD by as much as 5% absolute AUC across all models and classification settings, with training on both interviewer and participant speech yielding highest AUC.

### Mechanism 3
- Silence patterns encode hesitation and word-finding difficulty that improve binary dementia detection. Silence segments are retained as input features rather than removed, with extended pauses correlating with cognitive retrieval difficulty. The core assumption is that hesitation patterns differ systematically between cognitively normal and impaired speakers but are similar between MCI and dementia. Evidence shows including silence yields 10-15% AUC boost for 2-class but only 0-2% for 3-class classification, with silence often signaling hesitation and word-finding difficulty.

## Foundational Learning

- **State-space models (SSMs) vs. Transformers**
  - Why needed here: Understanding why Mamba can process 6-minute segments while transformers cannot requires grasping the O(n) vs. O(n²) complexity difference.
  - Quick check question: Given a 360-second audio segment at 100 frames/second, approximately how many operations would a 12-layer transformer require vs. a 12-layer SSM?

- **Speaker diarization**
  - Why needed here: The paper's preprocessing pipeline depends on SD for segmenting examiner vs. participant speech; understanding its error modes is critical for debugging.
  - Quick check question: If diarization swaps speaker labels mid-recording, which ablation result suggests the model might still perform adequately?

- **Late fusion of multimodal predictors**
  - Why needed here: The audio-text fusion uses weighted averaging (Equation 1); understanding when and why this works is essential for tuning λ.
  - Quick check question: If the text classifier's ASR input has 60% character error rate (as reported), would you expect optimal λ to increase or decrease compared to clean transcripts?

## Architecture Onboarding

- **Component map**: Raw audio -> Mel-filterbank (128 bins, 10ms shift, 25ms window) -> VMamba encoder -> mean pooling -> classification head
- **Critical path**: Raw audio → mel-filterbank → VMamba encoder → segment-level logits → selective majority voting → recording-level paudio
- **Design tradeoffs**: Segment length vs. memory (360s segments require batch size 1 on 48GB GPU); VAD vs. SD (SD preserves speaker context +5% AUC but adds complexity); BERT vs. LLM for text (BERT outperforms LLM on noisy ASR transcripts +6% AUC)
- **Failure signatures**: AUC plateaus at ~0.85 with audio-only (likely insufficient training data); text classifier underperforms on new data (check ASR error rate); selective voting degrades vs. standard voting (k may be too small)
- **First 3 experiments**:
  1. Train Demenba-small on 100 hours/class with VAD boundaries, 180s segments. Target: AUC ≈ 0.72 (2-class).
  2. Compare speech-only vs. speech+silence on 2-class task. Expect 10-15% AUC gain.
  3. With Demenba-medium + BERT, sweep λ from 0 to 1. Expect optimal λ ≈ 0.5.

## Open Questions the Paper Calls Out

### Open Question 1
- Can integrating the audio-based SSM with multimodal Large Language Models (LLMs) improve both classification performance and model interpretability compared to the current late-fusion approach? The conclusion states interest in improving performance and interpretability by integrating with multimodal LLMs. This is unresolved because the current late-fusion strategy limits joint multimodal reasoning and explainability. Evidence would include evaluation of an end-to-end multimodal architecture using visual/audio attention maps or text explanations to justify predictions.

### Open Question 2
- Can the model effectively distinguish between fine-grained dementia subtypes (e.g., Alzheimer's disease vs. vascular dementia) rather than just severity levels? The conclusion lists studying more fine-grained classification of dementia subtypes as a future direction. This is unresolved because the current work focuses on classifying cognitive status into three severity buckets rather than differentiating underlying pathological causes. Evidence would include training and validation results on a dataset containing adjudicated labels for specific dementia etiologies.

### Open Question 3
- To what extent does Demenba generalize to external datasets, particularly those lacking speaker-identifying metadata (de-identified speech)? The conclusion identifies generalization performance on other dementia datasets, including those with de-identified speech recordings, as a research direction. This is unresolved because the model was trained exclusively on FHS dataset utilizing specific diarization and speaker information. Evidence would include performance metrics when applied zero-shot or fine-tuned on diverse external datasets like ADReSS or Pitt Corpus.

### Open Question 4
- Does the reliance on interviewer speech features (which may reflect unconscious examiner bias) hinder the model's robustness when applied to interviewers with different interaction styles? The paper notes that "significant amount of dementia-related information" is found in interviewer speech and links this to "unconscious interviewer bias." This is unresolved because while the paper demonstrates that including interviewer speech improves AUC, it doesn't test if this reliance generalizes across diverse clinical settings. Evidence would include a cross-dataset ablation study evaluating models on data from different clinical sites with varying interviewer behaviors.

## Limitations
- The Framingham Heart Study (FHS) data requires formal application and approval for access, preventing immediate independent reproduction and creating a significant barrier to external validation.
- The specific value of $k$ (number of top segments for selective majority voting) is tuned on a held-out validation set but not reported, affecting reproducibility and requiring dataset-specific tuning.
- The paper references DASS [13] for VMamba backbone configuration but doesn't specify exact layer depths/widths for "small" and "medium" variants, requiring implementers to infer from the DASS paper.

## Confidence

- **High confidence**: Linear scaling of SSMs enabling full-length recording processing, speaker diarization improving classification (+5% AUC), silence patterns encoding hesitation (10-15% AUC boost for 2-class), and overall performance superiority over EfficientNet (21% relative AUC gain).
- **Medium confidence**: The claim that dementia markers are distributed across extended conversational turns is supported by ablation but not definitively proven. The fusion weight λ ≈ 0.5 is empirically derived but may vary with ASR quality.
- **Low confidence**: The hypothesis that examiner speech contains diagnostic signal is suggested by results but not rigorously tested. The diminishing returns beyond 180s segments is based on limited ablation points.

## Next Checks

1. **Selective voting ablation**: Systematically vary $k$ (top segments) from 1 to 10 and measure AUC impact on Demenba-small with SD segmentation to validate the robustness of the voting mechanism.

2. **ASR quality sensitivity**: Train Demenba-medium with synthetic ASR noise at 30%, 60%, and 90% CER to measure AUC degradation and optimal λ shifts under varying transcription quality.

3. **Examiner-only classification**: Train a model on only interviewer speech (no participant audio) using SD boundaries to quantify the examiner-bias hypothesis by comparing performance to participant-only and full conversation.