---
ver: rpa2
title: 'SiniticMTError: A Machine Translation Dataset with Error Annotations for Sinitic
  Languages'
arxiv_id: '2509.20557'
source_url: https://arxiv.org/abs/2509.20557
tags:
- cantonese
- translation
- chinese
- error
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SINITICMTERROR, a novel dataset providing
  error span, type, and severity annotations for machine-translated examples from
  English to Mandarin, Cantonese, and Wu Chinese. The dataset builds on existing parallel
  corpora (FLORES+) and enables fine-tuning models for error detection, supporting
  research on translation quality estimation and low-resource language evaluation.
---

# SiniticMTError: A Machine Translation Dataset with Error Annotations for Sinitic Languages

## Quick Facts
- arXiv ID: 2509.20557
- Source URL: https://arxiv.org/abs/2509.20557
- Reference count: 38
- Introduces SINITICMTERROR, a dataset with error annotations for machine-translated text in Mandarin, Cantonese, and Wu Chinese

## Executive Summary
SINITICMTERROR is a novel dataset providing detailed error span, type, and severity annotations for machine-translated text in three Sinitic languages: Mandarin, Cantonese, and Wu Chinese. The dataset is built on the FLORES+ parallel corpus and aims to support research in translation quality estimation, error detection, and low-resource language evaluation. Native speakers conducted rigorous annotations, with iterative feedback improving inter-annotator agreement. Preliminary analyses reveal distinctive error distributions across languages, notably higher error density in Cantonese compared to Mandarin.

## Method Summary
The authors constructed SINITICMTERROR by annotating machine-translated examples from English to Mandarin, Cantonese, and Wu Chinese. Native speakers performed error span, type, and severity annotations, with iterative feedback sessions to improve consistency and inter-annotator agreement. The dataset is derived from the FLORES+ parallel corpus, and the annotation framework covers a range of error types and severity levels. Analyses were conducted to assess the reliability of annotations and to characterize error patterns across the three languages.

## Key Results
- SINITICMTERROR provides error span, type, and severity annotations for MT outputs in Mandarin, Cantonese, and Wu Chinese.
- Preliminary statistics show distinctive error distributions across languages, with higher error density in Cantonese than Mandarin.
- The dataset supports fine-tuning models for error detection and facilitates research on translation quality estimation and low-resource language evaluation.

## Why This Works (Mechanism)
The dataset's effectiveness stems from its use of native speaker annotations, which provide nuanced and accurate error detection across Sinitic languages. The iterative feedback process ensures high inter-annotator agreement, improving the reliability of the annotations. By covering multiple languages and error types, the dataset enables the development of robust error detection models and supports comparative analysis of translation quality across language varieties.

## Foundational Learning
- **Error annotation in MT**: Critical for evaluating and improving translation quality; quick check: compare annotated vs. raw MT outputs.
- **Inter-annotator agreement**: Ensures annotation reliability; quick check: report kappa scores or similar metrics.
- **Cross-lingual error analysis**: Enables understanding of language-specific challenges; quick check: statistical tests for error distribution differences.

## Architecture Onboarding
- **Component map**: FLORES+ corpus -> Native speaker annotation -> Error span/type/severity labels -> Dataset
- **Critical path**: Parallel corpus selection → Annotation protocol design → Native speaker annotation → Quality control (iterative feedback) → Dataset compilation
- **Design tradeoffs**: Balance between annotation depth (span/type/severity) and annotation cost; focus on three major Sinitic languages limits broader applicability
- **Failure signatures**: Low inter-annotator agreement, inconsistent error type definitions, or sampling bias in corpus selection
- **First experiments**:
  1. Measure inter-annotator agreement using kappa or similar metrics
  2. Analyze error type and severity distributions across languages
  3. Train a simple error detection model and evaluate on held-out data

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset size and representativeness may limit model training and evaluation robustness.
- Annotation quality and consistency are not fully characterized (e.g., annotator expertise, bias).
- Observed error density differences between languages lack statistical significance testing.

## Confidence
- Dataset Construction and Annotation Process: **High**
- Error Type and Severity Annotation Framework: **Medium**
- Cross-Lingual Error Distribution Findings: **Low**

## Next Checks
1. Conduct statistical significance testing to determine if observed error density differences between languages are robust and not due to sampling variance.
2. Perform error annotation on a separate, held-out subset of FLORES+ or an external parallel corpus to assess annotation consistency and generalizability.
3. Train and evaluate a baseline error detection model using SINITICMTERROR, reporting performance metrics (precision, recall, F1) and comparing against models trained on existing datasets to quantify practical impact.