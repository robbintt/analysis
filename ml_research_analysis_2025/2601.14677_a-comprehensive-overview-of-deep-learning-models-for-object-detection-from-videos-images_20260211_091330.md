---
ver: rpa2
title: A comprehensive overview of deep learning models for object detection from
  videos/images
arxiv_id: '2601.14677'
source_url: https://arxiv.org/abs/2601.14677
tags:
- object
- detection
- feature
- video
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This review provides a comprehensive overview of deep learning
  models for object detection in images and videos. It classifies methods into traditional
  and deep learning-based detectors, further subcategorizing the latter into one-stage
  and two-stage detectors.
---

# A comprehensive overview of deep learning models for object detection from videos/images

## Quick Facts
- **arXiv ID**: 2601.14677
- **Source URL**: https://arxiv.org/abs/2601.14677
- **Reference count**: 8
- **Primary result**: Comprehensive review of deep learning object detection methods for images and videos, covering architectures, datasets, metrics, and future research directions

## Executive Summary
This paper provides a comprehensive overview of deep learning models for object detection in images and videos. It systematically classifies detection methods into traditional and deep learning-based approaches, with deep learning further divided into one-stage and two-stage detectors. The review covers major CNN architectures like YOLO, SSD, and RetinaNet for images, as well as two-stage methods such as Faster R-CNN and Mask R-CNN. For video object detection, it explores flow-based, tracking-based, attention-based, and LSTM-based approaches that leverage temporal information. The paper also examines benchmark datasets, preprocessing techniques, and performance metrics, highlighting challenges such as real-time detection, occlusions, and motion blur.

## Method Summary
The review provides a structured taxonomy of object detection methods, categorizing them into traditional (HOG, SIFT) and deep learning-based approaches. For deep learning, it distinguishes between one-stage detectors (YOLO, SSD, RetinaNet) that perform detection in a single forward pass, and two-stage detectors (Faster R-CNN, Mask R-CNN) that first propose regions of interest before classification. For video detection, it covers flow-based methods using optical flow (FGFA), tracking-based methods (D&T), and attention-based methods (SELSA). The review analyzes these methods using benchmark datasets like ImageNet VID, MS COCO, and Pascal VOC, evaluating them on metrics such as mAP and FPS.

## Key Results
- One-stage detectors (YOLO, SSD) offer faster inference but lower accuracy compared to two-stage detectors (Faster R-CNN, Mask R-CNN)
- Video detection methods that incorporate temporal information (flow-based, tracking-based, attention-based) show improved robustness to motion blur and occlusions
- Memory-based and attention-based video detectors achieve higher mAP (up to 85.4%) compared to LSTM-based approaches due to better long-term dependency handling
- Real-time object detection faces challenges including small object detection, scale variation, and computational efficiency trade-offs

## Why This Works (Mechanism)

### Mechanism 1
Decoupling region proposal from classification improves localization accuracy at the cost of inference speed. Two-stage detectors (e.g., Faster R-CNN) first identify sparse regions of interest (RoIs) using a Region Proposal Network (RPN), effectively filtering background noise before applying expensive classification and bounding-box regression. This concentrates computational capacity on likely object regions.

Core assumption: Objects occupy sparse, localized regions in an image, and background regions can be discarded early without losing critical information.

Evidence anchors:
- [Section 4.2.2.3] Describes how RPN generates bounding boxes on feature maps using sliding windows and anchor boxes, feeding them to fully convolution layers
- [Section 4.2.1] Contrasts this with one-stage detectors that perform detection "without any intermediate tasks," leading to faster but potentially less accurate results
- [Corpus] Corpus signals (e.g., "Lightweight Multi-Frame Integration") emphasize efficiency trade-offs, implicitly supporting the need for structured region handling in complex scenes

Break condition: If objects are extremely dense or small (e.g., crowd counting), RPNs may generate excessive false positives or fail to propose valid regions, causing accuracy to drop.

### Mechanism 2
Temporal feature aggregation mitigates information loss in individual video frames caused by motion blur or occlusion. Video detectors (e.g., FGFA, STSN) use optical flow or deformable convolutions to align and aggregate feature maps from neighboring frames onto the current frame. This "averages" the representation of an object over time, effectively denoising the signal for a specific frame.

Core assumption: Temporal coherence exists between video frames; motion patterns are consistent enough to be modeled by flow networks.

Evidence anchors:
- [Section 5.1.2] Explains that FGFA warps feature maps of nearby frames to the current frame using motion information to improve the feature map of the current frame
- [Section 5.3.1] Notes that STSN uses deformable convolution to align feature maps between neighboring frames without explicit optical flow, addressing issues like motion blur and defocus
- [Corpus] The neighbor paper "Lightweight Multi-Frame Integration" discusses integrating temporal context, reinforcing the validity of multi-frame analysis

Break condition: If the video contains rapid, non-linear motion or shot changes (cuts), optical flow estimation fails, leading to "ghosting" or corrupted feature aggregation.

### Mechanism 3
Contextual memory modules enable long-term dependency tracking that standard recurrent units struggle to maintain. Architectures like STMN (Spatial-Temporal Memory Network) utilize dedicated memory components (STMM) to store and retrieve features across extended time steps. Unlike standard LSTMs which may suffer from state decay, these modules allow the network to reference specific features from distant frames to resolve ambiguities in the present frame.

Core assumption: Object identities persist over long sequences, and retrieving historical context is necessary to resolve temporary occlusions.

Evidence anchors:
- [Section 5.3.2] Describes STMM as a unit that receives feature maps from the current frame and information from previous frames, allowing for "bidirectional feature aggregation"
- [Section 6] Notes that LSTM-based methods often struggle with long-term dependency due to sigmoid saturation, whereas memory/attention-based methods (e.g., MEGA, SELSA) achieve higher mAP (up to 85.4%)
- [Corpus] Specific architectural details for memory modules are sparse in the provided corpus summaries, which focus more on general YOLO evolution and surveillance applications

Break condition: If memory retrieval is not properly gated or indexed, the system may retrieve irrelevant historical features, introducing noise rather than signal.

## Foundational Learning

**Region of Interest (RoI) Pooling / Alignment**
- Why needed here: Crucial for understanding Two-Stage detectors. It is the operation that rescales features from variable-sized proposed regions into fixed-size feature maps for classification
- Quick check question: How does a network handle a region proposal of 50x50 pixels differently from one of 200x200 pixels?

**Optical Flow**
- Why needed here: Essential for video object detection (Flow-based methods). It represents the vector field of pixel motion between frames, which is the basis for feature warping
- Quick check question: In the context of DFF (Deep Feature Flow), why is optical flow computed between a keyframe and a non-keyframe?

**Non-Maximum Suppression (NMS)**
- Why needed here: A post-processing mechanism required for almost all detectors to merge multiple overlapping bounding boxes into a single prediction
- Quick check question: If two boxes predict the same object with scores of 0.9 and 0.8, and an IoU threshold of 0.5, which box survives NMS?

## Architecture Onboarding

**Component map:**
- **Backbone:** (e.g., ResNet-101, Darknet-53) Extracts hierarchical feature maps
- **Neck:** (e.g., FPN, STMM) Aggregates features across scales (for multi-scale objects) or time (for video)
- **Head:** (e.g., RPN, YOLO head) Performs the final dense prediction (class probability + bounding box coordinates)

**Critical path:** Input Frame -> Backbone Feature Extraction -> (Video: Temporal Aggregation/Flow Warping) -> Proposal Generation (Two-stage only) -> Classification & Regression -> NMS -> Output

**Design tradeoffs:**
- **Speed vs. Accuracy:** YOLO/SSD prioritize low latency; Faster R-CNN/Mask R-CNN prioritize precision
- **Image vs. Video:** Image models treat frames independently (fast but jittery); Video models add temporal fusion (robust but computationally heavy)

**Failure signatures:**
- **Occlusion:** Image models fail completely; video models with memory/attention may recover identity
- **Small Objects:** One-stage detectors often miss small objects due to downsampling in the backbone; FPN or multi-scale training is required
- **Class Imbalance:** One-stage detectors suffer from background-foreground imbalance; RetinaNet's Focal Loss is the specific counter-measure

**First 3 experiments:**
1. **Static Baseline:** Train a Faster R-CNN (ResNet-50) and a YOLOv5s model on the Pascal VOC dataset. Compare mAP and FPS to validate the paper's reported trade-off
2. **Temporal Robustness Test:** Run a pre-trained image detector on the ImageNet VID validation set. Introduce synthetic motion blur. Measure the drop in mAP compared to clean frames
3. **Flow Ablation:** Implement a simple Feature Propagation module. Use FlowNet to warp features from a keyframe (t) to a subsequent frame (t+5). Compare the detection consistency (jitter) against per-frame detection

## Open Questions the Paper Calls Out

**Open Question 1**
- **Question:** How can deep learning models better exploit spatial and temporal correlations in video data to enhance real-time object detection accuracy?
- **Basis in paper:** [explicit] In Section 6.3, the authors state that enhancing detection performance by exploring spatial and temporal correlations is an area that needs further development, as traditional detectors often ignore these relationships
- **Why unresolved:** Current methods like LSTM-based approaches suffer from delayed state decay and struggle to maintain long-term dependencies due to sigmoid activation saturation, while tracking-based methods are prone to error propagation from the tracker to the detector
- **What evidence would resolve it:** A unified architecture that successfully models long-term temporal dependencies without the gradient decay issues of RNNs, demonstrating consistent accuracy improvements in dynamic environments (e.g., ImageNet VID) without compromising real-time frame rates

**Open Question 2**
- **Question:** What architectural modifications are necessary to improve the localisation and classification of small objects in complex environments?
- **Basis in paper:** [explicit] The authors explicitly list "Scale and size alteration" as a future direction in Section 6.3, noting that small objects exhibit significant variations and that existing detectors struggle with occlusions and lighting variations
- **Why unresolved:** Current feature pyramids and single-shot detectors often lose fine-grained details required for small objects during downsampling or feature aggregation, leading to lower precision for small instances compared to large ones
- **What evidence would resolve it:** A model that utilizes advanced multi-scale feature fusion or super-resolution techniques to achieve a statistically significant increase in Average Precision (AP) specifically for small objects (e.g., on MS COCO) while maintaining overall detection speed

**Open Question 3**
- **Question:** How can lightweight models be optimized to balance the trade-off between computational efficiency and detection accuracy?
- **Basis in paper:** [explicit] Section 6.3 identifies "Lightweight classification or detection" and "Model optimisation" as key future areas, emphasizing the continued struggle to balance speed, memory usage, and accuracy
- **Why unresolved:** While lightweight architectures like SqueezeDet or MobileNet-based detectors exist, they are frequently plagued by higher categorization errors and lower mAP compared to heavier, two-stage detectors
- **What evidence would resolve it:** The development of a streamlined network that achieves high mAP (competitive with ResNet-based backbones) on benchmark datasets while operating at high FPS on resource-constrained hardware (e.g., mobile devices or embedded systems)

## Limitations
- Incomplete architectural details for memory-based video detectors, particularly STMN and STMM modules
- Comparison of inference speed across different hardware platforms lacks standardization
- Does not address the impact of varying input resolutions on detection accuracy

## Confidence
- **High confidence**: Classification accuracy comparisons between one-stage and two-stage detectors (mAP values are consistently reported across benchmarks)
- **Medium confidence**: Video detection mechanisms (flow-based, tracking-based, and attention-based approaches are well-documented with specific examples)
- **Low confidence**: Temporal memory module implementations (architectural details are sparse and implementation-specific information is missing)

## Next Checks
1. **Architecture Verification**: Implement the STMN memory module based on the description in Section 5.3.2 and validate its feature aggregation behavior on a simple video sequence with known occlusions
2. **Hardware Standardization**: Reproduce YOLOv5s and Faster R-CNN ResNet-50 inference on identical hardware (specific GPU model, batch size) to verify the reported FPS trade-off claims
3. **Scale Sensitivity Test**: Systematically vary input resolutions (320x320 to 640x640) for both YOLO and Faster R-CNN models on MS COCO to quantify resolution impact on detection accuracy