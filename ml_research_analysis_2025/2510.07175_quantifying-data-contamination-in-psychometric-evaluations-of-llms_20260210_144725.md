---
ver: rpa2
title: Quantifying Data Contamination in Psychometric Evaluations of LLMs
arxiv_id: '2510.07175'
source_url: https://arxiv.org/abs/2510.07175
tags:
- item
- score
- contamination
- memorization
- psychometric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a framework to systematically measure data
  contamination in psychometric evaluations of large language models (LLMs). The framework
  evaluates three aspects: item memorization (direct exposure to psychometric items),
  evaluation memorization (familiarity with scoring protocols), and target score matching
  (ability to adjust responses to achieve specific scores).'
---

# Quantifying Data Contamination in Psychometric Evaluations of LLMs

## Quick Facts
- arXiv ID: 2510.07175
- Source URL: https://arxiv.org/abs/2510.07175
- Reference count: 18
- Models memorize psychometric items and scoring schemes, enabling strategic response generation

## Executive Summary
This paper introduces a systematic framework to measure data contamination in psychometric evaluations of large language models (LLMs). The framework evaluates three aspects: item memorization (direct exposure to psychometric items), evaluation memorization (familiarity with scoring protocols), and target score matching (ability to adjust responses to achieve specific scores). Applied to 21 models across four widely used inventories, the study finds strong contamination patterns: models not only memorize items but also understand scoring schemes and can strategically generate responses to match target scores. Advanced models like GPT-5, Claude, and GLM show the highest contamination, with item-dimension mapping F1-scores averaging 0.94 and target score matching MAEs around 0.1-0.2. The findings provide the first systematic evidence that psychometric evaluations of LLMs may suffer from data contamination, calling for contamination-aware evaluation practices.

## Method Summary
The study evaluates data contamination in psychometric assessments using a three-pronged framework: semantic memorization (cosine similarity between original and generated items), key information memorization (keyword masking and generation), evaluation memorization (item-dimension and option-score mapping), and target score matching (ability to generate responses matching target scores). Four inventories were tested (BFI-44, PVQ-40, MFQ, SD-3) across 21 models from major LLM families. The evaluation uses six prompt templates, with semantic similarity computed using text-embedding-3-large. Keyword annotations were created by psychology experts, and refusal responses were filtered using GPT-5.2-mini. All metrics are averaged over three runs with 95% confidence intervals at temperature 0.7.

## Key Results
- Item-dimension mapping F1-scores averaged 0.94 across models, indicating strong contamination
- Target score matching MAEs ranged from 0.1-0.2, showing models can strategically generate responses
- BFI-44 and PVQ-40 showed stronger contamination than MFQ and SD-3, likely due to greater academic visibility
- GPT-5, Claude, and GLM exhibited the highest contamination levels among evaluated models

## Why This Works (Mechanism)
The framework works by systematically measuring three distinct types of contamination: direct item memorization, understanding of evaluation protocols, and strategic response generation. By using multiple complementary metrics (semantic similarity, keyword generation, mapping accuracy, and score matching), the study captures both explicit memorization and implicit contamination effects. The use of multiple inventories and model families provides robust evidence across different psychometric assessments and model architectures.

## Foundational Learning
- **Item memorization**: Models retain direct exposure to psychometric items through training. *Why needed*: Baseline contamination measurement. *Quick check*: Semantic similarity between original and generated items should exceed random baseline.
- **Evaluation memorization**: Models learn scoring schemes and item-dimension mappings. *Why needed*: Captures protocol contamination beyond item-level. *Quick check*: Mapping F1-scores should correlate with model training data exposure.
- **Target score matching**: Models can strategically generate responses to achieve specific scores. *Why needed*: Reveals contamination's practical impact on evaluation validity. *Quick check*: MAE between target and achieved scores should decrease with model capability.

## Architecture Onboarding

**Component Map**: Keyword annotation -> Item generation -> Semantic similarity -> Keyword generation -> Mapping evaluation -> Score matching

**Critical Path**: The semantic memorization and keyword generation components form the critical path for contamination detection, as they directly measure item-level memorization that enables downstream effects.

**Design Tradeoffs**: The framework trades evaluation speed for comprehensiveness by using multiple metrics and inventories. While this provides robust contamination detection, it requires significant computational resources and careful prompt engineering.

**Failure Signatures**: High refusal rates (e.g., Claude 3.5 Sonnet: 98.5% on BFI-44) indicate model-specific filtering of inventory-related queries. Inconsistent keyword generation across runs suggests instability in key information capture.

**First Experiments**:
1. Test semantic memorization on a single inventory with one model to validate embedding-based similarity computation
2. Evaluate keyword generation consistency across multiple runs for a subset of items
3. Measure item-dimension mapping accuracy for a simple inventory to validate prompt template effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- The framework's reliance on GPT-5.2-mini for refusal detection introduces uncertainty, as this model variant may not be publicly available
- Keyword-based key information memorization may not capture all item uniqueness features, particularly for complex psychometric items
- The study's focus on four specific inventories limits generalizability to other psychometric assessments and cultural contexts

## Confidence

**High confidence in**: The overall framework for measuring data contamination across three distinct aspects (item memorization, evaluation memorization, target score matching) is methodologically sound and well-specified.

**Medium confidence in**: The quantitative contamination measurements (F1-scores averaging 0.94, MAEs around 0.1-0.2) are likely accurate within reasonable bounds, though exact values depend on model availability and refusal filtering accuracy.

**Low confidence in**: The comparative contamination rankings between inventories (BFI-44/PVQ-40 vs MFQ/SD-3) may be influenced by factors beyond academic visibility, such as item complexity or cultural specificity.

## Next Checks

1. Validate keyword annotations by having independent psychology experts annotate a subset of items and compare agreement rates with the original annotations (Fleiss' Kappa 0.75-0.92 reported)

2. Test semantic memorization using multiple embedding models (e.g., compare text-embedding-3-large with other sentence transformers) to assess metric robustness

3. Replicate contamination measurements using a different refusal detection approach (e.g., heuristic keyword filtering or human annotation) to verify the impact of the GPT-5.2-mini assumption