---
ver: rpa2
title: Accurate Target Privacy Preserving Federated Learning Balancing Fairness and
  Utility
arxiv_id: '2510.26841'
source_url: https://arxiv.org/abs/2510.26841
tags:
- fairness
- privacy
- constraints
- learning
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FedPF, a differentially private fair federated
  learning algorithm that formulates privacy-fairness-utility optimization as a zero-sum
  game. The algorithm uses Lagrangian duality to balance fairness constraints with
  privacy protection through differential privacy mechanisms, specifically targeting
  sensitive attribute protection in distributed settings.
---

# Accurate Target Privacy Preserving Federated Learning Balancing Fairness and Utility

## Quick Facts
- arXiv ID: 2510.26841
- Source URL: https://arxiv.org/abs/2510.26841
- Reference count: 30
- Primary result: Achieves up to 42.9% discrimination reduction across three datasets while maintaining competitive accuracy through privacy-fairness-utility optimization

## Executive Summary
This paper introduces FedPF, a differentially private fair federated learning algorithm that formulates the privacy-fairness-utility optimization as a zero-sum game. The algorithm uses Lagrangian duality to balance fairness constraints with privacy protection through differential privacy mechanisms, specifically targeting sensitive attribute protection in distributed settings. Theoretical analysis reveals an inverse relationship between privacy and fairness, showing that stricter privacy protection limits the system's ability to detect and correct demographic biases. Experiments demonstrate significant discrimination reduction while maintaining competitive accuracy, confirming that achieving both privacy and fairness requires careful balancing rather than optimizing either in isolation.

## Method Summary
FedPF formulates privacy-fairness-utility optimization as a zero-sum game where a Learner (client) minimizes prediction error and an Auditor (server) maximizes dual variables to enforce fairness constraints. The algorithm applies attribute-level perturbation using the Exponential Mechanism on sensitive attributes before training, allowing for privacy preservation while theoretically enabling the tracking of bias detection degradation. The system converges to a solution where fairness constraints are satisfied without arbitrarily sacrificing utility through iterative updates between minimizing the Lagrangian and maximizing the penalty. The approach targets binary classification tasks using three tabular datasets (Adult, Bank, Compas) with 5 clients, 200 global rounds, and a 3-layer feedforward neural network architecture.

## Key Results
- Achieves up to 42.9% discrimination reduction across Adult, Bank, and Compas datasets
- Demonstrates non-monotonic utility curve showing moderate fairness constraints initially improve generalization
- Reveals inverse relationship between privacy and fairness detection capability through theoretical error bounds

## Why This Works (Mechanism)

### Mechanism 1: Lagrangian Duality for Fairness-Utility Equilibrium
The system decouples optimization into two players: a Learner that minimizes prediction error and an Auditor that maximizes dual variables to penalize fairness violations. By iterating between minimizing the Lagrangian (learner) and maximizing the penalty (auditor), the model converges to a solution where fairness constraints are satisfied without arbitrarily sacrificing utility. The loss function is convex, and the hypothesis space is compact enough for strong duality to hold.

### Mechanism 2: Attribute-Level Perturbation
The algorithm perturbs sensitive attributes directly using the Exponential Mechanism, applying probability distributions scaled by privacy budget ε_p. This allows the server to receive "noisy" group statistics while explicitly revealing how noise obscures demographic patterns required for fairness detection. The sensitive attributes are categorical or can be discretized for the Exponential Mechanism to apply.

### Mechanism 3: Fairness as Regularization
Moderate fairness constraints act as a regularizer against overfitting to majority groups, creating a non-monotonic utility curve. By constraining the model to treat demographic groups similarly, the algorithm prevents learning spurious correlations specific to the majority group. Theoretically, this reduces generalization error up to a point, after which strict constraints degrade performance due to underfitting.

## Foundational Learning

- **Lagrangian Duality & Min-Max Optimization**
  - Why needed here: Constraints (fairness) can be moved into the objective function via Lagrange multipliers, turning a constrained problem into an unconstrained "game" between two objectives.
  - Quick check question: Can you explain why the "Auditor" wants to maximize the Lagrangian while the "Learner" wants to minimize it?

- **Differential Privacy (Exponential Mechanism)**
  - Why needed here: Unlike standard Gaussian noise injection, the Exponential Mechanism perturbs data based on utility scores. FedPF uses this to flip sensitive attributes with calibrated probability.
  - Quick check question: How does the parameter ε_p control the probability of retaining the true sensitive attribute value vs. flipping it?

- **Fairness Metrics (Demographic Parity vs. Equalized Odds)**
  - Why needed here: The system optimizes for specific definitions of fairness. Understanding the difference is critical for selecting the correct "cost-sensitive" loss formulation.
  - Quick check question: Does Equalized Odds require the True Positive Rate to be equal across groups, or just the overall positive rate?

## Architecture Onboarding

- **Component map:** Client Node (holds D_i = {(x, a, y)}; runs Local Trainer and Privacy Module) -> Server Node (runs Auditor and Aggregator)

- **Critical path:** 
  1. Client perturbs sensitive attribute a → ã (attribute-level perturbation)
  2. Client computes "cost-sensitive" classification error based on current λ
  3. Client updates local model θ_i
  4. Server aggregates models θ_G
  5. Server updates dual variables λ to penalize residual unfairness

- **Design tradeoffs:**
  - Privacy vs. Fairness Detection: Lower ε_p increases variance of group statistics, reducing Auditor's ability to detect bias
  - Utility vs. Fairness: There is a "sweet spot" for ε_f; too strict causes underfitting, too loose causes overfitting/discrimination

- **Failure signatures:**
  - Stagnant Unfairness: G_ya fails to decrease over rounds (privacy noise too high)
  - Utility Collapse: Loss increases sharply in later rounds (fairness constraint too strict)
  - Divergence: λ grows unbounded (optimization instability)

- **First 3 experiments:**
  1. Baseline Fairness: Run with ε_p → ∞ (no privacy) and varying ε_f to reproduce non-monotonic utility curve
  2. Privacy Isolation: Run with ε_f → ∞ (no fairness) and varying ε_p to measure raw utility loss from noise
  3. Joint Stress Test: Set strict privacy (ε_p=0.1) and strict fairness (ε_f=0.01) to observe "Privacy-Fairness Coupling" error bound

## Open Questions the Paper Calls Out

- **How do non-IID data distributions across clients impact the theoretical error bounds and convergence guarantees of the FedPF algorithm?**
  - Basis: Section IV-C explicitly assumes client datasets are "independently and identically distributed (i.i.d.)" to derive the error bounds in Theorem 1
  - Why unresolved: Federated learning systems typically involve heterogeneous data silos where data distributions vary significantly between clients
  - What evidence would resolve it: A theoretical extension of Theorem 1 that incorporates distribution divergence metrics between clients

- **Can the framework be extended to train personalized, client-specific fairness models rather than enforcing a single global fairness standard?**
  - Basis: Section II-A states that "training separate fairness models for each client remains an open problem"
  - Why unresolved: The current FedPF formulation uses a global Lagrangian multiplier to enforce fairness universally
  - What evidence would resolve it: A modification of the game-theoretic formulation allowing for local dual variables λ_i to optimize for client-specific fairness constraints

- **How does the privacy-fairness-utility tradeoff change when extending the algorithm from binary to multi-class classification tasks?**
  - Basis: Section IV-B explicitly restricts the formulation: "In this paper, we take binary classification as an example"
  - Why unresolved: The complexity of fairness constraints grows combinatorially with the number of classes and sensitive attribute groups
  - What evidence would resolve it: Deriving the cost terms c_j for multi-class settings and evaluating the resulting discrimination reduction and utility loss

## Limitations
- The theoretical analysis assumes i.i.d. client data distributions, which may not hold in real federated learning deployments
- Attribute-level perturbation assumes sensitive attributes are accessible and meaningful to perturb, limiting applicability when data remains private to clients
- The min-max optimization stability is not formally proven for non-convex neural network settings, relying on empirical convergence

## Confidence
- **High confidence:** The inverse relationship between privacy and fairness detection capability is well-grounded in differential privacy theory and supported by experimental results
- **Medium confidence:** The non-monotonic utility curve under fairness constraints is theoretically plausible and observed in experiments, but the "sweet spot" appears dataset-dependent
- **Low confidence:** The attribute perturbation mechanism's superiority over gradient-based DP methods is not directly compared in the experiments

## Next Checks
1. Test the algorithm on a dataset where sensitive attributes are continuous rather than categorical to evaluate the robustness of the Exponential Mechanism perturbation approach
2. Implement and compare against a gradient-based DP method (DP-SGD) to quantify the trade-offs between attribute-level and model-level privacy protection
3. Evaluate the algorithm's behavior under non-IID data distributions where different clients have systematically different demographic compositions to stress-test the fairness enforcement mechanism