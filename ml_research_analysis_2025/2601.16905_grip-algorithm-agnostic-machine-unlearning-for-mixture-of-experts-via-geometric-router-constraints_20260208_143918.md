---
ver: rpa2
title: 'GRIP: Algorithm-Agnostic Machine Unlearning for Mixture-of-Experts via Geometric
  Router Constraints'
arxiv_id: '2601.16905'
source_url: https://arxiv.org/abs/2601.16905
tags:
- unlearning
- routing
- expert
- router
- stability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of machine unlearning in Mixture-of-Experts
  (MoE) models, where existing methods fail by manipulating routers to bypass knowledgeable
  experts rather than erasing information. The authors propose GRIP, an algorithm-agnostic
  framework that enforces geometric constraints by projecting router gradient updates
  into expert-specific null spaces.
---

# GRIP: Algorithm-Agnostic Machine Unlearning for Mixture-of-Experts via Geometric Router Constraints

## Quick Facts
- arXiv ID: 2601.16905
- Source URL: https://arxiv.org/abs/2601.16905
- Reference count: 40
- This paper proposes GRIP, a geometric constraint framework that prevents router manipulation during MoE unlearning, restoring routing stability from 0.21 to over 0.94 while improving retain accuracy by 85% and reducing adversarial knowledge recovery from 61% to 3%.

## Executive Summary
GRIP addresses a critical vulnerability in Mixture-of-Experts unlearning where traditional methods manipulate router parameters to bypass knowledgeable experts rather than erasing knowledge. The authors identify that unlearning objectives can be minimized either by genuinely erasing knowledge from expert weights or trivially altering routers to bypass them, with optimizers preferentially choosing the latter. GRIP enforces geometric constraints by projecting router gradient updates into expert-specific null spaces, decoupling routing stability from parameter plasticity and forcing optimization to erase knowledge from expert parameters rather than exploiting router manipulation.

## Method Summary
GRIP implements geometric constraints through two approaches: Training-Time Enforcement (TTE) and Post-Training Correction (PTC). TTE applies equality and inequality constraints during optimization using projected gradient descent with Randomized Kaczmarz algorithm, guaranteeing routing preservation throughout unlearning. PTC provides a computationally efficient alternative by caching retain-set representations X_r,ℓ before unlearning, then computing an analytical correction: ∆Θ_ℓ = Θ_ℓ(X_r,ℓ - X'_r,ℓ)X'^†_r,ℓ using the least-squares solution to restore routing patterns post-unlearning. The framework decomposes global null-space constraints into per-expert asymmetric constraints, significantly expanding the feasible optimization landscape while preserving routing integrity.

## Key Results
- Routing stability restored from 0.21 to over 0.94 across all methods
- Retain accuracy improved by over 85% compared to baseline unlearning
- Adversarial knowledge recovery reduced from 61% to 3%
- Post-Training Correction achieves near-perfect stability with only 1.21x training time overhead

## Why This Works (Mechanism)

### Mechanism 1: Router Manipulation as Optimization Shortcut
Standard gradient-based unlearning methods exploit MoE routers as a cheap shortcut, redirecting queries away from knowledgeable experts rather than erasing knowledge from expert parameters. The unlearning loss can be minimized via two paths: (1) genuine parameter erasure in experts, or (2) trivial router modification that bypasses them. Optimizers preferentially converge to path (2) because it's structurally easier—small router shifts cascade through layers, altering input manifolds for deeper routers. Routing patterns encode semantic structure; disrupting them damages model utility independent of knowledge erasure.

### Mechanism 2: Null-Space Projection Preserves Routing Decisions
Projecting router gradient updates into the null space of retain-set representations guarantees routing decisions remain unchanged while allowing parameter updates in orthogonal directions. For retain representations X_r,ℓ, the null space N(X_r,ℓ) = {∆Θ : ∆ΘX_r,ℓ = 0} defines all parameter updates that produce zero score change. Since router's hidden dimension d typically exceeds retain-set effective rank, this null space remains high-dimensional, preserving optimization capacity. The retain-set representations approximately span the subspace that governs routing decisions for knowledge worth preserving.

### Mechanism 3: Expert-Specific Asymmetric Constraints
Decomposing global null-space constraints into per-expert equality/inequality constraints recovers near-full gradient space while guaranteeing routing preservation. For each expert j, apply hard equality constraints (∆Θ_j · x = 0) only for inputs that selected expert j; apply relaxed inequality constraints (∆Θ_j · x ≤ margin) for non-selected inputs. This permits benign modifications that don't threaten top-k boundaries. Top-k selection integrity, not absolute score values, determines routing behavior. This expert-specific decomposition significantly expands the feasible optimization landscape compared to global constraints.

## Foundational Learning

- **Mixture-of-Experts Routing Dynamics**: GRIP's entire contribution hinges on understanding how router parameters Θ control expert selection via top-k scoring, and why disrupting this damages model utility. Quick check: Given router scores s = Θx for input x with top-2 selection over 4 experts [s₁=0.8, s₂=0.6, s₃=0.4, s₄=0.2], which experts are selected and what happens if s₃ increases to 0.7?

- **Null Space and Projection**: The core technical mechanism requires computing projectors onto representation null spaces; misunderstanding this leads to incorrect implementations. Quick check: For matrix X ∈ ℝ^(3×2) with rank 1, what is the dimension of its null space, and how would you compute a projector onto it?

- **Projected Gradient Descent**: Training-time enforcement uses PGD with equality and inequality constraints; the Randomized Kaczmarz algorithm is non-obvious. Quick check: Why does cyclic projection through constraint half-spaces converge slower than randomized projection with probability proportional to constraint norm?

## Architecture Onboarding

- **Component map**: Input → [Retain Set Representations X_r,ℓ] → Eigendecomposition → Null-space basis Û_ℓ → [Router Θ_ℓ] ← Gradient ∇L ← Unlearning Objective → Projection: ∇̃ = P_N · ∇ (Training-Time) OR Correction: ∆Θ = Θ(X - X')X'† (Post-Training) → [Expert Parameters] → Knowledge Erasure

- **Critical path**: 
  1. Collect retain-set representations X_r,ℓ for all layers before unlearning (one forward pass)
  2. Compute eigendecomposition of covariance X_r,ℓX_r,ℓ^T per layer (O(Ld³) upfront cost)
  3. Apply constraints during unlearning OR correct post-hoc via Eq. (6)

- **Design tradeoffs**:
  - **Training-Time Enforcement (TTE)**: Guarantees constraints throughout but O(K·L·d³) cost (1.67× overhead); handles representation drift during training
  - **Post-Training Correction (PTC)**: O(L·d³) one-time cost (1.21× overhead); requires caching X_r,ℓ; assumes representation drift is correctable analytically
  - **Full null-space vs Expert-specific**: Full is simpler but over-constrains (FA=0.38); Expert-specific preserves unlearning efficacy (FA=0.24)

- **Failure signatures**:
  - RS ≈ 0.2-0.4: Router manipulation occurring; knowledge not truly erased (check adversarial recovery)
  - FA > 0.35 with high RS: Null-space threshold ϵ too restrictive; switch from 10⁻⁴ to 10⁻²
  - Empty null-spaces in later layers: Threshold too strict; increase ϵ or verify retain-set diversity
  - PTC fails to restore routing: Representation drift too large; fall back to TTE

- **First 3 experiments**:
  1. Validate the problem: Run standard NPO unlearning on MoE, measure RS (expect ~0.4), then perform expert-forcing attack (expect ~60% recovery vs ~25% on dense baseline)
  2. Test PTC overhead: Compare training time of PTC vs unconstrained baseline on 1B-parameter MoE; verify <25% overhead with RS > 0.95
  3. Ablate constraint granularity: Compare full null-space vs expert-specific constraints on WMDP-Cyber; expect FA gap of 0.14+ with identical RS

## Open Questions the Paper Calls Out

### Open Question 1
Can approximate projection techniques, such as randomized SVD or sketching methods, maintain constraint satisfaction while scaling geometric unlearning to models with thousands of experts? The authors state in Future Work that "Computational scalability remains a key challenge," specifically suggesting "approximate projection techniques with sublinear complexity" to enable performance on models with thousands of experts. This remains unresolved because the current method relies on eigendecomposition with O(d³) complexity, which becomes prohibitive for extremely high-dimensional routers found in massive MoE models. Empirical results showing that approximate solvers achieve comparable Routing Stability (RS > 0.94) and Forget Accuracy on MoE architectures with significantly larger expert counts (e.g., >1000 experts) than tested in the paper would resolve this question.

### Open Question 2
How can constraint mechanisms be adapted to selectively modify problematic routing behaviors—such as those causing bias—rather than preserving all retain-set routing patterns? The Future Work section proposes "Adaptive constraint mechanisms that distinguish beneficial routing patterns from biased ones," suggesting that the current equality constraints could be replaced with "fairness-aware" inequality constraints. This remains unresolved because GRIP currently assumes the retain set's routing is sacrosanct; it lacks the capacity to identify and correct harmful or biased routing patterns that exist in the original model. A modified GRIP framework that successfully alters biased routing distributions while maintaining general model utility and unlearning efficacy would resolve this question.

### Open Question 3
What validation mechanisms can effectively audit unlearning success in models treated with geometric constraints, given that the stabilization of routing patterns masks behavioral changes? The Limitations section highlights the "Verification Dilemma," noting that because GRIP makes unlearning "undetectable through behavioral observation," providers cannot audit success via standard routing analysis. This remains unresolved because by locking routing patterns to prevent adversarial extraction, the method inadvertently removes a potential side-channel for verifying that specific knowledge has been removed. The development and validation of an alternative auditing protocol (e.g., specific membership inference attacks or parameter inspection techniques) that reliably detects unlearned information in GRIP-treated models would resolve this question.

## Limitations

- The core assumption that router manipulation constitutes a fundamental failure mode for MoE unlearning lacks empirical validation in non-MoE contexts and may be specific to sparse architectures where routing decisions are separable from expert computation.
- The representation caching requirement assumes training-time stability of the retain set's embedding space, which may not hold across domain shifts or during extended fine-tuning.
- The Post-Training Correction's assumption that representation drift is analytically correctable may not generalize to catastrophic forgetting scenarios or when retain-set representations undergo nonlinear transformations during unlearning.

## Confidence

**High confidence**: The geometric formulation (null-space projection) is mathematically sound given standard linear algebra assumptions; the experimental improvements in routing stability (0.21→0.94) and adversarial recovery (61%→3%) are well-documented.

**Medium confidence**: The claim that router manipulation is the primary failure mode for all existing unlearning methods assumes no alternative explanations (e.g., gradient saturation, optimizer bias) - the ablation studies control for some variables but don't exhaustively test competing hypotheses.

**Low confidence**: The Post-Training Correction's assumption that representation drift is analytically correctable may not generalize to catastrophic forgetting scenarios or when retain-set representations undergo nonlinear transformations during unlearning.

## Next Checks

1. Apply GRIP's PTC to a scenario where retain-set representations shift due to non-adversarial reasons (e.g., domain adaptation during unlearning) and measure whether analytical correction still restores routing.

2. After GRIP unlearning, perform a targeted attack that tries to recover erased knowledge by manipulating input embeddings rather than expert parameters, measuring whether routing stability alone suffices for security.

3. Implement GRIP on a 1B-parameter MoE with 64 experts and measure whether the O(Ld³) overhead remains practical and whether expert-specific constraints still outperform global constraints at smaller scales.