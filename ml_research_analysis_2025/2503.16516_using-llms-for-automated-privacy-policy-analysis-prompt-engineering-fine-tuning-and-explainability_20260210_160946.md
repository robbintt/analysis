---
ver: rpa2
title: 'Using LLMs for Automated Privacy Policy Analysis: Prompt Engineering, Fine-Tuning
  and Explainability'
arxiv_id: '2503.16516'
source_url: https://arxiv.org/abs/2503.16516
tags:
- privacy
- policy
- llms
- prompt
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a comprehensive evaluation of large language
  models (LLMs) for automated privacy policy analysis. It combines prompt engineering
  with LoRA fine-tuning to create classifiers that outperform existing methods significantly
  and consistently across four privacy policy corpora and taxonomies.
---

# Using LLMs for Automated Privacy Policy Analysis: Prompt Engineering, Fine-Tuning and Explainability

## Quick Facts
- arXiv ID: 2503.16516
- Source URL: https://arxiv.org/abs/2503.16516
- Reference count: 27
- Primary result: LoRA fine-tuning of LLMs significantly outperforms prompt engineering and baselines for privacy policy classification across multiple corpora

## Executive Summary
This paper presents a comprehensive evaluation of large language models for automated privacy policy analysis, combining prompt engineering with LoRA fine-tuning to create classifiers that outperform existing state-of-the-art methods. The study demonstrates superior performance across four privacy policy corpora and taxonomies, with fine-tuned Llama3-8B achieving macro F1 scores of 0.836 on OPP-115 and 0.618 on GoPPC-150. Additionally, the research evaluates LLM explainability using human-annotated metrics (completeness, logicality, comprehensibility), achieving scores exceeding 91.1%. The findings establish LLMs as powerful tools for automating privacy policy analysis while maintaining human-understandable explanations.

## Method Summary
The methodology combines prompt engineering and LoRA fine-tuning on four privacy policy corpora: OPP-115 (single-level, 12 categories), GoPPC-150 (hierarchical, English), CAPP-130 (Chinese), and APPCP-100 (Chinese). For classification, the study employs both zero/few-shot prompting (best with 1-shot Prompt 3) and LoRA fine-tuning on Llama3-8B, Qwen1.5-7B, and ChatGLM3-6B models. The hierarchical datasets use cascaded classification where Level-1 predictions condition Level-2 predictions. For explainability evaluation, 50-100 segments are annotated by 3 human raters on completeness, logicality, and comprehensibility scales. All experiments are conducted on an RTX 4090 GPU.

## Key Results
- LoRA fine-tuning produces significantly stronger privacy policy classifiers than prompt engineering alone (micro F1 0.916 vs 0.694 for Llama3-8B)
- Hierarchical cascading improves performance on multi-level taxonomies but shows degradation at finer granularity levels
- LLM explanations achieve high human evaluation scores (>91.1%) across completeness, logicality, and comprehensibility metrics
- Fine-tuned models consistently outperform established baselines like PrivBERT across all four corpora
- Smaller models (0.5B parameters) achieve ~90% of 7B performance post-fine-tuning, suggesting efficiency gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoRA fine-tuning produces stronger privacy policy classifiers than prompt engineering alone.
- Mechanism: LoRA updates a small subset of model weights, enabling the LLM to internalize domain-specific patterns from labeled privacy policy segments rather than relying solely on in-context learning.
- Core assumption: Privacy policy language contains specialized legal/technical patterns underrepresented in general pre-training corpora.
- Evidence anchors:
  - [abstract] "combining prompt engineering and LoRA fine-tuning can make LLM-based classifiers outperform other SOTA methods, significantly and consistently"
  - [Section 4.5, Table 3] Fine-tuned Llama3-8B achieved macro F1 0.836 vs baseline 0.830 on OPP-115; on GoPPC-150, 0.618 vs 0.529 (All nodes)
  - [corpus] Weak external validation; corpus neighbors do not replicate this specific privacy policy classification finding
- Break condition: If target domain already well-represented in pre-training, LoRA gains may diminish; prompt engineering may suffice.

### Mechanism 2
- Claim: Hierarchical (cascaded) classification improves performance on multi-level taxonomies.
- Mechanism: The model first predicts top-level nodes (e.g., CONTROLLER), then conditions on that prediction to predict second-level nodes (e.g., CONTROLLER.CONTACT), reducing label space complexity at each step.
- Core assumption: Hierarchical taxonomies reflect meaningful semantic structure in privacy policies.
- Evidence anchors:
  - [Section 3.1, Figure 2] Describes two-level prediction process: Y1 = H(X), then Y2 = H(X, Y1)
  - [Section 4.5, Table 3] Shows Level-1 macro F1 0.717 (Llama3-8B) vs All nodes 0.618 on GoPPC-150, suggesting difficulty increases at finer granularity
  - [corpus] No direct external replication of hierarchical cascading for privacy taxonomies found
- Break condition: If second-level categories lack sufficient training data or are not semantically distinct, cascading may propagate errors.

### Mechanism 3
- Claim: LLMs can generate human-comprehensible explanations for classification decisions when prompted with structured reasoning requests.
- Mechanism: By providing task description, category definitions, and output format instructions, the LLM produces natural-language justifications that humans rate highly on completeness and comprehensibility.
- Core assumption: Explanation quality can be reliably assessed via human annotation on completeness, logicality, and comprehensibility.
- Evidence anchors:
  - [abstract] "a score exceeding 91.1% was observed in our evaluation" across three explainability metrics
  - [Section 5, Table 6] LLM explanations scored 2.84/3 (completeness), 2.73/3 (logicality), 2.87/3 (comprehensibility); logicality slightly lower
  - [corpus] No corpus neighbor directly validates this explanation evaluation framework
- Break condition: If domain requires rigorous logical guarantees (e.g., legal defensibility), LLM explanations may lack sufficient logicality.

## Foundational Learning

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: Enables parameter-efficient fine-tuning of large models on modest hardware (RTX 4090 used).
  - Quick check question: Can you explain why LoRA reduces computational cost compared to full fine-tuning?

- Concept: Multi-label multi-class classification
  - Why needed here: Privacy policy segments can belong to multiple concept categories simultaneously (e.g., "First Party Collection/Use" AND "User Choice/Control").
  - Quick check question: How does evaluation differ between single-label and multi-label classification?

- Concept: Macro vs Micro F1 averaging
  - Why needed here: Class imbalance is common in privacy corpora; macro F1 treats all classes equally while micro F1 weights by frequency.
  - Quick check question: When would you prioritize macro F1 over micro F1 for this task?

## Architecture Onboarding

- Component map:
  Input: Privacy policy segments (paragraphs) -> Stage 1 (optional): Prompt engineering for zero/few-shot classification -> Stage 2 (recommended): LoRA fine-tuning on labeled corpus -> Stage 3: Hierarchical prediction (Level-1 → Level-2) -> Stage 4: Explanation generation via structured prompts -> Output: Concept labels + natural-language explanations

- Critical path:
  1. Select corpus and taxonomy (OPP-115 for single-level; GoPPC-150/APPCP-100 for hierarchical)
  2. Format training data for fine-tuning (reformat as query-response pairs)
  3. Apply LoRA fine-tuning (separate models for each hierarchy level, or multi-task merge)
  4. Evaluate on held-out test set using macro/micro F1
  5. Generate explanations and conduct human evaluation

- Design tradeoffs:
  - Prompt engineering vs fine-tuning: Prompts faster to iterate; fine-tuning yields significantly higher F1 (Section 4.8: micro F1 0.694 vs 0.916)
  - Single-task vs multi-task fine-tuning: Multi-task reduces deployment complexity with small performance drop (Table 4: 0.685 → 0.679 micro F1)
  - Model size: 0.5B models achieve ~90% of 7B performance post-fine-tuning (Figure 3); smaller models may suffice if resources constrained

- Failure signatures:
  - High recall / low precision on ambiguous categories (e.g., "Data Retention," "Data Security") due to hallucination
  - Poor performance on vaguely defined categories like "Introductory/Generic" and "Practice Not Covered"
  - CoT prompting underperformed expectations in prompt-only experiments (Table 1)

- First 3 experiments:
  1. Replicate prompt engineering baseline on OPP-115 with Llama3-8B-Instruct using Prompt 3 (1-shot); expect macro F1 ~0.55
  2. Apply LoRA fine-tuning on OPP-115; compare macro F1 against PrivBERT baseline (0.830)
  3. Generate explanations for 20-50 segments and have 2-3 annotators score on completeness/logicality/comprehensibility; check if scores approach reported >91% threshold

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does continual pre-training on domain-specific privacy policy corpora significantly improve LLM performance for privacy policy analysis compared to LoRA fine-tuning alone?
- Basis in paper: [explicit] Section 7 states, "The effectiveness of the continual pre-training paradigm remains an area for future research," noting the authors relied solely on fine-tuning to impart domain knowledge.
- Why unresolved: The authors did not implement continual pre-training, leaving the potential added value of unsupervised learning on domain-specific corpora before fine-tuning untested.
- What evidence would resolve it: A comparative study evaluating models that undergo continual pre-training followed by fine-tuning against the current LoRA-only baselines using the same privacy policy corpora.

### Open Question 2
- Question: Is the observed poor performance of prompt engineering in this study attributable to insufficient model exposure to privacy policy data or the absence of more advanced prompting techniques?
- Basis in paper: [explicit] Section 7 notes, "The performance achieved using prompt engineering in our experiments is quite poor" and explicitly calls for "follow-up research to clarify" if advanced methods are necessary or if LLMs lack sufficient pre-training data.
- Why unresolved: The experiments showed poor PE results, but the specific cause—whether the models simply haven't "seen" enough privacy policies or if the prompting strategies were insufficient—remains ambiguous.
- What evidence would resolve it: Experiments utilizing state-of-the-art complex prompting strategies (e.g., self-consistency) compared against tests on models specifically pre-trained on legal/privacy texts.

### Open Question 3
- Question: Does full-parameter fine-tuning offer performance gains over LoRA that justify its higher computational cost for privacy policy classification?
- Basis in paper: [explicit] Section 7 states that "further research is required to fully explore the potential of full-parameter fine-tuning" to determine if the differing underlying mechanisms provide advantages over LoRA.
- Why unresolved: The authors limited their methodology to LoRA due to resource constraints, assuming it approximates full fine-tuning, but did not empirically verify if full fine-tuning achieves a higher performance ceiling for this task.
- What evidence would resolve it: A direct comparison of macro/micro F1 scores between LoRA-adapted models and fully fine-tuned models across the OPP-115 and GoPPC-150 datasets.

## Limitations
- Human evaluation of explanations involved only 3 annotators scoring 50-100 segments, which may not provide statistically robust results for this safety-relevant component
- Hierarchical cascading shows performance degradation at Level-2 prediction (macro F1 drops from 0.717 to 0.618 on GoPPC-150), indicating error propagation through the hierarchy
- Minimal error analysis and inadequate assessment of failure modes for ambiguous categories or potential hallucinations in classification

## Confidence
- **High Confidence**: Classification performance improvements from LoRA fine-tuning over prompt engineering are well-supported by multiple experimental results across four different corpora, with statistically significant F1 score improvements
- **Medium Confidence**: Explainability scores (91.1% across three metrics) are based on limited human evaluation with 3 annotators, raising questions about statistical significance and potential bias
- **Low Confidence**: The error analysis section is minimal, and the study does not adequately address failure modes for ambiguous categories or potential hallucinations in classification

## Next Checks
1. **Scale Human Evaluation**: Expand the explanation evaluation to 10+ annotators rating 200+ segments, with inter-annotator agreement analysis to establish statistical significance of the >91% scores.

2. **Error Propagation Analysis**: Conduct ablation studies on the hierarchical approach by isolating Level-1 classifier performance and measuring how Level-1 errors propagate to Level-2 predictions across different error rates.

3. **Out-of-Distribution Testing**: Evaluate model performance on privacy policies from different jurisdictions or domains (e.g., healthcare vs general consumer services) to assess generalization beyond the training corpora.