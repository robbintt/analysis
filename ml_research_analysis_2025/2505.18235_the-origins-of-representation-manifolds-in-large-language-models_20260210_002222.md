---
ver: rpa2
title: The Origins of Representation Manifolds in Large Language Models
arxiv_id: '2505.18235'
source_url: https://arxiv.org/abs/2505.18235
tags:
- which
- representation
- representations
- feature
- hypothesis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a mathematical framework for understanding
  how features are represented as manifolds in large language models. The authors
  formalize features as metric spaces and propose that neural representations of these
  features live on manifolds that are homeomorphic to their corresponding metric spaces.
---

# The Origins of Representation Manifolds in Large Language Models

## Quick Facts
- arXiv ID: 2505.18235
- Source URL: https://arxiv.org/abs/2505.18235
- Reference count: 39
- This paper presents a mathematical framework for understanding how features are represented as manifolds in large language models, showing that features formalized as metric spaces are homeomorphic to their representation manifolds and that cosine similarity encodes intrinsic geodesic distance.

## Executive Summary
This paper proposes a formal mathematical framework for understanding how features are represented as manifolds in large language models. The authors formalize features as metric spaces and demonstrate that neural representations of these features live on manifolds homeomorphic to their corresponding metric spaces. A key theoretical result shows that under plausible hypotheses, cosine similarity in representation space encodes the intrinsic geometry of a feature through shortest, on-manifold paths, making the feature and its representation geometrically indistinguishable.

## Method Summary
The authors formalize features as metric spaces (Z_f, d_f) and propose that representations live on manifolds homeomorphic to these metric spaces. They test two key hypotheses: (1) that there exists a continuous, invertible correspondence between feature values and unit-norm representation directions (homeomorphism), and (2) that locally, cosine similarity between representations is a smooth, decreasing function of squared geodesic distance in the feature's metric space. The methodology involves extracting unit-norm representations via sparse autoencoders, building K-NN graphs to estimate manifold structure, and computing geodesic distances to test isometry between feature-space and representation-space distances.

## Key Results
- Features formalized as metric spaces are homeomorphic to their representation manifolds, with continuous invertible maps between feature values and unit-norm representation directions
- Cosine similarity locally encodes intrinsic geodesic distance in the feature's metric space, making feature and representation geometrically indistinguishable up to scale
- Curved embeddings enable richer linear computations compared to linear embeddings, as polynomials of order p require at least p+2 dimensions for linear readout
- Empirical validation on text embeddings and token activations shows evidence of isometry between feature distances and representation distances

## Why This Works (Mechanism)

### Mechanism 1: Topological Preservation Through Continuous Correspondence
- Claim: Features formalized as metric spaces (Z_f, d_f) are homeomorphic to their representation manifolds M_f in the model's latent space.
- Mechanism: A continuous, invertible map φ_f: Z_f → S^{D-1} connects each feature value to a unit-norm representation direction. Compactness of Z_f guarantees a continuous inverse (Proposition 1), so loops map to loops, intervals to curves, discrete sets to point clouds.
- Core assumption: The continuous correspondence hypothesis (Hypothesis 1)—that v_f(x) = φ_f(z_f(x)) for all inputs x where feature f is present.
- Evidence anchors:
  - [Section 2.2] Formal statement of Hypothesis 1 and Proposition 1 proving homeomorphism.
  - [Section 2.3] Empirical validation: colour embeddings form a loop matching the colour wheel order; years form a curve with Kendall rank correlation 0.97 to temporal order.
  - [corpus] Related work "Shape Happens" (FMR 0.52) confirms automatic manifold discovery but lacks the formal metric-space-to-manifold homeomorphism proof.
- Break condition: If representations of a known continuous feature (e.g., dates) are scattered or disconnected, Hypothesis 1 fails—check via connectivity on K-NN graphs.

### Mechanism 2: Cosine Similarity Encodes Intrinsic Geodesic Distance
- Claim: Locally, cosine similarity between representations is a smooth, decreasing function of squared geodesic distance in the feature's metric space, making the feature and its representation geometrically indistinguishable up to scale.
- Mechanism: Under Hypothesis 2, CosSim(φ_f(z), φ_f(z')) = g_f(d_f(z, z')²) with g'_f(0) < 0. Theorem 1 proves that path lengths on M_f equal geodesic distances on Z_f up to constant √(-2g'_f(0)).
- Core assumption: Hypothesis 2 holds locally (for nearby points); the function g_f is C² with negative slope at origin.
- Evidence anchors:
  - [Section 3, Theorem 1] Mathematical proof linking manifold path lengths to feature-space path lengths.
  - [Section 3.1, Figure 3] Empirical validation: for log-transformed years and circular dates, geodesic distances on manifolds show Pearson ρ > 0.97 with putative metric-space distances.
  - [corpus] "The Shape of Beliefs" studies geometry along representation manifolds but does not prove the geodesic-isometry theorem.
- Break condition: If cosine similarity vs. squared-distance plots show non-monotonic or increasing trends near zero, Hypothesis 2 fails—g'_f(0) would not be negative.

### Mechanism 3: Curved Embeddings Enable Richer Linear Readout
- Claim: Models embed intrinsically low-dimensional features into higher-dimensional curved manifolds to maximize the expressivity of downstream linear projections.
- Mechanism: To linearly read out polynomials of order p from φ_f(z), the manifold must weave through ≥ p+2 dimensions. Straightforward arc embeddings suffice only for identity readout; curvature expands the function space accessible via linear probes.
- Core assumption: Superposition hypothesis—features are sparse and occupy nearly orthogonal subspaces, limiting interference.
- Evidence anchors:
  - [Section 2.4] Explicit construction showing how polynomial readout requires weaving through higher dimensions.
  - [Section 2.4] Links to linear probe success under superposition.
  - [corpus] Weak direct evidence—no corpus papers formally prove this expressivity–curvature relationship.
- Break condition: If a feature manifold lies entirely in a low-dimensional subspace but complex nonlinear functions of the feature are linearly probeable, alternative mechanisms (e.g., nonlinearity in probes) may be involved.

## Foundational Learning

- **Metric Spaces**:
  - Why needed here: The paper's core formalism defines features as (Z_f, d_f)—a set with a distance function. Without this, the homeomorphism and isometry claims are incoherent.
  - Quick check question: Given Z = [0, 2π) with d(x,y) = min(|x-y|, 2π−|x-y|), what topology does this induce? (Answer: a circle.)

- **Homeomorphism vs. Isometry**:
  - Why needed here: Homeomorphism preserves topology (loops, connectedness); isometry preserves distances. The paper finds homeomorphism common but isometry requires careful metric-space design.
  - Quick check question: A rubber band stretched into a wiggly curve is homeomorphic but not isometric to the original circle—explain why.

- **Geodesic vs. Euclidean Distance**:
  - Why needed here: Cosine similarity reflects on-manifold (geodesic) distance, not straight-line Euclidean distance in ambient space.
  - Quick check question: On a Swiss-roll manifold, two points close in Euclidean 3D may be far along the roll—how would you estimate geodesic distance from samples?

## Architecture Onboarding

- **Component map**: Raw tokens/text -> Representation extraction (SAEs or embeddings) -> Feature isolation (unit-norm normalization) -> Manifold estimation (K-NN graph, geodesic distances) -> Diagnostic plotting (PCA, cosine vs. distance plots)

- **Critical path**:
  1. Identify candidate feature with hypothesized metric space (e.g., years → interval, days → circle)
  2. Extract unit-norm representations via SAE or direct embedding
  3. Test Hypothesis 1: Check topological match (rank correlation, visual loop/curve structure)
  4. Test Hypothesis 2: Plot diagnostics; compute Chatterjee ξ and Pearson ρ
  5. Iterate on metric-space design if isometry fails (e.g., try log-transform for years)

- **Design tradeoffs**:
  - **K in K-NN graph**: Lower K preserves local structure but risks disconnected components; higher K smoothes but may shortcut geodesics
  - **PCA dimensions**: Too few → lose structure; too many → noise obscures isometry. Paper finds 3–5 dimensions often optimal
  - **Manual vs. learned metrics**: Manual metric-space design is interpretable but not scalable; learned metrics remain unexplored

- **Failure signatures**:
  - Disconnected K-NN graph → K too low or manifold poorly sampled
  - Non-monotonic cosine-similarity vs. distance plots → Hypothesis 2 violated; consider alternative metric or feature splitting
  - High rank correlation but low isometry → homeomorphism holds but geometry differs (e.g., logarithmic warping)

- **First 3 experiments**:
  1. Replicate the colour-wheel experiment on a different embedding model; verify loop topology and compute Chatterjee ξ for Hypothesis 2
  2. On GPT-2 layer 8, extract month-of-year feature via SAE; test both linear and circular metric hypotheses using the paper's diagnostic plots
  3. Introduce a synthetic feature (e.g., modular arithmetic on known angles) into a small transformer; verify that Theorem 1's geodesic-isometry prediction holds exactly under controlled conditions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the metric space Zf be learned automatically for features where no natural human hypothesis exists (e.g., abstract concepts like emotions)?
- Basis in paper: [explicit] Authors state: "This 'manual' approach is clearly not scalable... relies on there being some reasonable starting hypothesis for the metric space, which could be difficult for many features (say, emotions). There is an unexplored alternative approach of learning the metric."
- Why unresolved: The paper's methodology requires manually specifying a candidate metric space, which is impractical for complex or poorly understood features.
- What evidence would resolve it: Development of an algorithm that recovers interpretable metric structures from representations without prior specification, validated on features with unknown geometry.

### Open Question 2
- Question: Can sparse autoencoders be modified to explicitly learn manifold-tracing dictionaries rather than atomic direction vectors?
- Basis in paper: [explicit] "We conjecture that the sparsity penalty of a sparse autoencoder, trained on representation manifold, will encourage it to learn a collection of dictionary vectors which trace the manifold... We hope our work will encourage development of 'manifold-aware' SAEs."
- Why unresolved: Current SAEs are designed under the linear representation hypothesis and may fragment continuous manifolds into discrete features ("feature splitting").
- What evidence would resolve it: Demonstration that a modified SAE architecture recovers continuous manifold structure with lower reconstruction error and higher interpretability than standard SAEs.

### Open Question 3
- Question: What are the full distance metrics that language models encode, beyond human-interpretable approximations?
- Basis in paper: [explicit] "We expect that the true notions of distance used by the language model are more complex: we find additional structure in further principal components and think it is possible that a language model could encode distances in a way that is mechanistically useful, but does not correspond to any existing human understanding of the feature."
- Why unresolved: PCA projections discard higher-dimensional structure; the complete metric remains unknown and may not be human-legible.
- What evidence would resolve it: Identification of a learned metric that predicts downstream model behavior (e.g., attention patterns, output probabilities) better than human-specified metrics.

### Open Question 4
- Question: How can geodesic distance estimation on representation manifolds be made robust to noise and short-circuits without manual intervention?
- Basis in paper: [explicit] "This is prone to short-circuits causing enormous errors in the estimated manifold distances. It is often the case that one has to manually prune the graph... more robust methodology for manifold estimation would be required to scale up our approach."
- Why unresolved: K-nearest-neighbour graph approximations are sensitive to noise and outliers, requiring manual correction.
- What evidence would resolve it: An automated method that accurately recovers geodesic distances on known manifolds (e.g., synthetic swiss rolls, tori) with comparable or better accuracy than manually-pruned graphs.

## Limitations
- Framework applicability depends heavily on identifying appropriate metric spaces for features, which remains largely manual and feature-specific
- The stronger isometry claim requires careful metric-space design and shows variable success across features
- Theoretical mechanism linking curved embeddings to richer linear readout lacks direct empirical validation
- Reliance on SAE-extracted unit-norm representations may introduce artifacts or miss features that don't naturally project to unit vectors

## Confidence

*High Confidence:* The homeomorphism claims (Hypothesis 1) and the mathematical framework connecting cosine similarity to geodesic distance (Theorem 1) are rigorously proven and empirically validated across multiple features. The diagnostic methodology for testing these claims is sound and reproducible.

*Medium Confidence:* The practical implementation of the framework works well for certain features (colors, years, months) but requires significant manual tuning of metric spaces. The success rate across diverse features is not fully characterized, and the conditions for when isometry will hold versus just homeomorphism are not completely mapped.

*Low Confidence:* The theoretical claim that curved embeddings specifically enable richer linear readout through higher-dimensional weaving is plausible but not directly tested. The connection between the observed manifold geometry and computational advantages for the model remains speculative without controlled experiments.

## Next Checks

1. **Synthetic Feature Experiment**: Create a small transformer with known modular arithmetic features (e.g., representations of angles mod 360°) and verify that Theorem 1's geodesic-isometry prediction holds exactly under controlled conditions, isolating the effect from noise.

2. **Metric-Space Design Study**: Systematically test alternative metric-space formulations for years (linear, logarithmic, piecewise) and document which yield isometry versus just homeomorphism, establishing guidelines for metric design.

3. **Alternative Representation Extraction**: Apply the framework to representations from other isolation methods (sparse autoencoders, direct probing, linear probes) and compare whether the homeomorphism/isometry patterns persist or differ, testing the robustness of the framework to representation extraction method.