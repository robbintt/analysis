---
ver: rpa2
title: 'Memorize and Rank: Elevating Large Language Models for Clinical Diagnosis
  Prediction'
arxiv_id: '2501.17326'
source_url: https://arxiv.org/abs/2501.17326
tags:
- diagnosis
- prediction
- medical
- code
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MERA addresses the challenge of clinical diagnosis prediction by
  leveraging large language models to bridge the gap between natural language medical
  knowledge and structured medical codes. The method employs hierarchical contrastive
  learning and dynamic confidence thresholds to handle the large disease candidate
  space, while fine-tuning the model to memorize the mapping between medical codes
  and their natural language definitions.
---

# Memorize and Rank: Elevating Large Language Models for Clinical Diagnosis Prediction

## Quick Facts
- **arXiv ID:** 2501.17326
- **Source URL:** https://arxiv.org/abs/2501.17326
- **Reference count:** 6
- **Primary result:** Achieves state-of-the-art performance on clinical diagnosis prediction, with 5.89-point higher weighted F1 and nearly 8 points higher recall@20 compared to existing models on MIMIC-III

## Executive Summary
MERA addresses the challenge of clinical diagnosis prediction by leveraging large language models to bridge the gap between natural language medical knowledge and structured medical codes. The method employs hierarchical contrastive learning and dynamic confidence thresholds to handle the large disease candidate space, while fine-tuning the model to memorize the mapping between medical codes and their natural language definitions. This approach effectively incorporates clinical knowledge and captures dependencies among diseases within the ICD coding system.

Experimental results on MIMIC-III and MIMIC-IV datasets demonstrate that MERA achieves state-of-the-art performance in both general diagnosis and heart failure prediction tasks. Specifically, MERA shows a 5.89-point higher weighted F1 score and nearly 8 points higher recall@20 compared to existing best models on MIMIC-III. The model also exhibits near-perfect memorization of the bidirectional medical code-definition mapping, significantly improving the diagnosis prediction capabilities of generative language models.

## Method Summary
MERA is a three-stage pipeline for clinical diagnosis prediction that fine-tunes large language models to bridge natural language clinical knowledge with structured medical codes. First, it memorizes bidirectional mappings between ICD codes and their natural language definitions through synthetic QA pairs. Second, it applies hierarchical contrastive learning across ICD ontology levels to distinguish true diagnoses from similar candidates in the large candidate space. Third, it employs dynamic confidence thresholds via learned EOV token placement for flexible diagnosis shortlisting. The method was validated on MIMIC-III and MIMIC-IV datasets, achieving state-of-the-art performance in both general diagnosis prediction and heart failure prediction tasks.

## Key Results
- Achieves 5.89-point higher weighted F1 score and nearly 8 points higher recall@20 compared to existing best models on MIMIC-III
- Demonstrates near-perfect memorization of bidirectional medical code-definition mapping (>99% accuracy)
- Shows state-of-the-art performance in both general diagnosis prediction and heart failure prediction tasks on MIMIC datasets
- Exhibits robust performance with minimal degradation between MIMIC-III and MIMIC-IV datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuning LLMs to memorize bidirectional medical code-definition mappings bridges the gap between natural language clinical knowledge and structured medical codes.
- **Mechanism:** Special token embeddings for each ICD code are parameterized through synthetic QA pairs (code→definition and definition→code), plus code-to-hierarchy mappings. This creates a shared representation space where semantic meaning and code structure align.
- **Core assumption:** Medical code tokens should be single tokens rather than sequences to reduce noise in probability estimation.
- **Evidence anchors:** [abstract] "With concept memorization through fine-tuning, we bridge the natural language clinical knowledge with medical codes." [section 3.1] "GPT-4 can only recall 45% of ICD-9 codes given corresponding definitions... MERA explicitly teaches LM the semantic information associated with the medical codes." [corpus] Weak direct validation; related work on patient trajectory prediction notes "high granularity of medical codes" as a key challenge.
- **Break condition:** If the ICD code vocabulary expands significantly (e.g., 68k→100k+ codes), single-token representation may become memory-prohibitive.

### Mechanism 2
- **Claim:** Hierarchical contrastive learning across ICD ontology levels enables the model to distinguish true diagnoses from similar candidates in a large decision space.
- **Mechanism:** InfoNCE loss is computed at each hierarchy level (chapter→fine-grained). High-level losses teach broad disease scope recognition; low-level losses force nuanced discrimination among similar diseases. The aggregated loss provides dense supervision across the 13k+ candidate space.
- **Core assumption:** The ICD hierarchy meaningfully reflects clinical differential diagnosis reasoning.
- **Evidence anchors:** [abstract] "We apply hierarchical contrastive learning on a disease candidate ranking list to alleviate the large decision space issue." [section 3.3] "Optimizing the high-level loss mimics the clinician's training process of making differential diagnoses." [corpus] Knowledge graph approaches (KGxDP, PhenoKG) similarly leverage hierarchical structure, suggesting cross-method convergence.
- **Break condition:** If diseases are misclassified in ICD hierarchy or cross-group dependencies are critical, hierarchical CL may reinforce incorrect groupings.

### Mechanism 3
- **Claim:** Dynamic confidence thresholds via learned EOV token placement outperform fixed thresholds for diagnosis shortlisting.
- **Mechanism:** The EOV token's probability serves as an adaptive cutoff. Dynamic cross-entropy loss enforces: positive codes > P(EOV) and negative codes < P(EOV). This is calibrated through both implicit exposure (input sequences end with EOV) and explicit loss optimization.
- **Core assumption:** The model can learn when it has exhausted confident predictions without external calibration.
- **Evidence anchors:** [section 3.3] "Existing works apply a fixed threshold... This widely used strategy makes shortlisting less flexible, and the model tends to play it safe." [Table 4, row 7] Removing dynamic threshold causes -4.10 weighted F1 drop. [corpus] No direct comparison in neighbors; this appears novel to MERA.
- **Break condition:** If diagnosis count varies dramatically across patient types (e.g., complex chronic vs. simple acute), a single learned threshold may underfit the distribution.

## Foundational Learning

- **Concept: InfoNCE / Contrastive Learning Loss**
  - Why needed here: Core objective distinguishing positive (true) from negative diagnoses within hierarchical groups.
  - Quick check question: Given logits [2.1, 1.5, 0.8] for [positive, negative, negative], compute the InfoNCE loss term.

- **Concept: ICD Coding Hierarchy**
  - Why needed here: Determines the structure for hierarchical contrastive learning levels.
  - Quick check question: For ICD-9 code 250.23, identify at least two parent groups in the hierarchy.

- **Concept: Autoregressive Decoding with Special Tokens**
  - Why needed here: MERA generates diagnoses token-by-token but uses first-token probability ranking for final output.
  - Quick check question: How does MERA's output strategy differ from standard autoregressive generation?

## Architecture Onboarding

- **Component map:**
  1. **Input Layer:** Verbalized patient history sequence (visit codes + optional NL profile)
  2. **Token Embeddings:** Extended vocabulary with special tokens for each ICD code + EOV marker
  3. **LLM Backbone:** Decoder-only (LLaMA2-7B, BioMistral-7B) or encoder-decoder (T5, Flan-T5)
  4. **Probability Extraction:** First output token logits → softmax over code vocabulary only
  5. **Loss Computation:** Hierarchical CL (Eq. 5) + Dynamic CE (Eq. 6)

- **Critical path:**
  1. **Phase 1 (Memorization):** Fine-tune on code↔definition QA pairs with standard CE loss
  2. **Phase 2 (Diagnosis Learning):** Apply hierarchical CL + dynamic CE on patient history data with in-visit order perturbation
  3. **Inference:** Autoregressive decode until EOV, or use first-token ranking

- **Design tradeoffs:**
  - Decoder-only vs. encoder-decoder: Decoder-only (LLaMA2) achieves ~99% code accuracy; encoder-decoder (T5) excels at definition→code but fails code→definition (Table 2).
  - Ranking vs. decoding output: Ranking from first token outperforms full decoding by 10+ recall@20 points when trained with sparse CE (Table 4, rows 8-9).
  - Full-parameter vs. efficient fine-tuning: Paper uses full-parameter; LoRA compatibility not tested.

- **Failure signatures:**
  - Low code memorization accuracy (<80%): Check token vocabulary coverage and embedding initialization.
  - High recall@20 but low weighted F1: Model over-predicts; check EOV token learning.
  - Performance gap between MIMIC-III and IV: May indicate distribution shift; check patient population differences.

- **First 3 experiments:**
  1. **Memorization sanity check:** Fine-tune on code-definition pairs only; report bidirectional exact match accuracy. Target: >95% for 7B model.
  2. **Ablation by hierarchy level:** Train with only chapter-level CL, only fine-grained CL, and full hierarchy. Expect: removing 0-th level causes largest drop (-9.24 F1 per Table 4).
  3. **Output strategy comparison:** Compare (a) decode with CE, (b) rank with CE, (c) rank with MERA losses. Expect: (c) > (b) > (a) by 10+ recall@20 points.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the reliance on hierarchical contrastive learning based strictly on the ICD ontology constrain the model from capturing complex co-morbidities that span across disparate branches of the disease hierarchy?
- Basis in paper: [inferred] The method explicitly structures the contrastive loss (Equation 4) around the ICD tree hierarchy `G` (level 0 to depth). While this mimics differential diagnosis, it assumes clinically relevant dependencies primarily exist within these specific nested groups, potentially penalizing valid cross-branch correlations.
- Why unresolved: The paper evaluates performance based on aggregate metrics (F1, Recall) but does not analyze specific failure modes where the model might miss diagnoses that are clinically related but hierarchically distant in the ICD structure.
- What evidence would resolve it: A qualitative and quantitative error analysis specifically examining "cross-branch" prediction accuracy versus "in-branch" prediction accuracy on a held-out test set.

### Open Question 2
- Question: How does the dynamic confidence threshold (EOV token placement) perform when scaling to the full ICD-10 candidate space (68k+ codes) compared to the smaller ICD-9 space (13k codes)?
- Basis in paper: [inferred] The paper highlights the "large candidate space" challenge and mentions the ICD-10 system contains 68k+ unique codes. However, the primary experimental results (Table 1) focus on the ICD-9 dataset, with ICD-10 results (Table 3) showing a relative performance drop, suggesting the ranking mechanism may face increased difficulty with denser candidate pools.
- Why unresolved: The mechanism relies on the model learning to place the EOV token in a sorted probability list. As the list length increases drastically (from 13k to 68k), the granularity of the probability distribution changes, potentially making the distinction between true positives and noise harder for the threshold mechanism.
- What evidence would resolve it: Experiments comparing the precision-recall trade-off of the dynamic threshold specifically on the full ICD-10 vocabulary versus the ICD-9 vocabulary within the same patient cohort.

### Open Question 3
- Question: Can the assumption that intra-visit diagnosis order is irrelevant ("order perturbation") negatively impact predictions in clinical scenarios where the sequence of diagnoses indicates severity or causality?
- Basis in paper: [inferred] The paper states: "The order of diagnosis codes within a particular visit does not carry cognitive rationale... To achieve this goal... we propose to create n_perturb variants... randomly shuffles the diagnosis codes."
- Why unresolved: While EHR documentation might be inconsistent, clinical billing or complex cases often list primary/secondary diagnoses in an order that reflects the patient's severity or treatment plan. Forcing the model to ignore this might discard subtle signals available in ordered data.
- What evidence would resolve it: An ablation study comparing the current shuffled training against a "sorted" or "ordinal" training regimen (e.g., sorting by diagnosis priority or severity) to see if retaining order improves recall for the primary diagnosis.

## Limitations
- The method relies on unknown training hyperparameters including learning rates, batch sizes, and loss weighting that are not specified in the paper
- The hierarchical contrastive learning assumes ICD hierarchy meaningfully reflects clinical differential diagnosis reasoning without external validation
- The approach was validated only on MIMIC datasets with adult patients, limiting generalizability to other populations or EHR systems

## Confidence
**High Confidence:**
- The memorization mechanism (code↔definition fine-tuning) is technically sound and directly validated with >99% accuracy metrics.
- The hierarchical contrastive learning framework is well-defined with clear InfoNCE formulations and explicit validation through ablation studies.
- The overall performance improvements over baselines on MIMIC datasets are empirically demonstrated with significant F1 and recall gains.

**Medium Confidence:**
- The mechanism linking hierarchical CL to clinical differential diagnosis reasoning is conceptually plausible but not directly validated with clinician input.
- The dynamic confidence threshold's superiority over fixed thresholds is demonstrated empirically but the generalization across different patient populations remains untested.
- The assumption that single-token ICD code representation remains feasible as vocabulary size increases has not been stress-tested.

**Low Confidence:**
- The assumption that ICD hierarchy meaningfully captures clinical dependencies is accepted without external validation beyond internal ablation.
- The scalability of the approach to larger ICD code vocabularies (68k+ ICD-10 codes) is not demonstrated or discussed in detail.
- The robustness to distribution shifts between MIMIC-III and MIMIC-IV is suggested by performance maintenance but not systematically studied.

## Next Checks
1. **Generalization Stress Test:** Apply MERA to a different EHR dataset (e.g., eICU, or a non-MIMIC system) to validate performance stability across institutions and patient demographics. Measure whether the 5.89-point F1 improvement and near-perfect memorization hold in this new context.

2. **Hierarchy Validation Study:** Conduct a clinician review of MERA's hierarchical disease groupings to verify whether the ICD hierarchy actually reflects clinically meaningful differential diagnosis reasoning. Identify cases where hierarchical CL might reinforce incorrect groupings due to ICD misclassifications.

3. **Vocabulary Scalability Analysis:** Test MERA's memorization and inference performance as ICD code vocabulary increases from 13k to 68k+ codes. Measure memory requirements, training stability, and whether single-token representation becomes prohibitive at larger scales.