---
ver: rpa2
title: 'Fine-Tuning Small Language Models for Domain-Specific AI: An Edge AI Perspective'
arxiv_id: '2503.01933'
source_url: https://arxiv.org/abs/2503.01933
tags:
- shakti
- language
- shakti-250m
- datasets
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of deploying large-scale language
  models on resource-constrained edge devices, which face high computational demands,
  energy consumption, and data privacy risks. To tackle these issues, the authors
  introduce the Shakti Small Language Models (SLMs) - Shakti-100M, Shakti-250M, and
  Shakti-500M - designed with efficient architectures, quantization techniques, and
  responsible AI principles.
---

# Fine-Tuning Small Language Models for Domain-Specific AI: An Edge AI Perspective

## Quick Facts
- arXiv ID: 2503.01933
- Source URL: https://arxiv.org/abs/2503.01933
- Reference count: 40
- Primary result: Shakti-250M achieves Answer Relevancy Score of 0.85 in healthcare and 0.86 in finance domains while requiring only 8x less memory than FP32 counterparts when quantized

## Executive Summary
This paper introduces the Shakti Small Language Models (SLMs) - Shakti-100M, Shakti-250M, and Shakti-500M - designed to address the computational and memory constraints of deploying large language models on edge devices. The models leverage advanced architectural optimizations including Rotary Positional Embeddings, Variable Grouped Query Attention, and Block Sparse Attention to achieve efficient inference while maintaining competitive performance on both general and domain-specific benchmarks. Through quantization-aware training and domain-specific fine-tuning using SFT followed by DPO, the Shakti models demonstrate that compact architectures can deliver strong results in healthcare, finance, and legal applications with significantly reduced resource requirements compared to larger models.

## Method Summary
The Shakti models employ transformer architectures with RoPE positional embeddings, SiLU activation, and Pre-Normalization. Shakti-100M and Shakti-250M use Variable Grouped Query Attention to reduce key-value projections, while Shakti-500M implements Block Sparse Attention for efficient long-context processing. Models are pre-trained on diverse web corpora including Common Crawl and Fineweb-EDU-Dedup, then fine-tuned using SFT on domain-specific instruction datasets. Shakti-100M and Shakti-250M use DPO for preference alignment, while Shakti-500M employs RLHF. Quantization-aware training enables efficient deployment at int4/int5/int8 precision with up to 8x memory reduction. The models are evaluated on general benchmarks (MMLU, Hellaswag) and domain-specific tasks with metrics including Answer Relevancy, Summarization Score, and Factual Score.

## Key Results
- Shakti-250M achieves Answer Relevancy Scores of 0.85 (healthcare), 0.86 (finance), and 0.81 (legal) on domain-specific tasks
- Quantization reduces memory usage by up to 8x while maintaining task accuracy
- Shakti-250M demonstrates Factual Score of 0.83 in finance domain with DPO alignment
- Edge deployment achieves 29 TPS on Raspberry Pi 5 for single-stream inference
- Multilingual support includes Indian languages (Kannada, Hindi, Telugu, Tamil) plus Spanish, French, and German

## Why This Works (Mechanism)

### Mechanism 1
Variable Grouped Query Attention (GQA) and Block Sparse Attention reduce memory/compute overhead in attention layers, enabling efficient inference at small parameter scales. GQA reduces key-value projection heads in Shakti-100M/250M; Block Sparse Attention enables efficient long-context processing in Shakti-500M by sparsifying attention patterns. Core assumption: Attention's quadratic memory scaling is the primary bottleneck for edge deployment; reducing KV projections preserves model quality while cutting memory. Evidence anchors: [abstract] "Variable Grouped Query Attention (GQA), and Block Sparse Attention to optimize memory and computational efficiency." [section 3] "variable grouped query attention (GQA) is used in Shakti-100M and Shakti-250M to reduce key-value projections, whereas Block Sparse Attention in Shakti-500M enables efficient long-context processing." Break condition: If sequence lengths are short (<512 tokens) or batch sizes are small, GQA's memory savings diminish; sparse attention may introduce quality degradation on dense-retrieval tasks (not quantified in paper).

### Mechanism 2
Quantization-Aware Training (QAT) during pre-training preserves accuracy when deploying at int4/int5/int8 precision, achieving up to 8x memory reduction. QAT incorporates quantization effects into training, allowing the model to adapt weights to low-bit representations. Post-training, block-wise quantization with scaling factors further compresses models. Core assumption: Edge devices have limited RAM and benefit more from reduced memory footprint than from higher precision; accuracy loss from quantization is acceptable within task-specific tolerances. Evidence anchors: [section 4.1] "A key innovation in the Shakti-500M model is the inclusion of quantization-aware training (QAT). This technique optimizes performance for low-resource devices by reducing memory consumption with low-bit representations while preserving model accuracy." [section 8.2] "Q4 models requiring approximately 8x less memory than their FP32 counterparts." Break condition: If downstream tasks require high numerical precision (e.g., financial calculations, medical dosing), int4 quantization may introduce unacceptable errors; per-task accuracy evaluation is required.

### Mechanism 3
Domain-specific fine-tuning with Supervised Fine-Tuning (SFT) followed by Direct Preference Optimization (DPO) enables small models to achieve competitive domain performance without RLHF's computational cost. SFT adapts foundational language understanding to domain-specific instruction formats; DPO aligns outputs with human preferences using preference pairs, avoiding the reward model training required in RLHF. Core assumption: High-quality domain-specific instruction data is available; DPO provides sufficient alignment for resource-constrained scenarios where RLHF is impractical. Evidence anchors: [section 4.4] "Shakti-250M and Shakti-100M models employ Direct Preference Optimization (DPO) as a computationally efficient alternative to RLHF... This enables real-time application capabilities without sacrificing quality." [section 6.2.2, Table 2] Shakti-250M achieves Answer Relevancy Scores of 0.85 (healthcare), 0.86 (finance), 0.81 (legal); Factual Score of 0.83 in finance. Break condition: If domain preference data is noisy or inconsistent, DPO may amplify biases; RLHF may be necessary for high-stakes domains (clinical decision support, legal advice).

## Foundational Learning

- **Rotary Positional Embeddings (RoPE)**
  - Why needed here: Shakti models use RoPE for sequence position encoding without increasing parameter count, enabling longer context handling critical for domain tasks (legal contracts, medical records).
  - Quick check question: Can you explain how RoPE differs from learned absolute positional embeddings in terms of extrapolation to longer sequences?

- **Grouped Query Attention (GQA)**
  - Why needed here: Understanding the trade-off between Multi-Head Attention (MHA), Multi-Query Attention (MQA), and GQA is essential for selecting the right Shakti variant for memory-constrained deployments.
  - Quick check question: How does GQA balance the quality of MHA with the inference speed of MQA?

- **Quantization Fundamentals (PTQ vs. QAT)**
  - Why needed here: Shakti's edge deployment strategy relies on int4/int8 quantization; distinguishing post-training quantization from quantization-aware training determines expected accuracy trade-offs.
  - Quick check question: What is the primary risk of applying int4 post-training quantization without QAT to a model trained only in FP32?

## Architecture Onboarding

- **Component map:**
  Input Tokens → Tokenizer (multilingual)
       ↓
  Embedding Layer (hidden dim: 640/1024/2048)
       ↓
  RoPE Positional Encoding
       ↓
  Transformer Blocks (10/16/24 layers)
    ├── Attention: GQA (100M/250M) or Block Sparse (500M)
    ├── SiLU Activation + Pre-Normalization
    └── Sliding Window Cache (long-context reuse)
       ↓
  Output Head → Token Prediction
       ↓
  Quantization (Q4/Q5/Q8) for Deployment

- **Critical path:** Pre-training (diverse corpus + QAT for 500M) → SFT (domain-specific instructions) → DPO/RLHF (preference alignment) → Quantization → Edge deployment benchmark.

- **Design tradeoffs:**
  - Shakti-100M: Ultra-lightweight (IoT/mobile), fastest inference, limited reasoning depth.
  - Shakti-250M: Domain-optimized (healthcare/finance/legal), balanced accuracy/efficiency, uses DPO (not RLHF).
  - Shakti-500M: Most capable (multilingual, long-context), uses RLHF, highest memory footprint—requires QAT for edge.

- **Failure signatures:**
  - Degraded domain accuracy without domain-specific SFT data (check Table 1 datasets).
  - Int4 quantization causing >5% accuracy drop on factual tasks (may indicate insufficient QAT or task sensitivity).
  - Memory overflow on Raspberry Pi with Shakti-500M-Q4 at batch size >1 (verify RAM constraints: 8GB Pi5 handles ~29 TPS single-stream).

- **First 3 experiments:**
  1. **Baseline benchmark replication:** Run Shakti-250M-Q4 on healthcare QA prompts (medqa subset) and measure Answer Relevancy Score; compare to paper's 0.85 baseline.
  2. **Quantization ablation:** Deploy Shakti-100M in Q4, Q5, Q8 variants on target edge hardware (e.g., Raspberry Pi 5); measure TPS, memory footprint, and accuracy delta on a held-out domain task.
  3. **Domain adaptation test:** Fine-tune Shakti-250M on a small proprietary domain dataset (50-100 instruction pairs) using SFT + DPO; evaluate before/after on domain-specific factual accuracy to validate transfer efficiency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Shakti models be effectively integrated with federated learning frameworks to enable collaborative, privacy-preserving on-device training across distributed edge devices?
- Basis in paper: [explicit] The Future Scope section states: "Expanding support for edge computing scenarios, such as integrating Shakti models with federated learning frameworks, could provide robust solutions for collaborative and secure AI deployments."
- Why unresolved: The current paper focuses on inference-only deployment on edge devices; no experiments or architectural modifications address federated training, gradient aggregation, or communication-efficient updates under edge constraints.
- What evidence would resolve it: Implementation of a federated learning pipeline with Shakti models, measuring communication overhead, convergence speed, and final task performance compared to centralized training, while quantifying privacy benefits.

### Open Question 2
- Question: What is the quantitative accuracy degradation across int8, int5, and int4 quantization levels on both general and domain-specific benchmarks?
- Basis in paper: [inferred] Section 8 highlights memory reduction (up to 8x) and TPS improvements from quantization, and Section 4.1 mentions quantization-aware training, but no table or figure reports benchmark accuracy at each precision level.
- Why unresolved: The paper shows size and throughput gains but does not isolate how aggressive quantization (especially int4) affects MMLU, Hellaswag, or domain-specific scores, leaving a gap for accuracy-critical applications.
- What evidence would resolve it: A comparative table reporting benchmark scores (e.g., MMLU, Hellaswag, domain tasks) for FP32, int8, int5, and int4 variants of each Shakti model, with statistical significance tests.

### Open Question 3
- Question: How do Shakti models perform on standardized multilingual benchmarks across supported languages, including underrepresented Indian languages?
- Basis in paper: [explicit] Section 7 claims multilingual capabilities and support for Kannada, Hindi, Telugu, Tamil, Spanish, French, and German; [inferred] no multilingual benchmark results are provided anywhere in the evaluation.
- Why unresolved: Without empirical multilingual benchmarks, claims about democratizing AI access and breaking language barriers remain unvalidated.
- What evidence would resolve it: Evaluation on established multilingual benchmarks (e.g., XTREME-R, MMMLU, or IndicGLUE for Indian languages) with per-language performance breakdown.

### Open Question 4
- Question: What is the robustness of Shakti-250M on standardized legal domain benchmarks beyond prompt-based relevancy scoring?
- Basis in paper: [inferred] Figure 4 shows healthcare and finance domain benchmark comparisons, but legal domain performance is only reported via prompt-based Answer Relevancy (0.81) and Summarization Score (0.86) in Table 2, without comparison to other models or standard legal benchmarks.
- Why unresolved: The paper positions Shakti-250M for legal applications, yet legal benchmark evaluation is inconsistent with how healthcare and finance were evaluated.
- What evidence would resolve it: Evaluation on legal benchmarks (e.g., LegalBench, LexGLUE) comparing Shakti-250M against baseline models, similar to the healthcare/finance analysis in Figure 4.

## Limitations

- Training hyperparameters (learning rates, batch sizes, epochs) are not specified, making faithful reproduction challenging.
- GQA and Block Sparse Attention configurations remain underspecified with unclear query group counts and sparsity patterns.
- Domain-specific performance lacks comparison to established domain models beyond general benchmarks.
- The claimed superiority over larger models in efficiency benchmarks lacks ablation studies showing whether architecture choices or dataset quality drive improvements.

## Confidence

- **High Confidence**: Core architectural claims (RoPE, SiLU activation, Pre-Normalization) are standard and well-validated; quantization memory reduction (8x for Q4) is theoretically sound and aligns with established literature.
- **Medium Confidence**: Domain-specific performance metrics (0.85-0.86 Answer Relevancy) are plausible given the SFT+DPO approach, but lack comprehensive benchmarking against comparable domain models; edge inference throughput claims (29 TPS on Raspberry Pi 5) are reasonable but hardware-specific.
- **Low Confidence**: GQA and Block Sparse Attention implementation details are insufficient for exact replication; the claimed superiority over larger models in efficiency benchmarks lacks ablation studies showing whether architecture choices or dataset quality drive improvements.

## Next Checks

1. **Training Hyperparameter Replication**: Attempt to reproduce Shakti-250M pre-training on a subset of the specified corpus using standard transformer configurations with RoPE, SiLU+Pre-Norm, and reasonable hyperparameter defaults (learning rate 1e-4, batch size 32, 3 epochs). Compare loss curves and final perplexity to validate whether the architecture alone achieves competitive baseline performance.

2. **Quantization-Aware Training Verification**: Implement Shakti-100M with and without QAT during pre-training, then apply int4 quantization. Measure accuracy degradation on MMLU and a domain-specific task (e.g., medical QA). If non-QAT models show >5% accuracy drop versus QAT models, this validates the paper's claim that QAT is essential for low-bit deployment.

3. **Domain Transfer Efficiency Test**: Fine-tune Shakti-250M on a small, newly curated domain dataset (50-100 instruction pairs) using the SFT+DPO pipeline described. Evaluate before/after on domain-specific factual accuracy and answer relevancy. This tests whether the paper's claim of efficient domain adaptation with limited data holds true in practice.