---
ver: rpa2
title: 'MTCMB: A Multi-Task Benchmark Framework for Evaluating LLMs on Knowledge,
  Reasoning, and Safety in Traditional Chinese Medicine'
arxiv_id: '2506.01252'
source_url: https://arxiv.org/abs/2506.01252
tags:
- figure
- dataset
- llms
- evaluation
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MTCMB is the first multi-task benchmark for evaluating LLMs on
  Traditional Chinese Medicine (TCM), featuring 12 expert-curated datasets across
  five task categories: knowledge QA, language understanding, diagnostic reasoning,
  prescription generation, and safety evaluation. The benchmark integrates real-world
  clinical cases, national licensing exams, and classical texts to provide a comprehensive
  and authentic testbed for TCM-capable models.'
---

# MTCMB: A Multi-Task Benchmark Framework for Evaluating LLMs on Knowledge, Reasoning, and Safety in Traditional Chinese Medicine

## Quick Facts
- arXiv ID: 2506.01252
- Source URL: https://arxiv.org/abs/2506.01252
- Reference count: 40
- MTCMB is the first multi-task benchmark for evaluating LLMs on Traditional Chinese Medicine (TCM), featuring 12 expert-curated datasets across five task categories

## Executive Summary
MTCMB introduces the first comprehensive multi-task benchmark for evaluating large language models on Traditional Chinese Medicine capabilities. The framework comprises 12 expert-curated datasets spanning knowledge QA, language understanding, diagnostic reasoning, prescription generation, and safety evaluation. Drawing from real clinical cases, national licensing exams, and classical texts, MTCMB provides an authentic testbed for TCM AI systems. Evaluation of 14 diverse LLMs reveals significant gaps in TCM-specific reasoning and safety compliance, even among medical-tuned models, highlighting the need for domain-specific training approaches.

## Method Summary
MTCMB was constructed through systematic collection of TCM-specific data across five task categories. The benchmark integrates 12 expert-curated datasets including national licensing exam questions, real clinical cases, and classical medical texts. For each task category, the framework defines specific evaluation metrics: accuracy for knowledge QA, BLEU/F1 for language understanding, reasoning path evaluation for diagnostics, completeness for prescription generation, and safety compliance scoring. The automated evaluation pipeline employs both metric-based scoring and human expert validation to ensure reliability. The benchmark was applied to evaluate 14 diverse LLMs including general-purpose models, medical-tuned variants, and TCM-specialized systems.

## Key Results
- Strong correlation between automated and human expert evaluations (Pearson r = 0.87, Spearman Ï = 0.86)
- General LLMs struggle with TCM-specific reasoning despite performing well on factual recall
- Even medical-tuned models like HuatuoGPT-01-72B show limited improvement on TCM tasks
- Safety evaluation reveals critical gaps in compliance and risk assessment across all evaluated models

## Why This Works (Mechanism)
MTCMB works by providing a comprehensive, multi-dimensional evaluation framework that captures the complexity of TCM practice through diverse task types and authentic data sources. The benchmark's strength lies in its integration of theoretical knowledge, clinical reasoning, and practical application scenarios, which mirrors the actual demands of TCM practice. By including safety evaluation as a core component, MTCMB addresses the critical need for trustworthy AI systems in healthcare contexts where errors can have serious consequences.

## Foundational Learning
- Traditional Chinese Medicine Theory - why needed: TCM operates on fundamentally different principles than Western medicine, requiring specialized knowledge frameworks; quick check: understanding of Yin-Yang, Five Elements, and Qi concepts
- Syndrome Differentiation - why needed: Core diagnostic method in TCM that differs significantly from Western symptom-based diagnosis; quick check: ability to trace diagnostic reasoning paths
- Prescription Formulation - why needed: TCM prescriptions involve complex herb combinations and dosage considerations; quick check: understanding of herb properties and compatibility rules
- Clinical Safety Protocols - why needed: TCM treatments can have serious interactions and contraindications; quick check: knowledge of safety guidelines and risk assessment
- Classical Medical Texts - why needed: Many TCM concepts and practices are rooted in historical literature; quick check: familiarity with key classical texts and terminology

## Architecture Onboarding
- Component Map: Data Collection -> Dataset Curation -> Task Definition -> LLM Evaluation -> Automated Scoring -> Human Validation -> Result Analysis
- Critical Path: Expert-curated datasets -> Benchmark implementation -> Automated evaluation pipeline -> Human expert validation -> Performance analysis
- Design Tradeoffs: Breadth vs. depth of task coverage, automated vs. human evaluation, general vs. domain-specific metrics
- Failure Signatures: Overreliance on factual recall without reasoning, safety violations in prescription generation, inability to handle complex diagnostic scenarios
- First Experiments: 1) Run general LLMs on knowledge QA tasks to establish baseline performance, 2) Test medical-tuned models on diagnostic reasoning to measure improvement, 3) Evaluate safety compliance across all models to identify critical gaps

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset composition may not fully capture the breadth of TCM clinical scenarios, particularly rare or emerging treatment patterns
- Benchmark's focus on Traditional Chinese text limits applicability for multilingual TCM AI systems
- Safety evaluation may not comprehensively capture all potential risks in real-world clinical deployment

## Confidence
- High confidence: Benchmark construction methodology, dataset diversity, and consistent LLM performance patterns
- Medium confidence: Generalizability to real-world clinical settings and sufficiency of current safety evaluation metrics
- Low confidence: Long-term predictive validity of MTCMB scores for actual clinical performance

## Next Checks
1. Conduct longitudinal studies comparing MTCMB scores with actual clinical outcomes in supervised TCM practice settings, tracking both patient outcomes and practitioner adoption patterns over 6-12 months
2. Expand the benchmark to include cross-lingual evaluation capabilities by translating core datasets into multiple languages while maintaining semantic integrity, then reassessing LLM performance across language versions
3. Implement adversarial testing scenarios that introduce subtle but clinically significant variations in patient presentations to evaluate model robustness beyond the current benchmark's controlled task structure