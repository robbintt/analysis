---
ver: rpa2
title: Understanding Generalization, Robustness, and Interpretability in Low-Capacity
  Neural Networks
arxiv_id: '2507.16278'
source_url: https://arxiv.org/abs/2507.16278
tags:
- learning
- pruning
- size
- networks
- capacity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the relationship between model capacity,
  sparsity, and robustness in low-capacity neural networks using a controlled binary
  classification framework on MNIST. The authors systematically vary task difficulty
  by selecting visually similar digit pairs (e.g., 0-1 vs 4-9) and train fully connected
  networks with different hidden layer sizes (2-64 neurons).
---

# Understanding Generalization, Robustness, and Interpretability in Low-Capacity Neural Networks

## Quick Facts
- arXiv ID: 2507.16278
- Source URL: https://arxiv.org/abs/2507.16278
- Authors: Yash Kumar
- Reference count: 20
- Primary result: Minimum network capacity scales with task complexity; low-capacity networks achieve 95% pruning resilience while preserving learned representations.

## Executive Summary
This study investigates the relationship between model capacity, sparsity, and robustness in low-capacity neural networks using a controlled binary classification framework on MNIST. The authors systematically vary task difficulty by selecting visually similar digit pairs (e.g., 0-1 vs 4-9) and train fully connected networks with different hidden layer sizes (2-64 neurons). They find that the minimum required capacity scales directly with task complexity, with simpler tasks needing only 4-6 neurons while harder tasks require 12-24 neurons. Networks trained on these tasks demonstrate extreme robustness to magnitude pruning, maintaining performance with up to 95% sparsity. The identified sparse subnetworks preserve both the geometric structure of learned representations (shown via t-SNE) and the original model's reasoning process (shown via saliency maps). Over-parameterization provides a significant advantage in robustness against input corruption from Gaussian noise and occlusion, suggesting that excess capacity aids in learning more general representations.

## Method Summary
The authors use MNIST dataset with five binary classification tasks based on visually similar digit pairs: (0,1), (1,7), (5,6), (3,8), and (4,9). They train fully connected networks with hidden layer sizes ranging from 2 to 64 neurons, using ReLU activation, SGD optimizer with learning rate 0.03, and binary cross-entropy loss for 100 epochs. Three random seeds are used for each configuration. The study evaluates F1 score and AUC as primary metrics, with success defined as F1 > 0.90. One-shot magnitude pruning is applied post-training to assess sparsity-robustness trade-offs, and t-SNE visualizations plus saliency maps are used to analyze representation geometry and reasoning processes.

## Key Results
- Minimum required network capacity scales linearly with task complexity: 4-6 neurons for simple tasks (0-1), 12-24 neurons for complex tasks (4-9)
- Networks achieve 95% sparsity through magnitude pruning while maintaining performance, demonstrating extreme pruning resilience
- Sparse subnetworks preserve original representation geometry (t-SNE) and reasoning process (saliency maps)
- Over-parameterized networks show significantly better robustness to Gaussian noise and occlusion compared to minimal-capacity networks

## Why This Works (Mechanism)
The study demonstrates that low-capacity networks can learn efficient representations when task complexity is appropriately matched to model capacity. The pruning resilience occurs because the network learns to concentrate important weights in specific locations, making magnitude-based pruning effective at identifying non-essential connections. The preservation of representation geometry and reasoning suggests that these networks develop sparse but coherent decision boundaries that align with human-interpretable features. Over-parameterization provides a robustness advantage by creating redundant pathways that can compensate for corrupted inputs.

## Foundational Learning
- **Binary classification**: Classifying inputs into two distinct categories; needed to create controlled experimental conditions; quick check: verify output layer has 1 neuron with sigmoid activation
- **Magnitude pruning**: Removing weights with smallest absolute values; needed to assess sparse subnetwork performance; quick check: confirm pruning threshold correctly identifies bottom 95% of weights by magnitude
- **t-SNE visualization**: Dimensionality reduction technique for visualizing high-dimensional data; needed to analyze representation geometry; quick check: ensure perplexity parameter is appropriate for dataset size
- **Saliency maps**: Visualization showing input feature importance for model predictions; needed to analyze reasoning processes; quick check: verify gradient computation correctly attributes importance to input pixels
- **Cross-entropy loss**: Loss function measuring difference between predicted and actual probability distributions; needed for binary classification optimization; quick check: confirm BCE loss decreases during training
- **SGD optimization**: Stochastic gradient descent for parameter updates; needed for efficient training of simple networks; quick check: verify learning rate prevents divergence while ensuring convergence

## Architecture Onboarding
- **Component map**: 784-pixel input → Fully Connected layer (configurable size) → ReLU activation → Fully Connected layer (1 neuron) → Sigmoid output
- **Critical path**: Input flattening → Hidden layer computation → ReLU activation → Output layer computation → Sigmoid activation → Binary prediction
- **Design tradeoffs**: Small networks (2-6 neurons) minimize parameters but may underfit complex tasks; larger networks (16-64 neurons) provide robustness but risk overfitting and reduced interpretability
- **Failure signatures**: Convergence failure on complex tasks with small networks; sharp performance drop before 95% pruning; noisy saliency maps indicating learned artifacts rather than meaningful features
- **First experiments**: 1) Train network with 4 hidden neurons on 0-vs-1 task and verify F1 > 0.90; 2) Apply 95% magnitude pruning to trained 12-neuron network and confirm performance maintenance; 3) Generate t-SNE visualization comparing representations before and after pruning

## Open Questions the Paper Calls Out
- Can mutual information between individual neuron activations and class labels quantify neuron specialization in low-capacity networks?
- How does the "reasoning process" (attentional strategy) of low-capacity models shift when subjected to varying levels of input corruption?
- Do the scaling laws between task complexity, minimum capacity, and pruning robustness hold for Convolutional Neural Networks (CNNs) on complex datasets?

## Limitations
- Results based on fully connected networks on MNIST may not generalize to convolutional architectures or real-world datasets
- One-shot pruning without fine-tuning may underestimate achievable sparsity levels
- Noise corruption experiments lack specific parameter values for Gaussian variance and occlusion patterns
- Study focuses on binary classification, limiting applicability to multi-class scenarios

## Confidence
- High confidence: Linear relationship between task difficulty and minimum required capacity
- Medium confidence: 95% sparsity robustness claim
- Medium confidence: Over-parameterization robustness advantage

## Next Checks
1. Re-run experimental grid with explicit hyperparameters: batch size (128), weight initialization (Xavier uniform), and stratified 80/20 train/validation split
2. Implement iterative magnitude pruning with fine-tuning to compare against one-shot pruning results
3. Extend analysis to include convolutional baseline on same binary tasks to assess architectural differences in capacity-sparsity-robustness relationships