---
ver: rpa2
title: Image Classification Using a Diffusion Model as a Pre-Training Model
arxiv_id: '2505.06890'
source_url: https://arxiv.org/abs/2505.06890
tags:
- diffusion
- latent
- image
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a diffusion model with a representation-conditioning
  mechanism for image classification. The method uses representations from a Vision
  Transformer (ViT) to guide the denoising process in a Transformer-based diffusion
  model, enabling effective feature learning from unlabeled data.
---

# Image Classification Using a Diffusion Model as a Pre-Training Model

## Quick Facts
- arXiv ID: 2505.06890
- Source URL: https://arxiv.org/abs/2505.06890
- Reference count: 7
- This paper proposes a diffusion model with a representation-conditioning mechanism for image classification, achieving +6.15% higher accuracy and +13.60% higher F1-score compared to the strong contrastive learning baseline DINOv2 on a zero-shot classification task for hematoma detection in brain imaging.

## Executive Summary
This paper introduces a self-supervised learning framework that uses a diffusion model as a pre-training model for image classification. The method conditions the denoising process in a Transformer-based diffusion model on representations extracted from a Vision Transformer (ViT) encoder, enabling effective feature learning from unlabeled data. The approach was evaluated on a zero-shot classification task for hematoma detection in brain imaging, demonstrating superior performance compared to the strong contrastive learning baseline DINOv2.

## Method Summary
The method consists of a two-stage approach: (1) Pre-training a Representation-Conditioned Latent Diffusion Transformer (DiT) where a ViT encoder extracts representations from clean latent images and conditions the denoising process, and (2) Downstream adaptation using Diffusion Classifier Zero, which conditions on class labels instead of representations to perform classification by selecting the label that minimizes reconstruction error. The framework leverages Stable Diffusion's VAE for latent space encoding and trains the ViT encoder jointly with the denoising network.

## Key Results
- Representation-Conditioned Latent DiT achieved FID of 10.00 vs 24.42 for unconditional model on image generation
- Zero-shot classification on hematoma detection: +6.15% higher accuracy and +13.60% higher F1-score compared to DINOv2
- Joint training of ViT encoder and denoiser reduced loss to 3.39×10⁻⁴ vs 3.13×10⁻² for unconditional model (approximately 100× lower)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditioning the denoising process on intermediate representations improves both generation quality and downstream classification.
- Mechanism: A ViT encoder extracts representations r = f_φ(z₀) from the clean latent image. These representations are injected at two locations: (1) combined with the decoder output before the denoising network, and (2) at the beginning of each of the N transformer blocks within the denoiser. This guides the reverse diffusion process with semantic information.
- Core assumption: The representation-conditioning forces the denoiser to learn features that are simultaneously useful for reconstruction and discriminative tasks.
- Evidence anchors:
  - [abstract] "representations derived from a Vision Transformer (ViT) are used to condition the internal process of a Transformer-based diffusion model"
  - [Section 3.1] "the representation r is passed to two locations... combined with the output of 'decode'... combined at the beginning of each of the N blocks"
  - [Section 5.1] Representation-conditioned model achieved FID of 10.00 vs 24.42 (unconditional) and generated images most similar to original distribution

### Mechanism 2
- Claim: Jointly training the ViT encoder with the denoising network yields representations optimized for the diffusion objective, unlike pre-trained frozen encoders.
- Mechanism: Both encoder parameters φ and denoiser parameters θ are optimized jointly via the denoising loss L(θ,φ) = ||ε - g_θ(z_t, r, t)||². The encoder learns to produce representations that best support the denoising task.
- Core assumption: Representations useful for denoising will transfer effectively to downstream classification.
- Evidence anchors:
  - [Section 3.1] "the ViT encoder that encodes the representation is also trained simultaneously with the training of the denoising network"
  - [Section 5.2] Loss reduced to 3.39×10⁻⁴ (representation-conditioned) vs 3.13×10⁻² (unconditional)—approximately 100× lower
  - [corpus] SparseJEPA (arXiv:2504.16140) demonstrates sparse representation learning benefits in joint embedding architectures, suggesting representation quality matters

### Mechanism 3
- Claim: Diffusion Classifier Zero enables classification by comparing predicted latent reconstructions across class labels without explicit encoder outputs.
- Mechanism: For classification, the tuned model conditions on each candidate class label, predicts the denoised latent z'₀ = (1/γ_t)(z_t - δ_t · ĝ_θ(z_t, y, t)), and selects the class minimizing ||z₀ - z'₀||². The intuition is that the correct class yields the best reconstruction.
- Core assumption: The class-conditioned model reconstructs input more accurately when conditioned on the true label.
- Evidence anchors:
  - [Section 3.2] "Diffusion Classifier Zero obtains the image classification result as the predicted label... argmin_c(E[||z₀ - z'₀||²])"
  - [Section 5.2] Representation-conditioned model predicted original images accurately at t=800, while unconditional required t≤400, suggesting better class-conditional feature disentanglement

## Foundational Learning

- Concept: **Diffusion Models (Forward/Reverse Process)**
  - Why needed here: The entire method builds on understanding how noise is gradually added (forward) and removed (reverse) to model data distributions.
  - Quick check question: Can you explain why z_t = γ_t·z₀ + δ_t·ε represents a noisy latent at timestep t, and what the denoising network predicts?

- Concept: **Vision Transformer (ViT) and Self-Attention**
  - Why needed here: The encoder producing conditioning representations is a ViT; understanding patch embeddings and attention is necessary for debugging representation quality.
  - Quick check question: How does ViT process an image into a sequence of tokens, and what does the [CLS] token or pooled output represent?

- Concept: **Self-Supervised Learning Paradigms**
  - Why needed here: The paper positions itself against contrastive learning (DINOv2); understanding the difference between discriminative vs. generative pre-training objectives is essential.
  - Quick check question: What is the fundamental difference between contrastive learning (learning to distinguish instances) and reconstruction-based learning (learning to generate/reconstruct)?

## Architecture Onboarding

- Component map:
  Input Image (x) -> VAE Encoder (frozen, pre-trained) → z₀ (latent, 4×32×32) -> ViT Encoder (trainable) → r (representation vector) -> Add noise → z_t at random timestep t -> DiT Denoiser (trainable, 12 blocks) -> Output: Predicted noise ε̂

- Critical path:
  1. Pre-training: Train ViT encoder + DiT denoiser jointly on unlabeled data using representation-conditioning
  2. Tuning: Replace representation-conditioning with class-label-conditioning; fine-tune on small labeled dataset
  3. Testing: For each test image, compute reconstruction error under each class label; select minimum

- Design tradeoffs:
  - **Patch size 2**: Smaller patches improve generation quality but increase tokens/computation
  - **Model size B**: Chosen to balance capacity with available GPU memory (RTX 4090, 24GB)
  - **VAE encoder frozen**: Uses Stable Diffusion's pre-trained VAE; not retraining reduces complexity but may limit domain adaptation
  - **Timestep selection for classification**: Performance varies significantly with t; finding optimal t requires experimentation

- Failure signatures:
  - **High FID with low classification accuracy**: Model generates diverse images but hasn't learned discriminative features
  - **Mode collapse**: Generated samples lack diversity (check Precision/Recall metrics)
  - **Loss plateau during pre-training**: May indicate representation-conditioning isn't providing useful signal
  - **Class imbalance issues**: Medical datasets often imbalanced; F1-score more reliable than accuracy

- First 3 experiments:
  1. **Ablation: Unconditional vs. Representation-Conditioned**: Train both variants on pre-training data; compare FID and classification metrics to isolate the conditioning mechanism's contribution.
  2. **Timestep Sensitivity Analysis**: For Diffusion Classifier Zero, sweep t from 100-900 and plot classification accuracy; identify optimal t range for your dataset.
  3. **Encoder Freezing Comparison**: Train one model with joint encoder optimization (proposed) and one with frozen pre-trained ViT; compare downstream classification to validate the joint training hypothesis.

## Open Questions the Paper Calls Out
- **Open Question 1**: Does the Representation-Conditioned Latent Diffusion Transformer transfer effectively to dense prediction tasks like object detection and semantic segmentation? The paper states the framework "could benefit other vision tasks such as detection, segmentation," yet the experimental evaluation is restricted to a binary classification task on medical scans.
- **Open Question 2**: Can the "Diffusion Classifier Zero" method be optimized to remove the computational cost and sensitivity associated with selecting the optimal timestep t? The authors note that "selection of t takes time" and that classification performance is "greatly affected, especially by t," relying on optimization strategies from prior work.
- **Open Question 3**: Is the simultaneous joint training of the ViT encoder and the denoising network strictly necessary for superior performance, or would a frozen pre-trained encoder suffice? The authors distinguish their method by training the encoder simultaneously (L(θ, φ)) to "acquire representations that promote the optimization," but they do not compare this against a baseline using a frozen encoder.

## Limitations
- Computational cost: Diffusion Classifier Zero requires computing reconstruction errors for each class label, making it slower than standard feed-forward classifiers.
- Timestep sensitivity: Classification performance is heavily dependent on the selection of timestep t, requiring optimization strategies from prior work.
- Restricted evaluation: The method is validated only on a binary classification task for medical imaging, leaving transfer to other vision tasks unexplored.

## Confidence
- Method feasibility: High - The approach builds on established diffusion model and ViT architectures with clear implementation details.
- Classification performance claims: Medium - Results show significant improvement over DINOv2, but evaluation is limited to one medical imaging dataset.
- Generation quality claims: High - FID metrics demonstrate clear improvement with representation-conditioning over unconditional baseline.

## Next Checks
1. Verify the ViT encoder architecture details (variant, initialization, representation dimension) match the paper's implementation.
2. Implement and test the timestep sensitivity analysis for Diffusion Classifier Zero to identify optimal t range.
3. Conduct ablation study comparing joint training vs. frozen encoder to validate the necessity of simultaneous optimization.