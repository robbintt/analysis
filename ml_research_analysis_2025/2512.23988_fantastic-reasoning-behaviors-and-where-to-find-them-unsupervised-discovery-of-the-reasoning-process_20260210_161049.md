---
ver: rpa2
title: 'Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery
  of the Reasoning Process'
arxiv_id: '2512.23988'
source_url: https://arxiv.org/abs/2512.23988
tags:
- reasoning
- behaviors
- arxiv
- decoder
- reflection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents RISE, an unsupervised framework for discovering
  reasoning behaviors in large language models (LLMs) using sparse auto-encoders (SAEs).
  The method trains SAEs on step-level activations from chain-of-thought reasoning,
  revealing interpretable vectors in the decoder space that correspond to behaviors
  like reflection and backtracking.
---

# Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process

## Quick Facts
- arXiv ID: 2512.23988
- Source URL: https://arxiv.org/abs/2512.23988
- Reference count: 28
- One-line primary result: RISE framework uses sparse auto-encoders to discover interpretable reasoning behaviors in LLMs and enables controllable steering without retraining.

## Executive Summary
This paper presents RISE, an unsupervised framework for discovering reasoning behaviors in large language models (LLMs) using sparse auto-encoders (SAEs). The method trains SAEs on step-level activations from chain-of-thought reasoning, revealing interpretable vectors in the decoder space that correspond to behaviors like reflection and backtracking. Visualization and clustering analyses show these behaviors occupy separable regions, particularly in mid-to-late layers. Causal interventions on SAE-derived vectors can controllably amplify or suppress specific behaviors without retraining. The framework also discovers novel behaviors like response confidence, demonstrating SAEs' ability to reveal semantic and structural properties in reasoning processes. The approach generalizes across domains and tasks, offering a scalable path to both interpret and steer LLM reasoning behaviors.

## Method Summary
RISE discovers reasoning behaviors by training sparse auto-encoders on sentence-level chain-of-thought activations extracted from delimiter tokens (`<\n\n>`). The SAE learns a sparse dictionary where each decoder column approximates an atomic reasoning direction. Under incoherence and sparsity assumptions, the decoder recovers ground-truth latent directions up to permutation/scaling. The framework identifies interpretable behaviors through UMAP visualization, clustering analysis (Silhouette scores), and LLM-as-judge labeling. Causal interventions project out or add specific vector components from hidden states to amplify or suppress targeted behaviors. The method also discovers novel behaviors like confidence by optimizing a score vector to minimize output entropy. The approach is evaluated on multiple reasoning datasets and demonstrates generalizable behavior discovery and steering capabilities.

## Key Results
- RISE discovers interpretable reasoning vectors corresponding to reflection, backtracking, and confidence behaviors from unsupervised SAE training on CoT activations
- Silhouette scores and UMAP visualizations show these behaviors occupy separable regions, especially in mid-to-late layers (24-28)
- Targeted interventions on SAE-derived vectors can controllably amplify or suppress specific reasoning behaviors without retraining
- The framework generalizes across domains (MATH, GPQA, KnowLogic) and tasks, discovering consistent behavioral patterns

## Why This Works (Mechanism)

### Mechanism 1: Sparse Decomposition of Reasoning Activations
The SAE learns a sparse dictionary where each decoder column approximates an atomic reasoning direction. Under incoherence and sparsity assumptions (Theorem 1), the decoder recovers ground-truth latent directions up to permutation/scaling. Sentence-level step representations are extracted from `<\n\n>` delimiter tokens across the residual stream. Core assumption: reasoning behaviors are linearly encoded in activation space with k-sparse activation patterns over the true dictionary.

### Mechanism 2: Causal Steering via Vector Projection
For a decoder column w_i associated with a behavior, the intervention projects out (or adds) its component from hidden states: h' = h - α · w_i(w_i^T h). Negative α suppresses; positive amplifies. Injection occurs at the last token of each reasoning step. Core assumption: the SAE column direction is causally relevant to the behavior, not merely correlated.

### Mechanism 3: Novel Behavior Discovery via Entropy Minimization
Optimizing a score vector over decoder columns to minimize output entropy identifies a "confidence" direction that clusters coherently and causally affects reasoning style. Given activations {h_i}, optimize S ∈ R^D to minimize expected entropy: argmin E[-Σ p_k log p_k] where p = softmax(f_{l→L}(h + SW_decoder)). Top-scoring columns define the confidence vector.

## Foundational Learning

- **Sparse Autoencoders (SAEs)**: Core tool for decomposing activations into interpretable directions; understanding reconstruction loss, sparsity penalty (L0 or L1), and encoder-decoder structure is essential.
  - Quick check: Given an activation h, can you explain why enforcing sparsity in the latent code z helps disentangle features?

- **Linear Representation Hypothesis**: The paper's theoretical justification (Theorem 1) assumes concepts/behaviors are linear directions in activation space.
  - Quick check: If a behavior is nonlinearly encoded, would SAE decoder columns still recover it? Why or why not?

- **Activation Steering / Intervention**: The causal control mechanism relies on projecting out or adding vector components during forward passes.
  - Quick check: What is the difference between DiffMean steering (supervised contrastive pairs) and RISE's unsupervised SAE-based approach?

## Architecture Onboarding

- **Component map**: Data pipeline (CoT responses → sentence-level steps at `<\n\n>`) → SAE training (on residual stream activations) → Analysis (UMAP, Silhouette, labeling) → Intervention (projection formula)

- **Critical path**: Correct step segmentation and activation extraction → Layer selection for SAE training → Column filtering to exclude multi-behavior columns before computing steering vectors

- **Design tradeoffs**: SAE hidden dimension D: Larger D increases expressiveness but raises training cost; Layer choice: Earlier layers encode less behavioral structure; Intervention strength α: Larger magnitudes produce stronger effects but risk incoherence

- **Failure signatures**: High reconstruction error indicates SAE not capturing activation structure; Low Silhouette scores suggest behaviors not separable; Steering produces incoherent text → likely α too large or column selection entangled

- **First 3 experiments**: Replicate UMAP visualization on small dataset (100 MATH samples) with layer 24 SAE; Run negative/positive steering on reflection vectors for 10 held-out problems; Train SAE on different domain (GPQA) and compare discovered columns

## Open Questions the Paper Calls Out

### Open Question 1
What is the minimal SAE dictionary size (D) required to capture the full spectrum of reasoning behaviors, and do larger dictionaries reveal behaviors that are invisible at D=2048? The authors note they "adopt a relatively small hidden dimension of D=2048" but do not validate this assumption through ablation.

### Open Question 2
How do reasoning behaviors interact across layers when SAEs are trained jointly on multi-layer activations rather than independently on single layers? The paper trains SAEs "on a single chosen layer" but does not investigate cross-layer behavioral dependencies or compositional structure.

### Open Question 3
When suppressing one behavior (e.g., reflection), are other semantically unrelated behaviors unintentionally affected due to representational overlap in the decoder space? Figure 2 shows reflection and backtracking occupy "more overlapping representational subspaces," suggesting interventions may have off-target effects.

## Limitations
- Layer selection ambiguity: Exact layer index for SAE training is not specified, introducing uncertainty in reproducing results
- Behavior annotation method: Reliance on GPT-5 for labeling may not be publicly available, potentially affecting behavioral cluster validity
- Training duration unknown: Number of epochs/iterations for SAE training is not provided, affecting feature disentanglement quality

## Confidence
- **High Confidence**: Core mechanism of SAE-based feature discovery and causal intervention approach
- **Medium Confidence**: Novel behavior discovery via entropy minimization and generalization claims
- **Low Confidence**: Exact impact of SAE hyperparameters and applicability to other architectures

## Next Checks
1. **Layer sensitivity analysis**: Systematically train SAEs on multiple layers (20, 24, 28, final) and compare Silhouette scores, reconstruction loss, and steering effectiveness

2. **Annotation consistency test**: Replicate behavior labeling using both GPT-4o and keyword matching; compare resulting behavioral clusters and steering outcomes

3. **Hyperparameter ablation study**: Conduct systematic ablation of SAE hyperparameters (D: 1024, 2048, 4096; λ: 1e-3, 2e-3, 5e-3) while monitoring reconstruction loss, sparsity, and behavior separability