---
ver: rpa2
title: 'TUM-MiKaNi at SemEval-2025 Task 3: Towards Multilingual and Knowledge-Aware
  Non-factual Hallucination Identification'
arxiv_id: '2507.00579'
source_url: https://arxiv.org/abs/2507.00579
tags:
- hallucination
- facts
- fact
- answer
- wikipedia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MiKaNi, a multilingual and knowledge-aware
  hallucination detection system for large language models. The approach combines
  a retrieval-based fact verification model against Wikipedia with a BERT-based model
  fine-tuned to detect hallucination patterns, whose outputs are combined via support
  vector regression.
---

# TUM-MiKaNi at SemEval-2025 Task 3: Towards Multilingual and Knowledge-Aware Non-factual Hallucination Identification

## Quick Facts
- **arXiv ID**: 2507.00579
- **Source URL**: https://arxiv.org/abs/2507.00579
- **Reference count**: 40
- **Primary result**: Achieved top-10 rankings across 14 languages, including 2nd place in French (IoU: 0.6314) and 8th in English (IoU: 0.5249)

## Executive Summary
This paper presents MiKaNi, a multilingual hallucination detection system that combines retrieval-based fact verification with neural classification to identify non-factual hallucinations in large language model outputs. The approach leverages Wikipedia as a knowledge source and achieves competitive performance across 14 languages in SemEval-2025 Task 3. The system demonstrates that combining knowledge-aware retrieval with learned hallucination patterns produces robust detection capabilities, ranking in the top 10 for 8 languages including strong performance in French and English.

## Method Summary
MiKaNi employs a three-component pipeline: (1) RFVM uses GPT-4o to extract atomic facts from question-answer pairs, retrieves relevant Wikipedia passages via BM25 ranking, applies Maximal Marginal Relevance selection, and predicts hallucination probabilities; (2) BM uses multilingual BERT with POS embeddings to classify hallucinated tokens through fine-tuned fully connected layers; (3) SVR combines features from both components including POS tags, entity matches, RFVM scores, and BM outputs. The system is model-agnostic and supports languages beyond the shared task scope, with components running in parallel and outputs combined through support vector regression.

## Key Results
- Achieved top-10 rankings across 8 languages including English (IoU: 0.5249, rank 8/44) and French (IoU: 0.6314, rank 2/33)
- Demonstrated competitive performance across all 14 Mu-SHROOM languages
- Showed robustness to language variations with consistent top-tier results
- Validated multilingual capability through systematic evaluation across diverse language families

## Why This Works (Mechanism)
The system's effectiveness stems from combining complementary approaches: knowledge retrieval provides external verification against authoritative sources while learned classification captures subtle hallucination patterns. The parallel architecture allows each component to specialize in different aspects of hallucination detection, with SVR optimally combining their strengths. The multilingual design leverages language-agnostic features like POS tags and entity matching, enabling consistent performance across diverse languages.

## Foundational Learning
- **BM25 Ranking**: Probabilistic retrieval model that ranks documents by relevance score - needed for efficient Wikipedia passage selection; quick check: verify ranking scores correlate with passage relevance
- **Maximal Marginal Relevance (MMR)**: Diversification algorithm that balances relevance and redundancy in retrieved passages - needed to avoid repetitive evidence; quick check: measure diversity of selected passages
- **Support Vector Regression**: Machine learning model that predicts continuous values from feature vectors - needed to combine heterogeneous component outputs; quick check: validate SVR predictions correlate with ground truth probabilities
- **POS Tagging**: Part-of-speech annotation identifies grammatical roles of tokens - needed for feature extraction and entity matching; quick check: ensure POS tags align correctly across languages
- **Multilingual BERT**: Pretrained transformer model supporting multiple languages - needed as foundation for language-agnostic classification; quick check: verify BERT embeddings capture cross-lingual semantic similarities

## Architecture Onboarding

**Component Map**: RFVM -> SVR <- BM (parallel components)

**Critical Path**: Question/A → RFVM (GPT-4o fact extraction → Wikipedia retrieval → MMR selection → hallucination prediction) → SVR ← BM (BERT+POS → FC classification) → Hallucination probabilities

**Design Tradeoffs**: Knowledge retrieval vs. computational cost (RFVM expensive but accurate), BERT fine-tuning vs. generalization (partial layer freezing preserves pre-trained knowledge), SVR complexity vs. interpretability (combines multiple signals but opaque)

**Failure Signatures**: RFVM failures manifest as missed hallucinations when Wikipedia lacks coverage; BM failures show over-prediction on low-hallucination content (e.g., Spanish); SVR failures appear as inconsistent probability calibration across languages

**First Experiments**:
1. Baseline comparison: Run RFVM alone vs. BM alone vs. combined system to quantify individual contributions
2. Language sensitivity: Test performance drop when removing POS features from one language family
3. Wikipedia coverage analysis: Measure hallucination detection accuracy correlation with Wikipedia article availability per language

## Open Questions the Paper Calls Out
None

## Limitations
- High computational cost and API dependency due to multiple GPT-4o calls in RFVM component
- Performance contingent on Wikipedia coverage quality, which varies substantially across languages and domains
- BM component's POS embedding integration lacks specification of exact dimensions and encoding methods, affecting reproducibility

## Confidence

**High confidence**: Overall system architecture (three-component pipeline combining retrieval, classification, and regression); ranking results across 14 languages; language-agnostic design principle

**Medium confidence**: Training hyperparameters for BERT fine-tuning (learning rates, batch size, regularization); MMR implementation details; span merging algorithm parameters

**Low confidence**: Exact FC layer architecture in BM component; GPT-4o prompt specifications beyond basic format; Wikipedia retrieval ranking parameters

## Next Checks
1. Implement controlled experiments comparing BM performance with different POS embedding dimensions and encoding schemes to verify sensitivity to this underspecified component
2. Conduct ablation studies on RFVM components (fact extraction, retrieval, MMR selection) to quantify individual contributions to final performance
3. Test system generalization on languages beyond the 14 Mu-SHROOM languages using external QA datasets to validate true language-agnostic capability