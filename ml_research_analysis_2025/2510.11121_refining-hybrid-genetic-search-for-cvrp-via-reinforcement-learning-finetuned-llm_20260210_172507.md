---
ver: rpa2
title: Refining Hybrid Genetic Search for CVRP via Reinforcement Learning-Finetuned
  LLM
arxiv_id: '2510.11121'
source_url: https://arxiv.org/abs/2510.11121
tags:
- reward
- routes
- operators
- zhang
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RFTHGS, a reinforcement learning framework
  that fine-tunes a 14B-parameter reasoning LLM to generate crossover operators for
  the Hybrid Genetic Search (HGS) solver in Capacitated Vehicle Routing Problems (CVRP).
  The framework employs a multi-tiered reward function and anti-plagiarism caching
  to progressively guide the LLM from generating compilable code to executable operators
  and finally to those outperforming expert-designed ones.
---

# Refining Hybrid Genetic Search for CVRP via Reinforcement Learning-Finetuned LLM

## Quick Facts
- arXiv ID: 2510.11121
- Source URL: https://arxiv.org/abs/2510.11121
- Reference count: 40
- Key outcome: LLM-generated crossover operators significantly outperform expert-designed ones across small and large-scale CVRP instances (up to 1000 nodes), and surpass leading neuro-combinatorial baselines and commercial LLMs.

## Executive Summary
This paper introduces RFTHGS, a reinforcement learning framework that fine-tunes a 14B-parameter reasoning LLM to generate crossover operators for the Hybrid Genetic Search (HGS) solver in Capacitated Vehicle Routing Problems (CVRP). The framework employs a multi-tiered reward function and anti-plagiarism caching to progressively guide the LLM from generating compilable code to executable operators and finally to those outperforming expert-designed ones. Extensive experiments demonstrate that the LLM-generated operators significantly surpass expert-designed counterparts in HGS, maintaining consistent performance gains across small and large-scale instances (up to 1000 nodes), and outperforming leading neuro-combinatorial baselines, prompt-based methods, and commercial LLMs like GPT-4o and GPT-4o-mini.

## Method Summary
The paper proposes a reinforcement learning framework (RFTHGS) that fine-tunes a 14B-parameter reasoning LLM (Qwen-based) to generate crossover operators for HGS in CVRP. The framework uses a curriculum-based multi-tiered reward function that guides the LLM from generating compilable code to executable operators and finally to operators that outperform expert-designed ones. Anti-plagiarism caching via AST comparison prevents reward hacking through memorization of prompt examples. The model is trained using DAPO (an enhanced GRPO variant) and evaluated on CVRP instances up to 1000 nodes.

## Key Results
- LLM-generated crossover operators significantly outperform expert-designed counterparts in HGS across small and large-scale CVRP instances (up to 1000 nodes)
- The framework outperforms leading neuro-combinatorial baselines, prompt-based methods, and commercial LLMs like GPT-4o and GPT-4o-mini
- Continuous reward shaping enables sustained refinement beyond the "first success" that discrete rewards cannot achieve

## Why This Works (Mechanism)

### Mechanism 1: Curriculum-Based Multi-Tiered Reward Shaping
The reward function assigns -1 for non-compilable code, -0.8 for compilable but non-executable, -0.9 for plagiarized (via AST-matching against prompt examples), and a continuous reward up from -0.7 based on relative improvement over expert-designed operators. This creates a learnable gradient from syntactic correctness to semantic validity to optimization performance. Evidence shows density migrating from low to high reward regions across training (Section 5.3, Figure 3b).

### Mechanism 2: Anti-Plagiarism Caching via Abstract Syntax Trees
Generated operators are converted to ASTs and compared against cached ASTs of few-shot examples. Substantial matches trigger a -0.9 penalty, forcing the model to explore novel operator designs rather than copying existing ones. This structural code comparison prevents reward hacking through memorization of prompt examples.

### Mechanism 3: Continuous vs. Discrete Reward for Sustained Refinement
The continuous reward is proportional to improvement over baseline (max(-0.7, relative_improvement)), whereas a discrete alternative would give +1 for any improvement and 0 otherwise. Continuous reward maintains gradient signal after the model first beats baseline, enabling further refinement. FRTHGSc (continuous) outperforms FRTHGSd (discrete) across all problem sizes (Appendix A.1, Table 4).

## Foundational Learning

- **Hybrid Genetic Search (HGS) for CVRP**: Understanding the solver's architecture (population management, crossover + local search interplay) is essential to interpret what makes an operator "good." Quick check: Can you explain why crossover quality matters in HGS, and how it differs from the local search component's role?

- **Group Relative Policy Optimization (GRPO) and DAPO variants**: The framework uses DAPO (an enhanced GRPO) for policy updates; understanding advantage normalization, clip ranges, and token-level loss is necessary to debug training dynamics. Quick check: How does DAPO's "clip-higher" mechanism differ from standard PPO clipping, and why might this matter for code generation with long reasoning chains?

- **One-Step POMDP Formulation**: The operator generation task is framed as a single-step decision (no state transitions), which differs from typical multi-step RL. This affects how advantages are computed and how the policy learns. Quick check: Why does the paper call this a "Partially Observable" MDP, and what information is withheld from the LLM during generation?

## Architecture Onboarding

- **Component map**: Prompt Builder -> LLM (14B Qwen-based) -> Code Evaluator (compiles and executes) -> Reward Calculator (multi-tiered reward) -> RL Updater (DAPO) -> Incremental Compiler (selectively recompiles)

- **Critical path**: 1. Prompt construction (sample examples from buffer) -> 2. LLM generates 256 operators -> 3. Each operator compiled via incremental build -> 4. Executable operators run on 30 CVRP instances -> 5. Rewards computed and aggregated -> 6. DAPO updates policy via token-level gradient

- **Design tradeoffs**: Small LLM (14B) vs. Large Commercial Models trades off raw capability for specialization and API independence; Continuous vs. Discrete Reward maintains gradient signal post-success vs. simpler but plateaus after first win; Training instance size (â‰¤400) vs. Test generalization (up to 1000) relies on operator quality transferring across scales

- **Failure signatures**: Reward plateau at -0.8 (non-executable code), Reward stuck near -0.7 (cannot beat expert baseline), High plagiarism penalty rate (over-reliance on few-shot examples), Entropy collapse (model becomes deterministic)

- **First 3 experiments**: 1. Validate compilation rate baseline (generate 100 operators with base model), 2. Ablate reward tiers (train variants with only compilability, compilability+executability, full tiered reward), 3. Test AST plagiarism threshold sensitivity (vary similarity threshold 0.7, 0.8, 0.9)

## Open Questions the Paper Calls Out
None

## Limitations
- Generalization beyond CVRP remains unverified, limiting claims of broader applicability
- Reward calibration sensitivity is not analyzed; small changes to penalty values could significantly alter learning dynamics
- AST plagiarism detection only compares against prompt examples, not across generations during training

## Confidence
- **High**: Framework successfully fine-tunes LLM to generate compilable and executable crossover operators (Section 5.3, Figure 3b)
- **Medium**: LLM-generated operators consistently outperform expert-designed ones, but evaluation focuses on specific HGS solver (Section 5.4)
- **Low**: Continuous reward superiority claim based on single ablation study (Appendix A.1, Table 4) without broader comparison

## Next Checks
1. Test LLM-generated operators on TSP and other VRP variants to assess generalizability beyond CVRP
2. Vary penalty values in multi-tiered reward function to determine robustness of curriculum progression
3. Implement intra-run AST comparison to prevent copying across generations and measure impact on operator diversity and performance