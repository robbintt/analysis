---
ver: rpa2
title: Modeling Ranking Properties with In-Context Learning
arxiv_id: '2505.17736'
source_url: https://arxiv.org/abs/2505.17736
tags:
- query
- ranking
- relevance
- fairness
- diversity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an in-context learning approach for multi-objective
  ranking that eliminates the need for task-specific training. The method uses example
  rankings demonstrating desired trade-offs between objectives (like relevance, fairness,
  and diversity) as prompts for large language models to rerank retrieved documents.
---

# Modeling Ranking Properties with In-Context Learning

## Quick Facts
- arXiv ID: 2505.17736
- Source URL: https://arxiv.org/abs/2505.17736
- Authors: Nilanjan Sinhababu; Andrew Parry; Debasis Ganguly; Pabitra Mitra
- Reference count: 40
- Primary result: In-context learning with demonstration rankings improves fairness and diversity while maintaining relevance without task-specific training

## Executive Summary
This paper introduces an in-context learning approach for multi-objective ranking that eliminates the need for task-specific training. The method uses example rankings demonstrating desired trade-offs between objectives (like relevance, fairness, and diversity) as prompts for large language models to rerank retrieved documents. Experiments on four IR test collections show that the approach significantly improves fairness and diversity metrics while maintaining or improving relevance, outperforming both direct prompting and traditional supervised methods. Crucially, ablations confirm that demonstration examples are the causal factor in adapting model behavior, with adversarial examples degrading performance and random example ordering improving auxiliary objectives.

## Method Summary
The approach uses a two-stage pipeline: first-stage retrieval (BM25 or ColBERT) to get top-100 documents, then LLM reranking with in-context learning examples. For each query, a similar query is retrieved from a query log (MS MARCO), and documents for that similar query are reordered using a greedy KL-divergence minimization algorithm to match a target distribution encoding the auxiliary objective. This reordered example is prepended to the prompt with the current query and its documents. The LLM then generates a permutation of the documents. Sliding window reranking (window=20, stride=10) is used over the top-100 documents. The method works across four test collections (TREC Fairness 2022, Touché 2020, TREC DL 2019/2020) with different auxiliary objectives (fairness, diversity).

## Key Results
- Example-based in-context learning significantly improves fairness metrics (AWRF, M1) while maintaining or improving relevance (nDCG)
- Demonstration examples are the causal factor in adapting model behavior - adversarial examples (inverted distributions) degrade performance
- Random example ordering improves auxiliary objectives while ordered examples enhance relevance, revealing a trade-off
- GPT-4o-mini outperforms Llama-8B significantly, while Llama-70B achieves strong relevance but limited auxiliary gains
- The approach outperforms both direct prompting and traditional supervised methods across all tested collections

## Why This Works (Mechanism)

### Mechanism 1: Demonstration-Based Behavioral Conditioning
- Claim: Example rankings with desired property distributions causally control LLM reranking behavior for multi-objective trade-offs.
- Mechanism: The LLM infers the implicit ranking criterion from observing how documents are ordered in the demonstration, then applies this inferred criterion to the current query—without explicit instructions or gradient updates.
- Core assumption: LLMs can abstract a ranking policy from a single list-wise demonstration and transfer it to a new query with different documents.
- Evidence anchors: [abstract] "Crucially, ablations confirm that demonstration examples are the causal factor in adapting model behavior, with adversarial examples degrading performance"; [section 4.3] "Examples allow for the modeling of multiple objectives... ICL with task-guided examples enables effective optimization of auxiliary objectives while maintaining relevance"

### Mechanism 2: Target Distribution Alignment via Greedy Selection
- Claim: A KL-divergence minimization algorithm constructs demonstration orderings that encode auxiliary objectives (fairness/diversity) into example rankings.
- Mechanism: Documents are partitioned by attribute (e.g., PRO/CON stance, gender), then greedily selected to minimize KL divergence between the prefix distribution and a target distribution τ(R(Q)).
- Core assumption: The target distribution for the similar-query example generalizes to the current query; the greedy approximation yields orderings that LLMs can interpret.
- Evidence anchors: [section 3] Equation 3 defines D_{p+1} = argmin_{D∈C_{p+1}} KL(τ(R(Q)), τ(⟨D_1,...,D_p⟩ ∪ D)); [section 3, Example 3] Demonstrates greedy selection yielding ⟨D1, D3, D2, D5, D4⟩ to match target (0.6, 0.4)

### Mechanism 3: Query Similarity for Example Localization
- Claim: Using a similar query (not a fixed example) provides topical grounding that improves demonstration relevance.
- Mechanism: BM25 retrieves the most similar query from a query log (MS MARCO); documents retrieved for this similar query form the demonstration input.
- Core assumption: Topical similarity between queries transfers to similar ranking property requirements.
- Evidence anchors: [section 4.3, Table 3] "Static examples cause a substantial decline in the evaluation scores, occasionally falling below those of the base ranker. This validates that the locality of queries in ICL examples remains useful"

## Foundational Learning

- Concept: In-Context Learning (ICL)
  - Why needed here: The entire method hinges on conditioning LLMs via demonstration examples without parameter updates.
  - Quick check question: Can you explain why ICL differs from fine-tuning, and why this matters for adapting to new ranking objectives dynamically?

- Concept: List-wise Ranking with Inter-Document Dependencies
  - Why needed here: Auxiliary objectives (fairness, diversity) are properties of the ranking list, not individual documents.
  - Quick check question: Why can't point-wise scoring models directly optimize for fairness or diversity without post-hoc processing?

- Concept: KL Divergence for Distribution Matching
  - Why needed here: The example construction algorithm uses KL divergence to iteratively select documents that approximate a target attribute distribution.
  - Quick check question: Given a target distribution [0.5, 0.5] and current prefix [M, M], which candidate (M or F) minimizes KL divergence for the next position?

## Architecture Onboarding

- Component map:
  1. Query Retrieval Module: BM25 over query log → top-k similar queries (k=1 used)
  2. Example Synthesis Module: Retrieve m docs for similar query → partition by attribute → greedy KL-minimization ordering
  3. Prompt Constructor: Header + example (similar query, docs, reordered output) + current query + current docs
  4. LLM Reranker: List-wise generation of permutation (sliding window: size 20, stride 10 over top-100)
  5. Attribute Metadata: External labels (gender, stance) or induced clusters (topical diversity)

- Critical path:
  1. Query log must contain semantically similar queries (MS MARCO ~800K used)
  2. Attribute metadata must be available for target collection documents
  3. LLM must support list-wise output format "[i] > [j] > [k]"

- Design tradeoffs:
  - k=1 vs. k>1 examples: Paper follows prior work showing k=1 sufficient; may need more for complex objectives
  - Random vs. first-stage ordering of examples: Random improves auxiliary objectives; first-stage ordering improves nDCG but hurts auxiliary
  - Model size: GPT-4o-mini outperforms Llama-8B significantly; Llama-70B strong on relevance but limited auxiliary gains

- Failure signatures:
  - Adversarial examples (inverted distributions) degrade auxiliary metrics → confirms demonstration causality
  - Static examples fall below base ranker → confirms locality requirement
  - ColBERT first-stage reduces fairness gains vs. BM25 → stronger relevance signals may conflict with auxiliary objectives

- First 3 experiments:
  1. Ablation on example ordering: Compare target-aligned vs. adversarial vs. uniform vs. relevance-only examples on α-nDCG and AWRF
  2. First-stage retriever sensitivity: Run pipeline with BM25 vs. ColBERT; measure nDCG/auxiliary tradeoff gap
  3. Model scale test: Compare Llama-8B vs. Llama-70B vs. GPT-4o-mini on same ICL examples; identify minimum viable model size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does increasing the number of in-context examples (k > 1) improve multi-objective ranking effectiveness, and what is the optimal example count?
- Basis in paper: [explicit] Authors state "we fix k = 1 as gains beyond this value were found to be minimal, so we leave further parameter ablations to future work in which a single example may be insufficient."
- Why unresolved: Only single-example ICL was tested; the interaction between example quantity and multi-objective trade-offs remains unexplored.
- What evidence would resolve it: Systematic experiments varying k (e.g., 1, 3, 5, 10) across fairness and diversity tasks, measuring both relevance and auxiliary metrics.

### Open Question 2
- Question: Can the approach work effectively without requiring an existing query log, particularly in low-resource languages or domains?
- Basis in paper: [explicit] Limitations section states: "Our approach requires an existing query log, which in low information environments or low resource languages may present difficulties in adopting our approach. In future work, we look to rectify the need for a monolingual corpus."
- Why unresolved: Current method depends on MS MARCO query log for example retrieval; no alternative demonstrated.
- What evidence would resolve it: Experiments using synthetic query generation or zero-resource example construction in low-resource language settings.

### Open Question 3
- Question: Why does random example ordering improve auxiliary objective performance while ordered examples enhance relevance?
- Basis in paper: [inferred] Appendix C shows random ordering benefits auxiliary objectives while first-stage ordering improves nDCG, but the mechanism is not explained.
- Why unresolved: The trade-off between ordering strategies is empirically observed but theoretically unexplained.
- What evidence would resolve it: Analysis of attention patterns or probing studies to understand how ordering affects model behavior differently for relevance vs. auxiliary tasks.

### Open Question 4
- Question: Can demonstration-based control extend effectively to other ranking objectives beyond fairness and diversity (e.g., novelty, recency, personalization)?
- Basis in paper: [explicit] Conclusion states: "our findings present encouraging evidence for demonstration-based model adaptation as a mechanism for controlling ranking behaviour beyond the objectives investigated in this work."
- Why unresolved: Only fairness (group exposure) and diversity (topical coverage) were tested.
- What evidence would resolve it: Experiments applying the same ICL framework to additional objectives with appropriate target distribution formulations.

## Limitations
- The method requires an existing query log (MS MARCO), limiting applicability in low-resource languages or domains without query logs
- Greedy KL-divergence minimization lacks theoretical guarantees and corpus validation for optimality
- Performance depends on attribute metadata availability, with unclear methods for inferring attributes from document text
- The sliding window reranking approach may miss global optima for list-level objectives

## Confidence

**High Confidence**: The causal role of demonstration examples is well-supported by adversarial ablation results showing performance degradation when distributions are inverted. The two-stage pipeline architecture is clearly specified and experimentally validated across four collections.

**Medium Confidence**: The effectiveness of query similarity for example localization is demonstrated through static example comparisons, but the specific mechanism (why k=1 works vs. multiple examples) lacks theoretical grounding. The trade-off between relevance and auxiliary objectives is empirically observed but not analytically characterized.

**Low Confidence**: The KL-divergence minimization algorithm's optimality is assumed but not proven, and the lack of corpus evidence for this specific construction heuristic raises questions about its generalizability. The method's sensitivity to first-stage retrieval quality (BM25 vs. ColBERT) suggests fundamental coupling between retrieval and reranking stages.

## Next Checks

1. **Generalization test**: Run the pipeline with random vs. similar queries as example sources across all four collections to quantify the exact contribution of query locality vs. demonstration content alone.

2. **Distribution sensitivity analysis**: Systematically vary target distributions (from uniform to highly skewed) and measure the relationship between demonstration skew and achieved auxiliary objective scores to validate the KL-minimization mechanism.

3. **Model scaling validation**: Test GPT-4o-mini, Llama-8B, and Llama-70B on identical examples with varying numbers of documents (5, 10, 20) to identify the minimum viable model size for effective demonstration-based adaptation.