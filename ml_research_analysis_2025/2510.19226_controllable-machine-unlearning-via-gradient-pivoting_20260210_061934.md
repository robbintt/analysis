---
ver: rpa2
title: Controllable Machine Unlearning via Gradient Pivoting
arxiv_id: '2510.19226'
source_url: https://arxiv.org/abs/2510.19226
tags:
- unlearning
- methods
- class
- performance
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of machine unlearning, where
  the goal is to remove the influence of specific data from a trained model while
  maintaining model fidelity. The authors propose a novel approach, CUP (Controllable
  Unlearning by Piviting Gradient), which reframes machine unlearning as a multi-objective
  optimization (MOO) problem.
---

# Controllable Machine Unlearning via Gradient Pivoting

## Quick Facts
- **arXiv ID:** 2510.19226
- **Source URL:** https://arxiv.org/abs/2510.19226
- **Reference count:** 40
- **Primary result:** Introduces CUP, a method for controllable machine unlearning that navigates the Pareto frontier between forgetting efficacy and model fidelity via a single hyperparameter.

## Executive Summary
This paper addresses the challenge of machine unlearning, where the goal is to remove the influence of specific data from a trained model while maintaining model fidelity. The authors propose a novel approach, CUP (Controllable Unlearning by Pivoting Gradient), which reframes machine unlearning as a multi-objective optimization (MOO) problem. CUP features a unique pivoting mechanism that allows for precise navigation of the entire Pareto frontier between unlearning efficacy and model fidelity, governed by a single hyperparameter called 'unlearning intensity'. The paper also introduces the hypervolume indicator as a holistic metric to evaluate the quality and diversity of the solution set. Experimental results on various vision tasks demonstrate that CUP outperforms existing methods, achieving a superior trade-off between unlearning efficacy and model fidelity.

## Method Summary
CUP reframes machine unlearning as a multi-objective optimization problem to balance unlearning efficacy and model fidelity. It employs a "pivoting mechanism" that computes two orthogonal gradient anchors—one maximizing forgetting efficacy and the other preserving fidelity—then rotates between them using a single hyperparameter, the unlearning intensity. This allows precise navigation of the Pareto frontier without expensive weight tuning. The method also introduces the hypervolume indicator as a holistic metric to evaluate the quality and diversity of the solution set across the frontier.

## Key Results
- CUP achieves a superior trade-off between unlearning efficacy and model fidelity compared to existing methods.
- The pivoting mechanism allows navigation of the entire Pareto frontier using a single hyperparameter (unlearning intensity).
- CUP outperforms baselines on various vision tasks, demonstrating its effectiveness in controlled unlearning.

## Why This Works (Mechanism)

### Mechanism 1: Conflict Avoidance via Gradient Projection
The paper argues that standard Single-Objective Optimization (SOO) fails because aggregate gradients can degrade model fidelity during "gradient conflicts" (where improving unlearning hurts the remaining data). The algorithm projects the total gradient onto a "conflict-free space" ($G_t$). It defines two anchors: the **Efficacy Anchor** ($g_{eff}$, orthogonal to the remaining loss gradient) and the **Fidelity Anchor** ($g_{fid}$, orthogonal to the forgetting loss gradient). By constraining updates to the positive span of these anchors, the mechanism ensures the inner product with both conflicting gradients is non-negative.

### Mechanism 2: Pareto Navigation via Pivoting Intensity
CUP allows users to select a specific trade-off point on the Pareto frontier using a single intuitive hyperparameter, rather than searching arbitrary weights. The "Pivoting Gradient Principle" rotates the update vector between the Fidelity Anchor ($\gamma=0$) and the Efficacy Anchor ($\gamma=1$). The rotation uses the angle $\phi_t$ between the anchors and the unlearning intensity $\gamma \in [0,1]$ to determine the final update direction $g_\gamma$.

### Mechanism 3: Holistic Evaluation via Hypervolume
Traditional metrics (like accuracy on forgotten data) fail to capture the quality of the trade-off solution *set*. The paper adopts the **Hypervolume Indicator** to measure the volume of objective space dominated by the set of unlearned models generated by varying $\gamma$. This rewards both the proximity to the ideal "retrained" model (optimality) and the spread of solutions (diversity/controllability).

## Foundational Learning

- **Concept: Multi-Objective Optimization (MOO) & Pareto Frontiers**
  - **Why needed here:** The paper fundamentally reframes unlearning from "minimize error" to "balance forgetting vs. retention." You must understand that a Pareto-optimal solution is one where you cannot improve forgetting without hurting retention, and vice versa.
  - **Quick check question:** Can you explain why "Weighted Scalarization" (adding losses together) might fail to find certain Pareto-optimal points on a non-convex frontier?

- **Concept: Gradient Projection & Orthogonal Complements**
  - **Why needed here:** The core math of CUP relies on projecting gradients onto the orthogonal complement of the "conflicting" gradient (e.g., removing the component of the forgetting gradient that hurts the remaining data).
  - **Quick check question:** If gradient $A$ points North and gradient $B$ points East, what does the projection of $A$ onto the orthogonal complement of $B$ look like?

- **Concept: Machine Unlearning (MU) Metrics**
  - **Why needed here:** To verify the mechanism, you need to distinguish between **Unlearning Accuracy (UA)** (did we forget?), **Remaining Accuracy (RA)** (did we keep other knowledge?), and **MIA** (Membership Inference Attack score).
  - **Quick check question:** Why is high Remaining Accuracy useless if the Unlearning Accuracy is low?

## Architecture Onboarding

- **Component map:** Pre-trained model $\theta_0$ -> Loss Modules (computes $L_f$ and $L_r$) -> Gradient Projector (computes $\nabla L_f$, $\nabla L_r$, derives $g_{eff}$ and $g_{fid}$) -> Pivot Controller (takes $\gamma$, calculates $\phi_t$, synthesizes $g_{CUP}$) -> Evaluator (calculates Hypervolume over solution set).

- **Critical path:** The calculation of the angle $\phi_t$ between anchors and the subsequent trigonometric mixing. This is where the "control" happens; errors here will result in jagged or unresponsive trade-off curves.

- **Design tradeoffs:**
  - **$\gamma$ Selection:** Requires sweeping $\gamma \in [0, 1]$ to find the specific trade-off desired. The paper suggests this is more efficient than tuning weights in SOO, but it is still a search.
  - **Batch Selection:** The method relies on sampled data from $D_r$. If the sample is not representative, the "Fidelity Anchor" may point in a suboptimal direction.

- **Failure signatures:**
  - **Over-forgetting:** If $\gamma$ is set too high or the projection fails, accuracy on remaining data ($D_r$) collapses.
  - **Stagnation:** If $g_{eff}$ and $g_{fid}$ are nearly parallel or magnitudes are small, the pivot might produce negligible updates.
  - **Inconsistent Frontiers:** If the Pareto curve generated by varying $\gamma$ is discontinuous, the pivoting geometry may be invalid for the model architecture.

- **First 3 experiments:**
  1. **Toy Validation (2D):** Replicate the "Toy Example" (Figure 1) using a small network on Gaussian clusters. Visualize decision boundaries to confirm that CUP shifts boundaries for Class 2 without destroying Class 0/1 boundaries, unlike Gradient Ascent.
  2. **Gamma Sweep:** On CIFAR-10 (ResNet-18), run CUP with $\gamma = [0.1, 0.5, 0.9]$. Plot the resulting (RA, UA) points to verify they form a smooth Pareto frontier that dominates the Weighted Scalarization baseline.
  3. **Hypervolume Benchmark:** Compare CUP against a baseline (e.g., SalUn or Fine-tuning) by calculating the Hypervolume Indicator of the solution set. Confirm CUP yields a statistically larger hypervolume.

## Open Questions the Paper Calls Out
None

## Limitations
- The pivoting mechanism's effectiveness hinges on the assumption that the Pareto frontier can be well-approximated by linear interpolation between two gradient projections, which may not hold in highly non-convex optimization landscapes.
- The hypervolume metric depends critically on proper normalization of the constituent metrics (RA, UA, MIA), and improper scaling could distort comparisons.
- The method requires sweeping the γ parameter to find the desired trade-off, which, while more efficient than weight tuning, still represents a search cost.

## Confidence
- **High Confidence:** The core mathematical formulation of gradient projection and the pivoting mechanism (anchor computation, rotation formula) are sound and well-defined.
- **Medium Confidence:** The empirical superiority of CUP over baselines is supported by the experiments, but the paper's comparison set and ablation studies could be more extensive to fully validate the claims across diverse architectures and datasets.
- **Medium Confidence:** The theoretical justification for why the pivoting mechanism avoids "gradient conflicts" is logical, but a deeper analysis of failure modes or conditions under which the orthogonal projection could lead to stagnation would strengthen the claims.

## Next Checks
1. **Hyperparameter Sensitivity:** Conduct a thorough sensitivity analysis of CUP's performance to the batch size used for computing the anchors, and to the frequency of anchor re-computation during training. This will validate the stability of the gradient projections.
2. **Failure Mode Analysis:** Intentionally design scenarios where the gradients for forgetting and remaining data are highly aligned or anti-aligned. Analyze whether CUP's update mechanism degrades (e.g., stagnation or divergence) and compare its robustness to baselines.
3. **Metric Ablation:** Perform an ablation study on the hypervolume metric by comparing it to simpler Pareto frontier summaries (e.g., just the convex hull area). This will validate that the chosen reference point and normalization scheme are appropriate and that hypervolume is indeed a more informative measure for this problem.