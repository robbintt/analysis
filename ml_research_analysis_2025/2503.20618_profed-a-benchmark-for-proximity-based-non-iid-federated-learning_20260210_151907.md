---
ver: rpa2
title: 'ProFed: a Benchmark for Proximity-based non-IID Federated Learning'
arxiv_id: '2503.20618'
source_url: https://arxiv.org/abs/2503.20618
tags:
- data
- learning
- federated
- available
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ProFed is a benchmark for proximity-based non-IID federated learning
  that addresses the gap in existing FL benchmarks which ignore geographic clustering
  of devices. The key idea is to introduce subregions where data is non-IID across
  regions but IID within regions, enabling realistic evaluation of clustered federated
  learning algorithms.
---

# ProFed: a Benchmark for Proximity-based non-IID Federated Learning

## Quick Facts
- arXiv ID: 2503.20618
- Source URL: https://arxiv.org/abs/2503.20618
- Reference count: 40
- Current state-of-the-art FL algorithms struggle with extreme spatial clustering (accuracy drops from 80% IID to 50% Hard partitioning)

## Executive Summary
ProFed introduces a benchmark for proximity-based non-IID federated learning that addresses the gap in existing FL benchmarks which ignore geographic clustering of devices. The benchmark creates subregions where data is non-IID across regions but IID within regions, enabling realistic evaluation of clustered federated learning algorithms. Experiments with FedAvg, FedProx, and Scaffold show that all algorithms perform well under IID conditions (>95% accuracy) but experience significant performance drops under non-IID conditions, with accuracy falling to 50% under hard partitioning.

## Method Summary
ProFed implements established data partitioning methods (IID, Dirichlet with α=0.5, and hard partitioning) on popular computer vision datasets (MNIST, FashionMNIST, Extended MNIST, CIFAR-10, CIFAR-100) from PyTorch. The key mechanism introduces subregions where data is non-IID across regions but IID within regions, creating "clustered heterogeneity." The benchmark uses an MLP architecture (128 hidden neurons) with 30 global rounds, 2 local epochs per round, and ADAM optimizer to evaluate algorithm performance under varying levels of data heterogeneity.

## Key Results
- All algorithms achieve >95% accuracy under IID conditions
- Performance drops to 90% under Dirichlet partitioning (α=0.5) and 81% under hard partitioning for FedAvg
- Extreme spatial clustering causes accuracy to fall to 50% across all evaluated algorithms
- Current state-of-the-art algorithms (FedProx, Scaffold) still struggle with clustered heterogeneity despite regularization mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Regional clustering reveals performance degradation patterns that random device-level partitioning masks.
- Mechanism: The benchmark first partitions data heterogeneously across subregions (using Dirichlet or hard partitioning), then distributes data homogeneously within each subregion to devices. This creates "clustered heterogeneity" where intra-region data is IID but inter-region data is non-IID.
- Core assumption: Devices in geographic proximity experience similar environments and make comparable observations, leading to similar data distributions.
- Evidence anchors:
  - [abstract] "Such scenarios result in IID data within specific regions but non-IID data across regions."
  - [Section III-A] Formalizes that samples from the same region Θi have negligible difference m(d, d') ≤ δ, while samples from different regions exceed this bound.
  - [corpus] Related work on Clustered FL (survey paper 2501.17512) confirms that grouping clients by data similarity is an established approach, though ProFed's geographic clustering is distinct.
- Break condition: If real-world devices within the same geographic region actually have heterogeneous data (e.g., different user behaviors in same location), the clustered heterogeneity assumption fails.

### Mechanism 2
- Claim: Controllable skewness through partitioning parameters enables systematic stress-testing of FL algorithms.
- Mechanism: Three partitioning methods with increasing heterogeneity—IID (baseline), Dirichlet distribution (α=0.5 for moderate skew), and hard partitioning (each region sees only a subset of labels)—create a difficulty gradient.
- Core assumption: The α=0.5 Dirichlet parameter represents a "moderate" level of heterogeneity suitable for evaluating FL algorithms.
- Evidence anchors:
  - [Section III-B-a] "α values typically range from 0.1 to 1.0... A value of α = 0.5 is commonly used."
  - [Section IV-B] Results show accuracy drops from 95% (IID) to 90% (Dirichlet) to 81% (Hard) for FedAvg.
  - [corpus] Weak direct corpus evidence on optimal α values; this parameter choice draws from cited FL literature.
- Break condition: If the relationship between α and real-world heterogeneity is not validated, benchmark results may not generalize to actual deployments.

### Mechanism 3
- Claim: Existing state-of-the-art FL algorithms (FedProx, Scaffold) designed for non-IID data still struggle with extreme spatial clustering.
- Mechanism: Even algorithms with regularization terms (FedProx) or control variates (Scaffold) experience significant accuracy drops under hard partitioning, suggesting current approaches don't fully address clustered heterogeneity.
- Core assumption: The MLP architecture (128 hidden neurons) and hyperparameters (30 rounds, 2 local epochs) are sufficient to reveal algorithmic differences rather than model capacity limitations.
- Evidence anchors:
  - [Section IV-B] "None of the evaluated algorithms successfully handle extreme levels of data skewness... validation accuracy drops from 80% in the case of IID data to 50%."
  - [Table II] Scaffold achieves 0.889 ± 0.06 under Dirichlet but drops to 0.81 ± 0.01 under Hard partitioning.
  - [corpus] Paper 2503.17070 ("A Thorough Assessment of the Non-IID Data Impact in Federated Learning") corroborates that non-IID data remains an open problem with notable consequences.
- Break condition: If deeper models or different hyperparameters significantly close the performance gap, the observed degradation may reflect experimental setup rather than fundamental algorithmic limitations.

## Foundational Learning

- Concept: **Federated Learning basics (client-server training loop)**
  - Why needed here: ProFed assumes familiarity with the 4-step FL process (initialization → local learning → model sharing → aggregation) to understand what's being benchmarked.
  - Quick check question: Can you explain why FedAvg performs averaging of local models rather than aggregating raw data?

- Concept: **Non-IID data distributions (label skew)**
  - Why needed here: The entire benchmark is designed around label skew where Pk(y) ≠ Pj(y) across clients. Understanding this is essential to interpret the partitioning methods.
  - Quick check question: Given two clients, if Client A has 90% class 1 and 10% class 2, while Client B has 10% class 1 and 90% class 2, is this label skew or feature skew?

- Concept: **Dirichlet distribution for data partitioning**
  - Why needed here: The Dirichlet-based partitioning with concentration parameter α is a core mechanism. Lower α = more skewed distributions.
  - Quick check question: If α = 0.1 produces extreme skew and α = 1.0 produces near-uniform distribution, what would you expect from α = 0.5?

## Architecture Onboarding

- Component map: Partitioner -> partition() -> subregions_distributions_to_devices_distributions() -> Subset objects for each device

- Critical path:
  1. Download dataset via `download_dataset('MNIST')` → TorchVision automatically handles this
  2. Split into train/validation via `train_validation_split(dataset, 0.8)`
  3. Apply partitioning via `partition('Dirichlet', training_set, areas=5)`
  4. Distribute to devices via `subregions_distributions_to_devices_distributions(partitioning, number_of_devices=50)`
  5. Each device receives a `Subset` object for training

- Design tradeoffs:
  - Fixed datasets (MNIST, CIFAR, etc.) vs. custom data: Currently supports only TorchVision datasets; custom partitioning requires user-provided distribution matrix
  - Label skew only: Does not model feature skew or quantity skew
  - Static regions: Number of subregions is fixed; no dynamic clustering during training
  - MLP architecture in experiments: Results may differ with CNNs or transformers

- Failure signatures:
  - If accuracy under IID drops below 90%, check: model architecture, learning rate, or data loading
  - If Dirichlet and Hard show similar accuracy, verify: α parameter is correctly applied, hard partitioning actually restricts labels per region
  - If variance is extremely high across seeds, check: random seed handling in partitioning logic
  - If device datasets are empty, verify: `number_of_devices` is reasonable relative to dataset size and number of subregions

- First 3 experiments:
  1. **Reproduce baseline**: Run FedAvg on MNIST with IID partitioning, 3 subregions, 30 rounds. Expect >95% accuracy. This validates your setup matches the paper.
  2. **Introduce heterogeneity**: Same setup but with Dirichlet partitioning (α=0.5). Expect ~90% accuracy with higher variance. This confirms the benchmark correctly applies non-IID splits.
  3. **Test algorithm robustness**: Compare FedAvg vs. FedProx vs. Scaffold under Hard partitioning with 9 subregions. Expect all algorithms to struggle (~50-80% accuracy). This reveals which algorithm handles extreme spatial clustering best.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the ProFed benchmark be extended to support non-classification tasks such as regression and time-series prediction while maintaining the proximity-based non-IID structure?
- Basis in paper: [explicit] The authors state in the conclusion: "Future research directions could further extend ProFed by incorporating additional tasks (e.g., regression, time-series prediction)"
- Why unresolved: The current implementation only supports classification tasks with datasets like MNIST and CIFAR, which have discrete labels. Continuous targets require different partitioning logic.
- What evidence would resolve it: Implementation of ProFed extensions for regression and time-series datasets with appropriate partitioning strategies, demonstrating that the regional heterogeneity concept transfers to non-classification settings.

### Open Question 2
- Question: How does modeling heterogeneous computational power across devices affect FL algorithm performance and convergence under the ProFed benchmark?
- Basis in paper: [explicit] The authors list "modeling heterogeneous computational power across devices" as a future research direction in the conclusion.
- Why unresolved: Current experiments assume uniform device capabilities with identical local training epochs. Real-world edge devices vary significantly in computational resources.
- What evidence would resolve it: Experiments with variable local epoch counts, dropout rates, and straggler simulation across regions, showing performance degradation patterns for FedAvg, FedProx, and Scaffold.

### Open Question 3
- Question: Can integrating fine-grained neighborhood relationships among clients (beyond regional clustering) improve performance in clustered federated learning scenarios?
- Basis in paper: [explicit] The authors state: "Another promising avenue is integrating the concept of neighborhoods among clients, as certain algorithms (e.g., PBFL) leverage distributed localized networks for cluster formation."
- Why unresolved: ProFed currently uses a hierarchical model (regions contain devices) without modeling device-to-device proximity that could enable peer-to-peer coordination.
- What evidence would resolve it: Implementing neighborhood topology within regions, then evaluating algorithms like PBFL on ProFed to measure accuracy gains from exploiting fine-grained spatial relationships.

### Open Question 4
- Question: How robust are FL algorithms when feature skew (rather than label skew) is the primary source of regional heterogeneity?
- Basis in paper: [explicit] The authors mention "exploring different types of data skewness—for instance, feature skewness through Gaussian noise" as a future direction.
- Why unresolved: All ProFed experiments address only label skew. Section II-B explicitly defines feature skew as a distinct non-IID category where feature distributions differ while labels remain consistent.
- What evidence would resolve it: Applying region-specific feature transformations (noise, style variations) while maintaining identical label distributions, then comparing algorithm performance against label skew results.

## Limitations
- Assumes geographic proximity implies data similarity, which may not hold in practice
- Only models label skew, ignoring feature skew and quantity skew common in real deployments
- Uses MLP architecture which may not fully reveal algorithmic differences that deeper models would expose

## Confidence
- **High confidence**: The core contribution of introducing proximity-based non-IID benchmarks is well-founded and addresses documented gaps in FL evaluation.
- **Medium confidence**: The experimental results showing performance degradation under non-IID conditions align with established literature on FL challenges.
- **Low confidence**: The assumption that geographic clustering accurately reflects real-world data distributions needs empirical validation in actual deployment scenarios.

## Next Checks
1. Validate that devices within the same geographic region actually have more similar data distributions than devices from different regions in real-world deployments.
2. Test whether deeper neural architectures (CNNs, transformers) show different performance patterns under the same non-IID conditions.
3. Extend the benchmark to include feature skew and quantity skew to evaluate algorithm robustness across all common non-IID patterns.