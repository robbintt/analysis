---
ver: rpa2
title: 'Manifold Trajectories in Next-Token Prediction: From Replicator Dynamics to
  Softmax Equilibrium'
arxiv_id: '2508.21186'
source_url: https://arxiv.org/abs/2508.21186
tags:
- replicator
- simplex
- temperature
- manifold
- dynamics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the conceptual question of what the decoding
  step in large language models looks like as a dynamical process, specifically whether
  the intuition of "manifold traversal" during decoding can be formalized as a theorem.
  The core method idea is to model next-token decoding as a constrained variational
  problem on the probability simplex, where the softmax output distribution is the
  unique maximizer of a free energy objective.
---

# Manifold Trajectories in Next-Token Prediction: From Replicator Dynamics to Softmax Equilibrium

## Quick Facts
- **arXiv ID**: 2508.21186
- **Source URL**: https://arxiv.org/abs/2508.21186
- **Reference count**: 17
- **Primary result**: Next-token decoding follows a smooth manifold trajectory converging to softmax equilibrium, formalizing the "manifold traversal" intuition as a theorem.

## Executive Summary
This paper establishes a rigorous mathematical framework for understanding next-token prediction in large language models as a dynamical process on the probability simplex. By modeling the softmax output distribution as the unique maximizer of a free energy objective, the paper derives the discrete multiplicative-weights update as an entropic mirror ascent and its continuous-time limit as replicator dynamics. The central theorem proves that for fixed context and temperature, the output distribution follows a smooth trajectory converging to the softmax equilibrium, providing a formal foundation for the intuitive notion of "manifold traversal" during decoding.

## Method Summary
The paper addresses next-token decoding as a constrained variational problem on the probability simplex. For fixed context scores and temperature, the softmax distribution maximizes a free energy functional combining expected score and entropy. The discrete multiplicative-weights update emerges as entropic mirror ascent, monotonically increasing free energy. Taking the continuous-time limit yields replicator dynamics, whose trajectories converge to the softmax equilibrium. The framework proves temperature acts as exact time rescaling and that top-k/nucleus sampling restricts flow to faces with identical guarantees. Path-dependent score adjustments are qualitatively linked to loop-like, hallucination-style behavior.

## Key Results
- The softmax output distribution is the unique maximizer of a free energy functional on the probability simplex
- The discrete multiplicative-weights update monotonically increases free energy and converges to softmax equilibrium
- The continuous-time replicator flow inherits convergence guarantees and temperature acts as exact time rescaling

## Why This Works (Mechanism)

### Mechanism 1: Variational Characterization of Softmax
The softmax output distribution is the unique maximizer of the free energy functional $F(p) = \langle p, s \rangle + TH(p)$ on the probability simplex. This concavity ensures a unique solution for any fixed scores $s$ and temperature $T > 0$. The mechanism breaks when $T \to 0$ (non-unique maximizer) or when scores depend on $p$ (potential structure lost).

### Mechanism 2: Discrete Mirror Ascent = Multiplicative Weights
The multiplicative-weights update $p_i(t+1) \propto p_i(t) \exp((\eta/T)s_i)$ is derived as the unique solution to a KL-regularized optimization, guaranteeing monotonic free energy increase. This KL-regularization is essential for the multiplicative form and ascent property. The mechanism fails if the KL term is replaced or removed.

### Mechanism 3: Continuous-Time Limit = Replicator Flow
As the step size $\eta \to 0$, the discrete update converges to replicator ODE $\dot{p}_i = \frac{1}{T} p_i (s_i - \bar{s})$, which has the softmax as its unique interior equilibrium. The free energy serves as a Lyapunov function ensuring convergence. The mechanism assumes smooth vector fields and fails when scores are time-varying without potential structure.

## Foundational Learning

- **Probability Simplex**: All distributions analyzed lie on $\Delta^{V-1} = \{p \in \mathbb{R}^V : p_i \geq 0, \sum_i p_i = 1\}$. Why needed: The output distribution space is constrained. Quick check: Can you state the normalization constraint for a 3-token vocabulary?
- **Shannon Entropy**: Entropy $H(p) = -\sum_i p_i \log p_i$ regularizes the free energy objective. Why needed: Entropy balances exploitation and exploration in the objective. Quick check: What is the entropy of the uniform distribution over $V$ tokens?
- **KL Divergence**: The discrete update uses $D_{KL}(p \| q)$ as a proximity regularization. Why needed: KL divergence measures distributional proximity in the mirror update. Quick check: Is KL divergence symmetric? Why might this matter for the mirror update?

## Architecture Onboarding

**Component map**: Free energy functional $F(p)$ → Discrete mirror ascent step → Continuous-time replicator ODE → Equilibrium softmax $\pi$

**Critical path**: Understanding how softmax arises as the variational maximizer → Deriving the mirror update → Passing to the ODE limit → Proving convergence

**Design tradeoffs**: Analysis assumes fixed scores; extending to $p$-dependent scores may break potential structure. The ODE is a theoretical limit, not a practical decoder.

**Failure signatures**: Temperature approaching zero causes premature convergence to suboptimal modes. Strong path dependence in scores can create loops or non-convergent behavior.

**First 3 experiments**:
1. Implement the multiplicative-weights update for a small vocabulary and visualize $p(t)$ trajectories on the simplex
2. Vary temperature $T$ and verify that trajectories are reparameterizations of the same path
3. Introduce a simple $p$-dependent perturbation to $s$ and observe whether convergence is preserved or cycles appear

## Open Questions the Paper Calls Out

**Open Question 1**: Can the output-level replicator dynamics be rigorously derived as a projection of a contact-type flow defined over hidden state manifolds? The paper outlines a "bulk–fold–screen construction" where hidden states are coordinates on a high-dimensional manifold, but the formal derivation is deferred to future work.

**Open Question 2**: Under what specific conditions do non-potential (curl) components in path-dependent score adjustments $s(p)$ cause the decoding trajectory to form limit cycles or "hallucination" loops? The paper notes that non-potential preference fields can yield loops but states that formal analysis of path-dependent $s(p)$ lies beyond its scope.

**Open Question 3**: How does the convergence to softmax equilibrium change when the score vector $s$ is not fixed but varies dynamically during model training? The paper explicitly states that training dynamics are outside its scope and makes no claims about convergence under weight updates.

## Limitations

- The analysis assumes fixed score vectors, which may not hold in practical LLM decoding where scores depend on evolving context
- The connection between path-dependent dynamics and hallucination behavior remains qualitative without empirical validation
- The ODE limit provides theoretical insight but is not directly applicable to practical discrete decoding

## Confidence

- **High Confidence**: Variational characterization of softmax as the unique maximizer of the free energy functional
- **Medium Confidence**: Discrete mirror ascent property and convergence to softmax equilibrium for fixed scores
- **Medium Confidence**: Continuous-time limit and replicator dynamics convergence with temperature rescaling
- **Low Confidence**: Connection between path-dependent dynamics and hallucination-like behavior

## Next Checks

1. **Empirical Path Dependence**: Implement a controlled experiment where scores depend on the current distribution $s(p)$ through a simple parametric model. Measure whether trajectories exhibit convergence, divergence, or cyclic behavior, and correlate with observed generation quality.

2. **Temperature-Rescaling Test**: For a fixed score vector, numerically integrate the replicator ODE at multiple temperatures. Verify whether trajectories are perfect reparameterizations by testing if $p(t; T_1) = p(\alpha t; T_2)$ for appropriate $\alpha$.

3. **Discrete vs Continuous Gap**: Systematically compare discrete multiplicative-weights trajectories with their continuous ODE approximations across different step sizes $\eta$. Quantify the deviation from theoretical predictions and identify the regime where the ODE limit provides accurate guidance.