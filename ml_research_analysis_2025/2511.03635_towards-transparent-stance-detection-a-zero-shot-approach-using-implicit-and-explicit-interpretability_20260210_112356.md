---
ver: rpa2
title: 'Towards Transparent Stance Detection: A Zero-Shot Approach Using Implicit
  and Explicit Interpretability'
arxiv_id: '2511.03635'
source_url: https://arxiv.org/abs/2511.03635
tags:
- stance
- rationales
- iris
- implicit
- rationale
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IRIS, an interpretable zero-shot stance detection
  framework that uses both implicit (text sequences) and explicit (linguistic features)
  rationales. IRIS treats stance detection as an information retrieval ranking task
  to identify the relevance of rationales for different stances without requiring
  ground truth, ensuring inherent interpretability.
---

# Towards Transparent Stance Detection: A Zero-Shot Approach Using Implicit and Explicit Interpretability

## Quick Facts
- **arXiv ID:** 2511.03635
- **Source URL:** https://arxiv.org/abs/2511.03635
- **Reference count:** 26
- **Primary result:** Zero-shot stance detection framework achieving up to 12.62% improvement over SOTA baselines

## Executive Summary
This paper introduces IRIS, an interpretable zero-shot stance detection framework that treats stance detection as an information retrieval ranking task. IRIS extracts both implicit (text sequences) and explicit (linguistic features) rationales without requiring ground truth labels for those rationales. The framework uses a pre-trained ranker to score rationale relevance to stance-specific documents, selects diverse rationales via KL-divergence, and employs majority voting for final prediction. Experiments on four benchmark datasets demonstrate strong performance across varying training data levels (10%-50%) with significant improvements over state-of-the-art baselines.

## Method Summary
IRIS uses a three-stage pipeline: (1) Llama-3.1-8B-Instruct generates implicit rationales (text subsequences) and explicit rationales (linguistic measures from LIWC framework including empathy, absolutism, action, concrete, agency, communion, and approach); (2) FlagReranker scores the relevance of implicit rationales against three constructed documents representing "favor," "against," and "neutral" stances; (3) A diverse rationale selection mechanism using KL-divergence selects top-k rationales, which are then encoded and classified via majority voting. The system is trained with a custom "Reward-Punish" loss that adjusts relevance rankings based on downstream classification success.

## Key Results
- IRIS outperforms state-of-the-art baselines by up to 12.62% on VAST dataset
- Strong generalization across four benchmark datasets (VAST, EZ-STANCE, P-Stance, RFD) with varying training data (10%-50%)
- Human and automatic evaluations confirm high quality and interpretability of extracted rationales
- Ablation studies show significant performance drops when removing key components (ranker ablation drops F1 to ~57-65)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Treating stance detection as an information retrieval (IR) ranking task allows the system to assign relevance scores to text rationales for specific stance categories without requiring explicit ground-truth labels for those rationales.
- **Mechanism:** The system uses a pre-trained ranker (FlagReranker) to compare extracted text subsequences (rationales) against three constructed documents representing "favor," "against," and "neutral" contexts. The resulting relevance scores serve as proxies for stance alignment.
- **Core assumption:** The external documents constructed from benchmark datasets sufficiently cover the semantic space of the unseen targets and rationales, and the ranker can transfer relevance judgments to the stance domain.
- **Evidence anchors:**
  - [abstract] "IRIS considers stance detection as an information retrieval ranking task... without requiring the ground-truth of rationales."
  - [section 3.2] "We consider each rationale with the target as a query and prepare a set of 3 documents containing various posts... corresponding to 'favor', 'against', and 'neutral' stances."
  - [corpus] Related work in ZSSD often relies on contrastive learning; the IR ranking approach is a distinct strategy supported by the paper's experimental results.
- **Break condition:** If the constructed documents contain data too dissimilar from the test domain (low cosine similarity), or if the rationales are ambiguous, the ranker may assign near-uniform scores, failing to differentiate stances.

### Mechanism 2
- **Claim:** Integrating implicit (text subsequences) and explicit (linguistic features) rationales provides a multi-modal signal that decodes both the "what" (content) and the "how" (tone/emotion) of the stance.
- **Mechanism:** The system extracts word sequences as implicit rationales and psycholinguistic metrics (e.g., empathy, absolutism via LIWC) as explicit rationale. These are encoded separately and concatenated, allowing the classifier to weigh both the argument content and the communicative style.
- **Core assumption:** Linguistic features derived from frameworks like LIWC correlate strongly with stance intensity and direction (e.g., "absolutist language could indicate strong opposition"), and LLMs can reliably extract these features.
- **Evidence anchors:**
  - [abstract] "...explicit rationales based on communicative features help decode the emotional and cognitive dimensions of stance."
  - [section 3.1] "...linguistic measures of communication dynamics, such as empathy, absolutism, action... could offer deeper insights into how and why a user adopts a certain attitude."
- **Break condition:** If the input text is purely factual or sarcastic without standard linguistic markers, the explicit rationale may provide noisy or neutral signals, reducing classification accuracy.

### Mechanism 3
- **Claim:** A "Reward-Punish" loss mechanism stabilizes the pipeline by backpropagating the utility of selected rationales to the ranking stage.
- **Mechanism:** The loss function ($L_{rp}$) modifies the training signal based on whether the selected "relevant" rationales led to a correct final prediction (reward) or an incorrect one (punish). This creates a feedback loop that refines the relevance determiner.
- **Core assumption:** The relevance scores are plastic enough to be adjusted by downstream classification success, and the specific $\beta$ value (0.1) effectively balances exploration vs. constraint.
- **Evidence anchors:**
  - [section 3.4] "This custom loss calculates how much the relevance ranking phase should reward/penalize depending on whether or not the rationales selected resulted in the correct stance."
  - [figure 7 description] "...$\beta = 0.1$ performs best... encouraging IRIS to explore and learn useful patterns without being overly constrained."
- **Break condition:** If the initial ranking scores are extremely confident but wrong (high confidence error), the gradient from the punishment term may be insufficient to correct the relevance determination during training.

## Foundational Learning

- **Concept: Information Retrieval (IR) Re-Ranking**
  - **Why needed here:** IRIS relies on a reranker (FlagReranker) not just to retrieve, but to *score* the relationship between a text snippet and a broad stance category.
  - **Quick check question:** How does a cross-encoder reranker differ from a simple cosine similarity search in terms of computational cost and accuracy?

- **Concept: Psycholinguistic Feature Extraction (LIWC)**
  - **Why needed here:** The "explicit rationale" depends on understanding linguistic dimensions like "absolutism" or "agency," which are derived from the LIWC framework.
  - **Quick check question:** Can you identify a sentence where the sentiment is neutral, but the "absolutist" language score might be high?

- **Concept: Diverse Selection via KL-Divergence**
  - **Why needed here:** To avoid selecting redundant rationales, the system uses KL-divergence to ensure the selected set matches a target distribution of stance subgroups.
  - **Quick check question:** Why is maintaining a distribution over subgroups (favor/against/neutral) important for the final majority voting mechanism?

## Architecture Onboarding

- **Component map:** LLM Rationale Generator -> FlagReranker -> Grouping & Selection -> Classifier (Sentence Encoder + Dense Layers) -> Majority Vote
- **Critical path:** The quality of the **LLM-generated rationales**. If the LLM fails to extract the correct text spans in the first stage, the subsequent ranking and classification stages cannot recover the correct stance (garbage in, garbage out).
- **Design tradeoffs:**
  - **Latency vs. Interpretability:** The pipeline requires a generation step (LLM) and a ranking step before classification, making it significantly slower than a fine-tuned BERT classifier but providing explicit rationales.
  - **Generalizability vs. Specificity:** Using a general ranker trained on mixed documents allows zero-shot capability but may lose precision on highly niche domain-specific slang compared to a fine-tuned target-specific model.
- **Failure signatures:**
  - **Empty Subgroups:** If the input text is short or uniform, the "Diverse Rationale Selection" may fail to find 'k' rationales for specific stances, triggering the fallback mechanism.
  - **Contradictory Rationales:** If implicit rationales suggest "favor" but explicit linguistic rationales suggest "against" (e.g., sarcasm), the majority vote may oscillate or default to "neutral" incorrectly.
- **First 3 experiments:**
  1. **Sanity Check (Rationale Quality):** Run the LLM prompts on 10 samples and manually verify if the extracted "implicit rationales" actually align with the stance.
  2. **Ranker Ablation:** Replace FlagReranker with a simple cosine similarity search (using the same embeddings) to quantify the performance drop (Section 5.3/Table 7 suggests a significant drop to ~57-65 F1).
  3. **Hyperparameter Sensitivity (k):** Vary the number of selected rationales ($k$) on a validation set to find the "elbow" point where adding more rationales introduces noise (suggested $k=3$ in paper).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can IRIS maintain interpretability and performance in truly low-resource settings (e.g., <10% training data) or across languages without substantial modification to embeddings and ranking components?
- **Basis in paper:** [explicit] The authors state: "In the future, we plan to explore these advances and focus on the interpretability of low-resource stance detection" and note in Limitations: "We may need to explore other embeddings and pre-trained rankers if used for other languages."
- **Why unresolved:** Current experiments only test down to 10% training data and focus exclusively on English datasets; the framework's language-agnostic transferability remains unvalidated.
- **What evidence would resolve it:** Experiments on multilingual datasets (e.g., tweets in Spanish, Arabic) and settings with 1-5% training data, evaluating whether current NV-Embed-v2 and FlagReranker components transfer effectively or require language-specific alternatives.

### Open Question 2
- **Question:** How should the diverse rationale selection parameter (k) adapt to varying input text lengths and complexity, particularly when documents contain multiple conflicting stance signals?
- **Basis in paper:** [inferred] The paper uses k=3 for most datasets but notes using k=7 for RFD due to longer texts (416 avg words vs. 30-100 for others), with no systematic methodology for determining optimal k.
- **Why unresolved:** The threshold is empirically set without analysis of how text length, discourse complexity, or multi-stance content should inform selection parameter choices.
- **What evidence would resolve it:** Systematic ablation varying k across documents of different lengths and multi-stance texts, measuring performance against rationale diversity metrics and human judgments of completeness.

### Open Question 3
- **Question:** How robust is IRIS's performance to the quality of the underlying LLM (Llama 3.1) used for rationale generation, particularly when extracted rationales contain noise, redundancy, or miss key evidence?
- **Basis in paper:** [inferred] Human evaluation (Example 5 in Table 14) shows implicit rationales sometimes miss the most relevant justification or include redundancy, yet the framework's reliance on LLM-generated rationales was not stress-tested against degraded input quality.
- **Why unresolved:** The pipeline assumes LLM rationale extraction is reliable, but the paper notes LLMs achieved only 0.617 F1 on rationale extraction compared to ground truth before introducing the ranking stage.
- **What evidence would resolve it:** Controlled experiments injecting varying levels of noise/omissions into LLM-generated rationales, measuring downstream classification degradation and identifying failure modes in the relevance ranking stage.

## Limitations
- **Ranker Dependence:** Performance is tightly coupled to FlagReranker's ability to generalize relevance judgments to unseen stance domains
- **Prompt Sensitivity:** Quality of generated rationales heavily depends on specific LLM prompts that are not fully specified
- **Fallback Mechanisms:** Impact of fallback procedures for empty subgroups in diverse selection is not fully explored

## Confidence
- **High Confidence:** The core mechanism of treating stance detection as an IR ranking task and the integration of implicit/explicit rationales are well-supported by experimental results and ablation studies
- **Medium Confidence:** The effectiveness of the "Reward-Punish" loss mechanism and the specific choice of hyperparameters (e.g., Î²=0.1, k=3) are demonstrated, but could benefit from more extensive sensitivity analysis
- **Low Confidence:** The exact document preparation for the ranker and the full prompt templates for the LLM are not fully specified, creating potential reproducibility challenges

## Next Checks
1. **Rationale Quality Validation:** Run the LLM prompts on 50 unseen samples and manually verify if the extracted implicit rationales align with the actual stance. Compare the F1 score against human annotations to validate the baseline quality.
2. **Ranker Ablation with Domain-Specific Data:** Replace the FlagReranker with a cosine similarity search using embeddings fine-tuned on the specific target domain. Measure the performance drop to quantify the benefit of the cross-encoder reranker.
3. **Hyperparameter Sweep for Diverse Selection:** Vary the number of selected rationales (k) and the KL-divergence threshold on a validation set. Identify the point where adding more rationales introduces noise, and determine the optimal balance between diversity and redundancy.