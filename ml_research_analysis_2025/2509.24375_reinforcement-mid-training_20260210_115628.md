---
ver: rpa2
title: Reinforcement Mid-Training
arxiv_id: '2509.24375'
source_url: https://arxiv.org/abs/2509.24375
tags:
- reasoning
- token
- tokens
- reinforcement
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Reinforcement Mid-Training (RMT), a novel
  intermediate stage between pre-training and post-training for large language models.
  The authors identify three key challenges: inefficient reasoning due to overthinking,
  disregard of imbalanced token entropy distribution, and underutilization of token
  information.'
---

# Reinforcement Mid-Training

## Quick Facts
- arXiv ID: 2509.24375
- Source URL: https://arxiv.org/abs/2509.24375
- Authors: Yijun Tian; Shaoyu Chen; Zhichao Xu; Yawei Wang; Jinhe Bi; Peng Han; Wei Wang
- Reference count: 7
- One-line primary result: Up to 64.91% performance improvement with 21% of reasoning length on mathematical tasks

## Executive Summary
Reinforcement Mid-Training (RMT) introduces an intermediate stage between pre-training and post-training for large language models, addressing three key challenges: inefficient reasoning from overthinking, imbalanced token entropy distribution, and underutilization of token information. The framework combines a dynamic token budget mechanism, curriculum-based adaptive sampling, and dual training with token-selective reinforcement learning plus token-inclusive next-token prediction. Experiments demonstrate RMT achieves up to 64.91% performance improvement on mathematical tasks while generating only 21% of the reasoning length compared to state-of-the-art methods. Additionally, RMT checkpoints improve subsequent post-training by up to 18.76% in mathematical domains, establishing a stronger foundation for downstream tasks.

## Method Summary
RMT operates between pre-training and post-training, using pre-training-scale unlabeled data with targeted reinforcement learning objectives. The method categorizes tokens by entropy into difficulty levels (easy/medium/hard) and employs three components: (1) dynamic token budget with exponential decay to constrain reasoning length, (2) curriculum-based adaptive sampling that progresses from easy to hard tokens during training, and (3) dual training combining token-selective RL on high-entropy tokens with token-inclusive NTP on all tokens. The unified objective combines GRPO-based RL advantages with masked NTP likelihood maximization, optimizing both reasoning efficiency and comprehensive token information utilization.

## Key Results
- Up to 64.91% performance improvement on mathematical reasoning tasks compared to state-of-the-art methods
- 21% of reasoning length generated compared to baseline (186 tokens vs 872 tokens for RPT)
- 18.76% improvement in subsequent post-training on mathematical domains when starting from RMT checkpoints
- Ablation studies show each component is essential: removing dynamic budget drops accuracy from 59.12% to 46.87%, removing curriculum drops from 46.87% to 43.22%, and removing NTP drops from 59.12% to 42.40%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A dynamically decaying token budget with triangular reward shaping can reduce reasoning verbosity without sacrificing accuracy.
- **Mechanism:** The budget $B_t = \max(B_{min}, \lfloor B_0 \cdot \gamma^{t/T} \rfloor)$ decays exponentially across training steps. A triangular length reward $r_{len}(\ell; B_t)$ peaks at exact budget adherence, linearly penalizes under/over-generation up to $2B_t$, and provides zero reward beyond—creating pressure toward concise, budget-aligned reasoning chains.
- **Core assumption:** Overthinking is partially driven by unconstrained generation length rather than intrinsic task complexity; reducing available budget forces more efficient reasoning paths to emerge.
- **Evidence anchors:** [abstract] "dynamic token budget mechanism that constrains unnecessary reasoning steps and mitigates model overthinking"; [section 4.1] Equations 1-2 define the decay schedule and triangular reward; Table 3 shows RMT-Q3 generates 186 tokens vs. 872 for RPT (21% of length); [corpus] Related work OctoThinker explores mid-training effects on RL scaling but does not implement dynamic budgets; corpus evidence for this specific mechanism is weak.
- **Break condition:** If task complexity genuinely requires longer reasoning chains, aggressive budget decay may cap performance; the triangular reward assumes $B_t$ is approximately correct for the task distribution.

### Mechanism 2
- **Claim:** Curriculum-based sampling from easy→medium→hard tokens (partitioned by entropy) stabilizes early training and improves final performance.
- **Mechanism:** Tokens are categorized by entropy into difficulty levels. Sampling probabilities $p_t$ transition through two checkpoints $(t_1, t_2)$ via piecewise linear interpolation: early training emphasizes easy/medium tokens, then gradually shifts toward hard tokens as model capacity increases.
- **Core assumption:** Token entropy correlates with learning difficulty and model readiness; early overexposure to high-entropy tokens destabilizes training before sufficient capacity develops.
- **Evidence anchors:** [abstract] "curriculum-based adaptive sampling method that fosters a progressive learning trajectory from easy to hard tokens"; [section 4.2] Equation 4 defines the curriculum schedule; ablation (Table 4) shows removing CAS drops average accuracy from 46.87% to 43.22% (RMT-R1); [corpus] Corpus papers discuss mid-training stages but do not provide comparative evidence on entropy-based curricula; direct replication evidence is absent.
- **Break condition:** If entropy poorly proxies "difficulty" for a domain (e.g., creative writing), the curriculum may misrank tokens; the transition points $t_1, t_2$ are hyperparameters requiring tuning.

### Mechanism 3
- **Claim:** Combining token-selective RL on high-entropy tokens with token-inclusive next-token prediction on all tokens preserves information from low-entropy tokens while targeting reasoning learning where it matters most.
- **Mechanism:** Partition tokens into $\Phi_{RL}$ (high-entropy, selected for RL with reasoning chains) and $\Phi_{NTP} = S \setminus \Phi_{RL}$ (majority low-entropy, standard NTP). The unified loss $\mathcal{L} = \mathcal{L}_{RL}(\theta) + \lambda \cdot \mathcal{L}_{NTP}(\theta)$ combines GRPO-based RL advantages with masked NTP likelihood maximization.
- **Core assumption:** Most tokens contribute to language understanding but not reasoning; excluding them loses information. High-entropy tokens are where reasoning effort is most valuable.
- **Evidence anchors:** [abstract] "dual training strategy that combines reinforcement learning with next-token prediction, ensuring targeted learning on key tokens and full exploitation of all token information"; [section 4.3] Equations 5-9 define the composite reward and unified objective; Figure 1(b) shows majority of tokens are low-entropy; ablation shows removing NTP drops RMT-Q3 from 59.12% to 42.40% average; [corpus] OctoThinker and related mid-training work focus on RL scaling but do not systematically address token-information loss from selective training; this mechanism's novelty is not yet independently validated.
- **Break condition:** If low-entropy tokens contain subtle reasoning-relevant patterns, masking them from RL may underutilize signal; the $\lambda$ weight balances potentially conflicting gradients.

## Foundational Learning

- **Concept: GRPO (Group Relative Policy Optimization)**
  - **Why needed here:** RMT uses GRPO to compute group-relative advantages from multiple rollouts, normalizing rewards within groups rather than against a learned value function. Understanding this clarifies why Equation 6 computes $A_i = (r_i - \bar{r})/(\sigma + \delta)$ rather than using a critic.
  - **Quick check question:** Given 8 rollout samples with rewards [0.3, 0.5, 0.7, 0.9, 0.4, 0.6, 0.8, 0.2], what advantage would the sample with reward 0.9 receive?

- **Concept: Token Entropy as Difficulty Proxy**
  - **Why needed here:** The curriculum mechanism partitions tokens by entropy, assuming higher entropy = harder to predict = more reasoning required. This is the organizing principle for adaptive sampling.
  - **Quick check question:** If a model assigns probabilities [0.8, 0.1, 0.1] to three candidate tokens at a position, what is the entropy? How does it compare to a position with distribution [0.33, 0.33, 0.34]?

- **Concept: Mid-Training vs. Pre-Training vs. Post-Training**
  - **Why needed here:** RMT positions itself between pre-training (broad unsupervised learning) and post-training (task-specific alignment), using pre-training-scale unlabeled data but with targeted RL objectives. This distinguishes it from both standard continued pre-training and RLHF.
  - **Quick check question:** Why does the paper claim RPT (Reinforcement Pre-Training) is technically mid-training rather than pre-training? What capability does the base model already possess?

## Architecture Onboarding

- **Component map:** Entropy Computation Module -> Curriculum Sampler -> Dynamic Budget Injector -> RL Training Loop -> NTP Training Loop -> Loss Combiner

- **Critical path:**
  1. Start with a base model that already has instruction-following capability (R1-Distill-14B or Qwen3-14B in paper)
  2. Pre-compute token entropy on training corpus using frozen base model
  3. Partition tokens into difficulty buckets by entropy thresholds
  4. Initialize budget $B_0$, set decay schedule $\gamma$, set curriculum transition points $t_1, t_2$
  5. For each training step: sample tokens → inject budget → generate rollouts → compute rewards → update policy → compute NTP loss → combine and step optimizer

- **Design tradeoffs:**
  - **Budget decay rate $\gamma$:** Faster decay (lower $\gamma$) reduces computation but may under-train complex reasoning; paper uses $\gamma = 0.2$ with $B_0 = 800$, $B_{min} = 1$
  - **Curriculum transition points:** Earlier $t_1$ accelerates hard-token exposure but risks instability; paper uses 30%/70% of total steps
  - **Loss weight $\lambda$:** Higher $\lambda$ prioritizes language modeling over reasoning; paper uses $\lambda = 0.1$
  - **Rollout count $G$:** More rollouts improve advantage estimation but increase compute; paper uses $G = 8$

- **Failure signatures:**
  - **Budget too aggressive:** Model generates below $B_{min}$, reasoning becomes trivial, accuracy drops sharply on hard tokens
  - **Curriculum mis-specified:** Early training shows high loss spikes or NaN gradients; validation accuracy oscillates rather than smoothly increasing
  - **NTP weight too low:** Model loses fluency, generates ungrammatical reasoning chains; Table 4 shows this drops RMT-Q3 from 59.12% to 42.40%
  - **Entropy thresholds wrong:** Easy/medium/hard buckets are severely imbalanced; curriculum transitions have no effect

- **First 3 experiments:**
  1. **Baseline sanity check:** Run RMT on a small corpus subset (e.g., 500 examples) with full logging—verify budget is being injected correctly (check prompts), verify entropy distribution matches Figure 1(b) pattern, verify rewards are computed correctly (spot-check manual calculation)
  2. **Ablation on budget decay:** Compare $\gamma \in \{0.1, 0.2, 0.5\}$ with fixed other hyperparameters; plot response length and accuracy over training steps—expect faster decay to show earlier length reduction but potential accuracy cap
  3. **Token partition analysis:** After entropy computation, histogram tokens by difficulty level; verify easy/medium/hard proportions are reasonable (not 90% easy or 90% hard); if severely imbalanced, adjust entropy thresholds before full training run

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does RMT generalize effectively to non-mathematical domains such as natural language reasoning, code generation, or multilingual tasks?
- **Basis in paper:** [inferred] All experiments are conducted exclusively on mathematical datasets (OmniMATH and Skywork), with no validation on other reasoning domains.
- **Why unresolved:** The paper does not demonstrate whether the entropy-based curriculum and token budget mechanisms transfer to domains with different token distributions and reasoning patterns.
- **What evidence would resolve it:** Evaluation on diverse benchmarks spanning commonsense reasoning, code completion, and multilingual tasks, comparing RMT performance against current domain-specific methods.

### Open Question 2
- **Question:** How sensitive is RMT to hyperparameter choices such as the initial budget B₀, decay factor γ, curriculum transition points t₁ and t₂, and the trade-off weight λ?
- **Basis in paper:** [inferred] The paper reports specific values (B₀=800, γ=0.2, t₁=30%, t₂=70%, λ=0.1) without systematic ablation of these hyperparameters or analysis of their sensitivity.
- **Why unresolved:** Without sensitivity analysis, it remains unclear whether these values are robust across different datasets or require extensive tuning, which affects the framework's practical usability.
- **What evidence would resolve it:** Comprehensive hyperparameter sensitivity analysis showing performance variance across different settings, plus recommendations for tuning strategies across diverse scenarios.

### Open Question 3
- **Question:** Does RMT scale effectively to significantly larger models (e.g., 70B+ parameters) and smaller models (e.g., 1-7B parameters)?
- **Basis in paper:** [inferred] Experiments are limited to 14B parameter models (R1-Distill-14B and Qwen3-14B), leaving scalability unexplored.
- **Why unresolved:** The computational dynamics of reinforcement mid-training may differ substantially at different scales, and the entropy distribution patterns could shift with model capacity.
- **What evidence would resolve it:** Experiments applying RMT across a range of model sizes (1B, 7B, 14B, 70B) with analysis of performance scaling curves and computational efficiency trade-offs.

## Limitations

- **Entropy-Based Token Categorization:** The paper relies on token entropy as a proxy for learning difficulty, but the effectiveness of this assumption across diverse domains remains untested. The entropy thresholds for partitioning tokens into easy/medium/hard categories are not specified, making it unclear how robust this categorization is to different datasets or model scales.

- **Hyperparameter Sensitivity:** Several critical hyperparameters lack clear justification: the budget decay rate (γ=0.2), curriculum transition points (30%/70%), NTP weight (λ=0.1), and rollout count (G=8). The paper does not provide ablation studies showing sensitivity to these parameters or guidance on tuning them for different model sizes or domains.

- **Generalization Beyond Math:** While results show 64.91% improvement on mathematical reasoning tasks, the methodology's effectiveness on non-mathematical domains (creative writing, code generation, general knowledge) is not demonstrated. The entropy-based curriculum and reasoning-focused objectives may not transfer well to domains where token entropy doesn't correlate with task complexity.

## Confidence

- **High Confidence:** The core architectural components (dynamic budget mechanism, curriculum sampling, dual training) are well-defined with clear mathematical formulations. The empirical improvements on the tested mathematical benchmarks are substantial and the ablation studies support the importance of each component.

- **Medium Confidence:** The mechanism by which entropy-based token categorization improves learning trajectory is theoretically sound but lacks independent validation. The assumption that easy→medium→hard token progression stabilizes training is reasonable but not proven across different model scales or domains.

- **Low Confidence:** The generalizability of RMT to non-mathematical domains and the optimal hyperparameter settings for different model scales remain significant open questions. The paper's focus on a single dataset and model size limits broader applicability claims.

## Next Checks

1. **Cross-Domain Transfer Test:** Apply RMT to a non-mathematical domain (e.g., creative writing or code generation) using the same methodology. Compare performance against standard continued pre-training and evaluate whether entropy-based difficulty partitioning remains meaningful when token entropy doesn't correlate with reasoning complexity.

2. **Hyperparameter Sensitivity Analysis:** Systematically vary the key hyperparameters (budget decay rate γ, curriculum transition points, NTP weight λ, rollout count G) across a wider range than tested. Identify which parameters most strongly influence performance and establish guidelines for tuning based on model scale or domain characteristics.

3. **Total Cost-Benefit Analysis:** Measure the total computational cost of RMT training (including multiple rollouts and dual objectives) versus the claimed efficiency gains from reduced reasoning length. Calculate the break-even point where RMT becomes cost-effective compared to standard methods, considering both training and inference costs.