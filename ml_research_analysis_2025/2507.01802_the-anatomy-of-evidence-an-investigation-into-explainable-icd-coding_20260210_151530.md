---
ver: rpa2
title: 'The Anatomy of Evidence: An Investigation Into Explainable ICD Coding'
arxiv_id: '2507.01802'
source_url: https://arxiv.org/abs/2507.01802
tags:
- evidence
- code
- match
- coding
- codes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the MDACE dataset and evaluates explainable
  ICD coding systems. The authors conduct an in-depth analysis of ground truth evidence
  alignment with code descriptions and investigate state-of-the-art approaches for
  evidence extraction.
---

# The Anatomy of Evidence: An Investigation Into Explainable ICD Coding

## Quick Facts
- arXiv ID: 2507.01802
- Source URL: https://arxiv.org/abs/2507.01802
- Reference count: 25
- Most ICD codes show little overlap with code descriptions, though some codes like "snoring" and "pericardial effusion" have strong alignment

## Executive Summary
This paper conducts a comprehensive analysis of explainable ICD coding systems using the MDACE dataset, focusing on the quality of extracted evidence spans that justify medical code predictions. The authors evaluate both supervised and unsupervised approaches, finding that supervised models significantly outperform unsupervised ones in extracting exact matches with ground truth evidence. A key counterintuitive finding reveals that models are more likely to make errors when extracting fewer evidence tokens, challenging assumptions about evidence sparsity. The study also highlights systematic challenges including uppercase medical terms and duplicate evidence extraction, while proposing new proximity-based evaluation metrics.

## Method Summary
The study employs a modified PLM-ICD architecture built on RoBERTa pre-trained on PubMed and MIMIC-III discharge summaries, using cross-attention mechanisms for multi-label ICD-9 code prediction. Two evidence extraction strategies are compared: supervised training with KL divergence regularization on cross-attention weights against ground truth spans, and unsupervised Input Gradient Regularization (IGR). The AttInGrad explanation method combines attention weights with Input×Grad attributions, using a threshold tuned on validation data to binarize continuous scores into evidence spans. Evaluation employs custom match measures (empty, exact, proximate, partial, no match) alongside standard F1 metrics.

## Key Results
- Supervised models achieve 89 exact matches vs 9 for unsupervised approaches
- Models are more likely to be wrong when extracting fewer evidence tokens
- Approximately 80% of cases identify at least one correct token using supervised methods
- Most ICD codes show little semantic overlap with their descriptions (except specific cases like "snoring")

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Based Feature Attribution for Evidence Extraction
Combining attention weights with gradient-based attribution identifies tokens that influence ICD code predictions. AttInGrad computes L2 norm of Input×Grad attributions and multiplies by attention weights, using a validation-tuned threshold for binarization. Core assumption: gradients w.r.t. input embeddings reflect token importance while attention captures token-code associations. Break condition: documents exceeding token limits or unstable gradient norms degrade attribution quality.

### Mechanism 2: Supervised Evidence Alignment via Cross-Attention Regularization
Training with evidence span annotations improves exact match extraction compared to unsupervised approaches. The supervised objective minimizes KL divergence between cross-attention weights and binary evidence masks, explicitly teaching models which tokens justify each code. Core assumption: human-annotated evidence spans provide reliable supervision for token-code relevance. Break condition: inconsistent ground truth evidence (only 331 of 1602 codes align between annotation modes) degrades supervision quality.

### Mechanism 3: Evidence-Length Signal for Prediction Confidence
The number of tokens extracted as evidence correlates with classification correctness. Assumption: models extracting more evidence tokens have higher-confidence, correct predictions, while sparse attribution scores indicate likely errors. Core assumption: evidence quantity reflects model certainty with threshold calibration transferring from validation to test. Break condition: duplicates inflating evidence counts without adding information create noisy length signals.

## Foundational Learning

- **Multi-label classification with large label spaces**: ICD coding assigns multiple codes per document from thousands of possible codes. Standard softmax is inapplicable; models use sigmoid per code with threshold-based selection. Quick check: Given 31.4 average labels per Profee document, would accuracy be meaningful? Why or why not?

- **Feature attribution faithfulness vs. plausibility**: Faithfulness measures whether attribution accurately reflects model behavior; plausibility measures whether attribution matches human expectations. The paper evaluates plausibility via ground truth overlap but notes this doesn't guarantee faithful explanations. Quick check: If a model extracts "ACE inhibitor" as evidence for "hypertension," is this plausibility failure, faithfulness failure, or neither?

- **Threshold calibration for evidence binarization**: Attribution methods produce continuous scores requiring threshold conversion to binary evidence. The paper tunes this on validation data but notes unsupervised models still use annotated evidence for threshold selection. Quick check: What happens to evidence extraction if threshold is too high? Too low?

## Architecture Onboarding

- **Component map**: Clinical document -> RoBERTa backbone -> Cross-attention head -> Per-code sigmoid outputs -> AttInGrad attributions -> Threshold binarization -> Evidence spans

- **Critical path**: 1) Tokenize clinical document (watch for uppercase text, abbreviations) 2) Forward pass through RoBERTa + cross-attention 3) Compute per-code probabilities (threshold 0.5 for prediction) 4) For predicted codes, compute AttInGrad attributions 5) Apply threshold (validation-tuned) to extract evidence spans 6) Post-process: deduplicate, expand partial word tokens to full words

- **Design tradeoffs**: Supervised evidence training yields higher exact matches (89 vs 9) but requires expensive annotation; unsupervised with IGR shows similar code prediction performance (379 vs 374 TPs) but lower evidence quality; proximity-based matching vs strict F1 offers more forgiving evaluation but may hide precision issues

- **Failure signatures**: Uppercase text tokenized into subword fragments causes extraction failures; duplicate extraction of same evidence at multiple positions; spurious drug-diagnosis correlations without contextual justification; empty evidence when scores fall below threshold; semantic non-matches where valid paraphrases fail lexical matching

- **First 3 experiments**: 1) Baseline replication: Load pre-trained PLM-ICD weights, run AttInGrad on MDACE test set, compute match type distribution to verify reproduced numbers match Figure 6 2) Threshold sensitivity analysis: Vary attribution threshold (±20% from validation-tuned value) and measure impact on exact match count and empty prediction rate 3) Preprocessing ablation: Add uppercase normalization preprocessing step; compare evidence extraction on affected documents

## Open Questions the Paper Calls Out

- How do robustness strategies specifically influence feature sparsity and the trade-off between evidence length and classification accuracy? The finding that shorter evidence correlates with errors is counterintuitive, and the impact of specific training strategies on this phenomenon remains unclear.

- To what extent does model calibration improve the reliability of output probabilities in predicting the quality of evidence matches? Current models show low or inconsistent probabilities for high-quality exact matches, indicating confidence scores are not reliable indicators of evidence plausibility.

- How can semantic similarity be formally integrated into evaluation metrics for evidence that fails lexical matching but remains clinically valid? Standard metrics like F1 rely on token overlap, failing to capture synonyms or paraphrasing without resource-intensive manual expert annotation.

## Limitations

- Evidence quality and annotation bias: Only 331 of 1602 codes show alignment between Inpatient and Profee annotation modes, raising questions about whether evaluation metrics capture explanation quality or annotation noise.

- Threshold dependency and generalizability: AttInGrad threshold tuned on validation data creates potential overfitting, and unsupervised approaches still require annotated evidence for threshold selection.

- Uppercasing and tokenization artifacts: Fully capitalized medical terms severely degrade evidence extraction performance, treated as preprocessing issue rather than fundamental attribution method limitation.

## Confidence

**High Confidence**: Supervised models outperform unsupervised approaches in exact match extraction (89 vs 9 matches); models are more likely to be wrong when fewer evidence tokens are extracted; most ICD codes show little overlap with their descriptions.

**Medium Confidence**: Proximate match measures improve evaluation realism compared to strict F1; duplicate evidence extraction is a significant challenge; uppercasing presents a systematic challenge for evidence extraction.

**Low Confidence**: AttInGrad attribution method is superior to other explanation methods; KL divergence regularization is the optimal supervised training objective; specific value of k=10 for proximate matching is optimal.

## Next Checks

- **Annotation Consistency Validation**: Replicate evidence alignment analysis between Inpatient and Profee modes using larger sample; calculate Cohen's kappa for evidence span agreement and test whether evaluation metrics change significantly with different annotation sources.

- **Threshold Sensitivity Analysis**: Systematically vary AttInGrad attribution threshold from 0.5× to 1.5× validation-tuned value in 0.1 increments; measure impact on exact match count, empty prediction rate, and precision/recall trade-off; plot precision-recall curves.

- **Uppercasing Ablation Study**: Implement preprocessing pipeline normalizing medical terms to lowercase before tokenization; compare evidence extraction performance on documents containing uppercase medical terms; measure delta in exact match count and analyze impact on true positive extraction rates.