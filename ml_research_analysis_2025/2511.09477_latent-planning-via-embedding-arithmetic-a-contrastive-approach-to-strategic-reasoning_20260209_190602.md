---
ver: rpa2
title: 'Latent Planning via Embedding Arithmetic: A Contrastive Approach to Strategic
  Reasoning'
arxiv_id: '2511.09477'
source_url: https://arxiv.org/abs/2511.09477
tags:
- solis
- planning
- stockfish
- space
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SOLIS, a latent planning method for chess that
  uses supervised contrastive learning to embed positions into an evaluation-aligned
  space. Rather than training policies or value heads, SOLIS ranks candidate moves
  by their alignment with a global advantage vector that orients the space from losing
  to winning regions.
---

# Latent Planning via Embedding Arithmetic: A Contrastive Approach to Strategic Reasoning

## Quick Facts
- arXiv ID: 2511.09477
- Source URL: https://arxiv.org/abs/2511.09477
- Reference count: 40
- One-line primary result: SOLIS achieves 2500+ Elo rating at depth 5 using shallow search (W=3) guided by contrastive embeddings, outperforming Stockfish in nodes-per-move efficiency.

## Executive Summary
This paper introduces SOLIS, a latent planning method for chess that bypasses explicit policies or value heads by learning an embedding space aligned with Stockfish evaluations via supervised contrastive learning. Positions are mapped into a normalized latent space where proximity reflects outcome similarity, and a global advantage vector orients the space from losing to winning regions. Planning is performed through shallow min-max search guided by embedding alignment, requiring far fewer nodes than traditional search. Experiments show SOLIS is competitive with configured Stockfish baselines across multiple depths, with both Mini and Base models exceeding 2500 Elo at depth 5.

## Method Summary
SOLIS learns to embed chess positions into a latent space where proximity reflects evaluation similarity, using supervised contrastive loss to pull together positions with similar win probabilities (within margin δ=0.05) and push apart those with larger gaps. The model is a transformer encoder (Mini: 0.8M params, Base: 41M params) that outputs ℓ2-normalized CLS embeddings. Planning uses a global advantage vector computed from extreme position means (μWhite and μBlack), ranking candidate moves by their alignment with this vector. Search is shallow (width W=3, depth S∈{3,4,5}) but effective due to high-quality embeddings. Anchored scoring (subtracting μBlack before projection) provides consistent Elo gains.

## Key Results
- SOLIS achieves 2500+ Elo rating at depth 5 with width W=3, searching far fewer nodes than Stockfish
- Anchored scoring provides 10-30 Elo improvement over unanchored scoring
- Width W=3 is sufficient; broader search yields minimal gains, indicating embedding quality drives performance
- Both Mini (0.8M params) and Base (41M params) models exceed 2500 Elo at depth 5 against configured Stockfish baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Supervised contrastive learning organizes chess positions into an embedding space where proximity reflects outcome similarity.
- **Mechanism**: The SupCon loss pulls together positions with win probability differences less than margin δ=0.05 while pushing apart positions with larger evaluation gaps. Repeated across 5M positions, this creates a latent geometry where distance encodes strategic equivalence rather than surface similarity.
- **Core assumption**: Positions with similar evaluations share latent strategic features that a transformer encoder can learn to represent.
- **Evidence anchors**:
  - [abstract]: "outcome similarity is captured by proximity"
  - [Section 3.3]: "This margin-based sampling pulls together states of nearly equal evaluation and pushes apart positions with larger gaps"
  - [corpus]: Related work on latent spaces for reasoning suggests embedding structure can support higher-level operations, but corpus evidence specific to contrastive evaluation spaces is weak.
- **Break condition**: If the evaluation oracle (Stockfish) is miscalibrated or noisy beyond δ=0.05, positive/negative pair assignments become unreliable, fragmenting the embedding space.

### Mechanism 2
- **Claim**: A single global advantage vector provides sufficient directional information to rank candidate moves without explicit value estimation.
- **Mechanism**: Mean embeddings at evaluation extremes define a direction from losing to winning: a⃗ = (μWhite − μBlack)/||μWhite − μBlack||. Moves whose embeddings project further along a⃗ are ranked higher. This reduces planning to cosine similarity rather than rollout or value-function queries.
- **Core assumption**: The embedding space is approximately linear along the winning direction, so projections translate to meaningful strategic progress.
- **Evidence anchors**:
  - [abstract]: "a single global advantage vector orients the space from losing to winning regions"
  - [Section 3.4.2]: "shifting relative to the Black mean provides a more reliable basis for evaluation" (anchored scoring gives 10–30 Elo gain)
  - [corpus]: No direct corpus support; neighbor papers on latent arithmetic focus on sentence/robot control domains, not strategic games.
- **Break condition**: If winning trajectories require non-monotonic paths (e.g., temporary sacrifices), linear projection may misrank moves that are strategically optimal but locally unfavorable.

### Mechanism 3
- **Claim**: Shallow search with high-quality representations can match deeper search with weaker evaluation.
- **Mechanism**: SOLIS searches only W=3 branches to depth S∈{3,4,5}, evaluating ~40–364 nodes versus hundreds to thousands for Stockfish. The embedding quality compensates for limited depth: min-max propagates embedding-based scores up the tree.
- **Core assumption**: The top-3 legal moves (by embedding alignment) contain the strategically strongest continuations with high probability.
- **Evidence anchors**:
  - [Section 4.1]: "Width brings little benefit past three candidates, which suggests that most strength comes from the quality of the learned space rather than broad expansion"
  - [Section 4.2]: "Despite searching far fewer nodes, SOLIS is competitive with this Stockfish configuration"
  - [corpus]: Weak. Related work on planning addresses LLM reasoning, not constrained game tree search.
- **Break condition**: In tactical positions where the best move is not among top-3 by embedding alignment (e.g., non-obvious forcing sequences), SOLIS will miss critical lines.

## Foundational Learning

- **Contrastive learning (InfoNCE / SupCon loss)**
  - Why needed here: SOLIS uses supervised contrastive loss to structure the embedding space by evaluation similarity. Understanding how positives/negatives are constructed is essential for debugging training.
  - Quick check question: Given two positions with win probabilities 0.52 and 0.54 and δ=0.05, are they positive or negative pairs?

- **Transformer encoder architectures (CLS token, positional encoding)**
  - Why needed here: The encoder maps tokenized FEN strings to normalized embeddings via a CLS token aggregation. Understanding attention patterns helps diagnose what strategic features are captured.
  - Quick check question: Why does the model use only the CLS token output rather than pooled token representations?

- **Chess evaluation and FEN notation**
  - Why needed here: Positions are tokenized from FEN strings; evaluations (win probabilities) define training labels. Misunderstanding evaluation semantics (e.g., whose perspective) will corrupt positive/negative masks.
  - Quick check question: If White has a 0.9 win probability, what is the normalized evaluation from Black's perspective before the paper's conversion?

## Architecture Onboarding

- **Component map**: FEN string -> 77-token sequence -> 6-layer transformer encoder -> CLS embedding -> linear projection -> ℓ2 normalization -> cosine similarity scoring

- **Critical path**:
  1. Correct FEN tokenization (run-length expansion must match Ruoss et al., 2024)
  2. Accurate positive/negative mask construction during training (evaluation differences < δ)
  3. Precise computation of μWhite and μBlack from held-out extremes for advantage vector

- **Design tradeoffs**:
  - Mini vs Base: 0.8M vs 41M parameters trades Elo (100–250 gap) for compute/memory efficiency
  - Anchored vs unanchored scoring: Anchored adds μBlack subtraction for 10–30 Elo gain but requires storing reference mean
  - Width W: W=3 is sufficient; W>3 yields marginal gains but increases latency linearly

- **Failure signatures**:
  - Threefold repetition blindness (Section 5.1): model ignores move history, draws winning positions
  - Indecisiveness in winning positions: cannot distinguish mate-in-1 from mate-in-20 (both have p=1.0)
  - Evaluation collapse: if δ is too large, all positions cluster; if too small, fragmentation

- **First 3 experiments**:
  1. **Sanity check embedding space**: Visualize UMAP projection of held-out positions colored by win probability (replicate Figure 1). If no gradient appears, check positive/negative mask construction.
  2. **Advantage vector validation**: Compute μWhite and μBlack, verify ||μWhite − μBlack|| > 0 and direction correlates with evaluation on a test set.
  3. **Ablation on δ and τ**: Train Mini models with δ∈{0.03, 0.05, 0.10} and τ∈{0.05, 0.07, 0.10}; measure Elo at depth 3, width 3 against Stockfish cap 2000 to identify stable hyperparameters.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SOLIS transfer effectively to other perfect-information games or domains with learnable state evaluations beyond chess?
- Basis in paper: [explicit] Conclusion states: "We expect the same idea to extend to other perfect-information games and to control tasks with large action spaces" and lists "testing transfer beyond chess" as future work.
- Why unresolved: All experiments are confined to chess using Stockfish annotations; no cross-domain validation was performed.
- What evidence would resolve it: Demonstrating competitive performance on domains like Go, Othello, or planning tasks without game-specific architecture changes.

### Open Question 2
- Question: Can incorporating policy priors or MCTS improve SOLIS's search efficiency or playing strength beyond the current fixed branching search?
- Basis in paper: [explicit] Conclusion lists "combining SOLIS with policy priors or MCTS" as future work.
- Why unresolved: The paper deliberately avoids learned policies or value heads; hybrid approaches remain unexplored.
- What evidence would resolve it: Experiments showing Elo gains or reduced node counts when combining contrastive embeddings with learned priors or MCTS-guided expansion.

### Open Question 3
- Question: How can SOLIS be extended to handle non-Markovian game aspects such as threefold repetition and the fifty-move rule?
- Basis in paper: [explicit] Section 5.1 identifies "blindness to threefold repetition" as a limitation causing draws in winning positions; the authors note FEN tokenization omits move history.
- Why unresolved: The embedding encodes only current position state, discarding trajectory information needed for repetition detection.
- What evidence would resolve it: Augmenting input with move history or rule-aware features that prevent repeated positions without degrading Elo.

### Open Question 4
- Question: How can the embedding space or scoring mechanism be modified to prefer shorter forcing sequences (e.g., mate-in-1 over mate-in-20) when both have identical win probabilities?
- Basis in paper: [explicit] Section 5.1 identifies "indecisiveness in winning positions" where SOLIS cannot distinguish forcing positions of different lengths, leading to unnecessary draws or losses.
- Why unresolved: Win probability labels collapse all winning positions to similar values regardless of distance to terminal state.
- What evidence would resolve it: Incorporating distance-to-mate or ply-count information into the training signal, demonstrating faster conversion of winning positions.

### Open Question 5
- Question: How well-calibrated is SOLIS's embedding-based scoring, and can uncertainty estimates be derived from the latent representation?
- Basis in paper: [explicit] Conclusion lists "studying calibration and uncertainty" as future work.
- Why unresolved: The paper does not analyze whether embedding distances or scores correlate reliably with actual win probabilities across position types.
- What evidence would resolve it: Reliability diagrams showing predicted vs. actual outcomes, or methods extracting uncertainty from embedding neighborhoods.

## Limitations

- **Representation fragility**: The model cannot detect threefold repetitions or distinguish between different winning paths (mate-in-1 vs mate-in-20), suggesting the embedding space captures coarse evaluation rather than fine-grained strategic state.
- **Evaluation oracle dependency**: The entire training pipeline relies on Stockfish's win probability estimates being accurate and consistent, with no robustness testing against Stockfish version changes.
- **Dataset composition unknown**: The 5M position corpus size and distribution across game phases is unspecified, potentially limiting generalization to positions outside the training distribution.

## Confidence

- **High confidence**: The Elo performance claims against configured Stockfish baselines are well-supported by the described experimental protocol (equal-depth match points, multiple depth settings). The architectural specifications (transformer sizes, training hyperparameters) are detailed enough for reproduction.
- **Medium confidence**: The mechanism by which contrastive learning creates an evaluation-aligned embedding space is plausible but under-supported by corpus evidence. While the mathematical framework is sound, there's limited empirical validation that the embedding actually captures "strategic equivalence" rather than just evaluation proximity.
- **Low confidence**: The claim that shallow search with high-quality representations can match deeper search relies heavily on the assumption that top-3 candidates contain the strongest moves. This is asserted but not systematically tested—no analysis of move coverage or frequency of critical move omission is provided.

## Next Checks

1. **Positive pair construction validation**: Given a batch of positions with evaluations [0.48, 0.52, 0.63, 0.91], manually construct the positive/negative masks for δ=0.05. Verify that (0.48, 0.52) are positive, (0.48, 0.63) are negative, and (0.63, 0.91) are negative. Check edge cases where |p_i - p_j| = δ exactly.

2. **Advantage vector stability test**: Train two Mini models on different random seeds. For each, compute μWhite and μBlack from held-out extremes (p>0.98 and p<0.02). Calculate the angle between the resulting advantage vectors. If the angle exceeds 15°, the embedding space may be unstable across training runs.

3. **Critical move coverage analysis**: At depth 5, width 3, record the frequency with which Stockfish's top move appears in SOLIS's candidate set. Stratify by position type (opening, middlegame, endgame) and evaluation (losing, equal, winning). If critical moves are missing in >15% of positions, the shallow search assumption breaks down.