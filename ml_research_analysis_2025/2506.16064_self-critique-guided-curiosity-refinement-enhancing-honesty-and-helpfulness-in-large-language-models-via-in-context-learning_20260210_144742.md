---
ver: rpa2
title: 'Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness
  in Large Language Models via In-Context Learning'
arxiv_id: '2506.16064'
source_url: https://arxiv.org/abs/2506.16064
tags:
- prompting
- llms
- honesty
- language
- curiosity-driven
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the problem of enhancing the honesty and helpfulness
  of large language model (LLM) outputs. The core method is a self-critique-guided
  curiosity refinement prompting strategy that extends existing curiosity-driven prompting
  by adding two lightweight in-context learning steps: self-critique and refinement.'
---

# Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning

## Quick Facts
- arXiv ID: 2506.16064
- Source URL: https://arxiv.org/abs/2506.16064
- Authors: Duc Hieu Ho; Chenglin Fan
- Reference count: 38
- One-line primary result: Training-free prompting method improves honesty and helpfulness scores across 10 LLMs by 1.4%-4.3% using self-critique and refinement

## Executive Summary
This paper introduces a self-critique-guided curiosity refinement prompting strategy to enhance the honesty and helpfulness of large language model outputs. The approach extends curiosity-driven prompting with two lightweight in-context learning steps: self-critique and refinement. Experiments on the HONESET dataset using GPT-4o as a judge demonstrate consistent improvements across ten diverse LLMs, including both proprietary and open-weight models, with relative gains in H² scores ranging from 1.4% to 4.3%.

## Method Summary
The method implements a five-step prompting pipeline that is training-free and relies solely on in-context learning. It begins with generating a raw answer, followed by curiosity-driven prompting to identify confusion points. The optimized response then undergoes self-critique against three dimensions (explanation/honesty, guidance/helpfulness, solution appropriateness) before refinement. The refinement step uses critique feedback to make targeted edits while preserving response strengths. All steps use temperature=0, top_p=1, and max_tokens=2500 to ensure reproducibility.

## Key Results
- Self-critique-guided curiosity refinement achieves relative gains in H² scores ranging from 1.4% to 4.3% across ten diverse LLMs
- The method consistently outperforms baseline approaches including raw answers and curiosity-driven prompting alone
- Purely honest rate increases demonstrate improved alignment with honesty metrics

## Why This Works (Mechanism)

### Mechanism 1
Explicitly surfacing model uncertainty before answering improves the honesty of the final response. The curiosity-driven prompting step forces the model to generate a "confusion output"—identifying what it doesn't know or what information is missing—before it generates an optimized answer. This conditions the final response on a self-identified knowledge boundary, making it more likely to acknowledge limitations. The core assumption is that the model can reliably distinguish between what it knows and what it doesn't know when explicitly prompted to do so. Break condition: If a model is poorly calibrated and cannot identify its own knowledge gaps, the confusion output will be empty or misleading, negating the benefit.

### Mechanism 2
Structured self-critique acts as a learned quality control filter for alignment attributes. The model is prompted to evaluate its optimized response against three explicit dimensions (explanation/honesty, guidance/helpfulness, solution appropriateness) and output a score and justification. This externalizes an internal "review" process, making latent quality issues explicit in the context window. The core assumption is that the model's evaluation of its own past output is more objective or rigorous than its initial generation process, and it can follow multi-dimensional scoring rubrics accurately. Break condition: If the critique prompt is too vague or the model exhibits sycophancy (rating its own work highly regardless of quality), the critique will fail to identify flaws.

### Mechanism 3
Targeted refinement informed by specific critique yields greater gains than re-generation. The refinement step uses the critique's output to perform "minimal targeted edits" rather than generating a new answer from scratch. This preserves the strengths of the previous optimized response while surgically correcting the identified weaknesses (e.g., adding a disclaimer, making guidance more actionable). The core assumption is that the model can follow instructions to edit minimally and will not degrade the response by over-correcting or introducing new hallucinations during editing. Break condition: If the "minimal edit" instruction is not followed, the model may rewrite the response entirely, potentially losing the original coherence or introducing new errors.

## Foundational Learning

**In-Context Learning (ICL):** The entire method is "training-free" and relies on the model's ability to follow complex, multi-step procedures and adopt specific personas (critic, editor) solely through the prompt. Quick check: Can you explain how ICL differs from fine-tuning, specifically regarding parameter updates?

**Self-Correction / Reflexion:** The core loop is generate → critique → refine. Understanding that models can improve their outputs by "thinking aloud" or evaluating their own work is the foundational insight. Quick check: What are the primary failure modes of self-correction (e.g., shallow feedback, model sycophancy)?

**LLM-as-a-Judge:** The evaluation of success (H² score) and the internal critique step both depend on a model (GPT-4o or the self) acting as a reliable evaluator of honesty and helpfulness. Quick check: What are the known biases when using an LLM to evaluate another LLM's output (e.g., preference for longer, more verbose answers)?

## Architecture Onboarding

**Component map:** Input Query → Curiosity Pipeline (raw answer, confusion output, optimized response) → Self-Critique Module → Refinement Module → Evaluator (GPT-4o judge)

**Critical path:** The pipeline is sequential. A failure in the Curiosity step (e.g., empty confusion output) propagates. The Critique step is the most critical for quality improvement; if it fails to identify real flaws, the Refinement step will do nothing useful.

**Design tradeoffs:** Latency vs. Quality: This method requires 3 to 5 inference calls per query (depending on how "Curiosity" is implemented). It is unsuitable for real-time, low-latency applications without optimization (e.g., parallelizing critique dimensions). Judge Dependence: Results are contingent on the GPT-4o judge's alignment with human values. A different judge might yield different H² scores.

**Failure signatures:** Empty or Generic Critique: The model outputs "The response is good" with a score of 10/10 for all dimensions (sycophancy). This halts improvement. Regression in Refinement: The refined answer is shorter or removes key information, causing the H² score to drop. Latency Blow-up: The 2500-token generation limit combined with multiple sequential steps causes timeouts.

**First 3 experiments:** 1) Baseline Reproduction: Implement the "Curiosity-Driven Prompting" (Approach 1) and verify your evaluation setup can reproduce the H² scores for GPT-4o and one open-weight model (e.g., Llama 3 8B) on a subset of HONESET. 2) Ablation on Critique: Implement the full self-critique pipeline (Approach 2). Run an experiment where you replace the detailed, multi-dimensional critique prompt with a simple "Is this answer good? (Yes/No)" to measure the value of structured feedback. 3) Judge Sensitivity: Compare the H² scores generated by GPT-4o as a judge against a human-evaluated subset of 50 queries to calibrate the automatic evaluation and check for systematic bias.

## Open Questions the Paper Calls Out

**Open Question 1:** Can the self-critique-guided curiosity refinement framework be effectively extended to other alignment dimensions, specifically harmlessness and fairness? The authors state in the Conclusion and Section 5.4 that future research could explore extending the prompting framework to additional dimensions such as harmlessness and fairness. The current study restricted its evaluation scope to the qualities of honesty and helpfulness using the H² framework.

**Open Question 2:** How does the proposed prompting strategy perform in multimodal settings involving non-textual inputs? The Conclusion suggests that future work may adapt the framework to multimodal settings where trust and interpretability are critical. The methodology and experiments were conducted exclusively on text-based models using the HONESET dataset.

**Open Question 3:** Can the increased latency and computational costs resulting from multi-step self-critique be reduced for time-sensitive applications? The authors acknowledge in Section 5.4 that the multiple inference steps result in increased latency, potentially affecting use in time-sensitive applications. While the paper validates the quality improvement, it does not propose or test methods to mitigate the efficiency trade-offs of the multi-step pipeline.

## Limitations
- Heavy dependence on GPT-4o as both the refinement judge and the internal critic, raising questions about judge-specific improvements
- Lack of human evaluation results leaves true alignment with human notions of honesty and helpfulness uncertain
- Assumes models can reliably self-assess, but provides limited analysis of cases where self-critique fails (e.g., sycophancy or overly generic feedback)

## Confidence

**High Confidence:** The method's feasibility and general approach (generate → critique → refine) are well-established through existing work on LLM-as-a-judge and self-correction. The experimental results showing consistent relative gains across multiple models are robust.

**Medium Confidence:** The specific mechanisms (curiosity confusion, multi-dimensional critique, minimal refinement) are logically sound but lack detailed analysis of failure modes. The claim that these steps specifically improve honesty vs. helpfulness is plausible but not definitively proven without human evaluation.

**Low Confidence:** The generalizability of results to different evaluation judges or real-world deployment scenarios is uncertain due to the GPT-4o dependency and lack of latency analysis.

## Next Checks

1. **Human Evaluation Validation:** Conduct a human evaluation study on a subset of 100 queries comparing raw, curiosity-driven, and self-critique-guided outputs to validate that GPT-4o judge scores correlate with human ratings of honesty and helpfulness.

2. **Judge Sensitivity Analysis:** Re-run the full experiment using a different LLM judge (e.g., Claude 3.5 or GPT-4 Turbo) to quantify how sensitive the H² improvements are to the choice of evaluation model.

3. **Failure Mode Analysis:** Systematically analyze 50 cases where the method failed to improve (or degraded) outputs, categorizing failure modes (e.g., empty critique, sycophantic scoring, over-refinement) to identify when and why the approach breaks down.