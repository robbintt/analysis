---
ver: rpa2
title: Evaluating Uncertainty in Deep Gaussian Processes
arxiv_id: '2504.17719'
source_url: https://arxiv.org/abs/2504.17719
tags:
- deep
- calibration
- uncertainty
- learning
- gaussian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work evaluated uncertainty quantification methods\u2014Deep\
  \ Gaussian Processes (DGPs), Deep Sigma Point Processes (DSPPs), and Deep Ensembles\u2014\
  on regression (CASP) and classification (ESR) tasks. Models were assessed using\
  \ MAE, accuracy, Negative Log-Likelihood (NLL), and Expected Calibration Error (ECE),\
  \ both in-distribution and under synthetic feature-level distribution shifts."
---

# Evaluating Uncertainty in Deep Gaussian Processes

## Quick Facts
- arXiv ID: 2504.17719
- Source URL: https://arxiv.org/abs/2504.17719
- Reference count: 37
- Primary result: DSPPs show superior in-distribution calibration; Deep Ensembles demonstrate better robustness under distribution shift

## Executive Summary
This work evaluates uncertainty quantification methods—Deep Gaussian Processes (DGPs), Deep Sigma Point Processes (DSPPs), and Deep Ensembles—on regression (CASP) and classification (ESR) tasks. Models are assessed using MAE, accuracy, Negative Log-Likelihood (NLL), and Expected Calibration Error (ECE), both in-distribution and under synthetic feature-level distribution shifts. DSPPs demonstrate superior in-distribution calibration with the lowest ECE, leveraging deterministic sigma point approximations for uncertainty propagation. Deep Ensembles achieve the best overall accuracy and robustness under distribution shift, maintaining stable performance and calibration across increasing perturbation severities. DGPs show particular sensitivity to shifts, especially in regression tasks.

## Method Summary
The study compares three uncertainty quantification approaches on CASP regression (45,730 samples, 9 features) and ESR classification (11,500 samples, 178 features) datasets. Models are trained with Bayesian optimization (20 trials, 5 random initializations) optimizing NLL with 80:20 train/validation splits. Robustness is evaluated under five synthetic distribution shifts (Gaussian noise, feature masking, scaling, permutation, outlier injection) at severity levels {0.0, 0.1, 0.2, 0.4, 0.6, 0.8}. Performance metrics include MAE/accuracy, NLL, and ECE, with N=5 independent runs for shift experiments.

## Key Results
- DSPPs achieve lowest in-distribution ECE (0.0171 for ESR classification) due to deterministic sigma point approximations
- Deep Ensembles maintain most stable performance under distribution shift, with consistent accuracy and calibration
- DGPs show highest sensitivity to distribution shifts, particularly in regression tasks with increasing severity
- Increasing inducing points improves NLL for both DGP and DSPP up to a saturation point

## Why This Works (Mechanism)

### Mechanism 1: Sigma Point Approximation for Deterministic Uncertainty Propagation
DSPPs achieve better in-distribution calibration than DGPs by replacing Monte Carlo sampling with deterministic quadrature-based approximation. Sigma points {ξ_w^(j)} and weights {ω^(j)} evaluate the model at Q deterministic locations, creating a Gaussian mixture p_DSPP(y|x) = Σ ω^(j) p^(j)(y|x). This enables direct maximum likelihood training without sampling noise degradation.

### Mechanism 2: Ensemble Diversity for Robust Distribution Shift Handling
Deep Ensembles maintain stable calibration under distribution shift because independent random initializations explore different regions of the loss landscape, providing implicit functional diversity. The variance term σ²_*(x) = (1/K)Σ[σ²_j(x) + μ²_j(x)] - μ²_*(x) captures both aleatoric and epistemic uncertainty, smoothing degradation when different members fail differently.

### Mechanism 3: Hierarchical GP Composition with Inducing Point Sparsification
Sparse variational inference enables scalable deep GP training. SVGPs approximate the full GP posterior using M ≪ N inducing variables u at locations Z. The ELBO trades likelihood fit against KL regularization. For DGPs, this extends hierarchically with doubly stochastic gradients, reducing complexity to O(NM²) time.

## Foundational Learning

- **Variational Inference and the ELBO**: All GP-based methods use VI to approximate intractable posteriors. Understanding the trade-off between likelihood term (data fit) and KL divergence (regularization toward prior) is essential for interpreting training dynamics and β-tuning. *Quick check*: If validation loss increases while training loss decreases, should you increase or decrease the KL weight β?

- **Calibration Metrics (NLL vs. ECE)**: The paper critiques NLL as over-emphasizing tail probabilities. ECE provides bin-based calibration assessment. Knowing when each metric applies prevents misinterpreting model quality. *Quick check*: A model achieves NLL=0.5 but ECE=0.25. What does this suggest about its uncertainty estimates?

- **Gaussian Process Kernels and Inducing Points**: Kernel choice (RBF used here) defines the function class. Inducing points are variational parameters that summarize data. Poor inducing point initialization or insufficient M leads to underfitting. *Quick check*: For a dataset with 50K points, what range of inducing points would you start with, and how would you validate the choice?

## Architecture Onboarding

- **Component map**: Input → [GP Layer 1 (with inducing points Z₁, variational params m₁, S₁)] → ... → [GP Layer L] → Likelihood (Gaussian/Softmax) → ELBO objective with doubly stochastic gradients (DGP); Input → [Sigma point quadrature {ξ_w, ω} at each layer] → Gaussian mixture likelihood → regularized MLL objective (DSPP); Input → [MLP with dual-output (μ, σ²)] × K members → Gaussian mixture aggregation during inference (Deep Ensemble)

- **Critical path**: 1) Standardize features (zero mean, unit variance); 2) Initialize inducing points (sobol or k-means from training data); 3) Train with Adam, monitoring both ELBO/NLL and ECE on validation split; 4) For ensembles, train each member independently with different seeds

- **Design tradeoffs**: DSPP vs. DGP: DSPP has lower inference variance (deterministic) but fixed quadrature; DGP has flexible MC sampling but higher variance. M (inducing points): Higher M → better approximation but O(M²) memory/compute. Depth: More layers → richer representations but harder optimization and shift sensitivity. Ensemble size K: More members → better epistemic uncertainty but linear cost scaling.

- **Failure signatures**: DGP overconfidence: Calibration curve below diagonal - indicates sampling variance degrading uncertainty estimates. DSPP accuracy collapse under shift: Sharp accuracy degradation at high severity while ensemble remains stable. Underfitting: High NLL with flat validation curve - likely too few inducing points or excessive KL regularization (β too high).

- **First 3 experiments**: 1) Baseline calibration check: Train all three models on CASP/ESR with default hyperparameters; plot calibration curves and compute ECE to establish baseline before any tuning. 2) Inducing point ablation: For DGP and DSPP, vary M ∈ {32, 64, 128, 256} on a held-out validation set; plot NLL vs. M to identify the knee point where approximation quality saturates. 3) Single perturbation shift test: Apply only Gaussian noise corruption at severity s ∈ {0.1, 0.3, 0.5} to test data; compare ECE degradation rates across models to isolate which architecture is most sensitive to noise-type shift.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does integrating DGPs or DSPPs with deep kernel learning (e.g., using CNNs as feature extractors) preserve uncertainty calibration in high-dimensional domains like computer vision?
- Basis in paper: Section 5.1 discusses "deep kernel learning" to scale to image data using CNNs or vision transformers.
- Why unresolved: The current study is restricted to moderately complex tabular data, and the authors note it is difficult to scale these models to high-dimensional data without architectural modifications.
- What evidence would resolve it: Evaluation of calibration and robustness on image benchmarks (e.g., CIFAR or ImageNet) using hybrid DGP/DSPP architectures compared to standard ensembles.

### Open Question 2
- Question: Do alternative likelihoods (e.g., Student-t, Laplace, or Ordinal) improve the robustness of DSPPs and DGPs to outliers and distributional shifts compared to standard Gaussian/Softmax likelihoods?
- Basis in paper: Section 5.1 suggests future work should include a "larger class of likelihoods" to provide greater resilience to outliers.
- Why unresolved: The current experiments only utilized Gaussian likelihood for regression and Softmax for classification; the impact of heavy-tailed or ordinal likelihoods on the reported calibration metrics is unknown.
- What evidence would resolve it: Comparative performance analysis (NLL, ECE) on datasets with heavy-tailed noise or ranked labels, testing the specified alternative likelihoods.

### Open Question 3
- Question: Can DSPPs effectively quantify uncertainty in Reinforcement Learning (RL) policy estimation or value functions to enhance safe exploration?
- Basis in paper: Section 5.1 identifies RL as a "promising area" where DSPPs have not yet been explored for direct policy estimation.
- Why unresolved: Existing work applying DGPs to RL focuses on value functions, but the DSPP framework has not been validated in the sequential decision-making setting where uncertainty guides exploration.
- What evidence would resolve it: Experiments applying DSPPs to standard RL benchmarks (e.g., robotics control) measuring sample efficiency and adherence to safety constraints during training.

## Limitations

- Synthetic distribution shift scope: Only 5 synthetic perturbation types tested; real-world shifts may behave differently
- Dataset specificity: Results based on only two datasets with fixed feature dimensionalities; may not extend to high-dimensional or structured data
- Hyperparameter optimization constraints: Limited search space (20 trials, 5 random initializations) may miss optimal configurations

## Confidence

- **High confidence**: DGP sensitivity to distribution shifts (consistent degradation patterns across multiple severity levels and both tasks)
- **Medium confidence**: DSPP in-distribution calibration superiority (strong ECE results but limited ablation on sigma point quadrature quality)
- **Medium confidence**: Deep Ensemble robustness (robustness demonstrated but diversity mechanisms not quantified)

## Next Checks

1. **Real-world distribution shift validation**: Apply models to datasets with known natural distribution shifts (e.g., DomainNet, WILDS) to test whether synthetic shift performance correlates with real-world robustness.

2. **Sigma point sensitivity analysis**: Systematically vary the number of sigma points Q in DSPPs to determine the minimum required for stable calibration, and test performance when posterior distributions become multimodal.

3. **Ensemble diversity quantification**: Measure functional diversity metrics (e.g., pairwise agreement, loss landscape curvature) across ensemble members to validate whether diversity explains the robustness advantage, or if it's primarily an averaging effect.