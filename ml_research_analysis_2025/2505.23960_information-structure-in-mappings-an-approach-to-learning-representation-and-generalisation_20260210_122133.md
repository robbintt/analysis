---
ver: rpa2
title: 'Information Structure in Mappings: An Approach to Learning, Representation,
  and Generalisation'
arxiv_id: '2505.23960'
source_url: https://arxiv.org/abs/2505.23960
tags:
- information
- entropy
- language
- structure
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Information Structure in Mappings: An Approach to Learning, Representation, and Generalisation

## Quick Facts
- **arXiv ID:** 2505.23960
- **Source URL:** https://arxiv.org/abs/2505.23960
- **Reference count:** 0
- **Key outcome:** Information-theoretic primitives (Regularity, Variation, Disentanglement) in neural representations predict generalization performance, with larger models compressing representations to improve robustness.

## Executive Summary
This thesis introduces a framework for quantifying information structure in neural network representations using information-theoretic primitives. By analyzing mappings between inputs and outputs as distributions, it measures Regularity (mutual information), Variation (conditional entropy), and Disentanglement (Jensen-Shannon divergence). The framework reveals that successful learning involves a two-phase process: rapid token alignment followed by slow contextualization that improves robustness to noise and out-of-distribution generalization. The approach successfully scales to large language models through a "Soft Entropy" estimation technique.

## Method Summary
The core methodology centers on Soft Entropy Estimation, which transforms continuous vector representations into categorical distributions for information-theoretic analysis. This involves normalizing representations to a unit sphere, sampling anchor points, computing cosine similarities, and applying softmax to create a probability distribution over anchors. From this distribution, entropy, mutual information, conditional entropy, and Jensen-Shannon divergence are computed to quantify the three structural primitives. The method is applied across training stages and model scales, with correlation analysis linking structure measures to downstream performance.

## Key Results
- Language models develop two distinct training phases: rapid token-level alignment followed by slow contextualization that improves noise robustness
- Larger models compress representations through "smaller alphabets per dimension" while maintaining functional capacity
- Information structure measures (particularly token/bigram information proportion) correlate with downstream GLUE task performance
- Regularity and disentanglement increase with model size, while variation decreases

## Why This Works (Mechanism)

### Mechanism 1: Information Structure as a Universal Descriptor
The thesis proposes that representational success depends on balancing three structural primitives rather than maximizing any single one. Regularity ensures consistent mappings, variation allows for flexibility, and disentanglement maintains separability. This framework uses information-theoretic quantities to quantify these properties, revealing that natural language achieves robustness through "compositionality with variation." The core assumption is that high-dimensional vector spaces can be meaningfully discretized via Soft Entropy to capture true representational structure.

### Mechanism 2: The "Two-Phase" Learning Trajectory
Models learn structure non-uniformly, undergoing rapid alignment (Phase 1) where representations quickly match tokens, followed by slow contextualization (Phase 2) where representations become robust to noise. Generalization correlates with Phase 2 progress, suggesting contextualization is key to out-of-distribution performance. The training loss landscape naturally separates into these regimes, with noisy gradient updates serving as implicit regularization.

### Mechanism 3: Capacity-Dependent Regularization
The effect of capacity on structure depends on modality. In discrete settings, lower capacity forces more regularity. In continuous representations, larger capacity enables compression through expanded hidden dimensions, allowing models to use "smaller alphabets per dimension" and achieve robustness through redundancy rather than high per-dimension entropy.

## Foundational Learning

- **Concept: Information-Theoretic Primitives (Entropy, Mutual Information, JS Divergence)**
  - **Why needed here:** The entire framework maps representational structure to these quantities. Without understanding Regularity as Mutual Information, the metrics are uninterpretable.
  - **Quick check question:** Why would a "regular" mapping have high Mutual Information between input labels and output vectors?

- **Concept: Disentanglement vs. Independence**
  - **Why needed here:** The thesis distinguishes disentanglement (separability of clusters) from linear independence. It measures whether labels are encoded in distinct regions of space.
  - **Quick check question:** If two tokens have orthogonal representations but are statistically inseparable in clustering, are they "disentangled"?

- **Concept: Soft Entropy / Limiting Density of Discrete Points**
  - **Why needed here:** This enables analysis of LLMs by mapping continuous vectors to a unit sphere for categorical distribution estimation, solving the unbounded differential entropy problem.
  - **Quick check question:** How does the number of anchor points (n) affect the trade-off between precision and computational cost in Soft Entropy?

## Architecture Onboarding

- **Component map:** Raw text -> Token/Bigram/Trigram labels -> Hidden states (Y) -> Soft Entropy Engine -> Categorical Distribution P(Y) -> Structure Calculator -> Entropy, MI, Conditional Entropy, JS Divergence
- **Critical path:** Soft Entropy Estimation is the bottleneck, requiring normalization to unit sphere, anchor point sampling, dot-product distance computation, and softmax to create categorical distributions for large-scale models.
- **Design tradeoffs:** Dimension-wise vs. Subspace Entropy (latter captures angular structure but requires choosing n); scaling distances to [-100, 100] for gradient flow (incorrect scaling causes vanishing/exploding gradients).
- **Failure signatures:** Collapsed Entropy (check scaling factor), Missing Phase 2 (model stuck overfitting tokens), Capacity Mismatch (large model with higher entropy than small model).
- **First 3 experiments:**
  1. Reproduce Phase Transition: Train small Transformer and plot Regularity vs. Disentanglement over steps to verify crossing point.
  2. Benchmark Soft Entropy: Generate random Gaussian vectors and compare Soft Entropy estimates against closed-form differential entropy.
  3. Correlation Check: Analyze pre-trained vs. random models to verify lower Variation in pre-trained models confirms alignment hypothesis.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does regularity-correlation with generalization re-emerge at minimal variation extremes? (Basis: Chapter 3.5 notes this remains untested as no models approached minimal variation)
- **Open Question 2:** Can directly optimizing representational structure improve generalization? (Basis: Chapter 7.2 identifies this as future direction)
- **Open Question 3:** Can residual information in LLM representations be fully explained by richer labels? (Basis: Chapter 5.6.3 and 7.2 note significant unexplained residual)

## Limitations

- Soft Entropy estimation computational intensity may not scale efficiently to 100B+ parameter models
- Reported patterns may be artifacts of specific hyperparameters (n=50 anchors, [-100, 100] scaling) without systematic ablation
- Correlation between structure and capability does not establish causation - structure may drive capability or vice versa

## Confidence

**High Confidence**: Three-phase characterization and two-phase learning trajectory with information-theoretic quantification
**Medium Confidence**: Soft Entropy methodology and its relationship to functional generalization
**Low Confidence**: Capacity-dependent regularization hypothesis about compression through "smaller alphabets per dimension"

## Next Checks

1. **Systematic Anchor Point Sensitivity Analysis**: Test Soft Entropy across anchor counts (n=10, 25, 50, 100, 200) and scaling factors on BERT-base to determine robustness to estimation choices
2. **Causal Intervention Experiment**: Target specific information-theoretic primitives (e.g., increase disentanglement via adversarial training) to test whether framework captures causal structure
3. **Scaling Law Validation**: Apply framework to controlled scaling experiment (100M to 10B parameters) to verify predicted patterns hold across model families