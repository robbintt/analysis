---
ver: rpa2
title: 'ETS: Energy-Guided Test-Time Scaling for Training-Free RL Alignment'
arxiv_id: '2601.21484'
source_url: https://arxiv.org/abs/2601.21484
tags:
- reward
- pref
- sampling
- scaling
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ETS, a training-free inference method to sample
  directly from the optimal RL policy for LLM alignment. The key idea is to decompose
  the optimal transition kernel into a reference model's kernel and an energy term
  based on expected exponentiated rewards.
---

# ETS: Energy-Guided Test-Time Scaling for Training-Free RL Alignment

## Quick Facts
- arXiv ID: 2601.21484
- Source URL: https://arxiv.org/abs/2601.21484
- Reference count: 40
- Authors: Xiuyu Li; Jinkai Zhang; Mingyang Yi; Yu Li; Longqiang Wang; Yue Wang; Ju Fan
- Primary result: ETS improves generation quality over standard inference and test-time scaling baselines without any training

## Executive Summary
ETS proposes a training-free method to sample from the optimal RL policy for LLM alignment at inference time. The approach decomposes the optimal transition kernel into a reference model's kernel and an energy term based on expected exponentiated rewards. ETS estimates this energy term online via Monte Carlo sampling with provable convergence bounds, and accelerates inference using lightweight proposal models with importance sampling correction. Experiments on autoregressive and diffusion models across reasoning, coding, and science tasks show ETS consistently improves generation quality, even matching post-trained RL policy performance without any training.

## Method Summary
ETS operates by decomposing the optimal KL-regularized RL policy into a reference model component and an energy term based on expected exponentiated rewards. At each of I guidance steps, M candidates are sampled from the reference model, and for each candidate, K completions are generated to estimate the energy via Monte Carlo sampling. Candidates are reweighted by their estimated energies and one is selected for the next step. To accelerate, ETS-IS uses a lightweight proposal model for sampling and applies importance sampling correction using likelihood ratios between the reference and proposal models.

## Key Results
- ETS consistently improves generation quality over standard inference and test-time scaling baselines
- Achieves comparable performance to post-trained RL policy without any training
- ETS-IS provides 30-50% latency reduction with only 3-5% accuracy drop
- Scaling M (candidates) improves accuracy more than scaling K (completions per candidate)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The optimal KL-regularized RL policy can be sampled at inference time by decomposing its transition kernel into a reference model component and an energy term.
- Mechanism: Proposition 2 shows that for masked language models, the backward transition p(x_s | x_t, y) ∝ p_ref(x_s | x_t, y) · E_{p_ref}[exp(r(y,x_0)/λ)], where the energy term represents expected exponentiated future rewards.
- Core assumption: The KL-regularized objective admits the closed-form solution from RLHF theory.
- Evidence anchors: Energy-guided diffusion methods show similar decompositions in continuous domains; ETS extends this to discrete MLM.

### Mechanism 2
- Claim: Monte Carlo estimation of the energy term converges to the true energy with provable error bounds.
- Mechanism: Algorithm 1 samples K completions from the reference model and averages exp(r/λ). Proposition 3 bounds total variation distance as Õ(I/√M + Iε).
- Core assumption: Reward r(y,x_0) is bounded in [−D, D], ensuring sub-Gaussian concentration.
- Evidence anchors: Theoretical convergence bound with TV distance; lacks direct empirical validation.

### Mechanism 3
- Claim: Lightweight proposal models with importance sampling correction reduce latency while preserving unbiased energy estimates.
- Mechanism: Algorithm 2 samples from p_small instead of p_ref, then reweights by the likelihood ratio l(x_0) = p_ref/p_small. Theorem 1 shows TV distance scales as Õ(I/√M + I/√K).
- Core assumption: The likelihood ratio l(x_0) is bounded by some constant L.
- Evidence anchors: ETS-IS achieves 66.45% avg accuracy on Qwen3-8B vs 69.34% for ETS, with 5.26× latency vs 7.35×.

## Foundational Learning

- Concept: **KL-regularized RL optimal policy structure**
  - Why needed here: Understanding why p* ∝ p_ref · exp(r/λ) is fundamental to grasping how ETS samples without training.
  - Quick check question: Can you derive why this form emerges from the Lagrangian of max E[r] − λ·D_KL(p || p_ref)?

- Concept: **Importance sampling with proposal distributions**
  - Why needed here: Algorithm 2's acceleration depends on understanding when p_small can substitute for p_ref and how reweighting corrects the distribution mismatch.
  - Quick check question: If E_{p}[f(X)] is estimated using samples from q, what correction factor ensures unbiasedness?

- Concept: **Masked Language Modeling as backward Markov chain**
  - Why needed here: ETS operates on the backward transition x_T → x_{T−1} → ... → x_0; understanding this structure is required to see where energy guidance injects.
  - Quick check question: For ARMs vs DLMs, how does the mask schedule Mt differ, and what does this imply for where ETS can intervene?

## Architecture Onboarding

- Component map: Reference model -> Energy estimator -> Candidate selector -> Guidance scheduler
- Critical path:
  1. At guidance step i, sample M candidates from p_ref(x_{t_{i-1}} | y, x_{t_i})
  2. For each candidate, draw K completions (from p_ref or p_small)
  3. Compute reward for each completion, aggregate into energy estimate
  4. Normalize energies, sample one candidate, proceed to next step
  5. Repeat I times until x_0 is complete
- Design tradeoffs:
  - **M vs K**: Scaling M improves accuracy more than scaling K
  - **I (guidance steps)**: More steps = finer-grained alignment but linear error accumulation; authors recommend I=4-8
  - **ETS vs ETS-IS**: ETS for peak performance; ETS-IS when latency constrained
  - **Reward choice**: Self-consistency works for reasoning tasks with extractable answers
- Failure signatures:
  - Energy collapse: All candidates get similar energies → no guidance benefit
  - Proposal mismatch: ETS-IS degrades if p_small has low overlap with p_ref
  - Verification noise: Self-consistency sensitive to answer extraction quality
  - Latency blowup: Ensure parallel batching is working
- First 3 experiments:
  1. **Sanity check on a small model**: Run ETS with M=3, K=3, I=1 on GSM8K with Qwen3-1.7B; verify accuracy improves over base
  2. **Ablate M vs K**: Fix M×K=30, sweep (M,K) ∈ {(3,10), (5,6), (10,3), (15,2)}; confirm larger M yields better accuracy
  3. **Validate IS acceleration**: Compare ETS vs ETS-IS on Qwen3-8B with identical (M,K,I); measure accuracy drop vs latency reduction

## Open Questions the Paper Calls Out

- **Open Question 1**: Can a tighter, state-dependent sampling error bound be derived that accounts for guidance error varying across different intermediate states xt?
  - Basis: Remark 2 states guidance error ϵ may vary across different xt
  - Why unresolved: Current analysis assumes uniform guidance error
  - Resolution: Derive bounds with state-dependent ϵ(xt) and validate empirically

- **Open Question 2**: How can ETS be extended to open-ended generation tasks where self-consistency voting fails?
  - Basis: Reward design relies on extracting final answers for majority voting
  - Why unresolved: For summarization or dialogue, answer extraction is ill-defined
  - Resolution: Design alternative training-free rewards for open-ended tasks

- **Open Question 3**: How does ETS-IS performance degrade when the proposal model diverges significantly from the reference model?
  - Basis: Theorem 1's bound depends on bounded likelihood ratio
  - Why unresolved: No analysis of mismatched proposals
  - Resolution: Systematically vary p_small quality and measure accuracy-latency tradeoffs

## Limitations

- The convergence bounds are theoretical and lack empirical validation of their practical implications
- The choice of self-consistency voting as the sole reward function limits applicability to tasks with extractable, verifiable answers
- The importance sampling acceleration relies on unverified assumptions about bounded likelihood ratios between reference and proposal models

## Confidence

**High confidence**: The core mathematical formulation and Monte Carlo energy estimation algorithm are sound and well-established RL theory. Experimental results showing ETS outperforming baselines are reproducible.

**Medium confidence**: The convergence bounds and accuracy-latency tradeoffs in ETS-IS are mathematically correct but their practical implications are uncertain due to unverified assumptions.

**Low confidence**: The claim that ETS "consistently improves" across all tasks may be overstated, as the paper shows mixed results on different benchmarks and the approach could fail catastrophically on tasks with noisy rewards.

## Next Checks

1. **Validate convergence bounds empirically**: Run ETS with varying M and K on GSM8K with Qwen3-1.7B and measure energy estimation variance, accuracy degradation, and whether empirical error matches theoretical Õ(I/√M + Iε) scaling.

2. **Stress-test the importance sampling assumption**: For ETS-IS, compute empirical likelihood ratios l(x_0) = p_ref/p_small and measure maximum ratio, coefficient of variation of weights, and correlation between IS-corrected and standard energies.

3. **Test ETS with alternative reward functions**: Implement ETS using a verifier-based reward instead of self-consistency voting on GSM8K and compare accuracy and energy stability to the self-consistency baseline.