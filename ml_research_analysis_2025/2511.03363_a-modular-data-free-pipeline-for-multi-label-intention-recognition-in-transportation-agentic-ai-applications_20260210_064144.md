---
ver: rpa2
title: A Modular, Data-Free Pipeline for Multi-Label Intention Recognition in Transportation
  Agentic AI Applications
arxiv_id: '2511.03363'
source_url: https://arxiv.org/abs/2511.03363
tags:
- loss
- multi-label
- transportation
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents DMTC, a modular, data-free pipeline for multi-label
  intention recognition in transportation agentic AI systems. The approach eliminates
  the need for costly annotated datasets by generating synthetic training data through
  prompt-engineered LLMs, encoding text with Sentence-T5 for semantic representation,
  and training with a novel online focal-contrastive loss that emphasizes hard samples.
---

# A Modular, Data-Free Pipeline for Multi-Label Intention Recognition in Transportation Agentic AI Applications

## Quick Facts
- arXiv ID: 2511.03363
- Source URL: https://arxiv.org/abs/2511.03363
- Reference count: 6
- Primary result: 70.15% accuracy, 95.92% AUC, 5.35% Hamming loss on maritime multi-label intention recognition

## Executive Summary
This paper introduces DMTC, a modular pipeline for multi-label intention recognition in transportation agentic AI systems that eliminates the need for costly annotated datasets. The approach generates synthetic training data through prompt-engineered LLMs, encodes text using Sentence-T5, and employs a novel Online Focal-Contrastive loss to emphasize hard samples. Applied in maritime transportation with 8 intent categories, DMTC achieves state-of-the-art performance, outperforming both classical ML methods and end-to-end LLM baselines like GPT-4 and GPT-4o.

## Method Summary
DMTC addresses multi-label intention recognition by generating synthetic queries per class using LLaMA 2 prompt engineering, encoding text with Sentence-T5 embeddings using average pooling, and training with a novel Online Focal-Contrastive loss that emphasizes hard samples. The pipeline first synthesizes training data for each of the 8 maritime intent classes, then encodes these queries into semantic representations. A pre-training phase uses the OFC loss to optimize embeddings for contrastive learning, followed by fine-tuning with cross-entropy loss for multi-label classification. The modular design allows autonomous routing of user queries to task-specific modules without manual labeling.

## Key Results
- Achieves 70.15% subset accuracy, 95.92% AUC, and 5.35% Hamming loss on maritime transportation tasks
- Outperforms classical ML methods (SVM, RF, XGBoost) by 15% in accuracy and 18% in AUC
- Sentence-T5 embeddings provide at least 3.29% improvement in subset accuracy over alternatives
- OFC loss contributes an additional 0.98% performance gain through hard sample emphasis

## Why This Works (Mechanism)
The approach succeeds by addressing the data scarcity problem through synthetic data generation, leveraging pre-trained language models for semantic encoding, and optimizing the training process for multi-label classification through contrastive learning. The OFC loss specifically targets hard samples that are typically challenging for traditional loss functions, improving the model's ability to distinguish between similar intent classes. The modular architecture enables flexible deployment across different transportation domains while maintaining high accuracy.

## Foundational Learning

**Multi-label Classification**: Learning to predict multiple labels simultaneously from a single input, required for intention recognition where queries can have multiple intents. Quick check: Verify label distribution is balanced across classes.

**Contrastive Learning**: Training method that learns representations by comparing similar and dissimilar pairs, essential for distinguishing between related intent categories. Quick check: Monitor positive/negative pair similarities during training.

**Prompt Engineering**: Crafting specific input templates to guide LLM outputs, critical for generating high-quality synthetic training data. Quick check: Evaluate semantic diversity of generated samples.

**Hard Sample Mining**: Identifying and emphasizing difficult training examples, improves model robustness on challenging intent distinctions. Quick check: Track training loss trends for easy vs hard samples.

## Architecture Onboarding

**Component Map**: LLM Generator -> Sentence-T5 Encoder -> OFC Pre-training -> Cross-Entropy Fine-tuning -> Classification Module

**Critical Path**: The pipeline's performance bottleneck is synthetic data generation quality, as poor prompts lead to limited semantic diversity. The OFC loss implementation is also critical, as improper hard sample selection can destabilize training.

**Design Tradeoffs**: Data-free approach eliminates annotation costs but relies heavily on LLM quality and prompt engineering skill. The modular design offers flexibility but requires careful integration of components. Sentence-T5 provides good semantic encoding but may miss domain-specific nuances.

**Failure Signatures**: 
- Low accuracy despite good prompts suggests Sentence-T5 embedding limitations
- Unstable training indicates incorrect OFC loss hyperparameters
- Poor generalization implies insufficient synthetic data diversity

**First Experiments**:
1. Generate 100 synthetic samples per class using varied prompts, evaluate semantic diversity with clustering
2. Implement OFC loss with default hyperparameters, test on small synthetic dataset to verify stability
3. Compare Sentence-T5 vs BERT embeddings on fixed synthetic data with identical training procedure

## Open Questions the Paper Calls Out

None

## Limitations

- Missing critical hyperparameters for OFC loss (α, γ, margin m, hard sample percentage p) that affect performance
- Sentence-T5 model variant and training configurations unspecified, impacting reproducibility
- Synthetic data generation quality heavily dependent on prompt engineering and LLM variant
- Performance metrics require complete implementation details for independent verification

## Confidence

**High Confidence**: Classical ML methods' performance (55.35% accuracy, 77.92% AUC) aligns with established limitations of traditional approaches for complex multi-label tasks.

**Medium Confidence**: Sentence-T5 embedding superiority (3.29% improvement) is plausible but depends on implementation details and model variants.

**Low Confidence**: Exact OFC loss contribution (0.98% gain) cannot be fully validated without specified hyperparameters and training configurations.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Test OFC loss with multiple α (0.25, 0.5, 0.75), γ (1.0, 2.0, 3.0) values and hard sample percentages (10%, 20%, 30%) to identify optimal configuration achieving 0.98% improvement.

2. **Synthetic Data Quality Assessment**: Generate samples using LLaMA 2 with varying prompt templates and sample sizes (n=50, 100, 200 per class) to determine minimum dataset size maintaining 70.15% accuracy threshold.

3. **Embedding Model Comparison**: Train complete pipeline using different Sentence-T5 variants (base, large) and alternatives (BERT, RoBERTa) with identical OFC configurations to verify claimed 3.29% subset accuracy advantage.