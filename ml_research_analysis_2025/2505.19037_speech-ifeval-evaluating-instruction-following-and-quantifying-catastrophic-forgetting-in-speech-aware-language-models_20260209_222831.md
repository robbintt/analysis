---
ver: rpa2
title: 'Speech-IFEval: Evaluating Instruction-Following and Quantifying Catastrophic
  Forgetting in Speech-Aware Language Models'
arxiv_id: '2505.19037'
source_url: https://arxiv.org/abs/2505.19037
tags:
- speech
- slms
- arxiv
- instruction-following
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Speech-IFEval, a framework to evaluate instruction-following
  abilities and catastrophic forgetting in speech-aware language models (SLMs). The
  authors identify that existing benchmarks conflate speech perception with instruction-following,
  making it difficult to isolate these distinct capabilities.
---

# Speech-IFEval: Evaluating Instruction-Following and Quantifying Catastrophic Forgetting in Speech-Aware Language Models

## Quick Facts
- arXiv ID: 2505.19037
- Source URL: https://arxiv.org/abs/2505.19037
- Reference count: 0
- Key outcome: Introduces Speech-IFEval framework to evaluate instruction-following and quantify catastrophic forgetting in speech-aware language models, finding most SLMs struggle with basic instructions while earlier models show substantial performance degradation.

## Executive Summary
This paper introduces Speech-IFEval, a framework to evaluate instruction-following abilities and catastrophic forgetting in speech-aware language models (SLMs). The authors identify that existing benchmarks conflate speech perception with instruction-following, making it difficult to isolate these distinct capabilities. Speech-IFEval addresses this by applying verifiable output constraints to speech-instruction pairs, enabling evaluation of instruction-following independent of speech perception. The framework evaluates three categories: closed-ended questions, creative writing, and chain-of-thought reasoning. Results show most SLMs struggle with basic instructions, performing significantly worse than text-based LLMs, with earlier models like LTU-AS and SALMONN showing substantial degradation (-50.20 and -54.90) while DeSTA2 achieved the highest instruction-following rate (89.23) with minimal forgetting (-3.57).

## Method Summary
The study evaluates SLMs by applying verifiable output constraints to speech-instruction pairs, using a cascade pipeline with text-based LLMs as reference systems. The framework measures instruction-following rate (IFrate) and quantifies performance degradation through a "forgetting rate" metric. Speech-instruction pairs from Dynamic-SUPERB, AIR-bench, and MMAU speech subset are used, covering ASR, SER, gender recognition, and general speech understanding. Three task categories are evaluated: closed-ended questions, creative writing, and chain-of-thought reasoning. Rule-based verification is applied for constraints, with GPT-4o used for CoT evaluation.

## Key Results
- Most SLMs struggle with basic instructions, performing significantly worse than text-based LLMs
- DeSTA2 achieved highest instruction-following rate (89.23) with minimal forgetting (-3.57)
- Earlier models like LTU-AS and SALMONN showed substantial degradation (-50.20 and -54.90)
- SLMs exhibit high sensitivity to prompt variations, often modifying responses or generating hallucinated content when constraints are added

## Why This Works (Mechanism)

### Mechanism 1
Adding output constraints independent of speech content can isolate instruction-following ability from speech perception capability. Constraints (e.g., "format in capital letters," "wrap in JSON," "start with 'The answer is:'") rely purely on textual understanding. Since these constraints are unrelated to the speech input itself, success indicates the model's instruction-following capacity, not its speech understanding. Core assumption: The constraint processing pathway does not fundamentally interact with speech processing; they are separable cognitive functions. Evidence: [abstract] "Speech-IFEval addresses this by applying verifiable output constraints to speech-instruction pairs, enabling evaluation of instruction-following independent of speech perception." [section 3.1] "As a result, we assess the model's ability to follow the constraint rather than the correctness of its transcriptions."

### Mechanism 2
Speech-text training in SLMs causes catastrophic forgetting of textual instruction-following capabilities acquired by the base LLM. During speech-text fine-tuning, the model's weights are updated to prioritize speech-related patterns, potentially overwriting or degrading the instruction-following representations learned during text-only training. Core assumption: The weight updates during speech training compete with or overwrite text-only instruction representations rather than adding new capabilities orthogonally. Evidence: [abstract] "Recent SLMs integrate speech perception with large language models (LLMs), often degrading textual capabilities due to speech-centric training." [section 1] "Despite being built on powerful text-based LLMs, SLMs often experience a decline in textual capabilities after training on speech-text pairs."

### Mechanism 3
Test-time LoRA scaling can partially recover instruction-following capability by reducing the influence of speech-specialized adapters. LoRA adapters trained on speech tasks may overfit to speech-specific output patterns. Scaling down their contribution (increasing α in SALMONN's formulation) gives more weight to the frozen base LLM, restoring some original instruction-following behavior. Core assumption: The degradation is partially localized in the LoRA adapters rather than being distributed across the entire model. Evidence: [section 5.3] "This technique significantly improves IFrate, reducing the forgetting rate from -50.20 to -8.12 when α = 4, but at the cost of task-level performance." [table 2] Shows SALMONN (α = 4) improving IFrate from 36.89 to 68.06.

## Foundational Learning

- Concept: **Catastrophic Forgetting in Continual Learning**
  - Why needed here: The entire paper frames SLM development as a continual learning problem where new modality training overwrites prior capabilities.
  - Quick check question: Can you explain why fine-tuning on a new task distribution might degrade performance on the original task distribution?

- Concept: **Constraint-Based Evaluation (IFEval paradigm)**
  - Why needed here: Speech-IFEval adapts the text-based IFEval approach by adding verifiable constraints that don't require ground-truth answers.
  - Quick check question: How does adding format constraints help disentangle instruction-following from content correctness?

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: Several SLMs (SALMONN, LTU-AS) use LoRA adapters, and the paper analyzes test-time LoRA scaling as a mitigation technique.
  - Quick check question: What happens to a model's output when you scale the LoRA adapter contribution toward zero?

## Architecture Onboarding

- Component map: Speech Encoder (Whisper, wav2vec, etc.) -> Feature Projection / Adapter -> LLM Backbone (Vicuna, Llama, Qwen) + [Optional: LoRA adapters] -> Text Output

- Critical path:
  1. Speech → Encoder → Features
  2. Features → Adapter → LLM embedding space
  3. [Text Instruction] + [Projected Features] → LLM → Response
  4. Response → Constraint Verifier (rule-based or GPT-4o for CoT)

- Design tradeoffs:
  - Full fine-tuning vs. LoRA: LoRA preserves base LLM better but may limit speech integration quality
  - Training data composition: Speech-centric data improves perception but risks forgetting; balanced data may sacrifice speech quality
  - Adapter architecture: Aggressive alignment (LTU-AS, SALMONN) vs. gentle alignment (DeSTA2's descriptive speech-text alignment)

- Failure signatures:
  - High forgetting rate (>30%): Model ignores formatting constraints despite correct speech understanding
  - Constraint sensitivity: Adding constraints causes answer changes or hallucinations (LTU-AS in ASR task, Table 4)
  - Format rigidity: Model outputs correct content but wrong structure (Qwen2-Audio failing JSON/quotation constraints)

- First 3 experiments:
  1. Baseline cascade test: Run your speech encoder → ASR → text LLM pipeline. Measure IFrate on Speech-IFEval constraints. This establishes the reference performance ceiling.
  2. End-to-end SLM test: Run the same Speech-IFEval constraints through your integrated SLM. Compute forgetting rate (Δ) against the cascade baseline.
  3. Ablation on constraint types: Test each constraint category separately (CC, SE, WR, JS for closed-ended; BP, KW, LN for creative writing). Identify which constraint types your model fails on—this diagnoses whether the issue is formatting (JS, WR) vs. content manipulation (KW, LN).

## Open Questions the Paper Calls Out

### Open Question 1
What training strategies can jointly optimize speech perception and instruction-following capabilities without inducing trade-offs between them? Basis: The test-time LoRA scaling experiment shows that recovering instruction-following ability (from -50.20 to -8.12) comes at the cost of task-level performance, suggesting fundamental tension between these capabilities. Unresolved because current approaches either preserve instruction-following with degraded speech task performance, or achieve strong speech tasks with poor instruction adherence. Evidence: A training methodology that achieves both high IFrate (>85%) and competitive task-level performance across ASR, SER, GR, and MMAU simultaneously.

### Open Question 2
Why are SLMs highly sensitive to constraint prompt variations that are unrelated to the speech content, causing response modifications and hallucinations? Basis: The authors state "most models exhibit significant fluctuations" when constraints are added, and "most models modify their answer, while others generate hallucinated outputs" (p. 4). Unresolved because the paper demonstrates the sensitivity but does not identify the underlying mechanism causing models to alter task-relevant outputs when given format-only constraints. Evidence: Ablation studies identifying which training components (speech encoder alignment, instruction tuning data, LoRA rank) contribute to prompt sensitivity.

### Open Question 3
How generalizable is the constraint-based evaluation framework to languages and speech characteristics beyond English? Basis: The dataset sources from established English benchmarks, and constraint types (capitalization, JSON formatting) may not transfer to languages with different orthographic or structural conventions. Unresolved because the paper evaluates only English speech-instruction pairs, leaving cross-linguistic validity unexplored. Evidence: Evaluation of multilingual SLMs using language-appropriate constraints showing comparable IFrate patterns across languages.

## Limitations

- Dataset Generalization: The study relies on a curated subset of existing speech benchmarks, but the specific samples and distribution characteristics aren't fully specified, limiting confidence in how results generalize to broader speech tasks or real-world applications.
- Constraint Design Validity: There's uncertainty about whether certain constraints might inadvertently interact with speech processing, potentially confounding the isolation assumption of the evaluation framework.
- Evaluation Subjectivity: CoT reasoning evaluation uses GPT-4o as an oracle, introducing subjectivity due to unspecified verification criteria and prompt templates.

## Confidence

- High Confidence: The core finding that SLMs experience catastrophic forgetting of text-based instruction-following capabilities is well-supported by multiple SLM evaluations and corroborated by neighboring research (Paper ID 33110).
- Medium Confidence: The mechanism that constraint-based evaluation successfully isolates instruction-following from speech perception is theoretically sound but lacks extensive empirical validation beyond this study.
- Medium Confidence: Test-time LoRA scaling as a mitigation strategy shows promising results for SALMONN but is only demonstrated on a single model with limited ablation studies.

## Next Checks

1. Constraint Isolation Validation: Design an experiment where you systematically vary constraint types (format vs. content-based) and measure whether any constraint inadvertently affects speech perception accuracy. This would validate the core assumption of the evaluation framework.

2. Cross-Dataset Robustness Test: Evaluate the same SLMs on a completely independent speech instruction-following dataset (not used in the paper) to assess whether forgetting patterns hold across different speech domains and instruction types.

3. Architecture-Agnostic Forgetting Analysis: Test whether the forgetting phenomenon occurs across different SLM architectures (not just LoRA-based) by implementing a full fine-tuning baseline and comparing forgetting rates to establish whether the mechanism is architecture-specific.