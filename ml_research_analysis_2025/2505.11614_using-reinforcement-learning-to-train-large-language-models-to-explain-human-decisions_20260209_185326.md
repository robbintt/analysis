---
ver: rpa2
title: Using Reinforcement Learning to Train Large Language Models to Explain Human
  Decisions
arxiv_id: '2505.11614'
source_url: https://arxiv.org/abs/2505.11614
tags:
- human
- option
- cognitive
- training
- choice
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a reinforcement learning approach to elicit
  interpretable cognitive models from large language models. The core idea is to train
  LLMs to generate both behavioral predictions and reasoning traces explaining human
  risky choices, using outcome-based rewards derived from human choice data.
---

# Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions

## Quick Facts
- **arXiv ID:** 2505.11614
- **Source URL:** https://arxiv.org/abs/2505.11614
- **Reference count:** 36
- **Primary result:** RL-trained LLMs achieve comparable prediction accuracy to SFT while generating more interpretable cognitive reasoning for human risky choices.

## Executive Summary
This paper introduces a reinforcement learning approach to elicit interpretable cognitive models from large language models by training them to generate both behavioral predictions and reasoning traces explaining human risky choices. Using outcome-based rewards derived from human choice data, the RL-trained models produce reasoning chains that adapt to the structure of the training data—shifting from risk-averse explanations with human data to expected-value-focused reasoning with synthetic data. The approach demonstrates that RL can elicit scalable, interpretable cognitive theories from LLMs, though its effectiveness is bounded by the model's underlying knowledge capacity.

## Method Summary
The authors train LLMs to predict human choices in risky decision problems while generating chain-of-thought (CoT) reasoning explanations. They compare three post-training approaches on Qwen-2.5-7B-Instruct with LoRA adapters: supervised fine-tuning (SFT), Centaur-style SFT (masking non-bracketed tokens), and Group Relative Policy Optimization (GRPO). The GRPO reward function combines prediction accuracy (1 − |predicted − actual|) with JSON format compliance. Training uses 12 candidate generations per problem, with critical removal of standard deviation normalization to prevent mode collapse. The dataset consists of 13,102 training and 1,462 test problems from the choices13k dataset.

## Key Results
- RL achieves comparable Mean Squared Error to SFT on risky choice prediction tasks
- RL generates more interpretable cognitive mechanisms (expected value computation, risk aversion) in CoT reasoning
- Reasoning chains adapt to training data structure, shifting from risk-averse to expected-value explanations when switching between human and synthetic datasets
- RL performance degrades significantly on weaker backbone models (Gemma-2-2B), suggesting capacity requirements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reinforcement learning with outcome-based rewards acts as a selection mechanism for latent cognitive theories already present in the backbone LLM, rather than teaching novel reasoning from scratch.
- **Mechanism:** The "elicitation hypothesis" posits that RL post-training increases the probability of generating correct outputs by amplifying pre-existing theoretical representations stored in the model's weights during pre-training.
- **Core assumption:** The backbone LLM possesses sufficient prior knowledge of the domain (e.g., decision theory) to generate the target reasoning traces before fine-tuning begins.
- **Evidence anchors:** [abstract] "RL can elicit scalable, interpretable cognitive theories from LLMs, though its effectiveness is bounded by the model's underlying knowledge." [section 6] "RL post-training does not teach new capabilities but instead amplifies and selects from pre-existing knowledge."
- **Break condition:** If the backbone LLM lacks the specific domain knowledge, RL will likely fail to elicit coherent reasoning or will hallucinate invalid mechanisms.

### Mechanism 2
- **Claim:** The semantic content of generated reasoning chains (CoTs) adapts conditional on the statistical structure of the ground-truth reward signal, aligning explanations with the data's underlying causality.
- **Mechanism:** GRPO optimizes a policy where the reward is derived from prediction accuracy, forcing the model to utilize reasoning strategies that statistically minimize error relative to the specific dataset provided.
- **Core assumption:** The reward function is sufficiently shaped to differentiate between valid and invalid reasoning strategies for the specific dataset.
- **Evidence anchors:** [abstract] "...reasoning chains that adapt to the structure of the training data—shifting from risk-averse explanations with human data to expected-value-focused reasoning with synthetic data." [section 5] "RL is capable of adapting its reasoning strategies to match the structure of the training data."
- **Break condition:** If the dataset is noisy or random, the model collapses to nonsensical outputs or default predictions because no consistent reasoning strategy minimizes the loss.

### Mechanism 3
- **Claim:** Outcome-based RL provides a denser learning signal for reasoning alignment than standard SFT, but requires significantly higher model capacity to stabilize.
- **Mechanism:** Unlike SFT, which mimics token sequences, RL directly optimizes the consequence of the reasoning, allowing better generalization but requiring the model to explore valid reasoning paths.
- **Core assumption:** The weaker model failure is due to a lack of reasoning capacity/knowledge rather than just insufficient parameter count for storage.
- **Evidence anchors:** [section 5] "RL applied to the Gemma model performs significantly worse than both SFT... The absence of key cognitive mechanisms... likely contributes to the weaker performance." [section 4] "RL achieves comparable predictive accuracy while generating more interpretable cognitive mechanisms."
- **Break condition:** If the model size falls below the threshold required for the specific cognitive task, SFT may outperform RL because RL requires the model to explore valid reasoning paths, which a weak model cannot do.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** This is the core RL algorithm used. Unlike standard PPO, GRPO evaluates outputs relative to a group of candidates generated for the same prompt. You need to understand this to see why the "advantage function" calculation drives the selection of diverse reasoning paths.
  - **Quick check question:** How does GRPO determine which candidate completion is "better" without a separate value function model?

- **Concept: Chain-of-Thought (CoT) Elicitation**
  - **Why needed here:** The paper treats CoT not just as a tool for better answers, but as the *product* itself (a "verbal theory"). Understanding CoT prompts is necessary to grasp how the authors force the model to externalize internal states that look like cognitive mechanisms.
  - **Quick check question:** Does the model train on the tokens of the reasoning chain directly (SFT), or on the *outcome* of the reasoning chain (RL)?

- **Concept: Cognitive Modeling vs. Behavioral Cloning**
  - **Why needed here:** The paper differentiates "predicting choices" (behavioral cloning/SFT) from "explaining choices" (cognitive modeling). The RL approach aims for the latter. You must understand this distinction to see why the authors accept higher compute costs for interpretable CoTs.
  - **Quick check question:** Why might a model that predicts human choices perfectly (100% accuracy) still fail as a cognitive model?

## Architecture Onboarding

- **Component map:**
  Backbone LLM (Qwen-2.5-7B-Instruct + LoRA) -> Prompting Interface (natural language risky choice descriptions) -> Reward Module (prediction accuracy + JSON format checks) -> GRPO Trainer (updates LoRA weights)

- **Critical path:**
  The most sensitive component is the **Reward Function Design**. Using normalized advantages caused the model to collapse to predicting 50% for everything. The system only worked when normalization was removed, allowing the raw error signal to dictate the update strength based on problem difficulty.

- **Design tradeoffs:**
  - **Interpretability vs. Stability:** You get interpretable "thoughts" (CoT), but the training is unstable (risk of mode collapse) compared to SFT.
  - **Compute vs. Data Efficiency:** RL requires generating multiple candidates per step (G=12 in this paper), making it computationally expensive (approx. 6x longer inference during training) compared to standard SFT, even though it may converge in fewer epochs.

- **Failure signatures:**
  1. **Mode Collapse:** The model outputs a static prediction (e.g., "50/50") and repetitive reasoning for all inputs. (Caused by incorrect advantage normalization or weak backbone).
  2. **Incoherent CoT:** The reasoning traces devolve into multilingual gibberish. (Caused by training on random/noisy data where no valid strategy exists).
  3. **SFT-like Behavior:** The model predicts well but generates trivial explanations. (Caused by using a backbone that is too weak to support the reasoning required by the reward).

- **First 3 experiments:**
  1. **The "Data Control" Replication:** Train the RL agent on the *synthetic* (Expected Value) dataset. Verify that the CoT shifts from "Risk Aversion" talk to "Expected Value" talk. This validates that the CoT is reacting to the data, not just the prompt.
  2. **CoT Swapping Test:** Take a CoT generated by the RL model and feed it to the base model (and vice versa). If the RL-CoT improves the base model's prediction more than the Base-CoT helps the RL model, you have evidence that the RL process generates higher-quality reasoning, not just better final answers.
  3. **Ablate the Format Reward:** Remove the +0.5 bonus for correct JSON formatting. Observe if the model generates better reasoning but fails to output usable predictions, highlighting the need for the mixed reward signal.

## Open Questions the Paper Calls Out

- **Under what conditions does RL-based post-training outperform SFT-based methods for developing interpretable cognitive models, and when does the opposite hold?** The paper only shows comparable accuracy but notes RL significantly underperforms SFT on weaker models, leaving boundary conditions unclear.

- **Can RL post-training enable LLMs to discover genuinely novel cognitive theories, or is it limited to eliciting mechanisms already latent in the pretrained model?** The "elicitation hypothesis" remains untested—current results show adaptation but cannot distinguish between discovery and retrieval.

- **What is the optimal strategy for combining RL and SFT to produce cognitive models that are both accurate and interpretable?** The paper compares methods independently but does not explore hybrid training regimes (e.g., SFT warm-start followed by RL fine-tuning).

- **What minimum backbone LLM capacity is required for RL to successfully elicit meaningful cognitive mechanisms?** Only two model sizes were tested (7B and 2B), leaving the critical capacity threshold for different cognitive mechanisms unspecified.

## Limitations

- **Dependence on Model Capacity:** RL approach shows dramatically degraded performance on smaller models (Gemma-2-2B), raising questions about scalability and practical applicability.

- **Causal Attribution Uncertainty:** The paper demonstrates reasoning adaptation but cannot definitively prove this reflects genuine causal understanding versus statistical pattern matching.

- **Limited Domain Generalizability:** While successful for risky choice tasks, the approach provides limited evidence for effectiveness on other cognitive domains requiring different reasoning types.

## Confidence

**High Confidence:**
- RL achieves comparable predictive accuracy to SFT on risky choice prediction tasks
- RL generates more interpretable reasoning chains than SFT on the same backbone
- The approach works with the specific Qwen-2.5-7B-Instruct backbone when properly implemented

**Medium Confidence:**
- RL elicits genuine cognitive mechanisms rather than surface-level language patterns
- The reasoning chains adapt to the statistical structure of training data rather than just prompt conditioning
- Removing reward normalization is essential for training stability

**Low Confidence:**
- The "elicitation hypothesis" accurately describes how RL interacts with pre-existing knowledge in LLMs
- The approach generalizes to other cognitive domains beyond risky choice tasks
- The method scales effectively to smaller backbone models with appropriate training adjustments

## Next Checks

**Validation Check 1: Cross-Model Reasoning Transfer** - Take CoT outputs from the RL-trained Qwen model and feed them as prompts to both the base Qwen model and other LLMs (GPT-4, Claude). Measure whether the RL-CoT improves prediction accuracy more than base-CoT across models.

**Validation Check 2: Ablation of Format Reward** - Remove the JSON format reward component entirely and train RL models with only outcome-based rewards. Compare prediction accuracy, reasoning quality, and training stability.

**Validation Check 3: Synthetic Data Reasoning Analysis** - Systematically analyze the reasoning chains generated on synthetic expected-value datasets to verify whether they actually implement expected value calculations (showing intermediate arithmetic steps) or merely use expected-value language without computational content.