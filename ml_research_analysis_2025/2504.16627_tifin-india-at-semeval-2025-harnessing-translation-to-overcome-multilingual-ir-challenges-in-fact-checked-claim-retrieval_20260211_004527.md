---
ver: rpa2
title: 'TIFIN India at SemEval-2025: Harnessing Translation to Overcome Multilingual
  IR Challenges in Fact-Checked Claim Retrieval'
arxiv_id: '2504.16627'
source_url: https://arxiv.org/abs/2504.16627
tags:
- retrieval
- performance
- languages
- arxiv
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses multilingual fact-checked claim retrieval for
  combating misinformation across language barriers. The approach uses LLM-based translation
  to convert social media posts into English, followed by embedding-based retrieval
  with hard-negative mining and reranking using Qwen2.5-72B-Instruct.
---

# TIFIN India at SemEval-2025: Harnessing Translation to Overcome Multilingual IR Challenges in Fact-Checked Claim Retrieval

## Quick Facts
- arXiv ID: 2504.16627
- Source URL: https://arxiv.org/abs/2504.16627
- Reference count: 16
- Primary result: Success@10 scores of 0.938 (monolingual) and 0.81025 (crosslingual) on multilingual fact-checked claim retrieval

## Executive Summary
This work addresses the challenge of multilingual fact-checked claim retrieval for combating misinformation across language barriers. The TIFIN India team tackles SemEval-2025 Task 7 by developing a pipeline that translates social media posts into English and retrieves relevant fact-checks from the MultiClaim dataset. Their approach combines LLM-based translation with embedding-based retrieval and reranking, achieving strong performance on both monolingual and crosslingual test sets while maintaining efficiency on consumer GPUs.

## Method Summary
The approach translates social media posts from 27 source languages into English using Aya Expanse 8B, then fine-tunes the Stella 400M embedding model with hard-negative mining to create language-specific representations. The system retrieves top candidates using these fine-tuned embeddings and optionally reranks the top-50 results with Qwen2.5-72B-Instruct. A hybrid search combining fine-tuned Stella embeddings and reciprocal rank fusion produces the final results, achieving success@10 scores of 0.938 on monolingual and 0.81025 on crosslingual test sets.

## Key Results
- Achieved success@10 of 0.938 on monolingual test set and 0.81025 on crosslingual test set
- Translation combined with embedding model fine-tuning provides most performance gains
- LLM reranking offers minimal additional benefit, with some rerankers degrading performance
- Pipeline runs efficiently on consumer GPUs (NVIDIA RTX 4090) with translation taking 61 minutes and fine-tuning requiring 35 minutes

## Why This Works (Mechanism)
The system works by first standardizing the multilingual input space through translation to English, which allows a single embedding model (Stella 400M) to handle all languages effectively. Fine-tuning with hard-negative mining improves the embedding model's ability to distinguish relevant from irrelevant fact-checks, while the hybrid retrieval approach combines multiple ranking strategies to maximize recall and precision. The approach demonstrates that machine translation, when combined with proper embedding fine-tuning, can effectively bridge language barriers in information retrieval tasks.

## Foundational Learning
- **Machine translation for IR** - Why needed: Converts multilingual queries to a single language for consistent processing. Quick check: Verify translation quality on sample social media posts.
- **Hard-negative mining** - Why needed: Improves embedding model discrimination by exposing it to challenging negative examples. Quick check: Monitor training loss and retrieval precision during fine-tuning.
- **Embedding-based retrieval** - Why needed: Efficient similarity search for large-scale fact-check databases. Quick check: Validate embedding dimensionality and similarity computation.
- **LLM reranking** - Why needed: Refines initial retrieval results using deeper semantic understanding. Quick check: Compare reranked vs. base retrieval performance.
- **Reciprocal Rank Fusion** - Why needed: Combines multiple ranking strategies to improve overall performance. Quick check: Test RRF parameter settings (k=60) on validation set.
- **Crosslingual retrieval** - Why needed: Handles queries and documents in different languages. Quick check: Evaluate performance gap between monolingual and crosslingual scenarios.

## Architecture Onboarding

**Component Map**: Social Media Posts → Aya Expanse 8B Translation → Stella 400M Fine-tuning → Embedding Retrieval → Qwen2.5-72B-Instruct Reranking → Reciprocal Rank Fusion → Fact-Check Retrieval

**Critical Path**: The critical path for inference is translation → embedding retrieval → reranking → fusion. Translation and fine-tuning are preprocessing steps that enable the core retrieval pipeline.

**Design Tradeoffs**: The system trades off translation accuracy for processing efficiency by using a single English embedding model rather than multilingual embeddings. The minimal benefit from reranking suggests the embedding model is already highly effective, making the computationally expensive reranking step optional.

**Failure Signatures**: Reranking degrades performance in most cases (Table 4), indicating that the base embedding model is already optimized. Translation quality issues with noisy social media text can significantly impact downstream retrieval performance.

**Three First Experiments**:
1. Test translation quality impact by comparing retrieval performance using original non-English posts versus machine-translated versions on the development set
2. Implement hard-negative mining with different similarity thresholds and mining frequencies to optimize the Stella 400M fine-tuning process
3. Reproduce the ablation study showing reranking degradation by implementing the pipeline with and without Qwen2.5-72B-Instruct reranking on a subset of the development set

## Open Questions the Paper Calls Out
None

## Limitations
- Translation quality with Aya Expanse 8B on noisy social media text across 27 languages represents a significant bottleneck
- Several critical implementation details remain underspecified, including exact training hyperparameters for Stella fine-tuning
- The hard-negative mining strategy's configuration (frequency, similarity thresholds) is unclear
- Qwen2.5-72B-Instruct reranking shows minimal impact in experiments, suggesting potential optimization opportunities or implementation issues

## Confidence
- **High confidence**: Translation-to-English followed by embedding-based retrieval achieves strong results (0.938 monolingual, 0.81025 crosslingual)
- **Medium confidence**: Stella 400M fine-tuning with hard-negative mining provides performance gains; exact hyperparameters uncertain
- **Medium confidence**: LLM reranking offers minimal benefit; implementation details unclear
- **Low confidence**: Translation quality handling for noisy social media text across 27 languages

## Next Checks
1. Reproduce the ablation study showing reranking degradation by implementing the pipeline with and without Qwen2.5-72B-Instruct reranking on a subset of the development set
2. Test translation quality impact by comparing retrieval performance using original non-English posts versus machine-translated versions on the development set
3. Implement hard-negative mining with different similarity thresholds and mining frequencies to optimize the Stella 400M fine-tuning process, measuring impact on success@10 scores