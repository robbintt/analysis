---
ver: rpa2
title: 'Potent but Stealthy: Rethink Profile Pollution against Sequential Recommendation
  via Bi-level Constrained Reinforcement Paradigm'
arxiv_id: '2511.09392'
source_url: https://arxiv.org/abs/2511.09392
tags:
- attack
- reward
- policy
- sequences
- sequential
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces CREAT, a constrained reinforcement learning
  framework designed to conduct stealthy profile pollution attacks against sequential
  recommendation systems. The key innovation lies in addressing two limitations of
  existing attacks: low attack intensity due to sequence-level perturbations and high
  detectability from distributional shifts.'
---

# Potent but Stealthy: Rethink Profile Pollution against Sequential Recommendation via Bi-level Constrained Reinforcement Paradigm

## Quick Facts
- arXiv ID: 2511.09392
- Source URL: https://arxiv.org/abs/2511.09392
- Reference count: 40
- Primary result: Achieves up to ten times greater target item exposure compared to pure baselines while maintaining superior stealthiness

## Executive Summary
This paper introduces CREAT, a constrained reinforcement learning framework designed to conduct stealthy profile pollution attacks against sequential recommendation systems. The key innovation lies in addressing two limitations of existing attacks: low attack intensity due to sequence-level perturbations and high detectability from distributional shifts. CREAT reformulates profile pollution into a bi-level optimization problem, leveraging pattern inversion rewards to identify critical sequential patterns and distribution consistency rewards to minimize detectable shifts via unbalanced co-optimal transport. Experimental results on three real-world datasets demonstrate significantly higher attack effectiveness while maintaining stealthiness.

## Method Summary
CREAT implements profile pollution attacks through a two-stage reinforcement learning optimization. First, a perturbation masker learns to select positions for replacement with target items using pattern inversion rewards that maximize directionality and diversity. Second, a constrained group relative reinforcement learning paradigm fine-tunes the masker while enforcing distribution consistency via dynamic barrier constraints and unbalanced co-optimal transport. The method employs group-shared experience replay to enable step-wise, self-reflective perturbations that balance attack efficacy against stealthiness.

## Key Results
- Achieves up to ten times greater target item exposure (HR@10) compared to pure baselines
- Maintains superior stealthiness by causing less performance degradation to recommendation accuracy
- Better evades detection methods while successfully manipulating sequential recommenders

## Why This Works (Mechanism)

### Mechanism 1: Bi-Level Optimization Reformulation
Reformulating profile pollution as a constrained optimization problem separates attack efficacy from stealthiness, enabling independent control of competing objectives. The upper-level objective maximizes pattern inversion rewards while the lower-level constraint bounds distributional deviation via a dynamic threshold ρ_st, creating a Lagrangian formulation L(ψ,δ) that adaptively penalizes violations.

### Mechanism 2: Pattern Inversion via Directionality and Diversity Rewards
Targeting fine-grained sequential patterns rather than whole sequences amplifies attack impact through spurious attributional paths while requiring fewer perturbations. The directionality reward maximizes semantic distance between the target item and both predecessor and successor contexts, while the diversity reward ensures inverted patterns span the representation space.

### Mechanism 3: Distribution Consistency via Dual-Level Co-Optimal Transport
Constraining both sequence-level and pattern-level distributional shifts using unbalanced optimal transport enables perturbations that evade detection while maintaining semantic coherence. DLOT jointly optimizes two transport plans with KL-divergence penalties for marginal relaxation.

### Mechanism 4: Dynamic Barrier Constraint Adaptation
Adaptively adjusting the penalty coefficient δ based on real-time constraint violations and gradient alignment prevents oscillation between attack and stealth objectives. The barrier coefficient increases when constraints are violated or gradients align, and decreases when gradients conflict.

## Foundational Learning

- **Reinforcement Learning with Policy Gradients**: Needed to formulate perturbation selection as a sequential decision process. Quick check: Can you derive the policy gradient and explain how advantage estimation reduces variance?
- **Optimal Transport Theory**: Required to understand the distribution consistency reward using unbalanced co-optimal transport. Quick check: What is the difference between balanced and unbalanced optimal transport?
- **Bi-Level Optimization**: Essential for understanding the separation of attack maximization from stealth constraints. Quick check: How does the Lagrangian formulation convert a constrained problem into an unconstrained saddle-point problem?

## Architecture Onboarding

- **Component map**: Perturbation Masker M_ψ -> Representation Encoder φ_rec -> Pattern Balanced Rewarding Policy (PBRP) -> Constrained Group Relative RL (C-GRRL)
- **Critical path**: Initialize masker, sample training sequences, Stage 1 localization with inversion rewards, Stage 2 constrained optimization with dynamic barrier, update ψ using gradient alignment
- **Design tradeoffs**: Group size G vs. memory/computation, perturbation budget K vs. detection risk, stealth threshold λ_st vs. attack effectiveness, KL regularization λ_j in DLOT
- **Failure signatures**: Convergence to zero attack effectiveness, high detection rate, unstable training with reward oscillation, poor tail-item attack
- **First 3 experiments**: Ablation on reward components (w/o dir, w/o div, w/o dist), dynamic barrier vs. static penalty comparison, GRPO vs. REINFORCE convergence analysis

## Open Questions the Paper Calls Out

### Open Question 1
How can adaptive defenses be developed to detect and mitigate pattern-horizon attacks like CREAT without compromising recommendation accuracy? The authors state in the Conclusion: "In the future, we will extend adaptive defenses against such attacks for recommender safety." This remains unresolved because CREAT specifically minimizes distributional shifts to evade existing detectors.

### Open Question 2
Can the pattern inversion mechanism effectively manipulate emerging generative recommenders, such as Diffusion-based or Large Language Model (LLM)-enhanced Sequential Recommendation systems? The Empirical Study limits evaluation to NARM and BERT4Rec, but generative architectures process sequential dependencies differently.

### Open Question 3
To what extent does CREAT's effectiveness rely on the fidelity of the surrogate model in strict black-box settings? The Problem Formulation assumes the attacker knows the architecture or can extract a surrogate, but real-world extraction often introduces discrepancies.

## Limitations

- DLOT-based distribution consistency mechanism introduces substantial computational overhead with underspecified hyperparameters
- Dynamic barrier constraint relies on gradient alignment assumptions that may not hold in non-convex RL landscapes
- Evaluation focuses on synthetic attack scenarios rather than real-world black-box attack conditions

## Confidence

- **High confidence**: Bi-level optimization framework separation of attack and stealth objectives is well-justified
- **Medium confidence**: Pattern inversion mechanism's effectiveness depends on SR models relying on local sequential patterns
- **Low confidence**: Dynamic barrier constraint's superiority over static penalties lacks theoretical justification

## Next Checks

1. **DLOT hyperparameter sensitivity**: Systematically vary KL regularization weights and entropic regularization to identify stable parameter ranges
2. **Black-box attack evaluation**: Test CREAT against commercial or black-box SR systems where target items are unknown
3. **Real-time adaptation**: Implement online detection response where the attacker must adapt to observed detection signals