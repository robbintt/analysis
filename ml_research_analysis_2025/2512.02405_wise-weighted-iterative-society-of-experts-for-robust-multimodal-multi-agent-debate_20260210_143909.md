---
ver: rpa2
title: 'WISE: Weighted Iterative Society-of-Experts for Robust Multimodal Multi-Agent
  Debate'
arxiv_id: '2512.02405'
source_url: https://arxiv.org/abs/2512.02405
tags:
- problem
- answer
- wise
- solution
- debate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies multi-agent debate (MAD) for vision-and-language
  reasoning. It introduces WISE, a framework that partitions heterogeneous agents
  into Solvers (multimodal) and Reflectors (unimodal or multimodal), coordinated by
  an orchestrator.
---

# WISE: Weighted Iterative Society-of-Experts for Robust Multimodal Multi-Agent Debate

## Quick Facts
- arXiv ID: 2512.02405
- Source URL: https://arxiv.org/abs/2512.02405
- Authors: Anoop Cherian; River Doyle; Eyal Ben-Dov; Suhas Lohit; Kuan-Chuan Peng
- Reference count: 40
- Primary result: WISE improves accuracy by 2–7% over SOTA MAD setups across SMART-840, VisualPuzzles, EvoChart-QA, and SMART-840++.

## Executive Summary
This paper introduces WISE, a framework for multi-agent debate in multimodal reasoning tasks. WISE partitions agents into Solvers (multimodal) and Reflectors (unimodal or multimodal), coordinated by an orchestrator LLM. The iterative debate process refines answers through weighted feedback, and a modified Dawid–Skene (WISE-DS) algorithm aggregates responses while modeling agent error probabilities. Evaluated on four multimodal datasets, WISE consistently improves accuracy by 2–7% over state-of-the-art MAD setups and aggregation methods.

## Method Summary
WISE implements a multi-agent debate framework where heterogeneous agents are partitioned into Solvers (multimodal) and Reflectors (unimodal or multimodal). Solvers generate solutions, Reflectors evaluate them with weights (−1, 0, 1, 2) and feedback, and an orchestrator synthesizes feedback and generates challenge questions. The process iterates up to 8 rounds with early stopping on consensus. WISE-DS, a modified Dawid–Skene EM algorithm, jointly models solver and reflector error matrices to compute posteriors and select final answers. The framework was evaluated on SMART-840 (840 Math Olympiad problems), VisualPuzzles (1168), EvoChart-QA (320), and SMART-840++ (825 programmatically generated instances).

## Key Results
- WISE achieves 68.1% accuracy on SMART-840, improving over state-of-the-art MAD by 2–7%.
- Role partitioning into Solvers and Reflectors consistently outperforms homogeneous debate setups.
- WISE-DS aggregation outperforms majority voting, original Dawid-Skene, and MACE across all datasets.
- Iterative refinement with orchestrator-generated questions improves accuracy by 3–4% compared to passive feedback.

## Why This Works (Mechanism)

### Mechanism 1: Role Partitioning Separates Generation from Critique
Partitioning agents into Solvers and Reflectors improves debate efficiency and effectiveness compared to homogeneous debate setups. Solvers generate solutions while Reflectors evaluate correctness, assign weights (−1, 0, 1, 2), and provide textual feedback. This separation allows agents to specialize—some models excel at solving but not critiquing, or vice versa—reducing the quadratic message complexity of all-to-all debate.

### Mechanism 2: Orchestrator Synthesizes Actionable Feedback Questions
An orchestrator LLM that summarizes feedback and generates targeted follow-up questions drives iterative refinement more effectively than passive feedback aggregation. The orchestrator receives feedback matrix F and weight matrix W from reflectors, then produces consolidated feedback with "challenge questions" that force solvers to re-examine specific multimodal aspects.

### Mechanism 3: WISE-Dawid-Skene Jointly Models Solver and Reflector Error
Modeling both solver answer errors and reflector weight errors within a unified EM framework improves final answer selection over majority or weighted voting. WISE-DS extends classic Dawid-Skene by treating solver answers and reflector weights as jointly observed data, estimating separate confusion matrices (P^t for solvers, P^c for reflectors) and priors, then computing posteriors to select the highest-weighted answer.

## Foundational Learning

- **Multi-Agent Debate (MAD) Fundamentals**
  - Why needed: WISE extends MAD from language-only to multimodal settings; understanding debate protocols, message passing, and consensus mechanisms is prerequisite.
  - Quick check: Can you explain why majority voting fails when agents have systematically correlated errors?

- **Expectation-Maximization for Crowdsourcing**
  - Why needed: WISE-DS adapts the Dawid-Skene EM algorithm to estimate agent error matrices and posterior answer probabilities.
  - Quick check: Given observed agent responses, how would you compute the E-step posterior over true labels if error matrices were known?

- **Vision-Language Reasoning Challenges**
  - Why needed: MLLMs lag behind LLMs on multimodal benchmarks; understanding failure modes (hallucination, weak visual grounding) contextualizes why MAD helps.
  - Quick check: Why might a language-only reflector still provide useful feedback on a visual problem?

## Architecture Onboarding

- **Component map:**
  - Problem + Image -> Solvers (n multimodal agents) -> Initial responses
  - Solvers responses + Problem -> Reflectors (m unimodal/multimodal agents) -> Weight matrix W and Feedback matrix F
  - F + W -> Orchestrator (1 LLM) -> Consolidated feedback + Challenge questions
  - Challenge questions + Problem + Previous responses -> Solvers -> Refined responses
  - All responses + All weights -> WISE-DS Aggregator -> Final answer with posterior confidence

- **Critical path:**
  1. Initialize: Solvers receive problem → generate initial responses.
  2. Evaluate: Each reflector scores each solver response → produce F and W matrices.
  3. Check consensus: Orchestrator evaluates if all solvers agree and reflectors confirm correctness.
  4. Iterate: If no consensus, orchestrator synthesizes feedback → solvers refine → repeat.
  5. Aggregate: After K rounds or consensus, WISE-DS computes posteriors and selects final answer.

- **Design tradeoffs:**
  - More rounds improve accuracy but increase API costs (K(M+N+MN) calls worst-case).
  - Heterogeneous agent pools increase diversity but require careful role assignment; Table 8 shows mixed results for strong-weak pairings.
  - Weighted aggregation with WISE-DS adds computational overhead vs. simple majority voting.

- **Failure signatures:**
  - **Convergence failure**: Debate reaches max rounds without consensus; check if reflector weights are consistently low (agents uncertain) or feedback is contradictory.
  - **Error matrix degeneracy**: WISE-DS posteriors flat or uniform; may indicate too few problems or highly correlated agents.
  - **Token limit overflow**: Long debates exceed model context; Figure 6 shows WISE is robust to token limits, but individual models may truncate.

- **First 3 experiments:**
  1. Replicate Table 1 (SMART-840) with a small heterogeneous agent pool (e.g., 2 solvers, 2 reflectors) to verify role partitioning benefits over homogeneous debate.
  2. Ablate orchestrator question generation (replicate Table 9) to confirm 3-4% accuracy drop without targeted feedback.
  3. Compare WISE-DS against weighted majority voting on a subset of problems to validate EM-based aggregation gains; inspect error matrices for strongest/weakest agents.

## Open Questions the Paper Calls Out

### Open Question 1
How robust is the WISE-Dawid–Skene aggregation algorithm when the assumption of conditional independence is violated among agents with shared pre-training data? The paper demonstrates improved accuracy but does not analyze if correlations between specific models degrade the EM convergence of WISE-DS.

### Open Question 2
Can the quadratic communication overhead of the WISE debate protocol be reduced via pruning or early-exit strategies without sacrificing the observed accuracy gains? While average rounds are low, the worst-case cost remains high, and the trade-off between marginal accuracy improvements and token cost per round is not quantified.

### Open Question 3
Why does a heterogeneous "mix-mix" configuration of Solvers and Reflectors occasionally outperform a configuration composed solely of strong models? Table 8 shows a "Mix Mix" setup achieving 68.1% accuracy, surpassing the "Strong Strong" baseline of 67.7% on SMART-840.

## Limitations
- WISE-DS relies on conditional independence of solver and reflector responses; no empirical validation of correlation structure among heterogeneous agents is provided.
- The method's effectiveness on tasks requiring generation rather than selection remains untested at scale, despite promising results on SMART-840++.
- 2–7% gains over SOTA come with increased API calls (K(M+N+MN) per problem); no cost-benefit analysis or efficiency benchmarks are provided.

## Confidence
- **High confidence** in role partitioning: Clear quantitative improvements (Table 1) and ablation (Table 9) support the benefit of separating solvers from reflectors.
- **Medium confidence** in orchestrator question generation: Ablation shows 3–4% drop without questions, but no comparison to alternative feedback mechanisms is provided.
- **Medium confidence** in WISE-DS aggregation: Consistently outperforms baselines on accuracy, but lacks ablation of EM convergence thresholds or comparison to simpler weighted voting under controlled error conditions.

## Next Checks
1. Measure pairwise response correlations across solvers and reflectors on SMART-840 to empirically test WISE-DS independence assumptions; inspect error matrix conditioning.
2. Replace GPT-4o orchestrator with a simpler summarizer (no question generation) and compare against a setup where reflectors generate questions; quantify relative contribution of orchestration quality.
3. Benchmark API call counts and wall-clock time for WISE vs. homogeneous MAD as K increases; compute accuracy gain per dollar to assess practical viability.