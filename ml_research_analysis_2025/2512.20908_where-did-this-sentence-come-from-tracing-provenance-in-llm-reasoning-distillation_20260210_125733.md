---
ver: rpa2
title: Where Did This Sentence Come From? Tracing Provenance in LLM Reasoning Distillation
arxiv_id: '2512.20908'
source_url: https://arxiv.org/abs/2512.20908
tags:
- teacher
- reasoning
- student
- distillation
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the question of whether reasoning distillation
  actually transfers a teacher model''s capabilities to a student, or if the student
  simply reverts to its own patterns when facing new contexts. To tackle this, the
  authors introduce a provenance tracing framework that categorizes each action produced
  by the distilled model into four types: teacher-originated, student-originated,
  shared, or boosted.'
---

# Where Did This Sentence Come From? Tracing Provenance in LLM Reasoning Distillation

## Quick Facts
- **arXiv ID:** 2512.20908
- **Source URL:** https://arxiv.org/abs/2512.20908
- **Reference count:** 32
- **Primary result:** Provenance tracing framework categorizes distilled model actions as teacher-originated, student-originated, shared, or boosted, enabling teacher-guided data selection that improves reasoning distillation performance by 1.7%-2.5% over baselines.

## Executive Summary
This work addresses whether reasoning distillation transfers a teacher model's capabilities to a student or if the student simply reverts to its own patterns when facing new contexts. The authors introduce a provenance tracing framework that categorizes each action produced by the distilled model into four types: teacher-originated, student-originated, shared, or boosted. They find that distilled models can indeed produce teacher-originated actions even in novel test settings, and these actions correlate with correct answers. Based on this, they propose a teacher-guided data selection method that prioritizes training samples with more teacher-originated actions. Experiments across multiple teacher-student pairs show that this approach improves performance over prior heuristics by 1.7%-2.5% on reasoning benchmarks.

## Method Summary
The provenance framework works by comparing probability distributions from three models (teacher M_T, original student M_S, and distilled M_D) for each action in the distilled model's output. Sentence-level probabilities are computed as the geometric mean of token-level probabilities. Actions are classified into four categories using thresholds α (filtering negligible differences) and β (separating action types): Teacher Sentence when Δ_TS > β, Student Sentence when -Δ_TS > β, Shared when all differences ≤ α, and Boosted otherwise. For data selection, responses with the maximum absolute count of Teacher Sentences are prioritized, targeting samples where teacher-student divergence is highest. The approach is validated through experiments showing improved reasoning performance across multiple teacher-student pairs.

## Key Results
- Distilled models produce teacher-originated actions even in novel test settings, and these actions correlate with correct answers
- Teacher-guided data selection improves performance by 1.7%-2.5% over baselines across four teacher-student pairs
- "Maximize Absolute Count" selection method outperforms "Minimize Absolute Count" (58.7 vs 55.0 average) and other heuristics
- Teacher-originated actions appear more frequently in early reasoning steps and correlate with answer correctness

## Why This Works (Mechanism)

### Mechanism 1: Probabilistic Provenance Tracing via Cross-Model Comparison
- Claim: Actions in a distilled model's output can be attributed to specific origins by comparing probability assignments across three models under identical context.
- Mechanism: For each action (sentence) produced by distilled model M_D, compute the geometric mean of per-token probabilities from teacher M_T, original student M_S, and M_D. Classify via thresholds α (filtering negligible differences) and β (separating action types): Teacher Sentence when Δ_TS > β (teacher assigns higher probability), Student Sentence when -Δ_TS > β, Shared when all differences ≤ α, Boosted otherwise.
- Core assumption: Sentence-level probability differences reflect genuine behavioral origins rather than tokenization artifacts or vocabulary mismatches between models.
- Evidence anchors:
  - [abstract] "For each action (e.g., a sentence) produced by the distilled model, we obtain the predictive probabilities assigned by the teacher, the original student, and the distilled model under the same context."
  - [Section 3.2] Defines p(i,j) = exp(mean(log(p_k))) and classification rules using Δ_SD, Δ_TD, Δ_TS
  - [corpus] Weak direct evidence; related work on distillation detection (arXiv:2510.02302) addresses model provenance but not sentence-level attribution
- Break condition: If teacher and student models share near-identical output distributions on certain action types (high overlap in classification histograms), α/β thresholding becomes unreliable and provenance attribution degrades.

### Mechanism 2: Teacher-Guided Data Selection Maximizing Divergence
- Claim: Prioritizing training samples with higher counts of teacher-originated actions improves downstream reasoning performance compared to student-aligned or random selection.
- Mechanism: Pre-training, classify candidate responses using only M_T and M_S (Common/Teacher/Student). For each question with multiple teacher responses, select the response with maximum absolute count of Teacher Sentences. This targets samples where teacher-student divergence is highest.
- Core assumption: Higher teacher-originated action density in training data causally increases the distilled model's probability of producing teacher-originated actions at test time.
- Evidence anchors:
  - [Section 4.1] "Our method provides a clearer objective for data selection: prioritizing samples where the teacher and student models differ the most."
  - [Table 1] Shows 1.7%-2.5% average improvement over baselines across four teacher-student pairs
  - [Table 2] "Maximize Absolute Count yields the best performance, whereas Minimize Absolute Count performs the worst"
  - [corpus] GRAPE (Zhang et al., 2025, cited in paper) uses student-aligned selection; this work inverts that criterion
- Break condition: If training data domain differs substantially from test domain, teacher-originated actions may not transfer (paper acknowledges limited evidence for larger models in Section 3.3).

### Mechanism 3: Early-Stage Teacher Alignment Correlates with Correctness
- Claim: Teacher-originated actions appear more frequently in early reasoning steps and are positively correlated with answer correctness.
- Mechanism: Analysis on AIME24/GPQA-D shows higher Teacher Sentence probability in early action indices. Correct responses show elevated Teacher Sentence probability vs. incorrect responses (light-green solid line above dashed line in Figures 3-4). Early actions often involve input analysis and planning—behaviors hypothesized to be teacher-specific patterns.
- Core assumption: Correlation between teacher-originated actions and correctness reflects causal transfer of beneficial reasoning patterns, not mere distribution matching.
- Evidence anchors:
  - [Section 3.3, Observation 1] "Higher Teacher Sentence probability in the early inference stage"
  - [Section 3.3, Observation 3] "Teacher Sentence tends to be assigned higher probabilities to correct answers"
  - [Figure 3, 4] Visual evidence of probability patterns across action positions
  - [corpus] No direct external validation; this is an internal analysis framework
- Break condition: If student model already has strong reasoning capabilities, boosting student-internal patterns (Boosted Sentences) may outperform teacher alignment—paper notes smaller models show opposite trend where Boosted Sentences correlate with incorrect answers.

## Foundational Learning

- Concept: **Knowledge Distillation Basics** (teacher-student training, soft labels vs. hard labels, cross-entropy loss on teacher outputs)
  - Why needed here: The entire framework assumes understanding of how distillation transfers behavior from M_T to M_S via trajectory imitation.
  - Quick check question: Given a teacher probability distribution [0.7, 0.2, 0.1] and student [0.4, 0.4, 0.2] for a token, what does KL divergence measure?

- Concept: **Autoregressive Language Model Probability** (next-token prediction, log-probability aggregation, geometric mean for sequence scoring)
  - Why needed here: The provenance method computes sentence-level probabilities from token-level logits; understanding how p(i,j) = exp(mean(log(p_k))) aggregates information is essential.
  - Quick check question: Why use geometric mean rather than arithmetic mean for combining token probabilities?

- Concept: **Chain-of-Thought Reasoning Evaluation** (AIME, GPQA, MATH500 benchmarks; sampling multiple completions; accuracy metrics)
  - Why needed here: The paper's empirical validation relies on reasoning benchmarks where correctness can be objectively measured.
  - Quick check question: On AIME24 with 16 completions per question, how would you aggregate accuracy across completions?

## Architecture Onboarding

- Component map:
  1. **Inference Sampler**: Generates trajectories τ_i from M_D on test questions (multiple completions per question)
  2. **Sentence Segmenter**: Splits trajectories into actions using regex pattern + special token handling
  3. **Probability Extractor**: Forward-passes each (context, action) pair through M_T, M_S, M_D to collect p_T, p_S, p_D
  4. **Provenance Classifier**: Applies α/β thresholds to label each action as Teacher/Student/Shared/Boosted
  5. **Data Selector**: For training, ranks candidate responses by Teacher Sentence count; selects top-ranked per question
  6. **Training Loop**: Standard SFT on selected (question, response) pairs

- Critical path: Probability Extractor → Provenance Classifier → (for training) Data Selector → Training Loop. Forward passes for probability extraction are the compute bottleneck (single pass per action vs. autoregressive generation).

- Design tradeoffs:
  - Sentence-level vs. token-level classification: Coarser granularity reduces vocabulary mismatch issues but may miss fine-grained transfer patterns.
  - α threshold: Higher values filter more noise but risk misclassifying genuine small-difference origins. Paper uses α=0.1 via human annotation.
  - β selection: Adaptive histogram-based selection (Algorithm 1) vs. fixed threshold. Adaptive adds preprocessing cost but improves separation.
  - Response count per question: More candidates improve selection quality but increase teacher inference cost.

- Failure signatures:
  - High overlap in Teacher/Boosted Sentence histograms indicates β is poorly chosen (Figure 8 shows β=0.2 creates overlap vs. β=0.15 separation)
  - If Teacher Sentence probability doesn't exceed Student Sentence for correct answers, the core hypothesis fails (check Figure 3 early positions)
  - If "Minimize Absolute Count" performs comparably to "Maximize," the selection mechanism isn't working (Table 2 shows 55.0 vs 58.7 average—clear separation)

- First 3 experiments:
  1. **Reproduce provenance analysis**: Take an open distilled model (e.g., DeepSeek-Distill-Qwen-7B), sample 50 responses on AIME24, extract sentence probabilities from teacher/student/distilled models, plot action-type proportions vs. position. Verify Teacher Sentence elevation in correct answers.
  2. **Ablate β selection**: On a held-out validation set, test β ∈ {0.05, 0.1, 0.15, 0.2}. Measure histogram overlap (Algorithm 1 metric) and downstream accuracy correlation. Confirm paper's claim that pre-training β optimization transfers to post-training performance.
  3. **Cross-domain transfer test**: Train using teacher-guided selection on math data (AceReason), evaluate on science benchmarks (GPQA-D). Compare vs. training on science data (OpenScienceReasoning2). Paper's Table 4 shows science→math transfer works; verify reverse direction.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do teacher-originated actions remain the primary driver of correctness in student models larger than 8B parameters?
- **Basis in paper:** [explicit] The authors state in Section 3.3, "It remains an open question whether Teacher Sentence is equally beneficial for larger models," noting that resource constraints limited experiments to models of 8B parameters or fewer.
- **Why unresolved:** Larger models may rely more on "boosted" internal patterns (as hinted by LIMO-v2 results) rather than direct teacher imitation compared to smaller models.
- **What evidence would resolve it:** Experiments applying the provenance framework to student models in the 30B–70B range to measure the correlation between Teacher Sentences and answer accuracy.

### Open Question 2
- **Question:** What distinguishes beneficial "Boosted Sentences" from harmful ones across different student model capacities?
- **Basis in paper:** [inferred] Section 3.3 notes that while large models (LIMO-v2) benefit from Boosted Sentences, in smaller models (e.g., 7B), these patterns "consistently appear with higher probability in incorrect responses."
- **Why unresolved:** The paper observes the correlation but does not determine the semantic or structural differences causing this divergence in utility based on model size.
- **What evidence would resolve it:** A qualitative analysis of the specific reasoning behaviors (e.g., reflection vs. hallucination) found in Boosted Sentences for small versus large models.

### Open Question 3
- **Question:** How can the provenance framework be extended to determine the optimal data selection when distilling from an ensemble of heterogeneous teacher models?
- **Basis in paper:** [explicit] Appendix A.1 lists future work to "investigate principled approaches to data selection when training with ensembles of diverse teacher models."
- **Why unresolved:** The current method assumes a single teacher; multi-teacher setups introduce conflicting signals that complicate provenance attribution.
- **What evidence would resolve it:** A modified framework that traces provenance across multiple teachers simultaneously and evaluates the performance of students trained on resulting mixture distributions.

## Limitations
- The provenance classification framework relies on probability-based attribution that assumes token-level probability differences genuinely reflect behavioral origins rather than artifacts of vocabulary overlap, tokenization differences, or model calibration.
- The claim that teacher-originated actions causally improve reasoning performance versus merely correlating with it remains unproven—the framework cannot distinguish whether these actions are beneficial reasoning patterns or artifacts of training distribution overlap.
- The method's effectiveness for larger models (>8B parameters) is uncertain, as resource constraints limited experiments to smaller models, and larger models may rely more on "boosted" internal patterns.

## Confidence
- **High**: The provenance tracing framework correctly identifies teacher-originated actions exist in distilled model outputs (Section 3.1 observations are directly observable via probability comparisons)
- **Medium**: The correlation between teacher-originated actions and correct answers (Section 3.3 Observations 1-3) is statistically supported but causal interpretation requires further validation
- **Low**: The claim that teacher-guided data selection causally improves performance via increased teacher-originated action transfer (Section 4.1) lacks mechanistic proof beyond correlation with selection criteria

## Next Checks
1. **Mechanism Validation**: Train distilled models using responses selected by different provenance-based criteria (Maximize Teacher Sentences, Maximize Student Sentences, Random) and measure whether Teacher Sentence probability in test outputs actually differs proportionally across conditions, confirming causal transfer rather than selection artifact.

2. **Threshold Robustness**: Systematically vary α ∈ [0.05, 0.15] and β ∈ [0.1, 0.2] to measure impact on classification stability and downstream performance. Verify that the adaptive β selection algorithm consistently finds optimal thresholds across different model pairs and datasets.

3. **Domain Transfer Isolation**: Test whether teacher-guided selection improves performance when training and test domains differ substantially (e.g., train on math data, test on science benchmarks). This isolates whether provenance-based selection transfers beneficial patterns or merely matches training distribution.