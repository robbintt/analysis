---
ver: rpa2
title: Gender Bias in Emotion Recognition by Large Language Models
arxiv_id: '2511.19785'
source_url: https://arxiv.org/abs/2511.19785
tags:
- emotion
- gender
- llms
- bias
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines gender bias in emotion recognition by large
  language models (LLMs). It investigates whether LLMs exhibit gender biases when
  asked to infer emotions from image captions, using a dataset with gender-swapped
  and gender-neutral versions of the same descriptions.
---

# Gender Bias in Emotion Recognition by Large Language Models

## Quick Facts
- **arXiv ID:** 2511.19785
- **Source URL:** https://arxiv.org/abs/2511.19785
- **Reference count:** 13
- **Primary result:** Fine-tuning effectively reduces gender bias in LLM emotion recognition, while prompt-based approaches fail

## Executive Summary
This paper investigates gender bias in emotion recognition by large language models when inferring emotions from image captions. The authors test whether LLMs exhibit gender biases by using a dataset with gender-swapped and gender-neutral versions of the same descriptions. Multiple LLMs were evaluated, revealing significant gender biases in some models. The study tests debiasing strategies including prompt engineering and fine-tuning. While inference-time prompt-based approaches proved ineffective, fine-tuning methods significantly mitigated gender-related patterns in emotion predictions. The findings emphasize the importance of training-based interventions for achieving fairer emotion recognition in LLMs.

## Method Summary
The study uses 1,000 NarraCap captions from the EMOTIC validation set, each expanded to three versions: original gender, swapped gender, and undefined gender. Authors perform zero-shot evaluation across six LLMs, then apply debiasing to Mistral-7B-Instruct-v0.3 using QLoRA (r=16, lora_alpha=8, targeting attention projections). Two fine-tuning approaches are tested: FT1 using emotion labels only, and FT2 using gender desensitization plus emotion labels with KL divergence regularization to enforce equal gender probability. Bias is measured using chi-square (χ²) tests comparing emotion distributions across gender variants, with p<0.05 indicating significant bias.

## Key Results
- Fine-tuning methods significantly mitigated gender-related patterns in emotion predictions
- Inference-time prompt-based approaches did not effectively reduce bias
- In-context learning actually increased bias for some emotions (p≤0.05 for 5 categories)
- Fine-tuned models stopped predicting "anger" entirely

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning with counterfactual data augmentation decouples gender tokens from emotion predictions. By presenting the model with identical scenarios where only gender is swapped or neutralized while enforcing consistent emotion labels via loss minimization, the model learns to down-weight the predictive power of gender-specific tokens. Core assumption: The model can unlearn spurious correlations via supervised fine-tuning on a relatively small dataset (100 samples expanded to 300).

### Mechanism 2
Inference-time prompt strategies fail because pre-training priors dominate output logic. Explicit instructions to "disregard bias" or limited in-context examples are insufficient to override deep, implicit associations between gender and emotion formed during massive pre-training. Core assumption: The model's internal representation of "gender" is deeply entangled with its representation of "emotion" in a way that surface-level prompting cannot disentangle.

### Mechanism 3
Regularizing gender probability via KL divergence forces a uniform prior. The "FT2" method explicitly penalizes the model for preferring one gender token over another in the context of specific emotions, mathematically forcing a 50:50 probability distribution before emotion classification. Core assumption: Enforcing uncertainty on the gender dimension prevents the model from using gender as a feature for emotion inference.

## Foundational Learning

- **Concept: Counterfactual Data Augmentation**
  - Why needed: This is the core intervention for teaching the model fairness by creating parallel datasets (swapping man/woman)
  - Quick check: If a caption says "The man looked sad," what is the counterfactual version required for the fine-tuning set?

- **Concept: LoRA / QLoRA**
  - Why needed: The authors use Low-Rank Adaptation (LoRA) to modify the model efficiently by targeting only a small subset of weights
  - Quick check: Which weights are being targeted for modification in the described architecture (q_proj, k_proj, etc.)?

- **Concept: Equal Distribution Baseline**
  - Why needed: The definition of "unbiased" in this paper is artificial - it defines fairness as a 50:50 prediction split between genders
  - Quick check: Does this baseline claim to represent how humans actually feel, or is it strictly a measurement framework?

## Architecture Onboarding

- **Component map:** NarraCap Caption -> [Gender Swapper/Neutralizer] -> Base Model (Mistral-7B-Instruct-v0.3, Frozen) -> LoRA Adapter (Trainable) -> Chi-square Evaluator
- **Critical path:**
  1. Data Prep: Generate 3 versions of captions (Original, Swapped, Neutral)
  2. Fine-Tuning (FT1/FT2): Train LoRA adapters to predict consistent emotions across these versions
  3. Inference: Run zero-shot prediction on the test set
  4. Statistical Analysis: Compute χ² to verify if "Man" vs "Woman" predictions differ significantly
- **Design tradeoffs:** Prompting is cheaper/faster but failed (Table 2). Training is computationally more expensive but effective. Binary gender framework excludes non-binary identities.
- **Failure signatures:** In-Context Backfire: Adding examples increased bias (e.g., "Pleasure" bias got worse in Table 2). Label Loss: Fine-tuned models stopped predicting "Anger" entirely (Table 2 notes).
- **First 3 experiments:**
  1. Sanity Check: Run the base Mistral model on 100 captions to reproduce the gender bias baseline
  2. Intervention Validity: Implement "FT1" with the 100-sample augmented set. Verify if "Anger" drops to zero
  3. Ablation: Test the "FT2" method alone to see if desensitizing gender prediction is sufficient

## Open Questions the Paper Calls Out
- **Open Question 1:** Do fine-tuning interventions effectively mitigate gender bias for non-binary and gender-diverse identities, or are they limited to the binary (man/woman) distinction?
- **Open Question 2:** Do the observed gender biases in text-based emotion recognition persist or change in multimodal settings involving tone, body language, and facial expressions?
- **Open Question 3:** Does the application of debiasing fine-tuning techniques lead to a loss of model expressiveness or capability, specifically the suppression of valid emotion labels?

## Limitations
- The study uses an artificial 50:50 prediction baseline that may not reflect actual emotion distributions in human populations
- Binary gender framework excludes non-binary individuals, limiting generalizability
- The fine-tuning dataset is relatively small (100 samples expanded to 300), raising questions about robustness

## Confidence

**High Confidence:** The finding that inference-time prompt engineering fails to reduce gender bias is well-supported by statistical results. The fine-tuning effectiveness is also well-documented with clear χ² test improvements.

**Medium Confidence:** The mechanism explaining why prompts fail is plausible but not directly tested. The claim that fine-tuning decouples gender from emotion is supported by results but relies on the artificial 50:50 baseline.

**Low Confidence:** The assertion that models have "unlearned" spurious correlations versus simply learning to output balanced predictions is not clearly differentiated. Real-world impact on accuracy remains unclear.

## Next Checks
1. **Ground Truth Validation:** Test the fine-tuned models on a dataset with actual emotion ground truth labels to verify that bias reduction doesn't come at the cost of accuracy degradation
2. **Cross-Dataset Generalization:** Evaluate the debiased models on independent emotion recognition datasets (e.g., facial expression databases) to assess whether the debiasing generalizes beyond the NarraCap caption domain
3. **Non-Binary Gender Testing:** Extend the analysis framework to include non-binary gender representations to assess whether the debiasing approach works across the full gender spectrum