---
ver: rpa2
title: Anomaly Detection by Effectively Leveraging Synthetic Images
arxiv_id: '2512.23227'
source_url: https://arxiv.org/abs/2512.23227
tags:
- anomaly
- images
- synthetic
- defect
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of generating high-quality synthetic
  defect images for industrial anomaly detection when real defect data is scarce.
  Previous approaches either produce low-fidelity defects (rule-based) or are computationally
  expensive (generative models).
---

# Anomaly Detection by Effectively Leveraging Synthetic Images

## Quick Facts
- arXiv ID: 2512.23227
- Source URL: https://arxiv.org/abs/2512.23227
- Authors: Sungho Kang; Hyunkyu Park; Yeonho Lee; Hanbyul Lee; Mijoo Jeong; YeongHyeon Park; Injae Lee; Juneho Yi
- Reference count: 40
- One-line primary result: Achieves 98.7% AUROC on MVTec AD by combining large-scale rule-based synthetic defects with high-quality training-free generative defects

## Executive Summary
This paper addresses industrial anomaly detection where real defect data is scarce by proposing a two-stage training strategy that leverages both low-fidelity rule-based and high-fidelity training-free generative synthetic defects. The approach uses a pre-trained text-guided image-to-image translation model to generate realistic defects and an image retrieval model to filter out structurally inconsistent results. Experiments on the MVTec AD dataset show that this method significantly outperforms single-stage approaches while reducing the computational cost of generating high-quality synthetic data.

## Method Summary
The method employs a two-stage training strategy: first pre-training on large-scale rule-based synthetic defects generated via Perlin noise and texture blending, then fine-tuning on a smaller set of high-quality defects generated by a training-free pipeline. The pipeline uses a pre-trained MagicBrush model for local editing and LightGlue for structural consistency filtering based on keypoint matching. This approach balances the cost of data generation with the need for realistic defect representation, achieving superior performance compared to using either synthetic data source alone.

## Key Results
- Achieves 98.7% average AUROC on MVTec AD dataset, outperforming rule-based only (96.6%) and generative only (87.2%) approaches
- Two-stage training strategy significantly reduces computational cost compared to fine-tuning on large-scale generative data
- Image retrieval filtering effectively removes structurally inconsistent or irrelevant synthetic defects

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Curriculum Transfer
Sequential training (rule-based → generative) establishes broad reconstruction competence before specializing to realistic defects. Stage 1 pre-training on large-scale rule-based synthetic data teaches the model to restore normal appearance from corrupted inputs, building general reconstruction capabilities. Stage 2 fine-tuning on fewer high-fidelity samples bridges the domain gap to realistic industrial defects without requiring massive generative sampling. Core assumption: Features learned from low-fidelity defects transfer to high-fidelity defect detection.

### Mechanism 2: Retrieval-Based Structural Consistency Filtering
An image retrieval model computes keypoint matches between normal and synthetic images to remove low-quality samples. Well-generated defects preserve structural correspondence except at defect regions (moderate match count), while failed generations show either near-identity (too many matches) or structural corruption (too few matches). Core assumption: Defects are localized perturbations; global structural preservation indicates valid synthesis.

### Mechanism 3: Training-Free Local Editing for Defect Injection
A pre-trained text-guided image-to-image model synthesizes realistic defects without domain-specific training when constrained to local editing. The model takes a normal image and text prompt describing the defect and modifies only relevant regions while preserving background context. Core assumption: The pre-trained model's learned editing priors generalize from natural scenes to industrial objects despite domain shift.

## Foundational Learning

- **Reconstruction-based anomaly detection**: Why needed: The approach trains a model to restore normal appearance from defective inputs; understanding reconstruction error as an anomaly signal is essential. Quick check: Can you explain why high reconstruction error indicates a defect in autoencoder-based detection?

- **Local feature matching (keypoint detection)**: Why needed: The filtering mechanism relies on comparing matched keypoints between normal and synthetic images to assess structural preservation. Quick check: How does SIFT/SuperPoint feature matching determine if two images show the same object from different views?

- **Diffusion model conditioning**: Why needed: The synthesis pipeline uses text-guided conditioning to control defect generation without fine-tuning. Quick check: What is the difference between global image-to-image translation and local editing in diffusion models?

## Architecture Onboarding

- **Component map**: Rule-based synthetic defects → Pre-training → High-quality synthetic defects (MagicBrush + LightGlue filtering) → Fine-tuning → Anomaly detection model

- **Critical path**: 1) Generate rule-based defects using Perlin noise + texture blending (DRAEM method) 2) Pre-train detection model on large rule-based dataset 3) Generate high-quality defects via MagicBrush with text prompts 4) Filter generated samples using LightGlue match counts 5) Fine-tune pre-trained model on filtered generative samples

- **Design tradeoffs**:
  - Data volume vs. quality: Rule-based synthesis is cheap but low-fidelity; generative synthesis is expensive but realistic
  - Filtering thresholds: Too strict → insufficient training data; too loose → noisy labels degrade performance
  - Prompt engineering: Text prompts control defect type but cannot precisely specify size/location

- **Failure signatures**:
  - Reverse fine-tuning (generative → rule-based) causes catastrophic forgetting (77.4% AUROC vs. 98.7%)
  - Unfiltered generative data produces structural inconsistencies that confuse the model
  - Single-stage generative-only training yields insufficient data volume (87.2% AUROC)

- **First 3 experiments**:
  1. Train detection model on rule-based synthetic data only; measure AUROC on MVTec AD validation set
  2. Generate defects with MagicBrush, compute match counts with LightGlue, visualize distribution across desired/irrelevant cases to set thresholds
  3. Compare pre-train→fine-tune vs. fine-tune→pre-train on a single object category to confirm reverse training degradation

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can precise spatial control over defect size and location be achieved in training-free generative frameworks?
- **Basis in paper**: The authors state that "text alone cannot describe the exact size or location of a defect" and list investigating methods for controlling these attributes without additional training as a future goal.
- **Why unresolved**: Text prompts allow for semantic generation but lack granularity to specify exact geometric parameters necessary for precise industrial simulation.
- **What evidence would resolve it**: A training-free integration of spatial constraints (e.g., bounding boxes or segmentation masks) that generates defects at user-specified locations with high fidelity.

### Open Question 2
- **Question**: Can an adaptive training strategy optimize the ratio of rule-based to generative synthetic data based on object or defect complexity?
- **Basis in paper**: The paper notes that "different products have different types of defects" and some "might not need as much training with high-quality generated images." The authors propose developing an adaptive strategy to adjust data ratios accordingly.
- **Why unresolved**: The current work utilizes a fixed two-stage strategy; it is unknown if simple objects can rely solely on rule-based data to save costs, or if complex objects require a higher proportion of generative data.
- **What evidence would resolve it**: A complexity metric that correlates with the optimal ratio of synthetic data types, demonstrating maintained or improved AUROC with reduced computational overhead.

### Open Question 3
- **Question**: To what extent can prompt optimization or dynamic guidance stabilize the generation process against textual sensitivity?
- **Basis in paper**: The authors acknowledge that "relying only on text prompts makes it difficult to control the generation process" and that "small changes in the text can produce unwanted results," proposing prompt optimization as a solution.
- **Why unresolved**: The current framework requires a filtering mechanism precisely because the text-guided generation is unstable and prone to irrelevant outputs.
- **What evidence would resolve it**: Demonstrated robustness where minor prompt paraphrasing yields consistent structural results, significantly reducing the rejection rate of the filtering mechanism.

## Limitations
- Base model architecture not specified (assumes DRAEM or similar reconstruction network)
- Exact filtering thresholds for LightGlue matching not disclosed
- Training hyperparameters (learning rate, batch size, epochs) absent
- Prompt engineering details per object/defect category unspecified

## Confidence

- **High**: Two-stage training strategy (rule-based → generative) outperforms single-source approaches (98.7% vs 96.6% vs 87.2% AUROC)
- **Medium**: Training-free synthesis via MagicBrush generalizes from MS COCO to industrial objects
- **Low**: Structural consistency filtering using match counts generalizes across defect types and object categories

## Next Checks

1. **Ablation on Training Order**: Systematically compare pre-train→fine-tune vs fine-tune→pre-train on individual object categories to confirm order sensitivity
2. **Filtering Threshold Sensitivity**: Vary LightGlue match count thresholds and measure impact on AUROC to determine optimal filtering criteria
3. **Cross-Dataset Generalization**: Evaluate the two-stage approach on a different anomaly detection dataset (e.g., VisA or real-world industrial data) to assess domain transfer capability