---
ver: rpa2
title: 'Beyond the Prompt: Gender Bias in Text-to-Image Models, with a Case Study
  on Hospital Professions'
arxiv_id: '2510.00045'
source_url: https://arxiv.org/abs/2510.00045
tags:
- hospital
- male
- gender
- images
- portrait
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates gender bias in six state-of-the-art open-weight
  text-to-image models by generating 100 images per model for five hospital professions
  (cardiologist, hospital director, nurse, paramedic, surgeon) and five portrait qualifiers
  ("", corporate, neutral, aesthetic, beautiful). Results show systematic occupational
  stereotypes: all models depict nurses exclusively as women and surgeons predominantly
  as men, with variations in male dominance across roles and models.'
---

# Beyond the Prompt: Gender Bias in Text-to-Image Models, with a Case Study on Hospital Professions

## Quick Facts
- arXiv ID: 2510.00045
- Source URL: https://arxiv.org/abs/2510.00045
- Reference count: 23
- One-line primary result: All models show systematic gender stereotypes for hospital professions, with nurses exclusively female and surgeons predominantly male, while portrait qualifiers significantly influence gender representation.

## Executive Summary
This study investigates gender bias in six state-of-the-art open-weight text-to-image models by generating 100 images per model for five hospital professions (cardiologist, hospital director, nurse, paramedic, surgeon) and five portrait qualifiers ("", corporate, neutral, aesthetic, beautiful). Results show systematic occupational stereotypes: all models depict nurses exclusively as women and surgeons predominantly as men, with variations in male dominance across roles and models. Qwen-Image and SDXL enforce rigid male dominance, while FLUX.1-dev skews female overall. Portrait qualifiers significantly influence gender representation, with corporate reinforcing male depictions and beautiful favoring female ones. Sensitivity to qualifiers varies: Qwen-Image remains unaffected, while FLUX.1-dev, SDXL, and SD3.5 show strong prompt dependence. The findings highlight the need for bias-aware design, balanced defaults, and user guidance to prevent reinforcement of occupational stereotypes in generative AI.

## Method Summary
The study generated 15,000 images across six open-weight text-to-image models using a unified prompt grammar with five hospital professions and five portrait qualifiers. Each configuration produced 100 images at 1024×1024 resolution using model-specific samplers, schedulers, and CFG scales. Manual gender classification was performed by authors, and sensitivity was measured as the Max-Min variation across qualifiers. The analysis focused on binary gender representation and occupational stereotypes, with results validated through quantitative comparison across models and roles.

## Key Results
- All models depict nurses exclusively as women and surgeons predominantly as men, showing rigid occupational stereotypes
- Qwen-Image and SDXL enforce consistent male dominance across all roles (80-100% male), while FLUX.1-dev skews female overall (40-60% female)
- Portrait qualifiers significantly influence gender representation, with corporate reinforcing male depictions and beautiful favoring female ones
- Model sensitivity to qualifiers varies dramatically: Qwen-Image shows near-zero sensitivity (<1% variation) while FLUX.1-dev, SDXL, and SD3.5 show 50-73 point swings in gender ratios

## Why This Works (Mechanism)

### Mechanism 1: Training Distribution Capture and Reproduction
- Claim: Models encode gender-profession associations from training corpora and reproduce them during inference.
- Mechanism: LAION-5B and similar web-scale datasets contain stereotypical associations (e.g., surgical imagery historically dominated by men). During training, the joint probability P(gender | profession, context) is learned implicitly. At inference, conditioning on profession tokens activates these learned associations.
- Core assumption: The training data contains systematic gender-profession correlations that the model architecture can capture.
- Evidence anchors:
  - [Abstract/Section 1]: "all models produced nurses exclusively as women and surgeons predominantly as men"
  - [Section 4.1]: "These results highlight two types of biases: rigid stereotypes (e.g., female nurses, male surgeons) and model-specific skews"
  - [Corpus]: T2IBias paper confirms "reproduction and amplification of race- and gender-related stereotypes" in TTI latent spaces
- Break condition: If training data were balanced across gender-profession combinations, this mechanism would weaken but not disappear (model architecture still matters).

### Mechanism 2: Semantic Modifier Drift
- Claim: Portrait qualifiers shift gender ratios by activating distinct semantic subspaces in the text encoder.
- Mechanism: Terms like "corporate" and "beautiful" have gendered connotations in training data. When appended to prompts, they bias the cross-attention maps toward gendered visual features. FLUX shows 62-point swing in surgeon gender (5% male with "beautiful" → 67% with "corporate").
- Core assumption: Text encoders embed semantic modifiers in ways that correlate with gender in training data.
- Evidence anchors:
  - [Section 4.2]: "terms like corporate tend to be semantically linked with male representations, whereas beautiful skews strongly female"
  - [Table 5]: FLUX surgeon gender varies from 5% to 67% male across qualifiers
  - [Corpus]: "Prompting Away Stereotypes?" finds similar occupational bias patterns but limited evidence on modifier effects
- Break condition: If text encoders were debiased or modifiers were orthogonal to gender subspaces, this effect would attenuate.

### Mechanism 3: Differential Prompt Sensitivity
- Claim: Model architecture and training choices determine responsiveness to prompt reformulation.
- Mechanism: QWEN and HUNYUAN show near-zero sensitivity (max-min variation ≤1% for most roles), suggesting strongly entrenched priors. FLUX, SD3.5, and SDXL show 50-73 point swings, indicating weaker profession-gender priors or stronger modifier influence.
- Core assumption: Architectural differences (e.g., text encoder choice, CFG scale, training data curation) affect how strongly prompt tokens influence outputs.
- Evidence anchors:
  - [Table 9]: QWEN max-min variation is 0-1 percentage points; FLUX shows 62-point variation for surgeons
  - [Section 3.7]: "some models enforce rigid stereotypes independent of prompt formulation (QWEN, HUNYUAN), while others amplify or mitigate biases depending on prompt wording"
  - [Corpus]: Sparse Autoencoder work suggests model-agnostic bias control is possible, implying architectural variation in bias entrenchment
- Break condition: If all models had similar architectures and training data, sensitivity differences would narrow.

## Foundational Learning

- Concept: **Conditional Generation in Diffusion Models**
  - Why needed here: Understanding how text embeddings condition the denoising process explains why prompt wording affects outputs.
  - Quick check question: Can you explain why "a portrait of a surgeon" produces different results than "a surgeon"?

- Concept: **Cross-Attention in Text-to-Image Architectures**
  - Why needed here: The cross-attention mechanism determines which prompt tokens influence which image regions; modifier effects depend on this.
  - Quick check question: How might "corporate" vs. "beautiful" shift attention patterns differently?

- Concept: **Evaluation Metrics for Bias in Generative Models**
  - Why needed here: The study uses binary gender classification and percentage-based analysis; understanding limitations of this approach is critical.
  - Quick check question: What biases might be introduced by the manual binary classification method?

## Architecture Onboarding

- Component map: Prompt → Text Encoder → Cross-Attention → Diffusion Process → VAE Decode → Image Classification
- Critical path: Prompt → Text Encoder → Cross-Attention → Diffusion Process → VAE Decode → Image Classification. Bias can enter at encoding (semantic associations), cross-attention (modifier influence), or diffusion (prior entrenchment).
- Design tradeoffs:
  - **Open-weight vs. Proprietary**: Open-weight enables reproducibility but may lag in bias mitigation efforts by providers.
  - **Manual vs. Automated Classification**: Manual ensures accuracy but doesn't scale; automated classifiers may introduce their own biases.
  - **Binary vs. Spectrum Analysis**: Binary classification misses non-binary representation but simplifies analysis.
- Failure signatures:
  - **Rigid outputs**: If variation across qualifiers is <5 percentage points, the model likely has entrenched priors (like QWEN).
  - **High variance but consistent direction**: If variation is high but all conditions skew male or female, modifier effects are present but insufficient for balance.
  - **Classifier disagreement**: If manual classification produces ambiguous cases, consider intersectional factors (e.g., surgical masks obscuring gender cues).
- First 3 experiments:
  1. **Baseline replication**: Replicate the 100-image generation for one role (e.g., surgeon) with all five qualifiers on one model to verify sensitivity patterns.
  2. **Cross-model comparison**: Run the same prompt set on two models with contrasting sensitivity (e.g., QWEN vs. FLUX) to confirm differential responsiveness.
  3. **Modifier ablation**: Test additional modifiers (e.g., "professional," "friendly," "authoritative") to map the semantic space of gender bias more comprehensively.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do intersectional dimensions such as ethnicity, age, and disability interact with the observed gender biases in text-to-image outputs for medical professions?
- Basis: [explicit] The conclusion states that future work should "expand beyond gender to examine intersectional dimensions such as ethnicity, age, or disability."
- Why unresolved: The current study isolated gender as the primary variable, classifying images only as male or female without analyzing other demographic traits.
- Evidence: A multi-attribute analysis of generated images coding for the co-occurrence of gender with various ethnicities, age groups, and visible disabilities.

### Open Question 2
- Question: To what extent do biased AI-generated medical images influence decision-making or perceptions in downstream applications like medical education, recruitment, or patient trust?
- Basis: [explicit] The conclusion notes that "the impact of such biases on downstream applications—from education to recruitment—deserves closer investigation."
- Why unresolved: The paper focuses on quantifying bias within the model outputs rather than measuring the behavioral or societal effects of these outputs on human viewers.
- Evidence: User studies measuring shifts in implicit bias or hiring preferences after exposure to stereotyped versus balanced AI-generated professional imagery.

### Open Question 3
- Question: Do proprietary models with hidden safety filters and pre-prompting strategies exhibit the same occupational stereotypes as the open-weight models studied?
- Basis: [explicit] The conclusion suggests researchers should "systematically compare open-weight and proprietary models" to understand bias across different systems.
- Why unresolved: The study deliberately excluded proprietary models (e.g., DALL-E 3) due to lack of transparency and reproducibility, leaving their bias profiles unknown.
- Evidence: A comparative benchmark using the identical prompt grammar across major proprietary APIs to measure their gender distributions against the open-weight baselines.

## Limitations
- Binary gender classification fails to capture non-binary identities and intersectional factors (race, age, disability)
- Manual classification process limits scalability and introduces potential subjectivity in edge cases
- Study focuses on hospital professions, which may not generalize to other occupational domains or cultural contexts

## Confidence
- **High Confidence**: The systematic gender stereotyping across all six models (female nurses, male surgeons) is robust given the large sample size (15,000 images) and consistent patterns across multiple models
- **Medium Confidence**: The attribution of bias patterns to training data distributions and semantic modifier drift is supported by corpus evidence but not directly validated through ablation studies
- **Low Confidence**: Claims about the specific mechanisms by which different text encoders contribute to bias patterns are speculative without controlled architectural experiments

## Next Checks
1. **Seed Sensitivity Analysis**: Run multiple random seeds for the same prompt configurations to quantify variance and determine whether observed patterns are stable or subject to stochastic variation
2. **Intersectional Bias Extension**: Expand the analysis to include intersectional dimensions (race, age, disability) by either manual classification or automated tools, to understand how gender bias interacts with other demographic factors
3. **Architectural Ablation Study**: Conduct controlled experiments varying only the text encoder component while keeping other model parameters constant to isolate the contribution of different encoders to gender bias patterns