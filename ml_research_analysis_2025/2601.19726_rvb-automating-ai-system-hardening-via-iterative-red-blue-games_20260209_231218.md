---
ver: rpa2
title: 'RvB: Automating AI System Hardening via Iterative Red-Blue Games'
arxiv_id: '2601.19726'
source_url: https://arxiv.org/abs/2601.19726
tags:
- team
- blue
- attack
- arxiv
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Red Team vs. Blue Team (RvB) framework,
  which automates AI system hardening through iterative adversarial interactions between
  offensive and defensive agents.
---

# RvB: Automating AI System Hardening via Iterative Red-Blue Games

## Quick Facts
- **arXiv ID**: 2601.19726
- **Source URL**: https://arxiv.org/abs/2601.19726
- **Reference count**: 23
- **Primary result**: RvB achieves high Defense Success Rates (90% for code hardening, 45% for guardrails) while maintaining near-zero False Positive Rates

## Executive Summary
The RvB framework automates AI system hardening through iterative adversarial interactions between Red and Blue agents without model parameter updates. Operating as a training-free sequential game, RvB uses environmental feedback to drive continuous capability enhancement across two domains: dynamic code hardening against CVEs and guardrail optimization against jailbreaks. The framework achieves high Defense Success Rates while maintaining near-zero Service Disruption Rates and demonstrates strong generalization to unseen attacks. RvB consumes fewer tokens than cooperative baselines, establishing a practical paradigm for continuous AI security hardening.

## Method Summary
RvB implements a turn-based sequential game where a Red Team agent actively attacks a vulnerable environment while a Blue Team agent defends by patching vulnerabilities discovered through attack logs. The framework operates without model parameter updates, using environmental state transitions to encode interaction history and drive belief updates. In code hardening mode, the Red Team uses CAI with Bash/MCP interfaces to probe PHP web applications, while the Blue Team employs Mini-SWE-Agent to generate and apply git diff patches. For guardrail optimization, the Red Team uses NeMo Guardrails to test defensive policies while the Blue Team refines rules through adversarial feedback. The process iterates until convergence criteria are met, with a maximum of 5 epochs and Count_delay=3 for stability verification.

## Key Results
- Achieves 90% Defense Success Rate for dynamic code hardening against CVEs
- Maintains 45% Defense Success Rate for guardrail optimization against jailbreaks
- Demonstrates strong generalization with out-of-domain performance improving from 96%→99% on JailBreakBench

## Why This Works (Mechanism)

### Mechanism 1
State transitions serve as externalized memory, enabling belief updates without parameter changes. The environment encodes interaction history (S_k ≅ Encode(M_k)), and when Red Team attacks and Blue Team patches, the state transition S_k → S_k+1 physically invalidates previously exploitable strategies. Agents, though computationally stateless, encounter a mathematically equivalent posterior distribution when observing the new state.

### Mechanism 2
Adversarial pressure compels the Blue Team to learn generalizable defensive principles rather than overfitting to specific exploits. The Red Team continuously probes for blind spots, functioning as a regularizer that penalizes superficial pattern matching and forces inductive reasoning about underlying vulnerability logic. The "hard negatives" synthesized from the tail of the vulnerability distribution drive generalization.

### Mechanism 3
Semantic verification through immediate exploitation prevents destructive remediation. Coupling remediation with targeted exploitation creates check-and-balance dynamics where the Red Team's directed adversarial reports provide precise, actionable contexts that prevent the Blue Team from making indiscriminate modifications.

## Foundational Learning

- **Concept: Bayesian belief updating under partial observability**
  - Why needed here: The entire framework models security hardening as sequential belief refinement (Equation 3). Understanding how evidence updates priors is essential for interpreting convergence.
  - Quick check question: Given posterior b_{k+1}(π_B) = P(π_B|E_k, S_k), what happens to belief entropy when evidence E_k is uninformative?

- **Concept: Imperfect-information sequential games**
  - Why needed here: The Red Team observes state S_k but not Blue Team's defense logic π_B. This asymmetry drives the exploration-exploitation dynamics and distinguishes RvB from self-play.
  - Quick check question: Why does imperfect information force the Red Team to maintain a belief distribution rather than directly optimizing against known defenses?

- **Concept: Epistemic uncertainty vs. aleatoric uncertainty**
  - Why needed here: The framework quantifies "Attack Complexity" as reduction in epistemic uncertainty (H(b_k)). Distinguishing reducible uncertainty (model/strategy gaps) from irreducible noise is critical for interpreting convergence metrics.
  - Quick check question: If Attack Success Count (ASC) increases while Defense Success Rate (DSR) also increases, what does this imply about the uncertainty landscape?

## Architecture Onboarding

- **Component map**: Red Team Agent -> Planner -> Executor -> Reporter; Blue Team Agent -> Fault localizer -> Patch generator -> Regression verifier; Environment State Manager; Stopping Criteria Monitor
- **Critical path**: 1. Initialize vulnerable environment (S_1) 2. Red Team reconnaissance → active probing → structured Attack Log 3. Blue Team receives Attack Log → localizes fault → generates git diff patch → applies and restarts 4. State transition S_k → S_{k+1} 5. Verification: vulnerability check + regression test → TDSR/FDSR calculation 6. Repeat until stopping criteria
- **Design tradeoffs**: max_epoch (5) balances convergence depth vs. compute costs; retry attempts (3) prevent premature termination but risk overfitting; stronger Red Team models discover more diverse attacks; Count_delay=3 confirms convergence but slows termination
- **Failure signatures**: Fake DSR > True DSR indicates destructive patches; ASC flatlines early suggests exploration exhaustion; FPR spikes reveal overfitting to training attacks; Cross-Round Defense Efficacy decreases indicate catastrophic forgetting
- **First 3 experiments**: 1. Baseline comparison: Run RvB vs. cooperative MAS on identical CVE set; compare TDSR, SDR, token consumption 2. Ablation on stopping criteria: Vary Count_delay (1, 3, 5) and max_epoch (3, 5, 7); measure convergence speed vs. final DSR 3. Out-of-domain generalization test: Train guardrail on HarmBench subset; evaluate on JailBreakBench, AdvBench, SorryBench; compare Initial vs. Final Guard performance

## Open Questions the Paper Calls Out

- **Question 1**: What are the theoretical convergence bounds of the RvB framework, and can formal Nash Equilibrium be achieved in the high-dimensional discrete space of natural language?
  - Basis: The discussion notes "While a formal Nash Equilibrium in the high-dimensional discrete space of natural language remains theoretically elusive, the monotonic state progression confirms that offensive and defensive utilities balance at a robust security standard."
  - Evidence to resolve: Theoretical analysis proving convergence bounds, or counter-examples showing non-convergence scenarios

- **Question 2**: How does the RvB framework generalize to multi-modal security tasks beyond text-based code and guardrail domains?
  - Basis: Future work will "explore the theoretical bounds of this convergence in multi-modal environments."
  - Evidence to resolve: Empirical validation on image-based adversarial attacks, audio spoofing defense, or cross-modal vulnerability scenarios

- **Question 3**: Why does guardrail optimization achieve significantly lower Defense Success Rate (45%) compared to code hardening (90%), and what architectural modifications could close this gap?
  - Basis: The paper reports disparate DSRs across domains but does not analyze the root causes of this performance gap or propose remediation strategies
  - Evidence to resolve: Ablation studies isolating factors (attack surface complexity, rule expressiveness, iteration depth) affecting guardrail learning

## Limitations

- The turn-based simulation serves as a controlled abstraction, simplifying asynchronous dynamics inherent in real-world security operations
- The framework's effectiveness critically depends on sustained Red Team attack diversity, with no guarantee this pattern holds across domains
- Environmental state encoding assumptions may not capture complex multi-vulnerability interactions or structural dependencies

## Confidence

- **High Confidence**: Training-free paradigm works for immediate deployment; near-zero SDR demonstrates effective adversarial auditing; computational efficiency advantage is well-supported
- **Medium Confidence**: Adversarial pressure drives generalizable principles (limited by Red Team diversity assumptions); state transitions provide sufficient memory (unverified encoding completeness); 5-epoch convergence pattern generalizes (no long-run analysis)
- **Low Confidence**: Framework robustness against highly capable Red Teams (no stress testing beyond standard models); performance guarantees on complex multi-vulnerability systems (only single-vulnerability testing shown); security improvements persist under adaptive attack strategies (no adaptive adversary evaluation)

## Next Checks

1. **Red Team Diversity Stress Test**: Run extended experiments (20+ epochs) on the same CVE set to measure Red Team capability decay rate. Compare attack diversity metrics over time to identify when diversity collapses before convergence.

2. **State Encoding Completeness Analysis**: For each iteration, extract the environmental state representation and analyze whether it captures all relevant vulnerability dimensions. Test by attempting exploits that use different attack vectors but target the same underlying vulnerability.

3. **Multi-Vulnerability Interaction Testing**: Deploy RvB on a system with 5+ interdependent vulnerabilities where patching one affects others. Track whether the framework correctly handles cascading effects and compare against manual security audit results.