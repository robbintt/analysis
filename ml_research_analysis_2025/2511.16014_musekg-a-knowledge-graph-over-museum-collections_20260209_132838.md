---
ver: rpa2
title: 'MUSEKG: A Knowledge Graph Over Museum Collections'
arxiv_id: '2511.16014'
source_url: https://arxiv.org/abs/2511.16014
tags:
- museum
- object
- musekg
- query
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MuseKG is an end-to-end framework that constructs a typed property
  graph from museum collections and answers natural language queries using retrieval-augmented
  generation. The system integrates structured records and media-derived labels into
  a unified knowledge graph, then maps queries to graph operations for interpretable
  reasoning.
---

# MUSEKG: A Knowledge Graph Over Museum Collections

## Quick Facts
- arXiv ID: 2511.16014
- Source URL: https://arxiv.org/abs/2511.16014
- Reference count: 12
- Primary result: End-to-end framework that constructs typed property graph from museum collections and answers NL queries using RAG, achieving 0.84/0.42/0.34 accuracy vs 0.20/0.04/0.02 (zero-shot) and 0.80/0.32/0.12 (SPARQL-prompt)

## Executive Summary
MuseKG builds a typed property graph from heterogeneous museum collection records and answers natural language queries through retrieval-augmented generation. The system normalizes structured data into a unified knowledge graph with 7 entity types and 7 relation labels, then retrieves attributes plus one-hop neighbors as context for LLM-based query answering. Evaluated on 150 museum-based questions, MuseKG outperforms zero-shot, few-shot, and SPARQL-prompt baselines by providing interpretable symbolic grounding for cultural heritage reasoning.

## Method Summary
MuseKG consists of two modules: (1) KG Constructor that normalizes JSON records into a typed property graph with 7 entity types (object, person, organisation, image, image_label, place, concept) and 7 relation labels, creating nodes for objects, people, organizations, images, and linked entities while deduplicating by title/accession number; (2) NL Query Interface that extracts entities from user queries via LLM, retrieves node attributes plus one-hop neighbors, and prompts LLM to answer using only this KG context. The system uses NetworkX for graph operations and was tested on University of Melbourne museum collection (15,829 objects, 3 collections) with 45,713 images.

## Key Results
- Attribute lookup (C1): MuseKG 0.84 vs zero-shot 0.20 vs SPARQL-prompt 0.80
- Relation retrieval (C2): MuseKG 0.42 vs zero-shot 0.04 vs SPARQL-prompt 0.32
- One-hop reasoning (C3): MuseKG 0.34 vs zero-shot 0.02 vs SPARQL-prompt 0.12
- Outperforms baselines by providing interpretable symbolic grounding vs LLM hallucination

## Why This Works (Mechanism)

### Mechanism 1: Symbolic Grounding via Typed Property Graph
Converting heterogeneous museum records into a typed property graph with explicit relations enables more accurate retrieval than prompting LLMs to reason over raw data. The KG constructor normalizes entities, assigns types from fixed vocabulary, and creates edges from curated relation set R, providing deterministic traversal paths that LLMs can reliably follow. Break condition: if source records have highly variable relationship fields that don't map cleanly to R, normalization fails.

### Mechanism 2: Context-Constrained RAG Generation
Retrieving only the KG neighborhood (attributes + 1-hop neighbors) of query entities and prompting LLM to answer using this context reduces hallucination compared to unconstrained generation. NL Query Interface extracts entities via LLM, retrieves textualized KG context, then instructs LLM: "Answer using ONLY the KG context." Break condition: if entity extraction misidentifies the query subject or the answer requires multi-hop traversal, retrieved context is incomplete.

### Mechanism 3: Structured Query Decomposition for One-Hop Reasoning
Breaking queries into (entity, relation, target_attribute) components enables systematic graph traversal that outperforms SPARQL-prompt generation. Category 3 queries require: identify primary object, traverse relation edge, retrieve attribute from related node. MuseKG's entity extraction + neighbor retrieval performs this decomposition implicitly via KG structure rather than relying on LLM to generate correct SPARQL. Break condition: if relation labels in queries don't match normalized relation vocabulary, traversal may select wrong path.

## Foundational Learning

- **Property Graph Data Model**: Understanding typed nodes, labeled edges, and attribute dictionaries is prerequisite to reading the schema. Quick check: Given nodes A (type: object) and B (type: person) with edge ρ(A→B) = "has_primary_producer", what query retrieves all objects produced by B?
- **Retrieval-Augmented Generation (RAG)**: The query interface is explicitly RAG-based; understanding context retrieval → LLM synthesis pipeline explains both strengths (grounding) and limitations (1-hop constraint). Quick check: If retrieved context is empty but the LLM still generates an answer, what failure mode has occurred?
- **Entity Resolution / Deduplication**: KG Constructor Step 5 merges duplicate nodes by canonical titles; understanding deduplication strategy is critical for diagnosing missing edges or spurious duplicates. Quick check: Two records refer to "WPA Ltd" and "Walden Precision Apparatus Limited" — without normalization, how many nodes are created? With normalization, what information is required to merge them?

## Architecture Onboarding

- **Component map**: JSON records → KG Constructor → Typed Property Graph → NL Query Interface → User Answer
- **Critical path**: 1. Map source JSON fields to node types, 2. Normalize relation identifiers, 3. Implement entity extraction prompt, 4. Retrieve and textualize KG context, 5. Apply constrained generation prompt
- **Design tradeoffs**: 7 curated relations vs open schema (limits expressiveness but improves consistency), 1-hop context vs multi-hop (simpler retrieval but C3 drops to 0.34), NetworkX (in-memory) vs persistent graph DB (scalability unclear), generic KG-RAG prompt vs task-specific prompts (reduces prompt engineering overhead)
- **Failure signatures**: Entity extraction mismatch → empty context, Missing relation → traversal fails, Ambiguous deduplication → wrong attributes, Multi-hop queries → insufficient context
- **First 3 experiments**: 1. Attribute lookup validation (10 queries), 2. Relation traversal test (5 queries), 3. Failure mode probing (5 one-hop reasoning queries, log error patterns)

## Open Questions the Paper Calls Out

- **Generalization to larger, heterogeneous collections**: How does MuseKG perform when applied to larger, more diverse museum partners? The current study is restricted to a single institution.
- **Support for deeper multi-hop reasoning**: How does the system perform when extended to support deeper multi-hop queries? Accuracy drops significantly from 0.84 to 0.34 for one-hop queries.
- **LLM-based evaluation scalability**: Can LLM-based evaluation effectively replace human evaluation to ensure scalability? Current results rely entirely on human assessment.

## Limitations
- Schema coverage gaps: 7 curated relations may not handle all edge cases in heterogeneous collections
- Single-hop constraint: One-hop context insufficient for complex queries, multi-hop performance unquantified
- Reproducibility gaps: Key implementation details (relation labels, prompts, validation rules) pending in technical report

## Confidence
- **High confidence**: Comparative results against baselines are internally consistent and methodologically sound
- **Medium confidence**: Mechanism claims are logically coherent but lack ablation studies
- **Low confidence**: Scalability and generalizability claims are not empirically tested

## Next Checks
1. Test entity extraction fidelity across varied query phrasings, log false negatives/positives
2. Manually inspect source relationshipId fields to verify complete mapping to 7 normalized relations
3. Construct benchmark of 10-15 two-hop queries to measure performance degradation and identify failure sources