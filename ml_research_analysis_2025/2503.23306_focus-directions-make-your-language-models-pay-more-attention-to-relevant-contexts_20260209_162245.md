---
ver: rpa2
title: Focus Directions Make Your Language Models Pay More Attention to Relevant Contexts
arxiv_id: '2503.23306'
source_url: https://arxiv.org/abs/2503.23306
tags:
- heads
- attention
- contextual
- focus
- relevant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of distraction in long-context
  large language models (LLMs), where models are prone to being distracted by irrelevant
  contexts. The core method idea is to identify "contextual heads," a special group
  of attention heads that control the overall attention of LLMs, and to introduce
  "focus directions," which are located at the key and query activations of these
  heads, enabling them to allocate more attention to relevant contexts without explicitly
  specifying which context is relevant.
---

# Focus Directions Make Your Language Models Pay More Attention to Relevant Contexts

## Quick Facts
- arXiv ID: 2503.23306
- Source URL: https://arxiv.org/abs/2503.23306
- Authors: Youxiang Zhu; Ruochen Li; Danqing Wang; Daniel Haehn; Xiaohui Liang
- Reference count: 36
- Primary result: 67.1% EM accuracy with top-20 heads and α=0.3 on Llama-3.2-3B-Instruct (baseline: 59.4%)

## Executive Summary
This paper addresses distraction in long-context LLMs by identifying "contextual heads" - a sparse subset of attention heads that control overall attention behavior. The core innovation is "focus directions," vector additions to key and query activations that steer these heads to allocate more attention to relevant contexts without explicitly specifying which content is relevant. The method improves task alignment by reallocating attention from sink tokens to semantic content, achieving significant performance gains on the HELMET benchmark across various task types.

## Method Summary
The method identifies contextual heads via contextual scoring, then learns focus direction vectors (dK, dQ) through gradient descent to maximize attention on relevant contexts. During inference, these vectors are added to key and query activations with strength α. The approach targets middle-to-late layer heads and works by shifting attention probability mass away from sink tokens toward relevant semantic content. Training uses gold responses to optimize focus directions, which are then applied at inference time to improve extraction accuracy on long-context tasks.

## Key Results
- Improved overall average performance on HELMET benchmark for 5/5 LLMs on 32k contexts
- Achieved 67.1% EM accuracy on Llama-3.2-3B-Instruct with top-20 heads and α=0.3
- Focus directions mitigate poor task alignment by reallocating attention from sinks to relevant contexts
- Performance peaks at intermediate α values (0.2-0.3) and specific head counts (top-20)

## Why This Works (Mechanism)

### Mechanism 1: Sparse Contextual Head Control
A model's distraction susceptibility is governed by a small subset of attention heads ("contextual heads") rather than the entire network. These heads are identified via contextual scoring and are typically sparse, located in middle-to-late layers. When contextual heads fail to attend to relevant tokens, the model generates incorrect responses regardless of other heads' behavior.

### Mechanism 2: Query/Key Activation Steering
Contextual head behavior can be controlled at inference by adding focus direction vectors to Key (K) and Query (Q) activations. Following the linear representation hypothesis, "focusing" is a directional concept in activation space. By calculating gradient-descent vectors that maximize attention to relevant context on training data, these vectors shift attention distributions when added during inference.

### Mechanism 3: Attention Reallocation from Sink
Focus directions improve performance primarily by moving attention probability mass from "attention sink" tokens (e.g., BOS, padding) to actual semantic content. In long contexts, models often "hide" probability mass on sink tokens to avoid noise. The focus direction provides a learned signal that allows safe redirection of this "unused" attention to relevant contexts.

## Foundational Learning

- **Attention Sinks**: LLMs attend heavily to initial tokens (sinks) for stability. Understanding this is crucial because the method works by explicitly shifting probability mass away from these sinks. Quick check: If you remove the [BOS] token attention from a standard LLM, why does generation quality often collapse?

- **Activation Steering/Representation Engineering**: The core intervention adds vectors in activation space rather than editing weights. Model behaviors can often be represented as directions in high-dimensional space. Quick check: How does adding a vector αd to a residual stream differ from fine-tuning the weights W?

- **Linear Representation Hypothesis**: "Focus" can be linearly decoupled from specific token content, justifying why a single vector learned on one dataset can generalize to others. Quick check: Does the hypothesis imply concepts are represented as individual dimensions or as directions in activation space?

## Architecture Onboarding

- **Component map**: Input -> Tokenizer -> Transformer -> Key/Query Projection Hook -> Vector Addition (K+Q+αd) -> Attention Calculation -> Output

- **Critical path**: The most sensitive step is identifying Contextual Heads. Wrong layer/head selection (early layers or random heads) causes failure or performance degradation.

- **Design tradeoffs**:
  - Top-k Heads: <10 heads have weak effect; >50 heads introduce noise and degrade accuracy
  - Strength α: 0.2-0.3 balances focus; >0.7 causes over-focusing on irrelevant context

- **Failure signatures**:
  - Over-focusing: EM accuracy drops below baseline when α > 0.5
  - No Effect: Wrong head selection (retrieval vs. contextual heads) or wrong projection target
  - Noise introduction: Too many heads (>50) or too strong α (>0.5)

- **First 3 experiments**:
  1. Head Localization: Run contextual scoring method on validation set to identify top-20 heads with highest attention on ground-truth spans
  2. Ablation on k: Test performance on top-10 vs top-50 heads to find sweet spot around 20
  3. Hyperparameter Scan: Sweep α ∈ [-0.5, 0.5] to confirm positive values increase relevant context score

## Open Questions the Paper Calls Out

### Open Question 1
Are focus directions task-dependent, and do optimal directions exist for specific tasks? The authors acknowledge focus directions may be task-dependent since "relevant context" varies across tasks. Experiments deriving directions on specific task datasets and measuring transferability would resolve this.

### Open Question 2
Can focus directions serve as a parameter-efficient fine-tuning mechanism to adapt LLMs to specific domains? Section 5 posits this as an alternative to methods like LoRA. Comparative studies measuring training efficiency and performance against standard PEFT baselines would provide evidence.

### Open Question 3
Can focus directions function as a switch to resolve knowledge conflicts between parametric memory and retrieved context? The paper hypothesizes focus directions may control LLM's use of contextual vs. internal knowledge. Experiments on counterfactual/contradiction benchmarks would test if focus directions bias models toward provided context over pre-trained weights.

## Limitations
- Performance degrades significantly on contexts longer than 64k tokens, suggesting scalability limitations
- Method's effectiveness depends on accurate identification of contextual heads, which may not generalize across all model architectures
- Generalizability to non-QA long-context tasks (multi-hop reasoning, summarization) remains untested

## Confidence
- **High Confidence**: Sparse contextual head control mechanism is supported by empirical evidence showing performance improvement with targeted head intervention
- **Medium Confidence**: Attention reallocation from sinks to semantic content mechanism is supported by attention distribution analysis but alternative explanations exist
- **Low Confidence**: Generalizability of learned focus directions across different document types and tasks, as only NaturalQuestions-Open data is tested

## Next Checks
1. **Head Selection Robustness Test**: Systematically vary head count (k=5, 10, 20, 30, 50) and document positions to identify optimal configuration and sensitivity to selection criteria.

2. **Cross-Document Generalization**: Train focus directions on Wikipedia articles and evaluate on scientific papers or legal documents to test domain-general vs. document-specific pattern capture.

3. **Long-Context Scalability Analysis**: Evaluate on HELMET tasks with 128k+ context lengths to identify breaking point and determine whether limitations stem from focus direction saturation or fundamental scalability constraints.