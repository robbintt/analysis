---
ver: rpa2
title: A Dataset for Analysing News Framing in Chinese Media
arxiv_id: '2503.04439'
source_url: https://arxiv.org/abs/2503.04439
tags:
- news
- framing
- dataset
- chinese
- frames
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first Chinese News Framing dataset, designed
  to facilitate research on automatic news framing detection in Chinese. The dataset
  contains 353 articles annotated with 14 framing dimensions and is structured to
  complement the SemEval-2023 task 3 dataset.
---

# A Dataset for Analysing News Framing in Chinese Media

## Quick Facts
- **arXiv ID**: 2503.04439
- **Source URL**: https://arxiv.org/abs/2503.04439
- **Reference count**: 11
- **Primary result**: Introduces first Chinese News Framing dataset (353 articles, 14 framing dimensions) with strong performance using XLM-RoBERTa (F1-micro 0.719 Chinese-only, 0.753 augmented).

## Executive Summary
This paper introduces the first Chinese News Framing dataset designed to facilitate automatic news framing detection in Chinese media. The dataset contains 353 manually annotated articles covering 14 framing dimensions, structured to complement the SemEval-2023 task 3 dataset. Experiments demonstrate that fine-tuning XLM-RoBERTa on this dataset yields strong performance, achieving F1-micro scores of 0.719 for Chinese-only detection and 0.753 when augmented with SemEval data. The results validate the dataset's utility both as a standalone resource and as a supplement to multilingual news framing research.

## Method Summary
The method involves fine-tuning XLM-RoBERTa-base for multi-label classification of news articles into 14 framing dimensions. The dataset (353 articles total: 233 train, 50 dev, 70 test) was created through semantic-guided stratified sampling using BERTopic and SentenceTransformer embeddings to ensure class balance. Articles were preprocessed to remove metadata and formatted with newline separation between title and body. The model was trained using Binary Cross-Entropy loss with logits, AdamW optimizer (learning rate 5e-5), batch size 8, and 100 epochs. Performance was evaluated using F1-micro score.

## Key Results
- XLM-RoBERTa fine-tuned on Chinese News Framing dataset achieves F1-micro of 0.719 for Chinese-only detection
- Augmentation with SemEval data improves F1-micro to 0.753, demonstrating cross-lingual transfer benefits
- GPT-4o zero-shot inference performs significantly worse (F1-micro ~0.584) due to over-labeling and lack of task-specific constraints
- Model struggles with "Legality" frame, showing low precision (0.26) due to co-occurrence confusion with other frames

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Supervised fine-tuning on domain-specific data constrains model predictions more effectively than zero-shot inference for multi-label framing tasks
- **Mechanism**: Fine-tuning XLM-RoBERTa minimizes BCE loss, learning precise decision boundaries for 14 framing dimensions, while GPT-4o relies on broad semantic associations without rigid boundary enforcement
- **Core assumption**: Performance gap is due to lack of task-specific constraints in zero-shot prompting rather than fundamental LLM capability
- **Evidence anchors**: GPT-4o performs significantly worse than fine-tuned XLM-RoBERTa; GPT-4o tends to assign more labels introducing false positives; generic LLMs underperform in specific linguistic settings without adaptation
- **Break condition**: Optimized prompting strategies (few-shot or chain-of-thought) could narrow the performance gap

### Mechanism 2
- **Claim**: Performance improvements in augmented setting suggest multilingual models leverage language-agnostic framing features
- **Mechanism**: XLM-RoBERTa shares parameters across languages; training on combined datasets reinforces shared representations of abstract concepts, improving generalization for Chinese subset via cross-lingual transfer
- **Core assumption**: Chinese frame definitions are sufficiently compatible with SemEval definitions to allow positive transfer
- **Evidence anchors**: F1-micro improved from 0.719 to 0.753 when augmenting with SemEval; maintains similar F1-micro performance while improving Chinese framing detection
- **Break condition**: If Chinese frame definitions diverged semantically from English definitions, negative transfer could occur

### Mechanism 3
- **Claim**: Semantic-guided stratified sampling enables effective training on small, class-balanced datasets
- **Mechanism**: BERTopic and cosine similarity pre-assign likely frames before sampling, maximizing value per annotation and ensuring sufficient support for rare classes
- **Core assumption**: Topic embeddings from BERTopic correlate strongly enough with ground-truth framing labels to act as reliable proxy for stratification
- **Evidence anchors**: Authors ensure class balance through subsampling based on topics; maximization of cosine similarity between topic embeddings and framing category vectors
- **Break condition**: If BERTopic clusters were orthogonal to framing dimensions, sampling would be random and likely yield class imbalance

## Foundational Learning

- **Concept**: **Multi-Label Classification Loss (BCE)**
  - **Why needed here**: Standard cross-entropy assumes single correct class, but news framing is multi-label (articles can be both "Economic" and "Political"); BCE with logits independently penalizes each frame prediction
  - **Quick check question**: Why would a Softmax activation layer fail on this dataset?

- **Concept**: **Cross-Lingual Transfer (Zero-Shot)**
  - **Why needed here**: Evaluates XLM-RoBERTa trained on non-Chinese data against Chinese test sets; understanding shared sub-word vocabularies is key to interpreting 0.584 baseline score
  - **Quick check question**: How does XLM-RoBERTa generate embeddings for Chinese characters it has potentially seen rarely during pre-training?

- **Concept**: **Inter-Annotator Agreement (Krippendorff's Alpha)**
  - **Why needed here**: Paper relies on α of 0.465 to validate dataset quality; understanding this metric accounts for chance agreement in multi-label settings is crucial for judging "Gold Standard" reliability
  - **Quick check question**: Why is Krippendorff's alpha often preferred over simple percent agreement for subjective framing tasks?

## Architecture Onboarding

- **Component map**: Raw news URLs (13 sources) → Scraper → Clean Text (de-duplication, author removal) → BERTopic ($z_i$) + SentenceTransformer ($f_j$) → Cosine Sim → Stratified Sample → GATE Teamware (2 Annotators/Article) → Adjudication → Gold Labels → XLM-RoBERTa-Base (Encoder) → Classifier Head (14 sigmoid outputs)

- **Critical path**: The **Sampling Pipeline** is the bottleneck; if SentenceTransformer embeddings don't align well with 14 framing categories, the annotated set will lack coverage for rare frames (e.g., "Cultural Identity"), rendering the model blind to them

- **Design tradeoffs**: GPT-4o offers rapid deployment but suffers from hallucination/over-labeling; XLM-R requires labeled data and compute for fine-tuning but yields higher precision; dataset is small (353 samples) risking overfitting on high-frequency frames but is high-quality due to manual adjudication

- **Failure signatures**: Co-occurrence over-prediction (model struggles with "Legality" frame, consistently over-predicting its co-occurrence with "Economic" and "Political" frames); language specifics (GPT-4o's lower performance suggests it may struggle with complex character meanings specific to Chinese news discourse)

- **First 3 experiments**:
  1. Reproduce Monolingual Baseline: Fine-tune XLM-R only on provided Chinese training split (233 samples) to verify reported 0.719 F1-micro score
  2. Zero-Shot Probe: Run SemEval-trained model (without Chinese data) on Chinese test set to confirm cross-lingual transfer capability (target: ~0.58 F1)
  3. Error Analysis on "Legality": Isolate test samples with "Legality" frame to determine if low precision (0.26) is due to confusion with "Political" or "Crime and Punishment" frames

## Open Questions the Paper Calls Out

- **Question**: How can individual annotator metadata be utilized to mitigate impact of subjective interpretations in frame classification?
- **Basis in paper**: Explicit - Limitations section states releasing individual annotations allows for further research investigating uncertainty and subjectivity involved in this task
- **Why unresolved**: Authors note subjective interpretations are likely present despite training, but only provided data resource without testing methods to handle this uncertainty
- **What evidence would resolve it**: Experiments demonstrating that weighting training samples by annotator reliability scores improves model robustness or accuracy compared to using majority-vote labels

- **Question**: Does explicitly modeling dependency between news frames improve detection of co-occurring frames?
- **Basis in paper**: Inferred - Error Analysis notes baseline model under-predicts co-occurrence of "Political" frame with others (e.g., "Policy," "Cultural Identity"), failing to capture that "Politics often influences policy... but this is not captured very well"
- **Why unresolved**: Baseline XLM-RoBERTa model apparently struggles with frame dependencies, but no architecture modifications were tested to address this specific failure mode
- **What evidence would resolve it**: Revised model architecture incorporating multi-label dependencies achieving higher F1-scores on currently under-predicted co-occurring pairs

- **Question**: How does distribution of news frames in Chinese media evolve when analyzing articles from time periods prior to 2020?
- **Basis in paper**: Explicit - Limitations section states dataset (2020–2024) does not allow for analysis of news framing over time, allowing for future work to supplement with annotation of articles from different periods
- **Why unresolved**: Current dataset is temporally bounded, preventing any diachronic analysis of how framing in Chinese media may have shifted before COVID-19 era
- **What evidence would resolve it**: Annotation and analysis of supplemented dataset containing pre-2020 articles showing distinct framing trends or shifts over time

## Limitations

- **Dataset Size and Representativeness**: The dataset comprises only 353 articles, which is relatively small for a multi-label classification task with 14 categories, potentially leading to overfitting and limiting generalizability
- **GPT-4o Prompting Strategy**: Exact prompting strategy for zero-shot experiments is not specified beyond temperature setting of 0.0, making it unclear whether optimized prompting could have narrowed the performance gap
- **Annotation Reliability**: Krippendorff's alpha of 0.465 indicates only moderate inter-annotator agreement, suggesting inherent ambiguity in labels that may reflect annotators' interpretation as much as actual framing

## Confidence

- **High Confidence**: Supervised fine-tuning on domain-specific data outperforms zero-shot LLM inference for multi-label framing task (well-supported by experimental results and significant performance gap)
- **Medium Confidence**: Dataset is first of its kind for Chinese news framing and effectively complements SemEval-2023 dataset (plausible given stated novelty and performance gains, but deeper semantic alignment analysis needed)
- **Medium Confidence**: Semantic-guided stratified sampling effectively creates class-balanced dataset (reasonable claim supported by methodology, but lack of direct comparison with random sampling makes exact benefit difficult to quantify)

## Next Checks

1. **Optimized GPT-4o Prompting**: Re-run zero-shot experiments using optimized prompt strategy (detailed chain-of-thought or few-shot examples explicitly defining 14 framing dimensions) and compare results to original to determine if performance gap is due to model limitations or prompting strategy

2. **Cross-Lingual Frame Definition Alignment**: Conduct qualitative and quantitative analysis of framing dimension definitions in Chinese News Framing dataset vs SemEval-2023 dataset through bilingual annotator survey rating semantic similarity of each frame pair and statistical test for model confusion of conceptually similar but differently defined frames

3. **Dataset Size Sensitivity Analysis**: Train models on subsets of varying sizes (100, 200, 300 articles) to determine point at which model's performance plateaus or degrades due to insufficient data, quantifying impact of small size on utility and guiding future data collection