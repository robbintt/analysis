---
ver: rpa2
title: 'Learning to Act Through Contact: A Unified View of Multi-Task Robot Learning'
arxiv_id: '2510.03599'
source_url: https://arxiv.org/abs/2510.03599
tags:
- contact
- policy
- learning
- tasks
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a unified framework for learning multi-task\
  \ locomotion and manipulation policies grounded in contact-explicit representations.\
  \ Instead of designing separate policies for different tasks, the authors define\
  \ tasks through sequences of contact goals\u2014desired contact positions, timings,\
  \ and active end-effectors\u2014enabling a single policy to perform diverse contact-rich\
  \ tasks."
---

# Learning to Act Through Contact: A Unified View of Multi-Task Robot Learning

## Quick Facts
- arXiv ID: 2510.03599
- Source URL: https://arxiv.org/abs/2510.03599
- Reference count: 37
- A unified framework for learning multi-task locomotion and manipulation policies grounded in contact-explicit representations, enabling a single policy to perform diverse contact-rich tasks across multiple robotic embodiments.

## Executive Summary
This paper presents a unified framework for learning multi-task locomotion and manipulation policies grounded in contact-explicit representations. Instead of designing separate policies for different tasks, the authors define tasks through sequences of contact goals—desired contact positions, timings, and active end-effectors—enabling a single policy to perform diverse contact-rich tasks. A goal-conditioned reinforcement learning policy is trained to execute these contact plans across different robotic embodiments including a quadruped (performing multiple gaits like trot, pace, bound, jump, and crawl), a humanoid (performing biped and quadrupedal gaits), and a humanoid for bimanual object manipulation tasks. The results demonstrate that explicit contact reasoning significantly improves generalization to unseen scenarios, with the contact-explicit policy showing better velocity tracking across directions and improved object pose tracking for out-of-distribution shapes and poses compared to task-conditioned alternatives. The approach positions contact-explicit policy learning as a promising foundation for scalable loco-manipulation.

## Method Summary
The method uses a goal-conditioned reinforcement learning policy trained with PPO and a recurrent GRU architecture to execute contact plans defined by target contact positions, timings, and active end-effectors. The policy receives proprioception (joint positions/velocities), current and next contact sequences, contact locations in base frame, command duration, and for manipulation tasks, object states and goal poses. Contact phases (reach, hold, detach) are classified from binary contact indicators and timing, with dense rewards provided for each phase: exponential distance-based rewards for reaching contact locations, sustained bonuses for holding at correct locations, and binary rewards for clean detachment. Additional penalties encourage smooth motion and discourage slippage. The policy is trained across multiple gaits and manipulation tasks simultaneously, with contact goals sampled from diverse distributions to enable generalization.

## Key Results
- Multi-task contact-explicit policy outperforms single-task policies on contact plan adherence across varying command durations
- Contact-explicit policy generalizes to lateral velocities despite only being trained on forward/backward contact goals, while velocity-conditioned policy fails
- Unified contact-explicit representation enables a single policy to perform diverse locomotion gaits (trot, pace, bound, jump, crawl) and manipulation tasks (reorienting objects, bimanual lifting)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explicit contact goal representations enable better out-of-distribution generalization than task-conditioned alternatives.
- **Mechanism:** By decomposing tasks into contact primitives (target locations, timings, active end-effectors), the policy learns a shared physical interaction vocabulary. This abstracts away task-specific syntax (e.g., "trot" vs "pace") into common contact patterns, allowing interpolation to unseen contact configurations that were not explicitly rewarded during training.
- **Core assumption:** Contact patterns share underlying motor structure across locomotion and manipulation tasks.
- **Evidence anchors:**
  - [abstract] "explicit contact reasoning significantly improves generalisation to unseen scenarios"
  - [section IV, Fig 5] Contact-explicit policy generalizes to lateral velocities despite only being trained on forward/backward contact goals, while velocity-conditioned policy fails.
  - [corpus] Related work on contact-conditioned locomotion (Ciebielski & Khadiv, 2024) shows similar generalization benefits in behavioral cloning settings, but this paper extends to RL and manipulation.
- **Break condition:** If contact goals become too sparse or high-level (e.g., only final object pose without intermediate contacts), the policy loses the dense supervision needed for learning.

### Mechanism 2
- **Claim:** Multi-task training with contact-explicit representations outperforms single-task policies on contact plan adherence.
- **Mechanism:** Training a single policy across multiple gaits exposes the learner to diverse contact mode transitions, forcing it to learn robust contact timing and sequencing strategies. Single-task policies overfit to narrow contact patterns and fail when command durations deviate from training distribution.
- **Core assumption:** Multiple tasks provide complementary coverage of the contact mode space.
- **Evidence anchors:**
  - [section IV, Fig 6] Multi-gait policy (M) shows lower Hamming distance (contact plan deviation) than single-gait policies (S) across all command durations, including out-of-distribution durations [0.2s, 0.9s] vs training [0.34s, 0.36s].
  - [section III-A] "by composing several different contact goals, we can perform various long-horizon tasks"
  - [corpus] MLM framework (2025) shows multi-task loco-manipulation for quadrupeds but uses separate task encodings rather than unified contact representation.
- **Break condition:** If tasks have fundamentally incompatible contact dynamics (e.g., high-frequency hopping vs slow precision manipulation), shared policy capacity may be insufficient.

### Mechanism 3
- **Claim:** Phase-based reward decomposition (reach, hold, detach) provides denser learning signal than sparse contact-only rewards.
- **Mechanism:** Rather than rewarding only successful contact maintenance, the framework provides continuous rewards during each phase: exponential distance-based rewards for reaching, sustained bonuses for holding at correct locations, and binary rewards for clean detachment. This reduces credit assignment difficulty across contact mode switches.
- **Core assumption:** Contact phases can be reliably determined from binary contact indicators and timing thresholds.
- **Evidence anchors:**
  - [section III-B, equations 1-3] Explicit reward formulations for r_reach, r_hold, r_detach with dense distance-based terms.
  - [section II] "different from [11], we present a denser reward for contact that facilitates the training procedure and qualitatively produces smoother motions"
  - [corpus] WoCoCo (Zhang et al., 2024) uses sparse contact-based rewards combined with task-specific rewards; this paper shows dense contact rewards alone suffice.
- **Break condition:** If contact detection is unreliable (sim-to-real gap in contact sensing), phase misclassification can corrupt reward signals.

## Foundational Learning

- **Goal-Conditioned Reinforcement Learning:**
  - Why needed here: The policy must map from contact goals + robot state → joint actions, requiring conditioning on varying goal inputs rather than fixed task objectives.
  - Quick check question: Can you explain how goal-conditioned RL differs from multi-task RL with discrete task indices?

- **Contact Dynamics and Hybrid Systems:**
  - Why needed here: Understanding that contact mode switches (stance → swing) create discontinuities in system dynamics is essential for interpreting why contact-explicit representations help.
  - Quick check question: Why do contact events make standard continuous dynamics models insufficient for legged locomotion?

- **Proximal Policy Optimization (PPO) with Recurrent Architectures:**
  - Why needed here: The paper uses PPO with GRU for policy training; understanding temporal credit assignment and recurrent state is necessary for debugging training issues.
  - Quick check question: What advantage does a recurrent policy provide over MLP policies for contact sequence tracking?

## Architecture Onboarding

- **Component map:** High-Level Planner → Contact Goals (p_con, I_con, S) + Object Pose Goals → Goal-Conditioned RL Policy (GRU-based, PPO-trained) → Joint Torque Commands → Reward Computation (r_reach, r_hold, r_detach, r_pose)

- **Critical path:** Contact goal generation → phase classification (reach/hold/detach) → phase-specific reward computation → policy gradient update. Errors in phase classification propagate directly to reward quality.

- **Design tradeoffs:**
  - Command duration sampling: Narrow range [0.34s, 0.36s] during training improves convergence but requires robustness to broader ranges at test time.
  - Contact sensing: Paper notes contact sensing on feet did not improve simulation performance, but this may not hold for real-world deployment.
  - Horizon of contact goals: Two contact switches provided; longer horizons may improve planning but increase observation complexity.

- **Failure signatures:**
  - High Hamming distance on contact plan: Policy not learning correct contact timing; check command duration encoding and phase reward magnitudes.
  - Poor lateral generalization (Fig 5 pattern): Policy may be overfitting to forward-biased contact locations; increase lateral offset sampling during training.
  - Object slip during manipulation: Hold reward insufficient; increase α_hold weight or add slip detection.

- **First 3 experiments:**
  1. **Single-gait contact tracking:** Train policy on trot gait only with contact goals. Verify tracking error < 3cm matches paper baseline before attempting multi-gait.
  2. **Multi-gait interpolation:** Add pace and bound contact patterns. Plot Hamming distance vs command duration to verify multi-gait policy outperforms single-gait at out-of-distribution durations.
  3. **Unseen velocity generalization:** Train contact-explicit policy on forward-only contact goals (vx ∈ [-0.65, 0.65]), evaluate on lateral velocities (vy ≠ 0). Compare against velocity-conditioned baseline to replicate Fig 5.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can a learned high-level planner autonomously generate contact goals that enable long-horizon loco-manipulation in complex environments?
  - Basis in paper: [explicit] The authors state: "In this work, we have prespecified the contact goals required to achieve the various tasks. However, our method can be integrated with more sophisticated learned contact planners" and "Moving forward, we aim to extend this framework to hierarchical reinforcement learning by coupling our low-level contact-conditioned policy with a learned high-level planner."
  - Why unresolved: The current framework assumes contact goals are provided by an external source; the policy only learns low-level execution, not contact planning.
  - What evidence would resolve it: Demonstration of an end-to-end system where a learned planner generates contact goals for novel long-horizon tasks (e.g., navigating obstacle courses while manipulating objects) without prespecified contact sequences.

- **Open Question 2:** Would off-policy RL algorithms with goal relabeling improve sample efficiency and performance compared to the current PPO-based approach?
  - Basis in paper: [explicit] In the Training section: "In the future, we'd also like to explore off-policy RL algorithms to make use of goal relabling."
  - Why unresolved: PPO (on-policy) was chosen for effectiveness in learning motion primitives, but off-policy methods may better leverage the goal-conditioned structure through hindsight experience replay.
  - What evidence would resolve it: Comparative experiments showing training curves, final performance, and sample complexity between PPO and off-policy methods (e.g., SAC with HER) on the same contact-explicit tasks.

- **Open Question 3:** Can the contact-explicit representation scale to prehensile manipulation requiring grasp planning and force-sensitive object interactions?
  - Basis in paper: [explicit] The authors state: "We also plan to explore prehensile interactions" and current manipulation experiments only cover non-prehensile tasks (reorienting on table, lifting without grasping).
  - Why unresolved: Prehensile manipulation introduces additional complexity (grasp stability, force distribution, finger coordination) not addressed by the current contact goal representation.
  - What evidence would resolve it: Extension of the framework to dexterous hand manipulation tasks (e.g., tool use, in-hand reorientation) using the same contact-explicit policy structure.

## Limitations

- The framework requires careful engineering of contact goals and phase detection, making it less plug-and-play than task-conditioned alternatives.
- The absence of contact sensing in simulation represents a significant gap between simulated and real-world deployment.
- The claim that explicit contact reasoning enables generalization is primarily supported by single-direction experiments and would benefit from broader validation.

## Confidence

**High Confidence:** The multi-gait training results showing improved contact plan adherence (Hamming distance) compared to single-task policies are well-supported by quantitative metrics and controlled experiments. The contact phase reward decomposition appears technically sound.

**Medium Confidence:** The generalization claims (especially lateral velocity tracking) are supported by experimental evidence but rely on a narrow test case. The mechanism by which contact-explicit representations enable generalization is plausible but not fully proven across diverse scenarios.

**Low Confidence:** The scalability claims for complex manipulation tasks with multiple end-effectors are primarily demonstrated through qualitative examples. The real-world applicability remains speculative given the lack of contact sensing in simulation and limited discussion of sim-to-real transfer challenges.

## Next Checks

1. **Multi-Directional Generalization Test:** Extend the lateral velocity generalization experiment to include diagonal and arbitrary direction commands (vx, vy combinations). Measure contact plan adherence and velocity tracking across the full 2D velocity space to validate whether contact-explicit reasoning provides systematic generalization benefits.

2. **Contact Sensing Ablation Study:** Train and evaluate policies with and without simulated contact sensing on the feet. Compare contact plan adherence, object manipulation accuracy, and robustness to terrain variations to quantify the actual impact of contact sensing on performance.

3. **Longer Horizon Planning Evaluation:** Modify the framework to support 3-4 contact switches per command. Evaluate whether the policy maintains contact plan adherence and velocity tracking quality as planning horizon increases, and whether contact-explicit representations provide particular benefits for longer-term planning.