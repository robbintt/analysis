---
ver: rpa2
title: Distribution Matching for Graph Quantification Under Structural Covariate Shift
arxiv_id: '2601.00864'
source_url: https://arxiv.org/abs/2601.00864
tags:
- kdey
- shift
- quantification
- covariate
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses quantification learning under structural covariate
  shift in graph data, where the training and test data come from different regions
  of the graph. Standard quantification methods like KDEy assume prior probability
  shift and perform poorly under such conditions.
---

# Distribution Matching for Graph Quantification Under Structural Covariate Shift

## Quick Facts
- **arXiv ID:** 2601.00864
- **Source URL:** https://arxiv.org/abs/2601.00864
- **Reference count:** 40
- **Primary result:** KDEy with Structural Importance Sampling (SIS) significantly outperforms standard KDEy, PACC, and PCC on graph quantification under structural covariate shift.

## Executive Summary
This paper addresses quantification learning under structural covariate shift in graph data, where training and test data come from different regions of the graph. Standard quantification methods like KDEy assume prior probability shift and perform poorly under such conditions. The authors extend structural importance sampling (SIS) to the KDEy framework by weighting training instances based on their proximity to test instances using a graph-based kernel (PPR or shortest path). This approach adapts KDEy to structural covariate shift. Evaluation on five graph datasets with citation and co-purchasing networks shows that KDEy with SIS significantly outperforms standard KDEy, PACC, and PCC across different types of distribution shift (prior probability shift, random walk-based, and BFS-based covariate shift). The best performance is achieved with the interpolated PPR kernel (λ=0.9). Overall, KDEy with SIS ranks first on average across all experiments, demonstrating its effectiveness for quantification under structural covariate shift.

## Method Summary
The method extends KDEy (Kernel Density Estimation for Quantification) by incorporating Structural Importance Sampling (SIS) weights. The SIS weights are computed using a graph-based kernel (PPR or shortest path) that estimates the density ratio between test and training distributions. These weights are then used in the KDEy optimization to estimate test prevalence. The key innovation is adapting importance sampling to graph structures, allowing KDEy to handle structural covariate shift where training and test nodes come from different graph regions.

## Key Results
- KDEy with SIS significantly outperforms standard KDEy, PACC, and PCC across all tested distribution shift types
- The interpolated PPR kernel with λ=0.9 achieves the best overall performance
- KDEy with SIS ranks first on average across all experiments on five graph datasets
- Performance improvements are most pronounced under structural covariate shift compared to prior probability shift

## Why This Works (Mechanism)

### Mechanism 1: Structural Importance Sampling (SIS) Recovers Test-Conditional Distributions
Weighting training instances by their graph proximity to test instances approximates Q(X|Y) from P(X|Y) under structural covariate shift. The density ratio ρ_X(x) = q_X(x)/p_X(x) is estimated via kernel density estimation using graph-based kernels (PPR or shortest path). This ratio reweights class-conditional distributions so that training instances closer to test instances contribute more to the quantification estimate.

### Mechanism 2: Weighted KDE Substitutes Q(Ŝ|Y) for P(Ŝ|Y) in Distribution Matching
Replacing standard class-conditional KDE estimates with SIS-weighted versions allows the mixture model to match the true test distribution. Standard KDEy solves ̂q(ŝ) = Σ q(i)·p(ŝ|i). Under covariate shift, p(ŝ|i) ≠ q(ŝ|i). SIS computes ̂q(ŝ|y) = (Σ k(ŝ, h_s(x))·ρ̂_X(x)) / (Σ ρ̂_X(x)), effectively estimating the test-conditional distribution directly.

### Mechanism 3: Interpolated PPR Kernel Balances Locality Against Effective Sample Size
Interpolation κ_λ(x,x') = λ·κ_PPR(x,x') + (1-λ) prevents zero-weight scenarios on disconnected graphs while retaining structural bias. λ=1.0 assigns zero weight to disconnected components; λ<1 guarantees minimum weight for all training vertices. Experiments show λ=0.9 achieves optimal tradeoff.

## Foundational Learning

- **Quantification Learning vs. Classification**: Quantification estimates label prevalence Q(Y), not individual predictions; understanding this distinction clarifies why biased classifiers can still yield accurate quantification. Quick check: If a classifier has FP=FN on a test set, does it produce perfect quantification? (Answer: Yes, errors cancel in prevalence estimation.)

- **Distribution Shift Taxonomy (PPS vs. Covariate vs. Concept)**: The method is motivated by PPS assumption failure under structural covariate shift; knowing when each assumption applies is critical for method selection. Quick check: Under covariate shift, which quantity remains invariant across train and test? (Answer: P(Y|X) = Q(Y|X).)

- **Importance Sampling and Density Ratio Estimation**: SIS is fundamentally an importance sampling technique; understanding how ρ = q/p enables reweighting is essential to follow the mathematical derivations. Quick check: Why is the density ratio ρ_X(x) easier to estimate than directly estimating q_X(x)? (Answer: Ratio estimation can avoid normalizing constants; kernel-based methods estimate both densities from samples.)

## Architecture Onboarding

- **Component map**: Soft classifier (h_s) -> Graph kernel (κ) -> Density ratio estimator (ρ̂_X) -> Weighted KDE module -> Diversity minimizer
- **Critical path**: 1. Train soft classifier on D_L → obtain h_s 2. Compute PPR/SP kernel matrix between all train and test vertices 3. Estimate ρ̂_X(x) for each training vertex via kernel density ratio 4. Compute weighted class-conditional KDE estimates using ρ̂_X 5. Optimize q via constrained gradient descent minimizing KLD
- **Design tradeoffs**: Kernel choice (PPR vs SP), λ interpolation (0.9 optimal), divergence ℓ (KLD tractable), bandwidth σ (cross-validation)
- **Failure signatures**: Quantification error spikes → check disconnected components, reduce λ; SIS underperforms → verify train/test overlap; PPR gives zero weights → graph disconnected, use λ<1; Optimization fails → check classifier calibration
- **First 3 experiments**: 1. Sanity check: Reproduce PPS baseline comparing KDEy vs KDEy+SIS 2. Structural shift validation: Create RW-sampled test sets, verify KDEy+SIS outperforms standard KDEy by >10% relative AE reduction 3. Ablation on λ: Sweep λ∈{0.5, 0.9, 1.0} on CiteSeer (disconnected) vs Amazon Computers (connected)

## Open Questions the Paper Calls Out

- Can SIS be effectively adapted for quantification tasks in non-graph domains subject to covariate shift, such as time series or geospatial data?
- How can the choice of the kernel κ be optimized or automated to balance the trade-off between localization and effective sample size?
- Can non-aggregative quantification approaches be extended to handle structural covariate shift?

## Limitations

- Graph kernel effectiveness varies by shift type; PPR shows robustness but theoretical justification for kernel choice remains limited
- Bandwidth sensitivity of Gaussian kernel not systematically analyzed across different graph densities
- λ=0.9 works empirically but lacks theoretical bounds on how it should scale with graph connectivity measures

## Confidence

- **High confidence**: SIS effectively recovers test-conditional distributions under structural covariate shift when graph proximity reflects sampling process
- **Medium confidence**: Interpolated PPR kernel (λ=0.9) represents optimal balance for general graphs
- **Low confidence**: Claim that PPR kernel is "robust" to different shift types requires more rigorous testing

## Next Checks

1. Systematically vary λ across graphs with known component counts to establish relationship between λ and graph connectivity metrics
2. Create synthetic graphs with controlled sampling biases to isolate when PPR outperforms SP and why
3. Perform ablation studies varying Gaussian kernel bandwidth σ across orders of magnitude to quantify its effect on SIS weight stability and quantification accuracy