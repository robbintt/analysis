---
ver: rpa2
title: Automata Learning of Preferences over Temporal Logic Formulas from Pairwise
  Comparisons
arxiv_id: '2505.18030'
source_url: https://arxiv.org/abs/2505.18030
tags:
- pdfa
- sample
- algorithm
- preference
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning user preferences over
  temporal logic formulas from pairwise comparisons. The authors formalize this as
  learning a Preference Deterministic Finite Automaton (PDFA) that encodes a preorder
  over regular languages.
---

# Automata Learning of Preferences over Temporal Logic Formulas from Pairwise Comparisons

## Quick Facts
- arXiv ID: 2505.18030
- Source URL: https://arxiv.org/abs/2505.18030
- Reference count: 40
- Primary result: Learning a Preference Deterministic Finite Automaton (PDFA) from pairwise comparisons is NP-Complete, with an algorithm that learns minimal consistent automata from characteristic samples

## Executive Summary
This paper introduces a novel approach to learning user preferences over temporal logic formulas through pairwise comparisons. The authors formalize this as learning a Preference Deterministic Finite Automaton (PDFA) that encodes a preorder over regular languages. They prove that the learning problem is NP-Complete and develop an algorithm that learns the minimal PDFA consistent with a characteristic sample. The algorithm extends the RPNI state-merging approach by incorporating preference information through partial orders over ranks. Experimental results on a robotic motion planning problem demonstrate the algorithm's correctness: when given characteristic samples, it learns the true PDFA; even with non-characteristic samples, it often recovers the correct model. The approach enables learning complex temporal preferences from human feedback, which is valuable for human-AI alignment in sequential decision-making tasks.

## Method Summary
The paper proposes a passive learning algorithm that extends the RPNI (Regular Positive and Negative Inference) state-merging framework to handle preference information encoded as partial orders over ranks. The algorithm operates on a characteristic sample - a set of labeled words where each state in the target PDFA appears as the endpoint of at least one word in each rank. The method works by building an initial hypothesis automaton from the sample and then merging states that are consistent with the preference preorder. The consistency check involves verifying that the merge respects the partial order constraints: if one state is ranked higher than another, they cannot be merged unless all constraints are preserved. The algorithm outputs the minimal PDFA consistent with the input sample, leveraging the fact that the characteristic sample size is linear in the number of states.

## Key Results
- Proves that learning a minimal PDFA from a characteristic sample is NP-Complete
- Demonstrates that the proposed algorithm correctly learns the true PDFA when given characteristic samples
- Shows empirical success on a robotic motion planning task with 8 states and 7 accepting states
- Establishes that characteristic samples of size O(nÂ²) are sufficient for learning an n-state PDFA

## Why This Works (Mechanism)
The algorithm works by leveraging the structure of characteristic samples, which guarantee that each state in the target PDFA is represented by at least one word in each rank. This allows the state-merging process to distinguish between states based on their rank relationships. The partial order constraints encoded in the PDFA structure provide a mechanism to verify whether proposed state merges are consistent with the preference information. By systematically checking all possible merges against these constraints, the algorithm can identify the minimal automaton that preserves the preference preorder while minimizing the number of states.

## Foundational Learning

1. **Preference Deterministic Finite Automaton (PDFA)**: A deterministic finite automaton where each state has a rank from a partially ordered set, encoding preferences over regular languages. Needed to formally represent user preferences over temporal logic formulas. Quick check: Can verify that the automaton correctly accepts/rejects strings and respects the partial order constraints.

2. **Characteristic Sample**: A set of labeled words where each state in the target PDFA appears as the endpoint of at least one word in each rank. Needed to guarantee that the learning algorithm can distinguish all relevant states. Quick check: Can verify that every state is represented in each rank class by examining the sample.

3. **State-merging algorithms**: Learning techniques that start with a hypothesis automaton and iteratively merge states while preserving language properties. Needed as the core inference mechanism. Quick check: Can verify that merges preserve the accepted language and preference constraints.

4. **Partial order constraints**: Mathematical structure used to encode preference relationships between states. Needed to ensure the learned automaton respects user preferences. Quick check: Can verify that the partial order is consistent and properly constrains state merges.

5. **Preorder over regular languages**: A reflexive and transitive binary relation used to model preferences. Needed as the mathematical foundation for preference representation. Quick check: Can verify that the relation is reflexive and transitive.

## Architecture Onboarding

Component map: Sample -> Initial Automaton -> State Merging -> Minimal PDFA

Critical path: The algorithm's correctness depends critically on having a characteristic sample that includes representatives for each state in each rank. Without this, the state-merging process cannot distinguish between states that should be separate.

Design tradeoffs: The choice between passive learning (batch processing) versus active learning (querying human for specific comparisons) represents a fundamental tradeoff between sample efficiency and annotation burden. The algorithm prioritizes correctness guarantees over sample efficiency.

Failure signatures: If the sample is not characteristic, the algorithm may merge states that should be separate, resulting in an incorrect preference preorder. This manifests as accepting/rejecting strings incorrectly or violating preference constraints.

First experiments:
1. Verify the algorithm correctly learns a simple 2-3 state PDFA from a characteristic sample with clear preference relationships
2. Test the algorithm's behavior when given non-characteristic samples to understand failure modes
3. Evaluate the algorithm's performance on increasingly complex temporal logic formulas with nested operators

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can an active learning algorithm strategically select word pairs for pairwise comparison queries to minimize the number of human annotations required while guaranteeing identification of the canonical PDFA?
- Basis in paper: [explicit] "Future work will explore an active learning of preference, where the learner can query a limited number of words and solicit human feedback on pairwise comparisons of specifically selected pairs based on the current dataset. In this context, the learner must strategically select pairs to minimize the number of queries required from the human."
- Why unresolved: The current algorithm is a passive batch learner with no mechanism for query selection or sample efficiency.
- What evidence would resolve it: An algorithm with provable query complexity bounds that achieves correct identification with fewer comparisons than random sampling.

### Open Question 2
- Question: How can the learning algorithm be made incremental, updating the learned PDFA model efficiently as new pairwise comparisons arrive without restarting from scratch?
- Basis in paper: [explicit] "Incremental learning algorithms could also be desired, where the learner receives data sequentially and incrementally updates the learned model with newly seen data."
- Why unresolved: The state-merging approach requires reprocessing the entire sample when new data arrives.
- What evidence would resolve it: An incremental variant with complexity depending only on new data size rather than total sample size.

### Open Question 3
- Question: How robust is the algorithm to noisy or inconsistent human feedback, where comparisons may violate transitivity or contain errors?
- Basis in paper: [inferred] The paper assumes samples are drawn from a consistent underlying PDFA, but human preferences may be noisy, inconsistent, or evolve over time. The characteristic sample definition requires consistency with a true preorder.
- Why unresolved: Real human feedback often contains errors or temporary inconsistencies that violate preorder axioms.
- What evidence would resolve it: Modified algorithm with probabilistic guarantees under bounded noise models, or empirical robustness analysis with synthetic noise injection.

### Open Question 4
- Question: Can preference learning be integrated with online planning to enable sample-efficient learning of control strategies directly from human feedback?
- Basis in paper: [explicit] "Additionally, future research will investigate integrating preference learning with preference-based decision-making in online planning setups."
- Why unresolved: The current work separates learning (offline) from planning, without closing the loop between execution and preference elicitation.
- What evidence would resolve it: A unified framework where learned preferences directly guide planning while new trajectories generate feedback for preference refinement.

## Limitations
- The algorithm requires characteristic samples, which may be difficult to obtain in practice without prior knowledge of the target automaton
- Computational complexity is exponential in the worst case due to the NP-Completeness of the learning problem
- The approach assumes static preferences, but human preferences may evolve over time or be context-dependent
- Limited scalability demonstrated, with experiments only on relatively small automata (8 states)

## Confidence

High confidence in:
- Theoretical framework and NP-Completeness proof
- Algorithm correctness when given characteristic samples
- Formal definitions and mathematical foundations

Medium confidence in:
- Empirical results on the robotic motion planning task
- Sample complexity bounds for characteristic samples
- Performance on non-characteristic samples

Low confidence in:
- Scalability to large automata with many states
- Robustness to noisy or inconsistent human feedback
- Generalizability across diverse application domains

## Next Checks

1. Test algorithm performance on larger automata with more states and complex temporal logic formulas to assess scalability
2. Evaluate robustness under noisy or inconsistent pairwise comparisons by introducing controlled perturbations in preference labels
3. Validate across multiple application domains beyond robotics (e.g., natural language processing, planning in complex environments) to assess generalizability