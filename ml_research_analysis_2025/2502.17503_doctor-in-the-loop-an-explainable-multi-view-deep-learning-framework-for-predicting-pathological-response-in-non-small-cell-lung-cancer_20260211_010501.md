---
ver: rpa2
title: 'Doctor-in-the-Loop: An Explainable, Multi-View Deep Learning Framework for
  Predicting Pathological Response in Non-Small Cell Lung Cancer'
arxiv_id: '2502.17503'
source_url: https://arxiv.org/abs/2502.17503
tags:
- doctor-in-the-loop
- lung
- training
- focus
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of predicting pathological response
  (pR) in non-small cell lung cancer (NSCLC) patients undergoing neoadjuvant therapy,
  aiming to guide personalized treatment strategies. The core method, Doctor-in-the-Loop,
  integrates expert-driven domain knowledge with intrinsic explainability techniques
  through a gradual multi-view deep learning framework, progressively refining model
  focus from global to lesion-specific regions using segmentation masks.
---

# Doctor-in-the-Loop: An Explainable, Multi-View Deep Learning Framework for Predicting Pathological Response in Non-Small Cell Lung Cancer

## Quick Facts
- arXiv ID: 2502.17503
- Source URL: https://arxiv.org/abs/2502.17503
- Reference count: 26
- One-line primary result: AUC of 62.93% on 64 NSCLC patients, outperforming state-of-the-art methods

## Executive Summary
This paper introduces Doctor-in-the-Loop, a deep learning framework for predicting pathological response (pR) in non-small cell lung cancer (NSCLC) patients undergoing neoadjuvant therapy. The method integrates expert-driven domain knowledge with intrinsic explainability techniques through a gradual multi-view training strategy. By progressively refining model focus from global anatomy to lesion-specific regions using segmentation masks, the approach aims to improve prediction accuracy while maintaining clinical interpretability. Evaluated on a dataset of 64 patients, the framework achieved an AUC of 62.93%, demonstrating superior performance over standard deep learning approaches and ablation studies.

## Method Summary
The Doctor-in-the-Loop framework employs a three-step gradual learning process using 3D DenseNet169 as the backbone architecture. In Step 1, the model trains on global CT images using only classification loss. Step 2 introduces lung segmentation masks and an explainability loss that aligns model attention with clinical regions of interest. Step 3 refines the model using lesion (PTV) masks with the full composite loss. The explainability loss is computed as MSE between Grad-CAM heatmaps and expert-provided segmentation masks, forcing the model to focus on clinically relevant anatomical regions while maintaining predictive accuracy.

## Key Results
- Achieved AUC of 62.93% for pathological response prediction
- Outperformed standard state-of-the-art methods and ablation studies
- Maintained high TPR (92.31%) while achieving TNR of 44.58%
- Showed statistically significant improvement over single-view approaches

## Why This Works (Mechanism)

### Mechanism 1: Intrinsic Attention Alignment via XAI Loss
The framework integrates an explainability loss directly into training, forcing the model to prioritize features within clinically annotated regions. The composite loss function $L = L_{cls} + \lambda L_{xai}$ uses MSE between Grad-CAM heatmaps and expert masks to penalize attention outside relevant regions. This reduces spurious correlations common in high-dimensional medical imaging. The core assumption is that gradients through Grad-CAM computation are sufficient to alter backbone feature extraction weights.

### Mechanism 2: Gradual Context Refinement (Curriculum Strategy)
A three-stage training progression from global context to local lesion details improves convergence on small datasets. The sequential training (global → lung → lesion) prevents immediate overfitting to limited variance in small region-of-interest inputs. Features learned from global anatomical structure provide contextual prior that supports final discrimination at the lesion level. The assumption is that global features offer necessary contextual support for the lesion-level task.

### Mechanism 3: Explicit Clinical Knowledge Injection
Hard-coding clinical focus via segmentation masks bridges the gap between data-driven learning and clinical reasoning. Instead of relying on the network to discover lesions autonomously, expert masks are fed into the loss calculation, reducing the effective hypothesis space to clinically meaningful solutions. The assumption is that predictive signal for pR is stronger within provided segmentation masks than outside them.

## Foundational Learning

- **Concept: Grad-CAM (Gradient-weighted Class Activation Mapping)**
  - Why needed here: Grad-CAM is the mathematical core used not just for visualization but as a differentiable layer within the loss function to guide training
  - Quick check question: Can you explain why Grad-CAM is differentiable with respect to model parameters, allowing use in a loss function for backpropagation?

- **Concept: Curriculum Learning**
  - Why needed here: The gradual learning process (Global → Lung → Lesion) requires understanding curriculum learning to diagnose why immediate lesion training causes overfitting versus stable gradual convergence
  - Quick check question: How does entropy of the loss landscape change when moving from coarse (global) to fine (lesion) views?

- **Concept: Multi-View Learning in Medical Imaging**
  - Why needed here: The framework requires understanding how to fuse or sequence inputs of different scales (global anatomy vs. local pathology)
  - Quick check question: What are the risks of transfer learning between steps in a multi-view pipeline (e.g., catastrophic forgetting of global context)?

## Architecture Onboarding

- **Component map:**
  - Input CT volume (324×324) → 3D DenseNet169 → Feature Maps + Logits → Grad-CAM Heatmap → Composite Loss (L_cls + λL_xai) → Updated weights

- **Critical path:**
  1. Input: 3D CT volume (324×324)
  2. Forward Pass: Generate Logits and Feature Maps
  3. Heatmap Generation: Compute Grad-CAM using gradients of true class logit w.r.t feature maps
  4. Loss Computation: Calculate L_cls and L_xai
  5. Backprop: Update DenseNet weights using composite loss

- **Design tradeoffs:**
  - λ (Loss Weight): High λ forces perfect mask focus but may ignore subtle textural features; low λ ignores doctor guidance
  - Mask Granularity: PTV includes margins for setup errors, providing data augmentation/regularization for attention maps

- **Failure signatures:**
  - High Validation Loss, Low L_xai: Model looks at right spot but cannot distinguish responders; increase model capacity or check label noise
  - High L_xai, High L_cls: Model finds predictive features outside expert mask (ignoring doctor); inspect false positives or increase λ
  - Performance Drop in Step 3: Focus too narrow; reduce learning rate or revert to Step 2 weights

- **First 3 experiments:**
  1. Baseline Validation: Train DenseNet169 on lesion crop only using L_cls to establish performance floor
  2. Ablation (XAI-only): Train on global image with L_xai active immediately, skipping gradual steps
  3. Sensitivity Analysis (λ): Run full pipeline with λ ∈ [0.1, 1.0, 2.0] to observe AUC vs Dice score trade-off

## Open Questions the Paper Calls Out
- Can automated segmentation replace physician-provided masks without degrading predictive performance or explainability fidelity?
- Does the framework generalize to multi-center, heterogeneous NSCLC populations with varying imaging protocols?
- What is the optimal number and granularity of progressive views in the gradual learning curriculum?

## Limitations
- Small sample size (64 patients) from single institution limits external validity
- Performance heavily depends on accuracy and consistency of expert-provided segmentation masks
- Unclear protocol for weight transfer between training steps in gradual learning process

## Confidence
- **High Confidence:** Integration of XAI loss into training objective is clearly defined and implemented
- **Medium Confidence:** Reported AUC of 62.93% outperforms baselines, but absolute value and clinical significance uncertain due to small sample size
- **Low Confidence:** "State-of-the-art" claim difficult to verify without broader comparison to published studies

## Next Checks
1. External Validation: Test trained model on independent, multi-institutional NSCLC dataset to assess generalizability
2. Mask Ablation Study: Systematically vary segmentation mask quality/accuracy to quantify impact on performance
3. Clinical Utility Assessment: Conduct reader study with radiologists to evaluate if Grad-CAM explanations improve clinical decision-making