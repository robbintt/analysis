---
ver: rpa2
title: 'Investigating Structural Pruning and Recovery Techniques for Compressing Multimodal
  Large Language Models: An Empirical Study'
arxiv_id: '2507.20749'
source_url: https://arxiv.org/abs/2507.20749
tags:
- pruning
- compression
- performance
- recovery
- finetuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates how to compress Multimodal Large Language\
  \ Models (MLLMs) through structured pruning and recovery training, aiming to reduce\
  \ computational costs while maintaining performance. The authors explore two pruning\
  \ paradigms\u2014layerwise and widthwise\u2014applied to the language model backbone,\
  \ paired with recovery strategies such as supervised finetuning and knowledge distillation."
---

# Investigating Structural Pruning and Recovery Techniques for Compressing Multimodal Large Language Models: An Empirical Study

## Quick Facts
- **arXiv ID**: 2507.20749
- **Source URL**: https://arxiv.org/abs/2507.20749
- **Reference count**: 40
- **Primary result**: Widthwise pruning is most effective without recovery training; hidden-state matching distillation achieves highest performance recovery across all compression levels.

## Executive Summary
This paper investigates structured pruning and recovery techniques for compressing Multimodal Large Language Models (MLLMs), focusing on layerwise and widthwise pruning paradigms applied to the language model backbone. The authors find that widthwise pruning preserves more performance in low-resource scenarios without recovery training, while layerwise pruning with recovery training performs better at moderate compression ratios (<40%). Finetuning only the multimodal projector is sufficient for small compression ratios (<20%), but larger ratios benefit from joint finetuning of both the projector and LLM. Knowledge distillation, particularly with hidden-state matching, consistently achieves the highest performance recovery. The study demonstrates that even with minimal training data (5% of original), over 95% of the model's performance can be retained at moderate compression levels.

## Method Summary
The authors employ two pruning strategies: layerwise pruning using Block Influence scores based on hidden state differences, and widthwise pruning using dependency-aware structural pruning with Taylor expansion importance scores. Recovery training includes supervised finetuning (SFT) with cross-entropy loss and knowledge distillation (KD) using KL divergence, reverse KL, and L2 hidden-state matching. The multimodal projector is finetuned alone for compression ratios <20%, and jointly with the LLM for higher ratios. Experiments use LLaVA-v1.5-7B and Bunny-v1.0-3B models evaluated on visual question-answering benchmarks including GQA, SQA-I, POPE, MME-Cognition, MME-Perception, and MMMU.

## Key Results
- Widthwise pruning consistently preserves more accuracy than layerwise pruning without recovery training, retaining 95% of baseline performance on Bunny and 93% on LLaVA at 15% compression
- Finetuning only the multimodal projector achieves results comparable to jointly finetuning the LLM at compression ratios <20%
- Hidden-state matching distillation combined with supervised finetuning yields optimal performance recovery across all compression levels
- With only 5% of original training data, over 95% of model performance can be retained at moderate compression levels (<30%)

## Why This Works (Mechanism)

### Mechanism 1: Widthwise Pruning Preserves Information Flow in Low-Resource Settings
- Claim: Widthwise pruning retains more performance than layerwise pruning without recovery training, particularly at compression ratios <30%
- Mechanism: By removing less important attention heads and MLP neurons within each layer rather than entire layers, the overall transformer depth and residual connections remain intact, preserving information flow through the network
- Core assumption: Importance scores derived from gradient-based Taylor expansion on calibration data accurately identify truly redundant components
- Evidence anchors: Abstract states "Widthwise pruning is more effective in low-resource scenarios without recovery"; Section 4.1 shows widthwise pruning preserves 95% of baseline performance at 15% compression; corpus work on layer pruning limitations supports depth preservation arguments

### Mechanism 2: Projector-Only Finetuning Recovers Cross-Modal Alignment
- Claim: For compression ratios <20%, finetuning only the multimodal projector recovers ~95%+ of original performance
- Mechanism: Pruning the LLM backbone shifts the latent space distribution of hidden states that the projector maps to, creating misalignment with the unchanged vision encoder outputs; retraining the projector adapts the linear projection to remap visual features to the compressed LLM's altered hidden space
- Core assumption: The pruned LLM retains sufficient language modeling capacity; the primary degradation is from modality misalignment rather than lost linguistic reasoning
- Evidence anchors: Abstract states "Finetuning only the multimodal projector is sufficient at small compression levels (<20%)"; Section 4.2 confirms projector-only achieves comparable results to joint finetuning at <20% compression

### Mechanism 3: Hidden-State Matching Distillation Transfers Feature Representations
- Claim: Supervised finetuning combined with L2 hidden-state distillation from the final layer yields the highest performance recovery across all compression ratios
- Mechanism: Hidden-state matching directly aligns the student's intermediate representations with the teacher's, transferring learned feature hierarchies rather than just output distributions; L2 loss on the final hidden layer provides direct feature-level supervision that complements task-specific cross-entropy loss
- Core assumption: The final layer's hidden states encode task-relevant information that transfers across model capacities
- Evidence anchors: Abstract states "Combination of supervised finetuning and hidden-state distillation yields optimal recovery"; Section 4.3 shows L2 hidden-state matching yields best performance by enabling direct capture of teacher's feature representations

## Foundational Learning

- Concept: **Transformer Layer Structure (attention heads, MLP blocks, residual connections)**
  - Why needed here: Understanding what widthwise vs. layerwise pruning removes is essential for predicting their behavioral differences
  - Quick check question: Can you explain why removing a layer (depth reduction) affects residual path lengths differently than removing heads within a layer?

- Concept: **Knowledge Distillation Objectives (logits vs. hidden-state matching)**
  - Why needed here: The paper compares KL divergence, reverse KL, and L2 hidden-state losses; understanding their inductive biases explains the performance differences
  - Quick check question: Why would reverse KL encourage a student to focus on "major modes" while forward KL encourages full distribution matching?

- Concept: **Cross-Modal Projection in MLLMs**
  - Why needed here: The multimodal projector is the critical recovery point for low-compression scenarios; understanding its role explains why projector-only finetuning works
  - Quick check question: If the LLM hidden space shifts after pruning but vision encoder outputs stay fixed, what must the projector learn to do?

## Architecture Onboarding

- Component map: Vision Encoder -> Multimodal Projector -> LLM Backbone
- Critical path:
  1. Select calibration samples (n=10) from training data
  2. Compute importance scores (Block Influence for layers, Taylor expansion for widthwise groups)
  3. Prune lowest-importance components
  4. Recovery training: projector-only for <20% compression; projector+LLM for >40%
  5. Add hidden-state L2 distillation if compute permits (requires original model as teacher)

- Design tradeoffs:
  - Widthwise vs. Layerwise: Widthwise better without recovery; layerwise slightly better at <30% compression with recovery; widthwise better at >40% with recovery
  - Recovery scope: Projector-only is faster but insufficient beyond moderate compression; full LLM finetuning scales better but costs more
  - Data budget: 5% of training data suffices at <30% compression; full data needed at >50% compression

- Failure signatures:
  - Sudden collapse at high compression: At 60% compression, layerwise pruning with no recovery yields near-zero scores (GQA: 0.00, SQA: 0.00)
  - Distillation-only instability: KD without supervised finetuning degrades severely at >45% compression (RKL drops to 12.6% at 60%)
  - Modality misalignment: Performance drops not solely from lost language capacity; finetuning projector recovers 60-80% even at 60% compression

- First 3 experiments:
  1. Baseline widthwise prune-only at 15% compression on your target MLLM to validate that the pruned model retains ~90%+ performance without any recovery training
  2. Projector-only finetuning on the 15% pruned model to confirm that alignment recovery restores performance to ~95%+ of original with minimal compute
  3. Layerwise pruning + projector finetuning + L2 distillation at 30% compression to establish a practical operating point that balances compression ratio (~25% memory reduction) with maintained performance (>95% recovery)

## Open Questions the Paper Calls Out
None

## Limitations
- Paper lacks specification of critical hyperparameters for recovery training (learning rates, epochs, LoRA configurations, distillation loss weights), making exact reproduction challenging
- Study focuses primarily on visual question-answering benchmarks and may not generalize to other multimodal tasks like image generation or document understanding
- Effectiveness of projector-only recovery at low compression ratios assumes the pruned LLM retains sufficient language capacity, which may not hold across different model architectures

## Confidence
- **High confidence**: Widthwise pruning effectiveness in low-resource scenarios, projector-only finetuning sufficiency at <20% compression, hidden-state matching distillation benefits
- **Medium confidence**: Optimal compression ratio thresholds (~30% for layerwise with recovery), data efficiency scaling with compression level
- **Low confidence**: Generalization to non-VQA tasks, stability of KD-only approaches at high compression, optimal hyperparameter selection across different MLLM architectures

## Next Checks
1. **Hyperparameter sensitivity test**: Systematically vary learning rates, LoRA ranks, and distillation loss weights to identify robust configurations across different compression levels
2. **Cross-task generalization**: Apply the pruning and recovery pipeline to multimodal tasks beyond visual QA (e.g., image captioning, visual reasoning with diagrams) to validate benchmark specificity
3. **Architectural transfer**: Test the methodology on different MLLM backbones (e.g., Llava-v1.6, Qwen-VL) to assess model architecture dependencies and generalization limits