---
ver: rpa2
title: 'STELLA: Self-Evolving LLM Agent for Biomedical Research'
arxiv_id: '2507.02004'
source_url: https://arxiv.org/abs/2507.02004
tags:
- stella
- agent
- biomedical
- tool
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "STELLA is a self-evolving AI agent for biomedical research that\
  \ overcomes the limitations of static, manually-curated toolsets by autonomously\
  \ discovering and integrating new bioinformatics tools. Its multi-agent architecture\
  \ employs four key agents\u2014Manager, Developer, Critic, and Tool Creation\u2014\
  that work together to improve reasoning strategies and expand capabilities over\
  \ time."
---

# STELLA: Self-Evolving LLM Agent for Biomedical Research

## Quick Facts
- arXiv ID: 2507.02004
- Source URL: https://arxiv.org/abs/2507.02004
- Reference count: 0
- STELLA achieves state-of-the-art accuracy on biomedical benchmarks, scoring 26% on Humanity's Last Exam: Biomedicine, 54% on LAB-Bench: DBQA, and 63% on LAB-Bench: LitQA

## Executive Summary
STELLA is a self-evolving AI agent for biomedical research that overcomes the limitations of static, manually-curated toolsets by autonomously discovering and integrating new bioinformatics tools. Its multi-agent architecture employs four key agents—Manager, Developer, Critic, and Tool Creation—that work together to improve reasoning strategies and expand capabilities over time. STELLA achieves state-of-the-art accuracy on biomedical benchmarks, scoring 26% on Humanity's Last Exam: Biomedicine, 54% on LAB-Bench: DBQA, and 63% on LAB-Bench: LitQA, outperforming leading models by up to 6 percentage points. Crucially, its performance systematically improves with experience, nearly doubling its accuracy on the Humanity's Last Exam benchmark with increased trials, demonstrating its capacity to learn and grow like a human scientist.

## Method Summary
STELLA employs a multi-agent architecture where a Manager agent coordinates with Developer, Critic, and Tool Creation agents to solve biomedical research problems. The system uses Gemini 2.5 Pro for planning and critique roles, and Claude 4 Sonnet for development and tool creation. It features a Template Library that stores successful reasoning workflows and a Tool Ocean that dynamically expands as the Tool Creation agent discovers and integrates new bioinformatics tools from sources like GitHub and PubMed. The agent iteratively improves through a feedback loop where the Critic evaluates intermediate results and triggers tool creation when gaps are identified.

## Key Results
- STELLA achieves 26% accuracy on Humanity's Last Exam: Biomedicine, outperforming baseline models by 6 percentage points
- On LAB-Bench: DBQA, STELLA scores 54%, and on LAB-Bench: LitQA, it achieves 63% accuracy
- Performance systematically improves with computation budget, nearly doubling accuracy on Humanity's Last Exam with increased trials

## Why This Works (Mechanism)

### Mechanism 1: Iterative Gap Closure via Critic-Driven Feedback
The system employs a Critic Agent that evaluates intermediate results not just for correctness, but for actionability. If the Critic identifies a gap (e.g., "hypothesis is correct but not actionable"), it triggers the Tool Creation Agent rather than halting. This works because the underlying LLMs possess sufficient domain knowledge to recognize when a result is insufficient and propose specific technical solutions.

### Mechanism 2: Dynamic Tool Integration (The Tool Ocean)
Unlike agents with fixed APIs, STELLA's Tool Creation Agent searches resources (e.g., GitHub, PubMed) and writes/tests Python code to create new tools on demand. These are added to the "Tool Ocean" for immediate and future use. This mechanism assumes the necessary external libraries and data are publicly accessible and executable within the agent's sandbox environment.

### Mechanism 3: Strategy Distillation (The Template Library)
When a workflow succeeds, the Manager Agent distills this sequence into a reusable template. This reduces the planning horizon for subsequent similar tasks. This mechanism assumes past successful workflows generalize well to new problems (low distribution shift between tasks).

## Foundational Learning

- **Concept: Multi-Agent Role Specialization**
  - Why needed here: STELLA relies on a separation of concerns (Planning vs. Execution vs. Critique) to function. You cannot understand the system as a single monolithic LLM prompt.
  - Quick check question: Can you distinguish when the "Manager" stops working and the "Dev" agent starts coding?

- **Concept: Tool Use vs. Tool Creation**
  - Why needed here: Standard agents *use* provided tools. STELLA *creates* them. This distinction is the core of its "self-evolving" claim.
  - Quick check question: Does the agent fail if the required API is not in the initial list, or does it write code to build the interface?

- **Concept: Compute-Time Scaling**
  - Why needed here: The paper explicitly shows performance improves with "trials" (Figure 2B). This is not just about model size, but allowing the agent time to iterate and evolve during the test.
  - Quick check question: How does the agent's accuracy change if you restrict it to a single inference step vs. 9 trials?

## Architecture Onboarding

- **Component map:**
  Input Prompt -> Manager (Plan) -> Dev (Execute Code) -> **Critic (Evaluate)** -> [IF FAIL] -> Tool Creation (Build Tool) -> Dev (Re-execute) -> Output

- **Critical path:**
  The Manager plans steps and selects templates, the Dev agent executes Python code, the Critic evaluates output and triggers tool creation if necessary, and the Tool Creation agent builds new tools for the Tool Ocean.

- **Design tradeoffs:**
  - Speed vs. Capability: The "Self-Evolving" loop (Tool Creation) is slow and expensive. It solves harder problems but requires high latency/compute budgets.
  - Stability vs. Growth: Dynamically installing packages (Tool Ocean) allows growth but risks environment contamination or breaking changes.

- **Failure signatures:**
  - Infinite Debug Loops: The Dev agent writes code that fails; the Critic suggests fixes that also fail; the loop exceeds token limits.
  - Hallucinated Dependencies: Tool Creation Agent attempts to `pip install` a library that does not exist.
  - Template Overfitting: Manager forces an irrelevant template on a novel problem, ignoring actual prompt requirements.

- **First 3 experiments:**
  1. Baseline Static Run: Run STELLA with Tool Creation disabled to establish a performance floor on a known benchmark like LitQA.
  2. Tool Creation Trigger: Feed a prompt requiring a specific, obscure database not in the initial Tool Ocean; verify if the Tool Creation Agent successfully builds and utilizes an interface for it.
  3. Template Retrieval Check: Execute the same complex query twice; verify if the second run utilizes a cached Template and completes faster/with higher accuracy.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can STELLA effectively bridge the gap between benchmark performance and real-world laboratory application?
  - Basis in paper: The Conclusion states, "While challenges remain in bridging the gap between benchmark performance and real-world laboratory application... future work will focus on deploying STELLA in real-world research workflows."
  - Why unresolved: The current evaluation is restricted to Q&A benchmarks (HLE, LAB-Bench) and does not validate the agent on end-to-end experimental design or physical laboratory execution.
  - What evidence would resolve it: Successful deployment of STELLA in an active laboratory setting where it proposes hypotheses that are experimentally validated by human researchers.

- **Open Question 2:** How does STELLA ensure the accuracy and safety of tools autonomously generated by the Tool Creation Agent?
  - Basis in paper: The paper describes the Tool Creation Agent searching GitHub and creating scripts, but relies primarily on the Critic Agent's reasoning rather than formal unit testing or ground-truth verification to validate these new tools.
  - Why unresolved: The "Tool Ocean" grows dynamically, but the paper does not detail a robust mechanism to prevent the integration of tools that might hallucinate data or execute incorrect bioinformatics logic.
  - What evidence would resolve it: An analysis of error rates or "hallucination" frequency in the autonomously created tools compared to the pre-defined toolset.

- **Open Question 3:** Does the performance improvement from test-time compute saturate, or does it scale indefinitely with increased trials?
  - Basis in paper: Figure 2B shows a positive correlation between computation budget (trials) and accuracy, but the paper does not establish an upper bound or plateau for this self-evolution.
  - Why unresolved: It remains unclear if the observed doubling of accuracy is sustainable or if the system encounters diminishing returns once the immediate Tool Ocean and Template Library are exhausted for a specific query type.
  - What evidence would resolve it: Scaling curves showing performance trends at significantly higher trial counts (e.g., >9x budget) on the most difficult benchmark questions.

## Limitations
- The self-evolving mechanism's generalizability remains unproven beyond the tested biomedical benchmarks
- The 26% accuracy on Humanity's Last Exam: Biomedicine, while outperforming baselines by 6 points, still represents limited success on extremely difficult questions
- The claimed "near doubling" of accuracy with increased trials may represent optimization of existing patterns rather than genuine capability expansion

## Confidence
- **High confidence:** The multi-agent architecture with specialized roles is technically feasible and the reported benchmark results are internally consistent
- **Medium confidence:** The self-evolution mechanism works as described for biomedical domain tasks, but scalability and generalizability to other scientific domains remain uncertain
- **Low confidence:** The claimed "near doubling" of accuracy with increased trials represents meaningful learning rather than optimization of existing patterns

## Next Checks
1. Test STELLA on a completely different scientific domain (e.g., materials science or astrophysics) to verify cross-domain generalization of the self-evolving capabilities
2. Implement a blinded evaluation where tool creation is disabled mid-experiment to determine whether accuracy gains come from genuine tool discovery versus iterative refinement of existing approaches
3. Conduct ablation studies removing the Template Library to isolate whether performance improvements stem from strategy distillation or other factors like increased computation budget