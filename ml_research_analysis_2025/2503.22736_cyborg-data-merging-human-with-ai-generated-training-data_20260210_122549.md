---
ver: rpa2
title: 'Cyborg Data: Merging Human with AI Generated Training Data'
arxiv_id: '2503.22736'
source_url: https://arxiv.org/abs/2503.22736
tags:
- data
- training
- original
- synthetic
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reducing the costs associated
  with developing Automated Essay Scoring (AES) systems, which traditionally require
  large amounts of manually scored data. The authors propose a model distillation
  pipeline called "Cyborg Data" that uses a large generative language model (Teacher)
  to score essays, which are then used to train a smaller, more computationally efficient
  model (Student).
---

# Cyborg Data: Merging Human with AI Generated Training Data

## Quick Facts
- arXiv ID: 2503.22736
- Source URL: https://arxiv.org/abs/2503.22736
- Authors: Kai North; Christopher Ormerod
- Reference count: 40
- Primary result: "Student models trained on 'Cyborg Data' show performance comparable to training on the entire dataset, while only requiring 10% of the original hand-scored data."

## Executive Summary
This paper addresses the challenge of reducing the costs associated with developing Automated Essay Scoring (AES) systems, which traditionally require large amounts of manually scored data. The authors propose a model distillation pipeline called "Cyborg Data" that uses a large generative language model (Teacher) to score essays, which are then used to train a smaller, more computationally efficient model (Student). The key finding is that Student models trained on this augmented dataset, which combines human and machine-scored responses, achieve performance comparable to models trained on the entire dataset, but only require 10% of the original hand-scored data. Specifically, the approach achieved a Quadratic Weighted Kappa (QWK) score of 0.809 compared to 0.658 for the baseline, demonstrating a significant improvement in AES performance while substantially reducing data collection costs.

## Method Summary
The "Cyborg Data" pipeline fine-tunes a large Teacher model (Llama 3.1, 8B) on a small subset of human-scored essays, then uses this Teacher to generate synthetic scores for the remaining unlabeled essays. These synthetic scores are combined with the original human-scored subset to create an augmented dataset. A smaller Student model (ELECTRA or ModernBERT) is then trained on this augmented dataset. The approach leverages knowledge distillation through synthetic label generation rather than direct weight transfer, enabling the Student to learn scoring patterns from the Teacher's fine-tuning and pretraining. The pipeline significantly reduces the need for expensive human annotation while maintaining scoring accuracy.

## Key Results
- Student models trained on 10% human data + 90% synthetic data achieved QWK of 0.809, compared to 0.658 for models trained on 10% human data only
- The augmented approach showed statistically significant improvement (p ≤ 0.05) over baseline
- ELECTRA Student achieved comparable performance to ModernBERT despite being smaller and having shorter token limits
- The approach demonstrated potential for reducing data collection costs while maintaining scoring quality

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Distillation via Synthetic Label Generation
A fine-tuned large Teacher model can generate synthetic scores of sufficient quality to train a smaller Student model to near-full-data performance. The Teacher (Llama 3.1, 8B) is fine-tuned on a small human-scored subset, then generates scores for remaining unlabeled essays. The Student trains on the combined "Cyborg Data" (human + synthetic labels), learning scoring patterns the Teacher acquired from pretraining plus limited supervision. The core assumption is that the Teacher's synthetic labels are more informative than random or majority-class baselines and do not introduce catastrophic noise. Evidence shows ELECTRA QWK improved from 0.658 (10% original only) to 0.809 (10% original + 90% synthetic), p ≤ 0.05. Break condition: If Teacher accuracy falls below Student's potential on the same data, synthetic labels will degrade performance rather than improve it.

### Mechanism 2: Pretraining-Induced Sample Efficiency
GLMs with extensive pretraining require fewer human labels to reach scoring competence than traditional models. The Teacher's pretraining on diverse text and instruction tuning provides a strong initialization for the AES task, enabling effective fine-tuning with limited human supervision. The core assumption is that the essay scoring task shares sufficient representational structure with pretraining objectives for effective transfer. Evidence suggests these models can exceed human-human levels of agreement even when fine-tuned on small amounts of data. Break condition: If scoring requires domain-specific rubrics or knowledge not captured during pretraining, few-shot fine-tuning yields insufficient accuracy.

### Mechanism 3: Data Augmentation Tradeoff (Benefit vs. Bias)
Synthetic labels expand effective training size but introduce systematic negative bias, particularly affecting underrepresented demographics. The Teacher, trained on limited data, normalizes predictions and suppresses high scores, resulting in stricter scoring that disproportionately lowers scores for ELL students, students with disabilities, and economically disadvantaged groups. The core assumption is that the bias stems from the Teacher's stricter rubric adherence or RL-based training rather than data distribution issues alone. Evidence shows negative SMDs across demographics; fewer high scores (6s) with more synthetic data. Break condition: If bias exceeds operational thresholds (SMDs too large), the pipeline fails fairness requirements even if QWK improves.

## Foundational Learning

- **Concept: Model Distillation**
  - Why needed here: The core pipeline transfers knowledge from Teacher to Student via generated labels, not weights
  - Quick check question: Can you explain how label-based distillation differs from weight-based distillation?

- **Concept: Quadratic Weighted Kappa (QWK)**
  - Why needed here: Primary evaluation metric; measures agreement while penalizing larger disagreements more heavily
  - Quick check question: Why does QWK use quadratic weights rather than linear weights for ordinal scoring?

- **Concept: Standardized Mean Difference (SMD)**
  - Why needed here: Critical for detecting systematic bias between model predictions and human scores across subgroups
  - Quick check question: What SMD threshold would indicate unacceptable bias in an operational scoring system?

## Architecture Onboarding

- **Component map**: PERSUADE essays -> Prompt template -> Teacher (Llama 3.1) -> Synthetic scores -> Augmented dataset (U + synthetic) -> Student (ELECTRA/ModernBERT) -> QWK/SMD evaluation

- **Critical path**:
  1. Format PERSUADE essays with prompt template (Figure 1)
  2. Fine-tune Teacher on subset U (e.g., 10% of training data)
  3. Generate synthetic scores for remaining X\U using fine-tuned Teacher
  4. Create augmented dataset X_U = U (human labels) + X\U (synthetic labels)
  5. Train Student on X_U, evaluate QWK and SMD on test set Y

- **Design tradeoffs**:
  - More synthetic data → lower annotation cost but higher negative bias (SMD)
  - ELECTRA → faster, smaller, but 512-token limit truncates longer essays
  - ModernBERT → handles full essay length but more parameters; shows less relative gain from augmentation

- **Failure signatures**:
  - SMD | > 0.1| for any demographic group
  - Synthetic scores systematically lower than human (negative SMD trend)
  - Fewer high scores (5s, 6s) in synthetic distribution (Table 7)
  - QWK plateaus or degrades despite more synthetic data

- **First 3 experiments**:
  1. Baseline: Train Student on 10% human data only; record QWK (expected: ~0.658 for ELECTRA) and SMD across demographics.
  2. Augmentation test: Train Teacher on same 10%, generate synthetic labels for 90%, train Student on augmented set; expect QWK improvement (0.658→0.809) but monitor SMD for negative bias.
  3. Bias calibration: Implement regression-based Student with mean-constraint calibration on development set to control SMD; compare QWK and SMD tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can regression-based calibration constraints effectively neutralize the demographic biases (e.g., negative SMDs for ELL students) introduced by the synthetic Teacher data?
- Basis: The authors state that "SMDs between the predictions of our Student models and the original test set... need to be addressed" and propose regression constraints as a specific solution.
- Why unresolved: The paper identifies the bias (stricter grading for specific demographics) as a barrier to production but does not experimentally validate the proposed constraint solution.
- Evidence: An experiment where the Student model is trained with a mean-score constraint applied to a development set, followed by an analysis of SMDs across demographics.

### Open Question 2
- Question: Is the "Cyborg Data" pipeline as effective for multi-dimensional trait scoring (e.g., conventions, elaboration) as it is for holistic scoring?
- Basis: The introduction highlights trait-based assessment as a major cost driver requiring separate scores for multiple aspects of writing, yet the methodology tests only holistic scores.
- Why unresolved: It remains unclear if synthetic data can capture the nuances of specific writing traits as effectively as general holistic quality.
- Evidence: Replicating the distillation pipeline using the trait-specific annotations available in the PERSUADE corpus rather than just the holistic scores.

### Open Question 3
- Question: Does the observed suppression of high scores (normalization) by the Teacher GLM stem primarily from the Reinforcement Learning (RL) alignment techniques used in its pre-training?
- Basis: The authors hypothesize that "lower scores... may be a result of the Reinforcement Learning (RL) techniques used to train the Teacher GLM exacerbating SMDs."
- Why unresolved: The study utilizes a specific instruction-tuned model (Llama 3.1) but does not isolate the model's pre-training alignment method as a variable to confirm the source of the bias.
- Evidence: A comparative analysis of Teachers fine-tuned from base models versus those fine-tuned from RL-aligned instruction models to observe differences in score distribution bias.

## Limitations

- The approach introduces systematic negative bias in synthetic scoring, disproportionately affecting ELL students, students with disabilities, and economically disadvantaged groups
- Evaluation is limited to a single dataset (PERSUADE) and two Student architectures, constraining generalizability
- The 10% human data threshold appears effective but may vary with dataset characteristics or scoring rubrics
- The paper does not provide mechanisms for calibration or mitigation of the demographic bias introduced by synthetic data

## Confidence

- **High Confidence**: The core finding that augmented datasets (10% human + 90% synthetic) achieve comparable QWK scores to full human-labeled datasets (0.809 vs baseline ~0.658) is well-supported by statistical testing and clear numerical results
- **Medium Confidence**: The mechanism explaining why pretraining enables sample-efficient AES scoring has theoretical support but lacks direct empirical validation in the AES context specifically
- **Low Confidence**: The explanation for systematic negative bias in synthetic scoring, particularly its demographic disparities, remains speculative without deeper investigation into the Teacher model's decision boundaries or calibration methods

## Next Checks

1. **Bias Mitigation Experiment**: Implement regression-based Student models with mean-constraint calibration on the development set to evaluate whether QWK improvements can be maintained while reducing negative SMD values below acceptable thresholds (e.g., |SMD| < 0.05) for all demographic groups.

2. **Cross-Dataset Generalization**: Replicate the Cyborg Data pipeline on a different AES dataset with distinct scoring rubrics and demographic distributions to assess whether the 10% human data threshold and bias patterns persist across domains.

3. **Teacher Quality Analysis**: Systematically compare Teacher-generated scores against human scores on the human-labeled subset to quantify agreement levels, identify systematic scoring patterns, and determine whether Teacher bias stems from instruction tuning, pretraining data, or the fine-tuning process itself.