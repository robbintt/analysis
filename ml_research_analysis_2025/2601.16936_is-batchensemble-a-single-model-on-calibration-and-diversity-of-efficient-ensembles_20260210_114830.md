---
ver: rpa2
title: Is BatchEnsemble a Single Model? On Calibration and Diversity of Efficient
  Ensembles
arxiv_id: '2601.16936'
source_url: https://arxiv.org/abs/2601.16936
tags:
- batchensemble
- deep
- ensemble
- ensembles
- single
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether BatchEnsemble\u2014a low-cost\
  \ alternative to Deep Ensembles\u2014actually functions as a true ensemble. Theoretically,\
  \ it shows that BatchEnsemble can only access a strict, measure-zero subset of the\
  \ parameter space available to Deep Ensembles, due to its rank-1 multiplicative\
  \ perturbations."
---

# Is BatchEnsemble a Single Model? On Calibration and Diversity of Efficient Ensembles

## Quick Facts
- arXiv ID: 2601.16936
- Source URL: https://arxiv.org/abs/2601.16936
- Reference count: 39
- Key outcome: BatchEnsemble underperforms Deep Ensembles and MC Dropout on accuracy, calibration, and OOD detection, and exhibits near-zero epistemic uncertainty, suggesting it functions more like a single model than a diverse ensemble.

## Executive Summary
This paper investigates whether BatchEnsemble—a low-cost alternative to Deep Ensembles—actually functions as a true ensemble. Theoretically, it shows that BatchEnsemble can only access a strict, measure-zero subset of the parameter space available to Deep Ensembles, due to its rank-1 multiplicative perturbations. Empirically, on CIFAR-10/10C/SVHN, BatchEnsemble underperforms both Deep Ensembles and MC Dropout in accuracy, calibration, and out-of-distribution detection, while its epistemic uncertainty (measured by JSD) remains near zero. A controlled MNIST study further reveals that BatchEnsemble members are nearly identical in both function and parameter space, indicating limited capacity to generate distinct predictive modes. The results suggest BatchEnsemble behaves more like a single model than a diverse ensemble, and recommend reporting uncertainty metrics and diversity diagnostics to assess efficient ensemble methods.

## Method Summary
The paper compares BatchEnsemble against Deep Ensembles, MC Dropout, and single models on image classification tasks with uncertainty quantification. Using ResNet-18, models are trained on CIFAR-10 (in-distribution), CIFAR-10C severity 5 (distribution shift), and SVHN (out-of-distribution) for 75 epochs with SGD+Nesterov. BatchEnsemble uses rank-1 multiplicative perturbations initialized from N(0, 0.5). Evaluation metrics include accuracy, NLL, ECE (15 bins), predictive entropy (H), Jensen-Shannon divergence (JSD), and AUROC/AUPR/FPR95 for OOD detection. A controlled MNIST study with larger networks confirms BatchEnsemble members are functionally and parametrically nearly identical, while Deep Ensembles show high diversity.

## Key Results
- BatchEnsemble achieves lower accuracy, worse calibration (higher ECE), and weaker OOD detection than Deep Ensembles and MC Dropout across CIFAR-10, CIFAR-10C, and SVHN.
- BatchEnsemble's JSD between ensemble members remains near zero, indicating minimal epistemic uncertainty and functional diversity.
- Controlled MNIST experiments show BatchEnsemble members are nearly identical in both parameter space (high cosine similarity) and function space (low JSD), while Deep Ensemble members exhibit high diversity.

## Why This Works (Mechanism)
BatchEnsemble's low diversity stems from its rank-1 multiplicative parameterization, which restricts the ensemble to a strict, measure-zero subset of the parameter space accessible to Deep Ensembles. This structural constraint prevents the model from exploring distinct predictive modes, resulting in near-identical ensemble members that fail to capture epistemic uncertainty.

## Foundational Learning
- **Jensen-Shannon Divergence (JSD)**: Measures the average KL divergence between predictive distributions of ensemble members; quantifies epistemic uncertainty and diversity.
  - Why needed: To detect whether ensemble members make distinct predictions (high JSD) or are functionally identical (near-zero JSD).
  - Quick check: Compute pairwise JSD between ensemble members; expect near-zero for BatchEnsemble, high for Deep Ensembles.

- **BatchEnsemble parameterization**: Uses rank-1 perturbations (r_i ⊗ s_i) applied element-wise to weights, creating k ensemble members with shared weights but different scaling.
  - Why needed: Understands the structural constraint limiting BatchEnsemble's diversity.
  - Quick check: Verify perturbations are initialized from N(0, 0.5) and applied channel-wise (not kernel-wise).

- **Calibration metrics (ECE, NLL)**: Expected Calibration Error (binned) and Negative Log-Likelihood measure how well predicted probabilities match empirical accuracy.
  - Why needed: Poor calibration indicates the model cannot quantify uncertainty reliably.
  - Quick check: Compare ECE across methods; expect BatchEnsemble to have higher ECE than Deep Ensembles.

## Architecture Onboarding

### Component Map
Input -> ResNet-18 backbone -> BatchEnsemble perturbations (rank-1) -> k ensemble members -> Aggregation (mean) -> Output

### Critical Path
Forward pass: input → shared ResNet-18 → k scaled variants (r_i ⊗ s_i) → k predictions → average → output. Diversity depends on perturbation-induced weight-space separation.

### Design Tradeoffs
BatchEnsemble trades diversity for efficiency: shared weights reduce memory and computation vs Deep Ensembles, but rank-1 constraints limit access to distinct predictive modes, reducing uncertainty quantification.

### Failure Signatures
- JSD ≈ 0 between members → insufficient diversity
- High cosine similarity between member weights → functional collapse
- Poor calibration (high ECE) and weak OOD detection → unreliable uncertainty estimates

### First Experiments
1. Train BatchEnsemble and Deep Ensemble on CIFAR-10; compute pairwise JSD and weight cosine similarity.
2. Evaluate calibration (ECE, NLL) and OOD detection (AUROC/AUPR) on CIFAR-10C and SVHN.
3. Visualize predictive disagreement maps between ensemble members.

## Open Questions the Paper Calls Out
- Does increasing model scale (width, depth, training time) restore BatchEnsemble's ability to capture distinct predictive modes and epistemic uncertainty?
- Do the observed diversity limitations generalize to Transformer architectures and regression tasks?
- Can a formal theoretical account link the rank-1 multiplicative parameterization to the empirical lack of functional diversity?
- Would increasing the rank of the perturbations (beyond rank-1) allow the model to access distinct predictive modes while maintaining efficiency?

## Limitations
- Theoretical proof assumes idealized weight initialization and optimization; real training may occasionally escape rank-1 subspace.
- Controlled MNIST study uses simplified architecture that may not fully represent larger networks.
- Analysis assumes channel-wise perturbations as implemented in torch-uncertainty; kernel-wise variants may behave differently.

## Confidence
- Confidence in BatchEnsemble underperforming Deep Ensembles/MC Dropout on accuracy, calibration, and OOD detection: **High** (consistent results across multiple datasets).
- Confidence in BatchEnsemble lacking diversity and behaving as a single model: **Medium-High** (strong evidence from MNIST study, but simplified architecture).
- Confidence in theoretical claims about parameter-space limitations: **High** (mathematical proof provided).

## Next Checks
1. Test kernel-wise BatchEnsemble perturbations to determine if diversity limitation is universal or specific to channel-wise variant.
2. Evaluate BatchEnsemble with different initialization scales (e.g., N(0,1) instead of N(0,0.5)) to test hyperparameter sensitivity.
3. Apply pairwise JSD and parameter similarity diagnostics to a larger-scale architecture (e.g., WideResNet-28-10) to verify results generalize beyond ResNet-18.