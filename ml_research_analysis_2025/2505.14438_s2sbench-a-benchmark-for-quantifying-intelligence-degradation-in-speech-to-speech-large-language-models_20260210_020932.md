---
ver: rpa2
title: 'S2SBench: A Benchmark for Quantifying Intelligence Degradation in Speech-to-Speech
  Large Language Models'
arxiv_id: '2505.14438'
source_url: https://arxiv.org/abs/2505.14438
tags:
- audio
- speech
- training
- language
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces S2SBench, a benchmark designed to quantify
  intelligence degradation in end-to-end speech-to-speech large language models. The
  benchmark evaluates performance gaps between audio and text inputs across three
  diagnostic datasets focused on sentence continuation and commonsense reasoning.
---

# S2SBench: A Benchmark for Quantifying Intelligence Degradation in Speech-to-Speech Large Language Models

## Quick Facts
- arXiv ID: 2505.14438
- Source URL: https://arxiv.org/abs/2505.14438
- Reference count: 7
- This paper introduces S2SBench, a benchmark designed to quantify intelligence degradation in end-to-end speech-to-speech large language models. The benchmark evaluates performance gaps between audio and text inputs across three diagnostic datasets focused on sentence continuation and commonsense reasoning. A pairwise evaluation protocol based on perplexity differences between plausible and implausible samples is used to measure degradation. Experiments on Baichuan-Audio demonstrate that a two-stage training strategy effectively reduces intelligence degradation compared to single-stage training, with accuracy improvements of 2.1–2.6% in speech-to-text settings.

## Executive Summary
This paper addresses the critical challenge of intelligence degradation in speech-to-speech large language models (S2S-LLMs), where performance drops when processing audio inputs compared to text inputs. The authors introduce S2SBench, a novel benchmark that quantifies this degradation using a pairwise evaluation protocol based on perplexity differences. The benchmark focuses on sentence continuation and commonsense reasoning tasks, providing a systematic framework for measuring and diagnosing performance gaps between modalities. Through experiments on the Baichuan-Audio model, the authors demonstrate that a two-stage training strategy can effectively reduce intelligence degradation compared to single-stage approaches.

## Method Summary
S2SBench employs a pairwise evaluation protocol where models process both plausible and implausible sample pairs from diagnostic datasets. The key metric is the perplexity difference between these pairs, with larger differences indicating better discrimination capability. The benchmark focuses on three diagnostic datasets targeting sentence continuation and commonsense reasoning tasks. To address intelligence degradation, the authors propose a two-stage training strategy: first pre-training on text data, then fine-tuning on paired speech-text data. This contrasts with single-stage training that directly trains on audio inputs. The approach aims to establish robust language understanding before adapting to the speech modality.

## Key Results
- S2SBench reveals significant intelligence degradation in S2S-LLMs when processing audio versus text inputs
- Two-stage training strategy improves accuracy by 2.1–2.6% compared to single-stage training in speech-to-text settings
- Baichuan-Audio experiments demonstrate that the two-stage approach effectively reduces perplexity differences between plausible and implausible samples

## Why This Works (Mechanism)
The two-stage training strategy works by first establishing strong language priors through text pre-training, which provides a solid foundation for language understanding. When the model is then fine-tuned on paired speech-text data, it can leverage these pre-trained language representations to better handle the speech modality. This sequential approach allows the model to separate the challenges of language modeling from the complexities of speech processing, resulting in more robust cross-modal generalization. The pairwise evaluation protocol effectively captures degradation by measuring the model's ability to discriminate between semantically coherent and incoherent inputs, which is fundamental to language intelligence.

## Foundational Learning
- **Speech-to-Speech Large Language Models**: Why needed - These models bridge the gap between spoken and written language processing, enabling natural voice interactions. Quick check - Can the model generate coherent speech responses to spoken queries?
- **Intelligence Degradation**: Why needed - Understanding performance differences between modalities is crucial for developing robust multimodal systems. Quick check - Is there a measurable performance gap between audio and text inputs?
- **Pairwise Evaluation Protocol**: Why needed - This method provides a systematic way to measure model discrimination capabilities. Quick check - Does the model assign significantly different perplexity scores to plausible versus implausible samples?
- **Perplexity as Metric**: Why needed - Perplexity quantifies how well a model predicts a sample, serving as a proxy for language understanding. Quick check - Are perplexity scores consistent across repeated evaluations?
- **Two-Stage Training**: Why needed - Separating text pre-training from speech fine-tuning helps establish strong language priors before modality adaptation. Quick check - Does the model show improved performance after fine-tuning compared to direct audio training?
- **Diagnostic Datasets**: Why needed - Specialized datasets focused on sentence continuation and commonsense reasoning help isolate specific types of intelligence degradation. Quick check - Do the datasets adequately cover the target reasoning capabilities?

## Architecture Onboarding

**Component Map:**
Speech Input -> Audio Encoder -> Cross-Modal Fusion -> Language Model -> Text Decoder -> Speech Output

**Critical Path:**
Audio Encoder → Cross-Modal Fusion → Language Model → Speech Output

**Design Tradeoffs:**
The choice between single-stage and two-stage training represents a fundamental tradeoff between training efficiency and performance optimization. Single-stage training is simpler and faster but may not establish strong enough language priors before adapting to speech. Two-stage training requires more computational resources and time but potentially achieves better cross-modal generalization.

**Failure Signatures:**
Intelligence degradation manifests as increased perplexity on plausible samples, reduced discrimination between plausible and implausible pairs, and inconsistent performance across different types of reasoning tasks. Specific failure modes include difficulty with long-range dependencies in spoken language and sensitivity to speech quality variations.

**Three First Experiments:**
1. Compare perplexity distributions between audio and text inputs on the diagnostic datasets
2. Measure pairwise evaluation accuracy for distinguishing plausible from implausible samples
3. Evaluate performance degradation across different speech quality levels (clean vs. noisy audio)

## Open Questions the Paper Calls Out
The paper raises several important open questions regarding the generalizability of the S2SBench benchmark across different model architectures and the potential for expanding diagnostic datasets to cover a broader range of linguistic tasks. The authors also highlight the need for further investigation into alternative evaluation metrics beyond perplexity that might capture different aspects of intelligence degradation.

## Limitations
- The benchmark relies on only three diagnostic datasets focused on sentence continuation and commonsense reasoning, which may not capture the full spectrum of intelligence degradation across diverse linguistic tasks
- The evaluation protocol is based solely on perplexity differences, potentially missing other forms of performance degradation related to semantic coherence or contextual understanding
- Experiments are conducted only on the Baichuan-Audio model, limiting generalizability to other speech-to-speech LLM architectures
- The benchmark does not account for variations in speech quality, accent, or speaking rate, which could significantly impact intelligence degradation measurements

## Confidence

**High confidence**: The observation that speech-to-text performance differs from text-to-text performance, as this is a well-established phenomenon in multimodal models

**Medium confidence**: The claim that the two-stage training strategy reduces intelligence degradation, given that this is demonstrated only on one model architecture

**Medium confidence**: The benchmark's effectiveness in quantifying intelligence degradation, as validation is limited to a small number of diagnostic datasets

## Next Checks
1. Test the S2SBench methodology across multiple speech-to-speech LLM architectures (e.g., MOSS-Speech, SALM-Duplex) to assess generalizability of the intelligence degradation measurements
2. Expand the diagnostic datasets to include diverse linguistic tasks such as summarization, question answering, and dialogue generation to capture broader patterns of degradation
3. Implement cross-modal evaluation metrics beyond perplexity, such as semantic similarity measures and human evaluation protocols, to validate the benchmark's sensitivity to different types of intelligence degradation
4. Investigate the impact of speech quality variations on intelligence degradation measurements by testing with noisy, accented, and varied-rate speech inputs