---
ver: rpa2
title: 'Identifying Imaging Follow-Up in Radiology Reports: A Comparative Analysis
  of Traditional ML and LLM Approaches'
arxiv_id: '2511.11867'
source_url: https://arxiv.org/abs/2511.11867
tags:
- follow-up
- reports
- radiology
- gpt-4o
- advanced
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new annotated corpus of 6,393 radiology
  reports from 586 patients, designed to support development and benchmarking of models
  for identifying imaging follow-up recommendations. It systematically compares traditional
  ML classifiers (Logistic Regression, SVM, Longformer) with generative LLMs (GPT-4o,
  GPT-OSS-20B) for this task.
---

# Identifying Imaging Follow-Up in Radiology Reports: A Comparative Analysis of Traditional ML and LLM Approaches

## Quick Facts
- arXiv ID: 2511.11867
- Source URL: https://arxiv.org/abs/2511.11867
- Reference count: 0
- Primary result: GPT-4o (Advanced) achieved F1 = 0.832, closely matching inter-annotator agreement (F1 = 0.846), while traditional ML models also performed strongly (F1 = 0.775–0.776)

## Executive Summary
This paper introduces a new annotated corpus of 6,393 radiology reports from 586 patients to support development and benchmarking of models for identifying imaging follow-up recommendations. The study systematically compares traditional ML classifiers (Logistic Regression, SVM, Longformer) with generative LLMs (GPT-4o, GPT-OSS-20B) for this task. GPT-4o (Advanced) achieved the best performance (F1 = 0.832), closely matching inter-annotator agreement (F1 = 0.846), while GPT-OSS-20B (Advanced) performed nearly as well (F1 = 0.828). Traditional models like LR and SVM also performed strongly (F1 = 0.775–0.776), underscoring their value as interpretable, resource-efficient baselines. Optimized prompt engineering was critical for maximizing LLM performance, and task-specific input design significantly improved results. The corpus and evaluation provide a valuable foundation for advancing clinical NLP in radiology.

## Method Summary
The study frames follow-up identification as pair-based binary classification, where index reports containing recommendations are matched with candidate subsequent reports. Traditional ML models used TF-IDF vectors, metadata, and shared-word features with SVM and LR classifiers. LLMs were evaluated in two settings: "Base" (full report text) and "Advanced" (metadata + recommendation sentence + preceding context). GPT-4o and GPT-OSS-20B were tested with prompt optimization, while Longformer and fine-tuned Llama3-8B-Instruct served as additional baselines. Performance was evaluated using precision, recall, and F1 scores with 95% confidence intervals via non-parametric bootstrapping.

## Key Results
- GPT-4o (Advanced) achieved F1 = 0.832, matching inter-annotator agreement (F1 = 0.846)
- GPT-OSS-20B (Advanced) performed nearly as well with F1 = 0.828
- Traditional ML models (LR and SVM) achieved strong performance with F1 = 0.775–0.776
- Llama3-8B-Instruct showed high precision (0.907) but low recall (0.623), suggesting overfitting to negative class
- Task-optimized "Advanced" input configuration improved LLM performance by reducing noise from full reports

## Why This Works (Mechanism)

### Mechanism 1
- Condensed, task-specific inputs improve LLM performance by reducing noise and focusing attention on clinically relevant signals
- The "Advanced" setting restricted inputs to metadata + recommendation sentences + preceding context, reducing false positives from spurious lexical overlap
- Core assumption: Recommendation sentences contain the majority of discriminative signal; full reports introduce distractors that degrade reasoning
- Evidence anchors: "task-optimized (Advanced) setting that focused inputs on metadata, recommendation sentences, and their surrounding context" and "the Advanced setting—by restricting input to the recommendation sentence and metadata—correctly identified the mismatch in anatomy and modality... leading to improved precision (0.740 vs. 0.841)"

### Mechanism 2
- Model-specific prompt refinements can harmonize LLM reasoning with clinical expectations, particularly for open-source models
- GPT-OSS-20B exhibited over-strict reasoning—rejecting valid follow-ups when not all recommended findings were addressed
- Adding explicit clarification ("a follow-up remains valid even if only a subset of the recommended findings is discussed") improved F1 from 0.799 to 0.828
- Core assumption: Different model families have distinct default reasoning patterns that can be corrected through targeted instructions

### Mechanism 3
- Well-engineered traditional features (TF-IDF + metadata + shared-word vectors) can match unoptimized LLM performance while remaining interpretable
- LR/SVM combined text vectors with metadata (modality, time gap) and binary shared-word features
- Feature weight analysis revealed clinically meaningful predictors—"nodule," "lesion," "unremarkable," "benign"—aligning with radiologist reasoning
- Core assumption: Lexical overlap and metadata capture sufficient signal for follow-up identification without deep semantic reasoning

## Foundational Learning

- **Concept: Pair-based binary classification**
  - Why needed here: The task frames index–candidate report pairs as positive/negative rather than classifying single reports, reflecting the clinical reality of matching recommendations to subsequent imaging
  - Quick check question: Why is this framed as pair classification rather than extracting follow-up status from a single report?

- **Concept: Inter-annotator agreement as performance ceiling**
  - Why needed here: Human agreement (F1=0.846) establishes an empirical upper bound; models approaching this (GPT-4o Advanced: 0.832) are near-optimal for this task
  - Quick check question: What does it imply when model performance overlaps with human agreement confidence intervals?

- **Concept: Non-parametric bootstrapping for CI estimation**
  - Why needed here: Statistical testing determines whether performance differences are meaningful (e.g., GPT-4o Advanced vs. GPT-OSS-20B Advanced showed no significant difference despite numerical gaps)
  - Quick check question: Why might overlapping confidence intervals not guarantee non-significance, and why use bootstrap rather than parametric methods?

## Architecture Onboarding

- **Component map:** Index reports (CT/MRI with recommendations) → SVM-based sentence classifier → Extracts recommendation sentences → Feature/Input preparation → Model inference → Binary prediction (follow-up / not follow-up) → Index-level evaluation (first positive prediction if multiple candidates)

- **Critical path:**
  1. De-identification → Sampling (CT/MRI with lesion findings + recommendations)
  2. Annotation (identify earliest qualifying follow-up among candidates)
  3. Input preparation varies by model paradigm
  4. Training (supervised models) or prompt design (generative LLMs)
  5. Chronological evaluation at index-report level

- **Design tradeoffs:**
  - Precision vs. Recall: Llama3-8B achieved 0.907 precision but only 0.623 recall—too conservative for clinical alerting
  - Interpretability vs. Performance: LR/SVM offer transparent features; LLMs require prompt optimization but approach human performance
  - Open vs. Closed models: GPT-OSS-20B achieves ~99% of GPT-4o performance with secure deployment options

- **Failure signatures:**
  - Class imbalance effects: Low recall (0.623) for fine-tuned Llama3 suggests overfitting to negative class (only 7.2% positive pairs)
  - Over-strict reasoning: GPT-OSS-20B rejecting valid follow-ups before prompt clarification
  - Lexical overlap errors: Full-text inputs cause false positives from irrelevant finding matches

- **First 3 experiments:**
  1. Replicate LR/SVM baseline with TF-IDF + metadata + shared-word features to establish interpretable baseline and validate feature weight interpretability
  2. Ablate "Advanced" vs. "Base" input configuration on GPT-4o to measure gain from task-specific input design isolation
  3. Test prompt transfer: Apply GPT-4o-optimized prompt directly to GPT-OSS-20B, then add model-specific clarification to quantify prompt portability costs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can class-balanced sampling or focal loss optimization improve the recall of fine-tuned Llama models on the follow-up identification task without sacrificing precision?
- Basis in paper: The authors state "Future work could explore class-balanced sampling or focal loss optimization to mitigate this effect and improve model robustness on rare follow-up cases" regarding Llama3-8B-Instruct's low recall (0.623) despite high precision (0.907)
- Why unresolved: The paper identifies class imbalance (only 7.2% of pairs were true follow-ups) as a likely cause but does not test interventions
- What evidence would resolve it: Experiments comparing Llama fine-tuning with and without class-balanced sampling or focal loss, measuring recall and F1 on the same test set

### Open Question 2
- Question: How well do the evaluated models generalize to multi-institutional datasets with different reporting styles and documentation practices?
- Basis in paper: The authors explicitly state "future work should leverage multi-institutional datasets encompassing a broader range of imaging modalities and clinical scenarios" and note that report style and documentation differences across sites may affect generalizability
- Why unresolved: The corpus was drawn from a single academic medical center, limiting conclusions about external validity
- What evidence would resolve it: Benchmarking the top models (GPT-4o Advanced, GPT-OSS-20B Advanced, LR, SVM) on independently annotated corpora from other institutions

### Open Question 3
- Question: What evaluation frameworks beyond precision/recall/F1 can effectively assess clinical reasoning quality and factual consistency of LLM outputs for radiology tasks?
- Basis in paper: The authors state "evaluating LLM outputs should extend beyond standard performance metrics such as precision, recall, and F1... future evaluations should incorporate radiologist review, clinical reasoning assessments, and systematic evaluations of reasoning quality, factual consistency, and clinical validity"
- Why unresolved: Standard NLP metrics do not capture whether model predictions reflect sound clinical logic or factually grounded reasoning
- What evidence would resolve it: Development of evaluation protocols including radiologist adjudication of reasoning chains and comparison of LLM explanations against expert rationales

### Open Question 4
- Question: Can multi-modal approaches integrating imaging data with radiology text improve follow-up identification over text-only models?
- Basis in paper: The authors state "Multi-modal approaches that integrate imaging data with radiology text, as well as the use of domain-adapted vision-language models (e.g., MedGemma, MedVLM-R1), represent promising avenues for advancing performance"
- Why unresolved: Current work relies exclusively on text; visual information from imaging studies may provide complementary signal
- What evidence would resolve it: Comparative experiments evaluating vision-language models with access to both images and reports versus text-only baselines

## Limitations
- The annotated corpus is not publicly available, preventing independent verification and benchmarking
- Study relies on a single institution's radiology reports, limiting generalizability to different reporting styles and institutional protocols
- Cost and accessibility differences between closed (GPT-4o) and open (GPT-OSS-20B) models remain significant practical barriers

## Confidence
- **High Confidence:** Traditional ML baseline performance (F1 = 0.775-0.776) and interpretability claims supported by feature weight analysis and comparison to prior work (Dalal et al., 2020)
- **Medium Confidence:** LLM performance claims, particularly GPT-OSS-20B improvements after prompt refinement, though limited by lack of external validation and potential overfitting to development set error patterns
- **Low Confidence:** Generalization claims to other institutions and reporting styles due to single-institution data source

## Next Checks
1. **External Corpus Validation:** Apply the trained models to radiology reports from a different institution to assess performance degradation and identify domain-specific failure modes
2. **Clinical Workflow Integration Test:** Deploy the highest-performing model (GPT-4o Advanced) in a simulated clinical environment to measure false positive/negative rates in realistic alerting scenarios
3. **Prompt Transfer Robustness:** Systematically test prompt portability across different open-source LLMs (e.g., Llama3, Mistral) using the same task-specific instructions to quantify model-family dependencies