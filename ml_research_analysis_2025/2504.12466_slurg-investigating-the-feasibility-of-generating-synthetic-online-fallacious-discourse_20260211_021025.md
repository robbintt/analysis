---
ver: rpa2
title: 'SLURG: Investigating the Feasibility of Generating Synthetic Online Fallacious
  Discourse'
arxiv_id: '2504.12466'
source_url: https://arxiv.org/abs/2504.12466
tags:
- fallacy
- data
- fallacies
- synthetic
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the feasibility of generating synthetic
  fallacious discourse about the Ukraine-Russia conflict using large language models.
  The authors scraped Reddit and 4chan data, annotated it for three types of fallacies
  (credibility, logic, and emotional), and found moderate inter-annotator agreement
  (0.51-0.69).
---

# SLURG: Investigating the Feasibility of Generating Synthetic Online Fallacious Discourse

## Quick Facts
- arXiv ID: 2504.12466
- Source URL: https://arxiv.org/abs/2504.12466
- Reference count: 18
- Primary result: LLMs can generate realistic synthetic fallacious discourse with improved diversity when provided appropriate few-shot examples

## Executive Summary
This study investigates the feasibility of generating synthetic fallacious discourse about the Ukraine-Russia conflict using large language models. The authors scraped Reddit and 4chan data, annotated it for three types of fallacies (credibility, logic, and emotional), and found moderate inter-annotator agreement (0.51-0.69). They evaluated LLMs' annotation performance and found that few-shot prompting with 20% gold-labeled data improved strict and relaxed F1 scores compared to zero-shot prompting. For synthetic data generation, models better replicated vocabulary diversity when few-shot prompted with gold-labeled examples. Generated comments scored highest in realism, fallacy accuracy, and span accuracy when prompted with 20% annotated data.

## Method Summary
The study scraped Reddit and 4chan for Ukraine-Russia conflict discussions, filtering comments longer than 32 characters. Four annotators labeled 600 samples using Helwe et al.'s tier-1 fallacy taxonomy, selecting gold labels where inter-annotator agreement exceeded 0.80. DeepHermes-3-Mistral-24B-Preview was evaluated for both annotation and generation tasks using various few-shot splits (0%, 10%, 20%, 30% gold data). The model was prompted with XML-tagged examples to identify fallacy spans and labels. Generated samples were evaluated on human-rated realism, fallacy accuracy, and span accuracy, plus quantitative metrics including hapax-legomena ratio and syntactic phrase distribution.

## Key Results
- Few-shot prompting with 20% gold-labeled data improved strict F1 from 0.286 to 0.290 and relaxed F1 from 0.347 to 0.390
- Generated samples achieved highest realism, fallacy accuracy, and span accuracy ratings when prompted with 20% annotated data
- Hapax-legomena ratio increased from 0.762 (zero-shot) to 0.852 with few-shot examples, approaching real data ratios of 0.887-0.872

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot prompting with ~20% gold-labeled examples improves both annotation accuracy and synthetic generation quality for fallacious discourse.
- Mechanism: Labeled examples provide task-specific demonstrations that ground the model's understanding of fallacy spans, label semantics, and stylistic conventions. This reduces ambiguity in the annotation task and biases generation toward realistic patterns.
- Core assumption: The gold-labeled examples are representative of the target distribution and the model can generalize from limited demonstrations.
- Evidence anchors:
  - [abstract] "few-shot prompting with 20% gold-labeled data improved strict and relaxed F1 scores compared to zero-shot prompting"
  - [section 5.1] Strict F1 increased from 0.286 to 0.290; relaxed F1 from 0.347 to 0.390 with 20% few-shot data
  - [corpus] Neighbor paper "Artificial Conversations, Real Results" similarly finds LLM-generated synthetic data effective for language detection tasks, suggesting cross-task generalization
- Break condition: Performance degraded at 30% few-shot data (Strict F1: 0.221), indicating potential overfitting or example interference. Optimal few-shot ratio is task-dependent and requires empirical tuning.

### Mechanism 2
- Claim: Exposure to gold-labeled forum examples increases vocabulary diversity in synthetic outputs, approaching real-user distributions.
- Mechanism: Few-shot examples expose the model to idiosyncratic, domain-specific vocabulary (e.g., "ruZZian," "putler") that would otherwise be suppressed by RLHF safety training or default stylistic priors.
- Core assumption: The model requires explicit demonstrations to overcome its default tendency toward formal or sanitized language.
- Evidence anchors:
  - [abstract] "high-quality few-shot prompts enhance LLMs' ability to mimic the vocabulary diversity of online forums"
  - [section 5.3] Hapax-legomena ratio increased from 0.762 (zero-shot) to 0.852 with more few-shot examples, approaching real data (Reddit: 0.887, 4chan: 0.872)
  - [corpus] Limited direct corpus support for this specific mechanism; neighbor papers focus on detection rather than diversity metrics
- Break condition: Assumption—uncensored models like DeepHermes may be necessary; highly RLHF-aligned models may resist replicating profanity or extremist rhetoric even with few-shot examples.

### Mechanism 3
- Claim: LLMs can replicate syntactic distributions of forum discourse without explicit syntactic supervision.
- Mechanism: Pre-training on web-scale corpora likely includes forum-like content, enabling the model to reproduce phrase structure distributions (NP, VP, PP, subclauses) characteristic of informal online discourse.
- Core assumption: The target syntactic patterns are already represented in the model's pre-training distribution.
- Evidence anchors:
  - [section 5.3] "each of the datasets exhibited very similar distributions of subclauses, noun phrases (NP), prepositional phrases (PP), and verb phrases (VP)"
  - [section 5.3] Authors speculate "DeepHermes-3-Mistral-24B has trained on this content and is thus able to closely replicate the syntactic distributions"
  - [corpus] No direct corpus validation of pre-training provenance; claim remains speculative
- Break condition: Syntactic similarity does not guarantee semantic or pragmatic authenticity. Generated text may be syntactically valid but pragmatically implausible.

## Foundational Learning

- Concept: **Fallacy taxonomy hierarchies**
  - Why needed here: The paper uses Helwe et al.'s 3-category taxonomy (credibility, logic, emotion) with 23 subtypes. Annotators and models must map spans to these categories consistently.
  - Quick check question: Can you distinguish between an *ad hominem* (credibility) and an *appeal to ridicule* (emotion) when both attack a person?

- Concept: **Inter-annotator agreement (Jaccard Index)**
  - Why needed here: IAA scores (0.51-0.69) indicate moderate disagreement, which affects gold-label quality and downstream model evaluation. Understanding IoU for spans is critical.
  - Quick check question: If Annotator A labels tokens 5-10 and Annotator B labels tokens 6-12, what is the Jaccard Index?

- Concept: **Hapax-legomena ratio**
  - Why needed here: Used as a proxy for vocabulary diversity and authorial style. High ratios (~0.87-0.89 for real data) indicate high unique-token density per sentence.
  - Quick check question: If a 10-word sentence contains 7 words that appear nowhere else in the corpus, what is the hapax-legomena ratio?

## Architecture Onboarding

- Component map:
  Data pipeline: Reddit/4chan scraper -> length filter (>32 chars) -> random sampling -> human annotation
  Annotation layer: 4 annotators × 150 samples -> Jaccard IAA calculation -> gold-label selection (IAA > 0.80)
  Model layer: DeepHermes-3-Mistral-24B with two modes: (1) annotation (temp=0.7, top_p=0.9), (2) generation (temp=1.2, top_p=0.9)
  Evaluation layer: Strict/relaxed F1 for annotation; realism/fallacy accuracy/span accuracy Likert scores + hapax-legomena ratio for generation

- Critical path:
  1. Build gold-label dataset (high IAA samples only)
  2. Tune few-shot ratio (optimal: 20% in this study)
  3. Generate synthetic samples with specified fallacy constraints
  4. Evaluate using both qualitative (human ratings) and quantitative (vocabulary/syntax metrics) measures

- Design tradeoffs:
  - **Taxonomy granularity**: Tier-1 (3 categories) reduces annotation complexity but loses fine-grained fallacy signals
  - **Model selection**: Uncensored models replicate profanity/extremism better but raise ethical/deployment concerns
  - **Few-shot ratio**: Higher ratios improve diversity but may degrade task performance (30% showed decline)
  - **Temperature**: Higher temp (1.2) increases novelty but risks incoherence

- Failure signatures:
  - Over-repetitive vocabulary (hapax ratio < 0.75) -> increase few-shot diversity or temperature
  - Declining F1 with more examples -> reduce few-shot ratio, check for contradictory examples
  - Generated text appears "too coherent" compared to real forum posts -> model may be over-smoothing; consider adding noise or using lower-quality examples

- First 3 experiments:
  1. Replicate the 80/20 gold/few-shot split on a held-out portion of scraped data to validate F1 improvements
  2. Ablate temperature settings (0.7 vs 1.2) for generation to assess coherence-diversity tradeoff
  3. Test a more RLHF-aligned model (e.g., GPT-4) on the same task to quantify the impact of censorship/safety training on fallacy replication

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the synthetic data generation pipeline maintain its effectiveness when applied to diverse political topics or languages outside the Ukraine-Russia conflict?
- Basis in paper: [explicit] The authors state in the limitations that they "limited the scope of our ideation to that topic" and suggest it "would be beneficial to source data from more politically charged boards / forums to provide a even more generaliz-able dataset."
- Why unresolved: The current study restricted the domain to a single conflict to narrow the scope of fallacy detection, leaving the generalizability of the SLURG methodology untested.
- What evidence would resolve it: Replicating the study using data from distinct political events (e.g., US elections, Middle Eastern conflicts) and comparing the realism/diversity scores of the resulting synthetic data.

### Open Question 2
- Question: Does fine-tuning models on the specific task of fallacy detection outperform the few-shot prompting methods used in this study?
- Basis in paper: [explicit] In the limitations, the authors propose to "experiment with fine tuning models on this particular task to potentially reduce the computational cost of the task."
- Why unresolved: The study relied exclusively on prompting DeepHermes-3-Mistral-24B due to time and resource constraints, without exploring weight updates.
- What evidence would resolve it: A comparative benchmark showing F1 scores and inference costs of a fine-tuned smaller model versus the few-shot prompted 24B model on the gold-label dataset.

### Open Question 3
- Question: To what extent does rigorous training on historical context and domain-specific slang improve inter-annotator agreement (IAA) for informal discourse?
- Basis in paper: [explicit] The paper notes that annotating requires "extensive historical understanding" and suggests that "ensuring [annotators] are all on the same page about the historical context... would greatly increase our annotator agreement."
- Why unresolved: The current IAA scores were moderate (0.51–0.69), largely due to the subjective interpretation of evolving slang (e.g., "ruZZian") and context-heavy rhetoric.
- What evidence would resolve it: A follow-up annotation experiment where one group receives specific domain training and another does not, resulting in a statistically significant increase in Jaccard Index scores for the trained group.

## Limitations

- Limited taxonomy granularity: The study uses only the three-category (tier-1) fallacy taxonomy rather than the full 23-subtype classification, potentially missing nuanced fallacy patterns.
- Synthetic-to-real generalization gap: The study does not validate whether improved realism and diversity in synthetic samples transfer to downstream detection models.
- Model and domain specificity: Results are derived from a single uncensored LLM and restricted to Ukraine-Russia conflict data, limiting generalizability.

## Confidence

**High Confidence**:
- Few-shot prompting with ~20% gold data improves annotation F1 scores (0.286→0.290 strict; 0.347→0.390 relaxed).
- Few-shot examples increase vocabulary diversity (hapax ratio: 0.762→0.852), approaching real data (0.887–0.872).
- Generated samples achieve high human-rated realism and fallacy accuracy with 20% few-shot prompts.

**Medium Confidence**:
- The 20% few-shot ratio is optimal; performance degrades at 30%, suggesting overfitting.
- Syntactic distributions of generated text closely match real forum discourse.
- Human raters consistently preferred synthetic samples over real ones in realism and accuracy.

**Low Confidence**:
- Cross-task generalization of synthetic data benefits (e.g., for other fallacy detection tasks).
- Pre-training provenance enabling syntactic replication (no corpus evidence provided).
- Unbiased annotation given the highly partisan, offensive nature of source data.

## Next Checks

1. **Downstream Detection Transfer**: Train a fallacy detector on synthetic data (various few-shot ratios) and evaluate on held-out real data. Compare performance to detectors trained on real data alone.

2. **Model Ablation Study**: Repeat the full pipeline using an RLHF-aligned model (e.g., GPT-4 or Llama-2) to quantify the impact of safety training on fallacy replication and vocabulary diversity.

3. **Taxonomy Granularity Impact**: Annotate a subset of samples using the full 23-subtype taxonomy. Generate synthetic data with both tier-1 and full taxonomy few-shot examples, then evaluate whether finer granularity improves detection performance or human perception of fallacy accuracy.