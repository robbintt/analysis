---
ver: rpa2
title: Hierarchical Geometry of Cognitive States in Transformer Embedding Spaces
arxiv_id: '2512.22227'
source_url: https://arxiv.org/abs/2512.22227
tags:
- embedding
- cognitive
- energy
- tier
- structure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates whether transformer sentence embeddings
  encode a structured, hierarchical organization aligned with human-defined cognitive
  states. We construct a dataset of 480 sentences annotated with seven ordered cognitive
  tiers and continuous energy scores reflecting a low-to-high psychological spectrum.
---

# Hierarchical Geometry of Cognitive States in Transformer Embedding Spaces

## Quick Facts
- arXiv ID: 2512.22227
- Source URL: https://arxiv.org/abs/2512.22227
- Authors: Sophie Zhao
- Reference count: 5
- Key outcome: Cognitive hierarchy information is reliably decodable from transformer embeddings using linear and nonlinear probes, with statistical significance confirmed via permutation tests.

## Executive Summary
This work investigates whether transformer sentence embeddings encode a structured, hierarchical organization aligned with human-defined cognitive states. We construct a dataset of 480 sentences annotated with seven ordered cognitive tiers and continuous energy scores reflecting a low-to-high psychological spectrum. Using fixed embeddings from BGE, MPNet, and MiniLM, we show that both tier labels and energy scores are reliably decodable via linear and nonlinear probes, with nonlinear probes outperforming linear ones. Permutation tests confirm statistical significance under label-randomization nulls (p < 0.005). Confusion matrices reveal predominantly adjacent-tier errors, consistent with a smooth low-to-high gradient observed in UMAP visualizations. These results indicate that embedding spaces encode a robust, model-dependent hierarchical structure beyond surface lexical cues.

## Method Summary
The study uses 480 manually annotated sentences with seven ordered cognitive tiers and continuous energy scores (-5 to +5). Frozen sentence embeddings from BGE-large, MPNet-base, and MiniLM are extracted and L2-normalized. Linear probes (Ridge regression, logistic regression) and shallow MLPs are trained to predict energy scores and tier labels from these embeddings. Performance is evaluated across 30 random train/test splits with 80/20 ratios. Permutation tests with 200 iterations establish statistical significance, while UMAP visualizations provide qualitative confirmation of hierarchical geometry.

## Key Results
- Both cognitive tier labels and energy scores are reliably decodable from frozen transformer embeddings via linear and nonlinear probes.
- Nonlinear probes (shallow MLPs) consistently outperform linear probes, indicating hierarchical structure is distributed across embedding dimensions.
- Confusion matrices show predominantly adjacent-tier errors, and UMAP visualizations reveal a smooth low-to-high gradient in embedding space.
- Permutation tests confirm statistical significance (p < 0.005) under label-randomization null hypothesis.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cognitive hierarchy information is partially recoverable along linear directions in embedding space.
- Mechanism: Transformer embeddings trained on human-generated text encode semantic regularities that correlate with psychological/affective structure; linear probes find directions in this space that predict energy scores and tier labels.
- Core assumption: The annotated cognitive tiers correspond to genuine patterns in human language usage, which pretrained models capture through exposure to similar text distributions.
- Evidence anchors: [abstract]: "both tier labels and energy scores are reliably decodable via linear and nonlinear probes"; [Section 4.1]: Ridge R² = 0.808 (BGE), 0.750 (MPNet), 0.671 (MiniLM); [corpus]: Related work "From Topology to Retrieval" shows high correlation between geometric measures and downstream task performance.
- Break condition: If linear probe performance drops to chance levels on held-out data, or if TF-IDF baselines match transformer performance, the claimed linear structure is likely artifactual.

### Mechanism 2
- Claim: Additional hierarchical structure is organized nonlinearly in embedding space.
- Mechanism: Shallow MLP probes capture modest nonlinear interactions that linear probes miss, indicating the cognitive hierarchy is distributed across embedding dimensions in a structured but not purely linear manner.
- Core assumption: The MLP architecture is sufficiently expressive to capture real structure without overfitting to dataset artifacts.
- Evidence anchors: [abstract]: "nonlinear probes outperforming linear ones"; [Section 4.1]: MLP R² consistently exceeds Ridge R² across all models; [corpus]: "Visualizing LLM Latent Space Geometry" confirms transformer latent spaces exhibit complex geometry.
- Break condition: If MLP gains disappear with stricter regularization or larger held-out test sets, the apparent nonlinear structure may reflect probe overfitting.

### Mechanism 3
- Claim: Embedding geometry preserves ordinal relationships between adjacent cognitive tiers.
- Mechanism: Sentences from neighboring tiers occupy proximate regions in embedding space, producing classification errors that are predominantly local rather than random.
- Core assumption: The 7-tier taxonomy reflects an underlying continuum rather than arbitrary discrete categories.
- Evidence anchors: [abstract]: "Confusion matrices reveal predominantly adjacent-tier errors"; [Section 4.2]: "Misclassifications are concentrated between adjacent tiers"; [corpus]: "The Geometry of Meaning" demonstrates hierarchical structures can be embedded with causal/ordinal structure preserved.
- Break condition: If confusion matrices show uniform error distribution across all tiers, ordinal structure is not preserved.

## Foundational Learning

- Concept: **Probing classifiers**
  - Why needed here: Probing is the core methodology for assessing what information is encoded in embeddings without modifying the model.
  - Quick check question: If a probe achieves high accuracy on a task, can you conclude the model "understands" that task? (No—probing measures recoverability, not causality.)

- Concept: **Permutation testing for null hypothesis evaluation**
  - Why needed here: The paper uses permutation tests to rule out that probe performance arises from incidental correlations.
  - Quick check question: Under the label-randomization null, what should happen to probe performance? (It should drop to chance levels if the original performance reflected genuine structure.)

- Concept: **UMAP for qualitative geometry visualization**
  - Why needed here: UMAP provides intuitive visual confirmation of graded structure that complements quantitative probing.
  - Quick check question: Why is UMAP visualization insufficient as standalone evidence? (Dimensionality reduction can create apparent structure from noise; quantitative probing and statistical tests are required.)

## Architecture Onboarding

- Component map:
  Input sentences -> Sentence transformers (BGE, MPNet, MiniLM) -> L2-normalized embeddings -> Linear probes (Ridge, logistic regression) and MLP probes -> Performance metrics (R², accuracy, F1) -> Permutation tests and UMAP visualization

- Critical path:
  1. Annotation consistency (single annotator is a limitation)
  2. Embedding extraction and L2 normalization
  3. Probe training with appropriate regularization
  4. Permutation testing for statistical significance

- Design tradeoffs:
  - Linear vs nonlinear probes: Linear provides conservative lower bound; MLP captures additional structure but risks overfitting
  - Model scale: Larger models (BGE) show clearer structure but require more compute
  - Annotation granularity: 7 tiers provide resolution but introduce boundary ambiguity

- Failure signatures:
  - Permutation test p-values > 0.05 → structure may be artifactual
  - TF-IDF matching or exceeding transformer performance → structure is lexical, not semantic
  - Uniform confusion matrix (no diagonal concentration) → no ordinal hierarchy preserved

- First 3 experiments:
  1. Replicate probing on an independently annotated dataset (different annotators) to assess annotation subjectivity
  2. Test intermediate transformer layers to localize where hierarchical structure emerges
  3. Apply the same protocol to a different cognitive taxonomy (e.g., valence/arousal) to test generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does similar hierarchical organization emerge within intermediate transformer layers or during generation dynamics?
- Basis in paper: [explicit] The authors state: "Future research may... investigate whether similar hierarchical organization emerges within intermediate transformer layers or during generation dynamics."
- Why unresolved: This study only analyzed final sentence embeddings from frozen models, not layer-wise representations or generation processes.
- What evidence would resolve it: Layer-wise probing experiments across transformer depth and analysis of embedding trajectories during autoregressive generation.

### Open Question 2
- Question: How robust is the observed structure across multiple annotators, alternative annotation schemes, and larger naturalistic corpora?
- Basis in paper: [explicit] Limitations section notes: "Future work may assess robustness using multi-annotator agreement, alternative annotation schemes, larger or more naturalistic corpora, and additional languages."
- Why unresolved: Dataset was annotated by a single author (480 sentences), introducing subjectivity and limiting generalization claims.
- What evidence would resolve it: Multi-annotator studies with inter-rater agreement metrics, validation on larger naturalistic datasets, and comparison across different cognitive taxonomies.

### Open Question 3
- Question: Can embedding geometry support non-manipulative generation steering toward higher-coherence regions?
- Basis in paper: [explicit] The conclusion proposes "the possibility of non-manipulative generation steering, in which model outputs are guided toward regions of embedding space associated with higher coherence or alignment-related attributes without reliance on explicit rule-based filtering."
- Why unresolved: This work only analyzed static embeddings; no generation or steering experiments were conducted.
- What evidence would resolve it: Controlled experiments guiding model outputs toward specific embedding regions and measuring resulting text coherence and alignment properties.

### Open Question 4
- Question: Do probing results reflect causal representation of cognitive structure or merely correlational recoverability?
- Basis in paper: [inferred] Authors explicitly caveat: "probing analyses measure the recoverability of information from fixed embeddings, but do not imply explicit or causal representation by the model."
- Why unresolved: Probing identifies decodable information but cannot distinguish between causally organized representations and incidental correlations.
- What evidence would resolve it: Causal intervention studies (e.g., representation editing) that modify embedding geometry and measure resulting changes in generated text's cognitive attributes.

## Limitations
- The study relies on a single annotator for the 480-sentence dataset, introducing potential subjectivity in the cognitive hierarchy annotations.
- Dataset size (480 sentences) limits statistical power for detecting subtler hierarchical patterns and evaluating probe generalization.
- MLP probe hyperparameters (hidden layer sizes, activation functions, learning rate) are unspecified, creating ambiguity in reproducing exact probe performance.

## Confidence

- **High confidence**: The observation that nonlinear probes outperform linear probes in decoding cognitive structure is robustly supported by reported R² values and theoretical expectations.
- **Medium confidence**: The claim that embedding geometry preserves ordinal relationships (adjacent-tier errors dominate) is plausible given confusion matrices and UMAP visualizations, but could be influenced by dataset construction.
- **Low confidence**: The assertion that the hierarchical structure is "model-dependent" is based on consistent performance across three models but lacks deeper analysis of architectural differences.

## Next Checks

1. **Replicate with independent annotations**: Recruit multiple annotators to label a held-out subset of sentences with the same 7-tier schema. Compute inter-annotator agreement and repeat the probing experiments to assess whether hierarchical structure is preserved across subjective judgments.

2. **Test probe generalization**: Apply the trained probes to an external dataset (e.g., a different corpus annotated with a similar psychological taxonomy) to determine whether the hierarchical structure is an artifact of the specific 480-sentence sample or a more general property of transformer embeddings.

3. **Layer-wise probing analysis**: Extract embeddings from intermediate layers of the three transformer models and repeat the probing experiments to identify where in the network hierarchical cognitive structure emerges.