---
ver: rpa2
title: A Fused Gromov-Wasserstein Approach to Subgraph Contrastive Learning
arxiv_id: '2502.20885'
source_url: https://arxiv.org/abs/2502.20885
tags:
- graph
- learning
- node
- contrastive
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FOSSIL, a novel subgraph-level contrastive
  learning framework that integrates node-level and subgraph-level contrastive learning
  using the Fused Gromov-Wasserstein Distance (FGWD). FOSSIL addresses the challenge
  of capturing both structural patterns and node similarities in graph representation
  learning, particularly in homophilic and heterophilic graphs.
---

# A Fused Gromov-Wasserstein Approach to Subgraph Contrastive Learning

## Quick Facts
- arXiv ID: 2502.20885
- Source URL: https://arxiv.org/abs/2502.20885
- Authors: Amadou S. Sangare; Nicolas Dunou; Jhony H. Giraldo; Fragkiskos D. Malliaros
- Reference count: 25
- One-line primary result: FOSSIL achieves state-of-the-art or competitive performance on 7 benchmark datasets by integrating node-level and subgraph-level contrastive learning using Fused Gromov-Wasserstein Distance

## Executive Summary
This paper proposes FOSSIL, a novel subgraph-level contrastive learning framework that integrates node-level and subgraph-level contrastive learning using the Fused Gromov-Wasserstein Distance (FGWD). FOSSIL addresses the challenge of capturing both structural patterns and node similarities in graph representation learning, particularly in homophilic and heterophilic graphs. The method employs a decoupled encoding scheme combining GNN and MLP with shared weights, followed by adaptive fusion based on node centrality. It uses FGWD to jointly capture feature and structural information during subgraph comparison, and includes both OT-based subgraph-level and InfoNCE-based node-level contrastive losses. Extensive experiments on seven benchmark datasets demonstrate that FOSSIL either outperforms or achieves competitive performance compared to nine state-of-the-art methods, with particular robustness across datasets with varying homophily levels. Ablation studies validate the effectiveness of FGWD and the decoupled encoder design.

## Method Summary
FOSSIL is a subgraph-level contrastive learning framework that integrates node-level and subgraph-level contrastive learning using Fused Gromov-Wasserstein Distance (FGWD). The method employs a decoupled encoding scheme combining GNN and MLP with shared weights, followed by adaptive fusion based on node centrality. FGWD is used to jointly capture feature and structural information during subgraph comparison, with an α parameter balancing Wasserstein Distance (feature similarity) and Gromov-Wasserstein Distance (structural similarity). The framework includes both OT-based subgraph-level contrastive losses and InfoNCE-based node-level contrastive losses. Subgraphs are sampled using BFS from randomly selected anchor nodes, and perturbations are generated using a GAT-based generator. The method demonstrates robustness across both homophilic and heterophilic graphs.

## Key Results
- FOSSIL achieves 80.02±0.26 test accuracy on Cora, outperforming state-of-the-art methods
- The method shows strong performance on heterophilic graphs (e.g., 35.61±0.15 on Actor dataset) where traditional GNNs struggle
- Ablation studies confirm the effectiveness of FGWD and the decoupled encoder design
- FOSSILv2 variant addresses memory constraints of node-level contrastive loss for large-scale graphs

## Why This Works (Mechanism)

### Mechanism 1: Joint Feature-Structure Comparison via FGWD
- Claim: Simultaneous measurement of subgraph feature and structural similarity improves contrastive learning
- Mechanism: FGWD parameterizes a trade-off (α∈[0,1]) between Wasserstein Distance (feature similarity via cost M) and Gromov-Wasserstein Distance (structural similarity via internal costs C1,C2), optimizing a joint transport plan P
- Core assumption: Feature and structural information contribute independently and additively to subgraph similarity
- Evidence anchors:
  - [abstract]: "integrates node-level and subgraph-level contrastive learning using the Fused Gromov-Wasserstein Distance (FGWD), which combines Wasserstein Distance and Gromov-Wasserstein Distance"
  - [Section 3.3, Eq. 1]: FGWD_α = min_P⟨αM + (1-α)L(C1,C2)⊗P, P⟩
  - [Table 3]: FGWD+L_node achieves 80.02±0.26 on Cora vs 75.19±0.42 (FGWD alone) and 60.73±0.89 (GWD alone)
- Break condition: When α=1 (pure features) or α=0 (pure structure), joint comparison benefit degrades to single-modality limitations

### Mechanism 2: Decoupled Encoding for Homophily Robustness
- Claim: Separating MLP-based feature encoding from GNN-based structural encoding enables adaptation to varying homophily levels
- Mechanism: H = σ(IXW) + diag(λ)σ(ÃXW) produces feature embeddings H_f and structural embeddings H_s, with node-wise fusion coefficients λ_i adaptively combining them
- Core assumption: Individual nodes require different proportions of structural vs feature information depending on local graph properties
- Evidence anchors:
  - [abstract]: "decoupled encoder design...making it robust to both homophilic and heterophilic graphs"
  - [Section 4.2]: "on homophilic graphs, traditional GNN encoding typically outperforms MLPs, whereas on heterophilic graphs, MLPs tend to perform better"
  - [Table 4]: Decoupled encoder achieves 80.02 on Cora vs 74.72 (GCN), and 35.61 on Actor vs 13.54 (GCN)
- Break condition: If λ values converge uniformly or fusion MLP ψ fails to learn discriminative weights, architecture reverts to standard GCN with homophily limitations

### Mechanism 3: Learnable View Generation via Attention-Based Perturbation
- Claim: Adaptive perturbations learned through attention mechanisms preserve semantic content better than hand-crafted augmentations
- Mechanism: GAT applied to encoder outputs generates Ĥ_f and Ĥ_s; attention weights implicitly define adaptive feature scaling and edge relevance
- Core assumption: Attention mechanisms can identify which features/edges are semantically important to preserve during augmentation
- Evidence anchors:
  - [Section 4.3]: "we leverage the attention weights of the GAT to adaptively generate perturbations. At the feature level, this is equivalent to applying adaptive scaling"
  - [Table 5]: GAT-E achieves 80.02 on Cora vs 78.28 (Random perturbation), 74.47 (GAT on features)
  - [corpus]: Weak direct corpus evidence on learnable graph augmentations; related work focuses on sampling strategies
- Break condition: If GAT learns uniform attention or attention collapses to trivial patterns, perturbations become equivalent to random augmentations

## Foundational Learning

- **Optimal Transport Distances (Wasserstein, Gromov-Wasserstein)**:
  - Why needed here: FGWD is the core similarity metric; understanding WD (feature alignment) and GWD (structure preservation) separately enables proper α tuning
  - Quick check question: Given two subgraphs with identical node features but different edge structures, would WD detect any difference? Would GWD?

- **Graph Homophily and GNN Limitations**:
  - Why needed here: The decoupled encoder is motivated by GCN failures on heterophilic graphs; understanding this guides architecture selection
  - Quick check question: On a heterophilic graph where connected nodes typically have different labels, why does neighborhood aggregation in standard GCNs hurt performance?

- **Contrastive Learning Loss Functions (InfoNCE, Jensen-Shannon)**:
  - Why needed here: FOSSIL uses InfoNCE for node-level contrast (Eq. 12) and Jensen-Shannon estimator for subgraph-level (Eq. 11); understanding their different properties aids debugging
  - Quick check question: InfoNCE requires negative samples—what happens to the loss if all negative pairs are too easy to distinguish?

## Architecture Onboarding

- **Component map**: Input Graph G=(A,X) -> Encoder: GNN(Ã,X;W) → H_s | MLP(I,X;W) → H_f | shared weights W -> Fusion: λᵢ = MLP_ψ(Hᶠᵢ, Hˢᵢ, degreeᵢ) → H = Hᶠᵢ + λᵢHˢᵢ -> Generator: GAT(Hᶠ, Hˢ) → perturbed Ĥᶠ, Ĥˢ → Ĥ -> Sampling: BFS from anchors S → subgraph pairs (Gˢᵢ, Ĝˢᵢ) -> Loss: L_ot (FGWD via BAPG, Alg 1) + L_node (InfoNCE) + L_θ (fusion reg)

- **Critical path**: Encoder forward → Fusion (λ computation) → Subgraph sampling → FGWD computation (BAPG iterations) → Loss aggregation → Backprop through transport plan P

- **Design tradeoffs**:
  - **α ∈ [0,1]**: Higher values favor features (appropriate for heterophilic), lower favors structure; Fig 3 shows optimal α≈0.5 for homophilic, α→1 for heterophilic
  - **Subgraph size k**: Cubic complexity O(k³) per FGWD computation; k∈[10,30] typically sufficient
  - **FOSSIL vs FOSSILv2**: v2 restricts L_node to sampled subgraph nodes, trading slight performance drops for O((|S|k)²D) vs O(N²D) memory

- **Failure signatures**:
  - **Homophilic performance matches heterophilic (both poor)**: Check if λ values collapsed to uniform; fusion MLP ψ may need higher learning rate
  - **Large performance gap between FOSSIL and FOSSILv2**: Node-level contrast contributing significantly; increase subgraph sampling |S| if memory permits
  - **Training instability or NaN losses**: FGWD computation (Alg 1) may diverge; reduce learning rate or increase β regularization parameter

- **First 3 experiments**:
  1. **Baseline reproduction**: Run FOSSIL on Cora (H=0.81, homophilic) and Actor (H=0.22, heterophilic) with default hyperparameters (k=12, D=512, |S|=300); verify 80.02±0.26 and 35.61±0.15 respectively
  2. **α sensitivity analysis**: Sweep α∈{0.0, 0.25, 0.5, 0.75, 1.0} on both datasets; confirm Fig 3 pattern where heterophilic graphs favor α→1
  3. **Encoder ablation**: Replace decoupled encoder with vanilla GCN; expect performance drops on heterophilic datasets per Table 4 (e.g., Actor: 35.61→13.54)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating a dedicated high-pass feature extractor into the FOSSIL framework significantly improve performance on heterophilic graph datasets?
- Basis in paper: [explicit] The conclusion suggests future research avenues, specifically "integrating a high-pass feature extractor to enhance performance in such scenarios."
- Why unresolved: The current decoupled encoder uses a GNN (low-pass) and an MLP (identity), lacking an explicit high-pass channel to fully capture complex heterophilic patterns.
- What evidence would resolve it: Empirical results showing improved node classification accuracy on heterophilic benchmarks when a high-pass filter is added to the encoder.

### Open Question 2
- Question: How effectively does the FOSSIL framework transfer to downstream tasks beyond node classification, such as link prediction or graph classification?
- Basis in paper: [explicit] The conclusion states that "extending FOSSIL to other downstream tasks such as link prediction or graph classification deserves further research."
- Why unresolved: The method is currently designed and validated exclusively for self-supervised node classification, leaving its utility for other graph learning tasks unproven.
- What evidence would resolve it: Experimental benchmarks demonstrating competitive performance on standard link prediction and graph classification datasets using FOSSIL embeddings.

### Open Question 3
- Question: Would replacing random subgraph sampling with adaptive, data-driven sampling techniques improve the quality of the learned representations?
- Basis in paper: [explicit] Section 4.5 notes that "more adaptive and data-driven subgraph sampling techniques... present an interesting direction for future work."
- Why unresolved: The authors use random sampling for simplicity, which may fail to consistently capture the most informative structural motifs compared to learned sampling policies.
- What evidence would resolve it: A comparative study showing that integrating adaptive sampling algorithms (e.g., reinforcement learning policies) yields higher quality representations than the current random approach.

## Limitations

- **Computational complexity**: FGWD solver (BAPG) has cubic complexity O(k³) in subgraph size, limiting scalability to larger graphs
- **Hyperparameter sensitivity**: Performance heavily depends on α tuning and subgraph sampling parameters, with no clear heuristics for setting |S| or k
- **Memory constraints**: Node-level contrastive loss L_node scales as O(N²D), making full-graph contrast prohibitive for large N (mitigated but not eliminated by FOSSILv2)

## Confidence

- **High confidence**: Decoupled encoder's advantage on heterophilic graphs (robustly shown across multiple datasets); joint FGWD benefit over single-modality baselines (Table 3)
- **Medium confidence**: Attention-based perturbation generator's advantage (Table 5) - limited ablation and unclear GAT architecture details
- **Medium confidence**: Overall method performance - strong results but dependent on extensive hyperparameter tuning via Optuna

## Next Checks

1. **Runtime profiling**: Measure actual training time and memory usage for varying subgraph sizes k; verify claimed O(k³) complexity
2. **Homophily transferability**: Train on homophilic datasets, evaluate on heterophilic datasets (and vice versa) to test claimed robustness
3. **Subgraph size sensitivity**: Systematically vary k from 10 to 30; measure performance trade-offs and computational cost scaling