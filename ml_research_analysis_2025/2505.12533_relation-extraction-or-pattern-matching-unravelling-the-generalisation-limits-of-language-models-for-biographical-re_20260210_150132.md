---
ver: rpa2
title: Relation Extraction or Pattern Matching? Unravelling the Generalisation Limits
  of Language Models for Biographical RE
arxiv_id: '2505.12533'
source_url: https://arxiv.org/abs/2505.12533
tags:
- biographical
- tacred-re
- relation
- relations
- cross-dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the generalisation capabilities of relation
  extraction (RE) models across datasets, focusing on biographical relations. The
  authors find that models struggle to generalise to out-of-distribution data, with
  cross-dataset performance often dropping significantly compared to in-distribution
  results.
---

# Relation Extraction or Pattern Matching? Unravelling the Generalisation Limits of Language Models for Biographical RE

## Quick Facts
- arXiv ID: 2505.12533
- Source URL: https://arxiv.org/abs/2505.12533
- Reference count: 40
- Key outcome: Language models struggle to generalise across RE datasets, with high in-distribution performance often masking overfitting to dataset-specific artefacts rather than learning robust relational patterns.

## Executive Summary
This paper investigates the generalisation capabilities of language models for biographical relation extraction across different datasets. Through systematic cross-dataset evaluations, the authors demonstrate that models consistently underperform when transferred to new datasets, regardless of lexical similarity. They identify data quality as the primary factor for successful transfer, with fine-tuning excelling on high-quality data and few-shot in-context learning (ICL) being more effective for noisy datasets. The study also highlights structural limitations in current RE benchmarks, including single-relation constraints and non-standardized negative classes, which further hinder model generalization and may force models to rely on pattern matching rather than genuine relational understanding.

## Method Summary
The authors conducted extensive cross-dataset experiments using multiple RE datasets including TACRED-RE and NYT, evaluating both encoder models (DeBERTa) and decoder models (LLaMA) with different adaptation strategies. They systematically compared fine-tuning, few-shot ICL, and zero-shot baselines across dataset pairs while standardizing relation schemas for fair comparison. The experiments measured both in-distribution (intra-dataset) and out-of-distribution (cross-dataset) performance using macro-F1 metrics. Additionally, they analyzed error patterns to understand the mechanisms behind generalization failures, particularly examining cases where models predicted valid but unlabeled relations or exhibited systematic biases.

## Key Results
- Cross-dataset performance consistently drops by 20-30 F1 points compared to in-distribution results, regardless of lexical similarity between datasets
- Higher in-distribution performance often indicates overfitting to dataset-specific artefacts rather than better generalisation capabilities
- Data quality, not lexical similarity, is the key determinant of successful transfer; fine-tuning excels with high-quality data while ICL is more effective with noisy datasets
- Zero-shot baselines occasionally outperform all cross-dataset adaptation methods, suggesting overfitting to noisy training data
- Structural issues in RE benchmarks (single-relation constraints, ambiguous negative classes) directly hinder model transferability and genuine relational learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning yields superior cross-dataset transfer when trained on high-quality, manually annotated data, whereas few-shot In-Context Learning (ICL) is more effective for adapting to noisy datasets.
- Mechanism: High-quality data provides cleaner supervisory signals, allowing fine-tuning to learn robust relational patterns. Noisy data (e.g., from distant supervision) causes fine-tuning to overfit to spurious correlations. ICL, by limiting exposure to noisy examples and leveraging pre-trained knowledge, mitigates overfitting and catastrophic forgetting, making it preferable in low-quality data regimes.
- Core assumption: Data "quality" is primarily determined by annotation fidelity and the absence of labels requiring external knowledge not present in the text.
- Evidence anchors:
  - [abstract] "our results also show that data quality, rather than lexical similarity, is key to robust transfer... fine-tuning yields the best cross-dataset performance with high-quality data, few-shot in-context learning (ICL) is more effective with noisier data."
  - [section 5.2] "With high-quality data like TACRED-RE, fine-tuning consistently achieves the best cross-dataset performance... when adaptation data are noisy, as with NYT, few-shot ICL becomes a more effective strategy... likely because ICL limits the signal from noisy training data..."
  - [corpus] Corpus signals are weak; no direct evidence links data quality to adaptation strategy for RE.
- Break condition: This mechanism may not hold if the "high-quality" dataset contains systematic biases absent in the target data, or if zero-shot baselines (which occasionally outperform all methods) are superior for a given dataset.

### Mechanism 2
- Claim: High intra-dataset performance in RE models often signals overfitting to dataset-specific artifacts rather than learning generalizable relational patterns.
- Mechanism: Models exploit surface-level patterns and spurious correlations (e.g., lexical cues, annotation artifacts) unique to the training distribution to maximize in-dataset scores. These patterns do not transfer, causing significant performance drops on out-of-distribution (OOD) data even within the same domain.
- Core assumption: A large gap between in-distribution and OOD performance primarily indicates a failure to learn the underlying relational task, not just domain shift.
- Evidence anchors:
  - [abstract] "higher intra-dataset performance does not indicate better transferability, instead often signaling overfitting to dataset-specific artefacts."
  - [section 5.2] "RE Models Struggle to Generalise across Datasets Cross-dataset evaluations (almost) always perform worse... NYT and TACRED-RE show substantial drops of 20-30 points..."
  - [corpus] Corpus signal "You Are What You Eat" discusses data-generalisation links but provides no RE-specific evidence.
- Break condition: This mechanism does not account for poor transfer caused by fundamental structural inconsistencies between datasets, which the paper identifies as a separate causal factor.

### Mechanism 3
- Claim: Structural inconsistencies in RE benchmarks—single-relation constraints, ambiguous negative class definitions, and reliance on external knowledge—directly hinder model transferability and genuine relational learning.
- Mechanism: Single-relation training causes models to ignore valid but unlabeled relations. Non-standardized negative classes create conflicting learning signals. Annotations requiring external knowledge force models to memorize facts rather than extract textual evidence, degrading performance on datasets adhering to textual grounding.
- Core assumption: Effective RE requires datasets to enforce multi-relation labeling, standardize negative classes, and base annotations solely on textual evidence.
- Evidence anchors:
  - [abstract] "Structural issues in RE benchmarks, such as single-relation per sample constraints and non-standardised negative class definitions, further hinder model transferability."
  - [section 3.2] "Single Relation per Sample... may confuse a model... Unclear 'Negative' Class... highlights the general challenge... Expected Factual Knowledge... annotations extend beyond RE's scope and corrupt models..."
  - [section 6] "Models often detect valid but unlabeled relations... revealing limitations of single-relation per sample." and "...necessary world knowledge for entity interpretation."
  - [corpus] Weak/missing: No corpus evidence directly supports these structural benchmark criticisms.
- Break condition: This mechanism implies performance is dataset-constrained; fixing benchmark structure is a prerequisite for evaluating true model capabilities, independent of model architecture.

## Foundational Learning

- Concept: Out-of-Distribution (OOD) Generalization vs. In-Distribution Performance
  - Why needed here: The paper's central finding is that high in-distribution performance masks generalization failures. Practitioners must understand this decoupling to avoid deploying models that will fail on real-world, unseen data.
  - Quick check question: If your RE model achieves 90% F1 on a validation set but 55% on a held-out dataset from a similar domain, which metric better reflects its true capability for a production deployment?

- Concept: Dataset-Specific Artifacts vs. Robust Patterns
  - Why needed here: The study shows models exploit spurious cues (artifacts) in training data. Recognizing this helps in designing datasets and evaluation protocols that test for genuine pattern learning.
  - Quick check question: A model trained on a dataset where "born in" always precedes a city of birth achieves high accuracy. Why might this performance not generalize to a dataset where birthplace is stated as "native of [city]"?

- Concept: Adaptation Strategies: Fine-tuning vs. In-Context Learning (ICL)
  - Why needed here: The paper provides evidence that the optimal adaptation method is conditional on data quality. This challenges a one-size-fits-all approach to model deployment.
  - Quick check question: You have a small, noisy dataset for a new RE task. Based on this paper, should you prioritize full fine-tuning of a large language model or use few-shot ICL? Why?

## Architecture Onboarding

- Component map:
  1. Data Standardization Module: Prepares raw text from various RE datasets into a unified format (e.g., `<e1>...</e1>`) and maps fine-grained relations to a shared schema for cross-dataset comparison.
  2. Model Adapter: Implements two distinct pathways:
      a. Fine-tuning: Uses methods like LoRA for LLaMA or full fine-tuning for DeBERTa on training data.
      b. In-Context Learning (ICL): Constructs prompts with task instructions and few-shot demonstrations for models like LLaMA.
  3. Evaluator: Computes performance on both intra-dataset (ID) and cross-dataset (OOD) splits, using metrics like macro-F1 to account for class imbalance.

- Critical path: The flow for a robustness experiment is: **Data Selection** (choose dataset based on quality/noise) → **Adaptation** (select fine-tuning or ICL based on data quality) → **Cross-Dataset Evaluation** (test on a held-out dataset with a shared relation schema). The choice at the **Adaptation** step is the critical decision point determined by data quality.

- Design tradeoffs:
  1. Performance vs. Generalization: A model architecture/strategy optimized for peak intra-dataset performance may sacrifice cross-dataset generalization.
  2. Fine-tuning vs. ICL: Fine-tuning can achieve higher performance ceiling on clean data but is more brittle to noise. ICL is more robust to noise but may underperform fine-tuning on high-quality data.
  3. Lexical Similarity vs. Data Quality: Designing for transfer based on lexical overlap between datasets is less effective than prioritizing the use of high-quality, cleanly annotated data.

- Failure signatures:
  1. High ID, Low OOD: A large drop (e.g., >20 points) in macro-F1 between intra- and cross-dataset evaluation signals overfitting to artifacts.
  2. Systematic Overprediction of Negative Class: Consistently high false negatives for positive relations on OOD data suggests the model learned dataset-specific cues for "no relation" rather than the absence of patterns.
  3. Prediction of Unlabeled Relations: Model predicts a valid relation that exists in the sentence but was not included in the single-label annotation, revealing a failure imposed by benchmark constraints.

- First 3 experiments:
  1. Reproduce the Adaptation Strategy Comparison: On a high-quality dataset (e.g., TACRED-RE subset), compare the cross-dataset performance of a fine-tuned encoder model (e.g., DeBERTa) against a few-shot ICL decoder model (e.g., LLaMA). Hypothesis: Fine-tuning should win.
  2. Test the Noise Robustness Hypothesis: On a known noisy dataset (e.g., NYT subset), compare the same two adaptation strategies. Hypothesis: Few-shot ICL should outperform or be competitive with fine-tuning.
  3. Analyze Failure Cases for Structural Bias: Take a high-performing cross-dataset model and manually analyze its errors on the target dataset. Specifically, count how many errors involve the model predicting a valid relation that was unlabeled (single-relation constraint) or a misclassification due to an ambiguous negative class example. Hypothesis: A significant portion of errors will stem from these structural benchmark issues.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does testing Relation Extraction (RE) models on perturbed evaluation sets reveal robustness failures not captured by standard cross-dataset testing?
- Basis in paper: [explicit] The authors state in the Conclusion: "We see many promising directions for future work, including testing RE robustness on perturbed evaluation sets..."
- Why unresolved: Current evaluations rely on static datasets which may not expose vulnerabilities to minor input shifts or adversarial examples that target spurious correlations.
- What evidence would resolve it: Empirical results from evaluating current SOTA RE models on specifically perturbed or adversarial test sets (e.g., contrast sets) compared to standard cross-dataset performance.

### Open Question 2
- Question: How does relaxing the single-relation constraint and standardizing negative class definitions in RE benchmarks impact the cross-dataset generalization of language models?
- Basis in paper: [explicit] The abstract and conclusion identify "Structural issues in RE benchmarks, such as single-relation per sample constraints and non-standardised negative class definitions," as factors that hinder model transferability.
- Why unresolved: Existing models are trained and evaluated on datasets (like TACRED-RE and NYT) that enforce these constraints, masking their ability to handle the complexity of real-world text where multiple relations coexist.
- What evidence would resolve it: A comparative study of model performance on a newly constructed benchmark that allows multi-label annotations and defines a standardized "no_relation" class versus "none_of_the_above."

### Open Question 3
- Question: Under what specific data quality or annotation noise conditions does zero-shot prompting outperform fine-tuning and few-shot learning for cross-dataset transfer?
- Basis in paper: [explicit] The abstract notes: "However, even in these cases, zero-shot baselines occasionally outperform all cross-dataset results." The authors later hypothesize this is linked to overfitting on noisy data (Section 5.2).
- Why unresolved: While the paper establishes that zero-shot can win, the precise threshold of noise or the specific dataset artifacts that trigger this inversion of expected performance hierarchy remain undefined.
- What evidence would resolve it: Controlled experiments systematically varying the noise ratio in training data to identify the crossover point where fine-tuning degrades below zero-shot performance.

## Limitations

- The study does not control for differences in entity types, relation definitions, or annotation guidelines across datasets, which may independently affect transfer performance
- The definition and measurement of "data quality" relies on subjective assessments rather than quantitative quality metrics
- The sample size of datasets tested is limited, and the boundary conditions for when each adaptation strategy is optimal could be more precisely defined

## Confidence

**High Confidence**: The observation that cross-dataset performance consistently drops compared to in-distribution results is well-supported by the experimental data and represents a clear empirical finding.

**Medium Confidence**: The claim that data quality is more important than lexical similarity for robust transfer is supported by the results, but the definition and measurement of "data quality" could be more rigorous.

**Medium Confidence**: The distinction between fine-tuning and ICL effectiveness based on data quality is supported by the experiments, but the sample size of datasets tested is limited.

**Low Confidence**: The assertions about structural issues in RE benchmarks (single-relation constraints, ambiguous negative classes) lack direct empirical support from the corpus and rely primarily on the authors' analysis.

## Next Checks

1. **Controlled Dataset Variation Experiment**: Create controlled variations of the same dataset with different quality levels (e.g., clean vs. artificially noisy annotations) and test whether the same adaptation strategy consistently performs better on high-quality vs. noisy data, independent of domain differences.

2. **Benchmark Structure Analysis**: Design an experiment that systematically varies the benchmark structure (e.g., single-label vs. multi-label annotations, standardized vs. ambiguous negative classes) on the same underlying data to quantify the impact of these structural choices on model generalization.

3. **Zero-Shot Baseline Investigation**: Conduct a detailed analysis of when zero-shot baselines outperform fine-tuning and ICL, including examination of dataset characteristics (relation complexity, entity types, annotation style) that might predict zero-shot success, to better understand the boundary conditions of each approach.