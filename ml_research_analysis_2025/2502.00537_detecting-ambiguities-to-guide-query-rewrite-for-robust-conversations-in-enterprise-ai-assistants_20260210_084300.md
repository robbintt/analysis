---
ver: rpa2
title: Detecting Ambiguities to Guide Query Rewrite for Robust Conversations in Enterprise
  AI Assistants
arxiv_id: '2502.00537'
source_url: https://arxiv.org/abs/2502.00537
tags:
- query
- queries
- ambiguous
- ambiguity
- rewrite
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of ambiguity in multi-turn conversations
  with enterprise AI assistants. The authors propose an NLU-NLG framework that first
  detects whether a query is ambiguous using a hybrid model combining rules and a
  small language model, and then rewrites only the detected ambiguous queries to resolve
  them.
---

# Detecting Ambiguities to Guide Query Rewrite for Robust Conversations in Enterprise AI Assistants

## Quick Facts
- **arXiv ID:** 2502.00537
- **Source URL:** https://arxiv.org/abs/2502.00537
- **Reference count:** 17
- **Primary result:** Hybrid NLU-NLG framework achieves 90.19% F1 for ambiguity detection, reducing downstream errors from 18% to 8% by selectively rewriting only ambiguous queries.

## Executive Summary
This paper addresses ambiguity resolution in multi-turn enterprise AI assistant conversations by introducing an NLU-NLG framework that detects and conditionally rewrites ambiguous queries. The authors propose a taxonomy identifying pragmatic (63.55%), syntactic (31.90%), and lexical (4.55%) ambiguities, then implement a hybrid detection model combining a small language model with rule-based filters. Their approach selectively rewrites only predicted-ambiguous queries using an LLM with context, achieving 90.19% F1 for detection and reducing production errors by more than half compared to always-rewriting.

## Method Summary
The authors create a taxonomy of three ambiguity types from analyzing 3,402 real queries, then augment training data with 1,372 synthetic ambiguous queries using targeted rules. They build a hybrid detection model using a Sentence Transformer encoder combined with three handcrafted features (query length, referential word count, Coleman-Liau readability index) feeding a fully connected classifier for pragmatic and syntactic ambiguities, plus regex-based rule filtering for lexical ambiguity. Ambiguous queries are rewritten using an LLM (GPT-3.5-Turbo or Llama-3.1-70B) with 5-turn chat history context, while clear queries bypass rewriting entirely.

## Key Results
- Ambiguity detection model achieves 90.19% F1 and 92.16% accuracy, outperforming few-shot LLM baselines (78.02% F1) and logistic regression
- Selective rewriting improves overall performance with 0.5417 BLEU vs. 0.4755 for always-rewriting, while reducing downstream errors from 18% to 8% in production
- Classifier adds only 0.01s latency, making it suitable for real-time applications

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hybrid detection combining small LM with rules outperforms LLM-based baselines.
- **Mechanism:** Two-path system routes queries through (1) Sentence Transformer + handcrafted features → FC classifier for semantic patterns, and (2) regex rules for domain-specific lexical cases. Small LM captures semantic distinctions while rules catch entity-specific patterns.
- **Core assumption:** Ambiguous queries show measurable distributional differences across semantic embeddings and structural features.
- **Evidence:** 90.19% F1 vs. 78.02% for few-shot Llama-70B; Figure 3 shows clear patterns between ambiguous and clear queries.
- **Break condition:** Taxonomy misses new ambiguity types or regex rules fail on novel entity formats.

### Mechanism 2
- **Claim:** Conditionally rewriting only ambiguous queries reduces downstream errors.
- **Mechanism:** Classifier acts as gatekeeper—clear queries bypass LLM rewrite entirely, while ambiguous queries get rewritten with 5-turn history. This prevents LLM from unnecessarily rephrasing already-specific queries.
- **Core assumption:** LLM rewrites introduce error probability even on clear queries; classifier false negatives are less harmful than LLM false positive rewrites.
- **Evidence:** 18%→8% error reduction; Table 3 shows example where always-rewriting corrupted dataset name.
- **Break condition:** High classifier false negatives let ambiguous queries through unchanged, or latency budget prohibits even 0.01s overhead.

### Mechanism 3
- **Claim:** Taxonomy-driven data augmentation addresses class imbalance.
- **Mechanism:** Authors derive three ambiguity categories from logs, then apply targeted augmentation rules: omitting proper nouns, inserting referential pronouns, creating vague statements. This transforms 12% minority class into balanced training signal.
- **Core assumption:** Synthetic queries approximate real ambiguous query distribution without introducing harmful distribution shift.
- **Evidence:** 1,372 synthetic queries generated from taxonomy insights.
- **Break condition:** Real-world ambiguities diverge from augmentation patterns or overfit to specific regex-based synthetic generation.

## Foundational Learning

- **Concept:** Coreference resolution in conversational QA
  - **Why needed here:** Pragmatic ambiguities stem from unresolved pronouns referencing prior turns. Understanding coreference is prerequisite to grasping why detection requires referential count features.
  - **Quick check question:** Given "What is a segment?" → "How many do I have?", what entity must be resolved, and what feature would flag this as ambiguous?

- **Concept:** Feature engineering for text classification
  - **Why needed here:** Model combines dense embeddings with sparse numerical features (query length, readability index). Engineers must understand why purely embedding-based approaches may miss structural signals.
  - **Quick check question:** Why would a pure transformer classifier potentially miss "What is it?" vs. "What is segment?" without referential count features?

- **Concept:** Query rewriting vs. query expansion
  - **Why needed here:** Paper positions rewrite as context-dependent reformulation, not just term addition. Distinguishing these helps understand why selective rewriting matters.
  - **Quick check question:** If user asks "Tell me about pricing" → "Is it included?", what should rewrite produce, and what would query expansion alone fail to achieve?

## Architecture Onboarding

- **Component map:**
  User Query → Ambiguity Detection (Sentence Transformer + Features + FC Classifier + Rule-based Filter) → (if ambiguous) LLM Rewrite with 5-turn History → Conversational Agent

- **Critical path:** Query → Feature extraction + embedding → Classifier prediction → (if ambiguous) LLM rewrite with 5-turn history → Agent. LLM rewrite (~1-2s) is latency-sensitive bottleneck; classifier adds only 0.01s.

- **Design tradeoffs:**
  - Small LM vs. LLM for detection: 90.19% F1 vs. 78.02% for few-shot Llama-70B, 100x lower latency. Tradeoff: requires feature engineering vs. pure prompting.
  - Rewrite all vs. selective: 0.4755 vs. 0.5417 BLEU; selective requires classifier maintenance but reduces error surface.
  - Threshold tuning: Adjusts precision vs. recall for ambiguous class; current deployment optimizes for reducing false positive rewrites.

- **Failure signatures:**
  - Lexical rule drift: Novel entity formats bypass regex, causing false negatives
  - Cascading rewrite errors: Initial rewrite mistake corrupts context for subsequent turns
  - Classifier confidence miscalibration: Overconfident "clear" predictions on edge-case ambiguities

- **First 3 experiments:**
  1. Baseline validation: Run classifier on held-out production logs (manually labeled) to confirm F1 > 88% before any code change. Compare against few-shot GPT-4 prompt on same data.
  2. A/B test selective vs. always-rewrite: Route 10% traffic to always-rewrite path, measure downstream error rate, latency p95, and user satisfaction signals.
  3. Rule coverage audit: Extract entity patterns from last 30 days of logs, compute regex match rate. If <95% match, flag for rule set expansion.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How effectively does the proposed ambiguity taxonomy and detection model transfer to open-domain conversational datasets or enterprise verticals with different business objects?
- **Basis in paper:** The authors note existing taxonomies from open domains "may not provide relevant insights for an industry CQA system" and rules rely on domain-specific entities.
- **Why unresolved:** Model's features and synthetic data generation are tightly coupled to Adobe Experience Platform's specific log patterns and terminology.
- **What evidence would resolve it:** Zero-shot or few-shot evaluation on public conversational datasets (like CoQA) or enterprise logs from distinct sectors (healthcare or finance).

### Open Question 2
- **Question:** Would fine-tuning LLMs on the proposed augmented dataset yield higher detection accuracy than current few-shot baselines or SLM approach?
- **Basis in paper:** Authors compare against "Few-shot" LLMs which underperform, but don't assess if LLMs could surpass SLM if fine-tuned rather than prompted.
- **Why unresolved:** Paper establishes LLMs struggle via prompting, but leaves open whether their capacity could be better harnessed through training on 1,372 synthetically generated queries.
- **What evidence would resolve it:** Experiment fine-tuning Llama-3.1-70B on augmented training set and comparing F1 score against SLM+Rules model.

### Open Question 3
- **Question:** How does performance degrade when resolving pragmatic ambiguities requiring context beyond five interactions?
- **Basis in paper:** Methodology explicitly limits "surrounding context" to "chat history of past five interactions."
- **Why unresolved:** Paper demonstrates success on current test set but doesn't evaluate scenarios where antecedent for referential word lies outside fixed window.
- **What evidence would resolve it:** Ablation study varying context window size (5, 10, 20 turns) and measuring recall for long-range pragmatic ambiguities.

## Limitations
- Taxonomy generalizability: Three-category taxonomy derived from 3K Adobe logs lacks validation across different enterprise domains or conversational contexts.
- Production measurement validity: 18%→8% error reduction presented as aggregate metric without breakdown by ambiguity type or quantification of classifier false negatives.
- Rule coverage limitations: Regex-based lexical rules tightly coupled to Adobe's entity naming conventions require domain-specific adaptation for other enterprises.

## Confidence

- **High confidence:** Hybrid NLU-NLG architecture design and selective rewriting mechanism are well-specified with clear implementation details. Quantitative superiority over baselines (90.19% F1 vs. 78.02%) is reproducible given access to feature engineering pipeline.
- **Medium confidence:** Production error reduction claim depends on accurate measurement methodology and complete accounting of all failure modes. Lacks sensitivity analysis showing how classifier threshold tuning affects error rates.
- **Low confidence:** Taxonomy-driven augmentation rules and their impact on generalization are not validated beyond Adobe dataset. No ablation study isolates contribution of synthetic data versus real training samples.

## Next Checks
1. **Taxonomy generalization test:** Apply taxonomy and detection model to held-out corpus from different enterprise domain (e.g., financial services vs. marketing). Measure whether pragmatic/syntactic/lexical proportions remain stable and whether F1 drops >5 points.
2. **False negative impact analysis:** Instrument production system to log classifier confidence scores for all queries resulting in downstream errors. Calculate false negative rate among failed queries to quantify how many ambiguous queries bypass rewriting.
3. **Rule coverage audit:** Extract entity patterns from last 30 days of production logs and compute regex match rates. Identify entity formats falling outside current lexical rules and measure false negative rate for lexical ambiguity detection.