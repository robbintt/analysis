---
ver: rpa2
title: 'MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in Agricultural
  Expert-Guided Conversations'
arxiv_id: '2506.20100'
source_url: https://arxiv.org/abs/2506.20100
tags:
- reasoning
- qwen2
- benchmark
- mmst
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MIRAGE is a benchmark for multimodal expert-level reasoning in
  agricultural consultations, built from 35K+ real expert-user interactions covering
  7K+ biological entities. It features two tasks: MMST (single-turn identification/management
  with images) and MMMT (multi-turn clarify-or-respond decision-making).'
---

# MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in Agricultural Expert-Guided Conversations

## Quick Facts
- **arXiv ID:** 2506.20100
- **Source URL:** https://arxiv.org/abs/2506.20100
- **Reference count:** 40
- **Key outcome:** MIRAGE is a benchmark for multimodal expert-level reasoning in agricultural consultations, built from 35K+ real expert-user interactions covering 7K+ biological entities. It features two tasks: MMST (single-turn identification/management with images) and MMMT (multi-turn clarify-or-respond decision-making). Evaluated on 22 VLMs, even top models like GPT-4.1 achieve only 43.9% accuracy on identification, with open-source models lagging significantly. Models struggle with rare entities, long-tail generalization, and contextual inference. LoRA fine-tuning yields gains on seen entities but limited transfer to unseen ones. The benchmark exposes current gaps in open-world multimodal reasoning and decision-making, offering both a challenging testbed and development suite for next-generation agricultural AI assistants.

## Executive Summary
MIRAGE is a novel benchmark designed to evaluate multimodal information-seeking and reasoning in agricultural expert consultations. Unlike existing benchmarks that rely on well-specified user inputs and closed-set taxonomies, MIRAGE features underspecified, context-rich scenarios with open-world settings, requiring models to infer latent knowledge gaps, handle rare entities, and either proactively guide the interaction or respond. Built from 35K+ real expert-user interactions, the benchmark includes more than 7,000 unique biological entities and presents a challenging long-tail generalization setting. The benchmark includes two core tasks: MMST (Multimodal Single-Turn) for identification and management recommendation, and MMMT (Multimodal Multi-Turn) for decision-making on whether to clarify or respond based on dialogue history and images.

## Method Summary
MIRAGE consists of two tasks: MMST (Multimodal Single-Turn) for entity identification and management recommendation, and MMMT (Multimodal Multi-Turn) for clarify-or-respond decision-making. The dataset is constructed from 35K+ real expert-user interactions, cleaned, categorized into 7 types, and reformatted with GPT-4.1-enhanced answers. MMST includes 8,184 Standard and 3,934 Contextual samples, with a training set of 17,532 samples. MMMT contains 861 test dialogues. Evaluation uses an ensemble of three reasoning-capable LLM judges (DeepSeek-R1-Distill, Qwen3-32B, Phi-4-Reasoning) with nine evaluations per sample. LoRA fine-tuning is performed with global batch size 128, lora_alpha=64, lora_dropout=0.05, and AdamW optimizer with cosine LR schedule.

## Key Results
- GPT-4.1 achieves 43.9% identification accuracy on MMST-ID, with open-source models lagging significantly
- Models struggle with rare entities, showing a persistent 14-point gap in identification accuracy between seen and unseen entities
- Metadata (location, time) provides negligible accuracy gains (+1.6%) in both MMST and MMMT tasks
- LoRA fine-tuning yields gains on seen entities but limited transfer to unseen ones, highlighting challenges in open-world generalization
- Decision accuracy for MMMT tops at 65.5% for GPT-4o, even with chain-of-thought prompting

## Why This Works (Mechanism)

### Mechanism 1: Real-World Underspecification Driving Contextual Inference
By grounding tasks in naturally underspecified, open-world user queries derived from 35K+ real expert-user interactions, the benchmark forces models to perform contextual inference and latent knowledge-gap identification rather than relying on closed-set pattern matching. The MMST and MMMT tasks are constructed from real conversations where user inputs often lack explicit context (e.g., location, timing, intent). This requires models to infer missing information from dialogue history, images, and metadata to decide whether to ask clarifying questions or provide responses. The open-world setting with 7K+ biological entities, including many unseen during evaluation, further tests long-tail generalization.

### Mechanism 2: Joint Decision-Making and Generation via Goal-State Tracking
The MMMT task's clarify-or-respond formulation, combined with explicit goal-state modeling (known/missing information), creates a testbed for models to learn when to seek clarification versus respond, mirroring expert consultation behavior. Given dialogue history D and images I, models must infer a user goal G and a goal state S_G = (known, missing). A policy π: (D, I) → (a, r) selects action a ∈ {<Clarify>, <Respond>} based on whether essential missing information exists. The model then generates an appropriate clarification question or expert response r. This joint decision-generation setup is evaluated on decision accuracy and goal relevance.

### Mechanism 3: Ensemble LLM-as-Judge Evaluation for Long-Form, Multidimensional Reasoning
Using an ensemble of reasoning-capable LLMs (DeepSeek-R1-Distill, Qwen3-32B, Phi-4-Reasoning) with multi-run assessments provides more robust, interpretable, and reproducible evaluation of long-form responses than single-model metrics. Each candidate response is generated three times and evaluated by all three judge models across dimensions (Accuracy, Relevance, Completeness, Parsimony), yielding nine evaluations per sample. Statistical reliability is quantified via Fleiss' κ and Kendall's W, ensuring inter-rater agreement and reducing single-model bias.

## Foundational Learning

- **Concept: Long-tail distribution and open-world generalization**
  - Why needed here: MIRAGE's 7K+ biological entities follow a long-tail distribution, with rare species dominating real-world agricultural problems. Models must recognize entities not seen during training, a key challenge exposed by the benchmark.
  - Quick check question: Can your model correctly identify a pest species that appears only 2-3 times in the training data? How does performance degrade from common to rare entities?

- **Concept: Contextual grounding and latent inference**
  - Why needed here: Tasks require inferring missing context (location, time, user intent) from images, dialogue, and metadata. This goes beyond standard VQA, demanding reasoning under partial observability.
  - Quick check question: Given a user query about leaf spots without location data, can your model infer climate-relevant diseases and ask a targeted clarification question?

- **Concept: Multidimensional evaluation beyond accuracy**
  - Why needed here: MIRAGE evaluates responses on Accuracy, Relevance, Completeness, and Parsimony, reflecting real expert communication standards where conciseness and actionability matter as much as correctness.
  - Quick check question: Does your model's response to a management query cover all expert recommendations without speculative or overly verbose additions?

## Architecture Onboarding

- **Component map:**
  - Data Curation Pipeline (Section 3.1, Appendix C.2): Raw AskExtension dialogues → Cleaning → Categorization (7 categories) → Entity extraction + synonymy collection → Reformating (GPT-4.1-enhanced answers) → Splitting (Standard/Contextual/Training)
  - MMST Benchmark: Single-turn (q, I, meta) → Model → Response (entity, causal explanation, management) → Evaluated by LLM-Judge ensemble
  - MMMT Benchmark: Multi-turn (D, I) → Model → (Decision a, Utterance r) → Evaluated on decision accuracy + goal relevance
  - Evaluation Framework (Section 5.2, Appendix D): Ensemble of 3 LLM judges, 3 generations per sample → 9 scores → Aggregated with inter-rater reliability checks (Fleiss' κ, Kendall's W)

- **Critical path:**
  1. Data quality: Ensure cleaning and reformatting preserve underspecification while removing PII and broken URLs
  2. Task alignment: MMST-ID/ID+MG and MMMT decision tasks must maintain consistent entity annotations and goal-state labels
  3. Evaluation robustness: Ensemble judges must be calibrated to avoid systematic bias; κ/W scores should meet "good" thresholds (0.75–0.88)
  4. Model integration: For fine-tuning, use LoRA on MMST training data (17,532 samples) with appropriate hyperparameters (Appendix E.4)

- **Design tradeoffs:**
  - Realism vs. scalability: Using real user-expert dialogues ensures ecological validity but introduces noise and variable image quality. Synthetic alternatives would lose this grounding
  - Evaluation complexity vs. reproducibility: Ensemble judging is resource-intensive but reduces bias; single-model evaluation is faster but less reliable
  - Open-world vs. closed-set: Including 7K+ entities, many unseen, tests generalization but risks low accuracy on rare cases; restricting to common entities would inflate scores artificially

- **Failure signatures:**
  - Low performance on rare entities (ID accuracy drop from 53.1% on common to 38.5% on rare for GPT-4.1; Table 12–13)
  - Poor metadata utilization (≤1.6% accuracy gain with location/time; Table 9–10)
  - Persistent clarify-respond errors even with CoT prompting (Decision accuracy tops at 65.5% for GPT-4o; Table 3)
  - Over-verbosity and speculative responses (low Parsimony scores in open-source models; Table 7)

- **First 3 experiments:**
  1. Baseline VLM evaluation on MMST-ID and MMST-MG (zero-shot) to measure gaps vs. GPT-4.1 (43.9% ID accuracy, 3.0 reasoning score)
  2. LoRA fine-tuning of mid-size VLM (e.g., Qwen2.5-VL-7B) on MMST training data, tracking seen vs. unseen entity accuracy across epochs to identify overfitting (target: <14% gap as per paper)
  3. MMMT decision accuracy test with and without chain-of-thought prompting, comparing zero-shot vs. CoT gains (expect ~2–6% improvement as in Table 3)

## Open Questions the Paper Calls Out

- **How can vision-language models be improved to handle unseen biological entities without relying solely on fine-tuning on known taxonomies?**
  - Basis in paper: [explicit] The authors observe that LoRA fine-tuning yields gains on seen entities but shows a "persistent 14-point gap" on unseen entities, highlighting the difficulty of "open-world generalization" in long-tail settings
  - Why unresolved: Current fine-tuning leads to memorization rather than transferable reasoning; models fail to recognize novel pests or plants not present in the training set
  - What evidence would resolve it: A method that achieves comparable identification accuracy on held-out, taxonomically novel entities without requiring explicit training data for those specific classes

- **How can explicit spatiotemporal context (location, time) be effectively integrated into VLMs to enhance reasoning in context-dependent agricultural queries?**
  - Basis in paper: [explicit] The paper notes "Limited gains from metadata," stating that models often treat location and time as "irrelevant or distracting" despite their importance in real-world reasoning
  - Why unresolved: Even top models like GPT-4.1 show minimal accuracy improvements (+1.6%) when metadata is included, suggesting current architectures fail to leverage contextual priors like seasonality
  - What evidence would resolve it: A model demonstrating statistically significant performance increases on the Contextual subset by effectively grounding reasoning in provided temporal and geographic inputs

- **What methodologies are required to evaluate and enhance agentic capabilities for dynamic, interactive clarification in agricultural consultations?**
  - Basis in paper: [explicit] The Limitations section notes the current benchmark "does not yet simulate interactive dialogue" or "visual follow-ups," limiting the evaluation of adaptive response strategies
  - Why unresolved: The MMMT task is currently static (based on fixed dialogue history) and does not assess a model's ability to recover from errors or adapt to real-time user feedback
  - What evidence would resolve it: An extension of the benchmark involving user simulators that test a model's ability to perform conversational repair and integrate new visual information dynamically

## Limitations
- The real-world data curation process (GPT-4.1-based reformatting, entity extraction, synonym collection) may inadvertently resolve or introduce ambiguities, potentially weakening the benchmark's ability to test true contextual inference under underspecification
- Long-tail generalization performance on rare entities remains a persistent challenge, with models showing significant accuracy drops, and LoRA fine-tuning offers limited transfer to unseen entities
- The benchmark does not yet simulate interactive dialogue or visual follow-ups, limiting the evaluation of adaptive response strategies and conversational repair

## Confidence

- **High:** Task formulation (MMST and MMMT) and dataset construction from real expert-user interactions are clearly specified and methodologically sound
- **Medium:** The mechanism by which underspecification drives contextual inference is plausible but hinges on data curation preserving natural ambiguity; no direct validation is provided
- **Medium:** The ensemble LLM-as-Judge evaluation protocol is detailed, but its robustness and correlation with expert judgments for domain-specific reasoning remain to be proven

## Next Checks

1. **Data Curation Fidelity:** Verify that the GPT-4.1 reformatting and entity extraction steps do not artificially resolve the underspecification in user queries by comparing original and processed dialogues for retained ambiguity
2. **Evaluation Reliability:** Conduct inter-rater reliability checks (Fleiss' κ, Kendall's W) on a held-out subset of MMST and MMMT samples using the three judge models to confirm "good" agreement thresholds
3. **Generalization Gap:** Systematically measure model performance degradation from common to rare entities on the MMST-ID task, quantifying the absolute accuracy gap and comparing it to the reported ~14% drop