---
ver: rpa2
title: 'RPGBENCH: Evaluating Large Language Models as Role-Playing Game Engines'
arxiv_id: '2502.00595'
source_url: https://arxiv.org/abs/2502.00595
tags:
- game
- human
- personality
- type
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces RPGBench, the first benchmark for evaluating
  large language models (LLMs) as text-based role-playing game (RPG) engines. The
  benchmark comprises two tasks: Game Creation (GC), where models must generate valid,
  playable game worlds with structured mechanics, and Game Simulation (GS), where
  models simulate interactive gameplay across multiple rounds while maintaining consistent
  state updates and enforcing game rules.'
---

# RPGBENCH: Evaluating Large Language Models as Role-Playing Game Engines

## Quick Facts
- **arXiv ID**: 2502.00595
- **Source URL**: https://arxiv.org/abs/2502.00595
- **Reference count**: 40
- **Primary result**: Introduces RPGBench benchmark evaluating LLMs as text-based RPG engines across Game Creation and Game Simulation tasks

## Executive Summary
RPGBench is the first benchmark for evaluating large language models as text-based role-playing game engines. It comprises two tasks: Game Creation (generating valid, playable game worlds with structured mechanics) and Game Simulation (simulating interactive gameplay across multiple rounds while maintaining consistent state updates and enforcing game rules). The evaluation combines objective metrics (verifying adherence to game mechanics and variable updates) with subjective assessments (content interestingness, action quality, and role-playing capability) using an LLM-as-a-judge framework. Results show that while state-of-the-art LLMs can produce engaging stories, they struggle with consistent game mechanics, particularly in complex scenarios.

## Method Summary
The benchmark uses fictional characters from Wikipedia as seed material for game creation. Models generate structured JSON games with state variables, events, and termination conditions. A BFS Validity Checker verifies game soundness by ensuring all events are reachable and both success/failure outcomes are achievable within 10 million explored states. Game Simulation runs 10 rounds using temperature 0.2, with three-stage loops per round (Event Planning → Game Narration → State Updates). Evaluation combines automated checks (mechanic scores, event condition errors, variable update errors) with LLM-as-judge subjective metrics (factual consistency, personality consistency via TIPI, interestingness, action quality).

## Key Results
- GPT-4o achieves 49% Validity-Check Pass Rate for game creation, with other models performing significantly worse
- Best-performing model (Gemini 2.0 Flash Exp) achieves mechanic score of 0.765, highlighting challenges in following complex game mechanics
- Temperature creates measurable trade-off: higher temperatures improve subjective scores but degrade mechanical correctness
- Human evaluation reveals significant discrepancies with LLM-as-judge assessments, particularly for personality consistency

## Why This Works (Mechanism)

### Mechanism 1: BFS Validity Checker for Game Soundness Verification
- Claim: The BFS Validity Checker provides automated, deterministic verification of game mechanical correctness without requiring human or LLM judgment.
- Mechanism: Starting from the initial game state, the algorithm performs breadth-first search through all reachable states by iteratively checking which events have satisfied entering conditions, applying their success/failure effects, and tracking whether both success and losing termination conditions are achievable. A game is valid only if every declared event is triggered at least once AND both terminal outcomes are reachable within 10,000,000 explored states.
- Core assumption: Game mechanics can be represented as a finite, enumerable state space where logical correctness maps to reachability.
- Evidence anchors: [abstract] states "Objective measures verify adherence to event mechanics and check variable updates without requiring human intervention." [section 4.1] shows Algorithm 1 with complete BFS procedure.

### Mechanism 2: Event-State Representation Decouples Narrative from Mechanics
- Claim: Structured event-state representation enables independent evaluation of narrative quality (subjective) and mechanical correctness (objective).
- Mechanism: State variables (numerical with min/max bounds) are modified exclusively by game events through defined entering conditions, success conditions, and effects. The LLM generates narrative text that describes event outcomes, but mechanical correctness is verified by comparing actual state updates against declared event effects—completely bypassing narrative content.
- Core assumption: LLMs can maintain internal consistency between their narrative generation and the underlying state machine they're simulating.
- Evidence anchors: [section 3.1] defines State Variables and Game Events, while [section 4.2] describes automated checks for event condition and variable update errors.

### Mechanism 3: Temperature-Mediated Trade-off Between Creativity and Consistency
- Claim: Sampling temperature creates a measurable trade-off: higher temperatures improve subjective scores (interestingness, factual consistency) while lower temperatures improve mechanical correctness.
- Mechanism: Higher temperature increases output diversity, reducing repetitive/neutral content (improving FAC, INT) but introducing more randomness into state updates and event planning (degrading MEC). Lower temperature constrains outputs toward more deterministic mechanical execution at the cost of creative variation.
- Core assumption: The relationship between temperature and output quality is monotonic within the evaluated range for each metric dimension.
- Evidence anchors: [section 5.3, Table 4] shows FAC increasing from 0.920→0.952, INT increasing 0.502→0.538, but MEC peaking at 0.693 (temperature 0.2) and dropping to 0.629 (temperature 0.5).

## Foundational Learning

- **Concept**: State Machine Formalism (finite state automata with transitions)
  - Why needed here: The entire RPGBench evaluation framework treats games as state machines where events are transitions and termination conditions define accepting states. Understanding reachability analysis is prerequisite to interpreting validity check results.
  - Quick check question: Given events E1→E2→E3 where E3 requires state variable V>50 but only E2 modifies V (from initial 0 to +30), is this game valid? (Answer: No—E3 is unreachable)

- **Concept**: BFS vs DFS for Exhaustive Search
  - Why needed here: The validity checker uses BFS specifically to explore states level-by-level. Understanding why BFS is chosen over DFS helps explain its guarantees (finds shortest paths, systematically explores all states up to limit) and limitations (memory-intensive for wide state spaces).
  - Quick check question: Why might BFS fail to validate a game with a long but valid event chain that exceeds the state limit before reaching termination? (Answer: BFS explores breadth-first, potentially hitting the 10M state limit before reaching deep terminal states)

- **Concept**: LLM-as-a-Judge Evaluation Paradigm
  - Why needed here: Subjective metrics (interestingness, action quality, personality consistency) rely entirely on GPT-4o as evaluator. Understanding the limitations and biases of this approach is critical for interpreting results—the human evaluation shows Pearson correlation as low as -0.691 for personality consistency.
  - Quick check question: If LLM-as-judge shows 0.95 agreement with human evaluation on metric A but -0.3 correlation on metric B, what can you conclude? (Answer: Metric A's automated evaluation is reliable; Metric B's automated evaluation may be measuring something different from human judgment)

## Architecture Onboarding

- **Component map**: Game Creation Pipeline (Wikipedia character → 5-shot prompt → LLM generates JSON game → Format Check → BFS Validity Checker → Valid Game Pool) -> Game Simulation Loop (Valid Game + Simulated Player Action → Event Planning → Game Narration → State Update → Repeat) -> Evaluation Suite (Objective metrics via automated checks + Subjective metrics via GPT-4o judge) -> Human Evaluation Interface (Round-by-round annotation with TIPI personality assessment)

- **Critical path**: Game Creation validity bottleneck—only 49% of GPT-4o games pass validity check; lower-performing models produce <5% valid games. The bottleneck determines downstream GS evaluation sample size.

- **Design tradeoffs**:
  1. **10-round truncation vs. complete gameplay**: Truncation maintains computational feasibility but may miss late-game mechanical degradation (results show MEC declines with more rounds).
  2. **TIPI-based PER vs. direct personality rating**: TIPI is more structured but shows negative correlation with human judgment; direct rating has higher human agreement but more subjective bias.
  3. **Automated vs. human evaluation**: Automated metrics provide scalability and consistency but human evaluation reveals LLM-judge misalignment on subjective dimensions.

- **Failure signatures**:
  1. **Event Condition Error (ECE)**: LLM triggers events when entering conditions are unsatisfied—suggests poor state tracking
  2. **Variable Update Error (VUE)**: State variables don't update per event effects—suggests arithmetic/logical failures in state management
  3. **Reachability failure**: Generated games contain unreachable events—suggests incomplete planning during game creation
  4. **Format check rejection**: Malformed JSON (especially Claude 3.5 Sonnet at 95% rejection)—suggests instruction-following failures

- **First 3 experiments**:
  1. **Baseline reproduction**: Run Game Creation on 10 characters with GPT-4o, validate BFS checker produces reported ~0.49 VCR, inspect failed games to categorize failure modes (reachability vs. termination)
  2. **Temperature sweep**: Fix one valid game, run Game Simulation at temperatures [0.0, 0.2, 0.5, 0.8, 1.0] for 10 rounds each, plot FAC/INT/MEC curves to validate claimed trade-offs
  3. **Long-trajectory stress test**: Take 5 valid games, simulate for 50 rounds (vs. standard 10), measure MEC degradation rate to quantify long-context mechanical consistency failure

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What methods can substantially improve LLMs' ability to maintain consistent game mechanics beyond the current ~0.765 MEC ceiling?
- **Basis in paper**: [explicit] "Even the best-performing model, Gemini 2.0 Flash Exp, only achieves a 0.765 MEC score, highlighting the inherent difficulty of precisely following complex game mechanics in a text-based RPG setting."
- **Why unresolved**: The paper identifies the problem but does not propose solutions. The gap between strong storytelling scores (FAC, INT) and weaker mechanical correctness suggests fundamental challenges in balancing creativity with rule adherence.
- **What evidence would resolve it**: Experiments with fine-tuning, Chain-of-Thought prompting for state tracking, external memory mechanisms, or hybrid neuro-symbolic approaches achieving MEC > 0.9 while maintaining creative quality.

### Open Question 2
- **Question**: How would an agent-based player simulation (vs. random action selection) affect LLM game engine performance and the benchmark's ability to reveal model weaknesses?
- **Basis in paper**: [explicit] "future work could focus on expanding the character pool and exploring agent-based simulation framework."
- **Why unresolved**: The current framework uses random action selection. An intelligent agent might expose different failure modes by finding edge cases or exploiting mechanic inconsistencies.
- **What evidence would resolve it**: Comparison experiments using strategic LLM-based player agents versus random selection, analyzing differences in error rates, game completion rates, and types of mechanic violations.

### Open Question 3
- **Question**: What evaluation methodology can reliably assess personality consistency given the low agreement between human annotators and divergence between TIPI-based (PER) and direct evaluation (PERd) approaches?
- **Basis in paper**: [explicit] The paper reports negative correlation between automatic and human PER scores (-0.691 Pearson), negative human-human agreement for PER (-0.310), and notes "warranting further research into more reliable personality evaluation methodologies."
- **Why unresolved**: Neither TIPI-based nor direct evaluation achieves stable, reproducible assessments; the mismatch between LLM and human assessments through TIPI remains unexplained.
- **What evidence would resolve it**: A new evaluation protocol achieving high inter-annotator agreement and high human-automatic correlation.

## Limitations
- Evaluation methodology constraints: LLM-as-judge approach shows variable correlation with human evaluation, suggesting automated subjective assessment may not reliably capture what humans value
- Generalization concerns: Benchmark uses fictional characters from Wikipedia; unclear whether performance generalizes to real-world RPG scenarios or different domain types
- Scope boundaries: Results focus on English-language RPGs with structured state-event mechanics; doesn't address open-ended creative generation or real-time interactive scenarios

## Confidence
- **High Confidence**: BFS Validity Checker correctly identifies game soundness through reachability analysis; Temperature affects output diversity and mechanical consistency; LLMs can generate valid game structures when properly prompted
- **Medium Confidence**: Decoupling narrative from mechanics is feasible through state representation; Subjective metrics capture distinct aspects of game quality; Mechanical consistency degrades over longer trajectories
- **Low Confidence**: LLM-as-judge provides reliable automated evaluation for all subjective metrics; The benchmark captures all relevant aspects of LLM game engine capabilities; Temperature effects generalize across all model architectures

## Next Checks
1. **Human-LLM Judge Alignment Study**: Select 50 game simulation outputs spanning the full range of LLM-as-judge scores. Have human raters evaluate these using identical rubrics for FAC, PER, INT, and ACT metrics. Compute Pearson/Spearman correlations to identify which subjective metrics show reliable automated assessment versus those requiring human judgment.

2. **Long-Trace Mechanical Consistency Test**: Using the 125 valid games from GC evaluation, run Game Simulation for 50 rounds (vs. standard 10) on a subset of 10 games per model. Track MEC, ECE, and VUE metrics per round to quantify degradation rates and identify failure modes that only emerge in extended gameplay sessions.

3. **Temperature Sweep Generalization**: For each model (Claude 3.5 Sonnet, DeepSeek V3, Gemini 1.5 Pro, Gemini 2.0 Flash Exp, GPT-4o), run GS evaluation at temperatures [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] using the same valid game set. Plot FAC/INT/MEC curves to determine whether the observed trade-off generalizes and identify optimal temperature ranges for different models.