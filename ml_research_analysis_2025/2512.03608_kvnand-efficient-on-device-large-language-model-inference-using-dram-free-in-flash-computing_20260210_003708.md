---
ver: rpa2
title: 'KVNAND: Efficient On-Device Large Language Model Inference Using DRAM-Free
  In-Flash Computing'
arxiv_id: '2512.03608'
source_url: https://arxiv.org/abs/2512.03608
tags:
- uni00000013
- uni00000012
- uni00000011
- cache
- flash
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes KVNAND, the first DRAM-free architecture that
  stores both LLM weights and KV cache entirely in compute-enabled 3D NAND flash,
  addressing the severe memory pressure from long-context LLMs on edge devices. The
  key innovation is offloading all memory-bound operations (QKV generation, self-attention,
  FFN) to IFC, eliminating costly KV cache transfers to DRAM.
---

# KVNAND: Efficient On-Device Large Language Model Inference Using DRAM-Free In-Flash Computing

## Quick Facts
- arXiv ID: 2512.03608
- Source URL: https://arxiv.org/abs/2512.03608
- Reference count: 40
- Primary result: DRAM-free IFC architecture storing weights and KV cache entirely in compute-enabled 3D NAND flash achieves 1.98×/1.94×/2.05× geomean speedup at 128/1K/10K-token contexts versus DRAM-equipped IFC, resolving OOM at 100K context and reducing memory cost by 69%.

## Executive Summary
This paper presents KVNAND, the first DRAM-free architecture for on-device LLM inference that stores both model weights and KV cache entirely in compute-enabled 3D NAND flash. By offloading all memory-bound operations (QKV generation, self-attention, FFN) to IFC, KVNAND eliminates costly KV cache transfers to external DRAM and enables long-context inference on resource-constrained edge devices. The design introduces two variants - KVNAND-Discrete (weight and KV cache in separate IFC groups) and KVNAND-Compact (co-located storage) - along with a design space exploration framework to balance performance and resource allocation. Evaluations demonstrate significant performance improvements and resolution of out-of-memory failures at extreme context lengths.

## Method Summary
KVNAND extends compute-enabled 3D NAND flash with integrated processing elements to offload all memory-bound GEMV operations from the NPU during LLM decode phase. The architecture uses hybrid-bonded 3D NAND with PEs integrated on the logic die, performing near-data computation without moving large KV tensors to external DRAM. Two variants are proposed: KVNAND-Discrete partitions IFC dies into groups for weights/QKV generation and KV cache/attention to enable pipeline parallelism, while KVNAND-Compact co-locates storage for maximum tensor parallelism. A design space exploration framework determines optimal G1/G2 die allocation based on model characteristics and quantization. The system employs head-group parallelism to overlap QKV generation with self-attention computation and implements page-level KV cache mapping to align with flash access patterns and reduce redundant reads.

## Key Results
- KVNAND achieves 1.98×/1.94×/2.05× geomean speedup at 128/1K/10K-token contexts compared to DRAM-equipped IFC designs
- Resolves out-of-memory failures at 100K context length that plague baseline systems
- Reduces memory cost by 69% through elimination of external DRAM requirements
- Page-level KV cache mapping reduces attention latency to 1.9%–66.7% of unmapped baseline depending on model and context length
- Head-group parallelism reduces latency to 82.4%–98.2% of baseline without optimization

## Why This Works (Mechanism)

### Mechanism 1: Unified Weight-KV Storage with IFC Offload
KVNAND stores both model weights and KV cache entirely in compute-enabled 3D NAND flash while offloading all memory-bound GEMV operations to IFC. This eliminates costly DRAM transfers by performing QKV generation, attention computation, and FFN layers directly on the flash dies using integrated PEs. Newly generated KV entries accumulate in SRAM-based buffers before batch writes to flash, amortizing program latency. The architecture leverages flash internal bandwidth (32 GB/s across 4 dies) combined with IFC compute throughput to service memory-bound decode operations faster than traditional DRAM-NPU round-trips.

### Mechanism 2: Head-Group Parallelism with Discrete Group Partitioning
KVNAND-D partitions IFC dies into two groups (G1 for weights/QKV generation, G2 for KV cache/attention) enabling head-group pipeline parallelism that overlaps QKV generation with self-attention computation. Head groups are defined as one KV pair plus its associated Q heads, allowing KVNAND-D to execute HG1 QKV generation in G1 while simultaneously performing HG1 self-attention in G2. The two-stage register pipeline overlaps flash page reads with PE computation. For GQA models, each KV fetch is reused across multiple Q heads, requiring scaled PE count to balance bandwidth and compute.

### Mechanism 3: Page-Level KV Cache Mapping for Flash Alignment
KVNAND organizes KV cache storage so that entries from the same layer and head group occupy contiguous flash pages, reducing redundant page reads and improving both performance and reliability. Without optimization, KV entries written in generation order mix data across layers/heads within pages, causing attention to read multiple pages for required tokens. KVNAND's mapping ensures spatial locality: each page holds contiguous tokens for a single (layer, head) pair. Head-parallel KV generation distributes different heads across planes, allowing each plane's KV buffer to accumulate entries until page-full before write-back.

## Foundational Learning

- **Concept: KV Cache Scaling and Memory Pressure**
  - Why needed: Understanding that KV cache grows linearly with context length and can exceed model weight size motivates the DRAM-free design goal
  - Quick check: For a 7B model with 32 layers, 32 heads, and 128 head dimension at BF16 precision, how much KV memory is needed for a 50K token context? (Answer: ~1.6 GB using formula from Section III-B)

- **Concept: Arithmetic Intensity and Memory-Boundedness**
  - Why needed: Decode-phase GEMV has extremely low arithmetic intensity (~1 OPS/Byte at FP16), making it memory-bound and justifying offloading to IFC
  - Quick check: Why does single-batch decode exhibit lower arithmetic intensity than prefill, and how does GQA's h/k ratio affect attention's position on the roofline? (Answer: Prefill uses GEMM with weight reuse; decode uses GEMV without batch-level reuse. GQA increases attention's arithmetic intensity by h/k×, potentially pushing it past the roofline intersection per Figure 4(a))

- **Concept: Flash Access Granularity and Page Organization**
  - Why needed: Flash operates at page-level granularity (4KB + ECC in KVNAND); understanding that reading a page retrieves all its data explains why KV cache layout misalignment causes redundant reads
  - Quick check: If attention requires K vectors from tokens 0-999 for a specific layer and head, but the flash page layout mixes tokens 0-99 from multiple layers, how many page reads are wasted? (Answer: Proportional to the number of layers/heads sharing the page; Section IV.D quantifies this as a major latency contributor)

## Architecture Onboarding

- **Component map:**
  - NPU -> Flash Controller -> IFC Dies (8 channels, each with independent flash controller interface)
  - IFC Dies: 3D NAND flash arrays hybrid-bonded to logic dies containing PEs (16 FMACs per plane), cache/data registers, ECC, and global buffers
  - SoC-side components: 5 MB SRAM for KVNAND-D, 8 KB KV buffer per plane for KVNAND-C

- **Critical path:**
  1. Prefill: NPU streams weights from flash, performs QKV projection (GEMM), self-attention, FFN; KV tensors written to flash
  2. Decode - QKV Generation: IFC reads W_QKV pages from G1 (Discrete) or local (Compact); PEs compute Q/K/V; results sent to NPU then G2/flash
  3. Decode - Attention: IFC reads KV cache from G2/flash; computes Q×K^T (Logit) in PEs; NPU performs Softmax; IFC computes S×V (Attend)
  4. Decode - FFN: IFC reads FFN weights, computes FC layers; NPU handles activation/residual
  5. KV Write-back: Accumulated KV entries in buffers written to flash when page-full

- **Design tradeoffs:**
  - KVNAND-D vs. KVNAND-C: Discrete enables pipeline parallelism but sacrifices tensor parallelism; Compact maximizes tensor parallelism but cannot overlap QKV and attention due to bandwidth contention
  - G1/G2 Allocation: More G1 dies accelerate FFN (dominant at short contexts); more G2 dies accelerate attention (dominant at long contexts)
  - Buffer Size vs. Write Frequency: Larger buffers enable more page-aligned writes but increase area/power; KVNAND-C constrained to 8 KB/plane necessitates head-parallel generation

- **Failure signatures:**
  - OOM at long contexts: Occurs when G2 allocation insufficient for KV cache storage; manifests as blank cells in DSE heatmaps
  - Latency degradation vs. Base-2: If external bandwidth or PE count is insufficient for GQA's higher arithmetic intensity, IFC provides limited benefit
  - Read disturb accumulation: Blocks storing early KV entries experience high cumulative page-read counts

- **First 3 experiments:**
  1. Baseline comparison at 1K/10K context: Measure decode throughput for LLaMA3.1-8B and LLaMA2-70B on Base-1 (DRAM KV), Base-2 (naive flash KV), KVNAND-C, and KVNAND-D. Verify ~2× speedup vs. Base-1 and resolution of OOM at 100K context.
  2. DSE sweep for G1/G2 allocation: Run 8-die configuration with G1=1 to G1=7 for 30B MHA and 70B GQA models under W4A16 and W8A8 quantization at 1K/10K/50K/100K contexts. Verify optimal allocation shifts toward G2 as context increases.
  3. Ablation of HG parallelism and page-level mapping: Disable HG parallelism in KVNAND-D and measure latency increase. Disable page-level mapping in KVNAND-C and measure attention latency increase.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does KVNAND's performance scale under multi-batch inference scenarios, beyond the single-batch autoregressive decoding evaluated in this work?
- Basis: The paper states: "In single-batch on-device LLM inference, the absence of batch-level parallelism eliminates weight reuse across tokens, further lowering arithmetic intensity" and focuses evaluation exclusively on single-batch workloads
- Why unresolved: The design optimizes for memory-bound single-batch GEMV operations; batched inference would change arithmetic intensity, potentially altering the IFC-NPU workload partitioning and invalidating current dataflow assumptions
- What evidence would resolve it: Comparative throughput and latency measurements across batch sizes 1-8, with analysis of how head-group parallelism and KV cache mapping perform when serving multiple concurrent requests

### Open Question 2
- Question: What are the actual thermal characteristics of KVNAND under sustained long-context inference, rather than relying on projections from prior IFC designs?
- Basis: The paper states: "In terms of thermal throttling, prior analysis of 3D-stacked IFC dies [67] has shown that their power density is acceptable. KVNAND adopts the same flash configuration... so its thermal profile is expected to remain similar"
- Why unresolved: KVNAND-C adds per-plane KV buffers and ECC encoding logic on-die, and KVNAND-D sustains higher I/O activity during long-context attention—these may create thermal hotspots not present in weight-only IFC designs
- What evidence would resolve it: Thermal simulation or silicon measurements under continuous 100K-token inference workloads, with junction temperature monitoring across the logic die

### Open Question 3
- Question: Can KVNAND's architecture effectively support sparse attention mechanisms and token eviction strategies without degrading the benefits of page-level KV cache mapping?
- Basis: The paper states: "Building on its ability to mitigate KV cache fragmentation, the KVNAND architecture has the potential to support diverse sparsity schemes as well" and mentions sparse attention as orthogonal future work
- Why unresolved: Page-level mapping assumes sequential token access within heads; sparse attention patterns (e.g., sliding window, heavy-hitter eviction) would create non-contiguous access, potentially increasing page-read amplification and undermining locality gains
- What evidence would resolve it: Evaluation of KVNAND with representative sparse attention algorithms (H2O, StreamingLLM), measuring page-read counts, latency, and accuracy tradeoffs

### Open Question 4
- Question: What is the actual silicon area and power overhead of integrating ECC encoding capability on the IFC logic die for KVNAND-C?
- Basis: The paper notes KVNAND-C requires "ECC encoding must be performed on the logic die to protect the KV data before writing to flash... This adds some area and power overhead on the logic die, though the cost is modest since ECC encoding typically requires only about one-quarter of the area of decoding"
- Why unresolved: This estimate derives from general ECC characteristics, not from synthesizing the specific BCH(9088, 8192, 64) encoder in the target 20nm process; actual overhead may impact the feasibility of per-plane KV buffer + encoder integration within area constraints
- What evidence would resolve it: Post-synthesis area and power numbers for the complete KVNAND-C plane logic including ECC encoder, verified against the stated 0.0737 mm² per-plane budget

## Limitations
- Flash timing model assumes uniform latencies that may not hold under variable wear conditions and temperature effects
- KV buffer size constraint of 8KB per plane may become prohibitive for models with very large head dimensions
- Head-group parallelism relies on precise latency matching that may not be achievable across all model architectures
- Page-level mapping assumes uniform token distribution, but real workloads may exhibit non-uniform access patterns
- Simulation accuracy depends on fidelity of SSDsim and NVSim models to real 3D NAND behavior

## Confidence
- **High Confidence**: The fundamental premise that DRAM-free IFC architecture can eliminate KV cache transfer bottlenecks; the mechanism of head-group parallelism for overlapping QKV generation and attention; the page-level KV cache mapping for reducing redundant page reads
- **Medium Confidence**: The optimal G1/G2 die allocation determined by the DSE framework; the scalability claims for 100K context length without OOM failures; the memory cost reduction of 69%
- **Low Confidence**: The exact performance improvement factor of 1.98× geomean; the energy efficiency claims (J/token); the read disturbance mitigation through access-aware block allocation

## Next Checks
1. **Long-term endurance validation**: Implement a wear-leveling simulation that tracks cumulative page reads per block over extended operation (e.g., 100K inference cycles at 10K context length). Measure read disturbance-induced bit error rate growth and verify that access-aware block allocation prevents premature block retirement compared to naive round-robin allocation.

2. **Cross-model scalability verification**: Test KVNAND-D and KVNAND-C on additional model architectures including transformer variants with different attention mechanisms (e.g., FlashAttention-2, state-space models) and MoE models with sparse expert activation patterns. Measure whether the head-group parallelism benefits persist when attention arithmetic intensity deviates significantly from the assumed baseline.

3. **Hardware-in-the-loop timing validation**: Compare simulation results against measurements from a prototype IFC implementation using real 3D NAND hardware. Focus on measuring actual page read/write latencies under varying load conditions, quantifying the gap between simulated 32 GB/s internal bandwidth and measured performance, and validating the timing of head-group synchronization under realistic noise conditions.