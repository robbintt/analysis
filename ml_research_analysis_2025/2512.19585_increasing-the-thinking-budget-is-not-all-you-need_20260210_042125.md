---
ver: rpa2
title: Increasing the Thinking Budget is Not All You Need
arxiv_id: '2512.19585'
source_url: https://arxiv.org/abs/2512.19585
tags:
- reasoning
- thinking
- budget
- configurations
- compute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how reasoning budgets (compute allocated to
  thinking tokens) interact with different reasoning strategies in Large Language
  Models. The authors compare configurations like vanilla, self-consistency, summary,
  and reflection across varying thinking budgets using three models (Qwen3-8B, Qwen3-4B,
  DeepSeek-R1-Distill-Llama-8B) on AIME24/AIME25 benchmarks.
---

# Increasing the Thinking Budget is Not All You Need

## Quick Facts
- arXiv ID: 2512.19585
- Source URL: https://arxiv.org/abs/2512.19585
- Reference count: 10
- Simply increasing thinking budget does not guarantee better performance; optimal compute allocation depends on strategy choice rather than raw budget size.

## Executive Summary
This paper investigates how reasoning budgets interact with different reasoning strategies in Large Language Models. The authors evaluate vanilla, self-consistency, summary, and reflection configurations across varying thinking budgets using three models on AIME24/AIME25 benchmarks. They find that summary consistently yields the highest accuracy, while self-consistency struggles due to answer extraction issues. The results demonstrate that optimal compute allocation depends on strategy choice rather than just increasing the thinking budget.

## Method Summary
The study evaluates four reasoning strategies—Vanilla (single call), Self-Consistency (3x/5x calls with majority voting), Summary (3x/5x calls plus consolidation), and Reflection (1-2 refinement steps)—across thinking budgets of 0-24K tokens on AIME24 and AIME25 datasets. The "Wait" token mechanism ensures minimum thinking token allocation, while temperature settings vary by strategy (0 for most, 1.0 for ensemble methods). Three models are tested: Qwen3-8B, Qwen3-4B, and DeepSeek-R1-Distill-Llama-8B. Performance is measured by accuracy (% correct answers) across different budget levels.

## Key Results
- Summary configuration consistently yields the highest accuracy across all tested models and budgets
- Self-consistency struggles due to formatting issues during answer extraction from parallel traces
- Judge-LLM setups show promise by refining or regenerating answers based on correctness feedback
- Increasing parallel traces from 3x to 5x fails to improve performance in Self-Consistency and Summary configurations

## Why This Works (Mechanism)
None

## Foundational Learning
- **Thinking Budget Management**: Controlling compute allocation for reasoning tokens
  - Why needed: Ensures fair comparison across strategies and prevents token starvation
  - Quick check: Verify actual thinking tokens match specified budgets using token counting
- **Ensemble Methods**: Generating multiple responses and consolidating
  - Why needed: Leverages diverse reasoning paths to improve accuracy
  - Quick check: Measure answer diversity across parallel traces
- **Answer Extraction**: Parsing model outputs to identify final answers
  - Why needed: Critical for majority voting in Self-Consistency; failures degrade performance
  - Quick check: Test extraction regex on sample outputs with varying formats

## Architecture Onboarding

**Component Map**: User Query -> Strategy Selection -> Budget Allocation -> LLM Calls -> Answer Consolidation -> Final Output

**Critical Path**: The most compute-intensive path involves multiple parallel LLM calls followed by consolidation, particularly in Summary and Self-Consistency configurations. The Judge-LLM path adds an additional verification step that can trigger regeneration.

**Design Tradeoffs**: Parallel calls (Self-Consistency, Summary) trade increased API costs for potential accuracy gains through diversity, while sequential refinement (Reflection) trades latency for potentially deeper reasoning. The choice depends on whether diversity or depth is more valuable for the task.

**Failure Signatures**: Self-consistency fails when answer extraction consistently misidentifies responses across parallel traces, causing majority voting to select incorrect answers. Reflection may get stuck in reasoning loops when refinement steps don't converge to correct answers.

**First Experiments**: 
1. Run Summary 3x at 2000 tokens budget and verify consolidation quality
2. Test Self-Consistency answer extraction on 10 random outputs to identify formatting issues
3. Compare Judge-LLM performance against Vanilla at 4000 token budget

## Open Questions the Paper Calls Out

**Open Question 1**: How do these findings generalize across a broader set of LLMs and reasoning benchmarks?
- Basis in paper: [explicit] "A larger sample both in terms of models and benchmarks is necessary to fully validate our current findings."
- Why unresolved: Only three models tested on AIME mathematical reasoning tasks
- What evidence would resolve it: Systematic evaluation across additional model families and diverse reasoning domains

**Open Question 2**: Why do weaker reasoning models fail to benefit from sophisticated configurations like Self-Consistency and Summary?
- Basis in paper: [inferred] Weaker models "struggle to fully benefit from the configurations lagging behind on most experiments against Vanilla"
- Why unresolved: Authors observe phenomenon but don't investigate underlying causes
- What evidence would resolve it: Controlled experiments analyzing intermediate outputs and error patterns across model capabilities

**Open Question 3**: What is the optimal allocation between parallel API calls versus extended sequential reasoning under fixed compute budgets?
- Basis in paper: [explicit] "No effort yet to understand how the amount of compute performs depending on the configuration where it is used"
- Why unresolved: Summary outperforms Vanilla, but trade-offs remain underexplored
- What evidence would resolve it: Fine-grained ablation studies varying number of calls and per-call thinking budget

**Open Question 4**: Why does increasing parallel traces from 3x to 5x fail to improve performance in Self-Consistency and Summary configurations?
- Basis in paper: [inferred] "Increasing the amount of parallel traces is not bringing further benefits"
- Why unresolved: Counterintuitive finding not analyzed further
- What evidence would resolve it: Analysis of answer diversity and convergence patterns across parallel traces

## Limitations
- Evaluation limited to mathematical reasoning benchmarks (AIME24/25), limiting generalizability
- "Wait" token forcing mechanism lacks detailed implementation specifications
- Self-consistency's underperformance attributed to "formatting issues" without providing extraction code
- Limited exploration of temperature sensitivity and its interaction with budget levels

## Confidence

**High Confidence**: The finding that increasing thinking budget alone doesn't guarantee improved accuracy is well-supported by systematic budget sweep results across all three models.

**Medium Confidence**: The claim that Summary consistently yields highest accuracy has strong support on AIME benchmarks but may not extend to other domains.

**Low Confidence**: The comparative effectiveness of Judge-LLM setups remains unclear due to insufficient detail on implementation parameters.

## Next Checks

1. **Reproduce the "Wait" token mechanism**: Implement the exact token insertion frequency and format from the referenced Muennighoff et al. work, then verify that actual thinking tokens match the specified budgets across all configurations.

2. **Validate answer extraction for self-consistency**: Implement the answer extraction regex/pattern using sample outputs from the paper's appendix, then test on 50 random responses from each budget level.

3. **Cross-domain generalization test**: Apply the best-performing configuration (Summary) to a non-mathematical reasoning benchmark like coding problems (HumanEval) or commonsense reasoning (StrategyQA).