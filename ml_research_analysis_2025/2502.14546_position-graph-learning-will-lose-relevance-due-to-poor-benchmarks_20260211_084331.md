---
ver: rpa2
title: 'Position: Graph Learning Will Lose Relevance Due To Poor Benchmarks'
arxiv_id: '2502.14546'
source_url: https://arxiv.org/abs/2502.14546
tags:
- graph
- learning
- graphs
- datasets
- benchmarks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that graph learning research is hindered by poor
  benchmarking practices, which prevent the development of impactful, generalizable
  models. Current benchmarks often focus on narrow domains (like 2D molecular graphs)
  without real-world justification, use datasets that poorly represent underlying
  data, and lack standardized evaluation protocols.
---

# Position: Graph Learning Will Lose Relevance Due To Poor Benchmarks

## Quick Facts
- arXiv ID: 2502.14546
- Source URL: https://arxiv.org/abs/2502.14546
- Authors: Maya Bechler-Speicher; Ben Finkelshtein; Fabrizio Frasca; Luis Müller; Jan Tönshoff; Antoine Siraudin; Viktor Zaverkin; Michael M. Bronstein; Mathias Niepert; Bryan Perozzi; Mikhail Galkin; Christopher Morris
- Reference count: 40
- Primary result: Graph learning research is hindered by poor benchmarking practices that prevent development of impactful, generalizable models

## Executive Summary
This paper argues that graph learning research is stagnating due to flawed benchmarking practices that prevent meaningful progress. Current benchmarks often use datasets that poorly represent real-world data, lack standardized evaluation protocols, and encourage overfitting to spurious correlations rather than task-relevant signal. The authors demonstrate through experiments that graph structure is often not meaningful for the task at hand, and that simple baselines can be competitive when properly tuned. They call for a shift toward transformative real-world applications, improved dataset construction with clear justifications for graph structure, and robust evaluation frameworks that would drive impactful advances in the field.

## Method Summary
The paper conducts four key experiments to demonstrate flaws in current graph learning benchmarks. First, they compare GNNs against DeepSets (empty graphs) and Cayley expander graphs to test whether graph structure contributes meaningfully to performance. Second, they re-tune a simple GINE baseline on PCQM4Mv2 with modern practices (20-layer depth, LayerNorm after skip connections) to show that baseline inflation explains much of the performance gap to newer architectures. Third, they analyze heterophilic datasets to understand where standard GNNs fail. Fourth, they explore multi-task pre-training viability for foundation models, showing that frozen processors can enable sample-efficient fine-tuning across domains.

## Key Results
- Graph structure is often not causally necessary for prediction performance, with empty graphs and Cayley expanders matching or exceeding original graph connectivity in multiple experiments
- Reported performance gaps between architectures are inflated by unequal hyperparameter tuning effort, with re-tuned GINE closing >70% of the gap to graph transformers
- Current benchmarks encourage overfitting to spurious correlations rather than meaningful progress, focusing on narrow domains without real-world justification
- The authors propose shifting focus to transformative real-world applications and developing robust evaluation frameworks with clearer dataset construction justifications

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Graph structure in many benchmarks does not causally contribute to prediction performance.
- **Mechanism:** When synthetic or null graph structures match or outperform original graph connectivity, the original edges encode spurious correlations rather than task-relevant signal. MPNNs overfit to structural noise while set-based baselines generalize better by ignoring connectivity entirely.
- **Core assumption:** If graph structure were causally necessary, removing it should consistently degrade performance.
- **Evidence anchors:**
  - [abstract]: "demonstrate through experiments that graph structure is often not meaningful for the task at hand"
  - [Section 7.1, Table 1]: DeepSets (empty graph) outperforms GraphConv with original structure on MOLBBBP (64.90 vs 64.11); Cayley graphs match or exceed original structure in 5/9 experiments
  - [corpus]: Limited direct corroboration; neighbor papers focus on molecular applications rather than benchmark critique
- **Break condition:** If domain-specific preprocessing (e.g., 3D molecular conformers) restores predictive graph structure, the mechanism fails—suggesting the issue is construction quality, not graph abstraction itself.

### Mechanism 2
- **Claim:** Reported performance gaps between architectures are inflated by unequal hyperparameter tuning effort.
- **Mechanism:** New architectures receive extensive tuning while baselines use outdated configurations. When baselines are re-tuned with modern practices (deeper networks, better normalization, longer training), performance gaps collapse.
- **Core assumption:** Performance differences persisting under equal tuning budgets reflect genuine architectural advantages.
- **Evidence anchors:**
  - [Section 7.2, Table 2]: Re-tuned GINE (20-layer, 22.7M params) achieves 0.0898 MAE vs. original 0.1195, closing >70% of gap to graph transformers
  - [Section 4]: "newly proposed architectures are often unfairly compared to outdated baselines, with hyperparameters fine-tuned on a small number of datasets but not for baselines"
  - [corpus]: GraphBench paper echoes fragmented evaluation concerns but provides no empirical tuning analysis
- **Break condition:** If re-tuned complex architectures show proportional gains, the mechanism fails—suggesting both benefit from better practices equally.

### Mechanism 3
- **Claim:** Multi-task pre-training with frozen processors enables cross-domain transfer for sample-efficient fine-tuning.
- **Mechanism:** A shared processor learns general graph representations from diverse upstream tasks (molecules, vision, malware detection). Task-specific encoders/decoders adapt these representations, with frozen processors providing stable initialization that accelerates early fine-tuning.
- **Core assumption:** Graph representations transfer across domains despite differing symmetries and feature spaces.
- **Evidence anchors:**
  - [Section 7.4, Figure 2]: Pre-trained models show stronger early-step performance on PASCAL VOC-SP and STARGAZERS downstream tasks
  - [Section 5]: Proposes encoder-processor-decoder architecture as pathway to foundation models
  - [corpus]: Neighbor papers explore molecular pre-training but don't address cross-domain transfer explicitly
- **Break condition:** If pre-trained processors show negative transfer across dissimilar domains (e.g., PEPTIDES-STRUCT for MPNN), the universality claim weakens—suggesting domain-specific processors remain necessary.

## Foundational Learning

- **Concept: Message-Passing Neural Networks (MPNNs)**
  - **Why needed here:** The paper's critique targets MPNN assumptions about structural inductive bias. Without understanding aggregation and message functions, you cannot evaluate whether benchmarks test meaningful capabilities.
  - **Quick check question:** Can you explain why adding an MLP after each aggregation layer (as in Platonov et al.) might inflate baseline performance independently of the graph structure?

- **Concept: Heterophily vs. Homophily**
  - **Why needed here:** Section 7.3 discusses heterophilic datasets where standard GNNs struggle. Understanding why local aggregation fails under heterophily clarifies why architectural modifications matter more in some domains.
  - **Quick check question:** In a citation network where connected papers tend to have opposite classes, would you expect GCN to underperform a DeepSets baseline?

- **Concept: Evaluation Protocol Standardization**
  - **Why needed here:** The paper attributes variance in reported results to inconsistent splits, metrics, and training budgets. Understanding cross-validation vs. holdout evaluation reveals why benchmark comparisons lack validity.
  - **Quick check question:** Why might reporting validation accuracy instead of test accuracy systematically overestimate model performance on small datasets like ENZYMES?

## Architecture Onboarding

- **Component map:** Task-Specific Encoder → Processor (MPNN/GT) → Task-Specific Decoder

- **Critical path:**
  1. Identify whether your task genuinely benefits from graph structure (run DeepSets baseline first)
  2. If structure helps, select processor architecture based on graph properties (heterophily → consider rewiring or transformers; homophily → MPNN sufficient)
  3. Tune baselines with equal budget before claiming architectural improvements

- **Design tradeoffs:**
  - MPNN (GINE): Faster, scales to larger graphs, but limited long-range expressivity
  - Graph Transformer (Graphormer): Better long-range modeling, but O(n²) attention and position encoding dependencies
  - Frozen pre-trained processor: Faster fine-tuning, but may transfer poorly to dissimilar domains

- **Failure signatures:**
  - Cayley/empty graphs matching original structure performance → graph construction likely flawed
  - Large variance across random seeds (std > 5% of mean) → dataset too small for reliable comparison
  - New architecture improves only when baseline uses outdated hyperparameters → not a genuine advance

- **First 3 experiments:**
  1. **Sanity check:** Run DeepSets (ignore graph structure) on your benchmark. If it matches MPNN performance, reconsider graph construction before investing in complex architectures.
  2. **Baseline calibration:** Re-implement the simplest MPNN (GCN or GINE) with modern tuning (LayerNorm, deeper networks, cosine LR schedule). Compare against reported baselines to quantify inflation in your target benchmark.
  3. **Ablation audit:** For any claimed architectural improvement, isolate the contribution of each component (e.g., MLP layers, activation changes) by adding them to the baseline individually. Report percentage gain attributable to core innovation vs. implementation choices.

## Open Questions the Paper Calls Out

None

## Limitations
- Claims about graph structure irrelevance depend on benchmark construction quality, not universal properties of graph learning
- Cross-domain transfer claims (pre-training section) remain largely theoretical without systematic validation
- Limited empirical evidence that proposed applications (combinatorial optimization, databases) would benefit from current graph learning approaches

## Confidence

- **High:** MPNNs overfit to spurious correlations in poorly constructed benchmarks (Section 7.1 experiments)
- **Medium:** Under-tuning of baselines explains much of reported architectural improvements (Section 7.2 calibration results)
- **Low:** Pre-training with frozen processors will enable foundation models for graphs (Section 7.4 remains conceptual)

## Next Checks

1. Apply the DeepSets sanity check to 3 additional molecular property prediction datasets from different sources to test generalizability of structure-irrelevance claims
2. Implement a systematic hyperparameter sweep (10+ configurations) for 3 "strong" architectures on a single benchmark to quantify baseline inflation
3. Test cross-domain transfer by pre-training on molecular graphs and fine-tuning on non-molecular tasks (e.g., social networks) to validate processor universality claims