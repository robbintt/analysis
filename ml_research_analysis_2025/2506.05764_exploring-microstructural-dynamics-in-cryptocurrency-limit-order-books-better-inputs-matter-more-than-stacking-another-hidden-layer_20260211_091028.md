---
ver: rpa2
title: 'Exploring Microstructural Dynamics in Cryptocurrency Limit Order Books: Better
  Inputs Matter More Than Stacking Another Hidden Layer'
arxiv_id: '2506.05764'
source_url: https://arxiv.org/abs/2506.05764
tags:
- data
- order
- prediction
- limit
- savitzky
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines whether deeper neural networks genuinely improve
  short-term cryptocurrency price forecasting from limit order book (LOB) snapshots,
  or if simpler models with better preprocessing suffice. It benchmarks logistic regression,
  XGBoost, CatBoost, CNN+LSTM, CNN+XGBoost hybrid, and DeepLOB architectures on BTC/USDT
  100 ms LOB snapshots from Bybit, using binary (up/down) and ternary (up/flat/down)
  classification.
---

# Exploring Microstructural Dynamics in Cryptocurrency Limit Order Books: Better Inputs Matter More Than Stacking Another Hidden Layer

## Quick Facts
- **arXiv ID**: 2506.05764
- **Source URL**: https://arxiv.org/abs/2506.05764
- **Reference count**: 27
- **Primary result**: Simpler models (XGBoost, Logistic Regression) with Savitzky-Golay preprocessing match or exceed deep learning performance in LOB forecasting, achieving 71.5% accuracy at 1-second horizons.

## Executive Summary
This study challenges the assumption that deeper neural networks improve cryptocurrency limit order book (LOB) forecasting. Through systematic benchmarking of six architectures (logistic regression, XGBoost, CatBoost, CNN+LSTM, CNN+XGBoost hybrid, and DeepLOB) on BTC/USDT 100ms LOB snapshots, the research demonstrates that simpler models with proper preprocessing can match or slightly exceed complex networks. The key finding is that Savitzky-Golay smoothing dramatically improves predictive accuracy by isolating local trends from high-frequency noise, while increasing LOB depth from 5 to 40 levels provides substantial accuracy gains. The work emphasizes that data quality and preprocessing often outweigh model complexity in LOB forecasting.

## Method Summary
The study benchmarks six architectures on BTC/USDT LOB snapshots from Bybit (2025-01-30), using binary and ternary classification of mid-price movements at 100ms, 500ms, and 1000ms horizons. LOB data includes top 200 bid/ask levels with millisecond timestamps, missing values represented as NaN. Two denoising methods are applied: Savitzky-Golay smoothing (cubic polynomial, window=21) and Kalman filtering. Hand-crafted features include mid-price, first-level and five-level order imbalance, and weighted mid-price change. Models are evaluated on 80/20 train-test splits with inverse-frequency class weighting for ternary tasks. The analysis focuses on out-of-sample accuracy, F1 score, and inference latency.

## Key Results
- Savitzky-Golay preprocessing consistently outperforms raw data and Kalman filtering across all model architectures
- XGBoost and logistic regression match or slightly exceed DeepLOB performance with faster inference
- Increasing LOB depth from 5 to 40 levels improves accuracy from 58.0% to 71.5% at 1-second horizons
- Kalman filtering underperforms due to sensitivity to parameter tuning

## Why This Works (Mechanism)

### Mechanism 1: Savitzky-Golay Filtering Improves Signal-to-Noise Ratio
SG filtering fits local polynomials to time series data, preserving local curvature while suppressing high-frequency noise. This increases the signal-to-noise ratio before models see the data. The core assumption is that high-frequency noise is largely uninformative while the underlying price signal is smoother than observed snapshots. Evidence shows consistent accuracy improvements when SG filtering is applied across all architectures.

### Mechanism 2: Tree-Based Models Excel at Threshold-Based Relationships
Simpler models match complex networks because hand-crafted features like order flow imbalances explicitly quantify supply-demand asymmetry. Tree-based ensembles partition these specific thresholds efficiently, while deep learning must learn these representations from scratch, risking overfitting on finite datasets. The assumption is that the relationship between order imbalance and future price movement is largely threshold-based or monotonic.

### Mechanism 3: Deeper LOB Levels Reveal Hidden Liquidity
Increasing LOB depth from 5 to 40 levels improves mid-price forecasting by capturing deeper liquidity reservoirs that buffer or predict price moves. The core assumption is that liquidity at deeper levels is genuine and influences price discovery over the next 1000ms. Evidence shows accuracy rising from 58.0% to 71.5% when using 40 levels versus 5 levels.

## Foundational Learning

- **Concept**: Order Imbalance & Mid-Price
  - **Why needed here**: The core predictive target is mid-price movement driven by volume differences between buyers and sellers
  - **Quick check**: If best bid volume is 10 and best ask volume is 2, is order imbalance positive or negative, and what does this typically predict for mid-price?

- **Concept**: Savitzky-Golay Filter
  - **Why needed here**: Identified as the most effective preprocessing step by fitting polynomials to sliding windows while preserving peaks
  - **Quick check**: Why would polynomial fitting be better than simple averaging for smoothing financial time series data?

- **Concept**: XGBoost (Gradient Boosted Trees)
  - **Why needed here**: The "winning" architecture that works by sequentially fitting trees to residuals of previous trees
  - **Quick check**: Why is a model outputting "if-then" rules potentially more robust to noise in 100ms snapshots than a Neural Network?

## Architecture Onboarding

- **Component map**: Raw LOB Snapshots -> Savitzky-Golay Filter -> NaN Drop -> Feature Engineering -> XGBoost Classifier -> Mid-Price Movement Probability
- **Critical path**: Data Preprocessing is the most sensitive step; without SG smoothing, accuracy drops approximately 10%
- **Design tradeoffs**:
  - Depth vs. Coverage: 40 levels improves accuracy (0.715) but reduces samples (5,442) due to NaNs; 5 levels increases samples (18,336) but lowers accuracy (0.58)
  - Latency vs. Complexity: DeepLOB offers slightly lower accuracy but significantly higher inference latency compared to simpler models
- **Failure signatures**:
  - Kalman Filter degradation if process noise Q and measurement noise R are rigidly fixed
  - Flat predictions if ternary classification threshold Îµ is too wide
- **First 3 experiments**:
  1. Implement Logistic Regression with 3 engineered features on raw vs. SG-smoothed data to validate "Better Inputs" hypothesis
  2. Train XGBoost on 5-level vs. 40-level data at 1000ms horizon to reproduce accuracy jump
  3. Retrain best model on 100ms vs. 1000ms horizons to confirm signal strength increases with time aggregation

## Open Questions the Paper Calls Out

- Can adaptive denoising techniques that dynamically adjust smoothing parameters based on market regime outperform fixed-window methods like Savitzky-Golay?
- Do prediction findings from BTC/USDT generalize to less liquid cryptocurrencies and traditional asset LOBs with different microstructural dynamics?
- How do snapshot-based LOB predictions compare to transaction-level event stream predictions in both accuracy and latency?
- Can properly tuned Kalman filtering with adaptive Q/R covariance estimation recover performance to match or exceed Savitzky-Golay smoothing?

## Limitations
- Results based on single day of BTC/USDT data from Bybit, limiting generalizability
- Kalman filtering parameter sensitivity not extensively explored
- Critical model hyperparameters not fully specified
- Ternary classification threshold sensitivity not thoroughly analyzed

## Confidence

**High Confidence**:
- Savitzky-Golay filtering improves predictive accuracy across architectures
- Increasing LOB depth from 5 to 40 levels improves accuracy at 1-second horizons
- Simpler models match or exceed complex networks when inputs are properly preprocessed

**Medium Confidence**:
- Kalman filtering is inferior due to parameter tuning difficulty
- Model complexity is secondary to data preprocessing quality
- 71.5% accuracy represents state-of-the-art performance

**Low Confidence**:
- Generalizability to other cryptocurrencies, exchanges, or market regimes
- Performance on horizons beyond 1 second
- Whether findings extend to institutional-grade trading volumes

## Next Checks

1. **Cross-exchange validation**: Replicate experiment using LOB data from different exchange to assess preprocessing advantage across venues
2. **Extended horizon analysis**: Evaluate model performance at 5s, 10s, and 30s horizons to determine if simpler models maintain advantage
3. **Parameter sensitivity analysis**: Systematically vary Savitzky-Golay window size and polynomial degree to identify optimal parameters and assess robustness