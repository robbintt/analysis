---
ver: rpa2
title: 'Optimising Language Models for Downstream Tasks: A Post-Training Perspective'
arxiv_id: '2506.20917'
source_url: https://arxiv.org/abs/2506.20917
tags:
- performance
- tasks
- training
- learning
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis presents a comprehensive framework for optimizing large
  language models (LLMs) for downstream tasks through advanced post-training techniques.
  The work introduces Decomposed Prompt Tuning (DEPT), which substantially improves
  the efficiency of prompt tuning by decomposing soft prompts into shorter prompts
  and low-rank matrices, achieving over 20% improvements in training efficiency while
  maintaining or improving performance.
---

# Optimising Language Models for Downstream Tasks: A Post-Training Perspective

## Quick Facts
- arXiv ID: 2506.20917
- Source URL: https://arxiv.org/abs/2506.20917
- Authors: Zhengyan Shi
- Reference count: 0
- Primary result: Comprehensive framework for optimizing LLMs for downstream tasks through Instruction Modelling, Decomposed Prompt Tuning, and Prompt-based Continued Pre-training

## Executive Summary
This thesis presents a comprehensive framework for optimizing large language models (LLMs) for downstream tasks through advanced post-training techniques. The work introduces Decomposed Prompt Tuning (DEPT), which substantially improves the efficiency of prompt tuning by decomposing soft prompts into shorter prompts and low-rank matrices, achieving over 20% improvements in training efficiency while maintaining or improving performance. The research also develops Instruction Modelling (IM), a novel approach that applies loss computation to both instructions and outputs, demonstrating substantial improvements in instruction-following capabilities across various NLP tasks and open-ended generation benchmarks. Additionally, the thesis introduces the StepGame dataset, a new benchmark specifically designed to test robust multi-hop spatial reasoning in texts, revealing significant limitations in current LLMs' spatial reasoning abilities.

## Method Summary
The thesis proposes three main techniques for optimizing LLMs: Instruction Modelling (IM) applies the loss function to both instruction and output tokens to mitigate overfitting and improve generalization; Decomposed Prompt Tuning (DePT) decomposes soft prompts into shorter sequences and low-rank matrices to reduce computational costs while maintaining performance; and Prompt-based Continued Pre-training (PCP) integrates task templates into unlabeled data during pre-training to better align with downstream task distributions. These methods are evaluated across 23 NLP and vision-language tasks using models ranging from 7B to 13B parameters, with comprehensive ablation studies and efficiency benchmarks.

## Key Results
- Decomposed Prompt Tuning (DePT) achieves over 20% improvements in training efficiency while maintaining or improving performance compared to vanilla prompt tuning
- Instruction Modelling (IM) demonstrates substantial improvements in instruction-following capabilities across various NLP tasks and open-ended generation benchmarks, with higher training losses but lower test losses indicating reduced overfitting
- StepGame benchmark reveals significant limitations in current LLMs' spatial reasoning abilities, with performance gaps ranging from 10% to 50% on relational reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1: Instruction Modelling (IM) as Overfitting Mitigation
- Claim: Training on instruction tokens in addition to output tokens acts as a regularizer, reducing overfitting to small datasets and improving generalization on open-ended tasks
- Mechanism: By computing loss over the entire sequence (instruction + output), the model allocates capacity to understanding context, preventing memorization of specific output patterns
- Core assumption: LMs rapidly overfit to output distribution when trained on short completions; instruction tokens provide necessary variance
- Evidence anchors: Abstract and section 5.3.3 show IM exhibits higher training losses but lower test losses, suggesting reduced overfitting

### Mechanism 2: Decomposed Prompt Tuning (DePT) for Computational Efficiency
- Claim: Decomposing long soft prompts into shorter prompts and low-rank matrices reduces input sequence length, cutting quadratic computational cost of Transformer attention
- Mechanism: Replaces long trainable prompt matrix with short prompt and low-rank matrices that update frozen word embeddings, significantly speeding up inference and training
- Core assumption: Expressiveness of long soft prompts can be approximated by combination of short prompt and global embedding shift
- Evidence anchors: Section 4.3.3 shows DePT substantially improves efficiency of vanilla PT, saving more than 20% in training time and memory costs

### Mechanism 3: Prompt-based Continued Pre-training (PCP) for Task Alignment
- Claim: Integrating task templates into unlabeled data during continued pre-training aligns pre-training distribution closer to fine-tuning distribution
- Mechanism: Formats raw text into prompt template before applying MLM, ensuring model learns to predict tokens in specific context required for downstream task
- Core assumption: Discrepancy between pre-training (raw text) and fine-tuning (formatted text) is bottleneck; bridging gap with templates improves knowledge transfer
- Evidence anchors: Abstract shows prompt-based continued pre-training outperforms state-of-the-art semi-supervised approaches

## Foundational Learning

### Concept: Instruction Tuning & Alignment Tax
- Why needed here: Understanding that fine-tuning on instructions can degrade performance on other NLP tasks (alignment tax) is crucial to grasp why Instruction Modelling (IM) is proposed as solution to mitigate this degradation
- Quick check question: What is the "alignment tax" in the context of LLM post-training?

### Concept: Prompt Tuning vs. Fine-Tuning
- Why needed here: DePT modifies standard Prompt Tuning. You must understand that Prompt Tuning adds trainable vectors to input to avoid updating model weights, and associated computational cost of longer sequences
- Quick check question: Why does standard Prompt Tuning increase inference latency compared to frozen models?

### Concept: Semi-Supervised Learning (Self-Training vs. TAPT)
- Why needed here: Thesis frames PCP as superior semi-supervised method. Understanding baseline of TAPT (just training on raw domain text) highlights novelty of adding templates
- Quick check question: How does Task-Adaptive Pre-training (TAPT) differ from standard supervised fine-tuning?

## Architecture Onboarding

### Component map:
Instruction Modelling (IM): Modified loss function (-log P(Input + Output))
Decomposed Prompt Tuning (DePT): Input layer modification ([Short_Prompt, Word_Embeddings + Low_Rank_Update])
Prompt-based Continued Pre-training (PCP): Data pipeline (Format raw text -> Apply Template -> Mask -> Train MLM)

### Critical path:
1. PCP Implementation: Create data collator that injects templates and labels into unlabeled text for MLM training before main fine-tuning loop
2. IM Implementation: Modify loss masking in trainer to ensure loss calculated over prompt/input tokens, not just response tokens
3. DePT Implementation: Decompose prompt embeddings module into short sequence and LoRA-like adapter on embedding layer

### Design tradeoffs:
- IM: Gain - Generalization & Reduced Overfitting. Cost - Slightly higher compute per step (predicting more tokens)
- DePT: Gain - Speed & Memory Efficiency. Cost - Potential drop in expressiveness if rank is too low; hyperparameter sensitivity (two learning rates)
- PCP: Gain - Strong semi-supervised performance. Cost - Requires designing template and access to unlabeled data

### Failure signatures:
- IM Failure: Performance drops on tasks with long outputs and short instructions (regularization signal is weak)
- DePT Failure: Convergence unstable or performance lags significantly behind full prompt tuning (Rank is too low, or learning rates for prompt vs. adapter are not separated)
- PCP Failure: Performance degrades compared to standard TAPT (Template conflicts with data distribution, or mask placement disrupts semantic integrity)

### First 3 experiments:
1. Ablation on IM Loss: Train model with standard Instruction Tuning vs. IM on small dataset (e.g., LIMA). Plot train/test loss divergence to verify regularization claim
2. DePT Efficiency Benchmark: Benchmark inference tokens-per-second for standard PT vs. DePT on T5-Base model. Vary soft prompt length (m) to find efficiency sweet spot
3. PCP vs. TAPT: On low-resource classification task, compare (a) Fine-tuning, (b) TAPT + Fine-tuning, and (c) PCP + Fine-tuning to isolate value of template during pre-training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does Task-Adaptive Pre-training (TAPT) degrade performance on sentence-pair tasks compared to single-sentence tasks?
- Basis in paper: [explicit] The author explicitly asks this in Section 3.5 and states investigation into causes like token separation is left for "future work"
- Why unresolved: Paper hypothesizes high similarity within sentence pairs or token separation issues might be causes, but experimental results were inconclusive
- What evidence would resolve it: Ablation studies varying sentence similarity distributions and separator token types specifically on sentence-pair benchmarks

### Open Question 2
- Question: Does Decomposed Prompt Tuning (DePT) maintain efficiency gains when applied to tasks with extremely long input sequences?
- Basis in paper: [explicit] Section 4.4 notes method's evaluation limited to tasks with "hundreds of input tokens," leaving performance when "$s$ is extremely large" for future work
- Why unresolved: Method's low-rank matrix decomposition is tied to sequence length; unclear if memory savings persist when sequence length scales significantly
- What evidence would resolve it: Benchmarks comparing DePT against vanilla Prompt Tuning on long-context tasks to measure memory consumption and latency relative to sequence length

### Open Question 3
- Question: Do benefits of Instruction Modelling (IM) persist in frontier-scale models (70B–1T parameters) and different architectures?
- Basis in paper: [explicit] Section 7.1 states empirical studies limited to models up to 13B parameters and insights "require careful re-validation at larger scales"
- Why unresolved: "Overfitting mitigation" effect of IM empirically demonstrated on 7B/13B models, but scaling laws suggest training dynamics change significantly at trillion-parameter scales
- What evidence would resolve it: Applying IM to 70B+ model and comparing training/test loss curves and AlpacaEval win rates against standard instruction tuning baselines

### Open Question 4
- Question: Can spatial reasoning capabilities learned from synthetic benchmarks like StepGame transfer to complex, naturalistic narratives?
- Basis in paper: [explicit] Section 6.3.4 notes StepGame uses "finite set of templates" and abstract entities, meaning high accuracy "does not automatically imply robustness to real-world prose"
- Why unresolved: Benchmark isolates relational reasoning but lacks lexical diversity (coreference, ellipsis) found in natural text
- What evidence would resolve it: Evaluating models fine-tuned on StepGame against unstructured, human-authored spatial reasoning datasets to measure zero-shot generalization

## Limitations
- Empirical studies limited to models up to 13B parameters, requiring re-validation at frontier scales (70B–1T parameters)
- StepGame benchmark uses synthetic templates and abstract entities, limiting transfer to naturalistic narratives
- PCP effectiveness depends on quality of template design and availability of unlabeled data

## Confidence

### High confidence claims:
- DePT achieves >20% efficiency improvements (supported by section 4.3.3)
- IM demonstrates regularization effects (supported by train/test loss divergence in section 5.3.3)
- StepGame reveals significant spatial reasoning limitations in current LLMs (supported by performance gap data)

### Medium confidence claims:
- PCP outperforms standard TAPT (supported by abstract claim, though detailed ablation not provided)
- Template quality impacts PCP effectiveness (supported by methodological description)

### Low confidence claims:
- Exact template definitions for various datasets (not fully specified in paper)
- Long-context performance of DePT (explicitly noted as future work)

## Next Checks
1. Verify instruction tokens are receiving gradients in IM implementation by checking loss mask alignment
2. Benchmark DePT memory consumption at different sequence lengths to validate efficiency claims
3. Test PCP on tasks with varying template complexity to determine template design sensitivity