---
ver: rpa2
title: 'Teaching LLMs Music Theory with In-Context Learning and Chain-of-Thought Prompting:
  Pedagogical Strategies for Machines'
arxiv_id: '2503.22853'
source_url: https://arxiv.org/abs/2503.22853
tags:
- music
- llms
- learning
- encoding
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates how large language models (LLMs) learn music\
  \ theory through in-context learning and chain-of-thought prompting. Three models\u2014\
  ChatGPT, Claude, and Gemini\u2014were tested on RCM Level 6 theory exam questions\
  \ encoded in ABC, Humdrum, MEI, and MusicXML formats."
---

# Teaching LLMs Music Theory with In-Context Learning and Chain-of-Thought Prompting: Pedagogical Strategies for Machines

## Quick Facts
- arXiv ID: 2503.22853
- Source URL: https://arxiv.org/abs/2503.22853
- Reference count: 40
- Key outcome: ChatGPT (no context) achieved 52% accuracy with MEI; Claude (with context) scored 75% accuracy with MEI on RCM Level 6 music theory exam questions.

## Executive Summary
This study evaluates how large language models (LLMs) learn music theory through in-context learning and chain-of-thought prompting. Three models—ChatGPT, Claude, and Gemini—were tested on RCM Level 6 theory exam questions encoded in ABC, Humdrum, MEI, and MusicXML formats. Without context, ChatGPT with MEI achieved 52% accuracy; with context, Claude with MEI scored 75%. Contextual prompts consistently improved performance, especially for intervals, scales, transposition, and cadences. However, chords and rhythmic understanding showed limited gains. Results suggest LLMs can be effectively taught music theory using structured prompts, with potential applications in education and AI music tools.

## Method Summary
The study tested three LLMs (GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro) on RCM Level 6 music theory exam questions using in-context learning and chain-of-thought prompting across four music encoding formats (ABC, Humdrum, MEI, MusicXML). Questions were encoded in MuseScore and exported/converted to target formats. Prompts included system prompts, format identifiers, encoded content, theory guides, and CoT examples. Experiments ran without context (baseline) and with context (guides + 2-3 shot CoT). Three trials per condition were averaged, with manual scoring against RCM rubrics. Question 8 (melody completion) was excluded due to subjective evaluation.

## Key Results
- Without context, ChatGPT with MEI achieved 52% accuracy; with context, Claude with MEI achieved 75% accuracy
- Contextual prompts consistently improved performance across all models and formats
- Largest gains seen for intervals, scales, transposition, and cadences
- Chords and rhythmic understanding showed limited improvement with contextual prompting
- Encoding format choice affected performance, with MEI and MusicXML outperforming ABC and Humdrum

## Why This Works (Mechanism)

### Mechanism 1: In-Context Learning via Structured Prompt Context
- Claim: Providing domain-specific guides and examples within large context windows enables LLMs to solve problems they would otherwise fail, without parameter updates.
- Mechanism: Large context windows (up to 2M tokens in Gemini 1.5 Pro) allow inclusion of instructional content; models pattern-match from structured examples to generalize solutions. Performance improved from 52% (best no-context) to 75% (best with-context).
- Core assumption: Models can retrieve and apply relevant patterns from context when prompted, without requiring weight changes.
- Evidence anchors:
  - [abstract] "without context, ChatGPT with MEI performs the best at 52%, while with context, Claude with MEI performs the best at 75%"
  - [section 2.1] "in-context learning is a prompting technique in which an LLM learns patterns, rules, and concepts solely from the context provided within a prompt without fine-tuning"
  - [corpus] "Understanding Prompt Tuning and In-Context Learning via Meta-Learning" explores conceptual foundations of prompting, though mechanisms remain empirically driven
- Break condition: Excessive prompt length overwhelms retrieval capacity ("needle-in-a-haystack" degradation); information density must be balanced.

### Mechanism 2: Chain-of-Thought Prompting for Sequential Reasoning
- Claim: Providing step-by-step worked examples elicits intermediate reasoning, improving performance on multi-step music theory tasks.
- Mechanism: 2-3 shot CoT examples demonstrate decomposition of complex problems; models learn to articulate intermediate steps rather than jumping to answers, mimicking human problem-solving.
- Core assumption: Music theory tasks benefit from explicit sequential reasoning, similar to logic problems.
- Evidence anchors:
  - [abstract] "contextual prompts consistently improved performance, especially for intervals, scales, transposition, and cadences"
  - [section 2.2] "CoT improves performance by guiding models to use a more structured and systematic problem-solving methodology"
  - [corpus] "Rethinking the Chain-of-Thought" examines CoT from dual perspectives of in-context learning and pre-trained priors
- Break condition: Hierarchical or rule-heavy domains (rhythmic groupings, chord construction) showed limited gains—suggesting CoT insufficient for tasks requiring nested rule application.

### Mechanism 3: Encoding Format as Pretraining Artifact
- Claim: Music encoding format choice affects performance based on model's pretraining exposure to that format's patterns.
- Mechanism: MEI and MusicXML showed higher accuracy than ABC; likely reflects token distribution and structural conventions in training data rather than format expressiveness alone.
- Core assumption: LLMs have unequal familiarity with encoding formats due to pretraining corpus composition.
- Evidence anchors:
  - [abstract] ChatGPT (no-context) achieved 52% with MEI vs. 41% with ABC; Claude (with-context) achieved 75% MEI vs. 57% ABC
  - [section 2.3] "LLMs exhibit varying familiarity with different encoding formats, likely due to differences in their pretraining data"
  - [corpus] Weak direct evidence—neighbor papers do not address encoding format effects on LLM performance
- Break condition: Format complexity or training corpus obscurity limits comprehension; corrupted output files suggest format understanding failure rather than theory failure.

## Foundational Learning

- Concept: **In-Context Learning vs. Fine-Tuning**
  - Why needed here: Distinguishes prompt-based adaptation (no weight updates) from parameter modification; determines deployment strategy.
  - Quick check question: What happens to learned patterns when you start a new session with an LLM trained via in-context learning?

- Concept: **Few-Shot Chain-of-Thought Prompting**
  - Why needed here: Explains how 2-3 worked examples create reasoning scaffolds for complex domain tasks.
  - Quick check question: How does 3-shot CoT differ from simply providing 3 input-output pairs?

- Concept: **Symbolic Music Encoding Formats**
  - Why needed here: Input representation directly affects model comprehension; format choice is a design variable.
  - Quick check question: Why might MEI's XML-based structure be more parseable than ABC's compact notation for an LLM?

## Architecture Onboarding

**Component map:**
- LLM layer: GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro (accessed via API)
- Encoding pipeline: MuseScore → native export (MEI, MusicXML) or conversion (ABC via xml2abc, Humdrum via batch converter)
- Prompt structure: System prompt → format identifier → encoded content → subject guides → CoT examples → question
- Evaluation: Temperature=0, 3-trial averaging, manual scoring against RCM rubric

**Critical path:**
1. Encode examination questions in MuseScore; export/convert to target format
2. Build prompt with guides (theory explanations) + 2-3 CoT worked examples; cap length to avoid retrieval degradation
3. Query API three times per condition; average scores; manually validate output validity

**Design tradeoffs:**
- Context length vs. retrieval accuracy: More examples help, but overload degrades recall
- CoT shot count vs. token budget: Use 3-shot for single-topic; reduce to 2 for multi-component questions
- Format expressiveness vs. model familiarity: MEI most expressive but requires pretraining alignment

**Failure signatures:**
- Corrupted/unreadable output files → format comprehension failure, not theory failure
- Near-zero scores on rhythmic tasks → hierarchical rule application failure
- High cross-trial variance → prompt overload or format unfamiliarity

**First 3 experiments:**
1. **Baseline calibration:** Query all models on MEI-encoded questions with no context; establish floor performance per model
2. **Context ablation:** Test guides-only vs. CoT-only vs. combined to isolate mechanism contributions
3. **Format robustness:** Take best-performing prompt; run identical conditions across all four encodings to measure format sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What prompting strategies can effectively improve LLM performance on chord analysis and rhythmic understanding tasks, which showed limited gains from current in-context learning and chain-of-thought approaches?
- Basis in paper: [explicit] Authors state: "questions related to chords showed limited improvement... Rhythmic understanding proved even more challenging, as attempts to teach rests and rhythmic groupings were unsuccessful."
- Why unresolved: Current structured guides and chain-of-thought examples failed for these multi-step sequential and hierarchical rule-based tasks.
- What evidence would resolve it: Demonstration of significantly improved accuracy on chord and rhythm questions using alternative prompting architectures or examples.

### Open Question 2
- Question: How do open-weight LLMs such as DeepSeek, LLaMA, and Mistral perform on music theory tasks using the same in-context learning and chain-of-thought prompting methodology?
- Basis in paper: [explicit] Future work section explicitly states: "the performance of other LLMs like DeepSeek, LLaMA, and Mistral should be explored."
- Why unresolved: Only three proprietary models (ChatGPT, Claude, Gemini) were tested; open-weight models may have different capabilities or training data exposure.
- What evidence would resolve it: Comparative evaluation using identical RCM exam questions, encoding formats, and prompting protocols.

### Open Question 3
- Question: How can subjective music theory tasks, such as melody completion and composition, be reliably evaluated in LLMs?
- Basis in paper: [inferred] Question 8 (worth 10 points) was excluded entirely because it asked students to complete an unfinished melody and compose an answer phrase: "Due to the subjectivity of evaluation, this question was omitted."
- Why unresolved: The paper lacks any proposed methodology for handling creative or compositional assessment, leaving a gap in comprehensive music theory evaluation.
- What evidence would resolve it: Development and validation of rubrics or automated metrics that correlate with expert human evaluation of LLM-generated musical completions.

### Open Question 4
- Question: What causes format-specific anomalies in in-context learning, such as Gemini's performance decrease with ABC format contextual prompts?
- Basis in paper: [inferred] Results show Gemini's ABC accuracy dropped from 39% to 35% with context, "likely due to variability among outputs" — but the precise mechanism remains unexplained.
- Why unresolved: The paper attributes the anomaly to variability without investigating whether tokenization differences, pre-training data distribution, or prompt length effects are responsible.
- What evidence would resolve it: Ablation studies varying prompt length, content density, and format-specific tokenization across models to isolate the cause of performance degradation.

## Limitations
- Limited model diversity: Only three proprietary LLMs tested, restricting generalizability
- Manual evaluation criteria not fully specified, potentially introducing scorer variability
- Encoding conversion workflows rely on specific tools that may produce inconsistent outputs
- Single exam version and music theory level limits external validity
- Subjective creative tasks excluded entirely due to evaluation challenges

## Confidence
- High confidence in observed performance patterns due to clear numerical results and controlled experimental design
- Medium confidence in mechanism explanations, particularly around encoding format effects, as supporting literature is sparse
- Low confidence in cross-model comparisons beyond the three tested systems, given the narrow scope

## Next Checks
1. Reconstruct and test prompts from the repository to verify that reported improvements are reproducible with the actual materials used
2. Validate encoding conversion workflows by checking syntax correctness of all output files before model input
3. Run format ablation studies to isolate whether performance differences stem from pretraining exposure versus encoding expressiveness