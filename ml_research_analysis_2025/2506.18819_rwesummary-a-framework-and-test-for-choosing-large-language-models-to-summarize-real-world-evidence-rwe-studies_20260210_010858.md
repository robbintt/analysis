---
ver: rpa2
title: 'RWESummary: A Framework and Test for Choosing Large Language Models to Summarize
  Real-World Evidence (RWE) Studies'
arxiv_id: '2506.18819'
source_url: https://arxiv.org/abs/2506.18819
tags:
- outcome
- summary
- visit
- prompt
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces RWESummary, a new benchmark for evaluating
  large language models' ability to summarize structured real-world evidence (RWE)
  study outputs. The framework uses one scenario with 13 proprietary RWE studies and
  three LLM-based evaluations covering direction of effects, numeric accuracy, and
  completeness of reported outcomes.
---

# RWESummary: A Framework and Test for Choosing Large Language Models to Summarize Real-World Evidence (RWE) Studies

## Quick Facts
- arXiv ID: 2506.18819
- Source URL: https://arxiv.org/abs/2506.18819
- Reference count: 0
- Introduces RWESummary benchmark for evaluating LLMs on summarizing structured real-world evidence study outputs

## Executive Summary
This paper introduces RWESummary, a benchmark framework designed to evaluate large language models' ability to summarize structured real-world evidence (RWE) studies. The framework was tested on nine models from Anthropic, Google, and OpenAI using one scenario containing 13 proprietary RWE studies. The evaluation employed three LLM-based metrics covering direction of effects, numeric accuracy, and completeness of reported outcomes. Gemini 2.5 models (Flash and Pro) achieved the highest performance across all metrics, demonstrating superior capabilities for RWE summarization tasks. The authors position RWESummary as a practical tool for selecting LLMs in RWE applications and suggest its potential integration into the MedHELM benchmark suite.

## Method Summary
RWESummary employs a structured evaluation framework using one scenario with 13 proprietary RWE studies as input. The benchmark assesses LLM performance through three automated evaluation metrics: direction of effects accuracy (measuring whether models correctly identify positive/negative associations), numeric accuracy (evaluating precision of reported values), and completeness (assessing coverage of key outcomes). Nine models from three major providers (Anthropic, Google, OpenAI) were systematically tested using standardized prompts and evaluation procedures. The framework relies on LLM-based evaluation rather than human assessment, using reference summaries as ground truth for comparison.

## Key Results
- Gemini 2.5 Flash and Pro models achieved the highest overall performance
- Completeness scores ranged from 0.82-1.0 across models
- Direction of effect accuracy ranged from 0.72-0.95
- Numeric accuracy ranged from 0.77-0.97

## Why This Works (Mechanism)
The framework leverages structured RWE study inputs with consistent formatting to enable systematic evaluation of LLM summarization capabilities. By using LLM-based evaluation metrics rather than human assessment, the framework provides scalable and reproducible performance measurement across multiple models and scenarios.

## Foundational Learning
- RWE Study Structure: Understanding standardized formats for real-world evidence studies is crucial for consistent summarization evaluation. Quick check: Verify input studies follow predictable structural elements.
- LLM-based Evaluation: Using models to evaluate other models requires careful prompt engineering and reference standards. Quick check: Validate evaluation prompts produce consistent results across test cases.
- Benchmark Design Principles: Creating representative test scenarios that capture real-world variation while maintaining comparability. Quick check: Assess whether 13 studies adequately represent RWE diversity.

## Architecture Onboarding
- Component Map: Proprietary RWE Studies -> LLM Summarization -> LLM-based Evaluation -> Performance Metrics
- Critical Path: Input Studies → Model Generation → Evaluation Scoring → Result Aggregation
- Design Tradeoffs: Automated evaluation enables scalability but may miss nuanced quality aspects; proprietary data ensures consistency but limits generalizability.
- Failure Signatures: Inconsistent formatting in input studies could lead to evaluation errors; evaluation prompt sensitivity may affect metric reliability.
- First Experiments: 1) Test model performance on individual metric components separately; 2) Vary input study complexity to assess performance scaling; 3) Compare automated vs. human evaluation for selected cases.

## Open Questions the Paper Calls Out
None

## Limitations
- Results may not generalize beyond the single proprietary scenario used for testing
- Lack of independent validation across diverse RWE sources and study designs
- Proprietary nature of studies prevents full transparency regarding potential selection biases

## Confidence
- Confidence in comparative model rankings: High
- Confidence in framework utility for practical RWE tasks: Medium
- Confidence in RWESummary enhancing MedHELM: Low to Medium

## Next Checks
1. Expand benchmark to include diverse RWE study sources beyond current proprietary dataset, testing across different therapeutic areas and study designs
2. Conduct head-to-head comparisons between LLM-generated summaries and human expert summaries to validate automated evaluation metrics
3. Test framework portability to different summarization formats and clinical decision support contexts to assess real-world applicability