---
ver: rpa2
title: Mapping Faithful Reasoning in Language Models
arxiv_id: '2510.22362'
source_url: https://arxiv.org/abs/2510.22362
tags:
- reasoning
- safety
- cases
- internal
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Concept Walk, a method to track how language\
  \ models\u2019 internal safety-related representations evolve across reasoning steps.\
  \ By projecting step-level activations onto a learned safety direction, it measures\
  \ temporal shifts in safety alignment during chain-of-thought reasoning."
---

# Mapping Faithful Reasoning in Language Models

## Quick Facts
- arXiv ID: 2510.22362
- Source URL: https://arxiv.org/abs/2510.22362
- Reference count: 40
- This paper introduces Concept Walk, a method to track how language models' internal safety-related representations evolve across reasoning steps.

## Executive Summary
This paper introduces Concept Walk, a method to track how language models' internal safety-related representations evolve across reasoning steps. By projecting step-level activations onto a learned safety direction, it measures temporal shifts in safety alignment during chain-of-thought reasoning. Applying this to Qwen 3-4B on synthetic safety prompts, the authors find that in "hard" cases (where perturbed CoTs change outputs), safety activation changes are sustained and structured, consistent with faithful reasoning. In "easy" cases (where CoTs are decorative), changes are transient and self-corrected. This suggests Concept Walk can distinguish genuine from superficial reasoning, helping identify when CoTs are trustworthy. The contribution is methodological: providing a concept-specific, activation-space lens for studying reasoning faithfulness.

## Method Summary
Concept Walk extracts a safety direction from contrastive safe/unsafe prompt activations, then tracks how safety-related activations evolve across chain-of-thought reasoning steps by projecting step-level averaged token activations onto this direction. The method first generates CoTs and filters for "hard" vs "easy" cases by injecting flawed reasoning at the midpoint and checking if outputs change. For hard cases, the model's reasoning is causally integrated; for easy cases, it's decorative. The safety vector is computed via difference of means on contrastive datasets, selecting the optimal layer/token position. Step-level trajectories are compared between original and perturbed CoTs to distinguish faithful from superficial reasoning.

## Key Results
- Safety activation trajectories show sustained, structured changes in "hard" cases (where perturbed CoTs change outputs) versus transient, self-correcting changes in "easy" cases
- The safety direction extracted from non-thinking mode activations successfully tracks safety-relevant reasoning in thinking mode
- Concept Walk can distinguish computational reasoning (where CoT influences output) from decorative rationalization (where CoT is ignored)

## Why This Works (Mechanism)

### Mechanism 1: Safety Direction Extraction via Contrastive Activation Differences
- Claim: A single direction in activation space encodes safety-related behavior, extractable via difference of means between safe and unsafe prompt activations.
- Mechanism: Compute μ_unsafe^(ℓ,t) - μ_safe^(ℓ,t) at each layer and token position, normalize to unit vector. Select the direction that maximally suppresses refusal on unsafe prompts while inducing refusal on safe prompts, subject to KL divergence constraints.
- Core assumption: Safety behavior is encoded in a linearly separable subspace that transfers across prompts.
- Evidence anchors: [abstract]: "projecting each reasoning step onto the concept direction learned from contrastive data"; [section 3.2]: "We use the Difference of Means approach... constructing contrastive datasets that differ in the specific concept of interest"; [corpus]: Related work (Arditi et al., cited in paper) demonstrates refusal is mediated by a single direction, but generalization to all concepts is not proven.
- Break condition: If safety representations are highly non-linear or distributed across multiple orthogonal directions, the single-vector approximation collapses signal.

### Mechanism 2: Step-wise Projection Creates Temporal Reasoning Signature
- Claim: Averaging token-level activations per reasoning step and projecting onto safety direction yields a trajectory distinguishing computational from decorative reasoning.
- Mechanism: For step s, compute h_s = (1/|T_s|)Σ_{t∈T_s} x[t], then α_s = cos(h_s, v^(ℓ^*)). The scalar time series reveals whether safety signals persist (computation) or decay (rationalization).
- Core assumption: Step-averaging preserves directional information; safety vector computed in non-thinking mode applies to thinking-mode activations.
- Evidence anchors: [section 3.3]: "By plotting α_s over s for many prompts, we obtain the Concept Walk"; [results]: "In hard cases, perturbing the reasoning trace leads to sustained and structured changes in internal safety activation... In contrast, easy cases show much smaller, transient perturbation effects"; [corpus]: Venhoff et al. (cited) use similar positional averaging for steering vectors, but corpus evidence for step-level averaging specifically is limited.
- Break condition: If within-step token activations are highly heterogeneous, averaging cancels signal. If thinking-mode representations diverge significantly from non-thinking mode, the safety direction misaligns.

### Mechanism 3: Perturbation Sensitivity Filters for Causal CoT Integration
- Claim: Injecting flawed reasoning steps and measuring output changes identifies when CoT is causally integrated versus decorative.
- Mechanism: Generate baseline CoT → inject error near midpoint → regenerate continuation → compare final output. Cases that flip are "hard" (computational); cases that self-correct are "easy" (decorative).
- Core assumption: Computational reasoning propagates perturbations to outputs; decorative reasoning has pre-determined answers unaffected by trace content.
- Evidence anchors: [section 3.1]: "We retain only examples where perturbing the CoT, through injected errors, leads to significant degradation in model performance"; [section 7.2]: Full procedure for mistake injection, regeneration, and filtering; [corpus]: Lanham et al. and Emmons et al. (cited) show task difficulty induces CoT reliance; "Can Aha Moments Be Fake?" proposes similar causal scoring.
- Break condition: If models use hidden computation not verbalized in CoT, perturbation sensitivity underestimates unfaithfulness. If injected format is unnatural, effects may be artifacts.

## Foundational Learning

- Concept: **Residual Stream Activations**
  - Why needed here: Concept Walk operates on residual stream activations x[ℓ, t] ∈ ℝ^d extracted at specific layers and token positions; understanding what these represent is prerequisite.
  - Quick check question: Can you explain why the residual stream accumulates information across layers, and why late layers may encode output token predictions rather than conceptual features?

- Concept: **Cosine Similarity in High-Dimensional Spaces**
  - Why needed here: The method projects activations onto safety direction via cosine similarity; interpreting these scalar values requires understanding directional alignment vs magnitude.
  - Quick check question: If two vectors have cosine similarity 0.8, what does this tell you about their relationship, and why might magnitude matter less for concept detection?

- Concept: **CoT Faithfulness Literature**
  - Why needed here: The paper builds on prior work distinguishing CoT-as-computation from CoT-as-rationalization; knowing this distinction motivates the method.
  - Quick check question: What evidence would convince you that a model's stated reasoning actually caused its output, versus being generated post-hoc?

## Architecture Onboarding

- Component map: Synthetic datasets -> Safety vector extraction -> CoT generation -> Perturbation filtering -> Concept Walk execution
- Critical path: 1. Compute/validate safety vector on contrastive dataset (Harm/Hate splits) 2. Generate baseline CoTs for test prompts 3. Apply perturbation filtering to identify hard vs easy cases 4. Run Concept Walk on original and perturbed traces 5. Compare trajectories: sustained divergence = computational, transient spike = decorative
- Design tradeoffs:
  - Non-thinking vs thinking-mode safety vector: Paper uses non-thinking for stability; thinking-mode vectors might capture reasoning-specific safety but introduce variance
  - Single vs multi-step perturbation: Single injection simplifies analysis but may miss distributed reasoning
  - Layer/token selection for vector: Mid-layers encode features; late layers encode output tokens (excluded per Arditi et al.)
- Failure signatures:
  - No divergence between hard and easy trajectories → safety vector may not capture meaningful dimension
  - High variance across prompts within same category → step averaging may be too coarse
  - Perturbation format causes unnatural activation shifts → verify injected text matches model's CoT style exactly
- First 3 experiments:
  1. **Validate safety vector**: On held-out safe/unsafe prompts, confirm ablation suppresses refusal and addition induces refusal with low KL divergence
  2. **Perturbation sanity check**: Manually verify that "hard" cases show coherent error propagation in surface text, not just output flips
  3. **Cross-concept transfer**: Attempt Concept Walk with a different concept direction (e.g., helpfulness) to test framework generality beyond safety

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the safety representation differ between thinking and non-thinking modes, and would mode-specific safety vectors yield more accurate activation trajectories?
- Basis in paper: [explicit] The authors compute safety directions in non-thinking mode and apply them to thinking-mode activations, noting "future work could compare mode-specific safety vectors directly" to address potential representation shifts.
- Why unresolved: The study assumes core safety encoding is similar across modes, but this assumption is untested. If representations shift between modes, stepwise estimates may be biased.
- What evidence would resolve it: Computing safety vectors separately in thinking mode and comparing their alignment with non-thinking vectors, then testing whether mode-matched vectors improve predictive accuracy for safety behavior.

### Open Question 2
- Question: How does Concept Walk's ability to distinguish computational from decorative reasoning generalize across model architectures, sizes, and training paradigms?
- Basis in paper: [explicit] The authors state that "Scaling this analysis to models of varying sizes, architectures, and training paradigms would provide greater clarity on how these factors shape the faithfulness of reasoning."
- Why unresolved: The study only examines Qwen 3-4B, leaving unclear whether the observed patterns reflect fundamental properties of reasoning architectures or model-specific characteristics.
- What evidence would resolve it: Applying the same Concept Walk methodology to models like Llama, GPT, or Claude with varying parameter counts and comparing whether hard/easy case patterns persist.

### Open Question 3
- Question: Can the Concept Walk framework effectively trace concept dynamics for domains beyond safety, such as fairness, bias, or toxicity?
- Basis in paper: [explicit] The authors describe this as "an initial step" and explicitly note "the same approach could be applied to other domains such as fairness, bias or toxicity."
- Why unresolved: The safety direction was computed using contrastive datasets specifically designed for safety. Whether other concepts exhibit similarly traceable activation trajectories with sustained versus transient patterns remains unknown.
- What evidence would resolve it: Constructing contrastive datasets for fairness or bias concepts, computing those directions, and testing whether perturbed CoTs show analogous sustained shifts in "hard" cases.

### Open Question 4
- Question: How does perturbation timing and strength affect the observed temporal sensitivity of safety activations?
- Basis in paper: [explicit] The authors note that "systematic variation of perturbation timing and strength would clarify temporal sensitivity."
- Why unresolved: Perturbations are currently injected near the CoT midpoint. Whether early versus late perturbations produce different activation dynamics, or whether stronger/weaker flawed reasoning changes the sustained versus transient pattern, is unexplored.
- What evidence would resolve it: Running controlled experiments varying injection position (early/mid/late) and perturbation severity, then measuring whether the hard/easy distinction holds or shifts.

## Limitations
- The single safety direction assumption may not capture complex, distributed safety representations across prompts
- Synthetic Harm/Hate datasets may not reflect real-world safety reasoning patterns, limiting external validity
- The method assumes safety concepts are stable and not context-dependent, which may not hold across domains or model states

## Confidence
**High Confidence**: The Concept Walk methodology for tracking temporal activation changes is well-defined and reproducible. The perturbation pipeline for filtering hard vs. easy cases is clearly specified. The empirical observation that hard cases show sustained vs. transient activation changes is directly supported by the reported trajectories.

**Medium Confidence**: The safety direction extraction mechanism generalizes beyond the specific Harm/Hate concepts tested. The single-vector approximation adequately captures safety-related behavior across diverse prompts. The perturbation method reliably distinguishes computational from decorative reasoning in all contexts.

**Low Confidence**: The method reliably identifies faithful reasoning in naturalistic, non-synthetic safety scenarios. The safety direction remains stable and meaningful across different model architectures or training paradigms. The approach scales to concepts more complex than safety (e.g., factual consistency, ethical reasoning).

## Next Checks
1. **Cross-Concept Transfer Validation**: Apply Concept Walk to a different concept direction (e.g., helpfulness or factual consistency) using the same framework. Verify that the method produces interpretable trajectories and distinguishes computational from decorative reasoning for this new concept.

2. **Real-World Dataset Test**: Replace the synthetic Harm/Hate datasets with a naturalistic safety dataset (e.g., real user queries to a safety-filtered model). Run the full pipeline and assess whether hard/easy distinctions and trajectory patterns replicate.

3. **Model Architecture Ablation**: Test Concept Walk on a different model family (e.g., Llama or GPT) with the same safety vector extraction and reasoning analysis. Determine whether the single-direction assumption and trajectory interpretation hold across architectures.