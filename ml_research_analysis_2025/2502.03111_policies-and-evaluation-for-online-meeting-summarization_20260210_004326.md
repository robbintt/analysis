---
ver: rpa2
title: Policies and Evaluation for Online Meeting Summarization
arxiv_id: '2502.03111'
source_url: https://arxiv.org/abs/2502.03111
tags:
- summarization
- systems
- meeting
- summary
- summaries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first systematic study of online meeting
  summarization, proposing several policies for real-time summary generation during
  meetings. The authors introduce novel metrics for evaluating latency and intermediate
  summary quality, comparing systems that process meeting transcripts chunk-by-chunk
  versus those that dynamically select summarization units.
---

# Policies and Evaluation for Online Meeting Summarization

## Quick Facts
- arXiv ID: 2502.03111
- Source URL: https://arxiv.org/abs/2502.03111
- Authors: Felix Schneider; Marco Turchi; Alex Waibel
- Reference count: 35
- One-line primary result: Model-based policy achieves 42.6 ROUGE-1; sliding window shows competitive quality with lower latency

## Executive Summary
This paper presents the first systematic study of online meeting summarization, proposing several policies for real-time summary generation during meetings. The authors introduce novel metrics for evaluating latency and intermediate summary quality, comparing systems that process meeting transcripts chunk-by-chunk versus those that dynamically select summarization units. Experiments on the AutoMin dataset show that online models can produce strong summaries, with the model-based policy achieving 42.6 ROUGE-1 score and sliding window policy showing competitive quality with lower latency. Human evaluations validate these findings, demonstrating that dynamic segmentation strategies outperform static ones, with the sliding window system receiving the highest ratings for adequacy, fluency, and relevance at 40% meeting completion.

## Method Summary
The study evaluates six online summarization policies (length-based, model-based, sliding window, full rewriting, fully incremental) on the AutoMin 2023 test set using two backend models (Bart-large fine-tuned on SamSum, and GPT-4-32k). Policies determine when to invoke the summarizer based on fixed chunk sizes or adaptive criteria. The sliding window policy maintains a summary prefix and only appends new content when input window shifts significantly. Evaluation uses ROUGE-1 F1 for quality, Expected Latency for timing, Normalized Erasure for rewriting intensity, and R1-AUC for intermediate summary quality. Chunk sizes tested include 256, 512, 1024, and 2048 tokens.

## Key Results
- Model-based policy achieves highest quality with ROUGE-1 score of 42.6
- Sliding window policy shows competitive quality with lower latency (25.9 vs 11.7)
- Adaptive policies outperform fixed scheduled ones in human evaluations
- Sliding window system receives highest ratings for adequacy, fluency, and relevance at 40% meeting completion

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Segmentation Aligns Semantic Boundaries
- **Claim:** If summarization units are dynamically selected based on content density or model signals (rather than fixed token counts), the resulting output exhibits higher adequacy and relevance.
- **Mechanism:** Fixed-length segmentation often bisects coherent topics, forcing the model to summarize incomplete information. Adaptive policies, such as Model-Based Segmentation (using quality estimation) or Sliding Window (using output saturation), implicitly detect topic boundaries or information saturation points before triggering a summary generation, ensuring the input context is semantically complete.
- **Core assumption:** The underlying summarization model performs significantly better when the input window aligns with actual discourse topics or "turns" rather than arbitrary token limits.
- **Evidence anchors:**
  - [abstract] "adaptive policies perform better than fixed scheduled ones."
  - [section 7.1] "Human annotators rate the model-based and length-based systems... preference toward dynamic segmentation models."
  - [corpus] (Weak/Derived) *M³FinMeeting* suggests multi-task/domain complexity requires precise segmentation, supporting the need for dynamic boundaries over static ones.
- **Break condition:** If the model's confidence scores (Model-Based) or output generation probabilities (Sliding Window) become decoupled from actual semantic coherence (e.g., hallucination confidence), dynamic segmentation may actually increase latency without quality gains.

### Mechanism 2: Prefix-Conditioning Reduces Redundancy and Latency
- **Claim:** Constraining the model to treat previous outputs as a fixed prefix (Sliding Window) minimizes the "unbounded growth" problem and reduces processing redundancy compared to full re-writing, provided the model can reliably determine when *not* to update.
- **Mechanism:** By maintaining a summary prefix and only appending new content when the input window shifts significantly, the system avoids re-processing the entire meeting history for every update. This simulates a "stateful" summarization where the model only computes the delta (new information) rather than the global state.
- **Core assumption:** The underlying LLM can effectively follow instructions to "continue or reply 'No update necessary'" without hallucinating updates or getting stuck in repetitive loops.
- **Evidence anchors:**
  - [section 3.1] Describes Sliding Window: "If there is no new output... repeat 3... The lower bound for latency would be equal to the chunk-based policy."
  - [section 7.1] "Sliding window Bart... competitive quality with lower latency (25.9 vs 11.7)."
  - [corpus] (Weak/Derived) *ARQUSUMM* highlights the difficulty of capturing rationale in evolving conversations; prefix-conditioning helps maintain "rationale" continuity.
- **Break condition:** If the conversation topic drifts rapidly or circularly, the prefix may act as a rigid constraint, forcing the model to maintain outdated context or failing to "forget" irrelevant previous points.

### Mechanism 3: Processing Redundancy (RF) Approximates System Cost
- **Claim:** The Redundancy Factor (RF)—the ratio of tokens processed to tokens spoken—is a critical proxy for system cost and latency; lower RF generally correlates with better user experience in real-time settings.
- **Mechanism:** Policies like Length-Based have RF=1.0 (minimal compute) but lower quality. Full Rewriting has high RF (e.g., 12.2x) and high quality but prohibitive cost/latency. Sliding Window offers a middle ground (RF ~2-4x), re-processing only the sliding context window rather than the full history.
- **Core assumption:** In an online deployment, computational resources and API costs (per token) are limiting factors, making RF a necessary constraint alongside ROUGE scores.
- **Evidence anchors:**
  - [section 6.2] "It is defined as the number of tokens sent to the summarization model divided by the number of tokens in the meeting."
  - [table 4] Shows Sliding Window Bart (4.1x RF) outperforming Full Rewriting GPT-4 (12.2x RF) in latency.
  - [corpus] (Missing/Weak) No direct corpus neighbor addresses the computational cost of online summarization specifically.
- **Break condition:** If the backend model has a highly non-linear cost curve (e.g., quadratic attention for long contexts), RF might underestimate the true cost of "Sliding Window" or "Full Incremental" approaches as the meeting extends.

## Foundational Learning

### Concept: Simultaneous Translation (Agent) Paradigm
- **Why needed here:** The paper explicitly adapts the READ/WRITE policy framework from simultaneous machine translation to summarization. Understanding "Wait" (Read) vs "Emit" (Write) decisions is central to the proposed architecture.
- **Quick check question:** How does the "policy" differ from the "model" in this architecture?

### Concept: Quality Estimation (QE)
- **Why needed here:** The Model-Based policy relies on estimating the quality of a summary without a reference (using model confidence) to decide when to output.
- **Quick check question:** Can a model's internal token probabilities reliably predict the "completeness" of a summary for a partial meeting transcript?

### Concept: ROUGE-AUC (Area Under Curve)
- **Why needed here:** Standard ROUGE only measures the final output. Online systems require a metric that captures the *trajectory* of quality over time, which R1-AUC provides.
- **Quick check question:** Why is a "convex" ROUGE curve (high scores early) preferred over a curve that spikes only at the end, even if the final ROUGE score is identical?

## Architecture Onboarding

### Component map:
Input Buffer -> Policy Agent -> Backend Summarizer -> Output Stream

### Critical path:
The implementation of the Sliding Window Policy (Algo 2). Specifically, the logic that appends chunks until "new output" is generated, then rotates the input window and output prefix. This requires precise management of the model's prompt context (including the forced prefix).

### Design tradeoffs:
- Length-Based: Lowest latency (RF=1.0), lowest coherence (bad boundaries)
- Model-Based: Highest quality (adaptive boundaries), higher latency (must fill buffer to test quality)
- Sliding Window: Best balance of intermediate quality (R1-AUC) and latency, but requires complex state management (window rotation)

### Failure signatures:
- High Normalized Erasure (NE): Indicates the "Full Rewriting" policy is flipping the summary text too often, causing user confusion
- Unbounded Growth: "Length-Based" or "Full Incremental" policies generating summaries that grow indefinitely linear with input length
- Stagnation: Sliding window failing to output new sentences because the "prefix" constraint is too strong or the model is stuck

### First 3 experiments:
1. **Establish Baseline:** Implement Length-Based policy with Bart on a subset of AutoMin to verify RF=1.0 and ROUGE-1 baseline (~36-42)
2. **Latency vs. Quality:** Implement Sliding Window with varying chunk sizes (256 vs 512) to observe the trade-off between Redundancy Factor (RF) and Expected Latency (EL)
3. **Metric Validation:** Calculate R1-AUC for both systems and correlate with human evaluation of "intermediate summaries" (at 40% meeting completion) to validate the paper's claim that R1-AUC reflects user utility

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the input units selected by adaptive policies (sliding window or model-based) correspond to semantic topic boundaries, or do they optimize for a latent quality learned from training data?
- Basis in paper: [explicit] The authors state in Section 7.1: "More research is needed to determine whether the units selected by either the sliding window or the model-based algorithm constitute topic segments, or if they optimize some unseen quality that the model learns from the training data."
- Why unresolved: The current study focused on evaluating the quality and latency of the resulting summaries but did not perform a linguistic or semantic analysis of the specific segmentation boundaries chosen by the models.
- What evidence would resolve it: A comparative analysis mapping the segmentation points generated by these policies against a gold-standard topic segmentation dataset (e.g., comparing boundary overlaps).

### Open Question 2
- Question: Can learned policies or specialized model architectures improve upon the quality-latency trade-off compared to the parameter-free, post-training policies evaluated in this study?
- Basis in paper: [explicit] The Conclusion states: "Particularly promising directions are the development of learned policies... specialized architectures that can reuse more internal states to reduce redundancy and cost, and/or explicitly training with partial information..."
- Why unresolved: The authors restricted their experiments to "parameter-free policies that are applied post-training" to ensure ease of deployment and model agnosticism.
- What evidence would resolve it: Training a model specifically for online summarization (e.g., using reinforcement learning or imitation learning) and comparing its ROUGE/Latency scores against the sliding window baseline.

### Open Question 3
- Question: How can latency be evaluated using explicit content alignment rather than relying on the simplifying assumption that the summary captures all relevant content since the last update?
- Basis in paper: [explicit] In Section 6.2, regarding the measurement of latency: "Producing such an alignment is a research topic in itself... we propose our own metric, Expected Latency (EL), based on a simplifying assumption."
- Why unresolved: The authors utilized "Expected Latency" because establishing a fine-grained alignment between source facts and output tokens was outside the scope of the current methodology.
- What evidence would resolve it: Developing a metric that tracks specific factual units from the source transcript to the summary output to calculate precise delays per fact.

## Limitations
- Quality estimation mechanism for model-based policy is underspecified with no implementation details
- Study limited to English-language meetings on a single dataset (AutoMin)
- Scalability to long meetings (2-3+ hours) and multi-language domains not tested
- Relationship between Redundancy Factor and actual computational costs not empirically validated

## Confidence
- **High Confidence:** The comparative effectiveness of sliding window versus length-based policies, supported by multiple metrics (ROUGE-1, R1-AUC, latency, human evaluation) and consistent across different backend models (Bart, GPT-4)
- **Medium Confidence:** The superiority of dynamic segmentation over fixed policies, as this conclusion depends on the specific quality estimation approach and may vary with different model architectures or meeting domains
- **Low Confidence:** The scalability and cost-effectiveness claims based on Redundancy Factor (RF), as the relationship between RF and actual computational costs is not empirically validated beyond the tested models and may vary significantly with model architecture (e.g., quadratic attention costs for long contexts)

## Next Checks
1. **Quality Estimation Validation:** Implement and test the model-based quality estimation mechanism on held-out data to verify that model confidence scores correlate with actual summary quality, and assess whether this correlation holds across different meeting domains and topics.

2. **Long Meeting Scalability:** Evaluate the sliding window policy on meetings exceeding 2-3 hours to test whether the state management approach maintains quality and whether latency remains bounded, particularly examining the impact of context window limitations.

3. **Cross-Dataset Generalization:** Apply the best-performing policies to at least one additional meeting summarization dataset (e.g., AMI, ICSI, or a multilingual corpus) to verify that the observed quality-latency trade-offs and human preference patterns hold across different meeting types, domains, and languages.