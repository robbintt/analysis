---
ver: rpa2
title: Convergence and Divergence of Language Models under Different Random Seeds
arxiv_id: '2509.26643'
source_url: https://arxiv.org/abs/2509.26643
tags:
- convergence
- training
- tokens
- across
- phase
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the convergence behavior of language models\
  \ trained with different random seeds by measuring per-token Kullback\u2013Leibler\
  \ divergence across seeds. The authors identify four distinct phases of convergence:\
  \ an initial uniform phase, a sharp-convergence phase, a sharp-divergence phase,\
  \ and a slow-reconvergence phase."
---

# Convergence and Divergence of Language Models under Different Random Seeds

## Quick Facts
- arXiv ID: 2509.26643
- Source URL: https://arxiv.org/abs/2509.26643
- Reference count: 11
- Primary result: Language models trained with different random seeds exhibit a four-phase convergence pattern measured by per-token KL divergence.

## Executive Summary
This paper investigates how language models trained with different random seeds converge or diverge in their predictions. By measuring KL divergence between seed pairs across training, the authors identify four distinct phases: initial uniform output, sharp convergence to unigram distributions, sharp divergence as context dependence emerges, and slow reconvergence in later training. The analysis reveals that larger models reconverge faster while smaller models may plateau in divergence, and that convergence patterns vary systematically across token frequency and linguistic categories.

## Method Summary
The study analyzes 10 independently trained models per size from the PolyPythia suite (14M–410M parameters) at logarithmically-spaced checkpoints. Per-token KL divergence is computed between all seed pairs on a 4,662-token Pile validation subset, then averaged to produce expected convergence scores. Conditional convergence is analyzed by binning tokens according to frequency, part-of-speech tags (using NLTK with subword alignment), and final surprisal. No training is performed; the work analyzes pre-trained checkpoints.

## Key Results
- Language models exhibit four distinct convergence phases: uniform → sharp-convergence → sharp-divergence → slow-reconvergence
- Larger models reconverge faster in later training stages, while smaller models never fully reconverge
- Frequent tokens and function words converge more reliably than infrequent tokens and content words
- Convergence patterns vary systematically across linguistic categories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LM convergence follows a four-phase pattern driven by transitions from uniform → unigram → context-dependent → stabilized representations.
- Mechanism: Early training produces near-uniform output distributions (enforced by random initialization with small learning rates). Models then converge rapidly as they learn a shared unigram distribution that is context-agnostic. Divergence occurs when models begin leveraging context in seed-specific ways. Reconvergence emerges as induction heads form, stabilizing in-context learning and reducing cross-seed variability.
- Core assumption: KL divergence between seeds is a valid proxy for representational convergence toward a shared solution.
- Evidence anchors:
  - [abstract] "identify a four-phase convergence pattern: (i) an initial uniform phase, (ii) a sharp-convergence phase, (iii) a sharp-divergence phase, and (iv) a slow-reconvergence phase"
  - [Page 3-4] Sharp-convergence phase "corresponds quite clearly to a shift in LMs from mimicking a uniform to a unigram distribution"; slow-reconvergence "coincides with an increase in in-context learning (ICL) scores"
  - [corpus] Weak direct evidence; neighbor papers address KL scales but not the four-phase dynamic.
- Break condition: If models do not form induction heads (e.g., very small architectures or constrained training), reconvergence may not occur.

### Mechanism 2
- Claim: Larger models achieve faster and more complete reconvergence than smaller models, which may plateau without reaching stable cross-seed agreement.
- Mechanism: Increased parameter count provides redundant representational capacity, allowing different seeds to find functionally similar solutions despite different optimization trajectories. Smaller models lack this capacity, leading to persistent divergence on tokens requiring complex contextual integration.
- Core assumption: Model size is the primary driver of reconvergence capacity, not data or compute scaling independently.
- Evidence anchors:
  - [abstract] "larger models reconverge faster in later training stages, while smaller models never actually reconverge"
  - [Page 4] "for the smallest models, model convergence seems to mostly stabilise at this point, and they end training with similar E[conv] to what they begin with"
  - [corpus] PolyPythias paper (van der Wal et al., 2025) is cited as the source of multi-seed training runs; no corpus paper directly confirms the size-reconvergence link.
- Break condition: If training duration is insufficient even for large models, reconvergence may not complete.

### Mechanism 3
- Claim: Token-level convergence is stratified by frequency and linguistic category, with frequent tokens and function words converging faster and more reliably than infrequent tokens and content words.
- Mechanism: Frequent tokens receive more gradient updates, leading to earlier stabilization across seeds. Function words have lower contextual variability (more predictable distributional patterns), while content words require context-sensitive predictions that diverge across seeds. Infrequent tokens may end training with worse convergence than initialization.
- Core assumption: Frequency and PoS effects are separable, though they correlate (function words tend to be frequent).
- Evidence anchors:
  - [abstract] "frequent tokens and function words converge faster and more reliably than their counterparts (infrequent tokens and content words)"
  - [Page 5] "final convergence in infrequent tokens is smaller than initial convergence, suggesting LMs diverge on these tokens across training"
  - [corpus] No corpus paper directly replicates this token-level analysis.
- Break condition: If a dataset has unusual frequency distributions (e.g., synthetic balanced corpora), the frequency-convergence relationship may weaken.

## Foundational Learning

- Concept: Kullback–Leibler (KL) divergence
  - Why needed here: The paper's central metric for measuring cross-seed agreement; measures how one probability distribution diverges from another.
  - Quick check question: If two models assign probabilities (0.7, 0.3) and (0.5, 0.5) to two outcomes, which direction of KL is larger and why?

- Concept: Unigram vs. context-conditioned language models
  - Why needed here: Explains the sharp-convergence phase—models first learn frequency-based predictions before leveraging context.
  - Quick check question: A unigram LM assigns P("the") = 0.06 regardless of context. What information is it ignoring that a full LM would use?

- Concept: Induction heads and in-context learning
  - Why needed here: The slow-reconvergence phase coincides with induction head formation; understanding this connects architecture-level circuits to training dynamics.
  - Quick check question: What capability do induction heads enable that would cause models to stabilize their predictions across seeds?

## Architecture Onboarding

- Component map: PolyPythia models -> KL divergence computation -> Conditional analysis (frequency/PoS/surprisal)
- Critical path:
  1. Load multiple seeds of same architecture at matching checkpoints
  2. For each token position, compute KL divergence between seed pairs on the output vocabulary distribution
  3. Average across seed pairs, then across tokens in dataset
  4. For conditional analysis, filter tokens by property before averaging

- Design tradeoffs:
  - Using KL (vs. correlation) captures distributional similarity, not just rank-order agreement
  - Subword tokenization complicates PoS mapping (requires majority-vote heuristics)
  - Limited to same tokenizer across models; cannot compare across model families

- Failure signatures:
  - No divergence phase: Check if learning rate warmup extends too long, masking the transition
  - No reconvergence: Model may be too small or training too short
  - High variance in convergence: May indicate unstable data subset or tokenizer artifacts

- First 3 experiments:
  1. Replicate the four-phase pattern on a single model size (e.g., 70M) using 3+ seeds, plotting E[conv] across training steps to verify phase boundaries.
  2. Ablate token frequency: Bin tokens by frequency and plot conditional convergence separately for top-10% vs. bottom-10% frequent tokens.
  3. Test the model-size hypothesis: Compare final-step convergence across all available sizes to confirm larger models achieve higher convergence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the four-phase convergence dynamics transfer to non-English languages or multilingual training settings?
- Basis in paper: [explicit] The authors state in the Limitations section that the analysis is "restricted to English-language data, leaving open questions about whether similar convergence dynamics occur in other languages and in multilingual settings."
- Why unresolved: The current experiments rely solely on the English Pile dataset, and convergence patterns (especially for low-frequency tokens) might differ in languages with different morphological complexity or tokenization efficiency.
- What evidence would resolve it: Replicating the expected convergence analysis on models trained on multilingual corpora (e.g., mC4) and comparing the divergence phases across specific languages.

### Open Question 2
- Question: Is there a causal relationship between induction-head formation and the onset of the slow-reconvergence phase?
- Basis in paper: [explicit] The paper notes that the start of the final phase "coincides with an increase in in-context learning (ICL) scores" and suggests induction heads "may... also stabilise the training," but stops short of claiming a mechanism.
- Why unresolved: The evidence is currently correlational, based on the observation that the timing of induction-head formation aligns with when models begin to reconverge.
- What evidence would resolve it: Ablation studies specifically targeting induction heads to see if their removal prevents or delays the slow-reconvergence phase in later training steps.

### Open Question 3
- Question: Do these convergence phases persist when comparing across different model families with diverse tokenizers?
- Basis in paper: [explicit] The authors explicitly note that their metric prevents comparing different families and they "leave that for future work" regarding converting distributions to byte- or word-level to mitigate this.
- Why unresolved: The token-level KL divergence calculation requires identical distribution supports (vocabularies), making it currently impossible to determine if the observed dynamics are specific to the Pythia suite's architecture and tokenizer.
- What evidence would resolve it: Developing and applying alignment techniques (e.g., byte-level probability mapping) to compare training dynamics across distinct architectures like Llama vs. GPT.

## Limitations

- Analysis limited to a single small English dataset (4,662 tokens from Pile validation)
- Cannot compare convergence patterns across different model families due to tokenizer requirements
- Interpretation of reconvergence as being driven by induction heads is correlational rather than mechanistic

## Confidence

**High Confidence**: The existence of the four-phase convergence pattern and the relationship between model size and reconvergence capability are well-supported by the empirical data and the consistency of the KL divergence trajectories across seeds. The stratification of convergence by token frequency and PoS categories is directly observable in the conditional analysis results.

**Medium Confidence**: The interpretation of sharp-convergence as a shift from uniform to unigram distributions and the link between slow-reconvergence and induction head formation are plausible but rely on external evidence and mechanistic assumptions that aren't fully validated within the paper. The claim that larger models reconverge "faster" requires careful interpretation of what constitutes the start and end of the reconvergence phase.

**Low Confidence**: The specific attribution of divergence to context-dependent seed-specific solutions, while intuitive, lacks direct mechanistic evidence showing that the divergent predictions are actually context-dependent rather than stemming from other factors like different weight initialization basins.

## Next Checks

1. **Dataset Generalization Test**: Replicate the convergence analysis using multiple datasets of varying sizes and domains (e.g., Pile subsets, C4, or synthetic data) to verify that the four-phase pattern and token-level stratification persist across data distributions.

2. **Mechanistic Circuit Analysis**: Perform circuit analysis on the models to identify induction heads and correlate their emergence with the reconvergence phase. Verify that the reconvergence is specifically driven by in-context learning capabilities rather than general optimization convergence.

3. **Architectural Scaling Test**: Extend the analysis to include both smaller models (below 14M parameters) and larger models (beyond 410M parameters) to test whether the reconvergence pattern holds at the extremes of the scaling spectrum and to identify potential scaling break points.