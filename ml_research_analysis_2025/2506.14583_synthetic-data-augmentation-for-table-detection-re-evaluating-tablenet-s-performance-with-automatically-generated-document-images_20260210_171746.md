---
ver: rpa2
title: 'Synthetic Data Augmentation for Table Detection: Re-evaluating TableNet''s
  Performance with Automatically Generated Document Images'
arxiv_id: '2506.14583'
source_url: https://arxiv.org/abs/2506.14583
tags:
- table
- document
- data
- detection
- tablenet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of table detection in document
  images, which is currently hindered by a lack of annotated datasets. The authors
  propose an automated LaTeX-based pipeline to synthesize realistic two-column document
  pages with diverse table layouts and aligned ground-truth masks.
---

# Synthetic Data Augmentation for Table Detection: Re-evaluating TableNet's Performance with Automatically Generated Document Images

## Quick Facts
- arXiv ID: 2506.14583
- Source URL: https://arxiv.org/abs/2506.14583
- Reference count: 34
- Primary result: Training TableNet on synthetic data achieves 4.04% pixel-wise XOR error on synthetic test set (256x256), outperforming baseline on Marmot benchmark (9.18%)

## Executive Summary
This paper addresses the challenge of table detection in document images by proposing an automated LaTeX-based pipeline to synthesize realistic document pages with diverse table layouts. The synthetic dataset augments the Marmot benchmark, enabling a systematic resolution study of TableNet, a deep learning model for table detection. The authors demonstrate that training TableNet on synthetic data achieves superior performance on synthetic test sets compared to real-world benchmarks, highlighting the potential of synthetic data augmentation while also revealing domain gaps between synthetic and real document images.

## Method Summary
The authors developed an automated LaTeX-based pipeline to generate synthetic document images with diverse table layouts and aligned ground-truth masks. This pipeline produces two-column document pages with realistic formatting, including text, figures, and tables. The synthetic dataset is used to augment the Marmot benchmark, a commonly used dataset for table detection. TableNet, a deep learning model originally designed for medical image segmentation, is adapted for table detection by modifying its architecture to output binary masks indicating table regions. The model is trained and evaluated across different input resolutions (256x256 and 1024x1024) to study the impact of resolution on performance.

## Key Results
- TableNet trained on synthetic data achieves 4.04% pixel-wise XOR error on synthetic test set at 256x256 resolution
- Best performance on Marmot benchmark is 9.18% (at 256x256 resolution)
- Higher input resolution (1024x1024) does not universally improve performance, particularly when transferring to real-world data
- Domain differences between synthetic and real data significantly impact table detection performance

## Why This Works (Mechanism)
The synthetic data generation pipeline works by leveraging LaTeX's ability to create structured documents with precise control over layout and formatting. By programmatically generating LaTeX code with randomized but realistic parameters (font sizes, column widths, table structures), the system produces document images that capture essential visual characteristics of real documents while maintaining perfect ground-truth annotations. The TableNet architecture, originally designed for biomedical image segmentation, proves effective for table detection because both tasks involve identifying contiguous regions of interest within complex backgrounds.

## Foundational Learning
- LaTeX document generation - Why needed: Provides controlled environment for creating labeled data with perfect annotations; Quick check: Verify generated documents render correctly across different LaTeX engines
- Pixel-wise XOR error metric - Why needed: Quantifies difference between predicted and ground-truth masks at pixel level; Quick check: Compare XOR error with other metrics like IoU on sample predictions
- TableNet architecture - Why needed: CNN-based encoder-decoder structure suitable for pixel-level segmentation tasks; Quick check: Validate receptive field matches expected table sizes in documents

## Architecture Onboarding

Component map: LaTeX generator -> Document renderer -> Image preprocessor -> TableNet model -> XOR error calculator

Critical path: Synthetic document generation → Model training → Resolution scaling → Performance evaluation

Design tradeoffs: The LaTeX-based approach prioritizes control and perfect annotations over capturing real-world document imperfections; higher resolutions increase computational cost without guaranteed performance gains on real data.

Failure signatures: Poor performance on real data despite good synthetic results indicates domain gap; inconsistent table detection across similar layouts suggests overfitting to synthetic patterns.

First experiments:
1. Generate and visualize 10 synthetic documents with varying table layouts to verify diversity
2. Train TableNet for 1 epoch on synthetic data and examine prediction masks qualitatively
3. Compare XOR errors at different resolutions on a small validation set to identify trends

## Open Questions the Paper Calls Out
None

## Limitations
- LaTeX templates may not fully capture real-world document complexity and variability
- Marmot benchmark contains only 2,000 annotated pages, limiting statistical power
- Study focuses exclusively on TableNet, leaving generalizability to other architectures unclear
- Pixel-wise XOR error metric doesn't capture all aspects of table detection quality

## Confidence
- Synthetic data generation pipeline effectiveness: High
- Performance improvements on synthetic test set: High
- Performance on Marmot benchmark: Medium
- Input resolution impact analysis: Medium

## Next Checks
1. Test the synthetic data augmentation approach with alternative table detection models (e.g., Faster R-CNN, Mask R-CNN) to assess generalizability
2. Generate synthetic data using alternative methods (e.g., GAN-based approaches) to compare performance and identify potential biases in the LaTeX-based approach
3. Conduct a user study to evaluate the practical utility of detected tables in downstream tasks, as pixel-wise error may not correlate with real-world usability