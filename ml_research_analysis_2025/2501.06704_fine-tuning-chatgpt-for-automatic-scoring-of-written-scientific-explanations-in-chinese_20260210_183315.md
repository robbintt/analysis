---
ver: rpa2
title: Fine-tuning ChatGPT for Automatic Scoring of Written Scientific Explanations
  in Chinese
arxiv_id: '2501.06704'
source_url: https://arxiv.org/abs/2501.06704
tags:
- scoring
- responses
- reasoning
- accuracy
- students
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study fine-tuned ChatGPT for automatic scoring of Chinese
  student responses to scientific explanation tasks. The model achieved accuracy rates
  between 75% and 96% across seven different items, demonstrating strong performance
  in a non-English context.
---

# Fine-tuning ChatGPT for Automatic Scoring of Written Scientific Explanations in Chinese

## Quick Facts
- arXiv ID: 2501.06704
- Source URL: https://arxiv.org/abs/2501.06704
- Reference count: 10
- Primary result: Fine-tuned ChatGPT achieved 75-96% accuracy for scoring Chinese student scientific explanations

## Executive Summary
This study demonstrates that domain-specific fine-tuning of ChatGPT can effectively score written scientific explanations in Chinese, achieving accuracy rates between 75% and 96% across seven different assessment items. The research reveals an important asymmetry: scoring accuracy correlates negatively with reasoning complexity for lower-performing students but positively for higher-performing students. Qualitative analysis shows that linguistic features like sentence structure and terminology usage significantly influence scoring outcomes, with simpler responses yielding better accuracy for lower-level explanations while longer, information-rich responses improve accuracy for higher-level responses.

## Method Summary
The study fine-tuned ChatGPT using 7,626 Chinese student responses across seven scientific explanation tasks, with data split into training and testing sets (e.g., 964 train/242 test for Item 1). Human raters achieved Cohen's Kappa > 0.8 inter-rater reliability. The fine-tuning process used classification loss through the OpenAI API with a "cautiously low" learning rate and multiple epochs. Evaluation included classification accuracy for both holistic (0/1) and analytic rubrics (four PTDR elements), with Kendall correlation analysis examining the relationship between reasoning complexity and scoring accuracy. The dataset was anonymized and formatted as JSON per OpenAI fine-tuning specifications.

## Key Results
- Fine-tuned models achieved scoring accuracy above 75% for all scoring categories across seven items
- Accuracy varied by item (80-94%) and element (75-96%), with Reasoning element showing significant variation
- Negative correlation (-0.31 average) between reasoning complexity and accuracy for lower-level responses
- Positive correlation between reasoning complexity and accuracy for higher-level responses
- Linguistic comprehensiveness and structural clarity exist in tension, with optimal features differing by response quality level

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Domain-specific fine-tuning adapts general LLM capabilities to logographic language scoring tasks with acceptable accuracy.
- **Mechanism:** Fine-tuning adjusts model weights to recognize Chinese linguistic patterns (implicit reasoning, synthetic structure) and align them with scoring rubrics through supervised learning on human-scored responses.
- **Core assumption:** The base model's pretrained representations transfer sufficiently to Chinese scientific text despite English-centric pretraining.
- **Evidence anchors:** [abstract] "Results indicate that through domain-specific adaptation, the fine-tuned ChatGPT can accurately score students' written explanations in Chinese." [section 5.1] "All fine-tuned models achieved scoring accuracy above 0.75 for all scoring categories, though varying by item." [corpus] Neighbor paper on Chinese AES (arXiv:2504.05736) similarly finds LLMs underexplored for Chinese automated scoring, suggesting transfer is possible but requires task-specific adaptation.
- **Break condition:** Accuracy drops below 70% when base model lacks sufficient Chinese pretraining data, or when scoring criteria require reasoning beyond model's pretraining distribution.

### Mechanism 2
- **Claim:** Reasoning complexity exhibits an asymmetrical relationship with scoring accuracy that inverts based on student performance level.
- **Mechanism:** For lower-level responses, complex sentence structures and technical terminology create false-positive signals that the model interprets as competence; for higher-level responses, concise causal reasoning lacks sufficient diagnostic features for accurate identification.
- **Core assumption:** The model learns surface-level correlates of competence (length, terminology, structure complexity) that may not align with actual reasoning quality.
- **Evidence anchors:** [abstract] "Scoring accuracy correlates with reasoning complexity: a negative correlation for lower-level responses and a positive one for higher-level responses." [section 5.2.1] "The average correlation coefficient across all items and scores was -0.31... indicating that for lower-level performing students, the higher the complexity of reasoning in their explanations, the lower the scoring accuracy." [section 5.2.2] "For higher-level students, the higher the reasoning complexity is, the higher of scoring accuracy yielded by ChatGPT." [corpus] Weak direct corpus evidence for this asymmetry pattern; related work on LLM scoring bias (arXiv:2505.10643) examines ELL bias but not complexity-accuracy inversion.
- **Break condition:** Pattern inverts or disappears when training data is balanced across performance levels and reasoning complexity categories, or when rubrics explicitly weight linguistic simplicity/complexity.

### Mechanism 3
- **Claim:** Linguistic comprehensiveness and structural clarity exist in tension, with optimal features differing by response quality level.
- **Mechanism:** Short, simple responses reduce parsing ambiguity for incorrect explanations (fewer false diagnostic features); long, information-rich responses provide sufficient signal density for correct explanations to be distinguished from near-misses.
- **Core assumption:** The model's token-level attention mechanisms aggregate linguistic features without explicit reasoning verification.
- **Evidence anchors:** [abstract] "Simpler, shorter responses tend to score more accurately at lower levels, whereas longer, information-rich responses yield better accuracy at higher levels." [section 5.3.1] "LC subgroup used fewer technical terms, with a higher proportion of everyday language, and averaged only 20 Chinese characters, significantly shorter than the 40 Chinese characters found in the LM subgroup." [section 5.3.2] "HC subgroup averaged about 46 Chinese characters, which had 20 more characters compared with the HM subgroup... used mathematical symbols or formulas more frequently." [corpus] No direct corpus validation of this tension pattern in Chinese scientific writing assessment.
- **Break condition:** Tension resolves when model is trained with explicit linguistic feature augmentation that normalizes for response length and complexity independent of content quality.

## Foundational Learning

- **Concept: Kendall Rank Correlation (Kendall's Tau)**
  - Why needed here: Used to measure ordinal association between reasoning complexity (ranked: low/medium/high) and scoring accuracy (binary consistent/inconsistent) while controlling for student performance level.
  - Quick check question: Given paired rankings of reasoning complexity and accuracy for 10 responses, would you use Pearson's r or Kendall's Tau? Explain why.

- **Concept: Holistic vs. Analytic Scoring Rubrics**
  - Why needed here: The study uses both holistic (overall correct/incorrect) and analytic rubrics (PTDR framework: Phenomenon-Theory-Data-Reasoning elements), with accuracy varying across rubric types and elements.
  - Quick check question: If a student correctly identifies the phenomenon and invokes the right theory but makes a reasoning error, how would holistic vs. analytic scoring differ?

- **Concept: Fine-Tuning Loss Functions for Ordinal Classification**
  - Why needed here: The study mentions regression-based loss for continuous scores and classification loss for categorical scores; choosing the wrong loss function can prevent the model from learning proper ordinal relationships.
  - Quick check question: For a 5-point rubric where scores are ordered (0-4), would you use standard cross-entropy or ordinal regression loss? What breaks if you choose incorrectly?

## Architecture Onboarding

- **Component map:** Handwritten text transcription -> PII removal -> tokenization with Chinese character awareness -> JSON formatting for OpenAI API -> Fine-tuned GPT variant with domain-specific adaptation -> Accuracy metrics (MAE for continuous, classification accuracy for categorical) + Kendall correlation for complexity-accuracy analysis -> Manual coding of linguistic features

- **Critical path:** 1. Establish human-human inter-rater reliability (Cohen's Kappa > 0.8) before any model training 2. Balance training data across reasoning complexity levels and performance groups to mitigate asymmetry bias 3. Validate on held-out test set with stratified sampling across complexity levels

- **Design tradeoffs:** Lower learning rate vs. training time: Paper used "cautiously low" learning rate for Chinese adaptation, extending epochs but preserving pretrained knowledge; Data augmentation vs. authenticity: Added artificially generated samples for multi-step reasoning, but this may introduce distribution shift; Response length normalization: Not implemented, but could reduce false-positive rates for verbose incorrect responses

- **Failure signatures:** Systematic over-scoring of verbose but incorrect responses (high complexity, low accuracy in lower-level group); Under-scoring of concise correct responses (low complexity, lower accuracy in higher-level group); Element-specific accuracy collapse (e.g., Reasoning element at 75% on Item 4 vs. 96% on Item 6); Accuracy variance across items (80-94%) suggests domain generalization gaps

- **First 3 experiments:** 1. **Baseline calibration:** Fine-tune on single item, evaluate on same item's held-out set to establish item-specific accuracy ceiling before cross-item generalization. 2. **Complexity stratification:** Train separate models for low/medium/high complexity responses; compare accuracy to unified model to quantify complexity-mediated error contribution. 3. **Linguistic feature ablation:** Add explicit features (response length, sentence count, technical term density) as auxiliary inputs; test whether explicit normalization reduces asymmetry in complexity-accuracy correlation.

## Open Questions the Paper Calls Out

- **Open Question 1:** Do other large language models (LLMs) exhibit the same negative correlation between reasoning complexity and scoring accuracy for lower-level Chinese responses as fine-tuned ChatGPT? Basis: [explicit] The authors state that "predicting similar outcomes with other LLMs remains hypothetical and requires empirical validation." Why unresolved: The study exclusively utilized ChatGPT; different model architectures may process implicit Chinese reasoning differently. What evidence would resolve it: A comparative analysis using the same dataset across multiple LLM families (e.g., BERT, Llama) to compare accuracy-complexity correlations.

- **Open Question 2:** How can a structured quantitative framework isolate the specific impact of Chinese linguistic features (e.g., implicit reasoning, idioms) on scoring accuracy? Basis: [explicit] The conclusion highlights the "exploratory nature" of the current qualitative analysis and calls for a "more structured quantitative framework." Why unresolved: Current findings rely on manual qualitative coding, which limits the ability to statistically generalize the impact of specific linguistic features. What evidence would resolve it: A regression analysis modeling linguistic markers (implicitness, sentence structure) as independent variables against scoring error rates.

- **Open Question 3:** Can specific fine-tuning interventions reduce the model's tendency to overrate complex reasoning in incorrect responses? Basis: [inferred] The paper identifies that the model "tends to overrate complex reasoning for low-level responses with complex sentence structures," but offers no solution. Why unresolved: While the study identifies this systematic bias, it does not test algorithmic methods to decouple linguistic complexity from logical correctness in the scoring process. What evidence would resolve it: Ablation studies testing penalty mechanisms for syntactic complexity in responses lacking correct theoretical elements.

## Limitations

- Data access and reproducibility: The core dataset (7,626 student responses across 7 items) is not publicly available, preventing independent verification of results.
- Model specification ambiguity: The base model version is not explicitly stated beyond "ChatGPT," which could refer to different gpt-3.5-turbo variants with varying capabilities.
- Reasoning complexity asymmetry validation: While the study identifies an intriguing pattern, the corpus evidence for this pattern is weak and the mechanism remains largely speculative.

## Confidence

- **High confidence:** Domain-specific fine-tuning achieves acceptable accuracy (75-96%) for Chinese AES tasks, supported by direct evidence from the study's results section showing all models exceeded 75% accuracy.
- **Medium confidence:** Linguistic features influence scoring accuracy differently for low vs. high performers, based on qualitative analysis but lacking direct quantitative validation of the tension mechanism.
- **Low confidence:** The reasoning complexity-accuracy asymmetry mechanism, as the underlying cause remains theoretically grounded but empirically under-supported in the Chinese AES context.

## Next Checks

1. **Dataset release and replication:** Obtain or create a comparable Chinese AES dataset with human-scored labels, fine-tune the same model architecture using specified protocols, and verify accuracy ranges and complexity-accuracy patterns independently.

2. **Controlled complexity stratification experiment:** Train separate models for low/medium/high complexity responses, systematically measure accuracy differences, and test whether balancing training data across complexity levels reduces the asymmetry bias.

3. **Linguistic feature ablation study:** Add explicit linguistic features (response length, sentence count, technical term density) as auxiliary inputs to the fine-tuning process and evaluate whether this normalization eliminates the observed tension between comprehensiveness and structural clarity in scoring accuracy.