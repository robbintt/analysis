---
ver: rpa2
title: Bayesian continual learning and forgetting in neural networks
arxiv_id: '2504.13569'
source_url: https://arxiv.org/abs/2504.13569
tags:
- learning
- mesu
- task
- prior
- uncertainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Metaplasticity from Synaptic Uncertainty
  (MESU), a continual learning framework for neural networks that addresses catastrophic
  forgetting and catastrophic remembering. MESU uses Bayesian neural networks with
  a forgetting mechanism that retains only the most recent N tasks, balancing learning
  and forgetting through a principled update rule.
---

# Bayesian continual learning and forgetting in neural networks

## Quick Facts
- arXiv ID: 2504.13569
- Source URL: https://arxiv.org/abs/2504.13569
- Reference count: 38
- Primary result: MESU achieves 91.37% average accuracy on Permuted MNIST's last five tasks while preventing catastrophic forgetting and remembering

## Executive Summary
This paper introduces Metaplasticity from Synaptic Uncertainty (MESU), a continual learning framework for neural networks that addresses catastrophic forgetting and catastrophic remembering. MESU uses Bayesian neural networks with a forgetting mechanism that retains only the most recent N tasks, balancing learning and forgetting through a principled update rule. Unlike standard Bayesian approaches that accumulate constraints from all past data, MESU's forgetting term ensures the network maintains plasticity for new information.

## Method Summary
MESU implements continual learning through a truncated Bayesian posterior that maintains only the last N tasks' information. The method uses mean-field Gaussian posteriors with per-weight parameters (μᵢ, σᵢ), updating them via variance-scaled gradients that implement metaplasticity. The free-energy objective includes separate learning and forgetting terms, preventing the network from becoming over-constrained while preserving recent knowledge. MESU is trained with Monte Carlo sampling for both inference and gradient estimation, using a memory window hyperparameter N to control the forgetting rate.

## Key Results
- On Permuted MNIST with 200 sequential tasks, MESU achieves 91.37% average accuracy on the last five tasks
- Outperforms established methods like EWC and Synaptic Intelligence on CIFAR-10/CIFAR-100 incremental learning across various task splits
- Provides reliable epistemic uncertainty estimates for out-of-distribution detection, maintaining performance even after extensive training
- Theoretical connections to Hessian-based regularization and biological metaplasticity provide additional insight into its effectiveness

## Why This Works (Mechanism)

### Mechanism 1
Truncated Bayesian posterior with a controlled forgetting window prevents both catastrophic forgetting and catastrophic remembering. Standard Bayesian continual learning accumulates constraints from all past data indefinitely. MESU introduces a truncated posterior that retains only the last N tasks, explicitly removing the contribution of task t-N-1 via a "forgetting term." This yields a free-energy objective with separate learning and forgetting components, preventing the network from becoming over-constrained while preserving recent knowledge. Core assumption: Datasets within the memory window have approximately equal marginal likelihoods.

### Mechanism 2
Variance-scaled gradients implement metaplasticity—uncertain weights update faster, confident weights stabilize. The MESU update rules place σ² explicitly in front of gradient terms, replacing a fixed learning rate with an uncertainty-weighted one. High σ indicates low confidence, permitting larger updates; low σ indicates high confidence, damping updates. This is derived from free-energy minimization under small-update assumptions. Core assumption: The small-update assumption |Δσ/σ| ≪ 1 holds during training.

### Mechanism 3
Variance asymptotically approximates the inverse diagonal Hessian, connecting MESU to second-order optimization and EWC/SI. Under i.i.d. assumptions, the variance dynamics converge to values inversely proportional to the estimated curvature. The variance encodes curvature-based parameter importance without explicit Hessian computation. Core assumption: Mini-batches approximate i.i.d. data for curvature estimation.

## Foundational Learning

**Variational Inference with Mean-Field Gaussian Posteriors**: MESU maintains q_θ(ω) = Π N(ωᵢ; μᵢ, σᵢ²) as an approximation to the true posterior; understanding KL divergence minimization is essential. Quick check: Can you explain why mean-field independence enables tractable KL computation but ignores weight correlations?

**Catastrophic Forgetting vs. Catastrophic Remembering**: MESU explicitly addresses both—forgetting (losing old knowledge) and remembering (becoming over-constrained, losing plasticity). Quick check: Why does standard Bayesian updating without forgetting lead to "vanishing uncertainty"?

**Reparameterization Trick for Gradient Estimation**: Gradients ∂C/∂μ and ∂C/∂σ are computed via ω = μ + ε·σ with ε ~ N(0, I), enabling backpropagation through sampling. Quick check: How many weight samples are needed for stable gradient estimates, and what is the computational cost?

## Architecture Onboarding

**Component map**: Variational posterior (μᵢ, σᵢ) per weight → Free-energy loss (KL + NLL + forgetting term) → MESU update rules (Eqs. 10-11) → Inference (weight sampling for predictions and uncertainty)

**Critical path**: 1) Initialize μ via reweighted Kaiming, σ as constant; 2) Per mini-batch: sample weights → compute loss → compute gradients ∂C/∂μ, ∂C/∂σ → apply MESU updates; 3) Tune N based on desired retention horizon; σ² converges on timescale t_c = N/γ

**Design tradeoffs**: Larger N retains more tasks but risks variance collapse and reduced plasticity; smaller N maintains plasticity but forgets older tasks faster; more samples give better uncertainty estimates but higher compute

**Failure signatures**: Catastrophic remembering (accuracy plateaus low, variance collapsed → N too large); Catastrophic forgetting (accuracy on old tasks drops sharply → N too small or σ² initialized too high); Vanishing OOD detection (epistemic uncertainty no longer separates in/out-of-distribution → variance collapsed)

**First 3 experiments**: 1) Sanity check on MNIST (single task): Train 10-50 epochs; verify σ² converges to stable non-zero values and OOD detection (vs. Fashion-MNIST) remains high; 2) Permuted MNIST (5-10 tasks): Validate that accuracy on first task remains >85% after learning subsequent tasks; compare N = 50k, 100k, 300k; 3) Catastrophic remembering test: Run FOO-VB Diagonal vs. MESU on 50+ Permuted MNIST tasks; confirm MESU maintains plasticity (accuracy on final tasks >85%) while FOO-VB degrades

## Open Questions the Paper Calls Out

**Open Question 1**: Can MESU be extended to retain information from tasks older than the forgetting window N by combining it with memory-replay strategies? The current formulation explicitly truncates the posterior to the most recent N tasks, discarding information from time t-N-1 without a mechanism to recover it.

**Open Question 2**: How can the computational cost of sampling be reduced to make MESU tractable for very large-scale neural networks? The method relies on Monte Carlo sampling for variational inference, which scales poorly with model size compared to standard backpropagation.

**Open Question 3**: How does MESU's performance vary when the assumption of equal marginal likelihood across tasks is violated? The theoretical derivation assumes each dataset has equal marginal likelihood to create a tractable update rule, but the text concedes this may not hold in real scenarios where tasks differ in complexity or size.

## Limitations

- The truncated posterior assumption (equal marginal likelihoods across N tasks) may break down with non-stationary data distributions or varying task complexities
- The small-update assumption for variance dynamics could fail during early training or when σ² approaches zero, potentially destabilizing the metaplasticity mechanism
- Numerical implementation details for the reparameterization trick under MESU's variance-scaled gradients are not fully specified

## Confidence

- **High Confidence**: MESU's effectiveness in preventing catastrophic forgetting on Permuted MNIST (91.37% avg. accuracy) and its OOD uncertainty calibration performance
- **Medium Confidence**: The theoretical connection between MESU's variance and diagonal Hessian approximation, given dependence on i.i.d. mini-batch assumptions
- **Low Confidence**: The generality of MESU across diverse CL scenarios, as current evaluations focus on relatively simple architectures and controlled task distributions

## Next Checks

1. Test MESU's variance stability across heterogeneous task distributions (varying dataset sizes, complexity) to validate the equal-marginal-likelihood assumption
2. Perform systematic ablation studies varying N (memory window) and σ² initialization to map the stability boundary between catastrophic forgetting and catastrophic remembering
3. Extend evaluation to more challenging CL benchmarks (e.g., CORe50, RL-CIFAR) with longer task sequences and non-i.i.d. data to stress-test the variance-Hessian approximation under realistic streaming conditions