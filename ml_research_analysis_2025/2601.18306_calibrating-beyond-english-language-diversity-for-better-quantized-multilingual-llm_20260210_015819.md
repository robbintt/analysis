---
ver: rpa2
title: 'Calibrating Beyond English: Language Diversity for Better Quantized Multilingual
  LLM'
arxiv_id: '2601.18306'
source_url: https://arxiv.org/abs/2601.18306
tags:
- calibration
- gptq
- multilingual
- quantization
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Quantization of multilingual LLMs typically relies on English-only\
  \ calibration, which can degrade performance for non-English languages. This study\
  \ systematically evaluates eight calibration strategies\u2014five single-language\
  \ and three multilingual mixes\u2014across ten languages, two quantizers (GPTQ,\
  \ AWQ), and two model families (Llama3.1 8B, Qwen2.5 7B)."
---

# Calibrating Beyond English: Language Diversity for Better Quantized Multilingual LLM

## Quick Facts
- **arXiv ID**: 2601.18306
- **Source URL**: https://arxiv.org/abs/2601.18306
- **Reference count**: 14
- **Primary result**: Multilingual calibration sets consistently outperform English-only baselines in multilingual LLM quantization, with up to 3.52 perplexity reduction.

## Executive Summary
This study investigates how the choice of calibration data language affects the quality of post-training quantization (PTQ) for multilingual large language models (LLMs). Contrary to the standard practice of using English-only calibration, the authors find that non-English and multilingual calibration sets consistently yield better perplexity and downstream accuracy across ten languages. They evaluate eight calibration strategies—single-language and multilingual mixes—across two model families (Llama3.1 8B, Qwen2.5 7B) and two quantizers (GPTQ, AWQ). The results show that language-diverse calibration is crucial for robust multilingual quantization, with multilingual mixes delivering the largest average perplexity reductions.

## Method Summary
The authors systematically evaluate eight calibration strategies using ten languages (English, French, Swahili, Chinese, isiXhosa, Southern Sotho, Zulu, Yoruba, Igbo, Hausa). Calibration sets are derived from C4 and Wikipedia, translated into target languages where necessary. Two 4-bit quantizers (GPTQ, AWQ, group size 128) are applied to Llama3.1 8B and Qwen2.5 7B models, with perplexity and downstream task accuracy (XNLI, XStoryCloze, Global MMLU) as evaluation metrics. Fixed token budgets are used for each calibration variant, and results are compared against an English-only baseline.

## Key Results
- Non-English and multilingual calibration sets consistently improve perplexity over English-only baselines.
- Multilingual calibration mixes (e.g., multi10, multimix) achieve the largest average perplexity reductions (up to 3.52 points).
- Language-matched calibration yields the best per-language perplexity gains.
- GPTQ is more sensitive to calibration language than AWQ, with up to 3.52-point perplexity swing due to its Hessian-based error compensation.

## Why This Works (Mechanism)
Quantization quality depends on the activation distributions in calibration data matching those encountered at inference. English-only calibration sets have narrow, language-specific activation tails, leading to outlier clipping and higher quantization error for non-English inputs. Multilingual mixes broaden the activation distribution, reducing clipping and improving fidelity. GPTQ's Hessian-based error compensation is especially sensitive to calibration language, while AWQ's salient channel selection is more robust.

## Foundational Learning
- **Post-training quantization (PTQ)**: Reducing model precision (e.g., 16-bit to 4-bit) without retraining; needed to deploy large models on resource-constrained hardware. Quick check: Does your hardware support 4-bit inference?
- **Calibration data**: Representative samples used to set quantization ranges; critical for minimizing quantization error. Quick check: Are calibration and test data from the same distribution?
- **Perplexity**: A measure of how well a language model predicts a test set; lower is better. Quick check: Is perplexity computed on a held-out, representative dataset?
- **Activation clipping**: Truncation of outlier activations during quantization; causes information loss and higher perplexity. Quick check: Plot per-layer activation histograms for calibration vs. test sets.
- **Hessian-based error compensation**: GPTQ uses second-order statistics to minimize quantization error; sensitive to calibration data diversity. Quick check: Compare perplexity for GPTQ with single-language vs. multilingual calibration.
- **Salient channel selection**: AWQ identifies important activation channels for finer-grained quantization; less sensitive to calibration language. Quick check: Verify AWQ correctly identifies high-variance channels.

## Architecture Onboarding
- **Component map**: C4/Wikipedia data -> Calibration sets (single/multilingual) -> Quantizers (GPTQ/AWQ) -> 4-bit models -> Evaluation (perplexity, downstream tasks)
- **Critical path**: Calibration set generation -> Quantization -> Evaluation
- **Design tradeoffs**: Language diversity vs. computational cost; single-language vs. multilingual calibration sets.
- **Failure signatures**: High perplexity variance across languages; outlier clipping in activation distributions (especially for GPTQ).
- **First experiments**:
  1. Generate and compare activation distributions for English vs. multilingual calibration sets.
  2. Quantize a model using English-only vs. multilingual calibration; compare perplexity on non-English test sets.
  3. Apply GPTQ and AWQ with the same calibration set; analyze sensitivity to calibration language.

## Open Questions the Paper Calls Out
- Can dynamic calibration sample selection based on real-time quantization error signals outperform the static multilingual mixes identified as optimal in this study?
- To what extent do the perplexity gains from multilingual calibration translate to complex, generative downstream tasks like machine translation or abstractive summarization?
- Do the benefits of language-diverse calibration persist, diminish, or amplify when applied to significantly larger models (e.g., 70B+ parameters) or different architectures (e.g., Mixture-of-Experts)?
- Is there a theoretically optimal ratio or specific algorithmic method for mixing "rare tokens" (code/math) with natural language to maximize activation tail coverage without degrading linguistic fluency?

## Limitations
- The study does not address potential semantic drift or safety implications of using translated or multilingual calibration data.
- Results are limited to two model families and two 4-bit quantization methods; generalizability to other architectures or bit-widths is unknown.
- The underlying mechanisms for why multilingual calibration improves Hessian-based compensation are not rigorously analyzed.
- The trade-off between calibration set size, computational cost, and performance gains is not explored.

## Confidence
- **High Confidence**: Claims about the consistent improvement of non-English and multilingual calibration sets over English-only baselines for perplexity reduction.
- **Medium Confidence**: The assertion that language-matched calibration yields the best per-language gains is supported, but improvement magnitude varies.
- **Medium Confidence**: The observation that GPTQ is more sensitive to calibration language than AWQ is clear, but exact reasons are inferred rather than directly validated.

## Next Checks
1. Plot per-layer activation distributions for calibration vs. test sets to diagnose outlier clipping and activation-range mismatches, particularly for GPTQ.
2. Evaluate the impact of calibration data language on additional quantization methods (e.g., 8-bit, 2-bit) and model architectures to assess robustness of findings.
3. Systematically vary calibration set sizes (both in token count and number of languages) to quantify the trade-off between diversity, computational cost, and quantization quality.