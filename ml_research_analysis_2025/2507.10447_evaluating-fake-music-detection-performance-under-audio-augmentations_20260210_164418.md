---
ver: rpa2
title: Evaluating Fake Music Detection Performance Under Audio Augmentations
arxiv_id: '2507.10447'
source_url: https://arxiv.org/abs/2507.10447
tags:
- music
- audio
- augmentations
- fake
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the robustness of a deepfake music detection
  model (SONICS) against audio augmentations. The researchers created a dataset containing
  real and synthetic music from multiple generators (Suno, Udio, YuE, MusicGen) and
  applied 15 types of audio transformations such as pitch shifting, equalization,
  compression, and noise addition.
---

# Evaluating Fake Music Detection Performance Under Audio Augmentations
## Quick Facts
- arXiv ID: 2507.10447
- Source URL: https://arxiv.org/abs/2507.10447
- Reference count: 0
- The SONICS model's fake music detection performance degrades significantly under audio augmentations, with empty audio files classified as fake

## Executive Summary
This study evaluates the robustness of the SONICS deepfake music detection model against various audio augmentations. Using a dataset of real and synthetic music from multiple generators (Suno, Udio, YuE, MusicGen), researchers applied 15 types of audio transformations including pitch shifting, equalization, compression, and noise addition. The model showed high sensitivity to even light augmentations, with pitch shifting by just two semitones causing Suno-generated music to be misclassified as real. Most strikingly, empty audio files were classified as fake, suggesting the model may rely on spectral artifacts rather than holistic audio analysis. The SONICS model also failed to generalize well to music from unseen generators, particularly MusicGen, which was classified as more real than expected.

## Method Summary
The researchers created a dataset containing real music and synthetic music generated by Suno, Udio, YuE, and MusicGen. They applied 15 different audio augmentations including pitch shifting, equalization, compression, noise addition, and frequency masking. The SONICS "SpecTTTra-α" model was used to evaluate detection performance on these augmented samples. All audio was downsampled to 16kHz and processed in 120-second windows. The evaluation focused on mean probability of fakeness (0-100%) compared to baseline performance on unmodified samples.

## Key Results
- Pitch shifting by just two semitones caused Suno-generated music to be misclassified as real
- The model showed high sensitivity to frequency corruption and added silence
- Empty audio files were classified as fake, indicating potential reliance on spectral artifacts
- The SONICS model failed to generalize well to music from unseen generators, particularly MusicGen

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The SONICS detector appears to rely on spectral artifacts present in generated audio rather than analyzing musical structure holistically.
- Mechanism: The model learns to identify frequency-domain signatures specific to certain generators during training. When these signatures are disrupted (by pitch shifting, frequency masking) or absent (in silence), the model's decision boundary shifts erratically rather than returning to uncertainty.
- Core assumption: The model should maintain calibrated uncertainty when artifact cues are removed or ambiguous, rather than swinging to extreme predictions.
- Evidence anchors:
  - [abstract]: "The model's performance decreased significantly even with the introduction of light augmentations."
  - [section]: "An empty file is classified as fake... We also observed a correlation between corrupting high frequencies of an audio and the probability of it being classified as fake. This might imply the model mistakenly learned to rely on certain artifacts present in the spectrum."
  - [corpus]: Limited direct corpus evidence on spectral artifact dependence; related work (Detecting Musical Deepfakes, arXiv:2505.09633) discusses structural analysis but not spectral over-reliance specifically.

### Mechanism 2
- Claim: Detection models trained on specific generators fail to generalize to unseen generation architectures due to distribution shift in artifact patterns.
- Mechanism: Each generative model produces distinct artifact signatures (codec remnants, spectral discontinuities, temporal inconsistencies). A detector trained on a subset learns generator-specific rather than universal "synthetic" features, causing misclassification on novel generators.
- Core assumption: Generators share some common detectable signatures; the failure is due to insufficient coverage rather than fundamental impossibility.
- Evidence anchors:
  - [abstract]: "The SONICS model also failed to generalize well to music from unseen generators, particularly MusicGen, which was classified as more real than expected."
  - [section]: "Model doesn't generalize well, especially on MusicGen... This is to be expected, since machine learning models are known for being unable to handle distribution shift between training and test data."
  - [corpus]: Segment Transformer (arXiv:2509.08283) addresses generalization via structural analysis; Fusion Segment Transformer (arXiv:2601.13647) targets long-form detection across generators—both suggest architecture diversity is a known challenge.

### Mechanism 3
- Claim: Pitch and silence manipulations create asymmetric prediction shifts that expose non-robust feature dependencies.
- Mechanism: Pitch shifting relocates spectral energy across frequency bands, disrupting learned artifact locations. Silence augmentation dilutes artifact density, and the model interprets low artifact concentration as signal absence (real) or anomaly (fake) depending on training distribution exposure.
- Core assumption: Robust detection should be invariant to perceptually minor transformations that don't change the "generated" nature of the content.
- Evidence anchors:
  - [abstract]: "Pitch shifting by just two semitones caused Suno-generated music to be misclassified as real."
  - [section]: "A shift down of two semitones already fools the model into classifying suno as highly real, while barely influencing the perception for a human. Another observation is that the more silence is being added to a song, the higher the probability of being fake."
  - [corpus]: Double Entendre (arXiv:2506.15981) addresses robustness via multi-view fusion; Fake Speech Wild (arXiv:2508.10559) documents cross-domain degradation—both reinforce that augmentation robustness is an open problem.

## Foundational Learning

- **Spectrogram-based audio classification**
  - Why needed here: The SONICS model uses spectral inputs (SpecTTTra architecture), and the failure modes (frequency masking, pitch shift sensitivity) are best understood through spectral representations.
  - Quick check question: Can you explain why pitch shifting by 2 semitones would change a spectrogram's appearance without making audio unrecognizable to humans?

- **Distribution shift and out-of-distribution (OOD) generalization**
  - Why needed here: The core failure is generalization to unseen generators (MusicGen, YuE) despite good performance on training generators.
  - Quick check question: If a model achieves 96% accuracy on Suno but 35% on MusicGen, what does this suggest about the training distribution?

- **Audio augmentations for robustness testing**
  - Why needed here: The paper's methodology centers on 15 augmentations (MP3 compression, noise, reverb, bit crush) to probe model brittleness.
  - Quick check question: Which augmentations would you expect to be most disruptive to a spectral artifact detector versus a temporal pattern detector?

## Architecture Onboarding

- **Component map**: 16kHz downsampled audio -> 120-second windowing -> Spectral transformation -> SpecTTTra architecture -> Binary classification with probability score
- **Critical path**: 1) Audio preprocessing → 16kHz resampling → 120s windowing 2) Spectral feature extraction 3) Classification head → probability output 4) Threshold decision
- **Design tradeoffs**: 120s context window improves accuracy on unmodified audio but increases sensitivity to silence insertion; Spectral focus is effective for seen generators but fails on frequency-domain augmentations; Training on limited generators yields high accuracy on in-distribution but poor OOD transfer
- **Failure signatures**: Empty audio → classified as fake; +2 semitone pitch shift on Suno → classified as real; MusicGen audio → classified as more real than expected; High-frequency corruption → increased fake probability
- **First 3 experiments**:
  1. Baseline replication: Run SONICS on the provided dataset (20 songs per generator) without augmentation to confirm reported probabilities
  2. Pitch shift sweep: Apply pitch shifts from -4 to +4 semitones in 0.5-semitone increments to identify the decision boundary flip point for each generator
  3. Silence injection calibration: Insert silence at beginning/middle/end of clips in 5-second increments to map how silence position and duration affect fake probability scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: At what point do audio augmentations become "too destructive" for fake music detection models to be expected to handle?
- Basis in paper: [explicit] The authors state, "The question whether the model should be able to handle such samples is hard to answer, because it’s not obvious where to place the line of augmentations being too destructive."
- Why unresolved: While the paper identifies augmentations that skew results, there is no established threshold or standard defining the limit of augmentation intensity a detector should reasonably withstand.
- What evidence would resolve it: A benchmark defining augmentation limits correlated with human perceptual thresholds, or a standardized evaluation protocol that distinguishes between "adversarial" and "destructive" transformations.

### Open Question 2
- Question: Can a specialized loss term effectively train detection models to classify silence as ambiguous rather than biasing them toward "fake" predictions?
- Basis in paper: [explicit] The authors observe that empty files are classified as fake and suggest, "A solution might be to introduce a loss term during model training which ensures that silence is classified as ambiguous."
- Why unresolved: This is a proposed mitigation strategy for the observed silence bias, but it was not implemented or tested in the study.
- What evidence would resolve it: A comparative training run showing that the proposed loss term successfully neutralizes the probability scores for silent or near-silent audio inputs without reducing detection accuracy for actual fake music.

### Open Question 3
- Question: To what extent is the SONICS model's decision-making dependent on specific high-frequency spectral artifacts versus holistic musical content?
- Basis in paper: [explicit] The authors note, "A further path to explore is the explainability of the predictions and understanding of the dependence on certain frequency bands by the model."
- Why unresolved: The study observed a correlation between high-frequency corruption and "fake" classification, implying artifact reliance, but did not perform the internal analysis required to confirm exactly what the model learned.
- What evidence would resolve it: Explainability analysis (e.g., saliency maps or frequency masking tests) identifying the specific spectral regions that maximally activate the "fake" classification neurons.

## Limitations
- The study lacks explicit hyperparameter documentation for augmentations, making exact reproduction difficult
- The model's sensitivity to spectral artifacts raises questions about whether these failures represent fundamental limitations or specific architectural choices
- Generalization results on MusicGen may reflect the smaller sample size (20 songs) rather than true distribution shift
- The model was only evaluated on inference without retraining with augmented data

## Confidence
- High confidence in the mechanism of spectral artifact dependence: Multiple experiments show consistent sensitivity to frequency-domain manipulations
- Medium confidence in distribution shift generalization failure: Results are clear but sample size and lack of cross-validation introduce uncertainty
- Low confidence in the empty file artifact explanation: While observed, the model's internal reasoning for this behavior is speculative

## Next Checks
1. Test pitch shift sensitivity on real music samples to determine if the effect is asymmetric or generator-specific
2. Retrain the model with augmented versions of the training data to measure improvement in robustness
3. Compare SONICS performance against a temporal-pattern-based detector on the same augmented dataset to isolate whether spectral or temporal features are more vulnerable