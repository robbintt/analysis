---
ver: rpa2
title: 'Forgetting: A New Mechanism Towards Better Large Language Model Fine-tuning'
arxiv_id: '2508.04329'
source_url: https://arxiv.org/abs/2508.04329
tags:
- arxiv
- forgetting
- data
- tokens
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of supervised fine-tuning (SFT)
  for large language models (LLMs), where performance heavily depends on data quality
  and volume. The authors propose a novel forgetting mechanism that categorizes tokens
  into positive and negative sets based on their influence on model performance.
---

# Forgetting: A New Mechanism Towards Better Large Language Model Fine-tuning

## Quick Facts
- **arXiv ID:** 2508.04329
- **Source URL:** https://arxiv.org/abs/2508.04329
- **Reference count:** 40
- **Primary result:** Forgetting mechanism achieves 2.51-8.25% average performance improvements across model scales (1B-13B) by explicitly forgetting negative tokens during SFT

## Executive Summary
This paper addresses the challenge of supervised fine-tuning (SFT) for large language models where performance heavily depends on data quality and volume. The authors propose a novel forgetting mechanism that categorizes tokens into positive (informative) and negative (misleading/noisy) sets based on their influence on model performance. Positive tokens are trained normally while negative tokens are explicitly forgotten to prevent overfitting to noise. Experiments across multiple benchmarks and model architectures demonstrate consistent improvements over standard SFT and ignoring baselines.

## Method Summary
The method partitions training tokens into positive and negative sets based on influence scores computed using a reference model. Tokens are ranked by quality scores derived from influence (difference in loss between reference and base models), with the top ρ% selected as positive. Training employs a dual objective: maximizing likelihood of positive tokens while minimizing likelihood of negative tokens through gradient ascent with an adaptive coefficient λ(step). The approach uses LoRA fine-tuning with specified hyperparameters and shows particular effectiveness when setting ρ between 70-80%.

## Key Results
- Forgetting mechanism consistently outperforms standard SFT and ignoring baselines across all tested model scales
- Average performance improvements range from 2.51% to 8.25% across different model scales
- Optimal performance achieved with positive token proportion ρ between 70% and 80%
- Method maintains robustness across various hyperparameter settings and benchmarks

## Why This Works (Mechanism)
The mechanism works by preventing the model from overfitting to noisy or misleading tokens in the training data. By explicitly forgetting negative tokens through gradient ascent (rather than ignoring them), the model is pushed away from spurious patterns that could degrade generalization. The adaptive coefficient λ(step) ensures the forgetting gradient remains effective throughout training, preventing it from vanishing as the model converges on positive tokens.

## Foundational Learning
- **Influence computation:** Measuring token importance by comparing losses between reference and base models (why needed: identifies which tokens are most informative; quick check: verify influence scores correlate with known quality tokens)
- **Token partitioning:** Dividing tokens into positive/negative sets based on quality scores (why needed: enables selective forgetting; quick check: ensure partitioning doesn't create data leakage between sets)
- **Adaptive forgetting coefficient:** Linearly scaling λ from t_min to t_max during training (why needed: prevents forgetting gradients from vanishing; quick check: monitor λ progression across training steps)

## Architecture Onboarding
- **Component map:** Base model → Reference model (trained on D_ref) → Token influence scores → Partitioned tokens → Dual training with forgetting
- **Critical path:** Token quality computation → Partition selection → Dual forward pass → Adaptive loss combination → Backward step
- **Design tradeoffs:** Fixed ρ vs. dynamic thresholding (paper found fixed proportion more reliable); adaptive vs. static λ (adaptive prevents vanishing gradients)
- **Failure signatures:** Performance collapse when λ too high or ρ too low; ignoring vs. forgetting distinction crucial for effectiveness
- **First experiments:** 1) Verify reference model training on D_ref produces meaningful influence scores; 2) Test token partitioning with different ρ values; 3) Validate adaptive λ schedule prevents gradient vanishing

## Open Questions the Paper Calls Out
- **Robust token identification:** How to develop more reliable methods for identifying high-quality tokens beyond fixed proportion thresholds, since zero-threshold partitioning degraded performance
- **Scaling behavior:** Whether the mechanism maintains effectiveness on models larger than 13B parameters and massive-scale training datasets, given current experiments were limited by computational budget
- **Lifecycle extension:** Adapting the forgetting mechanism to other LLM stages including pre-training, preference optimization, and inference, as current formulation is specific to SFT

## Limitations
- Computational budget restricted experiments to models up to 13B parameters with limited-scale training data
- Optimal reference dataset size remains unclear as a critical hyperparameter
- Method's effectiveness on tasks beyond tested benchmarks and with longer training epochs remains unverified

## Confidence
- **High confidence:** Consistent improvements across all tested model scales (1B-13B) with average gains of 2.51-8.25% on benchmark tasks
- **Medium confidence:** Adaptive coefficient λ(step) effectively balances positive and negative token training
- **Low confidence:** Performance on tasks beyond tested benchmarks and behavior with training beyond 1 epoch

## Next Checks
1. Systematically vary reference dataset size (5k, 10k, 20k samples) to identify optimal reference training requirements
2. Verify whether positive and negative tokens within same sequence can be effectively trained together
3. Test forgetting mechanism with multiple training epochs (1, 2, 3) to determine long-term effectiveness