---
ver: rpa2
title: Vector Graph-Based Repository Understanding for Issue-Driven File Retrieval
arxiv_id: '2510.08876'
source_url: https://arxiv.org/abs/2510.08876
tags:
- graph
- files
- repository
- code
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a vectorized knowledge graph framework for
  large software repositories, combining static code analysis, LLM-derived summaries,
  and embeddings to model architectural and semantic relationships. A hybrid retrieval
  pipeline integrates semantic search with graph-aware expansion and an LLM-based
  assistant to support issue-driven file retrieval.
---

# Vector Graph-Based Repository Understanding for Issue-Driven File Retrieval

## Quick Facts
- **arXiv ID:** 2510.08876
- **Source URL:** https://arxiv.org/abs/2510.08876
- **Reference count:** 0
- **Key outcome:** Vector graph framework + hybrid retrieval pipeline achieves recall@50=1.00 on some repos, sub-2s search latency, but lacks runtime/dynamic dependency coverage and robust precision metrics.

## Executive Summary
This work introduces a vectorized knowledge graph framework for large software repositories, combining static code analysis, LLM-derived summaries, and embeddings to model architectural and semantic relationships. A hybrid retrieval pipeline integrates semantic search with graph-aware expansion and an LLM-based assistant to support issue-driven file retrieval. Evaluated across diverse repositories, the system achieves median recall@50 of 1.00 in some cases, with search performance under 2 seconds for large graphs. Limitations include graph staleness and reliance on static analysis, while future work targets dynamic runtime data integration and supervised reranking for improved precision.

## Method Summary
The framework builds a vectorized knowledge graph by combining static code analysis, LLM-derived summaries, and embeddings to capture both structural and semantic relationships in software repositories. The graph integrates ASTs, control-flow, and dependency structures, with LLM embeddings summarizing file content and relationships. A hybrid retrieval pipeline combines semantic search (via vector similarity), graph-aware expansion (traversing neighboring nodes), and an LLM-based assistant for issue-to-file mapping. Retrieval uses cosine similarity on embeddings and BFS-like expansion on the graph to surface relevant files. The system is evaluated on multiple open-source repositories, reporting recall@50 and search latency, with ablation studies on embedding quality and expansion depth.

## Key Results
- Median recall@50 of 1.00 on some repositories, indicating high recall for issue-driven retrieval.
- Search latency under 2 seconds for large graphs, demonstrating scalability.
- Ablation studies show hybrid retrieval outperforms pure semantic or graph-based methods, but precision metrics are limited.

## Why This Works (Mechanism)
The system works by integrating static code analysis with LLM-derived semantic embeddings to create a vectorized knowledge graph. Static analysis captures syntactic and structural dependencies (e.g., ASTs, control-flow, file imports), while LLMs generate natural language summaries and embeddings for semantic understanding. This dual representation allows the retrieval pipeline to leverage both precise structural relationships and flexible semantic matching. The hybrid retrieval combines vector similarity (for semantic closeness) with graph expansion (for contextual relevance), ensuring that issue descriptions—often high-level and informal—can be mapped to relevant implementation files. The LLM assistant further bridges the gap between natural language issues and code artifacts by interpreting intent and suggesting candidate files.

## Foundational Learning
- **Static Code Analysis (ASTs, CFGs):** Extracts syntactic and control-flow structure; needed to model precise code relationships; quick check: verify AST extraction accuracy on sample files.
- **LLM Embeddings for Code:** Summarizes file semantics; needed for natural language issue matching; quick check: test embedding similarity on known related files.
- **Vector Similarity (Cosine):** Measures semantic closeness; needed for efficient ranking; quick check: validate on labeled code pairs.
- **Graph Expansion (BFS/Neighborhood):** Traverses related nodes; needed for contextual retrieval; quick check: confirm expansion depth retrieval.
- **Hybrid Retrieval:** Combines semantic and structural search; needed for robustness; quick check: compare hybrid vs. single-method recall.
- **Knowledge Graph Construction:** Integrates multiple data sources; needed for unified representation; quick check: ensure node/edge consistency.

## Architecture Onboarding
- **Component Map:** Code Analysis -> Knowledge Graph Construction -> Embedding Generation -> Hybrid Retrieval -> LLM Assistant
- **Critical Path:** Issue input -> LLM Assistant (intent parsing) -> Hybrid Retrieval (semantic + graph expansion) -> Result ranking -> Output
- **Design Tradeoffs:** Static analysis trades runtime coverage for precision; LLM embeddings trade computational cost for semantic flexibility; graph expansion trades recall for latency.
- **Failure Signatures:** Stale graph → missed recent changes; poor embeddings → irrelevant matches; shallow expansion → incomplete context; LLM misinterpretation → wrong intent mapping.
- **3 First Experiments:**
  1. Run hybrid retrieval on a small repo with known issue-file mappings; measure recall@5 and latency.
  2. Ablate LLM embeddings: use only static analysis + graph expansion; compare recall and runtime.
  3. Vary graph expansion depth (1-3 hops); plot recall vs. latency to find optimal balance.

## Open Questions the Paper Calls Out
None provided.

## Limitations
- Static analysis cannot capture runtime behavior or dynamic dependencies, leading to incomplete or outdated representations.
- Reliance on LLM-derived summaries introduces variability in semantic quality; no error bounds or robustness metrics are provided.
- Evaluation focuses on recall@50, but precision metrics are sparse and no user study is conducted to assess real-world effectiveness.

## Confidence
- **High:** Core retrieval pipeline (graph construction + hybrid search) — quantitative results and ablation support effectiveness.
- **Medium:** LLM integration and semantic quality — limited ablation and no error analysis.
- **Low:** Long-term maintenance and scalability — missing data on memory, indexing, and runtime drift.

## Next Checks
1. Conduct a precision-focused evaluation with ground-truth relevance labels and ablation of the LLM summary component.
2. Benchmark memory usage and query latency under sustained, concurrent workloads on large-scale repositories.
3. Perform a longitudinal study to measure graph staleness and retrieval accuracy decay over time, with a comparison of static versus dynamic graph updates.