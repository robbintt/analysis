---
ver: rpa2
title: 'CarMem: Enhancing Long-Term Memory in LLM Voice Assistants through Category-Bounding'
arxiv_id: '2501.09645'
source_url: https://arxiv.org/abs/2501.09645
tags:
- preference
- user
- preferences
- extraction
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a category-bound preference memory system
  for in-car voice assistants, addressing privacy concerns and the limited action
  space of automotive systems. The system uses hierarchical categories to restrict
  information extraction, leveraging Large Language Models for structured preference
  extraction, maintenance, and retrieval.
---

# CarMem: Enhancing Long-Term Memory in LLM Voice Assistants through Category-Bounding

## Quick Facts
- **arXiv ID:** 2501.09645
- **Source URL:** https://arxiv.org/abs/2501.09645
- **Reference count:** 40
- **Primary result:** Category-bound memory system for in-car voice assistants achieving F1-scores of .78-.95, 95% redundancy reduction, and 92% contradiction reduction

## Executive Summary
This work introduces a category-bound preference memory system for in-car voice assistants that addresses privacy concerns and limited action space in automotive systems. The system uses hierarchical categories to restrict information extraction, leveraging Large Language Models for structured preference extraction, maintenance, and retrieval. A synthetic dataset (CarMem) grounded in real industry data was created to evaluate the system's core components, demonstrating strong performance in industrial applications while enhancing transparency and user control.

## Method Summary
The system employs LLM function calling with a predefined hierarchical schema (4 main, 11 sub, 41 detail categories) to extract user preferences from voice conversations. Preferences are then maintained through semantic comparison against existing database entries using maintenance functions (Pass, Update, Append). Retrieval is performed using enriched embeddings that concatenate category metadata with user utterances before vectorization. The approach is evaluated on a synthetic CarMem dataset consisting of 1,000 extraction conversations, 1,000 retrieval utterances, and 3,000 maintenance utterances.

## Key Results
- F1-scores of .78 to .95 in preference extraction at different hierarchical levels
- 95% reduction in redundant preferences through maintenance strategy
- 92% reduction in contradictory preferences
- .87 accuracy in optimal retrieval with enriched embeddings

## Why This Works (Mechanism)

### Mechanism 1: Schema-Constrained Extraction
The system uses LLM Function Calling with a Pydantic-defined parameter schema to enforce hierarchical category boundaries. This replaces open-ended text generation with structured JSON output, ensuring "out-of-category" preferences are ignored because no parameter exists for them.

### Mechanism 2: Stateful Maintenance via Comparison
Before database insertion, the system compares incoming preferences against existing ones within the same detail category. The LLM selects maintenance functions (Pass for duplicates, Update for contradictions, Append for new items) to maintain consistency.

### Mechanism 3: Enriched Embedding Retrieval
Retrieval accuracy improves by concatenating Category and Attribute metadata with user sentences before embedding. This grounds semantic vectors in specific domain contexts, reducing ambiguity during similarity search.

## Foundational Learning

- **Concept: LLM Function Calling / Tool Use**
  - Why needed: Enforces rigid JSON schema, allowing programmatic rejection of data based on type validity
  - Quick check: Can you explain the difference between asking an LLM "Extract preferences as JSON" versus providing it with a strict `get_preferences` function signature?

- **Concept: Semantic Densification (Metadata Enrichment)**
  - Why needed: Raw text embeddings fail to capture domain-specific relationships in short queries
  - Quick check: Why would embedding "NavFlow" alone perform worse than embedding "Traffic Source: NavFlow"?

- **Concept: Data Minimization (Privacy by Design)**
  - Why needed: The architecture is defined by what it prevents itself from doing (storing non-actionable data)
  - Quick check: How does the "Out-of-Schema" experiment simulate a user privacy opt-out?

## Architecture Onboarding

- **Component map:** Conversation -> Schema Validation -> Maintenance Logic -> Enrichment -> Vector Storage
- **Critical path:** Conversation -> Is this category allowed? -> Is this a duplicate/contradiction? -> Category + Attribute + Sentence -> Vector Store
- **Design tradeoffs:**
  - Granularity vs. Accuracy: F1 drops from .94 (Main) to .78 (Detail), suggesting shallow hierarchies perform better
  - Precision vs. Recall: System opts for high precision by using Optional parameters; if unsure, extracts nothing (6% non-extraction)
- **Failure signatures:**
  - Semantic Bleed: Confusion between "Music" and "Radio & Podcasts" categories
  - Over-extraction: Monitor "no_or_other_preference" bucket size to detect schema drift
- **First 3 experiments:**
  1. Category Isolation Test: Generate conversations with only out-of-schema preferences, verify empty extraction JSON
  2. The "Flip-Flop" Test: Feed "I love station X" -> "I hate station X", assert single entry (updated) or none
  3. Retrieval Noise Test: Compare "Sentence Only" vs. "Enriched" retrieval on ambiguous queries like "Play it"

## Open Questions the Paper Calls Out

- **Open Question 1:** Can in-context learning with few-shot examples effectively mitigate the 12-15% incorrect over-extraction rate observed in zero-shot setting?
  - Basis: Authors propose leveraging in-context learning capabilities to address over-extraction in Limitations section
  - Evidence needed: Comparative experiments measuring extraction precision between zero-shot and few-shot prompting

- **Open Question 2:** Does incorporating temporal decay or importance ratings improve handling of changing user preferences over long periods?
  - Basis: Limitations section suggests incorporating temporal decay or importance ratings for maintenance methods
  - Evidence needed: Longitudinal evaluation of maintenance accuracy with decay functions or importance weights

- **Open Question 3:** How effectively does the category-bound memory system generalize to other industry domains like smart homes?
  - Basis: Conclusion states future work could explore generalizing to other domains
  - Evidence needed: Benchmarking results from deploying in simulated smart home environment

## Limitations
- Synthetic dataset lacks real-world conversational noise and edge cases that could stress-test extraction boundaries
- 6% non-extraction rate suggests some prompts still produce ambiguous outputs despite schema constraints
- Privacy benefits demonstrated through controlled tests but real-world user behavior may include sophisticated bypass attempts

## Confidence
- **High confidence:** Maintenance mechanism effectiveness (95% redundancy reduction, 92% contradiction reduction)
- **Medium confidence:** Extraction F1 scores (.78-.95 range) - synthetic data limits generalizability
- **Medium confidence:** Retrieval accuracy (.87) - depends heavily on quality of category extraction upstream

## Next Checks
1. **Real Conversation Test:** Deploy extraction pipeline on actual in-car voice assistant logs to measure performance degradation from synthetic to real data
2. **Boundary Stress Test:** Systematically generate utterances at category boundaries (e.g., "I like news podcasts" between Music/Radio) to quantify confusion rates
3. **Privacy Bypass Attempt:** Conduct red-teaming where users deliberately attempt to store out-of-category preferences using indirect language to test robustness of "no_or_other_preference" filter