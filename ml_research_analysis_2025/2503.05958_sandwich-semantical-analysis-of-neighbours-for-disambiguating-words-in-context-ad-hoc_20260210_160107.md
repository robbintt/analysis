---
ver: rpa2
title: 'SANDWiCH: Semantical Analysis of Neighbours for Disambiguating Words in Context
  ad Hoc'
arxiv_id: '2503.05958'
source_url: https://arxiv.org/abs/2503.05958
tags:
- word
- sense
- association
- linguistics
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SANDWiCH addresses the limitations of current word sense disambiguation
  (WSD) models by reframing the task as cluster discrimination over a sense-separated
  semantic network. The framework combines coarse sense retrieval with a cross-encoder
  model that evaluates semantic clusters rather than individual senses, and separates
  models by part-of-speech.
---

# SANDWiCH: Semantical Analysis of Neighbours for Disambiguating Words in Context ad Hoc

## Quick Facts
- **arXiv ID:** 2503.05958
- **Source URL:** https://arxiv.org/abs/2503.05958
- **Reference count:** 40
- **Primary result:** Achieves state-of-the-art F1 scores on WSD benchmarks, improving by up to 8% on English tasks and 20-30% on rare senses while reducing parameter count by 72%

## Executive Summary
SANDWiCH addresses fundamental limitations in word sense disambiguation by reframing the task as cluster discrimination over a sense-separated semantic network. The framework combines coarse sense retrieval with cross-encoder models that evaluate semantic clusters rather than individual senses, achieving state-of-the-art performance across multiple benchmarks. By separating models by part-of-speech and leveraging equivalence classes from semantic neighborhoods, SANDWiCH demonstrates significant improvements particularly on rare senses and underrepresented languages.

## Method Summary
SANDWiCH reframes WSD as cluster discrimination over a sense-separated semantic network derived from BabelNet. The method preprocesses the semantic graph to ensure sense-separability by removing edges between senses of the same word, creating disjoint equivalence classes. A coarse retrieval stage uses DeBERTa-v3-xsmall to identify top-30 candidate senses, followed by POS-specialized cross-encoders (DeBERTa-v3-small) that score semantic clusters. The final prediction selects the highest-scoring equivalence class, with POS separation capturing distinct contextual dependencies for different word categories.

## Key Results
- Achieves 89.0 F1 score on ALL benchmark, improving over ConSeC by 8 percentage points
- Reduces parameter count by 72% compared to larger models while maintaining performance
- Improves rare sense disambiguation by 20-30 percentage points on 42D and hardEN datasets
- Generalizes effectively to multilingual settings, demonstrating strong performance on XL-WSD languages

## Why This Works (Mechanism)

### Mechanism 1: Cluster-based sense discrimination over individual senses
SANDWiCH reframes WSD from selecting individual senses to selecting semantic equivalence classes, improving performance particularly on rare senses and out-of-domain cases. The scoring function aggregates encoder scores across all members of each class using confidence-weighted averaging, reducing reliance on individual sense frequency by leveraging shared contextual patterns within semantic clusters. This works because senses that are semantic neighbors in a knowledge graph share contextual cues that make them jointly discriminable from other clusters.

### Mechanism 2: Sense-separability condition eliminates label noise
The semantic graph is pruned so that for any word, neighborhoods of different senses are disjoint, preventing contradictory labels during training. This ensures each definition in training belongs to exactly one equivalence class, avoiding cases where the same definition-sentence pair would be labeled both positive and negative. The mechanism works because semantic relationships across words provide more disambiguation signal than relationships between senses of the same word.

### Mechanism 3: POS-specialized cross-encoders capture distinct disambiguation strategies
Training separate encoder models for different part-of-speech groups improves performance by capturing POS-specific contextual dependencies. The framework uses two encoder groups: one for nouns and verbs, another for nouns, adjectives, and adverbs, reflecting that verbs rely on objects/subjects/tense while nouns/adjectives rely on interrelations. This works because different parts of speech use fundamentally different contextual features for sense resolution.

## Foundational Learning

- **Cross-encoder architecture**: A model that jointly encodes context and candidate together, allowing full attention between them during encoding. More accurate than bi-encoders but computationally expensive at inference. Why needed here: SANDWiCH uses DeBERTa-v3 cross-encoders for both coarse retrieval and fine-grained cluster scoring. Quick check question: Why can't you pre-compute embeddings for all candidate senses with a cross-encoder?

- **Semantic networks / Knowledge graphs**: Structured representations where nodes represent concepts/senses and edges encode semantic relationships (hypernyms, synonyms, holonyms, etc.). Why needed here: BabelNet provides the graph structure that SANDWiCH transforms into sense-separable equivalence classes. Quick check question: What specific edge type does SANDWiCH remove to enforce sense-separability?

- **Equivalence relations and quotient sets**: A binary relation that is reflexive, symmetric, and transitive, partitioning a set into disjoint equivalence classes. Why needed here: Sense-separability ensures the relation is valid, enabling the mapping to operate on classes rather than individual senses. Quick check question: Why does having overlapping neighborhoods break transitivity?

## Architecture Onboarding

- **Component map**: Input sentence + target word -> Coarse Retrieval (DeBERTa-v3-xsmall) -> Semantic Graph Preprocessing (BabelNet) -> POS-Separated Cross-Encoders (DeBERTa-v3-small) -> Context-Cluster Score Computation -> Output: Highest-scoring equivalence class -> sense

- **Critical path**:
  1. Load BabelNet, remove all edges connecting senses of the same word (enforce separability)
  2. Pre-compute equivalence classes: for each sense, collect its immediate neighbors
  3. At inference: retrieve K=30 candidate senses via coarse encoder
  4. For each equivalence class containing a candidate: compute Ev + Ev scores for all members
  5. Apply δ_j weighting (higher confidence → higher weight), sum within each class
  6. Return sense from highest-scoring class

- **Design tradeoffs**:
  - DeBERTa-v3-small vs large: 44M params achieves 89.0 F1; 304M params only reaches 89.1—diminishing returns
  - K=30 retrieval threshold: 98-99% recall; lower K speeds inference but risks missing correct sense
  - Two-way POS split vs more granular: Current split balances specialization vs data sparsity; three-way or unified both underperform

- **Failure signatures**:
  - Rare senses with sparse training coverage: Even cluster-level approach depends on some training signal
  - Languages without BabelNet coverage: Framework requires pre-existing semantic network
  - Words with many senses (>15): Error rate increases with gloss count, though SANDWiCH reduces this vs ConSeC
  - Unified encoder without separability: Catastrophic drop to 55.1 F1

- **First 3 experiments**:
  1. Ablation replication (Table 3): Start with classes-only baseline (~53.5 F1 expected), add separability (~79.5), add POS encoders (~89.0). This validates each component before custom modifications.
  2. Rare-sense evaluation on Maru et al. (2022): Test on 42D and hardEN splits to verify cluster approach reduces frequency bias. Expect 20-30 point gains over prior art on 42D.
  3. Backbone swap to mBART-50 for multilingual: Replace DeBERTa with mBART-50 and evaluate on XL-WSD languages. Critical for non-English deployment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What alternative methods for creating sense clusters could improve upon the current neighborhood-based approach?
- Basis in paper: The conclusion explicitly lists exploring "alternative methods for creating sense clusters" as a direction for future work.
- Why unresolved: The paper currently relies on immediate neighborhoods within BabelNet to form equivalence classes, but other structural or semantic groupings remain unexplored.
- What evidence would resolve it: Comparative experiments using different graph clustering algorithms or varying neighborhood depths.

### Open Question 2
- Question: Can SANDWiCH be effectively combined with existing solutions to improve translation into low-resource languages?
- Basis in paper: The authors state a need to investigate if the framework can be "combined with existing solutions for translation into low-resource languages."
- Why unresolved: The current study focuses strictly on Word Sense Disambiguation benchmarks rather than downstream applications like machine translation.
- What evidence would resolve it: Integrating SANDWiCH into a machine translation pipeline and measuring translation quality (e.g., BLEU scores) for low-resource language pairs.

### Open Question 3
- Question: How robust is the framework when the underlying semantic network is sparse or incomplete?
- Basis in paper: The limitations section notes that obtaining a complete semantic network is difficult for some languages, potentially limiting usability.
- Why unresolved: The mathematical formulation requires a "sense-separable" graph, an assumption that may fail if the semantic network lacks sufficient edges to form distinct clusters.
- What evidence would resolve it: Evaluating performance on languages with deliberately pruned or sparse semantic resources to analyze performance degradation.

## Limitations
- Dependency on BabelNet coverage limits applicability to languages without comprehensive semantic network resources
- Computational expense of cross-encoder architecture remains unquantified for production deployment
- Rare sense disambiguation still shows ~23% error rate even with cluster-level approach on 42D dataset

## Confidence
**High confidence**: State-of-the-art performance on English WSD benchmarks and significant improvements on rare senses, validated through ablation studies and comparisons to ConSeC.

**Medium confidence**: Parameter efficiency claims, though systematic comparisons across model families and trade-offs with inference speed are not addressed.

**Low confidence**: Claims about POS-specific contextual dependencies, as linguistic assumptions about verb vs noun contextual features are asserted but not empirically validated.

## Next Checks
1. **Inference Efficiency Benchmark**: Measure end-to-end inference latency and memory usage for SANDWiCH compared to standard bi-encoder WSD approaches on identical hardware.

2. **Cross-Lingual Transfer Evaluation**: Test SANDWiCH's zero-shot performance on languages with BabelNet coverage but no task-specific training data.

3. **Semantic Network Sensitivity Analysis**: Systematically vary the radius of neighborhood definition (1-hop vs 2-hop vs 3-hop) in the semantic graph preprocessing step and measure impact on WSD performance.