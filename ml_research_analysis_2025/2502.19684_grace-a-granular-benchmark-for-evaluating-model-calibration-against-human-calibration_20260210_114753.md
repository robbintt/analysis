---
ver: rpa2
title: 'GRACE: A Granular Benchmark for Evaluating Model Calibration against Human
  Calibration'
arxiv_id: '2502.19684'
source_url: https://arxiv.org/abs/2502.19684
tags:
- human
- calibration
- question
- questions
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GRACE is a granular benchmark for evaluating language model calibration
  by comparing model and human calibration in incremental, adversarial question-answering.
  The dataset consists of 243 multi-clue questions designed to become progressively
  easier, allowing fine-grained measurement of how early, accurately, and confidently
  models answer.
---

# GRACE: A Granular Benchmark for Evaluating Model Calibration against Human Calibration

## Quick Facts
- arXiv ID: 2502.19684
- Source URL: https://arxiv.org/abs/2502.19684
- Reference count: 34
- Primary result: Models are more overconfident than humans in incorrect answers and underconfident in correct ones, with humans typically buzzing correctly at higher rates despite lower accuracy.

## Executive Summary
GRACE introduces a granular benchmark for evaluating language model calibration by comparing model and human calibration in incremental, adversarial question-answering. The dataset consists of 243 multi-clue questions designed to become progressively easier, allowing fine-grained measurement of how early, accurately, and confidently models answer. Human responses were collected from live competitions involving 17 teams and three models, yielding 1,749 data points. The proposed CALSCORE metric incorporates human performance to penalize models for being confidently incorrect when humans are uncertain. Results show models struggle with abstract descriptions and descriptions vs. titles, while excelling at concrete proper nouns.

## Method Summary
GRACE uses a two-stage approach: adversarial question authoring and human-model competition. Writers create questions via an interface showing model guesses and confidence per sentence, refining clues until models answer incorrectly until late in the question. Model guesses are generated using TF-IDF retrieval + LLM prompting, with confidence extracted from both logits and verbalized probabilities. Live competitions involve 17 expert teams competing against three LLMs, with human buzzpoints logged in real-time. CALSCORE weights calibration error by the proportion of humans who have not yet answered correctly, penalizing high-stakes errors where models are confidently wrong while humans remain uncertain.

## Key Results
- Models are more overconfident than humans in incorrect answers and underconfident in correct ones
- Humans typically buzz correctly at higher rates despite lower accuracy
- Models struggle with abstract descriptions and descriptions vs. titles, while excelling at concrete proper nouns
- CALSCORE consistently reveals higher calibration errors than traditional metrics, demonstrating that models are especially miscalibrated relative to humans

## Why This Works (Mechanism)

### Mechanism 1: Incremental Adversarial Question Design
Questions with progressively easier clues enable fine-grained calibration measurement at multiple decision points rather than single-pass evaluation. Each clue represents a decision node where a model must choose to answer or wait. Writers use a human-AI collaborative interface showing model guesses and confidence per sentence, refining clues until models answer incorrectly until late in the question.

### Mechanism 2: CALSCORE Human-Grounded Penalty Weighting
Weighting calibration error by the proportion of humans who have not yet answered correctly penalizes high-stakes errors where models are confidently wrong while humans remain uncertain. The metric computes CALSCORE_q = 1 - r(E_t[(1-h_t)g_t c_t]) where h_t is cumulative human correct buzz rate, g_t is model correctness (+1/-1), and c_t is confidence.

### Mechanism 3: Dual Confidence Elicitation (Logit vs. Verbalized)
Using both logit-derived and verbalized confidence provides complementary calibration signals; they can disagree, revealing different miscalibration patterns. Logit confidence averages exponentials of token log probabilities during generation, while verbalized confidence explicitly prompts the model to output a probability (0.0-1.0).

## Foundational Learning

- Concept: Expected Calibration Error (ECE)
  - Why needed here: CALSCORE is compared against ECE as a baseline; understanding ECE's binning approach clarifies why per-instance granularity matters.
  - Quick check question: Why does ECE aggregate over bins, and what information might this lose for incremental QA?

- Concept: Confidence elicitation paradigms (logit vs. verbalized)
  - Why needed here: GRACE uses both; knowing when each is appropriate helps interpret Table 1 results showing different rankings.
  - Quick check question: What are the tradeoffs between logit-based confidence (model-internal) and verbalized confidence (model-generated)?

- Concept: Quizbowl/trivia competition mechanics
  - Why needed here: The benchmark models a real competition format (buzzing, scoring: +10 correct early, -5 incorrect). Understanding scoring incentives explains why timing matters for calibration.
  - Quick check question: Why would a team buzz early with moderate confidence rather than wait for certainty?

## Architecture Onboarding

- Component map: Question Authoring Interface -> Model Guesser Module -> Confidence Extractor -> Buzzer Threshold System -> Competition Runner -> CALSCORE Calculator
- Critical path: Write/refine questions via interface until models struggle early -> Extract model guesses and confidence for all clue prefixes (offline) -> Set buzzer thresholds using held-out human data -> Run live competitions, logging human buzzpoints -> Compute CALSCORE using both teams' data
- Design tradeoffs: Precomputed model buzzpoints vs. live model inference; Expert human players vs. general population; 243 questions vs. scalability
- Failure signatures: Model accuracy > 80% early in questions; Human accuracy near 0% until final clue; CALSCORE much lower than ECE; Verbalized confidence extraction failures
- First 3 experiments: Replicate CALSCORE computation on the released dataset using both confidence types; Ablate the human adjustment by setting h_t = 0 for all t; Evaluate a new model on GRACE and compare CALSCORE trajectory across clues

## Open Questions the Paper Calls Out

- Question: Does GRACE's calibration evaluation generalize to other NLP domains beyond incremental question-answering, such as open-ended generation tasks?
- Question: How can model abstention be personalized based on individual human skill levels to improve human-AI collaboration?
- Question: How can CALSCORE be extended to capture forms of miscalibration relevant to human-AI collaboration, such as when users should defer to model vs. human judgment?
- Question: What mechanisms cause models to be more overconfident on incorrect answers and underconfident on correct ones compared to humans, and can this asymmetry be corrected?

## Limitations
- The adversarial question design may not generalize to all QA domains beyond incremental, clue-based answering
- Human calibration baseline comes from expert trivia competitors, which may not represent typical user populations
- CALSCORE relies on human performance data that may be sparse at early clues where few humans buzz correctly
- Verbalized confidence extraction shows significant corner cases that required iterative filtering

## Confidence

- High confidence: Models are more overconfident than humans in incorrect answers, and the CALSCORE metric consistently reveals higher calibration errors than traditional metrics
- Medium confidence: The adversarial question design effectively creates challenging calibration scenarios; however, generalizability to other domains requires further validation
- Medium confidence: The specific numerical results are reliable for the tested models and dataset, but methodology's sensitivity to human skill variance suggests caution when interpreting cross-model comparisons

## Next Checks

1. Replicate the CALSCORE computation on the released dataset using both confidence types (logit and verbalized) to verify the reported correlations and ranking stability across models
2. Evaluate a new model family (e.g., Claude or LLaMA) on GRACE to determine whether the observed overconfidence patterns persist across different LLM architectures
3. Conduct a sensitivity analysis by varying the human adjustment parameter h_t to test whether the CALSCORE-human penalty weighting remains robust when using non-expert human data or synthetic human baselines