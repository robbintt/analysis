---
ver: rpa2
title: 'DiTraj: training-free trajectory control for video diffusion transformer'
arxiv_id: '2509.21839'
source_url: https://arxiv.org/abs/2509.21839
tags:
- trajectory
- video
- control
- generation
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DiTraj, a training-free framework for trajectory
  control in text-to-video generation tailored for Diffusion Transformers (DiT). The
  method uses Large Language Models to separate foreground and background prompts,
  then applies Spatial-Temporal Decoupled 3D-RoPE (STD-RoPE) to modify position embeddings,
  enhancing attention between foreground tokens across frames.
---

# DiTraj: training-free trajectory control for video diffusion transformer

## Quick Facts
- arXiv ID: 2509.21839
- Source URL: https://arxiv.org/abs/2509.21839
- Authors: Cheng Lei; Jiayu Zhang; Yue Ma; Xinyu Wang; Long Chen; Liang Tang; Yiqiang Yan; Fei Su; Zhicheng Zhao
- Reference count: 24
- Primary result: Achieves 50.5% AP50 trajectory control score, outperforming training-free methods while maintaining video quality

## Executive Summary
DiTraj introduces a training-free framework for precise trajectory control in text-to-video generation using Diffusion Transformers (DiTs). The method leverages Large Language Models to separate foreground and background prompts, then applies Spatial-Temporal Decoupled 3D-RoPE (STD-RoPE) to modify position embeddings and enhance attention between foreground tokens across frames. This approach enables 3D-aware trajectory control by regulating position embedding density, achieving state-of-the-art performance among training-free methods while adding only 5.9% inference overhead.

## Method Summary
DiTraj operates by first using an LLM to decompose user prompts into foreground and background descriptions. During the cross-attention step in DiT blocks, a binary mask enforces that tokens inside the trajectory bounding box attend only to foreground prompt embeddings. The method then applies STD-RoPE to modify position embeddings of foreground tokens across frames, aligning their spatial dimensions to an anchor frame to boost inter-frame attention scores. Finally, an R-token masking mechanism prevents artifacts by blocking attention between tokens with identical position embeddings, ensuring clean trajectory adherence without retraining the base model.

## Key Results
- Achieves 50.5% AP50 trajectory control score, significantly outperforming the second-best training-free method (25.6%)
- Maintains high video quality with only 5.9% additional inference overhead
- Demonstrates 3D-aware control capability through density upsampling using the smallest bounding box as anchor
- Shows robust performance across both Wan2.1 and CogVideoX architectures

## Why This Works (Mechanism)

### Mechanism 1: Foreground-Background Separation Guidance
The system uses an LLM to decompose prompts into foreground ($P_{fg}$) and background ($P_{bg}$) descriptions. A binary mask $M_{cross}$ applied during cross-attention forces tokens inside the trajectory to attend exclusively to $P_{fg}$ embeddings and background tokens to $P_{bg}$ embeddings. This spatial separation allows trajectory bounding boxes to be injected without retraining. The mechanism assumes the LLM can accurately separate object features from environmental features and that cross-attention correlates with spatial layout.

### Mechanism 2: Trajectory Alignment via STD-RoPE
STD-RoPE exploits the "diagonal highlighting" property of 3D full attention in DiTs, where tokens with similar Rotary Position Embeddings (RoPE) yield higher attention scores. By replacing the spatial position embeddings of foreground tokens in all frames with those from a reference anchor frame, the method eliminates spatial discrepancies between frames for the moving object. This boosts inter-frame attention while preserving temporal coherence through unchanged temporal PE, forcing the model to perceive spatially continuous objects across frames.

### Mechanism 3: Artifact Suppression via R-Token Masking
STD-RoPE creates tokens with repeated position embeddings ($S_{repeat}$) in non-anchor frames. To prevent the model from copying anchor content into background regions (artifacts), a self-attention mask $M_{self}$ sets attention scores between foreground tokens and these "R-tokens" to $-\infty$. This blocks the model's default behavior of replicating content when encountering duplicate position embeddings, ensuring clean generation outside the foreground region.

## Foundational Learning

- **Rotary Position Embeddings (RoPE) in Video DiTs**: RoPE encodes position via rotation angles in complex space, and modifying these angles changes attention scores. This is critical because DiTs rely on RoPE for spatial-temporal localization, unlike U-Net architectures. *Quick check*: How does changing the rotation angle of a token's feature vector affect its dot product with a token at a different position?

- **3D Full Attention vs. Factorized Attention**: The paper targets DiTs for their 3D full attention (joint spatial-temporal), contrasting with U-Net's separated attention. This mechanism relies on tokens from Frame N being able to attend directly to tokens in Frame 1. *Quick check*: In a 3D full attention matrix, what does the diagonal represent, and how does it differ from a factorized spatial-then-temporal attention mechanism?

- **Cross-Attention Control (Prompt-to-Latent)**: Mechanism 1 relies on masking the cross-attention map to separate foreground and background generation. *Quick check*: If you mask the cross-attention map for a specific pixel region to ignore the prompt "dog", what is the expected visual outcome in that region during generation?

## Architecture Onboarding

- **Component map**: Input Processor -> LLM Separator -> DiT Backbone -> STD-RoPE Module -> Attention Masking
- **Critical path**: The intervention occurs primarily during the first $t_b=5$ denoising steps (Layout Phase). The DiT block's attention layer must be hooked to compute union condition embedding, apply cross-attention mask, and modify RoPE query/key projections before self-attention calculation.
- **Design tradeoffs**: Step count ($t_b$) balances trajectory adherence against temporal freezing artifacts. Anchor selection between random frame vs. smallest bounding box affects 3D vs. 2D control capability. LLM dependency quality impacts prompt separation effectiveness.
- **Failure signatures**: "Copy-Paste" Artifacts (multiple object copies in background) indicate R-Token mask issues. Static Object (object stays in initial position) suggests STD-RoPE not applied. Degraded Quality (blurry or ungrounded foreground) indicates overly strong separation guidance or incorrect $t_a$ setting.
- **First 3 experiments**:
  1. **Attention Map Visualization**: Plot attention maps for Frame 1 vs Frame N tokens before and after STD-RoPE to confirm increased scores in bounding box region (Fig 4).
  2. **Ablation on $t_b$**: Run inference with $t_b \in \{0, 1, 5, 10, 20\}$ on fast-moving trajectory to measure mIoU and visual artifacts.
  3. **Prompt Sensitivity**: Test LLM separator with ambiguous prompts like "A reflection of a car" to inspect if $P_{fg}$ erroneously includes background elements.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can DiTraj be extended to control multiple objects simultaneously, given that the current STD-RoPE algorithm selects a single anchor frame and bounding box?
- **Basis**: Algorithm 1 selects a single anchor for position embedding modification, and Eq. 3 defines a single trajectory. The paper exclusively demonstrates single-object control.
- **Why unresolved**: STD-RoPE aligns spatial dimensions of all frames to one specific bounding box region; extending this to multiple disjoint trajectories would require resolving conflicts in spatial alignment within the same latent space.
- **What evidence would resolve it**: Demonstration of successful generation with two or more distinct objects following independent, non-overlapping trajectories.

### Open Question 2
- **Question**: Can the Foreground-Background Separation Guidance be achieved without relying on an external LLM to ensure robustness against prompt ambiguity?
- **Basis**: The method depends on Qwen3 LLM with a specific instruction template to strictly separate prompts.
- **Why unresolved**: LLM dependency introduces a black-box dependency whose performance characteristics are not thoroughly characterized across different prompt types or languages.
- **What evidence would resolve it**: Analysis of performance sensitivity to LLM output quality, or proposal of a training-free attention-masking mechanism that bypasses explicit text separation.

### Open Question 3
- **Question**: How can artifacts caused by repeated position embeddings (R-tokens) be fully eliminated to allow for precise control over longer denoising steps ($t_b$)?
- **Basis**: Table 5 shows increasing $t_b$ to 10 or 20 significantly degrades Imaging Quality despite R-token mask.
- **Why unresolved**: The proposed R-token mask mitigates but doesn't fully solve the shift in attention score distribution caused by repeated embeddings, creating a trade-off between control precision and visual quality.
- **What evidence would resolve it**: Modification to STD-RoPE or masking strategy maintaining high Imaging Quality (>0.68 IQ) even when trajectory control is applied throughout entire denoising process.

## Limitations

- Missing bounding box trajectory coordinates for the 56 test prompts prevent exact replication of quantitative results
- Heavy dependency on LLM separator quality introduces black-box uncertainty not characterized across edge cases or non-English prompts
- 3D-aware capability claim relies on density upsampling interpretation not directly validated through depth measurements
- Demonstrates effectiveness only on specific DiT architectures without evidence for generalizability to other models or significant object deformation scenarios

## Confidence

**High Confidence**: Core STD-RoPE mechanism leveraging RoPE similarity for attention manipulation is theoretically well-founded and mathematically supported. Artifact suppression through R-token masking is straightforward and demonstrably effective based on attention visualization.

**Medium Confidence**: Trajectory alignment performance metrics are impressive but difficult to verify without exact trajectory data. 5.9% inference overhead claim appears reasonable but depends on implementation efficiency variations.

**Low Confidence**: "3D-aware" capability relies heavily on density upsampling interpretation not directly validated. LLM prompt separation quality is asserted but not empirically validated across edge cases or non-English inputs.

## Next Checks

1. **Trajectory Reconstruction and Sensitivity Analysis**: Generate synthetic trajectories with known ground truth (linear, curved, accelerating motions) and measure detection-based alignment metrics (AP50, mIoU) while varying $t_b$ and anchor selection strategies to validate core mechanism independently of missing test data.

2. **LLM Separator Robustness Testing**: Systematically test prompt separation with ambiguous cases (reflections, shadows, occluded objects), compound subjects, and non-English prompts to measure correlation between separation quality and final trajectory alignment.

3. **Cross-Detector Validation**: Repeat trajectory evaluation using multiple object detectors (OWL-ViT variants, grounded SAM, CLIP-based detectors) to verify alignment metrics are robust to detector performance variations and not artifacts of single model biases.