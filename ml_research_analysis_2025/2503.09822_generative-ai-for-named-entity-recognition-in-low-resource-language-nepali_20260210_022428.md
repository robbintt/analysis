---
ver: rpa2
title: Generative AI for Named Entity Recognition in Low-Resource Language Nepali
arxiv_id: '2503.09822'
source_url: https://arxiv.org/abs/2503.09822
tags:
- entity
- nepali
- language
- tion
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the effectiveness of generative Large Language
  Models (LLMs) for Named Entity Recognition (NER) in Nepali, a low-resource language.
  The researchers conducted experiments using state-of-the-art LLMs with various prompting
  techniques, including zero-shot and few-shot settings with random and semantic example
  selection.
---

# Generative AI for Named Entity Recognition in Low-Resource Language Nepali

## Quick Facts
- arXiv ID: 2503.09822
- Source URL: https://arxiv.org/abs/2503.09822
- Reference count: 4
- F1-micro score achieved: 0.64 with semantic example selection and English prompts

## Executive Summary
This study evaluates generative Large Language Models (LLMs) for Named Entity Recognition (NER) in Nepali, a low-resource language. The researchers conducted experiments using GPT-4o with various prompting techniques, including zero-shot and few-shot settings with random and semantic example selection. They also explored prompt language (English vs. Nepali) and implemented a self-verification mechanism to improve prediction accuracy. Results showed that semantic selection and English prompts yielded better performance than random selection and Nepali prompts, with F1-micro scores of 0.64 and 0.57, respectively. Self-verification improved precision but reduced recall. Overall, generative LLMs demonstrated promise for NER in low-resource languages, particularly for common entities like person names, though traditional methods still outperformed them in accuracy.

## Method Summary
The study uses GPT-4o for NER on the EverestNER benchmark dataset (847 training articles, 149 test articles). The method employs few-shot prompting with k=10 examples, using semantic selection via k-NN on NPVec1 BERT embeddings. Entities are marked with @@ and ## delimiters. Self-verification is optionally applied as a binary validation step. Evaluation is performed both entity-wise (non-merged) and merged with priority: LOCATION > ORGANIZATION > PERSON > DATE > EVENT. English prompts consistently outperform Nepali prompts.

## Key Results
- Semantic example selection achieved F1-micro of 0.64, outperforming random selection (F1-micro 0.55)
- English prompts yielded F1-micro of 0.64, compared to 0.57 for Nepali prompts
- Self-verification improved precision from 0.50 to 0.66 but reduced recall from 0.85 to 0.71
- EVENT and DATE entities showed the weakest performance (F1 0.11 and 0.40 respectively)

## Why This Works (Mechanism)

### Mechanism 1: Semantic Example Selection for Few-Shot Learning
- Claim: Semantically similar in-context examples improve NER performance more than randomly selected examples for low-resource languages.
- Mechanism: k-NN selection using NPVec1BERT embeddings retrieves training examples with highest cosine similarity to the test instance. These contextually relevant examples provide the LLM with domain-specific patterns, reducing the distribution gap between demonstration and target.
- Core assumption: The embedding space captures semantic relationships that correlate with entity recognition patterns.
- Evidence anchors:
  - [abstract] "semantic example selection significantly outperforms random selection"
  - [section: Results] "semantic selection outperformed both, reaching the highest F1-micro score of 0.64 with k=10, showing improvements in precision (0.51) and recall (0.86)" vs random k=10 achieving F1-micro 0.55
  - [corpus] Neighbor paper on cross-lingual data augmentation for Pakistani languages (FMR=0.57) suggests semantic transfer benefits low-resource NER, though direct comparison is limited.
- Break condition: If the embedding model fails to capture Nepali linguistic nuances, or if the training set lacks semantically diverse examples for rare entity types (e.g., EVENT with only 371 instances), retrieval quality degrades.

### Mechanism 2: Self-Verification as False Positive Filter
- Claim: A secondary LLM verification step increases precision by filtering hallucinated entities, but reduces recall.
- Mechanism: After initial entity prediction, each extracted entity is re-submitted to the LLM with a binary validation prompt (e.g., "Is 'Barcelona' an EVENT entity? Yes/No"). Invalid predictions are discarded.
- Core assumption: The LLM's self-assessment aligns with ground truth entity boundaries and types.
- Evidence anchors:
  - [abstract] "Self-verification improves precision at the cost of recall"
  - [section: Self-verification] "It improved precision from 0.50 to 0.66, while recall dropped from 0.85 to 0.71" on 100 random test instances
  - [corpus] Related work on GPT-NER (Wang et al. 2023, cited in paper) introduced this mechanism; corpus neighbors show no direct replication for Nepali.
- Break condition: If the verification prompt is ambiguous or the entity type is inherently confusing (e.g., football clubs named after locations), the model may reject valid entities. DATE entities showed recall drop from 0.71 to 0.23—suggesting entity-specific thresholds may be needed.

### Mechanism 3: Prompt Language Asymmetry in Multilingual LLMs
- Claim: English-language prompts yield better NER performance than native-language prompts for Nepali, despite the target text being Nepali.
- Mechanism: GPT-4's pre-training corpus is English-dominant; instruction-following capacity is stronger in English. Nepali prompts introduce additional translation ambiguity at the instruction level.
- Core assumption: The model's multilingual generation capability does not extend equally to instruction comprehension.
- Evidence anchors:
  - [abstract] "English prompts consistently yield better performance than Nepali prompts"
  - [section: Prompt Language] "English prompts outperformed Nepali prompts across all metrics, achieving an F1-micro score of 0.64 compared to 0.57 for Nepali prompts"
  - [corpus] No direct neighbor papers examine this prompt-language effect; this remains an underexplored area.
- Break condition: If the task requires deep cultural or linguistic context not captured in English prompts, or if end-users cannot provide English instructions, this approach is impractical.

## Foundational Learning

- Concept: **BIO Tagging Format**
  - Why needed here: The paper converts LLM-generated delimited output (e.g., `@@Nepal##`) into BIO format for CoNLL evaluation. Understanding B-/I-/O- labels is essential for parsing predictions and computing metrics.
  - Quick check question: Given the output `"@@Kathmandu## is in Nepal"`, what BIO labels would be assigned to tokens "Kathmandu", "is", "in", "Nepal" if Nepal is not marked?

- Concept: **k-Nearest Neighbors for Example Retrieval**
  - Why needed here: Semantic selection uses k-NN on embedding vectors to retrieve the most similar training examples. This requires understanding cosine similarity thresholds and embedding quality.
  - Quick check question: If your embedding model poorly represents Nepali morphology, would semantic retrieval help or hurt compared to random selection?

- Concept: **Precision-Recall Trade-off in Self-Verification**
  - Why needed here: The self-verification mechanism is a precision-enhancing filter. Engineers must decide whether their application prioritizes reducing false positives (high precision) or capturing all entities (high recall).
  - Quick check question: For a news aggregation system that must not miss any person names, would you enable self-verification for PERSON entities?

## Architecture Onboarding

- Component map:
  1. **Base Prompt Templates** (English/Nepali) → Task instructions + entity type definitions
  2. **Example Selector** → Random or Semantic (k-NN via NPVec1BERT embeddings)
  3. **LLM Generator** (GPT-4o) → Produces delimited entity output
  4. **Output Parser** → Converts `@@Entity##` markers to BIO tags
  5. **Self-Verification Module** (optional) → Binary validation prompt per predicted entity
  6. **Merger** (optional) → Combines entity-type-specific predictions with priority resolution (LOCATION > ORGANIZATION > PERSON > DATE > EVENT)

- Critical path:
  1. For each test sentence and each entity type, construct prompt with k examples (semantic selection recommended with k=10).
  2. Call GPT-4o API with prompt; parse delimited output.
  3. (Optional) Run self-verification on each predicted entity.
  4. Convert to BIO format; merge if multi-entity evaluation needed.
  5. Compute CoNLL metrics against ground truth.

- Design tradeoffs:
  - **Random vs Semantic Selection**: Semantic requires pre-computed embeddings and similarity search infrastructure; random is trivial but yields ~9 F1-micro points lower performance.
  - **Merged vs Entity-wise Evaluation**: Merged evaluation reflects real-world multi-entity scenarios but introduces label conflicts; entity-wise isolates per-type performance.
  - **Self-Verification On/Off**: On = higher precision (0.50→0.66), lower recall (0.85→0.71), doubled API cost. Off = higher recall, more false positives.
  - **Prompt Language**: English prompts perform better but require users to write English instructions; Nepali prompts are accessible but yield ~7 F1-micro points lower.

- Failure signatures:
  - **Inconsistent output format**: Llama-3-8B and Mistral Large produced non-compliant outputs (e.g., explanatory text instead of delimited entities). Mitigation: Use GPT-4o or add output format post-processing.
  - **Low precision on rare entities**: EVENT F1=0.11 (Generative) vs 0.44 (Non-generative). Semantic retrieval may fail due to sparse training examples.
  - **Merged evaluation conflicts**: Tokens predicted as multiple entity types cause priority-based overwrites, masking true predictions.

- First 3 experiments:
  1. **Baseline Zero-Shot**: Run English prompts with k=0 on a 50-sentence subset; measure F1-micro and output format compliance. Expected: F1≈0.31.
  2. **Semantic vs Random (k=5)**: Compare semantic and random example selection on the same subset. Expected: Semantic outperforms random by ~7-10 F1-micro points.
  3. **Self-Verification Ablation**: Enable self-verification for PERSON entities only on 100 sentences; measure precision/recall change. Expected: Precision increase ~0.15, recall stable or slight drop.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can self-verification mechanisms be refined to improve precision for low-resource NER without significantly degrading recall, particularly for temporal entities like DATE?
- Basis in paper: [explicit] The authors explicitly state in the conclusion that "Further research should refine self-verification method to boost precision without sacrificing recall, especially for low-performing entities."
- Why unresolved: While self-verification improved overall precision (0.50 to 0.66), it caused a severe drop in recall for DATE entities (0.71 to 0.23), making the current method unsuitable for applications requiring comprehensive temporal extraction.
- What evidence would resolve it: A modified verification protocol that maintains DATE recall above 0.70 while preserving the precision gains observed in other entity categories.

### Open Question 2
- Question: Can instruction fine-tuning close the performance gap between generative LLMs and traditional supervised models for Nepali NER?
- Basis in paper: [explicit] The paper concludes that "Future work includes instruction fine-tuning to enhance LLM’s understanding of Nepali entity patterns and task-specific instructions."
- Why unresolved: The study utilized prompting (in-context learning) with GPT-4o, which lagged behind non-generative baselines (F1 0.64 vs. 0.85). It is unknown if model weight updates are required to surpass supervised systems.
- What evidence would resolve it: An evaluation of an instruction-tuned Nepali-capable model achieving an F1-score greater than the 0.85 baseline established by non-generative models.

### Open Question 3
- Question: To what extent can prompt engineering or lightweight fine-tuning enable smaller, open-source models (e.g., Llama-3-8B, Mistral) to generate consistent, structured NER outputs for Nepali?
- Basis in paper: [inferred] The authors excluded Llama and Mistral from the final evaluation because they "generated very inconsistent output" compared to GPT-4o, highlighting a stability issue with smaller models in this context.
- Why unresolved: Reliance on proprietary models like GPT-4o incurs high operational costs ($175 for this study). It remains unclear if open-source alternatives can achieve the output consistency required for structured parsing without extensive retraining.
- What evidence would resolve it: A demonstrated prompting strategy or adapter module for an 8B-parameter model that produces parsable, format-compliant output for over 95% of input instances.

## Limitations
- Prompt Template Completeness: Only one full prompt example (for ORGANIZATION) and one self-verification example (for EVENT) are provided, requiring inference for other entity types.
- Model Dependency: Only GPT-4o produced compliant outputs; Llama-3-8B and Mistral Large were discarded, introducing strong API and cost dependency.
- Per-Entity Generalization: EVENT and DATE show notably poor F1 scores (0.11 and 0.40 generative vs 0.44 and 0.66 non-generative), with unclear root causes.

## Confidence

- **High Confidence**: Semantic example selection consistently outperforms random selection across metrics (F1-micro 0.64 vs 0.55); English prompts outperform Nepali prompts (F1-micro 0.64 vs 0.57); self-verification improves precision at the cost of recall.

- **Medium Confidence**: The overall F1-micro score of 0.64 is competitive within the low-resource setting, but direct comparison to non-generative baselines is limited to one paper (FMR=0.54). Generalization to other low-resource languages is plausible but unverified.

- **Low Confidence**: The specific prompt templates for PERSON, LOCATION, DATE, and EVENT are not shown, making exact replication uncertain. The self-verification mechanism's effectiveness for each entity type is not individually reported.

## Next Checks

1. **Prompt Template Validation**: Replicate the ORGANIZATION prompt exactly as given, then generate and test PERSON, LOCATION, DATE, and EVENT prompts using the same format. Measure output compliance and F1-micro for each.

2. **Self-Verification Ablation per Entity**: Run self-verification on all five entity types separately on a 100-sentence validation set. Report per-entity precision/recall changes to identify which entities benefit or degrade.

3. **Semantic Retrieval Robustness**: Compare semantic selection with random selection on a small subset using a different embedding model (e.g., multilingual BERT) to test whether the advantage is due to NPVec1 quality or the semantic retrieval approach itself.