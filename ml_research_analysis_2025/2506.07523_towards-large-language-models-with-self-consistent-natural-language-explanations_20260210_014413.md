---
ver: rpa2
title: Towards Large Language Models with Self-Consistent Natural Language Explanations
arxiv_id: '2506.07523'
source_url: https://arxiv.org/abs/2506.07523
tags:
- attribution
- explanations
- explanation
- self-consistency
- llama3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Post-hoc Self-Consistency Bank (PSCB),
  a large-scale benchmark for evaluating natural language explanations (NLEs) from
  LLMs using feature attribution alignment. PSCB includes over 23,000 decisions across
  four multiple-choice QA datasets and two LLMs, each with five explanations and corresponding
  attribution vectors.
---

# Towards Large Language Models with Self-Consistent Natural Language Explanations

## Quick Facts
- arXiv ID: 2506.07523
- Source URL: https://arxiv.org/abs/2506.07523
- Authors: Sahar Admoni; Ofra Amir; Assaf Hallak; Yftah Ziser
- Reference count: 11
- Primary result: DPO fine-tuning with attribution-based preference pairs improves self-consistency up to 44% and generalizes across domains

## Executive Summary
This paper introduces the Post-hoc Self-Consistency Bank (PSCB), a benchmark for evaluating natural language explanations (NLEs) from LLMs through feature attribution alignment. The authors find that standard cosine similarity metrics fail to distinguish between correct and incorrect explanations, and propose Spearman rank correlation as a more effective alignment metric. Using attribution-derived preference pairs, they fine-tune LLMs via Direct Preference Optimization (DPO), achieving substantial improvements in self-consistency that generalize to unseen domains.

## Method Summary
The PSCB benchmark constructs multiple-choice QA datasets with five explanations per decision, paired with LIME attribution vectors for both decisions and explanations. For evaluation, explanations are ranked by their alignment with decision attributions using both cosine similarity and Spearman correlation. Preference pairs are constructed from highest and lowest ranked explanations, then used to fine-tune LLMs via DPO. The approach aims to improve the alignment between a model's reasoning (decision attributions) and its explanations.

## Key Results
- Cosine similarity scores show minimal variation between correct and incorrect predictions, failing to distinguish explanation quality
- Spearman rank correlation provides better discriminative ability for explanation quality assessment
- DPO fine-tuning with attribution-based preferences improves self-consistency up to 44% in worst-case Spearman scores
- Improvements generalize to unseen domains despite training on attribution-derived preferences

## Why This Works (Mechanism)

### Mechanism 1: Spearman Rank Correlation Captures Feature Prioritization Better Than Cosine Similarity
Cosine similarity measures directional alignment of attribution magnitude vectors, which compresses scores into a narrow range. Spearman correlation measures rank-based alignment of feature importance, abstracting away from exact attribution values and capturing which features the model prioritizes relative to others. This rank-based approach is more stable and provides greater variability for identifying improvements.

### Mechanism 2: Attribution-Based Preference Pairs Enable DPO Optimization for Self-Consistency
For each input, k=5 explanations are generated, LIME attributions are computed, and explanations are ranked by Spearman alignment with decision attributions. The highest and lowest ranked explanations form preference pairs for DPO training. The model learns to prefer explanations whose feature importance patterns align with its decision-making, improving internal consistency without requiring reward models or RL.

### Mechanism 3: Cross-Metric Transfer From Rank Optimization to Magnitude Alignment
Optimizing for feature ranking consistency implicitly improves directional alignment, as both metrics share the underlying goal of matching decision and explanation attributions. Better ranking reduces the set of features that can diverge in magnitude. Training with Spearman correlation produces spillover improvements in cosine similarity despite not being directly optimized.

## Foundational Learning

- **Feature Attribution Methods (LIME/SHAP)**: Essential for computing token-level attribution vectors that the entire framework relies upon. LIME perturbs inputs and measures output changes to assign importance scores.
  - Quick check: Can you explain why LIME is preferred over attention-based attribution for this task, and what its computational tradeoffs are?

- **Direct Preference Optimization (DPO)**: Replaces RL-based alignment by directly optimizing a classification objective on preference pairs. Understanding the loss function is essential for debugging training.
  - Quick check: How does DPO differ from PPO for RLHF, and what does the β hyperparameter control?

- **Self-Consistency vs. Faithfulness**: The paper explicitly notes self-consistency is a "necessary prerequisite" for faithfulness, not equivalent. Conflating these leads to overclaiming.
  - Quick check: If a model produces self-consistent explanations that are factually wrong, is it faithful? Why or why not?

## Architecture Onboarding

- **Component map**: Input Processing -> LLM decision generation -> Explanation Generation -> Attribution Module -> Alignment Scoring -> Preference Construction -> DPO Training
- **Critical path**: Attribution computation is the bottleneck (~1 minute per example with LIME); preference pair quality depends on attribution reliability; DPO β and learning rate are sensitive hyperparameters
- **Design tradeoffs**: LIME vs. SHAP (faster but less theoretically grounded); Spearman vs. Cosine (better discrimination but less interpretable); Offline vs. Online DPO (practical but creates model drift)
- **Failure signatures**: Low Spearman variance across explanations indicates metric cannot discriminate; cosine similarity degrades after DPO suggests rank optimization misalignment; cross-domain transfer fails suggests feature importance distribution mismatch
- **First 3 experiments**: (1) Reproduce PSCB statistics on ECQA subset; (2) Ablate attribution method comparing LIME vs. Integrated Gradients; (3) Test cross-domain transfer training on ARC-Easy, evaluating on ARC-Challenge

## Open Questions the Paper Calls Out

- Does higher attribution-based self-consistency lead to explanations that users find more plausible, helpful, or trustworthy? The paper evaluated only computational metrics, not human perception.
- Can online learning with continuous attribution recalculation achieve better self-consistency than offline DPO? The computational cost and potential instability make online approaches impractical with current methods.
- Do the findings generalize beyond MCQA tasks to open-ended generation tasks like summarization or chain-of-thought reasoning? The structured outputs of MCQA may not extend to more complex reasoning tasks.

## Limitations

- Attribution reliability is a major concern since the framework depends entirely on LIME attributions, which are known to be unstable across runs and sensitive to perturbation sampling
- Domain transferability assumptions are limited to multiple-choice QA tasks with similar structure; generalization to open-ended generation or non-text modalities remains unproven
- Self-consistency vs. faithfulness conflation exists despite careful distinction, as attribution alignment serves as a proxy for both, potentially optimizing for internal consistency over external correctness

## Confidence

- High confidence: PSCB benchmark construction and baseline statistics (clear methodology, reproducible)
- Medium confidence: DPO training improvements and cross-domain transfer (controlled experiments, but attribution dependency)
- Low confidence: Attribution-based quality assessment (methodologically sound but practically noisy)

## Next Checks

1. **Attribution variance analysis**: Generate 10 attribution vectors for the same (input, explanation) pair using different random seeds. Measure coefficient of variation and test whether preference pair selection remains stable across variance levels.

2. **Alternative attribution method ablation**: Repeat DPO training using Integrated Gradients instead of LIME on a subset of PSCB. Compare final self-consistency gains and training stability to isolate attribution method impact.

3. **Open-domain transfer test**: Apply the trained DPO model to a non-MCQA task (e.g., summarization or sentiment analysis). Measure whether self-consistency improvements transfer and whether attribution alignment remains meaningful without predefined answer choices.