---
ver: rpa2
title: Exploring the Effects of Alignment on Numerical Bias in Large Language Models
arxiv_id: '2601.16444'
source_url: https://arxiv.org/abs/2601.16444
tags:
- bias
- score
- numerical
- evaluation
- range
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how alignment processes affect numerical\
  \ bias in LLM-as-a-judge systems. Through experiments comparing pre- and post-alignment\
  \ models on MTQE, GECQE, and LCP tasks, the study finds that alignment significantly\
  \ increases numerical bias\u2014post-alignment models produce highly skewed score\
  \ distributions concentrated around specific values, particularly for high-resource\
  \ languages."
---

# Exploring the Effects of Alignment on Numerical Bias in Large Language Models

## Quick Facts
- arXiv ID: 2601.16444
- Source URL: https://arxiv.org/abs/2601.16444
- Reference count: 25
- This paper investigates how alignment processes affect numerical bias in LLM-as-a-judge systems, finding that alignment significantly increases numerical bias while improving evaluation accuracy.

## Executive Summary
This paper investigates how alignment processes affect numerical bias in LLM-as-a-judge systems. Through experiments comparing pre- and post-alignment models on MTQE, GECQE, and LCP tasks, the study finds that alignment significantly increases numerical bias—post-alignment models produce highly skewed score distributions concentrated around specific values, particularly for high-resource languages. While alignment improves instruction-following and overall evaluation accuracy, it also introduces stronger numerical bias that correlates with lower evaluation performance. The research explores mitigation strategies including temperature scaling, distribution calibration, and score range adjustment, finding that adjusting the score range is most effective at reducing bias and improving accuracy, though it remains a heuristic solution. The findings suggest that score range should be treated as a tunable hyperparameter rather than a fixed design choice in LLM-as-a-judge systems.

## Method Summary
The paper compares pre- and post-alignment versions of four model pairs (Gemma, Mistral, Llama-3, Qwen-2) on three evaluation tasks: MTQE (machine translation quality estimation), GECQE (grammar error correction quality estimation), and LCP (language complexity prediction). For each input, 10 scores are generated with temperature=0.7, max_tokens=5 to suppress non-numeric outputs, then averaged after clipping to score range boundaries. Kurtosis measures numerical bias while Pearson's r measures correlation with human gold scores. The study tests mitigation strategies including temperature scaling, distribution calibration, and score range adjustment.

## Key Results
- Post-alignment models exhibit significantly higher kurtosis (numerical bias) than pre-alignment models across all tasks and language pairs
- Higher kurtosis correlates with lower evaluation accuracy (r = -0.60)
- Score range adjustment is most effective mitigation strategy, though effectiveness varies by task and model
- Alignment improves instruction-following and overall evaluation accuracy despite introducing numerical bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Alignment processes induce numerical bias by reducing output diversity and creating token-level preferences for specific numeric values.
- Mechanism: Instruction tuning and preference tuning optimize models toward consistent, aligned outputs. This optimization inadvertently creates strong preferences for certain numeric tokens (e.g., "8" in a 0-9 range), causing post-alignment models to produce highly skewed score distributions regardless of input quality.
- Core assumption: The reduction in diversity observed in other aligned outputs extends to numeric token generation.
- Evidence anchors:
  - [abstract] "Given that most evaluator LLMs are aligned through instruction tuning and preference tuning, and that prior research suggests alignment reduces output diversity, we hypothesize that numerical bias arises from alignment."
  - [section 3.3] "Post-alignment models generate scores that are heavily concentrated around 8... while the gold score distributions vary across data, post-alignment models produce evaluation score distributions with strikingly similar shapes."
  - [corpus] Weak direct evidence—corpus neighbors focus on political/social bias, not numerical token bias. No direct validation of the diversity reduction mechanism.
- Break condition: If alignment data contains diverse numeric annotations uniformly distributed across score ranges, this mechanism would weaken.

### Mechanism 2
- Claim: Score range adjustment mitigates numerical bias by altering the available token set and redistributing model probability mass.
- Mechanism: Different score ranges activate different numeric token vocabularies. A 1-100 range dilutes token-level bias across more possibilities, while a 1-5 range may avoid tokens the model has learned to over-predict. This changes which tokens receive concentrated probability mass.
- Core assumption: The model's token-level biases are range-dependent rather than intrinsic to all numeric generation.
- Evidence anchors:
  - [abstract] "Among these, score range adjustment is most effective in reducing bias and improving performance, though still heuristic."
  - [section 4.3] "In Gemma with the MTQE task, the 1–100 scale achieved 0.14 in r, outperforming the results in Figure 5 and Table 5."
  - [corpus] The paper "Contrastive Decoding Mitigates Score Range Bias in LLM-as-a-Judge" (arXiv:2510.18196) independently validates score range bias as a measurable phenomenon requiring mitigation.
- Break condition: If optimal score ranges are found to be task-specific without transferability, the mechanism becomes primarily empirical rather than principled.

### Mechanism 3
- Claim: Numerical bias directly harms evaluation accuracy by reducing distinguishability between inputs of different quality levels.
- Mechanism: When models concentrate scores around specific values, the variance needed to differentiate quality levels is compressed. This reduces correlation with ground truth because the model cannot express fine-grained quality distinctions.
- Core assumption: Evaluation accuracy requires score distributions that can capture quality variations present in the data.
- Evidence anchors:
  - [abstract] "alignment significantly increases numerical bias—post-alignment models produce highly skewed score distributions concentrated around specific values."
  - [section 3.3] "Kurtosis and r show a strong negative correlation of −0.60 for both MTQE and GECQE, indicating that greater numerical bias leads to lower accuracy."
  - [corpus] No direct corpus support for the kurtosis-accuracy correlation mechanism specifically.
- Break condition: If a task has genuinely binary quality distinctions, some score concentration may not harm accuracy.

## Foundational Learning

- Concept: **Kurtosis as a bias metric**
  - Why needed here: The paper uses kurtosis (not variance) to quantify numerical bias because it measures local concentration around specific values rather than overall distribution narrowness.
  - Quick check question: If a model outputs only scores 4 and 6 with equal frequency vs. outputs all integers 0-9 uniformly, which has higher kurtosis?

- Concept: **Alignment as a double-edged process**
  - Why needed here: Understanding that alignment improves instruction-following while potentially introducing biases helps contextualize why post-alignment models outperform pre-alignment models despite having higher numerical bias.
  - Quick check question: Why might a pre-alignment model with low kurtosis still have worse evaluation accuracy than a biased post-alignment model?

- Concept: **Temperature scaling in token prediction**
  - Why needed here: The paper tests temperature adjustment as a mitigation strategy; understanding how temperature affects the softmax distribution over tokens is essential for interpreting these results.
  - Quick check question: Would increasing temperature from 0.7 to 1.3 make the output distribution more uniform or more peaked?

## Architecture Onboarding

- Component map:
  Input text + prompt template -> LLM evaluator (pre/post-aligned) -> Score extraction (max_tokens=5) -> Numeric filtering and clipping -> Aggregation (average over 10 generations) -> Kurtosis and correlation computation

- Critical path:
  1. Select evaluator model -> 2. Define score range in prompt -> 3. Generate scores with specified temperature -> 4. Extract and clip numeric outputs -> 5. Aggregate via averaging (or weighted calibration) -> 6. Compute kurtosis and correlation metrics

- Design tradeoffs:
  - Pre-alignment models: Lower bias but poor instruction-following → lower practical accuracy
  - Post-alignment models: Higher bias but better instruction-following → higher practical accuracy despite bias
  - Small score range (1-5): Lower kurtosis but risk of random outputs if range doesn't match task
  - Large score range (1-100): More expressive but may not reduce bias for all models
  - Temperature scaling: Can reduce bias but may also reduce accuracy if set too high

- Failure signatures:
  - Kurtosis > 20 for a score range: Strong numerical bias likely present
  - Scores concentrated at single value (e.g., all 8s): Model not differentiating inputs
  - Pre-alignment model with near-zero kurtosis but poor correlation: Model ignoring instructions
  - Calibration improving kurtosis but worsening accuracy: Original distribution better matched to task

- First 3 experiments:
  1. Run pre- and post-alignment versions of same model on held-out data; plot score distributions and compute kurtosis to confirm alignment-induced bias.
  2. Test score ranges {1-5, 0-9, 1-100} with fixed temperature 0.7; identify range that minimizes kurtosis while maximizing correlation for your specific task.
  3. For best-performing range, sweep temperature {0.4, 0.7, 1.0, 1.3} to determine if further bias reduction is possible without accuracy loss.

## Open Questions the Paper Calls Out

- How do specific alignment methods (e.g., instruction tuning vs. preference tuning) and data compositions distinctively contribute to numerical bias?
  - Basis in paper: The Conclusion states future work should "explore various alignment strengths," and the Limitations section notes that public models do not disclose alignment details.
  - Why unresolved: The authors relied on public models where the specific alignment data and methods are unknown, and in-house alignment was computationally prohibitive.
  - What evidence would resolve it: Controlled experiments aligning base models with varied, documented data compositions and strengths to isolate the causal factors of bias.

- Can a theoretically grounded, task-agnostic mitigation strategy be developed to replace heuristic score range adjustment?
  - Basis in paper: The Abstract highlights the need for "more robust mitigation strategies," and the Limitations section describes the current best solution, score range adjustment, as "heuristic and task-specific."
  - Why unresolved: While effective, current mitigation relies on tuning for specific datasets rather than addressing the fundamental causes of token probability skew.
  - What evidence would resolve it: A mitigation method that consistently reduces kurtosis and improves correlation across MTQE, GECQE, and LCP tasks without requiring dataset-specific tuning.

- Does the alignment-induced numerical bias observed in scoring tasks also manifest in LLM-as-a-judge systems that output natural language labels or rankings?
  - Basis in paper: The Limitations section states the study is restricted to "numerical scores," leaving the effect on "evaluations with natural language labels and rankings... unresolved."
  - Why unresolved: The experimental design focused exclusively on regression tasks, leaving classification or ranking-based evaluation biases unexplored.
  - What evidence would resolve it: Comparative experiments measuring output distribution bias in pre- and post-alignment models using non-numeric evaluation protocols.

## Limitations
- Score range adjustment is presented as a mitigation strategy but remains a heuristic solution rather than a principled approach, with effectiveness varying significantly across tasks without clear theoretical justification.
- The kurtosis-accuracy correlation mechanism lacks direct empirical validation, with corpus review finding no direct support for this specific relationship between bias and evaluation accuracy.
- The impact of non-numeric output filtering on bias measurements is unclear, as the paper doesn't specify the exact filtering procedure or how frequently this occurs.

## Confidence
- **High confidence:** The observation that post-alignment models exhibit higher numerical bias (measured via kurtosis) than pre-alignment models is well-supported by experimental results across multiple model pairs and tasks.
- **Medium confidence:** The finding that score range adjustment is the most effective mitigation strategy is supported by experimental results, but the heuristic nature of this solution limits confidence in its generalizability.
- **Medium confidence:** The correlation between numerical bias (kurtosis) and evaluation accuracy (Pearson's r) is reported with statistical significance, but the corpus review suggests this relationship may not be as well-established in the broader literature as presented.

## Next Checks
- Replicate the kurtosis-accuracy correlation analysis using a different bias metric (e.g., variance or entropy) to determine whether the negative correlation is specific to kurtosis or represents a more general relationship between score distribution concentration and evaluation accuracy.
- Conduct ablation studies on the non-numeric output filtering process by varying the max_tokens parameter and analyzing how different filtering thresholds affect kurtosis measurements and the reported pre/post-alignment bias differences.
- Test the score range adjustment mitigation across a broader set of tasks beyond the three used in the paper (MTQE, GECQE, LCP) to determine whether the effectiveness of specific score ranges is task-dependent or follows generalizable patterns.