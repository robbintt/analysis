---
ver: rpa2
title: Towards Proactive Personalization through Profile Customization for Individual
  Users in Dialogues
arxiv_id: '2512.15302'
source_url: https://arxiv.org/abs/2512.15302
tags:
- user
- alignment
- preferences
- arxiv
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PersonalAgent addresses the challenge of personalized alignment
  in conversational AI by modeling multi-turn dialogues as a sequential inference
  process. It dynamically constructs and updates user profiles through turn-by-turn
  preference inference, framed as a multi-turn Markov Decision Process.
---

# Towards Proactive Personalization through Profile Customization for Individual Users in Dialogues

## Quick Facts
- arXiv ID: 2512.15302
- Source URL: https://arxiv.org/abs/2512.15302
- Reference count: 26
- Primary result: PersonalAgent achieves up to 87.5% accuracy and 69.1% alignment level on ALOE, PrefEval, and ALOE-Unseen benchmarks, outperforming strong baselines.

## Executive Summary
PersonalAgent addresses personalized alignment in conversational AI by modeling multi-turn dialogues as a sequential inference process. It dynamically constructs and updates user profiles through turn-by-turn preference inference, framed as a multi-turn Markov Decision Process. The approach maintains long-term consistency across sessions and proactively queries users in cold-start scenarios. Experiments show PersonalAgent outperforms strong baselines on ALOE, PrefEval, and ALOE-Unseen benchmarks, achieving up to 87.5% accuracy and 69.1% alignment level, with consistent gains in noisy contexts and long-term alignment scenarios.

## Method Summary
PersonalAgent treats multi-turn dialogues as sequential decision-making tasks, decomposing conversations into single-turn units and framing preference inference as a Markov Decision Process. Each dialogue turn produces an inferred preference snippet that updates the user profile, which persists across sessions. When profile coverage is insufficient, the system proactively queries users before response generation. The approach uses Group Relative Policy Optimization (GRPO) with a multi-criteria reward system evaluating Completeness, No Hallucination, Informativeness, and Consistency. Training uses Qwen3-4B-Instruct with Qwen3-30B-A3B as the reward model judge.

## Key Results
- ALOE benchmark: 87.5% accuracy (vs 86.4% baseline)
- PrefEval alignment level: 69.1% (vs 59.7% baseline)
- ALOE-Unseen cold-start accuracy: 68.4% (vs 34.7% baseline)
- 22% improvement over supervised fine-tuning through GRPO
- Consistent performance gains in noisy conversational contexts and long-term alignment scenarios

## Why This Works (Mechanism)

### Mechanism 1: Turn-Level Sequential Inference with MDP Formulation
Decomposing multi-turn dialogues into single-turn units and formulating preference inference as a Markov Decision Process enables more accurate and consistent preference tracking than feeding entire conversation history. Each dialogue turn produces an inferred preference snippet that becomes part of the state for the next turn, creating a sequential decision process that mimics human memory updating.

### Mechanism 2: Policy-Based Reward Optimization via GRPO
Reinforcement learning with a multi-criteria policy-based judge outperforms supervised fine-tuning for preference inference because preferences are dynamic and not directly equivalent to explicit user statements. GRPO samples candidate outputs per turn, computes advantages using group-normalized rewards, and optimizes with clipped objective plus KL penalty across four evaluation criteria.

### Mechanism 3: Cross-Session Profile Persistence with Proactive Querying
Maintaining persistent user profiles across sessions and proactively querying when profile coverage is insufficient improves cold-start alignment and long-term consistency. The system checks profile relevance before response generation and triggers user queries when necessary, enabling personalized interactions from the first message in new sessions.

## Foundational Learning

- **Markov Decision Processes (MDPs) in Sequential Settings:**
  - Why needed: The core formulation treats preference inference as sequential decision-making under the Markov assumption
  - Quick check: Can you explain why the next state st+1 depends only on (st, at) and not on earlier history?

- **Group Relative Policy Optimization (GRPO):**
  - Why needed: Training uses GRPO, a variant of PPO with group-based advantage normalization
  - Quick check: How does group normalization of advantages differ from standard advantage estimation in PPO?

- **Profile-Based User Modeling:**
  - Why needed: The system relies on structured profile templates with 11 categories and 300+ subcategories
  - Quick check: What tradeoffs exist between fixed schema profiles and open-ended preference representations?

## Architecture Onboarding

- **Component map:** Dialogue Decomposer -> Preference Inferencer -> Profile Store -> Proactive Query Decision -> Response Generator
- **Critical path:** Dialogue Decomposer → Preference Inferencer → Profile Store → Proactive Query Decision → Response Generator. The inference loop runs turn-by-turn within sessions; profile persistence crosses sessions.
- **Design tradeoffs:** Fixed profile schema enables structured training but limits expressiveness for novel preference types. Turn-level decomposition reduces context length but may miss cross-turn dependencies. Policy-based judge enables training but introduces dependency on judge quality (human agreement κ≈0.77-0.78).
- **Failure signatures:** Preference drift: Profile becomes stale as user preferences change; mitigate with decay or re-inference triggers. Cold-start over-querying: Agent queries excessively when profile is empty; implement query budget limits. Reward hacking: Model optimizes for judge score without true alignment; monitor with human evaluation.
- **First 3 experiments:** 1) Baseline comparison on single-turn inference: Measure accuracy of pn vs. ground truth without profile persistence to isolate inference capability. 2) Ablation on reward criteria: Train with subsets of {Completeness, No Hallucination, Informativeness, Consistency} to identify which criteria drive alignment gains. 3) Long-term consistency stress test: Insert increasing amounts of irrelevant dialogue (following PrefEval protocol) and measure alignment degradation rate compared to MemBank baseline.

## Open Questions the Paper Calls Out

### Open Question 1
How does PersonalAgent's performance scale when the number of interaction turns increases significantly beyond current evaluation limits (e.g., >10 turns) or spans indefinite lifespans? The authors note that future research would benefit from increasing interaction turns and broadening the evaluation horizon, but current experiments are limited due to computational costs and lack of established benchmarks for very long-term interaction.

### Open Question 2
How can the fixed profile template be adapted to dynamically identify and store user preferences that fall outside the 11 pre-defined major categories? The profile construction relies on a specific template derived from LMSYS-Chat-1M, which assumes user preferences fit into 11 major categories and 300 subcategories. A static template risks failing to capture niche or novel user attributes that do not map cleanly to the pre-defined schema.

### Open Question 3
To what extent does reliance on LLM-as-a-Judge (GPT-4.1) for evaluation introduce systematic bias compared to human judgment in complex, multi-turn alignment scenarios? While human evaluation was conducted, the validation of the automatic judge was performed on only 100 random samples, which may not fully represent the complexity of sequential preference accumulation. The agreement score (0.78) is high but not perfect, and the paper leaves unexplored whether this discrepancy widens in noisy conversational contexts or highly subjective preference domains.

## Limitations
- Turn-level decomposition may lose cross-turn dependencies critical for complex preference understanding
- Fixed 11-category profile schema limits expressiveness for novel preference types not covered in the template
- GRPO training relies heavily on reward model quality, with moderate human subjectivity in reward criteria (κ≈0.77-0.78)

## Confidence

- **High Confidence:** The sequential MDP formulation and GRPO training methodology are technically sound and well-supported by ablation studies showing 22% improvement over SFT.
- **Medium Confidence:** Cross-session profile persistence and proactive querying show strong cold-start performance (68.4% vs 34.7% baseline) but depend heavily on stable preference assumptions.
- **Low Confidence:** The claim that turn-level decomposition preserves all necessary preference information lacks direct validation against full-context baselines.

## Next Checks

1. **Context Dependency Test:** Compare turn-level inference accuracy against full-conversation context baselines to quantify information loss from decomposition.
2. **Profile Stability Analysis:** Track profile update frequency and content changes across sessions to measure staleness risk and validate decay mechanisms.
3. **Reward Model Fidelity:** Conduct human evaluation comparing policy outputs with and without the judge model to assess whether optimization aligns with true preference quality rather than reward hacking.