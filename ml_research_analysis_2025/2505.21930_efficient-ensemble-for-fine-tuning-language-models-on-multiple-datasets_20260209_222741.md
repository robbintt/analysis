---
ver: rpa2
title: Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets
arxiv_id: '2505.21930'
source_url: https://arxiv.org/abs/2505.21930
tags:
- fine-tuning
- adapters
- adapter
- qlora
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an ensemble method for efficiently fine-tuning
  language models on multiple datasets. The key idea is to group similar datasets
  based on task affinity scores estimated via first-order gradient approximations,
  then train one adapter per group and combine them via weighted averaging.
---

# Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets

## Quick Facts
- arXiv ID: 2505.21930
- Source URL: https://arxiv.org/abs/2505.21930
- Authors: Dongyue Li; Ziniu Zhang; Lu Wang; Hongyang R. Zhang
- Reference count: 40
- One-line primary result: Achieves up to 10% accuracy improvement over QLoRA on ten text classification tasks with only 9% more FLOPs and 9 GB extra memory.

## Executive Summary
This paper introduces an ensemble method for efficiently fine-tuning language models on multiple datasets. The key idea is to group similar datasets based on task affinity scores estimated via first-order gradient approximations, then train one adapter per group and combine them via weighted averaging. The gradient-based estimation allows performance prediction without actual fine-tuning, achieving up to 105× speedup. When applied to Llama and GPT models on ten text classification tasks, the approach improves QLoRA's average test accuracy by up to 10% with only 9% more FLOPs and 9 GB extra memory. It also reduces generalization errors compared to single adapters and achieves 3% accuracy gain on a 34-billion parameter Llama model with just 8% additional FLOPs.

## Method Summary
The method first estimates task affinity scores using a first-order Taylor expansion of the base model's output, which allows rapid prediction of fine-tuning performance without training. These scores are used to cluster datasets into groups that minimize negative transfer. One LoRA adapter is then trained per group. Finally, an ensemble is created by weighted averaging the adapter outputs, with an optional gradient boosting step to fit residual errors. The approach requires only one pass over all data to compute gradients, making it computationally efficient.

## Key Results
- Achieves up to 10% accuracy improvement over QLoRA on ten text classification tasks
- Reduces generalization errors compared to single adapters
- Achieves 3% accuracy gain on 34-billion parameter Llama model with just 8% additional FLOPs
- Enables 105× speedup in performance estimation compared to actual fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: First-Order Gradient Approximation for Performance Estimation
The paper claims that fine-tuned adapter weights remain extremely close to the base model (typically within 0.2% relative distance), enabling first-order Taylor expansion to accurately approximate model outputs and performance without actual fine-tuning. A Taylor expansion around base model parameters approximates the output function, allowing the fine-tuning problem to be formulated as linear regression on gradients evaluated at the base model.

### Mechanism 2: Task Affinity Grouping via Gradient-Based Estimation
Grouping similar datasets for multi-task learning reduces negative interference, a common problem when fine-tuning a single adapter on diverse tasks. The method computes a task affinity matrix where each element represents the affinity between tasks, calculated as the average estimated fine-tuning performance over random dataset subsets containing both tasks. This matrix is used by a clustering algorithm to partition datasets into groups that maximize intra-cluster affinity.

### Mechanism 3: Ensemble of Specialized Adapters with Gradient Boosting
An ensemble of adapters, each fine-tuned on a task group, combined via weighted averaging, outperforms a single adapter trained on all tasks. A small number of boosting steps further refines performance on difficult groups by training new adapters to fit the negative gradient (residual error) of the current ensemble on the highest-loss group's data.

## Foundational Learning

- **First-Order Taylor Expansion**
  - Why needed here: This is the mathematical foundation for efficiently estimating fine-tuning performance without training.
  - Quick check question: Can you explain why a Taylor expansion is a good approximation for the output of a fine-tuned LoRA adapter but might be a poor one for a fully fine-tuned model?

- **Task Affinity and Negative Transfer**
  - Why needed here: The paper's primary motivation is to solve the problem of negative transfer in multi-task learning.
  - Quick check question: If you were to train a single LoRA adapter jointly on a sentiment analysis dataset and a code summarization dataset, what would you expect to happen to its performance on each individual task compared to training separate adapters?

- **Gradient Boosting**
  - Why needed here: The paper incorporates a boosting step to refine the adapter ensemble by fitting new adapters to residual errors.
  - Quick check question: In the context of this paper's boosting step, what specific target is the new adapter trained to predict, and how does this relate to the loss function of the current ensemble?

## Architecture Onboarding

- **Component map**: Input datasets -> Base Language Model (frozen) -> Task Affinity Estimation Module -> Clustering Module -> Adapter Ensemble Construction Module -> Ensemble of adapters with learned weights

- **Critical path**: The entire architecture hinges on the accuracy of the Task Affinity Estimation Module. If the first-order approximation is inaccurate, the affinity matrix will be noisy, leading to suboptimal clustering and poor performance.

- **Design tradeoffs**:
  - Projection Dimension (d): Larger d yields more accurate affinity estimation but increases regression cost (paper finds d=400 provides good tradeoff)
  - Number of Groups (m): Smaller m reduces computation/memory but risks larger groups with higher negative interference
  - Number of Boosting Steps (b): More steps can further reduce training error but add adapters to the ensemble, increasing memory usage

- **Failure signatures**:
  - High Approximation Error: If relative error between estimated and true performance exceeds 5%, the grouping will be unreliable
  - No Generalization Gain: If ensemble test accuracy isn't significantly better than single multi-task adapter, grouping failed to isolate positively transferring tasks
  - Memory Overflow: If final number of adapters M is too large, inference won't fit in GPU memory

- **First 3 experiments**:
  1. Validate First-Order Approximation: Compare estimated vs. actual fine-tuning performance for random task subsets, targeting <5% relative error
  2. Cluster and Compare Groupings: Test clustering on SuperGLUE tasks, comparing performance of affinity-based vs. random groupings
  3. End-to-End Ensemble Evaluation: Run full pipeline on multi-task benchmark, comparing against QLoRA and MTL-FT baselines on accuracy, FLOPs, and memory

## Open Questions the Paper Calls Out

- Can fine-tuning performance be estimated for closed-source models (e.g., GPT-4, Gemini) without access to internal weights and gradients?
- Can this method be extended to handle dynamically incoming tasks in a continual learning scenario?
- Do sharpness measures like Hessian trace correlate with out-of-distribution (OOD) generalization or adversarial robustness?

## Limitations

- Approximation validity range uncertainty: Method reliability for extreme cases (very high adapter rank or unusual training durations) remains untested
- Task affinity generalization concerns: Base model's internal representations may not align well with truly orthogonal tasks, potentially producing false positives
- Computational overhead scaling: Memory cost scales with final number of adapters, and initial gradient computation is significant upfront cost for very large models

## Confidence

- High Confidence: 105× speedup claim is well-supported by approximation error data and explicit statements about weight distances
- Medium Confidence: 10% accuracy improvement claim is supported by results but depends on specific task set and hyperparameters
- Low Confidence: 3% accuracy gain on 34-billion parameter model with 8% additional FLOPs is based on single experiment without comprehensive ablation

## Next Checks

1. Boundary Condition Testing: Systematically vary LoRA rank and training epochs to identify when first-order assumption breaks down
2. Cross-Domain Task Affinity: Apply method to tasks from completely different domains to test clustering quality and performance gains
3. Large-Scale Memory Profiling: Profile GPU memory usage at each stage for moderately large models to quantify overhead claims