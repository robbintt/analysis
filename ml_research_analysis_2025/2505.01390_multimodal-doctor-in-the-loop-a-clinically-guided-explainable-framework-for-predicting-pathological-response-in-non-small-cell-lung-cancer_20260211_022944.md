---
ver: rpa2
title: 'Multimodal Doctor-in-the-Loop: A Clinically-Guided Explainable Framework for
  Predicting Pathological Response in Non-Small Cell Lung Cancer'
arxiv_id: '2505.01390'
source_url: https://arxiv.org/abs/2505.01390
tags:
- clinical
- multimodal
- fusion
- data
- intermediate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of predicting pathological response
  in non-small cell lung cancer patients undergoing neoadjuvant therapy, which is
  critical for treatment planning and patient outcomes. The authors propose a novel
  Multimodal Doctor-in-the-Loop framework that integrates imaging and clinical data
  through intermediate fusion while incorporating domain knowledge via intrinsic eXplainable
  AI techniques.
---

# Multimodal Doctor-in-the-Loop: A Clinically-Guided Explainable Framework for Predicting Pathological Response in Non-Small Cell Lung Cancer

## Quick Facts
- arXiv ID: 2505.01390
- Source URL: https://arxiv.org/abs/2505.01390
- Reference count: 33
- Primary result: 73.00% accuracy, 72.49% AUC, and 40.30% MCC for predicting pathological response in 100 NSCLC patients using intermediate fusion with doctor-in-the-loop guidance

## Executive Summary
This study introduces a Multimodal Doctor-in-the-Loop framework that integrates imaging and clinical data to predict pathological response in non-small cell lung cancer patients undergoing neoadjuvant therapy. The approach combines intermediate fusion of multimodal features with clinician-guided learning through segmentation masks, while embedding explainability directly into the training process via intrinsic XAI loss. The framework was evaluated on a dataset of 100 NSCLC patients, demonstrating that this clinically-guided methodology significantly improves both predictive performance and interpretability compared to standard fusion approaches and ablation studies.

## Method Summary
The framework uses a two-stage training approach: first, unimodal CT (3D DenseNet169) and clinical (MLP) models are trained separately with gradual multi-view learning from global lung region to specific lesion regions guided by expert segmentation masks, incorporating XAI loss to align Grad-CAM heatmaps with masks. Second, the models are fused at the feature level through concatenation and fine-tuned end-to-end. The composite loss function combines classification loss with XAI loss that penalizes misalignment between Grad-CAM visualizations and expert masks. This intermediate fusion strategy preserves modality-specific processing while enabling cross-modal interaction during joint training.

## Key Results
- Achieved 73.00% accuracy, 72.49% AUC, and 40.30% MCC for pathological response prediction
- Intermediate fusion outperformed early and late fusion strategies
- Doctor-in-the-Loop methodology significantly enhanced performance versus ablation studies (MCC: 40.30% vs 28.11% for XAI-guide and 12.84% for Segmentation)
- Gradual multi-view learning (global → lung → lesion) improved model focus and convergence

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Intermediate fusion of imaging and clinical features outperforms early and late fusion for multimodal integration.
- **Mechanism:** Modality-specific encoders (3D CNN for CT, MLP for clinical) extract specialized representations before fusion at the feature level, allowing cross-modal interactions during joint training while preserving modality-specific processing strengths.
- **Core assumption:** Clinical and imaging features contain complementary information that benefits from interaction at intermediate layers rather than at input or decision levels.
- **Evidence anchors:**
  - [abstract]: "intermediate fusion strategy that integrates imaging and clinical data, enabling efficient interaction between data modalities"
  - [section IV-B]: "Intermediate fusion achieves an optimal balance by preserving modality-specific processing initially and enabling joint learning at later stages"
  - [corpus]: Related work on multimodal fusion (Guarrasi et al., 2025 systematic review) supports intermediate fusion for biomedical applications, though specific evidence for NSCLC remains limited to this study.

### Mechanism 2
- **Claim:** Gradual multi-view learning (global → lung → lesion) improves model focus and performance compared to direct lesion-only training.
- **Mechanism:** A three-stage progressive refinement process—Step 0 (global lung region with L_cls only), Step 1 (lung mask guidance with L_xai), Step 2 (lesion mask guidance)—allows the model to first learn broader anatomical context before narrowing to specific pathological regions.
- **Core assumption:** Small medical datasets (n=100) require curriculum-style guidance to identify clinically relevant regions; models cannot autonomously discover optimal attention patterns.
- **Evidence anchors:**
  - [section II-A]: "The network first aims to achieve high accuracy in image classification, learning from global image content... then leverages segmentation masks to iteratively refine its focus"
  - [section IV-C]: "Comparing Multimodal Doctor-in-the-Loop with XAI-guide highlights... without the benefit of GL, limits the model's ability to fully exploit the available data"
  - [corpus]: No direct corpus comparison; gradual learning is relatively novel in this clinical domain.

### Mechanism 3
- **Claim:** Intrinsic XAI loss (aligning Grad-CAM with expert masks during training) simultaneously improves accuracy and explainability.
- **Mechanism:** The composite loss L = L_cls + λL_xai optimizes classification while penalizing misalignment between Grad-CAM heatmaps (from first convolutional layer) and expert segmentation masks via MSE. This embeds explainability directly into weight updates rather than applying post-hoc.
- **Core assumption:** Expert-provided segmentation masks represent ground-truth clinically relevant regions; first-layer Grad-CAM captures spatially meaningful low-level features comparable to masks.
- **Evidence anchors:**
  - [section II-A, Eq. 3]: "L_xai enhances the model's transparency by encouraging alignment between the network-generated heatmaps and the expert-provided masks"
  - [section IV-C, Table I]: Doctor-in-the-Loop achieves MCC=40.30% vs XAI-guide (28.11%) and Segmentation (12.84%) with intermediate fusion
  - [corpus]: Post-hoc XAI methods (Ye et al., She et al.) applied to NSCLC response prediction exist but lack intrinsic explainability comparison.

## Foundational Learning

- **Concept: Grad-CAM (Gradient-weighted Class Activation Mapping)**
  - Why needed here: Core to XAI loss computation; generates spatial heatmaps indicating influential image regions.
  - Quick check question: Can you explain why the first convolutional layer (rather than deeper layers) was selected as the Grad-CAM target in this framework?

- **Concept: Fusion Strategies (Early/Intermediate/Late)**
  - Why needed here: Understanding where and how modalities combine determines feature interaction depth.
  - Quick check question: What is the key difference between late fusion (decision-level) and intermediate fusion (feature-level) in terms of gradient flow during training?

- **Concept: Curriculum Learning / Progressive Refinement**
  - Why needed here: The gradual multi-view approach implements a form of curriculum learning from coarse to fine.
  - Quick check question: Why might training on the global lung region before the lesion improve convergence on small datasets?

## Architecture Onboarding

- **Component map:**
  - CT Encoder: DenseNet169 (3D CNN) → feature vector z_i
  - Clinical Encoder: MLP → feature vector z_c
  - Fusion Module: Concatenation([z_i, z_c]) → MLP → softmax output
  - XAI Module: Grad-CAM (first conv layer) → heatmap H → MSE loss vs mask M_t

- **Critical path:**
  1. Pre-train CT model: Step 0 (global, L_cls) → Step 1 (lung mask, L_cls+L_xai) → Step 2 (lesion mask, L_cls+L_xai)
  2. Train clinical model independently (L_cls only)
  3. Fuse: Concatenate encoders, train end-to-end with composite loss L

- **Design tradeoffs:**
  - First-layer vs deeper Grad-CAM: First layer captures edges/textures (more mask-comparable); deeper layers capture abstract features (potentially more semantically meaningful but less spatially aligned).
  - λ=1 was empirically selected; higher λ prioritizes explainability at potential accuracy cost.
  - DenseNet169 chosen for lung cancer track record; other architectures untested.

- **Failure signatures:**
  - MCC near 0 or negative (Table I, Segmentation late fusion: -23.27%): model predictions inversely correlated with labels—check data leakage or severe class imbalance issues.
  - Grad-CAM diffuse/unfocused: XAI loss may be underweighted (λ too low) or masks misaligned with CT coordinates.
  - Large gap between train and validation accuracy: Overfitting likely; increase augmentation or reduce model capacity.

- **First 3 experiments:**
  1. **Baseline check:** Train unimodal CT and clinical models separately; verify clinical model (ACC~67%) outperforms CT (ACC~62%) as reported.
  2. **Fusion ablation:** Implement early, intermediate, and late fusion with identical training configs; confirm intermediate fusion achieves highest MCC (~40%).
  3. **XAI loss sweep:** Vary λ ∈ {0.1, 0.5, 1.0, 1.5, 2.0} and observe accuracy vs. heatmap alignment quality on validation set.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the Multimodal Doctor-in-the-Loop framework maintain its predictive performance and explainability when validated on larger, multi-center datasets?
  - Basis in paper: [explicit] The authors state, "Future work should explore further validation of our method on larger, multicentric datasets to improve generalizability."
  - Why unresolved: The current study is limited to a single-center dataset of only 100 patients, which restricts the generalizability of the deep learning model to broader populations with different clinical protocols.
  - What evidence would resolve it: Reporting accuracy, AUC, and MCC metrics from external validation cohorts collected from diverse geographical and clinical institutions.

- **Open Question 2:** Can the integration of longitudinal patient data enhance the predictive accuracy and robustness of the model compared to the current single-timepoint approach?
  - Basis in paper: [explicit] The conclusion suggests, "Additional research could investigate the integration of longitudinal patient data to enhance predictive accuracy and robustness."
  - Why unresolved: The current framework relies on a static "snapshot" of clinical and imaging data acquired before treatment, potentially missing dynamic changes in tumor behavior over time.
  - What evidence would resolve it: A comparative study evaluating the current model against a modified architecture that ingests time-series imaging and clinical logs.

- **Open Question 3:** Is the proposed framework transferable to other clinical endpoints or cancer subtypes without significant architectural modifications?
  - Basis in paper: [explicit] The authors propose, "expanding our approach to other clinical endpoints or cancer subtypes could extend its applicability in personalized oncology."
  - Why unresolved: The specific loss functions and segmentation masks (lung/lesion) are tailored to NSCLC and pathological response prediction.
  - What evidence would resolve it: Demonstration of the framework's successful application and performance stability on datasets involving different cancer types (e.g., breast or rectal cancer) or targets (e.g., survival prediction).

## Limitations

- Small sample size (n=100) limits generalizability; results may not hold on larger or external cohorts
- Lack of direct ablation for Grad-CAM layer selection (first conv layer vs deeper layers)
- No comparison to purely radiomic or clinical baseline models
- Grad-CAM-XAI alignment assumes masks are perfectly aligned with CT coordinates

## Confidence

- **High confidence**: Intermediate fusion outperforms early/late fusion (consistent with multimodal literature)
- **Medium confidence**: Gradual multi-view learning improves performance (novel approach, limited ablation)
- **Medium confidence**: Intrinsic XAI loss improves both accuracy and explainability (strong ablation vs XAI-guide, but λ tuning not fully explored)

## Next Checks

1. Implement ablation study comparing first-layer vs deeper-layer Grad-CAM for XAI loss alignment
2. Test model on external NSCLC dataset or k-fold with varying random seeds to assess robustness
3. Compare against purely radiomic/clinical baselines to quantify multimodal benefit