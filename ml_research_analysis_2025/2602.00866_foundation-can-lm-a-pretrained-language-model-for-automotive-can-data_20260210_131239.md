---
ver: rpa2
title: 'Foundation CAN LM: A Pretrained Language Model For Automotive CAN Data'
arxiv_id: '2602.00866'
source_url: https://arxiv.org/abs/2602.00866
tags:
- data
- signals
- across
- foundation
- decoded
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Foundation CAN LM: A Pretrained Language Model For Automotive CAN Data

## Quick Facts
- arXiv ID: 2602.00866
- Source URL: https://arxiv.org/abs/2602.00866
- Authors: Akiharu Esashi; Pawissanutt Lertpongrujikorn; Justin Makino; Yuibi Fujimoto; Mohsen Amini Salehi
- Reference count: 20
- One-line primary result: Foundation CAN LM shows cross-task transferability on automotive CAN data, outperforming CNN baselines in multiclass POI classification and matching GLM in binary collision detection.

## Executive Summary
Foundation CAN LM introduces a BERT-style foundation model for automotive CAN bus data, addressing the challenge of heterogeneous discrete and continuous signals through a unified tokenization framework. The model is pretrained on decoded CAN signals from ~10,000 vehicles using masked language modeling, then fine-tuned for downstream tasks including binary collision detection and multiclass point-of-impact classification. Results demonstrate that a single pretrained backbone can adapt effectively across diverse predictive tasks, validating the foundation modeling paradigm for automotive time-series data.

## Method Summary
The approach converts heterogeneous CAN signals into a bounded vocabulary through outlier handling, empirical normalization, and fixed-bin discretization, with special tokens for missing values and new sessions. A BERT-style encoder is pretrained via MLM on 450-token sequences (10 timesteps × 44 features) from ~19B tokens of fleet data. Full-parameter fine-tuning with class-weighted loss adapts the model to binary collision detection and 8-class point-of-impact tasks, outperforming CNN baselines and matching GLM performance in most settings.

## Key Results
- Binary collision detection: Foundation model matches GLM baseline (F1 54.2%) under balanced conditions
- Multiclass POI classification: Achieves 27.0% macro-F1 and 32.6% weighted-F1, outperforming CNN baseline by 3-5%
- Extreme imbalance (100:1): Foundation model trails GLM baseline by 11% F1, indicating rare-event detection limitations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A unified tokenization scheme can convert mixed discrete–continuous CAN signals into a bounded, reproducible vocabulary suitable for transformer pretraining.
- Mechanism: Continuous signals undergo outlier handling → empirical min–max normalization → fixed-bin discretization (bins calibrated by temporal variation ratio rᵢ). Discrete enumerations map one-to-one to tokens; symbolic identifiers (VIN, trip ID) become meta-tokens (<NEW_CAR>, <NEW_TRIP>). The resulting unified vocabulary (~1,420 tokens) enables consistent representation across datasets.
- Core assumption: Empirical operating ranges estimated from fleet data generalize to unseen vehicles and conditions.
- Evidence anchors:
  - [section] "A naïve one-to-one mapping of continuous values would produce an unbounded, non-reproducible vocabulary. To address this, we develop a unified tokenization framework... employing a fixed, predefined binning and enumeration schema."
  - [section] "Values outside these bounds are replaced with a dedicated <OUTLIER> token, predefined invalid entries are replaced with <ERROR>, and <NULL> for missing values."
- Break condition: If sensor ranges shift significantly across vehicle generations or OEMs without recalibration, token boundaries become misaligned and transfer degrades.

### Mechanism 2
- Claim: MLM pretraining on decoded CAN sequences learns cross-feature and cross-temporal dependencies that transfer to downstream tasks.
- Mechanism: The BERT-style encoder masks 15% of tokens and learns to reconstruct them using bidirectional context. Since tokens are serialized as <TS> + 44 features per timestep across 10 timesteps (450 tokens), attention can capture both within-moment feature correlations (e.g., speed–acceleration) and across-time dynamics.
- Core assumption: The MLM reconstruction objective forces learning of representations relevant to downstream classification tasks (collision detection, impact localization).
- Evidence anchors:
  - [section] "Tokens corresponding to semantically related signals (e.g., speed and acceleration) form coherent clusters as well as moderate intra-clusters (e.g., speed 0 - speed 100), indicating successful contextual learning."
  - [section] "The model learns to reconstruct masked tokens using bidirectional context, capturing dependencies across temporal and semantic dimensions."
- Break condition: If downstream tasks require dynamics at timescales finer than 1 Hz / 10-second windows, pretraining may not encode sufficient resolution.

### Mechanism 3
- Claim: A single pretrained backbone can generalize across heterogeneous downstream tasks without task-specific architecture changes.
- Mechanism: Full-parameter fine-tuning updates all 50M encoder weights alongside a new classification head. The pretrained representations provide a shared initialization that encodes general CAN dynamics; fine-tuning specializes to task-specific label structures (binary vs. 8-class).
- Core assumption: Pretraining corpus diversity (44 signals, ~10K vehicles, 9 days) is sufficient to capture transferable patterns despite lacking seasonal/environmental variation.
- Evidence anchors:
  - [abstract] "Our results show that one pretrained CAN model can adapt effectively to diverse predictive tasks, validating that the foundation modeling paradigm, proven in NLP and CV, also holds for CAN data."
  - [section] "In the multi-class point-of-impact task... the foundation model attains a macro-F1 of 27.0% and a weighted-F1 of 32.6%, outperforming the CNN baseline by ~3% and ~5%."
- Break condition: Under extreme class imbalance (100:1), pretrained representations underperform hand-engineered baselines (F1 54.2% vs. 65.3%), suggesting limited rare-event sensitivity without specialized fine-tuning.

## Foundational Learning

- Concept: **Masked Language Modeling (MLM)**
  - Why needed here: CAN signals have no explicit "next token" predictability like text; MLM enables bidirectional context learning without autoregressive assumptions.
  - Quick check question: Can you explain why 15% masking with 80/10/10 replacement is used rather than 100% mask tokens?

- Concept: **Quantization/Discretization for Continuous Signals**
  - Why needed here: Transformers require discrete token inputs; continuous automotive signals must be binned reproducibly without losing discriminative information.
  - Quick check question: How does the temporal variation ratio rᵢ determine bin count for a given feature?

- Concept: **Fine-tuning vs. Feature Extraction**
  - Why needed here: The paper uses full-parameter fine-tuning; understanding this choice is critical for adaptation to new tasks with limited labels.
  - Quick check question: What is the trade-off between freezing encoder weights vs. full fine-tuning when adapting to a severely imbalanced downstream task?

## Architecture Onboarding

- Component map: Input Layer (450-token sequences) → Tokenizer (unified vocabulary) → Encoder (9-layer BERT-style Transformer) → Pretraining Head (MLM) → Fine-tuning Head (task-specific classification)

- Critical path:
  1. Decode raw CAN → 44 features at 1 Hz
  2. Apply feature-specific normalization using empirical bounds
  3. Quantize continuous signals / enumerate discrete signals
  4. Serialize into 450-token sequences with <TS> markers
  5. Pretrain with MLM until loss convergence
  6. Fine-tune on labeled downstream data

- Design tradeoffs:
  - 1 Hz sampling vs. higher rates (5 Hz proposed for future): loses fine-grained dynamics but reduces sequence length
  - 10-second window: limits long-range dependency capture
  - 50M parameters: ablation shows no gain from 20M→100M, suggesting data complexity bottleneck

- Failure signatures:
  - Extreme class imbalance (100:1): F1 drops 11% below GLM baseline
  - Seasonal/environmental shifts: 9-day pretraining corpus lacks diversity
  - Out-of-distribution vehicles: empirical normalization bounds may not transfer

- First 3 experiments:
  1. **Tokenization validation**: Verify vocabulary coverage on a held-out vehicle subset; check <OUTLIER> token frequency to detect range mismatches.
  2. **Pretraining convergence probe**: Plot MLM loss and embedding clustering (t-SNE) at checkpoints; confirm semantically related signals cluster before fine-tuning.
  3. **Baseline comparison on binary task**: Replicate the 10:1 and 100:1 imbalance experiments; if 100:1 gap persists, experiment with class-weighted loss or focal loss during fine-tuning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does extending the input temporal window beyond 10 seconds (450 tokens) improve capture of long-range dependencies in CAN signal modeling?
- Basis in paper: [explicit] Ablation study states future directions include "extending the temporal window to capture long-range dependencies."
- Why unresolved: Current experiments used fixed 10-second windows; model scaling alone did not improve convergence, suggesting data structure rather than capacity is the bottleneck.
- What evidence would resolve it: Pretraining with longer windows (e.g., 30–60 seconds) showing improved cross-task transfer or MLM convergence beyond the current 10k-step plateau.

### Open Question 2
- Question: Does increasing the sampling rate from 1 Hz to 5 Hz improve the model's ability to encode subtle temporal dynamics for downstream tasks?
- Basis in paper: [explicit] Ablation study proposes "increasing the sampling rate from 1 Hz to 5 Hz may reveal finer temporal dynamics, improving transferability across downstream tasks."
- Why unresolved: All experiments resampled signals to 1 Hz; higher-frequency dynamics were discarded during preprocessing.
- What evidence would resolve it: Comparing pretrained models at 1 Hz vs. 5 Hz on tasks requiring fine-grained event discrimination (e.g., collision detection under severe imbalance).

### Open Question 3
- Question: Does pretraining on more diverse driving corpora (weather, seasonal, regional conditions) improve generalization to underrepresented contexts?
- Basis in paper: [explicit] Limitations section notes the corpus "does not capture longer-term seasonal or environmental variability such as snow, rain, or regional driving conditions" and "Extending pretraining across longer time horizons would reduce seasonality bias."
- Why unresolved: Pretraining used only 9 days of data from a single OEM source.
- What evidence would resolve it: Evaluation on held-out datasets containing adverse weather or seasonal conditions, comparing models pretrained on narrow vs. diverse corpora.

### Open Question 4
- Question: What architectural or training modifications are needed to improve detection of ultra-rare events (100:1 imbalance) without task-specific feature engineering?
- Basis in paper: [inferred] Results show the foundation model trails the GLM baseline by ~11% F1 under 100:1 imbalance, and authors note "the current encoder under-represents fine-grained dynamics needed for extremely rare-event detection under standard fine-tuning."
- Why unresolved: Standard MLM pretraining and class-weighted fine-tuning were insufficient for extreme imbalance scenarios.
- What evidence would resolve it: Systematic comparison of rare-event detection performance across different pretraining objectives, masking strategies, or fine-tuning techniques designed for long-tail distributions.

## Limitations

- Extreme class imbalance (100:1) causes 11% F1 drop vs. GLM baseline, indicating rare-event detection limitations
- 9-day pretraining corpus lacks seasonal and environmental diversity, limiting generalization
- Empirical normalization bounds may not transfer across OEMs or vehicle generations without recalibration

## Confidence

- **High confidence**: The unified tokenization framework successfully converts heterogeneous CAN signals into a bounded vocabulary suitable for transformer pretraining.
- **Medium confidence**: MLM pretraining learns cross-feature and cross-temporal dependencies that transfer to downstream tasks.
- **Medium confidence**: A single pretrained backbone can generalize across heterogeneous downstream tasks.

## Next Checks

1. **Extreme imbalance validation**: Replicate the 100:1 class imbalance experiment with focal loss or class-weighted fine-tuning. If the 11% F1 gap persists, this confirms the need for specialized rare-event fine-tuning strategies beyond standard foundation pretraining.

2. **Temporal resolution ablation**: Repeat pretraining and fine-tuning with 5 Hz sampling instead of 1 Hz (as suggested in ablation). Measure downstream task performance to determine if higher temporal resolution improves collision detection or POI classification, particularly for fine-grained dynamics.

3. **Cross-OEM generalization test**: Apply the pretrained model to CAN data from a different OEM or vehicle generation. Measure tokenization coverage (<OUTLIER> token frequency) and downstream task performance to quantify the limits of empirical normalization bounds and assess the need for OEM-specific calibration.