---
ver: rpa2
title: 'RM-PoT: Reformulating Mathematical Problems and Solving via Program of Thoughts'
arxiv_id: '2502.12589'
source_url: https://arxiv.org/abs/2502.12589
tags:
- problem
- problems
- reasoning
- original
- solve
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes RM-PoT, a three-stage framework designed to
  improve the mathematical reasoning performance of large language models (LLMs).
  The framework addresses the vulnerability of LLMs to surface-level variations in
  mathematical problem formulations by integrating problem reformulation (RM), code-aided
  reasoning (PoT), and domain-aware few-shot learning.
---

# RM-PoT: Reformulating Mathematical Problems and Solving via Program of Thoughts

## Quick Facts
- arXiv ID: 2502.12589
- Source URL: https://arxiv.org/abs/2502.12589
- Reference count: 0
- Primary result: RM-PoT improves mathematical reasoning accuracy on GSM8K, AQuA, and SVAMP datasets compared to CoT, SC, and PoT baselines.

## Executive Summary
This paper proposes RM-PoT, a three-stage framework designed to improve the mathematical reasoning performance of large language models (LLMs). The framework addresses the vulnerability of LLMs to surface-level variations in mathematical problem formulations by integrating problem reformulation (RM), code-aided reasoning (PoT), and domain-aware few-shot learning. RM-PoT first reformulates input problems into diverse surface forms to reduce structural bias, retrieves semantically aligned examples from a domain-specific question bank for contextual guidance, and finally generates executable Python code to ensure precise computation. Experiments on GSM8K, AQuA, and SVAMP datasets demonstrate that RM-PoT outperforms baseline methods such as Chain-of-Thought (CoT), self-consistency (SC), and Program of Thoughts (PoT) in accuracy. The results highlight the effectiveness of reformulation and code generation in enhancing LLM robustness and accuracy in solving mathematical problems.

## Method Summary
RM-PoT is a three-stage framework for mathematical problem solving. First, it reformulates each input problem into K diverse surface variants (K=4) using either naive or in-context prompts to reduce structural bias. Second, it classifies the problem into a domain and retrieves top-5 semantically aligned (question, solution) pairs from a pre-constructed domain-specific question bank using sentence-BERT embeddings to provide contextual guidance. Third, for each reformulated problem, it generates executable Python code implementing the solution, executes the code in a sandbox, and aggregates N=16 total reasoning paths via majority voting. The framework uses GLM-4-9B with temperature=0.7, top-p=0.8, and top-k=3, and requires no fine-tuning.

## Key Results
- RM-PoT achieves higher accuracy than CoT, SC, and PoT baselines on GSM8K, AQuA, and SVAMP datasets.
- Reformulation increases solve rates by reducing structural bias in the original problem formulation (e.g., from 43.8% to 81.3% in a concrete example).
- In-Context reformulation consistently outperforms naive reformulation across K=1,2,4 settings.
- Code execution eliminates arithmetic errors that pure natural-language CoT produces (e.g., 21.90/1.60 ≠ 20.741 in CoT vs. correct Python execution in PoT).

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Generating diverse surface forms of a mathematical problem improves solve rates by reducing structural bias in the original formulation.
- **Mechanism:** The LLM generates K paraphrased versions of the input problem. Each formulation may activate different reasoning paths. A voting mechanism aggregates answers across reformulations, making the final answer robust to any single poorly-structured surface form.
- **Core assumption:** The correct solution is invariant to surface form; the underlying mathematical structure can be recovered from at least one reformulation even if obscured in the original.
- **Evidence anchors:**
  - [abstract] "reformulates the input problem into diverse surface forms to reduce structural bias"
  - [section 4.2, Figure 5] A concrete example shows solve rate increasing from 43.8% to 81.3% after reformulation.
  - [corpus] Neighbor papers on paraphrase-based robustness are sparse for this specific mechanism; corpus evidence is weak.
- **Break condition:** If all K reformulations share the same misleading linguistic pattern (e.g., consistent ambiguity in "original price" vs. "discounted price"), voting cannot recover the correct interpretation.

### Mechanism 2
- **Claim:** Decoupling computation from reasoning via executable Python code improves numerical accuracy compared to pure natural-language chain-of-thought.
- **Mechanism:** The LLM generates Python code that encodes the reasoning steps. The code is executed by an interpreter, removing floating-point or arithmetic errors that LLMs commonly produce. The result is stored in a fixed variable name for extraction.
- **Core assumption:** The LLM correctly translates the problem semantics into syntactically valid and logically correct Python code.
- **Evidence anchors:**
  - [abstract] "generates executable Python code to ensure precise computation"
  - [section 1, Figure 1(a)] GPT-4 fails direct computation of 124712948×91274182; Python execution yields the correct result.
  - [section 5, Figure 7] CoT produces both a reasoning error (rearranging terms incorrectly) and a computation error (21.90/1.60 ≠ 20.741). PoT avoids the computation error.
- **Break condition:** If the generated code contains logical errors (e.g., misinterpreting "original price" as "discounted price"), execution will produce a wrong answer despite no runtime errors.

### Mechanism 3
- **Claim:** Retrieving semantically aligned, domain-specific few-shot examples improves both reformulation quality and solution accuracy by priming the LLM with relevant reasoning patterns.
- **Mechanism:** A lightweight embedding model classifies the problem into a domain (e.g., algebra, arithmetic). The system retrieves the top-5 (question, solution) pairs from a pre-constructed domain bank using cosine similarity over sentence-BERT embeddings. These examples are injected into the prompt.
- **Core assumption:** The embedding space captures semantic similarity such that retrieved examples are genuinely relevant and transferable to the target problem.
- **Evidence anchors:**
  - [section 1] "retrieve five semantically aligned examples from a pre-constructed domain-specific question bank to provide contextual guidance"
  - [section 4.4, Table 2] In-Context reformulation consistently outperforms naive reformulation across K=1,2,4 (e.g., AQuA: 67.6 → 69.4 → 72.2 vs. 66.0 → 67.7 → 69.1).
  - [corpus] Weak direct evidence; neighbor papers focus on program-based reasoning rather than retrieval-augmented few-shot prompting.
- **Break condition:** If the question bank lacks coverage for a problem's domain, retrieved examples may be tangentially related, introducing noise rather than signal.

## Foundational Learning

- **Concept:** Chain-of-Thought (CoT) prompting
  - **Why needed here:** RM-PoT builds on and compares against CoT. Understanding how CoT decomposes problems into intermediate reasoning steps is essential to see what PoT adds (explicit code) and where CoT alone fails (computation errors, sensitivity to phrasing).
  - **Quick check question:** Given a word problem, can you write out a CoT-style solution with explicit intermediate steps before the final answer?

- **Concept:** Self-Consistency (SC) over multiple reasoning paths
  - **Why needed here:** RM-PoT uses a voting mechanism over N total reasoning paths generated from K reformulated problems. SC is the baseline for this aggregation strategy.
  - **Quick check question:** If you sample 16 reasoning paths and 9 yield answer A while 7 yield answer B, what does SC select and why might this fail?

- **Concept:** Dense vector retrieval (embedding + cosine similarity)
  - **Why needed here:** The domain-aware few-shot module retrieves examples using sentence-BERT embeddings and cosine similarity. Understanding this retrieval pipeline is necessary to debug or extend the question bank.
  - **Quick check question:** For two sentences, can you explain why cosine similarity might rate them as highly similar even if they address different mathematical domains?

## Architecture Onboarding

- **Component map:** Input → Reformulation (K versions) → [For each: retrieve examples → PoT code generation → execution] → Aggregate N answers → Majority vote → Output

- **Critical path:** Input → Reformulation (K versions) → [For each: retrieve examples → PoT code generation → execution] → Aggregate N answers → Majority vote → Output

- **Design tradeoffs:**
  - **K vs. N:** Total reasoning paths N is fixed (e.g., 16). Increasing K (more reformulations) diversifies problem surface forms but reduces paths per reformulation, increasing variance per form. The paper finds K=2–4 optimal depending on dataset difficulty.
  - **Naive vs. In-Context Reformulation:** In-Context uses manually selected (original, reformulated) pairs with large solve-rate margins as few-shot examples. Higher quality but requires curation.
  - **Code execution overhead:** PoT requires a secure sandboxed environment. Latency increases compared to pure CoT.

- **Failure signatures:**
  - **Consistent misinterpretation:** All K reformulations misread the same semantic cue (e.g., "original price" vs. "discounted price"), leading to unanimous wrong answers.
  - **Code generation errors:** Syntax errors, undefined variables, or logic bugs in generated Python code produce execution failures or silent wrong answers.
  - **Retrieval mismatch:** Poor domain classification or sparse question bank leads to irrelevant few-shot examples, degrading reformulation and solution quality.
  - **Voting tie:** For small N or high variance, no clear majority emerges; a tie-breaking rule is required (paper does not specify).

- **First 3 experiments:**
  1. **Baseline Reproduction:** Implement vanilla PoT on GSM8K using GLM-4-9B with the paper's generation configs (T=0.7, Top-p=0.8, Top-k=3, N=16). Verify that accuracy is close to the reported ~78.9%.
  2. **Reformulation Ablation:** Add naive reformulation with K=4. Measure accuracy delta and compare to Table 1. Inspect cases where reformulation degrades performance to understand failure modes.
  3. **In-Context Reformulation Pilot:** Curate 5–10 (original, reformulated) pairs from AQuA with observed solve-rate improvements. Run In-Context reformulation with K=4 on a held-out subset. Compare to naive reformulation and analyze whether gains are consistent or problem-type dependent.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the mechanistic reasons that naive reformulation improves performance, and why do specific reformulations sometimes lower solve rates compared to the original problem?
- **Basis in paper:** [explicit] The paper states that the "underlying reasons why naive reformulation can enhance performance are still unclear" and notes that "in some cases, the reformulated problem even exhibits a lower solve rate than the original."
- **Why unresolved:** The authors observe the empirical phenomenon where surface form changes affect reasoning (e.g., grasping "original price" in one form or not another), but they have not identified the specific model internals or attention patterns causing this discrepancy.
- **What evidence would resolve it:** Mechanistic interpretability studies analyzing attention heads and token importance in the original versus reformulated problems to identify structural biases or reasoning shortcuts.

### Open Question 2
- **Question:** Can the RM-PoT framework be generalized to non-textual mathematical domains like geometry and calculus using diagram-to-code mappings?
- **Basis in paper:** [explicit] The Conclusion suggests extending the method to "geometry and calculus by curating domain-specific banks with diagram-to-code mappings" as a direction for future work.
- **Why unresolved:** The current framework relies heavily on text-based reformulation and Python code generation for arithmetic/algebraic word problems; visual input processing and symbolic calculus translation remain unimplemented.
- **What evidence would resolve it:** Successful application of RM-PoT to geometry benchmarks (requiring visual parsing) or calculus datasets, demonstrating that the reformulation and retrieval logic holds for non-word-problem structures.

### Open Question 3
- **Question:** How does dynamically adjusting the number of few-shot examples ($K$) based on problem complexity affect the efficiency and accuracy of the RM-PoT framework?
- **Basis in paper:** [explicit] The paper lists "Adaptive Retrieval: Dynamically adjust the number of few-shot examples (K) based on problem complexity" as a method for improvement.
- **Why unresolved:** The current experiments use a fixed number of examples and reasoning paths; the interaction between problem difficulty, the required number of shots, and computational cost has not been optimized.
- **What evidence would resolve it:** Ablation studies showing that an adaptive $K$ reduces token usage/inference time while maintaining or exceeding the accuracy of the fixed-$K$ baseline.

## Limitations
- The effectiveness of RM-PoT depends on the quality and coverage of the domain-specific question bank, which is not released.
- Reformulation can occasionally lower solve rates compared to the original problem, and the paper does not fully explain why.
- The framework assumes code execution succeeds for all generated programs, but runtime errors or logical bugs could silently reduce effective reasoning paths.

## Confidence
- **High Confidence:** The core mechanism of using executable Python code to avoid arithmetic errors is well-demonstrated through concrete examples and is grounded in known LLM limitations.
- **Medium Confidence:** The reformulation and voting strategy is supported by controlled experiments, but the in-context variant's performance gains are based on a small curated example set not released to readers.
- **Medium Confidence:** The retrieval module improves accuracy, but the exact contribution of domain classification and few-shot examples versus reformulation alone is not isolated.

## Next Checks
1. **Retrieval Ablation Test:** Run RM-PoT with and without the retrieval module on GSM8K while keeping reformulation fixed. Measure the delta in accuracy to quantify the contribution of few-shot examples.
2. **Reformulation Robustness Audit:** For a subset of problems where naive reformulation degrades performance, log and categorize the types of semantic drift introduced. Determine if certain problem structures are more vulnerable.
3. **Execution Failure Analysis:** Instrument the code execution step to log syntax errors, runtime exceptions, and incorrect variable naming. Compute the percentage of reasoning paths lost to execution failures and test fallback strategies (e.g., parser-based answer extraction).