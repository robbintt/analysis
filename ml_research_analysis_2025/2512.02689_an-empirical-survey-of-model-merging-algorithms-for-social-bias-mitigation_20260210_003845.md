---
ver: rpa2
title: An Empirical Survey of Model Merging Algorithms for Social Bias Mitigation
arxiv_id: '2512.02689'
source_url: https://arxiv.org/abs/2512.02689
tags:
- bias
- linear
- slerp
- karcher
- nuslerp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper empirically compares seven model merging algorithms
  for mitigating social bias in large language models (LLMs). The authors use task
  arithmetic to create bias-inverted models and then merge them with pre-trained models
  using Linear, Karcher Mean, SLERP, NuSLERP, TIES, DELLA, and Nearswap methods.
---

# An Empirical Survey of Model Merging Algorithms for Social Bias Mitigation

## Quick Facts
- arXiv ID: 2512.02689
- Source URL: https://arxiv.org/abs/2512.02689
- Reference count: 40
- Key outcome: Model merging creates a trade-off between bias reduction and downstream performance, with SLERP at α=0.2-0.3 emerging as the most balanced approach

## Executive Summary
This paper conducts an empirical comparison of seven model merging algorithms for mitigating social bias in large language models. The authors systematically evaluate Linear, Karcher Mean, SLERP, NuSLERP, TIES, DELLA, and Nearswap methods by first creating bias-inverted models through task arithmetic and then merging them with pre-trained models. Experiments across 13 models from GPT, LLAma, and Qwen families reveal that while all methods can reduce social bias, they also introduce trade-offs with downstream task performance. The study identifies SLERP with moderate interpolation weights as the most effective approach for balancing bias mitigation with preserved linguistic capabilities.

## Method Summary
The study employs task arithmetic to create bias-inverted models by subtracting biased model parameters from debiased counterparts. These inverted models are then merged with original pre-trained models using seven different merging algorithms: Linear interpolation, Karcher Mean (Riemannian geometry-based), SLERP (spherical interpolation), NuSLERP (normalized SLERP), TIES (Task-specific Iterative Elimination and Selection), DELLA (Diffusion-based Latent Space Alignment), and Nearswap (nearest-neighbor based swapping). The merged models are evaluated on SuperGLUE benchmark tasks to measure downstream performance degradation while also quantifying social bias reduction through unspecified bias evaluation metrics.

## Key Results
- Linear, SLERP, and Nearswap consistently reduce social bias while maintaining better downstream performance compared to other methods
- Excessive debiasing through aggressive merging weights degrades important linguistic abilities, particularly in reading comprehension and causal reasoning
- SLERP with interpolation weights α=0.2-0.3 provides the optimal balance between bias reduction and performance preservation

## Why This Works (Mechanism)
Assumption: Model merging algorithms operate by finding intermediate parameter configurations between biased and debiased models in the weight space. Linear interpolation creates convex combinations of parameters, while geometric methods like SLERP preserve directional relationships on the hypersphere. The effectiveness of SLERP at moderate weights (α=0.2-0.3) suggests that social bias manifests as localized directional components in parameter space that can be partially removed without disrupting the overall parameter distribution governing general capabilities. The trade-off emerges because bias-mitigating directions often overlap with directions encoding general linguistic knowledge, creating an unavoidable tension between complete bias removal and functional performance.

## Foundational Learning
Unknown: The paper does not explicitly discuss how the merging algorithms relate to fundamental principles of representation learning, parameter space geometry, or the nature of social bias in neural network embeddings. Understanding whether bias manifests as orthogonal, parallel, or overlapping directions in weight space relative to general capabilities could inform more targeted mitigation strategies.

## Architecture Onboarding
Unknown: The paper does not provide specific guidance for applying these merging techniques to different model architectures beyond the tested GPT, LLaMA, and Qwen families. Questions remain about whether architectural differences (transformer depth, attention mechanisms, embedding sizes) affect the efficacy of different merging algorithms or the optimal interpolation weights.

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly identify open questions or directions for future research. However, implicit questions include: How do different bias types (gender, racial, cultural) respond to various merging approaches? Can merging algorithms be adapted to target specific bias dimensions while preserving other capabilities? What is the relationship between bias reduction magnitude and downstream performance degradation across different task domains?

## Limitations
- Limited empirical scope with only 13 models from three families may not generalize to broader LLM landscape
- Task arithmetic assumption of linear separability for bias features may not hold for complex social biases
- SuperGLUE benchmark may not capture all relevant aspects of linguistic capability degradation
- Lack of transparency regarding specific bias evaluation metrics used makes reproducibility challenging
- No analysis of bias type specificity or whether different social biases respond differently to merging approaches
- Limited exploration of how merging affects model behavior on out-of-distribution prompts or edge cases

## Confidence
- **High confidence**: The general trade-off finding between bias reduction and downstream performance is well-supported across multiple models and methods
- **Medium confidence**: SLERP at α=0.2-0.3 as optimal balance is supported but may be sensitive to evaluation tasks and bias metrics
- **Medium confidence**: Observation about excessive debiasing degrading linguistic abilities is plausible but needs more comprehensive capability assessment

## Next Checks
1. Cross-dataset validation using alternative bias evaluation datasets (StereoSet, CrowS-Pairs) and additional downstream task suites to verify robustness across different evaluation paradigms

2. Fine-grained capability analysis isolating specific linguistic capabilities (logical reasoning, factual recall, creative generation) to identify which abilities are most vulnerable to degradation

3. Temporal generalization test evaluating whether optimal merging weights maintain balance when models are deployed on temporally evolving data distributions with shifting social biases and language usage patterns