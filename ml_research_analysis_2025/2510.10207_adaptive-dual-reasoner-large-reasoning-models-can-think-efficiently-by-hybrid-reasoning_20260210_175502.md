---
ver: rpa2
title: 'Adaptive Dual Reasoner: Large Reasoning Models Can Think Efficiently by Hybrid
  Reasoning'
arxiv_id: '2510.10207'
source_url: https://arxiv.org/abs/2510.10207
tags:
- reasoning
- arxiv
- preprint
- easy
- hard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the inefficiency of long reasoning models (LRMs)
  due to overthinking, which increases computational costs and latency. To solve this,
  the authors propose Adaptive Dual Reasoner (ADR), a model that dynamically switches
  between fast and slow reasoning modes based on contextual complexity.
---

# Adaptive Dual Reasoner: Large Reasoning Models Can Think Efficiently by Hybrid Reasoning

## Quick Facts
- arXiv ID: 2510.10207
- Source URL: https://arxiv.org/abs/2510.10207
- Reference count: 40
- Primary result: Achieves up to 6.1% accuracy gains while reducing reasoning output length by 49.5%–59.3%

## Executive Summary
This paper addresses the inefficiency of long reasoning models (LRMs) due to overthinking, which increases computational costs and latency. The authors propose Adaptive Dual Reasoner (ADR), a model that dynamically switches between fast and slow reasoning modes based on contextual complexity. ADR is trained in two stages: (1) cold-start supervised fine-tuning with hybrid reasoning data constructed via CoT decomposition and rewriting, and (2) reinforcement learning using Entropy-guided Hybrid Policy Optimization (EHPO) that leverages entropy trends for dynamic rollout and difficulty-aware penalties. Experiments on mathematical reasoning benchmarks show ADR outperforms state-of-the-art approaches in balancing reasoning efficiency and accuracy.

## Method Summary
The method consists of two-stage training: first, cold-start supervised fine-tuning on hybrid reasoning data where units are split by entropy and rewritten in CoD-style for easy cases; second, reinforcement learning with EHPO that uses entropy-guided dynamic rollout and difficulty-aware mode control rewards. The model uses special tokens (`<easy>`/`<hard>`) to mark reasoning modes and employs entropy differences as branching signals during generation.

## Key Results
- Achieves up to 6.1% accuracy gains on mathematical reasoning benchmarks
- Reduces reasoning output length by 49.5%–59.3% compared to baselines
- Outperforms state-of-the-art approaches in balancing reasoning efficiency and accuracy

## Why This Works (Mechanism)

### Mechanism 1: Unit-Level Entropy as Complexity Signal
The authors observe that transitions from easy to hard reasoning units exhibit elevated entropy values, using this as a branching probability signal. The core assumption is that token-level entropy correlates with cognitive demand at sub-problem boundaries. Evidence shows transitions from easy to hard mode exhibit higher entropy, consistent with the requirement for deeper exploration.

### Mechanism 2: Hybrid Data Construction via CoT Decomposition
Raw CoT trajectories are split into reasoning units using boundary detection, classified as "hard" if they contain reflection/verification keywords or exhibit high entropy. Easy units are rewritten in Chain-of-Draft style using a teacher model. The core assumption is that keyword-based classification approximates cognitive complexity, though this may be domain-specific.

### Mechanism 3: Difficulty-Aware Mode Control Reward
A reward dynamically penalizes over-use of hard mode on easy problems and under-use on hard problems. The mode control reward uses sample-level correctness rates to determine appropriate mode usage, with R_mode = β + (1-β) × [(N_pass/N) × p_easy + (1 - N_pass/N) × p_hard].

## Foundational Learning

- **Chain-of-Thought (CoT) Reasoning** - Why needed: ADR builds on and modifies CoT trajectories; understanding how CoT decomposes problems is prerequisite to unit-level manipulation. Quick check: Can you explain why CoT improves multi-step reasoning over direct prompting?

- **Reinforcement Learning from Verifiable Rewards (RLVR)** - Why needed: EHPO extends GRPO with custom rewards; familiarity with policy gradient methods is assumed. Quick check: How does GRPO differ from PPO in terms of advantage estimation?

- **Entropy in Language Models** - Why needed: Entropy serves as the core signal for dynamic rollout; understanding token-level entropy and its relationship to model uncertainty is essential. Quick check: What does high next-token entropy indicate about a model's internal state during generation?

## Architecture Onboarding

- **Component map:** Data Pipeline (Raw CoT → Unit Splitter → Entropy/Keyword Classifier → CoD Rewriter → Tagged Hybrid Data) → Cold-Start SFT (Base LRM fine-tuned on hybrid data) → EHPO RL Loop (Policy → Generate with EDR branching → Compute R_format, R_accuracy, R_unit, R_mode → GRPO update) → Entropy Monitor (Tracks H_0 and ΔH for branching decisions)

- **Critical path:** Verify unit splitting produces semantically coherent segments (inspect 50-100 samples manually); confirm keyword list covers domain-specific reflection markers; monitor entropy distribution during SFT to ensure bimodal separation; during RL, log branching frequency and correlate with downstream accuracy.

- **Design tradeoffs:** Coarse vs. fine-grained units (smaller units enable finer control but increase annotation noise); static vs. dynamic rollout (static is simpler but less adaptive); keyword vs. entropy classification (keywords are interpretable but brittle; entropy is continuous but requires calibration).

- **Failure signatures:** Mode collapse (model uses only `<easy>` or only `<hard>` tags → check R_unit reward weighting); excessive branching (training stalls due to high rollout count → reduce α or add entropy normalization cap); keyword drift (hard units lack reflection keywords → re-validate keyword list on generated outputs).

- **First 3 experiments:** Ablate entropy-guided rollout (train ADR without EDR, compare Avg AES to full model); vary unit granularity (compare sentence-level vs. paragraph-level splitting on MATH500 subset); cross-domain transfer (apply ADR trained on math to code reasoning benchmark like HumanEval).

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Entropy calibration across domains may not generalize beyond mathematical reasoning
- Keyword-based unit classification relies on domain-specific reflection markers that may be incomplete
- Training distribution assumptions may not hold for diverse or noisy inference-time challenges

## Confidence
- **High Confidence** - The two-stage training methodology is clearly specified and follows established patterns in LRM optimization
- **Medium Confidence** - The entropy-guided dynamic rollout mechanism is theoretically sound but requires domain-specific validation
- **Low Confidence** - Performance claims depend heavily on specific training configuration and may not transfer to other model architectures without significant hyperparameter tuning

## Next Checks
1. **Cross-Domain Transfer Study** - Apply ADR trained on mathematical reasoning to code generation benchmarks (e.g., HumanEval). Measure whether entropy signals transfer or require domain-specific recalibration, and compare accuracy-efficiency tradeoffs across domains.

2. **Keyword List Validation** - Manually annotate 200 reasoning units from generated outputs to assess keyword-based classification accuracy. Identify missing reflection markers and evaluate whether alternative classification methods improve performance.

3. **Entropy Distribution Analysis** - Analyze entropy distributions across easy/hard units during both training and inference. Verify that entropy differences remain consistent across different problem types and that normalization procedures prevent extreme branching probabilities.