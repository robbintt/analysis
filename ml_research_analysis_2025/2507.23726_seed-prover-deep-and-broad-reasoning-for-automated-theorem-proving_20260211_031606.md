---
ver: rpa2
title: 'Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving'
arxiv_id: '2507.23726'
source_url: https://arxiv.org/abs/2507.23726
tags:
- problems
- proof
- problem
- lean
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Seed-Prover introduces a lemma-style whole-proof reasoning model
  that outperforms previous automated theorem provers by combining iterative refinement
  with test-time scaling strategies. Unlike step-level provers that generate proofs
  line-by-line, Seed-Prover produces entire Lean proofs at once, then refines them
  using compiler feedback, proved lemmas, and self-summarization.
---

# Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving

## Quick Facts
- arXiv ID: 2507.23726
- Source URL: https://arxiv.org/abs/2507.23726
- Reference count: 34
- One-line primary result: Proves 78.1% of formalized IMO problems, achieves 99.6% on MiniF2F, and solves 331/657 problems on PutnamBench

## Executive Summary
Seed-Prover introduces a lemma-style whole-proof reasoning model that outperforms previous automated theorem provers by combining iterative refinement with test-time scaling strategies. Unlike step-level provers that generate proofs line-by-line, Seed-Prover produces entire Lean proofs at once, then refines them using compiler feedback, proved lemmas, and self-summarization. The model employs three inference tiers: light (iterative refinement with 8-16 passes), medium (outer and inner refinement targeting difficult lemmas), and heavy (broad conjecture generation with extensive search). This approach enables Seed-Prover to prove 78.1% of formalized IMO problems, achieve 99.6% on MiniF2F, and solve 331/657 problems on PutnamBench. The system also includes Seed-Geometry, a geometry reasoning engine that outperforms AlphaGeometry 2 by solving 43 IMO-AG-50 problems and the IMO 2025 geometry problem in 2 seconds. Together, these systems fully proved 5 out of 6 IMO 2025 problems, demonstrating the effectiveness of formal verification combined with long chain-of-thought reasoning.

## Method Summary
Seed-Prover uses a lemma-style approach to automated theorem proving, generating entire Lean proofs at once rather than step-by-step. The model employs iterative refinement with compiler feedback, proved lemmas, and self-summarization. Three inference tiers handle problems of varying difficulty: light (8-16 refinement passes), medium (nested refinement for difficult lemmas), and heavy (broad conjecture generation followed by lemma assembly). Seed-Geometry uses a C++ forward-chaining engine with policy-only model and distributed beam search. The system is trained via multi-stage multi-task reinforcement learning using the VAPO framework with binary rewards from Lean compiler verification.

## Key Results
- Proves 78.1% of formalized IMO problems (vs 50.9% for best prior work)
- Achieves 99.6% accuracy on MiniF2F-test benchmark
- Solves 331/657 problems on PutnamBench, fully proving 5 out of 6 IMO 2025 problems
- Seed-Geometry solves 43 IMO-AG-50 problems and the IMO 2025 geometry problem in 2 seconds
- Outperforms AlphaGeometry 2 on geometry reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1: Lemma-Style Modular Proof Construction
- Claim: Generating intermediate lemmas before the main theorem improves proof success rates by enabling modular verification and cross-trajectory knowledge sharing.
- Mechanism: The model first produces `lemma` statements (each independently compilable), then constructs the main `theorem` by applying proved lemmas. Lemmas are stored in a shared pool with metadata (difficulty, dependencies), allowing failed proofs to retain partial progress and different inference paths to combine useful intermediate results.
- Core assumption: Breaking proofs into independently verifiable units exposes granular supervision signals and allows reuse across attempts.
- Evidence anchors:
  - [section 2.2.1]: "lemmas can be processed independently and combined freely; lemmas from different inference trajectories can be combined to address more challenging problems"
  - [section 2.2.1]: "proofs of lemmas may provide inspiration to the model for proving unproved lemmas and the main problem"
  - [corpus]: Neighbor paper "Aristotle" similarly decomposes proofs but uses informal-formal pairs rather than lemma pools
- Break condition: If lemma statements are syntactically invalid or semantically unrelated to the main theorem, the pool accumulates noise without advancing the proof.

### Mechanism 2: Iterative Refinement with Compiler-Grounded Feedback
- Claim: Multi-pass refinement using Lean compiler errors, proved lemmas, and self-summarization outperforms single-pass generation at equivalent compute budgets.
- Mechanism: After each failed proof attempt, the model receives (1) exact compiler error messages, (2) successfully proved lemmas from the current run, and (3) a self-generated summary of previous attempts. This context is fed back as prompt augmentation for the next iteration.
- Core assumption: Compiler feedback provides dense, verifiable supervision that neural models can translate into targeted corrections.
- Evidence anchors:
  - [section 2.2.3]: "our approach randomly incorporates natural language hints... failed lemmas, failed attempts, summaries of previous attempts, and Lean compiler feedback into the prompt"
  - [section 2.2.4 Light]: "iteratively refining proofs using Lean compiler feedback in conjunction with self-summarization can surpass the limits of single-pass inference token budgets"
  - [corpus]: "Solving Formal Math Problems by Decomposition and Iterative Reflection" (neighbor paper) reports similar gains from iterative reflection but without lemma pools
- Break condition: If the model cannot parse or act on compiler errors (e.g., unfamiliar error types), refinement degrades to random search.

### Mechanism 3: Depth-Breadth Tradeoff via Tiered Inference Scaling
- Claim: Allocating inference compute across three tiers (light/medium/heavy) enables solving problems of varying difficulty more efficiently than uniform scaling.
- Mechanism: Light (8-16 refinement passes, ~1-2 hours) handles syntax fixes and minor reasoning adjustments. Medium adds nested refinement on difficult lemmas. Heavy generates thousands of conjectures via a proposer module, proves them independently, then assembles the highest-value lemmas into a final proof over days.
- Core assumption: Difficult problems require breadth (exploring diverse properties) before depth (proving specific lemmas), and the proposer can generate useful conjectures without solving the problem.
- Evidence anchors:
  - [section 2.2.4 Heavy]: "the proposer generates thousands of conjectures... Successfully proved conjectures are moved into the lemma pool... After days of thinking, the lemma pool accumulates several thousand nontrivial math facts"
  - [section 2.2.2]: "this process differs from chain-of-thought reasoning to solve the problem directly; rather, it emphasizes broad exploration of the problem space without committing to a particular approach"
  - [corpus]: No direct corpus comparison; this conjecture-proposer pattern appears novel among neighbors
- Break condition: If the proposer generates low-quality conjectures (syntactically valid but semantically useless), the heavy tier wastes compute without improving solve rates.

## Foundational Learning

- Concept: **Lean 4 proof syntax and tactic basics**
  - Why needed here: Seed-Prover outputs Lean code directly; understanding `lemma`, `theorem`, `have`, and common tactics (`ring`, `linarith`) is necessary to debug model outputs and interpret compiler errors.
  - Quick check question: Can you write a one-line Lean proof that `x + y = y + x` for natural numbers?

- Concept: **Reinforcement learning with binary rewards**
  - Why needed here: Seed-Prover is trained via RL where reward = 1 if the proof compiles, 0 otherwise. Understanding sparse reward RL helps interpret why iterative refinement and lemma decomposition matter.
  - Quick check question: Why might sparse binary rewards fail to shape intermediate proof steps without curriculum or decomposition?

- Concept: **Forward-chaining vs. backward-chaining reasoning**
  - Why needed here: Seed-Geometry uses forward-chaining (derive all facts until closure, then backward-trace dependencies). Understanding this clarifies why the engine needs extensive search and why auxiliary constructions matter.
  - Quick check question: In forward-chaining, how do you know when to stop deriving new facts?

## Architecture Onboarding

- Component map:
  Seed-Prover -> LooKeng -> Lean 4 compiler -> Lemma pool
  Seed-Geometry -> C++ engine -> Policy model -> Beam search

- Critical path:
  1. Problem formalized into Lean statement
  2. Light tier: Single-pass proof → compiler feedback → up to 16 refinement iterations
  3. Medium tier (if unsolved): Identify failed lemmas → run inner refinement (8×8 budget) on each → feed proved lemmas back to outer refinement
  4. Heavy tier (if unsolved): Proposer generates ~5000 conjectures → light inference on each → rank by proof rate/relevance → assemble top lemmas into medium inference on main problem

- Design tradeoffs:
  - **Light vs. Medium**: Light is faster (~2 hours) but cannot decompose difficult lemmas; Medium handles 1000+ line proofs but requires nested refinement loops
  - **Policy-only vs. Actor-Critic for Geometry**: Paper experimented with value models but found they harmed performance under extensive search due to estimation errors; final system uses policy-only with beam search
  - **Step-by-step vs. Whole-sequence auxiliary generation**: Step-by-step beam search significantly outperforms one-shot generation for geometry auxiliaries

- Failure signatures:
  - Proof stalls with repeated syntax errors across iterations → model cannot interpret compiler feedback (check prompt formatting)
  - Lemma pool fills with short/irrelevant lemmas → conjecture proposer generating low-quality properties (tune proposer temperature or filtering)
  - Geometry engine timeout on deep searches → forward-chaining explosion (reduce beam width or add depth limit)
  - Heavy tier runs for days without progress → conjectures not semantically connected to main problem (increase relevance scoring weight)

- First 3 experiments:
  1. **Baseline pass@k comparison**: Run light inference (8×8 refinement) vs. single-pass pass@64 on MiniF2F-test subset to quantify refinement gains.
  2. **Lemma pool ablation**: Disable lemma pool sharing between trajectories on a held-out IMO problem set; measure solve rate drop to validate cross-trajectory knowledge reuse.
  3. **Proposer quality check**: On 10 unsolved problems, manually inspect top-10 ranked conjectures from heavy tier to verify semantic relevance before running full multi-day inference.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the computational cost of the heavy inference setting (requiring "days of thinking" and thousands of conjectures) be substantially reduced while maintaining problem-solving capability?
- Basis in paper: [explicit] The authors state heavy inference "begins with a conjecture pool... initially, the proposer generates thousands of conjectures (by default 5000)" and "After days of thinking, the lemma pool accumulates several thousand nontrivial math facts."
- Why unresolved: The trade-off between search breadth/depth and computational efficiency is not analyzed; no ablations on reduced conjecture budgets are provided.
- What evidence would resolve it: Ablation studies showing solve rates under reduced time/conjecture budgets, or methods that achieve comparable results with significantly fewer resources.

### Open Question 2
- Question: What architectural or training modifications could improve performance on combinatorics problems, where Seed-Prover achieves only 7/14 compared to 72/85 on algebra?
- Basis in paper: [explicit] "relative to other benchmarks, our model still struggles with proving combinatorics problems" and achieves 7/14 on combinatorics versus 72/85 algebra and 42/55 number theory.
- Why unresolved: The paper does not analyze the specific failure modes for combinatorics or propose targeted improvements for this domain.
- What evidence would resolve it: Analysis of combinatorics failure cases, domain-specific training interventions, or architectural modifications that close the performance gap.

### Open Question 3
- Question: Can value models be effectively incorporated into Seed-Geometry's search without harming performance, contrary to the finding that they "could harm the general performance due to large errors in value estimation"?
- Basis in paper: [explicit] "We considered training two models in an actor–critic setup... However, we note from preliminary experiments that a single Seed model serving as the policy would suffice, contradicting the design of both a policy model and a value model in existing work."
- Why unresolved: The value model was abandoned after preliminary experiments; no analysis of why value estimation errors occur or how to mitigate them.
- What evidence would resolve it: Diagnostic analysis of value prediction errors, improved value training methods, or hybrid approaches that selectively use value estimates.

## Limitations

- Model architecture and training details are not disclosed, making it impossible to assess whether improvements stem from architectural innovations or scale advantages.
- Heavy inference tier's success relies on a conjecture-proposer module whose design and quality filtering are underspecified, raising concerns about reproducibility.
- The broad-baseline comparisons are less convincing than the narrow-baseline analyses, suggesting potential overfitting to specific problem types.
- The cost of heavy inference (days of compute per problem) is not contextualized against the solve rates achieved.

## Confidence

- **Lemma-style modular proof construction (Medium)**: High confidence
- **Iterative refinement with compiler feedback (High)**: High confidence
- **Tiered inference scaling (Medium)**: Medium confidence
- **Seed-Geometry performance claims (Low)**: Low confidence

## Next Checks

1. **Architecture disclosure validation**: Contact authors to obtain model specifications and training details; without this, claims about architectural innovation vs. scale advantages cannot be properly evaluated.
2. **Proposer quality assessment**: Implement the conjecture-proposer module on a held-out problem set and conduct blind review of top-10 ranked conjectures for semantic relevance and novelty to verify the heavy tier's effectiveness.
3. **Cost-benefit analysis of heavy inference**: Measure wall-clock time and compute cost per problem solved in the heavy tier, then calculate cost per additional problem solved compared to light/medium tiers to assess practical utility.