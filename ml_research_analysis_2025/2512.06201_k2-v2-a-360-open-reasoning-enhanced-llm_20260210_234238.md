---
ver: rpa2
title: 'K2-V2: A 360-Open, Reasoning-Enhanced LLM'
arxiv_id: '2512.06201'
source_url: https://arxiv.org/abs/2512.06201
tags:
- arxiv
- reasoning
- data
- training
- stage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'K2-V2 is a 70B parameter dense LLM built from scratch to serve
  as a reasoning-centric foundation model. It uses a progressive training strategy
  across three phases: (1) large-scale pretraining on TxT360, a diverse natural dataset,
  (2) mid-training on TxT360-Midas with extended context and reasoning data, and (3)
  light supervised fine-tuning introducing tool use and reasoning modes.'
---

# K2-V2: A 360-Open, Reasoning-Enhanced LLM

## Quick Facts
- arXiv ID: 2512.06201
- Source URL: https://arxiv.org/abs/2512.06201
- Authors: K2 Team; Zhengzhong Liu; Liping Tang; Linghao Jin; Haonan Li; Nikhil Ranjan; Desai Fan; Shaurya Rohatgi; Richard Fan; Omkar Pangarkar; Huijuan Wang; Zhoujun Cheng; Suqi Sun; Seungwook Han; Bowen Tan; Gurpreet Gosal; Xudong Han; Varad Pimpalkhute; Shibo Hao; Ming Shan Hee; Joel Hestness; Haolong Jia; Liqun Ma; Aaryamonvikram Singh; Daria Soboleva; Natalia Vassilieva; Renxi Wang; Yingquan Wu; Yuekai Sun; Taylor Killian; Alexander Moreno; John Maggs; Hector Ren; Guowei He; Hongyi Wang; Xuezhe Ma; Yuqi Wang; Mikhail Yurochkin; Eric P. Xing
- Reference count: 31
- 70B parameter dense LLM with reasoning specialization, achieving 94.8% on GSM8K, 80.2% on AIME 2025, and 69.3% on GPQA-Diamond

## Executive Summary
K2-V2 is a 70B parameter dense LLM built from scratch to serve as a reasoning-centric foundation model. It uses a progressive training strategy across three phases: (1) large-scale pretraining on TxT360, a diverse natural dataset, (2) mid-training on TxT360-Midas with extended context and reasoning data, and (3) light supervised fine-tuning introducing tool use and reasoning modes. This design yields strong reasoning behaviors and long-context consistency. K2-V2 achieves 94.8% on GSM8K, 80.2% on AIME 2025, and 69.3% on GPQA-Diamond, outperforming Qwen2.5-72B and approaching Qwen3-235B. It also attains 90.0 on IFEval and 42.6% on LongBench V2. The model balances general capabilities with reasoning specialization, making it a robust base for further alignment and a transparent alternative to closed reasoning models.

## Method Summary
K2-V2 uses a progressive training strategy: pre-training on 12.25T tokens of TxT360 (natural web/text data), mid-training on 2.7T tokens of TxT360-Midas (reasoning traces, long context data) with context extension from 8K to 512K tokens, and SFT on TxT360-3efforts with three reasoning effort levels. The model uses an 80-layer Transformer with GQA (8 KV heads), RoPE with base frequency scaling (500K→10M), and a 70B parameter count. Training employs Megatron-LM backend, H200 cluster with TP=8/SP, and includes fault tolerance mechanisms for long runs.

## Key Results
- GSM8K: 94.8% accuracy, surpassing Qwen2.5-72B (90.2%) and approaching Qwen3-235B (95.3%)
- AIME 2025: 80.2% pass@k, significantly outperforming Qwen2.5-72B (59.6%) and approaching Qwen3-235B (86.8%)
- GPQA-Diamond: 69.3% accuracy, outperforming Qwen2.5-72B (58.5%) and approaching Qwen3-235B (71.9%)

## Why This Works (Mechanism)

### Mechanism 1: Reasoning Data Infusion During Mid-Training
Introducing thinking traces and reasoning behaviors in mid-training (not just SFT) prepares the model for complex reasoning before post-training. The model is exposed to 250M+ math problems with explicit thinking traces from Qwen3-32B/GPT-OSS-120B, plus 100+ synthetic reasoning behavior demonstrations (System 1/System 2, red-blue teaming, abductive reasoning). This creates reasoning primitives that can be elicited via simple prompts.

### Mechanism 2: Progressive Context Extension with RoPE Base Scaling
Successful long-context extension (to 512K tokens) requires both staged context-length increases and sufficient RoPE base frequency. Four stages expand context (8K→64K→128K→512K) while RoPE base scales (0.5M→1M→10M). Insufficient base frequency (1M) caused performance degradation at 128K; increasing to 10M recovered performance.

### Mechanism 3: Unified Model with Controllable Reasoning Effort
A single model can serve both quick responses and extended reasoning via chat-template-controlled reasoning effort levels. Three special tokens (, <think_fast>, <think_faster>) delimit reasoning blocks. Training mixes low/medium/high effort data (25%/35%/40%). At inference, effort level controls generation length and reasoning depth.

## Foundational Learning

- **Decoder-only Transformer with GQA (Grouped-Query Attention)**
  - Why needed here: K2-V2 uses 64 query heads with 8 shared KV heads for memory/compute efficiency at 70B scale.
  - Quick check question: Can you explain why GQA reduces KV-cache memory compared to standard multi-head attention?

- **RoPE (Rotary Position Embeddings) with configurable base frequency**
  - Why needed here: Long-context extension depends on scaling RoPE base (θ = 500K→1M→10M) to maintain position encoding quality at extended lengths.
  - Quick check question: How does increasing RoPE base frequency affect the model's ability to distinguish positions at long sequences?

- **Progressive training curriculum with data mixing**
  - Why needed here: K2-V2 shifts data mix across stages—pre-training emphasizes natural data (57.4% English Web), mid-training adds synthetic reasoning data (up to 52% thinking traces by stage 3).
  - Quick check question: What risks arise from abrupt distribution shifts between training stages?

## Architecture Onboarding

- **Component map:**
  Pre-training (Megatron-LM + Transformer Engine) -> Mid-training (in-house framework with CP) -> SFT (in-house framework)
  TxT360 (12.25T tokens) -> TxT360-Midas (2.7T tokens) -> TxT360-3efforts (~10B loss tokens)
  8K context (RoPE θ=500K) -> 512K context (RoPE θ=10M) -> 3 reasoning effort modes

- **Critical path:**
  1. Pre-training establishes broad knowledge foundation (12.25T tokens)
  2. Mid-training stage 1 introduces reasoning behaviors (8K context, 1.77T tokens)
  3. Mid-training stages 2-4 extend context progressively (64K→512K, ~0.95T tokens total)
  4. SFT unifies reasoning efforts and adds tool-use (3 epochs)

- **Design tradeoffs:**
  - Disabled pipeline parallelism (PP) in favor of TP+SP due to small per-GPU batch size (3) and H200 memory capacity—avoids pipeline bubbles but limits batch-size flexibility
  - RoPE base 10M required for >128K contexts, but may affect short-context performance (mitigated by mixing 30%+ short-context data throughout mid-training)
  - Decay-to-zero LR caused training stagnation below 1.5×10⁻⁶; fixed with minimum LR floor

- **Failure signatures:**
  - Loss spikes concentrated in first 40% of training (§3.5.3); addressed via τ_epoch tuning and rollback on "wide, malignant" spikes
  - RoPE base 1M insufficient for 128K+ contexts (§4.4); symptom is long-context benchmark degradation
  - SFT with default self-identity prompt causes model to ignore system instructions; fixed by random injection (p=0.1–0.5)

- **First 3 experiments:**
  1. **Baseline checkpoint evaluation:** Evaluate pre-training, mid-1/2/3/4 checkpoints on core benchmarks (MMLU, GSM8K, AIME, GPQA-Diamond) to map capability emergence across stages—use Eval360 pipeline with 32K generation length.
  2. **RoPE base frequency ablation:** Train mid-training stage 3 with RoPE base 1M vs 10M on identical data mix; evaluate on RULER/NIAH at 64K/128K context lengths to confirm the paper's finding.
  3. **Reasoning effort analysis:** Run K2-SFT on AIME/HMMT with low/medium/high effort modes; measure both accuracy and generation length to verify adaptive reasoning depth (hypothesis: high effort should show large length spread, indicating problem-difficulty adaptation).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does the "high" reasoning effort setting yield lower accuracy than the "medium" setting on specific function-calling tasks?
- Basis in paper: Section 7.1.2 states that the superior performance of the "medium" effort over "high" effort on tool calling is a "counter-intuitive result" that "warrants further investigation."
- Why unresolved: The authors note that the majority of K2's tool-calling training data lacks reasoning traces, which might explain the mismatch, but this does not explain why the trend appears in other models like GPT-OSS.
- What evidence would resolve it: A comparison of models trained on tool-use data explicitly synthesized with high-effort reasoning traces versus standard data.

### Open Question 2
- Question: Does a learning rate below $1.5 \times 10^{-6}$ in 70B models cause training stagnation due to numerical precision limits or optimizer dynamics?
- Basis in paper: Section 3.5.1 notes that when the learning rate dropped below this threshold, the parameter norm ceased to change, leading to the hypothesis of training stagnation in the "long tail" of Decay-to-Zero schedules.
- Why unresolved: While Decay-to-Zero is theoretically supported, empirical observations at this specific scale and token count suggest a potential floor effect not predicted by smaller-scale experiments.
- What evidence would resolve it: Controlled ablations monitoring gradient variance and downstream task performance when holding the learning rate at different minimum floors.

### Open Question 3
- Question: Does intense mid-training specialization for mathematical reasoning inherently degrade generalist explainability capabilities?
- Basis in paper: Section 8.3 and Figure 27 document an "Alignment Trade-off" where the model's ability to explain quantum entanglement to a general audience degraded significantly (score 0.85 to 0.25) as it specialized for math in Stage 4.
- Why unresolved: The paper documents the capability shift (from generalist peak to specialization loop) but does not isolate the mechanism or propose a method to mitigate this loss of versatility.
- What evidence would resolve it: Longitudinal evaluation of models trained with varying ratios of generalist instructional data mixed into the reasoning-heavy mid-training curriculum.

## Limitations

- The proprietary TxT360 dataset makes independent verification of data quality and composition difficult
- Limited ablation studies prevent isolating the relative contributions of mid-training reasoning data versus SFT reasoning behaviors
- Evaluation suite lacks robustness checks including adversarial prompting and multi-turn conversation quality assessment

## Confidence

- **High Confidence**: GSM8K/AIME/GPQA-Diamond benchmark results and the progressive context extension mechanism with RoPE base scaling
- **Medium Confidence**: The reasoning data infusion hypothesis during mid-training
- **Medium Confidence**: The unified model with controllable reasoning effort mechanism
- **Low Confidence**: Claims about the relative importance of mid-training versus SFT for reasoning behaviors

## Next Checks

1. **Stage-wise Capability Emergence**: Evaluate pre-training and each mid-training checkpoint on GSM8K, AIME 2025, and GPQA-Diamond to quantify capability emergence timing and isolate contributions from each training stage.

2. **RoPE Base Frequency Ablation**: Train mid-training stage 3 with identical data mix but RoPE base 1M versus 10M, evaluating on RULER/NIAH at 64K/128K context lengths to independently verify the paper's finding about base frequency requirements.

3. **Reasoning Effort Transfer**: Apply K2-SFT's low/medium/high effort modes to out-of-domain reasoning tasks (e.g., coding problems, logical puzzles) to test whether the reasoning-effort control generalizes beyond math benchmarks.