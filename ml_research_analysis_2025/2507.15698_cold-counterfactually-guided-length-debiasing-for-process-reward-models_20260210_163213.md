---
ver: rpa2
title: 'CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models'
arxiv_id: '2507.15698'
source_url: https://arxiv.org/abs/2507.15698
tags:
- length
- reward
- bias
- arxiv
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies a pervasive length bias in Process Reward
  Models (PRMs), where longer reasoning steps receive higher scores even when semantic
  content and logical validity remain unchanged. This undermines PRM reliability by
  conflating verbosity with reasoning quality.
---

# CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models

## Quick Facts
- arXiv ID: 2507.15698
- Source URL: https://arxiv.org/abs/2507.15698
- Authors: Congmin Zheng; Jiachen Zhu; Jianghao Lin; Xinyi Dai; Yong Yu; Weinan Zhang; Mengyue Yang
- Reference count: 23
- Primary result: CoLD reduces reward-length correlation, improves accuracy in step selection, and encourages more concise, logically valid reasoning on MATH500 and GSM-Plus

## Executive Summary
Process Reward Models (PRMs) suffer from length bias, where longer reasoning steps receive higher scores even when semantic content and logical validity remain unchanged. This undermines PRM reliability by conflating verbosity with reasoning quality. To address this, the authors propose CoLD, a Counterfactually-Guided Length Debiasing framework. CoLD integrates three components: an explicit length-penalty adjustment, a learned bias estimator trained to capture spurious length-related signals, and a joint training strategy that enforces length-invariance. Grounded in counterfactual reasoning and causal graph analysis, CoLD reduces reward-length correlation, improves accuracy in step selection, and encourages more concise, logically valid reasoning. Extensive experiments on MATH500 and GSM-Plus demonstrate that CoLD consistently outperforms baselines, achieving higher accuracy while selecting shorter, higher-quality reasoning steps.

## Method Summary
CoLD addresses length bias in PRMs through three integrated components: (1) an explicit length penalty that subtracts a linear function of token length from the raw reward, (2) a learned bias estimator that captures length-related spurious signals through separate optimization, and (3) joint training with asymmetric correlation objectives that specialize the PRM for semantic correctness and the bias estimator for stylistic artifacts. The framework is trained on PRM800K and Math-Shepherd datasets and evaluated using best-of-N accuracy and reward-length correlation metrics. During inference, a hyperparameter c adjusts the strength of debiasing, with c=1.1 generally providing robust performance across different policy models.

## Key Results
- CoLD reduces reward-length correlation and improves best-of-N accuracy on MATH500 and GSM-Plus
- Joint training achieves the best accuracy-length tradeoff, with 61.5% average accuracy and 257.9 average length on MATH500
- The learned bias estimator successfully captures length bias while preserving correctness discrimination
- Length penalty alone shows limited effectiveness, dropping accuracy to 44.8% compared to 49.2% for full CoLD

## Why This Works (Mechanism)

### Mechanism 1: Explicit Length Penalty
- Claim: A linear penalty on step length reduces reward-length correlation by directly discouraging verbosity in scoring.
- Mechanism: The corrected reward is computed as r*(x) = r(x) - αℓ(x), where α > 0 controls penalty strength and ℓ(x) is token length. This forces the effective reward to decrease as length increases, counteracting the PRM's inherent bias toward longer steps.
- Core assumption: The length bias manifests approximately linearly, so a proportional subtraction can neutralize it.
- Evidence anchors: [abstract]: "CoLD integrates three components: an explicit length-penalty adjustment..." [Methodology - Length Penalty]: Equation (4) formalizes the penalty; Figure 1 shows raw reward-length correlation.
- Break condition: If accuracy drops substantially when penalty is applied alone (Table 2: penalty-only yields 44.8% vs 49.2% full CoLD on MATH500), the linear assumption is insufficient—semantic content may be inadvertently penalized.

### Mechanism 2: Learned Bias Estimator
- Claim: A separate neural module can learn to isolate the spurious reward component caused by length, enabling more flexible debiasing than a fixed penalty.
- Mechanism: A bias estimator bϕ(x) is trained to minimize the correlation between debiased reward r*(x) = r(x) - λbϕ(x) and length ℓ(x) within each correctness group, while preserving correctness discrimination via BCE loss (Equations 6-9). This encourages bϕ to capture length-related noise without removing semantic signal.
- Core assumption: Length bias can be disentangled from correctness signals through separate optimization on the same data.
- Evidence anchors: [abstract]: "...a learned bias estimator trained to capture spurious length-related signals..." [Methodology - Bias Estimator]: Training objective Lbias = λcorrLcorr + Lbce explicitly targets decorrelation.
- Break condition: If correlation loss (Lcorr) fails to decrease, or BCE loss (Lbce) rises sharply, bϕ is either not capturing length bias or is removing correctness information—check gradient conflicts between objectives.

### Mechanism 3: Joint Training with Decorrelation Constraints
- Claim: Co-training the PRM and bias estimator end-to-end, with asymmetric correlation objectives, explicitly disentangles correctness from length effects more effectively than post-hoc correction.
- Mechanism: The PRM is regularized to minimize correlation with length (LPRM = Lce + λr·ρ²r), while the bias estimator is encouraged to maximize its correlation with length (LBias = Lce - λb·ρ²b). This creates specialization: PRM focuses on semantic correctness, bias estimator absorbs stylistic artifacts. Composed reward is r̂(x) = r(x)·σ(bϕ(x+N)) with noise injection N to prevent bϕ from capturing semantic features.
- Core assumption: Asymmetric gradient signals can successfully partition the feature space into semantic vs. stylistic components without collapsing.
- Evidence anchors: [abstract]: "...a joint training strategy that enforces length-invariance in reward predictions." [Methodology - Joint Training]: Equations 11-16 define the architecture and loss; Table 1 shows joint training (CoLD-Base-PRM) achieves best accuracy (61.5% avg) with shortest length (257.9 avg).
- Break condition: If PRM correlation (ρr) remains high or bias estimator correlation (ρb) stays low after training, the specialization is failing—inspect whether noise injection N is sufficient and whether λr, λb are appropriately balanced.

## Foundational Learning

- Concept: **Process Reward Models (PRMs)**
  - Why needed here: PRMs assign scalar rewards to intermediate reasoning steps, enabling fine-grained evaluation beyond final-answer accuracy. CoLD targets biases in this scoring process.
  - Quick check question: Given a 4-step solution with rewards [0.8, 0.3, 0.9, 0.6], how would a vanilla PRM select the "best" trajectory if using minimum-step aggregation?

- Concept: **Causal Graphs and Counterfactual Reasoning**
  - Why needed here: The paper models PRM predictions using a causal graph (S→L→P, S→C→P) and defines bias counterfactually as the reward change under length intervention while holding semantics fixed.
  - Quick check question: In the causal graph, what does the edge S→L→P represent, and why is it considered "spurious"?

- Concept: **Pearson Correlation for Bias Measurement**
  - Why needed here: Correlation coefficients quantify reward-length relationships within correctness groups, serving as both diagnostic metrics and training objectives.
  - Quick check question: If ρy=1 = 0.6 and ρy=0 = 0.4 before debiasing, what would you expect these values to be after successful CoLD training?

## Architecture Onboarding

- Component map:
  - Input: (question q, partial solution prefix xj)
  - PRM backbone: LLM-based classifier outputting logits for correct/incorrect tokens
  - Bias estimator bϕ: Separate neural module (architecture details not fully specified in paper)
  - Combiner: Multiplies/divides PRM output with bias estimator output (varies by training mode)
  - Output: Debiased scalar reward r*(x) ∈ (0,1)

- Critical path:
  1. Tokenize (q, s≤j) → feed to PRM → extract logits l+, l- → compute raw reward r(xj)
  2. Feed xj to bias estimator → compute bϕ(xj)
  3. Apply debiasing formula (Equation 5 or 16 depending on mode) → output r*(xj)
  4. For best-of-N: aggregate step scores (minimum) → rank trajectories → select top

- Design tradeoffs:
  - Penalty-only vs. learned estimator: Penalty is simple but risks over-penalizing; estimator is flexible but requires additional training data and hyperparameter tuning (λ, λcorr)
  - Joint vs. post-hoc training: Joint achieves better accuracy-length tradeoff (Table 1) but requires retraining PRM; post-hoc is cheaper (can apply to pre-trained PRMs)
  - Noise injection magnitude: Too little → bϕ captures semantics; too much → bϕ becomes uninformative

- Failure signatures:
  - Accuracy drops below baseline: Likely over-regularization; reduce λr or λcorr
  - Selected solutions remain verbose: ρb may be too low; increase λb or check bias estimator capacity
  - Training instability: Conflicting gradients between Lce and correlation losses; try alternating updates or gradient surgery

- First 3 experiments:
  1. Replicate Figure 1: Score original vs. DeepSeek-extended steps with vanilla PRM; confirm positive reward-length correlation (Pearson ρ > 0.3)
  2. Ablate components: Train CoLD variants (penalty-only, estimator-only, penalty+estimator without joint) on PRM800K subset; measure accuracy and length on MATH500 validation set
  3. Sensitivity analysis: Sweep hyperparameter c ∈ {0.7, 0.9, 1.1, 1.3, 1.5} on held-out questions; plot accuracy vs. length curves to identify model-specific optima (referencing Figure 4 patterns)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the optimal regularization strength ($c$) be determined adaptively rather than requiring manual tuning for each specific policy model?
- **Basis in paper:** [explicit] The Hyperparameter Study states that "The best-performing value of c varies across policy models... likely due to differences in the verbosity and reasoning style of each model."
- **Why unresolved:** The current implementation relies on a grid search over fixed values (e.g., $c \in \{0.7, \dots, 1.5\}$), which may not scale efficiently to new or diverse models without re-tuning.
- **What evidence would resolve it:** A dynamic or meta-learning mechanism that adjusts $c$ based on input statistics, achieving performance comparable to the manually tuned baseline across multiple distinct policy models.

### Open Question 2
- **Question:** Does removing length bias inadvertently increase the PRM's reliance on other latent noise factors ($N$), such as linguistic fluency or writing style?
- **Basis in paper:** [inferred] While the causal graph (Figure 2) includes a node for "latent noise factors (N)," the proposed framework exclusively targets the length path ($S \rightarrow L \rightarrow P$), leaving the influence of other spurious features unaddressed.
- **Why unresolved:** The paper measures the reduction in reward-length correlation but does not analyze correlations with other surface-level features that might substitute for length as a shortcut.
- **What evidence would resolve it:** A correlation analysis showing that the debiased reward does not exhibit increased statistical dependence on fluency metrics or stylistic markers compared to the baseline.

### Open Question 3
- **Question:** Does the debiasing effectiveness persist on naturally occurring human verbosity, or is it specific to the semi-synthetic extension patterns generated by DeepSeek?
- **Basis in paper:** [explicit] The authors note they "construct a semi-synthetic extension... by prompting DeepSeek to rewrite each step" to evaluate length bias (Page 2).
- **Why unresolved:** Synthetic verbosity (e.g., duplication or LLM rewriting) may possess distinct statistical signatures compared to natural, human-written long-form reasoning, potentially limiting the generalizability of the bias estimator.
- **What evidence would resolve it:** Evaluation results on a dataset of unaltered, naturally verbose human solutions demonstrating that CoLD effectively reduces reward-length correlation in non-synthetic settings.

## Limitations
- The explicit length-penalty assumption of linear bias is not fully validated across PRM architectures, as evidenced by the performance drop when penalty is used alone
- The learned bias estimator's architecture is underspecified, creating uncertainty about whether it can generalize beyond the specific PRM backbones tested
- While joint training shows the strongest results, the asymmetric correlation objectives require careful hyperparameter tuning (λr, λb) that may not transfer well across datasets

## Confidence
- **High:** CoLD effectively reduces reward-length correlation and improves best-of-N accuracy on MATH500 and GSM-Plus
- **Medium:** The three-component framework (penalty + estimator + joint training) is necessary for optimal performance
- **Low:** The explicit length-penalty assumption holds across all PRM architectures and datasets

## Next Checks
1. Test CoLD with different PRM backbones (e.g., Qwen2.5, Llama) to verify architectural independence
2. Conduct ablation studies varying the noise injection magnitude N in joint training to find optimal trade-offs
3. Evaluate on non-mathematical reasoning tasks (e.g., code generation, multi-hop QA) to test domain generalization