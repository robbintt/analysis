---
ver: rpa2
title: 'S2D-ALIGN: Shallow-to-Deep Auxiliary Learning for Anatomically-Grounded Radiology
  Report Generation'
arxiv_id: '2511.11066'
source_url: https://arxiv.org/abs/2511.11066
tags:
- report
- alignment
- generation
- stage
- reports
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of anatomically-grounded radiology
  report generation, where existing methods fail to establish fine-grained alignment
  between visual findings and their textual descriptions. To address this, the authors
  propose S2D-ALIGN, a novel training paradigm that progressively guides a multimodal
  large language model through three stages: coarse image-report alignment, contextual
  enhancement with reference reports, and fine-grained grounding using clinically-relevant
  key phrases.'
---

# S2D-ALIGN: Shallow-to-Deep Auxiliary Learning for Anatomically-Grounded Radiology Report Generation

## Quick Facts
- **arXiv ID:** 2511.11066
- **Source URL:** https://arxiv.org/abs/2511.11066
- **Reference count:** 13
- **Primary result:** Achieves F1 score of 0.608 on MIMIC-CXR, outperforming state-of-the-art methods for anatomically-grounded radiology report generation

## Executive Summary
S2D-ALIGN introduces a novel training paradigm for anatomically-grounded radiology report generation that progressively aligns visual findings with textual descriptions through three sequential stages. The method addresses the challenge of fine-grained anatomical grounding that plagues existing approaches by implementing a shallow-to-deep curriculum that first establishes coarse image-report alignment, then enriches context with reference reports, and finally enforces fine-grained grounding using clinically-relevant key phrases. A key innovation is the Shallow-to-Deep Memory Adapter (SMA), which enables cross-modal feature sharing across stages through a shared memory bank. Experiments demonstrate state-of-the-art performance on MIMIC-CXR and IU X-Ray datasets, with ablation studies confirming the effectiveness of the progressive curriculum and memory-based adapter design.

## Method Summary
S2D-ALIGN employs a three-stage progressive alignment strategy built on an MLLM backbone (Vicuna-7B + Rad-DINO visual encoder). Stage 1 trains only visual-to-text alignment using SMA_v. Stage 2 adds reference report context by initializing from Stage 1 and training SMA_v + SMA_t with shared memory. Stage 3 incorporates key phrase grounding by initializing from Stage 2 and training all three SMA variants (SMA_v, SMA_t, SMA_p) with clinically-relevant phrases extracted via RadGraph. The SMA uses shared memory queries (64 vectors) across all stages to enable implicit feature sharing, with 8-head cross-attention and 3-layer MLP architecture. Training uses sequential curriculum with decreasing learning rates (3e-4 → 1e-4 → 5e-5) and LoRA fine-tuning (rank=16) applied to all linear layers of the Vicuna decoder.

## Key Results
- Achieves F1 score of 0.608 on MIMIC-CXR, significantly outperforming single-stage (0.449) and joint training (0.470) baselines
- Progressive curriculum training shows 16-point F1 improvement over joint training, demonstrating curriculum effectiveness
- SMA with shared memory bank achieves 0.608 F1 vs 0.559 without shared memory, confirming cross-stage feature sharing benefits
- Stage 3 key phrase grounding provides 9.5-point F1 gain over Stage 2 alone (0.608 vs 0.513), validating fine-grained anatomical grounding
- Outperforms state-of-the-art methods including XrayGPT (0.508 F1) and CoLLaGen (0.532 F1) on MIMIC-CXR

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Progressive curriculum training from coarse-to-fine alignment improves anatomical grounding over single-stage or joint training approaches.
- **Mechanism:** The PAG strategy sequentially trains through three stages, allowing the model to first establish foundational visual-textual mappings before learning finer-grained correspondences. This prevents the optimization difficulty of simultaneously learning multi-granularity alignments.
- **Core assumption:** The sequential curriculum enables more stable convergence than exposing the model to all granularities simultaneously.
- **Evidence anchors:**
  - [abstract]: "S2D-ALIGN implements a shallow-to-deep strategy, progressively enriching the alignment process"
  - [section - Table 3]: Single-stage (F1=0.449) and joint training (F1=0.470) substantially underperform progressive training (F1=0.608); reversed order (S1→S3→S2) yields F1=0.527, confirming curriculum ordering matters
  - [corpus]: Related work MCA-RG similarly addresses "difficulties in accurately mapping pathological and anatomical features" though uses different alignment strategy
- **Break condition:** If joint training or single-stage training matched progressive performance, the curriculum hypothesis would be weakened.

### Mechanism 2
- **Claim:** Shared memory bank across PAG stages enables cross-modal feature integration that simple projection layers cannot achieve.
- **Mechanism:** The SMA uses learnable memory query vectors (Q_mem ∈ R^{Nmem×Dv}) that attend to features from different modalities across stages. Since the same memory bank is shared, visual features from Stage 1 implicitly inform text processing in later stages, creating a unified representation space.
- **Core assumption:** Feature sharing across stages creates synergistic representations that exceed independent stage training.
- **Evidence anchors:**
  - [section - Methodology]: "the same memory bank is shared across all distinct alignment stages of PAG, enabling implicit feature sharing"
  - [section - Table 4]: SMA with shared memory (F1=0.608) outperforms SMA without shared memory (F1=0.559) and MLP baseline (F1=0.473)
  - [corpus]: Weak corpus evidence—neighbor papers do not explicitly test memory-based cross-stage sharing mechanisms
- **Break condition:** If removing memory sharing caused minimal performance drop (<2% F1), the sharing mechanism would be non-essential.

### Mechanism 3
- **Claim:** Key phrase auxiliary signals extracted via RadGraph enforce anatomically-grounded text-to-region correspondence.
- **Mechanism:** Stage 3 injects clinically-relevant key phrases (e.g., "enlarged heart size," "no pleural effusion") derived from ground-truth reports. The model must attend to these phrases while generating, forcing explicit mapping between anatomical concepts and visual features consolidated in earlier stages.
- **Core assumption:** RadGraph-extracted entities capture clinically meaningful anchors that improve grounding beyond instance-level report text.
- **Evidence anchors:**
  - [abstract]: "ultimately utilizes key phrases to ground the generation in specific anatomical details"
  - [section - Table 3]: Removing fine-grained grounding (w/o Stage 3, S1→S2) drops F1 from 0.608 to 0.513—a 9.5 point degradation
  - [corpus]: "Leveraging LLMs for Multimodal Retrieval-Augmented RRG via Key Phrase Extraction" validates key phrase utility in related work
- **Break condition:** If Stage 3 provided no gain over Stage 2 (within 1-2% F1), key phrase grounding would be redundant.

## Foundational Learning

- **Concept: Multimodal Large Language Models (MLLMs)**
  - **Why needed here:** S2D-ALIGN builds on an MLLM backbone (Vicuna-7B + visual encoder). Understanding how vision encoders connect to LLMs via projection/adapter layers is prerequisite.
  - **Quick check question:** Can you explain why a simple MLP projector might fail to capture fine-grained visual-textual mappings compared to attention-based adapters?

- **Concept: Curriculum Learning**
  - **Why needed here:** PAG is explicitly motivated by curriculum learning principles—progressively increasing task difficulty (alignment granularity) across stages.
  - **Quick check question:** What distinguishes curriculum learning from multi-task learning, and why might sequential training outperform joint training here?

- **Concept: Cross-Attention Mechanisms**
  - **Why needed here:** The SMA uses multi-head cross-attention between memory queries and modality features. Understanding Q/K/V attention is essential for debugging adapter behavior.
  - **Quick check question:** How does cross-attention differ from self-attention, and why is cross-attention appropriate for fusing visual and textual features?

- **Concept: LoRA (Low-Rank Adaptation)**
  - **Why needed here:** The LLM decoder uses LoRA fine-tuning rather than full parameter updates. Understanding rank-constrained adaptation clarifies what is learned.
  - **Quick check question:** What are the trade-offs of LoRA vs. full fine-tuning in terms of parameter efficiency and representational capacity?

## Architecture Onboarding

- **Component map:**
  - Rad-DINO (frozen ViT) → outputs patch embeddings V ∈ R^{N×Dv} → SMA_v → Vicuna-7B LoRA adapter → text generation
  - BiomedVLP-CXR-BERT-specialized → reference reports/key phrases → SMA_t/SMA_p → Vicuna-7B LoRA adapter → text generation
  - Shared memory bank Q_mem ∈ R^{64×768} across all SMA variants

- **Critical path:**
  1. Stage 1: Train SMA_v only (8 epochs, LR=3e-4) → visual-to-LLM alignment
  2. Stage 2: Initialize from Stage 1, train SMA_v + SMA_t with shared Q_mem (5 epochs, LR=1e-4) → add reference report context
  3. Stage 3: Initialize from Stage 2, train SMA_v + SMA_t + SMA_p (3 epochs, LR=5e-5) → add key phrase grounding
  4. Inference: Discard all auxiliary signals, use only C(1) = SMA_v(Ev(I))

- **Design tradeoffs:**
  - **Memory bank size (Nmem=64):** Larger banks capture more features but increase compute; not ablated in paper (assumption: tuned on validation)
  - **LoRA rank=16:** Higher rank = more capacity but more parameters; paper does not compare ranks
  - **Sequential vs. joint training:** Sequential enables stable curriculum but requires 3× training passes; Table 3 shows joint training fails (F1=0.470 vs. 0.608)
  - **Inference-time auxiliary discard:** Reduces inference cost but loses explicit grounding signals—relies on learned representations

- **Failure signatures:**
  - **Coarse-grained hallucinations:** If model generates findings not visually present (e.g., XrayGPT in Figure 2: "no abnormalities visible" when pathology exists), suspect Stage 1-only alignment or weak SMA
  - **Missing anatomical specificity:** If reports describe findings without anatomical locations (e.g., "opacity present" vs. "opacity in right lower lobe"), Stage 3 key phrase grounding may be undertrained
  - **F1 degradation on CE metrics:** If BLEU reasonable but clinical efficacy (F1) low, fine-grained grounding is failing—check RadGraph extraction quality and Stage 3 convergence

- **First 3 experiments:**
  1. **Reproduce single-stage baseline:** Train only Stage 1 (SMA_v) and evaluate—should achieve ~F1=0.449 per Table 3; confirms visual-text alignment works at coarse level
  2. **Ablate shared memory:** Train full PAG but use independent memory banks per stage—should see F1 drop from 0.608 toward 0.559 per Table 4; validates memory sharing contribution
  3. **Test stage ordering:** Train in reversed order (S1→S3→S2)—should achieve ~F1=0.527 per Table 3; confirms curriculum directionality matters (coarse→fine is superior)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the reliance on longitudinal reference reports in Stage 2 impact generalization to datasets lacking patient history?
- **Basis in paper:** [Inferred] The method explicitly utilizes the "inherent longitudinal nature of the MIMIC-CXR dataset" for Stage 2 context.
- **Why unresolved:** While IU X-Ray results are provided, the paper does not ablate the impact of missing longitudinal data or analyze how the absence of reference reports affects the "contextual enhancement" stage.
- **What evidence would resolve it:** An ablation study on MIMIC-CXR simulating the absence of prior reports (e.g., using zero-vectors or random reports) to quantify the performance drop.

### Open Question 2
- **Question:** Is the specific "shallow-to-deep" progression the theoretical optimal curriculum, or merely a viable heuristic?
- **Basis in paper:** [Explicit] The authors note that "fine-tuning in a reversed order might harm the performance," but do not fully characterize the curriculum's sensitivity.
- **Why unresolved:** The paper demonstrates that the specific order matters (S1→S2→S3) but leaves the full combinatorial space of curriculum strategies and the theoretical reasoning for this specific optimality unexplored.
- **What evidence would resolve it:** A comprehensive analysis of all permutations of the three stages or a theoretical analysis of the loss landscape curvature during the progressive stages.

### Open Question 3
- **Question:** How robust is the fine-grained grounding mechanism to noise or omissions in the RadGraph entity extraction process?
- **Basis in paper:** [Inferred] Stage 3 relies on RadGraph and an external LLM to generate key phrases.
- **Why unresolved:** The paper assumes the correctness of the RadGraph annotations and refinement prompts without evaluating how extraction errors propagate to the final anatomical grounding.
- **What evidence would resolve it:** Experiments injecting synthetic noise into the RadGraph key phrases (e.g., removing entities or adding false positives) to measure the degradation in F1 score.

## Limitations
- The specific three-stage curriculum ordering may be dataset-dependent and not universally optimal
- Heavy reliance on RadGraph annotations and LLM refinement introduces potential error propagation from entity extraction
- The strong performance improvements may partly reflect the specific CE metric formulation rather than purely improved anatomical grounding

## Confidence
- **High confidence:** Progressive curriculum ordering matters (F1 0.608 vs 0.470 joint training), shared memory bank improves performance (F1 0.608 vs 0.559 independent), Stage 3 key phrase grounding provides substantial gain (F1 0.608 vs 0.513 without it)
- **Medium confidence:** The SMA architecture is essential (F1 0.608 vs 0.394 with Q-Former), the three-stage granularity decomposition is optimal (no exploration of more/less stages)
- **Low confidence:** Specific hyperparameter choices (memory bank size, LoRA rank, learning rates) are truly optimal, RadGraph-extracted key phrases capture the most clinically-relevant entities

## Next Checks
1. **Cross-dataset generalization test:** Evaluate S2D-ALIGN on an independent radiology dataset (e.g., PadChest) without additional training to verify anatomical grounding transfers beyond MIMIC-CXR domain shift.

2. **Human clinical evaluation:** Have board-certified radiologists assess whether S2D-ALIGN reports demonstrate superior anatomical specificity and clinical accuracy compared to baselines using standardized rubric scoring.

3. **Failure case analysis:** Systematically categorize hallucinations and anatomical misalignments in S2D-ALIGN outputs to determine whether failures stem from Stage 1 visual encoding, Stage 2 reference integration, or Stage 3 key phrase grounding—enabling targeted architectural improvements.