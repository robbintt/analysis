---
ver: rpa2
title: 'SCARE: A Benchmark for SQL Correction and Question Answerability Classification
  for Reliable EHR Question Answering'
arxiv_id: '2511.17559'
source_url: https://arxiv.org/abs/2511.17559
tags:
- question
- questions
- ambiguous
- unanswerable
- schema
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SCARE, the first benchmark designed to evaluate
  post-hoc verification layers in EHR QA systems that perform both SQL correction
  and question answerability classification. SCARE contains 4,200 triples of questions,
  candidate SQL queries, and expected outputs across three major EHR databases (MIMIC-III,
  MIMIC-IV, eICU), with queries generated by seven different text-to-SQL models.
---

# SCARE: A Benchmark for SQL Correction and Question Answerability Classification for Reliable EHR Question Answering

## Quick Facts
- arXiv ID: 2511.17559
- Source URL: https://arxiv.org/abs/2511.17559
- Authors: Gyubok Lee; Woosog Chay; Edward Choi
- Reference count: 40
- Primary result: First benchmark for post-hoc verification layers in EHR QA systems, evaluating both SQL correction and answerability classification

## Executive Summary
SCARE introduces the first benchmark for evaluating post-hoc verification layers in EHR QA systems, addressing the critical need for reliable question answering in clinical settings. The benchmark contains 4,200 triples of questions, candidate SQL queries, and expected outputs across three major EHR databases, with queries generated by seven different text-to-SQL models. Through extensive experiments with seven baseline methods, the paper reveals a fundamental trade-off between preserving correct SQL and identifying problematic questions, with hybrid approaches combining iterative refinement and explicit classification signals performing best.

## Method Summary
SCARE evaluates post-hoc verification layers that inspect candidate SQL queries after generation, determining whether to preserve, correct, or reject them based on question answerability. The benchmark defines three answerability classes (answerable, ambiguous, unanswerable) and six sub-categories of problematic questions. Four evaluation scenarios test different system requirements, from correcting erroneous SQL to detecting all problematic questions. Seven baseline methods are evaluated, ranging from decoupled two-stage classification to integrated single-turn verification with iterative refinement and hybrid approaches that explicitly combine classification reasoning with SQL correction.

## Key Results
- Hybrid approaches combining iterative refinement and explicit classification signals achieve the best balance of SQL preservation and problematic question detection
- Single-Turn-Veri-Cls achieves highest correction rate (53.8%) while maintaining strong preservation (95.9%) and unanswerable detection (80.8% F1)
- Models struggle significantly with nuanced ambiguity detection, achieving F1 scores up to only 54.0%
- Global SQL error correction remains challenging, with only 30.8% correction rate for complex structural errors compared to 55.1% for table/column errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid approaches combining explicit classification signals with iterative refinement outperform purely modular or integrated methods
- Mechanism: The `-Cls` variants feed the Two-Stage classifier's reasoning output as an explicit guiding signal to integrated models, providing both answerability classification grounding and SQL correction capability in a unified pass
- Core assumption: Explicit answerability reasoning helps models calibrate when to correct SQL versus when to reject the question, reducing the inherent tension between preservation and detection
- Evidence anchors:
  - [abstract] "hybrid approaches combining iterative refinement and explicit classification signals performing best"
  - [Section 5.3] "Single-Turn-Veri-Cls achieves the highest CR (53.8%) and the best F1 score for unanswerable (80.8%), while maintaining a strong PR (95.9%)"
  - [corpus] PRACTIQ corpus work similarly addresses ambiguous/unanswerable queries but focuses on conversational settings rather than post-hoc verification

### Mechanism 2
- Claim: Iterative self-refinement improves SQL correction rates without sacrificing preservation of already-correct queries
- Mechanism: Multi-turn self-refinement generates an initial answer, produces internal feedback, then uses that feedback to guide refinement attempts—allowing localized error correction while avoiding over-modification of correct queries
- Core assumption: Models can reliably identify their own SQL errors through verification feedback and use this to improve without introducing new errors
- Evidence anchors:
  - [Section 5.3] "Multi-Turn-SelfRef achieves a CR of 51.4%, compared to 49.6% for the basic Single-Turn, while maintaining a high PR (97.2%)"
  - [Section 5.3] Table 4 shows 55.1% CR for table/column errors vs. only 30.8% for global structural errors
  - [corpus] SQLens and LitE-SQL also demonstrate execution-guided self-correction improving text-to-SQL reliability

### Mechanism 3
- Claim: Decoupled two-stage classification achieves highest recall for unanswerable detection but sacrifices coverage of answerable questions
- Mechanism: A dedicated classifier first determines answerability before any SQL processing, treating question filtering as an independent gate that prioritizes safety over coverage
- Core assumption: Question answerability can be reliably determined from question text and schema alone, without seeing the candidate SQL output
- Evidence anchors:
  - [Section 5.3] "Two-Stage... achieves the highest recall for unanswerable (93.0%). However, this comes at a significant cost to Cov"
  - [Section 5.3] Trade-off where "maximizing preservation often leads to overlooking problematic inputs, and vice versa"
  - [corpus] LatentRefusal corpus paper addresses refusal for unanswerable queries using latent signals, complementing this explicit classification approach

## Foundational Learning

- Concept: **Question Answerability Taxonomy**
  - Why needed here: SCARE defines six sub-categories (vague-question, vague-word, ambiguous-reference, small-talk, out-of-scope, missing-column) that models must distinguish to correctly route inputs
  - Quick check question: Can you explain why "How many patients meet the high-risk criteria?" is ambiguous rather than unanswerable?

- Concept: **SQL Error Type Hierarchy**
  - Why needed here: Table/column errors (55.1% correctable) vs. global structural errors (30.8% correctable) require different correction strategies; understanding this hierarchy informs architecture design
  - Quick check question: What distinguishes a "local" SQL error from a "global" one in the SCARE taxonomy?

- Concept: **Post-hoc vs. Pre-hoc Verification**
  - Why needed here: SCARE specifically evaluates independent post-hoc layers that audit candidate SQL after generation, distinct from pre-hoc input filtering or end-to-end generation with abstention
  - Quick check question: Why does post-hoc verification require handling SQL for all question types including unanswerable ones?

## Architecture Onboarding

- Component map: Question q + Database schema S + Candidate SQL ŷ -> Classification Module -> SQL Verifier/Corrector -> Iterative Refinement Loop -> Output Layer
- Critical path: Question + candidate SQL → classification signal → verification/correction → output; the -Cls variants make the classification-to-correction handoff explicit rather than implicit
- Design tradeoffs:
  - Modular (Two-Stage): High recall for problematic inputs, low coverage for answerable
  - Integrated (Single-Turn): High preservation, weak ambiguity detection
  - Hybrid (-Cls): Best balance but adds complexity and dependency on classifier quality
  - Iterative refinement: Improves correction but adds latency and may amplify errors if feedback is poor
- Failure signatures:
  - **Undetected ambiguity**: Models approve syntactically valid SQL for vague questions (e.g., "Sodium?" → SELECT label FROM d_labitems)
  - **Over-correction**: Modifying already-correct SQL during verification
  - **Global error persistence**: Failing to fix structural SQL logic errors requiring substantial rewrites
  - **Schema ignorance**: Hallucinating columns or relationships not in the provided schema
- First 3 experiments:
  1. Reproduce the Two-Stage vs. Single-Turn trade-off on a subset to confirm the preservation-detection tension exists in your setup
  2. Ablate the classification reasoning signal in -Cls variants to quantify its contribution to the hybrid performance gains
  3. Stratify error correction performance by SQL error type (T/C, J/G, PV, OL, OG) to identify which categories your implementation handles poorly

## Open Questions the Paper Calls Out
- How can ambiguity resolution be effectively handled through multi-turn dialogue rather than mere detection? The authors state in Section 7 that "the resolution of ambiguity, as opposed to its mere detection, is often best handled through multi-turn interaction," identifying this as a key extension for future research. SCARE evaluates only single-turn interactions in controlled settings; the paper does not address how detected ambiguities should trigger clarification dialogues or how such dialogues should be structured.
- What architectural or training modifications can substantially improve detection of nuanced ambigu

## Limitations
- Dataset bias toward MIMIC and eICU schemas may not generalize to other EHR systems with different structures
- Focus on post-hoc verification rather than end-to-end generation means it doesn't evaluate the complete reliability pipeline
- Evaluation methodology (execution-guided correctness) may miss semantic equivalence cases

## Confidence
- High: The hybrid approach superiority for observed trade-offs
- Medium: Generalizability beyond the studied schemas
- Low: Ambiguity detection performance (F1 up to 54.0%) suggesting fundamental limitations

## Next Checks
1. Evaluate the benchmark methods on a held-out EHR database with substantially different schema structure to test generalizability of the observed trade-offs
2. Implement a semantic equivalence oracle to verify whether uncorrected SQL queries are truly wrong versus semantically equivalent to the expected output
3. Conduct human evaluation of the ambiguous and unanswerable question classifications to establish ground truth quality and identify systematic model errors in intent understanding