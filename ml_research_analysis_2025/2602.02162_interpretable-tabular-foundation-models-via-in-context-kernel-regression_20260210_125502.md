---
ver: rpa2
title: Interpretable Tabular Foundation Models via In-Context Kernel Regression
arxiv_id: '2602.02162'
source_url: https://arxiv.org/abs/2602.02162
tags:
- kernel
- training
- kernelicl
- learning
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KernelICL enhances tabular foundation models with sample-based
  interpretability by replacing the final prediction layer with explicit kernel functions.
  The approach makes every prediction a transparent weighted average of training labels,
  with weights determined by kernel similarity in learned embedding space.
---

# Interpretable Tabular Foundation Models via In-Context Kernel Regression

## Quick Facts
- arXiv ID: 2602.02162
- Source URL: https://arxiv.org/abs/2602.02162
- Authors: Ratmir Miftachov; Bruno Charron; Simon Valentin
- Reference count: 23
- Primary result: Achieves 82.88% accuracy within 0.2% of TabICL while providing interpretable, sample-based predictions

## Executive Summary
KernelICL enhances tabular foundation models with interpretable sample-based predictions by replacing the final prediction layer with explicit kernel functions. Every prediction becomes a transparent weighted average of training labels, with weights determined by kernel similarity in learned embedding space. The approach achieves TabICL-level performance (82.88% accuracy on 55 TALENT benchmark datasets) while providing quantifiable inspectability through perplexity metrics of the weight distribution. Symmetric embeddings enable distance-based kernels (Gaussian, kNN) for in-context learning, achieving up to 5 percentage points higher accuracy than standard kNN while concentrating similarity on clinically relevant features in case studies.

## Method Summary
KernelICL fine-tunes TabICL's embedding module with a learnable projection matrix and replaces the MLP prediction head with explicit kernel functions. The method enables symmetric embeddings where training samples are duplicated through the TFicl layer as both context and query samples, creating a shared geometric space for distance-based kernels. Three kernel variants are evaluated: Gaussian, dot-product, and kNN. The model is trained end-to-end on synthetic prior data with cross-entropy loss, then calibrated via 5-fold cross-validation on target datasets to select optimal kernel scales. Predictions are explicit weighted averages of training labels, with perplexity quantifying the sparsity of explanations.

## Key Results
- Achieves 82.88% accuracy across 55 TALENT benchmark datasets, within 0.2% of TabICL baseline
- Gaussian and dot-product kernels outperform TabICL-MLP by 0.1-0.4 percentage points
- Perplexity of 5 indicates sparsity level similar to using 5 nearest neighbors
- KernelICL-kNN concentrates 61% more weight on glucose and 35% more on BMI versus isotropic standard kNN in diabetes prediction case study

## Why This Works (Mechanism)

### Mechanism 1: Symmetric Embeddings Unlock Distance-Based Kernels for ICL
- Claim: Enforcing identical embedding functions for test and training samples enables distance-based kernels in in-context learning settings where they were previously unavailable.
- Mechanism: TabICL's three-stage architecture applies identical operations in the first two stages regardless of sample role. By reprocessing training samples as queries through TFicl and applying a shared projection, identical inputs produce identical embeddings, allowing Euclidean distance to have semantic meaning.
- Core assumption: Distance-based kernels provide more intuitive interpretability than alignment-based kernels because "near vs. far" is cognitively simpler than "aligned vs. misaligned."
- Evidence anchors: Abstract claim about symmetric embeddings enabling distance-based kernels; section 3.2 description of TFcol/TFrow position-agnostic operations; limited direct evidence in corpus.

### Mechanism 2: Explicit Kernel Constraints Preserve Performance While Enabling Inspectability
- Claim: Replacing the opaque MLP prediction head with explicit kernel functions achieves within 0.2% accuracy while making every prediction a transparent weighted average.
- Mechanism: The MLP head can learn arbitrary functions, but constraining to kernel form (ŷ = Σ wi yi with wi ∝ KD(h(x), h(xi))) limits the hypothesis space while concentrating representational complexity in the learned embedding hD.
- Core assumption: The embedding space learned by the foundation model already encodes sufficient task-relevant structure that a simple kernel suffices for final prediction.
- Evidence anchors: Abstract accuracy claim; Table 2 showing all variants match TabICL-MLP within 0.12 points; xRFM paper claims interpretable feature learning but uses different approach.

### Mechanism 3: Perplexity Quantifies the Accuracy-Inspectability Tradeoff
- Claim: The perplexity of the weight distribution provides a tunable knob for practitioners to control sparsity of explanations at known accuracy cost.
- Mechanism: Kernel scale γ controls weight concentration. Large γ (Gaussian) or small k (kNN) concentrates weight on few samples (low perplexity, high inspectability) but may miss relevant support. The paper measures this tradeoff explicitly with ~5 percentage point accuracy gain over standard kNN at comparable perplexity.
- Core assumption: Lower perplexity correlates with human-usable inspectability.
- Evidence anchors: Section 3.4 defines perplexity metric; Figure 6 shows explicit accuracy vs. relative perplexity curves; section 4.4 Pima Diabetes case study shows KernelICL-kNN concentration on glucose (+61%) and BMI (+35%).

## Foundational Learning

- Concept: **In-Context Learning (ICL)**
  - Why needed here: KernelICL builds on the insight that ICL implicitly performs kernel regression. Understanding this connection is essential for grasping why explicit kernel constraints work without retraining.
  - Quick check question: Can you explain why retrieving similar demonstrations improves ICL performance in terms of kernel regression?

- Concept: **Nadaraya-Watson Kernel Regression**
  - Why needed here: The paper's prediction formula (Equation 2) is exactly Nadaraya-Watson estimation. The taxonomy (Levels 0-3) progressively constrains this estimator.
  - Quick check question: What happens to the prediction as the kernel bandwidth γ → 0? As γ → ∞?

- Concept: **Attention as Kernel Smoothing**
  - Why needed here: Standard dot-product attention IS a kernel (exponential dot-product). The paper unifies attention and classical kernels under one framework.
  - Quick check question: Why can't standard attention use a Gaussian kernel without architectural changes?

## Architecture Onboarding

- Component map:
  TFcol (column-wise Set Transformer) -> TFrow (row-wise self-attention) -> TFicl (label-conditioned in-context transformer) -> Projection W (512×dk) -> Kernel head (Kγ(h(x), h(xi)) → weights → prediction)

- Critical path:
  1. Load pre-trained TabICL embedding weights
  2. Enable symmetric embedding mode (duplicate training samples as queries through TFicl only)
  3. Initialize projection W (512→512 default)
  4. Fine-tune end-to-end on synthetic prior (5,000 batches, 64 datasets/batch)
  5. For kNN: use embeddings from Gaussian training (non-differentiable neighbor selection)
  6. Calibrate scale γ via 5-fold CV on target dataset

- Design tradeoffs:
  - Symmetric vs. asymmetric embeddings: +0.4-0.5% accuracy, lower perplexity, but 9-16% runtime overhead (up to 2× in large sample limit)
  - Calibration vs. default scale: +0.1-1.5% accuracy, lower perplexity, but 21-50× runtime overhead
  - dk=512 vs. smaller: Best accuracy and perplexity at 512; smaller dimensions increase perplexity substantially

- Failure signatures:
  - High perplexity (>50%) with calibration: Kernel scale may be mismatched to embedding scale; check γ grid coverage
  - Accuracy drop with symmetric mode: Verify training samples are duplicated correctly as both context AND query through TFicl
  - kNN underperforming Gaussian significantly: Ensure embeddings were trained with Gaussian kernel (kNN is non-differentiable)

- First 3 experiments:
  1. Replicate Table 2 on a held-out subset of TALENT datasets: Verify symmetric KernelICL-Dot matches TabICL-MLP within 0.2%
  2. Ablate symmetric embeddings on 5 diverse datasets: Measure accuracy gap and perplexity reduction to justify overhead
  3. Inspect Pima Diabetes neighbors manually: Confirm glucose/BMI concentration aligns with clinical expectations before deploying in high-stakes settings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the alignment of learned embeddings with clinically relevant features reflect learned causal structure or spurious correlations?
- Basis in paper: The paper states "Whether this alignment reflects learned causal structure requires further clinical validation" regarding the Pima Indians Diabetes case study.
- Why unresolved: The case study shows KernelICL prioritizes medically recognized predictors, but this could emerge from correlation patterns rather than causal understanding. No causal validation framework was applied.
- What evidence would resolve it: Systematic evaluation across multiple clinical datasets with known causal structures, potentially using causal discovery benchmarks or interventional data.

### Open Question 2
- Question: Can differentiable relaxations of kNN selection enable end-to-end training that improves KernelICL-kNN performance beyond using Gaussian-trained embeddings?
- Basis in paper: The paper notes "there are also differentiable relaxations of top-k ranking operations (Swezey et al., 2021), which would enable end-to-end kNN training" but does not implement or evaluate them.
- Why unresolved: KernelICL-kNN currently uses embeddings trained with Gaussian kernels as a workaround for non-differentiability. Specialized training could improve kNN-specific embeddings.
- What evidence would resolve it: Implementing differentiable top-k relaxations (e.g., PiRank) and comparing KernelICL-kNN accuracy with end-to-end trained embeddings versus transferred Gaussian embeddings.

### Open Question 3
- Question: Does the accuracy-inspectability tradeoff exhibit systematic variation across domains, dataset sizes, or data characteristics?
- Basis in paper: Figure 6 shows the tradeoff aggregated across 55 datasets, but the paper does not analyze whether optimal perplexity levels vary meaningfully across different data regimes or application domains.
- Why unresolved: Aggregated results obscure whether certain domains (e.g., healthcare vs. finance) benefit more from sparse predictions, or whether dataset scale affects the optimal operating point.
- What evidence would resolve it: Per-domain or per-dataset analysis correlating optimal perplexity with dataset characteristics (size, feature count, class balance, noise level) to identify systematic patterns.

## Limitations
- Computational overhead of symmetric embeddings (9-16% runtime increase, approaching 2× in large-sample regimes)
- Generalizability beyond binary classification not systematically evaluated
- Reliance on synthetic prior data for fine-tuning may limit domain transfer

## Confidence

- Accuracy preservation claim: **High confidence** - Table 2 results across 55 datasets show within 0.2% of TabICL
- Perplexity as interpretability metric: **Medium confidence** - Supported by Figure 6 and case studies but lacks ablation studies on alternative sparsity metrics
- Symmetric embeddings enabling distance-based kernels: **High confidence** - Explicit description of TFcol/TFrow position-agnostic operations and TFicl as sole source of asymmetry

## Next Checks

1. Measure accuracy drop when applying asymmetric embeddings to KernelICL on 5 diverse datasets to quantify the claimed 0.4-0.5% gain
2. Evaluate KernelICL on multi-class classification tasks to test generalizability beyond binary problems
3. Compare perplexity-based sparsity against alternative metrics (e.g., top-k weight concentration) on the same datasets