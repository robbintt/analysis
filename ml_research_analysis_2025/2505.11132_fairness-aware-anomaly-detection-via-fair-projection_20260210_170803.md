---
ver: rpa2
title: Fairness-aware Anomaly Detection via Fair Projection
arxiv_id: '2505.11132'
source_url: https://arxiv.org/abs/2505.11132
tags:
- uni00000013
- uni00000011
- fairness
- detection
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles fairness in unsupervised anomaly detection by
  addressing the challenge of group fairness across demographic groups. The authors
  propose a novel method, FairAD, which learns a projection to map data from different
  groups into a common target distribution, ensuring fairness while maintaining detection
  accuracy.
---

# Fairness-aware Anomaly Detection via Fair Projection

## Quick Facts
- arXiv ID: 2505.11132
- Source URL: https://arxiv.org/abs/2505.11132
- Authors: Feng Xiao; Xiaoying Tang; Jicong Fan
- Reference count: 40
- Primary result: Novel FairAD method achieves better fairness-accuracy trade-off in unsupervised anomaly detection through projection to shared target distribution

## Executive Summary
This paper addresses fairness in unsupervised anomaly detection by proposing FairAD, a method that learns a projection mapping different demographic groups to a common target distribution. The approach ensures group fairness while maintaining detection accuracy through a combination of Sinkhorn distance minimization and reconstruction loss. The authors introduce ADPD, a threshold-free fairness metric, and validate their method across six datasets showing robustness under both balanced and skewed data splits. The results demonstrate that fairness achieved on normal training data can transfer to anomalous test data under reasonable assumptions.

## Method Summary
FairAD learns an encoder-decoder architecture that projects data from different demographic groups into a shared truncated isotropic Gaussian target distribution. The training objective combines Sinkhorn distance between projected samples and target samples with reconstruction loss. The method includes two variants: Im-FairAD (implicit, no explicit fairness term) and Ex-FairAD (explicit fairness regularization). Anomaly scoring uses the norm of the projected latent vector. The approach theoretically guarantees predictive equality on training data and empirically shows fairness transfer to anomalous test data through assumptions about gradual anomaly evolution.

## Key Results
- FairAD achieves better trade-off between detection accuracy (AUC) and fairness (ADPD) compared to baselines on six real-world datasets
- Method demonstrates robustness under both balanced and skewed data splits, with ADPD remaining low even in extreme imbalance scenarios
- Introduced threshold-free ADPD metric provides more stable fairness evaluation than threshold-dependent metrics
- Empirical results validate the feasibility of ensuring fairness on anomalous data indirectly through normal data

## Why This Works (Mechanism)

### Mechanism 1: Fairness via Shared Target Distribution Projection
Mapping different demographic groups to a common target distribution achieves group fairness without explicit fairness regularization. The encoder learns to transform each group's data distribution toward a shared compact target distribution, making learned representations statistically independent of the protected attribute.

### Mechanism 2: Anomaly Scoring via Latent Space Density
The norm of the projected latent vector provides a reliable anomaly score. After training, normal samples cluster near the origin in the compact target distribution, while anomalous samples project to regions with larger norms, directly measuring deviation from high-density regions.

### Mechanism 3: Fairness Transfer from Normal to Anomalous Data
Fairness achieved on normal training data transfers to anomalous test data under reasonable assumptions. The paper argues that anomalies often emerge as perturbations of normal samples, allowing fairness properties to extend via composition of the learned projection with mappings between normal and abnormal distributions.

## Foundational Learning

- **Sinkhorn Distance (Entropy-Regularized Optimal Transport)**: Core loss component measuring distribution alignment between projected samples and target Gaussian. Preferred over MMD for high-dimensional distributions and computational efficiency.
  - Quick check: Can you explain why Sinkhorn distance is preferred over MMD for this application?

- **Group Fairness Definitions**: The paper formally proves that in unsupervised AD, only predictive equality can be guaranteed without assumptions; demographic parity and equal opportunity require additional assumptions about data structure.
  - Quick check: Why can't demographic parity be meaningfully guaranteed in unsupervised AD without additional assumptions?

- **Autoencoder Architecture with Distributional Constraints**: Reconstruction term ensures information preservation while Sinkhorn distance enforces distribution matching. Trade-off parameter β is critical for balancing accuracy and fairness.
  - Quick check: What happens to fairness (ADPD) when β increases too much?

## Architecture Onboarding

- **Component map**: Encoder h_φ (MLP/CNN) -> Decoder g_ψ (symmetric) -> Target sampler (truncated Gaussian) -> Sinkhorn solver -> Score function (norm)

- **Critical path**: Sample batch from each demographic group → Sample equal-sized batch from target distribution → Compute Sinkhorn distance between projected samples and target samples → Compute reconstruction loss → Backpropagate combined loss

- **Design tradeoffs**: Im-FairAD vs Ex-FairAD (implicit vs explicit fairness); β (reconstruction weight) affects fairness-accuracy balance; Sinkhorn entropy coefficient α=0.1

- **Failure signatures**: Skewed data splits increase ADPD; high reconstruction weight prevents group convergence to target; undefined fairness ratio when P(Score(X) > t | S=s) = 0

- **First 3 experiments**: 1) Sanity check on balanced split (COMPAS, AUC~64%, ADPD<5%), 2) Skew robustness test (800/200 split), 3) β ablation on Adult dataset (sweep β values)

## Open Questions the Paper Calls Out

- **Individual vs Group Fairness**: The method focuses on group fairness but doesn't address whether it can be extended to ensure individual fairness, where similar individuals receive similar treatment.

- **Theoretical Bounds for Transfer Assumptions**: The paper relies on Assumptions 2-3 for fairness transfer but doesn't provide theoretical bounds for constants μ and τ that govern the degradation of fairness on unseen anomalies.

- **Optimal Target Distribution**: The choice of truncated isotropic Gaussian is heuristic; it's unclear whether this is optimal for all data modalities or if performance could improve with adaptive target distributions.

## Limitations

- The fairness transfer mechanism relies heavily on assumptions that are reasonable but unproven in extreme cases where anomalies have fundamentally different structure from normal samples
- Architecture details (specific layer dimensions, latent space size) are underspecified, requiring assumptions for reproduction
- No comparison against fairness-aware supervised anomaly detection methods, limiting claims about superiority in the unsupervised setting

## Confidence

- **High**: Core mechanism of projection to shared target distribution achieving fairness on normal data (validated by Proposition 2 and experimental results)
- **Medium**: Claims about fairness transfer to anomalous data (supported by experiments but dependent on unverified assumptions)
- **Medium**: Ablation results showing β parameter impact (Figure 6), though exact hyperparameter settings are not fully specified

## Next Checks

1. **Extreme skew test**: Evaluate FairAD on highly skewed splits (e.g., 900/100) to verify robustness of ADPD metric under severe imbalance

2. **Assumption boundary test**: Design adversarial anomalies that are perturbations of normal samples but project to origin, testing Assumption 1 limits

3. **Alternative target distribution**: Replace truncated Gaussian with other distributions (uniform, Laplace) to verify if fairness guarantees depend on specific target choice