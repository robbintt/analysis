---
ver: rpa2
title: Self-Attention at Constant Cost per Token via Symmetry-Aware Taylor Approximation
arxiv_id: '2602.00294'
source_url: https://arxiv.org/abs/2602.00294
tags:
- token
- conventional
- formulation
- taylor
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a new method for computing self-attention with
  constant per-token cost, regardless of sequence length. The authors derive an efficient
  approximation of the exponential kernel in self-attention using Taylor expansion,
  reorganizing the computation into expressions over symmetric tensor chains.
---

# Self-Attention at Constant Cost per Token via Symmetry-Aware Taylor Approximation

## Quick Facts
- **arXiv ID:** 2602.00294
- **Source URL:** https://arxiv.org/abs/2602.00294
- **Reference count:** 30
- **Primary result:** New method computes self-attention with constant per-token cost regardless of sequence length

## Executive Summary
This paper presents a novel approach to self-attention that achieves constant computational cost per token, independent of sequence length. The method approximates the exponential kernel in self-attention using a Taylor expansion, reorganizing computation into expressions over symmetric tensor chains. By exploiting the symmetry structure of outer products, the authors create a minimal polynomial-kernel feature basis that allows attention to be computed in fixed time and memory per token. Experiments demonstrate that the method accurately recovers conventional attention to within typical floating-point precision using only 4 Taylor terms, while reducing memory use and compute time by orders of magnitude for long sequences.

## Method Summary
The authors approximate the exponential kernel exp(q^T k / c) in self-attention using a Taylor series expansion. Instead of computing all d_K^p monomials for power p, they exploit the symmetry of outer products q^⊗p and k^⊗p by retaining only the "upper hyper-triangular" elements of symmetric tensors. This creates a minimal polynomial feature basis Φ_p that maps softmax operations to weighted inner products in compressed feature space. The sequential aggregation of key-value products can be decoupled from the query and computed via recurrent accumulation of two fixed-size states (S for key-value products, Z for key normalizers), similar to RNNs. The per-token cost decreases inversely with head size, enabling more heads without quadratic memory penalties.

## Key Results
- Accurately recovers conventional attention to within typical floating-point precision using only 4 Taylor terms
- Reduces memory use and compute time by orders of magnitude for long sequences
- Enables unbounded token generation at modest fixed cost per token
- Per-token costs decrease inversely with head size, enabling application over greater numbers of heads

## Why This Works (Mechanism)

### Mechanism 1
The exponential kernel in self-attention can be approximated as a finite sum of polynomial inner products with a constrained feature basis. The authors expand exp(q^T k / c) into a Taylor series and exploit symmetry of outer products q^⊗p and k^⊗p. By retaining only the "upper hyper-triangular" elements of these symmetric tensors, they construct a minimal polynomial feature basis Φ_p that maps softmax to weighted inner products in compressed feature space. Core assumption: Taylor truncation error remains within acceptable bounds for low orders (P≈4) and inputs don't regularly fall into regions where higher-order terms dominate. Break condition: If d_K is extremely large or input magnitude scaling is incorrect, the Taylor approximation may require excessive terms to converge.

### Mechanism 2
Converting attention to weighted inner products enables recurrent accumulation, replacing growing KV cache with fixed-size state. The sequential aggregation ΣΦ(k)v^T can be decoupled from the query. The system maintains two accumulated states: S (key-value products) and Z (key normalizers), updated incrementally with constant cost per token. Core assumption: Recurrent accumulation remains numerically stable over millions of steps without gradient instability. Break condition: Numerical overflow in accumulated state S or Z during unbounded generation if normalization techniques fail.

### Mechanism 3
Per-token costs decrease inversely with head size, inverting scaling dynamics of conventional attention. The size of hidden state depends on combinations of head dimension d_K. Smaller heads reduce combinatorial cost of polynomial basis significantly, allowing architectures to scale by increasing number of heads rather than size of each head. Core assumption: Model quality can be maintained using many small heads rather than fewer large heads. Break condition: If tasks require high-dimensional key vectors to resolve fine-grained semantic distinctions, combinatorial growth may render fixed cost prohibitive.

## Foundational Learning

- **Concept: Linear Attention / Kernel Trick**
  - Why needed here: This paper is a specific instance of linear attention. Understanding that Softmax(QK^T)V can be rewritten as feature maps φ(Q)(φ(K)^T V) is required to grasp how O(1) complexity is achieved.
  - Quick check question: Can you explain why the associative property of matrix multiplication allows Linear Attention to avoid the N^2 memory bottleneck?

- **Concept: Symmetric Tensors & Combinatorics**
  - Why needed here: Core efficiency gain comes from avoiding full computation of outer products. Understanding why q_i q_j is distinct from q_j q_i in storage but identical in value shows why "tight packing" works.
  - Quick check question: If a key vector has dimension d=4 and we use power p=2, how many unique monomials exist compared to total raw monomials?

- **Concept: Taylor Series Approximation**
  - Why needed here: Precision of entire method hinges on how well few polynomial terms approximate exponential function.
  - Quick check question: Why does scale factor c = √d_K matter for convergence of Taylor series in this context?

## Architecture Onboarding

- **Component map:** Input Projectors (Q, K, V) -> Feature Maps (Φ_p) -> State Buffers (S_p, Z_p) -> Aggregator
- **Critical path:** Feature Map computation (Φ_p) is the critical path where current PyTorch implementations make unnecessary temporary copies
- **Design tradeoffs:** Smaller heads = drastically smaller hidden states, trading per-head representational resolution for efficiency; Precision (P) vs FLOPs - P=4 for Float16, lower saves compute but risks divergence; Parallelism - P scans can run in parallel, trading width for latency
- **Failure signatures:** "NaN in State" from large norm inputs causing Taylor terms to explode; Drift at Long Context if approximation error accumulates; Slow Inference if Feature Map isn't fused, negating theoretical speedups
- **First 3 experiments:**
  1. Unit Test: Feature Map Correctness - Verify Φ_p(q)^T Φ_p(k) ≈ (q^T k)^p for random vectors
  2. Precision Floor Analysis - Profile reconstruction error vs Float64 attention across sequence lengths
  3. Latency vs Context Length - Benchmark against FlashAttention, plot latency from 1k to 100k tokens

## Open Questions the Paper Calls Out
- How does training with this attention mechanism affect model convergence dynamics compared to conventional softmax attention?
- Can the approach be extended to other analytic kernels beyond the exponential?
- How does downstream task performance scale with approximation order P and head dimension dK in real-world applications?
- Can exploiting hierarchical structure of symmetric indices yield further asymptotic or practical efficiency gains?

## Limitations
- The approximation error bounds across the full input space are not rigorously established
- Core implementation details for computing index mapping M_p and weight matrices C_p are not fully specified
- Numerical stability during unbounded generation is stated but not empirically validated beyond standard sequence lengths

## Confidence
- **High Confidence:** The theoretical framework for converting self-attention to linear operations via Taylor expansion is mathematically sound
- **Medium Confidence:** Claims about per-token cost decreasing inversely with head size are supported by combinatorial analysis but practical implications for model quality are not fully explored
- **Low Confidence:** Claims about orders-of-magnitude reductions in compute time and memory for long sequences lack validation in full-scale model training scenarios

## Next Checks
1. Implement and verify the exact algorithm for computing minimal polynomial basis - generate M_p and C_p for small dimensions and test that Φ_p(q)^T C_p Φ_p(k) ≈ (q^T k)^p within 1e-6 precision
2. Run linear attention implementation for sequences of 1M tokens with fixed inputs, monitoring accumulated states S and Z for numerical drift, overflow, or underflow
3. Implement symmetry-aware Taylor approximation as drop-in replacement for standard attention in small Transformer model (2-layer, 4-head), train on language modeling, and compare training stability and inference speed against baseline