---
ver: rpa2
title: 'CLAQS: Compact Learnable All-Quantum Token Mixer with Shared-ansatz for Text
  Classification'
arxiv_id: '2510.06532'
source_url: https://arxiv.org/abs/2510.06532
tags:
- quantum
- classical
- token
- claqs
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CLAQS, a compact, fully quantum token mixer
  for text classification that jointly learns complex-valued mixing and nonlinear
  transformations within a unified quantum circuit. To enable stable end-to-end optimization,
  the authors apply L1 normalization to regulate amplitude scaling and introduce a
  two-stage parameterized quantum architecture that decouples shared token embeddings
  from a window-level quantum feed-forward module.
---

# CLAQS: Compact Learnable All-Quantum Token Mixer with Shared-ansatz for Text Classification

## Quick Facts
- **arXiv ID:** 2510.06532
- **Source URL:** https://arxiv.org/abs/2510.06532
- **Reference count:** 8
- **Primary result:** 91.64% accuracy on SST-2 and 87.08% on IMDB with 8 qubits

## Executive Summary
CLAQS introduces a compact, fully quantum token mixer for text classification that jointly learns complex-valued mixing and nonlinear transformations within a unified quantum circuit. The architecture employs a two-stage design with shared token embeddings and a window-level quantum feed-forward module, achieving state-of-the-art results on SST-2 and IMDB benchmarks while using only eight data qubits. The model applies L1 normalization to regulate amplitude scaling and uses QSVT polynomials as learnable nonlinearities, outperforming both classical Transformer baselines and strong hybrid quantum-classical counterparts.

## Method Summary
CLAQS operates on classical token embeddings mapped to quantum circuits via a shared ansatz-14 parameterized quantum circuit. The architecture processes text in sliding windows, where each token embedding maps to angles parameterizing a shared PQC per window. An LCU mixer with learnable complex coefficients combines token unitaries, followed by QSVT polynomial transformation and a separate window-level quantum feed-forward module. XYZ multi-axis measurements produce feature vectors fed through an MLP classifier. The model uses 8 data qubits, AdamW optimization with cosine-annealed learning rate, and TorchQuantum framework on RTX A5000 GPU.

## Key Results
- Achieves 91.64% accuracy on SST-2 sentiment classification
- Achieves 87.08% accuracy on IMDB sentiment classification
- Outperforms both classical Transformer baselines and hybrid quantum-classical models

## Why This Works (Mechanism)

### Mechanism 1: Learnable LCU Mixing with L1 Normalization Stabilizes Training
The Linear Combination of Unitaries (LCU) mixer uses learnable complex coefficients to encode phase-sensitive token interactions. L1 normalization per forward pass constrains coefficient magnitudes, satisfying block-encoding assumptions and preventing numerical instability during optimization.

### Mechanism 2: QSVT Polynomial Acts as Learnable Quantum Nonlinearity
Quantum Singular Value Transformation with trainable polynomial coefficients provides data-driven nonlinear transformation on mixed quantum states. This enables higher-order token interactions beyond linear superposition, approximating semantic composition functions needed for classification.

### Mechanism 3: Two-Stage Architecture Decouples Embedding from Consumption
Separating token-level shared embedding ansatz from window-level quantum feed-forward increases modeling capacity without proportional parameter growth. This mirrors Transformer's attention→FFN structure but in quantum state space.

## Foundational Learning

- **Linear Combination of Unitaries (LCU):**
  - Why needed: Core mechanism for token mixing—unitaries represent tokens, LCU combines them with learnable weights.
  - Quick check: Can you explain why the sum of unitaries Σ b_j U_j requires normalized coefficients for block-encoding validity?

- **Quantum Singular Value Transformation (QSVT):**
  - Why needed: Provides learnable nonlinearity layer; understanding polynomial transformations on quantum operators is essential.
  - Quick check: How does QSVT differ from simply applying a parameterized rotation gate as a nonlinearity?

- **Block Encoding and Post-Selection:**
  - Why needed: LCU realization requires auxiliary qubits for selection/preparation unitaries; measurement success probability affects trainability.
  - Quick check: What does the post-selection regularizer (PSR) target τ=0.5 imply about expected measurement outcomes?

## Architecture Onboarding

- **Component map:** Input tokens → Classical embeddings e_w → Angle mapping θ_w = W_E e_w → Shared ansatz-14 PQC U(θ_w) per token → {U_j} window of unitaries → LCU mixer M(b̃) with L1-normalized coefficients → QSVT transformation P_c(M) → Quantum feed-forward U_FF(ϕ) → XYZ multi-axis readout → 3q-dimensional feature vector → 2-layer MLP with dropout → C-class logits

- **Critical path:**
  1. Implement ansatz-14 template (RY/c-RX pattern, ℓ layers, q=8 qubits)
  2. Build differentiable LCU layer with runtime L1 normalization
  3. Implement QSVT via repeated M application (classical simulation) or controlled phases (hardware)
  4. Add separate U_FF PQC before measurement
  5. Wire XYZ expectations through MLP head

- **Design tradeoffs:**
  - Window length n vs. parameter count: Parameters scale O(n), so n=256 (IMDB) uses 454 vs. n=128 (SST-2) uses 326 attention parameters
  - Polynomial degree d vs. expressivity: Higher d captures more complex interactions but increases circuit depth
  - Strong regularization (mask + L2 smoothing) vs. flexibility: Ablation shows 21-point SST-2 drop when over-constrained

- **Failure signatures:**
  - Training divergence with NaN losses → Check L1 normalization is applied per forward pass
  - Accuracy near random (50%) → Verify ansatz is shared across tokens, not instantiated separately
  - Post-selection probability collapsing toward 0 or 1 → Adjust PSR regularizer weight λ_ps or target τ

- **First 3 experiments:**
  1. **Sanity check:** Run CLAQS on SST-2 with default settings (window=128, degree=5, q=8) and verify ~91% accuracy matches paper; this validates implementation correctness.
  2. **Ablation—aggregation mechanism:** Replace LCU+QSVT with mean_logits pooling; expect ~12-13 point drop (78.9% per Table 2), confirming quantum mixer contribution.
  3. **Scaling test:** Vary window length from 64 to 256 on IMDB subset; verify parameter count scales linearly and accuracy remains stable or improves with longer context.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does CLAQS maintain performance advantages on actual NISQ hardware under realistic noise and shot-based readout constraints?
- Basis in paper: [explicit] The authors explicitly state in the Future Directions that they "will validate CLAQS on quantum hardware with shot-based readout under realistic noise."
- Why unresolved: All reported results are derived from GPU-based simulations (TorchQuantum) which abstract away physical noise, decoherence, and the stochastic variance of shot-based measurement.
- What evidence would resolve it: Successful replication of classification accuracy on a physical QPU (e.g., IBM or IonQ) with comparable qubit counts.

### Open Question 2
- Question: Does the learnable LCU–QSVT mixer generalize to complex NLP tasks requiring structural reasoning or token-level prediction?
- Basis in paper: [explicit] The authors note the need to "broaden the empirical scope beyond sentiment analysis to natural language inference, topic classification, and token-level tasks."
- Why unresolved: The current study is restricted to binary sentiment classification (SST-2, IMDB), which may rely heavily on lexical keyword presence rather than complex, long-range token interactions.
- What evidence would resolve it: Benchmark results on datasets requiring deeper logic (e.g., MNLI) or token granularity (e.g., NER) showing competitive performance.

### Open Question 3
- Question: Can the architecture scale effectively through multi-head and multi-layer variants without destabilizing the ℓ1 normalization or gradient flow?
- Basis in paper: [explicit] The paper lists plans to "explore scaling via multi-head and multi-layer variants alongside parameter-efficient training."
- Why unresolved: The current implementation is a compact, single-stage mixer; it is unclear if depth-stacking would amplify barren plateaus or conflict with the stability constraints.
- What evidence would resolve it: Training dynamics and accuracy metrics for deep (e.g., 4+ layer) CLAQS models demonstrating stable convergence.

## Limitations

- **Major hyperparameter uncertainty:** Critical hyperparameters like ansatz depth ℓ, polynomial degree d, and regularization strength lack ablation studies.
- **Limited task scope:** Results are restricted to binary sentiment classification without testing on more complex NLP tasks requiring structural reasoning.
- **Hardware validation missing:** All results are from GPU simulations; performance under realistic NISQ noise and shot-based readout is unknown.

## Confidence

- **High Confidence:** Core architectural framework (two-stage design with shared embedding ansatz and window-level feed-forward) is well-specified and reproducible. Parameter count calculations are verifiable and internally consistent.
- **Medium Confidence:** Claims about LCU+QSVT outperforming classical baselines are supported by experimental results, but magnitude of improvement could vary with different classical models or hyperparameter tuning.
- **Low Confidence:** Claim that this represents fundamental advance in quantum NLP is limited by lack of comparison to more recent classical models and absence of scaling studies beyond two benchmark datasets.

## Next Checks

1. **LCU Coefficient Stability Ablation:** Train CLAQS with and without L1 normalization on SST-2 while monitoring coefficient magnitude distributions and training loss curves to test whether L1 constraint is truly essential for preventing gradient instability.

2. **QSVT Degree Sensitivity Study:** Systematically vary polynomial degree d from 2 to 8 on both SST-2 and IMDB, measuring accuracy, training stability, and parameter efficiency to clarify whether degree 5 is optimal.

3. **Post-Selection Regularizer Target Sweep:** Run SST-2 experiments with τ values ranging from 0.3 to 0.7 while keeping other hyperparameters constant to reveal sensitivity of training stability and final accuracy to the PSR target.