---
ver: rpa2
title: 'Mechanistic Interpretability with SAEs: Probing Religion, Violence, and Geography
  in Large Language Models'
arxiv_id: '2509.17665'
source_url: https://arxiv.org/abs/2509.17665
tags:
- features
- religion
- language
- across
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how religious identities are internally
  represented in large language models (LLMs) using mechanistic interpretability.
  The authors analyze latent feature activations across five models (GPT-2, Gemma-2,
  and Llama3.1) using Sparse Autoencoders (SAEs) via the Neuronpedia API, examining
  overlap between religion- and violence-related concepts and probing semantic patterns.
---

# Mechanistic Interpretability with SAEs: Probing Religion, Violence, and Geography in Large Language Models

## Quick Facts
- arXiv ID: 2509.17665
- Source URL: https://arxiv.org/abs/2509.17665
- Authors: Katharina Simbeck; Mariam Mahran
- Reference count: 33
- Primary result: Islam shows consistently higher association with violence-related features across five analyzed models

## Executive Summary
This paper investigates how religious identities are internally represented in large language models (LLMs) using mechanistic interpretability. The authors analyze latent feature activations across five models (GPT-2, Gemma-2, and Llama3.1) using Sparse Autoencoders (SAEs) via the Neuronpedia API, examining overlap between religion- and violence-related concepts and probing semantic patterns. Islam shows consistently higher association with violence-related features across all models, with Violence Association Index (VAI) scores above 100 compared to other religions (94-117 range). While all five religions show comparable internal cohesion, geographic associations largely reflect real-world distributions but with Western bias (Europe/North America most frequent). Crime-related activation texts show Islam with highest proportion in most models (2.44-3.46%), though some models show Hinduism higher. The findings reveal how models embed both factual distributions and cultural stereotypes, highlighting the value of structural analysis for auditing internal representations beyond just outputs.

## Method Summary
The study uses pre-trained Sparse Autoencoders (SAEs) from the Neuronpedia API to analyze latent feature activations in five language models. Researchers collect top-20 activating features for religion-related prompts (Islam, Hinduism, Buddhism, Christianity, Judaism) and violence-related prompts, then compute overlap between these feature sets. The Violence Association Index (VAI) normalizes raw overlap by each model's mean across religions. They also collect activation texts for religion-related features and apply keyword-based searches for crime terms and geographic regions to analyze semantic patterns.

## Key Results
- Islam shows consistently higher violence association across all five models (VAI scores above 100)
- All five religions demonstrate comparable internal cohesion in feature overlap
- Geographic associations reflect real-world distributions but show Western bias (Europe/North America most frequent)
- Islam has highest proportion of crime-related activation texts in most models (2.44-3.46%)
- Hinduism shows higher crime associations than Islam in GPT-2 small and Llama3.1-8B models

## Why This Works (Mechanism)

### Mechanism 1
Sparse Autoencoders (SAEs) decompose polysemantic LLM neurons into interpretable monosemantic features, enabling isolation of concept-specific activations. SAEs are shallow networks trained to reconstruct LLM activations under sparsity constraints. The encoder identifies which latent features activate for a given input; the decoder reconstructs the original state. Sparsity ensures only a few features fire per input, reducing polysemanticity and making individual features more likely to correspond to single semantic concepts.

### Mechanism 2
Shared latent feature activations between religion-related and violence-related prompts reveal internalized conceptual associations beyond output-level behavior. For each religion, the authors collect the top 20 activating features per prompt, aggregate into a set per religion, and compute overlap with the feature set activated by violence/crime prompts. The Violence Association Index (VAI) normalizes raw overlap by the model's mean across religions (100 = average), enabling cross-model comparison.

### Mechanism 3
Semantic analysis of top activating texts validates what concepts features encode and reveals patterns (crime, geography) that may not appear in explicit outputs. The Neuronpedia API returns example texts that most strongly activate each feature. The authors collect these for religion-related features and apply keyword-based searches for crime terms (e.g., "terrorism," "extremist") and geographic regions (e.g., "Europe," "Middle East"). Match percentages reveal how often crime/geography semantics appear in the feature's activation context.

## Foundational Learning

- **Concept**: Polysemanticity in neural networks
  - **Why needed here**: SAEs are explicitly designed to address polysemanticity, where single neurons respond to multiple unrelated concepts. Without understanding this problem, the rationale for SAEs is unclear.
  - **Quick check question**: Can you explain why a single neuron activating for both "mosque" and "terrorism" would complicate interpretability?

- **Concept**: Sparse coding / sparsity constraints
  - **Why needed here**: The core mechanism of SAEs relies on sparsity penalties to force few features to activate per input. Understanding this is essential for tuning and debugging SAEs.
  - **Quick check question**: If an SAE has 65,536 features but only ~20 activate per prompt, what does this imply about representation distribution?

- **Concept**: Residual stream vs. attention layer activations
  - **Why needed here**: The paper uses SAEs trained at different positions (residual stream, attention). These capture different information flows in transformers.
  - **Quick check question**: Would you expect attention-layer SAEs to capture more syntactic or more semantic patterns compared to residual-stream SAEs?

## Architecture Onboarding

- **Component map**: Prompts -> LLM backbone (GPT-2, Gemma-2, Llama3.1) -> SAE layer (pre-trained from Neuronpedia) -> Feature extraction -> Analysis layer (overlap, semantic validation)

- **Critical path**: 1. Define prompt sets for each religion and violence category 2. Query Neuronpedia API for top activating features per prompt 3. Aggregate feature sets per religion; compute overlap with violence feature set 4. Normalize via VAI = (religion-violence overlap / mean overlap) Ã— 100 5. Collect activation texts for each feature; run keyword searches for crime and geography

- **Design tradeoffs**: Prompt design uses fixed sentence templates for consistency but may introduce template artifacts; overlap metric uses discrete counts instead of cosine similarity due to sparsity; pre-trained SAEs limit control over training distribution and hyperparameters

- **Failure signatures**: Very low intra-group overlap (<10 features shared) suggests religion not encoded as coherent concept; high crime percentages for all religions suggests keyword list too broad; geographic analysis showing uniform distributions may indicate features not religion-specific

- **First 3 experiments**: 1. Replicate analysis for new concept axis (gender, profession) to validate methodology generalizes 2. Compare residual-stream vs. attention-layer SAEs to identify which captures religion-violence associations more strongly 3. Test prompt sensitivity by varying sentence templates or using non-English religion terms

## Open Questions the Paper Calls Out

### Open Question 1
Do internal religion-violence associations in latent SAE features causally influence downstream model outputs, such as generated text, recommendations, or decisions? The discussion states "downstream harms do not always stem from what models say, but from how they internally prioritize and structure information," but the study does not test whether these internal associations actually affect outputs.

### Open Question 2
What training data or architectural factors cause Hinduism to show higher crime associations than Islam in some models (GPT-2 small, Llama3.1-8B) while Islam shows higher associations in others? The crime analysis notes "In GPT2-small and Llama3.1-8B, Hinduism showed unexpectedly higher rates than Islam" and suggests "training data co-occurrence patterns shaped by regional sociopolitical discourse," but does not identify specific causal factors.

### Open Question 3
Do SAE-based religion-violence associations generalize beyond English-language models to multilingual settings, and do they reflect language-specific cultural biases? The conclusion states "Future work could extend this analysis to more religions, multilingual models, or other identity-linked concepts."

## Limitations

- Relies on pre-trained SAEs from Neuronpedia API, limiting control over training distribution and hyperparameters
- Fixed sentence template approach may introduce artifacts that don't reflect natural language processing
- Keyword-based semantic validation may miss non-English terms and nuanced semantic relationships
- Does not systematically compare residual-stream vs. attention-layer SAEs or test methodology robustness

## Confidence

- **High Confidence**: Islam consistently shows higher violence association across all five models (VAI scores above 100)
- **Medium Confidence**: All five religions demonstrate comparable internal cohesion in feature overlap
- **Low Confidence**: Geographic association findings showing Western bias are based on keyword searches that may miss non-English terms

## Next Checks

1. **Prompt Robustness Test**: Repeat the analysis using varied sentence templates and non-English religion terms to assess whether the Islam-violence association persists across different linguistic framings.

2. **Layer Position Comparison**: Systematically compare residual-stream vs. attention-layer SAEs for the same model to determine which position captures religion-violence associations more strongly, and whether this varies by religion.

3. **Concept Generalization Validation**: Apply the same methodology to a different conceptual axis (e.g., gender, profession, or political ideology) to determine whether the methodology reliably detects known associations or cultural stereotypes in LLM representations.