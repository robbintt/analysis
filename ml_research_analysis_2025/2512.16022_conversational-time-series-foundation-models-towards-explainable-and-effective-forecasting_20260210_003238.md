---
ver: rpa2
title: 'Conversational Time Series Foundation Models: Towards Explainable and Effective
  Forecasting'
arxiv_id: '2512.16022'
source_url: https://arxiv.org/abs/2512.16022
tags:
- uni00000013
- uni00000011
- optimization
- ensemble
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TSOrchestr addresses the challenge of selecting optimal ensemble
  weights for time series foundation models by positioning a large language model
  as an intelligent judge that reasons about weight quality rather than directly predicting
  values. The system uses R1-style fine-tuning guided by SHAP-based faithfulness scores
  to teach the LLM to interpret ensemble weights as meaningful causal statements about
  temporal dynamics, enabling it to evaluate and refine weight configurations through
  iterative multi-turn conversations.
---

# Conversational Time Series Foundation Models: Towards Explainable and Effective Forecasting

## Quick Facts
- **arXiv ID:** 2512.16022
- **Source URL:** https://arxiv.org/abs/2512.16022
- **Reference count:** 40
- **Primary result:** LLM-guided ensemble weight optimization achieves rank 7.3 MASE and 8.2 CRPS, 25.5% improvement over best individual model

## Executive Summary
TSOrchestr addresses the challenge of selecting optimal ensemble weights for time series foundation models by positioning a large language model as an intelligent judge that reasons about weight quality rather than directly predicting values. The system uses R1-style fine-tuning guided by SHAP-based faithfulness scores to teach the LLM to interpret ensemble weights as meaningful causal statements about temporal dynamics. Evaluated on the GIFT-Eval benchmark across 23 datasets and 97 settings, the approach achieves state-of-the-art performance while providing interpretable explanations for weight selection decisions.

## Method Summary
The method employs a two-stage R1-style fine-tuning process on Qwen-2.5-3B-Instruct: supervised fine-tuning (SFT) for 500 steps followed by group relative policy optimization (GRPO) for 50 steps. Foundation models (Moirai-2, Sundial, Toto, TabPFN-TS) are optimized using SLSQP with sum-to-one and non-negativity constraints. The LLM acts as a metacognitive controller, evaluating weights across three dimensions—alignment, matching, and future assessment—then voting to continue optimization or accept. Faithfulness scores ensure explanations genuinely reflect causal contributions via SHAP values computed on temporal decomposition.

## Key Results
- Achieves rank 7.3 on MASE and 8.2 on CRPS across GIFT-Eval benchmark
- 25.5% improvement over best individual model (Moirai-2)
- Establishes new state-of-the-art performance with interpretable explanations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** An LLM acting as a metacognitive controller over numerical optimization produces better ensemble weights than pure metric-based selection.
- **Mechanism:** The agent evaluates weights across three dimensions—alignment, matching, and future assessment—then votes to continue optimization with different metrics or accept. This transforms static SLSQP optimization into adaptive weighting that can anticipate regime shifts.
- **Core assumption:** The LLM can meaningfully reason about the gap between historical validation windows and future forecasting regimes using metadata patterns.
- **Evidence anchors:** Abstract states "trained agent then engages in iterative, multi-turn conversations to perform forward-looking assessments," and section 3.3 describes the three-dimensional evaluation. TimeCopilot independently validates the agentic forecasting approach.

### Mechanism 2
- **Claim:** SHAP-based faithfulness scores during GRPO training prevent the LLM from generating plausible-but-wrong justifications for weight decisions.
- **Mechanism:** The system computes SHAP values for temporal components and correlates them with LLM-expressed importance. The reward function includes r_faith = PCC(CE(x,C), EE(x,C))—Pearson correlation between SHAP-derived causal effects and explanation-implied effects.
- **Core assumption:** SHAP values computed on temporal decomposition accurately reflect true causal contributions to ensemble performance.
- **Evidence anchors:** Abstract mentions "R1-style finetuning process, guided by SHAP-based faithfulness scores," and section 3.4.2 details the faithfulness score computation. No direct corpus comparison available—this appears to be a novel contribution.

### Mechanism 3
- **Claim:** Ensemble diversity combined with temporal incompatibility in time series creates theoretical advantage that no single model can achieve.
- **Mechanism:** The Temporal Incompatibility Index I_T(D) measures regime heterogeneity. Theorem 3.1 shows ensemble advantage scales with I_T and model diversity M: Ω(I_T, M) = I_T · log(M) / (1 + exp(-κ·I_T)).
- **Core assumption:** The candidate pool contains models with genuinely diverse architectural strengths.
- **Evidence anchors:** Section 3.2 presents the theoretical theorem, and appendix C.3 compares different model combinations showing comparable performance. "One-Embedding-Fits-All" confirms "no single TSFM excels universally."

## Foundational Learning

- **Concept: SHAP Values and Feature Attribution**
  - Why needed here: The faithfulness reward requires understanding how Shapley values quantify each temporal component's contribution to model performance.
  - Quick check question: Can you explain why a negative SHAP value for "seasonality" means the seasonal component improves (reduces) forecast error?

- **Concept: Constrained Optimization (SLSQP)**
  - Why needed here: Weight optimization uses Sequential Least Squares Programming with sum-to-one and non-negativity constraints.
  - Quick check question: What happens to the ensemble if SLSQP converges to a degenerate solution like [1.0, 0.0, 0.0]?

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: The R1-style fine-tuning uses GRPO, an actor-only PPO variant that estimates advantages through group comparisons rather than a separate critic.
  - Quick check question: How does GRPO's group-normalized advantage Â(y,x) = (r(y,x) - r̄(x)) / σ_r(x) differ from standard PPO advantage estimation?

## Architecture Onboarding

- **Component map:** Time Series Input → Foundation Model Pool (Moirai, Sundial, Toto) → SLSQP Weight Optimization (per metric) → LLM Reasoning Agent (Qwen-2.5-3B fine-tuned) → Multi-turn Conversation ←→ User Proxy (validation loop) → SHAP Faithfulness Validator → Final Ensemble Weights → Forecast Output

- **Critical path:** The GRPO training pipeline (Section 3.4) is the bottleneck. You need: (1) 2,800+ synthetic optimization trajectories, (2) SHAP computation for each, (3) teacher model for reasoning traces, (4) faithfulness filtering at τ > 0.8.

- **Design tradeoffs:**
  - 3-model vs 4-model ensemble: Paper shows 3-model (Moirai-Sundial-Toto) hits sweet spot—4-model adds TabPFN-TS with high computational cost for marginal gain
  - Qwen-2.5-3B vs larger models: Fine-tuned 3B matches GPT-4o/Sonnet-3.7 on weight selection accuracy while being deployable in resource-constrained environments
  - SLSQP vs L-BFGS-B: Ablation shows L-BFGS-B underperforms on non-convex optimization landscapes

- **Failure signatures:**
  - "Mirage Trend" (Appendix D.2): Agent detects when validation spike is volatility noise, not trend shift—SHAP shows near-zero trend contribution despite good numeric performance
  - Premature termination: Model stops at local improvement when Δ < τ but genuine convergence not reached
  - Faithfulness collapse: LLM generates correct decisions with wrong justifications (detected by r_faith < threshold)

- **First 3 experiments:**
  1. Reproduce single-domain baseline: Run TSOrchestra on one dataset (e.g., ETT1/H) with fixed metric optimization (no LLM voting). Compare against random metric selection to isolate agent contribution.
  2. Ablate faithfulness reward: Train with r = r_base + α·r_conf (no r_faith) and measure explanation quality degradation on held-out trajectories.
  3. Test model pool swap: Replace Toto with Chronos in the candidate pool on 5 datasets. Verify the framework's model-agnosticism claim holds.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can probabilistic weight distributions over ensemble members improve uncertainty quantification compared to the current deterministic weight assignments?
  - Basis in paper: Authors state future work will explore "developing probabilistic weight distributions for improved uncertainty quantification."
  - Why unresolved: Current SLSQP optimization produces point estimates for weights lacking calibration for prediction intervals.

- **Open Question 2:** How can the framework be extended to handle online adaptation under continuous concept drift without requiring explicit recalibration triggers?
  - Basis in paper: Authors identify "online adaptation for concept drift" as a future direction; current system relies on periodic calibration when regime shifts are detected.
  - Why unresolved: The amortized calibration approach assumes batch recalibration periods, but real-world streaming scenarios exhibit gradual distribution shifts.

## Limitations

- Faithfulness framework assumes SHAP values from STL decomposition accurately capture causal contributions, but temporal decomposition quality varies significantly across datasets
- Multi-turn LLM evaluation relies on synthetic reasoning traces from GPT-4o, raising questions about transfer to real-time forecasting scenarios
- Theoretical advantage bounds assume perfect model diversity, which may not hold in practice with foundation model pretraining convergence

## Confidence

- **High Confidence:** Ensemble performance improvements (25.5% MASE gain), constrained optimization methodology, and multi-turn conversation framework are well-specified and reproducible.
- **Medium Confidence:** The faithfulness reward mechanism, forward-looking assessment capabilities, and theoretical advantage bounds require careful validation across diverse temporal regimes.
- **Low Confidence:** SHAP-based causal attribution reliability across heterogeneous time series, synthetic training data quality for agent reasoning, and real-world deployment performance under distributional shift.

## Next Checks

1. **Faithfulness Stress Test:** Generate synthetic trajectories where SHAP values contradict intuitive causal relationships (e.g., negative seasonality contribution improving forecast). Measure if LLM explanations align with SHAP vs human intuition.
2. **Domain Transfer Experiment:** Evaluate TSOrchestr on out-of-distribution temporal patterns (e.g., COVID-19 shock effects) to test forward-looking assessment reliability beyond GIFT-Eval benchmark.
3. **Diversity Sensitivity Analysis:** Systematically vary candidate model pool diversity (M) while holding individual model performance constant to empirically validate the Ω(I_T, M) scaling relationship from Theorem 3.1.