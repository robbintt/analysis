---
ver: rpa2
title: 'Federated Majorize-Minimization: Beyond Parameter Aggregation'
arxiv_id: '2507.17534'
source_url: https://arxiv.org/abs/2507.17534
tags:
- federated
- algorithm
- surrogate
- learning
- majorize-minimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a unified framework for federated majorize-minimization
  (MM) optimization, addressing the challenges of data heterogeneity, partial participation,
  and communication constraints in federated learning. The core method, FedMM, learns
  and aggregates surrogate function parameters rather than model parameters, which
  is shown to be more effective in many federated settings.
---

# Federated Majorize-Minimization: Beyond Parameter Aggregation
## Quick Facts
- arXiv ID: 2507.17534
- Source URL: https://arxiv.org/abs/2507.17534
- Authors: Aymeric Dieuleveut; Gersende Fort; Mahmoud Hegazy; Hoi-To Wai
- Reference count: 18
- Key outcome: Unified framework for federated majorize-minimization (FedMM) that learns and aggregates surrogate function parameters rather than model parameters, showing superior performance in handling data heterogeneity and communication constraints

## Executive Summary
This paper introduces a unified framework for federated majorize-minimization (MM) optimization that addresses the challenges of data heterogeneity, partial participation, and communication constraints in federated learning. The core method, FedMM, learns and aggregates surrogate function parameters rather than model parameters, which is shown to be more effective in many federated settings. The framework incorporates control variates to handle heterogeneity and uses compression for communication efficiency. Theoretical analysis provides convergence guarantees, demonstrating robustness to data heterogeneity without additional assumptions. The framework is validated on federated dictionary learning and federated optimal transport map computation, showing superior performance compared to naive aggregation approaches and other federated algorithms.

## Method Summary
The FedMM framework introduces a novel approach to federated optimization by learning and aggregating surrogate function parameters instead of directly aggregating model parameters. This is achieved through a majorize-minimization (MM) framework where each local device computes a surrogate function that upper bounds the true objective. The key innovation is that the surrogate parameters are aggregated across devices rather than the model parameters themselves. The framework handles data heterogeneity through control variates, which provide unbiased estimates of local gradients. Communication efficiency is achieved through compression techniques applied to the surrogate parameters. The method is designed to work under partial device participation and provides theoretical convergence guarantees for both convex and strongly convex objectives.

## Key Results
- FedMM shows improved convergence rates and objective values across various data settings compared to naive parameter aggregation approaches
- The framework demonstrates robustness to data heterogeneity without requiring additional assumptions beyond convexity
- Superior performance is demonstrated on federated dictionary learning and federated optimal transport map computation tasks
- Controlled update sizes in both parameter and surrogate spaces lead to more stable convergence

## Why This Works (Mechanism)
FedMM works by leveraging the structure of surrogate functions in the MM framework to overcome the limitations of direct parameter aggregation in federated learning. Instead of aggregating model parameters directly, which can be problematic under data heterogeneity, FedMM aggregates the parameters that define the surrogate functions. This approach naturally handles the differences in local objectives across devices because each device's surrogate function is tailored to its local data distribution. The use of control variates provides unbiased gradient estimates even when local objectives are heterogeneous, while compression techniques make communication more efficient without sacrificing convergence guarantees.

## Foundational Learning
- Majorize-Minimization (MM) algorithms: These are optimization methods that replace a difficult objective function with a simpler surrogate function that upper bounds the original objective at each iteration. Why needed: MM algorithms provide a framework for solving non-smooth or non-convex optimization problems by breaking them into simpler subproblems. Quick check: Verify that the surrogate function satisfies the MM conditions (upper bounds the objective and touches it at the current iterate).
- Control variates: These are auxiliary random variables used to reduce the variance of Monte Carlo estimators. Why needed: In federated learning with heterogeneous data, control variates help provide unbiased gradient estimates despite differences in local objectives. Quick check: Confirm that the control variate has expectation zero under the local data distribution.
- Communication compression: Techniques that reduce the amount of data transmitted between devices and the central server. Why needed: In federated learning, communication is often the bottleneck, so compression reduces the communication cost per iteration. Quick check: Verify that the compression operator satisfies the required unbiasedness and boundedness conditions.
- Surrogate parameter aggregation: The process of aggregating parameters that define surrogate functions rather than model parameters directly. Why needed: This approach is more robust to data heterogeneity because surrogate parameters can better capture local objective structures. Quick check: Ensure that aggregated surrogate parameters still satisfy the MM conditions.
- Partial device participation: The scenario where only a subset of devices participate in each training round. Why needed: In practical federated learning systems, not all devices are available or willing to participate in every round. Quick check: Verify that convergence guarantees hold under the assumed participation rate.

## Architecture Onboarding
Component map: Local devices -> Surrogate computation -> Parameter aggregation -> Global model update -> Compressed communication -> Next iteration

Critical path: The critical path in FedMM involves the local computation of surrogate functions, aggregation of surrogate parameters, and communication of compressed updates. Each iteration requires local devices to compute their surrogate functions based on their local data, send compressed surrogate parameters to the server, where they are aggregated to form a global surrogate, and then distributed back to devices for the next iteration.

Design tradeoffs: The main tradeoff in FedMM is between communication efficiency and convergence speed. Compression reduces communication costs but may slow convergence. The use of control variates adds computational overhead but improves robustness to heterogeneity. The choice of surrogate function affects both computational complexity and convergence properties.

Failure signatures: Potential failures include divergence due to poor surrogate function choice, slow convergence from excessive compression, and instability from improper control variate implementation. Convergence issues may arise when data heterogeneity is extreme or when participation rates are too low.

First experiments:
1. Compare FedMM with standard FedAvg on a simple convex federated learning problem with controlled heterogeneity
2. Test the impact of different compression levels on convergence speed and final objective value
3. Evaluate the performance of FedMM under varying participation rates to assess robustness to partial device availability

## Open Questions the Paper Calls Out
None

## Limitations
- The framework assumes strongly convex objectives, which may not hold in many practical federated learning scenarios
- Performance with non-convex objectives remains unclear, limiting applicability to deep learning scenarios
- While robustness to data heterogeneity is claimed, theoretical guarantees are primarily for strongly convex cases
- Extension to optimal transport problems is only demonstrated on specific applications without broader validation

## Confidence
High: Core FedMM methodology and its effectiveness in handling data heterogeneity through surrogate parameter learning
Medium: Theoretical convergence guarantees, particularly for non-convex settings
Low: Scalability and practical implementation in large-scale, real-world federated learning systems with millions of devices

## Next Checks
1. Empirical evaluation on non-convex federated learning problems, particularly in deep learning scenarios with realistic datasets
2. Scalability testing with thousands of participating devices and varying participation rates to assess practical feasibility
3. Comparison with state-of-the-art federated learning algorithms on diverse tasks beyond dictionary learning and optimal transport, including computer vision and natural language processing applications