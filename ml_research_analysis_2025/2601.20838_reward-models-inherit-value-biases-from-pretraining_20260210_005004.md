---
ver: rpa2
title: Reward Models Inherit Value Biases from Pretraining
arxiv_id: '2601.20838'
source_url: https://arxiv.org/abs/2601.20838
tags:
- gemma
- llama
- word
- what
- thing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Reward models inherit value biases from the LLMs on which they
  are built, shaping their behavior along multiple dimensions of human values such
  as agency and communion. By analyzing 10 leading open-weight RMs, we found systematic
  differences between Llama- and Gemma-based RMs, with Llama favoring agency-oriented
  values (e.g., freedom, success) and Gemma favoring communion-oriented values (e.g.,
  love, friendship).
---

# Reward Models Inherit Value Biases from Pretraining

## Quick Facts
- arXiv ID: 2601.20838
- Source URL: https://arxiv.org/abs/2601.20838
- Reference count: 40
- Primary result: Reward models inherit value biases from the LLMs on which they are built, shaping their behavior along multiple dimensions of human values such as agency and communion

## Executive Summary
Reward models (RMs) inherit value biases from the LLMs on which they are built, shaping their behavior along multiple dimensions of human values such as agency and communion. By analyzing 10 leading open-weight RMs, we found systematic differences between Llama- and Gemma-based RMs, with Llama favoring agency-oriented values (e.g., freedom, success) and Gemma favoring communion-oriented values (e.g., love, friendship). This bias originates from the base models themselves and persists even when trained with identical preference data. Experiments training our own RMs with ablations for preference data source and quantity showed that while sufficient data can reduce the bias, it is surprisingly durable and difficult to fully eliminate.

## Method Summary
The study analyzed 10 leading open-weight reward models (RMs) to identify systematic value biases. Researchers compared Llama-based and Gemma-based RMs using value lexicons (Abridged Hedonometer and Beppu-LDA) to measure agency- and communion-oriented preferences. Controlled experiments trained custom RMs using various amounts and sources of preference data to test bias durability. The team measured bias persistence across different training configurations to understand how pretraining choices influence downstream RM values.

## Key Results
- Llama-based RMs systematically favored agency-oriented values (freedom, success) while Gemma-based RMs favored communion-oriented values (love, friendship)
- Value biases persisted even when RMs were trained with identical preference data
- While sufficient preference training data could reduce bias, it proved surprisingly durable and difficult to fully eliminate

## Why This Works (Mechanism)
Reward models learn to predict human preferences during their training phase. Since these models are typically initialized from pre-trained LLMs, they inherit the statistical patterns and value representations embedded in those base models. The base model's pretraining corpus and architecture shape its internal representations of what constitutes "good" or "preferred" outputs, which then influences how the reward model evaluates responses across different value dimensions.

## Foundational Learning
- **Value lexicons** (Abridged Hedonometer, Beppu-LDA) - Why needed: Provide structured frameworks to quantify and compare different value dimensions in model outputs. Quick check: Verify that lexicons capture meaningful distinctions between agency and communion values through human validation studies.
- **Reward model training** - Why needed: Understanding how RMs learn to predict human preferences is crucial for identifying where value biases emerge. Quick check: Confirm that RMs trained on identical preference data still show systematic differences when initialized from different base models.
- **Pretraining influence** - Why needed: Establishes the mechanism by which base model characteristics persist in downstream applications. Quick check: Test whether freezing base model parameters during RM training maintains or reduces value bias inheritance.

## Architecture Onboarding
- **Component map**: Base LLM -> Reward Model initialization -> Preference data training -> Value-biased evaluation
- **Critical path**: Pretraining corpus → Base model representations → RM initialization → Preference learning → Biased value judgments
- **Design tradeoffs**: Using pre-trained LLMs accelerates RM development but introduces inherited value biases; training from scratch avoids this but requires significantly more resources
- **Failure signatures**: Systematic preference for agency vs. communion values; consistent bias patterns across different preference datasets; difficulty eliminating bias even with large training datasets
- **3 first experiments**: 1) Train identical RMs from scratch vs. pre-trained initialization to measure bias reduction, 2) Vary preference data quantity (10k, 100k, 500k samples) to find bias elimination threshold, 3) Test cross-lingual value bias inheritance using multilingual base models

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis relies on specific value lexicons that may not capture the full spectrum of human values
- Results are based primarily on English-language models, limiting cross-linguistic generalizability
- Experiments focus on only two base model families, leaving uncertainty about whether similar patterns hold across other architectures

## Confidence
- High confidence in the existence of value biases in RMs and their correlation with base model characteristics
- Medium confidence in the durability and difficulty of eliminating these biases through preference training
- Medium confidence in the generalizability of findings beyond the specific model families studied

## Next Checks
1. Test whether similar value inheritance patterns emerge when using base models from other families (e.g., Mistral, DeepSeek) or different training methodologies
2. Conduct cross-linguistic validation using multilingual value lexicons to assess whether the observed biases persist across languages
3. Experiment with larger-scale preference training (beyond 500k samples) to determine if bias elimination is possible with sufficient data quantity and diversity