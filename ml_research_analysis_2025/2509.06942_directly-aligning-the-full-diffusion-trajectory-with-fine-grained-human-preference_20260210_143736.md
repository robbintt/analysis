---
ver: rpa2
title: Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference
arxiv_id: '2509.06942'
source_url: https://arxiv.org/abs/2509.06942
tags:
- reward
- image
- srpo
- arxiv
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to improve the realism and aesthetic
  quality of diffusion models through direct alignment with human preferences. The
  key challenges addressed are the computational expense of gradient computation in
  multi-step denoising and the reliance on offline reward fine-tuning for desired
  visual qualities.
---

# Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference

## Quick Facts
- **arXiv ID:** 2509.06942
- **Source URL:** https://arxiv.org/abs/2509.06942
- **Reference count:** 40
- **Primary result:** 3.7x increase in perceived realism and 3.1x improvement in aesthetic quality over baseline on FLUX.1.dev

## Executive Summary
This paper addresses the challenge of aligning diffusion models with human preferences while avoiding reward hacking and computational inefficiency. The authors introduce Direct-Align, a method that injects noise into clean images and uses interpolation to recover images at any timestep, avoiding over-optimization in late timesteps. They also propose Semantic Relative Preference Optimization (SRPO), which formulates rewards as text-conditioned signals for online adjustment via prompt augmentation. The method achieves state-of-the-art performance in human evaluations and demonstrates 75x greater training efficiency compared to DanceGRPO while matching or exceeding other online RL baselines in image quality.

## Method Summary
Direct-Align improves diffusion model alignment by optimizing the full denoising trajectory rather than just late timesteps. It injects ground-truth Gaussian noise into clean images to reach any timestep, then recovers the original image using a closed-form interpolation that relies on the noise prior. This avoids the computational expense of multi-step backpropagation while preventing reward hacking. SRPO enhances this by formulating rewards as relative differences between semantically augmented prompts (e.g., "Realistic photo" vs "CG Render"), allowing online adjustment without offline reward fine-tuning. The method was evaluated on FLUX.1.dev using human preference datasets and achieved significant improvements in realism and aesthetics.

## Key Results
- 3.7x increase in perceived realism and 3.1x improvement in aesthetic quality over baseline
- 75x greater training efficiency compared to DanceGRPO
- Matches or exceeds other online RL baselines in image quality metrics
- Effectively mitigates reward hacking through full-trajectory optimization
- Achieves state-of-the-art performance in human evaluations

## Why This Works (Mechanism)

### Mechanism 1: Prior Noise Injection for Trajectory Coverage
Direct-Align injects ground-truth Gaussian noise into clean images to reach any timestep, then recovers the original image using a closed-form interpolation. This allows gradient flow from any timestep without accumulating errors through a long computational graph, effectively avoiding over-optimization in late timesteps that leads to reward hacking.

### Mechanism 2: Semantic Relative Preference (SRPO) for Bias Cancellation
SRPO formulates rewards as relative differences between semantically augmented prompts (r_SRP = r_positive - r_negative). By feeding the same image against "realistic" and "CG/Render" prompts, common-mode biases in the reward model cancel out, leaving only the gradient direction for the specific attribute being optimized.

### Mechanism 3: Inversion-Based Regularization
The method applies gradient ascent on the "positive" reward during denoising and gradient descent on the "negative" reward during the inversion/noising direction. This decouples the reward and penalty terms across different timesteps, enhancing robustness against hacking by pushing weights away from negative inversion states.

## Foundational Learning

- **Concept: Diffusion Forward/Reverse Process** - Understanding that you can derive x_0 from x_t if you know the noise ε is the core trick of Direct-Align. *Quick check:* Can you explain why standard multi-step backpropagation causes gradient explosion, but single-step recovery does not?

- **Concept: Reward Hacking (Goodhart's Law)** - The paper is designed to fix models "over-optimizing" for metrics like HPSv2 (e.g., generating purple images to please PickScore). *Quick check:* What visual artifact would you expect if a model overfits a reward model that prefers "smoothed" images?

- **Concept: CLIP-style Contrastive Learning** - SRPO operates on text embedding space where f_img^T · f_txt is a cosine similarity in latent space. *Quick check:* If C_1 is "Realistic" and C_2 is "Blurry", does C_1 - C_2 point toward or away from "Sharpness"?

## Architecture Onboarding

- **Component map:** Data Loader -> Noise Injector -> Direct-Align Block -> SRPO Reward Head
- **Critical path:** The Noise Injection → Recovery step. If the mixing weight for the model prediction (Δσ·ε_θ) is too high at early timesteps, the reconstruction will be poor, and the reward signal will be meaningless.
- **Design tradeoffs:** Efficiency vs. Accuracy (shorter predicted steps are more accurate but provide less gradient signal); Online vs. Offline (SRPO is cheaper than retraining the reward model but relies on base reward model knowing the concept).
- **Failure signatures:** Texture Washout (over-smoothed outputs from late-only optimization); Style Collapse (gradient cancellation from overlapping positive/negative prompts); Reward hacking (oversaturation, artifacts).
- **First 3 experiments:** (1) Timestep ablation comparing early-only, late-only, and full trajectory optimization; (2) Prompt sensitivity testing "Realistic" vs "CG" reward differences; (3) Convergence speed comparison between Direct-Align and DDIM Backprop.

## Open Questions the Paper Calls Out

### Open Question 1
Can incorporating learnable tokens or a systematic control strategy improve the controllability of SRPO for tokens outside the reward model's domain? The authors note that current control tokens are outside the domain of existing reward models and aim to develop more systematic control strategies or incorporate learnable tokens.

### Open Question 2
Does fine-tuning a Vision Language Model (VLM) to be explicitly responsive to control words improve the alignment between the intended reinforcement learning direction and the actual image generation? The authors identify interpretability limitations where encoder mappings may misalign control texts with the intended RL direction.

### Open Question 3
Can the SRPO framework be effectively extended to online RL algorithms that do not support inversion or differentiable rewards? The paper notes the potential of SRPO for other online RL algorithms unable to support inversion or non-differentiable rewards, but the current implementation relies on direct backpropagation.

## Limitations
- The 75x training efficiency gain relies on comparison to a baseline method not fully detailed in the paper
- Generalization to other diffusion architectures (DDPM vs. rectified flow) remains unverified
- SRPO's effectiveness depends heavily on the reward model's text encoder quality and may not generalize beyond tested prompt pairs
- The method assumes reward model biases are additive and cancelable through text embedding subtraction

## Confidence
- **Direct-Align trajectory optimization:** Medium-High - mathematically sound core mechanism though limited empirical validation across diverse reward functions
- **SRPO bias cancellation:** Medium - theoretically plausible but effectiveness depends on reward model text encoder quality
- **Reward hacking mitigation:** Medium - demonstrates reduced hacking rates but relies on synthetic test cases rather than comprehensive real-world preference shifts

## Next Checks
1. **Cross-architecture generalization test:** Implement Direct-Align on a standard DDPM (e.g., Stable Diffusion) and evaluate whether the 75x efficiency gain and hacking mitigation hold with different noise schedules and diffusion formulations.

2. **Reward model bias analysis:** Systematically test SRPO with multiple positive/negative prompt pairs beyond "realistic/CG" to verify bias cancellation works across different semantic dimensions and doesn't suppress legitimate stylistic variations.

3. **Long-horizon preference stability:** Generate sequences of images using the aligned model and measure how preference distributions shift over multiple sampling steps, testing whether the method maintains alignment consistency throughout the full generation trajectory.