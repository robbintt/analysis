---
ver: rpa2
title: Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented
  Contrastive Alignment
arxiv_id: '2512.04356'
source_url: https://arxiv.org/abs/2512.04356
tags:
- object
- action
- video
- hallucination
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles object and action hallucinations in multimodal
  large language models (MLLMs) for video understanding. Existing methods focus on
  static images or frame-by-frame processing, failing to address temporal dynamics.
---

# Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment

## Quick Facts
- **arXiv ID:** 2512.04356
- **Source URL:** https://arxiv.org/abs/2512.04356
- **Reference count:** 40
- **Primary result:** Significantly reduces object and action hallucinations in video MLLMs, achieving state-of-the-art performance on MiraData-9k and VidHal benchmarks.

## Executive Summary
This paper addresses the critical problem of object and action hallucinations in video understanding by multimodal large language models (MLLMs). Unlike existing methods that focus on static images or frame-by-frame processing, the proposed Self-Augmented Contrastive Alignment (SANTA) framework introduces hallucinative self-augmentation to identify and contrast spurious language correlations, and employs tracklet-phrase contrastive alignment to link regional objects and relation-guided actions with their corresponding visual and temporal phrases. Experiments demonstrate SANTA significantly reduces both object and action hallucinations, achieving state-of-the-art performance on hallucination examination benchmarks.

## Method Summary
SANTA introduces a novel framework for mitigating object and action hallucinations in video MLLMs through three key mechanisms: hallucinative self-augmentation that uses the model's own high-probability incorrect predictions as hard negative samples, tracklet-phrase contrastive alignment that binds object tracklets (spatial-temporal tubes) to their corresponding phrases, and relation-guided action squeezing that treats actions as interactions between objects using a specialized cross-attention module. The framework operates on pre-extracted object tracklets from Grounding-SAM2 and parses ground-truth captions using GPT-4o to create paired visual-text data, then trains a base MLLM with contrastive losses that push away hallucinated concepts while pulling in grounded ones.

## Key Results
- Achieves state-of-the-art hallucination reduction on MiraData-9k and VidHal benchmarks
- Significantly improves both object (F1_obj) and action (F1_act) hallucination metrics
- Maintains consistent gains across multiple captioning subtasks and video QA tasks
- Outperforms baselines using GPT-4 generated negatives in self-augmentation

## Why This Works (Mechanism)

### Mechanism 1: Hallucinative Self-Augmentation
Using the model's own high-probability incorrect predictions as negative samples is more effective for mitigating hallucinations than using generic negatives. During training, the framework constrains the frozen MLLM to select the highest-probability token at each step that falls outside the ground-truth set, forcing the model to explicitly surface its intrinsic "spurious correlations" (language priors). These self-generated hallucinations serve as hard negatives in a contrastive loss, pushing the visual features away from the model's specific failure modes.

### Mechanism 2: Tracklet-Phrase Binding for Objects
Object hallucinations in video stem from a lack of fine-grained regional grounding; they are better corrected by aligning text to object tracklets (spatial-temporal tubes) rather than global frame features. The architecture uses an offline tracker to extract visual features for specific objects across frames, then enforces a contrastive alignment between these regional tracklet features and parsed object phrases. This binds the noun phrase not just to "a frame," but to the specific pixels moving through time.

### Mechanism 3: Relation-Guided Action Squeezer
Action hallucinations are reduced by treating actions as "interactions between objects" rather than independent verbs, using a specialized cross-attention module to distill action features from object tracklets. An "Action Squeezer" module takes the tracklet features of objects involved in an action and aggregates them into a single action feature, which is contrasted with action phrases, forcing the model to base its action verbs on the visual relationship between the participating objects.

## Foundational Learning

- **Concept: Contrastive Learning (InfoNCE)**
  - **Why needed here:** SANTA relies entirely on contrastive losses to "push" away hallucinated concepts and "pull" in grounded ones. You must understand how temperature and batch size affect the hardness of these negatives.
  - **Quick check question:** If the batch size is too small, why might the contrastive loss fail to distinguish specific hallucinations from ground truth?

- **Concept: Object Tracking & Tracklets**
  - **Why needed here:** Unlike image-based hallucination mitigation, this method depends on tracklets (features persisted across frames). Understanding the difference between a "bounding box" and a "tracklet ID" is essential for debugging the object-alignment pipeline.
  - **Quick check question:** What happens to the alignment loss if an object leaves the frame and the tracker creates a new ID when it re-enters?

- **Concept: Language Priors in LLMs**
  - **Why needed here:** The paper aims to decouple the LLM's "prior" (e.g., "beaches usually have sand") from the "visual fact" (e.g., "this beach has rocks"). Understanding why LLMs hallucinate (statistical likelihood vs. visual evidence) is key to understanding why self-augmentation works.
  - **Quick check question:** Why does the paper restrict the self-augmentation to tokens outside the synonym/hypernym set of the ground truth?

## Architecture Onboarding

- **Component map:** Offline Pipeline: Grounding-SAM2 (Tracklets) + GPT-4o (Text Parser) → Pre-computed paired data → Base MLLM + Action Squeezer → Contrastive Losses
- **Critical path:**
  1. Input Video + Ground-Truth Caption
  2. Self-Augmentation: Frozen MLLM generates Hallucinated Caption
  3. Parsing: Extract Object Phrases and Action Phrases from GT/Hallucinated captions
  4. Visual Extraction: Retrieve Object Tracklets and feed to Action Squeezer to get Action Features
  5. Alignment: Compute contrastive loss between tracklet-phrase pairs, pushing away hallucinated captions

- **Design tradeoffs:**
  - Pipeline Complexity: Requires running SAM2 and GPT-4o offline, creating frozen errors if trackers miss objects
  - Hard Negatives: Uses the model's own errors as negatives, highly effective for specific model but may require regeneration if base model changes

- **Failure signatures:**
  - Semantic Drift: Action Squeezer collapses to predicting dominant motion regardless of text query
  - Tracker Noise: High variance in loss values indicates Grounding-SAM2 tracker is struggling

- **First 3 experiments:**
  1. Sanity Check (Offline): Visualize "Hallucinated Captions" generated by Eq. (1). Are they plausible hallucinations or random noise?
  2. Component Ablation: Train with only video-caption contrast vs. only object-action contrast to confirm which contributes most to hallucination reduction
  3. Squeezer Analysis: Visualize attention maps of Action Squeezer. Does it attend to relevant object pairs when extracting actions?

## Open Questions the Paper Calls Out
None

## Limitations

- **Hallucination Detection Sensitivity:** Evaluation relies on strict string matching, potentially missing semantically equivalent hallucinations
- **Offline Tracklet Quality Dependency:** Performance fundamentally limited by accuracy of Grounding-SAM2 tracklets
- **Model-Specific Self-Augmentation:** Approach may not generalize to significantly different architectures or training distributions

## Confidence

**High Confidence Claims:**
- SANTA significantly reduces object and action hallucination rates on standard benchmarks
- Tracklet-phrase contrastive alignment improves regional object grounding
- Self-augmentation using model-specific hallucinations outperforms generic external model negatives

**Medium Confidence Claims:**
- Action Squeezer effectively captures action-object interactions through cross-attention
- Method generalizes across different MLLM architectures
- Offline processing pipeline is scalable and practical

**Low Confidence Claims:**
- Contrastive learning framework would perform equally well on non-video datasets
- Specific hyperparameter choices are optimal across all scenarios
- Method would maintain performance gains when integrated with additional training objectives

## Next Checks

1. **Semantic Equivalence Evaluation:** Implement a paraphrase-aware hallucination detection metric to verify whether F1 score improvements reflect actual semantic corrections rather than just lexical changes.

2. **Tracker Failure Analysis:** Systematically corrupt Grounding-SAM2 tracklets and measure degradation in hallucination scores to quantify true robustness and identify failure modes.

3. **Cross-Architecture Generalization:** Apply SANTA's training procedure to a completely different MLLM architecture and measure whether hallucination reduction patterns replicate.