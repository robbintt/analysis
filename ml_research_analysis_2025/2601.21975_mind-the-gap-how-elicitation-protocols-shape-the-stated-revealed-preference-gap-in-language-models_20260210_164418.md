---
ver: rpa2
title: 'Mind the Gap: How Elicitation Protocols Shape the Stated-Revealed Preference
  Gap in Language Models'
arxiv_id: '2601.21975'
source_url: https://arxiv.org/abs/2601.21975
tags:
- revealed
- preference
- values
- elicitation
- stated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The stated-revealed preference gap measures inconsistency between
  the values language models endorse abstractly and their choices in concrete scenarios.
  Binary forced-choice prompting collapses preference strength and uncertainty into
  a single decision, conflating genuine preferences with protocol artifacts.
---

# Mind the Gap: How Elicitation Protocols Shape the Stated-Revealed Preference Gap in Language Models

## Quick Facts
- **arXiv ID:** 2601.21975
- **Source URL:** https://arxiv.org/abs/2601.21975
- **Reference count:** 21
- **Primary result:** Expanded-choice stated preference elicitation improves Spearman's rank correlation (ρ) between stated and revealed preferences by filtering weak signals, increasing ρ from ~0.2 to ~0.7 in some cases.

## Executive Summary
This paper investigates how different elicitation protocols affect the stated-revealed (SvR) preference gap in language models. The authors evaluate 24 models using three protocol configurations: forced-choice for both stated and revealed preferences, expanded-choice (allowing neutrality) for stated and forced-choice for revealed, and expanded-choice for both. They find that binary forced-choice prompting collapses preference strength and uncertainty into a single decision, conflating genuine preferences with protocol artifacts. Allowing neutrality in stated preference elicitation substantially improves SvR correlation by filtering out weak signals, while allowing neutrality in revealed preferences drives correlation toward zero or negative values due to high neutrality rates. System prompt steering using stated preference rankings does not reliably improve SvR correlation and often degrades performance, particularly with larger value sets.

## Method Summary
The authors evaluate 24 language models on the AIRiskDilemmas dataset using three elicitation protocol configurations. Stated preferences are elicited through pairwise value comparisons using 5 prompt templates across all 16P2 value pairs, with responses parsed by GPT-4o-mini judge. Revealed preferences come from contextualized scenarios where value-mapped actions are scored using Elo ratings. Spearman's ρ measures correlation between stated and revealed rankings. Neutrality rates are tracked across protocols, and system prompt steering injects stated rankings into revealed elicitation. Deterministic decoding (temperature=0, top_p=0.01) ensures reproducibility.

## Key Results
- Expanded-choice stated preference elicitation improves Spearman's ρ between stated and revealed preferences by filtering weak signals, increasing ρ from ~0.2 to ~0.7 in some cases
- Allowing neutrality in revealed preferences drives ρ near zero or negative due to high neutrality rates (some models select neutral responses in most scenarios)
- System prompt steering using stated preference rankings does not reliably improve SvR correlation and often degrades performance, especially with 16-value sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expanded-choice stated preference elicitation improves SvR correlation by filtering weak or indeterminate comparisons.
- Mechanism: When models can select "Equal Preference" or "Depends" during abstract pairwise value comparisons, responses reflecting uncertainty or weak preference strength are excluded from ranking construction. The remaining decisive comparisons yield rankings that better reflect robust value hierarchies, which correlate more strongly with forced-choice revealed behavior.
- Core assumption: Models' neutral responses meaningfully indicate weak or indeterminate preferences rather than task confusion or format misinterpretation.
- Evidence anchors: Abstract states expanded-choice stated improves ρ substantially; LLaMA-3.1-405B-Instruct improves from ρ≈0.2 to ρ≈0.7.

### Mechanism 2
- Claim: Expanded-choice revealed preference elicitation drives SvR correlation toward zero or negative values due to high neutrality rates.
- Mechanism: When models are allowed neutrality in contextualized scenarios, many consistently select "Depends" or "Equal Preference," preventing construction of complete binary rankings. Residual binary choices provide weak and unstable signals for correlation-based comparison.
- Core assumption: High neutrality rates in revealed contexts reflect genuine indeterminacy or context-sensitivity rather than prompt comprehension failures.
- Evidence anchors: Abstract states expanded-choice revealed drives ρ to near-zero or negative; Mistral-3-8B variants select neutral responses in nearly all revealed scenarios.

### Mechanism 3
- Claim: System prompt steering using stated preference rankings does not reliably improve SvR correlation and often degrades it.
- Mechanism: Injecting a model's own stated value hierarchy into the context window is insufficient to override existing behavioral priors. With larger value sets (16 values), prompt-based steering may introduce interference that degrades decision consistency rather than improving alignment.
- Core assumption: Degradation stems from interference between injected priorities and learned behavioral priors rather than from prompt formatting issues.
- Evidence anchors: Abstract states steering does not reliably improve SvR correlation; Claude family consistently regresses; prior work shows steering effectiveness decreases as value set size increases.

## Foundational Learning

- **Stated vs. Revealed Preferences**
  - Why needed here: The entire paper examines the gap between what models say they value (stated) and what they choose in contextualized scenarios (revealed). Without this distinction, the protocol variations and their effects cannot be interpreted.
  - Quick check question: If a model says it prioritizes "Truthfulness" in an abstract comparison but chooses to lie in a concrete scenario to protect someone, what type of preference does each response represent?

- **Elicitation Protocol Artifacts**
  - Why needed here: The central claim is that binary forced-choice prompting introduces artifacts that conflate genuine preferences with measurement noise. Understanding how protocol constraints shape responses is essential for interpreting results.
  - Quick check question: Why might a model select a value in a forced binary choice that it would mark "Depends" if given that option?

- **Spearman's Rank Correlation (ρ)**
  - Why needed here: The paper uses Spearman's ρ to measure SvR correlation. Understanding that this metric captures monotonic relationships between ordinal rankings—not the absolute values—is necessary to interpret why correlation improves or degrades under different protocols.
  - Quick check question: If a model's revealed preference ranking perfectly matches its stated ranking but both are based on only 3 decisive comparisons out of 120 pairs, would a high ρ be meaningful?

## Architecture Onboarding

- **Component map:**
  Stated Preference Elicitation (pairwise value comparison prompts) -> Response parsing (GPT-4o-mini judge) -> Win-rate aggregation -> Global ranking
  Revealed Preference Elicitation (AIRiskDilemmas scenarios) -> Action selection -> Value mapping -> Elo scoring -> Ordinal ranking
  SvR Correlation Calculation: Spearman's ρ between stated and revealed ordinal rankings
  Steering Module: System prompt construction from stated rankings -> Prepended to revealed elicitation -> Compare ρ before/after

- **Critical path:**
  1. Run expanded-choice stated elicitation -> filter neutral responses -> construct ranking
  2. Run forced-choice revealed elicitation -> map actions to values -> compute Elo ratings -> convert to ranks
  3. Compute Spearman's ρ between the two rankings
  4. (Optional) Inject stated ranking into system prompt and re-run revealed elicitation

- **Design tradeoffs:**
  - Expanded vs. Forced stated: Expanded filters weak signals but may produce sparse rankings if neutrality is too high; forced preserves ranking density but includes noise
  - Expanded vs. Forced revealed: Expanded surfaces indeterminacy but can collapse correlation; forced yields comparable rankings across models but may misrepresent model uncertainty
  - Greedy decoding (temp=0): Reduces stochasticity for reproducibility but may not reflect typical model behavior

- **Failure signatures:**
  - Neutrality rate >99% in either stated or revealed elicitation -> cannot construct meaningful rankings
  - SvR correlation near zero or negative under expanded–expanded protocol -> models not expressing stable preferences in this regime
  - Steering reduces correlation -> injected priorities conflicting with behavioral priors or interference from large value set

- **First 3 experiments:**
  1. Baseline replication: Run forced–forced protocol on 2–3 model families to reproduce the high-variance baseline, confirming implementation matches LitmusValues framework
  2. Neutrality rate profiling: Run expanded-choice stated and revealed elicitation on a single model; compute neutrality rates for each. If revealed neutrality >80%, expect near-zero SvR correlation
  3. Protocol ablation: For one model with moderate neutrality rates, compare SvR correlation across all three protocol configurations (forced–forced, expanded–forced, expanded–expanded) to verify the ρ trajectory (baseline -> improved -> collapsed)

## Open Questions the Paper Calls Out

- **Open Question 1:** Can stronger interventions (e.g., fine-tuning or activation steering) bridge the stated-revealed preference gap for large value sets more effectively than system prompt steering?
  - Basis in paper: Discussion states "bridging the SvR gap likely requires stronger interventions than stated-value system-prompting when many values are in play."
  - Why unresolved: Authors demonstrated system prompt steering is unreliable/detrimental for 16-value set but did not test alternative alignment methods
  - What evidence would resolve it: Repeating steering experiment using fine-tuning on stated preferences or sparse autoencoder steering, showing improved SvR correlation on 16-value set

- **Open Question 2:** How can preference rankings be constructed to explicitly incorporate neutrality and indeterminacy rather than discarding neutral responses?
  - Basis in paper: Discussion suggests "SvR measurement should explicitly model neutrality/indeterminacy rather than discarding it."
  - Why unresolved: Current methodology excludes "Equal" or "Depends" responses to generate binary rankings, potentially losing information about preference stability
  - What evidence would resolve it: Development of new ranking metric (e.g., partial orders or probabilistic rankings) that weights neutral responses to predict model behavior more accurately than binary win-rates

- **Open Question 3:** Do expanded-choice protocols improve stated-revealed correlation in non-moral or low-stakes domains?
  - Basis in paper: Study relies exclusively on AIRiskDilemmas dataset focusing on high-stakes moral conflicts where "Depends" responses may be more prevalent than in routine tasks
  - Why unresolved: Unclear if high neutrality rates and protocol sensitivity are artifacts of moral dilemma domain or general features of LM preference structures
  - What evidence would resolve it: Evaluating three protocol configurations on dataset of low-stakes or functional preferences (e.g., formatting styles, tone settings) and comparing neutrality rates

## Limitations
- High neutrality rates in revealed preferences (particularly for Mistral-3-8B variants) suggest fundamental indeterminacy that may reflect either genuine model uncertainty or prompt comprehension failures
- Reliance on GPT-4o-mini as judge for response classification introduces second-model dependency that could compound measurement errors
- AIRiskDilemmas dataset, while thematically consistent, may not capture full behavioral space needed to establish stable preference rankings

## Confidence
- Binary forced-choice protocols conflate genuine preference strength with measurement artifacts: **High confidence** (systematic protocol variation shows dramatic ρ changes)
- Expanded-stated/forced-revealed protocol improves correlation: **High confidence** for models with moderate neutrality rates (though effect size depends on model-specific thresholds)
- System prompt steering failure: **Medium confidence** (consistent degradation demonstrated but mechanism incompletely characterized)

## Next Checks
1. Replicate protocol effect using alternative judge model (e.g., Claude-3.5-Sonnet) to assess robustness to classification pipeline choices
2. Test expanded-stated/forced-revealed protocol on non-risk domain dataset to verify generalizability beyond AIRiskDilemmas
3. Conduct ablation studies on the 5 prompt templates to determine whether template variation or neutrality filtering drives correlation improvements