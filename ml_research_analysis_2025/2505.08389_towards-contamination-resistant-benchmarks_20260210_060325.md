---
ver: rpa2
title: Towards Contamination Resistant Benchmarks
arxiv_id: '2505.08389'
source_url: https://arxiv.org/abs/2505.08389
tags:
- text
- cipher
- shift
- table
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces the concept of contamination resistance to
  address the challenge of data leakage in LLM evaluation. The authors propose a benchmark
  based on Caesar ciphers that dynamically generates infinite unique instances, ensuring
  models cannot memorize all possible queries.
---

# Towards Contamination Resistant Benchmarks

## Quick Facts
- **arXiv ID:** 2505.08389
- **Source URL:** https://arxiv.org/abs/2505.08389
- **Reference count:** 40
- **Primary result:** Proposes contamination-resistant Caesar cipher benchmarks; shows LLMs exhibit inconsistent performance suggesting memorization rather than reasoning

## Executive Summary
This work introduces contamination resistance as a critical evaluation paradigm for large language models, addressing the widespread problem of data leakage in existing benchmarks. The authors propose a novel benchmark based on Caesar ciphers that dynamically generates infinite unique instances, ensuring models cannot memorize all possible queries. Through systematic experiments on widely used LLMs including GPT-4o, LLaMA3.1, Qwen2.5, and QwQ-32B, the paper reveals inconsistent performance across different shift values, particularly for shifts other than 3, indicating reliance on memorization rather than genuine reasoning capabilities.

The study demonstrates that even the most capable models struggle with random non-sense words and fail to benefit from few-shot demonstrations, highlighting the limitations of current evaluation methodologies. GPT-4o shows some success but still exhibits significant failures, suggesting that even state-of-the-art models have fundamental weaknesses in handling truly novel tasks. The findings underscore the urgent need for contamination-resistant benchmarks to ensure reliable LLM evaluation and provide insights into the current limitations of language models in genuine reasoning versus pattern matching.

## Method Summary
The authors evaluate LLMs on Caesar cipher encoding and decoding tasks to test contamination resistance, requiring models to shift letters by specified amounts (e.g., shift=3: "ab" â†’ "de"). The benchmark uses 25 natural English plain texts and 25 random non-sense words with 4 shift values [3, 6, 9, 12], creating 200 total cipher instances. Models are tested in zero-shot and 50-shot settings using four prompt types: open (free-form), base (output only), dict (lookup table + answer), and code (Python function). Exact match accuracy serves as the primary metric while character error rate captures partial improvements. GPT-4o is evaluated via API with temperature=0 and seed=2266, while local models use 4-bit quantization with temperature=0.01.

## Key Results
- GPT-4o is the only model showing success on Caesar cipher tasks, but even it struggles with random non-sense words and fails to benefit from few-shot demonstrations
- All models show inconsistent performance across shift values, with near-perfect accuracy on shift=3 but near-zero on shifts 6/9/12, indicating memorization rather than genuine reasoning
- Despite generating functionally correct Python code for Caesar ciphers, GPT-4o fails to produce correct text answers, revealing an inconsistency between code generation and direct text-based task completion

## Why This Works (Mechanism)
The Caesar cipher benchmark works as a contamination-resistant evaluation because it creates an infinite space of possible instances that cannot be fully memorized during pre-training. Unlike static benchmarks that models can encounter and memorize, each cipher instance is uniquely generated with random text and shift values. The fundamental mechanism relies on the fact that Caesar ciphers require algorithmic application of a shift rule rather than pattern recognition from memorized examples. When models perform well only on specific shifts (particularly shift=3) but fail on others, this demonstrates they are relying on memorized examples rather than understanding the underlying transformation principle. The use of random non-sense words further ensures that models cannot leverage language patterns or statistical regularities, forcing genuine algorithmic reasoning.

## Foundational Learning
- **Contamination resistance**: The ability of a benchmark to generate unique instances that cannot be fully memorized during pre-training. Needed because existing benchmarks often contain leaked data, leading to inflated performance metrics. Quick check: Verify that benchmark instances are dynamically generated and cannot be enumerated.
- **Exact match accuracy**: Primary evaluation metric measuring whether the model's output exactly matches the ground truth. Critical for tasks requiring precise answers like cipher encoding/decoding. Quick check: Calculate percentage of perfectly matching outputs across all test instances.
- **Character error rate**: Secondary metric capturing partial improvements by measuring the proportion of incorrectly transformed characters. Important for understanding model performance when exact matches fail. Quick check: Compute Levenshtein distance between predictions and ground truth.
- **Dynamic benchmark generation**: Creating test instances on-the-fly rather than using static datasets. Essential for preventing memorization and ensuring genuine reasoning. Quick check: Confirm that each evaluation instance is uniquely generated for the test run.
- **Few-shot prompting**: Providing demonstrations in the prompt to guide model behavior. Tested to see if models can learn cipher rules from examples rather than memorized knowledge. Quick check: Compare performance with and without demonstration examples in the prompt.

## Architecture Onboarding

### Component Map
Benchmark Generator -> Cipher Instance Creation -> Prompt Template Application -> Model Inference -> Answer Extraction -> Metric Calculation -> Performance Analysis

### Critical Path
The critical evaluation path begins with benchmark generation where Caesar cipher instances are created by applying shifts to natural English phrases and random letter sequences. These instances flow into prompt template application where four different prompting strategies (open, base, dict, code) are applied. Model inference executes with specific temperature and seed settings for each model family. Answer extraction processes model outputs to obtain final predictions, which then flow into metric calculation for exact match accuracy and character error rate. Performance analysis examines patterns across shifts and prompt types to identify memorization versus reasoning behaviors.

### Design Tradeoffs
The benchmark design trades off between task simplicity and contamination resistance. Caesar ciphers are simple enough to be universally applicable across model families yet complex enough to require genuine algorithmic reasoning. The use of both natural English phrases and random non-sense words balances real-world applicability with memorization prevention. The four prompt strategies explore different levels of guidance, from fully open-ended to structured code generation. However, the simplicity of Caesar ciphers may limit generalizability to more complex reasoning tasks, and the focus on English text may not capture multilingual reasoning capabilities.

### Failure Signatures
- High performance on shift=3 but near-zero on shifts 6/9/12 indicates memorization rather than reasoning
- Correct intermediate reasoning chains but incorrect final answers suggest generation issues rather than comprehension failures
- Few-shot prompting showing no improvement indicates inability to induce algorithmic rules from demonstrations
- Successful code generation but failed text-based task completion reveals inconsistency between procedural and direct reasoning

### First 3 Experiments
1. Generate Caesar cipher instances with shift=3 and verify near-perfect accuracy across all models to establish baseline memorization patterns
2. Test shift=6 instances and confirm dramatic performance drop compared to shift=3 to demonstrate contamination
3. Apply 50-shot prompts with explicit cipher demonstrations and verify lack of improvement on non-memorized shifts

## Open Questions the Paper Calls Out
### Open Question 1
Why do LLMs exhibit inconsistent competence by generating functionally correct Python code for a Caesar cipher while failing to produce the correct text answer when prompted directly? The authors note that GPT-4o generates code that executes successfully but fails the text-based task, suggesting an "inconsistency in LLM competence between generating code and final answers." This disconnect remains unexplained and requires mechanistic interpretability studies comparing attention heads during code generation versus direct text generation for the same logical mapping task.

### Open Question 2
Why does providing extensive few-shot demonstrations fail to improve LLM performance on cipher shifts that were not memorized during pre-training? Section 5.5 reports that GPT-4o showed "almost no improvement" on shifts 6, 9, and 12 even with 50-shot prompts, contrary to expectations that in-context learning should enable simple function learning. The paper confirms this failure but leaves open why models cannot induce the linear mapping rule from demonstrations, suggesting the need for experiments varying the diversity and explicitness of few-shot examples.

### Open Question 3
Does fine-tuning on contamination-resistant tasks with random instances (e.g., nonsense words) lead to catastrophic forgetting or degraded performance on other standard natural language tasks? The authors explicitly state uncertainty about whether fine-tuning on large numbers of random non-sense words would affect model performance on other tasks. While the paper explores zero-shot and few-shot performance, it does not conduct fine-tuning experiments due to potential negative side effects, leaving this risk unquantified.

## Limitations
- The exact benchmark dataset (25 natural English phrases and 25 random words) is not fully specified in the paper, requiring external access for exact reproduction
- The 50-shot demonstrations that inform the few-shot experiments are referenced but not included, making it difficult to verify the exact few-shot setup
- The analysis focuses on Caesar ciphers as a specific contamination-resistant task, leaving open questions about generalizability to other benchmark types

## Confidence
- **High confidence** in the core finding that current LLMs show shift-dependent performance inconsistent with genuine reasoning
- **Medium confidence** in the proposed contamination-resistant benchmark design, given the task-specific nature of Caesar ciphers
- **Low confidence** in broader generalizability without testing on diverse contamination-resistant tasks

## Next Checks
1. Reconstruct the full benchmark dataset by generating Caesar cipher instances from the examples provided and verify the exact match accuracy patterns across shifts
2. Implement and test the 50-shot prompt demonstrations to confirm whether few-shot prompting genuinely fails to improve Caesar cipher performance as reported
3. Extend evaluation to additional contamination-resistant tasks (e.g., dynamic programming problems, novel logical puzzles) to assess whether observed memorization patterns generalize beyond Caesar ciphers