---
ver: rpa2
title: 'Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for
  LLMs'
arxiv_id: '2511.01202'
source_url: https://arxiv.org/abs/2511.01202
tags:
- information
- semantic
- theory
- llms
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a semantic information-theoretic framework
  for large language models (LLMs), replacing Shannon's bit-based approach with a
  token-centric view. The core idea is to treat LLMs as generative models that optimize
  directed information flows rather than focus on exact reconstruction.
---

# Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs

## Quick Facts
- **arXiv ID:** 2511.01202
- **Source URL:** https://arxiv.org/abs/2511.01202
- **Reference count:** 40
- **Primary result:** Establishes semantic information theory for LLMs, replacing Shannon bits with tokens and defining optimal training and inference frameworks.

## Executive Summary
This paper introduces a semantic information-theoretic framework for large language models, fundamentally shifting from Shannon's bit-based information theory to a token-centric approach. The framework treats LLMs as generative models optimizing directed information flows rather than exact reconstruction. Key contributions include defining directed rate-distortion functions for training, semantic information flow for inference, and establishing a general autoregressive LLM definition from which the Transformer architecture can be mathematically derived.

## Method Summary
The paper establishes a semantic information-theoretic framework for LLMs by defining directed rate-distortion functions for pre-training, directed rate-reward functions for post-training, and semantic information flow for inference. It provides an information-theoretically optimal semantic embedding method and establishes a general definition of autoregressive LLMs, showing that the Transformer architecture can be derived from this framework through matrix decomposition of a time-varying vector autoregression process.

## Key Results
- Defines Directed Rate-Distortion function for pre-training that minimizes directed information to filter hallucinations
- Establishes semantic information flow as a sub-martingale process during inference
- Derives Transformer architecture from general autoregressive LLM definition via TV-VAR decomposition
- Provides metrics like ELBO, generalization bounds, and memory capacity for LLM analysis

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pre-training acts as a semantic filter that minimizes directed information to suppress hallucinations.
- **Mechanism:** The LLM is modeled as a discrete-time channel with feedback. The framework proposes a Directed Rate-Distortion function where the optimization goal minimizes the directed information $I(S_{1:n} \rightarrow U_{n+1:N}; \Phi)$. This minimization theoretically filters out information in the source that is irrelevant to generating the target sequence, preventing the propagation of extraneous signals (hallucinations).
- **Core assumption:** The paper assumes that standard cross-entropy loss is equivalent to optimizing this directed information flow under specific constraints (Theorem 1).
- **Evidence anchors:**
  - [section III-B] Defines the Directed Rate-Distortion function (Eq. 11) and explicitly links minimization to filtering irrelevant information.
  - [section III-B] "Minimizing the directed information by adjusting $\Phi$ filters out information irrelevant to generate the output."
  - [corpus] Corpus evidence on "unlearning" and "filtering" (e.g., *Selective Unlearning*, *Reinforcement UnLEarning*) supports the general concept of removing irrelevant information, but does not explicitly validate this specific directed information formulation.
- **Break condition:** If the model generates statistically likely but factually incorrect sequences (hallucinations) that persist despite low training loss, the assumption that cross-entropy fully captures the directed semantic constraint may be invalid.

### Mechanism 2
- **Claim:** The Transformer architecture functions as a specific factorization of a Time-Varying Vector Autoregression (TV-VAR) process.
- **Mechanism:** The paper establishes a general AR-LLM definition (Eq. 44) using a coefficient matrix $A_{ij}$. The Transformer is derived by decomposing this matrix into a time-invariant value matrix $A$ and time-variant scalar weights $\pi_{ij}$ (attention scores). This shows that the attention mechanism is mathematically equivalent to weighting the contribution of past states in a VAR process.
- **Core assumption:** The semantic relevance between tokens can be captured entirely by the bilinear form $u_{i-1}^T B u_j$ used to calculate $\pi_{ij}$.
- **Evidence anchors:**
  - [section V-B] Theorem 5 formally derives the Transformer structure from the general TV-VAR definition via matrix decomposition.
  - [abstract] Mentions establishing a general definition of autoregressive LLMs where the Transformer is a derived case.
  - [corpus] *Information Gravity* proposes a field-theoretic interaction model for tokens, offering a parallel but distinct physical analogy to the VAR/Attention mechanism described here.
- **Break condition:** If linear attention variants (like Mamba) outperform Transformers on tasks requiring complex semantic retrieval, the assumption that the full softmax-based bilinear form ($\pi_{ij}A$) is necessary for optimal semantic flow is challenged.

### Mechanism 3
- **Claim:** Inference behaves as a sub-martingale process where semantic information accumulates monotonically.
- **Mechanism:** The Semantic Information Flow is defined using directed information density (Eq. 22). The paper proves (Theorem 2) that this flow is a sub-martingale, meaning the expected information content increases or stays constant with each generated token.
- **Core assumption:** The KL divergence term in the conditional expectation remains non-negative, ensuring the "drift" of the process is non-negative.
- **Evidence anchors:**
  - [section III-D] Theorem 2 and Eq. 25 establish the sub-martingale property and the inequality $E\{...\} \ge \imath(..._{i-1})$.
  - [section III-D] Corollary 3 connects this to the stopping time of generation.
  - [corpus] No direct corpus validation for the sub-martingale nature of LLM inference specifically.
- **Break condition:** If generation loops or degrades into incoherence (entropy collapse), the sub-martingale property (accumulation of meaningful information) is violated.

## Foundational Learning

- **Concept: Directed Information**
  - **Why needed here:** Unlike standard mutual information which assumes static distributions, directed information $I(X \rightarrow Y)$ accounts for the causal, sequential flow of data essential to autoregressive models.
  - **Quick check question:** How does directed information differ from transfer entropy in the context of channel feedback?

- **Concept: Rate-Distortion Theory**
  - **Why needed here:** The paper reframes LLM training not as classification, but as a lossy compression problem where "rate" is the model complexity/information flow and "distortion" is the semantic error.
  - **Quick check question:** In the Directed Rate-Distortion function (Eq. 11), what does the constraint $D_{KL} < D$ represent regarding the output distribution?

- **Concept: Vector Autoregression (VAR)**
  - **Why needed here:** This provides the mathematical substrate for the "General Definition of AR-LLM." Understanding VAR is required to see how the Transformer attention mechanism acts as a time-varying coefficient.
  - **Quick check question:** In a standard VAR model, how does the coefficient matrix determine the influence of past time steps on the current state?

## Architecture Onboarding

- **Component map:** Input Layer (Semantic Embedding) -> Core Engine (TV-VAR Processor) -> Output Layer (De-embedding)

- **Critical path:** The transition from the **Probabilistic Model** (Fig. 1) to the **TV-VAR Definition** (Eq. 44) and finally the **Transformer Derivation** (Eq. 49). The core logic relies on the decomposition $A_{ij} = \pi_{ij}A$ (Eq. 48), which bridges abstract autoregression to concrete attention mechanisms.

- **Design tradeoffs:**
  - **Transformer vs. Mamba:** The paper posits Transformer as a non-linear TV-VAR (via softmax $\pi_{ij}$) and Mamba as a linear TV-VAR. The tradeoff is the computational complexity of the bilinear form vs. the theoretical capacity for semantic nuance.
  - **Vector Dimensionality:** Choosing embedding dimension $m$ involves a trade-off described by the Johnson-Lindenstrauss lemma and compressive sensing limits (Theorem 3).

- **Failure signatures:**
  - **Hallucinations:** Identified as a failure to minimize directed information effectively during pre-training, allowing "noise" to dominate the semantic flow.
  - **Generalization Error:** If the logits $z_k$ fluctuate wildly, the generalization bound (Theorem 8) implies the model is unstable.

- **First 3 experiments:**
  1. **Estimate Directed Information:** Implement the Donsker-Varadhan representation (Section V-F) to measure $I(S_{1:n} \rightarrow U_{n+1:i})$ during training to verify if it correlates with hallucination reduction.
  2. **Semantic Flow Analysis:** Plot the Semantic Information Flow $\imath(S_{1:n} \rightarrow U_{n+1:i})$ during inference to check for sub-martingale behavior (monotonic increase) vs. degradation.
  3. **Embedding Distortion Test:** Use the Gromov-Wasserstein distance (Definition 12) to compare the semantic geometry of a base model vs. a compressed (quantized/pruned) model to measure semantic loss.

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical framework relies on abstract probabilistic constructs that lack empirical validation and experimental demonstrations.
- The equivalence between cross-entropy loss and directed information optimization depends on strong distributional assumptions that may not generalize to real-world training dynamics.
- The derivation of Transformer from TV-VAR assumes a specific bilinear form for attention scores that may not capture the full complexity of self-attention variants.

## Confidence

- **High Confidence:** The mathematical derivations connecting directed information theory to autoregressive processes (Theorem 2, Theorem 5) are internally consistent and follow established information-theoretic principles. The sub-martingale property of semantic information flow is rigorously proven given the stated assumptions.

- **Medium Confidence:** The interpretation of cross-entropy as minimizing directed information (Theorem 1) and the claim that this filters hallucinations is theoretically sound but depends on strong assumptions about the training distribution and loss landscape that require empirical verification.

- **Low Confidence:** The practical applicability of the semantic embedding method and the claim that it provides "information-theoretically optimal" representations lacks experimental validation. The generalization bounds (Theorem 8) are asymptotic results that may not reflect finite-sample behavior in large models.

## Next Checks
1. **Directed Information Measurement:** Implement the Donsker-Varadhan representation from Section V-F to empirically estimate directed information during LLM training. Validate whether this quantity correlates with hallucination reduction and whether minimizing it through explicit regularization improves model faithfulness.

2. **Semantic Flow Dynamics:** During inference, track the Semantic Information Flow $\imath(S_{1:n} \rightarrow U_{n+1:i})$ across generated sequences. Test whether this quantity exhibits the predicted sub-martingale behavior (monotonic increase) and whether deviations predict generation collapse or loop formation.

3. **Embedding Geometry Preservation:** Using the Gromov-Wasserstein distance framework (Definition 12), compare semantic geometry preservation between standard Transformer embeddings and alternative architectures (Mamba, Mamba2) across multiple tasks. Quantify whether the bilinear attention form provides measurable advantages in maintaining semantic consistency under compression or quantization.