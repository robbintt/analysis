---
ver: rpa2
title: 'The Birth of Knowledge: Emergent Features across Time, Space, and Scale in
  Large Language Models'
arxiv_id: '2505.19440'
source_url: https://arxiv.org/abs/2505.19440
tags:
- concepts
- arxiv
- features
- across
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how interpretable categorical features
  emerge in large language models (LLMs) across training time, transformer layers,
  and model scale. Using sparse autoencoders with automated interpretability labeling,
  the researchers tracked concept activation for nine academic domains in the Pythia
  model family.
---

# The Birth of Knowledge: Emergent Features across Time, Space, and Scale in Large Language Models

## Quick Facts
- arXiv ID: 2505.19440
- Source URL: https://arxiv.org/abs/2505.19440
- Authors: Shashata Sawmya; Micah Adler; Nir Shavit
- Reference count: 40
- Primary result: Sparse autoencoders can map interpretable features across training time, layers, and model scale, revealing structured emergence patterns

## Executive Summary
This study investigates how interpretable categorical features emerge in large language models (LLMs) across training time, transformer layers, and model scale. Using sparse autoencoders with automated interpretability labeling, the researchers tracked concept activation for nine academic domains in the Pythia model family. Across training checkpoints, concept activation grew from under 3% at initialization to over 99% at convergence, with different domains showing distinct temporal patterns. Spatial analysis revealed that features identified in early layers temporarily disappear during middle layers but re-emerge at later layers, challenging assumptions about representation dynamics. Scale analysis showed a sharp threshold at 410M parameters, where activation jumped from under 5% to over 95% for most domains.

## Method Summary
The researchers used top-k sparse autoencoders (SAEs) with k=1 and h=512 latent dimensions to extract semantic features from Pythia model activations across 25 training checkpoints, 37 transformer layers, and 10 model scales (14M-12B parameters). They employed an automated interpretability pipeline (AutoInterp) that uses LLM-based labeling and verification to identify interpretable neurons with F1 scores >0.9. Concept matching was performed using MPNet embeddings and cosine similarity (τ=0.3) to nine academic domains from MMLU and MMLU-PRO datasets. Cross-scale comparisons used orthogonal Procrustes alignment to map activations into a common feature space.

## Key Results
- Concept activation grew from under 3% at initialization to over 99% at convergence across training checkpoints
- Features identified in early layers temporarily disappeared during middle layers but re-emerged at later layers
- A sharp activation threshold occurred at 410M parameters, jumping from under 5% to over 95% for most domains
- Different academic domains showed distinct temporal emergence patterns reflecting their semantic characteristics

## Why This Works (Mechanism)

### Mechanism 1: Top-k Sparse Autoencoder Decomposition
Enforcing k=1 sparsity on an overcomplete latent space (h=512) extracts human-interpretable semantic directions from residual stream activations. The encoder projects d-dimensional residual stream vectors into an overcomplete basis, then hard top-k masking forces exactly one basis vector to participate per sample. This constraint accelerates dictionary formation compared to L1-regularized variants where sparsity emerges gradually. Core assumption: Semantic concepts in the residual stream decompose into approximately linear, near-orthogonal directions that can be captured by a single active latent.

### Mechanism 2: AutoInterp Label-Verify Pipeline
An automated LLM-based labeling loop can validate SAE neuron interpretability at scale with F1 > 0.9 fidelity. For each neuron, collect activating vs. non-activating samples, prompt a teacher LLM to generate a concept label, then verify against held-out samples by asking the LLM whether the label applies. Neurons below F1 threshold τ are discarded. Core assumption: The teacher LLM's concept recognition aligns with the ground-truth activation pattern of each neuron.

### Mechanism 3: Procrustes Cross-Scale Alignment
Orthogonal Procrustes rotation aligns activations from different-scale models into a shared feature space while preserving geometry. For each smaller model m, solve for rotation W* minimizing Frobenius distance between projected activations XmW and the 12B reference X12B. This preserves both global (CKA) and local (pairwise cosine) structure. Core assumption: Different-scale models develop representation geometries that differ primarily by rotation, not fundamental structure.

## Foundational Learning

- **Residual Stream in Transformers**: Why needed: SAEs operate on residual stream activations; understanding that this stream accumulates information across layers via additive skip connections is essential for interpreting why early features re-emerge later. Quick check: If layer 3 adds a "Physics" direction to the residual stream, how could that signal still influence the final token prediction without being explicitly active at layer 20?

- **Overcomplete Dictionary Learning**: Why needed: The SAE uses h=512 latents for d=5120-dimensional inputs—an undercomplete setup here, but the principle of learning an overcomplete basis where each input activates a sparse subset is the theoretical foundation. Quick check: Why might forcing only k=1 latent to activate produce more interpretable features than allowing k=10?

- **Orthogonal Procrustes Problem**: Why needed: Cross-scale comparison requires mapping activations into a common space; Procrustes provides a mathematically grounded way to find the best rotation aligning two point clouds. Quick check: What geometric property does an orthogonal matrix preserve that makes it suitable for comparing neural representation similarity?

## Architecture Onboarding

- **Component map**: SAE Encoder (E) -> Top-k Masking (k=1) -> SAE Decoder (D) -> AutoInterp (Label+Verify) -> EyeSee Framework (MPNet matching)

- **Critical path**: 1) Extract final-token hidden states from Pythia models at target checkpoints/layers 2) Train top-k SAE (k=1, h=512) on these activations with aux-k and multi-k dead-neuron mitigation 3) Run AutoInterp to label all 512 neurons; retain only those with F1 ≥ 0.9 4) For each query subject, compute cosine similarity between subject embedding and all neuron label embeddings 5) Track activation percentage of matched neurons across time/space/scale axes

- **Design tradeoffs**: k=1 vs. higher k maximizes per-neuron interpretability (peak F1 at k=1) but may miss compositional concepts requiring multiple active features; h=512 ≪ d=5120 trades completeness for manageable concept inventory; τ=0.3 cosine threshold captures more neurons per subject but increases false positives

- **Failure signatures**: Dead latents (neurons that never activate during training); Low AutoInterp F1 (<0.9) labels that don't generalize; No cross-scale alignment if CKA < 0.7 or cosine correlation < 0.5 for small models

- **First 3 experiments**: 1) Temporal replication on single domain: Physics samples from Pythia-12B checkpoints, plot activation %—should show early-onset pattern 2) Spatial re-emergence probe: Train SAE on layer 0 activations, test layers 0, 5, 15, 25, 36—expect sharp drop mid-stack followed by re-emergence 3) Scale threshold verification: Biology domain from all 10 Pythia scales, project via Procrustes to 12B space, plot activation %—should observe jump from <5% at 160M to >90% at 410M

## Open Questions the Paper Calls Out

1. What specific internal network circuitry or computational mechanisms drive the observed "disappear-and-return" motif where early-layer features suppress in mid-layers before reactivating at later layers? [explicit] The authors explicitly state they "did not conduct a finer-grained analysis to elucidate the underlying reasons for these emergent patterns... such as... internal network circuitry."

2. To what extent do training data distributions (e.g., frequency of formal symbols vs. natural language context) causally determine the distinct temporal emergence patterns of different domains? [explicit] The conclusion lists "specific contributions of training data distributions" as a key limitation and avenue for future research.

3. What mechanistic factors explain the temporary decline in feature activation observed between 20,000 and 30,000 training steps? [inferred] Section 4 documents a "-31.8 pp dip" and suggests it may reflect "re-organisation" or optimization efficacy issues, but does not conclude which factor is responsible.

## Limitations

- The study relies on LLM-based interpretability labeling without human verification or ground truth comparison
- Cross-scale alignment assumes geometric similarity across scales that may not hold for models below 410M parameters
- Temporal analysis covers only Pythia checkpoints from 0 to 143K steps, missing potential late-stage emergence patterns

## Confidence

- **High Confidence**: Temporal emergence patterns showing activation growth from <3% to >99% across training, and the spatial disappearance/re-emergence pattern in middle layers
- **Medium Confidence**: The 410M parameter scale threshold, based on only 10 discrete model sizes
- **Low Confidence**: The semantic interpretability of individual SAE neurons, as this relies entirely on LLM-based labeling without human verification

## Next Checks

1. Have human experts independently label a random sample of high-F1 SAE neurons to verify AutoInterp's F1 scores and assess whether the LLM-generated labels accurately capture human semantic understanding.

2. Apply the same SAE and AutoInterp pipeline to a completely different domain (e.g., scientific abstracts or legal documents) to determine if the emergence patterns and scale thresholds are universal or Pythia/MMLU-specific artifacts.

3. Train intermediate-scale Pythia models between 160M and 410M parameters (e.g., 250M, 320M, 380M) to precisely characterize the activation threshold and determine whether it represents a sharp phase transition or gradual increase.