---
ver: rpa2
title: 'Beyond the Textual: Generating Coherent Visual Options for MCQs'
arxiv_id: '2508.18772'
source_url: https://arxiv.org/abs/2508.18772
tags:
- visual
- question
- generation
- options
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CmOS, a framework for generating multiple-choice
  questions with visual options using multimodal large language models. The method
  integrates content discrimination, question generation, and visual option generation
  through a multimodal chain-of-thought reasoning process, with retrieval-augmented
  generation to improve quality.
---

# Beyond the Textual: Generating Coherent Visual Options for MCQs

## Quick Facts
- **arXiv ID**: 2508.18772
- **Source URL**: https://arxiv.org/abs/2508.18772
- **Reference count**: 40
- **Primary result**: CmOS achieves 88.2% content discrimination accuracy and generates visual options with higher structural similarity and semantic alignment than baselines

## Executive Summary
This paper introduces CmOS, a framework for generating multiple-choice questions with visual options using multimodal large language models. The method integrates content discrimination, question generation, and visual option generation through a multimodal chain-of-thought reasoning process, with retrieval-augmented generation to improve quality. CmOS achieves state-of-the-art performance, with accuracy of 88.2% on content discrimination and BLEU-4 scores of 76.8 for question generation. Visual options generated by CmOS show significantly higher structural similarity and semantic alignment compared to baselines. Human evaluation confirms the quality and educational value of the generated questions and visual distractors.

## Method Summary
CmOS operates as a four-stage pipeline: (1) Exemplar retrieval and content discrimination using FARE encoder to identify convertible content, (2) Question and reason generation via multimodal chain-of-thought prompting producing multiple candidate pairs, (3) Optimal question-reason matching (OQRM) that selects the highest-scoring pair based on internal coherence and external alignment, and (4) Visual option generation using template retrieval, text-to-image generation, and iterative MLLM-guided refinement. The framework uses Qwen2.5-VL-7B-Instruct as the backbone, retrieves educational image templates from a constructed database, and employs iterative tuning with a threshold of 0.8 over up to 3 rounds.

## Key Results
- Content discrimination accuracy reaches 88.2% on ScienceQA test set
- Question generation achieves BLEU-4 score of 76.8 and ROUGE-L of 82.5
- Visual options show SSIM improvement of 11.2 points with templates and CLIP-T improvement of 12.5 points with tuning
- Human evaluation scores average 3.55/5 for generated visual options

## Why This Works (Mechanism)

### Mechanism 1: Exemplar-Based Content Discrimination
- Claim: Retrieving similar multimodal exemplars improves the model's ability to identify content suitable for visual-option conversion.
- Mechanism: The FARE encoder retrieves exemplars by computing cross-modality similarities (text, answer, image) against a constructed dataset of 482 annotated examples. These concrete examples provide reference cases for the discriminator rather than relying on abstract judgment criteria alone.
- Core assumption: The convertibility of new content correlates with similarity to previously-annotated convertible instances.
- Evidence anchors: Removing exemplar retrieval causes accuracy to drop by 19.5%, suggesting concrete reason exemplars contribute more effectively than predefined standards.

### Mechanism 2: Optimal Question-Reason Matching (OQRM)
- Claim: Selecting question-reason pairs based on both internal coherence and external alignment produces higher-quality questions.
- Mechanism: OQRM computes a Total Match Score (TMS) combining internal consistency (how coherent the pair is relative to other generated pairs) and external consistency (alignment with original image and answer). The highest-scoring pair is selected rather than defaulting to the first generated.
- Core assumption: Self-consistency across multiple reasoning paths correlates with question quality and semantic fidelity to references.
- Evidence anchors: Removing OQRM causes BLEU-4 to plummet by 33.1 and ROUGE-L to drop by 7.3.

### Mechanism 3: Template Retrieval and Iterative Tuning for Visual Options
- Claim: Using retrieved image templates combined with iterative MLLM-guided refinement improves structural similarity and semantic alignment of generated visual options.
- Mechanism: For each option description, the system retrieves the most similar image from an educational database using combined image-description and caption-description similarity. The T2I model generates using this template, then an MLLM evaluates output against the description (threshold σ=0.8). If insufficient, suggestions guide up to 3 refinement rounds.
- Core assumption: Educational images share structural and stylistic properties that can be transferred via template conditioning.
- Evidence anchors: Removing templates drops SSIM by 11.2; removing tuning drops both SSIM (-4.4) and CLIP-T (-12.5).

## Foundational Learning

- **Concept: Multimodal Chain-of-Thought (MCoT)**
  - Why needed here: CmOS uses MCoT to decompose content discrimination, question generation, and visual option synthesis into reasoned steps rather than end-to-end generation.
  - Quick check question: Can you explain how MCoT differs from standard prompting when the input contains both an image and text?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: RAG is used twice—first to retrieve exemplars for discrimination, then to retrieve image templates for visual option generation.
  - Quick check question: What retrieval signal would you use to find a similar educational image: visual embedding alone, caption embedding alone, or a weighted combination?

- **Concept: Self-Consistency Decoding**
  - Why needed here: OQRM adapts self-consistency (sampling multiple reasoning paths and selecting the most consistent) to multimodal question-reason pairs.
  - Quick check question: Why might selecting the most frequent answer across reasoning paths reduce hallucinations compared to greedy decoding?

## Architecture Onboarding

- **Component map:**
  Input (Context + Image + Answer) -> [Discriminator + Exemplar Retrieval] -> Convertible? -> [Question Generator + Reason Generator] -> Multiple Q-R pairs -> [OQRM] -> Optimal (Q*, R*) -> [Option Generator] -> Text options + visual descriptions -> [Template Retrieval + T2I + Tuning Loop] -> Visual options -> Output (Question + Visual options)

- **Critical path:** The discriminator gates all downstream processing. If content is misclassified as non-convertible, question and visual generation are skipped. OQRM is the second critical node—without it, BLEU-4 drops 33 points.

- **Design tradeoffs:**
  - Template retrieval improves SSIM (visual consistency) but can constrain CLIP-T (semantic alignment)—Table 3 shows CLIP-T improves without templates.
  - Exemplar pool size vs. coverage: Small pool (482 examples) enables efficient retrieval but limits generalization to out-of-distribution content.
  - Tuning rounds (max 3) balance quality vs. latency; human evaluation shows even tuned outputs score only 3.55/5.

- **Failure signatures:**
  - Low discrimination accuracy on Natural Sciences (NAT) and image-containing (IMG) questions—Figure 3 shows these are hardest.
  - Generated visual options with "limited visual detail, occasional content hallucinations, and inconsistent style" (Limitations section).
  - Language Science (LAN) questions underperform across all metrics—Table 1, Table 3.

- **First 3 experiments:**
  1. Ablate exemplar retrieval: Run discriminator with zero-shot prompting only. Expect ~19% accuracy drop. Confirms exemplar contribution.
  2. Vary OQRM weight α: Sweep α from 0.1 to 1.2 on a held-out sample. Expect peak at α=0.6 (per Figure 6). Confirms internal vs. external consistency balance.
  3. Test template vs. no-template on same descriptions: Generate visual options with and without retrieved templates for identical option descriptions. Compare SSIM and CLIP-T divergence. Confirms tradeoff between consistency and alignment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework maintain performance when input content falls outside the distribution of the fixed exemplar pool (e.g., non-science domains)?
- Basis in paper: [explicit] The "Limitations" section states that the current strategy relies on a fixed pool of exemplars, and retrieved exemplars may exhibit low semantic similarity for out-of-distribution content, degrading performance.
- Why unresolved: The current retrieval mechanism is heavily dependent on the source dataset (ScienceQA), limiting generalizability to broader educational contexts.
- What evidence would resolve it: Demonstrating stable discrimination accuracy on datasets from disparate domains (e.g., history or arts) using adaptive retrieval or synthetic exemplar generation.

### Open Question 2
- Question: Can text-to-image models be optimized to prevent visual hallucinations and ensure pedagogical accuracy in generated distractors?
- Basis in paper: [explicit] The "Limitations" section notes that generated visual options suffer from "content hallucinations" and "inconsistent style," contributing to low human evaluation scores (3.55/5) for plausibility.
- Why unresolved: General-purpose image generation models are not fine-tuned for the specific constraints and semantic precision required for educational visual options.
- What evidence would resolve it: Fine-tuning a T2I model on a curated educational dataset and showing a statistically significant increase in human-rated plausibility and semantic alignment.

### Open Question 3
- Question: Does the use of generated visual options lead to improved learning outcomes or reasoning skills compared to textual options?
- Basis in paper: [inferred] The paper relies on human evaluation of "educational value" by annotators, but does not conduct experiments measuring actual student performance, retention, or cognitive load.
- Why unresolved: Expert evaluation confirms surface-level quality, but the cognitive impact of visual distractors on students remains unverified.
- What evidence would resolve it: A comparative user study measuring student test scores and cognitive engagement when using CmOS-generated questions versus standard text-based MCQs.

## Limitations
- Exemplar pool size (482 examples) may not generalize to all educational domains, particularly for abstract concepts or highly specialized content
- Generated visual options show "limited visual detail, occasional content hallucinations, and inconsistent style" with average human evaluation score of only 3.55/5
- Template retrieval improves visual consistency (SSIM) but can constrain semantic alignment (CLIP-T), creating a fundamental tradeoff

## Confidence

**High Confidence**: Content discrimination accuracy (88.2%), OQRM contribution to question quality (BLEU-4 drop of 33.1 without it), and the basic efficacy of exemplar-based retrieval.

**Medium Confidence**: Visual option quality metrics, particularly human evaluation scores and the SSIM/CLIP-T tradeoff with templates. The human evaluation methodology and scoring criteria are not fully detailed.

**Low Confidence**: Generalizability to domains outside the exemplar pool and performance on abstract or specialized educational content.

## Next Checks

1. **Out-of-distribution test**: Run CmOS on a held-out subset of ScienceQA containing question types not represented in the exemplar pool (e.g., rare NAT or LAN question types). Measure discrimination accuracy and visual option quality to quantify generalization limits.

2. **Template ablation on same content**: Generate visual options for identical option descriptions with and without template retrieval, then compute the correlation between SSIM improvement and CLIP-T degradation across all generated options. This would quantify the consistency-alignment tradeoff.

3. **Exemplar pool size sensitivity**: Create progressively smaller exemplar subsets (100, 200, 400 examples) and measure discrimination accuracy at each size. This would establish whether the current 482-example pool is near-optimal or could be reduced for efficiency.