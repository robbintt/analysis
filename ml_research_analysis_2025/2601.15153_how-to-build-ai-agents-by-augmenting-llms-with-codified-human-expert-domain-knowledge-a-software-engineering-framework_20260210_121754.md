---
ver: rpa2
title: How to Build AI Agents by Augmenting LLMs with Codified Human Expert Domain
  Knowledge? A Software Engineering Framework
arxiv_id: '2601.15153'
source_url: https://arxiv.org/abs/2601.15153
tags:
- expert
- visualization
- knowledge
- system
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a software engineering framework that captures
  human expert domain knowledge to build AI agents capable of autonomous expert-level
  performance in visualization generation. The framework systematically codifies tacit
  expert knowledge through complementary strategies: request classification, Retrieval-Augmented
  Generation (RAG) for code generation, codified expert rules, and visualization design
  principles.'
---

# How to Build AI Agents by Augmenting LLMs with Codified Human Expert Domain Knowledge? A Software Engineering Framework

## Quick Facts
- arXiv ID: 2601.15153
- Source URL: https://arxiv.org/abs/2601.15153
- Reference count: 29
- 206% improvement in output quality metrics and expert-level ratings achieved

## Executive Summary
This paper presents a software engineering framework for building AI agents that achieve autonomous expert-level performance in visualization generation by systematically codifying human expert domain knowledge. The approach combines request classification, Retrieval-Augmented Generation (RAG) for code generation, codified expert rules, and visualization design principles to transform tacit expert knowledge into executable agent capabilities. An industrial case study across electrochemical, electromagnetic, and mechanical engineering domains demonstrates that the framework enables non-experts to generate expert-level visualizations through simple prompts, effectively democratizing domain knowledge while maintaining superior code quality with lower variance.

## Method Summary
The framework captures human expert domain knowledge through a systematic codification process, addressing the bottleneck of limited domain experts in simulation data visualization. The method involves interviewing experts to extract rules, implementing Python scripts for explicit rules like convergence checking and data normalization, creating a RAG database with domain-specific visualization code snippets, building an orchestrator that classifies queries, runs rule scripts, retrieves RAG context, and constructs the final prompt with visualization guidelines. The approach was evaluated through comparative tests between a baseline (LLM + RAG only) and the proposed framework (Agent + Rules) across five scenarios in three engineering domains, with human experts rating generated visualizations on a 0-3 scale for insight and visual clarity.

## Key Results
- 206% improvement in output quality metrics, achieving expert-level ratings (Mode=3) consistently versus baseline's poor performance
- Superior code quality with lower variance maintained across physics-agnostic design principles enabling cross-domain application without retraining
- Non-experts able to generate expert-level visualizations through simple prompts, democratizing domain knowledge and addressing organizational bottlenecks

## Why This Works (Mechanism)
The framework works by systematically codifying tacit expert knowledge into executable rules and guidelines that guide the LLM's generation process. By combining request classification for routing, RAG for domain-specific code retrieval, codified expert rules for analytical reports, and visualization design principles injected into prompts, the system creates a structured knowledge base that compensates for the LLM's lack of domain-specific expertise. This multi-layered approach ensures that even when the LLM lacks specific training data, the codified rules and RAG components provide the necessary context and constraints for generating expert-level outputs.

## Foundational Learning
- **Expert Knowledge Codification**: Converting tacit expert knowledge into explicit, executable rules and scripts; needed to create a structured knowledge base that guides the LLM, quick check is verifying rule scripts produce correct analytical reports
- **Request Classification**: Categorizing user queries to determine appropriate processing pathways; needed to route requests through correct rule sets and RAG components, quick check is testing classifier accuracy across query types
- **RAG Integration**: Retrieval-Augmented Generation for accessing domain-specific code snippets; needed to supplement LLM's general training with specialized knowledge, quick check is measuring retrieval relevance and impact on output quality
- **Prompt Engineering**: Constructing prompts that inject visualization guidelines and expert rules; needed to ensure consistent application of domain knowledge, quick check is validating prompt structure produces expected visualizations
- **Cross-Domain Application**: Applying framework across different engineering domains without retraining; needed to demonstrate scalability and generality, quick check is testing framework performance across diverse physics domains

## Architecture Onboarding
**Component Map**: User Request -> Request Classifier -> Rule Script Execution -> RAG Retrieval -> Prompt Constructor -> LLM Generation -> Output Evaluation
**Critical Path**: Request Classifier -> Rule Script Execution -> RAG Retrieval -> Prompt Constructor -> LLM Generation (this sequence ensures proper knowledge injection before generation)
**Design Tradeoffs**: Expert interview-based knowledge extraction provides high-quality domain knowledge but introduces potential bias and scalability concerns; physics-agnostic design enables cross-domain application but may sacrifice domain-specific optimizations; codified rules ensure consistency but require ongoing maintenance as expert knowledge evolves
**Failure Signatures**: Catastrophic Y-axis scaling (-1E+99 to 2E+98) indicating normalization rules not being triggered; LLM defaulting to standard libraries instead of domain-specific APIs suggesting insufficient RAG retrieval; inconsistent output quality indicating incomplete knowledge codification
**3 First Experiments**:
1. Implement and test convergence checking and data normalization rule scripts independently to verify they produce correct analytical reports
2. Build a minimal RAG system with domain-specific code snippets and test retrieval accuracy for common visualization patterns
3. Create a simple prompt constructor that injects basic visualization guidelines and evaluate its impact on baseline LLM output quality

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on expert interviews for knowledge extraction introduces potential bias and scalability concerns, with quality of codified knowledge directly impacting agent performance
- Small sample size of 12 evaluators and limited scenarios across three domains raises questions about generalizability to other engineering fields
- Framework's dependence on specific domain experts for knowledge codification creates potential bottleneck that contradicts stated goal of democratizing expertise
- Implementation details for critical components like Request Classifier and RAG system remain underspecified, making direct replication challenging

## Confidence
- **High Confidence**: 206% improvement in output quality metrics and consistent expert-level ratings (Mode=3) achieved by proposed framework versus baseline; methodology combining RAG, codified rules, and expert guidelines is sound and demonstrates measurable improvements
- **Medium Confidence**: Framework's ability to democratize domain knowledge and enable non-experts to generate expert-level visualizations; broader organizational implementation would require additional validation
- **Low Confidence**: Claims about physics-agnostic design principles enabling cross-domain application without retraining, given limited testing across only three engineering domains

## Next Checks
1. **Scalability Test**: Evaluate framework's performance when codified knowledge is extracted from multiple experts versus single-domain specialists to assess robustness to knowledge diversity
2. **Cross-Domain Generalization**: Apply framework to an entirely different engineering domain (e.g., civil engineering or aerospace) without additional retraining to validate physics-agnostic claims
3. **Implementation Replication**: Attempt to recreate Request Classifier and RAG components using only specifications provided, measuring fidelity to described functionality and impact on overall system performance