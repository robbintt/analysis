---
ver: rpa2
title: 'LADFA: A Framework of Using Large Language Models and Retrieval-Augmented
  Generation for Personal Data Flow Analysis in Privacy Policies'
arxiv_id: '2601.10413'
source_url: https://arxiv.org/abs/2601.10413
tags:
- data
- privacy
- personal
- policies
- flow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents LADFA, an end-to-end framework that combines
  LLMs with retrieval-augmented generation (RAG) and a customised knowledge base to
  automate the extraction of personal data flows from privacy policies. The framework
  processes unstructured text, extracts data flows, constructs data flow graphs, and
  performs analysis to discover insights.
---

# LADFA: A Framework of Using Large Language Models and Retrieval-Augmented Generation for Personal Data Flow Analysis in Privacy Policies

## Quick Facts
- **arXiv ID:** 2601.10413
- **Source URL:** https://arxiv.org/abs/2601.10413
- **Reference count:** 40
- **Primary result:** High inter-rater reliability (Gwet's AC1 ~0.94-0.96) for extracting personal data flows from privacy policies using LLM + RAG.

## Executive Summary
LADFA is an end-to-end framework that automates the extraction of personal data flows from privacy policies by combining Large Language Models with Retrieval-Augmented Generation and a customized knowledge base. The system processes unstructured HTML text, extracts data flow tuples, constructs graphs, and performs analysis to uncover data collection and sharing practices. A case study on ten connected-vehicle mobile apps demonstrated high accuracy, with domain experts validating most extraction tasks at 6-7 on a 7-Likert scale. Graph network analysis revealed key entities and potential privacy concerns in data practices.

## Method Summary
The framework uses a multi-stage pipeline: a custom HTML parser segments policies while preserving context, followed by a sequential chain of LLM agents that extract and classify data flows. RAG retrieves relevant knowledge from a domain-specific vector database to ground LLM outputs in standardized taxonomies. The agents process text segments to identify senders, data types, receivers, and attributes like purpose and method. Finally, a post-processor disambiguates entities and generates a graph for analysis using centrality metrics.

## Key Results
- Inter-rater reliability for data type and flow identification reached Gwetâ€™s AC1 of 0.94 and 0.96, with percentage agreement of 0.82 and 0.86.
- Average 7-Likert scores between 6 and 7 for most tasks, indicating high expert validation.
- Graph analysis highlighted primary data aggregators and revealed insights about data collection and sharing practices across entities.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** RAG reduces hallucination and aligns outputs with a standardized taxonomy.
- **Mechanism:** Custom knowledge typologies are embedded in a vector database; relevant definitions are retrieved via semantic similarity and injected into prompts, constraining LLM classification.
- **Core assumption:** Semantic similarity threshold (0.6) effectively separates relevant context from noise, and LLM adheres to provided context over parametric memory.
- **Evidence anchors:** Combines LLMs with RAG and customized knowledge base; constructs four knowledge typologies derived from existing studies; RAG utility in personalizing LLMs supported by corpus references.
- **Break condition:** Novel domain-specific terminology not well-represented in vector embeddings may cause RAG retrieval failure and "Unspecified" categorization.

### Mechanism 2
- **Claim:** Sequential prompt chaining improves reliability of extracting comprehensive data flows.
- **Mechanism:** Decomposes task into agents: Screening (filters irrelevant text), Data Flow (extracts raw tuples), and subsequent agents classify attributes individually, reducing cognitive load per step.
- **Core assumption:** Errors do not propagate catastrophically; correct flow extraction is robust to subsequent classification variations.
- **Evidence anchors:** Framework consists of distinct processing steps; explicit use of prompt chains and LLM agents; validated by corpus study on LLM word-level annotation in privacy policies.
- **Break condition:** Initial agent missing or hallucinating a tuple leads to propagation of error to final graph.

### Mechanism 3
- **Claim:** Context-preserving segmentation enables extraction from complex HTML structures.
- **Mechanism:** Custom HTML parser merges list items with parent headers and flattens nested lists, preserving local semantic context required for accurate tuple extraction.
- **Core assumption:** Critical context is contained within immediate structural hierarchy of HTML elements.
- **Evidence anchors:** Module TS and Algorithm 1 for extracting non-table content while maintaining hierarchy; table-to-flow conversion relies on headers.
- **Break condition:** Implied context across non-hierarchical boundaries may be lost, isolating lists and removing necessary context.

## Foundational Learning

- **Concept:** Contextual Integrity (CI) Theory
  - **Why needed here:** Framework maps privacy policies to CI parameters (actors, attributes, transmission principles); understanding difference between "data sender" and "data subject" is required.
  - **Quick check question:** Can you define the difference between a "transmission principle" and a "data processing purpose" in the framework context?

- **Concept:** Vector Embeddings & RAG
  - **Why needed here:** Core innovation uses RAG to ground LLM; must understand how cosine similarity on vector embeddings retrieves "knowledge contexts" to augment prompts.
  - **Quick check question:** If semantic similarity threshold is lowered from 0.6 to 0.4, what is likely impact on prompt context?

- **Concept:** Prompt Chaining
  - **Why needed here:** LLM-based processor relies on sequence of agents; understanding output-to-input flow is vital for debugging.
  - **Quick check question:** Which agent is responsible for determining if a data flow is "First-party" vs "Third-party"?

## Architecture Onboarding

- **Component map:** Pre-processor (HTML parser + Knowledge Typology) -> LLM-based Processor (Orchestrator + 4-5 agents) -> Post-processor (Data Parser -> Graph Generator -> Analyser)
- **Critical path:** The LLM Data Flow Agent; if it fails to extract initial tuple (Sender, Type, Receiver) correctly, subsequent agents have nothing to process.
- **Design tradeoffs:** Mix of model sizes (70B for complex extraction, 8B for classification) balances efficiency and accuracy; tradeoff between strict ground-truth and flexible LLM generation; relies on manual validation rather than gold-standard dataset.
- **Failure signatures:** High volume of "Unspecified" or "Other" categories (RAG failure or vague policy); "Unknown" nodes in graph (incomplete data flows); misclassification of "First-party" as "Third-party" (ambiguity in Data Parser logic).
- **First 3 experiments:**
  1. **HTML Robustness Test:** Run Module TS parser on privacy policy with heavy nested tables to verify correct header merging with data rows.
  2. **RAG Retrieval Calibration:** Run single segment through "Data Category Agent" with different similarity thresholds (e.g., 0.5 vs 0.7) to observe classification confidence changes.
  3. **Graph Analysis:** Generate data flow graph for single policy and verify if "Betweenness Centrality" correctly identifies primary data aggregator (usually First-Party node).

## Open Questions the Paper Calls Out

- **Open Question 1:** What is the optimal text segmentation strategy to balance cross-paragraph contextual understanding with granular precision required for accurate data flow extraction?
  - **Basis in paper:** Section 5 notes sequential processing may overlook cross-paragraph information, while feeding entire texts causes loss of granularity and incorrect associations.
  - **Why unresolved:** Current implementation relies on fixed segmentation pipeline trading off context window size against extraction precision.
  - **What evidence would resolve it:** Comparative study testing dynamic or context-aware segmentation algorithms against current static approach using cross-reference accuracy metric.

- **Open Question 2:** Does high inter-rater reliability among co-authors generalize to independent external domain experts when validating extracted data flows?
  - **Basis in paper:** Section 5 explicitly states relying on three co-authors as evaluators "can introduce potential biases" and that "more independent validation study" is needed.
  - **Why unresolved:** Reported Gwet's AC1 scores (0.94, 0.96) may be inflated by authors' familiarity with system's design and intent.
  - **What evidence would resolve it:** User study recruiting independent privacy experts to validate LADFA outputs, replicating 7-Likert scale evaluation to compare against co-authors' scores.

- **Open Question 3:** How effectively does LADFA framework extract comprehensive data flows when applied to privacy policies outside automotive domain?
  - **Basis in paper:** Section 5 highlights while case study focused on automotive industry, future work will "explore applications of LADFA to different types of documents."
  - **Why unresolved:** Customized knowledge base and preprocessing rules were tailored to specific linguistic structures and data types of connected-vehicle apps.
  - **What evidence would resolve it:** Case study applying LADFA to privacy policies from diverse sectors (e.g., finance, healthcare) with minimal re-engineering of knowledge typologies.

- **Open Question 4:** To what extent does ambiguity in knowledge base definitions, specifically regarding "active" vs. "passive" data processing methods, degrade classification accuracy of LLM-RAG agents?
  - **Basis in paper:** Section 5 acknowledges "concepts such as active and passive/automatic data processing are inherently difficult to define with great clarity," causing ambiguous outputs.
  - **Why unresolved:** Paper admits vague definitions lead to different interpretations by both humans and LLMs, contributing to lower agreement scores seen in "Data Processing Method" evaluations.
  - **What evidence would resolve it:** Ablation study comparing agent performance using current knowledge base against rigorously refined version co-created by multiple independent legal experts.

## Limitations

- Exact prompt templates for "Data Consumer Type" and "Processing Purpose/Method" agents are not fully specified in paper, affecting complete reproducibility.
- Complete list of keywords for post-processor heuristic rules disambiguating "first-party" vs. "user-party" entities is only partially provided, creating uncertainty in classification accuracy.
- Inter-rater reliability may be inflated by authors' familiarity with system; needs independent external expert validation.

## Confidence

**High Confidence Claims:**
- Sequential prompt chain architecture improves reliability over monolithic prompts (supported by explicit methodology and domain expert validation)
- RAG effectively grounds LLM outputs to standardized taxonomy, reducing hallucination (demonstrated by high inter-rater reliability scores and context injection mechanism)

**Medium Confidence Claims:**
- Context-preserving HTML segmentation is essential for accurate data flow extraction (algorithm described but no ablation study provided)
- System successfully identifies primary data aggregators through graph analysis (validated on 10 apps but sample size is small)

## Next Checks

1. **HTML Segmentation Robustness:** Test Module TS parser on privacy policies with nested tables and complex list structures to verify context preservation.
2. **RAG Retrieval Threshold Sensitivity:** Run "Data Category Agent" with varying similarity thresholds (0.5, 0.6, 0.7) to assess impact on classification accuracy.
3. **Prompt Template Verification:** Compare publicly available code repository prompts against paper appendix examples to identify any discrepancies in agent instructions.