---
ver: rpa2
title: 'MoNE: Replacing Redundant Experts with Lightweight Novices for Structured
  Pruning of MoE'
arxiv_id: '2507.00390'
source_url: https://arxiv.org/abs/2507.00390
tags:
- pruning
- mone
- expert
- experts
- mc-smoe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MoNE is a novel expert pruning method for Mixture-of-Experts models
  that replaces redundant experts with lightweight novices to reduce memory overhead
  while preserving performance. It identifies redundant experts using access frequency
  and output variance metrics, then replaces them with constant novice vectors that
  approximate their outputs.
---

# MoNE: Replacing Redundant Experts with Lightweight Novices for Structured Pruning of MoE

## Quick Facts
- arXiv ID: 2507.00390
- Source URL: https://arxiv.org/abs/2507.00390
- Reference count: 40
- Key outcome: MoNE achieves up to 2.71% average zero-shot accuracy improvement under 25% pruning and 3.61% under 50% pruning across nine downstream tasks while reducing memory overhead.

## Executive Summary
MoNE is a novel expert pruning method for Mixture-of-Experts models that replaces redundant experts with lightweight novices to reduce memory overhead while preserving performance. It identifies redundant experts using access frequency and output variance metrics, then replaces them with constant novice vectors that approximate their outputs. Extensive experiments show MoNE outperforms baseline methods across different model architectures, calibration data sources, and sample sizes.

## Method Summary
MoNE evaluates expert redundancy using two metrics: access frequency and output variance. Experts with low usage and stable outputs are pruned and replaced with lightweight novices—constant vectors representing the mean output of the pruned expert. The method operates layer-wise to avoid intractable global search, computing redundancy scores for each expert independently per layer. Novices are stored as d-dimensional vectors rather than full expert parameters, enabling memory-efficient inference while maintaining output quality through approximation.

## Key Results
- Improves average zero-shot accuracy by up to 2.71% under 25% pruning and 3.61% under 50% pruning across nine downstream tasks
- Reduces memory overhead while maintaining performance across different calibration data sources and sample sizes
- Outperforms baseline methods including Angular, FLAP, and GShard on OLMoE-1B-7B and DeepSeek-V2-Lite models

## Why This Works (Mechanism)

### Mechanism 1
Combining access frequency and output variance improves expert redundancy identification compared to either metric alone. Low-frequency experts contribute less to outputs; low-variance expert outputs can be approximated by constants. Their product score filters experts that satisfy both conditions.

### Mechanism 2
Replacing pruned experts with lightweight novices (mean output vectors) reduces output discrepancy compared to hard removal. The novice Ni = Ēi minimizes L2 discrepancy in closed form, approximating expert contribution without storage.

### Mechanism 3
Layer-wise pruning avoids intractable global search while preserving performance due to localized routing decisions. Each layer independently selects experts to prune based on its own redundancy scores, keeping the problem O(L·M) rather than combinatorial.

## Foundational Learning

- **Mixture-of-Experts (MoE) routing and top-k selection**
  - Why needed here: MoNE modifies expert output composition; understanding router scores G(x) and top-k set Sk,x is essential
  - Quick check question: Given router scores [0.4, 0.3, 0.2, 0.1] and k=2, which experts contribute to the output?

- **Structured vs. unstructured pruning in LLMs**
  - Why needed here: MoNE is structured (expert-level) pruning; contrast with weight-level or layer-level methods clarifies design space
  - Quick check question: Why does structured pruning generally have lower accuracy recovery but better hardware efficiency than unstructured?

- **Calibration data and its role in pruning**
  - Why needed here: MoNE computes variance and frequency on a calibration set C; sample size and source affect robustness
  - Quick check question: If calibration data is biased toward one domain, how might expert pruning decisions be skewed?

## Architecture Onboarding

- **Component map**: MoE Layer (Router G + Expert set E) → MoNE Extension (Redundancy scorer ϕvar, ϕfreq + Novice vectors N) → Calibration pipeline (Dataset C → forward pass → collect outputs and routing stats)

- **Critical path**: 
  1. Run calibration data through model; record expert outputs and routing decisions per layer
  2. Compute ϕvar and ϕfreq for each expert; fuse into ϕ
  3. Select bottom-P experts per layer by ϕ; compute novices as mean outputs
  4. Replace pruned experts with novices in inference graph

- **Design tradeoffs**: Higher pruning ratio → more memory savings but higher accuracy variance; Calibration sample size: 100 samples often sufficient; Calibration source: Zyda2 vs. C4 shows minor differences

- **Failure signatures**: High variance in accuracy drop across tasks suggests domain-specific expert importance; Inference latency not reducing proportionally indicates kernel/implementation bottlenecks

- **First 3 experiments**: 
  1. Replicate Table 1 on OLMoE with 100 Zyda2 samples at 25% pruning; verify avg accuracy within ±0.5 of reported 61.04
  2. Ablation: Run MoNE without novice replacement (hard removal); compare drop to confirm novice contribution
  3. Stress test: Use domain-shifted calibration set (e.g., code-only data for general LLM); measure variance in expert selection vs. Zyda2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Are the frequency and variance metrics sufficient for identifying redundancy in MoE models significantly larger than 16B parameters, or do scaling laws require additional complexity metrics?
- Basis in paper: Section D (Limitations) states authors did not conduct experiments on larger models and suggests "Larger models may require more redundancy features other than the two metrics employed in this paper."
- Why unresolved: Current experiments limited to 7B and 16B models; unknown if linear relationship holds at scales involving hundreds of billions of parameters.
- What evidence would resolve it: Evaluating MoNE on larger architectures (e.g., Mixtral 8x22B or Grok-1) to determine if current redundancy score correlates with performance retention or if higher-order statistics are needed.

### Open Question 2
- Question: Can MoNE be combined with general structured pruning methods (e.g., layer or channel pruning) to achieve finer-grained compression without additive performance degradation?
- Basis in paper: Section D (Limitations) notes authors "do not explore the combination of different structured pruning methods" and suggest that doing so "can potentially expand the search space."
- Why unresolved: Paper treats expert pruning (MoNE) and general pruning (Angular, FLAP) as separate baselines; interaction between removing entire experts and pruning channels within experts remains unexplored.
- What evidence would resolve it: Study applying MoNE followed by FLAP (or vice versa) on same model to analyze cumulative accuracy drop and memory savings compared to unified pruning approach.

### Open Question 3
- Question: What specific inference kernels or hardware optimizations are required to translate MoNE's memory reduction into substantial latency speedups beyond the observed ~10%?
- Basis in paper: Section 5.6 observes MoNE only achieves roughly 10% speedup and suggests "a more efficient inference implementation is necessary to fully unlock the computational advantages."
- Why unresolved: Current HuggingFace implementation likely suffers from overhead when handling conditional execution of "novices" versus standard experts.
- What evidence would resolve it: Development of custom CUDA kernel or system-level optimization that fuses novice constant-vector addition into MoE routing cycle, demonstrating latency reduction proportional to pruning ratio.

## Limitations

- Layer-wise pruning may miss cross-layer expert dependencies that could impact certain tasks or model architectures
- Constant novice approximation assumes pruned experts have stable, low-variance outputs which may not hold for all expert types
- Inference speedup results are mixed, with some configurations showing less than 10% improvement despite significant memory savings

## Confidence

**High Confidence**: Core mechanism of combining access frequency and output variance for redundancy identification; novice-as-mean-output approach; layer-wise pruning as practical compromise.

**Medium Confidence**: Generalization across different calibration datasets and sample sizes; task-specific performance variations suggesting underlying expert importance patterns.

**Low Confidence**: Inference speedup claims requiring additional validation; cross-layer dependency assumption in layer-wise pruning not thoroughly stress-tested.

## Next Checks

1. **Domain Shift Stress Test**: Run MoNE with calibration data from completely different domain (e.g., code-only data for general-purpose LLM) and measure both expert selection stability and downstream performance degradation compared to in-domain calibration.

2. **Global vs. Layer-wise Pruning Comparison**: Implement simplified global pruning baseline considering expert usage across all layers simultaneously and compare performance/memory tradeoffs against MoNE's layer-wise approach on at least two different model architectures.

3. **Adaptive Novice Validation**: Replace constant novices with simple parametric approximations (e.g., linear projections of layer inputs) for experts with moderate variance, and measure whether this hybrid approach reduces accuracy variance while maintaining memory savings.