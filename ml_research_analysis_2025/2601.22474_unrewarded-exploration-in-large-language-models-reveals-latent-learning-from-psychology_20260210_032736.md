---
ver: rpa2
title: Unrewarded Exploration in Large Language Models Reveals Latent Learning from
  Psychology
arxiv_id: '2601.22474'
source_url: https://arxiv.org/abs/2601.22474
tags:
- learning
- unrewarded
- rewards
- llms
- exploration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work establishes that large language models exhibit latent\
  \ learning dynamics analogous to Tolman\u2019s psychological theory. By introducing\
  \ an initial phase of unrewarded exploration\u2014where models generate responses\
  \ without reward signals and learn directly from their own outputs\u2014LLMs demonstrate\
  \ measurable performance gains even in the absence of reinforcement."
---

# Unrewarded Exploration in Large Language Models Reveals Latent Learning from Psychology

## Quick Facts
- arXiv ID: 2601.22474
- Source URL: https://arxiv.org/abs/2601.22474
- Reference count: 19
- One-line primary result: Large language models show latent learning dynamics analogous to Tolman's psychological theory, improving performance through unrewarded exploration followed by rewarded fine-tuning

## Executive Summary
This work establishes that large language models exhibit latent learning dynamics analogous to Tolman's psychological theory. By introducing an initial phase of unrewarded exploration—where models generate responses without reward signals and learn directly from their own outputs—LLMs demonstrate measurable performance gains even in the absence of reinforcement. Performance is further enhanced when rewards are introduced afterward, outperforming models trained with rewards throughout. Theoretical analysis confirms that ratio-clipped Group Relative Policy Optimization with KL regularization yields monotone improvement in expected utility during unrewarded exploration, whether in discrete or continuous output spaces. Extensive experiments across multiple model families, scales, and diverse tasks (mathematical reasoning and GUI agent tasks) validate these findings, bridging cognitive psychology and modern AI learning paradigms.

## Method Summary
The method employs a two-stage Group Relative Policy Optimization (GRPO) training approach. Stage 1 uses unrewarded exploration with fixed advantage A=1 for all tokens, allowing the model to explore response space without external rewards. Stage 2 introduces standard reward signals for fine-tuning. The theoretical foundation shows that ratio-clipped GRPO with KL regularization produces water-filling policy updates that transfer probability mass toward higher-utility tokens. Experiments use Qwen and Llama model families across mathematical reasoning tasks (GSM8K, MATH500, AMC23, AIME24, OlympiadBench, Minerva) and GUI agent tasks (ScreenSpot variants, AndroidControl, GUI-Odyssey, GUIAct-Web, OmniAct-Web/Desktop).

## Key Results
- Unrewarded exploration alone improves performance across all model scales and tasks compared to baseline
- Two-stage training (unrewarded → rewarded) consistently outperforms reward-only training, with avg scores showing +2.1 to +7.7 point improvements
- Larger models (7B) show smaller relative gains from unrewarded exploration, suggesting stronger priors resist unguided exploration
- Theoretical guarantee of monotone improvement holds under ratio clipping and KL regularization assumptions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Unrewarded exploration improves performance because ratio-clipped GRPO with constant advantage and KL regularization produces a water-filling policy update that transfers probability mass toward higher-utility tokens even without explicit reward signals.
- **Mechanism**: When advantage is fixed to A≡1, the GRPO objective reduces to maximizing overlap between the current policy and a clipped proposal distribution, regularized by KL divergence to a reference policy. The optimal solution takes the form π*(y_t|x, y_<t) = min((1+ε)π_old, τ·π_ref), where τ is a normalizing constant. This transfers probability mass from low likelihood-ratio regions to high likelihood-ratio regions.
- **Core assumption**: The monotone-likelihood-ratio condition holds: tokens with higher likelihood ratio h = π_old/π_ref have higher latent utility u*.
- **Evidence anchors**: [abstract]: "Theoretical analysis explains this via monotone improvement in latent utility under GRPO with ratio clipping and KL regularization"; [section 3.3]: Theorem 1 and Theorem 2 derive the water-filling closed-form solution; [corpus]: Related work (ReLaX, arXiv:2512.07558) similarly addresses exploration challenges in RLVR.
- **Break condition**: If the monotone-likelihood-ratio assumption fails (e.g., π_old is not aligned with latent utility), the guarantee J(π*) ≥ J(π_ref) does not hold.

### Mechanism 2
- **Claim**: The KL regularization term acts as a trust region that prevents policy collapse while still permitting exploration-induced improvement.
- **Mechanism**: The KL penalty -β·D_KL[π_θ||π_ref] keeps the updated policy close to the reference policy. This constraint ensures the water-filling solution cannot deviate arbitrarily, maintaining coherence while allowing probability mass redistribution toward higher-utility regions.
- **Core assumption**: The KL coefficient β is appropriately tuned (β=0.001 for math, β=0.01 for GUI tasks per Section 4.4).
- **Evidence anchors**: [section 3.1]: The objective includes "-βD_KL[π_θ||π_ref]" explicitly; [section 3.3]: Equation (4) and (33) show the KL term is essential for the concave optimization; [corpus]: No corpus papers directly validate this KL-trust-region mechanism for unrewarded exploration.
- **Break condition**: If β is too small, policy may collapse to a degenerate solution; if too large, updates are overly constrained and no improvement occurs.

### Mechanism 3
- **Claim**: Two-stage training (unrewarded → rewarded) outperforms reward-only training because unrewarded exploration first organizes task-relevant knowledge without reward-driven bias, then rewards fine-tune this organized representation.
- **Mechanism**: During unrewarded exploration, the model explores the response space broadly. When rewards are introduced, the model has already formed coherent internal representations, enabling more efficient reward-guided optimization.
- **Core assumption**: Unrewarded exploration does not reinforce incorrect patterns that rewards cannot correct.
- **Evidence anchors**: [abstract]: "unrewarded exploration allows LLMs to organize task-relevant knowledge without being constrained by reward-driven biases"; [tables 1-2]: "+ Unrewarded→Rewarded" consistently achieves slightly higher avg scores than "+ Rewarded" (e.g., Qwen-1.5B: 35.5 vs 32.4; Llama-3B: 23.8 vs 22.2); [corpus]: "Zero-Incentive Dynamics" (arXiv:2507.01470) examines unrewarded subgoals.
- **Break condition**: If unrewarded exploration duration is excessive, the model may converge to suboptimal patterns before rewards can redirect it.

## Foundational Learning

- **Concept**: GRPO (Group Relative Policy Optimization)
  - **Why needed here**: The entire theoretical analysis is built on GRPO's objective function with ratio clipping. Understanding how advantages are computed from group rewards is prerequisite.
  - **Quick check question**: Can you explain why setting advantage A≡1 eliminates reward dependence while preserving the clipping structure?

- **Concept**: Monotone Likelihood Ratio and Chebyshev's Association Inequality
  - **Why needed here**: The theoretical guarantee relies on the comonotonicity between likelihood ratio and latent utility, proved via association inequalities.
  - **Quick check question**: If π_old/π_ref is not monotone in latent utility, does the performance improvement guarantee still hold?

- **Concept**: Water-Filling Optimization
  - **Why needed here**: The closed-form solution is a water-filling distribution; understanding this helps interpret how probability mass is redistributed.
  - **Quick check question**: In the water-filling solution π* = min((1+ε)π_old, τπ_ref), what determines the partition into active-cap and interior regions?

## Architecture Onboarding

- **Component map**:
  ```
  Input: Question q → Model π_θ → Sample G responses {o_i}
                              ↓
  Training: Compute ratios r_i,t(θ) = π_θ(o_i,t|q,o_i,<t) / π_old(o_i,t|q,o_i,<t)
            ↓
            Apply clipping: min(r_i,t, 1+ε)  [unrewarded: A≡1]
            ↓
            KL penalty: -β·D_KL[π_θ||π_ref]
            ↓
            Update θ via gradient ascent
  ```

- **Critical path**:
  1. Set advantage A≡1 for unrewarded phase (not zero—this preserves the clipping structure)
  2. Choose ε (clip parameter) and β (KL coefficient) before training
  3. Sample multiple responses per question (group size G=4-5)
  4. Transition timing: when to switch from unrewarded to rewarded

- **Design tradeoffs**:
  - ε larger → more aggressive exploration, but risk of instability
  - β larger → more conservative updates, but slower learning
  - Unrewarded phase longer → more knowledge organization, but compute cost
  - Group size larger → better advantage estimation (when rewarded), but more compute

- **Failure signatures**:
  - No improvement during unrewarded phase: check if A is incorrectly set to 0 instead of 1
  - Performance collapse after reward introduction: β may be too small, or unrewarded phase introduced harmful patterns
  - Larger models showing smaller gains (Table 6): expected—stronger priors resist unguided exploration

- **First 3 experiments**:
  1. **Baseline validation**: Replicate unrewarded → rewarded vs. rewarded-only on GSM8K with Qwen2.5-3B using reported hyperparameters (steps=500, lr=5e-7, β=0.001, ε from GRPO defaults)
  2. **Ablation on ε**: Test ε ∈ {0.1, 0.2, 0.3} to characterize sensitivity of unrewarded exploration to clip range
  3. **Phase duration**: Compare switching points (steps 250, 500, 750) to identify optimal unrewarded phase length before introducing rewards

## Open Questions the Paper Calls Out
- Does the latent learning effect persist under reinforcement learning algorithms other than GRPO, such as PPO?
- Does the benefit of unrewarded exploration scale consistently to models significantly larger than 8B parameters?
- Does the "monotone-likelihood-ratio" assumption required by the theory hold empirically in LLM latent spaces?
- Does unrewarded exploration improve performance in less structured domains like open-ended dialogue?

## Limitations
- Theoretical analysis relies on monotone-likelihood-ratio assumption that requires empirical validation
- Experimental validation limited to mathematical reasoning and GUI agent tasks, leaving generalization to other domains open
- Study uses relatively small model scales (1.5B-7B parameters), limiting conclusions about larger frontier models
- Unrewarded exploration phase duration fixed at 500 steps without systematic analysis of optimal duration scaling

## Confidence
- **High confidence**: Empirical observation that unrewarded exploration provides performance benefits across multiple tasks and model families
- **Medium confidence**: Theoretical mechanism (water-filling solution derivation is sound but depends on assumptions requiring further validation)
- **Medium confidence**: Generalization claims (experiments limited to specific task types and model scales)

## Next Checks
1. **Empirical validation of monotone-likelihood-ratio assumption**: Measure the correlation between likelihood ratios (π_old/π_ref) and actual token utilities across multiple tasks to verify this critical assumption.

2. **Scale-dependent analysis**: Test whether the relative benefits of unrewarded exploration scale predictably with model size, particularly examining whether larger models show diminishing returns as suggested by Table 6.

3. **Duration sensitivity analysis**: Systematically vary unrewarded exploration phase length (e.g., 100, 500, 1000, 2000 steps) across different model sizes and task types to identify optimal exploration-to-refinement ratios.