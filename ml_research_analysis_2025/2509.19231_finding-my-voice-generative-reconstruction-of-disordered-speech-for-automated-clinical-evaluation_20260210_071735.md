---
ver: rpa2
title: 'Finding My Voice: Generative Reconstruction of Disordered Speech for Automated
  Clinical Evaluation'
arxiv_id: '2509.19231'
source_url: https://arxiv.org/abs/2509.19231
tags:
- speech
- chiressd
- reconstruction
- speaker
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ChiReSSD is a style-based TTS reconstruction framework designed
  to improve speech intelligibility for children with speech sound disorders while
  preserving speaker identity. The method adapts StyleTTS2 to disentangle acoustic
  and prosodic style embeddings, suppressing pronunciation-linked acoustic distortions
  while retaining pitch and prosody.
---

# Finding My Voice: Generative Reconstruction of Disordered Speech for Automated Clinical Evaluation

## Quick Facts
- arXiv ID: 2509.19231
- Source URL: https://arxiv.org/abs/2509.19231
- Reference count: 0
- Primary result: ChiReSSD achieves 42% WER reduction and 48% CER reduction on child SSD speech while preserving speaker identity (0.62 similarity)

## Executive Summary
ChiReSSD is a style-based TTS reconstruction framework designed to improve speech intelligibility for children with speech sound disorders while preserving speaker identity. The method adapts StyleTTS2 to disentangle acoustic and prosodic style embeddings, suppressing pronunciation-linked acoustic distortions while retaining pitch and prosody. Evaluation on the STAR dataset shows significant improvements in lexical accuracy, with relative reductions in word error rate (42%) and character error rate (48%) compared to original disordered speech. Clinical assessment reveals moderate correlation (ρ=0.63) between automatic consonant correction metrics and human expert annotations, suggesting potential to reduce manual transcription burden.

## Method Summary
ChiReSSD builds on StyleTTS2, a diffusion-based TTS model, by fine-tuning it to reconstruct disordered speech. The framework disentangles acoustic style embeddings (which encode pronunciation distortions) from prosodic style embeddings (which encode pitch and rhythm). During fine-tuning, the model emphasizes pitch reconstruction loss to better handle children's higher fundamental frequencies. At inference, guidance parameters α and β control the trade-off between pronunciation correction and prosodic naturalness. The system is trained on CSSD (child speech sound disorder) data and evaluated on STAR (children with SSD) and TORGO (adult dysarthria) datasets, using wav2vec2 phone recognition and Levenshtein distance on consonants to estimate clinical PCC metrics.

## Key Results
- Relative reductions in word error rate (42%) and character error rate (48%) compared to original disordered speech
- Speaker similarity scores (0.62) exceed commonly cited thresholds for speaker verification
- Moderate correlation (ρ=0.63) between automatic consonant correction metrics and human expert annotations
- Cross-disorder experiments show consistent improvements across severity levels, with character error rates reduced to below 0.03 even for severe cases

## Why This Works (Mechanism)

### Mechanism 1
Disentangling acoustic from prosodic style embeddings allows selective suppression of pathological pronunciation patterns while retaining speaker identity. StyleTTS2's dual-encoder architecture separates acoustic style (encoding pronunciation-linked distortions) from prosodic style (pitch, timing). By conditioning generation primarily on prosodic style and target phonemes, the model reconstructs "corrected" speech that preserves child-like prosody but replaces distorted articulation patterns with canonical pronunciations from the text encoder.

### Mechanism 2
Increased pitch reconstruction loss weight adapts the model to children's higher fundamental frequency ranges that adult-trained models mishandle. StyleTTS2 was trained on adult speech with lower pitch ranges. By increasing the weight of pitch reconstruction loss during fine-tuning and partially freezing the diffusion model for the first two epochs, the model learns to accurately reproduce children's higher F0 distributions without overfitting to SSD-specific pronunciation errors.

### Mechanism 3
Guidance parameters (α, β) enable fine-grained control over the trade-off between pronunciation correction and prosodic naturalness during inference. α scales acoustic text encoder contribution (higher α = more canonical pronunciation, less influence from disordered acoustic patterns). β regulates prosodic similarity to source style (higher β = more faithful to original prosody). Setting α=0.8 and β=0.6 balances intelligibility gains against natural variation and identity preservation.

## Foundational Learning

- **StyleTTS2 architecture (diffusion-based TTS with style disentanglement)**: Why needed here: ChiReSSD builds directly on StyleTTS2; understanding its dual-encoder design, diffusion denoising process, and style injection mechanism is prerequisite to grasping how disentanglement is achieved. Quick check: Can you explain how StyleTTS2's acoustic and prosodic text encoders differ in what they encode and how they condition the decoder?

- **Percentage of Correct Consonants (PCC) as a clinical metric**: Why needed here: The paper's clinical evaluation hinges on PCC correlation; understanding what PCC measures and why consonants are emphasized in pediatric SSD assessment clarifies the significance of the ρ=0.63 correlation result. Quick check: Why might PCC be more clinically relevant than word error rate for assessing childhood speech sound disorders?

- **Phone recognition and IPA transcription**: Why needed here: The automated clinical evaluation pipeline uses wav2vec2 phone recognition and Levenshtein distance on consonant sequences; understanding phone-to-IPA mapping is necessary to reproduce or extend this analysis. Quick check: How does the choice of phonemizer (e.g., en-gb vs. en-us) affect alignment between predicted and target phonemes in disordered speech?

## Architecture Onboarding

- **Component map**: Child SSD audio → Mel-spectrogram → Acoustic Style Encoder + Prosodic Style Encoder + Pitch Extractor (all fine-tuned). Target text → Phonemizer → Acoustic/Prosodic Text Encoders. Audio-Text Aligner connects modalities. Style Diffusion Denoiser (partially frozen) + Duration/Prosody Predictor → HiFi-GAN Decoder. Inference: Reference child SSD sample → Style Encoders → embeddings. Target text → Phonemizer → Text Encoders → Duration/Prosody prediction. Diffusion Sampler (10 steps) with α/β guidance → Decoder → Reconstructed speech. Evaluation: Original + Reconstructed pairs → wav2vec2 phone recognizer → IPA extraction → consonant filtering → Levenshtein distance → PCC estimation.

- **Critical path**:
  1. Acquire and preprocess child SSD training data (UltraSuite subsets: UXSSD, UPX, UX2020) — clean therapist segments, verify alignments.
  2. Initialize all components from pre-trained StyleTTS2 checkpoints.
  3. Fine-tune for 4 epochs: epochs 1-2 with frozen diffusion parameters + increased pitch loss weight; epochs 3-4 with diffusion unfrozen.
  4. For inference: extract style embeddings from 4s reference → set α=0.8, β=0.6 → run 10-step diffusion → decode.
  5. Evaluate with whisperX for WER/CER, Resemblyzer for speaker similarity, YIN for F0, and wav2vec2 for automated PCC.

- **Design tradeoffs**:
  - **Short fine-tuning (4 epochs) vs. longer training**: Chosen to avoid overfitting to SSD-specific mispronunciations; trade-off is potential underfitting on child prosody patterns.
  - **One-shot style transfer vs. speaker-specific fine-tuning**: One-shot enables deployment without per-child training; trade-off is lower identity fidelity compared to personalized models.
  - **Diffusion steps (10) vs. more steps**: 10 steps balances speed and quality; more steps might improve naturalness but increase latency for real-time applications.

- **Failure signatures**:
  - **Adult-like pitch in outputs**: Indicates insufficient pitch loss weight or overfitting to original StyleTTS2 priors; remedy by increasing pitch reconstruction weight or extending fine-tuning.
  - **Speaker similarity <0.6**: May indicate α is too high (over-suppressing style) or reference sample quality issues; check reference audio SNR and try lower α.
  - **CER not improving**: ASR model may not generalize to reconstructed child speech; verify whisperX confidence scores and consider child-specific ASR fine-tuning.
  - **PCC correlation low with human annotations**: Phone recognizer may misalign on disordered speech; examine IPA outputs manually for systematic errors.

- **First 3 experiments**:
  1. **Baseline replication**: Train ChiReSSD on CSSD, evaluate on STAR, reproduce reported WER (0.49), CER (0.29), and speaker similarity (0.62). Verify setup correctness before any modifications.
  2. **Ablation on α/β guidance**: Run grid search α∈{0.6, 0.7, 0.8, 0.9} × β∈{0.4, 0.5, 0.6, 0.7} on STAR; plot WER vs. speaker similarity trade-off surface to validate chosen operating point.
  3. **Cross-accent stress test**: Evaluate on TORGO using en-gb phonemizer (mismatched to Canadian English) vs. en-us phonemizer; measure sensitivity of CER improvements to phonemizer choice.

## Open Questions the Paper Calls Out

### Open Question 1
Can integrating robust phoneme-aware loss functions into the ChiReSSD framework effectively reduce residual phonetic errors in reconstructed speech? The authors state, "In future work, we aim to further reduce residual phonetic errors by integrating more robust phoneme-aware loss functions." The current implementation relies on style diffusion and fine-tuning, which improved intelligibility but left residual errors, particularly in non-sentential word sequences. A comparative study showing a statistically significant reduction in Character Error Rate (CER) and improved phonetic accuracy in a ChiReSSD variant trained with phoneme-aware losses would resolve this.

### Open Question 2
Is an end-to-end training architecture that directly incorporates Automatic Speech Recognition (ASR) from original disordered speech feasible within this framework? The authors plan to "expand the framework to end-to-end training that includes ASR directly from the original SSD speech." The current method uses a cascaded approach involving external phonemizers and post-hoc ASR analysis (WhisperX) rather than a unified model optimized for both reconstruction and recognition simultaneously. Successful implementation and evaluation of a single model that accepts raw SSD input and outputs reconstructed speech without modular dependencies, maintaining or improving current WER metrics, would resolve this.

### Open Question 3
Can the alignment between automatic consonant correction metrics and human expert annotations be strengthened to enable fully automated clinical evaluation? While the paper reports a moderate correlation (ρ=0.63), it notes substantial absolute discrepancies between automatic PCC (50.65%) and clinical PCC (70.13%) on original samples. The universal phone recognizer (wav2vec2) utilized for automatic transcription appears to struggle with the specific pathological articulations found in SSD, limiting the automatic metric's accuracy. Fine-tuning the phone recognizer on disordered speech data to achieve a higher correlation coefficient (e.g., ρ > 0.8) and reduced absolute error variance compared to human clinicians would resolve this.

## Limitations

- Dataset Generalization: The STAR dataset contains only 6 children with Central Scottish accents, limiting generalizability to other dialects and severity distributions.
- Clinical Metric Correlation: While ρ=0.63 between automated PCC and human annotations suggests moderate agreement, the automated pipeline relies on wav2vec2 phone recognition, which may misalign on reconstructed speech.
- Style Disentanglement Assumptions: The claim that acoustic embeddings encode pronunciation distortions relies on inference from related work rather than direct ablation or probing experiments.

## Confidence

- **High Confidence**: Pitch Adaptation mechanism (clear, measurable improvement with specific strategy)
- **Medium Confidence**: Overall CER/WER Improvements (impressive relative reductions but sensitive to evaluation pipeline)
- **Medium Confidence**: Clinical Utility (PCC correlation shows potential but depends on accurate phone recognition)
- **Low Confidence**: Style Disentanglement Mechanism (core claim lacks direct experimental validation)

## Next Checks

1. **Ablation Study on Style Components**: Train ablations with (a) acoustic style fully suppressed, (b) prosodic style fully suppressed, and (c) both styles used equally. Measure WER, speaker similarity, and F0 differences to quantify the contribution of each style component.

2. **Phonemizer Robustness Test**: Evaluate the same TORGO reconstructions using both en-gb and en-us phonemizers. Compare PCC correlations and CER improvements to quantify sensitivity to accent-phonemizer alignment.

3. **Per-Category Error Analysis**: Extract confusion matrices for wav2vec2 phone recognition on original vs. reconstructed STAR speech. Identify systematic substitution/deletion patterns to determine whether the model corrects specific phonological processes (e.g., fronting, stopping) more effectively than others.