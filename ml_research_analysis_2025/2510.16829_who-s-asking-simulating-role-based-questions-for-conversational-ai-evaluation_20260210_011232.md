---
ver: rpa2
title: Who's Asking? Simulating Role-Based Questions for Conversational AI Evaluation
arxiv_id: '2510.16829'
source_url: https://arxiv.org/abs/2510.16829
tags:
- support
- your
- help
- role
- cravings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CORUS, a framework for simulating role-based
  questions by embedding social context implicitly in queries. Drawing on role theory
  and posts from an online OUD recovery community, the authors construct a taxonomy
  of three information-seeking roles (patients, caregivers, practitioners) and use
  it to simulate 15,321 role-specific questions.
---

# Who's Asking? Simulating Role-Based Questions for Conversational AI Evaluation

## Quick Facts
- **arXiv ID**: 2510.16829
- **Source URL**: https://arxiv.org/abs/2510.16829
- **Reference count**: 40
- **Primary result**: Role-based simulation framework reveals LLMs systematically adjust knowledge, support, and readability based on implicit user roles, with vulnerable roles receiving more supportive but less informative responses.

## Executive Summary
This paper introduces CORUS, a framework for simulating role-based questions by embedding social context implicitly in queries. Drawing on role theory and posts from an online OUD recovery community, the authors construct a taxonomy of three information-seeking roles (patients, caregivers, practitioners) and use it to simulate 15,321 role-specific questions. Evaluation shows these questions are highly believable (90% human-like, 99% role-faithful) and comparable to real-world data. When used to evaluate five LLMs, role-based contexts systematically shape responses: patient/caregiver framing increases support (+15–19%) but reduces knowledge (−15–23%) and increases readability, while practitioner framing leaves knowledge unchanged, decreases support (−9%), and reduces readability. This reveals that current role-agnostic evaluations miss critical variations in how LLMs respond to the same question across user roles, especially in sensitive domains like OUD recovery.

## Method Summary
The CORUS framework builds a role taxonomy from OUD recovery community posts using LLM-assisted qualitative analysis, then simulates role-embedded questions without explicit demographic labels. The method extracts role-defining facets (goals, behaviors, experiences) via Claude 3 Haiku summarization and clustering, validates them with expert annotators, then uses Claude 3.7 Sonnet to generate role-specific variants of role-agnostic seed questions. Responses from five LLMs are evaluated using classifiers for knowledge, support, and readability metrics. The approach avoids explicit role declarations to prevent stereotyping while maintaining ecological validity through implicit behavioral framing.

## Key Results
- CORUS simulates 15,321 role-specific questions that achieve 90% human-like believability and 99% role faithfulness without explicit role mentions
- When evaluating five LLMs, patient/caregiver framing increases support (+15–19%) but reduces knowledge content (−15–23%) compared to practitioner framing
- Role-based evaluation reveals systematic LLM response calibration: vulnerable roles trigger supportive-but-less-informative responses, while practitioner framing decreases support (−9%) and readability
- 18–72% of baseline questions rely on explicit role mentions, while CORUS achieves role faithfulness through implicit behavioral embedding

## Why This Works (Mechanism)

### Mechanism 1: Role Taxonomy Derivation via Facet-Based Summarization
The pipeline applies role theory's goal-behavior-experience framework as a deductive lens: Claude 3 Haiku generates facet-based summaries of each Reddit post, which are embedded and clustered via k-means, then validated by expert annotators. This hybrid top-down/bottom-up approach surfaces latent role categories that pure keyword analysis would miss. If community discourse lacks consistent role differentiation, or if facet-based summarization introduces systematic distortion, the taxonomy will reflect model artifacts rather than genuine user roles.

### Mechanism 2: Implicit Role Embedding Through Behavioral Framing
Rather than prompting with "act as a patient," the system conditions on behavioral/experiential descriptors drawn from the taxonomy plus sampled facet summaries. This grounds simulation in observable discourse patterns rather than demographic assumptions. If implicit role embedding fails to transfer to out-of-distribution topics, or if the model defaults to explicit role declarations despite instructions, ecological validity degrades.

### Mechanism 3: Role-Contingent Response Calibration in LLMs
The correlation analysis (r=0.3 between supportive query cues and supportive responses) suggests partial "mirroring"—models detect vulnerability/emotional cues and adjust their response strategy. This may reflect RLHF training on helpful/harmless preferences that prioritize emotional safety for distressed users. If calibration patterns are model-specific rather than generalizable, or if the knowledge-support tradeoff reflects dataset artifacts rather than intentional model behavior, evaluation insights won't transfer across deployment contexts.

## Foundational Learning

- **Concept: Role theory (Biddle 1979; Turner 1990)**
  - Why needed here: Provides the theoretical scaffolding for decomposing roles into goal-behavior-experience facets, which operationalizes abstract social constructs into concrete generation features.
  - Quick check question: Can you explain why "patient" is treated as a role defined by goals/behaviors/experiences rather than a demographic category?

- **Concept: Ecological validity in NLP evaluation**
  - Why needed here: The paper's core critique is that role-agnostic evaluations fail to capture real-world use patterns; understanding this distinction is essential for interpreting the CORUS contribution.
  - Quick check question: Why does stripping personal context from evaluation questions reduce ecological validity, even if it improves comparability?

- **Concept: LLM-as-judge validation protocols**
  - Why needed here: The believability evaluation relies on GPT-4.1 judgments validated against 7,365 human annotations; understanding agreement metrics (Gwet's AC1, accuracy) is necessary to assess reliability claims.
  - Quick check question: What is the validation protocol for the GPT-4.1 judge, and what agreement levels were achieved?

## Architecture Onboarding

- **Component map**: Reddit posts → Claude 3 Haiku facet summarization → sentence embedding (all-mpnet-base-v2) → k-means clustering → LLM cluster labeling → human validation → Role-agnostic seed questions (extracted from Reddit, rewritten by GPT-4o-mini) + taxonomy facets + sampled behavioral/experiential summaries → Claude 3.7 Sonnet role-based question generation → 5 LLMs (GPT-4o, Gemini-2.5-Flash, Llama-3.1-8B/70B, OpenBioLLM-70B) → response classification (knowledge/support/readability) via Choi et al. classifiers → statistical comparison

- **Critical path**: Taxonomy quality determines simulation fidelity—errors propagate downstream; Role-agnostic seed question quality affects content preservation during role embedding; Classifier validation (Cohen's κ 0.7, accuracy 0.9) anchors response evaluation interpretability

- **Design tradeoffs**: Single-community grounding (r/OpiatesRecovery) vs. multi-source improves domain depth but limits taxonomy generalizability; Implicit role embedding vs. explicit role declaration avoids stereotyping but increases prompt engineering complexity; Binary evaluation metrics vs. continuous scales improves annotation reliability but reduces granularity

- **Failure signatures**: Low inter-annotator agreement on taxonomy validation (κ < 0.6) indicates facet definitions are ambiguous; High explicit role mention rate (>20%) in simulated questions indicates implicit embedding failure; Non-significant response differences across roles (p > 0.05) suggests either taxonomy invalidity or model insensitivity to implicit cues

- **First 3 experiments**: 1) Domain transfer test: Apply CORUS methodology to a different stigmatized domain (e.g., mental health, maternal health) using the same taxonomy construction process—assesses generalizability of the framework; 2) Ablation of facet components: Generate questions using only goals, only behaviors, only experiences, and all three—measures which facets drive believability and role faithfulness; 3) Cross-model calibration analysis: Test whether knowledge-support tradeoffs are consistent across model families (GPT vs. Llama vs. Gemini)—determines if findings reflect general LLM behavior or training-specific artifacts

## Open Questions the Paper Calls Out
- Does the CORUS framework generalize to other stigmatized or high-stakes domains, such as mental health or maternal health, where role definitions may differ from OUD recovery?
- Does the observed reduction in knowledge content for vulnerable roles correspond to higher factual accuracy, or does it obscure potential hallucinations?
- How do role-based effects manifest in multi-turn conversations where roles can shift (e.g., a patient becoming a caregiver) or accumulate context over time?

## Limitations
- Taxonomy construction relies heavily on LLM-generated facet summaries without independent human annotation of original posts, creating potential for model-induced bias
- Single-community focus on r/OpiatesRecovery severely limits generalizability to other domains and contexts
- Classifier-based response evaluation depends on unspecified training data and performance characteristics of Choi et al. (2020) models on OUD-related content

## Confidence

- **High Confidence**: The 90% human-like and 99% role-faithful simulation quality metrics, validated against 7,365 human annotations using established agreement metrics (Gwet's AC1), provide strong evidence for CORUS's technical feasibility.
- **Medium Confidence**: The systematic response differences across roles (patient/caregiver vs practitioner) are well-documented, but the mechanism—whether models genuinely "mirror" vulnerability cues or reflect dataset artifacts—remains uncertain without ablation studies isolating individual behavioral signals.
- **Low Confidence**: Claims about generalizability to other domains or different model families lack empirical support; the methodology may not transfer beyond OUD recovery contexts or to smaller, less capable models.

## Next Checks
1. **Cross-Domain Transfer Test**: Apply the CORUS methodology to mental health or maternal health communities using identical taxonomy construction—assess whether the knowledge-support tradeoff pattern replicates across different stigmatized domains.

2. **Behavioral Signal Ablation**: Generate questions varying individual behavioral/experiential facets (goals only, behaviors only, experiences only, all three) and measure which components drive believability and role faithfulness—quantify the contribution of each facet type.

3. **Model Family Consistency Analysis**: Test the knowledge-support calibration patterns across distinct model architectures (GPT vs. Llama vs. Gemini families) and training paradigms (RLHF vs. pure supervised learning) to determine if findings reflect general LLM behavior or training-specific artifacts.