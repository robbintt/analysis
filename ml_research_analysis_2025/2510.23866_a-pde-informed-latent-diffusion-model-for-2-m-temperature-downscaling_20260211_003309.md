---
ver: rpa2
title: A PDE-Informed Latent Diffusion Model for 2-m Temperature Downscaling
arxiv_id: '2510.23866'
source_url: https://arxiv.org/abs/2510.23866
tags:
- diffusion
- data
- temperature
- loss
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a physics-informed latent diffusion model for
  downscaling 2-m temperature fields. The approach builds on a pre-existing latent
  diffusion architecture and incorporates a partial differential equation (PDE) loss
  term into the training objective to enforce physical consistency.
---

# A PDE-Informed Latent Diffusion Model for 2-m Temperature Downscaling

## Quick Facts
- arXiv ID: 2510.23866
- Source URL: https://arxiv.org/abs/2510.23866
- Reference count: 38
- Primary result: Physics-informed latent diffusion model improves 2-m temperature downscaling with better physical consistency than baselines

## Executive Summary
This paper presents a physics-informed latent diffusion model for downscaling 2-m temperature fields from coarse to fine resolution. The approach builds on existing latent diffusion architecture by incorporating a partial differential equation (PDE) loss term that enforces physical consistency during training. The PDE loss approximates an advection-diffusion balance using finite differences in pixel space after decoding the latent representation. Experimental results on ERA5-to-COSMO-CLM temperature downscaling show that the physics-constrained model achieves competitive statistical metrics while significantly outperforming baseline models on physics-aware metrics, including flux-ratio loss and spectral-slope difference loss. Qualitative results demonstrate improved visual fidelity and smoother, more coherent temperature gradients compared to UNet, GAN, and standard latent diffusion models.

## Method Summary
The method extends a pre-trained latent diffusion model by adding a physics-informed loss term to the training objective. The architecture uses a VQ-VAE encoder-decoder to compress the input temperature fields into a latent space, where the diffusion U-Net operates. The key innovation is the PDE loss, which is computed by decoding the latent representation back to pixel space and then approximating the spatial derivatives using finite differences. This loss enforces an advection-diffusion balance in the reconstructed temperature field, encouraging physically realistic gradients and flux patterns. The model is trained end-to-end on ERA5 data downscaled to COSMO-CLM resolution, with hyperparameters tuned to balance statistical fidelity and physical consistency.

## Key Results
- Competitive statistical performance (MSE, MAE, SSIM) against UNet, GAN, and standard latent diffusion baselines
- Significant improvement in physics-aware metrics: lower flux-ratio loss and spectral-slope difference loss
- Qualitative improvements in visual fidelity with smoother, more coherent temperature gradients
- Demonstrated value of explicit physical constraints in generative downscaling models

## Why This Works (Mechanism)
The physics-informed loss term acts as a regularizer that guides the diffusion model toward solutions that satisfy fundamental physical laws governing temperature transport. By approximating the advection-diffusion equation in the decoded space, the model learns to preserve realistic spatial patterns and gradients that would be physically plausible. This constraint prevents the model from generating artifacts or unrealistic temperature distributions that might minimize the statistical loss but violate physical principles. The approach leverages the strengths of diffusion models in generating high-quality samples while ensuring those samples adhere to known physics.

## Foundational Learning
- **Advection-diffusion equation**: Governs transport of scalar quantities like temperature in fluid dynamics; needed to formulate the physics constraint
- **Finite difference methods**: Numerical technique for approximating spatial derivatives; used to compute the PDE loss from pixel data
- **Latent diffusion models**: Generative models that operate in compressed latent space; provide the base architecture
- **VQ-VAE**: Vector quantized variational autoencoder; used for dimensionality reduction before diffusion
- **Spectral slope analysis**: Measures power spectral density; used to evaluate spatial coherence of generated fields
- **Flux-ratio metrics**: Quantifies conservation properties; evaluates physical consistency of temperature transport

## Architecture Onboarding

**Component Map**: ERA5 data -> VQ-VAE Encoder -> Latent Space -> Diffusion U-Net -> Latent Space -> VQ-VAE Decoder -> Pixel Space -> PDE Loss + Reconstruction Loss

**Critical Path**: The diffusion U-Net operates in latent space where most computation occurs. The PDE loss is computed in pixel space after decoding, creating a back-and-forth between spaces during training.

**Design Tradeoffs**: The single-variable approach (temperature only) maintains comparability with baselines but forces approximation of advection via temperature gradients rather than explicit wind vectors. This simplifies the model but may misrepresent dynamics where winds are strong but thermal contrasts are weak.

**Failure Signatures**: Over-regularization by PDE loss could lead to overly smooth fields that lose fine-scale features. Under-regularization would result in physically inconsistent artifacts. Poor finite-difference approximations could introduce numerical errors.

**3 First Experiments**:
1. Ablation study varying λ_PDE to find optimal balance between statistical and physical consistency
2. Test transfer learning by fine-tuning on a different region with minimal retraining
3. Generate ensembles and evaluate calibration metrics (rank histograms, CRPS) to assess probabilistic reliability

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the PDE formulation be extended to utilize explicit wind vector fields rather than approximating advection via temperature gradients?
- **Basis in paper:** [explicit] Section 6.1 states that the single-variable design "forces us to approximate advective fluxes with latent temperature gradients rather than explicit wind vectors, a simplification that can misrepresent dynamics where winds are strong but thermal contrasts are weak."
- **Why unresolved:** The authors intentionally excluded wind data to maintain comparability with baseline models that only use temperature snapshots.
- **What evidence would resolve it:** A comparative study where the PDE loss is calculated using explicit wind inputs, compared against the current gradient-approximation method in scenarios with high wind and low thermal contrast.

### Open Question 2
- **Question:** Does the tuned hyperparameter (λ_PDE) and the resulting model performance generalize to regions with different orographies or land-sea contrasts?
- **Basis in paper:** [explicit] Section 6.1 notes that training was restricted to a single regional dataset (Italy) and that "Broader intercomparisons across climates and resolutions are necessary to confirm robustness."
- **Why unresolved:** The current results rely on hyperparameters tuned specifically for the ERA5-to-COSMO-CLM (Italy) configuration.
- **What evidence would resolve it:** Evaluating the fixed model weights on geographically distinct datasets (e.g., mountainous vs. coastal regions) or different grid spacings without re-tuning the loss weighting.

### Open Question 3
- **Question:** Does the PDE constraint impact the probabilistic reliability and ensemble spread of the diffusion model?
- **Basis in paper:** [explicit] Section 6.1 highlights that while diffusion models support ensembles, the authors "have not yet assessed their ensemble spread, calibration, or reliability."
- **Why unresolved:** The study focused on deterministic metrics (RMSE, R²) and single-sample visualizations.
- **What evidence would resolve it:** Generating large ensembles of downscaled fields and computing calibration metrics (e.g., rank histograms, CRPS) to see if physical constraints improve statistical reliability.

## Limitations
- Finite-difference PDE loss approximations may introduce discretization errors that propagate into latent space representations
- Hyperparameter tuning (λ_PDE) is empirical and may not generalize across different regions or resolutions
- Evaluation relies heavily on synthetic physics-aware metrics that lack validation against real-world utility
- Single-variable approach limits representation of complex atmospheric dynamics

## Confidence

**High confidence**: Statistical downscaling performance metrics (MSE, MAE, SSIM) showing competitive results against baseline UNet and GAN models.

**Medium confidence**: Superiority of physics-informed constraints for improving physical consistency metrics, given that these metrics are novel and their correlation with real-world utility is not established.

**Low confidence**: Generalization of the diffusion-based approach to other meteorological variables or different downscaling ratios, as the study focuses exclusively on 2-m temperature from ERA5 to COSMO-CLM.

## Next Checks
1. Conduct ablation studies systematically varying the PDE loss weight λ to identify optimal trade-offs between statistical and physical consistency.
2. Validate the learned latent representations by testing transfer learning capabilities on related downscaling tasks or meteorological variables.
3. Perform out-of-sample testing on extreme weather events to assess model robustness under conditions potentially underrepresented in the training data.