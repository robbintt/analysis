---
ver: rpa2
title: 'What You See is What You Ask: Evaluating Audio Descriptions'
arxiv_id: '2510.00808'
source_url: https://arxiv.org/abs/2510.00808
tags:
- answer
- question
- questions
- context
- description
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces ADQA, a new benchmark for evaluating audio\
  \ descriptions (ADs) in movies. It addresses the subjectivity in AD writing by aligning\
  \ two independent AD tracks and finding that about 25\u201330% of ADs don\u2019\
  t align temporally, with even aligned pairs often differing in wording or visual\
  \ details."
---

# What You See is What You Ask: Evaluating Audio Descriptions

## Quick Facts
- arXiv ID: 2510.00808
- Source URL: https://arxiv.org/abs/2510.00808
- Authors: Divy Kala; Eshika Khandelwal; Makarand Tapaswi
- Reference count: 39
- Primary result: ADQA benchmark shows current AD generation methods lag far behind human-authored ADs in both visual appreciation and narrative understanding tasks

## Executive Summary
This work introduces ADQA, a new benchmark for evaluating audio descriptions (ADs) in movies. It addresses the subjectivity in AD writing by aligning two independent AD tracks and finding that about 25-30% of ADs don't align temporally, with even aligned pairs often differing in wording or visual details. ADQA shifts evaluation from short clips to coherent video segments and uses a multiple-choice QA framework to test if generated ADs help blind and low-vision users appreciate visual details and understand the narrative. Visual appreciation questions are based on ADs, while narrative understanding questions use plot summaries. Results show current AD generation methods lag far behind human-authored ADs, with human performance setting the top-line benchmark. The study also recommends moving from clips to videos, focusing on narratives, leveraging VLMs for rich details, and incorporating more hands-on evaluation. ADQA is hosted as a public leaderboard to fairly compare methods.

## Method Summary
ADQA evaluates audio descriptions through a multiple-choice QA framework using Gemini-2.5-Pro to generate questions from ground-truth ADs (for visual appreciation) and plot summaries (for narrative understanding), then using Gemini-2.0-Flash to answer these questions with candidate ADs as context. The benchmark uses two datasets: CMD-AD with 98 movies and MAD-eval with 10 movies, both providing video segments, dialog, ADs, and plots. Questions require identifying visual facts or narrative connections, with correctness determined by both answer accuracy and whether the reasoning was grounded in the provided context (verified via rationales). The primary metric is CC (correct answer AND context-based rationale), normalized against human and dialog-only baselines to produce accuracy ratios.

## Key Results
- About 25-30% of ADs don't align temporally between independent AD tracks, with even aligned pairs often differing in wording or visual details
- Current AD generation methods lag far behind human-authored ADs, with human performance setting the top-line benchmark
- Dense descriptions from VLMs achieve high visual appreciation but poor narrative understanding scores, suggesting information overload hinders comprehension

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Question-answering accuracy using ADs as context serves as a proxy measure for whether those ADs would help BLV users appreciate visuals and understand narratives.
- **Mechanism:** Generated ADs are mixed with dialog and provided as context to an LLM, which must answer questions grounded in that context. Correct answers require the AD to contain relevant visual details (for VA questions) or narrative information (for NU questions). The LLM also outputs a rationale and binary flag indicating whether the answer derived from context vs. prior knowledge.
- **Core assumption:** If an LLM can extract information from ADs to answer questions, BLV users would similarly benefit from those ADs. This assumes LLM comprehension patterns approximate human AD consumer needs.
- **Evidence anchors:** [abstract] "ADQA features visual appreciation (VA) questions about visual facts and narrative understanding (NU) questions based on the plot." [section 5.1] "A prediction is considered correct only if the chosen answer option is correct and the binary label confirms context-based reasoning."

### Mechanism 2
- **Claim:** Evaluating ADs at video segment level (few minutes) rather than trimmed clip level (few seconds) reveals narrative gaps that clip-level metrics miss.
- **Mechanism:** Trimmed clips only capture isolated AD-caption pairs, forcing one-to-one comparison with a single ground truth. Video segments contain multiple ADs with dialog, requiring models to produce coherent, non-redundant descriptions that advance the storyline. NU questions specifically test whether ADs help connect events into a story.
- **Core assumption:** Narrative understanding requires temporal coherence across multiple ADs and dialog, which cannot be assessed by isolated clip evaluation.
- **Evidence anchors:** [abstract] "Existing works in automatic AD generation mostly focus on few-second trimmed clips... we show that working with trimmed clips is inadequate." [section 4.1] "This shift not only provides more context for generating ADs, but also encourages producing coherent, non-redundant descriptions rather than isolated captions."

### Mechanism 3
- **Claim:** Subjectivity in AD writing (what/when/how to describe) undermines single-reference evaluation; dual-track alignment quantifies this subjectivity.
- **Mechanism:** Two independent AD tracks for the same movies are temporally aligned using dialog anchors and dynamic time warping. Mapping ADs between tracks via overlap threshold reveals: (1) 25-30% of ADs lack counterparts in the other track (non-aligned), and (2) even aligned AD pairs often describe different details or use different words, yielding low CIDEr scores despite high temporal overlap.
- **Core assumption:** If two expert human describers frequently disagree on what/when/how to describe, then evaluating against a single reference AD is inherently flawed.
- **Evidence anchors:** [section 3] "about 25-30% of ADs still remain non-aligned... fundamentally challenging the validity of sentence-pair evaluation." [table 1] Aligned ADs (60.7%) with high overlap (85.6%) still yield CIDEr of only 37.3.

## Foundational Learning

- **Concept: Audio Description (AD) guidelines and goals**
  - **Why needed here:** The benchmark's VA and NU questions operationalize two central AD goals from Snyder (2014): visual appreciation and narrative understanding. Understanding guidelines (describe visual facts only, no interpretation, fit within dialog gaps) clarifies what ADQA is testing.
  - **Quick check question:** An AD states "a beautiful woman enters." According to AD guidelines, what is problematic about this description?

- **Concept: Context-grounded vs. prior-knowledge reasoning in LLMs**
  - **Why needed here:** ADQA relies on distinguishing whether an LLM answered questions using provided AD context vs. its internal knowledge. The rationale + binary label mechanism addresses this.
  - **Quick check question:** If an LLM answers "Who is Luke's father?" correctly without any provided context, does this indicate the AD was helpful? How does ADQA address this?

- **Concept: Temporal alignment via dynamic time warping**
  - **Why needed here:** The dual-track analysis uses DTW anchored on dialog to map timestamps between two AD sources, enabling overlap computation and subjectivity analysis.
  - **Quick check question:** Why anchor alignment on dialog rather than AD content directly?

## Architecture Onboarding

- **Component map:** Question generation (Gemini-2.5-Pro) -> AD/dialog classification (LLM) -> Dual-track alignment (DTW + LLM) -> Answering pipeline (Gemini-2.0-Flash) -> Accuracy ratio calculation

- **Critical path:** 1. Obtain video segments from CMD-AD or MAD 2. Generate VA questions from one AD track, NU questions from plot 3. Feed candidate ADs + dialog as context to answering LLM 4. Compute CC (correct + context-grounded), compare to baseline and topline 5. Submit to leaderboard for private-set evaluation

- **Design tradeoffs:** LLM-generated vs. human-authored questions (LLM scales better but may leak patterns; human study validated 93-98% validity); public vs. private benchmark split (5 CMD + 1 MAD public for understanding; remainder private for leaderboard); rationale requirement (forces context-grounding but reduces absolute accuracy); not evaluating temporal feasibility (generated ADs may be too long for real dialog gaps).

- **Failure signatures:** Prior knowledge leakage (high CA with "No context" indicates LLM knows popular movies); ADs not advancing narrative (high VA scores but low NU scores); repetitive/irrelevant ADs (models like DistinctAD show negative accuracy ratios on MAD-eval NU).

- **First 3 experiments:** 1. Run context ablation on public set: test answering accuracy with no context / movie name / dialog-only / AD-only / dialog+AD to validate that ADs contribute meaningfully. 2. Evaluate baseline AD generation methods: compare fine-tuned (AutoAD-III) vs. zero-shot (Shot-by-Shot with GPT-4o) on both VA and NU questions. 3. Analyze failure cases in rationale grounding: examine instances where AC=False despite correct answers; check if this correlates with popular movies where LLM has prior knowledge.

## Open Questions the Paper Calls Out

- **Question:** How can automatic AD generation methods ensure temporal feasibility, i.e., that generated descriptions fit naturally within dialog-free intervals without overwhelming viewers?
  - **Basis in paper:** [explicit] Section 8 (Limitations) states: "We do not currently assess whether model-generated ADs are too long to fit naturally between dialog segments."
  - **Why unresolved:** The authors note that naive length-capping may encourage verbose descriptions that saturate silent gaps, detracting from the cinematic experience.
  - **What evidence would resolve it:** A benchmark or metric evaluating whether generated ADs respect timing constraints while maintaining informativeness; user studies with BLV participants on pacing preferences.

- **Question:** How can AD generation models better connect individual events into coherent narratives rather than producing repetitive, isolated descriptions?
  - **Basis in paper:** [explicit] Section 6 (Recommendations): "Current models fail to connect individual events into a story, often producing repetitive ADs."
  - **Why unresolved:** The authors found that even best models (Shot-by-Shot) achieve only ~51.7% CC on MAD-eval Narrative Understanding, far below human performance (65.2%).
  - **What evidence would resolve it:** Models that achieve near-human accuracy ratios on ADQA narrative understanding questions; qualitative analysis showing reduced repetition.

- **Question:** What is the optimal balance between visual richness and narrative coherence when distilling VLM-generated dense descriptions into concise ADs?
  - **Basis in paper:** [inferred] Results show Qwen2VL dense descriptions achieve high Visual Appreciation (17.1% CC) but poor Narrative Understanding (51.5%), suggesting information overload hinders comprehension.
  - **Why unresolved:** The paper demonstrates tension between detail richness (good for appreciation) and coherence (essential for narrative), but offers no solution.
  - **What evidence would resolve it:** Models that match or exceed human performance on both VA and NU metrics simultaneously; ablation studies on description density vs. comprehension.

## Limitations

- The benchmark doesn't evaluate temporal feasibility of ADs - generated descriptions may be too long to fit between dialog gaps in practice
- Heavy reliance on LLM-generated questions and answers introduces potential biases from model-specific knowledge and reasoning patterns
- The dual-track subjectivity analysis depends on accurate dialog alignment and may not capture all forms of subjectivity (e.g., artistic interpretation vs. factual description)

## Confidence

- **High:** The core finding that AD evaluation should move from clips to video segments is well-supported by both empirical evidence and logical reasoning about narrative coherence
- **Medium:** The dual-track analysis showing 25-30% non-alignment is convincing, but the subjectivity quantification depends on the quality of dialog-based temporal alignment
- **Medium:** The QA framework's validity as a proxy for BLV user experience is reasonable but assumes LLM comprehension patterns approximate human needs

## Next Checks

1. **Context ablation study replication**: Run the answering pipeline with varying context conditions (no context, movie name only, dialog only, AD only, both) on the public test set to verify that ADs meaningfully improve accuracy beyond dialog baseline and that prior knowledge leakage is controlled.

2. **Temporal feasibility assessment**: For top-performing AD generation methods on VA and NU questions, measure average AD length per segment and compare against available dialog gaps to quantify the practical delivery constraint gap.

3. **Rationale grounding analysis**: Systematically examine instances where correct answers are provided but rationales indicate no context usage (AC=False), particularly for popular movies, to quantify the extent of prior knowledge leakage and its impact on CC scores.