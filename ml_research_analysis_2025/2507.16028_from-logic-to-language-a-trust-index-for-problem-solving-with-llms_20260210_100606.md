---
ver: rpa2
title: 'From Logic to Language: A Trust Index for Problem Solving with LLMs'
arxiv_id: '2507.16028'
source_url: https://arxiv.org/abs/2507.16028
tags:
- language
- solution
- problem
- quality
- natural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a formal framework for understanding problem-solving
  with Large Language Models (LLMs) that moves beyond classical binary correctness
  measures. It defines distinct problem spaces: formally solvable problems (PFormal),
  natural language addressable problems (PNL), and LLM-addressable problems (PLLM).'
---

# From Logic to Language: A Trust Index for Problem Solving with LLMs

## Quick Facts
- arXiv ID: 2507.16028
- Source URL: https://arxiv.org/abs/2507.16028
- Authors: Tehseen Rug; Felix Böhmer; Tessa Pfattheicher
- Reference count: 17
- Primary result: Introduces formal trust index Q for LLM problem-solving quality with bi-semantic entropy and emotional valence metrics

## Executive Summary
This paper proposes a formal framework for evaluating problem-solving with Large Language Models (LLMs) that moves beyond binary correctness measures. The authors define distinct problem spaces (formally solvable, natural language addressable, and LLM-addressable) and introduce a trust index Q as a vector-valued measure capturing the continuous adequacy spectrum of natural language solutions. Two quality dimensions are proposed: normalized bi-semantic entropy for semantic robustness and emotional valence for subjective solution valuation.

The framework addresses the fundamental challenge that traditional correctness metrics fail to capture the nuanced nature of LLM-generated solutions, which often exist on a spectrum of adequacy rather than being simply right or wrong. Through a toy model application, the authors demonstrate how the framework can be implemented, showing that both emotional valence and variance can be computed using LLM-based personas evaluating generated solutions. The work provides a rigorous foundation for evaluating LLM applications in domains where ambiguity and subjectivity are inherent.

## Method Summary
The authors develop a formal framework distinguishing between formally solvable problems (PFormal), natural language addressable problems (PNL), and LLM-addressable problems (PLLM). They propose a trust index Q as a vector-valued measure of solution quality, moving beyond classical binary correctness metrics. The framework introduces two concrete quality dimensions: normalized bi-semantic entropy, which measures semantic robustness across different problem formulations, and emotional valence, which quantifies subjective valuation of solutions. A toy model application demonstrates practical implementation, using LLM-based personas to evaluate emotional responses to generated solutions. The approach provides a rigorous mathematical foundation for evaluating solutions in domains where ambiguity and subjectivity are inherent characteristics.

## Key Results
- Framework distinguishes three problem spaces: PFormal (binary correct/incorrect), PNL (continuous adequacy spectrum), and PLLM (LLM-addressable problems)
- Trust index Q captures solution quality through vector-valued metrics rather than binary measures
- Normalized bi-semantic entropy measures semantic robustness across problem formulations
- Emotional valence quantifies subjective valuation of solutions through LLM-based persona evaluation
- Toy model demonstrates practical implementation with 20 generated problems showing feasibility of the approach

## Why This Works (Mechanism)
The framework works by recognizing that LLM problem-solving operates in a fundamentally different space than traditional computational problem-solving. By defining problem spaces (PFormal, PNL, PLLM) that reflect the inherent ambiguity and subjectivity of natural language tasks, the trust index Q can capture solution quality on a continuous spectrum. The bi-semantic entropy metric measures how consistently a solution performs across semantically equivalent problem formulations, providing robustness assessment. Emotional valence captures the subjective human response to solutions, acknowledging that in many LLM applications, user satisfaction is as important as technical correctness. The iterative algorithm for determining "good enough" thresholds allows for practical deployment by establishing concrete quality benchmarks.

## Foundational Learning

**Problem Space Formalization**
- Why needed: LLMs operate across domains with different correctness criteria
- Quick check: Can map specific problem to correct problem space category

**Bi-Semantic Entropy**
- Why needed: Measures solution consistency across semantically equivalent formulations
- Quick check: Higher entropy indicates less robust solutions across problem variants

**Emotional Valence Metrics**
- Why needed: Captures subjective user satisfaction beyond technical correctness
- Quick check: LLM-based personas can reliably assess solution appeal

**Trust Index Q**
- Why needed: Vector-valued measure captures multi-dimensional solution quality
- Quick check: Q correlates with human expert evaluations of solution adequacy

**Iterative Threshold Algorithm**
- Why needed: Provides practical mechanism for determining "good enough" solutions
- Quick check: Algorithm converges to stable thresholds within finite iterations

## Architecture Onboarding

**Component Map**
User Problem -> PNL/PLLM Classification -> Solution Generation -> Q Score Computation -> Threshold Comparison -> Output

**Critical Path**
Problem formulation → Solution generation → Bi-semantic entropy calculation → Emotional valence assessment → Trust index Q computation → Quality threshold evaluation

**Design Tradeoffs**
- Precision vs. scalability in Q computation
- Human vs. LLM-based evaluation for emotional valence
- Computational cost vs. accuracy in bi-semantic entropy measurement

**Failure Signatures**
- High bi-semantic entropy variance suggests semantic inconsistency
- Low emotional valence indicates poor user appeal despite technical adequacy
- Algorithm divergence in threshold setting suggests problematic problem space

**First Experiments**
1. Validate Q correlation with human expert evaluations on benchmark problem sets
2. Test bi-semantic entropy stability across linguistically diverse problem formulations
3. Evaluate emotional valence consistency across different LLM-based persona implementations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the trust index Q be operationalized at scale using automated methods?
- Basis in paper: [explicit] The conclusion states future work should focus on "developing scalable methodologies for measuring and operationalizing the trust index Q, potentially through advanced LLM-as-a-Judge architectures."
- Why unresolved: The current framework relies on theoretical definitions or resource-intensive human/LLM ensembles, which may not be feasible for high-volume real-time applications.
- What evidence would resolve it: A demonstration of an automated "LLM-as-a-Judge" system that efficiently computes Q and maintains high correlation with human evaluator ensembles across diverse tasks.

### Open Question 2
- Question: Does the iterative algorithm for determining "good enough" thresholds converge reliably in real-world applications?
- Basis in paper: [explicit] Future work includes "implementing the proposed algorithm for identifying 'good enough' solutions in real-world application domains."
- Why unresolved: Algorithm 1 is presented as conceptual pseudo-code; its convergence rate, computational cost, and stability when applied to complex, ambiguous domains remain untested.
- What evidence would resolve it: Empirical data from a live deployment showing that the algorithm stabilizes at specific thresholds $(\hat{q}_k, \hat{v}_k)$ within a finite number of human-in-the-loop iterations.

### Open Question 3
- Question: Do Normalized Bi-Semantic Entropy and Emotional Valence serve as valid proxies for solution quality in practice?
- Basis in paper: [explicit] The authors call for "empirically evaluating the proposed quality metrics in practical deployments."
- Why unresolved: While theoretically grounded, these specific metrics ($NSE_{Bi}$ and $V_E$) are novel constructs in the paper and lack extensive validation against real-world user satisfaction or problem-solving success.
- What evidence would resolve it: Large-scale A/B testing showing that optimizing for these metrics leads to statistically significant improvements in user-defined success metrics (e.g., acceptability ratings, task completion).

## Limitations
- Theoretical framework lacks empirical validation across diverse real-world problem sets
- Novel quality metrics (bi-semantic entropy, emotional valence) unproven as reliable quality proxies
- Framework may not adequately address safety-critical domains where objective correctness is paramount
- Reliance on LLM-based personas for emotional valence introduces potential circularity and subjectivity

## Confidence

**Formal framework construction**: High
- Mathematical foundations are rigorous and well-defined

**Problem space delineation**: Medium
- Novel categorization but real-world boundaries may be fuzzy

**Quality dimension selection**: Low
- Bi-semantic entropy and emotional valence are theoretically interesting but untested

**Practical applicability**: Low
- Toy model demonstrates concept but scalability to real applications unproven

**Empirical validation**: None (unvalidated)
- Framework remains entirely theoretical without quantitative testing

## Next Checks

1. Empirical validation study: Apply the framework to a diverse corpus of 100+ problems across multiple domains (mathematics, commonsense reasoning, creative writing) and measure correlation between Q scores and human expert evaluations of solution quality.

2. Cross-linguistic robustness test: Implement the framework in at least three non-English languages and assess whether bi-semantic entropy and emotional valence capture meaningful quality differences across linguistic and cultural contexts.

3. Model architecture independence verification: Test whether Q scores remain stable and meaningful across different LLM architectures (transformer variants, retrieval-augmented models) and parameter scales (7B, 70B, 175B parameters).