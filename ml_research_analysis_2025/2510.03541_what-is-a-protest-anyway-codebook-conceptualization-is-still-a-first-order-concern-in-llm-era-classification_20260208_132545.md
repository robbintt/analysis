---
ver: rpa2
title: What is a protest anyway? Codebook conceptualization is still a first-order
  concern in LLM-era classification
arxiv_id: '2510.03541'
source_url: https://arxiv.org/abs/2510.03541
tags:
- codebook
- protest
- llms
- conceptualization
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the critical role of conceptualization in large
  language model (LLM)-based text classification for computational social science
  (CSS). The authors decompose annotation errors into conceptualization errors (incomplete
  codebook definitions) and operationalization errors (incorrect application of definitions).
---

# What is a protest anyway? Codebook conceptualization is still a first-order concern in LLM-era classification

## Quick Facts
- arXiv ID: 2510.03541
- Source URL: https://arxiv.org/abs/2510.03541
- Authors: Andrew Halterman; Katherine A. Keith
- Reference count: 25
- Primary result: LLM-era classification requires the same careful conceptualization as pre-LLM methods; conceptualization-induced bias cannot be corrected by post-hoc methods.

## Executive Summary
This paper argues that conceptual errors in LLM-based classification—specifically incomplete codebook definitions—create systematic bias that cannot be corrected by post-hoc bias correction methods like DSL or PPI. The authors decompose annotation errors into conceptualization errors (incomplete definitions) and operationalization errors (misapplication of definitions). Through simulations, they demonstrate that when codebooks omit relevant aspects (e.g., violence in protests), both human experts and LLMs produce systematically biased labels, rendering post-hoc correction ineffective. The paper advocates for a "pragmatist" approach that combines LLM predictions with gold-standard human annotations while maintaining rigorous codebook development.

## Method Summary
The paper uses simulation experiments based on a Data Generating Process (DGP) modeling protest events with attributes (violence, size, publicity) and regression outcomes. The DGP samples binary attributes and derives true labels using an AND logic function. Two codebook types are tested: "Complete" (all aspects) and "Incomplete" (excludes violence). LLM labels are generated by adding random noise to gold-standard labels. DSL and PPI methods are then applied to estimate effects, measuring bias as deviation from the true effect size. The simulation varies LLM error rates to demonstrate how conceptualization-induced bias persists regardless of accuracy improvements.

## Key Results
- Conceptualization-induced bias persists through post-hoc correction methods regardless of LLM accuracy
- DSL and PPI methods fail to correct bias when codebook definitions are incomplete
- The silent failure mechanism allows LLMs to generate plausible labels without flagging ambiguity in incomplete definitions
- A pragmatist approach combining LLM predictions with gold-standard human annotations is recommended

## Why This Works (Mechanism)

### Mechanism 1: Error Decomposition into Conceptualization and Operationalization
Annotation errors decompose into conceptualization errors (incomplete codebook definitions) and operationalization errors (misapplying definitions). Conceptualization errors bias gold-standard labels themselves, while operationalization errors add noise that post-hoc methods can correct.

### Mechanism 2: LLM Silent Failure Removes Conceptualization Feedback Loop
LLMs generate plausible labels without signaling ambiguity, removing the forcing mechanism that human annotators provide. Human annotators ask clarifying questions, forcing analysts to refine stipulative definitions. LLMs produce outputs regardless of prompt specificity, masking incomplete conceptualization until downstream inference fails.

### Mechanism 3: Conceptualization-Induced Bias Persists Through Post-Hoc Correction
Post-hoc bias correction methods (DSL, PPI) cannot fix bias from incomplete codebooks because expert labels inherit the same conceptualization error. If the codebook omits a relevant aspect (e.g., violence by protesters), both LLM and expert labels are systematically wrong in the same direction.

## Foundational Learning

- **Concept: Stipulative vs. Dictionary Definitions**
  - Why needed here: The paper's taxonomy (Type I/II/III definitions) is central to understanding why LLMs enable conceptualization shortcuts.
  - Quick check question: If you prompt an LLM with "Is this a protest?", which definition type are you using?

- **Concept: Prediction-Powered Inference (PPI) / DSL**
  - Why needed here: The "pragmatist" approach relies on combining LLM predictions with human labels. Understanding the rectifier term explains why conceptualization errors break correction.
  - Quick check question: What does the Δ̂ term in Line 2 estimate, and why would it be near-zero if both label sources share the same conceptualization error?

- **Concept: Construct Validity and Reliability**
  - Why needed here: The paper links codebook completeness to inter-annotator agreement. "Complete" codebooks produce near-perfect expert agreement; incomplete codebooks hide ambiguity.
  - Quick check question: If two experts disagree on labels using the same codebook, does this indicate a conceptualization or operationalization problem?

## Architecture Onboarding

- **Component map:** Codebook (C) → LLM Predictor f(·) → Predictions Ŷ; Codebook (C) → Expert Annotator → Gold labels Y; Ŷ, Y, Outcomes → Post-hoc Corrector (DSL/PPI) → Unbiased estimates

- **Critical path:**
  1. Draft initial codebook with domain experts (address all aspects: violence, size, location, online/offline)
  2. Pilot with 2+ expert annotators; refine codebook until near-perfect agreement
  3. Collect n human labels (e.g., n = N/10) on random sample
  4. Run LLM on all N documents with complete codebook in prompt
  5. Apply PPI/DSL correction

- **Design tradeoffs:**
  - Longer codebooks → better conceptualization but LLM context degradation
  - Fine-tuning → higher accuracy but consumes annotations that could be used for correction
  - Multiple LLMs → reduces variance but increases compute cost

- **Failure signatures:**
  - Silent conceptualization gaps: LLM produces high-confidence labels but downstream estimates diverge from theory
  - Expert disagreement: Indicates incomplete codebook, not annotator error
  - DSL rectifier near zero: Check if codebook addresses all aspects correlated with dependent variable

- **First 3 experiments:**
  1. Codebook completeness audit: List all aspects for your concept; verify codebook explicitly includes/excludes each. Flag omissions.
  2. Inter-annotator agreement test: Have 2 experts label 100 documents independently using only the codebook. If agreement < 0.9, refine codebook before proceeding.
  3. Simulation sanity check: Before collecting data, simulate the DGP with your hypothesized effect structure. Verify that incomplete codebooks produce biased estimates in simulation.

## Open Questions the Paper Calls Out

- Does using LLMs for codebook conceptualization cause analysts to overestimate codebook completeness?
- Which post-hoc bias correction method (e.g., PPI, DSL) performs best with finite samples in computational social science tasks?
- How should analysts optimally allocate a gold-standard annotation budget between training models and calibrating confidence scores?

## Limitations

- The paper assumes perfect expert operationalization, but real-world expert disagreements could compound bias
- The proposed "pragmatist" solution requires substantial human annotation resources, potentially limiting scalability
- The silent failure mechanism is theoretically compelling but lacks empirical validation beyond the protest example

## Confidence

- **High Confidence:** The decomposition of errors into conceptualization vs. operationalization errors, and the demonstration that post-hoc corrections fail for conceptualization errors
- **Medium Confidence:** The pragmatist solution's effectiveness in practice, as it requires empirical validation beyond the single protest example
- **Medium Confidence:** The silent failure mechanism, as it is theoretically argued but not empirically tested with diverse concepts or LLM architectures

## Next Checks

1. **Expert Disagreement Audit:** Conduct a pilot study with 3+ expert annotators on a diverse set of concepts. Measure agreement and explicitly trace disagreements to conceptualization vs. operationalization issues.

2. **Cross-Concept Silent Failure Test:** Apply the protest study's methodology to a conceptually distinct domain (e.g., policy diffusion, misinformation). Test whether LLMs silently fail when prompted with incomplete definitions.

3. **Pragmatist Resource Trade-off:** For a real-world dataset, empirically measure the annotation burden of the pragmatist approach (n = N/10) versus traditional expert-only annotation. Compare results and resource costs.