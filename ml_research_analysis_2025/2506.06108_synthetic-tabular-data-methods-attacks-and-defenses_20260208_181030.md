---
ver: rpa2
title: 'Synthetic Tabular Data: Methods, Attacks and Defenses'
arxiv_id: '2506.06108'
source_url: https://arxiv.org/abs/2506.06108
tags:
- data
- synthetic
- privacy
- generation
- tabular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively covers methods, attacks, and defenses
  for synthetic tabular data generation. The paper addresses the challenge of creating
  synthetic data that preserves privacy while maintaining fidelity to sensitive source
  data.
---

# Synthetic Tabular Data: Methods, Attacks and Defenses

## Quick Facts
- **arXiv ID:** 2506.06108
- **Source URL:** https://arxiv.org/abs/2506.06108
- **Reference count:** 40
- **Primary result:** Marginal-based methods consistently outperform deep learning approaches under differential privacy constraints, especially at strict privacy levels (ε < 1).

## Executive Summary
This comprehensive survey examines the evolving field of synthetic tabular data generation, addressing the fundamental challenge of creating data that preserves privacy while maintaining fidelity to sensitive source data. The authors systematically categorize approaches into marginal-based methods (graphical models, probabilistic modeling) and deep learning methods (GANs, diffusion models, LLMs), analyzing the utility-privacy tradeoff across multiple dimensions. Through extensive literature review and technical analysis, they demonstrate that marginal-based methods often provide superior performance under differential privacy constraints, while also presenting membership inference attacks that quantify privacy leakage and discussing differential privacy-based defenses.

## Method Summary
The paper synthesizes approaches to synthetic tabular data generation into two main categories: marginal-based methods that use a "select-measure-generate" paradigm to capture statistical properties through noisy marginals and graphical models, and deep learning methods including GANs, diffusion models, and LLMs trained with differential privacy mechanisms. The survey systematically analyzes the utility-privacy tradeoff, demonstrating that marginal-based methods consistently outperform deep learning approaches under strict differential privacy constraints. The authors also present membership inference attacks that quantify empirical privacy leakage and discuss defenses based on differential privacy mechanisms, while exploring advanced topics including graph data, federated settings, and public-assisted methods.

## Key Results
- Marginal-based methods consistently outperform deep learning approaches under differential privacy constraints, especially at strict privacy levels (ε < 1)
- Deep learning methods struggle with utility recovery under DP-SGD, showing significant degradation in statistical fidelity
- Membership inference attacks provide empirical privacy leakage measurement that complements formal DP guarantees
- Recent diffusion models and LLM-based approaches lack systematic comparison to marginal-based methods

## Why This Works (Mechanism)

### Mechanism 1: Marginal-based methods outperform deep learning under DP constraints
- Claim: Marginal-based methods consistently outperform deep learning approaches when differential privacy is required, especially at strict privacy levels (ε < 1).
- Mechanism: Marginal-based methods use a "select-measure-generate" paradigm that distributes privacy budget per column/attribute rather than per training iteration. They measure noisy marginals directly and generate via probabilistic graphical models, avoiding the gradient noise accumulation from DP-SGD that destroys neural network utility.
- Core assumption: Key statistical properties of tabular data can be captured through low-dimensional marginal distributions.
- Evidence anchors:
  - [abstract]: "demonstrating that marginal-based methods often outperform deep learning approaches under differential privacy constraints"
  - [section 4.4]: "the majority of GAN-based methods struggle to accurately reproduce basic statistical properties... marginal-based methods consistently show robust performance"
  - [corpus]: Ganev et al. [22] cited within paper; related work "Privacy-Preserving Tabular Synthetic Data Generation" (arxiv 2508.06647) explores similar tensions
- Break condition: When data has extremely high dimensionality with complex multi-way interactions that cannot be captured by low-order marginals, deep learning may become necessary.

### Mechanism 2: Differential privacy provides privacy through calibrated noise injection
- Claim: Adding carefully calibrated random noise ensures output distributions do not depend strongly on any individual's contribution.
- Mechanism: DP achieves privacy through noise calibrated to computation sensitivity. For synthetic data: (1) add noise to marginal measurements, or (2) use DP-SGD with gradient clipping and noise for neural networks. The post-processing property ensures subsequent operations on privatized data maintain guarantees.
- Core assumption: The privacy parameter ε appropriately captures acceptable privacy loss; data is fixed-size and modelable.
- Evidence anchors:
  - [abstract]: "defenses based on differential privacy mechanisms"
  - [section 2.3]: "any property of the output should be approximately equally likely whether or not any individual was included in the input"
  - [corpus]: Weak direct corpus evidence on DP mechanisms specifically; corpus papers focus on generative models rather than DP foundations
- Break condition: When privacy budget is extremely tight (ε → 0), required noise overwhelms all signal.

### Mechanism 3: Membership inference attacks quantify empirical privacy leakage
- Claim: Membership inference attacks provide empirical privacy leakage measurement that complements formal DP guarantees.
- Mechanism: Attackers exploit synthetic generators' tendency to overfit, producing samples closer to training points than unseen points. By comparing likelihood estimates under synthetic distribution vs. reference distribution (auxiliary data), attackers infer membership. Empirical ε derives from TPR/FPR ratios.
- Core assumption: Attacker has auxiliary information and can build shadow models or estimate densities.
- Evidence anchors:
  - [abstract]: "membership inference attacks that can quantify privacy leakage"
  - [section 5.1.2]: Equation (3) shows empirical ε derivation from attack TPR/FPR
  - [corpus]: "Membership Inference over Diffusion-models-based Synthetic Tabular Data" (arxiv 2510.16037) directly studies MIAs on TabDDPM/TabSyn
- Break condition: When generator is well-regularized with sufficient DP noise, attack accuracy approaches random guessing.

## Foundational Learning

- Concept: Differential Privacy (DP) fundamentals
  - Why needed here: All formal privacy guarantees rely on DP. Understanding ε, δ, sensitivity, and composition is essential for choosing methods and interpreting guarantees.
  - Quick check question: Why does "post-processing" allow training a generator on noisy marginals without additional privacy cost?

- Concept: Probabilistic Graphical Models (PGMs)
  - Why needed here: Marginal-based methods (PrivBayes, MST, AIM, PrivMRF) are built on Bayesian networks and Markov Random Fields. Understanding how marginals encode correlations is prerequisite to Section 3.
  - Quick check question: Given a Bayesian network structure, which marginals must be materialized to sample from it?

- Concept: Generative Adversarial Networks and Diffusion Models
  - Why needed here: Deep learning approaches (CTGAN, DP-CTGAN, TabDDPM, TabSyn) use these architectures. Understanding their training dynamics explains why they struggle under DP.
  - Quick check question: Why might adversarial training in GANs be particularly problematic combined with DP-SGD noise?

## Architecture Onboarding

- Component map:
  ```
  [Reference Data] → [Privacy Mechanism] → [Synthetic Model] → [Synthetic Data]
                          ↓                        ↓
                    Marginal-based:           Deep Learning:
                    - Select marginals        - GANs (CTGAN, DP-CTGAN)
                    - Measure with DP noise   - Diffusion (TabDDPM, TabSyn)
                    - Generate via PGM        - LLMs (GReaT, SynLM)
                          ↓
                    [Attacks for Validation]
                    - Density-based (DOMIAS, MAMA-MIA)
                    - Shadow modeling
  ```

- Critical path:
  1. Start with marginal-based methods (PrivBayes or MST) as baselines—they're more stable under DP
  2. Implement AIM or RAP++ for state-of-the-art marginal-based generation
  3. If high-dimensional data requires deep learning, try TabDDPM or SynLM with DP-SGD
  4. Always validate with membership inference attacks to measure empirical privacy

- Design tradeoffs:
  | Factor | Marginal-based | Deep Learning |
  |--------|----------------|---------------|
  | Privacy budget efficiency | Better (per column) | Worse (per iteration) |
  | High-dimensional data | May miss complex interactions | More flexible |
  | Reproducibility | High | Variable (GAN instability) |
  | DP utility under strict ε | Robust | Unpredictable |

- Failure signatures:
  - Marginal-based: Selected marginals miss important correlations → implausible value combinations
  - Deep learning: DP noise causes mode collapse → limited diversity; convergence instability → non-reproducible results
  - Privacy failure: Membership inference attack TPR significantly exceeds FPR

- First 3 experiments:
  1. **Baseline comparison**: Implement PrivBayes vs. CTGAN on a reference dataset (e.g., Adult census), measuring statistical fidelity (marginal accuracy) and privacy (membership inference). Expected: PrivBayes matches marginals better under same ε budget.
  2. **Privacy budget sweep**: Run AIM across ε values (0.1, 1.0, 5.0, ∞) and measure utility-privacy tradeoff curve. Establishes your "Pareto frontier" for the dataset.
  3. **Attack validation**: Implement density-based membership inference attack (DOMIAS framework) on synthetic data. Compute empirical ε from attack TPR/FPR to verify it doesn't exceed theoretical guarantee—safety check before deployment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can recent diffusion-based models and LLM-based approaches systematically outperform marginal-based methods for differentially private synthetic tabular data, particularly on high-dimensional datasets?
- Basis in paper: [explicit] Section 4.4 states: "there still remains a gap where SOTA approaches based on diffusion models and LLMs have not been systematically compared to marginal-based methods."
- Why unresolved: Diffusion models (TabDDPM, TabSyn) and LLM approaches (GReaT, SynLM) are recent and lack comprehensive benchmarking against marginal SOTA methods like AIM.
- What evidence would resolve it: Systematic benchmarks comparing diffusion, LLM-based, and marginal methods across datasets with varying dimensionality under consistent DP budgets.

### Open Question 2
- Question: Under what data characteristics (dimensionality, sparsity, privacy budget) do deep learning approaches become more effective than marginal-based methods?
- Basis in paper: [inferred] Section 4.4 notes deep learning may have potential for "high-dimensional tabular data" but systematic characterization of boundary conditions remains unstudied.
- Why unresolved: GAN-based methods show "utility recovery incapability," but whether diffusion/LLM methods overcome this under specific conditions is unknown.
- What evidence would resolve it: Experiments varying feature counts, ε values, and data complexity to identify crossover points where deep learning outperforms marginal methods.

### Open Question 3
- Question: How can empirical privacy attacks effectively validate or calibrate formal differential privacy guarantees?
- Basis in paper: [inferred] Section 5 notes attacks provide "more realistic privacy leakage" versus theoretical DP's worst-case guarantees, yet the relationship between these perspectives remains unclear.
- Why unresolved: Formal DP bounds may be loose practically; empirical attacks may miss worst-case vulnerabilities. Bridging these perspectives is an open challenge.
- What evidence would resolve it: Studies correlating attack success rates with DP parameters across algorithms, potentially yielding tighter practical bounds.

## Limitations

- Internal Library Dependence: The survey references an internally developed synthetic data generation library not yet open-sourced, limiting direct experimental reproduction and verification of reported benchmarks
- Cross-Domain Generalization: While the paper demonstrates marginal-based methods excel on demographic and structured data, performance claims for graph data, federated settings, and LLM-based generation remain theoretical or limited to small-scale studies
- DP-SGD Optimization Gap: Deep learning methods under DP-SGD face inherent utility-privacy tradeoffs that current optimization techniques cannot fully bridge, particularly for complex multi-way interactions

## Confidence

- **High Confidence**: Marginal-based methods' superiority under strict DP constraints (ε < 1) for standard tabular benchmarks; membership inference attack effectiveness on non-DP generators
- **Medium Confidence**: Claims about LLM-based generation scalability and privacy; federated synthetic data generation feasibility
- **Low Confidence**: Graph data generation performance comparisons; real-world deployment case studies

## Next Checks

1. **Reproduce AIM vs. DP-CTGAN Benchmark**: Implement both methods on UCI Adult dataset with ε=1.0, measuring 1-way marginal accuracy and membership inference TPR/FPR to verify the claimed utility gap
2. **Empirical Privacy Budget Sweep**: Run multiple marginal-based methods (AIM, MST) across ε ∈ {0.1, 0.5, 1.0, 5.0, 10.0}, plotting utility-privacy Pareto curves to identify practical sweet spots
3. **Shadow Model Attack Validation**: Implement shadow modeling membership inference on TabDDPM synthetic data, calculating empirical ε from attack TPR/FPR to confirm it doesn't exceed theoretical DP guarantees before deployment