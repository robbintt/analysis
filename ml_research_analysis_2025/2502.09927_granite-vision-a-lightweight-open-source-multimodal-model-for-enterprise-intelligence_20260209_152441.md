---
ver: rpa2
title: 'Granite Vision: a lightweight, open-source multimodal model for enterprise
  Intelligence'
arxiv_id: '2502.09927'
source_url: https://arxiv.org/abs/2502.09927
tags:
- vision
- granite
- visual
- document
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Granite Vision, a lightweight 3-billion-parameter
  multimodal model designed for enterprise document understanding. The model extends
  IBM's Granite LLM with a vision encoder and projector, and is trained on a comprehensive
  dataset of 13 million images and 80 million instructions covering document QA, charts,
  tables, diagrams, and general images.
---

# Granite Vision: a lightweight, open-source multimodal model for enterprise Intelligence

## Quick Facts
- arXiv ID: 2502.09927
- Source URL: https://arxiv.org/abs/2502.09927
- Authors: Granite Vision Team; Leonid Karlinsky; Assaf Arbelle; Abraham Daniels; Ahmed Nassar; Amit Alfassi; Bo Wu; Eli Schwartz; Dhiraj Joshi; Jovana Kondic; Nimrod Shabtay; Pengyuan Li; Roei Herzig; Shafiq Abedin; Shaked Perek; Sivan Harary; Udi Barzelay; Adi Raz Goldfarb; Aude Oliva; Ben Wieles; Christoph Auer; Dan Gutfreund; David Beymer; David Wood; Hilde Kuehne; Jacob Hansen; Joseph Shtok; Ken Wong; Luis Angel Bathen; Mayank Mishra; Maksym Lysak; Michele Dolfi; Mikhail Yurochkin; Nikolaos Livathinos; Nimrod Harel; Ophir Azulai; Oshri Naparstek; Rafael Teixeira de Lima; Rameswar Panda; Sivan Doveh; Shubham Gupta; Subhro Das; Syed Zawad; Yusik Kim; Zexue He; Alexander Brooks; Gabe Goodhart; Anita Govindjee; Derek Leist; Ibrahim Ibrahim; Aya Soffer; David Cox; Kate Soule; Luis Lastras; Nirmit Desai; Shila Ofek-koifman; Sriram Raghavan; Tanveer Syeda-Mahmood; Peter Staar; Tal Drory; Rogerio Feris
- Reference count: 31
- Primary result: 88% on DocVQA, 86% on ChartQA among small models, outperforming much larger proprietary models

## Executive Summary
Granite Vision is a 3-billion-parameter multimodal model designed for enterprise document understanding. It extends IBM's Granite LLM with a vision encoder and projector, achieving state-of-the-art performance on document understanding benchmarks among small models. The model is trained on a comprehensive dataset of 13 million images and 80 million instructions covering document QA, charts, tables, diagrams, and general images. Released under an Apache 2.0 license, it offers full transparency into training data and methods.

## Method Summary
Granite Vision employs a three-stage training approach: projector pre-training, joint pre-training with the LLM, and instruction tuning. The model uses a SigLIP-384 vision encoder with a 2-layer MLP projector to map multi-layer visual features to visual tokens matching the LLM embedding dimension. Training data includes 13M images and 80M instructions, with specific focus on document understanding tasks. The model processes images using AnyRes tiling with up to 10 patches at 384×384 resolution. After training, a novel safety classification method using sparse attention vectors is applied for identifying harmful inputs.

## Key Results
- Achieves 88% accuracy on DocVQA and 86% on ChartQA, state-of-the-art among small models
- Outperforms much larger proprietary models on document understanding tasks
- Excels at table and chart extraction tasks
- Demonstrates competitive safety performance using sparse attention vector classification
- Released under Apache 2.0 license with full transparency into training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A three-stage training progression enables lightweight multimodal models to achieve enterprise-grade performance on document understanding tasks.
- Mechanism: Stage 1 freezes vision encoder and LLM, training only the projector MLP. Stage 2 unfreezes LLM and projector while keeping vision encoder frozen, then discards fine-tuned LLM weights. Stage 3 uses supervised fine-tuning on curated document pairs.
- Core assumption: Projector learned in Stage 2 transfers better than random initialization, and frozen vision encoder's multi-layer features contain sufficient document-relevant information.
- Evidence anchors: Three-stage approach explicitly stated in abstract; projector-only weight retention shown to provide better starting point for Stage 3 in section 4.2.
- Break condition: If Stage 2 joint pre-training degrades projector alignment, transfer benefit fails and Stage 3 initialization is compromised.

### Mechanism 2
- Claim: Extracting and concatenating multi-layer features from vision encoder preserves fine-grained textual and spatial details critical for document understanding.
- Mechanism: Concatenates intermediate layer representations from SigLIP before projection, allowing projector to access both low-level edge/texture features and high-level semantic features.
- Core assumption: Intermediate transformer layers encode spatially precise information that final layer may have pooled away.
- Evidence anchors: Multi-layer concatenation provides rich representations beneficial for visual document understanding (section 4.1).
- Break condition: If concatenated features exceed projector's capacity to compress into useful visual tokens, performance plateaus or degrades.

### Mechanism 3
- Claim: Sparse attention vectors derived from few-shot demonstrations can serve as discriminative features for safety classification at test time.
- Mechanism: Extracts attention vectors from specific heads and layers for few-shot labeled examples, computes class centroids, identifies top-K heads maximizing classification accuracy, and aggregates predictions via majority vote.
- Core assumption: Safety-relevant information is sparsely distributed across a small subset of attention heads that generalize from few-shot examples.
- Evidence anchors: VLGuard accuracy improves from 86.0 to 96.2 with Safety Vectors; MHalu improves from 78.0 to 80.7 (section 4.3, Table 4).
- Break condition: If selected heads overfit to few-shot examples or fail to generalize to new safety categories, classification accuracy drops on distribution-shifted inputs.

## Foundational Learning

- **Concept: Vision-Language Alignment via Projectors**
  - Why needed here: Understanding how a learnable connector maps visual features to LLM's embedding space is essential for diagnosing alignment failures and tuning multi-stage training.
  - Quick check question: If you freeze both vision encoder and LLM, what component must be trained to enable cross-modal reasoning?

- **Concept: Multi-Resolution Tiling for High-Resolution Inputs**
  - Why needed here: The "AnyRes" technique allows processing document images at varied aspect ratios and scales, critical for reading small text in tables and charts.
  - Quick check question: How does tiling a 2000×1000 pixel document into 384×384 patches affect the number of visual tokens passed to the LLM?

- **Concept: Sparse Feature Selection for Classification**
  - Why needed here: Safety Vectors approach relies on identifying a small subset of attention heads that encode task-relevant information, rather than using full representation.
  - Quick check question: Why might a nearest-centroid classifier on sparse attention vectors outperform a generative model's direct output for safety classification?

## Architecture Onboarding

- **Component map**: Image input → AnyRes tiling → SigLIP multi-layer feature extraction → concatenation → Projector → visual tokens → Granite LLM → generated response → (optional) Safety classification via sparse attention vectors

- **Critical path**: Image input → AnyRes tiling → SigLIP multi-layer feature extraction → concatenation → Projector → visual tokens → Granite LLM → generated response. Safety classification: Extract attention vectors from HSAV → centroid similarity → majority vote.

- **Design tradeoffs**:
  - Multi-layer vs. single-layer features: Multi-layer improves fine-grained detail capture but increases projector input dimension and computational cost.
  - Projector-only weight retention in Stage 2: Provides better initialization for Stage 3 but requires extra training effort; discarding LLM weights prevents overfitting to limited pre-training data.
  - Safety Vectors vs. generative safety prompting: Classification approach is more interpretable and computationally lightweight but requires labeled few-shot examples and may not cover all edge cases.

- **Failure signatures**:
  - Low OCR/text extraction accuracy: Likely due to insufficient resolution or tiling configuration; check AnyRes settings and patch count.
  - Safety classification drops on new attack types: Sparse heads may not generalize; re-evaluate HSAV selection with broader few-shot coverage.
  - Hallucination in chart/table extraction: Vision encoder may lack fine-grained features; consider augmenting multi-layer feature extraction or increasing projector capacity.

- **First 3 experiments**:
  1. Ablate multi-layer feature extraction: Train with only final-layer SigLIP features and compare DocVQA/ChartQA performance.
  2. Vary Stage 2 retention strategy: Compare discarding LLM weights vs. retaining them for Stage 3.
  3. Stress-test Safety Vectors: Evaluate classification accuracy on VLGuard and RTVLM with reduced few-shot examples.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can context compression techniques effectively enable multi-page document processing in lightweight multimodal models without significant loss of detail?
- Basis in paper: The Conclusion states future work includes "enabling multi-page document processing through context compression."
- Why unresolved: Current model processes single images/pages, and 2B parameter size limits context window required for raw multi-page token sequences.
- What evidence would resolve it: Performance benchmarks on multi-page datasets demonstrating compressed representations maintain accuracy comparable to single-page performance.

### Open Question 2
- Question: Does scaling up Safety Vectors with larger training datasets improve identification of harmful inputs compared to current few-shot approach?
- Basis in paper: Section 5.3 notes current SVs rely on "few-shot samples which may have limited scope" and plans to "investigate scaling up SVs using more training data."
- Why unresolved: Unclear if sparse attention vector method generalizes better with data scaling or is fundamentally limited by sparse representation.
- What evidence would resolve it: Comparison of Attack Success Rate and Helpfulness scores between current few-shot SVs and version trained on comprehensive safety dataset.

### Open Question 3
- Question: What specific training techniques are required to preserve text-only reasoning capabilities when extending language model with vision encoders?
- Basis in paper: Conclusion lists "exploring techniques to preserve Granite Vision language-only capabilities" as key future investigation area.
- Why unresolved: Visual instruction tuning often leads to catastrophic forgetting of text-only capabilities, and paper does not yet define solution for this architecture.
- What evidence would resolve it: Comparative evaluation of base LLM and final vision model on text-only benchmarks showing minimal performance degradation.

## Limitations
- Three-stage training methodology lacks full specification of critical hyperparameters including stage durations and learning rate schedules
- Safety classification approach validated only on limited datasets without comprehensive adversarial testing
- Performance on general visual reasoning tasks and out-of-distribution document types remains underexplored

## Confidence
- **High Confidence**: Document understanding benchmark results (88% DocVQA, 86% ChartQA) and core architectural design are well-documented and reproducible
- **Medium Confidence**: Superiority over larger proprietary models supported by benchmarks but may not account for training data differences or evaluation protocol variations
- **Low Confidence**: Generalization claims for document types beyond benchmarked categories not empirically validated; projector-only weight retention strategy lacks ablation studies

## Next Checks
1. Stress-test Safety Vectors: Evaluate sparse attention vector classifier on adversarial safety test sets and measure performance degradation with reduced few-shot examples.
2. Document Type Generalization: Test Granite Vision on out-of-distribution document categories (legal contracts, medical records, technical manuals) not represented in training data.
3. Alternative Projector Retention Strategies: Implement and compare Stage 2 with retaining fine-tuned LLM weights, using random projector initialization, and the proposed projector-only retention.