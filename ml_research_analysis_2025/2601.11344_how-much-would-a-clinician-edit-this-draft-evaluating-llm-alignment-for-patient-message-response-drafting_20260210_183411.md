---
ver: rpa2
title: How Much Would a Clinician Edit This Draft? Evaluating LLM Alignment for Patient
  Message Response Drafting
arxiv_id: '2601.11344'
source_url: https://arxiv.org/abs/2601.11344
tags:
- patient
- response
- clinician
- responses
- theme
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) can draft patient portal responses,
  but aligning them with individual clinician preferences remains challenging. We
  develop a thematic taxonomy and evaluation framework measuring clinician editing
  load at content and theme levels.
---

# How Much Would a Clinician Edit This Draft? Evaluating LLM Alignment for Patient Message Response Drafting

## Quick Facts
- arXiv ID: 2601.11344
- Source URL: https://arxiv.org/abs/2601.11344
- Reference count: 40
- Primary result: Theme-driven adaptation improves response quality by 33% over zero-shot models

## Executive Summary
This paper evaluates how well large language models (LLMs) can draft patient portal responses that align with clinician preferences. The authors develop a thematic taxonomy and evaluation framework measuring clinician editing load at content and theme levels. Testing seven LLMs with five adaptation strategies across three datasets, they find that theme-driven adaptation improves response quality by 33% over zero-shot models. While LLMs excel at generating empathetic content, they struggle with question-asking themes and face substantial inter-clinician variability, indicating the need for individual-level adaptation. The best-performing models achieve 25-26% reduction in editing workload, suggesting potential for reliable LLM-assisted patient communication while highlighting remaining alignment challenges.

## Method Summary
The study evaluates LLM alignment for patient portal response drafting using three datasets (IPPM, SyPPM, SoCPPM) totaling 700 samples. Seven models were tested with five adaptation strategies: zero-shot, theme-guided, SFT (144k training pairs), RAG (5-shot retrieval), and TADPOLE (thematic preference optimization). The EditJudge framework provides dual-level evaluation measuring content-level edit-F1 (expected additions/deletions) and theme-level edit-F1 (alignment of communicative intent across 9 themes). Training involved SFT with LoRA fine-tuning and TADPOLE using DPO with thematic preference pairs. The thematic taxonomy was validated by 13 clinicians and implemented through a fine-tuned Llama3-8B classifier (F1=0.82).

## Key Results
- Theme-driven adaptation improves response quality by 33% over zero-shot models
- Best-performing models achieve 25-26% reduction in editing workload
- LLMs excel at generating empathetic content but struggle with question-asking themes
- Inter-clinician variability is substantial (IAP=0.24 at content level, 0.62 at theme level)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Theme-driven adaptation strategies improve LLM-clinician alignment by providing structured communicative goals.
- Mechanism: Explicit theme definitions (empathy, symptom questions, medical assessment, etc.) guide the model toward clinically-relevant response components rather than generic helpfulness patterns. The TADPOLE method extends this by creating theme-specific preference pairs for DPO training, reinforcing theme-appropriate generation.
- Core assumption: Clinician responses decompose into learnable thematic components that transfer across patients and contexts.
- Evidence anchors: [abstract] "theme-driven adaptation improves response quality by 33% over zero-shot models"; [section 5.1] "TADPOLE adaptation strategy offers the best blend of precision and recall with the highest average content-level edit-F1 scores"

### Mechanism 2
- Claim: Two-level evaluation (content vs. theme) distinguishes factual alignment from communicative intent alignment.
- Mechanism: Content-level edit-F1 measures exact matching of clinical facts/instructions (requires precision). Theme-level edit-F1 measures whether responses address similar clinical goals (more forgiving). Together, they separate "wrong content" from "right intent, different phrasing."
- Core assumption: Clinician edits are driven by both factual correctness needs and stylistic/preference differences.
- Evidence anchors: [section 3] "theme-level alignment is a more achievable goal when drafting clinician responses"; [section 5.1] IAP at content level (0.24) vs theme level (0.62), showing humans align better thematically than textually

### Mechanism 3
- Claim: Individual clinician variation creates epistemic uncertainty that limits task-level model performance.
- Mechanism: Different clinicians make different clinical judgments, use different communication styles, and prioritize different information. Using one clinician's response as a draft for another achieves only 24% edit reduction (IAP), establishing an upper bound for non-personalized systems.
- Core assumption: Clinician preferences are meaningfully individual rather than purely situational.
- Evidence anchors: [section 5.1] "using another clinician's response as a draft only reduces clinician edits by 24%"; [section 5.2] "some individual themes have very low IAP, e.g. treatment planning (0.07) and contingency planning (0.06)"

## Foundational Learning

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: TADPOLE builds on DPO to align models with thematic preferences without a separate reward model.
  - Quick check question: Can you explain how DPO avoids training a separate reward model compared to RLHF?

- Concept: **Retrieval-Augmented Generation (RAG) for style/content transfer**
  - Why needed here: 5-shot RAG with similar patient messages is tested as an adaptation strategy, though performs worse than SFT.
  - Quick check question: What retrieval embedding would you use for patient messages—semantic similarity, symptom overlap, or clinician identity?

- Concept: **Multi-label classification with class imbalance**
  - Why needed here: Theme classification (9 classes including "Other") has skewed distributions; empathy appears in 85% of responses, contingency planning in only 22%.
  - Quick check question: How would you handle severe class imbalance when fine-tuning the theme classifier?

## Architecture Onboarding

- Component map:
  Patient Message + EHR Summary → Adapted LLM → Response Draft
                                        ↓
  Expert Response (reference) → EditJudge Framework → Content Edit-F1 + Theme Edit-F1

- Critical path:
  1. Define/validate theme taxonomy with domain experts (13 clinicians consulted)
  2. Train and validate editJudge models on human annotations (450 train, 50 eval samples)
  3. Run SFT on 144k conversation pairs
  4. Generate TADPOLE preference pairs using enhancement/corruption agents
  5. Apply DPO to SFT model using preference pairs
  6. Evaluate using EditJudge against held-out expert responses

- Design tradeoffs:
  - Local vs. commercial models: Local models preferred for PHI but underperform frontier models (Gemini 2.5 Pro + theme = 0.26 edit-F1 vs. best local TADPOLE = 0.25)
  - SFT vs. TADPOLE: SFT higher precision; TADPOLE better recall blend. SFT outperforms TADPOLE on real-time responses (SoCPPM)—hypothesis: TADPOLE themes less suitable for messier real-world data
  - Theme-guided vs. zero-shot: Theme prompting improves frontier models consistently but provides mixed results for local models

- Failure signatures:
  - Low question-asking: 0-shot models rarely generate symptom/medication questions (0.02-0.05 frequency vs. 0.30-0.36 in clinician responses)
  - Over-medicalization: Unadapted LLMs over-generate medical assessment themes (0.89 vs. clinician 0.34), potentially leading to overdiagnosis concerns
  - Low IAP themes: Treatment planning, contingency planning show near-zero agreement across clinicians—these themes require individual-level adaptation

- First 3 experiments:
  1. Validate editJudge on your data: Before trusting automated metrics, manually annotate 20-30 response pairs and compare editJudge outputs to human judgments for your specific use case.
  2. Establish baseline IAP: Collect 2-3 clinician responses for the same 20 messages to measure inter-clinician variability in your setting—this sets realistic performance expectations.
  3. Theme distribution analysis: Use the theme classifier to compare your clinicians' response patterns to the paper's reported distributions; significant deviations may require taxonomy revision before adaptation.

## Open Questions the Paper Calls Out

- Can suggesting theme-based "nudges" rather than generating full content drafts improve efficiency for themes with high epistemic uncertainty?
- How can LLMs be effectively specialized to align with individual clinician preferences to overcome performance limitations caused by inter-clinician variability?
- What are the safety, bias, and robustness characteristics of adapted LLMs when deployed specifically in rural healthcare settings?
- How does the estimated 25-26% reduction in editing load translate to actual time savings and cognitive burden in live clinical workflows?

## Limitations

- The thematic taxonomy may not generalize across different healthcare systems and patient populations
- Limited sample size of 13 clinicians may not represent the broader population of healthcare providers
- Significant uncertainty exists around the TADPOLE preference pair generation process and exact implementation details
- The study focuses on automated evaluation, limiting in-depth qualitative analysis by clinicians and patients

## Confidence

**High Confidence:**
- Theme-driven adaptation improves response quality by 33% over zero-shot models
- LLMs excel at generating empathetic content but struggle with question-asking themes
- Inter-clinician variability is substantial (IAP = 0.24 at content level)

**Medium Confidence:**
- Best-performing models achieve 25-26% reduction in editing workload
- TADPOLE adaptation strategy offers the best blend of precision and recall
- Theme-level alignment is a more achievable goal than content-level alignment

**Low Confidence:**
- Individual-level adaptation is necessary for optimal alignment
- SFT model performs better than TADPOLE on real-time responses due to "messier" data
- Frontier models consistently outperform local models across all adaptation strategies

## Next Checks

1. Validate taxonomy generalizability: Apply the thematic taxonomy to patient messages from a different healthcare system or patient population. Compare the resulting theme distributions to those reported in the paper to assess whether the taxonomy captures the same communication patterns across contexts.

2. Test EditJudge robustness: Manually annotate 50-100 response pairs from your specific clinical setting and compare the EditJudge outputs to human judgments. Calculate inter-annotator agreement among human reviewers and compare to EditJudge performance to assess whether the automated metrics capture the same aspects of alignment that matter in your context.

3. Establish local IAP baseline: Collect 3-5 clinician responses for the same 30-50 patient messages in your setting. Calculate the inter-clinician agreement at both content and theme levels. This will establish realistic performance expectations for your specific population and help determine whether individual-level adaptation is truly necessary in your context.