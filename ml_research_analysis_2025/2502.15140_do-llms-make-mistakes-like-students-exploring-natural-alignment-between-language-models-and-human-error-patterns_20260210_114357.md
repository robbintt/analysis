---
ver: rpa2
title: Do LLMs Make Mistakes Like Students? Exploring Natural Alignment between Language
  Models and Human Error Patterns
arxiv_id: '2502.15140'
source_url: https://arxiv.org/abs/2502.15140
tags:
- student
- patterns
- llms
- students
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether Large Language Models (LLMs) align
  with student error patterns in multiple-choice questions (MCQs). The researchers
  analyze whether LLMs assign higher generation probabilities to incorrect answers
  that students commonly select and whether LLM errors match student misconceptions.
---

# Do LLMs Make Mistakes Like Students? Exploring Natural Alignment between Language Models and Human Error Patterns

## Quick Facts
- **arXiv ID**: 2502.15140
- **Source URL**: https://arxiv.org/abs/2502.15140
- **Reference count**: 40
- **Primary result**: Moderate correlations (r = 0.28-0.37) between LLM generation probabilities and student distractor selection patterns; LLMs select same incorrect answers as students 51-59% of the time

## Executive Summary
This study investigates whether Large Language Models (LLMs) align with student error patterns in multiple-choice questions (MCQs). The researchers analyze whether LLMs assign higher generation probabilities to incorrect answers that students commonly select and whether LLM errors match student misconceptions. Using a dataset of 3,202 MCQs with real student response data, they test various LLM sizes (0.5B-72B parameters) using both index-based and text-based likelihood approaches. Results show moderate correlations between LLM generation probabilities and student distractor selection patterns, with remarkable alignment when LLMs make mistakes.

## Method Summary
The researchers used a dataset of 3,202 MCQs with real student response data to test alignment between LLM behavior and student error patterns. They evaluated various LLM sizes ranging from 0.5B to 72B parameters using two approaches: index-based likelihood (measuring probability of selecting answer choices) and text-based likelihood (measuring probability of generating answer text). The study measured Pearson correlation coefficients between LLM generation probabilities and student distractor selection frequencies, and analyzed whether LLM errors corresponded to the same incorrect answers that commonly mislead students.

## Key Results
- Moderate correlations (Pearson r = 0.28-0.37) between LLM generation probabilities and student distractor selection patterns
- Even small models (0.5B parameters) show 51-59% alignment with student error patterns when making mistakes
- Larger models (72B parameters) achieve 54-59% alignment with student misconceptions
- Index-based and text-based likelihood approaches yield similar correlation results

## Why This Works (Mechanism)
The alignment between LLM error patterns and student misconceptions likely stems from LLMs' training on vast amounts of human-generated text that contains common misconceptions and reasoning errors. When LLMs generate responses, they may inadvertently reproduce the same flawed reasoning patterns present in their training data, which coincidentally matches how students arrive at incorrect answers. This suggests that LLM training data captures some aspects of human cognitive processes related to common misconceptions, even without explicit modeling of student thinking.

## Foundational Learning
- **MCQ structure and distractor design**: Understanding how multiple-choice questions are constructed and how incorrect answer choices (distractors) are designed to reflect common misconceptions
  - Why needed: Essential for analyzing alignment between LLM outputs and student error patterns
  - Quick check: Can identify what makes an effective distractor versus a random incorrect answer

- **Probability distributions in LLMs**: Understanding how LLMs generate probabilities over answer choices and text sequences
  - Why needed: Critical for interpreting LLM behavior and comparing it to student response patterns
  - Quick check: Can explain softmax outputs and temperature scaling effects

- **Statistical correlation analysis**: Understanding Pearson correlation coefficients and their interpretation in educational contexts
  - Why needed: Required for evaluating the strength of alignment between LLM and student behaviors
  - Quick check: Can interpret correlation values and distinguish between weak, moderate, and strong relationships

## Architecture Onboarding
- **Component map**: MCQs dataset -> LLM models (0.5B-72B parameters) -> Probability generation -> Student response data comparison -> Correlation analysis
- **Critical path**: MCQ input -> LLM processing -> Probability output -> Student pattern comparison -> Statistical validation
- **Design tradeoffs**: Smaller models are computationally efficient but may capture less nuanced patterns; larger models are more computationally expensive but potentially more aligned with student thinking
- **Failure signatures**: Low correlation coefficients suggest poor alignment; inconsistent results across model sizes indicate instability; high computational cost with minimal improvement suggests diminishing returns
- **First experiments**: 1) Test correlation between LLM probabilities and student responses on held-out MCQs, 2) Compare distractor effectiveness from different model sizes, 3) Analyze specific question types where alignment is strongest/weakest

## Open Questions the Paper Calls Out
None

## Limitations
- Moderate correlation coefficients (r = 0.28-0.37) indicate far from perfect alignment between LLM and student patterns
- Sample size of 3,202 MCQs may not capture full diversity of question types and student populations
- Focus on MCQs limits generalizability to other assessment formats or subject areas
- Unclear whether alignment reflects genuine understanding of misconceptions or coincidental pattern matching

## Confidence
- **High Confidence**: Moderate correlations between LLM probabilities and student distractor selection are empirically supported
- **Medium Confidence**: LLM errors matching student misconceptions requires further validation to distinguish genuine cognitive alignment from statistical artifacts
- **Low Confidence**: Claims about smaller models effectively generating pedagogically relevant distractors lack direct evidence of effectiveness

## Next Checks
1. Conduct cross-validation studies using MCQs from different educational contexts and student populations to assess generalizability
2. Implement controlled experiments comparing student performance on questions with LLM-generated versus expert-designed distractors
3. Analyze reasoning processes of both LLMs and students using think-aloud protocols to determine shared cognitive mechanisms