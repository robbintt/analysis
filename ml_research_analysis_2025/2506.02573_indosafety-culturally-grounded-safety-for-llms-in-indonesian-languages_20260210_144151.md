---
ver: rpa2
title: 'IndoSafety: Culturally Grounded Safety for LLMs in Indonesian Languages'
arxiv_id: '2506.02573'
source_url: https://arxiv.org/abs/2506.02573
tags:
- safety
- language
- indonesian
- response
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces IndoSafety, the first high-quality, human-verified
  safety evaluation dataset tailored for the Indonesian context, covering five language
  varieties: formal and colloquial Indonesian, along with three major local languages:
  Javanese, Sundanese, and Minangkabau. IndoSafety extends prior safety frameworks
  with a taxonomy capturing Indonesia''s sociocultural context, including ethnic and
  religious sensitivities, traditional practices, historical controversies, and Pancasila
  misinterpretation.'
---

# IndoSafety: Culturally Grounded Safety for LLMs in Indonesian Languages

## Quick Facts
- arXiv ID: 2506.02573
- Source URL: https://arxiv.org/abs/2506.02573
- Reference count: 40
- First high-quality, human-verified safety evaluation dataset tailored for Indonesian context across five language varieties

## Executive Summary
IndoSafety introduces the first comprehensive safety evaluation dataset for large language models in Indonesian languages, covering formal and colloquial Indonesian plus three major regional languages: Javanese, Sundanese, and Minangkabau. The dataset extends prior safety frameworks with a taxonomy capturing Indonesia's unique sociocultural context, including ethnic and religious sensitivities, traditional practices, historical controversies, and Pancasila misinterpretation. Evaluations of 10 LLMs reveal that existing Indonesian-centric models often generate unsafe outputs, particularly in colloquial and local language settings. Fine-tuning on the IndoSafety-Train dataset significantly improves safety while preserving task performance, demonstrating the critical need for culturally grounded safety evaluation frameworks.

## Method Summary
The authors constructed IndoSafety through a three-phase process: translating and localizing existing safety benchmarks to formal Indonesian, augmenting with handcrafted Indonesian-specific prompts addressing regional sensitivities, and extending to five language variants through human translation and stratified sampling. The evaluation employed GPT-4o-based automatic assessment using binary question sets per harm category, validated against human annotations on 100-sample subsets. Safety fine-tuning was performed using LoRA on Sailor2-8B-Chat with the IndoSafety-Train dataset (2,014 examples), achieving safety improvements without catastrophic forgetting on downstream benchmarks.

## Key Results
- Existing Indonesian-centric models show unsafe rates of 14.3-40.2% across language variants, with regional languages exhibiting 1.3-2.5x higher unsafe rates than formal Indonesian
- Fine-tuning on formal Indonesian safety data reduces unsafe responses in Javanese from 196 to 45 cases and Sundanese from 178 to 45 cases, demonstrating cross-lingual safety transfer
- Safety improvements achieved without significant degradation on downstream benchmarks (IndoMMLU, IndoCareer, IndoCulture, MAPS, COPAL-ID, IndoCloze), with most metrics showing <1% performance loss

## Why This Works (Mechanism)

### Mechanism 1: Cross-lingual Safety Generalization via High-Resource Fine-tuning
Safety fine-tuning in formal Indonesian can reduce unsafe outputs in related regional languages (Javanese, Sundanese) even without direct training data in those languages. The mechanism relies on linguistic and cultural proximity between formal Indonesian and regional variants allowing safety-related representations to transfer through shared linguistic patterns and overlapping vocabulary. The core assumption is that safety reasoning capability is partially language-agnostic once grounded in a related high-resource variant, enabling models to apply learned refusal patterns to semantically similar inputs. Evidence shows fine-tuning Sailor2 with IndoSafety-Train reduced unsafe responses in regional languages with statistically significant differences (α = 0.05). The transfer effect likely degrades for linguistically distant variants or cultures with divergent norms.

### Mechanism 2: Taxonomy Extension Enables Culturally-Specific Harm Detection
Extending standard safety taxonomies with region-specific categories enables detection of harms that translated benchmarks miss. By defining granular harm types tied to Indonesian sociocultural context (ethnic stereotypes, Pancasila misinterpretation, supernatural claims, etc.), evaluators can identify model behaviors considered safe in Western contexts but harmful locally. The core assumption is that safety is not universal but culturally contingent, requiring contextual grounding in local norms, laws, and sensitivities. The taxonomy includes eight new harm types developed through expert discussion, covering areas like "Pancasila Misinterpretation and Corruption" and "Regional Separatism Advocacy." The taxonomy remains incomplete, covering only "straightforward prompts" without adversarial attacks or indirect harms.

### Mechanism 3: Verbosity-Unsafe Response Correlation in Regional Models
Models producing longer average responses show higher unsafe output rates, particularly in regional language settings. The mechanism suggests that models optimized for helpfulness without adequate safety constraints may generate verbose, detailed responses that increase exposure to harmful content, especially when operating in lower-resource language variants where safety training data is scarce. The core assumption is that longer responses provide more opportunities for safety violations to manifest, and regional-language capability often develops without proportional safety alignment. Evidence shows Sailor2-8B generated the longest responses (1,996-2,154 characters) and showed the highest unsafe rates (32.8-40.2% in IndoSafety-Eval-2). This pattern requires further investigation as correlation does not imply causation.

## Foundational Learning

- **Safety Alignment in LLMs**
  - Why needed here: The paper builds on prior safety frameworks and assumes familiarity with concepts like RLHF, jailbreaking, and safety fine-tuning. Without this foundation, the motivation for IndoSafety and its evaluation methodology would be unclear.
  - Quick check question: Can you explain why translating English safety benchmarks to Indonesian might fail to capture local harms?

- **Cultural Grounding in NLP**
  - Why needed here: The core contribution is culturally-specific safety taxonomy. Understanding how cultural norms differ across contexts—and why direct translation is insufficient—is essential to appreciating why IndoSafety is necessary.
  - Quick check question: What is an example of content that might be considered safe in English contexts but harmful in Indonesian contexts?

- **Parameter-Efficient Fine-Tuning (LoRA)**
  - Why needed here: The safety tuning experiment uses LoRA with specific hyperparameters (r=4, alpha=16, one epoch). Understanding how LoRA works is necessary to replicate or extend this work.
  - Quick check question: Why might LoRA be preferred over full fine-tuning for safety alignment, and what tradeoffs does it introduce?

## Architecture Onboarding

- **Component map:**
  IndoSafety-Eval-1 (2,514 prompts) -> IndoSafety-Eval-2 (2,500 prompts across 5 variants) -> IndoSafety-Train (2,014 examples) -> GPT-4o evaluation pipeline -> LoRA fine-tuning (r=4, alpha=16) -> Safety improvement validation

- **Critical path:**
  1. Dataset Creation: Start with IndoSafety-Eval-1 construction (translate + localize + handcraft), then extend to multilingual IndoSafety-Eval-2
  2. Evaluation Setup: Define binary question sets per harm type, configure GPT-4o evaluator, validate against human annotations
  3. Model Testing: Collect responses from target LLMs across all variants, run automatic evaluation, compute unsafe percentages
  4. Safety Tuning: Prepare training data with safe responses, apply LoRA fine-tuning (r=4, alpha=16, lr=1e-5, 1 epoch), evaluate on both safety and downstream benchmarks

- **Design tradeoffs:**
  - Taxonomy depth vs. coverage: Eight region-specific harm types capture key sensitivities but may miss emerging or subtle harms; the paper acknowledges gaps in adversarial/indirect attack coverage
  - Automation vs. human validation: GPT-4o evaluation enables scale but introduces model bias; human validation on 100 samples shows strong correlation but may not generalize to all harm types
  - Language coverage: Five variants prioritize major languages but exclude hundreds of Indonesian languages; Minangkabau was excluded from fine-tuning evaluation due to model support issues
  - Training data size: 2,014 examples improved safety without catastrophic forgetting, but the optimal dataset size for regional language transfer remains unknown

- **Failure signatures:**
  - High unsafe rate in regional languages: Models show 1.3-2.5x higher unsafe rates in local languages vs. formal Indonesian (SahabatAI: 13.6% formal vs. 34.8% Sundanese)
  - Over-refusal or under-refusal: GPT-4o evaluation may misclassify nuanced cultural content; manual inspection required for edge cases
  - Catastrophic forgetting: Monitor downstream benchmark performance post-fine-tuning; Table 4 shows minimal degradation (<1% on most benchmarks)
  - Language mixing: GPT-4o responses in Minangkabau showed balanced Indonesian/Minangkabau mix, potentially affecting evaluation accuracy

- **First 3 experiments:**
  1. Baseline safety audit: Run IndoSafety-Eval-1 and IndoSafety-Eval-2 on your target model, compute unsafe rates by risk area and language variant. Compare against Table 2 benchmarks to identify critical vulnerabilities.
  2. Ablation on training data composition: Fine-tune separate models using (a) only general-safety prompts, (b) only Indonesian-specific prompts, (c) combined. Measure transfer to regional languages to isolate which components drive cross-lingual generalization.
  3. Human evaluation calibration: Manually annotate 100-200 responses from your model, compare against GPT-4o predictions, compute confusion matrix. If correlation falls below 0.65, refine evaluation question sets or add context-specific prompts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust are models aligned with IndoSafety against complex adversarial attacks and indirect prompt injections?
- Basis in paper: [explicit] The Limitations section states the dataset currently relies on "straightforward prompts" and lacks "indirect attacks" or "adversarial attacks," unlike prior work.
- Why unresolved: The current dataset construction methodology focused on direct safety threats, leaving the model's vulnerability to more sophisticated jailbreaking techniques untested.
- What evidence would resolve it: Evaluation of fine-tuned models using a new dataset specifically designed with adversarial and indirect attack strategies tailored to the Indonesian context.

### Open Question 2
- Question: To what extent does GPT-4o agree with human experts when evaluating culturally nuanced safety violations in low-resource Indonesian languages?
- Basis in paper: [explicit] The Limitations section acknowledges that relying on a single LLM (GPT-4o) for evaluation introduces potential biases and may struggle to interpret "cultural nuance and sociolinguistic context."
- Why unresolved: While the authors validated a subset, a comprehensive comparison between automated evaluation and human judgment across all local languages (Javanese, Sundanese, Minangkabau) was not performed.
- What evidence would resolve it: A large-scale human annotation study of model outputs by native speakers to calculate correlation metrics (e.g., Cohen's Kappa) against GPT-4o's safety judgments.

### Open Question 3
- Question: Does safety fine-tuning exclusively on formal Indonesian data leave performance gaps or "safety holes" in colloquial and regional language variants?
- Basis in paper: [inferred] The IndoSafety-Train dataset contains only standard Indonesian examples (2,014 instances), yet results show safety improvements in local languages.
- Why unresolved: It is unclear if formal training covers the full lexical and syntactic range of colloquialisms, or if specific harms expressed in local dialects remain unmitigated.
- What evidence would resolve it: Comparing the safety performance of models fine-tuned on formal Indonesian against models fine-tuned on mixed-code or dialect-specific safety data.

## Limitations
- Taxonomy and evaluation dataset cover only "straightforward prompts" without adversarial or indirect harm scenarios, limiting generalizability to real-world safety challenges
- Cross-lingual safety transfer was only demonstrated for Javanese and Sundanese; Minangkabau was excluded from fine-tuning evaluation due to model support issues
- GPT-4o evaluation introduces model bias; while human validation showed strong correlation (0.71-0.72), the 100-sample subset may not capture all cultural nuances or edge cases

## Confidence
- **High**: IndoSafety is the first high-quality, human-verified safety dataset for Indonesian languages with comprehensive taxonomy covering sociocultural context
- **High**: Existing Indonesian-centric models show significantly higher unsafe rates in colloquial and local languages compared to formal Indonesian (1.3-2.5x difference)
- **Medium**: Fine-tuning on formal Indonesian safety data transfers to regional languages, but mechanism remains incompletely understood and untested for all variants
- **Low**: Verbosity-unsafe response correlation is observational without established causal mechanism or broader corpus validation

## Next Checks
1. **Cross-lingual transfer ablation**: Fine-tune separate models using only general-safety prompts vs. only Indonesian-specific prompts vs. combined, then measure transfer to regional languages to isolate which components drive generalization
2. **Minangkabau safety evaluation**: Manually collect and evaluate 100-200 responses from target models on Minangkabau prompts to establish baseline unsafe rates and test cross-lingual transfer limits
3. **Adversarial prompt testing**: Construct and evaluate adversarial variants of IndoSafety prompts (e.g., indirect harm, context manipulation) to assess taxonomy completeness and identify safety blind spots