---
ver: rpa2
title: 'Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By
  Checking Inconsistencies About Key Facts'
arxiv_id: '2511.12236'
source_url: https://arxiv.org/abs/2511.12236
tags:
- confactcheck
- facts
- fact
- generated
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CONFACTCHECK is a hallucination detection method that checks for
  consistency in LLM-generated facts by generating targeted questions around key entities
  and comparing the answers with the original text. It does not require external knowledge
  bases or model weights and is designed for constrained environments.
---

# Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By Checking Inconsistencies About Key Facts

## Quick Facts
- arXiv ID: 2511.12236
- Source URL: https://arxiv.org/abs/2511.12236
- Authors: Raavi Gupta; Pranav Hari Panicker; Sumit Bhatia; Ganesh Ramakrishnan
- Reference count: 40
- Key outcome: CONFACTCHECK detects hallucinations by checking consistency in LLM-generated facts through targeted question generation and comparison, achieving higher accuracy than self-checking baselines while using fewer LLM calls and offering faster inference times.

## Executive Summary
CONFACTCHECK is a hallucination detection method that checks for consistency in LLM-generated facts by generating targeted questions around key entities and comparing the answers with the original text. It does not require external knowledge bases or model weights and is designed for constrained environments. The approach consists of a fact alignment check followed by a uniform distribution check to confirm confidence in the generated facts. Evaluated on QA and summarization tasks, it achieves higher accuracy than self-checking baselines while using fewer LLM calls and offering faster inference times.

## Method Summary
CONFACTCHECK detects hallucinations by extracting key facts from LLM-generated text using NER/POS tagging, generating targeted questions about each fact, and having the LLM regenerate answers. A judge LLM compares regenerated facts against originals for alignment, while a Kolmogorov-Smirnov test on token probabilities confirms confidence. The method operates without external knowledge bases, using beam decoding for better fact regeneration and achieving explainable results at the fact level.

## Key Results
- Achieves higher accuracy than self-checking baselines (SelfCheckGPT) on NQ_Open and WikiBio datasets
- Uses fewer LLM calls compared to baselines while maintaining better performance
- Ablation studies show uniform distribution check improves performance by up to 18%
- NER-based key fact extraction slightly outperforms POS-based alternatives

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Targeted probing of key facts reveals hallucinations through consistency failures.
- **Mechanism:** NER/POS identifies factual tokens (nouns, cardinal numbers, entities) → T5-based model generates questions targeting each key fact → LLM answers at low temperature → judge LLM compares regenerated facts against originals. Mismatches indicate hallucinations.
- **Core assumption:** Hallucinated facts will produce inconsistent answers when probed, while grounded facts remain consistent across reformulations.
- **Evidence anchors:** [abstract] "checks for consistency in LLM-generated facts by generating targeted questions around key entities and comparing the answers with the original text"; [section 3.1] "The initial step is to identify the factual components within a sentence. According to Kai et al. (2024), factual information in a sentence is typically conveyed through specific parts of speech, viz., nouns, pronouns, cardinal numbers, and adjectives."

### Mechanism 2
- **Claim:** Token probability distributions indicate model confidence; uniform distributions signal uncertainty.
- **Mechanism:** For regenerated facts, extract top-5 token probabilities → apply Kolmogorov-Smirnov test against uniform distribution → p < 0.05 indicates confidence (non-hallucination); p ≥ 0.05 indicates uncertainty (hallucination).
- **Core assumption:** Confident LLM predictions exhibit skewed probability distributions; uncertain predictions approximate uniform distributions.
- **Evidence anchors:** [abstract] "uniform distribution check to confirm confidence in the generated facts"; [section 3.2] "if the LLM is confident in regenerating a fact correctly, the probability distribution of the generated tokens will be skewed... if the LLM is uncertain... their values will be closer to those of alternative tokens (closer to a uniform distribution)".

### Mechanism 3
- **Claim:** Beam decoding improves fact regeneration quality over greedy decoding.
- **Mechanism:** Beam search (beam_size=5) explores multiple answer paths → selects highest cumulative probability sequence → reduces factual errors in regeneration compared to greedy selection.
- **Core assumption:** Higher cumulative probability sequences correlate with factual accuracy in regeneration.
- **Evidence anchors:** [section 5.4.2] "Beam decoding improves the detection of hallucinations during fact regeneration compared to greedy search... maintains multiple candidate generations, reduces the likelihood of factual errors".

## Foundational Learning

- **Concept:** Named Entity Recognition (NER) and Part-of-Speech (POS) Tagging
  - **Why needed here:** Core mechanism for identifying "key facts" in generated text. Without accurate entity/tag extraction, the pipeline cannot generate targeted questions.
  - **Quick check question:** Given "Argentina won the World Cup in 2022," which tokens would NER extract as key facts? (Answer: "Argentina," "World Cup," "2022")

- **Concept:** Kolmogorov-Smirnov (KS) Test
  - **Why needed here:** Statistical test used to determine if token probability distribution deviates from uniform. Critical for confidence-based hallucination classification.
  - **Quick check question:** If top-5 token probabilities are [0.6, 0.2, 0.1, 0.05, 0.05] vs. [0.22, 0.20, 0.20, 0.19, 0.19], which would KS test more likely reject as non-uniform? (Answer: First distribution—more skewed)

- **Concept:** LLM-as-a-Judge Paradigm
  - **Why needed here:** Used for comparing original vs. regenerated facts. Enables semantic matching beyond lexical overlap.
  - **Quick check question:** Why use a judge LLM instead of exact string matching? (Answer: Handles synonyms, abbreviations, partial names—e.g., "UCLA" vs. "University of California, Los Angeles")

## Architecture Onboarding

- **Component map:** Input preprocessing: Coreference resolution → sentence splitting → Key fact extraction: NER/POS tagging (Stanza library) → Question generation: T5-based fine-tuned model → Fact regeneration: Target LLM (or cross-evaluation LLM) at temperature 0 → Fact alignment check: Judge LLM (GPT-4.1-mini) for pairwise comparison → Uniform distribution check: Token probability extraction → KS test → Score aggregation: Average hallucination scores per sentence

- **Critical path:** Key fact extraction → question generation → fact regeneration → alignment check → distribution check. The distribution check only applies to facts that passed alignment (score=0), recategorizing uncertain ones as hallucinated.

- **Design tradeoffs:** NER vs. POS tagging: NER slightly outperforms POS in more settings; Beam vs. greedy decoding: Beam better accuracy; greedy faster; LLM judge vs. F1-score matching: LLM judge more semantically robust; F1-score enables fully offline operation; Self-evaluation vs. cross-evaluation: Cross-evaluation with different LLMs viable.

- **Failure signatures:** Over-flagging: Correct answers with one hallucinated fact may cause entire paragraph flagged as hallucinated; Ambiguous questions: "Who was the building named after?" without context may cause false positives; Missing token probabilities: API-based LLMs without probability access cannot use uniform distribution check; Low-resource languages: Dependence on NER/POS tools limits applicability.

- **First 3 experiments:** Reproduce baseline comparison: Run CONFACTCHECK vs. SelfCheckGPT on NQ_Open subset using LLaMA3.1-8B-Instruct; verify AUC-PR ~0.73 and inference time ~9.5s; Ablate uniform distribution check: Run fact alignment alone vs. full pipeline; expect 1-18% degradation without distribution check; Test cross-evaluation: Apply Qwen2.5-7B as evaluator on LLaMA3.1 outputs on NQOpen; expect AUC-PR ~0.71 vs. self-evaluation ~0.73.

## Open Questions the Paper Calls Out

- **Question:** How can the uniform distribution check be adapted for black-box LLM APIs that restrict access to output token probabilities?
- **Question:** Can the key fact extraction step be replaced by a language-agnostic method to support low-resource languages lacking robust NER/POS tools?
- **Question:** How can the discrepancy between strict fact-level alignment and human judgment regarding "minor" hallucinations be resolved?
- **Question:** Does replacing the T5-based question generator with a larger LLM improve accuracy enough to justify the computational overhead?

## Limitations

- **Token probability dependency:** The uniform distribution check requires access to LLM token probabilities, which many commercial APIs do not provide, limiting applicability in constrained environments.
- **NER tool dependence:** The method's performance relies on the accuracy of external NER tools (Stanza), which may underperform on low-resource languages or domains with specialized entities.
- **False positive sensitivity:** The method may over-flag correct answers containing a single hallucinated fact, potentially lowering accuracy on evaluation datasets where humans accept small factual slips.

## Confidence

**High Confidence:**
- Fact alignment check through question generation and comparison reliably detects inconsistent facts
- Beam decoding improves fact regeneration quality over greedy decoding
- LLM-as-judge provides more robust semantic matching than exact string matching

**Medium Confidence:**
- NER-based key fact extraction outperforms POS-based alternatives
- Self-evaluation with same LLM performs comparably to cross-evaluation
- The method achieves higher accuracy than self-checking baselines while using fewer LLM calls

**Low Confidence:**
- The Kolmogorov-Smirnov test reliably distinguishes confident from uncertain predictions
- The approach generalizes well to constrained environments (token probability requirement contradicts this claim)

## Next Checks

1. **API compatibility test:** Implement the pipeline with both an open-source LLM providing token probabilities (e.g., LLaMA) and a commercial API without probabilities (e.g., GPT-4 via API). Measure the performance drop and assess whether the fact alignment check alone provides sufficient accuracy for constrained environments.

2. **NER error propagation analysis:** Systematically corrupt the NER output with synthetic errors (e.g., drop 10%, 20%, 30% of entities) and measure the impact on hallucination detection accuracy. This would quantify the method's robustness to NER tool limitations.

3. **Multi-entity fact verification:** Design test cases with complex facts involving multiple entities (e.g., "Company X acquired Company Y for $Z in year W") and evaluate whether the current single-token approach maintains accuracy or requires modification for joint fact verification.