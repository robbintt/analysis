---
ver: rpa2
title: 'Explainable Artificial Intelligence for Economic Time Series: A Comprehensive
  Review and a Systematic Taxonomy of Methods and Concepts'
arxiv_id: '2512.12506'
source_url: https://arxiv.org/abs/2512.12506
tags:
- shap
- economic
- time
- causal
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey reviews and organizes the growing literature on Explainable
  Artificial Intelligence (XAI) for economic time series, addressing challenges such
  as autocorrelation, non-stationarity, seasonality, mixed frequencies, and regime
  shifts that can make standard explanation techniques unreliable or economically
  implausible. The authors propose a taxonomy that classifies methods by explanation
  mechanism (propagation-based, perturbation and game-theoretic attribution, function-based
  global tools) and time-series compatibility (preservation of temporal dependence,
  stability over time, respect for data-generating constraints).
---

# Explainable Artificial Intelligence for Economic Time Series: A Comprehensive Review and a Systematic Taxonomy of Methods and Concepts

## Quick Facts
- arXiv ID: 2512.12506
- Source URL: https://arxiv.org/abs/2512.12506
- Reference count: 0
- Primary result: A taxonomy classifying XAI methods for economic time series by explanation mechanism and time-series compatibility, with emphasis on adaptations like Vector SHAP and WindowSHAP that preserve temporal dependence and reduce computational cost.

## Executive Summary
This survey systematically reviews and organizes the growing literature on Explainable Artificial Intelligence (XAI) for economic time series, addressing challenges such as autocorrelation, non-stationarity, seasonality, mixed frequencies, and regime shifts. The authors propose a comprehensive taxonomy that classifies methods by their explanation mechanism (propagation-based, perturbation and game-theoretic attribution, function-based global tools) and time-series compatibility (preservation of temporal dependence, stability over time, respect for data-generating constraints). The review highlights the need for time-series-specific adaptations to avoid economically implausible explanations and connects explainability to causal inference and policy analysis through interventional attributions and constrained counterfactual reasoning.

## Method Summary
The survey synthesizes existing XAI literature and organizes it into a systematic taxonomy for economic time series. It covers propagation-based methods (e.g., LRP, Integrated Gradients), perturbation-based methods (e.g., SHAP, LIME, ALE), and their time-series adaptations (e.g., Vector SHAP, WindowSHAP). The authors propose a taxonomy based on explanation mechanism and time-series compatibility, emphasizing methods that preserve temporal dependence, stability, and data-generating constraints. The review also discusses intrinsically interpretable architectures (notably attention-based transformers) and provides guidance for decision-grade applications such as nowcasting, stress testing, and regime monitoring, emphasizing attribution uncertainty and explanation dynamics as indicators of structural change.

## Key Results
- The survey proposes a taxonomy that classifies XAI methods by explanation mechanism and time-series compatibility, addressing challenges unique to economic time series.
- Time-series-specific adaptations such as Vector SHAP and WindowSHAP are highlighted for reducing lag fragmentation and computational cost while improving interpretability.
- The review connects explainability to causal inference and policy analysis through interventional attributions (Causal Shapley values) and constrained counterfactual reasoning, and emphasizes attribution uncertainty as an indicator of structural change.

## Why This Works (Mechanism)
The proposed taxonomy works by explicitly addressing the unique challenges of economic time series—autocorrelation, non-stationarity, and regime shifts—that can make standard explanation techniques unreliable or economically implausible. By classifying methods based on both their explanation mechanism and their compatibility with time-series data, the framework ensures that explanations preserve temporal dependence, remain stable over time, and respect data-generating constraints. This approach prevents the destruction of meaningful temporal relationships and reduces the risk of generating economically nonsensical attributions, making explanations more actionable for policy and decision-making.

## Foundational Learning
- **Vector SHAP**: Groups all lags of each variable as a single "player" in Shapley computation to reduce lag fragmentation and computational cost. Needed because standard SHAP treats each lag as a separate feature, destroying temporal structure. Quick check: Compare attribution quality and runtime vs. standard SHAP on a macroeconomic dataset.
- **WindowSHAP**: Adapts SHAP to handle sliding windows in time-series data, preserving local temporal context. Needed to maintain coherence in explanations across overlapping time windows. Quick check: Verify that explanations are consistent across adjacent windows.
- **Causal Shapley Values**: Extends Shapley values to incorporate causal relationships via a causal graph, enabling interventional attributions. Needed because standard attributions may not reflect true causal effects in economic systems. Quick check: Validate attributions against known economic theory or expert knowledge.
- **Attention-based Transformers**: Intrinsically interpretable architectures that can provide feature importance via attention weights. Needed as an alternative to post-hoc explanations that may be unreliable. Quick check: Compare attention-based explanations with post-hoc methods for coherence and plausibility.
- **Block Bootstrapping for Uncertainty**: Estimates confidence intervals for attributions by resampling blocks of time series data. Needed to distinguish genuine economic signals from estimation noise. Quick check: Identify whether intervals cross zero during stable vs. volatile periods.

## Architecture Onboarding
- **Component Map**: Time-series model (LSTM/Transformer) -> Feature attribution method (SHAP/Vector SHAP/WindowSHAP) -> Explanation evaluation (fidelity, temporal coherence, economic plausibility) -> Uncertainty quantification (block bootstrapping)
- **Critical Path**: Train model on macroeconomic data → Apply attribution method → Evaluate explanation quality and economic plausibility → Quantify uncertainty → Flag fragile explanations
- **Design Tradeoffs**: Standard SHAP is widely applicable but computationally expensive and destroys temporal structure; Vector SHAP preserves structure but requires custom implementation; attention-based models are interpretable but may be less flexible; uncertainty quantification improves reliability but increases computation.
- **Failure Signatures**: Standard permutation/Shapley destroys autocorrelation, producing economically nonsensical attributions; exponential compute cost with many lags; counterfactuals generate "adversarial" scenarios violating economic constraints.
- **3 First Experiments**:
  1. Train an LSTM on FRED-MD data and apply standard SHAP to obtain per-timestep feature attributions.
  2. Implement Vector SHAP by grouping all lags of each variable and compare attribution quality and runtime vs. standard SHAP.
  3. Apply block bootstrapping to standard SHAP attributions to quantify uncertainty and flag fragile explanations during regime shifts.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can causal explanation methods (e.g., Causal Shapley Values) be rigorously applied when the underlying causal DAG is unknown or only partially specified from observational economic data?
- Basis in paper: Section 5.2 states that implementing Causal SHAP requires a causal graph, noting that "When the causal graph is unknown, causal discovery algorithms or expert knowledge (priors) are necessary," but does not offer a standard solution.
- Why unresolved: Causal discovery from observational time series is ill-posed without strong assumptions, and economic systems rarely have fully agreed-upon causal structures.
- What evidence would resolve it: A validated framework for deriving reliable causal explanations from partial graph structures or data-driven discovery that matches established economic theory.

### Open Question 2
- Question: How can counterfactual explanation methods be mathematically constrained to respect historical correlation structures to prevent generating "adversarial" scenarios that are mathematically valid but economically absurd?
- Basis in paper: Section 6.1 warns that pure ML counterfactuals risk generating "adversarial examples" (e.g., high inflation with zero interest rates) and calls for "impulse-constrained" or "manifold-constrained" counterfactuals.
- Why unresolved: Standard ML optimization prioritizes mathematical proximity to the decision boundary rather than economic plausibility or structural coherence.
- What evidence would resolve it: A counterfactual generator that enforces economic sign constraints and impulse-response patterns while maintaining model-agnostic flexibility.

### Open Question 3
- Question: How can uncertainty quantification for feature attributions (e.g., confidence intervals for SHAP values) be standardized to distinguish genuine economic signals from estimation noise in real-time nowcasting?
- Basis in paper: Section 9 argues that "Point estimates of feature importance are insufficient" and suggests methods like block bootstrapping, implying the need to flag fragile explanations where intervals cross zero.
- Why unresolved: Computing confidence intervals for attributions on complex deep learning models is computationally intensive and lacks a unified theoretical framework for time-series dependencies.
- What evidence would resolve it: A computationally efficient method providing calibrated uncertainty bounds for attributions that effectively signals model fragility during structural breaks.

## Limitations
- The review lacks empirical benchmarks or reproducible code for the proposed Vector SHAP and WindowSHAP adaptations, limiting independent verification of claimed improvements.
- No specific datasets or hyperparameter details are provided, restricting direct replication of experiments.
- Many claims about method performance and economic plausibility remain untested in practice, as the survey focuses on conceptual synthesis rather than empirical validation.

## Confidence
- Confidence in the classification framework: High (synthesizes established XAI taxonomies with economic peculiarities)
- Confidence in the treatment of intrinsically interpretable models and attention mechanisms: Medium (relies on secondary literature rather than original experiments)
- Confidence in the integration of causal and counterfactual explainability: Medium (discusses concepts without validating them empirically on real data)

## Next Checks
1. Implement and benchmark Vector SHAP and WindowSHAP on a standard macroeconomic dataset (e.g., FRED-MD) and compare against standard SHAP in terms of attribution fidelity, temporal coherence, and runtime.
2. Validate that time-series-specific adaptations (e.g., Vector SHAP) preserve autocorrelation and avoid generating economically implausible counterfactuals.
3. Evaluate the robustness of explainability outputs under regime shifts by testing whether attribution uncertainty or dynamics serve as reliable indicators of structural change.