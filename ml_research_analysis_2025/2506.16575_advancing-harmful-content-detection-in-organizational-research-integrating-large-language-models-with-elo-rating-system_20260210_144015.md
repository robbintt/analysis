---
ver: rpa2
title: 'Advancing Harmful Content Detection in Organizational Research: Integrating
  Large Language Models with Elo Rating System'
arxiv_id: '2506.16575'
source_url: https://arxiv.org/abs/2506.16575
tags:
- content
- harmful
- language
- large
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an Elo rating-based method to enhance large
  language models (LLMs) for detecting harmful content like microaggressions and hate
  speech. Traditional LLMs often underperform on such tasks due to built-in moderation
  systems that over-censor or refuse requests.
---

# Advancing Harmful Content Detection in Organizational Research: Integrating Large Language Models with Elo Rating System

## Quick Facts
- **arXiv ID**: 2506.16575
- **Source URL**: https://arxiv.org/abs/2506.16575
- **Reference count**: 0
- **Primary result**: Introduces Elo rating-based LLM method that outperforms traditional ML and standard LLM prompting on harmful content detection tasks.

## Executive Summary
This paper addresses the challenge of detecting harmful content like microaggressions and hate speech using large language models (LLMs), which often struggle due to over-censorship from built-in moderation systems. The proposed solution integrates the Elo rating system with LLMs through pairwise comparisons, where an LLM ranks texts by harmfulness and the Elo algorithm converts these into probability-based scores. Tested on two datasets, this approach demonstrates superior accuracy, precision, recall, F1 scores, and ROC AUC compared to conventional machine learning models and standard LLM prompting methods.

## Method Summary
The method employs an Elo rating-based framework where LLMs perform pairwise comparisons of texts to identify which exhibits more harmful content, outputting only binary choices ("A" or "B") to avoid moderation triggers. These comparisons are aggregated through the Elo algorithm to generate dynamic ratings for each text, which are then converted to probability scores using a logistic transformation. This comparative approach bypasses the need for labeled training data and reduces false positives while maintaining scalability for organizational and social science research applications.

## Key Results
- Outperformed conventional ML models and standard LLM prompting on accuracy, precision, recall, F1 scores, and ROC AUC
- Demonstrated superior performance on two datasets for detecting microaggressions and hate speech
- Reduced false positives and improved reliability while offering data-efficient solution for nuanced harmful content detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pairwise comparative framing reduces LLM content moderation trigger rates.
- Mechanism: The method bypasses direct classification or generation of harmful content, which is more likely to trigger LLM refusal or sanitization. Instead, it asks the LLM to perform a simpler, relative task: given two texts, return a binary token ("A" or "B") indicating which exhibits more harmful content. This abstracts the query, minimizes the output, and reduces the perceived severity by the moderation system.
- Core assumption: LLM content moderation is more sensitive to explicit analysis or generation requests than to indirect, comparative queries with minimal output.
- Evidence anchors:
  - [abstract] "Traditional LLMs often underperform on such tasks due to built-in moderation systems that over-censor or refuse requests."
  - [section 3.2.2] "For each entry pair (A, B), the LLM identifies the text exhibiting stronger harmful content, returning only 'A' or 'B' to avoid moderation triggers."
  - [corpus] Related work on commercial moderation APIs (Lost in Moderation, arXiv:2503.01623) documents over-moderation risks, but does not directly evaluate pairwise framing as a mitigation.

### Mechanism 2
- Claim: Elo rating transforms discrete pairwise comparisons into a continuous probabilistic score.
- Mechanism: The LLM outputs a binary preference for each text pair. The Elo algorithm aggregates these tournament-style comparisons. After many matchups, each text receives a dynamic Elo rating. A logistic transformation then converts this rating into a probability score (0-1), representing the relative likelihood of harmfulness. This provides a fine-grained, continuous measure rather than a hard binary label.
- Core assumption: The transitivity of harmfulness holds in practice (if A is more harmful than B, and B than C, then A is more harmful than C), and the number of comparisons is sufficient to stabilize scores.
- Evidence anchors:
  - [section 3.1] "The Elo score implements a probability model with a logistic transformation... P(A) = 1 / (1 + 10^((RB-RA)/Î²))."
  - [section 3.2.3] "We recalibrate Elo ratings based on LLM-derived comparisons and convert ratings to probabilities... normalized Elo rating ranges from 0 to 1, reflecting relative harmfulness."
  - [corpus] No direct corpus evidence for Elo in harmful speech detection; related work focuses on other detection approaches.

### Mechanism 3
- Claim: Relative ranking is more data-efficient for nuanced, low-resource tasks than traditional supervised ML.
- Mechanism: Traditional supervised models require large labeled datasets to learn features of harmful content. The Elo-LLM method requires no labeled training data. The LLM's pre-trained semantic understanding allows it to make meaningful pairwise comparisons immediately, making the approach suitable for small-sample contexts and subtle forms of harm (e.g., microaggressions).
- Core assumption: The pre-trained LLM has sufficient embedded knowledge to reliably distinguish degrees of harmfulness in a comparative task without task-specific fine-tuning.
- Evidence anchors:
  - [abstract] "It... offers a data-efficient solution for nuanced harmful content detection."
  - [section 2.1] "Traditional supervised ML approaches are often ill-suited... require large, labeled datasets... struggle with semantic complexities."
  - [corpus] The corpus paper "Towards Generalizable Generic Harmful Speech Datasets for Implicit Hate Speech Detection" (arXiv:2506.16476) notes difficulty in generalizing detection for veiled hate speech, supporting the need for alternative methods, but does not test Elo-LLM directly.

## Foundational Learning

- **Concept: LLM Content Moderation and Over-Refusal**
  - Why needed here: The core problem is that standard LLM prompting fails due to built-in moderation systems.
  - Quick check question: What are two ways LLM moderation systems interfere with harmful content analysis tasks?

- **Concept: The Elo Rating System**
  - Why needed here: This is the mathematical engine that converts raw comparisons into a usable score.
  - Quick check question: In the Elo update formula `R'A = RA + K(SA - P(A))`, what do the terms `SA` and `P(A)` represent?

- **Concept: Supervised vs. Comparative Classification Paradigms**
  - Why needed here: The paper's value proposition is framed against the limitations of traditional supervised machine learning.
  - Quick check question: Why would a traditional supervised model struggle to detect a new, emerging form of online slang used in hate speech?

## Architecture Onboarding

- **Component map:** Input Corpus -> Tournament Constructor -> LLM Comparator -> Elo Engine -> Probability Converter

- **Critical path:**
  1. Define the text corpus (N texts)
  2. Generate pairings using BIBD (each text compared to m random others)
  3. **Iterate:** For each pair, query LLM -> get binary choice -> update both texts' Elo scores
  4. After all comparisons, transform final Elo scores into probabilities
  5. (Optional) Apply a threshold to probabilities for discrete classification

- **Design tradeoffs:**
  - **Comparison budget (m):** A higher value yields more stable scores but increases API cost and time linearly. The paper does not specify the optimal m.
  - **LLM selection:** A more capable model (e.g., GPT-4) may give more reliable judgments than a smaller one (e.g., GPT-4o-mini, used here), but at higher cost.
  - **Incomplete vs. Complete Design:** A full round-robin (N*(N-1)/2 comparisons) is most accurate but computationally infeasible for large N. The BIBD is a necessary approximation.

- **Failure signatures:**
  - **High API Refusal Rate:** LLM consistently refuses comparative prompts, indicating moderation triggers have not been avoided.
  - **Ranking Instability:** Low correlation between Elo scores from different random tournament seeds, suggesting LLM judgments are inconsistent or m is too low.
  - **Score Convergence:** All texts converge to a similar Elo score, indicating the LLM fails to discriminate harmfulness.

- **First 3 experiments:**
  1. **Pipeline Validation:** Run the full pipeline on a small (n=50-100) hand-labeled validation set. Calculate the ROC-AUC of the final probability scores against the ground truth labels to ensure the method works end-to-end.
  2. **Comparison Budget Ablation:** Run the pipeline on the same dataset with varying `m` (e.g., m=3, 5, 10, 20). Plot the stability (variance across runs) and accuracy against `m` to find a cost-performance sweet spot.
  3. **Prompt Sensitivity Test:** Test multiple phrasings of the comparative prompt (e.g., "Which is more toxic?", "Which is more harmful?") to measure their impact on both LLM refusal rates and classification accuracy. Select the most robust version.

## Open Questions the Paper Calls Out
None

## Limitations
- **Content Moderation Bypass Generalization**: The underlying assumption that pairwise comparative framing consistently avoids moderation triggers remains unverified across different LLM providers and model versions.
- **Elo Algorithm Convergence**: The paper does not specify the optimal number of comparisons (m) per text or analyze convergence properties.
- **Semantic Transitivity Assumption**: The Elo method assumes transitivity of harmfulness judgments, but harmful content often contains subtle, context-dependent nuances where this assumption may break down.

## Confidence
**High Confidence**: The core mathematical framework (Elo rating system for pairwise comparisons) is well-established and correctly applied. The methodology for converting Elo scores to probabilities is sound.

**Medium Confidence**: The empirical results showing improved performance over baseline methods are based on two datasets. While promising, this sample size is limited, and the datasets' representativeness for broader organizational research contexts is uncertain.

**Low Confidence**: The claim that this method is universally data-efficient and scalable lacks empirical validation. The computational cost of O(N*m) API calls and the potential for high variability in LLM judgments for ambiguous cases are not fully characterized.

## Next Checks
1. **Robustness Across Moderation Systems**: Test the method against multiple LLM providers (OpenAI, Anthropic, Google) and different model versions to verify the comparative framing consistently reduces refusal rates.

2. **Convergence Analysis**: Conduct experiments to determine the minimum number of comparisons (m) required for stable Elo scores, and analyze score variance across multiple random tournament seeds.

3. **Transitivity Validation**: Design experiments to test the transitivity of LLM harmfulness judgments, identifying cases where the assumption breaks down and quantifying the impact on final rankings.