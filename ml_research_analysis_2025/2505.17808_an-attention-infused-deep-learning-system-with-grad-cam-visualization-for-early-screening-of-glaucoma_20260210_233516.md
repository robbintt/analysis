---
ver: rpa2
title: An Attention Infused Deep Learning System with Grad-CAM Visualization for Early
  Screening of Glaucoma
arxiv_id: '2505.17808'
source_url: https://arxiv.org/abs/2505.17808
tags:
- glaucoma
- images
- dataset
- optic
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This research introduces a hybrid deep learning model combining\
  \ EfficientNet-B0 CNN and Vision Transformer (ViT) architectures through a cross-attention\
  \ mechanism for glaucoma detection from fundus images. The model leverages the strengths\
  \ of both architectures\u2014CNN's strong feature extraction capabilities and ViT's\
  \ global context modeling\u2014to achieve improved performance in detecting glaucomatous\
  \ changes."
---

# An Attention Infused Deep Learning System with Grad-CAM Visualization for Early Screening of Glaucoma

## Quick Facts
- arXiv ID: 2505.17808
- Source URL: https://arxiv.org/abs/2505.17808
- Authors: Ramanathan Swaminathan
- Reference count: 24
- Achieved 94.79% accuracy on glaucoma detection using hybrid CNN-ViT with cross-attention

## Executive Summary
This research introduces a hybrid deep learning model combining EfficientNet-B0 CNN and Vision Transformer architectures through a cross-attention mechanism for glaucoma detection from fundus images. The model leverages the strengths of both architectures—CNN's strong feature extraction capabilities and ViT's global context modeling—to achieve improved performance in detecting glaucomatous changes. The approach was evaluated on two datasets (ACRIMA and Drishti), achieving 94.79% accuracy, outperforming standalone CNN (85.98%) and ViT (86.72%) models. The cross-attention mechanism enables the model to dynamically prioritize clinically relevant regions in fundus images, improving interpretability through Grad-CAM visualizations. This work advances automated glaucoma screening systems, particularly for resource-constrained environments, by providing a robust, explainable AI solution for early detection.

## Method Summary
The proposed method combines EfficientNet-B0 CNN and Vision Transformer architectures through a cross-attention mechanism for binary classification of glaucoma versus normal fundus images. The model processes 224×224 pixel images that are normalized and augmented with color jitter, random flips, affine transformations, Gaussian blur, and random crops. Features are extracted in parallel from both EfficientNet-B0 and ViT streams, then fused through bidirectional cross-attention before classification. The model is trained with binary cross-entropy loss, Adam optimizer (learning rate 5e-4), cosine annealing learning rate scheduler, gradient clipping, and dropout. Training is performed on a combined dataset of 814 images (618 training, 196 testing) from ACRIMA and Drishti repositories.

## Key Results
- Achieved 94.79% accuracy on combined ACRIMA+Drishti test set
- Cross-attention fusion outperformed standalone CNN (85.98%), ViT (86.72%), concatenation (91.67%), and self-attention (87.62%) variants
- Grad-CAM visualizations showed attention focused on clinically relevant optic disc and cup regions
- Model demonstrated robustness across two independent datasets with different acquisition conditions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Cross-attention fusion of CNN and ViT features improves glaucoma detection accuracy over standalone architectures.
- **Mechanism:** The cross-attention module receives features from both EfficientNet-B0 (CNN) and ViT streams, performing adaptive feature weighing through bidirectional exchange. This creates a fused representation where CNN-extracted local features (optic disc boundaries, neuroretinal rim) are dynamically combined with ViT-extracted global context. The attention weights learn which modality's features are more relevant for each spatial region.
- **Core assumption:** CNN and ViT architectures encode complementary information—CNNs capture fine-grained local spatial patterns while ViTs capture long-range dependencies—that, when properly fused, provide superior discriminative power for subtle pathological changes.
- **Evidence anchors:**
  - [abstract] "The Cross-Attention mechanism facilitates the model in learning regions in the fundus that are clinically relevant through bidirectional feature exchange between CNN and ViT streams."
  - [section IV-B] Ablation shows cross-attention (94.79%) outperforms self-attention (87.62%), concatenation (91.67%), CNN-only (85.98%), and ViT-only (86.72%)
  - [corpus] Neighboring paper "Geoinformatics-Guided Machine Learning for Power Plant Classification" similarly integrates CNN+ViT with GIS, suggesting the fusion pattern transfers across domains, though specific mechanism evidence for cross-attention superiority in medical imaging is limited in corpus
- **Break condition:** If CNN and ViT extract redundant features rather than complementary ones, cross-attention would provide diminishing returns over simpler concatenation. The 3.12% gap between cross-attention (94.79%) and concatenation (91.67%) suggests partial redundancy exists.

### Mechanism 2
- **Claim:** EfficientNet-B0 provides effective local feature extraction for subtle fundus pathologies despite limited training data.
- **Mechanism:** EfficientNet-B0 uses compound scaling (depth, width, resolution) optimized for efficiency. Its convolutional inductive biases—local receptive fields, translation equivariance, hierarchical feature learning—align with the spatial nature of glaucoma markers (optic disc enlargement, cup-to-disc ratio, nerve fiber thinning). Pretrained ImageNet weights provide transfer learning for feature initialization.
- **Core assumption:** Local convolutional features sufficient for detecting glaucoma markers are learnable from limited fundus images (618 training samples) via transfer learning, and ImageNet pretraining transfers meaningfully to medical imaging.
- **Evidence anchors:**
  - [section III-D] "EfficientNet-B0 excels in detecting all these...nuanced and subtle characteristics...[and] outperforms most models when dealing with class imbalances and limited real-time labeled data"
  - [section III-D] Paper acknowledges "CNNs have strong, ready-made rules on prioritization of the pixels...inductive biases that need to be neutralized"
  - [corpus] "MRI-Based Brain Tumor Detection" paper uses EfficientNetV2 with attention, supporting EfficientNet family's applicability in medical imaging, though direct evidence for B0 specifically is absent
- **Break condition:** If the domain gap between ImageNet natural images and fundus images is too large, pretrained weights may hinder rather than help. The 85.98% standalone CNN performance suggests reasonable but imperfect transfer.

### Mechanism 3
- **Claim:** Grad-CAM visualization enables clinically interpretable attention localization to optic disc and cup regions.
- **Mechanism:** Grad-CAM computes gradients flowing into final convolutional features to produce class-differentiating localization maps. For glaucoma classification, gradients highlight regions that most influence the "glaucoma" prediction. The cross-attention architecture means gradients propagate through both CNN and ViT pathways, creating fused attention maps.
- **Core assumption:** Model attention correlates with clinically relevant pathological regions rather than spurious correlations (image artifacts, illumination variations).
- **Evidence anchors:**
  - [section IV-C] "Mainly in figure 5 and 6 it is seen that the optic disc along with the optic cup has enlarged...The neuroretinal rim has undergone excessive thinning"
  - [section IV-C] For healthy eyes: "The heat map seems to be less highlighted...healthy optic disc and cup magnitudes. The ratio between the disc and cup is within acceptable limits"
  - [corpus] "A Clinically Interpretable Deep CNN Framework for Early Chronic Kidney Disease" uses Grad-CAM for explainability, as does "PCA- and SVM-Grad-CAM for CNNs," suggesting this is a broadly applicable interpretability pattern
- **Break condition:** If attention maps highlight non-pathological regions (blood vessels, image boundaries) or fail to distinguish glaucoma from normal eyes visually, interpretability claims would be undermined. Paper shows qualitative alignment but lacks quantitative attention-region validation.

## Foundational Learning

- **Concept: Cross-Attention Mechanism**
  - **Why needed here:** Understanding how query, key, value projections enable one modality to attend to another is essential for debugging the fusion module and interpreting why it outperforms self-attention.
  - **Quick check question:** Given CNN features F_cnn and ViT features F_vit, which stream provides the query and which provides key/value in cross-attention? (Assumption: Paper doesn't specify exact directionality—implementation may use bidirectional or single-direction attention.)

- **Concept: Vision Transformer Patch Embedding**
  - **Why needed here:** ViT processes images as sequences of patches. Understanding patch size, position embeddings, and the class token is necessary for architectural modifications and debugging dimension mismatches.
  - **Quick check question:** If input fundus images are 224×224 and patch size is 16×16, how many patch tokens does the ViT encoder receive (excluding class token)?

- **Concept: Transfer Learning Inductive Biases**
  - **Why needed here:** The hybrid architecture explicitly balances CNN's strong inductive biases (locality, translation equivariance) against ViT's weaker biases but greater flexibility. Understanding this trade-off explains why fusion works better than either alone.
  - **Quick check question:** Why might a CNN pretrained on ImageNet struggle with fundus images compared to natural images, and how does ViT integration potentially mitigate this?

## Architecture Onboarding

- **Component map:**
  Input (224×224×3 fundus image)
      ├──→ EfficientNet-B0 backbone → CNN features
      │                                    │
      └──→ ViT patch encoder → ViT features
                                          │
                              Cross-Attention Module
                                          │
                              Fused features (concatenated/weighted)
                                          │
                              Fully connected layer (dimensionality reduction)
                                          │
                              Binary classifier (sigmoid) → Glaucoma/Normal

- **Critical path:**
  1. Image preprocessing: Resize to 224×224, normalize pixels [0,1], apply augmentations (color jitter, flip, affine, Gaussian blur, random crop)
  2. Parallel feature extraction through CNN and ViT branches
  3. Cross-attention fusion: Bidirectional feature exchange (exact implementation not fully specified—assumption: multi-head attention with learnable projections)
  4. Classification head: Dense layer → sigmoid activation → binary cross-entropy loss

- **Design tradeoffs:**
  - **Memory vs. batch size:** 18GB GPU constraint forced batch size of 16; smaller batches increase gradient noise but reduce memory pressure
  - **Model capacity vs. overfitting:** 50 epochs achieved 98-99% training accuracy but testing accuracy fluctuated (94.8% peak), with testing loss higher and unstable at epochs 20-35—suggests mild overfitting despite augmentation and dropout
  - **Architecture complexity vs. interpretability:** Cross-attention adds ~3.6% accuracy over simple concatenation but makes attention attribution more complex to trace

- **Failure signatures:**
  - **Testing loss fluctuations (epochs 20-35):** Indicates overfitting to training distribution; outliers in small dataset may cause instability
  - **Class imbalance (470 glaucoma vs. 344 normal):** May bias model toward glaucoma predictions; monitor precision/recall balance per class
  - **Domain shift between ACRIMA (Spain) and Drishti (India):** If deployment population differs, performance may degrade

- **First 3 experiments:**
  1. **Reproduce ablation study:** Train CNN-only, ViT-only, concatenation, self-attention, and cross-attention variants on the combined ACRIMA+Drishti split to validate the 85.98% → 94.79% improvement trajectory
  2. **Cross-dataset validation:** Train on ACRIMA alone, test on Drishti (and vice versa) to assess generalization and identify dataset-specific overfitting
  3. **Attention map validation:** Quantitatively measure Grad-CAM overlap with clinically annotated optic disc/cup regions (if segmentation masks available) to move beyond qualitative assessment

## Open Questions the Paper Calls Out
- **Can the proposed hybrid architecture maintain high accuracy and low latency when deployed on cloud platforms for real-time inference?**
  - Basis in paper: [explicit] The conclusion states the work is anticipated to be deployed on open-source cloud platforms (AWS, Google Cloud) where "real-time data can be tested."
  - Why unresolved: The current evaluation is limited to offline validation on a local machine (Apple Silicon M3 Pro), and cloud deployment introduces network and compute latency variables not yet tested.
  - What evidence would resolve it: Benchmarking results showing inference time and throughput (FPS) when the model is served via a cloud API with concurrent requests.

- **Is "real-time training and improvisation" feasible without catastrophic forgetting or significant computational overhead?**
  - Basis in paper: [explicit] The conclusion explicitly sets a future goal to include "an option for real-time training and improvisation of the model."
  - Why unresolved: The current methodology relies on a static training protocol (50 epochs), and continuous learning on new data streams in a production environment remains unimplemented.
  - What evidence would resolve it: Demonstration of an online learning pipeline that updates model weights with new fundus images while maintaining validation accuracy on the original ACRIMA/Drishti datasets.

- **Does the cross-attention mechanism effectively mitigate overfitting in larger, multi-source clinical datasets?**
  - Basis in paper: [inferred] The results section acknowledges "mild overfitting" and fluctuating testing loss despite the use of cross-attention, suggesting the model may struggle to generalize if the dataset size or complexity increases significantly.
  - Why unresolved: The study utilizes a relatively small combined dataset (814 images); it is unclear if the attention mechanism is robust enough to handle the noise of larger, real-world screening populations without overfitting.
  - What evidence would resolve it: Performance metrics (specifically the training vs. testing loss gap) on larger public datasets (e.g., ORIGA or G1020) or prospective clinical trial data.

## Limitations
- Cross-attention mechanism implementation details remain unspecified, limiting exact replication
- Grad-CAM validation relies on qualitative assessment without quantitative metrics correlating attention maps to clinical ground truth
- Performance on ACRIMA+Drishti combined may not generalize to other populations due to dataset-specific characteristics

## Confidence
- **High confidence**: The 94.79% accuracy result is supported by ablation showing cross-attention outperforms all variants (CNN-only 85.98%, ViT-only 86.72%, concatenation 91.67%, self-attention 87.62%)
- **Medium confidence**: The claim of clinically relevant attention localization is supported qualitatively but lacks quantitative validation
- **Low confidence**: Transferability claims to other fundus datasets or populations without cross-dataset validation

## Next Checks
1. Conduct cross-dataset evaluation by training on ACRIMA and testing on Drishti (and vice versa) to assess true generalization
2. Quantitatively measure Grad-CAM spatial alignment with clinically annotated optic disc/cup boundaries using metrics like Intersection-over-Union or localization error
3. Implement and compare alternative fusion strategies (attention-weighted averaging, gate-based fusion) against the current cross-attention to validate architectural choice