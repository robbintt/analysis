---
ver: rpa2
title: Continual learning for rotating machinery fault diagnosis with cross-domain
  environmental and operational variations
arxiv_id: '2504.10151'
source_url: https://arxiv.org/abs/2504.10151
tags:
- learning
- domains
- fault
- diagnosis
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of fault diagnosis in rotating
  machinery under varying environmental and operational conditions, where traditional
  machine learning models struggle to isolate faults from real-world variability.
  The proposed method introduces a continual deep learning approach that learns across
  domains sharing underlying structure over time, addressing catastrophic forgetting,
  lack of plasticity, forward transfer, and backward transfer.
---

# Continual learning for rotating machinery fault diagnosis with cross-domain environmental and operational variations

## Quick Facts
- arXiv ID: 2504.10151
- Source URL: https://arxiv.org/abs/2504.10151
- Authors: Diogo Risca; Afonso Lourenço; Goreti Marreiros
- Reference count: 40
- Primary result: Proposed continual deep learning method achieves up to 88.96% average domain accuracy on a multi-domain bearing fault dataset with forgetting measures as low as 2.70 × 10⁻³

## Executive Summary
This paper addresses fault diagnosis in rotating machinery under varying operational conditions where traditional machine learning models struggle to isolate faults from environmental variability. The proposed method introduces a continual deep learning approach that learns across domains sharing underlying structure over time, addressing catastrophic forgetting, lack of plasticity, forward transfer, and backward transfer. Experiments on a multi-domain fault bearing dataset show high average domain accuracy (up to 88.96%), with forgetting measures as low as 2.70 × 10⁻³ across non-stationary class-incremental environments.

## Method Summary
The approach uses a feature generator with domain-specific classifiers that grow incrementally, and an experience replay mechanism that selectively revisits prior domains based on highest prior errors. A shared CNN-based feature extractor processes Markov Transition Field (MTF) spectrograms of vibration signals, while new domain-specific heads are instantiated for each new operational domain. The system employs error-weighted replay sampling to prioritize difficult domains and ensemble probability aggregation for robust predictions.

## Key Results
- Average domain accuracy up to 88.96% across 18 distinct operational domains
- Forgetting measures as low as 2.70 × 10⁻³, demonstrating effective retention of prior knowledge
- Performance degrades under high noise (30%) but maintains >86% accuracy under mild noise (5%)

## Why This Works (Mechanism)

### Mechanism 1: Modular Capacity Expansion
Decoupling a shared feature extractor from isolated domain-specific classifiers allows the system to scale capacity incrementally without overwriting previously learned representations. A shared CNN-based feature generator extracts general signal features (e.g., vibration patterns), while new "heads" (classifiers) are instantiated for new operational domains. This isolates parameter updates, preventing gradient interference between distinct environments.

### Mechanism 2: Boosting-Inspired Experience Replay
Prioritizing the replay of training samples from domains with the highest historical error rates focuses plasticity on difficult transitions, improving backward transfer. Instead of uniform sampling, the system weights domains based on prior loss and samples mini-batches preferentially from these high-error domains, forcing the ensemble to refine "confused" decision boundaries rather than wasting capacity on already-mastered domains.

### Mechanism 3: Ensemble Probability Aggregation
Aggregating predictions from multiple domain-specific classifiers provides robustness against the "fragility" of single-domain experts. When predicting for domain i, the system averages the probability outputs of all classifiers that were trained on domain i, leveraging the diversity of the "committee" of experts to smooth out individual classification errors.

## Foundational Learning

- **Catastrophic Forgetting**
  - Why needed: This is the primary failure mode the paper solves. It occurs when updating weights for a new domain degrades performance on an old domain.
  - Quick check: If I train the model on Domain B, does accuracy on Domain A drop significantly?

- **Stability-Plasticity Trade-off**
  - Why needed: The architecture balances retaining old knowledge (stability via replay) with learning new conditions (plasticity via new classifier heads).
  - Quick check: Is the model rigid (fails new tasks) or volatile (forgets old tasks)?

- **Markov Transition Field (MTF)**
  - Why needed: The input mechanism. Understanding that raw time-series is converted to an image-like state transition map is critical for debugging the feature generator.
  - Quick check: Can I visualize the input MTF to see if distinct faults look different?

## Architecture Onboarding

- **Component map:** MTF Spectrogram -> 2-layer CNN (Feature Generator h) with Batch Normalization -> Modular Ensemble of domain-specific Fully Connected layers (Classifiers g_k) -> Replay Manager (tracks domain errors w_k,i and samples mini-batches)

- **Critical path:**
  1. Initialization: Train the Feature Generator h and first Classifier g_1 on Domain 1
  2. Increment: When Domain k arrives, instantiate a new head g_k
  3. Replay Selection: Calculate errors for previous domains, sample high-error domains
  4. Joint Update: Train h and g_k on current data, while freezing/updating previous g heads using replay data

- **Design tradeoffs:**
  - Buffer Size vs. Forgetting: Increasing replay buffer size increases average accuracy but may slightly increase forgetting if the buffer overwhelms new data distribution
  - Domain Order: Starting with "hardest" domains can improve foundation but might hurt initial stability; the "Alternated" strategy offered the best forgetting/accuracy balance

- **Failure signatures:**
  - Persistent Low Accuracy on Specific Domains (e.g., Domain 7 & 11): Identifies specific domains with ~75% accuracy regardless of tuning, suggesting inherent data quality issues
  - High Forgetting Measure (FM): If FM spikes, check if the Replay Buffer is too small or if the "boosting" weight is forcing the model to overfit to a noisy domain

- **First 3 experiments:**
  1. Baseline Sanity Check: Run the model on a sequence of 2 domains with 0% replay. Confirm catastrophic forgetting occurs (Domain 1 accuracy drops to near zero after training on Domain 2)
  2. Ablation on Replay: Compare "Random Replay" vs. "Error-Weighted Replay" (Boosting) on 5 domains. Verify if the boosting mechanism actually reduces the Forgetting Measure (FM)
  3. Noise Robustness: Inject mild (5%) vs. high (30%) corruption. Confirm the ensemble architecture maintains >86% average accuracy under mild noise, identifying the breakdown point under high noise

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed modular continual learning approach maintain low forgetting and high accuracy when validated through field trials using authentic, uncontrolled industrial data with confounding factors?
- Basis in paper: The authors state it is "crucial to acknowledge the limitations and the necessity for validation through field trials using authentic data, uncovering unexpected outcomes, and influence of confounding factors."
- Why unresolved: The current experiments rely on a multi-domain partition of a dataset which, while diverse, simulates domains rather than capturing the full stochastic nature of real-world operational environments.
- What evidence would resolve it: Performance metrics (ACC, FM) derived from deploying the model on physical rotating machinery in an active industrial setting, comparing simulated vs. real domain performance.

### Open Question 2
- Question: How does the proposed modular ensemble method compare to classic regularization and replay-based techniques specifically regarding long-term parameter efficiency and the trade-off between capacity growth and performance?
- Basis in paper: The authors note that "choosing between the classic regularization and replay-based methods... and the present methodology of decomposing concepts into reusable modules... needs to be further studied in fault diagnosis applications."
- Why unresolved: The paper focuses on validating the modular approach but does not provide a comparative analysis against non-modular regularization baselines in terms of computational overhead or parameter economy.
- What evidence would resolve it: A comparative study measuring parameter count and inference latency against standard regularization baselines (e.g., EWC, LwF) over an extended sequence of domains.

### Open Question 3
- Question: To what extent do specific architectural inductive biases (e.g., skip connections, normalization layers) and training regimes influence the stability-plasticity balance in the proposed fault diagnosis framework?
- Basis in paper: The conclusion suggests "leveraging on the inductive biases of different architectural components can also yield great benefits... [including] width, depth, normalization layers, skip connections... as well as training regimes."
- Why unresolved: The study utilizes a standard two-layer CNN architecture, leaving the impact of deeper or more complex architectural variants on the boosting-inspired replay mechanism unexplored.
- What evidence would resolve it: Ablation studies varying model depth, width, and normalization techniques to observe their specific effect on the Learning Accuracy (LA) and Forgetting Measure (FM).

## Limitations

- The paper does not specify critical training hyperparameters (learning rate, batch size, optimizer choice), making exact reproduction difficult
- The modular approach's scalability to hundreds of domains or entirely different machinery types remains untested
- The boosting-inspired replay strategy's effectiveness could vary significantly with domain similarity and dataset characteristics

## Confidence

- **High Confidence:** The catastrophic forgetting problem is well-established, and the modular architecture (shared feature extractor + domain-specific heads) is a valid solution. The reported forgetting measures and accuracy gains are consistent with the proposed mechanism.
- **Medium Confidence:** The boosting-inspired experience replay strategy is theoretically sound, but the paper's evidence is primarily empirical. The effectiveness of error-weighted sampling versus random sampling is demonstrated but could benefit from additional ablation studies across different dataset characteristics.
- **Medium Confidence:** The ensemble probability aggregation improves robustness, but the paper does not deeply analyze whether this comes from averaging complementary models or simply smoothing noise. The mechanism's benefits could vary significantly with domain similarity.

## Next Checks

1. **Hyperparameter Sensitivity Analysis:** Systematically vary the learning rate, replay buffer size, and boosting weight parameters across multiple random seeds to quantify their impact on both accuracy and forgetting measures.

2. **Cross-Domain Transfer Test:** Train the model on a subset of domains (e.g., 5 out of 18) and test its ability to generalize to entirely unseen operational conditions to validate the shared feature generator's ability to capture truly domain-agnostic representations.

3. **Noise Robustness Benchmark:** Compare the proposed method against a standard EWC or replay-based baseline on datasets with increasing levels of label noise and sensor corruption to isolate the benefits of the ensemble architecture from the continual learning strategy.