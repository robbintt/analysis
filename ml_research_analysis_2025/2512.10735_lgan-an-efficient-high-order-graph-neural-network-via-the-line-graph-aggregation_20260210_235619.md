---
ver: rpa2
title: 'LGAN: An Efficient High-Order Graph Neural Network via the Line Graph Aggregation'
arxiv_id: '2512.10735'
source_url: https://arxiv.org/abs/2512.10735
tags:
- graph
- lgan
- node
- line
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Line Graph Aggregation Network (LGAN),
  a graph neural network designed to address the expressiveness limitations of standard
  message-passing GNNs, which are constrained by the 1-dimensional Weisfeiler-Lehman
  (1-WL) test. LGAN constructs line graphs from induced subgraphs centered at each
  node and performs localized aggregation over these line graphs to capture higher-order
  structural interactions.
---

# LGAN: An Efficient High-Order Graph Neural Network via the Line Graph Aggregation
## Quick Facts
- arXiv ID: 2512.10735
- Source URL: https://arxiv.org/abs/2512.10735
- Reference count: 6
- Introduces LGAN, a graph neural network that constructs line graphs from induced subgraphs to capture higher-order structural interactions

## Executive Summary
LGAN addresses the expressiveness limitations of standard message-passing GNNs, which are constrained by the 1-dimensional Weisfeiler-Lehman (1-WL) test. The model constructs line graphs from induced subgraphs centered at each node and performs localized aggregation over these line graphs to capture higher-order structural interactions. Theoretically, LGAN surpasses the expressive power of the 2-WL test while maintaining linear time complexity on sparse graphs. Empirically, LGAN achieves competitive or superior performance compared to state-of-the-art k-WL-based models on six graph classification benchmarks.

## Method Summary
LGAN constructs line graphs from induced subgraphs centered at each node, enabling the network to capture higher-order structural interactions that standard GNNs miss. The model performs localized aggregation over these line graphs, which allows it to theoretically surpass the expressive power of the 2-WL test. The architecture maintains linear time complexity on sparse graphs through efficient line graph construction and aggregation mechanisms. This approach enables edge-level attributions, providing better interpretability by identifying critical substructures contributing to classification decisions.

## Key Results
- LGAN surpasses the expressive power of the 2-WL test while maintaining linear time complexity on sparse graphs
- Achieves competitive or superior performance compared to state-of-the-art k-WL-based models on six graph classification benchmarks
- Provides better interpretability through edge-level attributions that effectively identify critical substructures

## Why This Works (Mechanism)
The mechanism works by constructing line graphs from induced subgraphs, which transforms edge relationships into node relationships in the line graph space. This transformation allows the model to reason about higher-order interactions between edges that would be difficult to capture in the original graph structure. The localized aggregation over line graphs enables the network to learn complex patterns about how edges interact within local neighborhoods, effectively capturing structural information that goes beyond pairwise node relationships.

## Foundational Learning
1. **Line Graphs**: Why needed - Transform edge relationships into node relationships to capture higher-order interactions. Quick check - Verify that the line graph construction preserves essential structural information.
2. **Weisfeiler-Lehman Test**: Why needed - Provides a theoretical framework for measuring graph isomorphism and expressive power. Quick check - Confirm that LGAN's theoretical claims align with established WL test results.
3. **Message Passing GNNs**: Why needed - Standard approach that LGAN aims to improve upon by addressing its expressive limitations. Quick check - Compare LGAN's performance against standard MP-GNN baselines.
4. **Graph Classification**: Why needed - The primary task LGAN is evaluated on, requiring the model to distinguish between different graph structures. Quick check - Validate performance across diverse graph classification datasets.
5. **Edge-level Attributions**: Why needed - Provide interpretability by identifying which substructures contribute most to classification decisions. Quick check - Assess the quality and reliability of attributions against known ground truth.
6. **Time Complexity Analysis**: Why needed - Critical for understanding scalability to large graphs. Quick check - Empirically verify linear time complexity on sparse graphs with varying sizes.

## Architecture Onboarding
**Component Map**: Input Graph -> Induced Subgraphs -> Line Graph Construction -> Localized Aggregation -> Output Classification
**Critical Path**: The most performance-critical path is the line graph construction and aggregation pipeline, as it directly impacts both computational efficiency and model expressiveness.
**Design Tradeoffs**: LGAN trades increased model complexity (line graph construction) for improved expressive power and interpretability, while maintaining computational efficiency on sparse graphs.
**Failure Signatures**: Potential failure modes include poor performance on dense graphs where line graph construction becomes computationally expensive, and reduced interpretability when line graphs become too complex to analyze effectively.
**First Experiments**:
1. Verify line graph construction correctness on small synthetic graphs with known properties
2. Compare performance against standard GNNs on a simple graph classification task
3. Test interpretability by checking if edge attributions align with known critical substructures in synthetic graphs

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical proof of linear time complexity on sparse graphs needs empirical validation across diverse real-world datasets
- Scalability to extremely large graphs (millions of nodes) is not addressed, limiting industrial applicability
- Experimental results lack comprehensive ablation studies to isolate the contribution of the line graph aggregation mechanism

## Confidence
- Theoretical Claims: High - The proof that LGAN surpasses 2-WL expressive power is mathematically sound given the assumptions about line graph construction and aggregation
- Empirical Performance: Medium - Results are promising but limited by the number of datasets and lack of comprehensive comparisons with all recent GNN variants
- Interpretability Claims: Low - Edge-level attributions are demonstrated qualitatively but lack quantitative validation against established interpretability benchmarks

## Next Checks
1. Conduct scalability experiments on graphs with 1M+ nodes to verify the claimed linear time complexity on sparse graphs holds in practice, measuring both memory usage and runtime across varying edge densities
2. Perform systematic ablation studies removing the line graph aggregation component to quantify its specific contribution to performance gains over standard GNN architectures
3. Design a controlled experiment comparing LGAN's edge attributions with ground-truth important edges in synthetic graphs where the critical substructures are known a priori, using metrics like precision@k and mean average precision