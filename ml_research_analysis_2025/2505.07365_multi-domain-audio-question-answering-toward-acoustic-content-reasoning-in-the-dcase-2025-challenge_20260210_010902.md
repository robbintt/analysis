---
ver: rpa2
title: Multi-Domain Audio Question Answering Toward Acoustic Content Reasoning in
  The DCASE 2025 Challenge
arxiv_id: '2505.07365'
source_url: https://arxiv.org/abs/2505.07365
tags:
- audio
- sound
- question
- acoustic
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The DCASE 2025 Challenge introduces a multi-domain Audio Question
  Answering (AQA) benchmark to evaluate audio-language models' reasoning abilities
  across Bioacoustics, Temporal Soundscapes, and Complex QA. The task requires models
  to interpret diverse acoustic scenes, integrate external knowledge, and answer specific
  questions grounded in audio content.
---

# Multi-Domain Audio Question Answering Toward Acoustic Content Reasoning in The DCASE 2025 Challenge

## Quick Facts
- **arXiv ID**: 2505.07365
- **Source URL**: https://arxiv.org/abs/2505.07365
- **Reference count**: 0
- **Primary result**: Multi-domain Audio Question Answering benchmark with 30-50% accuracy on zero-shot baselines, revealing substantial reasoning gaps.

## Executive Summary
The DCASE 2025 Challenge introduces a multi-domain Audio Question Answering (AQA) benchmark to evaluate audio-language models' reasoning abilities across Bioacoustics, Temporal Soundscapes, and Complex QA. The task requires models to interpret diverse acoustic scenes, integrate external knowledge, and answer specific questions grounded in audio content. Three baseline models—Qwen2-Audio-7B, AudioFlamingo 2, and Gemini-2.0-Flash—were evaluated in a zero-shot setting, achieving 30–50% accuracy across subsets, with performance varying significantly by domain. These results highlight the challenge of audio understanding and reasoning beyond classification or captioning tasks, revealing substantial room for improvement in developing more robust, generalizable audio-language models.

## Method Summary
The benchmark evaluates audio-language models on three distinct AQA subsets: Bioacoustics QA (marine mammal vocalizations), Temporal Soundscapes QA (temporal reasoning), and Complex QA (multi-faceted reasoning). Models are tested in zero-shot settings with answer choices provided for Parts 1 and 3 but withheld for Part 2 to assess pure audio-grounded generation. Three baselines use different architectures: Qwen2-Audio-7B with Whisper encoder and Sentence-BERT post-processing, AudioFlamingo 2 with CLAP encoder and Flamingo-style cross-attention, and Gemini-2.0-Flash. Performance is measured by top-1 accuracy with robustness testing via answer shuffling, and a sample-weighted average serves as tie-breaker.

## Key Results
- Three baseline models achieved 30-50% accuracy across AQA subsets in zero-shot evaluation
- Performance varied significantly by domain: AudioFlamingo 2 excelled at Bioacoustics (Part 1) but struggled with Temporal Soundscapes (Part 2), while Qwen2-Audio-7B showed the opposite pattern
- Qualitative analysis revealed key failure modes including hallucination (inventing non-existent sounds), multi-selection errors, and positional bias when answer choices were shuffled

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-domain benchmarking reveals distinct capability gaps in audio-language models that single-domain evaluation misses.
- Mechanism: The three subsets (Bioacoustics, Temporal Soundscapes, Complex QA) probe different reasoning faculties—fine-grained acoustic perception, temporal ordering, and contextual inference. Performance variance across subsets exposes model-specific strengths and weaknesses.
- Core assumption: Each subset captures a semantically distinct reasoning dimension that does not fully transfer across domains.
- Evidence anchors:
  - [abstract] "Preliminary results on the development set are compared, showing strong variation across models and subsets."
  - [section 4.2] "Qwen2-Audio-7B... performs notably poorly on Part 1... AudioFlamingo 2 excels in Part 1 but struggles with Part 2, which emphasizes temporal reasoning."
  - [corpus] Related work on Audio Question Answering with GRPO-Based Fine-Tuning (arXiv:2511.14307) addresses DCASE 2025 Track 5, confirming community uptake but no cited cross-domain analysis yet.
- Break condition: If models show near-uniform performance across all subsets, the domain distinction would not be diagnostic.

### Mechanism 2
- Claim: Answer-shuffling robustness testing exposes positional bias in model predictions.
- Mechanism: By permuting answer choice order and measuring accuracy variance, the evaluation detects whether models rely on option position rather than semantic content. Large best-worst accuracy gaps indicate positional bias.
- Core assumption: Models may exploit option ordering artifacts rather than grounding answers in audio content.
- Evidence anchors:
  - [section 3.1] "To assess robustness, we additionally evaluate each model by shuffling the order of answer choices multiple times. The best and worst accuracies across these permutations are reported."
  - [section 4.3] Hallucination examples show models inventing sounds not in audio or options, suggesting weak grounding.
  - [corpus] No direct corpus evidence on positional bias in audio QA; this mechanism is benchmark-specific.
- Break condition: If all models show negligible variance under shuffling, positional bias is not a significant confound.

### Mechanism 3
- Claim: Zero-shot transfer from pretrained audio-language models yields sub-50% accuracy, indicating pretraining objectives do not align with fine-grained reasoning requirements.
- Mechanism: Pretraining on classification, captioning, or broad QA does not enforce the multi-hop grounding (audio perception + external knowledge + question interpretation) required for AQA. The 30–50% accuracy range suggests a capability gap.
- Core assumption: The gap reflects missing reasoning skills rather than data scale alone.
- Evidence anchors:
  - [abstract] "Three baseline models... were evaluated in a zero-shot setting, achieving 30–50% accuracy across subsets."
  - [section 4.2] "Overall performance remains low... suggesting that the proposed AQA dataset probes essential aspects of audio understanding and reasoning that are not effectively addressed by naïve transfer."
  - [corpus] Solla (arXiv:2503.15338) and VoiceBBQ (arXiv:2509.21108) explore acoustic context in spoken LLMs but do not report AQA benchmarks; cross-task transfer remains understudied.
- Break condition: If fine-tuned models rapidly exceed 80% accuracy, the gap may be narrow and addressable with minimal adaptation.

## Foundational Learning

- Concept: Audio Feature Encoding (Whisper, CLAP)
  - Why needed here: Baselines use Whisper-large-v3 (Qwen2-Audio) and enhanced CLAP (AudioFlamingo 2) as audio encoders. Understanding spectrogram-to-embedding pipelines is prerequisite for modifying or replacing encoders.
  - Quick check question: Can you explain how a log-mel spectrogram is computed from raw waveform, and why Whisper uses 80 mel bins?

- Concept: Cross-Attention in Multimodal LLMs
  - Why needed here: AudioFlamingo 2 uses Flamingo-style cross-attention to integrate audio embeddings with language decoder. Qwen2-Audio uses a similar bridging mechanism. Understanding cross-attention is critical for debugging modality fusion.
  - Quick check question: In cross-attention, which modality provides the query and which provides key/value, and what does this imply for information flow?

- Concept: Semantic Similarity for Answer Mapping
  - Why needed here: Qwen2-Audio generates free-form text mapped to options via Sentence-BERT cosine similarity. This post-processing step is a potential failure point if model outputs diverge from expected formats.
  - Quick check question: Given a model output "the sound of a barking dog" and options "A. Canine bark", "B. Bird chirp", which option would highest cosine similarity select, and what could go wrong?

## Architecture Onboarding

- Component map:
  Audio Encoder -> Modality Bridge -> Language Backbone -> Post-Processor -> Evaluator
  (Whisper-large-v3/CLAP) -> (Cross-attention/adapters) -> (Qwen2-7B/Flamingo) -> (Sentence-BERT/string matching) -> (Top-1 accuracy)

- Critical path:
  1. Load audio → encode to embeddings.
  2. Format question + options (Parts 1, 3) or question only (Part 2) as prompt.
  3. Generate text response conditioned on audio.
  4. Map response to option via similarity or string matching.
  5. Compute accuracy; optionally shuffle options and re-evaluate.

- Design tradeoffs:
  - Providing answer options during inference (Parts 1, 3) may bias models toward option keywords but is necessary for contextual grounding; Part 2 withholds options to test pure audio-grounded generation.
  - Free-form generation + similarity matching (Qwen2) is flexible but introduces mapping errors; constrained generation (AudioFlamingo 2 reformatted options) is more deterministic but requires format engineering.
  - Zero-shot evaluation isolates pretrained capability but underestimates fine-tuning potential.

- Failure signatures:
  - Hallucination: Model invents sounds not present in audio (e.g., "mechanical fan" when not in options).
  - Multi-selection: Model outputs multiple species or events when only one is correct.
  - Temporal misalignment: Incorrect onset/offset timestamps in Part 2.
  - Positional bias: Accuracy drops significantly when answer order is shuffled.

- First 3 experiments:
  1. Establish baseline: Run Qwen2-Audio-7B-Instruct zero-shot on all three dev subsets, record per-domain accuracy and shuffle robustness. Compare to reported 45.0% weighted average.
  2. Ablate option visibility: For Part 2, compare performance with vs. without answer options provided; quantify grounding vs. option-bias contribution.
  3. Post-processor swap: Replace Sentence-BERT mapping with exact string matching and with an LLM-based verifier; measure accuracy delta to isolate mapping errors.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What training or architectural modifications enable audio-language models to achieve balanced, high performance across all three AQA domains (Bioacoustics, Temporal Soundscapes, Complex QA) simultaneously?
- Basis in paper: [explicit] The paper states "improving performance across diverse domains simultaneously requires more balanced and generalizable audio understanding and reasoning capabilities" and notes that current models show "complementary strengths and weaknesses" (AudioFlamingo 2 excels at Part 1 but struggles at Part 2; Qwen2-Audio-7B shows the opposite pattern).
- Why unresolved: No existing model achieves strong performance across all subsets; domain-averaged accuracy remains at 39.6–48.3%, with per-subset performance varying by up to 22 percentage points within a single model.
- What evidence would resolve it: A model achieving >60% accuracy on all three subsets with <10 percentage point variance across domains, along with ablation studies identifying which training data or architectural changes contributed to balanced performance.

### Open Question 2
- Question: What mechanisms can reduce hallucination in audio-language models during question answering, where models generate outputs grounded in non-existent acoustic evidence?
- Basis in paper: [explicit] The qualitative analysis identifies hallucination as a key failure mode, providing examples: Qwen2-Audio-7B "identifies sounds such as 'mechanical fan' and 'ticking clock'... none of which are part of the annotated ground truth," and shows "Multi-Selection" errors where the model "incorrectly identifies the sounds of 'Narwhal' as a combination of three types of whales."
- Why unresolved: The paper provides no analysis of why hallucinations occur or how to mitigate them; the issue is documented but not addressed.
- What evidence would resolve it: Studies correlating hallucination rates with model confidence scores, attention patterns, or training data composition; interventions (e.g., confidence thresholding, retrieval augmentation) demonstrating reduced hallucination rates on held-out examples.

### Open Question 3
- Question: How does fine-tuning on domain-specific AQA data affect cross-domain generalization versus zero-shot performance?
- Basis in paper: [inferred] All baseline evaluations were conducted in zero-shot settings only; the paper encourages participants to explore "efficient fine-tuning methods" but provides no fine-tuned results or analysis of the tradeoffs between specialization and generalization.
- Why unresolved: The 0.7K–6.4K training pairs per subset suggest supervised learning is feasible, but the impact on generalization remains unstudied; it is unclear whether fine-tuning improves within-domain performance at the cost of cross-domain transfer.
- What evidence would resolve it: Comparisons of zero-shot vs. fine-tuned performance within and across domains, measuring both accuracy gains on target domains and potential degradation on out-of-distribution audio types.

## Limitations

- The paper provides performance breakdowns across domains but lacks causal analysis of why specific architectures fail on particular subsets.
- Zero-shot evaluation results show capability gaps, but without fine-tuning ablation studies, it's unclear whether these reflect fundamental reasoning deficits or adaptation needs.
- Positional bias testing is included, but there's no established literature on positional effects in audio QA to validate the significance of observed variance.

## Confidence

- **Multi-domain benchmarking reveals distinct capability gaps**: Medium
- **Answer-shuffling exposes positional bias**: Medium
- **Zero-shot transfer indicates reasoning skill gaps**: Low-Medium

## Next Checks

1. **Fine-tuning ablation study**: Take Qwen2-Audio-7B and AudioFlamingo 2, fine-tune on the combined AQA training set for 1-3 epochs, then re-evaluate zero-shot performance on dev sets. Measure accuracy improvement to quantify how much of the 30-50% gap is addressable by adaptation versus fundamental reasoning deficits.

2. **Error type classification**: Manually annotate 100 failed predictions per baseline model, categorizing errors into: hallucination, multi-selection, temporal misalignment, positional bias, and knowledge deficit. Compute per-category failure rates to identify dominant failure modes and validate whether shuffling robustness correlates with specific error types.

3. **Cross-subset transfer test**: Train a model on Part 1 (BQA) only, evaluate on Parts 2 and 3; repeat for other single-domain training. Measure transfer performance to quantify how well reasoning skills generalize across acoustic domains, testing the assumption that each subset captures distinct reasoning dimensions.