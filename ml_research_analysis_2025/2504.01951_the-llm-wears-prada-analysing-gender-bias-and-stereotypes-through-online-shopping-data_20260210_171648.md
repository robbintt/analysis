---
ver: rpa2
title: 'The LLM Wears Prada: Analysing Gender Bias and Stereotypes through Online
  Shopping Data'
arxiv_id: '2504.01951'
source_url: https://arxiv.org/abs/2504.01951
tags:
- gender
- female
- items
- male
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates gender bias in LLMs by analyzing their ability
  to predict gender from online shopping histories and the stereotypical associations
  they make. Using a dataset of historical Amazon purchases, six LLMs were evaluated
  for gender classification accuracy and their reasoning patterns.
---

# The LLM Wears Prada: Analysing Gender Bias and Stereotypes through Online Shopping Data

## Quick Facts
- **arXiv ID:** 2504.01951
- **Source URL:** https://arxiv.org/abs/2504.01951
- **Reference count:** 40
- **Primary result:** Study investigates gender bias in LLMs through online shopping data analysis, finding moderate gender classification accuracy based on stereotypical product associations

## Executive Summary
This study examines gender bias in large language models by analyzing their ability to predict gender from online shopping histories and the stereotypical associations they make. Using historical Amazon purchase data, six LLMs were evaluated for gender classification accuracy and their reasoning patterns. The research reveals that while models can infer gender with moderate accuracy, their predictions often rely on stereotypical associations between product categories and gender. Notably, explicit instructions to avoid bias reduced prediction certainty but failed to eliminate stereotypical patterns, highlighting the persistent nature of gender biases in LLMs.

## Method Summary
The study employed a dataset of historical Amazon purchases to evaluate six different LLMs for gender classification accuracy. Researchers analyzed the models' ability to predict gender from shopping histories and examined the reasoning patterns behind their predictions. The methodology included testing how explicit bias-mitigation instructions affected model behavior and prediction certainty. The analysis focused on binary gender classification, comparing model predictions against self-reported gender labels in the dataset.

## Key Results
- LLMs can infer gender from shopping histories with moderate accuracy
- Model predictions rely heavily on stereotypical associations between product categories and gender
- Explicit bias-mitigation instructions reduce prediction certainty but don't eliminate stereotypical patterns

## Why This Works (Mechanism)
The mechanism underlying LLM gender bias in shopping contexts stems from training data that reflects historical consumer behavior patterns, which themselves contain gender-based purchasing stereotypes. When LLMs learn from large corpora of text that describe shopping behaviors, they internalize these associations as statistical correlations. The models then apply these learned patterns to make inferences about gender based on product categories, even when those associations may not accurately represent individual consumers' behaviors or identities.

## Foundational Learning
- **Gender binary classification** - Why needed: The study's methodology relies on binary gender labels; quick check: verify how the dataset handles non-binary individuals
- **Product-category associations** - Why needed: Models use these as primary features for gender inference; quick check: examine distribution of stereotypically gendered products in dataset
- **Bias mitigation strategies** - Why needed: The study tests explicit instructions to reduce bias; quick check: assess effectiveness of different mitigation approaches
- **Self-reported vs. model-inferred labels** - Why needed: The study compares these for accuracy assessment; quick check: analyze alignment between labels
- **LLM reasoning patterns** - Why needed: Understanding how models justify predictions reveals bias mechanisms; quick check: examine reasoning quality across different model architectures

## Architecture Onboarding

**Component Map:**
LLM model -> Input prompt (shopping history) -> Prediction output -> Confidence score -> Reasoning explanation

**Critical Path:**
Input shopping history → LLM processing → Gender classification → Confidence assessment → Bias analysis

**Design Tradeoffs:**
- Binary vs. non-binary gender classification affects both methodology and findings
- Dataset representativeness vs. practical availability of shopping data
- Model interpretability vs. prediction accuracy
- Explicit bias instructions vs. natural model behavior

**Failure Signatures:**
- High confidence predictions based solely on stereotypical associations
- Inability to handle non-traditional shopping patterns
- Persistent bias despite explicit mitigation instructions
- Over-reliance on product categories rather than individual behavior

**3 First Experiments:**
1. Test same models on shopping data from different e-commerce platforms to assess platform-specific bias patterns
2. Evaluate model performance when presented with shopping histories that deliberately violate gender stereotypes
3. Compare reasoning quality between models when asked to justify predictions with and without bias-mitigation prompts

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset relies on Amazon purchase histories, potentially limiting generalizability across different shopping contexts
- Binary gender classification framework excludes non-binary gender identities
- Six LLM models represent a limited sample, findings may not generalize to other architectures
- Self-reported gender labels may not align with gender identity, affecting validity of bias measurements

## Confidence
- LLM gender classification accuracy: Medium
- Stereotypical association identification: Medium
- Effectiveness of bias-mitigation instructions: Low-Medium

## Next Checks
1. Replicate the analysis using purchase data from multiple e-commerce platforms across different regions to assess cultural and platform-specific variations in stereotypical associations.

2. Expand the gender classification framework to include non-binary gender options and test model performance on this expanded taxonomy to understand intersectional bias patterns.

3. Conduct longitudinal analysis tracking how LLM predictions evolve as shopping patterns change over time, particularly examining whether stereotypes persist when shopping behaviors deviate from traditional gender norms.