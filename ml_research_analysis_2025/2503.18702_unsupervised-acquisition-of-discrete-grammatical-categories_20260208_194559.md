---
ver: rpa2
title: Unsupervised Acquisition of Discrete Grammatical Categories
arxiv_id: '2503.18702'
source_url: https://arxiv.org/abs/2503.18702
tags:
- language
- acquisition
- grammatical
- categories
- grammar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study presents experiments using a computational laboratory
  environment for language acquisition, implementing a multi-agent system with an
  adult and a daughter language model. The daughter learns the mother language through
  unsupervised acquisition, without access to internal grammatical knowledge.
---

# Unsupervised Acquisition of Discrete Grammatical Categories

## Quick Facts
- arXiv ID: 2503.18702
- Source URL: https://arxiv.org/abs/2503.18702
- Reference count: 19
- The study presents experiments using a computational laboratory environment for language acquisition, implementing a multi-agent system with an adult and a daughter language model. The daughter learns the mother language through unsupervised acquisition, without access to internal grammatical knowledge. Using statistical analyses of patterns in input data, discrete grammatical categories are acquired and represented by graph-based feature-value pairs. Hierarchical agglomerative cluster analysis was applied to utterances generated by the mother model, yielding non-trivial grammatical knowledge resembling traditional linguistic categories. The parameter configuration was validated using a test set, resulting in successful acquisition of grammatical categories. The MODOMA system provides a transparent, accountable AI model for language acquisition research, enabling controlled experimentation and analysis of language learning processes.

## Executive Summary
This paper presents MODOMA, a computational laboratory environment for studying language acquisition through a multi-agent mother-daughter system. The daughter language model learns grammatical categories from raw utterances generated by the mother model, without access to explicit grammatical knowledge. Using statistical analyses of distributional patterns, the system acquires discrete grammatical categories that are represented as feature-value pairs in a graph-based grammar formalism. The approach is validated through controlled experiments demonstrating successful acquisition and replication of grammatical categories on test data.

## Method Summary
The method employs hierarchical agglomerative clustering (HAC) with complete-linkage to induce grammatical categories from raw utterances. The daughter model receives 10,000+ Dutch sentences from the mother model (DELILAH), extracts n-gram concordances with 2 context words before and after each target word, computes Spearman correlations between word distributions, and builds a distance matrix. The HAC algorithm partitions words into 14 discrete clusters, which are then encoded as feature-value pairs (e.g., ⟨A:a⟩–⟨A:n⟩) in the daughter's graph-based grammar. The parameter configuration is validated by replicating the process on an independent test set and assessing cluster correspondence using Fisher's exact test.

## Key Results
- Distributional clustering over syntactic context windows successfully induces grammatical categories from raw utterances
- Acquired categories can be encoded as graph-based feature-value pairs that constrain combinatory operations
- Multi-agent mother-daughter architecture with unsupervised access enables controlled, reproducible experimentation on acquisition
- Statistical validation shows significant association between training and test category assignments (p = 2×10⁻⁶)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distributional clustering over syntactic context windows can induce grammatical categories from raw utterances.
- Mechanism: The system extracts n-gram concordances (2 words before/after each target word), computes position-sensitive co-occurrence frequencies, builds a Spearman correlation matrix, squares it to obtain distance, and applies hierarchical agglomerative clustering with complete-linkage to partition words into discrete categories.
- Core assumption: Words that share similar local contexts tend to share grammatical properties; clustering will recover linguistically meaningful partitions rather than spurious groupings.
- Evidence anchors:
  - [abstract]: "We demonstrate how statistical analyses of patterns in the input data corresponding to grammatical categories yield discrete grammatical rules."
  - [section 4]: "The input to the clustering algorithm consists of n-grams with concordances representing keywords in context... A fragment of this type of list containing 57,733 exemplars is provided in Table 1... The result is squared to consider only the relative distances between correlations while not taking into account the differences between positive and negative correlations."
  - [corpus]: No direct replication in corpus; closest is Shakouri et al. (2025) MODOMA prior work on functional/content category acquisition, but not this specific clustering procedure.
- Break condition: If context window size or minimum frequency threshold is too small, noise dominates; if too large, sparse data yields unstable correlations. The paper notes 10,000 utterances and minimum frequency of 10 were empirically necessary.

### Mechanism 2
- Claim: Acquired categories can be encoded as graph-based feature-value pairs that constrain combinatory operations.
- Mechanism: Each cluster is assigned a feature-value pair (e.g., ⟨A:a⟩–⟨A:n⟩) and attached to lexical entries in the daughter grammar. These pairs function as unification constraints during parsing/generation, restricting which structures can combine.
- Core assumption: The grammar formalism supports unification over arbitrary feature-value pairs, and these constraints are sufficient to enforce category-appropriate combinatorics.
- Evidence anchors:
  - [abstract]: "These rules are subsequently added to the grammatical knowledge of the daughter language model."
  - [section 2]: "In this framework, each linguistic feature such as the classification of syntactic categories is represented as a node in the graph, while the relationships between these features and their corresponding values are captured by edges."
  - [corpus]: Corpus has no direct evidence on this specific representation choice.
- Break condition: If multiple incompatible feature-values are assigned to the same lemma, or if feature-value pairs do not align with actual combinatorial restrictions, unification failures or overgeneration may occur.

### Mechanism 3
- Claim: Multi-agent mother-daughter architecture with unsupervised access enables controlled, reproducible experimentation on acquisition.
- Mechanism: DELILAH (mother) generates Dutch utterances from an explicit grammar; daughter receives only surface strings, applies acquisition procedures, updates her grammar, and can immediately use it to parse/generate. All parameters, inputs, and outputs are logged.
- Core assumption: The daughter's learning environment is sufficiently representative of naturalistic acquisition; the mother's grammar generates linguistically plausible exemplars.
- Evidence anchors:
  - [abstract]: "Crucially, the daughter agent does not have access to the internal knowledge of the mother language model but only to the language exemplars the mother agent generates."
  - [section 2]: "Similarly to human children acquiring their mother languages, the daughter language model is able to process utterances before it has fully acquired a grammar describing the target language."
  - [section 7]: Fisher's exact test (p = 2×10⁻⁶) shows significant association between training and test category assignments, supporting replicability.
  - [corpus]: No direct corpus comparison; ter Hoeve et al. (2022) teacher-student loop is noted as related but uses preconstructed artificial sentences rather than natural language generation.
- Break condition: If the mother's output distribution is skewed or unrepresentative, daughter may acquire idiosyncratic categories that do not generalize.

## Foundational Learning
- Concept: Hierarchical agglomerative clustering (HAC) with complete-linkage
  - Why needed here: Converts continuous distributional similarity into discrete categories; complete-linkage was selected empirically to produce compact clusters.
  - Quick check question: Can you explain why complete-linkage (maximum pairwise distance) might produce tighter clusters than single-linkage?
- Concept: Spearman rank correlation as distributional similarity
  - Why needed here: Robust to frequency skew; squaring eliminates sign, treating positive and negative correlations as equally informative for distance.
  - Quick check question: What happens if you use Pearson correlation instead on heavily skewed co-occurrence counts?
- Concept: Unification-based grammar (HPSG-like feature structures)
  - Why needed here: Acquired feature-value pairs must be operationally usable by parser/generator; unification enforces constraints.
  - Quick check question: How does unification differ from simple equality checking when combining two feature structures?

## Architecture Onboarding
- Component map: DELILAH (mother generator/parser; combinatory list grammar; Dutch) → Interaction system (utterance exchange, turn-taking) → Daughter (grammar as graph templates, parser/generator, acquisition device using R/Java bridge for clustering). All components log to session-identified records.
- Critical path: (1) Mother generates 10,000+ utterances → (2) Daughter extracts n-gram concordances → (3) Build correlation/distance matrix → (4) HAC produces dendrogram → (5) Cut into k clusters (14 used) → (6) Assign feature-value pairs → (7) Update daughter grammar → (8) Daughter parses/generates with new constraints.
- Design tradeoffs: Clustering granularity (k=14) is manually set based on training data inspection; smaller k yields coarse categories, larger k overfragments. Context window (2+2 words) trades local signal vs. data sparsity. Graph-based grammar is transparent but not as performant as neural models for large-scale generation.
- Failure signatures: If clusters contain mixed traditional categories (e.g., verbs with adjectives in Cluster 1, Table 3), check context window size and minimum frequency threshold. If test replication fails (Fisher p > 0.05), verify identical parameter settings and that test set was independently generated.
- First 3 experiments:
  1. Replicate training clustering on 10,000 DELILAH utterances with default parameters; verify cluster assignments match Table 3 for known words (e.g., perfect participles in Cluster 13).
  2. Vary context window (1+1 vs. 2+2 vs. 3+3) on held-out data; measure cluster purity against gold POS labels (if available) to identify optimal window.
  3. Reduce minimum frequency threshold from 10 to 5; assess whether noise increases (more mixed categories) or rare-word coverage improves without degrading cluster coherence.

## Open Questions the Paper Calls Out
- How can "internal annotation" be leveraged to allow the daughter agent to acquire more complex grammatical structures using previously acquired knowledge? [explicit] The Conclusion states that establishing a foundation for exploring internal annotation (a form of self-supervised learning) will be the "focus of upcoming studies." Why unresolved: The current study focused on the initial unsupervised acquisition of discrete categories, stopping short of demonstrating how these categories bootstrap further learning. What evidence would resolve it: Experiments showing the daughter agent successfully utilizing acquired feature-value pairs to learn higher-order syntactic rules or morphology without external supervision.

- To what extent does implementing explicit feedback from the mother agent improve the accuracy or efficiency of the daughter's grammar acquisition? [explicit] The Conclusion notes that evaluating results based on feedback suggests an "interesting direction for further exploration." Why unresolved: The reported experiments utilized a "single run" configuration without an active feedback loop to correct the daughter's generated utterances. What evidence would resolve it: Comparative simulations demonstrating faster convergence or higher accuracy in category acquisition when the mother agent provides corrective signals.

- Can the acquisition procedures be refined to achieve strong equivalence (identical parse trees) rather than just weak equivalence (generating the same strings) with the mother grammar? [inferred] Section 6 notes the current acquired grammar is not "strongly equivalent" to the mother's, implying this remains a theoretical goal. Why unresolved: The clustering method groups words by distributional similarity, which produces functional categories that do not strictly replicate the structural analyses of the DELILAH system. What evidence would resolve it: An evaluation showing the daughter's internal graph structures match the specific constituent structures assigned by the mother agent for identical sentences.

## Limitations
- The absence of public code for the DELILAH grammar model and complete MODOMA acquisition system makes exact replication challenging
- The study relies on a single Dutch grammar model for data generation, raising questions about generalizability across languages or naturalistic corpora
- The clustering procedure's parameter sensitivity (k=14, context window size, minimum frequency threshold) is not extensively explored
- The validation relies on a single test set rather than cross-validation, and lacks quantitative cluster purity metrics against gold POS tags

## Confidence
- **High Confidence**: The distributional clustering mechanism itself is well-established and the mathematical procedure (Spearman correlation, complete-linkage HAC) is clearly specified and reproducible.
- **Medium Confidence**: The claim that acquired categories resemble traditional linguistic POS categories is supported by cluster content examples but lacks quantitative evaluation against gold standards.
- **Low Confidence**: The specific parameter configuration (14 clusters, 2+2 context window, minimum frequency of 10) was empirically chosen without systematic optimization or sensitivity analysis.

## Next Checks
1. Replicate the clustering pipeline on a standard parsed corpus (e.g., Universal Dependencies Dutch) with gold POS tags, measuring cluster purity and adjusted Rand index against gold categories.
2. Perform parameter sensitivity analysis by varying k (number of clusters) and context window sizes on the same dataset, reporting cluster coherence metrics and replication rates.
3. Implement a simplified version of the mother-daughter interaction using an open-source Dutch grammar generator and assess whether the acquired categories enable successful parsing/generation without access to the mother's grammar.