---
ver: rpa2
title: 'MoVoC: Morphology-Aware Subword Construction for Geez Script Languages'
arxiv_id: '2509.08812'
source_url: https://arxiv.org/abs/2509.08812
tags:
- languages
- language
- tokenization
- computational
- morphological
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MoVoC, a morphology-aware subword tokenization
  method designed for low-resource, morphologically complex languages written in the
  Geez script. MoVoC integrates supervised morphological analysis with Byte Pair Encoding
  to preserve morpheme boundaries and improve tokenization quality.
---

# MoVoC: Morphology-Aware Subword Construction for Geez Script Languages

## Quick Facts
- arXiv ID: 2509.08812
- Source URL: https://arxiv.org/abs/2509.08812
- Reference count: 27
- Primary result: Morphology-aware tokenization improves intrinsic metrics (MorphoScore, boundary precision) but not translation quality for Geez script languages

## Executive Summary
This paper introduces MoVoC, a morphology-aware subword tokenization method designed for low-resource, morphologically complex languages written in the Geez script. MoVoC integrates supervised morphological analysis with Byte Pair Encoding to preserve morpheme boundaries and improve tokenization quality. The authors curate and release manually annotated morpheme datasets for four Geez script languages—Amharic, Tigrinya, Ge'ez, and Tigre—and construct corresponding vocabularies. While automatic translation quality improvements are not statistically significant, MoVoC achieves consistent gains in intrinsic metrics: MorphoScore (up to 0.731) and morpheme boundary precision (up to 88.3%). The method also reduces Rényi entropy, indicating more consistent segmentation. Results demonstrate that morphology-aware tokenization enhances linguistic fidelity and token efficiency, particularly for low-resource and morphologically rich languages. The datasets and tokenizer will be publicly available to support future research.

## Method Summary
MoVoC constructs a hybrid vocabulary by combining supervised morphological analysis with Byte Pair Encoding. The method uses an external morphological analyzer (HornMorpho for Amharic/Tigrinya, manual annotation for Ge'ez/Tigre) to identify morpheme boundaries in the corpus. A predefined portion of the total vocabulary is allocated to high-frequency morphemes from the analyzer, while the remainder is filled using standard BPE trained on the corpus. During BPE training, merge operations are constrained to prevent tokens from spanning morpheme boundaries. The final tokenizer preserves linguistically meaningful units while maintaining subword compression. The approach is evaluated on four Geez script languages using intrinsic metrics (MorphoScore, boundary precision, Rényi entropy) and extrinsic metrics (BLEU, chrF++) on machine translation tasks.

## Key Results
- MoVoC achieves MorphoScore improvements up to 0.731 across four Geez script languages
- Morpheme boundary precision reaches 88.3% with MoVoC compared to standard BPE
- Rényi entropy consistently decreases (e.g., 0.39 for Tigrinya vs. 0.40 for BPE)
- No statistically significant BLEU/chrf++ improvements in English→target translation tasks

## Why This Works (Mechanism)

### Mechanism 1: Constrained Merge Operations
Standard BPE merges adjacent tokens based solely on frequency, which can fragment semantically meaningful stems in morphologically complex languages. MoVoC introduces hard constraints during the BPE merge phase: if a candidate merge would create a token spanning across a gold-standard morpheme boundary, the merge is rejected. This forces optimal merges within morphological units rather than across them. The method assumes the morphological analyzer provides accurate boundary labels and that preserving these boundaries benefits model representation. Evidence shows MoVoC prevents errors like splitting the Amharic verb "break" into unrelated words like "gate."

### Mechanism 2: Hybrid Vocabulary Seeding
Rather than building vocabulary purely from the bottom up, MoVoC creates a hybrid vocabulary by pre-seeding high-frequency morphemes alongside statistical subwords. The method extracts top-k frequent morphemes from annotated data and unions them with a smaller BPE-derived vocabulary. This guarantees common functional units exist as single tokens, reducing sequence length for complex words. The approach assumes frequent morphemes in annotated data are representative of general usage. Evidence shows this strategy improves token efficiency while maintaining linguistic fidelity.

### Mechanism 3: Entropy Reduction via Boundary Precision
By enforcing morphological alignment, MoVoC reduces segmentation ambiguity and creates more consistent token distributions with lower Rényi entropy. Unsupervised tokenizers often produce inconsistent splits for similar words, but MoVoC's hard constraints create a single, linguistically grounded boundary per word. This sharpens the token distribution, measured as reduced entropy. The method assumes lower entropy correlates with better generalization for low-resource languages. Evidence shows consistent Rényi entropy reductions across all evaluated languages.

## Foundational Learning

**Fusional Morphology**
Why needed here: Geez script languages are fusional, where single morphemes carry multiple grammatical features (tense + person + gender). Standard BPE struggles because it looks for repetitive patterns obscured by complex surface forms. Quick check: How does "fusion" of grammatical features in Amharic differ from "agglutination" in Turkish, and why would standard BPE struggle specifically with the former?

**Tokenization Constraints (Hard vs. Soft)**
Why needed here: MoVoC's core contribution is applying hard constraints during BPE training, distinct from pre-tokenizing or soft regularization. Quick check: In MoVoC's algorithm, what specifically happens during the BPE merge step if a proposed merge candidate crosses a morpheme boundary defined by gold-standard annotation?

**Intrinsic vs. Extrinsic Evaluation in NLP**
Why needed here: MoVoC shows improved intrinsic metrics but not extrinsic translation quality, highlighting a critical evaluation gap. Quick check: Why might a tokenizer produce "linguistically better" tokens (high MorphoScore) yet fail to improve final machine translation output (BLEU score)?

## Architecture Onboarding

**Component map:** Pre-processor → Morphological Analyzer → Vocab Builder → Constrained BPE Trainer

**Critical path:** The dependency on the Morphological Analyzer. Unlike standard tokenizers which are self-contained, MoVoC cannot train without external supervised signals (HornMorpho outputs or manual annotations).

**Design tradeoffs:** Linguistic Fidelity vs. Statistical Flexibility - hard-coding morpheme boundaries gains semantic consistency but loses BPE's statistical flexibility to compress rare sequences efficiently. Resource Cost - requires expert linguistic knowledge or annotated data for every target language, unlike language-agnostic standard BPE.

**Failure signatures:** "Gate" Errors - look for semantic drift where BPE splits a stem into a valid but unrelated word (e.g., the "በሩ" / "gate" example). If MoVoC prevents this, it is working; if it introduces new splits that break stems, constraints are misaligned. OOV Spikes - if morphological vocabulary is too small or specific, expect spikes in Out-Of-Vocabulary tokens or longer sequences for modern/slang text not covered by traditional analyzer.

**First 3 experiments:**
1. **Boundary Ablation:** Train MoVoC using synthetically degraded morphological annotations (5%, 10%, 15% errors) to test robustness to analyzer noise.
2. **Vocabulary Size Sweep:** Compare compression ratio (tokens/word) of MoVoC vs. Standard BPE across different vocabulary sizes (10k, 30k, 50k) to find efficiency crossover point.
3. **Downstream Probing:** Train simple grammatical classifiers (e.g., tense classification) using frozen embeddings from MoVoC vs. BPE to isolate morphology-aware tokenization's impact on feature extraction.

## Open Questions the Paper Calls Out

**Cross-Linguistic Generalization:** The datasets are limited to Ge'ez script languages, affecting generalizability to other morphologically rich language families. Evidence of successful application to languages from Bantu or Turkic families would resolve this question.

**Resource-Constrained Deployment:** The increased complexity of hybrid tokenization may not translate to proportional performance improvements in resource-constrained settings. Comparative analysis of training efficiency and inference speed would determine if linguistic benefits justify computational overhead.

**MT Performance Gap:** The disconnect between intrinsic metric improvements and lack of significant BLEU gains suggests morphology-aware tokenization may not directly translate to translation quality. Modified training regimes or model architectures achieving significant BLEU improvements would resolve this question.

## Limitations

- No statistically significant BLEU/chrf++ improvements despite intrinsic metric gains
- Performance critically depends on quality and coverage of morphological analyzer
- Limited to four Geez script languages, raising questions about cross-linguistic generalization
- Hybrid vocabulary approach may lead to suboptimal compression for domain-specific text

## Confidence

**High Confidence:** Intrinsic metric improvements (MorphoScore up to 0.731, boundary precision up to 88.3%, Rényi entropy reduction) are well-supported by experimental results across all four languages with clear methodology.

**Medium Confidence:** Constrained merge operations prevent semantic fragmentation is supported by qualitative examples but lacks systematic error analysis of prevented vs. introduced errors.

**Low Confidence:** Practical utility claim for low-resource MT is weakened by lack of statistically significant BLEU improvements and acknowledged gap between intrinsic and extrinsic performance.

## Next Checks

1. **Analyzer Robustness Testing:** Systematically degrade morphological annotation quality (5%, 10%, 15% synthetic errors) and measure impact on MoVoC's intrinsic and extrinsic performance to quantify sensitivity to analyzer noise.

2. **Cross-Linguistic Transfer:** Apply MoVoC to morphologically rich languages outside Geez script family (Arabic or Turkish) using language-specific analyzers to test whether consistent improvements generalize to other morphological types.

3. **Downstream Task Probing:** Train simple grammatical classifiers (morphological feature prediction, POS tagging) using frozen embeddings from MoVoC vs. BPE to isolate whether morphology-aware tokenization improves feature extraction for grammatical tasks even when full translation quality doesn't improve.