---
ver: rpa2
title: Mitigating Catastrophic Forgetting in Large Language Models with Forgetting-aware
  Pruning
arxiv_id: '2509.08255'
source_url: https://arxiv.org/abs/2509.08255
tags:
- fapm
- task
- pruning
- downstream
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel pruning-based approach to mitigate
  catastrophic forgetting (CF) in large language models (LLMs) during fine-tuning.
  The method, called Forgetting-Aware Pruning Metric (FAPM), leverages the insight
  that the degree of overlap between task vectors (fine-tuning changes) and pre-trained
  model parameters is a critical factor for CF.
---

# Mitigating Catastrophic Forgetting in Large Language Models with Forgetting-aware Pruning

## Quick Facts
- arXiv ID: 2509.08255
- Source URL: https://arxiv.org/abs/2509.08255
- Reference count: 24
- Primary result: Prunes fine-tuning updates to limit catastrophic forgetting to 0.25% while maintaining 99.67% downstream accuracy

## Executive Summary
This paper addresses catastrophic forgetting (CF) in large language models (LLMs) during fine-tuning by proposing a pruning-based approach that leverages the insight that the degree of overlap between task vectors (fine-tuning changes) and pre-trained model parameters is a critical factor for CF. The method, called Forgetting-Aware Pruning Metric (FAPM), operates post-hoc without modifying the training process or model architecture. By computing a task vector (ΔW = W_ft - W_pre) and applying a novel pruning criterion that balances downstream task performance and CF mitigation, the approach achieves significant CF reduction while maintaining high downstream accuracy across eight diverse datasets using Llama3-8B and Qwen2-7B models.

## Method Summary
The method computes the task vector ΔW = W_ft - W_pre between pre-trained and fine-tuned models, then applies FAPM to score each parameter using S_i = |ΔW_i| - |W_pre|_avg × (|ΔW_i|/|W_pre_i|). This score balances the absolute magnitude of updates with their relative change compared to pre-trained weights. Parameters with lowest scores are pruned (set to zero) at high sparsity (90% default), effectively exposing the underlying pre-trained parameters. The final model is reconstructed as W_final = W_pre + (Mask ⊙ ΔW), where Mask is the binary pruning mask. The approach requires only access to both pre-trained and fine-tuned models and can be completed quickly as a post-processing step.

## Key Results
- Achieves 99.67% downstream task accuracy retention across eight datasets
- Limits catastrophic forgetting to just 0.25% on general benchmarks
- Demonstrates effectiveness for both full fine-tuning and LoRA adaptation
- Shows superior performance compared to magnitude-based and random pruning baselines

## Why This Works (Mechanism)

### Mechanism 1: Task Vector Redundancy Filter
The paper identifies that task vectors contain significant redundancy, where pruning them can expose original pre-trained knowledge without destroying downstream capabilities. By calculating ΔW = W_ft - W_pre and applying high sparsity (e.g., 90%), the method sets negligible updates to zero, effectively "re-exposing" the underlying pre-trained parameter at that index. This assumes pre-trained knowledge is locally stored in specific parameter indices and that fine-tuning updates are sparse. The evidence shows task accuracy remains unaffected even at 90% sparsity, though accuracy collapse may occur if the downstream task requires dense updates.

### Mechanism 2: Relative Change Penalization
CF mitigation requires penalizing parameters where the update magnitude significantly overshadows the original pre-trained weight. FAPM scores parameters using S_i = |ΔW_i| - |W_pre|_avg × (|ΔW_i|/|W_pre_i|), subtracting the "relative change magnitude" from the "absolute magnitude." High relative changes reduce the score, targeting those weights for pruning. This assumes that a large ratio |ΔW|/|W_pre| correlates strongly with disruption of general knowledge. The method must handle potential division by near-zero values in pre-trained weights.

### Mechanism 3: Post-Hoc Weight Restoration
CF can be treated as a post-processing problem rather than a training-time constraint. The method operates as a "plug-and-play" post-processor that takes the fine-tuned model, computes the delta, prunes based on FAPM, and reconstructs the final weights. This decouples the mitigation strategy from the optimization loop, requiring access to both pre-trained and fine-tuned weights. However, this mechanism cannot restore knowledge already overwritten in sequential fine-tuning scenarios where intermediate states are lost.

## Foundational Learning

- **Concept: Task Vectors / Task Arithmetic**
  - Why needed here: FAPM relies on the hypothesis that "knowledge" is directional in weight space, requiring understanding that subtracting W_pre from W_ft isolates the "skill" learned.
  - Quick check question: If a model is fine-tuned and then reverted to its pre-trained state, what is the value of the task vector?

- **Concept: Unstructured Pruning vs. Structured Pruning**
  - Why needed here: The paper uses unstructured pruning (zeroing individual weights), which preserves the exact architecture unlike methods that remove entire heads or layers.
  - Quick check question: Does unstructured pruning reduce the parameter count (tensor size) or just the number of non-zero values?

- **Concept: Relative vs. Absolute Magnitude**
  - Why needed here: The core innovation distinguishes between a large update to a small weight (high relative impact) vs. a large update to a large weight, driving the FAPM formula.
  - Quick check question: If W_pre = 0.001 and ΔW = 0.1, is the relative change higher or lower than if W_pre = 10.0 and ΔW = 1.0?

## Architecture Onboarding

- **Component map:** Input Loader -> Delta Calculator -> Metric Engine -> Pruner -> Merger
- **Critical path:** The Metric Engine is the bottleneck, requiring correct calculation of |W_pre|_avg per layer to scale the penalty term. Incorrect scaling causes one term to dominate, leading to either zero pruning or excessive pruning.
- **Design tradeoffs:**
  - Sparsity Ratio (90% default): Higher sparsity preserves generality but risks downstream accuracy; lower sparsity preserves accuracy but retains CF.
  - LoRA Support: Adapts FAPM for LoRA by treating W_loraB × W_loraA as the task vector, but suggests lower pruning rates (e.g., 10%) are needed because LoRA matrices are already low-rank.
- **Failure signatures:**
  - NaN/Inf generation: Likely due to division by zero if W_pre has exact zeros and no epsilon is added.
  - Accuracy Collapse (>90% CF): Suggests pruning criteria is inverted or scaling factor is mismatched between layers.
  - No Forgetting Mitigation: Likely sparsity ratio is too low (<70%).
- **First 3 experiments:**
  1. Magnitude vs. FAPM Ablation: Fine-tune on RTE, prune at 90% using raw magnitude vs. FAPM, plot downstream accuracy vs. general benchmarks.
  2. Sparsity Sweep: Run FAPM on Llama3-8B with sparsity [50%, 70%, 90%, 95%] to identify the "elbow" where generality is recovered without breaking the downstream task.
  3. LoRA Adaptation: Apply LoRA-specific version of FAPM to verify if reducing LoRA rank post-hoc improves generality.

## Open Questions the Paper Calls Out
The paper explicitly identifies determining the integration of FAPM with existing CF techniques as an important direction for future research, noting that it did not investigate synergistic effects with training-time methods like regularization or replay. The authors also acknowledge that the layer-wise averaging factor |W_pre|_avg was based on practical observations rather than theoretical derivation, leaving room for optimization. Additionally, they note that FAPM requires relatively small pruning ratios for LoRA fine-tuning due to the low-rank nature of LoRA matrices, questioning whether the criterion can be adapted for high sparsity in such scenarios.

## Limitations
- Only validates against magnitude-based and random pruning baselines, not other forgetting mitigation strategies like EWC or weight regularization
- Sequential fine-tuning experiments lack statistical significance measures across multiple runs
- LoRA adaptation experiments are limited to 10% sparsity with no comparison to standard LoRA fine-tuning baselines
- Claims of "plug-and-play" applicability are overstated given special handling required for LoRA

## Confidence
- **High confidence**: Experimental results showing 99.67% downstream retention and 0.25% CF are well-documented and reproducible
- **Medium confidence**: Theoretical justification for relative change magnitude predicting forgetting is plausible but lacks cross-task validation
- **Low confidence**: Claim that FAPM works as a "plug-and-play" post-processor for any fine-tuning method is overstated

## Next Checks
1. **Ablation on Relative Change Term**: Create modified FAPM removing the relative change penalty term to empirically validate whether this component is essential for CF mitigation using the same RTE fine-tuning experiment
2. **Cross-Task Generalization**: Apply FAPM to fine-tuning on massively different domains (e.g., medical to legal text) where task vectors are expected to be dense, measuring both accuracy collapse and forgetting recovery
3. **Statistical Significance Testing**: Re-run sequential fine-tuning experiments with 5 random seeds and report confidence intervals for CF measurements to establish statistical significance of observed improvements