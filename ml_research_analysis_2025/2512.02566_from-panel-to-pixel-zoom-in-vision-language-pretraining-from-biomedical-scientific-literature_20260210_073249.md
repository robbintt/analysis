---
ver: rpa2
title: 'From Panel to Pixel: Zoom-In Vision-Language Pretraining from Biomedical Scientific
  Literature'
arxiv_id: '2512.02566'
source_url: https://arxiv.org/abs/2512.02566
tags:
- panel
- biomedical
- figure
- region
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving fine-grained visual-textual
  alignment in biomedical vision-language models. Current approaches compress rich
  scientific figures and text into coarse figure-level pairs, missing the localized
  grounding clinicians rely on.
---

# From Panel to Pixel: Zoom-In Vision-Language Pretraining from Biomedical Scientific Literature

## Quick Facts
- arXiv ID: 2512.02566
- Source URL: https://arxiv.org/abs/2512.02566
- Reference count: 40
- Improves fine-grained visual-textual alignment in biomedical vision-language models using 60% less data than prior work

## Executive Summary
This paper addresses the challenge of improving fine-grained visual-textual alignment in biomedical vision-language models. Current approaches compress rich scientific figures and text into coarse figure-level pairs, missing the localized grounding clinicians rely on. The authors propose Panel2Patch, a data generation pipeline that automatically mines hierarchical structure from scientific literature, including multi-panel figures, visual markers, and surrounding text, to construct multi-granular supervision at figure, panel, and patch levels. They also design a hierarchical zoom-in pretraining framework with inter-level message passing that enhances biomedical multimodal representations using multi-granularity context and fine-grained correspondences.

## Method Summary
The authors propose Panel2Patch, a pipeline that uses an off-the-shelf large vision-language model (Qwen2.5-VL-72B) with Set-of-Marks prompting to parse layouts and detect visual markers in biomedical figures. This generates hierarchical data: figures decomposed into panels and regions with associated text. They design a hierarchical zoom-in pretraining framework that uses contrastive learning with inter-level message passing between figure, panel, and region embeddings. The model alternates training across granularities to prevent catastrophic forgetting while maintaining fine-grained alignment.

## Key Results
- Achieves state-of-the-art performance across multiple biomedical benchmarks using 60% less data than prior work
- Demonstrates superior fine-grained alignment through improved patch-level and panel-level retrieval tasks
- Shows the hierarchical message passing approach significantly outperforms single-granularity training baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing multi-panel figures into a hierarchy of figure, panel, and patch provides superior supervision for fine-grained alignment compared to coarse figure-level pairs.
- **Mechanism:** The Panel2Patch pipeline uses Qwen2.5-VL-72B with Set-of-Marks prompting to parse layouts and detect visual markers, fusing these with caption-based localization to extract region-level bounding boxes and hierarchical text descriptions.
- **Core assumption:** Scientific figures follow consistent pedagogical conventions that general-purpose LVLMs can reliably parse without domain-specific fine-tuning.
- **Evidence anchors:** "Panel2Patch parses layouts, panels, and visual markers... preserving local semantics" and "We treat the visual markers already overlaid... as implicit Set-of-Mark (SoM) cues."
- **Break condition:** If figures lack clear visual markers or use non-standard layouts, the LVLM may hallucinate bounding boxes or fail to align captions.

### Mechanism 2
- **Claim:** Inter-level message passing enhances panel-level embedding by injecting global context and local evidence.
- **Mechanism:** The framework aggregates embeddings from children to parents using average pooling and enforces consistency via contrastive loss between parent and aggregated child embeddings.
- **Core assumption:** The semantic content of a composite figure can be effectively summarized by the average of its component embeddings.
- **Evidence anchors:** "...inter-level message passing that enhances biomedical multimodal representations..." and "This ensures the figure embedding is consistent with what its constituent panels collectively encode."
- **Break condition:** If panels are semantically disjoint or contradictory, averaging their embeddings may create a confused representation.

### Mechanism 3
- **Claim:** Alternating training cycles prevent catastrophic forgetting and handle data imbalance between granularities.
- **Mechanism:** Instead of joint training, the model cycles through supervision levels (M → P → R) to ensure parameters are updated for one granularity before drifting too far from another's needs.
- **Core assumption:** Distinct granularities have competing optimization landscapes that cannot be easily traversed simultaneously without degradation.
- **Evidence anchors:** "Cycling supervision ensures that recently updated parameters are revisited... preserving previously learned semantics" and Table 5 showing single-level models overfit to specific scales.
- **Break condition:** If the cycle period is too long, the model may still suffer from interim forgetting of specific granularities.

## Foundational Learning

- **Concept: Set-of-Marks (SoM) Prompting**
  - **Why needed here:** Used to force the LVLM to output structured bounding boxes and identifiers for panels.
  - **Quick check question:** Can you explain how marking visual cues (like "A", "B") helps an LVLM localize objects compared to standard captioning?

- **Concept: Contrastive Learning (CLIP)**
  - **Why needed here:** The core loss function relies on maximizing similarity between positive image-text pairs and minimizing it for negatives.
  - **Quick check question:** How does the "inter-level" loss in this paper differ from standard image-text contrastive loss?

- **Concept: ROI (Region of Interest) Pooling**
  - **Why needed here:** Used to extract feature-level crops from the panel feature map to align with region-level visual evidence.
  - **Quick check question:** Why is ROI pooling necessary for enforcing consistency between pixel-level crops and feature-level representations?

## Architecture Onboarding

- **Component map:** Qwen2.5-VL-72B (Data Engine) -> ViT-L/14 Vision Encoder (initialized from BMC-CLIP) -> Hierarchical projection layers for Figure/Panel/Region embeddings

- **Critical path:**
  1. Run Panel2Patch on raw biomedical figures to generate hierarchical JSON/boxes
  2. Apply NMS and coordinate clipping to clean LVLM outputs
  3. Execute alternating coarse-to-fine training loop (M → P → R)

- **Design tradeoffs:**
  - Scalability vs. Precision: The pipeline relies on a 72B parameter LVLM (high GPU cost: ~2,900 GPU-hours total) but avoids human annotation
  - Frozen vs. Fine-tuned: Text encoder and early vision layers frozen while only last 5 vision blocks are updated, prioritizing retention of pre-trained knowledge

- **Failure signatures:**
  - Degenerate Boxes: Extremely small aspect ratios or areas (handled by post-processing)
  - Hallucinated Markers: LVLM generating boxes for markers that do not exist (mitigated by marker-proximity gating)
  - Empty JSON: LVLM failing to parse complex layouts (requires robust prompt engineering)

- **First 3 experiments:**
  1. Validation of Data Quality: Manually inspect a sample (e.g., 2,000 images) to verify panel decomposition accuracy (~80% reported)
  2. Single-Panel Retrieval: Test the model on external short-context retrieval benchmarks to verify panel-level alignment
  3. Ablation on Alternating Schedule: Compare "Single → Single" vs. "Ours → Single" to validate the catastrophic forgetting hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does Panel2Patch's fine-grained grounding improve performance on generative tasks compared to discriminative ones?
- Basis in paper: The introduction lists visual question answering (VQA) as a target application, but experiments focus exclusively on discriminative tasks like retrieval and classification.
- Why unresolved: It is unclear if the hierarchical embeddings optimize CLIP-style encoders for generation or if the lack of generative evaluation hides limitations in the learned semantic richness.
- What evidence would resolve it: Evaluation results on standard biomedical VQA benchmarks or qualitative analysis of generated captions for complex multi-panel figures.

### Open Question 2
- Question: Does the Panel2Patch pipeline maintain its efficacy when applied to non-biomedical scientific domains with different visual conventions?
- Basis in paper: Page 2 states the methodology is applicable to "any domain where visual documentation follows pedagogical conventions" (e.g., other scientific fields), though this work validates it only on biomedicine.
- Why unresolved: The reliance on biomedical markers may not transfer directly to fields like physics or material science where markers and layouts differ.
- What evidence would resolve it: Zero-shot or fine-tuning performance of a model trained on a Panel2Patch corpus derived from materials science or physics papers.

### Open Question 3
- Question: How robust is the training process to the inherent noise and hallucinations of the LVLM used to generate the supervision?
- Basis in paper: Section 3.1.2 acknowledges that LVLM proposals are vulnerable to "hallucinated regions" and uses marker fusion to suppress them, but does not quantify the residual noise in the final 1M+ pairs.
- Why unresolved: While the paper shows strong final performance, it does not analyze how the volume of incorrect bounding boxes or captions affects convergence or introduces bias.
- What evidence would resolve it: An ablation study varying the threshold for LVLM confidence or manually auditing a subset of the generated training pairs to correlate noise levels with downstream performance.

### Open Question 4
- Question: Does the demonstrated data efficiency scale to the full magnitude of existing corpora (e.g., 15M+ figures)?
- Basis in paper: The authors use a subset of 350k figures to demonstrate efficiency ("60% less data"), but they do not explore if the gains plateau or if the computational cost outweighs the benefits at web-scale.
- Why unresolved: It is possible that the "quality over quantity" approach has diminishing returns compared to simply scaling the baseline data to the full 24M pairs available.
- What evidence would resolve it: Training curves and performance metrics comparing the Panel2Patch model trained on the full Biomedica dataset against the current subset results.

## Limitations

- The method's reliance on Qwen2.5-VL-72B for parsing scientific figures without domain-specific fine-tuning creates potential for hallucination in marker detection and panel decomposition
- The alternating training schedule lacks specification of exact batch ratios between granularities, making replication challenging
- The ROI pooling approach for patch-level alignment assumes panels maintain sufficient resolution for meaningful sub-region extraction

## Confidence

- **High confidence**: State-of-the-art performance claims on biomedical benchmarks, data generation pipeline effectiveness (1.3M panels, 1.6M regions), and ablation results showing alternating training superiority
- **Medium confidence**: The hierarchical message passing mechanism's contribution (requires more direct ablation), and the generalizability of the approach beyond biomedical literature
- **Low confidence**: The exact caption decomposition methodology and the scalability of the 72B parameter LVLM approach to larger datasets

## Next Checks

1. **Replication stress test**: Generate data from 500 randomly selected biomedical figures using the Panel2Patch pipeline and independently verify panel decomposition accuracy through human annotation
2. **Cross-domain generalization**: Apply the trained model to scientific figures from non-biomedical domains (e.g., climate science, engineering) to test the assumption that visual markers and panel conventions are universally interpretable
3. **Fine-tuning sensitivity analysis**: Systematically vary the freezing strategy (e.g., unfreeze more vision layers) and training schedule (continuous vs. alternating) to quantify the robustness of the reported performance gains