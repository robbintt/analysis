---
ver: rpa2
title: 'Assessing the performance of 8 AI chatbots in bibliographic reference retrieval:
  Grok and DeepSeek outperform ChatGPT, but none are fully accurate'
arxiv_id: '2505.18059'
source_url: https://arxiv.org/abs/2505.18059
tags:
- references
- chatbots
- chatgpt
- academic
- journal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated eight AI chatbots in generating academic bibliographic
  references, revealing that only 26.5% of references were fully correct, with 39.8%
  being fabricated. Grok and DeepSeek outperformed others, producing no hallucinations,
  while Copilot, Perplexity, and Claude had the highest error rates.
---

# Assessing the performance of 8 AI chatbots in bibliographic reference retrieval: Grok and DeepSeek outperform ChatGPT, but none are fully accurate

## Quick Facts
- arXiv ID: 2505.18059
- Source URL: https://arxiv.org/abs/2505.18059
- Reference count: 12
- Key outcome: Only 26.5% of AI-generated bibliographic references were fully correct, with 39.8% being fabricated; Grok and DeepSeek outperformed others with zero hallucinations.

## Executive Summary
This study evaluated eight AI chatbots in generating academic bibliographic references, revealing that only 26.5% of references were fully correct, with 39.8% being fabricated. Grok and DeepSeek outperformed others, producing no hallucinations, while Copilot, Perplexity, and Claude had the highest error rates. Book references were more accurate than journal articles, which had an 78% fabrication rate. The findings underscore significant limitations in current AI models and emphasize the need for improved information literacy and verification practices among students.

## Method Summary
The study used 5 standardized prompts across 5 disciplines (Cardiology, Mechanical Engineering, Organic Chemistry, Sociology, Art History) to evaluate 8 free-tier AI chatbots. Each model was asked to provide 10 relevant academic references formatted in APA 7th edition. Researchers manually verified each reference against Google Scholar, classifying them based on 5 bibliographic elements (author, year, title, source, location) as fully correct, partially correct, or fabricated.

## Key Results
- Only 26.5% of references were fully correct; 39.8% were fabricated
- Grok and DeepSeek achieved 100% accuracy with no hallucinations
- Journal articles had 78% fabrication rate versus 12.9% for books
- Perplexity provided the most recent references (avg age 1.6 years) but 72% were fabricated

## Why This Works (Mechanism)

### Mechanism 1: Salience-Driven Retrieval from Training Data
- Claim: Chatbots are more likely to generate accurate references for "foundational" works that appear frequently in their training corpora than for niche or recent research.
- Mechanism: LLMs function as probabilistic engines; high-frequency sequences (titles/authors of canonical texts) have higher probability weights. When prompted for references, the model retrieves high-salience patterns rather than searching a live database.
- Core assumption: The model's training dataset includes substantial bibliographic text where frequently cited works appear more often.
- Evidence anchors:
  - [Page 12, Table 5]: The most frequently generated references (e.g., *March's Advanced Organic Chemistry*) are seminal works with thousands of citations.
  - [Page 16]: "One of the key parameters guiding reference generation... is the salience or recurrence of a work in the model's training data."

### Mechanism 2: Metadata Interpolation (Hallucination)
- Claim: When a model lacks precise memory of a reference's metadata, it generates plausible-sounding fillers rather than admitting ignorance.
- Mechanism: Generative models minimize perplexity by predicting the most likely next token. If exact metadata is unknown, the model interpolates based on the format of valid citations.
- Core assumption: The model is not connected to a live retrieval tool that can verify metadata at inference time.
- Evidence anchors:
  - [Page 5]: Models tend to "interpolate or 'fill in' data when it cannot retrieve real information, rather than acknowledging the absence."
  - [Page 9, Figure 2]: High error rates in specific elements (Year, Source, Locating data) support the interpolation theory.

### Mechanism 3: Document Type & Accessibility Bias
- Claim: Chatbots show higher accuracy for books than journal articles because books are often over-represented in accessible training data.
- Mechanism: The "data frontier" for books is broader (older, often free) whereas recent journal articles are often behind paywalls, reducing the training signal.
- Core assumption: A significant portion of the training corpus includes scanned books or text from sites hosting book content.
- Evidence anchors:
  - [Page 16]: "78% of journal article references were fabricated... dropped to 12.9% in the case of book references."
  - [Page 16]: Suggests AIs may draw on "websites that do not hold copyright licenses" for books.

## Foundational Learning

- Concept: **Hallucination vs. Fabrication**
  - Why needed here: To distinguish between a model "misremembering" a detail (error) vs. inventing a non-existent entity (fabrication).
  - Quick check question: Is the error a wrong page number (hallucination of detail) or a non-existent author (fabrication of entity)?

- Concept: **Parametric vs. Non-Parametric Memory**
  - Why needed here: Understanding if the answer comes from internal weights (parametric—prone to staleness/hallucination) or a search tool (non-parametric—prone to retrieval errors) is critical for diagnosis.
  - Quick check question: Does the model cite a paper from 2024 using internal knowledge or a search result?

- Concept: **Ground Truthing**
  - Why needed here: The study's methodology relies on verifying every reference against Google Scholar. Users must learn that AI output is a hypothesis, not a fact.
  - Quick check question: Before submitting this reference, have I copied the title into a database to confirm it exists?

## Architecture Onboarding

- Component map: User Request -> Model Processing (Retrieval or Generation) -> Output Formatting -> Manual Verification
- Critical path: 1. User Request -> 2. Model Processing (Retrieval or Generation) -> 3. Output Formatting -> 4. Manual Verification (primary failure point)
- Design tradeoffs:
  - Generalist (ChatGPT/Gemini) vs. Specialist (Elicit/SciSpace): Generalists hallucinate more (39.8% error) but are versatile. Specialists are accurate but limited in scope.
  - Recency vs. Accuracy: Perplexity provided very recent refs (avg age 1.6 yrs) but 72% were fabricated. Older refs (Grok/Books) were more accurate.
- Failure signatures:
  - Generic Names/Titles: "Green, M. L., & Brown, T. J." or "Journal of Social Movements"
  - Repetitive Metadata: Same volume/issue/page numbers across different citations
  - Recency Anomaly: A "2025" reference for a standard textbook query in early 2025
- First 3 experiments:
  1. **The Copilot Stress Test:** Ask for references in two distinct fields. If author names or page numbers repeat identically, the system is using template generation.
  2. **The Book vs. Article Split:** Request 5 book references and 5 journal articles on the same topic. Calculate the accuracy delta.
  3. **The "Pre-2020" Constraint:** Constrain the prompt to "references before 2020" to see if accuracy improves.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do premium versions of AI chatbots significantly reduce hallucination rates and improve bibliographic accuracy compared to their free-access counterparts?
- Basis in paper: [explicit] The authors state in the conclusion that "Future research could... include premium versions of the models..." to assess the "AI divide" between free and paid tools.
- Why unresolved: This study deliberately restricted its methodology to free versions to simulate typical economic constraints of university students.
- What evidence would resolve it: A comparative analysis using the same standardized prompts across both free and paid tiers of the same chatbots.

### Open Question 2
- Question: Is the higher accuracy observed in book references caused by AI models being trained on copyrighted content from legally ambiguous sources?
- Basis in paper: [explicit] The discussion notes that "certain AIs may be drawing on data sourced from websites that do not hold copyright licenses... although this remains a hypothesis that requires further research to be confirmed or refuted."
- Why unresolved: While the study observed higher accuracy for books, it could not verify the specific training data sources used by the proprietary models.
- What evidence would resolve it: Transparency reports from AI companies regarding training corpora, or forensic linguistic analysis matching generated references to known pirated repositories.

### Open Question 3
- Question: Do specialized academic discovery tools outperform general-purpose chatbots in generating valid and contextually relevant bibliographic references?
- Basis in paper: [explicit] The authors suggest that "Future research could... explore specialized academic discovery tools," noting that students are unlikely to use them effectively without guidance.
- Why unresolved: The current analysis focused exclusively on eight general-purpose chatbots, excluding tools specifically designed for academic research.
- What evidence would resolve it: A head-to-head comparison of reference accuracy and hallucination rates between general chatbots and academic-specific tools.

## Limitations

- The study used only free-tier web interfaces which may not reflect full model capabilities
- Manual verification relied solely on Google/Google Scholar searches, potentially missing references in other academic databases
- The narrow temporal window (February 2025) and focus on five specific disciplines limits generalizability

## Confidence

- **High Confidence:** The overall finding that chatbots exhibit significant reliability issues in bibliographic reference generation
- **Medium Confidence:** The comparative performance rankings between models (Grok and DeepSeek outperforming others)
- **Medium Confidence:** The document type bias (books being more accurate than journal articles)

## Next Checks

1. **Temporal Stability Test:** Repeat the study with the same prompts after three months to assess whether model performance changes with updates or fine-tuning.
2. **Database Cross-Verification:** Validate a subset of references using multiple academic databases (Scopus, Web of Science) rather than relying solely on Google Scholar.
3. **Prompt Variation Analysis:** Test whether more specific prompts yield better accuracy than open-ended requests for references on a topic.