---
ver: rpa2
title: 'Beyond MMD: Evaluating Graph Generative Models with Geometric Deep Learning'
arxiv_id: '2512.14241'
source_url: https://arxiv.org/abs/2512.14241
tags:
- graph
- graphs
- nodes
- networks
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of evaluating Graph Generative
  Models (GGMs) by proposing a novel methodology called RGM (Representation-aware
  Graph-generation Model evaluation) that overcomes the limitations of traditional
  MMD metrics. The core method leverages a Siamese Graph Neural Network trained to
  classify graphs into distinct domains, enabling evaluation of whether generated
  graphs preserve structural characteristics that distinguish different graph types.
---

# Beyond MMD: Evaluating Graph Generative Models with Geometric Deep Learning

## Quick Facts
- **arXiv ID**: 2512.14241
- **Source URL**: https://arxiv.org/abs/2512.14241
- **Reference count**: 40
- **Primary result**: MMD metrics fail to detect structural deficiencies in state-of-the-art GGMs, while RGM framework reveals that generated graphs lose domain-specific characteristics.

## Executive Summary
This paper addresses the fundamental problem that traditional Maximum Mean Discrepancy (MMD) metrics fail to adequately evaluate Graph Generative Models (GGMs). The authors propose RGM (Representation-aware Graph-generation Model evaluation), a Siamese Graph Neural Network framework that learns structural fingerprints distinguishing different graph domains. Through classification experiments, they demonstrate that while GGMs like GRAN and EDGE achieve low MMD values, they fail to preserve essential structural properties that distinguish graph types. The RGM framework achieves high classification accuracy on original graphs but reveals that generated graphs lose domain-specific characteristics, often being misclassified as biological regardless of their true class.

## Method Summary
The RGM framework uses a Siamese Graph Neural Network trained to classify graphs into distinct domains through triplet loss optimization. The model receives triplets (anchor, positive same-class, negative different-class) and learns embeddings where structurally similar graphs cluster. Node features (degree, chi-square statistic, local clustering coefficient, k-core number) are extracted using graph-tool and fed into a 3-layer GAT with 4 attention heads. The model is trained with AdamW optimizer and evaluated via dynamic k-NN classification. The framework is tested on a custom dataset containing 5 real-world graph classes (Biological, Connectome, Infrastructure, Internet, Social) and 5 synthetic classes (BA, ER, SBM, LFR, nPSO), with 300 graphs per class. Generated graphs from GRAN and EDGE models are evaluated by their classification accuracy in the learned embedding space.

## Key Results
- State-of-the-art GGMs achieve low MMD values but fail to preserve structural properties distinguishing graph domains
- RGM achieves near-perfect classification accuracy (98-100%) on synthetic classes and 83-100% on real-world classes for original graphs
- Generated graphs are predominantly misclassified as biological (97% for Connectome, 83% for Infrastructure, 81% for Internet using GRAN)
- MMD on individual statistics (degree, clustering, orbits) shows low values while RGM classification accuracy reveals significant structural drift

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A Siamese GNN trained on domain classification learns structural fingerprints that detect when generated graphs fail to preserve class-defining properties.
- **Mechanism:** The model receives triplets (anchor, positive same-class, negative different-class) and learns embeddings where structurally similar graphs cluster. When generated graphs are projected into this space, misclassification reveals structural drift invisible to distribution-level metrics like MMD.
- **Core assumption:** Graph domains (Biological, Social, Infrastructure, etc.) possess learnable structural signatures distinguishable by GNN message-passing over topological features.
- **Evidence anchors:**
  - [abstract] "RGM framework demonstrates this by achieving high classification accuracy (near-perfect for synthetic classes, 83-100% for real-world classes) on original graphs but poor performance on generated graphs, with most being misclassified as biological"
  - [Section IV-B] "The model was trained using a supervised approach, where the input consists of triplets of graphs: an anchor graph, a positive graph (same class as the anchor), and a negative graph (different class)"
  - [corpus] Related work (PolyGraph Discrepancy) confirms classifier-based metrics can rank generative models, supporting the discriminative approach.
- **Break condition:** If RGM classification accuracy on original graphs is low (<70%), the learned embeddings lack discriminative power and cannot reliably detect generation failures.

### Mechanism 2
- **Claim:** Topological node features (degree, clustering, k-core, chi-square) provide multiscale structural signals that enable GAT attention to capture both local and global graph properties.
- **Mechanism:** Node degree captures connectivity patterns; chi-square statistics identify local anomalies; clustering coefficient reveals community tightness; k-core number encodes core-periphery structure. GAT attention weights these during message-passing, propagating multiscale information.
- **Core assumption:** These four features jointly encode sufficient information to distinguish graph domains without task-specific hand-engineering.
- **Evidence anchors:**
  - [Section IV-B] "the node degree and chi-square statistic capture local connectivity patterns, the clustering coefficient and k-core number provide information about tightly connected groups of nodes"
  - [Figure 2/3] Generated graphs show misalignment in assortativity, clustering, diameter, and k-core distributions versus baselines.
  - [corpus] No direct corpus evidence on this specific feature set; assumption remains unvalidated externally.
- **Break condition:** If ablation shows removing any single feature causes minimal accuracy drop (<3%), features may be redundant; if accuracy collapses, the feature set is insufficient.

### Mechanism 3
- **Claim:** Low MMD on individual statistics (degree, clustering, orbits) does not guarantee structural fidelity because MMD aggregates distributions independently without capturing joint dependencies.
- **Mechanism:** MMD computes kernel-based distance between marginal distributions of single properties. It cannot detect when correlations between properties (e.g., degree-clustering coupling) are violated, nor when higher-order motifs are missing.
- **Core assumption:** Graph domains are defined by joint structural properties, not marginal distributions of individual statistics.
- **Evidence anchors:**
  - [Section III-A] "MMD primarily compares the average value of the extracted property across the two sets, and may fail to capture differences in the internal distribution of that property within individual graphs"
  - [Tables V/VI] MMD values for NSPDK, Orbits are low (<0.05) while RGM classification fails, demonstrating the disconnect.
  - [corpus] PolyGraph Discrepancy paper similarly notes MMD "does not provide an absolute measure of performance" and is sensitive to kernel parameters.
- **Break condition:** If a GGM achieves low MMD on all metrics AND high RGM classification accuracy, MMD may be sufficient for that domain—contradicting the claim.

## Foundational Learning

- **Concept:** Message-Passing Graph Neural Networks (GCN, GAT, GIN)
  - **Why needed here:** RGM uses GAT layers to aggregate neighborhood information into node embeddings, then pools to graph-level representations. Understanding how attention weights neighborhood contributions is essential for debugging embedding quality.
  - **Quick check question:** Given a 3-layer GAT with 4 heads, what is the receptive field of a node's final representation?

- **Concept:** Siamese Networks and Triplet Loss
  - **Why needed here:** The training objective explicitly shapes the embedding space via triplet margin loss—minimizing anchor-positive distance while maximizing anchor-negative distance. Without this, the classification mechanism fails.
  - **Quick check question:** If triplet margin is set to 1.0 and anchor-positive distance is 0.8 while anchor-negative distance is 1.5, what is the loss value?

- **Concept:** Maximum Mean Discrepancy (MMD) and Kernel Methods
  - **Why needed here:** The paper's central critique targets MMD. Understanding how MMD maps distributions to RKHS via kernels clarifies why marginal statistics can mislead.
  - **Quick check question:** Why does MMD between degree distributions not capture differences in degree-clustering correlation?

## Architecture Onboarding

- **Component map:** Preprocessing -> GAT Encoder -> Global Pooling -> Triplet Training -> Dynamic k-NN Inference
- **Critical path:** Preprocessing quality -> GAT attention learning -> triplet diversity -> embedding separation -> k-NN classification accuracy. If preprocessing is noisy (e.g., chi-square on small neighborhoods), downstream embeddings degrade.
- **Design tradeoffs:**
  - **Hidden dimension (8):** Very small—limits expressiveness but enables fast training. Assumption: graph domains are linearly separable in low-dimensional space.
  - **Dynamic k-NN vs. fixed k:** Handles class imbalance but introduces variability; deterministic alternative would require threshold calibration.
  - **Training/validation split (64/16/20):** Standard but may underrepresent minority classes like Social (only 57% recall).
- **Failure signatures:**
  - Generated graphs classified as "Biological" regardless of true class (Tables III/IV) -> embeddings collapse toward a common mode.
  - High variance in Social class accuracy (57%) -> class may lack consistent structural signature.
  - MMD low but RGM accuracy low -> GGM optimizing wrong objective.
- **First 3 experiments:**
  1. **Sanity check:** Train RGM on original graphs only; verify confusion matrix matches Table II (>90% on synthetic, >80% on most real classes). If not, debug feature extraction or triplet sampling.
  2. **Ablation:** Remove one node feature at a time; measure classification accuracy drop. Identify which features drive domain discrimination.
  3. **GGM evaluation:** Generate 100 graphs per class using GRAN/EDGE; pass through RGM; compare confusion matrix against Table III/IV. If classification accuracy is unexpectedly high (>50% per class), the GGM may have improved or the RGM is overfitting to trivial features.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the RGM framework's classification loss be integrated as a training objective to improve GGM generation quality, rather than serving only as a post-hoc evaluation metric?
- **Basis in paper:** [explicit] The authors state that "insights gained from this evaluation could inform the development of improved Graph Generative Models that explicitly optimize for preserving domain-specific structural characteristics rather than just statistical properties."
- **Why unresolved:** RGM is only used to evaluate already-trained models; the paper does not explore whether embedding-space distances or classification signals could guide generative training directly.
- **What evidence would resolve it:** Experiments training GGMs with an auxiliary RGM-based loss term, demonstrating improved classification accuracy on generated graphs compared to baseline training.

### Open Question 2
- **Question:** Why are generated graphs from diverse domains predominantly misclassified as "Biological" by the RGM framework?
- **Basis in paper:** [inferred] The confusion matrices reveal systematic misclassification toward Biological (97% of generated Connectome graphs, 83% of Infrastructure, 81% of Internet for GRAN), yet the paper does not explain this consistent bias.
- **Why unresolved:** The cause could lie in the embedding space geometry, shared structural properties across domains, limitations of the GGM architectures, or feature selection in RGM.
- **What evidence would resolve it:** Latent space visualization, analysis of which structural features drive Biological classification, and experiments generating from modified architectures to isolate the source of bias.

### Open Question 3
- **Question:** What specific structural properties make Social networks difficult for both GGMs to generate and RGM to classify accurately?
- **Basis in paper:** [explicit] The authors note "classification of the Social class proves more challenging" and attribute this to the "inherently heterogeneous nature of social graphs, which often exhibit a mix of structural patterns, ranging from small-world to scale-free properties."
- **Why unresolved:** The paper identifies the symptom but does not isolate which properties (e.g., community overlap, degree heterogeneity, dynamic structures) cause poor generation and classification.
- **What evidence would resolve it:** Controlled experiments generating graphs with specific structural combinations, feature importance analysis for Social vs. other classes, and per-property MMD decomposition.

## Limitations
- The evaluation framework relies on 4 node features without ablation studies proving their sufficiency, and no external validation exists for this specific feature set.
- The claim that MMD fails because it compares marginal distributions rather than joint properties remains theoretical—no explicit experiment isolates joint feature dependencies.
- Classification accuracy thresholds (83-100% original, 50-100% generated) are justified only by observed distributions, not statistical power analysis.

## Confidence
- **High confidence**: The core mechanism that low MMD does not guarantee structural fidelity, supported by direct comparison tables (Tables V/VI) showing MMD vs. RGM accuracy.
- **Medium confidence**: The four-node-feature set's sufficiency for domain discrimination, based on good RGM accuracy but lacking ablation or external validation.
- **Low confidence**: The claim that RGM embeddings are meaningful without confirming that high accuracy on original graphs translates to detecting generation failures rather than overfitting to spurious correlations.

## Next Checks
1. **Feature Ablation**: Remove each node feature (degree, chi-square, clustering, k-core) individually and measure RGM accuracy drop on original graphs. If accuracy falls >10% per removed feature, the set is sufficient; if minimal drops, the set may be redundant.
2. **Joint Distribution Test**: For a domain pair (e.g., Biological vs. Infrastructure), compute degree-clustering correlation coefficients for originals and generated graphs. Verify that MMD low but correlation mismatch indicates structural drift invisible to MMD.
3. **Controlled Generation**: Train a GGM to preserve only degree distribution (not other features), then classify generated graphs via RGM. If RGM accuracy is low despite perfect degree MMD, joint property preservation is essential.