---
ver: rpa2
title: Some Robustness Properties of Label Cleaning
arxiv_id: '2509.11379'
source_url: https://arxiv.org/abs/2509.11379
tags:
- consistency
- surrogate
- aggregation
- loss
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how aggregating multiple noisy labels (e.g.,
  from crowdsourced workers) affects the consistency of supervised learning models.
  The authors show that aggregation provides robustness that is impossible without
  it.
---

# Some Robustness Properties of Label Cleaning
## Quick Facts
- arXiv ID: 2509.11379
- Source URL: https://arxiv.org/abs/2509.11379
- Reference count: 40
- One-line primary result: Label aggregation provides robustness and consistency guarantees that are impossible without aggregation, even when standard methods fail.

## Executive Summary
This paper investigates how aggregating multiple noisy labels (e.g., from crowdsourced workers) affects the consistency of supervised learning models. The authors show that aggregation provides robustness that is impossible without it. They demonstrate two key benefits: (1) improved consistency when minimizing surrogate losses (like logistic loss) instead of the true task loss (like 0-1 error), and (2) consistency in restricted hypothesis classes where standard methods fail. Specifically, they prove that label aggregation can upgrade inconsistent losses to consistent ones, even with minimal assumptions on the surrogate loss. They also show that for parametric models (like linear classifiers), aggregation enables consistent estimation under model mis-specification, while methods without aggregation fail.

## Method Summary
The paper studies label aggregation in supervised learning where multiple noisy labels are available for each example. The key mechanism is majority vote aggregation that reduces noise in conditional risk estimates. For surrogate risk minimization, aggregation upgrades surrogate consistency from quadratic to near-linear calibration functions. For restricted hypothesis classes, aggregation recovers consistency even when standard surrogate risk minimization produces arbitrarily incorrect solutions. The framework applies to both binary and multiclass settings, with aggregation count m depending on the noise exponent α and output space size k.

## Key Results
- Aggregation upgrades surrogate consistency: Surrogate losses that fail on raw labels become consistent with aggregation, even with minimal assumptions.
- Consistency in restricted classes: Aggregation enables consistent estimation for linear/cone hypothesis classes where standard methods fail.
- Link mis-specification recovery: For multiclass logistic regression (k ≥ 3), aggregation recovers asymptotic consistency under link function mis-specification where raw labels fail.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Label aggregation upgrades surrogate consistency from quadratic to near-linear calibration functions, enabling consistency even for surrogates that fail on raw labels.
- Mechanism: Majority vote aggregation reduces noise in conditional risk estimates. When m labels are aggregated, the error function e_m(t) decreases as O(√(log(k)/m)), allowing level-(ξ_m, ζ) consistency where ψ(ϵ) ≥ ζϵ for ϵ ≥ ξ_m. As m → ∞, ξ_m → 0, recovering uniform linear comparison.
- Core assumption: Surrogate φ is (C_φ,1, C_φ,2)-identifying per Definition 3.1 and distribution satisfies Mammen-Tsybakov noise condition (N_α).
- Evidence anchors:
  - [abstract]: "procedures using label aggregation obtain stronger consistency guarantees than those even possible using raw labels"
  - [section 3.2.3, Theorem 1]: R(f) - R* ≤ (16/C_φ,1)(R_φ,Am(f) - R*_φ,Am) when excess risk exceeds ξ_m threshold
  - [corpus]: Weak support - related papers address label noise robustness but not surrogate consistency amplification
- Break condition: Aggregation count m insufficient such that ξ_m,k = Ω(1), or κ(X) unbounded violating noise conditions.

### Mechanism 2
- Claim: Aggregation recovers consistency for linear/cone hypothesis classes even when standard surrogate risk minimization on raw labels produces arbitrarily incorrect solutions.
- Mechanism: Without aggregation, surrogate minimizers θ_φ can be nearly orthogonal to optimal θ* (Proposition 4). With majority vote A_m, the conditional distribution P_m(·|x) concentrates on y*(x), and minimizing R_φ,Am over cone F yields R(f_m) → R* as ε_m → 0.
- Core assumption: Hypothesis class F is a linear cone and optimally predicts P (exists f ∈ F with argmax f_y(x) = argmax P(Y=y|x)).
- Evidence anchors:
  - [abstract]: "even when models are mis-specified or hypothesis classes are restricted, aggregated labels guarantee consistent classifiers while standard methods fail"
  - [section 4.1-4.2, Proposition 4 & Theorem 2]: Shows |cos∠(θ_φ, θ*)| ≤ ε without aggregation, but R(f_θm) → R(f_θ*) with aggregation
  - [corpus]: Limited - related work focuses on noise robustness, not hypothesis class restrictions
- Break condition: Optimal predictor not in F, or P(Δ(X) > 0) < 1 (no unique Bayes classifier).

### Mechanism 3
- Claim: Aggregation enables consistency under link function mis-specification in multiclass logistic regression (k ≥ 3), where raw labels fail.
- Mechanism: Corrupted link σ_ε violates logistic assumption but satisfies consistency condition (14). With m-vote aggregation, ρ_m converges to one-hot on y*(x). Parameters T_m satisfy ∥T_m∥ → ∞ but T_m/∥T_m∥ → T*/∥T*∥, preserving classification direction.
- Core assumption: Link σ satisfies argmax_i σ_i(t) = argmax_i t_i (consistency condition), and Θ* = U*T* decomposition exists.
- Evidence anchors:
  - [abstract]: "multiclass classification where aggregation recovers asymptotic consistency under link mis-specification"
  - [section 4.3, Proposition 5 & Theorem 3]: Θ_1(ϵ)/∥Θ_1(ϵ)∥ ≠ Θ*/∥Θ*∥ for any ϵ > 0, but Θ_m/∥Θ_m∥ → Θ*/∥Θ*∥ as m → ∞
  - [corpus]: Insufficient - no corpus papers address link mis-specification specifically
- Break condition: Link σ violates consistency condition (14), or covariance structure prevents invertibility of mapping M ↦ E[Z(∇σ_lr(MZ))^T].

## Foundational Learning

- Concept: **Surrogate Risk Minimization**
  - Why needed here: The paper's core contribution concerns when minimizing convex surrogate φ (e.g., logistic loss) implies minimizing task loss ℓ (e.g., 0-1 error). Understanding calibration functions ψ is essential.
  - Quick check question: Given a binary margin-based surrogate φ(s,y) = ϕ(ys) with ϕ convex, state the condition for Fisher consistency.

- Concept: **Mammen-Tsybakov Noise Conditions**
  - Why needed here: Bounds on required aggregation level m depend critically on noise exponent α. Higher α (easier problem) requires less aggregation.
  - Quick check question: If P(d∘f ≠ d∘f*) ≤ C(R(f) - R*)^α with α = 0.5, what does this imply about decision boundary sharpness?

- Concept: **Identifying Surrogates (Definition 3.1)**
  - Why needed here: The paper's main theorems require φ to be (C_φ,1, C_φ,2)-identifying. This is weaker than convexity or standard consistency—any surrogate with identifiable minimizers per class suffices.
  - Quick check question: For hinge loss φ(s,y) = max{1-ys,0}, verify the identifying constants C_φ,1, C_φ,2.

## Architecture Onboarding

- Component map:
  Raw labels Z = (Y¹,...,Yᵐ) ∈ Yᵐ
  ↓
  Aggregator Aₘ: Z → A (e.g., majority vote to a_ŷ ∈ {a_y})
  ↓
  Surrogate loss φ(f(X), Aₘ(Z))
  ↓
  Minimizer fₘ ∈ εₘ-argmin_{f∈F} E[φ(f(X), Aₘ(Z))]
  ↓
  Task predictor d∘fₘ(X)

- Critical path:
  1. Verify (C_φ,1, C_φ,2)-identifiability for chosen surrogate
  2. Determine required m from noise exponent α and output space size k
  3. Implement aggregator Aₘ with proper tie-breaking
  4. Train on aggregated labels using surrogate φ
  5. Apply decoder d at inference time

- Design tradeoffs:
  - Larger m → stronger consistency (smaller ξ_m) but higher labeling cost
  - Binary (k=2): M=1 suffices; Multiclass (k≥3): optimize M from noise conditions
  - Nonparametric F: Proposition 3 applies; Restricted F: need Theorem 2 conditions

- Failure signatures:
  - Aggregation m < O(log k): ξ_m,k bounded away from 0, comparison inequality fails for small excess risk
  - High noise condition number κ(X): requires larger m to overcome P(Aₘ(Z) ≠ a_y*)
  - Mis-specified link without aggregation: Θ_1(ϵ) diverges from Θ* direction (k ≥ 3)

- First 3 experiments:
  1. **Binary classification with inconsistent surrogate**: Use truncated quadratic ϕ(δ) = max{1-δ,0}². Compare calibration ψ_m with varying m ∈ {2^16, 2^20, ..., 2^32}. Expect transition from ψ(ϵ) = ϵ² to near-linear behavior.
  2. **Bipartite matching structured prediction**: N=4 vertices (k = N! = 24 labelings). Use structured hinge loss (Eq. 11). Verify Lemma 3.2 constants: C_φ,1 = 1/N, C_φ,2 = 2. Test with m ∈ {1, 4, 16, 64, 256}.
  3. **Mis-specified multiclass logistic regression**: k=3 classes, d=2 features. Corrupt link: σ̃(t) = σ_lr(t)1{∥t∥ ≤ r} + σ_lr(αt)1{∥t∥ > r}. Track ∥T_m/∥T_m∥ - T*/∥T*∥∥ vs m. Expect convergence to 0 despite mis-specification.

## Open Questions the Paper Calls Out
None

## Limitations
- The results rely on strong technical conditions (Mammen-Tsybakov noise, identifying surrogates, consistency conditions) that may not hold in practice.
- The required aggregation levels m = O((log k)/α) may be impractical for large output spaces or high noise regimes.
- The multiclass results (k ≥ 3) require specific consistency conditions on the link function that may not hold for realistic corruption models.

## Confidence
**High confidence**: The theoretical framework and proofs appear sound within their stated assumptions. The mechanism by which aggregation reduces noise in conditional risk estimates is well-established. The numerical experiments demonstrate the claimed benefits under controlled conditions.

**Medium confidence**: The practical relevance depends heavily on whether the technical conditions (noise conditions, identifying surrogates, consistency conditions) hold in real applications. The paper provides limited empirical validation beyond synthetic examples.

**Low confidence**: The generalization to arbitrary hypothesis classes and surrogate losses without careful verification of the identifying and consistency conditions.

## Next Checks
1. **Empirical noise condition verification**: Analyze real-world datasets to estimate the Mammen-Tsybakov noise exponent α and verify whether the required aggregation levels m = O((log k)/α) are practical.

2. **Surrogate identification testing**: For common surrogates (logistic loss, cross-entropy, hinge loss), systematically verify the identifying constants C_φ,1, C_φ,2 across diverse datasets and determine which losses fail without aggregation.

3. **Restricted class robustness**: Test the hypothesis class consistency claims on real structured prediction tasks where the optimal model is known to be outside the restricted class (e.g., linear models for nonlinear problems), measuring the gap between aggregated and raw label performance.