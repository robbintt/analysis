---
ver: rpa2
title: Contrastive Learning Using Graph Embeddings for Domain Adaptation of Language
  Models in the Process Industry
arxiv_id: '2510.04631'
source_url: https://arxiv.org/abs/2510.04631
tags:
- text
- graph
- data
- language
- logs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a graph-based domain adaptation approach for
  language models in the process industry. The method adapts SciNCL's contrastive
  learning framework by incorporating heterogeneous knowledge graphs from plant operations
  data, where text logs and functional locations are interconnected through various
  relationship types.
---

# Contrastive Learning Using Graph Embeddings for Domain Adaptation of Language Models in the Process Industry

## Quick Facts
- arXiv ID: 2510.04631
- Source URL: https://arxiv.org/abs/2510.04631
- Reference count: 30
- mBERT fine-tuned with graph embeddings outperforms mE5-large by 9.8-14.3% on Process Industry Text Embedding Benchmark (PITEB)

## Executive Summary
This paper presents a domain adaptation approach for language models in the process industry using graph embeddings for contrastive learning. The method adapts SciNCL's framework to incorporate heterogeneous knowledge graphs built from plant operations data, where text logs and functional locations are interconnected. The approach generates document triplets from graph embeddings to fine-tune small language models, achieving state-of-the-art performance on a proprietary benchmark while using three times fewer parameters than competing methods.

## Method Summary
The method constructs heterogeneous knowledge graphs from plant operation logs and functional locations, then applies link prediction to restore missing edges. Graph embeddings are trained using PyTorch BigGraph with sentence embedding initialization, creating continuous similarity signals. Document triplets are sampled using FAISS kNN search in the embedding space, combining positive pairs from direct graph connections with hard and easy negatives. The approach employs two-stage fine-tuning: first document-level similarity learning using triplet margin loss, then query-document retrieval training using Multiple Negatives Ranking Loss with bi-encoder architecture.

## Key Results
- mBERT fine-tuned with graph embeddings outperforms mE5-large text encoder by 9.8-14.3% on PITEB
- Best performance achieved with two-stage fine-tuning combining document similarity and query-document retrieval
- Models using graph embedding-derived triplets achieve superior results with three times fewer parameters
- Link prediction MRR of 47.80 achieved when graph embedding model initialized with sentence embeddings

## Why This Works (Mechanism)

### Mechanism 1
Graph embeddings encode domain-specific semantic relationships between documents that text-only approaches miss, particularly for jargon and abbreviations. Nodes connected via edges in the knowledge graph are positioned closer in embedding space, capturing indirect semantic relationships even without direct document-to-document links. Core assumption: structural relationships in the domain KG reflect genuine semantic similarity relevant to the target task.

### Mechanism 2
Two-stage fine-tuning (document-level similarity, then query-document retrieval) transfers graph-encoded structural knowledge into the text encoder more effectively than single-stage training. Stage 1 uses triplet margin loss to align document embeddings with graph-derived similarity; Stage 2 refines for retrieval using Multiple Negatives Ranking Loss with hard negatives from GE-triplets. Core assumption: document-level semantic alignment learned in Stage 1 provides better initialization for Stage 2 retrieval training than starting from a general-purpose LM.

### Mechanism 3
Hard negative mining from graph embedding neighbors creates more informative training signal than random negatives, improving contrastive learning efficiency. Graph embedding space provides semantically meaningful hard negatives (k=50th neighbor) that are dissimilar enough to avoid collision but challenging enough to improve discrimination. Core assumption: the graph embedding space correctly orders semantic similarity such that neighbors at k=50 are semantically distinct but retrieval-relevant.

## Foundational Learning

- **Contrastive Learning (Triplet Loss)**
  - Why needed here: Core training objective that positions similar documents close and dissimilar documents apart in embedding space using anchor-positive-negative triplets
  - Quick check question: Given three document embeddings, can you compute the triplet margin loss with ξ=0.5 margin?

- **Knowledge Graph Embeddings (Node2Vec/TransE families)**
  - Why needed here: Provides the continuous similarity signal for triplet sampling—nodes connected by edges are optimized to have similar embeddings
  - Quick check question: In a heterogeneous graph with `reports_about` edges connecting text logs to functional locations, how would a random walk embedding capture indirect document-document similarity?

- **Bi-encoder Architecture for Retrieval**
  - Why needed here: Stage 2 training architecture—separate encoders for query and document that produce comparable embeddings for similarity-based retrieval
  - Quick check question: How does Multiple Negatives Ranking Loss differ from triplet margin loss in how it treats in-batch negatives?

## Architecture Onboarding

- **Component map:** Knowledge Graph Construction → Link Prediction Module → Graph Embedding Model → Triplet Sampling Pipeline → Two-Stage Fine-Tuning
- **Critical path:** Graph embedding quality → triplet quality → Stage 1 alignment → Stage 2 retrieval performance. Table 6 shows link prediction MRR of 47.80 with sentence embedding initialization—this is the upstream bottleneck.
- **Design tradeoffs:**
  - k+ = 2 (positives) vs. k+ larger: Smaller k ensures direct graph connections are included, but may miss semantic neighbors in sparser graphs
  - GE-based triplets in Stage 2 as hard negatives vs. positives: Paper converts them to negatives to increase difficulty, but this discards positive signal
  - mBERT (179M params) vs. larger models: Smaller model wins on efficiency and this benchmark, but generalization to other domains unknown
- **Failure signatures:**
  - Low link prediction MRR (<0.3): Graph embeddings won't capture meaningful structure; triplet sampling becomes near-random
  - Text logs <100 chars filtered too aggressively: Loses graph nodes needed for connectivity (paper notes this would remove ~50% of documents)
  - Model performance degrades on unseen plants: Overfitting to training plant graph structure; Figure 3 shows mixed results across plants
- **First 3 experiments:**
  1. Train graph embedding model without sentence embedding initialization; verify link prediction MRR drops (Table 6 shows 16.96 → 47.80 difference)
  2. Run Stage 1 training with randomly sampled negatives vs. GE-derived hard negatives; measure MAP@10 delta
  3. Evaluate best model on plant data completely excluded from triplet generation (plants B, E, F in Table 2); assess generalization gap vs. plants A, C, D, G

## Open Questions the Paper Calls Out

- **Question:** Does expanding the knowledge graph (KG) using named entity recognition (NER) to extract entities like chemicals and products improve the performance of the fine-tuned language models?
- **Basis in paper:** [explicit] The authors state in the Discussion, "In future work, we plan to expand our KG by incorporating named entity recognition (NER), which will enable us to enrich the graph with additional entities and relationship types, such as chemicals and products."
- **Why unresolved:** The current implementation is limited to text logs and functional locations; the specific value added by granular technical entities has not yet been measured

- **Question:** How does the methodology perform across varying conditions, such as differing levels of graph connectivity, text log quality, and preprocessing link prediction methods?
- **Basis in paper:** [explicit] The Limitations section notes, "Further investigation is needed to assess the scalability and generalizability of the approach across various conditions, including the connectivity of knowledge graphs, the quality of text logs, the methods used in the preprocessing step for link prediction..."
- **Why unresolved:** The experiments were conducted on a restricted dataset (seven plants), and it is unclear if the positive results hold when graph sparsity or data noise increases significantly

- **Question:** Can the graph embedding-based triplet generation methodology be effectively utilized to create synthetic training data for auxiliary tasks like question answering, document ranking, or text classification?
- **Basis in paper:** [explicit] The Discussion states, "the methodology can be further explored to generate data for auxiliary tasks, such as question answering, document ranking, and text classification."
- **Why unresolved:** The current study only validates the approach for semantic search (bi-encoder fine-tuning); utility for other downstream tasks remains hypothetical

## Limitations
- Proprietary PITEB benchmark prevents independent validation and comparison with published methods
- Link prediction models for reconstructing missing KG edges lack detailed implementation specifications
- Limited evaluation across only 7 plants with uneven distribution raises concerns about generalizability

## Confidence
- **High**: The two-stage fine-tuning approach improves retrieval performance over single-stage training (supported by ablation showing systematic improvement across 3/4 models)
- **Medium**: Graph embeddings provide meaningful semantic signal for contrastive learning (supported by link prediction MRR of 47.80, though initialization dependence is unclear)
- **Low**: The approach generalizes to unseen plants (only mixed results shown in Figure 3; no rigorous out-of-domain testing performed)

## Next Checks
1. Evaluate the best model on plants completely excluded from the triplet generation process (plants B, E, F from Table 2) to measure true generalization capability
2. Perform ablation study comparing GE-derived hard negatives vs. random negatives in Stage 1 training to quantify contribution of graph-based triplet sampling
3. Train graph embedding model without sentence embedding initialization to validate the claimed improvement in link prediction MRR (16.96 → 47.80) and assess sensitivity to initialization