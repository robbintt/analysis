---
ver: rpa2
title: Federated Learning for ICD Classification with Lightweight Models and Pretrained
  Embeddings
arxiv_id: '2507.03122'
source_url: https://arxiv.org/abs/2507.03122
tags:
- deepresmlp
- deepmlp
- training
- federated
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates the feasibility of federated learning (FL)
  for multi-label ICD code classification from clinical notes. A modular pipeline
  is proposed that decouples embedding extraction from classification, using frozen
  LLM embeddings (Qwen3-Embed-0.6B, gte-Qwen2-1.5B-instruct, etc.) and lightweight
  MLP classifiers.
---

# Federated Learning for ICD Classification with Lightweight Models and Pretrained Embeddings

## Quick Facts
- **arXiv ID**: 2507.03122
- **Source URL**: https://arxiv.org/abs/2507.03122
- **Reference count**: 36
- **Primary result**: Modular FL pipeline with frozen embeddings and lightweight MLPs achieves competitive multi-label ICD classification while preserving privacy.

## Executive Summary
This study presents a modular federated learning pipeline for multi-label ICD code classification from clinical notes. The approach decouples embedding extraction from classification: frozen large language model embeddings (Qwen3-Embed-0.6B, gte-Qwen2-1.5B-instruct) are extracted locally and never shared, while lightweight MLPs (1.6M-7.1M parameters) are trained in a federated manner using Flower. Experiments on MIMIC-IV data show that embedding quality substantially outweighs classifier complexity in determining performance, with top configurations achieving micro F1 scores up to 0.43 on ICD-9 and 0.41 on ICD-10, closely matching centralized results. The findings demonstrate that privacy-conscious, modular FL systems can deliver competitive results in clinical NLP, offering a scalable path toward real-world deployment in healthcare.

## Method Summary
The method employs a two-stage pipeline: (1) offline embedding extraction using six pretrained models from MTEB (max tokens 8192), and (2) federated training of lightweight classifiers (MLP, DeepMLP, DeepResMLP) with Flower framework. The pipeline uses HybridLoss (BCE + Focal) and AdamW optimizer with cosine annealing. Data is stratified across 20 simulated clients (≥10 samples per label per client). Embeddings are standardized using training-set-only statistics, and labels are binarized. FedAvg aggregation runs for 100 communication rounds with all clients participating each round.

## Key Results
- Embedding quality substantially outweighs classifier complexity in determining predictive performance
- Federated learning results closely match centralized counterparts, with Qwen3-Embed-0.6B + MLP achieving micro F1 of 0.3944 on ICD-10 (vs. 0.3942 centralized)
- Top configurations achieve micro F1 scores up to 0.43 on ICD-9 and 0.41 on ICD-10
- Ablation studies confirm robustness across stratified splits

## Why This Works (Mechanism)

### Mechanism 1
Pretrained embedding models (0.6B–1.5B parameters) compress clinical text into semantically enriched dense vectors (768–1536 dim). This front-loads semantic abstraction, allowing shallow classifiers (1.6M–7.1M parameters) to focus on label mapping rather than representation learning. The embedding space captures diagnostic semantics across heterogeneous clinical text, reducing the burden on downstream architectures. Core assumption: Frozen embeddings transfer sufficiently to clinical domain without task-specific fine-tuning; rare or domain-specific codes may underperform. Evidence: "embedding quality substantially outweighs classifier complexity in determining predictive performance" and "Qwen3-Embed-0.6B with MLP achieves the highest micro F1 score on ICD-10 (0.3942)... deeper classifiers like DeepResMLP did not consistently outperform MLP."

### Mechanism 2
Each site encodes clinical notes locally using identical frozen embedding models. Dense vectors (768–1536 dim) are non-invertible—lossy, many-to-one transformations that prevent exact text reconstruction. Only embeddings and lightweight classifier gradients are shared, never raw text. This creates a two-layer privacy abstraction: (1) local embedding computation, (2) federated training of compact classifiers. Core assumption: Embedding inversion attacks remain impractical at these dimensions. Evidence: "dense vector embeddings are non-invertible, lossy representations... the transformation is many-to-one, nonlinear, and not uniquely decodable" and "embedding is performed offline and locally, and the resulting vectors serve as a standardized, privacy-preserving input."

### Mechanism 3
With standardized embedding inputs, federated averaging (FedAvg) aggregates gradients from simple classifiers across 20 simulated clients. The small parameter count (≤7.1M) reduces communication overhead and convergence sensitivity. Performance gap between centralized and FL is minimal (e.g., ICD-10 micro F1: 0.3942 centralized vs. 0.3944 FL with Qwen3-Embed + MLP). Core assumption: Data is stratified and reasonably balanced across clients (≥10 samples per label per client); real-world heterogeneity may degrade results. Evidence: "FL results closely match their centralized counterparts, especially for MLP classifiers... Qwen3-Embed-0.6B + MLP achieves a micro F1 of 0.3944 on ICD-10, nearly identical to centralized training (0.3942)."

## Foundational Learning

- **Multi-label classification with extreme label spaces (1,000+ codes)**: ICD coding assigns multiple codes per document; macro F1 reflects per-label performance while micro F1 reflects overall instance-level performance. Quick check: Can you explain why macro F1 (0.27–0.30) is much lower than micro F1 (0.39–0.43) in this task, and what this implies about model behavior on rare codes?

- **Federated averaging (FedAvg) with horizontal data partitioning**: The paper uses Flower framework with FedAvg across 20 simulated clients, each holding disjoint subsets of the same feature space. Understanding gradient aggregation and communication rounds is essential for reproducing the FL setup. Quick check: If client data is non-IID (e.g., some clients only see ICD-9, others only ICD-10), how would you expect FedAvg convergence to change compared to the stratified setup used here?

- **Frozen text embeddings as feature extractors**: The paper decouples representation (frozen LLM embeddings) from classification (trainable MLPs). This is a transfer learning paradigm where the embedding model is not updated during downstream training. Quick check: What are the trade-offs of using frozen vs. fine-tuned embeddings for clinical text, particularly for rare ICD codes?

## Architecture Onboarding

- **Component map**: Raw clinical text → Embedding model (Qwen3-Embed, gte-Qwen2, etc.) → Dense vectors (768–1536 dim) → MLP/DeepMLP/DeepResMLP classifier → Multi-label ICD predictions
- **Critical path**: Select embedding model from MTEB leaderboard → Preprocess MIMIC-IV discharge summaries → Compute embeddings offline → Train classifier (start with MLP baseline) in centralized mode → Deploy FL simulation with Flower → Compare centralized vs. FL performance → Run ablation with 10 random stratified splits
- **Design tradeoffs**: Embedding quality vs. inference latency (larger models achieve higher semantic quality but slower inference); classifier depth vs. stability (deeper classifiers occasionally outperform but are less stable in FL); privacy vs. end-to-end optimization (frozen embeddings prevent raw text leakage but may underfit rare codes)
- **Failure signatures**: DeepMLP/DeepResMLP underperformance in FL (convergence sensitivity); low macro F1 on rare codes (embedding representations insufficient); large performance gap between centralized and FL (data heterogeneity or client imbalance)
- **First 3 experiments**: (1) Train MLP classifier on Qwen3-Embed-0.6B embeddings in centralized mode; target micro F1 ≥0.39 on ICD-10; (2) Run FL simulation with 20 clients, 50 rounds; plot training loss per round; (3) Swap Qwen3-Embed for nomic-embed-text-v1.5; expect ~3–4% drop in micro F1

## Open Questions the Paper Calls Out

1. **How does the modular federated pipeline perform under realistic non-IID data conditions?** The study utilized a balanced split strategy with a minimum of 10 samples per label per client, which creates a homogeneous environment rarely found in real-world hospital networks. Experiments utilizing Dirichlet-based partitioning to simulate label distribution skew across the 20 simulated nodes would measure performance degradation compared to the centralized baseline.

2. **Can the integration of domain-specific fine-tuning or hybrid architectures improve performance over frozen embeddings without sacrificing the efficiency of the lightweight pipeline?** The current architecture decouples embedding extraction from classification to ensure privacy and reduce computational load, leaving the performance ceiling of this specific approach untested against adaptive methods. A comparative analysis evaluating the trade-offs between the current frozen-embedding MLPs and models that allow for partial or full fine-tuning of the embedding layers within the federated framework would provide insight.

3. **To what extent does label ambiguity and annotation noise in the MIMIC-IV dataset affect the reliability of the reported F1 scores?** The evaluation relies solely on the standard MIMIC-IV ground truth labels, which are known to be noisy, without differentiating between model errors and label ambiguity. A qualitative error analysis involving clinical experts or the implementation of uncertainty-aware loss functions would determine if the model is failing to learn or simply learning noisy labels.

## Limitations

- The study relies on frozen embeddings without task-specific fine-tuning, which may limit performance on highly specialized or rare ICD codes
- The federated learning simulations use idealized, stratified data partitions that may not reflect real-world non-IID distributions
- No empirical validation of embedding non-invertibility against inversion attacks is provided
- Real-world client heterogeneity, dropout, and communication efficiency are not tested

## Confidence

- **High Confidence**: Embedding quality dominates classifier complexity; modular privacy-preserving pipeline design
- **Medium Confidence**: FL performance closely matches centralized results under controlled simulations; embedding non-invertibility
- **Low Confidence**: Real-world applicability and scalability to multi-site hospitals

## Next Checks

1. **Ablation with End-to-End Fine-Tuning**: Compare frozen embeddings vs. fine-tuned embeddings (e.g., Qwen3-Embed-0.6B + MLP vs. same model fine-tuned on MIMIC) to quantify the trade-off between privacy and performance, especially for rare ICD codes.

2. **Non-IID Client Distribution Test**: Simulate federated training with heterogeneous client data (e.g., some clients only see ICD-9, others only ICD-10) and measure convergence degradation and performance gaps relative to the stratified setup.

3. **Embedding Inversion Feasibility Study**: Train a supervised decoder on clinical text ↔ embedding pairs (from MIMIC or similar) to assess whether approximate reconstruction is possible, thereby quantifying the actual privacy risk of the frozen embedding approach.