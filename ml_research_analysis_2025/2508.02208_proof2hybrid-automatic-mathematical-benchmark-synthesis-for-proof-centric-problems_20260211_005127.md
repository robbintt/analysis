---
ver: rpa2
title: 'Proof2Hybrid: Automatic Mathematical Benchmark Synthesis for Proof-Centric
  Problems'
arxiv_id: '2508.02208'
source_url: https://arxiv.org/abs/2508.02208
tags:
- mathematical
- proof
- items
- llms
- alggeotest
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Proof2Hybrid, a fully automated framework
  for synthesizing high-quality, proof-centric mathematical benchmarks from natural
  language corpora. The key innovation is Proof2X, a roadmap that converts mathematical
  proofs into verifiable question formats, enabling robust evaluation while mitigating
  guessing and pattern-matching.
---

# Proof2Hybrid: Automatic Mathematical Benchmark Synthesis for Proof-Centric Problems

## Quick Facts
- **arXiv ID:** 2508.02208
- **Source URL:** https://arxiv.org/abs/2508.02208
- **Reference count:** 14
- **Primary result:** Proof2Hybrid framework automatically synthesizes high-quality proof-centric mathematical benchmarks from natural language corpora, demonstrating significant deficits in LLM mathematical reasoning capabilities

## Executive Summary
Proof2Hybrid introduces an automated framework for synthesizing proof-centric mathematical benchmarks from natural language corpora, addressing the challenge of scalable evaluation in mathematical reasoning. The framework converts mathematical proofs into verifiable "m-out-of-n multiple judge" hybrid question formats, enabling robust evaluation while mitigating guessing and pattern-matching. Applied to algebraic geometry, the resulting AlgGeoTest benchmark contains 456 challenging items that effectively distinguish between model capabilities, revealing significant gaps in even state-of-the-art LLMs' mathematical reasoning abilities.

## Method Summary
The Proof2Hybrid framework employs a four-stage pipeline: (1) Seed filtration where 4 judge LLMs validate proposition-proof pairs, retaining items deemed correct ≥8/12 times; (2) Distractor generation where 5 LLMs create strategic variations by altering keywords, conditions, or formulas; (3) Distractor filtration where 4 judge LLMs filter out items that are too obviously incorrect or logically undecidable, retaining distractors deemed incorrect 7-10/12 times; (4) Aggregation into hybrid questions with m=2 correct items and n=6 total items per question. The framework uses two evaluation protocols: generation-based scoring (loose metric with full/half/zero credit, tight metric requiring both correct) and perplexity-based evaluation for base models.

## Key Results
- AlgGeoTest benchmark contains 456 challenging items synthesized from The Stacks Project algebraic geometry corpus
- Even best-performing LLMs achieve only moderate scores, revealing significant deficits in mathematical reasoning capabilities
- Benchmark effectively distinguishes between reasoning and non-reasoning models, with performance gaps widening as context length increases
- Base models show distinct scaling trends when evaluated via perplexity rather than generation tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The "m-out-of-n" hybrid question format reduces evaluation noise from random guessing and pattern matching
- **Mechanism:** Requires models to identify exactly m correct items from n total items, reducing random success probability to 1/C(n,m) and forcing evaluation of each item's logical merits
- **Core assumption:** Distractors are plausible enough to prevent trivial exclusion through superficial heuristics
- **Evidence anchors:** Abstract mentions format is "resilient to guessing"; Figure 3 compares random guessing accuracy; analysis details mathematical basis for reduced guessing probability

### Mechanism 2
- **Claim:** Multi-stage, multi-model filtration pipeline mitigates low-quality or ambiguous benchmark items
- **Mechanism:** Separates generation and verification, using multiple LLMs to assess items and retain only those passing consistency thresholds
- **Core assumption:** Collective judgment of SOTA LLMs provides reliable proxy for mathematical validity during filtration
- **Evidence anchors:** Figure 1 illustrates Filter and Judge Models workflow; filtration description details k3, k4 thresholds; contrasts with human-expert benchmarks

### Mechanism 3
- **Claim:** Automating synthesis from natural language corpora enables scalable evaluation of proof-centric reasoning
- **Mechanism:** Proof2X roadmap converts unstructured proposition-proof pairs into structured, verifiable judgment tasks
- **Core assumption:** Verification ability correlates with deeper ability to generate or understand mathematical concepts
- **Evidence anchors:** Claims to synthesize from natural language corpora; contrasts with manual creation's cost; addresses difficulty of evaluating research-level math

## Foundational Learning

- **Concept: Algebraic Geometry Schemes & Morphisms**
  - **Why needed:** AlgGeoTest is built on "The Stacks Project" - understanding schemes, morphisms, sheaves is necessary to interpret LLM failure modes
  - **Quick check:** Can you distinguish between "topological property" and "schematic property" in proof error context?

- **Concept: Perplexity-Based Evaluation**
  - **Why needed:** Paper uses perplexity protocol for base models rather than generation to measure how "surprising" correct answer is
  - **Quick check:** Why might base model score poorly on generation but show distinct scaling trends via perplexity?

- **Concept: Consensus Decoding / Model-as-Judge**
  - **Why needed:** Filtration phase relies on multiple models agreeing on validity - understanding judge model biases is critical
  - **Quick check:** If all judge models are conservative, how would this shift final benchmark difficulty distribution?

## Architecture Onboarding

- **Component map:** Natural Language Corpus -> Seed Filter (4 judges × 3 runs) -> Generator (5 models × 6 candidates) -> Distractor Filter (4 judges × 3 runs) -> Aggregator (m seeds + n-m distractors) -> Evaluator (Loose/Tight scoring)

- **Critical path:** Distractor Filtration step - failure here makes benchmark trivial (distractors too easy) or noisy (distractors ambiguous). k3 and k4 thresholds are primary quality control levers.

- **Design tradeoffs:** Automation vs. Accuracy (scale vs. potential judge model blind spots); Difficulty vs. Solvability (m=2, n=6 balances low guess rate with distinct seed items)

- **Failure signatures:** High baseline accuracy (>40% on weak models) indicates obvious distractors; Audit mismatch (find >10% flawed "correct" seeds) suggests too lenient Seed Filter

- **First 3 experiments:**
  1. Parameter Sweep on Filtration: Run pipeline on 50 seeds varying k3/k4 to visualize trade-off between distractor volume and "deceptiveness"
  2. Cross-Domain Validation: Apply framework to different corpus (Topology/Number Theory) to verify domain-agnosticism
  3. Contamination Check: Compare scores of models trained on Stacks Project vs. untrained to determine reasoning vs. memorization

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can Proof2Hybrid framework generalize to mathematical domains with less structured natural language corpora than Stacks Project?
- **Basis in paper:** [explicit] Claims framework enables "scalable generation across broad spectrum of mathematical domains," but only algebraic geometry demonstrated
- **Why unresolved:** AlgGeoTest relies on Stacks Project's specific "tag" structure; unclear if pipeline maintains quality with unstructured textbooks/research papers
- **What evidence would resolve it:** Successful synthesis of high-quality benchmarks in analysis/topology using unstructured LaTeX/plain text sources

### Open Question 2
- **Question:** Does "m-out-of-n" hybrid format correlate with ability to generate valid proofs de novo?
- **Basis in paper:** [inferred] Introduction emphasizes assessing ability to "construct valid proofs," yet methodology converts proofs into classification tasks
- **Why unresolved:** Discriminative tasks (identifying correct proofs) differ fundamentally from generative tasks (synthesizing them)
- **What evidence would resolve it:** Correlation study comparing model rankings on AlgGeoTest against generative theorem-proving benchmarks

### Open Question 3
- **Question:** To what extent does automated filtration pipeline introduce capability ceiling based on judge models?
- **Basis in paper:** [explicit] Distractor filtration relies on leading LLMs to discard errors too easy to spot or logically undecidable
- **Why unresolved:** If judge models fail to detect subtle flaws, benchmark will fail to filter "deceptively plausible" distractors
- **What evidence would resolve it:** Comparing benchmark quality when using different tiers of judge models to see if distractors remain challenging for stronger models

## Limitations

- Framework's reliance on LLMs as judges introduces uncertainty about mathematical validity as proxy for truth, particularly for research-level mathematics
- Domain-specific nature of AlgGeoTest (algebraic geometry) raises questions about generalizability of reasoning vs. non-reasoning model gaps
- Paper provides audit sampling evidence but lacks systematic error analysis of filtration pipeline for comprehensive quality characterization

## Confidence

- **High Confidence:** m-out-of-n hybrid format mechanism for reducing guessing probability is mathematically sound and well-documented
- **Medium Confidence:** Effectiveness of multi-stage filtration pipeline supported by audit sampling but not exhaustively validated
- **Medium Confidence:** Claim that AlgGeoTest effectively distinguishes between model capabilities supported by observed performance gaps but domain-specificity not fully explored

## Next Checks

1. **Filtration Error Analysis:** Systematically categorize and quantify types of errors (mathematical, logical, linguistic) that pass through Seed Filter and Distractor Filter stages using larger audit sample than 20 items
2. **Cross-Domain Replication:** Apply Proof2Hybrid to different mathematical domain (basic topology/number theory) and compare reasoning vs. non-reasoning model performance gap to AlgGeoTest
3. **Judge Model Bias Characterization:** Analyze consistency of judge model decisions across different mathematical domains and proof structures to identify systematic biases or blind spots