---
ver: rpa2
title: Peer-to-Peer Energy Trading in Dairy Farms using Multi-Agent Reinforcement
  Learning
arxiv_id: '2511.23148'
source_url: https://arxiv.org/abs/2511.23148
tags:
- energy
- trading
- battery
- learning
- demand
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses energy management challenges in dairy farming
  communities by developing a MARL-based P2P energy trading framework. The method
  integrates DQN and PPO algorithms with auction-based market clearing and a price
  advisor agent to optimize energy distribution while respecting dairy operational
  constraints.
---

# Peer-to-Peer Energy Trading in Dairy Farms using Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2511.23148
- Source URL: https://arxiv.org/abs/2511.23148
- Reference count: 40
- One-line primary result: MARL framework achieves 14.2% cost reduction in Ireland and 5.16% in Finland while reducing peak demand by up to 55.5%

## Executive Summary
This study develops a MARL-based P2P energy trading framework for dairy farming communities, integrating DQN and PPO algorithms with auction-based market clearing and a price advisor agent. The framework optimizes energy distribution while respecting dairy operational constraints like milking and cooling schedules. Results show significant improvements over traditional rule-based approaches, with DQN reducing electricity costs by 14.2% in Ireland and 5.16% in Finland while increasing revenue by 7.24% and 12.73%, respectively. PPO achieves the lowest peak hour demand, reducing it by 55.5% in Ireland.

## Method Summary
The framework implements a MARL approach where 10 dairy farms operate as prosumers with PV generation and battery storage. Agents use DQN or PPO algorithms to select from 8 discrete actions (charge/buy, sell, discharge, etc.) based on local observations of load, generation, SoC, and market prices. A central Price Advisor calculates dynamic internal prices using Supply-Demand Ratio, while a Double Auction mechanism clears trades between agents. The reward function is binary and highly conditional, encouraging actions that minimize grid purchases during peak hours and maintain battery charge for load shifting.

## Key Results
- DQN reduces electricity costs by 14.2% in Ireland and 5.16% in Finland
- PPO achieves lowest peak hour demand reduction of 55.5% in Ireland
- DQN reduces peak demand by 50.0% in Ireland and 27.02% in Finland
- Framework increases revenue by 7.24% (Ireland) and 12.73% (Finland) compared to rule-based approaches

## Why This Works (Mechanism)

### Mechanism 1: Temporal Arbitrage via Pre-Peak Battery Priming
If agents charge batteries during "Near-Peak" periods (15:00-17:00) rather than only when generation is available, they minimize grid purchases during expensive "Peak" hours (17:00-19:00). The reward signal incentivizes maintaining high State of Charge before grid tariffs spike, allowing discharge during peak windows.

### Mechanism 2: SDR-Driven Internal Pricing (The Price Advisor)
If internal community prices are calculated dynamically based on Supply-Demand Ratio rather than fixed rates, agents are incentivized to trade locally. When local supply is high (SDR > 1), Internal Selling Price drops toward Feed-in Tariff while Internal Buying Price stays lower than grid buy price, encouraging local transactions.

### Mechanism 3: Policy-to-Market Interface via Double Auction
If agents output discrete actions into a Double Auction mechanism, the system achieves global equilibrium while preserving policy privacy. Agents submit bids and asks based on local observations, and the auctioneer matches these orders, decoupling learning from market clearing logic.

## Foundational Learning

- **Concept: Markov Decision Process (MDP) Formulation**
  - Why needed here: The paper frames energy management as an MDP, requiring understanding of how continuous physical states translate into discrete observation vectors
  - Quick check question: Can you identify the 4 dimensions of the observation space and why historical data is not included?

- **Concept: On-Policy (PPO) vs. Off-Policy (DQN) Learning**
  - Why needed here: The paper compares DQN and PPO, explaining why PPO is more robust/scalable while DQN is more sample-efficient but sensitive to parameters
  - Quick check question: Why does the study suggest PPO is more robust to increasing agent counts compared to DQN?

- **Concept: Reward Shaping for Constraints**
  - Why needed here: The "Dairy-Constrained" aspect is implemented entirely via the reward function, not hard constraints
  - Quick check question: How does the reward function penalize selling energy when battery SoC is low (<20%) but the tariff is Peak?

## Architecture Onboarding

- **Component map:** Environment (ParallelEnv) -> Agents (DQN/PPO) -> Price Advisor (SDR calculation) -> Auctioneer (Double Auction clearing) -> Grid settlement
- **Critical path:** Load profiles → Battery initialization → Agent observation → Action selection → State update → Market submission → Price calculation → Order matching → Grid settlement
- **Design tradeoffs:** Homogeneous policies speed training but ignore individual farm differences; discrete actions simplify learning but reduce granularity compared to continuous control
- **Failure signatures:** Synchronized selling floods market crashing internal price to FiT levels; reward hacking where agents buy from grid just to sell to peers
- **First 3 experiments:** Baseline validation with rule-based simulator; ablation of "Near-Peak" condition to verify peak reduction mechanism; scalability stress test by increasing agent count from 10 to 50

## Open Questions the Paper Calls Out
None

## Limitations
- Binary reward structure creates convergence risks with zero rewards for invalid action-state combinations
- "Near-Peak" optimization variable is ambiguously defined in tariff structure
- Dataset access limitations prevent precise numerical replication of exact costs and parameters

## Confidence
- High confidence: Cost reduction claims (14.2% Ireland, 5.16% Finland) and peak demand reduction (55.5% Ireland)
- Medium confidence: Revenue increase claims (7.24% Ireland, 12.73% Finland) require careful interpretation
- Low confidence: Generalizability claims across different locations require validation beyond two studied regions

## Next Checks
1. Implement ablation study removing "Near-Peak" condition to verify if peak reduction is driven by pre-peak battery priming
2. Conduct scalability stress test by increasing agent count from 10 to 50 to observe training stability
3. Perform edge case analysis for SDR calculation to prevent division-by-zero errors when Total Buying Power approaches zero