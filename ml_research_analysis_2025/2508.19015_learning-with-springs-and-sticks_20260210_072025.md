---
ver: rpa2
title: Learning with springs and sticks
arxiv_id: '2508.19015'
source_url: https://arxiv.org/abs/2508.19015
tags:
- system
- energy
- learning
- sticks
- springs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a physical model for machine learning based
  on springs and sticks, showing that it can perform regression tasks with performance
  comparable to multi-layer perceptrons. The model uses sticks to approximate functions
  piecewise-linearly and springs to encode mean squared error loss, converging to
  optimal configurations via dissipation.
---

# Learning with springs and sticks

## Quick Facts
- **arXiv ID**: 2508.19015
- **Source URL**: https://arxiv.org/abs/2508.19015
- **Authors**: Luis Mantilla Calderón; Alán Aspuru-Guzik
- **Reference count**: 40
- **Primary result**: A physical model of springs and sticks can perform regression with performance comparable to multi-layer perceptrons, subject to a thermodynamic learning barrier.

## Executive Summary
This paper introduces a physical learning model composed of springs and sticks that can perform regression tasks by approximating functions piecewise-linearly and minimizing a mean squared error loss through damped stochastic dynamics. The system converges to a minimum-energy configuration via dissipation, with performance comparable to standard MLPs on synthetic 2D regression tasks. The authors identify a thermodynamic learning barrier (TLB) that represents a fundamental energy scale below which learning cannot occur, proportional to model expressivity and environmental conditions.

## Method Summary
The method uses a grid of sticks to approximate functions piecewise-linearly, where each stick's position represents the function value at that grid point. Data points are connected to the grid via springs, with the spring potential energy encoding the mean squared error loss. The system evolves according to Langevin dynamics (stochastic differential equations) that include deterministic forces from the springs, friction, and thermal noise. Through energy dissipation, the system naturally settles into a configuration that minimizes the loss function. The model uses mini-batch training with Euler-Maruyama integration for numerical simulation.

## Key Results
- The springs and sticks model achieves regression performance comparable to single-layer MLPs on synthetic 2D functions.
- A thermodynamic learning barrier exists that prevents learning when the system's free energy change falls below a minimum threshold.
- The TLB is empirically found to be proportional to model expressivity (number of sticks), environmental temperature, and friction coefficient.
- The approximation error scales as O(N⁻²) for smooth functions, matching the trapezoidal integration error.

## Why This Works (Mechanism)

### Mechanism 1: Energy-Based Optimization via Physical Dynamics
The mechanical system learns by minimizing potential energy through damped stochastic dynamics. Springs connect data points to the stick grid, with their restorative forces corresponding to prediction errors. The Langevin dynamics naturally dissipate energy, causing the system to settle into a minimum-energy state that represents the optimal piecewise-linear approximation. The mechanism fails if damping is insufficient (causing oscillation) or thermal noise is too high (preventing stable convergence).

### Mechanism 2: Function Approximation via Piecewise-Linear Interpolation
The model implements a piecewise-linear fit using a grid of sticks. Each stick's position represents the function value at that grid point, and predictions are linear interpolations between adjacent sticks. This achieves O(N⁻²) approximation error for smooth functions with bounded Hessians, equivalent to trapezoidal integration. The approximation fails for non-smooth functions or insufficient grid resolution.

### Mechanism 3: Thermodynamic Learning Barrier (TLB)
Learning requires the system to transition from high to low energy states, dissipating free energy. The TLB represents a minimum free energy change required for stable convergence. If the system is too small or environmental fluctuations are too high, the free energy change falls below this barrier and learning fails. This barrier is proportional to model expressivity, temperature, and friction coefficient.

## Foundational Learning

### Concept: Langevin Dynamics / Stochastic Differential Equations (SDEs)
**Why needed**: The core model is governed by SDEs that include deterministic forces, friction, and thermal noise. Understanding these terms is essential for simulation and control.
**Quick check**: Can you explain how the terms in dẋ/dt = M⁻¹f(x, ẋ, t) - γẋ + σξ̇(t) correspond to physical forces and thermal bath effects?

### Concept: Lagrangian Mechanics
**Why needed**: The system is formally defined using a Lagrangian derived from kinetic and potential energies, providing the physical basis for equations of motion.
**Quick check**: What are the components of the Lagrangian L for the springs and sticks system, and what physical principle does the Euler-Lagrange equation represent?

### Concept: Stochastic Thermodynamics and the Jarzynski Equality
**Why needed**: This framework is used to analyze thermodynamic properties and calculate free energy changes, which are central to the TLB concept.
**Quick check**: How does the Jarzynski equality allow calculation of free energy differences from non-equilibrium work trajectories, and why is this important for the paper's claims?

## Architecture Onboarding

### Component map:
Data Points -> Springs -> Stick Grid -> Langevin Dynamics -> Loss Minimization

### Critical path:
1. Map input/output dimensions to (d+m) space
2. Construct stick grid and initialize positions randomly
3. Select mini-batch of data points (springs) per epoch
4. Numerically integrate Langevin equations using Euler-Maruyama
5. Steady-state stick positions represent learned function

### Design tradeoffs:
- **Expressivity vs. Curse of Dimensionality**: More sticks improve approximation but scale exponentially with input dimension
- **Dataset Size vs. Simulation Speed**: Number of springs scales with dataset size; use mini-batches to reduce computational cost
- **Precision vs. Thermodynamic Cost**: Approximation error relates to minimum energy required to overcome TLB

### Failure signatures:
- **Oscillation/Non-Convergence**: Low friction relative to spring constant and mass causes underdamped oscillation
- **Thermodynamic Learning Barrier Failure**: System too small or temperature too high; loss plateaus at high value
- **Numerical Instability**: Improper SDE integration leads to simulation failure

### First 3 experiments:
1. **Replicate 1D/2D Regression**: Implement for simple function (e.g., y = x²) and compare training loss and final fit against MLP
2. **Probe the Thermodynamic Learning Barrier**: Vary system scale (spring constant k) and temperature T, then plot final steady-state loss against calculated free energy change
3. **Analyze Hyperparameter Effects**: Sweep friction coefficient (γ) and number of sticks (N_s), measuring impact on convergence speed and minimum achievable loss

## Open Questions the Paper Calls Out

### Open Question 1
**Question**: Can thermodynamic learning barriers be derived analytically rather than empirically, and do they generalize to other learning systems beyond the springs-and-sticks model?
**Basis**: The authors believe TLBs are common in other ML models and hope they help benchmark energy-efficient hardware, but only demonstrate empirically for SS model.
**What evidence would resolve it**: Analytical derivation of TLB from physical principles, or empirical demonstration in neural networks or other physical learning systems.

### Open Question 2
**Question**: How can the SS model be extended to "deep" architectures, and does depth provide the same benefits as in traditional deep learning?
**Basis**: The authors list extending SS to deep models as an interesting research direction.
**What evidence would resolve it**: Formulation of multi-layer SS dynamics and demonstration of improved performance on hierarchical tasks.

### Open Question 3
**Question**: Can the SS model be implemented efficiently in physical hardware (electronic circuits or quantum systems) to achieve practical energy advantages over conventional computing?
**Basis**: The authors propose mapping dynamics to electronic circuits and implementing in quantum computers, but only provide simulations.
**What evidence would resolve it**: Working hardware prototype with measured energy consumption compared to equivalent digital implementation.

## Limitations
- Key experimental parameters (number of epochs, initialization scheme, Boltzmann's constant value) are not fully specified, hindering reproducibility
- Thermodynamic learning barrier is presented empirically without complete theoretical derivation
- Universal approximation claims rely on smoothness assumptions that may not hold for complex, non-differentiable functions
- Only demonstrated on 2D regression tasks; scalability to high-dimensional problems unaddressed

## Confidence

### High Confidence:
- Piecewise-linear interpolation mechanism is well-defined and mathematically grounded
- Connection between springs and MSE loss is clear and verifiable

### Medium Confidence:
- Energy-based optimization via Langevin dynamics is plausible and supported by literature
- Convergence properties depend heavily on uncharacterized hyperparameters

### Low Confidence:
- Thermodynamic learning barrier is most speculative claim
- Relationship to neural scaling laws is not proven

## Next Checks

1. **Verify Reproducibility of Basic Regression**: Implement 1D and 2D regression experiments exactly as described to confirm performance parity with MLPs.

2. **Probe the TLB Under Controlled Conditions**: Systematically vary spring constant (k), temperature (T), and friction (γ) to test claimed proportionality relationships of the TLB.

3. **Stress-Test Approximation Limits**: Test model on functions with discontinuities or non-smooth regions to validate smoothness assumptions required for O(N⁻²) error bound.