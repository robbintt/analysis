---
ver: rpa2
title: Proxy Model-Guided Reinforcement Learning for Client Selection in Federated
  Recommendation
arxiv_id: '2508.10401'
source_url: https://arxiv.org/abs/2508.10401
tags:
- client
- training
- local
- selection
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of client selection in federated
  recommender systems, where traditional random selection strategies fail to account
  for the statistical heterogeneity of user data and lead to suboptimal performance.
  The authors propose ProxyRL-FRS, a proxy model-guided reinforcement learning framework
  that tackles two key issues: the high cost of contribution evaluation and sparse
  updates due to long-tail item distributions.'
---

# Proxy Model-Guided Reinforcement Learning for Client Selection in Federated Recommendation

## Quick Facts
- **arXiv ID:** 2508.10401
- **Source URL:** https://arxiv.org/abs/2508.10401
- **Reference count:** 40
- **Primary result:** ProxyRL-FRS achieves 16.5-16.7% higher HR@20 and NDCG@20 on sparse Fashion dataset compared to state-of-the-art baselines

## Executive Summary
This paper addresses the challenge of client selection in federated recommender systems, where traditional random selection strategies fail to account for the statistical heterogeneity of user data and lead to suboptimal performance. The authors propose ProxyRL-FRS, a proxy model-guided reinforcement learning framework that tackles two key issues: the high cost of contribution evaluation and sparse updates due to long-tail item distributions. The core innovation is a dual-branch ProxyNCF model that augments standard Neural Collaborative Filtering with a lightweight proxy branch for efficient contribution estimation, eliminating the need for expensive per-round local training. Additionally, a staleness-aware reinforcement learning agent is designed to select clients based on proxy-estimated contributions while balancing recommendation accuracy and embedding staleness. Experiments on three public recommendation datasets demonstrate that ProxyRL-FRS outperforms state-of-the-art baselines, achieving significant improvements in recommendation accuracy while maintaining comparable training efficiency and faster convergence.

## Method Summary
ProxyRL-FRS introduces a dual-branch Neural Collaborative Filtering architecture where a lightweight proxy branch predicts client training losses for efficient contribution estimation, eliminating the need for expensive per-round local training. A staleness-aware reinforcement learning agent selects clients based on these proxy predictions, balancing recommendation accuracy with embedding freshness through a weighted reward function. The framework uses FedAvg to aggregate local updates and incorporates an actor-critic architecture for sequential client selection decisions, with the state representation consisting of predicted losses from all clients.

## Key Results
- Achieves 16.5% and 16.7% higher HR@20 and NDCG@20 respectively on sparse Amazon Fashion dataset
- Outperforms state-of-the-art baselines across MovieLens-1M, Amazon Fashion, and Amazon Video Games datasets
- Maintains comparable training efficiency while demonstrating faster convergence rates
- Effectively addresses the sparse updates problem for long-tail items through staleness-aware selection

## Why This Works (Mechanism)

### Mechanism 1: Proxy-Based Contribution Estimation
- Claim: A lightweight proxy branch can predict client training loss accurately enough for selection decisions without requiring full local training.
- Mechanism: The proxy branch shares the same MLP architecture as the NCF branch but is trained via regression (MSE) to predict the actual triplet losses computed during NCF training. This decouples contribution estimation from recommendation learning.
- Core assumption: The relationship between (user embedding, item embedding) pairs and training loss is learnable and stable across rounds.
- Evidence anchors:
  - [abstract] "eliminating the need for expensive per-round local training traditionally required to evaluate a client's contribution"
  - [section IV-A-3b] Eq. 15: LProxy = Σ(ℓ̂u,v,j - ℓu,v,j)² showing regression training
  - [corpus] Weak direct validation; related work on contribution estimation exists (Owen Sampling, Detect & Score) but does not validate proxy prediction accuracy specifically.
- Break condition: If predicted and true losses become decorrelated (e.g., due to distribution shift), selection quality degrades.

### Mechanism 2: Staleness-Aware Reward Balancing
- Claim: Explicitly penalizing embedding staleness in the RL reward increases update coverage for long-tail items and improves convergence.
- Mechanism: The reward r(t) = λ·Acc(t) - (1-λ)·Staleness(t) combines validation accuracy with average embedding staleness (normalized rounds-since-last-update). The RL agent learns to select clients that refresh stale embeddings.
- Core assumption: Clients with stale item interactions can be identified via proxy losses and staleness statistics without knowing their private interaction sets.
- Evidence anchors:
  - [abstract] "reward function balancing recommendation accuracy and embedding staleness"
  - [section IV-B-1] Eq. 17-18 formalize the reward and staleness metric
  - [corpus] No direct validation of staleness-aware rewards in federated recommendation; concept is novel to this paper.
- Break condition: If λ is poorly tuned (too high or low), the agent over-emphasizes one objective, causing either poor accuracy or stale embeddings.

### Mechanism 3: Actor-Critic RL for Sequential Client Selection
- Claim: Framing client selection as an RL problem enables adaptive, context-aware selection that outperforms heuristic or clustering methods.
- Mechanism: State = predicted losses from all clients; Actor outputs probability distribution over clients via softmax; Critic estimates expected cumulative reward. Policy gradient updates guide selection toward high-reward clients.
- Core assumption: The federated training environment behaves sufficiently like an MDP for RL to learn effective policies.
- Evidence anchors:
  - [section IV-B-1] "Actor samples client subset U(t)+ ∼ πθ(·|st)"
  - [Table III] ProxyRL-FRS achieves highest HR@20 and NDCG@20 across all datasets
  - [corpus] FedRL and multi-agent RL approaches for FL exist, but specific actor-critic designs vary; no direct architectural comparison.
- Break condition: If state representation (predicted losses) is noisy or delayed, policy learning becomes unstable.

## Foundational Learning

- Concept: **Neural Collaborative Filtering (NCF)**
  - Why needed here: ProxyNCF extends NCF with a proxy branch; understanding the base architecture is prerequisite.
  - Quick check question: Can you explain how NCF combines user and item embeddings to predict interaction scores?

- Concept: **Federated Averaging (FedAvg)**
  - Why needed here: The global model update uses FedAvg to aggregate local parameters and embeddings.
  - Quick check question: How does FedAvg weight client updates, and what happens when client data sizes differ significantly?

- Concept: **Actor-Critic Reinforcement Learning**
  - Why needed here: The staleness-aware selector uses actor-critic for policy learning.
  - Quick check question: What roles do the actor and critic networks play, and how are they jointly trained?

## Architecture Onboarding

- Component map:
  - Client-side: User embedding (private, never uploaded) → Local item embedding table (downloaded from server, updated locally) → ProxyNCF (dual branches: NCF for training, proxy for loss prediction)
  - Server-side: RL agent (actor + critic) → Global item embedding table → Global ProxyNCF parameters
  - Communication: Clients upload predicted losses (for state), updated embeddings, and model gradients; server broadcasts global embeddings and model parameters.

- Critical path:
  1. All clients compute proxy-predicted losses locally (no training, single inference pass)
  2. Server constructs state from predicted losses
  3. Actor network samples client subset
  4. Selected clients perform full NCF + proxy training
  5. Server aggregates via FedAvg, computes reward, updates RL agent

- Design tradeoffs:
  - **Proxy accuracy vs. overhead:** Larger proxy branch improves prediction but increases client compute; current design uses shared MLP to minimize overhead.
  - **λ tuning:** High λ prioritizes accuracy; low λ prioritizes freshness. Dataset sparsity affects optimal λ (sparser datasets benefit from lower λ).
  - **Client subset size |U+|:** More clients improve coverage but increase communication and compute per round.

- Failure signatures:
  - Proxy predictions diverge from true losses → selection becomes random-like → convergence slows
  - Staleness term dominates (λ too low) → accuracy degrades despite fresh embeddings
  - RL agent overfits to recent states → poor generalization to new client distributions

- First 3 experiments:
  1. **Validate proxy prediction quality:** Correlate predicted losses (ℓ̂) with true losses (ℓ) across rounds; target correlation > 0.7.
  2. **Ablate reward components:** Run with λ ∈ {0, 0.2, ..., 1} on a sparse dataset; confirm performance peaks at intermediate λ (paper shows ~0.6-0.8 on MovieLens).
  3. **Compare selection coverage:** Measure unique clients selected over 100 rounds vs. random baseline; ProxyRL-FRS should achieve higher coverage while maintaining selection preference.

## Open Questions the Paper Calls Out
None

## Limitations
- Critical architectural details including MLP hidden layer dimensions, RL network sizes, and per-round client subset selection counts are missing from the paper
- Proxy prediction accuracy is assumed rather than empirically validated across different data distributions
- The staleness-aware reward mechanism's sensitivity to λ tuning across diverse datasets is not fully characterized

## Confidence
- **High Confidence:** The dual-branch ProxyNCF architecture and FedAvg-based aggregation are technically sound and well-defined
- **Medium Confidence:** The actor-critic RL framework for client selection is plausible but lacks detailed hyperparameter specifications that could affect performance
- **Low Confidence:** The claimed 16.5-16.7% improvements are based on specific hyperparameter settings that are incompletely specified in the paper

## Next Checks
1. **Proxy Prediction Validation:** Correlate predicted vs. actual training losses across multiple rounds to verify the proxy branch maintains >0.7 correlation under varying data distributions
2. **Hyperparameter Sensitivity Analysis:** Systematically vary λ (0.2 to 0.8) and client subset sizes (50-100) to identify optimal configurations for each dataset
3. **Convergence Speed Comparison:** Measure wall-clock time and rounds-to-convergence for ProxyRL-FRS versus random selection, accounting for communication overhead differences