---
ver: rpa2
title: 'AdaptiveGuard: Towards Adaptive Runtime Safety for LLM-Powered Software'
arxiv_id: '2509.16861'
source_url: https://arxiv.org/abs/2509.16861
tags:
- prompts
- jailbreak
- attacks
- continual
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of runtime safety in LLM-powered
  software, specifically how static guardrails fail against evolving jailbreak attacks.
  It proposes AdaptiveGuard, an adaptive guardrail that treats novel jailbreak prompts
  as out-of-distribution (OOD) inputs and uses continual learning to adapt to them.
---

# AdaptiveGuard: Towards Adaptive Runtime Safety for LLM-Powered Software

## Quick Facts
- arXiv ID: 2509.16861
- Source URL: https://arxiv.org/abs/2509.16861
- Reference count: 40
- Primary result: Adaptive runtime safety for LLMs that adapts to unseen jailbreak attacks in 2-38 update steps while maintaining 85%+ F1-score on in-distribution data

## Executive Summary
AdaptiveGuard addresses the critical challenge of runtime safety in LLM-powered software by introducing an adaptive guardrail system that can detect and adapt to novel jailbreak attacks. Unlike static guardrails that fail against evolving attacks, AdaptiveGuard treats novel jailbreak prompts as out-of-distribution (OOD) inputs and employs continual learning to adapt. Through empirical evaluation, it achieves 96% OOD detection accuracy and can adapt to new attacks with a median of just 2 update steps while retaining over 85% F1-score on in-distribution data, outperforming baselines like LlamaGuard.

## Method Summary
AdaptiveGuard combines OOD detection with continual learning to create an adaptive guardrail for LLM safety. It trains a GPT-2 model with an auxiliary OOD-aware loss that explicitly separates in-distribution safe/unsafe prompts from potential attack patterns. During inference, it uses Mahalanobis distance on penultimate-layer representations to detect OOD inputs, which trigger LoRA-based updates on embeddings, attention, and FFNN layers. This approach enables rapid adaptation to new attack patterns while minimizing catastrophic forgetting through low-rank decomposition that updates only a small subset of parameters.

## Key Results
- Achieves 96% OOD detection accuracy using Mahalanobis distance
- Adapts to new attacks in 2-38 update steps (median 2) with LoRA
- Maintains over 85% F1-score on in-distribution data post-adaptation
- Uses 1.3GB memory versus 27.1GB for LlamaGuard-8B

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mahalanobis distance-based OOD detection identifies unseen jailbreak prompts more effectively than confidence-based methods.
- Mechanism: The model extracts penultimate-layer representations and computes their distance to class-conditional Gaussian distributions fitted on in-distribution training data. Prompts with minimum distance to any class exceeding a threshold are flagged as OOD.
- Core assumption: Jailbreak prompts occupy distinct regions in the learned feature space compared to natural language unsafe prompts.
- Evidence anchors:
  - [abstract]: "achieves 96% OOD detection accuracy"
  - [Section V-A]: "The Mahalanobis Distances used in our framework achieves the highest F1-Score of 96.1% when the OOD detection threshold is set to the 99th quantile threshold."

### Mechanism 2
- Claim: Auxiliary OOD-aware loss during training creates better separation between in-distribution and OOD inputs.
- Mechanism: An energy-based margin loss penalizes in-distribution inputs with low confidence and OOD inputs with high confidence, combined with cross-entropy: L = λL_OOD + (1-λ)L_CE. This explicitly trains the model to maintain discriminative boundaries.
- Core assumption: Exposing the model to OOD examples during training improves its ability to recognize distributional shift at inference time.
- Evidence anchors:
  - [Section III-A]: "We then apply a margin-based regularization loss that penalizes in-distribution inputs with confidence below a lower threshold min and OOD inputs exceeding an upper threshold m_out"
  - [Section IV-B]: "a set of 17,176 prompts is used to guide the optimization of the OOD loss... enabling the model to distinguish between in-distribution and OOD prompts"

### Mechanism 3
- Claim: LoRA-based continual learning enables rapid adaptation with minimal catastrophic forgetting.
- Mechanism: Only three layers are updated (word embedding, masked self-attention, FFNN) via low-rank decomposition, preserving most parameters while allowing targeted adaptation to detected OOD prompts.
- Core assumption: New attack patterns can be learned through small parameter updates without disrupting previously learned safe/unsafe classifications.
- Evidence anchors:
  - [abstract]: "adapts to new attacks in 2-38 update steps (median 2), and retains over 85% F1-score on in-distribution data post-adaptation"
  - [Table II]: "Our LoRA-based approach achieves a median DSR of 100% and a median F1-Score of 85%... full fine-tuning yields a lower median F1-Score of 80%"

## Foundational Learning

- **Out-of-Distribution (OOD) Detection**
  - Why needed here: The entire AdaptiveGuard approach hinges on detecting inputs that fall outside training distribution. Without understanding OOD detection, the trigger mechanism for continual learning is opaque.
  - Quick check question: Can you explain why Mahalanobis distance captures distributional shift better than simple thresholding on softmax confidence?

- **Catastrophic Forgetting in Continual Learning**
  - Why needed here: The paper's core value proposition is adapting to new attacks without losing ID performance. Understanding why neural networks forget—and how LoRA mitigates this—is essential.
  - Quick check question: What happens to task A performance when a model is sequentially trained on task B without any regularization or parameter isolation?

- **LoRA (Low-Rank Adaptation)**
  - Why needed here: The efficiency claims (2-38 update steps, 1.3GB memory vs 27.1GB for LlamaGuard-8B) depend on understanding how LoRA decomposes weight updates into low-rank matrices.
  - Quick check question: If LoRA has rank r=32 and targets a weight matrix W ∈ R^(d×k), how many trainable parameters does it introduce?

## Architecture Onboarding

- **Component map**: Input → Tokenizer (BPE) → GPT-2 backbone (12 layers, 137M params) → Penultimate representation extraction → Two parallel outputs: Classification head → Safe/Unsafe prediction; Mahalanobis distance computation → OOD flag. If OOD detected → LoRA adapter updates on embeddings, attention, FFNN

- **Critical path**:
  1. Initial training: Combined L_CE + L_OOD on Aegis (ID) + Jailbreakv-28k (OOD auxiliary)
  2. Deployment: For each input, compute classification AND Mahalanobis distance
  3. If d_min(x) > τ_OOD: flag as OOD, block input, queue for continual learning update
  4. Periodic CL: LoRA update on accumulated OOD samples

- **Design tradeoffs**:
  - Model size vs. adaptability: GPT-2 (137M) is 60x smaller than LlamaGuard-8B, enabling fast updates but potentially weaker base detection
  - Threshold τ_OOD: Higher (99th percentile) → higher precision, lower recall; lower (90th) → more OOD flagged, more updates but more false positives
  - LoRA rank: r=32 chosen; higher rank = more capacity but more forgetting risk

- **Failure signatures**:
  - OOD detection recall drops: New attack type closely matches ID feature distribution (Mahalanobis distance fails to separate)
  - Catastrophic forgetting: ID F1 drops significantly after CL updates → LoRA rank may be too high or learning rate too aggressive
  - Adaptation plateaus: DSR never reaches 100% despite many updates → attack requires representation-level changes beyond LoRA capacity

- **First 3 experiments**:
  1. Reproduce RQ1: Train AdaptiveGuard on provided datasets, evaluate Mahalanobis vs. Energy vs. Likelihood Ratio on OOD detection. Confirm ~96% F1 achievable.
  2. Threshold sensitivity analysis: Sweep τ_OOD from 85th to 99.5th percentile. Plot precision-recall curve to validate 99th percentile choice for your deployment context.
  3. Single-model sequential CL: Run the "no reinitialization" experiment from Table I on your hardware. Measure actual training time per sample and validate the 0.60s claim.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the traditional sense. However, it identifies several areas for future work including evaluating against adversarial examples specifically crafted to minimize OOD detection scores, testing the approach on larger model architectures, and exploring active learning mechanisms for obtaining ground truth labels on novel OOD inputs in real deployment scenarios.

## Limitations
- Relies on Mahalanobis distance which may fail if novel attacks closely match ID feature distributions
- Uses a 60x smaller base model (137M vs 8B parameters) which may limit detection capabilities for subtle attacks
- Critical hyperparameters like OOD loss temperature and margin thresholds are not fully specified
- Long-term stability of continual learning under sustained attack evolution remains untested

## Confidence
- **High Confidence**: OOD detection accuracy (96% F1) on evaluated attack types; LoRA's memory efficiency advantage; basic adaptation mechanism functionality
- **Medium Confidence**: Catastrophic forgetting mitigation (85% ID F1 retention); adaptation speed (2-38 update steps); general approach validity
- **Low Confidence**: Long-term adaptation stability; performance against completely novel attack types; optimal hyperparameter settings across diverse scenarios

## Next Checks
1. **Cross-Attack Generalization Test**: Evaluate AdaptiveGuard on attack types not included in the original 10 (e.g., Chain-of-Thought manipulation, role-playing jailbreaks). Measure OOD detection F1 and adaptation speed to quantify performance degradation on unseen attack patterns.

2. **Base Model Scaling Study**: Compare AdaptiveGuard's performance when using different base model sizes (e.g., GPT-2 Medium 355M, Large 774M) while keeping LoRA configuration constant. This will reveal whether the 137M parameter choice is optimal or constrained by the LoRA approach.

3. **Extended Continual Learning Stability**: Run AdaptiveGuard through 100+ sequential adaptation cycles on diverse attack patterns. Track ID F1-score, OOD detection performance, and DSR over time to identify potential catastrophic forgetting accumulation or adaptation saturation points.