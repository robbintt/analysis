---
ver: rpa2
title: Can David Beat Goliath? On Multi-Hop Reasoning with Resource-Constrained Agents
arxiv_id: '2601.21699'
source_url: https://arxiv.org/abs/2601.21699
tags:
- search
- reasoning
- mosque
- retrieval
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DAVID-GRPO enables small language models (up to 1.5B parameters)
  to perform robust multi-hop reasoning under resource constraints by combining a
  few-shot warm-start with grounded retrieval rewards and adaptive expansion. Evaluated
  on six benchmarks with limited compute (4 RTX 3090 GPUs), it outperforms prior methods
  designed for large-scale settings, achieving parity with high-budget models while
  using only 4.7% of their rollout budget.
---

# Can David Beat Goliath? On Multi-Hop Reasoning with Resource-Constrained Agents

## Quick Facts
- arXiv ID: 2601.21699
- Source URL: https://arxiv.org/abs/2601.21699
- Authors: Hojae Han; Heeyun Jung; Jongyoon Kim; Seung-won Hwang
- Reference count: 40
- Primary result: Small language models (≤1.5B params) achieve robust multi-hop reasoning using few-shot warm-start + grounded retrieval rewards + adaptive expansion, outperforming prior methods designed for large-scale settings.

## Executive Summary
DAVID-GRPO is a resource-efficient RL framework enabling small language models to perform robust multi-hop reasoning under tight compute budgets. By combining a few-shot expert warm-start with grounded retrieval rewards and adaptive expansion, it stabilizes early learning, enforces evidence recall, and improves exploration efficiency. Evaluated across six benchmarks with only 4 RTX 3090 GPUs, DAVID-GRPO achieves parity with high-budget models while using only 4.7% of their rollout budget.

## Method Summary
DAVID-GRPO extends GRPO by mixing k=4 expert trajectories into on-policy rollouts for warm-start, using a grounded retrieval reward that measures cumulative evidence recall against ground-truth sets, and applying adaptive expansion to resample from near-miss trajectories. Training proceeds in two phases: (1) mixed GRPO warm-start on few-shot examples (50 steps, batch=4), then (2) main GRPO with grounded rewards and expansion (215 steps, batch=24). The framework targets small LMs (Qwen2.5-0.5B/1.5B, Llama-3.2-1B) and uses FlashRAG with E5-base-v2 retrieval.

## Key Results
- Achieves exact match and F1 scores competitive with high-budget models on 6 multi-hop QA benchmarks
- Uses only 4.7% of the rollout budget required by baseline methods
- Improves bridge- and answer-document hit rates compared to reward ablation baselines
- Expansion ratio peaks at ~25% early in training, declining as agent improves

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lightweight few-shot warm-start mixed with on-policy RL stabilizes early learning for small agents under tight rollout budgets.
- Mechanism: Mixed off-/on-policy GRPO using k=4 expert trajectories provides immediate non-zero rewards while preserving on-policy reinforcement.
- Core assumption: Small set of high-quality expert trajectories available.
- Evidence anchors: Abstract mentions "stabilizes early learning with minimal supervision"; Section 3.2/Figure 5 shows mixed GRPO outperforms pure SFT/off-policy RL; related work supports small-model viability with targeted biases.
- Break condition: Expert trajectories unavailable, noisy, or inconsistent.

### Mechanism 2
- Claim: Grounded retrieval rewards enforce faithful multi-hop reasoning by rewarding cumulative evidence recall.
- Mechanism: Reward combines grounded retrieval (recall of ground-truth evidence in cumulative retrieved set) with outcome reward.
- Core assumption: Ground-truth evidence sets D* available during training.
- Evidence anchors: Abstract states "enforces evidence recall... allowing small agents to ground reasoning in retrieved documents"; Section 3.3 defines r_g; Table 3 shows higher hit rates; Table 5 shows grounded rewards outperform alternatives.
- Break condition: D* annotations unavailable/incomplete or retrieval quality too poor.

### Mechanism 3
- Claim: Grounded expansion improves exploration efficiency by resampling from partially successful trajectories.
- Mechanism: When all trajectories suboptimal, truncate best at last grounded step and resample l=5 completions, replacing worst if improved.
- Core assumption: Near-miss trajectories with partial grounding exist.
- Evidence anchors: Abstract mentions "improves exploration by resampling truncated near-miss trajectories"; Section 3.4/Figure 2c describes procedure; Appendix A/Figure 6 shows expansion ratio ~25% early, declining later.
- Break condition: Near-miss trajectories rare or policy cannot generate diverse completions from prefixes.

## Foundational Learning

**Concept: Markov Decision Process (MDP) formulation for reasoning**
- Why needed: Formalizes multi-hop reasoning as MDP with states, actions, rewards; essential for RL framing.
- Quick check: In the paper's MDP, how is the state at step t+1 computed from the state and action at step t?

**Concept: Group Relative Policy Optimization (GRPO)**
- Why needed: DAVID-GRPO extends GRPO by mixing off-policy expert trajectories into on-policy groups; grasping GRPO's group-based advantage computation is prerequisite.
- Quick check: For a group of G=5 trajectories with rewards [0.2, 0.4, 0.6, 0.8, 1.0], what are the standardized advantages?

**Concept: Multi-hop QA with bridge vs. answer retrieval**
- Why needed: Task requires iteratively retrieving bridge documents (intermediate context) and answer documents (final evidence); grounded reward explicitly targets both.
- Quick check: Why might an agent that retrieves only answer documents but not bridge documents still fail at multi-hop reasoning?

## Architecture Onboarding

**Component map:**
Policy π_θ (small LM) -> Retrieval module (E5-base-v2 + FAISS) -> Reward module (r_g + r_o) -> GRPO trainer (mixed objective + KL penalty) -> Grounded expansion (truncate + resample)

**Critical path:**
1. Warm-start: Run mixed GRPO on k=4 expert + on-policy rollouts (50 steps, batch size 4)
2. Main training: GRPO with grounded rewards + expansion (215 steps, batch size 24, ~6 rollouts/example)
3. Evaluation: FlashRAG pipeline on 6 multi-hop QA benchmarks; report EM/F1

**Design tradeoffs:**
- Few-shot vs. full SFT warm-start: Few-shot uses <0.1% annotations but provides weaker priors; full SFT yields higher peaks but requires extensive labeling
- Grounded vs. sparse rewards: Grounded rewards require D* but enforce true multi-hop behavior; sparse rewards are simpler but risk hallucination
- Expansion overhead: Adds ~1.1 rollouts/example on average; improves sample efficiency but increases per-step compute

**Failure signatures:**
- Policy collapse: Agent fails to issue any search actions (Tree-GRPO baseline)
- Single-hop shortcut: Agent averages ~1 retrieval action even for multi-hop queries (Search-R1-v0.3)
- Hallucination: Agent answers from parametric knowledge without retrieving (Tree-GRPO on HotpotQA bridge questions)

**First 3 experiments:**
1. Warm-start ablation: Compare mixed GRPO vs. pure SFT vs. pure off-policy RL on few-shot set; track non-zero reward rate and early-step EM
2. Reward ablation: Compare grounded recall vs. answer-only vs. lexical rewards on MuSiQue; analyze bridge vs. answer hit rates and per-hop EM
3. Expansion ablation: Train with/without grounded expansion; plot expansion ratio over training; compare final EM on BamTwoogle (2-4 hops)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the efficiency advantage of DAVID-GRPO persist when scaling model capacity beyond the 1.5B parameter limit evaluated in the paper?
- Basis: Authors explicitly constrain experiments to "agents up to 1.5B parameters" and compare against high-budget baselines; unresolved if sample efficiency of proposed inductive biases scales linearly.
- Why unresolved: As model size increases, parametric knowledge improves, potentially reducing reliance on retrieval-based stabilization methods crucial for small models.
- What evidence would resolve it: Evaluation on 7B or 70B parameter models using same low rollout budget (4.7% of baseline) to observe if performance parity/gains are maintained.

### Open Question 2
- Question: Can the grounded retrieval reward function be adapted for open-domain tasks where intermediate ground-truth evidence documents (D*) are unavailable?
- Basis: Method formalization explicitly assumes access to "a set of ground truth evidence documents D*" and Section 3.3 defines reward based on recall against this set.
- Why unresolved: Framework relies on intermediate supervision (evidence sets) which is standard in specific benchmarks but may not exist in real-world, open-ended applications.
- What evidence would resolve it: Experiments substituting exact set overlap rewards with proxy rewards (e.g., consistency checking or weak supervision from LLMs) on datasets lacking annotated evidence chains.

### Open Question 3
- Question: How does the "Grounded Expansion" strategy impact performance on tasks requiring divergent reasoning paths or creative tool use, rather than factual recall?
- Basis: "Grounded Expansion" resamples trajectories specifically to rescue "near-miss" instances based on retrieval success, biasing search toward evidence accumulation paths.
- Why unresolved: While effective for multi-hop QA, directed resampling might constrain exploration in tasks benefiting from wider, more diverse policy exploration (e.g., creative writing or complex coding).
- What evidence would resolve it: Comparative analysis on multi-step coding or creative writing benchmark, measuring diversity of generated solutions against baseline GRPO.

## Limitations

- Results hinge on availability of ground-truth evidence annotations (D*), which may not exist in many real-world settings
- Few-shot warm-start effectiveness (k=4 trajectories) may not generalize to tasks where high-quality expert trajectories are difficult to obtain
- Grounded expansion assumes near-miss trajectories with partial grounding are common enough to be useful; may break down in very sparse reward settings
- Comparison to high-budget models uses different training regimes (full SFT vs. RL), making direct efficiency comparisons somewhat apples-to-oranges

## Confidence

- **High confidence**: Core empirical findings that DAVID-GRPO improves multi-hop reasoning performance on tested benchmarks compared to baselines; ablation studies showing individual contributions of warm-start, grounded rewards, and expansion
- **Medium confidence**: Efficiency claims relative to high-budget models given different training regimes; generalizability of few-shot warm-start approach to other tasks or domains
- **Low confidence**: Robustness of grounded expansion mechanism in very sparse reward settings, as this was not extensively tested

## Next Checks

1. **Robustness to missing expert trajectories**: Remove few-shot warm-start and evaluate DAVID-GRPO's performance on MuSiQue and BamTwoogle to quantify warm-start contribution and test cold-start recovery

2. **Ablation of grounded reward components**: Train with answer-document-only reward (λ=1, no bridge recall) and with lexical-based rewards to precisely measure impact of cumulative evidence recall signal on multi-hop reasoning behavior

3. **Expansion mechanism stress test**: Intentionally inject noise into retrieval results during training to create more near-miss trajectories, then measure whether grounded expansion maintains its benefit or if agent becomes overly reliant on partial grounding