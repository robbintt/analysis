---
ver: rpa2
title: 'Problems With Large Language Models for Learner Modelling: Why LLMs Alone
  Fall Short for Responsible Tutoring in K--12 Education'
arxiv_id: '2512.23036'
source_url: https://arxiv.org/abs/2512.23036
tags:
- llms
- knowledge
- education
- learner
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study examines whether large language models (LLMs) can replace\
  \ traditional learner modelling for adaptive tutoring in K\u201312 education, a\
  \ domain classified as high-risk under the EU AI Act. We compared a deep knowledge\
  \ tracing (DKT) model with a widely used LLM, both with and without fine-tuning,\
  \ using a large-scale student interaction dataset."
---

# Problems With Large Language Models for Learner Modelling: Why LLMs Alone Fall Short for Responsible Tutoring in K--12 Education

## Quick Facts
- arXiv ID: 2512.23036
- Source URL: https://arxiv.org/abs/2512.23036
- Reference count: 40
- Primary result: Deep Knowledge Tracing (DKT) outperformed both zero-shot and fine-tuned LLMs on AUC (0.83 vs. 0.77) and temporal coherence metrics in K-12 learner modelling

## Executive Summary
This study compares Deep Knowledge Tracing (DKT) with large language models (LLMs) for adaptive tutoring in K-12 education, a high-risk domain under the EU AI Act. Using a large-scale student interaction dataset, DKT achieved superior discrimination performance (AUC = 0.83) and maintained temporally coherent mastery trajectories, while LLM variants—even after fine-tuning—produced higher early-sequence errors and inconsistent mastery updates. Despite fine-tuning improving LLM AUC by 8%, it remained 6% below DKT and required 198 hours of high-compute training versus DKT's 51 seconds. The findings indicate that LLMs alone are unlikely to match established intelligent tutoring systems, and responsible tutoring requires hybrid frameworks incorporating learner modelling.

## Method Summary
The study compared DKT with zero-shot and fine-tuned Llama 3 8B on the ASSISTments 2009-2010 dataset (450K single-skill interactions). DKT used 2K encoding, 64-dim embedding, and a 128-unit GRU with BCE loss. LLMs were evaluated via prompt-based next-step prediction using the same interaction history format. Fine-tuning employed LoRA (rank=32) with Adam optimizer. Metrics included AUC, accuracy, precision, recall, F1, and temporal coherence measures (volatility, inconsistency, early/mid/late sequence errors). Data was split 80/10/10 by student to prevent leakage.

## Key Results
- DKT achieved highest AUC (0.83) and lowest early-sequence errors (0.2975 stable, 0.3118 switching students)
- Fine-tuned LLM improved AUC by 8% over zero-shot but remained 6% below DKT with higher volatility (0.2945) and inconsistency (0.4525)
- LLM variants exhibited wrong-direction mastery updates and temporal instability despite 198 hours fine-tuning versus DKT's 51 seconds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential neural architectures (DKT) maintain temporally coherent mastery trajectories that LLMs fail to replicate, even after fine-tuning.
- Mechanism: DKT uses a GRU-based recurrent network with explicit hidden states that encode accumulated learning history. The 2K encoding (skill + correctness at each timestep) creates a structured memory of prior performance, enabling smooth, directionally consistent probability updates across skills over time.
- Core assumption: Student knowledge evolves gradually through practice, and reliable learner modelling requires capturing these incremental state transitions rather than making isolated predictions.
- Evidence anchors:
  - [abstract] "DKT maintained stable, directionally correct mastery updates, whereas LLM variants exhibited substantial temporal weaknesses, including inconsistent and wrong-direction updates."
  - [Section 3.2.1] "Each interaction was encoded using the standard 2K DKT encoding, where each skill is represented twice, once for incorrect responses and once for correct responses."
  - [corpus] IntelliCode paper addresses this limitation by adding "centralized, versioned learner state" to LLM tutors—implicitly confirming that base LLMs lack persistent state.

### Mechanism 2
- Claim: Fine-tuning improves LLM learner modelling but does not resolve structural limitations in representing sequential learning dynamics.
- Mechanism: LoRA-based fine-tuning adapts LLM weights to predict correctness from interaction sequences, teaching the model some directionality (mastery should rise after correct responses). However, the model still lacks an explicit mechanism for maintaining coherent multi-skill mastery trajectories—it reacts locally rather than accumulating stable knowledge representations.
- Core assumption: Parameter-efficient fine-tuning captures surface patterns in training data but cannot fundamentally restructure how the model processes sequential dependencies over long horizons.
- Evidence anchors:
  - [abstract] "Although fine-tuning improves the LLM's AUC by approximately 8% over the zero-shot baseline, it remains 6% below DKT and produces higher early-sequence errors."
  - [Section 4.1, Table 5] Fine-tuned LLM shows higher volatility (0.2945) than both zero-shot LLM (0.1157) and DKT (0.1075), with inconsistency rate (0.4525) still above DKT (0.4061).
  - [corpus] LLM-driven KT paper notes LLMs "typically require fine-tuning and exhibit unstable or near-random performance"—consistent with unresolved structural issues.

### Mechanism 3
- Claim: Early-sequence prediction errors are disproportionately harmful for adaptive tutoring, and LLMs exhibit significantly higher error rates in this critical phase.
- Mechanism: Early in a learning session, the system must quickly establish accurate mastery estimates to provide appropriate scaffolding. DKT's lower early-sequence errors (0.2975 for stable, 0.3118 for switching students) vs. fine-tuned LLM (0.3563, 0.3309) stem from its ability to generalize from limited prior interactions using learned skill-to-skill transfer patterns.
- Core assumption: Incorrect early predictions lead to mistimed interventions that can frustrate learners or reinforce misconceptions before they gain momentum.
- Evidence anchors:
  - [abstract] "LLM variants produced higher early-sequence errors, where incorrect predictions are most harmful for adaptive support."
  - [Section 4.1, Table 4] DKT achieved lowest early-stage errors (0.3118 for switching students) vs. fine-tuned LLM (0.3309), with the gap persisting across mid and late stages.
  - [corpus] Corpus lacks direct replication of early-sequence error analysis; related work focuses on overall accuracy rather than temporal error distribution.

## Foundational Learning

- Concept: **Knowledge Tracing (KT)**
  - Why needed here: The paper's central comparison assumes understanding that KT models estimate a learner's evolving mastery state from sequential interaction data to predict future performance.
  - Quick check question: Can you explain why predicting "next-step correctness" differs from generating an open-ended adaptive response?

- Concept: **Temporal Coherence in Learning**
  - Why needed here: The paper evaluates models not just on accuracy but on whether their mastery updates follow pedagogically plausible trajectories (smooth, directionally correct).
  - Quick check question: Why would increasing mastery after an incorrect response be problematic for an adaptive tutoring system?

- Concept: **Responsible AI in High-Risk Domains**
  - Why needed here: K-12 education is classified as high-risk under the EU AI Act, requiring systems to demonstrate reliability, accuracy, and pedagogical soundness at the design level—not retrofitted via "responsible use."
  - Quick check question: What distinguishes "responsible development" from "responsible use" in educational AI?

## Architecture Onboarding

- Component map:
  - **DKT pipeline**: Raw interactions → 2K encoding → 64-dim embedding → 128-unit GRU → sigmoid output (mastery probabilities per skill) → masked BCE loss
  - **LLM pipeline**: Interaction history formatted as prompt → tokenization → Llama 3 8B forward pass → logit extraction for tokens "0"/"1" → probability conversion
  - **Evaluation framework**: AUC (primary), accuracy/precision/recall/F1, temporal coherence metrics (volatility, inconsistency), computational efficiency

- Critical path:
  1. Data preprocessing: Filter to single-skill interactions, sort chronologically, construct (skill, quiz, correctness) triplets
  2. Train DKT (~51 seconds on T4 GPU) vs. fine-tune LLM (~198 hours on dual H100s)
  3. Evaluate on held-out test set with identical splits; compare both global metrics and temporal trajectory quality

- Design tradeoffs:
  - **DKT**: Lightweight, interpretable hidden states, pedagogically aligned—but requires structured interaction data and may underutilize semantic content
  - **LLM (zero-shot)**: No training required, handles open-ended inputs—but near-random early predictions and wrong-direction updates
  - **LLM (fine-tuned)**: Improved accuracy, captures some learning dynamics—yet computationally expensive and still temporally unstable
  - **Hybrid approaches** (discussed but not tested): Use DKT for knowledge state estimation, LLM for generating instructional content conditioned on that state

- Failure signatures:
  - High volatility + high inconsistency → model not tracking learning meaningfully (fine-tuned LLM pattern)
  - Low volatility + high inconsistency → stable but directionally wrong updates (zero-shot LLM pattern)
  - Early-sequence error spikes → model struggling with cold-start generalization

- First 3 experiments:
  1. **Replicate AUC comparison** on the ASSISTments 2009-2010 dataset with identical preprocessing to validate baseline findings (target: DKT ~0.83, fine-tuned LLM ~0.77).
  2. **Temporal coherence stress test**: Generate mastery trajectories for synthetic student profiles with known ground-truth learning curves; measure volatility and inconsistency across models.
  3. **Hybrid prototype**: Build a minimal system where DKT provides mastery estimates that condition LLM prompt context for feedback generation—evaluate whether this improves both accuracy and pedagogical quality.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can hybrid neural-symbolic approaches for deep knowledge tracing improve transparency, reduce bias, and enhance human-centredness compared to purely data-driven models?
  - Basis: Authors state in Limitations: "Future research should extend our findings by exploring learner modelling methods that support these broader dimensions, such as hybrid neural-symbolic approaches for DKT."
  - Why unresolved: This study focused only on accuracy and reliability; broader responsible AI dimensions were beyond scope.
  - Evidence needed: Empirical comparison of neural-symbolic DKT vs. standard DKT on interpretability metrics, bias measures, and stakeholder trust assessments.

- **Open Question 2**: How does fine-tuning depth (parameter-efficient LoRA vs. full-model fine-tuning) affect LLM temporal coherence and pedagogical correctness in learner modelling?
  - Basis: Authors note: "We employed a parameter-efficient LoRA configuration... Future work should systematically examine how fine-tuning depth and adaptation strategies influence LLM performance in learner modelling."
  - Why unresolved: LoRA may not fully exploit model capacity; full fine-tuning could yield different temporal stability outcomes.
  - Evidence needed: Controlled experiments comparing LoRA, full fine-tuning, and other adaptation strategies on volatility, inconsistency, and early-sequence error metrics.

- **Open Question 3**: Do the observed limitations of LLMs for learner modelling generalize across diverse LLM architectures, scales, and training paradigms?
  - Basis: Authors acknowledge: "Our analysis evaluated only the Llama3-8B... future work should compare a wider range of LLM architectures and scales to assess the generalisability of our conclusions."
  - Why unresolved: Only one model family was tested; larger models or different architectures may exhibit different temporal behaviours.
  - Evidence needed: Replication across multiple LLM families (GPT, Claude, Mistral) and scales (7B–70B+ parameters) using identical evaluation protocols.

- **Open Question 4**: Which hybrid integration architecture (embedding-based, RAG-style, or neural-symbolic) most effectively combines LLMs with knowledge tracing for responsible tutoring?
  - Basis: Discussion proposes multiple hybrid pathways but does not evaluate them: "One pathway involves embedding representations from traditional KT models... Another complementary direction is the use of retrieval-augmented generation (RAG)-style designs."
  - Why unresolved: The paper advocates hybrid approaches but provides no empirical comparison of integration strategies.
  - Evidence needed: Comparative study of hybrid architectures on combined metrics of predictive accuracy, temporal coherence, computational efficiency, and pedagogical soundness.

## Limitations

- **Dataset Specificity**: All experiments used a single historical dataset (ASSISTments 2009-2010), limiting generalizability to modern curricula and diverse demographics.
- **Fine-tuning Scope**: Only LoRA fine-tuning was tested; full-model fine-tuning or architectural modifications were not explored as potential solutions.
- **Hybrid Approaches Not Tested**: While discussed, hybrid frameworks combining DKT and LLMs were not implemented or evaluated.

## Confidence

- **High Confidence**: Comparative performance results (AUC, early-sequence errors) are robust and directly supported by experimental data.
- **Medium Confidence**: Mechanisms explaining LLM temporal coherence issues are logically consistent but rely on architectural reasoning rather than direct causal analysis.
- **Low Confidence**: Claims about generalizability to all LLM architectures and educational contexts require further validation.

## Next Checks

1. **Dataset Generalization Test**: Replicate the complete experimental pipeline on at least two additional KT datasets (e.g., EdNet, ASSISTments 2015) to verify whether the temporal coherence gap persists across different student populations, curricula, and time periods.

2. **Fine-tuning Intensity Experiment**: Compare LoRA fine-tuning with full-model fine-tuning and with domain-specific pretraining (e.g., continued pretraining on educational corpora) to determine whether computational expense or architectural limitations primarily drive the LLM performance gap.

3. **Hybrid System Prototype**: Implement a minimal hybrid system where DKT provides mastery estimates that condition LLM prompt context for feedback generation, then evaluate whether this improves both accuracy metrics and pedagogical quality compared to either approach alone.