---
ver: rpa2
title: Can Large Language Models Learn Formal Logic? A Data-Driven Training and Evaluation
  Framework
arxiv_id: '2504.20213'
source_url: https://arxiv.org/abs/2504.20213
tags:
- implies
- proof
- reasoning
- proofs
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the logical reasoning capabilities of large
  language models (LLMs) through the task of constructing formal proofs in Boolean
  logic. The authors address the scarcity of real-world proof data by proposing an
  efficient randomized procedure for synthesizing valid proofs and introducing a Template
  Transformation technique for data augmentation.
---

# Can Large Language Models Learn Formal Logic? A Data-Driven Training and Evaluation Framework

## Quick Facts
- arXiv ID: 2504.20213
- Source URL: https://arxiv.org/abs/2504.20213
- Authors: Yuan Xia; Akanksha Atrey; Fadoua Khmaissia; Kedar S. Namjoshi
- Reference count: 35
- Key outcome: Fine-tuned Llama models achieve strong formal proof generation, with 8B model reaching 98% accuracy on depth-7 proofs, outperforming GPT-4o's few-shot performance despite fewer parameters.

## Executive Summary
This paper investigates whether large language models can learn formal logic by focusing on constructing Hilbert-style proofs in Boolean logic. The authors address the scarcity of real-world proof data by developing an efficient randomized procedure for synthesizing valid proofs and introducing a Template Transformation technique for data augmentation. Through systematic experiments, they demonstrate that fine-tuned Llama models significantly outperform few-shot GPT-4o on formal proof generation tasks, with larger models showing better generalization to deeper proofs while template transformation helps smaller models handle complex expressions.

## Method Summary
The authors propose an efficient, randomized procedure for synthesizing valid Hilbert-style proofs in Boolean logic, starting from randomly chosen goal formulas and constructing proofs backward through local resolution operations. They introduce Template Transformation as a data augmentation technique that substitutes variables with Boolean expressions while preserving proof validity. The approach uses fine-tuning of Llama models (8B and 1B parameters) with 4-bit quantization and LoRA adapters, trained on synthetic proofs balanced across depths 7, 10, and 13. Proofs are validated using automated symbolic checkers that verify Modus Ponens applications, axiom instantiations, and assumption references.

## Key Results
- Fine-tuned Llama-3.2-8B achieves 98% accuracy on depth-7 proofs, outperforming GPT-4o's 70% few-shot accuracy
- Larger models (8B) generalize better to deeper proofs, while template transformation particularly benefits smaller models (1B) on complex expressions
- Optimal template transformation probability is 0.7, with performance declining sharply beyond this threshold
- Models trained on synthetic data transfer successfully to held-out proof depths (4-28) and widths (0-3)

## Why This Works (Mechanism)

### Mechanism 1: Goal-Directed Proof Generation
- **Claim:** Backward proof synthesis from randomly chosen goals produces valid, diverse training data more efficiently than forward construction.
- **Mechanism:** The algorithm starts from a randomly selected goal formula and constructs a proof tree by expanding leaf nodes through one of three operations: stopping expansion (creates assumption), applying Modus Ponens decomposition (creates two subgoals), or closing with an axiom. This local resolution at each node runs in time independent of proof tree size.
- **Core assumption:** The goal-directed approach generates sufficiently diverse proof structures to prevent overfitting to specific patterns.
- **Evidence anchors:** [abstract] "We propose an efficient, randomized procedure for synthesizing valid proofs"; [Section 3.1] "Goal-directed proof generation is efficient for a Hilbert-style system as the generation process determines an appropriate resolution for every open formula locally"; [corpus] Related work (Morishita et al.) uses randomization but does not note the efficiency advantage of goal-directed synthesis.
- **Break condition:** If generated proofs cluster around narrow structural patterns despite randomization, diversity will be insufficient for generalization.

### Mechanism 2: Template Transformation as Structure-Preserving Augmentation
- **Claim:** Substituting variables with formulas preserves proof validity while forcing models to learn abstract proof patterns rather than surface tokens.
- **Mechanism:** A substitution σ maps each variable in X to a Boolean expression over Y. Applying σ uniformly across all formulas in a proof produces a new valid proof with identical justification structure but different syntax. Training with α_TT = 0.7 probability yielded optimal performance.
- **Core assumption:** Models that succeed on template-transformed instances have learned structural reasoning rather than token-level memorization.
- **Evidence anchors:** [abstract] "Template Transformation, a data augmentation technique that enhances the model's ability to handle complex logical expressions"; [Section 3.2] "supplying similar instances obtained through template transformation forces the LLM to focus on the abstract proof structure rather than on concrete detail such as variable names"; [Section 5.3.4] "performance improves as α_TT increases up to 0.7, beyond which accuracy declines sharply"; [corpus] Weak corpus evidence; related papers do not implement equivalent augmentation.
- **Break condition:** If template transformations produce syntactically similar outputs (low variance in generated expressions), augmentation benefit diminishes.

### Mechanism 3: Formal Verification as Ground-Truth Signal
- **Claim:** Automated proof checking provides binary, unambiguous feedback that enables reliable accuracy measurement and prevents reward hacking.
- **Mechanism:** A symbolic validator checks each proof step for: (i) valid Modus Ponens application, (ii) correct assumption reference, (iii) proper axiom invocation. Only syntactically well-formed, fully valid proofs count as correct.
- **Core assumption:** The validator's binary signal is sufficient for fine-tuning without intermediate reward shaping.
- **Evidence anchors:** [abstract] "Incorrect proofs are caught by an automated proof checker"; [Section 2.2] "A proof is valid if every justification in the proof is correct and the goal formula is claimed at some proof step"; [corpus] APOLLO and related neuro-symbolic systems similarly leverage formal verification for reliable evaluation.
- **Break condition:** If models learn to generate plausible-looking but invalid proofs that pass surface-level heuristics, evaluation becomes unreliable.

## Foundational Learning

- **Concept: Hilbert-style proof system**
  - **Why needed here:** The paper uses a minimal axiom system (A1, A2, Modus Ponens) for Boolean implication-only logic. Understanding how proofs compose from axioms via MP is essential for interpreting model outputs.
  - **Quick check question:** Given assumptions (p→q) and (q→r), can you trace why step 7 (p→r) follows from the transitivity proof in Section 2.2.1?

- **Concept: Co-NP completeness and proof complexity**
  - **Why needed here:** The paper attributes performance degradation at deeper proofs to inherent computational complexity. Understanding this baseline prevents misattributing failure to model architecture.
  - **Quick check question:** If validity checking is co-NP-complete even for this restricted implication-only sublogic, what performance ceiling should you expect for depth-28 proofs?

- **Concept: Fine-tuning with LoRA and quantization**
  - **Why needed here:** The paper uses 4-bit quantization and Low-Rank Adaptation for efficient training. Reproducing results requires understanding how these affect capacity.
  - **Quick check question:** How does LoRA's rank parameter trade off against model expressiveness for learning proof patterns versus memorizing specific derivations?

## Architecture Onboarding

- **Component map:** Proof Generator -> Template Transformer -> Fine-tuning Pipeline -> Validator
- **Critical path:** 1. Generate balanced training set (depths 7, 10, 13; 3000 proofs each); 2. Apply template transformation during batch preprocessing (α_TT = 0.7, max 4 variables per substitution, max recursion depth 4); 3. Fine-tune with early stopping (patience = 5 epochs) using exact match accuracy; 4. Evaluate on held-out depths (4-28) and widths (0-3)
- **Design tradeoffs:** Larger models (8B) generalize better to depth; smaller models (1B) benefit more from template transformation on width; higher α_TT improves width handling but can degrade depth performance above 0.7; training data scaling helps but shows diminishing returns beyond 9000 examples
- **Failure signatures:** Pre-trained models produce ≈0% accuracy (fail to infer Hilbert syntax from few-shot examples); overfitting to training depths: sharp accuracy cliff at depth 10 for 1B models; hallucination: generated proofs with incorrect MP justifications or unmatched brackets
- **First 3 experiments:** 1. Replicate baseline: fine-tune Llama-8B on 9000 proofs with α_TT = 0.7, verify ~98% accuracy on depth-7 test set; 2. Ablate template transformation: train with α_TT = 0 and compare width generalization (expect 20-25% drop on complex expressions per Section 5.2); 3. Depth sweep: evaluate trained model on depths 4-28, confirm performance degradation pattern matches Figure 2; if not, check for data leakage between train/test partitions

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the data generation and template transformation techniques developed for Boolean logic be effectively generalized to more complex formal systems, such as first-order, temporal, or modal logic?
  - **Basis:** [explicit] Section 7 states that "everyday human reasoning ranges over" these diverse frameworks and asks how to bridge the gap from the "C. elegans" of Boolean logic.
  - **Why unresolved:** The current study restricts itself to a sub-logic of propositional logic to manage complexity, leaving the scalability of the method unknown.
  - **What evidence would resolve it:** Successful fine-tuning and evaluation of LLMs on synthetic datasets generated via similar algorithms for first-order logic theorems.

- **Open Question 2:** How can formal reasoning capabilities be integrated with commonsense knowledge to solve real-world problems that rely on physical or social facts?
  - **Basis:** [explicit] Section 7 identifies the reliance on "commonsense facts" (e.g., physics) as a "key hurdle," noting that methods to integrate this with formal logic are "currently limited."
  - **Why unresolved:** The paper focuses strictly on formal logic derivation without addressing the semantic grounding or external knowledge bases required for general reasoning.
  - **What evidence would resolve it:** Demonstration of a model that can combine formal proof steps with retrieved commonsense assertions to solve hybrid reasoning tasks.

- **Open Question 3:** What is the theoretical or empirical explanation for the sharp decline in reasoning accuracy when Template Transformation probabilities exceed a specific threshold (e.g., α_TT > 0.7)?
  - **Basis:** [inferred] Section 5.3.4 notes a "consistent pattern" where accuracy improves up to 0.7 but then declines "sharply" for complex expressions, without providing a definitive cause.
  - **Why unresolved:** The paper observes the phenomenon as part of the ablation study but does not isolate whether the drop is due to data corruption, distribution shift, or loss of semantic coherence.
  - **What evidence would resolve it:** A mechanistic analysis of the model's attention patterns or a theoretical bound on distribution divergence at high transformation rates.

## Limitations

- The paper's claims rest on synthetic data generation, which may not fully capture the diversity of real-world logical reasoning tasks
- The 4-bit quantization and LoRA fine-tuning details remain underspecified, making exact reproduction challenging
- The optimal augmentation probability (α_TT = 0.7) was likely determined through validation, raising concerns about potential overfitting to this specific configuration

## Confidence

- **High confidence:** The core finding that fine-tuned Llama models significantly outperform few-shot GPT-4o on formal proof generation is well-supported by controlled experiments across multiple model sizes and augmentation settings.
- **Medium confidence:** The claim that template transformation enhances handling of complex expressions is supported but relies on synthetic evaluation; real-world applicability remains to be demonstrated.
- **Medium confidence:** The assertion that larger models generalize better to deeper proofs is reasonable given the evidence, but the relationship between model capacity and proof complexity warrants further investigation.

## Next Checks

1. **Generalization test:** Evaluate the fine-tuned models on manually constructed proofs with varying depths and widths to verify that synthetic training data transfers to hand-designed logical reasoning tasks.
2. **Ablation study:** Systematically vary template transformation probability (α_TT) and measure width/depth generalization to confirm the reported 0.7 optimum is robust and not an artifact of validation set selection.
3. **Real-world application:** Apply the fine-tuned models to proof generation tasks in existing formal verification systems (e.g., Lean) to assess practical utility beyond synthetic benchmarks.