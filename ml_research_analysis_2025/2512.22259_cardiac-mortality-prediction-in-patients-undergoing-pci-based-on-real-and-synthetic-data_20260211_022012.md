---
ver: rpa2
title: Cardiac mortality prediction in patients undergoing PCI based on real and synthetic
  data
arxiv_id: '2512.22259'
source_url: https://arxiv.org/abs/2512.22259
tags:
- data
- oversampling
- synthetic
- xgboost
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of predicting long-term cardiac
  mortality after percutaneous coronary intervention (PCI) in patients with bifurcation
  lesions, where class imbalance severely limits model performance. To tackle this,
  the authors generated synthetic data using multiple generative approaches (ARF,
  GAN, TVAE, Gaussian Copula, TabSyn) and incorporated edge-case profiles to enhance
  minority-class representation.
---

# Cardiac mortality prediction in patients undergoing PCI based on real and synthetic data

## Quick Facts
- arXiv ID: 2512.22259
- Source URL: https://arxiv.org/abs/2512.22259
- Reference count: 40
- Primary result: Synthetic data augmentation (especially ARF and GAN) improves minority-class detection in imbalanced cardiac mortality prediction

## Executive Summary
This study addresses the challenge of predicting long-term cardiac mortality after percutaneous coronary intervention (PCI) in patients with bifurcation lesions, where class imbalance severely limits model performance. To tackle this, the authors generated synthetic data using multiple generative approaches (ARF, GAN, TVAE, Gaussian Copula, TabSyn) and incorporated edge-case profiles to enhance minority-class representation. Across diverse models, ARF and GAN oversampling consistently improved minority-class recall and F1-score while maintaining strong AUC-ROC, whereas other methods showed modest or inconsistent gains. Feature importance analysis identified Age, Ejection Fraction, Peripheral Artery Disease, and Cerebrobrovascular Disease as key predictors. Synthetic augmentation reduced model overconfidence and improved probability calibration, though calibration trade-offs persisted. Models trained on a reduced set of high-impact features further enhanced performance. External validation on acute myocardial infarction patients confirmed the robustness of the approach.

## Method Summary
The study used a private registry dataset of 2,044 PCI patients (1,635 train, 409 test) with 22 features to predict 3-year cardiac mortality. After preprocessing (imputation, one-hot encoding), synthetic minority-class samples were generated using ARF, GAN, TVAE, Gaussian Copula, and TabSyn methods (500 samples each). Models including Logistic Regression, Random Forest, CatBoost, XGBoost, TabPFN, and KAN were trained with hyperparameter optimization via Hyperopt. Feature selection was performed using ANOVA F-value filtering and permutation importance. Probability calibration was applied to tree-based models using Platt scaling. Evaluation included AUC-ROC, F1, recall, precision, ECE, and Brier score, with edge-case analysis on synthetically generated critical patient profiles.

## Key Results
- ARF and GAN oversampling consistently improved minority-class recall and F1-score (e.g., Random Forest F1 improved from 0 to 0.29) while maintaining strong AUC-ROC (0.78)
- Synthetic augmentation reduced model overconfidence (average confidence dropped from 0.92 to 0.83 with ARF) but increased Expected Calibration Error (ECE increased from 0.03 to 0.11)
- Feature selection on Age, Ejection Fraction, Peripheral Artery Disease, and Cerebrovascular Disease further enhanced performance, with all models achieving AUC-ROC >0.80 on test set
- External validation on acute myocardial infarction patients confirmed the robustness of the approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ARF and GAN-based synthetic oversampling improves minority-class detection in imbalanced clinical datasets
- Mechanism: Synthetic samples expand the minority-class decision boundary by populating sparse regions of feature space, allowing models to learn more robust separation rather than defaulting to majority-class predictions. ARF uses adversarial random forests to approximate the minority distribution; GANs learn conditional distributions through generator-discriminator dynamics.
- Core assumption: Generated samples preserve meaningful feature relationships from the real minority class rather than introducing artifacts that mislead classifiers.
- Evidence anchors:
  - [abstract]: "ARF and GAN oversampling consistently improved minority-class recall and F1-score while maintaining strong AUC-ROC, whereas other methods showed modest or inconsistent gains"
  - [Results, Table 1]: Random Forest with ARF improved F1 from 0 to 0.29, Recall from 0 to 0.29 while maintaining AUC-ROC at 0.78
  - [corpus]: Limited direct corpus support for ARF specifically; related ICU mortality papers use standard oversampling (SMOTE) without systematic comparison of generative methods

### Mechanism 2
- Claim: Synthetic data reduces model overconfidence and exposes hidden brittleness in probability estimates
- Mechanism: Without augmentation, models achieve high accuracy by predicting the majority class with high confidence (avg confidence ~0.92), masking poor calibration on minority instances. Adding synthetic samples forces models to assign meaningful probabilities across both classes, reducing average confidence and revealing true uncertainty.
- Core assumption: Lower confidence reflects genuine uncertainty rather than miscalibration from poor-quality synthetic data.
- Evidence anchors:
  - [abstract]: "Synthetic augmentation reduced model overconfidence and improved probability calibration, though calibration trade-offs persisted"
  - [Results, Table 3]: Average confidence dropped from 0.92 (no oversampling) to 0.83 (Random Forest with ARF); ECE increased from 0.03 to 0.11
  - [corpus]: Weak corpus evidence for this specific mechanism; related mortality prediction papers do not systematically evaluate calibration trade-offs with synthetic data

### Mechanism 3
- Claim: Feature selection based on permutation importance improves discrimination by reducing noise
- Mechanism: Non-informative features introduce noise that obscures the signal from predictive variables. Permutation feature importance identifies the subset of features that actually contribute to model decisions; training on this reduced set focuses model capacity on meaningful patterns.
- Core assumption: The top features identified (Age, Ejection Fraction, Peripheral Artery Disease, Cerebrovascular Disease) capture the core predictive signal, and removed features provide negligible independent information.
- Evidence anchors:
  - [abstract]: "Models trained on a reduced set of high-impact features further enhanced performance"
  - [Results, Table 6]: With feature-selected training, all models achieved AUC-ROC >0.80 on test set vs. 0.75-0.82 without feature selection
  - [corpus]: Supported by related literature—feature selection is standard practice in ICU mortality models, though specific feature sets vary by population

## Foundational Learning

- **Class Imbalance and Metric Deception**:
  - Why needed here: High accuracy (0.92-0.93) with zero recall on minority class demonstrates that standard metrics mask catastrophic failure modes in imbalanced settings.
  - Quick check question: If your model achieves 95% accuracy on a dataset with 5% positive class, what is the minimum F1-score it could achieve? (Answer: 0, by predicting all negatives)

- **Probability Calibration (ECE, Brier Score)**:
  - Why needed here: The paper shows models can appear well-calibrated while being overconfident on the majority class; ECE and Brier score reveal whether predicted probabilities match observed frequencies.
  - Quick check question: A model predicts 0.90 probability for 100 patients, and 85 of them experience the event. What is the calibration error for this bin? (Answer: |0.90 - 0.85| = 0.05)

- **Permutation Feature Importance**:
  - Why needed here: Identifies which features actually drive predictions rather than relying on coefficient magnitude (which fails for tree ensembles and neural networks).
  - Quick check question: If shuffling a feature causes AUC-ROC to drop by 0.15, what does this suggest about that feature's importance? (Answer: High importance—it contributed significantly to model discrimination)

## Architecture Onboarding

- **Component map**: Data preprocessing -> Feature selection -> Synthetic data generation -> Model training -> Calibration -> Evaluation
- **Critical path**: Start with baseline (no oversampling) → measure calibration and minority-class metrics → add ARF oversampling → evaluate trade-offs → apply feature selection → validate on external dataset
- **Design tradeoffs**:
  - ARF/GAN: Better sensitivity, worse calibration, higher F1
  - TabPFN: Best calibration, near-zero recall (unsuitable for imbalanced tasks)
  - KAN: Highest recall, worst calibration (ECE >0.30 with ARF)
  - Feature selection: +0.03-0.08 AUC gain, but requires external validation to confirm generalization

- **Failure signatures**:
  - F1 = 0 with high AUC-ROC: Model is not predicting any positives (threshold or imbalance issue)
  - ECE drops but recall stays at 0: Synthetic data quality issue—generated samples don't expand useful decision boundary
  - External validation AUC drops >0.10: Overfitting to training distribution or covariate shift in features

- **First 3 experiments**:
  1. Establish baseline: Train all models on original data; verify that high accuracy masks zero minority-class detection (replicate Table 1 baseline row)
  2. Apply ARF oversampling with N=500 synthetic samples; compare F1/recall gains vs. ECE increase to determine acceptable trade-off point
  3. Train on feature-selected dataset (Age, Ejection Fraction, PAD, CVD + clinical features from Table 6); evaluate on held-out test set and edge-case cohort to assess robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal ratio of synthetic to real samples for minority-class oversampling, and does tuning this hyperparameter via cross-validation yield significantly better performance than the fixed count of 500 used in this study?
- Basis in paper: [explicit] The authors note that "only 500 positive-class samples were added to the training set" and that "This hyperparameter can be determined using the validation set or cross-validation... However, such an experiment was not conducted in the present study."
- Why unresolved: The study utilized a fixed augmentation size (N=500) to compare generative methods, leaving the sensitivity of model performance to the volume of synthetic data unexplored.
- What evidence would resolve it: Results from a grid search or Bayesian optimization over different synthetic sample counts (e.g., 100 to 2000), evaluating F1-score and AUC-ROC on a held-out validation set.

### Open Question 2
- Question: Do the generative models (specifically ARF and GAN) preserve the complex, non-linear conditional dependencies between clinical features better than other methods like TVAE or Gaussian Copula?
- Basis in paper: [explicit] The paper states that "further research is needed to assess whether synthetic data accurately preserves these dependencies in clinical contexts."
- Why unresolved: While the study demonstrates improved classification metrics, it does not quantitatively assess the structural fidelity or correlation matrices of the generated synthetic data against the real minority-class distribution.
- What evidence would resolve it: A comparative statistical analysis (e.g., using density plots or correlation difference matrices) between real high-risk patient data and the synthetic cohorts generated by each algorithm.

### Open Question 3
- Question: Can the observed trade-off between improved minority-class recall and degraded probability calibration (increased Expected Calibration Error) be mitigated while retaining the benefits of synthetic data?
- Basis in paper: [explicit] The authors highlight a clear "trade-off between calibration and sensitivity," noting that models achieving high recall often sacrifice calibration (e.g., KAN), whereas well-calibrated models often fail to identify minority cases (e.g., TabPFN).
- Why unresolved: The current results show that synthetic augmentation reduces model overconfidence but often increases calibration error; no method tested successfully optimized both simultaneously.
- What evidence would resolve it: Experiments combining synthetic oversampling with specialized calibration techniques (e.g., isotonic regression or temperature scaling applied post-hoc) to see if high recall and low ECE can coexist.

## Limitations
- The study relies on a private registry dataset, preventing direct replication and requiring surrogate data for verification
- Synthetic data generation methods lack systematic evaluation of sample fidelity and clinical plausibility
- The calibration trade-off (improved recall at cost of increased ECE) represents an unresolved clinical compromise
- Edge-case analysis uses arbitrary critical value definitions without empirical justification for specific thresholds

## Confidence

- **High confidence**: The core finding that synthetic oversampling improves minority-class detection in severely imbalanced clinical datasets is well-supported by consistent F1 and recall improvements across multiple model types when using ARF and GAN methods
- **Medium confidence**: The identification of Age, Ejection Fraction, PAD, and CVD as key predictors is reasonable given the feature importance analysis, though specific contribution weights may vary across populations
- **Medium confidence**: The calibration trade-off observation is supported by reported ECE and Brier score changes, but clinical implications require further investigation
- **Low confidence**: Edge-case analysis conclusions are limited by arbitrary critical value definitions and lack of comparative analysis

## Next Checks

1. **Synthetic data fidelity audit**: Conduct feature correlation analysis and distribution comparison between real and synthetic minority-class samples to verify that generated data preserves clinically meaningful relationships rather than introducing artifacts
2. **Calibration robustness test**: Perform threshold optimization analysis to determine whether probability calibration improvements can be achieved without sacrificing the minority-class detection gains observed with ARF/GAN augmentation
3. **Population generalizability study**: Apply the ARF oversampling approach to a publicly available imbalanced mortality dataset (e.g., MIMIC-IV) with similar feature sets to assess whether the methodology generalizes beyond the specific PCI patient population studied