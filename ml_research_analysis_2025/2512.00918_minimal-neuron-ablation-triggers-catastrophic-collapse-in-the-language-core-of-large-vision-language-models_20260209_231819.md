---
ver: rpa2
title: Minimal neuron ablation triggers catastrophic collapse in the language core
  of Large Vision-Language Models
arxiv_id: '2512.00918'
source_url: https://arxiv.org/abs/2512.00918
tags:
- neurons
- collapse
- critical
- clip
- neuron
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study identifies critical neurons in Large Vision-Language
  Models (LVLMs) that, when masked, trigger catastrophic collapse. The authors propose
  the Consistently Activated Neurons (CAN) method, which combines activation magnitude
  and gradient sensitivity to rank neuron importance.
---

# Minimal neuron ablation triggers catastrophic collapse in the language core of Large Vision-Language Models

## Quick Facts
- arXiv ID: 2512.00918
- Source URL: https://arxiv.org/abs/2512.00918
- Reference count: 40
- Key outcome: Masking 4-5 neurons in LVLMs can trigger complete model failure

## Executive Summary
This study reveals that Large Vision-Language Models (LVLMs) contain critical neurons whose targeted ablation can cause catastrophic collapse. The researchers developed the Consistently Activated Neurons (CAN) method to identify the most vulnerable neurons by combining activation magnitude and gradient sensitivity. Their experiments demonstrate that masking just 4-5 neurons in the language model's feed-forward networks of LLaVA-1.5-7b-hf and InstructBLIP-Vicuna-7b can cause complete model failure. The findings expose a significant vulnerability in LVLMs, showing that a tiny fraction of neurons can have disproportionate control over model functionality.

## Method Summary
The authors propose the Consistently Activated Neurons (CAN) method to identify critical neurons by combining activation magnitude and gradient sensitivity. The method ranks neurons based on their consistent activation patterns across input samples and their sensitivity to gradients during backpropagation. The researchers systematically mask neurons in both the vision and language components of two LVLMs (LLaVA-1.5-7b-hf and InstructBLIP-Vicuna-7b) to identify which neurons, when removed, cause the most severe performance degradation. They track performance degradation through a two-stage collapse pattern: initial expressive degradation followed by sudden complete failure.

## Key Results
- Masking just 4-5 neurons in the language model's feed-forward networks can trigger complete LVLM failure
- Critical neurons are predominantly located in the language model rather than vision components, with the down-projection layer being particularly vulnerable
- A consistent two-stage collapse pattern emerges: initial expressive degradation followed by sudden, complete failure
- The CAN method effectively identifies neurons whose removal causes disproportionate performance degradation

## Why This Works (Mechanism)
The vulnerability arises from the highly specialized nature of certain neurons that perform critical computational functions within the LVLM architecture. These neurons, when consistently activated across diverse inputs and highly sensitive to gradients, indicate their importance in maintaining model functionality. The two-stage collapse pattern suggests that LVLMs operate with significant redundancy until a critical threshold is reached, after which the loss of key computational pathways triggers cascading failures. The disproportionate impact of masking a tiny fraction of neurons reveals that these models may rely on specific neural pathways more than previously understood.

## Foundational Learning

- **Consistently Activated Neurons (CAN) method**: Combines activation magnitude and gradient sensitivity to rank neuron importance. Why needed: Traditional importance measures fail to capture the combined effect of consistent activation patterns and gradient-based sensitivity. Quick check: Compare CAN rankings against random neuron masking to validate effectiveness.

- **Two-stage collapse pattern**: Initial gradual degradation followed by sudden catastrophic failure. Why needed: Understanding failure dynamics helps predict vulnerability thresholds and potential recovery points. Quick check: Plot performance metrics against increasing masking ratios to confirm consistent two-stage pattern.

- **Feed-forward network criticality**: Specific layers (particularly down-projection) are more vulnerable than others. Why needed: Identifies architectural weak points that require additional robustness measures. Quick check: Compare masking effects across different layer types within the same model.

## Architecture Onboarding

Component map: Vision encoder -> Cross-modal fusion -> Language model (feed-forward networks) -> Output decoder

Critical path: Input image/text -> Vision encoder -> Cross-attention layers -> Feed-forward networks (down-projection layer) -> Language generation

Design tradeoffs: The study reveals that efficiency optimizations in LVLMs may have created single points of failure by concentrating critical functionality in specific neurons rather than distributing it across redundant pathways.

Failure signatures: Initial performance degradation appears as increased generation errors and reduced coherence, followed by complete inability to generate meaningful outputs.

First experiments:
1. Apply CAN method to identify critical neurons in additional LVLM architectures
2. Test random vs. targeted neuron masking to establish vulnerability thresholds
3. Investigate neuron criticality across different model sizes and training paradigms

## Open Questions the Paper Calls Out
Major uncertainties remain regarding the generalizability of these findings across different LVLM architectures and training paradigms. The study focuses on two specific models (LLaVA-1.5-7b-hf and InstructBLIP-Vicuna-7b), and while the authors claim consistent patterns across both, the results may not extend to other model families, sizes, or training methodologies. The CAN method's effectiveness for identifying critical neurons in models with different attention mechanisms or architectural designs requires further validation.

## Limitations
- Findings may not generalize across different LVLM architectures and training paradigms
- Analysis focuses primarily on feed-forward network neurons, potentially overlooking other critical components
- Practical exploitability of these vulnerabilities in real-world deployed systems remains unclear

## Confidence

- Neuron criticality identification: High
- Two-stage collapse pattern: Medium
- Architecture-specific vulnerabilities: Medium
- Generalizability across LVLM families: Low

## Next Checks

1. Test the CAN method on additional LVLM architectures (e.g., Flamingo, BLIP-2) to verify the consistency of critical neuron identification across different design paradigms.

2. Evaluate the robustness of critical neurons against different masking strategies (random vs. targeted) and varying masking ratios to establish the minimum threshold for catastrophic failure.

3. Investigate the transferability of critical neuron locations across models with similar architectures but different training datasets to assess the impact of training data on neuron vulnerability patterns.