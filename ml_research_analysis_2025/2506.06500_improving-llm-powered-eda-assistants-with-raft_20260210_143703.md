---
ver: rpa2
title: Improving LLM-Powered EDA Assistants with RAFT
arxiv_id: '2506.06500'
source_url: https://arxiv.org/abs/2506.06500
tags:
- data
- synthetic
- training
- document
- raft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RAFT fine-tuning with synthetic QA data significantly improves
  LLM performance for RAG-based EDA assistants, achieving up to 85.17% F1 score on
  Q2A test sets. Incorporating real user questions as RAFS examples further enhances
  performance on human-authored queries.
---

# Improving LLM-Powered EDA Assistants with RAFT

## Quick Facts
- arXiv ID: 2506.06500
- Source URL: https://arxiv.org/abs/2506.06500
- Authors: Luyao Shi; Michael Kazda; Charles Schmitter; Hemlata Gupta
- Reference count: 18
- RAFT fine-tuning with synthetic QA data significantly improves LLM performance for RAG-based EDA assistants, achieving up to 85.17% F1 score on Q2A test sets.

## Executive Summary
This paper introduces RAFT (Retrieval-Augmented Fine-Tuning), a method that fine-tunes large language models on synthetic question-answer pairs to improve their performance in RAG-based EDA (Electronic Design Automation) assistants. The approach leverages unlabeled EDA documents to generate synthetic QA data, optionally incorporating real user questions as Retrieval-Augmented Few-Shot (RAFS) examples to better align with user query distribution. RAFT achieves up to 85.17% F1 score on human-authored questions while addressing the scarcity of labeled data in specialized domains. However, the method introduces a memorization risk that requires access control measures to prevent unintended data leakage.

## Method Summary
The RAFT approach fine-tunes Llama-3.1-8B-Instruct using LoRA (rank=128, α=32) on a dataset combining refined Q2A posts and synthetic QA pairs generated from unlabeled EDA documents. Synthetic questions are generated using DeepSeek-V3, optionally guided by RAFS examples (Top-K real user questions retrieved via BM25). The model is trained to integrate retrieved context from hybrid search (BM25 + semantic embeddings via RRF) into response generation. Fine-tuning uses 5 epochs with max sequence length 8,192 tokens. Access control is implemented through Elasticsearch filtering to restrict document retrieval based on user access groups.

## Key Results
- RAFT with synthetic data significantly boosts LLM performance for RAG-based EDA tasks
- Incorporating RAFS examples further enhances performance on human-authored queries (85.17% F1 on Q2A test set)
- RAFT training leads to memorization risk requiring access control measures to prevent data leakage

## Why This Works (Mechanism)

### Mechanism 1: RAFT with Synthetic QA Improves RAG Integration
- Claim: Synthetic QA datasets enable RAFT to improve LLM performance on RAG-based EDA tasks, reducing dependence on scarce labeled data.
- Mechanism: Synthetic Q/A pairs generated from unlabeled EDA documents train the LLM to better integrate retrieved context into response generation. The model learns to condition outputs on dynamically retrieved chunks rather than relying solely on parametric knowledge.
- Core assumption: Synthetic questions accurately reflect the distribution and complexity of real EDA queries.
- Evidence anchors:
  - [abstract] "RAFT with synthetic data significantly boosts LLM performance for RAG-based EDA tasks."
  - [section II.A] "We leverage a larger pool of unlabeled EDA documents and employ DeepSeek-V3 to generate synthetic questions and corresponding answers based on these documents."
  - [corpus] Corpus evidence is weak; related work (JARVIS, CROP) uses synthetic data for EDA but does not isolate RAFT effects or report direct replication.
- Break condition: If synthetic questions diverge significantly from real user query patterns, performance gains will not transfer to production.

### Mechanism 2: RAFS Examples Align Synthetic Data with User Query Distribution
- Claim: Incorporating real user questions as Retrieval-Augmented Few-Shot (RAFS) examples in synthetic data generation improves performance on human-authored queries.
- Mechanism: BM25 retrieves Top-K relevant past user questions from a Q&A history database, which serve as few-shot examples for the Q/A-generation LLM (DeepSeek-V3). This guides the generator to produce questions matching real user complexity and style.
- Core assumption: Past user questions are representative of future user queries.
- Evidence anchors:
  - [abstract] "Incorporating real user questions as RAFS examples further enhances performance on human-authored queries."
  - [section III.D, Table II] D2-RAFS achieves 85.17% F1 on Q2A vs. 84.47% for D2 without RAFS.
  - [corpus] No corpus papers explicitly test RAFS; this appears to be a novel contribution specific to this work.
- Break condition: If the Q&A history database is sparse, outdated, or biased toward certain topics, RAFS examples may not generalize.

### Mechanism 3: RAFT Training Induces Memorization Risk
- Claim: RAFT fine-tuning on synthetic data leads to unintended memorization, creating data leakage risk in sensitive domains.
- Mechanism: During RAFT training, the model may internalize Q/A pairs as parametric knowledge rather than learning to rely solely on retrieved context. This causes the model to answer questions even when source documents are absent from the prompt.
- Core assumption: Memorization can be detected by measuring model responses when relevant context is intentionally withheld.
- Evidence anchors:
  - [abstract] "RAFT training leads to some memorization, requiring access control measures to prevent unintended data leakage."
  - [section III.D, Table III] Fine-tuned D2 model shows increased recall (21.01%) on SynthQA MissingContext vs. baseline (20.73%), indicating memorization persists despite missing context.
  - [corpus] Corpus papers do not address memorization risk in RAFT; security implications are underexplored in related EDA-LLM work.
- Break condition: If access control is implemented only at retrieval time without addressing memorization, unauthorized users may still extract sensitive information through carefully crafted queries.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAFT extends RAG by fine-tuning the model to better use retrieved context. Understanding baseline RAG failure modes (hallucination, retrieval failures) is essential to interpret RAFT improvements.
  - Quick check question: Can you explain why RAG alone may still produce inaccurate responses in domain-specific tasks?

- Concept: Parameter-Efficient Fine-Tuning (LoRA/PEFT)
  - Why needed here: The paper uses LoRA (rank=128, α=32) to fine-tune Llama-3.1-8B. Understanding LoRA is necessary to reproduce results and adjust capacity.
  - Quick check question: What happens to model performance if LoRA rank is set too low for a complex domain like EDA?

- Concept: Hybrid Retrieval (Semantic + Lexical with RRF)
  - Why needed here: The retrieval pipeline combines BM25 lexical search with semantic embeddings using Reciprocal Rank Fusion. This is core to both training data preparation and inference.
  - Quick check question: Why would BM25 outperform semantic search for queries containing specific EDA acronyms or command names?

## Architecture Onboarding

- Component map: Unlabeled EDA documents -> DeepSeek-V3 synthetic Q/A generation -> Hybrid retrieval (BM25 + semantic + RRF) -> RAFT fine-tuning on Llama-3.1-8B -> Elasticsearch access control filtering -> Response generation

- Critical path:
  1. Ingest EDA documents → chunk into 2,000-character segments with 200-character overlap → tag each chunk with access group metadata
  2. Generate synthetic Q/A pairs using DeepSeek-V3, optionally incorporating Top-K RAFS examples from Q&A history database
  3. Fine-tune EDA-LLM with RAFT on combined Q2A + SynthQA dataset (1,205 samples)
  4. At inference: retrieve user access group → filter documents → hybrid search → generate response

- Design tradeoffs:
  - **Synthetic vs. Real Q/A**: Synthetic data scales but may not match user query distribution; RAFS mitigates style mismatch but requires existing Q&A history
  - **Missing Context Training**: Adding "I don't know" samples (D3-MC10%, D3-MC20%) reduces memorization but also increases false refusals when context is present
  - **Access Control Granularity**: Per-chunk tagging enables precise control but requires consistent metadata maintenance across document updates

- Failure signatures:
  - **Memorization leakage**: Fine-tuned model answers questions even when source documents removed from context (Table III: D2 recall 21.01% on MissingContext)
  - **Over-refusal**: D3-MC models produce "I don't know" responses even when relevant context exists (8-12 #IDK on SynthQA with context present)
  - **Retrieval failure**: If hybrid search misses relevant chunks, RAFT-trained model may still hallucinate despite fine-tuning

- First 3 experiments:
  1. **Retrieval ablation**: Compare RAG performance with BM25-only vs. hybrid retrieval on Q2A test set to isolate retrieval quality contribution
  2. **RAFS sensitivity**: Train D2-RAFS with varying K (1, 3, 5, 10 few-shot examples) to identify optimal reference question count
  3. **Memorization quantification**: For each RAFT configuration, measure recall delta between SynthQA (context present) and SynthQA-MissingContext; target <5% delta while maintaining >80% recall on SynthQA

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can RAFT be adapted to prevent the memorization of sensitive data without causing the model to refuse valid answers when relevant context is present?
- Basis in paper: [explicit] The authors note in Section III.D that training with "Missing Context" samples to reduce leakage causes the model to generate "I don't know" responses even when source information is available.
- Why unresolved: The paper identifies a trade-off where current mitigation strategies for data leakage (using "I don't know" labels) negatively impact the model's usability by increasing false refusals.
- What evidence would resolve it: A training methodology that achieves near-zero recall on "SynthQA MissingContext" while maintaining high recall on the standard "SynthQA" test set.

### Open Question 2
- Question: Can Retrieval-Augmented Few-Shot (RAFS) examples be curated to enhance performance on complex reasoning tasks rather than primarily aligning with user query styles?
- Basis in paper: [inferred] Table II shows that while D2-RAFS improves performance on human-authored Q2A queries, it slightly decreases performance on the reasoning-heavy SynthQA questions compared to the D2 baseline.
- Why unresolved: The paper suggests RAFS aligns with user style but does not investigate why it fails to aid, or slightly hinders, the deeper reasoning required for synthetic questions.
- What evidence would resolve it: An ablation study comparing RAFS selection strategies based on question complexity versus stylistic similarity to measure impact on SynthQA scores.

### Open Question 3
- Question: To what extent does increasing the volume and diversity of synthetic QA data mitigate the need for expert-labeled data in specialized domains?
- Basis in paper: [explicit] The Conclusion states that "Future work will focus on utilizing a larger and more diverse set of synthetic QAs to further improve training."
- Why unresolved: While the current 900-sample synthetic set improved performance, the ceiling for synthetic data scaling versus the quality of human-labeled data remains undetermined.
- What evidence would resolve it: Training curves plotting F1 scores against increasing volumes of synthetic data compared against a fixed volume of expert-labeled data.

## Limitations

- Generalizability to other technical domains remains unverified, as the approach is demonstrated exclusively on EDA data
- RAFS performance critically depends on having a representative Q&A history database, with no quantification of degradation under sparse or biased historical data
- Memorization risk is identified but not fully characterized in terms of real-world security implications or complete mitigation strategies

## Confidence

- **High Confidence**: The core RAFT mechanism (fine-tuning with synthetic QA) works as described, supported by measurable performance improvements on both synthetic and human-authored test sets. The LoRA implementation details are sufficiently specified for reproduction.
- **Medium Confidence**: The RAFS contribution is validated on this specific dataset, but the reliance on representative historical data introduces uncertainty about generalizability. The memorization risk is correctly identified but not fully quantified in terms of real-world security implications.
- **Low Confidence**: Claims about access control effectiveness are based on methodology description rather than empirical validation. The paper describes implementing access filtering but doesn't test whether it successfully prevents all memorization-based information leakage.

## Next Checks

1. **Domain Transfer Test**: Apply the RAFT approach to a different technical domain (e.g., biomedical literature or legal documents) with similar document structure. Measure whether synthetic QA generation and RAFS examples provide comparable performance improvements, or if domain-specific adjustments are needed.

2. **RAFS Robustness Analysis**: Systematically vary the quality and quantity of RAFS examples by training models with: (a) random questions instead of BM25-retrieved similar ones, (b) synthetic questions generated without RAFS, and (c) different K values (1, 3, 5, 10 examples). Quantify the performance degradation to establish minimum requirements for effective RAFS.

3. **Security Vulnerability Assessment**: Design adversarial queries specifically crafted to trigger memorized responses. Measure the information disclosure rate by comparing model outputs when source documents are present vs. absent from context. Test whether the access control filtering prevents unauthorized access to sensitive information even when memorization occurs.