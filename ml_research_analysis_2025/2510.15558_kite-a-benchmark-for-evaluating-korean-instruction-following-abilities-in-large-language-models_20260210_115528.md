---
ver: rpa2
title: 'KITE: A Benchmark for Evaluating Korean Instruction-Following Abilities in
  Large Language Models'
arxiv_id: '2510.15558'
source_url: https://arxiv.org/abs/2510.15558
tags:
- kite
- korean
- arxiv
- evaluation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KITE, the first benchmark specifically designed
  to evaluate the instruction-following capabilities of Korean Large Language Models
  (LLMs). Unlike existing benchmarks that focus on factual knowledge or multiple-choice
  testing, KITE directly targets diverse, open-ended instruction-following tasks,
  addressing the unique linguistic and cultural nuances of Korean, such as complex
  syntax, rich morphological features, honorific systems, and dual numbering systems.
---

# KITE: A Benchmark for Evaluating Korean Instruction-Following Abilities in Large Language Models

## Quick Facts
- arXiv ID: 2510.15558
- Source URL: https://arxiv.org/abs/2510.15558
- Authors: Dongjun Kim; Chanhee Park; Chanjun Park; Heuiseok Lim
- Reference count: 15
- Primary result: KITE is the first benchmark specifically designed to evaluate Korean LLMs' instruction-following capabilities across linguistic and cultural nuances.

## Executive Summary
This paper introduces KITE, the first benchmark specifically designed to evaluate the instruction-following capabilities of Korean Large Language Models (LLMs). Unlike existing benchmarks that focus on factual knowledge or multiple-choice testing, KITE directly targets diverse, open-ended instruction-following tasks, addressing the unique linguistic and cultural nuances of Korean, such as complex syntax, rich morphological features, honorific systems, and dual numbering systems. The benchmark comprises two versions: KITE General, with 427 instructions derived from translated English datasets, and KITE Korean, with 100 instructions created from scratch to address Korean-specific challenges. Experiments show that GPT-4o-2024-05-13 outperforms other models, including Korean-specific ones like SOLAR 1 Mini Chat, HyperCLOV A X 003, and EEVE v1.0 10.8b Instruct, in both general and Korean-specific tasks. The study also reveals that shot settings do not significantly improve performance, highlighting the need for specialized tuning. Human evaluations confirm the reliability of the automated scoring, with high correlation between human and automated assessments. KITE is publicly released to foster research on culturally and linguistically inclusive LLM development.

## Method Summary
KITE comprises two benchmark versions: KITE General (427 instructions) translated from English IFEval dataset with contextual filtering to remove English-specific categories, and KITE Korean (100 instructions) created from scratch across four Korean-specific categories (acrostic poems, post-position drop, honorifics, dual number systems). The evaluation pipeline decomposes each instruction into verifiable sub-instructions, applies binary scoring (followed/not followed) per sub-instruction, and aggregates accuracy across all sub-instructions. Human evaluation with five expert annotators validates automated scoring reliability, achieving high correlation (r=0.96 for General, r=0.93 for Korean).

## Key Results
- GPT-4o-2024-05-13 achieves 89.35% accuracy on KITE General, outperforming other models including Korean-specific ones
- SOLAR 1 Mini Chat scores 56.21% on KITE Korean, lower than GPT-4o's 76.00%, despite Korean training
- Shot settings (0-5 shots) do not significantly improve performance, suggesting specialized instruction-tuning is needed
- Human evaluation confirms high correlation between automated and human scoring (r=0.96 and r=0.93)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translated benchmarks with contextual filtering preserve instruction-following evaluation validity while adapting to target language constraints.
- Mechanism: Automated translation (GPT-4o) of IFEval instructions followed by unanimous human filtering removes English-specific categories (e.g., capitalization constraints) while retaining verifiable constraints applicable to Korean.
- Core assumption: Instruction-following capabilities tested in English transfer linguistically when semantic constraints are preserved across translation.
- Evidence anchors:
  - [abstract] "KITE General, with 427 instructions derived from translated English datasets"
  - [section 3.2] "Out of the original 541 instructions, 114 were filtered out, resulting in a refined dataset of 427 instructions"
  - [corpus] XIFBench (2503.07539) similarly addresses multilingual instruction following but lacks Korean-specific morphological categories
- Break condition: If translation introduces systematic errors in constraint preservation, automated scoring will misclassify valid responses as failures.

### Mechanism 2
- Claim: Korean-specific linguistic categories enable targeted probing of morphological and sociolinguistic competencies absent in English-centric benchmarks.
- Mechanism: Four specialized categories (acrostic poems, post-position drop, honorifics, dual number systems) isolate distinct Korean linguistic phenomena through constrained generation tasks with rule-verifiable outputs.
- Core assumption: Successful performance on isolated linguistic tasks indicates broader instruction-following competence in Korean contexts.
- Evidence anchors:
  - [abstract] "KITE Korean, with 100 instructions created from scratch to address Korean-specific challenges"
  - [section 3.3] "For each of these four categories, we created 25 instructions, totaling 100 instructions"
  - [corpus] KoSimpleQA (2510.18368) focuses on Korean cultural knowledge factuality, not instruction-following—different capability axis
- Break condition: If models game specific categories through pattern matching without genuine linguistic understanding, category scores won't generalize to real-world instruction following.

### Mechanism 3
- Claim: Sub-instruction decomposition with binary verification enables objective, reproducible instruction-following scoring that correlates strongly with human judgment.
- Mechanism: Each instruction decomposes into verifiable sub-instructions; each sub-instruction receives binary score (followed=1, not=0); aggregate accuracy computed across all sub-instructions.
- Core assumption: Binary classification of constraint satisfaction captures instruction-following quality without requiring semantic quality assessment.
- Evidence anchors:
  - [section 4.3] "If the response meets the criteria of the sub-instruction, it is marked as 'followed' (f(s_ij) = 1)"
  - [appendix C] "Pearson correlation coefficients are r= 0.96 for KITE General and r= 0.93 for KITE Korean"
  - [corpus] Speech-IFEval (2505.19037) adapts similar decomposition approach for speech-aware models, validating methodology generalizability
- Break condition: If complex instructions require graded rather than binary satisfaction assessment, accuracy scores will fail to capture partial compliance.

## Foundational Learning

- Concept: **Instruction-following vs. knowledge evaluation**
  - Why needed here: KITE explicitly differentiates instruction-following (constraint satisfaction) from factual knowledge (MMLU-style testing); conflating these leads to misinterpretation of benchmark results.
  - Quick check question: Can a model score high on Ko-MMLU but low on KITE Korean? What would this indicate?

- Concept: **Agglutinative morphology and post-position flexibility**
  - Why needed here: Korean forms words through root+affix combination and uses post-positions (Josa) with flexible word order; understanding this explains why "post-position drop" is a meaningful test category.
  - Quick check question: Why does flexible word order make instruction-following evaluation more complex in Korean than English?

- Concept: **Honorific register systems**
  - Why needed here: Korean encodes social hierarchy through speech style (존댓말/반말); instruction-following must include sociolinguistic competence, not just semantic correctness.
  - Quick check question: If a model produces semantically correct output but wrong honorific level, should this count as instruction-following failure?

## Architecture Onboarding

- Component map:
  - **KITE General**: 427 translated+filtered instructions from IFEval categories (prompts, format constraints, length constraints, etc.)
  - **KITE Korean**: 100 original instructions across 4 categories (acrostic poems, post-position drop, honorifics, number systems)
  - **Evaluation pipeline**: Sub-instruction parser → rule-based verifier → accuracy aggregator
  - **Validation layer**: Human evaluation correlation (5 expert annotators, 20% sample)

- Critical path:
  1. Load benchmark data from HuggingFace (`junkim100/KITE`)
  2. Generate model responses for each instruction
  3. Parse sub-instructions from instruction metadata
  4. Apply rule-based verification per sub-instruction
  5. Compute aggregate accuracy via formula (Eq. 1)

- Design tradeoffs:
  - **Verifiability vs. cultural depth**: Authors prioritized rule-verifiable linguistic categories over complex cultural scenarios requiring human judgment
  - **Translation efficiency vs. authenticity**: KITE General uses translation for scale; KITE Korean uses original creation for authenticity—hybrid approach balances coverage vs. nuance
  - **Shot-setting variability**: Paper shows few-shot doesn't reliably improve IF performance; zero-shot may be preferable for evaluation consistency

- Failure signatures:
  - Korean-specific models (SOLAR, HyperCLOVA, EEVE) underperform GPT-4o on KITE Korean despite Korean training—indicates specialized training doesn't automatically yield instruction-following competence
  - Performance fluctuations across shot settings (Table 2) suggest instability in instruction-tuning; consistent 0-shot performance is more reliable signal
  - Low correlation between KITE and Ko-ARC (r=0.24) indicates instruction-following is distinct from reasoning capabilities—don't substitute benchmarks

- First 3 experiments:
  1. **Baseline reproduction**: Run KITE General zero-shot on GPT-3.5-turbo and GPT-4o; verify scores align with Table 2 (~75.92 vs ~89.35); confirms pipeline correctness
  2. **Ablation by category**: Evaluate Korean-specific model (e.g., EEVE v1.0) on each KITE Korean category separately; identify which linguistic feature (honorifics, number systems, etc.) drives lowest performance
  3. **Cross-benchmark correlation check**: Compute Pearson correlation between your model's KITE scores and Ko-MMLU/Ko-TruthfulQA; if r>0.5, instruction-following correlates with general competence; if r<0.3, capabilities are independent (consistent with paper's r=0.50, r=0.24 findings)

## Open Questions the Paper Calls Out
None

## Limitations

- Translation Validity (Confidence: Medium) - While the paper employs GPT-4o for automated translation followed by unanimous human filtering, the process cannot fully eliminate semantic drift or cultural nuance loss.
- Evaluation Scope (Confidence: Medium) - The benchmark focuses on rule-verifiable linguistic constraints but excludes pragmatic instruction-following scenarios involving context-dependent judgment or cultural appropriateness.
- Model Selection Bias (Confidence: Low) - The evaluation includes only six models, with Korean-specific models potentially unoptimized for instruction-following despite Korean training.

## Confidence

- KITE General construction validity: High
- KITE Korean category coverage: Medium
- GPT-4o superiority: High
- Few-shot instability: Medium

## Next Checks

1. **Inter-annotator reliability test** - Have five new annotators independently evaluate a 20% sample of KITE Korean instructions to establish Krippendorff's alpha for the verification process, particularly for honorific and number system categories.

2. **Cross-linguistic transfer analysis** - Evaluate English models on KITE Korean to quantify how much Korean-specific instruction-following ability transfers from English training, and whether Korean models show similar transfer when tested on English instruction-following benchmarks.

3. **Pragmatic instruction subset** - Create a small pilot subset of 25 instructions requiring contextual judgment (e.g., formality level based on inferred relationship, cultural appropriateness) and evaluate whether current automated verification can be extended or requires human-in-the-loop assessment.