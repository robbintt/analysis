---
ver: rpa2
title: Efficient Real-Time Aircraft ETA Prediction via Feature Tokenization Transformer
arxiv_id: '2508.09144'
source_url: https://arxiv.org/abs/2508.09144
tags:
- aircraft
- data
- arrival
- prediction
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a Transformer-based model for real-time aircraft
  ETA prediction, addressing the challenge of efficient and accurate ETA forecasting
  in dynamic airspace environments. The proposed approach uses feature tokenization
  to transform raw input data (e.g., aircraft position, speed, weather) into latent
  embeddings, which are then processed by a multi-head self-attention mechanism in
  the Transformer neural network.
---

# Efficient Real-Time Aircraft ETA Prediction via Feature Tokenization Transformer

## Quick Facts
- arXiv ID: 2508.09144
- Source URL: https://arxiv.org/abs/2508.09144
- Reference count: 27
- Primary result: 7.1% MAE improvement over XGBoost with 39% of training time and 51.7μs inference for 40 aircraft

## Executive Summary
This study presents a Transformer-based model for real-time aircraft ETA prediction, addressing the challenge of efficient and accurate ETA forecasting in dynamic airspace environments. The proposed approach uses feature tokenization to transform raw input data (e.g., aircraft position, speed, weather) into latent embeddings, which are then processed by a multi-head self-attention mechanism in the Transformer neural network. This design eliminates complex feature engineering and leverages the Transformer's parallel computation capabilities for high-frequency predictions (1Hz). Experiments using ADS-B data from Singapore Changi Airport demonstrate that the model outperforms the widely used XGBoost approach by improving accuracy by 7.1% while requiring only 39% of the training time.

## Method Summary
The approach transforms heterogeneous raw inputs through quantile normalization and feature tokenization into latent embeddings, which are processed by a pre-normalized Transformer encoder with 8-head self-attention. Continuous features undergo linear projection while categorical features use embedding lookup, producing unified d-dimensional representations. The model predicts ETA directly from the [CLS] token output without complex feature engineering, achieving microsecond-level inference times suitable for real-time arrival management.

## Key Results
- 7.1% MAE improvement over XGBoost baseline
- 39% of XGBoost training time required
- 51.7 microseconds inference time for 40 aircraft at 1Hz frequency

## Why This Works (Mechanism)

### Mechanism 1: Feature Tokenization for Unified Latent Representation
Feature tokenization projects heterogeneous raw inputs into a unified latent embedding space, enabling the Transformer to process aviation data without manual feature engineering. Continuous features undergo quantile normalization followed by linear projection, while categorical features use lookup table embeddings. Both produce d-dimensional embeddings stacked with a [CLS] token for prediction.

### Mechanism 2: Multi-Head Self-Attention for Cross-Feature Dependency Capture
The multi-head self-attention mechanism learns which feature combinations are most relevant for ETA prediction across varying arrival contexts. LayerNorm-normalized embeddings are projected into Q, K, V spaces across 8 attention heads. Each head computes scaled dot-product attention, capturing dynamic feature interactions that improve prediction accuracy.

### Mechanism 3: Parallel Architecture for High-Frequency Real-Time Inference
The Transformer's parallel computation enables microsecond-level batch inference for multiple aircraft, making it suitable for 1Hz real-time arrival management systems. Unlike sequential RNN/LSTM processing, Transformer encoder layers process all aircraft embeddings in parallel within each batch, achieving efficient scaling with batch size.

## Foundational Learning

- **Self-Attention Mechanism**: Why needed: The core innovation uses multi-head self-attention to learn feature interactions. Quick check: Given embeddings X of shape (batch, seq_len, d_model), what are the shapes of Q, K, V after projection, and what does the attention output represent?

- **Feature Tokenization for Tabular Data**: Why needed: Unlike NLP where tokenization is standard, tabular tokenization is less common. Quick check: For a continuous feature "ground_speed" with values [250, 300, 180] knots and a categorical feature "runway_direction" with values ["North", "South"], describe how each is converted to a d-dimensional embedding.

- **Quantile Normalization**: Why needed: The paper applies quantile normalization to continuous inputs before tokenization. Quick check: If a feature has a highly skewed distribution (e.g., distance to airport with many short-haul flights), how does quantile normalization change its representation, and what are potential failure modes?

## Architecture Onboarding

- **Component map**: Raw Inputs (ADS-B, METAR, FPL) → Quantile Normalization → Feature Tokenizer → Stack [CLS] token → LayerNorm → Multi-Head Self-Attention → Feed-Forward + Residual Connections → [CLS] token output → Linear projection → ETA prediction

- **Critical path**: 1) Data pipeline: ADS-B position, ground speed, theta degree, timestamp → normalized features; 2) Tokenization: Correct handling of continuous vs. categorical paths; [CLS] token must be prepended; 3) Transformer encoder: Pre-normalization is a key modification from original Transformer; 4) Inference: Batch all aircraft at timestamp t; extract [CLS] embedding for final prediction

- **Design tradeoffs**: Model dimension (d_model): Paper tests 192 and 256; higher dimensions improve accuracy but increase latency and memory. Number of blocks: Paper uses 2 blocks; deeper models may overfit on 31 days of data. Continuous vs. categorical encoding: Paper treats wake turbulence category and temporal features as continuous rather than categorical.

- **Failure signatures**: Prediction oscillation: Paper shows XGBoost produces oscillating predictions at 1Hz; FTT should produce smooth trajectories. Direction-specific degradation: West arrival direction shows higher ETA variance. Distance-dependent error: MAE increases with distance from airport.

- **First 3 experiments**: 1) Baseline replication with XGBoost: Reproduce MAE/MAPE/RMSE comparison using the same 23/4/4 day split. 2) Ablation on model dimension: Test d_model ∈ {64, 128, 192, 256, 512} with n_blocks=2 fixed. 3) Prediction stability analysis: Compute standard deviation of consecutive 1Hz predictions for same aircraft across landing trajectory.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the FTT model generalize to airports with different airspace structures, traffic densities, and runway configurations beyond Singapore Changi Airport?
- Basis: The study validates the approach using only one month of ADS-B data from a single airport (WSSS), with no cross-airport transferability testing.
- Why unresolved: Airports vary significantly in traffic complexity, STAR procedures, and geographical constraints; a model trained on one may not capture patterns relevant to others.
- What evidence would resolve it: Evaluation of the same model architecture on ADS-B datasets from multiple airports with reporting of comparative MAE/MAPE metrics.

### Open Question 2
- Question: How robust is the model to seasonal weather variations and atypical traffic conditions not present in the October 2022 training period?
- Basis: The dataset spans only 31 days (October 1–31, 2022), which cannot capture monsoon seasons, holiday traffic surges, or severe weather events.
- Why unresolved: Weather patterns and traffic demand vary seasonally; model performance may degrade under conditions outside the training distribution.
- What evidence would resolve it: Year-round evaluation with stratified analysis by season/weather type, including performance metrics during adverse weather events.

### Open Question 3
- Question: What post-processing mechanisms are required to integrate the FTT model into operational arrival management systems while ensuring safe aircraft separation?
- Basis: The paper states "further post-processing is necessary to ensure the separation of landing aircraft" for real runway slot allocation systems.
- Why unresolved: The paper demonstrates prediction accuracy but does not address how predictions translate into controller decisions or separation assurance logic.
- What evidence would resolve it: Simulation or field study integrating FTT predictions with an arrival manager (AMAN) system, measuring separation compliance and controller workload.

### Open Question 4
- Question: How does the model perform on edge cases such as aircraft in holding patterns, executing go-arounds, or undergoing diversion?
- Basis: The methodology excludes aircraft outside the 10–300NM range and does not discuss handling of abnormal flight trajectories or operational disruptions.
- Why unresolved: Real-time systems must gracefully handle non-nominal scenarios; model behavior under these conditions is uncharacterized.
- What evidence would resolve it: Analysis of prediction errors specifically on flights with holding events, go-arounds, or diversions, with comparison to nominal-flight baseline.

## Limitations
- Data Representativeness: Model trained and evaluated on only one month of ADS-B data from a single airport, limiting generalizability to other airports and time periods.
- Architectural Transparency: Lacks critical implementation details including exact hyperparameter settings, quantile normalization implementation, and attention pattern visualization.
- Scalability Validation: Inference time results based only on 40 aircraft batch size without exploring larger traffic volumes or different hardware configurations.

## Confidence
- High Confidence: Experimental methodology is rigorous with clear train/val/test splits, appropriate metrics, and statistically significant comparative results against XGBoost.
- Medium Confidence: Architectural innovations are sound but lack of attention visualization and hyperparameter specifications creates uncertainty about implementation-specific benefits.
- Low Confidence: Real-time performance claims based on narrow operational scenario without addressing failure modes, model robustness, or performance under different hardware constraints.

## Next Checks
1. **Attention Pattern Visualization**: Generate attention weight heatmaps for representative flights showing which feature combinations the model attends to across different flight phases.

2. **Cross-Airport Generalization Study**: Train and evaluate the model on ADS-B data from at least two additional major airports to assess whether the feature tokenization approach generalizes across different airport geometries.

3. **Extreme Weather Robustness Test**: Create synthetic test cases by injecting extreme weather conditions into the evaluation dataset and measure the model's performance degradation compared to XGBoost.