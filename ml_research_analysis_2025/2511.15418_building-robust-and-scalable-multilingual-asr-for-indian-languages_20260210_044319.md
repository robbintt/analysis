---
ver: rpa2
title: Building Robust and Scalable Multilingual ASR for Indian Languages
arxiv_id: '2511.15418'
source_url: https://arxiv.org/abs/2511.15418
tags:
- multi-decoder
- languages
- speech
- language
- baseline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes the development of multilingual automatic
  speech recognition (ASR) systems for 8 Indian languages across 33 dialects as part
  of the ASRU MADASR 2.0 challenge. The authors focused on leveraging phonemic similarities
  across Indian languages using a Common Label Set (CLS) as an intermediate representation,
  addressing the challenge of converting back to native graphemic scripts while retaining
  ASR performance gains.
---

# Building Robust and Scalable Multilingual ASR for Indian Languages

## Quick Facts
- arXiv ID: 2511.15418
- Source URL: https://arxiv.org/abs/2511.15418
- Reference count: 8
- Primary result: Multi-decoder ASR with phonemic CLS intermediate representation achieved highest language and dialect ID accuracy in MADASR 2.0 challenge, beating baseline in 3 languages for WER/CER metrics.

## Executive Summary
This paper presents a multi-decoder architecture for multilingual automatic speech recognition (ASR) across 8 Indian languages and 33 dialects as part of the MADASR 2.0 challenge. The key innovation is using a Common Label Set (CLS) as an intermediate phonemic representation between ASR and machine transliteration (MT) sub-networks, reducing output vocabulary confusion and exploiting cross-linguistic phonemic similarities. The system employs ASR initialization to balance convergence between sub-networks, achieving top performance in language and dialect identification while showing WER/CER improvements in 3 out of 8 languages compared to the baseline.

## Method Summary
The approach uses a multi-decoder architecture with two sub-networks: an ASR component that generates phonemic CLS transcriptions from audio, and an MT component that converts CLS representations to native scripts. Both sub-networks are trained jointly with the MT decoder incorporating cross-attention over speech encoder outputs to correct ASR errors during transliteration. Character-level tokenization was found superior to Byte Pair Encoding for this multi-dialect dataset. ASR initialization from a pre-trained cascaded model was critical to prevent MT overfitting during joint training. The system uses hybrid CTC-Attention loss for the ASR sub-network and cross-entropy loss for the MT sub-network.

## Key Results
- Multi-decoder with ASR initialization achieved 75.36% dialect ID accuracy for read speech and 33.33% for spontaneous speech in Track 2
- System beat baseline in 3 languages for WER/CER metrics
- Character-level tokenization outperformed BPE tokenization, particularly effective given the dataset's multi-dialect nature
- CLS ASR model comfortably beats conformer baseline in CLS space due to fewer target outputs reducing decoder confusion

## Why This Works (Mechanism)

### Mechanism 1: Phonemic Common Label Set (CLS) Reduces Decoder Confusion
Converting graphemic outputs to a shared phonemic representation reduces output vocabulary size and exploits cross-linguistic phonemic similarities across Indian languages. A unified parser converts native scripts from 8 languages into a common set of phonetic labels, reducing prediction confusion by abstracting away script-specific orthographic variations while preserving acoustic-phonemic mapping.

### Mechanism 2: Multi-Decoder Architecture with Acoustic Cross-Attention Error Correction
Jointly training separate ASR and MT sub-networks with acoustic cross-attention reduces error propagation compared to cascaded pipelines. The ASR sub-network produces intermediate CLS hidden states that flow directly to the MT sub-network encoder, while the MT decoder has cross-attention over the original speech encoder outputs, allowing it to access acoustic information to correct ASR errors during transliteration.

### Mechanism 3: ASR Initialization for Synchronized Sub-Network Convergence
Pre-training the ASR sub-network before joint training prevents the lighter MT sub-network from overfitting while ASR is still learning. The MT sub-network (2 Transformer layers) converges faster than the ASR sub-network (8 Conformer + 6 Transformer blocks), so pre-initializing ASR weights from a trained cascaded ASR model aligns convergence trajectories, enabling productive joint optimization.

## Foundational Learning

- **Hybrid CTC-Attention Loss**: Both baseline and ASR sub-network use combined CTC (alignment-focused) and attention (sequence modeling) losses. This dual objective handles variable-length sequences while maintaining alignment discipline—critical for multilingual settings where phoneme durations vary.
  - *Quick check*: Why might CTC alone underperform compared to CTC-Attention for multilingual ASR with shared phonemic outputs?

- **Grapheme-to-Phoneme (G2P) and Inverse Mapping**: The entire CLS approach depends on converting native scripts to phonemes (G2P) and back (P2G). Schwa deletion and geminate correction are explicitly mentioned as reconstruction challenges that can erode ASR gains.
  - *Quick check*: Why might P2G (CLS back to native script) be harder than G2P for Indian languages sharing scripts like Devanagari?

- **Cross-Attention in Encoder-Decoder Architectures**: The MT decoder's cross-attention over speech encoder outputs is the key mechanism for error correction. This allows the decoder to "look back" at raw acoustic features during text generation.
  - *Quick check*: In the multi-decoder, what three sources of information can the MT decoder attend to during native script generation?

## Architecture Onboarding

- **Component map**:
```
Audio (16kHz) → Log-Mel Spectrogram (80-dim, 10ms hop, 25ms window)
      ↓
[ASR Encoder: 8 Conformer blocks, 256 dim, 4 heads, kernel 31]
      ↓
[ASR Decoder: 6 Transformer blocks, 256 dim] → CLS tokens (CTC + Attention loss)
      ↓ (hidden states passed directly)
[MT Encoder: 2 Transformer blocks, 256 dim] + Optional [Hierarchical Encoder: 6 layers]
      ↓
[MT Decoder: 2 Transformer blocks + cross-attention to ASR encoder] → Native script
```

- **Critical path**:
  1. Feature extraction: Log-Mel spectrograms must match challenge specs (80 filter banks, 10ms hop, 25ms window)
  2. ASR sub-network: Hybrid CTC-Attention loss computed against CLS targets
  3. Hidden state transfer: ASR decoder outputs → MT encoder without intermediate decoding
  4. MT sub-network: Cross-entropy loss against native script; cross-attention to ASR encoder outputs
  5. Training sequence: Pre-train cascaded ASR → initialize multi-decoder ASR weights → joint training

- **Design tradeoffs**:
  - Character vs BPE tokenization: Character-level outperformed BPE for this multi-dialect dataset; BPE overfits when same utterances appear across dialects with different tokenizations
  - Hierarchical encoder: Optional auxiliary CTC loss on native script improves transliteration but adds parameters and complexity
  - MT sub-network capacity: Intentionally lightweight (2 blocks) to match task difficulty; larger capacity increased overfitting

- **Failure signatures**:
  - MT overfitting: Validation loss for MT decreases then increases while ASR still improving → implement ASR initialization
  - CLS→native degradation: Final WER higher than CLS-space WER → MT errors overwhelming ASR gains; add hierarchical encoder with auxiliary native-script CTC
  - Dialect confusion: Low DID with high LID → dialect distinctions may require acoustic features not fully captured by shared CLS

- **First 3 experiments**:
  1. Replicate baseline (Conformer encoder + Transformer decoder, native script output) with hybrid CTC-Attention to establish reference WER/CER/LID metrics on your target languages.
  2. Train cascaded CLS ASR + MT separately; quantify error propagation by comparing CLS-space accuracy to final native-script accuracy (Table III shows 3.92% → 5.22% CER degradation in Track 1 dev).
  3. Build multi-decoder with from-scratch initialization; plot per-subnetwork validation losses to confirm MT overfitting pattern before implementing ASR initialization fix.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can error propagation be further reduced when converting from CLS (phonemic) representations back to native graphemic scripts, particularly given linguistic phenomena like schwa deletion and geminate correction?
- **Basis in paper**: Authors state: "Reconstructing text in the native script from CLS representations is inherently challenging due to linguistic phenomena like schwa deletion, geminate correction, and the intricacies of syllable segmentation."
- **Why unresolved**: The multi-decoder approach reduced cascaded errors compared to separate ASR-MT pipelines, but WER still degrades when converting from CLS to native script (Tables III-IV show cascaded MT output has higher CER than CLS ASR alone).
- **What evidence would resolve it**: Ablation studies comparing different CLS-to-grapheme conversion strategies (e.g., neural vs rule-based, incorporating explicit linguistic features for schwa deletion), with error analysis categorizing failure modes.

### Open Question 2
- **Question**: What architectural or training modifications beyond ASR initialization can prevent the MT sub-network from overfitting in jointly-trained multi-decoder models?
- **Basis in paper**: Authors note: "the MT sub-network tends to overfit early during training" because it converges faster than the ASR sub-network.
- **Why unresolved**: ASR initialization helped balance convergence, but the paper does not explore alternatives like curriculum learning, gradient scheduling, or separate learning rates for each sub-network.
- **What evidence would resolve it**: Comparisons of different convergence-balancing strategies with training curves showing loss trajectories for both sub-networks, plus final WER/CER metrics.

### Open Question 3
- **Question**: Why does character-level tokenization outperform BPE in multi-dialect settings where the same utterance appears in multiple dialects, and does this generalize to other multilingual ASR scenarios?
- **Basis in paper**: Authors state character models outperformed BPE "due to the nature of the dataset, where the same utterance is spoken in multiple dialects, potentially causing the BPE-based models to overfit."
- **Why unresolved**: The explanation is brief; the mechanism by which shared utterances across dialects cause BPE overfitting is not analyzed in depth.
- **What evidence would resolve it**: Controlled experiments varying dialect overlap proportions, vocabulary analysis showing BPE token distributions across dialects, and testing on other multilingual corpora with similar properties.

## Limitations

- WER/CER improvements over baseline were limited to only 3 out of 8 languages, suggesting the approach may not generalize equally across all target languages.
- Significant degradation occurs when converting from CLS to native script (3.92% to 5.22% CER in Track 1 dev), indicating the MT sub-network introduces substantial errors that partially negate ASR gains.
- The dataset's multi-dialect nature creates potential overfitting risks, particularly with BPE tokenization, though this was partially mitigated by switching to character-level tokenization.

## Confidence

- **High Confidence**: The claim that multi-decoder architecture with ASR initialization prevents MT sub-network overfitting is well-supported by the training dynamics described and validated through reported results.
- **Medium Confidence**: The claim that CLS representation reduces decoder confusion by exploiting phonemic similarities across Indian languages is plausible given shared linguistic features but lacks direct corpus validation.
- **Low Confidence**: The claim that cross-attention in the MT decoder effectively corrects ASR errors is weakly supported, with no empirical analysis showing error correction rates.

## Next Checks

1. **Ablation Study on Component Contributions**: Re-run experiments removing either the CLS intermediate representation or the cross-attention mechanism to isolate which component drives performance gains and validate whether the multi-decoder architecture itself or the specific CLS-phonemic approach is responsible for improvements.

2. **Error Propagation Analysis**: Quantify the types and frequencies of errors introduced during CLS-to-native script conversion. Measure whether acoustic cross-attention actually corrects systematic phonemic confusions versus random errors, and determine if certain dialect pairs or language families show higher reconstruction error rates.

3. **Generalization Testing**: Test the trained multi-decoder model on held-out dialects or speakers not seen during training to evaluate whether the system genuinely learns cross-dialect generalization or simply memorizes training patterns. Compare performance against cascaded approaches on these novel data points to validate robustness claims.