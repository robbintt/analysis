---
ver: rpa2
title: 'PROMISE: Prompt-Attentive Hierarchical Contrastive Learning for Robust Cross-Modal
  Representation with Missing Modalities'
arxiv_id: '2511.10997'
source_url: https://arxiv.org/abs/2511.10997
tags:
- missing
- learning
- prompt
- multimodal
- modality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of robust multimodal representation
  learning when certain modalities are missing or unavailable. Existing approaches
  often fail to preserve cross-modal consistency, leading to suboptimal performance
  in real-world scenarios with incomplete data.
---

# PROMISE: Prompt-Attentive Hierarchical Contrastive Learning for Robust Cross-Modal Representation with Missing Modalities

## Quick Facts
- arXiv ID: 2511.10997
- Source URL: https://arxiv.org/abs/2511.10997
- Reference count: 17
- Primary result: Achieves up to 67.17% AUROC on Hateful Memes and 88.10% accuracy on UPMC Food101 with 70% missing modality rates

## Executive Summary
PROMISE addresses the challenge of robust multimodal representation learning when certain modalities are missing or unavailable. The framework integrates multimodal prompt learning with hierarchical contrastive learning, featuring a Prompt Attention mechanism that dynamically generates consistent representations for missing modalities. By employing dual-level contrastive learning (FNCL for cross-modal alignment and CCCL for discriminative power), PROMISE effectively bridges the representational gap between complete and incomplete data, outperforming state-of-the-art methods across multiple benchmark datasets.

## Method Summary
The PROMISE framework uses a frozen CLIP-ViT-Large-Patch14 encoder as backbone and learns modality-specific prompt pools (4 prompts per modality, length 16) to generate representations for missing modalities via Prompt Attention with multi-head attention. The method employs dual-level contrastive learning: Fusion-driven Nexus Contrastive Learning (FNCL) for cross-modal semantic consistency and Cohesion-driven Core Contrastive Learning (CCCL) for discriminative power within each modality. The complete training objective combines these with a classification head, using Adam optimizer (batch size 64, lr=1e-4, τ=0.07) under simulated 70% missing rate conditions.

## Key Results
- Achieves 67.17% AUROC on Hateful Memes with 70% missing modality rates
- Achieves 88.10% accuracy on UPMC Food101 under 70% missing modality conditions
- Maintains strong performance even under extreme missing data conditions (up to 90% missing rate)

## Why This Works (Mechanism)

### Mechanism 1
The Prompt Attention mechanism generates semantically consistent representations for missing modalities by conditioning on available modalities via learnable prompts. A modality-specific prompt pool (N prompts per modality) is concatenated with projected available-modality features, then processed through multi-head attention. The attention outputs are fused via separate linear layers to produce both original and augmented representations for the missing modality. This cross-modal conditioning leverages the semantic correlation between modalities (e.g., image and text describing the same instance).

Core assumption: Available modalities contain sufficient semantic signal to infer missing-modality representations, and learnable prompts can capture modality-specific characteristics that generalize across instances.

Evidence anchors:
- [abstract] "This mechanism dynamically generates robust and consistent representations for scenarios where particular modalities are absent, thereby effectively bridging the representational gap between complete and incomplete data."
- [section] "Prompt Attention mechanism, which enables precise cross-modal representation generation by leveraging modality-specific prompt pools in conjunction with multi-head attention." (Methodology, page 3)

Break condition: If available modalities are semantically uninformative or decorrelated from the missing modality (e.g., random image-text pairs), generated representations will not align with true missing features.

### Mechanism 2
Fusion-driven Nexus Contrastive Learning (FNCL) enforces cross-modal semantic consistency by pulling together all original and augmented views of the same instance across modalities. FNCL applies symmetric NT-Xent loss across four alignment objectives: (xm1, x̂m2), (xm1, x̂m2_a), (xm1_a, x̂m2), (xm1_a, x̂m2_a). This creates a unified representation space where cross-modal embeddings from the same instance are close regardless of which views are available or generated.

Core assumption: Instances have meaningful cross-modal correspondence (paired or semantically related data), and the NT-Xent formulation with temperature scaling appropriately balances positive/negative pair contributions.

Evidence anchors:
- [abstract] "The method employs a dual-level contrastive learning strategy (FNCL for cross-modal alignment..."
- [section] "FNCL aims to create a unified representation space, bridging the heterogeneity between modalities. It posits that cross-modal embeddings from the same instance should be semantically consistent, while those from different instances should be distinguishable." (Hierarchical Contrastive Learning, page 4)

Break condition: If batch size is too small (insufficient negatives), or temperature τ is poorly tuned, contrastive objectives may collapse or provide weak gradients.

### Mechanism 3
Cohesion-driven Core Contrastive Learning (CCCL) enhances discriminative power by encouraging same-class instances to cluster within each modality. For each modality, CCCL defines positive pairs as augmented views and same-label samples within a batch. The loss (Eq. 9) pulls positive pairs together while pushing apart all other samples. This complements FNCL's cross-modal focus with intra-modal class-aware discrimination.

Core assumption: Label information is reliable and semantically meaningful, and class-aware contrastive learning within modalities improves downstream task performance even when modalities are missing.

Evidence anchors:
- [abstract] "...CCCL for discriminative power) to effectively bridge the representational gap between complete and incomplete data."
- [section] "While FNCL focuses on inter-modal alignment, CCCL enhances discriminative power within each modality by promoting feature compactness for same-class instances and separation for different-class instances." (Hierarchical Contrastive Learning, page 4)

Break condition: If labels are noisy or class imbalance is severe within batches, CCCL may reinforce incorrect clustering or provide biased gradients.

## Foundational Learning

- Concept: Multi-head Self-Attention (Vaswani et al., 2017)
  - Why needed here: The Prompt Attention mechanism builds directly on multi-head attention to generate missing-modality representations. Understanding Q/K/V projections, scaled dot-product attention, and multi-head concatenation is essential.
  - Quick check question: Given input matrix A, can you write the formula for attention output using Q, K, V projections and explain why scaling by √d_k matters?

- Concept: Contrastive Learning (SimCLR, NT-Xent loss)
  - Why needed here: Both FNCL and CCCL use normalized temperature-scaled cross-entropy loss. Understanding positive/negative pair construction, temperature's effect on gradient hardness, and symmetric loss formulation is critical.
  - Quick check question: In a batch of B samples, how many positive and negative pairs does each anchor have in standard SimCLR, and how does temperature τ affect the gradient magnitude for hard negatives?

- Concept: Prompt Learning (Soft Prompts, Prefix Tuning)
  - Why needed here: PROMISE uses learnable prompt pools modality-specifically. Understanding how soft prompts modulate internal representations without architectural changes clarifies why this approach generalizes across missing patterns.
  - Quick check question: What is the difference between hard prompts and soft prompts, and why might soft prompts be preferable for handling varied missing-modality conditions?

## Architecture Onboarding

- Component map: Frozen CLIP encoder -> Modality-specific prompt pools -> Prompt Attention module -> Dual-level contrastive learning (FNCL + CCCL) -> Classification head

- Critical path: Input → augmentation → frozen encoder embeddings → Prompt Attention (if modality missing) → complete/partial multimodal representations → FNCL + CCCL losses → classification head → cross-entropy + contrastive loss

- Design tradeoffs:
  - Frozen vs. fine-tuned encoder: Freezing CLIP preserves pretrained alignment but limits domain adaptation. The paper freezes for parameter efficiency and stability.
  - Prompt dimension vs. layers: Ablation shows 6 layers with dimension 16 achieves best AUROC (67.17%), but 3 layers with dimension 16 is near-optimal (66.46%) with lower compute.
  - α weighting in Lcontrast: Balances FNCL (inter-modal consistency) vs. CCCL (intra-modal discrimination). Extreme missing rates may require adjusting α.

- Failure signatures:
  - Representation collapse: If generated missing-modality features become identical across instances, check CCCL positive set construction and temperature.
  - Poor cross-modal alignment: If FNCL loss plateaus high, verify augmentation quality and batch diversity.
  - Overfitting to available modality: If performance drops severely when the typically-available modality is missing, the prompt pool may have learned modality-specific shortcuts rather than true cross-modal semantics.

- First 3 experiments:
  1. Reproduce the 70% missing-rate baseline on Hateful Memes (Table 1) with balanced-missing scenario, confirming AUROC ≈ 67.17%.
  2. Ablate FNCL and CCCL separately (Table 2) to verify their individual and combined contributions on N24News or MM-IMDb.
  3. Sweep prompt dimension (16, 32, 48) and layer count (3, 6) on a validation split to identify the compute-performance frontier before full training.

## Open Questions the Paper Calls Out

### Open Question 1
Can the framework be generalized to tasks involving more than two modalities without combinatorial complexity? The methodology and contrastive loss formulations are strictly defined for two modalities, leaving high-modality interactions unexplored. The pairwise nature of FNCL may face scalability issues when aligning N distinct modalities.

### Open Question 2
How robust is the model when missing data is correlated with semantics (Missing Not At Random) rather than randomly distributed? The implementation simulates missing modalities by randomly discarding data, assuming a Missing Completely At Random (MCAR) distribution. Real-world data loss is often systematic (e.g., sensor failure in specific conditions); the model's reliance on random augmentation may fail to capture biased missing patterns.

### Open Question 3
How can prompt structures be dynamically optimized to adapt to varying missing rates? The conclusion explicitly suggests "optimizing prompt structures for different missing patterns" as a direction for future research. The current study relies on fixed prompt pool sizes and dimensions, which may be suboptimal as the missing rate fluctuates between 10% and 100%.

## Limitations
- Key hyperparameters like α (weighting between FNCL and CCCL losses) are unspecified, preventing exact reproduction
- Architectural details of the "Crossmodal Transformer" and "Fusion Layer" components are underspecified
- Claims about extreme robustness (maintaining performance under 90% missing rate) are supported by only a single table entry without detailed analysis

## Confidence
- **High Confidence (8/10)**: Core conceptual contribution and dual-level contrastive learning strategy are well-supported and mechanistically sound
- **Medium Confidence (6/10)**: Experimental results are impressive but difficult to verify without exact implementation and hyperparameter values
- **Low Confidence (4/10)**: Claims about extreme robustness lack statistical significance testing and detailed failure mode analysis

## Next Checks
1. Implement and verify the Prompt Attention mechanism with modality-specific prompt pools using frozen CLIP embeddings, measuring cosine similarity between generated and ground-truth embeddings on complete data
2. Determine the optimal α weighting between FNCL and CCCL through systematic hyperparameter sweep on validation data
3. Stress-test extreme missing modality conditions (80-90% missing rate) analyzing embedding variance collapse, cross-modal alignment quality, and class separation within modalities