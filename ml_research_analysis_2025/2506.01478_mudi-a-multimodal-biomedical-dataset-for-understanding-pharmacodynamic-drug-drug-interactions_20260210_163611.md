---
ver: rpa2
title: 'MUDI: A Multimodal Biomedical Dataset for Understanding Pharmacodynamic Drug-Drug
  Interactions'
arxiv_id: '2506.01478'
source_url: https://arxiv.org/abs/2506.01478
tags:
- drug
- mudi
- interaction
- dataset
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MUDI, a large-scale multimodal biomedical
  dataset for understanding pharmacodynamic drug-drug interactions. MUDI integrates
  pharmacological text, chemical formulas, molecular structure graphs, and images
  across 310,532 drug pairs labeled as Synergism, Antagonism, or New Effect.
---

# MUDI: A Multimodal Biomedical Dataset for Understanding Pharmacodynamic Drug-Drug Interactions

## Quick Facts
- arXiv ID: 2506.01478
- Source URL: https://arxiv.org/abs/2506.01478
- Reference count: 40
- Primary result: MUDI achieves 66.69% micro-F1 on pharmacodynamic DDI classification using intermediate fusion

## Executive Summary
This paper introduces MUDI, a large-scale multimodal biomedical dataset for understanding pharmacodynamic drug-drug interactions. MUDI integrates pharmacological text, chemical formulas, molecular structure graphs, and images across 310,532 drug pairs labeled as Synergism, Antagonism, or New Effect. The dataset includes unseen drug pairs in the test set to assess model generalization. Baseline experiments using intermediate and late fusion strategies achieve micro-F1 scores of 66.69% and 64.67% respectively under direction-agnostic evaluation, with intermediate fusion consistently outperforming late fusion. The results highlight the importance of multimodal integration and demonstrate MUDI's utility as a benchmark for pharmacodynamic DDI prediction research.

## Method Summary
MUDI addresses multi-class classification of pharmacodynamic drug-drug interactions (DDI) into Synergism, Antagonism, or New Effect using six modalities per drug: name, description text, SMILES, chemical formula, molecular graph, and molecular image. The dataset contains 310,532 drug pairs (221,115 train / 89,417 test) with 1,295 unique drugs, where test set includes deliberately unseen drug pairs. Models employ single-modality encoders (BioMedBERT for text/name/SMILES/formula, 2-layer GCN for graphs, ViT-B/16 for images) followed by fusion strategies. Late fusion uses majority voting while intermediate fusion concatenates all embeddings (4608-dim total) through a 2-layer MLP. Training uses Adam optimizer with lr=5e-5, batch size=32, linear warmup, weight decay=1e-2, dropout=0.1, 100 epochs, gradient clipping, and early stopping.

## Key Results
- Intermediate fusion achieves 66.69% micro-F1 (direction-agnostic) and 52.74% (direction-aware)
- Single-modality best performer is graph (65.44% micro-F1 direction-agnostic), followed by name (57.40%)
- Performance gap between direction-aware (52.74%) and direction-agnostic (66.69%) evaluation highlights importance of ordered pair handling

## Why This Works (Mechanism)
The multimodal approach captures complementary pharmacological information: text provides mechanism descriptions, molecular structures reveal chemical interactions, and graphs encode atomic connectivity. Intermediate fusion outperforms late fusion by enabling cross-modal feature interaction before classification, allowing the model to learn modality-specific contribution weights for each DDI type. The deliberate inclusion of unseen drug pairs in testing ensures models generalize beyond memorizing specific drug combinations.

## Foundational Learning

**BioMedBERT fine-tuning** - Why needed: Medical text contains specialized terminology requiring domain-specific pretraining. Quick check: Verify tokenization handles chemical names and medical terms correctly.

**Graph Convolutional Networks for molecular structures** - Why needed: Molecular graphs capture atomic relationships and chemical properties essential for interaction prediction. Quick check: Confirm atom feature encoding (37 one-hot features) and graph loading from GraphML files.

**Vision Transformers for molecular images** - Why needed: 2D molecular representations contain spatial patterns in chemical structures. Quick check: Verify image preprocessing (1000×800 PNG) and patch embedding dimensions.

**Multimodal fusion strategies** - Why needed: Different modalities contribute varying predictive power for each DDI type. Quick check: Confirm embedding dimensions match (768 per modality × 6 × 2 drugs = 4608 total).

**Direction-aware evaluation** - Why needed: Pharmacodynamic interactions can be asymmetric (A+B ≠ B+A). Quick check: Verify ordered pair handling in evaluation metrics.

## Architecture Onboarding

**Component map:** BioMedBERT -> text/name/SMILES/formula encoders; GCN -> graph encoder; ViT -> image encoder; MLP -> fusion classifier

**Critical path:** Drug pair → modality encoders → modality embeddings → fusion layer → classification

**Design tradeoffs:** Intermediate fusion (66.69% micro-F1) vs late fusion (64.67%) - intermediate allows cross-modal learning but increases parameter count; direction-agnostic evaluation simplifies task but may mask important asymmetries.

**Failure signatures:** Class imbalance (82.85% Synergism) causes poor minority class recall; large direction-aware/agnostic gap (13.95 points) indicates models failing to capture asymmetry; graph modality underperformance suggests GCN implementation issues.

**Three first experiments:** 1) Train single BioMedBERT on text modality only, verify ~55% micro-F1; 2) Train GCN on graph modality only, verify ~65% micro-F1; 3) Implement intermediate fusion with 2 modalities, verify performance improvement over individual modalities.

## Open Questions the Paper Calls Out

None

## Limitations
- Performance gap between direction-aware (52.74%) and direction-agnostic (66.69%) evaluation suggests models struggle with interaction directionality
- Minority class (New Effect, 4.52%) suffers from severe recall issues (17.97-35.59%) due to class imbalance
- Unspecified random seeds, validation strategies, and negative sampling methodology limit exact reproducibility

## Confidence

**High confidence:** Dataset construction methodology (310,532 drug pairs, 1,295 unique drugs, six modalities per drug) and basic evaluation framework (micro-F1/Macro-F1, direction-aware/agnostic matching) are well-specified and verifiable.

**Medium confidence:** Fusion architecture descriptions and hyperparameter settings are sufficiently detailed for implementation, though exact implementation nuances may affect performance.

**Low confidence:** Absolute performance numbers (66.69% micro-F1 for intermediate fusion) are challenging to reproduce exactly due to unreported implementation details like random seeds and validation strategies.

## Next Checks

1. Verify per-class performance matches reported values: Synergism (P/R/F1 ≈ 0.68/0.80/0.73), Antagonism (0.58/0.62/0.60), New Effect (0.53/0.18/0.27) under direction-agnostic evaluation.

2. Test direction-aware vs direction-agnostic performance gap matches reported 13.95 percentage points (66.69% vs 52.74% micro-F1).

3. Confirm single-modality performance hierarchy: graph (65.44%), name (57.40%), image (57.23%), SMILES (56.83%), formula (55.33%), description (55.16%) micro-F1 under direction-agnostic matching.