---
ver: rpa2
title: Compression Scaling Laws:Unifying Sparsity and Quantization
arxiv_id: '2502.16440'
source_url: https://arxiv.org/abs/2502.16440
tags:
- scaling
- quantization
- sparsity
- compression
- bits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how compression techniques like weight/activation
  quantization and weight sparsity affect the scaling behavior of large language models
  during pretraining. The key insight is that different compression methods can be
  unified under a common scaling law framework based on an "effective parameter count"
  metric.
---

# Compression Scaling Laws:Unifying Sparsity and Quantization

## Quick Facts
- arXiv ID: 2502.16440
- Source URL: https://arxiv.org/abs/2502.16440
- Reference count: 4
- Primary result: Compression techniques (sparsity, quantization) can be unified under a single scaling law framework via "effective parameter count"

## Executive Summary
This work investigates how compression techniques like weight/activation quantization and weight sparsity affect the scaling behavior of large language models during pretraining. The key insight is that different compression methods can be unified under a common scaling law framework based on an "effective parameter count" metric. Experimental results show that weight-only quantization maintains strong parameter efficiency even at very low bitwidths, with 4-bit achieving 92% efficiency compared to full precision. When quantizing both weights and activations, diminishing returns appear below 4 bits. Weight sparsity shows effective parameter multipliers of 0.87 (50% sparsity) and 0.62 (75% sparsity). These findings enable principled comparison and combination of compression methods for efficient language model training.

## Method Summary
The study analyzes pretraining of Llama-type autoregressive models on C4 dataset using Chinchilla-optimal token-to-parameter ratios. Compression is applied through STE-based quantization with dynamic scales (not learned) and top-k weight sparsification per row before each forward pass. Scaling laws are fitted by sweeping model sizes at fixed data ratios, computing effective parameter multipliers (EPM) that map compressed model performance to equivalent dense models. The same hyperparameters are used for both dense and compressed training to ensure fair comparison.

## Key Results
- Weight-only quantization maintains near-full parameter efficiency down to 4-bit (EPM=0.923), with diminishing returns below 4 bits when activations are also quantized
- Weight sparsity shows effective parameter multipliers of 0.87 (50% sparsity) and 0.62 (75% sparsity)
- At moderate compression (2x), sparsity and quantization achieve similar EPMs, but quantization wins at higher compression ratios
- The unified scaling framework validates that eff(C) remains approximately constant across model scales and data regimes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compression techniques can be unified under a single scaling law framework via an "effective parameter count" multiplier
- Mechanism: The compression scaling law L(N, D, C) = a/(N·eff(C))^b + c/D^d + e preserves the functional form of standard scaling laws by treating compression as a constant reduction in effective model size
- Core assumption: eff(C) is independent of model parameter count N and dataset size D, depending only on compression type and architecture
- Evidence anchors: Unified scaling law equation defined in Section 2.1; empirical validation via 2x Chinchilla experiments; corroborated by independent studies
- Break condition: If eff(C) varies significantly with data scale or model size, the unified framework degrades

### Mechanism 2
- Claim: Weight-only quantization preserves near-full parameter efficiency down to 4-bit, but full weight-and-activation quantization shows diminishing returns below 4 bits
- Mechanism: Weights carry learned information that can be compressed with limited loss; activations represent dynamic computation that, when quantized aggressively, introduces information bottlenecks at each layer
- Core assumption: STE with dynamic scaling provides a fair baseline for quantization-aware training comparisons
- Evidence anchors: 4-bit weight-only EPM=0.923 vs 4-bit full quantization EPM=0.747; Table 2 and Table 3 results; linear scaling of loss with activation bitwidth
- Break condition: If different quantization schemes yield substantially different EPMs, specific numerical claims may not transfer

### Mechanism 3
- Claim: Sparsity and quantization can be compared on equivalent FLOP grounds; 50% sparsity (EPM=0.87) approximates 8-bit full quantization (EPM=0.86)
- Mechanism: Both techniques reduce compute but affect effective parameter count differently. At moderate compression (2x), both achieve similar EPMs; at aggressive compression (4x+), quantization's EPM decays more gracefully
- Core assumption: Linear FLOP counting represents practical hardware behavior; quadratic counting represents theoretical multiplication cost
- Evidence anchors: Table 5 sparsity EPM values; comparison of 50% sparsity (0.871) vs 8W8A quantization (0.857); Section 3 analysis of compression ratio tradeoffs
- Break condition: If hardware efficiently supports sparse operations at studied sparsity levels, practical speedups may differ from theoretical FLOP reductions

## Foundational Learning

- Concept: Chinchilla scaling laws (compute-optimal training ratios)
  - Why needed here: The compression scaling law builds directly on Chinchilla's parameter-to-data ratios; paper uses "1x Chinchilla" and "2x Chinchilla" as standard training durations
  - Quick check question: Given a 7B parameter model trained on 140B tokens, is this under-trained, Chinchilla-optimal, or over-trained relative to the standard ratio?

- Concept: Straight-Through Estimation (STE) for quantization-aware training
  - Why needed here: Paper uses vanilla STE as its quantization method. Without understanding how gradients flow through discretization, experimental setup cannot be reproduced
  - Quick check question: In STE, what gradient is used during backpropagation through a quantization function Q(w), and why does this approximation work?

- Concept: Scaling law curve fitting (power law parameter estimation)
  - Why needed here: Paper fits a, b, c, d, e constants and eff(C) to loss data. Understanding how to validate scaling law fits is essential for applying this framework
  - Quick check question: When fitting a power law L = a/N^b + c/D^d + e, how would you detect whether a single power law is insufficient?

## Architecture Onboarding

- Component map: Model training -> Compression application (quantization/sparsity) -> Forward pass with quantized/sparse weights -> Loss computation -> Backward pass with STE -> Parameter update

- Critical path:
  1. Establish dense baseline scaling (BF16, multiple model sizes)
  2. Add compression with identical hyperparameters
  3. Fit eff(C) by matching compressed model loss to equivalent dense model size
  4. Validate data-independence via 2x Chinchilla experiments

- Design tradeoffs:
  - Per-tensor vs. per-row sparsity: Per-tensor is slightly more accurate; per-row is 2x+ faster
  - Weight-only vs. full quantization: Weight-only enables aggressive bitwidth reduction; full quantization caps at ~4-bit practical efficiency
  - Linear vs. quadratic FLOP counting: Linear reflects current hardware; quadratic reflects theoretical limits

- Failure signatures:
  - Loss divergence at low bitwidths with aggressive learning rates
  - eff(C) varying with model size or data scale
  - 1-bit full quantization failing to train stably

- First 3 experiments:
  1. Replicate weight-only quantization scaling at 4-bit and 2-bit on smaller model family (100M-1B parameters) to validate EPM transfer
  2. Run hyperparameter sweep comparing dense vs. compressed training LR sensitivity to confirm transfer claim
  3. Fit scaling curves for combined compression method (4-bit quantization + 50% sparsity) to test whether eff(C₁, C₂) = eff(C₁) × eff(C₂)

## Open Questions the Paper Calls Out

- How do compression scaling laws transfer to non-autoregressive architectures or architectures other than decoder-only Llama models tested?
- What are the scaling behaviors and "effective parameter" interactions when combining sparsity and quantization together?
- How does the choice of quantization scheme (e.g., floating point vs. integer) alter the effective parameter multiplier?

## Limitations

- The unified framework assumes eff(C) independence across model scales and data regimes, validated only through limited 2x Chinchilla experiments
- STE quantization may not represent optimal compression efficiency—learned scale methods could yield different EPM values
- Analysis focuses on pretraining dynamics without addressing downstream task transfer or inference efficiency

## Confidence

**High confidence**: The fundamental claim that compression techniques can be unified under scaling laws via effective parameter count is well-supported by mathematical framework and experimental validation. Relative comparisons between compression methods are robust.

**Medium confidence**: Specific EPM values are method-dependent and may vary with implementation details like quantization scheme, scale computation, or training hyperparameters. Extrapolation to extreme compression levels carries higher uncertainty.

**Low confidence**: Claims about practical deployment implications (inference efficiency, hardware considerations) are not directly tested and may differ from theoretical FLOP reductions.

## Next Checks

1. Run compressed models at 0.5x and 4x Chinchilla token counts to test whether eff(C) truly remains constant across data regimes beyond the 2x validation

2. Implement learned-scale quantization (LSQ or PACT) alongside STE baseline to measure how EPM values change and reveal whether reported efficiencies are method-specific

3. Train models with hybrid compression (4-bit quantization + 50% sparsity) to measure whether eff(C₁, C₂) = eff(C₁) × eff(C₂) holds or whether interaction terms emerge