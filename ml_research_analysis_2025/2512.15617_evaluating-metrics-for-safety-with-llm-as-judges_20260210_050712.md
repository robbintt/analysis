---
ver: rpa2
title: Evaluating Metrics for Safety with LLM-as-Judges
arxiv_id: '2512.15617'
source_url: https://arxiv.org/abs/2512.15617
tags:
- risk
- patient
- safety
- critical
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the challenges of ensuring safety in LLM-as-Judge
  (LaJ) evaluations for safety-critical contexts. It presents a case study of an agentic
  framework for peri-operative risk assessment, where LaJ agents evaluate concordance
  between an anaesthetist agent's risk brief and specialist agent briefs.
---

# Evaluating Metrics for Safety with LLM-as-Judges

## Quick Facts
- **arXiv ID**: 2512.15617
- **Source URL**: https://arxiv.org/abs/2512.15617
- **Reference count**: 22
- **Key outcome**: Current LLM-as-Judge metrics lack transparency and deterministic calculation, making them unsuitable for safety-critical evaluations without significant improvements in metric design and calculation methods.

## Executive Summary
This paper examines safety challenges in LLM-as-Judge evaluations for safety-critical contexts, using a peri-operative risk assessment framework as a case study. The authors identify critical issues with LaJ metrics including lack of transparency, over-reliance on vague qualitative judgments, insufficient handling of synonyms and abbreviations, and inappropriate weighting of metrics. They argue that safety assurance requires more deterministic calculations, explicit quantification, traceable evidence, and metrics aligned with patient risk rather than mere agreement rates. The paper concludes that current LaJ approaches cannot be fully assured for safety-critical applications without significant improvements in metric transparency and calculation methods.

## Method Summary
The framework evaluates concordance between an anaesthetist agent's risk brief and specialist agent briefs using five weighted dimensions: coverage (30%), critical items (30%), correctness (20%), prioritisation (10%), and actionability (10%). Quality gates act as hard caps - if a critical safety item is missed, the overall score is capped and flagged for human review. Evidence packs contain verbatim extracts from patient records with source locators. The adjudication process combines deterministic calculations (Jaccard index for coverage) with LLM-based qualitative assessments, though the latter lack explicit quantification.

## Key Results
- Mutual omission problem: High concordance scores can mask critical omissions if both anaesthetist and specialist agents miss the same safety-critical item
- Soft metrics gap: Correctness and specificity dimensions lack quantifiable definitions, weakening safety arguments
- Synonym handling: Abbreviations and variant terms are not reliably recognized, leading to undercounting of matches
- Determinism requirement: Safety assurance depends on deterministic calculations and evidence-based arguments rather than LLM-generated explanations

## Why This Works (Mechanism)

### Mechanism 1: Weighted Basket of Metrics with Hard Caps
- Claim: Aggregating multiple weighted evaluation dimensions with hard caps on safety-critical misses reduces risk that superficial agreement obscures dangerous omissions.
- Mechanism: Five sub-scores are computed independently with hard caps (≤0.40 for one major miss, ≤0.20 for multiple), which propagate to cap the overall score and flag for human review.
- Core assumption: Independent computation of dimensions prevents compensatory masking where high scores on "soft" metrics offset failures on safety-critical ones.
- Evidence anchors: [abstract] "combines coverage, critical item detection, correctness, prioritisation, and actionability with hard caps on scores if safety-critical criteria are missed"
- Break condition: If both anaesthetist and specialist agents miss the same critical item, the gate will not trigger, and the error remains undetected.

### Mechanism 2: Deterministic Calculation Offloading
- Claim: Offloading metric calculations to external tools increases traceability and auditability of safety claims.
- Mechanism: Explicit formulas (e.g., Jaccard index for coverage = matched items / total items) are executed by deterministic calculators rather than LLM inference.
- Core assumption: LLMs can reliably invoke external tools and pass correct inputs; the tool's output is accepted without further LLM interpretation that could introduce error.
- Evidence anchors: [abstract] "assurance depends on deterministic calculations, rigorous metric design, and evidence-based arguments rather than relying solely on LLM-generated explanations"
- Break condition: If the LLM incorrectly identifies items to match (e.g., fails to recognise synonyms), the deterministic calculator will compute correctly on wrong inputs.

### Mechanism 3: Verbatim Evidence Traceability
- Claim: Requiring agents to extract verbatim text with source locators creates an auditable chain from risk claims back to source documents.
- Mechanism: Each agent outputs an evidence pack (JSONL format) containing source_id, locator, extract_text (verbatim), and tags. The adjudicator checks that brief bullets are supported by at least one evidence pack line.
- Core assumption: Verbatim extraction reduces interpretation drift compared to paraphrased summaries.
- Evidence anchors: [section 2.2] "every claim in the briefs maps back to verbatim extracted JSONL lines from the patient record (source id + locator + extract text)"
- Break condition: Complex or ambiguous source text makes verbatim matching difficult; human review may still require significant time to confirm correctness.

## Foundational Learning

- **Concept: Context sensitivity of error severity**
  - Why needed here: The paper emphasises that error severity depends on where an error occurs, not just error type. Missing a bleomycin history is catastrophic; missing a routine notation may be trivial.
  - Quick check question: Can you explain why two errors of the same type (e.g., omission) could have vastly different safety implications in the same document?

- **Concept: Soft vs hard metrics in safety-critical evaluation**
  - Why needed here: The paper distinguishes metrics with explicit quantification (Jaccard index, gate checks) from "soft" metrics relying on LLM judgment ("factually supported"). Only the former provide assurance-grade evidence.
  - Quick check question: If an LLM judge penalises "vague statements" without defining the penalty quantitatively, what evidence gap does this create for a safety argument?

- **Concept: Quality gates as safety constraints**
  - Why needed here: Quality gates encode domain-specific rules (e.g., "K+ ≥6.0 → delay/correct first") that trigger hard caps, translating clinical policy into enforceable evaluation logic.
  - Quick check question: What happens to the safety argument if a quality gate is incompletely specified or a clinical scenario falls outside the gate definitions?

## Architecture Onboarding

- **Component map**:
  - Patient record + surgical context → Anaesthetist brief
  - Anaesthetist brief → Specialist briefs (parallel)
  - All briefs + evidence packs → Adjudicator comparison
  - Adjudicator scorecard → Dashboard display
  - If score ≤69 or major gate missed → Flag for human review

- **Critical path**:
  1. Patient record + surgical context → Anaesthetist brief
  2. Anaesthetist brief → Specialist briefs (parallel)
  3. All briefs + evidence packs → Adjudicator comparison
  4. Adjudicator scorecard → Dashboard display
  5. If score ≤69 or major gate missed → Flag for human review

- **Design tradeoffs**:
  - **Long agent profiles vs. consistency**: Extensive specialist profiles caused "context poisoning"; shortened profiles improved alignment but reduced domain specificity
  - **Fewer quality gates vs. coverage**: Minimal gate lists improved consistency but may miss edge-case risks; comprehensive gates increase false positives for human review
  - **Soft metrics inclusion vs. assurance**: Correctness/specificity metrics add nuance but lack quantifiable definitions, weakening safety arguments

- **Failure signatures**:
  - **Mutual omission**: Both anaesthetist and specialist miss the same critical item → high concordance score, undetected hazard
  - **Synonym mismatch**: Agent rewrites "arterial oxygen saturation" as "SpO₂" → coverage calculation undercounts matches
  - **Soft metric gaming**: High specificity scores on identified risks mask failure to identify risks at all
  - **Evidence-brief misalignment**: Complex source text makes verbatim extracts hard to match to brief claims during human review

- **First 3 experiments**:
  1. **Gate coverage test**: Run the framework on patient records with known critical conditions (e.g., bleomycin exposure) and verify whether any configuration of anaesthetist + specialist agents can produce mutual omission. Measure detection rate across gate configurations.
  2. **Deterministic vs LLM calculation comparison**: Implement coverage calculation both via explicit Jaccard code and via LLM prompt; compare results on 50 specialist/anaesthetist brief pairs to quantify divergence.
  3. **Soft metric ablation**: Run adjudication with and without correctness/specificity dimensions; assess whether overall safety (measured by human expert review of flagged cases) changes significantly.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can "soft" evaluation metrics (e.g., correctness, specificity) be grounded in deterministic calculations rather than opaque LLM token predictions to satisfy safety assurance requirements?
- Basis in paper: [explicit] The authors ask, "Does it pass it on to a deterministic calculator... rather than relying on next token prediction?" and note that parts of the scoring rubric remain "shrouded in mystery."
- Why unresolved: Current LLM-as-Judge implementations generate numerical scores for vague criteria via probabilistic next-token prediction, lacking the traceability required for safety cases.
- What evidence would resolve it: Implementation of externalized, code-based metric calculators that audibly trace the logic from input data to score without relying on the LLM's internal inference.

### Open Question 2
- Question: Can high concordance scores between agents effectively mask critical omissions (blind spots) in safety-critical assessments?
- Basis in paper: [explicit] The paper explicitly asks, "could a high concordance score still miss a critical factor in the patient record... leading to patient harm?"
- Why unresolved: If both the Anaesthetist agent and the Specialist agent (and the Judge) share a common failure mode or "blind spot," they may agree on a risk assessment that is objectively unsafe.
- What evidence would resolve it: Empirical testing using adversarial cases known to be difficult for LLMs to determine if agreement correlates with ground truth or shared hallucination.

### Open Question 3
- Question: What evidence is required to assure that LaJ systems correctly handle synonyms and domain-specific variants in unstructured text inputs?
- Basis in paper: [explicit] The text asks, "What evidence can be provided that all variants will be identified as synonyms?" citing the risk of missed medical abbreviations.
- Why unresolved: Input variance in records (e.g., "SpO2" vs. "arterial oxygen saturation") is high, and assuring coverage requires testing against a theoretically infinite input space.
- What evidence would resolve it: Formal verification of restricted vocabularies (JSON schemas) or rigorous boundary testing demonstrating robustness against linguistic variance in critical data points.

## Limitations
- Mutual omission problem remains unresolved - high concordance scores can mask critical omissions if both agents miss the same safety-critical item
- Soft metrics (correctness/specificity) lack quantifiable definitions, creating gaps in the safety argument
- Synonym handling and abbreviation recognition remain problematic, potentially undercounting matches and inflating discordance scores

## Confidence
- **High confidence**: The identification of mutual omission as a critical failure mode and the argument that deterministic calculations are necessary for safety assurance
- **Medium confidence**: The proposed weighted basket approach with hard caps, though effectiveness depends heavily on comprehensive quality gate coverage
- **Low confidence**: The safety contribution of soft metrics like correctness/specificity, given their lack of explicit quantification

## Next Checks
1. **Gate coverage validation**: Systematically test the framework on patient records with known critical conditions (e.g., bleomycin exposure, severe AS) to measure detection rates across different quality gate configurations, quantifying the false negative rate for critical items
2. **Deterministic calculation validation**: Implement and compare coverage scores calculated via explicit Jaccard code versus LLM-generated scores across 50+ brief pairs to measure divergence and identify systematic biases
3. **Soft metric ablation study**: Run adjudication with and without correctness/specificity dimensions, then have human experts review flagged cases to determine whether these metrics contribute to or obscure safety-relevant signals