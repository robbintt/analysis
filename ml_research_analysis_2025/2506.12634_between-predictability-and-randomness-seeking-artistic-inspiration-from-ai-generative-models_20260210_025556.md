---
ver: rpa2
title: 'Between Predictability and Randomness: Seeking Artistic Inspiration from AI
  Generative Models'
arxiv_id: '2506.12634'
source_url: https://arxiv.org/abs/2506.12634
tags:
- lines
- creative
- artistic
- lstm-v
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares two AI systems for generating poetic lines
  as creative stimuli. LSTM-VAE models produce lines with semantic openness, unconventional
  juxtapositions, and syntactic incompleteness, which enable organic narrative emergence
  through human engagement.
---

# Between Predictability and Randomness: Seeking Artistic Inspiration from AI Generative Models

## Quick Facts
- **arXiv ID**: 2506.12634
- **Source URL**: https://arxiv.org/abs/2506.12634
- **Reference count**: 24
- **Primary result**: LSTM-VAE models produce lines with semantic openness and syntactic incompleteness that enable organic narrative emergence through human engagement, while LLM-generated lines are technically polished but adhere to conventional poetic forms.

## Executive Summary
This paper investigates how different AI generative models affect artistic creativity by comparing LSTM-VAE and LLM approaches for generating poetic lines as creative stimuli. The study finds that LSTM-VAE's output—characterized by semantic openness, unconventional juxtapositions, and syntactic incompleteness—provides more effective creative stimuli than LLM-generated lines, which tend to be technically polished but conventional. A poem that emerged organically through engagement with LSTM-VAE lines demonstrates the architecture's potential to inspire authentic artistic expression through its balance of structure and unpredictability.

## Method Summary
The study compares two AI systems for generating poetic lines: LSTM-VAE models and LLMs. LSTM-VAE generates lines by sampling from a continuous latent space, introducing controlled randomness while preserving semantic coherence. The researchers curated a style-specific poetry corpus, trained the LSTM-VAE on sentence-level data, and systematically sampled from the latent space to generate candidate lines. Human curators then selected evocative fragments that were arranged into complete poems. The resulting output was contrasted with LLM-generated lines at various temperature settings to evaluate differences in creative potential.

## Key Results
- LSTM-VAE models produce output with semantic openness and syntactic incompleteness that enables organic narrative emergence
- LSTM-VAE's intermediate complexity engages creative cognition more effectively than LLM's polished but conventional output
- A complete poem emerged organically through engagement with LSTM-VAE lines, demonstrating authentic artistic expression

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LSTM-VAE models produce output at an intermediate complexity level that maximally engages creative cognition by balancing structure and unpredictability.
- **Mechanism**: The VAE's latent space sampling introduces controlled randomness while the learned manifold preserves semantic coherence. This positions output in the mid-region of the random-predictable spectrum, where Berlyne's inverted U-shaped (Wundt) effect suggests stimuli are most aesthetically engaging.
- **Core assumption**: The Wundt effect generalizes from visual/music domains to linguistic stimuli for creative ideation.
- **Evidence anchors**:
  - [Section 5]: "stimuli of intermediate complexity are most pleasing... it is in the middle ground—where patterns are discernible but not obvious—that the brain engages most deeply"
  - [Section 6.3]: "LSTM-VAE models introduce randomness through latent space sampling, leading to the generation of linguistically novel sentences that can disrupt habitual language patterns"
- **Break condition**: If the latent space is poorly structured, output collapses to either incoherence or cliché reproduction.

### Mechanism 2
- **Claim**: Syntactic defamiliarization forces active meaning-making by disrupting automatic language processing.
- **Mechanism**: LSTM-VAE's reconstruction objective can produce syntactic irregularities that resist immediate comprehension. This "productive ungrammaticality" functions like Shklovsky's ostranenie—making familiar language strange, which slows perception and demands active interpretation.
- **Core assumption**: Syntactic disruptions that slow processing correlate with increased creative engagement rather than mere confusion.
- **Evidence anchors**:
  - [Section 7.1]: "These moments of syntactic estrangement transform conventional linguistic patterns into spaces of semantic possibility"
  - [Section 3]: "readers spend less time on 'cake' in 'the baker rushed the wedding cake' than on 'pies' in 'the baker rushed the wedding pies'"
- **Break condition**: If syntactic disruptions exceed a threshold, incomprehension replaces engagement.

### Mechanism 3
- **Claim**: Semantic openness and deliberate incompleteness invite artist-driven narrative completion.
- **Mechanism**: LSTM-VAE lines often lack closure, creating interpretive gaps that the artist fills through imaginative projection. Unlike LLM output that provides complete narrative units, these fragments function as creative prompts—emotionally resonant but interpretively open.
- **Core assumption**: Incomplete stimuli are more generative for artistic creation than polished, complete works.
- **Evidence anchors**:
  - [abstract]: "semantic openness, unconventional combinations, and fragments that resist closure"
  - [Section 7.3]: "What began as an intuitive response to evocative imagery and emotional undertones gradually crystallized into a story of departure, loss, and memory"
- **Break condition**: If fragments lack emotional resonance or are too sparse, they fail to provide sufficient anchor points for creative projection.

## Foundational Learning

- **Concept: Variational Autoencoder (VAE) latent space**
  - Why needed here: Understanding how VAEs encode sentences into continuous latent vectors—and how sampling creates novel outputs—is essential to grasping why this architecture produces "intermediate complexity" text.
  - Quick check question: How does sampling from a VAE's latent space differ fundamentally from next-token prediction in LLMs?

- **Concept: Shklovsky's ostranenie (defamiliarization)**
  - Why needed here: Provides the theoretical framework for why syntactic disruption enhances rather than diminishes artistic engagement.
  - Quick check question: Why might a phrase like "she walks in a better moon" be more creatively stimulating than "she walks under moonlight"?

- **Concept: The Wundt curve (inverted U-shaped aesthetic response)**
  - Why needed here: Explains why neither predictable LLM output nor random gibberish optimally engages creative cognition.
  - Quick check question: At which point on the predictability-randomness spectrum should a creativity-supporting generative model ideally operate?

## Architecture Onboarding

- **Component map**: Encoder (LSTM) -> Latent space -> Decoder (LSTM) -> Output
- **Critical path**:
  1. Curate training corpus (style-specific, smaller than LLM requirements)
  2. Train LSTM-VAE on sentence-level data
  3. Sample from latent space (tune variance for desired unpredictability)
  4. Decode to generate candidate lines
  5. Human curator selects/resonates with evocative fragments
  6. Artist arranges fragments; narrative emerges organically
- **Design tradeoffs**:
  - Dataset curation vs. scale: Smaller curated corpora enable style control; LLMs require massive data, limiting curation
  - Line-level vs. document-level: LSTM-VAE excels at short, fragmentary output; LLMs produce coherent long-form
  - Novelty vs. coherence: Higher sampling variance increases novelty but risks incomprehensibility
  - Polish vs. generative potential: "Imperfections" that reduce technical quality may increase inspirational value
- **Failure signatures**:
  - Mode collapse: Repetitive, low-diversity output (latent space insufficiently regularized)
  - Over-coherence: Output indistinguishable from training data (sampling variance too low)
  - Under-coherence: Incomprehensible output (sampling variance too high or latent space poorly structured)
  - Emotional flatness: Syntactically varied lines lacking affective resonance
- **First 3 experiments**:
  1. Train LSTM-VAE on a curated poetry corpus; systematically sample across latent space regions; annotate which regions yield lines with highest "productive indeterminacy" (emotional resonance + semantic openness).
  2. Generate parallel line sets using LSTM-VAE and LLM (at multiple temperature settings); have artists blind-rate inspirational value; test whether LSTM-VAE lines outperform on creative stimulation metrics.
  3. Run a controlled study where writers create original work starting from either LSTM-VAE fragments or complete LLM poems; measure self-reported inspiration depth, narrative originality, and perceived authenticity of output.

## Open Questions the Paper Calls Out
None

## Limitations
- Core claims rest on untested assumptions connecting Berlyne's Wundt curve to linguistic creativity
- The superiority of LSTM-VAE's "imperfections" assumes they function as intentional defamiliarization rather than technical failure
- Single-case study provides insufficient statistical power to generalize about architectural superiority

## Confidence

**High confidence**: The technical description of LSTM-VAE architecture and its differences from LLM generation is accurate and well-documented in the literature. The observed differences in output style (fragmentary vs. complete, conventional vs. unconventional) are objectively verifiable.

**Medium confidence**: The theoretical framework connecting unpredictability to creative engagement through defamiliarization and the Wundt effect is plausible but not yet empirically validated for text generation specifically. The mechanisms described align with established cognitive theories but require direct testing in this domain.

**Low confidence**: The claim that LSTM-VAE is "more effective" for artistic inspiration than LLMs rests primarily on one case study and theoretical reasoning rather than systematic comparative evaluation with human subjects.

## Next Checks

1. **Direct comparison study**: Conduct a controlled experiment where artists engage with parallel sets of LSTM-VAE and LLM-generated lines (at multiple temperature settings), then rate each set on creativity, inspiration, and likelihood of use in their work. This would provide quantitative evidence for the claimed superiority of LSTM-VAE's output characteristics.

2. **Eye-tracking and cognitive load measurement**: Use eye-tracking to measure reading patterns and fixation durations for LSTM-VAE vs. LLM lines. Longer fixations on LSTM-VAE lines would provide objective evidence that syntactic defamiliarization forces active processing, supporting Mechanism 2.

3. **Latent space interpolation study**: Systematically sample from different regions of the LSTM-VAE latent space to identify where "productive indeterminacy" peaks. Map the relationship between sampling variance and output characteristics to empirically validate the Wundt curve application to text generation.