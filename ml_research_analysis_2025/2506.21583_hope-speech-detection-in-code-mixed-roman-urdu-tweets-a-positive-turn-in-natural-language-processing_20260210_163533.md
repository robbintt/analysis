---
ver: rpa2
title: 'Hope Speech Detection in code-mixed Roman Urdu tweets: A Positive Turn in
  Natural Language Processing'
arxiv_id: '2506.21583'
source_url: https://arxiv.org/abs/2506.21583
tags:
- hope
- speech
- urdu
- roman
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces the first dataset and deep learning model
  for hope speech detection in code-mixed Roman Urdu. The dataset contains 4,953 social
  media posts annotated into four categories: Generalized Hope, Realistic Hope, Unrealistic
  Hope, and Not Hope.'
---

# Hope Speech Detection in code-mixed Roman Urdu tweets: A Positive Turn in Natural Language Processing

## Quick Facts
- arXiv ID: 2506.21583
- Source URL: https://arxiv.org/abs/2506.21583
- Reference count: 27
- First dataset and deep learning model for hope speech detection in code-mixed Roman Urdu

## Executive Summary
This study introduces the first dataset and deep learning model for hope speech detection in code-mixed Roman Urdu. The dataset contains 4,953 social media posts annotated into four categories: Generalized Hope, Realistic Hope, Unrealistic Hope, and Not Hope. A custom attention-based transformer model, XLM-R, was designed and evaluated using 5-fold cross-validation. XLM-R achieved the highest performance with a cross-validation score of 0.78, outperforming baseline SVM (0.75) and BiLSTM (0.76) by 4% and 2.63% respectively. A t-test confirmed these improvements are statistically significant. This work addresses the gap in hope speech detection for informal, low-resource languages, providing a foundation for future inclusive NLP research.

## Method Summary
The study collected 30,000 Roman Urdu tweets from X (Twitter) using keywords like "umeed," "hausla," and "himmat na haaro," filtering down to 4,953 posts after preprocessing. Tweets were annotated by three annotators into four categories using majority voting, achieving substantial inter-annotator agreement (Kappa = 0.81). The dataset was balanced using random oversampling before being split into five folds for cross-validation. XLM-R with a custom attention mechanism was fine-tuned with learning rate 3×10⁻⁵, 3 epochs, batch size 16, and AdamW optimizer. Performance was compared against SVM with TF-IDF features and BiLSTM with GloVe/FastText embeddings.

## Key Results
- XLM-R achieved 0.78 cross-validation F1-score, outperforming SVM (0.75) and BiLSTM (0.76)
- Statistical significance confirmed via paired t-test (p < 0.05 for both comparisons)
- Class-wise performance showed perfect "Not Hope" detection (F1: 1.0) but poor "Generalized Hope" detection (F1: 0.37)
- Substantial inter-annotator agreement (Kappa = 0.81) validated annotation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual transformer pre-training provides superior representation for code-mixed Roman Urdu hope speech compared to monolingual embeddings.
- Mechanism: XLM-R's cross-lingual pre-training on 100 languages enables transfer of semantic patterns from high-resource languages to low-resource Roman Urdu, capturing contextual relationships that TF-IDF and GloVe miss. The model's attention heads learn to weight code-mixed tokens appropriately despite spelling variations.
- Core assumption: Roman Urdu shares sufficient semantic structure with languages in XLM-R's pre-training corpus for effective transfer.
- Evidence anchors:
  - [abstract] "The proposed model, XLM-R, achieves the best performance with a cross-validation score of 0.78, outperforming the baseline SVM (0.75) and BiLSTM (0.76)"
  - [section 4.3] "XLM-R clearly stands out... achieving the highest precision (0.78), recall (0.78), F1-score (0.77), and cross-validation score (0.78)"
  - [corpus] Related work on Roman Urdu-English code-mixed text (arxiv:2510.03683) confirms transformer fine-tuning effectiveness for this script, though specific hope speech transfer remains untested.
- Break condition: Performance degrades significantly if Roman Urdu expressions diverge structurally from pre-training languages, or if hope speech relies on culturally-specific metaphors absent from training data.

### Mechanism 2
- Claim: Custom attention mechanisms improve classification by focusing on hope-relevant tokens amid noisy, informal text.
- Mechanism: The paper's custom attention layer (applied to mBERT, XLM-R, DistilBERT) learns to weight tokens that signal hope categories—such as "umeed" (hope), "hausla" (courage)—while down-weighting noise from inconsistent spelling and code-mixed English.
- Core assumption: Hope speech contains identifiable lexical and contextual markers that attention can learn to prioritize.
- Evidence anchors:
  - [section 1] "We designed, proposed, implemented, and evaluated an advanced language-based model using a custom attention mechanism optimized to handle the syntactic and semantic variability of Roman Urdu"
  - [section 3.8] "transformer models—specifically mBERT, XLM-RoBERTA, and DistilBERT with a custom attention mechanism—we fine-tuned each model"
  - [corpus] Insufficient external evidence—no comparable attention mechanism evaluations for Roman Urdu hope speech found in corpus.
- Break condition: Attention mechanism overfits to annotation artifacts or specific keywords rather than learning generalizable hope patterns.

### Mechanism 3
- Claim: Psychologically-grounded multi-class annotation captures nuance that binary classification misses.
- Mechanism: Four-category schema (Generalized Hope, Realistic Hope, Unrealistic Hope, Not Hope) distinguishes between actionable optimism and abstract wishful thinking, enabling models to learn finer semantic distinctions.
- Core assumption: Annotators can reliably distinguish hope subtypes, and these distinctions are learnable from textual features.
- Evidence anchors:
  - [section 3.5] "The resulting average Kappa value was 0.81, which indicates a substantial level of agreement among the annotators"
  - [section 4.4, Table 9] Class-wise performance shows "not hope" achieved perfect scores (F1: 1.0) while "generalized hope" struggled (F1: 0.37), indicating some categories are harder to learn
  - [corpus] PolyHope dataset (Expert Systems with Applications, 2023) used similar multi-class schema for English, validating the psychological framing approach.
- Break condition: If "Generalized Hope" and "Realistic Hope" share too many features, model cannot reliably separate them—current low recall (0.28) suggests this may be occurring.

## Foundational Learning

- Concept: **Code-mixed text processing**
  - Why needed here: Roman Urdu lacks standardized orthography—"kaise," "kesay," and "kese" all mean "how"—requiring models tolerant of spelling variation.
  - Quick check question: Can you explain why TF-IDF struggles with spelling variants that represent the same word?

- Concept: **Cross-validation for small datasets**
  - Why needed here: With only 4,953 samples and class imbalance, 5-fold cross-validation provides more reliable performance estimates than single train/test splits.
  - Quick check question: Why would a single hold-out test set give misleading results on this dataset?

- Concept: **Statistical significance testing (paired t-test)**
  - Why needed here: Small performance differences (0.78 vs 0.76) could arise from random variation; t-test confirms gains are systematic.
  - Quick check question: What does a p-value of 0.00116 (XLM-R vs BiLSTM) actually tell you about model comparison?

## Architecture Onboarding

- Component map: Raw tweets → Preprocessing (hashtags/URLs/punctuation removal, 15-char filter) → Annotation (3 annotators, majority voting, 4-class labels) → Oversampling (class balance) → XLM-R encoder + Custom attention layer → Classification head (4 outputs) → 5-fold cross-validation evaluation

- Critical path: Data quality → annotation consistency (Kappa=0.81) → model selection. If annotation fails, no model architecture can recover. The "generalized hope" low recall (0.28) indicates annotation boundaries may need refinement before model iteration.

- Design tradeoffs:
  - XLM-R vs mBERT: XLM-R (+0.07 F1) chosen despite higher computational cost
  - Custom attention vs stock transformer: Paper claims custom attention helps but does not ablate—isolated contribution unverified
  - Oversampling vs class weighting: Oversampling used but may amplify annotation noise in minority classes

- Failure signatures:
  - Perfect "not hope" F1 (1.0) combined with poor "generalized hope" F1 (0.37) suggests model learns to default to majority class
  - If validation accuracy >> cross-validation average, check for data leakage in preprocessing

- First 3 experiments:
  1. **Baseline replication**: Run SVM with TF-IDF (unigram) to verify 0.75 cross-validation score before attempting transformer fine-tuning
  2. **Ablation study**: Train XLM-R without custom attention to isolate attention mechanism's contribution—paper does not report this
  3. **Class-wise error analysis**: Examine confusion matrix for "generalized hope" misclassifications; determine if annotations are ambiguous or features insufficient

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can hope speech detection models trained on Roman Urdu Twitter data generalize effectively to other social media platforms such as Facebook, Instagram, TikTok, and YouTube comments?
- Basis in paper: [explicit] The authors state in their future work: "we aim to extend our existing dataset by incorporating more diverse samples from multiple platforms such as Facebook, Instagram, TikTok, and YouTube comments to better capture the linguistic and contextual variability of Roman Urdu."
- Why unresolved: The current dataset is drawn exclusively from Twitter, which may not capture the full range of Roman Urdu expressions used on other platforms with different communication norms, character limits, and user demographics.
- What evidence would resolve it: A cross-platform evaluation study where the XLM-R model trained on Twitter data is tested on annotated samples from Facebook, Instagram, TikTok, and YouTube, with performance metrics compared against platform-specific training.

### Open Question 2
- Question: Can cross-lingual transfer learning enable Roman Urdu hope speech models to effectively adapt to related languages and scripts such as Nastaliq Urdu, Hindi, and Hinglish?
- Basis in paper: [explicit] The authors explicitly plan to "explore cross-lingual transfer learning, investigating how Roman Urdu hope speech models can be adapted or extended to related languages and scripts, such as Nastaliq Urdu, Hindi, and Hinglish."
- Why unresolved: While XLM-R has multilingual capabilities, it remains untested whether knowledge transfer between Roman Urdu and its script-variant (Nastaliq Urdu) or code-mixed relatives (Hindi, Hinglish) is effective given the orthographic and syntactic differences.
- What evidence would resolve it: Experiments involving zero-shot or few-shot transfer from Roman Urdu to these related languages, with quantitative comparison of F1-scores across transfer configurations.

### Open Question 3
- Question: What architectural or data-level modifications would improve detection of the "Generalized Hope" category, which currently achieves only 0.28 recall and 0.37 F1-score?
- Basis in paper: [inferred] The error analysis (Table 9) reveals that the XLM-R model struggles significantly with "generalized hope," showing substantially lower performance (Precision: 0.53, Recall: 0.28, F1: 0.37) compared to other categories like "not hope" (F1: 1.0).
- Why unresolved: The paper does not investigate why generalized hope is difficult to detect, nor does it propose solutions for this class imbalance in performance. The category may have vaguer linguistic markers or overlap with other hope types.
- What evidence would resolve it: Ablation studies testing class-specific attention mechanisms, data augmentation for generalized hope samples, or refined annotation guidelines that better differentiate generalized hope from other categories, with resulting improvements in recall and F1 for this class.

### Open Question 4
- Question: Would fine-tuning large language models (LLMs) outperform the current XLM-R benchmark of 0.78 cross-validation score on Roman Urdu hope speech detection?
- Basis in paper: [explicit] The authors state as future work: "we intend to fine-tune and evaluate large language models (LLMs) on our expanded dataset to assess their effectiveness in detecting nuanced hope speech across informal, multilingual, and code-mixed text settings."
- Why unresolved: The current study only evaluates transformer models up to XLM-R (a RoBERTa variant) and does not test larger generative models that may have stronger capabilities for understanding subtle, metaphorical, and culturally embedded expressions of hope.
- What evidence would resolve it: Systematic evaluation of fine-tuned LLMs (e.g., LLaMA, GPT variants, or instruction-tuned multilingual models) on the same dataset using identical cross-validation protocol, with statistical comparison to the XLM-R baseline.

## Limitations

- Dataset accessibility limited to "upon request" rather than public repository, restricting independent validation
- Custom attention mechanism architecture underspecified, making it impossible to determine whether performance gains stem from XLM-R's pre-training or the attention modification
- Significant class imbalance in performance (perfect "Not Hope" F1: 1.0 vs poor "Generalized Hope" F1: 0.37) suggests potential annotation ambiguity or model bias toward majority class

## Confidence

**High Confidence** (supported by multiple evidence anchors and external validation):
- XLM-R outperforms baseline models (SVM, BiLSTM) with statistically significant differences (p < 0.05)
- Dataset annotation shows substantial inter-annotator agreement (Kappa = 0.81)
- Transformer-based approaches outperform traditional ML methods for this task

**Medium Confidence** (supported by paper claims but limited external verification):
- Custom attention mechanism contributes to performance gains (mechanism described but not ablated)
- Four-category schema captures meaningful distinctions in hope speech (psychologically grounded but class imbalance evident)
- Cross-lingual transfer from XLM-R pre-training benefits Roman Urdu (plausible given related work but not directly tested)

**Low Confidence** (insufficient evidence or unverified assumptions):
- XLM-R's specific architectural advantage over other transformers for this task (no ablation study)
- Long-term generalizability beyond the specific keyword-based corpus (dataset collection method not fully specified)
- Claim of being "first" in this domain (unverified given rapid growth in low-resource NLP)

## Next Checks

1. **Replicate baseline results**: Independently implement and evaluate the SVM-TF-IDF baseline to verify the claimed 0.75 cross-validation score before attempting transformer fine-tuning.

2. **Attention mechanism ablation**: Train XLM-R without the custom attention layer to isolate its contribution to the 0.78 performance score, as the paper does not report this critical comparison.

3. **Per-class significance testing**: Conduct paired t-tests for each of the four categories individually to determine whether performance improvements are uniform across all hope speech types or concentrated in specific classes.