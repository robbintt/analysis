---
ver: rpa2
title: 'TrajSelector: Harnessing Latent Representations for Efficient and Effective
  Best-of-N in Large Reasoning Model'
arxiv_id: '2510.16449'
source_url: https://arxiv.org/abs/2510.16449
tags:
- reasoning
- arxiv
- score
- process
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "TrajSelector improves external test-time scaling by exploiting\
  \ the sampler LLM\u2019s hidden states for lightweight process-level scoring. Instead\
  \ of deploying a large process reward model, it uses a 0.6B verifier that scores\
  \ each reasoning step using the sampler\u2019s last hidden states, then aggregates\
  \ these scores to select the best trajectory."
---

# TrajSelector: Harnessing Latent Representations for Efficient and Effective Best-of-N in Large Reasoning Model

## Quick Facts
- arXiv ID: 2510.16449
- Source URL: https://arxiv.org/abs/2510.16449
- Authors: Bin Yu; Xinming Wang; Shijie Lian; Haotian Li; Changti Wu; Ruina Hu; Bailing Wang; Yuliang Wei; Kai Chen
- Reference count: 13
- Key outcome: TrajSelector improves Best-of-32 accuracy by 4.61% over majority voting and 4.31%–12.21% over 7B PRMs using only a 0.6B verifier by exploiting sampler LLM hidden states.

## Executive Summary
TrajSelector addresses the challenge of selecting the best reasoning trajectory from multiple candidates (Best-of-N) by leveraging the internal hidden states of the sampler LLM. Instead of using a large process reward model, it employs a tiny 0.6B verifier that scores each reasoning step using the sampler's last hidden states, then aggregates these scores to select the optimal trajectory. Trained end-to-end without step-level annotations, it demonstrates significant accuracy improvements while maintaining low inference costs.

## Method Summary
TrajSelector trains a lightweight verifier (Qwen3-0.6B-Base) to score reasoning trajectories generated by a frozen sampler (Qwen3-8B). The verifier extracts the last hidden state of each reasoning step (delimited by `\n\n`) from the sampler, projects it to the verifier's dimension, and processes it through a small LLM with a 3-class score head (Right, Wrong, Buffer). The trajectory score is the mean of step scores, and the highest-scoring trajectory is selected. Training uses weak supervision from final-answer labels with a three-class loss to handle noisy step-level annotations, trained on 133K examples from OpenR1-Math-220K and DeepMath-103K.

## Key Results
- Outperforms majority voting by 4.61% accuracy in Best-of-32 settings
- Beats existing PRMs by 4.31%–12.21% using only 0.6B parameters vs. 7B
- Ablation confirms effectiveness of 3-class loss, base LLM choice, and scalability to larger N
- Maintains low inference cost compared to traditional large PRMs

## Why This Works (Mechanism)

### Mechanism 1
The sampler LLM's internal hidden states encode "introspective signals" about reasoning validity that a lightweight external model can decode for verification. TrajSelector extracts the last hidden state of the final token in each reasoning step, bypassing the need for the verifier to re-encode text. This transfers the "semantic heavy lifting" to the sampler. The core assumption is that the frozen sampler encodes sufficient correctness signals in its latent space during generation. If the sampler is too small or undertrained, the hidden states won't differentiate between correct and hallucinated steps.

### Mechanism 2
A three-class loss function (Right, Wrong, Neutral) mitigates label noise from using only final-outcome supervision for step-level training. The "Neutral" class acts as a buffer for ambiguous or noisy step representations, allowing the model to "abstain" on uncertain steps without penalizing the entire trajectory. This addresses the problem that correct final answers may contain flawed intermediate steps. If the dataset contains overwhelmingly correct trajectories with almost no flawed steps, the buffer class may remain underutilized.

### Mechanism 3
Verification is simpler than generation, enabling massive parameter reduction (8B Generator → 0.6B Verifier). The system decouples generation from evaluation, with the verifier mapping high-level features to scores rather than solving problems. This assumes discriminative tasks require less capacity than generative ones when input representations are semantically rich. If verification requires deep logical re-evaluation rather than pattern matching, the 0.6B model may fail to catch subtle logical fallacies.

## Foundational Learning

- **Concept: Best-of-N (External TTS)**
  - **Why needed here:** This is the deployment paradigm where the model generates N solutions and TrajSelector picks the best one.
  - **Quick check question:** How does the computational cost of generating N solutions compare to training a larger model? (Hint: It shifts cost to inference).

- **Concept: Hidden States vs. Logits**
  - **Why needed here:** The core innovation relies on accessing the model's internal vectors (hidden states) rather than just the output probabilities (logits).
  - **Quick check question:** If you only used the output logits (token probabilities), would you capture the model's "uncertainty" as effectively as the hidden states?

- **Concept: Weak Supervision & Label Noise**
  - **Why needed here:** Training uses final answers (weak supervision) rather than human-annotated steps.
  - **Quick check question:** Why might a correct final answer contain an incorrect reasoning step?

## Architecture Onboarding

- **Component map:** Sampler LLM -> Projection Layer -> Process Score Model (Tiny LLM) -> Score Head -> Aggregator
- **Critical path:**
  1. Run Sampler LLM on query; extract hidden state of the *last token* of every step (delimited by `\n\n`)
  2. Pass these vectors through Projection -> Tiny LLM -> Score Head to get $p_{\text{right}}$
  3. Average $p_{\text{right}}$ across steps to rank $N$ candidates
- **Design tradeoffs:**
  - **Base vs. Instruct Verifier:** Paper suggests using the *Base* (unhinged) version of the tiny LLM for the score model, as RLHF-aligned models showed worse performance
  - **Complexity:** Requires modifying the inference server to return hidden states, adding memory/transfer overhead compared to text-only methods
- **Failure signatures:**
  - **Dimension Mismatch:** Forgetting the projection layer if Sampler and Verifier dimensions differ
  - **Tokenizer Drift:** Step segmentation relies on `\n\n`; if the Sampler doesn't format output this way, extraction fails
  - **Hidden State Extraction:** Accidentally extracting the first token or padding tokens instead of the step-final token
- **First 3 experiments:**
  1. **Sanity Check (Hidden State Extraction):** Verify that extracting the last hidden state of the token preceding `\n\n` correctly corresponds to the end of a reasoning step
  2. **Loss Ablation:** Train two tiny verifiers—one with standard Binary Cross Entropy (BCE) and one with the proposed 3-class loss—on a small dataset subset. Confirm the 3-class loss converges to lower validation error
  3. **Best-of-N Mock Test:** Generate 2 trajectories (one clearly correct, one clearly wrong/hallucinated). Check if the TrajSelector score for the correct one is significantly higher ($>0.1$ delta)

## Open Questions the Paper Calls Out

### Open Question 1
Can the latent representation signals exploited by TrajSelector for mathematical reasoning generalize effectively to open-ended question answering domains where rigorous answer verification is unavailable? The current training and evaluation rely on verifiable mathematical domains, and it's unclear if the "introspective signals" in hidden states correlate with correctness in subjective or diverse reasoning tasks.

### Open Question 2
How can TrajSelector be integrated with search algorithms like Monte Carlo Tree Search (MCTS) to fully unlock the potential of process-level scoring? TrajSelector currently operates as a post-hoc selector, but has not been validated as a value function to guide the generation process in tree-based search.

### Open Question 3
Can the trajectory scores generated by TrajSelector be utilized to dynamically determine the optimal sampling size N based on query difficulty? The current study evaluates fixed N values, and it's unknown if the verifier's confidence scores can reliably predict when to stop sampling to balance accuracy-compute trade-offs.

## Limitations

- The approach relies heavily on the assumption that sampler LLM hidden states contain sufficient introspective signals about reasoning validity, without direct evidence beyond what would be captured by text-based features
- The use of weak supervision with final-answer labels introduces label noise at the step level, and the paper doesn't quantify how much noise is actually present in the training data
- Computational advantage claims assume verification is simpler than generation, but the paper doesn't benchmark the full end-to-end cost including overhead of extracting and processing hidden states

## Confidence

**High Confidence**: The empirical results showing TrajSelector outperforming majority voting and PRMs on Best-of-32 selection are well-supported by ablation studies and consistent performance improvements across multiple benchmarks.

**Medium Confidence**: The core hypothesis that hidden states contain introspective signals is plausible given the results, but lacks direct validation. The effectiveness of the three-class loss is demonstrated, but the exact nature and magnitude of the label noise problem remains unquantified.

**Low Confidence**: The claim that verification is fundamentally simpler than generation (enabling the 10x parameter reduction) is intuitive but not rigorously proven. The paper doesn't test whether larger verifiers provide additional gains or whether the 0.6B size is optimal.

## Next Checks

1. **Hidden State vs. Text Feature Ablation**: Train an otherwise identical verifier that receives the step text (encoded through the verifier's tokenizer) instead of the projected hidden states. Compare performance to quantify how much of the gain comes from exploiting hidden states versus simply having a better verifier architecture.

2. **Label Noise Quantification**: Analyze a sample of training trajectories to measure the actual rate of incorrect steps in otherwise correct trajectories (and vice versa). This would validate whether the three-class loss buffer is addressing a significant problem or providing marginal benefit.

3. **End-to-End Cost Analysis**: Measure the total inference time and memory usage for TrajSelector versus baseline PRMs, including the overhead of extracting hidden states from the sampler. Compare the cost per selected trajectory to determine if the parameter reduction translates to practical efficiency gains.