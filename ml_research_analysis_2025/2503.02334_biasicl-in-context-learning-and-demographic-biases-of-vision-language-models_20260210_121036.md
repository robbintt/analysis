---
ver: rpa2
title: 'BiasICL: In-Context Learning and Demographic Biases of Vision Language Models'
arxiv_id: '2503.02334'
source_url: https://arxiv.org/abs/2503.02334
tags:
- demographic
- bias
- learning
- prompt
- vlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how in-context learning (ICL) affects
  demographic fairness in vision-language models (VLMs) for medical imaging tasks.
  The authors examine two medical imaging tasks: skin lesion malignancy prediction
  and pneumothorax detection from chest radiographs, analyzing how demographic composition
  of demonstration examples impacts VLM performance.'
---

# BiasICL: In-Context Learning and Demographic Biases of Vision Language Models

## Quick Facts
- arXiv ID: 2503.02334
- Source URL: https://arxiv.org/abs/2503.02334
- Reference count: 26
- Primary result: VLMs show demographic biases in medical imaging tasks that are amplified by in-context learning prompts

## Executive Summary
This study investigates how in-context learning (ICL) affects demographic fairness in vision-language models (VLMs) for medical imaging tasks. The authors examine two medical imaging tasks: skin lesion malignancy prediction and pneumothorax detection from chest radiographs, analyzing how demographic composition of demonstration examples impacts VLM performance. The research reveals that VLMs exhibit both majority label bias and demographic group majority label bias, where the frequency of labels within specific demographic subgroups influences prediction outcomes.

The findings demonstrate that ICL alone can increase demographic subgroup bias even when controlling for base rates, and that VLMs are less capable of identifying demographic subgroups than previous supervised deep learning models. The study recommends that developers should evaluate models' performance stratified across relevant subgroups and carefully control both overall and subgroup-specific label frequencies in prompts.

## Method Summary
The researchers conducted a comprehensive analysis of VLM performance across two medical imaging tasks using synthetically generated prompts with varying demographic compositions. They tested multiple prompt configurations including different demographic subgroups (age, sex, race/ethnicity) and label distributions within those subgroups. The study controlled for base rates while systematically varying the frequency of positive and negative examples across demographic categories. Performance metrics were calculated separately for each demographic subgroup to identify potential biases introduced or amplified by ICL.

## Key Results
- VLMs show "majority label bias" where they predict more frequent labels in prompts more often
- VLMs exhibit "demographic group majority label bias" where they are sensitive to label frequency within specific demographic subgroups
- ICL alone can increase demographic subgroup bias even when controlling for base rates

## Why This Works (Mechanism)
The mechanism underlying VLM demographic biases stems from how in-context learning relies on demonstration examples within prompts to guide predictions. When prompts contain disproportionate representation of certain demographic groups or label frequencies within those groups, the model implicitly learns these patterns as decision criteria. The VLMs appear to weight demographic characteristics and label frequencies from demonstration examples heavily in their inference process, leading to systematic prediction biases that mirror the composition of the prompt examples rather than the actual clinical features of the input images.

## Foundational Learning
- **In-context learning**: The ability of models to learn from demonstration examples within prompts without parameter updates; needed to understand how VLMs adapt to new tasks through prompt composition
- **Demographic stratification**: Analysis of model performance across different demographic subgroups; needed to identify potential fairness issues in healthcare applications
- **Base rate control**: Statistical adjustment to isolate the effect of prompt composition from underlying class distributions; needed to determine whether observed biases are due to ICL or pre-existing model tendencies
- **Synthetic prompt generation**: Method of creating controlled prompt variations for systematic testing; needed to establish causal relationships between prompt composition and model behavior
- **Medical image classification**: Task of categorizing medical images into diagnostic categories; needed as the application domain for testing VLM fairness
- **Vision-language models**: AI systems that process both visual and textual information; needed as the target architecture for bias analysis

## Architecture Onboarding

### Component Map
VLMs typically follow a pipeline where input images and text prompts are encoded separately, then combined through cross-attention mechanisms for joint reasoning and prediction output.

### Critical Path
The critical path for demographic bias manifestation occurs when demonstration examples in prompts influence the attention weights during cross-modal reasoning, causing the model to overweight patterns matching the prompt composition rather than clinical features.

### Design Tradeoffs
The architecture trades general reasoning capability for task-specific performance through ICL, but this flexibility introduces vulnerability to systematic biases when prompt composition reflects societal imbalances or when developers inadvertently encode demographic patterns into demonstration examples.

### Failure Signatures
Primary failure signatures include systematic prediction differences across demographic subgroups that correlate with label frequencies in prompts rather than actual clinical presentation, and reduced performance on minority demographic groups when they are underrepresented in demonstration examples.

### First Experiments
1. Test baseline VLM performance on both tasks without any ICL prompts to establish reference performance levels
2. Evaluate performance using balanced prompts with equal representation across all demographic subgroups
3. Test prompts with reversed demographic compositions to confirm bias direction is determined by prompt rather than inherent model preference

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability across different VLM architectures and training paradigms remains unclear
- Analysis limited to specific demographic subgroups (age, sex, race/ethnicity) may miss other relevant dimensions
- Synthetic prompt generation may not fully represent real-world clinical prompting scenarios

## Confidence
- High confidence: VLMs exhibit majority label bias and demographic group majority label bias (supported by robust statistical analysis)
- Medium confidence: ICL exacerbates demographic subgroup bias compared to traditional supervised learning (based on previous studies rather than direct validation)
- Medium confidence: VLMs are less capable of identifying demographic subgroups than previous models (relies on comparing current findings with earlier research)

## Next Checks
1. Conduct ablation studies testing multiple VLM architectures to determine if observed demographic biases are architecture-dependent
2. Perform real-world clinical prompting studies with actual healthcare practitioners to validate synthetic prompt generation approach
3. Expand demographic subgroup analysis to include additional protected characteristics beyond those studied