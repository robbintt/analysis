---
ver: rpa2
title: HCQA-1.5 @ Ego4D EgoSchema Challenge 2025
arxiv_id: '2505.20644'
source_url: https://arxiv.org/abs/2505.20644
tags:
- arxiv
- video
- cards
- reasoning
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents HCQA-1.5, a method for egocentric video question
  answering that secured third place in the Ego4D EgoSchema Challenge 2025. The approach
  extends the HCQA framework by introducing a two-stage decision-making process: multi-source
  aggregation of diverse LLM predictions followed by confidence-based filtering, and
  fine-grained reasoning for low-confidence cases using vision-based and thought-based
  strategies.'
---

# HCQA-1.5 @ Ego4D EgoSchema Challenge 2025

## Quick Facts
- arXiv ID: 2505.20644
- Source URL: https://arxiv.org/abs/2505.20644
- Reference count: 27
- Third place in CVPR 2025 Ego4D EgoSchema Challenge with 77% accuracy

## Executive Summary
HCQA-1.5 is a two-stage egocentric video question answering system that secured third place in the Ego4D EgoSchema Challenge 2025. The method extends the HCQA framework by introducing confidence-based filtering and selective reasoning for low-confidence cases. By aggregating predictions from multiple LLMs and applying dual-path reasoning (vision-based and thought-based) to uncertain samples, HCQA-1.5 achieves 77% accuracy on a 5,000+ question blind test set, outperforming last year's winning solution.

## Method Summary
HCQA-1.5 uses a two-stage pipeline: (1) Multi-source aggregation where Gemini-1.5-Pro, GPT-4.1, and Qwen2.5 generate predictions from HCQA outputs with confidence scores, filtering answers above threshold 4; (2) Fine-grained reasoning for low-confidence cases using 45 uniformly sampled frames for Qwen2.5-VL-72B and aggregated textual context for DeepSeek-R1, selecting the higher-confidence output. The approach builds on HCQA preprocessing and demonstrates the effectiveness of selective reasoning and model ensemble in egocentric video understanding.

## Key Results
- Achieved 77% accuracy on EgoSchema blind test set with 5,000+ human-curated questions
- Outperformed last year's winning solution and majority of participating teams
- Individual model performance ranges 71.0%-76.1%, while full pipeline achieves 77.3%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-source aggregation with heterogeneous LLMs improves answer reliability through complementary error distributions.
- Mechanism: Multiple LLMs (Gemini-1.5-Pro, GPT-4.1, Qwen2.5) generate predictions independently; their diverse training and architectures produce different failure modes. Aggregating by selecting the highest-confidence answer increases the probability that at least one model produces a correct, confident prediction.
- Core assumption: Model errors are not perfectly correlated across different LLM families.
- Evidence anchors:
  - [section 2.1] "The use of multiple LLMs introduces complementary perspectives, which improves robustness and coverage across different input cases."
  - [table 2] Individual models range 71.0%-76.1%; integration achieves 76.3%, showing aggregation benefit.
  - [corpus] Weak external validation; neighbor papers focus on different Ego4D tasks (localization, action anticipation), not ensemble strategies for video QA.
- Break condition: If models converge on identical errors for systematically difficult samples (e.g., ambiguous social interactions), aggregation provides no gain.

### Mechanism 2
- Claim: Confidence-based filtering enables selective compute allocation—high-confidence answers bypass expensive reasoning.
- Mechanism: Each LLM outputs a confidence score (1-5). Predictions scoring >4 proceed directly as final answers; low-confidence predictions trigger secondary processing. This creates a computational shortcut for "easy" cases while reserving capacity for uncertain ones.
- Core assumption: LLM self-reported confidence correlates positively with answer correctness.
- Evidence anchors:
  - [section 2.1] "predictions with a confidence score higher than 4... are considered reliable and retained as final answers."
  - [abstract] "confidence-based filtering mechanism that selects high-confidence answers directly."
  - [corpus] No direct external evidence; confidence-calibration in VLMs remains an open research question.
- Break condition: If confidence scores are systematically miscalibrated (e.g., overconfidence on incorrect predictions), filtering will propagate errors.

### Mechanism 3
- Claim: Dual-path fine-grained reasoning (vision + thought) recovers answers from uncertain predictions through modality-specific re-analysis.
- Mechanism: Low-confidence samples receive parallel processing: (1) Vision-based: 45 uniformly sampled frames input to Qwen2.5-VL-72B for direct visual evidence; (2) Thought-based: All textual context (captions, summaries, prior predictions) fed to DeepSeek-R1 for chain-of-thought reasoning. Higher-confidence output wins.
- Core assumption: Vision-based and thought-based reasoning have different failure modes; at least one path can recover the correct answer.
- Evidence anchors:
  - [section 2.2] "Qwen2.5-VL-72B and DeepSeek-R1 perform similarly (76.8% vs. 76.6%) when applied to low-confidence samples."
  - [table 2] Full pipeline (both stages) achieves 77.3%, exceeding single-stage approaches.
  - [corpus] No external validation of dual-path reasoning for egocentric video QA specifically.
- Break condition: If visual evidence is genuinely insufficient (e.g., occlusion, fast motion) AND textual context is misleading, both paths fail.

## Foundational Learning

- Concept: Egocentric video characteristics (rapid motion, limited FOV, occlusions)
  - Why needed here: HCQA-1.5 targets first-person video where these challenges are inherent; understanding them explains why single-model approaches fail and why frame-sampling + textual summarization are necessary.
  - Quick check question: Why would extracting 45 uniform frames potentially miss critical information in egocentric video?

- Concept: LLM confidence calibration
  - Why needed here: The entire filtering mechanism depends on whether self-reported confidence reflects actual correctness; miscalibration would cause systematic error propagation.
  - Quick check question: What happens to the pipeline if a model assigns confidence=5 to incorrect answers 30% of the time?

- Concept: Model ensemble diversity
  - Why needed here: Multi-source aggregation only helps if models make different mistakes; understanding diversity sources (training data, architecture, objective) guides model selection.
  - Quick check question: Would ensembling GPT-4.1 with GPT-4o provide the same benefit as ensembling GPT-4.1 with Gemini-1.5-Pro? Why or why not?

## Architecture Onboarding

- Component map:
  - Stage 1 (Multi-source Aggregation): Video → HCQA preprocessing (captions, summaries) → 3 LLMs in parallel (Gemini-1.5-Pro, GPT-4.1, Qwen2.5) → confidence-scored predictions → filter (threshold=4)
  - Stage 2 (Fine-grained Reasoning): Low-confidence samples → parallel paths: (a) 45-frame extraction → Qwen2.5-VL-72B, (b) text aggregation → DeepSeek-R1 → confidence comparison → final answer
  - Integration: High-confidence from Stage 1 OR higher-confidence from Stage 2 paths

- Critical path: Video input → HCQA caption/summary extraction → multi-LLM prediction → confidence evaluation → (if <4) → dual-path reasoning → answer selection

- Design tradeoffs:
  - Threshold=4 balances throughput vs. accuracy; lower threshold = more Stage 2 processing, higher latency
  - 45 frames chosen for efficiency; more frames improve coverage but increase cost
  - Three specific LLMs selected empirically; other combinations untested

- Failure signatures:
  - Surface-level visual bias: Model selects answers based on object presence (dice + cards → "board game") rather than interaction semantics (Figure 2 failure case)
  - Social dynamics misunderstanding: Collaborative activities misclassified as individual actions
  - Confidence miscalibration: High-confidence wrong answers bypass correction

- First 3 experiments:
  1. Threshold sweep: Test confidence thresholds {3, 3.5, 4, 4.5} on validation set to measure accuracy-latency tradeoff.
  2. Frame ablation: Compare 30 vs. 45 vs. 60 uniformly sampled frames for vision-based reasoning to identify information saturation point.
  3. Model diversity check: Measure pairwise error correlation between Gemini-1.5-Pro, GPT-4.1, and Qwen2.5; replace lowest-diversity pair with a different model family (e.g., Claude).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can egocentric video QA models be improved to distinguish between object-centric activities and collaborative social intent?
- Basis in paper: [explicit] The authors analyze a failure case where the model focused on game components (dice, cards) rather than the interaction, noting a "weakness in understanding social dynamics and intent."
- Why unresolved: The current architecture prioritizes visual cues and caption-based reasoning, which fails to capture the nuances of human interaction in the provided example.
- What evidence would resolve it: Success on a benchmark specifically designed to test social reasoning, or an architectural extension that weights interaction verbs higher than object nouns.

### Open Question 2
- Question: Is the confidence score threshold of 4 optimal for filtering low-confidence predictions, or does it introduce a fixed bias against valid but uncertain answers?
- Basis in paper: [inferred] The methodology sets a static confidence threshold of 4 (out of 5) to trigger fine-grained reasoning, but provides no sensitivity analysis or calibration metrics for this parameter.
- Why unresolved: The paper demonstrates that the pipeline works but does not clarify if the threshold generalizes across different question types or if it merely acts as a heuristic for this specific dataset.
- What evidence would resolve it: Ablation studies showing performance variance across different threshold values (e.g., 3, 4, 5) and calibration plots of model confidence versus accuracy.

### Open Question 3
- Question: Does the selection of the higher confidence score between vision-based and thought-based reasoning effectively resolve ambiguity when the two modalities contradict?
- Basis in paper: [inferred] The fine-grained reasoning module selects the strategy (Vision vs. Thought) with the higher confidence score, without analyzing cases where these strategies might yield conflicting answers.
- Why unresolved: It is unclear if "max confidence" is the best fusion strategy or if it simply reinforces high-confidence hallucinations from one of the models.
- What evidence would resolve it: An analysis of the "disagreement set" where Vision and Thought strategies differ, evaluating if the selected answer is actually superior to the rejected one.

## Limitations
- Confidence calibration unverified: The approach relies on LLM self-reported confidence scores without external validation of their reliability for egocentric video tasks.
- Dual-path strategy untested: The effectiveness of vision-based vs. thought-based reasoning for low-confidence cases is demonstrated only through internal ablation studies.
- Prompt engineering undisclosed: Specific prompts for eliciting predictions and confidence scores from each LLM are not provided, limiting reproducibility.

## Confidence
- High confidence: The 77% accuracy result on the EgoSchema blind test set is verifiable through the CVPR 2025 Ego4D EgoSchema Challenge leaderboard.
- Medium confidence: The claim that confidence-based filtering improves accuracy relies on the assumption that LLM self-reported confidence correlates with correctness.
- Medium confidence: The dual-path reasoning strategy (vision + thought) shows internal improvement over single-path approaches, but the relative contribution of each path remains unclear.

## Next Checks
1. **Confidence calibration validation**: Create a validation set where you manually verify the correlation between LLM-reported confidence scores and actual accuracy for each model individually and after aggregation.
2. **Threshold sensitivity analysis**: Systematically vary the confidence threshold (3.0, 3.5, 4.0, 4.5, 5.0) on a held-out validation set to quantify the accuracy-latency tradeoff and identify the optimal threshold.
3. **Model diversity measurement**: Compute pairwise error correlations between the three LLMs on a validation set to empirically verify that they have complementary failure modes, and test whether replacing the lowest-diversity pair with a different model family improves performance.