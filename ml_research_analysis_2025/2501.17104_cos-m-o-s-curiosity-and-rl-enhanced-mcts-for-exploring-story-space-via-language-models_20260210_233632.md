---
ver: rpa2
title: 'COS(M+O)S: Curiosity and RL-Enhanced MCTS for Exploring Story Space via Language
  Models'
arxiv_id: '2501.17104'
source_url: https://arxiv.org/abs/2501.17104
tags:
- uni00000013
- uni00000011
- story
- mcts
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of generating novel and engaging
  plots with language models, which often default to formulaic outputs. COS(M+O)S
  introduces a System 2-inspired framework that combines Monte Carlo Tree Search (MCTS)
  with a step-level value model that rewards moderate surprisal and penalizes incoherence,
  further refined by Odds Ratio Preference Optimization (ORPO).
---

# COS(M+O)S: Curiosity and RL-Enhanced MCTS for Exploring Story Space via Language Models

## Quick Facts
- arXiv ID: 2501.17104
- Source URL: https://arxiv.org/abs/2501.17104
- Authors: Tobias Materzok
- Reference count: 40
- Primary result: COS(M+O)S enables a 3B-parameter model to approach 70B-level plot quality via MCTS with curiosity-indexed value model and ORPO fine-tuning

## Executive Summary
COS(M+O)S introduces a System 2-inspired framework combining Monte Carlo Tree Search (MCTS) with a step-level value model that rewards moderate surprisal while penalizing incoherence, refined through Odds Ratio Preference Optimization (ORPO). This iterative approach enables a 3B-parameter model to achieve plot quality approaching that of a 70B model. Human A/B tests showed 67%-77% preference for COS(M+O)S outputs, while GPT-4o ratings placed it 0.59 SD above the 3B baseline and within 0.06 SD of the 70B model. The method demonstrates how structured exploration and policy refinement can overcome the formulaic tendencies of language models in creative tasks.

## Method Summary
COS(M+O)S implements a three-round iterative process. First, MCTS explores story space using a policy model (Llama 3.2 3B) to generate CoT action proposals and a simulation model to expand these into story segments. A step-level value model evaluates partial stories (≥50% completion) using an inverted-U curiosity signal based on token surprisal plus coherence features. Second, Q-values from MCTS guide ORPO fine-tuning, where preference pairs are formed from high-value trajectories. Third, the fine-tuned policy is mixed 50:50 with the base policy for subsequent rounds. The framework uses 18 short-story prompts, with the value model trained on human-labeled and LLM-generated stories normalized to 32 bullet points each.

## Key Results
- Human A/B tests showed 67%-77% preference for COS(M+O)S highest-value expansions over baseline
- GPT-4o ratings placed COS(M+O)S 0.59 SD above the 3B baseline and within 0.06 SD of the 70B model
- Pairwise o1 evaluations found 1.5 SD improvement over baseline
- V_max(final) showed log-linear improvement with iterations: 10-20% quality gains achieved 1.4-1.9× faster with ORPO

## Why This Works (Mechanism)

### Mechanism 1: Inverted-U Curiosity Signal for Branch Pruning
The value model maps token-level surprisal through a Gaussian "interest function" I(i) = e^(-(S(i)-S₀)²/2σ²), centered at S₀=4 bits. This inverted-U rewards moderate surprisal (originality proxy) while penalizing extremes that signal incoherence. MCTS backpropagates V(s) to update Q(s,a), steering exploration away from both boring and chaotic branches. The core assumption is that moderate surprisal correlates with intellectual engagement. Break condition: if optimal surprisal S₀ shifts dramatically for different genres, the fixed Gaussian peak may systematically misvalue certain story types.

### Mechanism 2: Process-Level Credit Assignment via Deferred Evaluation
Step-level value signals at ≥50% story completion enable meaningful MCTS guidance without waiting for full rollouts. V(s) is computed once depth reaches 50% of intended length, allowing partial-branch pruning while avoiding unreliable early scoring. The SVC classifier outputs probability ∈ [0,1] as the value estimate, backpropagating via Q(s,a) ← W(s,a)/N(s,a). Core assumption: mid-story quality predicts final story quality. Break condition: if mid-story quality diverges from final quality (e.g., strong setup with weak resolution), partial evaluations may reinforce dead-end branches.

### Mechanism 3: Policy Internalization via ORPO Preference Tuning
ORPO fine-tuning on MCTS-derived preferences accelerates convergence to high-value trajectories, reducing brute-force search cost. After each MCTS round, Q(s,a) values are thresholded to form (chosen, rejected) action pairs. ORPO loss L_ORPO = L_SFT + β·L_OR increases likelihood of chosen actions while suppressing rejected ones. The fine-tuned policy is mixed 50:50 with base policy to retain diversity. Core assumption: Q-values from MCTS reflect genuine quality. Break condition: if the value model is miscalibrated, ORPO may amplify systematic biases.

## Foundational Learning

- **Monte Carlo Tree Search (MCTS)**: Core search algorithm; understanding UCB selection, expansion, simulation, backpropagation is prerequisite to grasping how COS(M+O)S explores story space. Quick check: Can you explain why UCB balances Q(s,a) with √(ln N(s)/N(s,a)), and what happens if the exploration constant c is set too low?

- **Surprisal and Perplexity**: The curiosity index depends on token-level surprisal S(i) = -log₂ P(i|context); understanding this connects information theory to the value model's engagement proxy. Quick check: If a model assigns P(the) = 0.01 in a given context, what is the surprisal in bits? Would high surprisal for common words likely indicate coherence or incoherence?

- **Preference Optimization (DPO/ORPO)**: ORPO differs from DPO by not requiring a reference model; the odds-ratio formulation L_OR = -log σ(log odds ratio) directly shapes policy without KL constraints. Quick check: How does ORPO's monolithic loss differ from standard RLHF with a separate reward model? What is the computational advantage?

## Architecture Onboarding

- **Component map**: Policy Model (Llama 3.2 3B) -> Simulation Model (same backbone) -> Value Model (SVC classifier) -> MCTS Engine -> ORPO Trainer -> Mixed Policy (50:50 base:fine-tuned)

- **Critical path**: Initial prompt → MCTS expands (κ=300 initially, then 8, then 2) → deferred V(s) evaluation at ≥50% depth → backpropagation updates Q(s,a) → repeat 100 iterations → extract high-Q trajectories → ORPO fine-tuning → mix trained policy 50:50 with base → next round with fresh prompts

- **Design tradeoffs**: 
  - Deferred evaluation threshold (50%): Higher thresholds delay pruning but risk more wasted compute on bad branches
  - Policy mixing ratio (50:50): Higher base-policy ratio preserves diversity but slows convergence
  - Quantization (Q5_K_M): Enables single-GPU operation but may degrade value model calibration

- **Failure signatures**:
  - Reward hacking: Policy generates CoTs with artificially elevated surprisal peaks without genuine narrative coherence
  - Premature convergence: MCTS tree narrows to single trajectory early with no Q improvement
  - Reference drift: Character names or plot elements contradict earlier segments

- **First 3 experiments**:
  1. Validate value model alignment: Run MCTS on 10 held-out prompts, extract highest/lowest V(final) trajectories, run GPT-4o evaluation—confirm ≥0.5 SD separation before proceeding to ORPO
  2. Ablate curiosity index: Set I(i)=1 (uniform interest) and retrain value model—compare F1 scores and GPT-4o ratings to isolate surprisal contribution vs. coherence features
  3. Scale κ and iterations: Run with κ∈{50,150,300} and iterations∈{50,100,200} on same prompts—measure V(final)_max vs. PF-days to characterize log-linear scaling and identify saturation point

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the MCTS+ORPO framework improve plot quality when applied to larger language models (70B+ parameters), or do the gains diminish or plateau at scale?
- Basis in paper: [explicit] "Using far larger models (e.g., 70B+ parameters) would boost story quality and may reveal emergent capabilities, but could also complicate the log-linear improvement pattern."
- Why unresolved: All experiments used a 3B-parameter model due to computational constraints; the scaling behavior is unknown.
- What evidence would resolve it: Run the same MCTS+ORPO pipeline on a 70B backbone and compare convergence speed and absolute quality gains against the 3B baseline.

### Open Question 2
- Question: Can the curiosity-index value model generalize to literary genres that favor low surprisal (e.g., comfort fiction, cozy mysteries), or does it systematically undervalue them?
- Basis in paper: [explicit] "These metrics may undervalue genres favoring lower surprisal (e.g., comfort fiction, cozy stories)."
- Why unresolved: The value model was trained primarily on surprisal and coherence features calibrated for "engaging" plots, not genre-diverse preferences.
- What evidence would resolve it: Evaluate COS(M+O)S outputs across multiple genres with both human readers and the value model, measuring alignment gaps.

### Open Question 3
- Question: Does adding explicit reference tracking or knowledge graphs measurably reduce incoherence and character inconsistencies in longer narratives?
- Basis in paper: [explicit] "A multi-pronged evaluation setup, a reference-tracking system, or a knowledge graph would help prevent such loopholes and maintain consistency."
- Why unresolved: The current framework relies solely on the model's context window and surprisal-based features without structured memory.
- What evidence would resolve it: Implement reference tracking and compare rates of character/state inconsistencies in stories of varying lengths (e.g., 32 vs. 64 vs. 128 bullet points).

## Limitations

- Fixed surprisal peak (S₀=4 bits) may not capture diverse reader preferences across different story genres
- Value model's partial-story evaluation threshold (50%) assumes mid-story quality predicts final quality, which may not hold for all narrative structures
- Study's evaluation on only 18 prompts limits generalizability claims

## Confidence

- **High**: MCTS mechanism for exploration - algorithm properties are well-established with clear implementation details
- **Medium**: Value model's alignment with human preferences - shows intuitive appeal but relies on fixed surprisal parameters that may not generalize across genres
- **Low**: ORPO fine-tuning component's long-term stability - only reports results after two rounds without investigating potential overfitting or policy collapse

## Next Checks

1. **Generalization test**: Apply COS(M+O)S to prompts from different genres (mystery, romance, horror) and evaluate whether the fixed S₀=4 bits curiosity peak remains optimal or requires adaptation.

2. **Long-form evaluation**: Extend story length beyond the current K=32 bullet points and assess whether the 50% evaluation threshold maintains predictive validity for longer narratives.

3. **Robustness to value model noise**: Intentionally misalign the curiosity index (e.g., S₀=1 or S₀=10 bits) and measure degradation in final story quality to quantify sensitivity to this hyperparameter.