---
ver: rpa2
title: 'Learning-To-Measure: In-context Active Feature Acquisition'
arxiv_id: '2510.12624'
source_url: https://arxiv.org/abs/2510.12624
tags:
- features
- tasks
- feature
- acquisition
- missingness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Learning-to-Measure (L2M) addresses active feature acquisition
  (AFA) by learning to acquire features across diverse tasks with systematic missingness
  in retrospective data. L2M uses sequence modeling to quantify uncertainty and guide
  greedy feature selection, operating in-context without per-task retraining.
---

# Learning-To-Measure: In-context Active Feature Acquisition

## Quick Facts
- **arXiv ID**: 2510.12624
- **Source URL**: https://arxiv.org/abs/2510.12624
- **Reference count**: 40
- **Primary result**: L2M matches or surpasses task-specific baselines in active feature acquisition, especially under scarce labels and high missingness.

## Executive Summary
Learning-to-Measure (L2M) is a meta-learning framework for active feature acquisition (AFA) that operates in-context without per-task retraining. It leverages sequence modeling to learn across diverse tasks with systematic missingness in retrospective data, using a differentiable approximation to conditional mutual information for greedy feature selection. The approach combines a predictor for uncertainty estimation with a policy network optimized via Gumbel-Softmax relaxation. Across synthetic and real-world tabular benchmarks, L2M demonstrates robust uncertainty quantification and improved log loss, particularly in low-data, high-missingness settings.

## Method Summary
L2M addresses active feature acquisition by learning to acquire features across diverse tasks with systematic missingness in retrospective data. It uses sequence modeling to quantify uncertainty and guide greedy feature selection, operating in-context without per-task retraining. The approach combines a predictor for uncertainty estimation with a policy network optimized via a differentiable approximation to conditional mutual information. L2M learns from retrospective missingness by assuming Missing At Random (MAR) conditions and positivity, enabling identifiability of the conditional mutual information objective. The transformer-based architecture processes variable-length contexts of historical samples and uses a custom attention mask to satisfy permutation invariance.

## Key Results
- L2M matches or surpasses task-specific baselines across synthetic and real-world tabular benchmarks
- Largest performance gains observed in low-data, high-missingness settings
- Robust uncertainty quantification demonstrated through improved log loss and coverage calibration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequence models trained autoregressively on diverse tasks approximate Bayesian inference for uncertainty quantification under missingness.
- Mechanism: The transformer conditions on a variable-length context and infers task-specific mechanisms through autoregressive factorization with permutation equivariant architectures. This amortizes uncertainty estimation across partially observed queries without explicit latent variable modeling.
- Core assumption: Conditional independence across query points given context and inputs; pretraining task distribution provides sufficient coverage of downstream mechanisms.
- Evidence anchors: Abstract mentions sequence-modeling underpins reliable uncertainty quantification; section 4.1 describes autoregressive decomposition; neighboring papers discuss AFA but not this sequence-modeling-as-inference connection.
- Break condition: If pretraining tasks poorly match downstream tasks, or if context is insufficient to disambiguate task identity, uncertainty estimates will be miscalibrated.

### Mechanism 2
- Claim: A smooth, differentiable surrogate loss approximates the greedy CMI objective, enabling end-to-end policy optimization.
- Mechanism: Rather than computing CMI directly, L2M optimizes the one-step-ahead predictive loss after acquiring a candidate feature. With a Bayes-optimal predictor, minimizing this surrogate is equivalent to selecting the CMI-maximizing action. Gumbel-Softmax relaxation enables gradient flow through discrete action sampling.
- Core assumption: The predictor is Bayes-optimal; blocked policy constraints hold (only features with retrospective support are selected).
- Evidence anchors: Abstract mentions differentiable approximation to CMI; section 4.2 describes optimizing one-step-ahead predictive loss; GDFS uses similar surrogate loss but per-task.
- Break condition: If the predictor is undertrained or misspecified, policy gradients will follow incorrect reward signals. High Gumbel-Softmax temperature introduces bias; low temperature increases variance.

### Mechanism 3
- Claim: Identifiability conditions (MAR, exclusion restriction, positivity) enable CMI estimation from retrospective data with systematic missingness.
- Mechanism: Under MAR, exclusion restriction, and positivity, the CMI computed from complete cases equals the CMI under the full joint. This allows learning from real-world data without imputation bias.
- Core assumption: Missingness mechanism satisfies MAR; sufficient coverage of acquisition actions in historical data.
- Evidence anchors: Abstract mentions L2M operates directly on datasets with retrospective missingness; section 3.1 formalizes identifiability conditions; neighboring papers discuss retrospective missingness but not formal identifiability.
- Break condition: If missingness is Not Missing At Random (NMAR), or if positivity is violated, CMI estimates will be biased.

## Foundational Learning

- **Concept**: **Conditional Mutual Information (CMI)**
  - Why needed here: CMI quantifies the expected reduction in uncertainty about Y from acquiring feature X_j given current observations. It is the theoretical objective L2M approximates.
  - Quick check question: Given current observations X_t, which feature X_j maximizes I(Y; X_j | X_t)?

- **Concept**: **Missing Data Mechanisms (MAR, MCAR, NMAR)**
  - Why needed here: The identifiability of CMI from retrospective data depends on the missingness mechanism. L2M assumes MAR for theoretical guarantees.
  - Quick check question: If feature X_j is missing more often for high-risk patients (who are also sicker on Y), does this violate MAR?

- **Concept**: **Gumbel-Softmax Reparameterization**
  - Why needed here: Enables backpropagation through discrete action sampling. The temperature τ controls the tradeoff between bias (high τ) and variance (low τ).
  - Quick check question: What happens to gradient estimates as τ → 0? As τ → ∞?

## Architecture Onboarding

- **Component map**: [Transformer encoder] -> [Predictor head] -> [Policy head] -> [Greedy action selection]

- **Critical path**:
  1. **Stage 1**: Pretrain predictor f_ϕ on random feature subsets across tasks (Algorithm 1)
  2. **Stage 2**: Jointly train policy π_θ with frozen or jointly-updated predictor using Gumbel-Softmax (Algorithm 2)
  3. **Stage 3**: Inference: For new task, feed context + query; policy outputs greedy action; repeat for k steps; predictor outputs final label (Algorithm 3)

- **Design tradeoffs**:
  - **Blocked policy vs. imputation**: Blocked policy avoids generative modeling of missing outcomes but cannot select never-observed features
  - **Greedy vs. RL**: Greedy is tractable and near-optimal under submodularity, but may miss multi-step optimal policies
  - **Temperature scheduling**: Fixed low temperature (0.1 in experiments) for policy; higher temperature (0.5) used in GDFS baseline

- **Failure signatures**:
  - Deteriorating performance at later acquisition steps → likely insufficient joint coverage in retrospective data
  - Policy collapses to single action → check positivity violations or insufficient task diversity in pretraining
  - Poor calibration on coverage plots → predictor undertrained or task prior misaligned

- **First 3 experiments**:
  1. **Sanity check on synthetic GP tasks**: Train on RBF kernel tasks, evaluate on held-out RBF tasks. Verify log loss decreases monotonically with acquisitions and coverage tracks diagonal.
  2. **Ablation on context length**: Vary m (number of context samples) from 50 to 900 on GP and MIMIC tasks. Expect L2M to maintain performance at low m while task-specific baselines degrade.
  3. **Missingness robustness test**: Introduce synthetic MAR missingness at rates [0%, 25%, 50%, 75%] and evaluate ΔNLL relative to GDFS baseline.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the model's uncertainty estimates serve as reliable diagnostics for violations of positivity or MAR assumptions? The authors plan to investigate whether uncertainty estimates can serve as informative bounds for assumption violations.
- **Open Question 2**: How can the framework be extended to handle time-varying feature dynamics? The authors note they restrict attention to time-invariant settings and extending to time-varying dynamics is crucial future work.
- **Open Question 3**: How does the method perform when scaling pre-training to diverse, large-scale real-world domains? The authors note empirical utility was demonstrated with simple synthetic or real task priors and scaling to diverse real datasets is deferred to future work.

## Limitations

- The work assumes MAR missingness and sufficient retrospective coverage for identifiability, yet real-world datasets often violate these conditions
- The Gumbel-Softmax relaxation introduces bias that is not fully characterized
- Empirical scope is limited to small-scale tabular benchmarks; scalability to high-dimensional or unstructured data remains untested

## Confidence

- **High**: Theoretical identifiability results under MAR, Gumbel-Softmax gradient approximation (Theorem 4.2), and empirical improvements in log loss over baselines
- **Medium**: Claims about robustness to scarce labels and missingness, due to limited ablation on diverse data regimes
- **Low**: Generalization to non-tabular data and claims about practical deployment without per-task retraining

## Next Checks

1. Test L2M on datasets with controlled NMAR missingness to quantify robustness violations of identifiability assumptions
2. Perform an ablation study varying Gumbel-Softmax temperature schedules to assess bias-variance tradeoffs in policy optimization
3. Scale experiments to larger feature spaces (e.g., clinical time-series) to evaluate computational feasibility and performance retention