---
ver: rpa2
title: 'VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery'
arxiv_id: '2509.17191'
source_url: https://arxiv.org/abs/2509.17191
tags:
- arxiv
- visual
- question
- reasoning
- link
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VaseVQA, a benchmark of 31,773 images and
  67,614 question-answer pairs for expert-level reasoning about ancient Greek pottery.
  It addresses the challenge that current MLLMs struggle with domain-specific cultural
  heritage artifacts due to limited specialized data.
---

# VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery

## Quick Facts
- arXiv ID: 2509.17191
- Source URL: https://arxiv.org/abs/2509.17191
- Reference count: 40
- Primary result: Introduced benchmark of 31,773 images and 67,614 QA pairs; proposed VaseVL model achieving 75.71% accuracy vs 14.10% for best general model

## Executive Summary
VaseVQA addresses the challenge of applying multimodal large language models (MLLMs) to specialized cultural heritage domains, specifically ancient Greek pottery. The work introduces a new benchmark and evaluation framework consisting of 31,773 images and 67,614 question-answer pairs covering various reasoning tasks about Athenian black and red-figure pottery. The benchmark is designed to test expert-level reasoning capabilities that current general-purpose MLLMs struggle with due to limited domain-specific training data. To address this gap, the authors develop VaseVL, a two-stage training approach that combines supervised fine-tuning with reinforcement learning using taxonomy-aware reward shaping.

## Method Summary
The approach employs a two-stage training methodology for the VaseVL model. First, supervised fine-tuning (SFT) is performed on a curated dataset of ancient Greek pottery images and expert-annotated question-answer pairs. This is followed by reinforcement learning fine-tuning using a custom reward function that incorporates domain-specific taxonomies for attributes like shape, period, painter, and scene type. The reward shaping mechanism is designed to guide the model toward expert-level reasoning patterns rather than generic image descriptions. The training leverages existing vision-language models as base architectures, fine-tuning them on the specialized vase dataset with careful attention to preserving multimodal capabilities while adapting to the cultural heritage domain.

## Key Results
- VaseVL achieves 75.71% overall accuracy on the VaseVQA benchmark, significantly outperforming the best general-purpose MLLM at 14.10%
- On attribution tasks, VaseVL reaches 60.83% accuracy compared to 2.92% for general models
- For decoration description tasks, VaseVL achieves 9.82 BLEU-1 score versus 6.53 for baseline models
- The two-stage training approach (SFT + RL) shows marked improvements over SFT-only baselines across all evaluation metrics

## Why This Works (Mechanism)
The approach succeeds by explicitly incorporating domain expertise into both the training data and the reward function design. By leveraging expert-annotated question-answer pairs and taxonomy-aware reward shaping, the model learns to reason about cultural heritage artifacts using the same conceptual frameworks that human experts employ. The reinforcement learning phase is particularly crucial as it allows the model to refine its responses based on domain-specific evaluation criteria rather than generic language modeling objectives.

## Foundational Learning
- **Multimodal Large Language Models (MLLMs)**: Foundation for integrating visual and textual understanding; needed for reasoning about both image content and cultural context
- **Reinforcement Learning with Human Feedback (RLHF)**: Enables fine-tuning based on domain-specific reward signals; required to align model outputs with expert reasoning patterns
- **Cultural Heritage Taxonomies**: Classification systems for pottery attributes; essential for structuring domain knowledge and evaluation criteria
- **Visual Grounding**: Connecting textual descriptions to specific visual elements; critical for accurate artifact analysis
- **Cross-modal Reasoning**: Integrating visual and textual information for complex inferences; necessary for expert-level attribution and analysis

## Architecture Onboarding

**Component Map**: Vision Encoder -> Multimodal Transformer -> Text Decoder -> Reward Evaluator

**Critical Path**: Image input → Visual feature extraction → Multimodal fusion → Reasoning generation → Taxonomy-aware evaluation

**Design Tradeoffs**: The model balances between general-purpose vision-language capabilities and domain-specific expertise. The two-stage training approach trades additional computational resources for significantly improved performance on specialized tasks. The taxonomy-aware reward function adds complexity but enables more nuanced evaluation compared to standard accuracy metrics.

**Failure Signatures**: The model may struggle with artifacts outside its training distribution, particularly pottery from non-Athenian traditions. It may also produce overly generic descriptions when faced with damaged or incomplete artifacts, and could exhibit bias toward attributes that are overrepresented in the training data.

**First Experiments**:
1. Evaluate model performance on held-out examples from underrepresented pottery types
2. Test sensitivity to varying levels of artifact damage or incompleteness
3. Compare reward function effectiveness across different task categories

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Training data remains relatively modest at 17K examples compared to large-scale MLLM pretraining datasets
- Benchmark construction may introduce biases through reliance on specific online databases and question templates
- Evaluation metrics may not fully capture expert-level reasoning quality, particularly for complex attribution tasks
- Limited testing on pottery traditions beyond Athenian black and red-figure ware raises questions about generalizability

## Confidence
**High Confidence (95%+):** The empirical demonstration that general-purpose MLLMs perform poorly on domain-specific cultural heritage tasks is well-supported by experimental results.

**Medium Confidence (75-95%):** The effectiveness of the two-stage training approach (SFT + RL) is reasonably established, though specific component contributions could benefit from more granular ablation studies.

**Low Confidence (50-75%):** Claims about generalization to unseen artifacts or handling diverse pottery traditions beyond Athenian ware are not thoroughly tested.

## Next Checks
1. **Cross-domain Transferability Test:** Evaluate VaseVL's performance on pottery from different cultural traditions (e.g., Corinthian, Ionian) and time periods not represented in training data.

2. **Expert Human Evaluation:** Conduct blind evaluations with classical archaeology experts comparing VaseVL outputs against human expert annotations on a separate test set.

3. **Ablation Study on Training Components:** Systematically isolate and measure the contribution of supervised fine-tuning versus reinforcement learning with taxonomy-aware rewards.