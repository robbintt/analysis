---
ver: rpa2
title: Multi-modal Knowledge Graph Generation with Semantics-enriched Prompts
arxiv_id: '2504.13631'
source_url: https://arxiv.org/abs/2504.13631
tags:
- images
- knowledge
- entity
- image
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes VSNS, a neighbor selection method to improve
  image generation for MMKG construction. VSNS filters visually descriptive relations
  (VNS) and selects structurally relevant neighbors (SNS) for entities, generating
  enriched prompts for image generation.
---

# Multi-modal Knowledge Graph Generation with Semantics-enriched Prompts

## Quick Facts
- arXiv ID: 2504.13631
- Source URL: https://arxiv.org/abs/2504.13631
- Reference count: 27
- Primary result: VSNS improves image generation for MMKG construction, achieving lower FID (267.7 vs 275.7) and higher CLIPscore (0.666 vs 0.659) on MKG-Y and DB15K datasets.

## Executive Summary
This paper introduces VSNS, a neighbor selection method to enhance image generation for Multi-modal Knowledge Graph (MMKG) construction. VSNS combines Visualizable Neighbor Selection (VNS) to filter non-visual relations and Structural Neighbor Selection (SNS) to identify structurally relevant neighbors, generating enriched prompts for image generation. The method outperforms baselines on MKG-Y and DB15K datasets, with lower FID and higher CLIPscore. Human evaluation confirms improved image quality and relevance, and generated images also enhance MMKGC tasks.

## Method Summary
VSNS processes KG triples through three main stages: (1) Visualizable Neighbor Selection (VNS) filters relations by sampling triples, generating images, and scoring alignment with ImageReward, keeping relations with scores above 0.5; (2) Structural Neighbor Selection (SNS) uses CompGCN to embed entities and relations, selecting neighbors based on cosine similarity; (3) Semantics-enriched prompts are generated via an LLM and used with Stable Diffusion 2.0-base to create entity images. The method is evaluated on MKG-Y and DB15K datasets, focusing on image generation quality and downstream MMKGC task performance.

## Key Results
- VSNS achieves lower FID (267.7 vs 275.7) and higher CLIPscore (0.666 vs 0.659) compared to baselines on MKG-Y and DB15K.
- Human evaluation shows better image quality, entity relevance, and KG relevance for VSNS-generated images.
- Generated images from VSNS improve entity prediction and relation prediction in MMKGC tasks.

## Why This Works (Mechanism)

### Mechanism 1: Visualizability-based Relation Filtering (VNS)
- **Claim:** Filtering relations based on their "visualizability" score prevents noisy, abstract triples from degrading image generation quality.
- **Mechanism:** The VNS module evaluates a relation $r$ by calculating an average score using ImageReward on sampled triples (text-to-image alignment). If the score exceeds a threshold $\mu$, the relation is kept; otherwise, it is discarded before neighbor selection.
- **Core assumption:** Relations that score poorly on text-to-image alignment in a sample set will consistently produce non-visual or misleading context for specific entities.
- **Evidence anchors:**
  - [abstract]: "The VNS module filters relations that are difficult to visualize."
  - [section IV.A.1]: Describes Eq. (1) where $r_{vis}$ is calculated using ImageReward to filter relations.
  - [corpus]: Weak direct evidence; neighbor papers focus on multimodal alignment/completion rather than specific relation filtering for generation.
- **Break condition:** If a relation is abstract (e.g., "influences") but critical for the entity's visual identity (e.g., an artist known for a style), this filtering might accidentally remove structural context.

### Mechanism 2: Structural Neighbor Relevance (SNS)
- **Claim:** Selecting neighbors based on structural similarity (via graph embeddings) captures the most representative features of an entity better than naive methods like token length.
- **Mechanism:** The SNS module uses CompGCN to embed entities and relations. It calculates cosine similarity between the entity embedding $e_h$ and the combined neighbor embedding $e_{(r,t)}$. Only neighbors exceeding the average similarity are kept.
- **Core assumption:** Structural proximity in the embedding space correlates with semantic relevance for visual depiction.
- **Evidence anchors:**
  - [abstract]: "[SNS] selects neighbors that most effectively capture the structural characteristics."
  - [section IV.A.2]: Details the use of CompGCN and cosine similarity (Eq. 4) to select neighbors.
  - [corpus]: [103053] (HERGC) supports the general principle that exploiting heterogeneous structural signals aids multimodal tasks, though not specific to this generation method.
- **Break condition:** If the graph embedding (CompGCN) is poorly trained or the graph is sparse, structural similarity may not reflect visual relevance.

### Mechanism 3: LLM-driven Semantic Enrichment
- **Claim:** Large Language Models (LLMs) act as a "semantic refiner," transforming structured triples into descriptive natural language while correcting potential KG noise.
- **Mechanism:** Selected neighbors are formatted as triples and input into an LLM (e.g., ChatGPT) with specific instructions to generate a description. The LLM infers additional context (e.g., "British actor") and filters logical inconsistencies.
- **Core assumption:** The LLM possesses external knowledge about the entity that is not present in the KG triples, and can synthesize this into a coherent visual prompt.
- **Evidence anchors:**
  - [section IV.B]: "This process allows us to distill knowledge from the LLM... verifying the accuracy of neighbor information."
  - [Figure 3]: Shows the LLM adding "a British actor" to the description of "Julian Glover" despite it not being in the input triples.
  - [corpus]: [10951] highlights the sensitivity of RAG/LLM performance to context quality, indirectly supporting the need for high-quality prompt generation.
- **Break condition:** If the LLM hallucinates visual details that contradict the entity's actual appearance in the KG context.

## Foundational Learning

- **Concept: Knowledge Graph Embeddings (CompGCN)**
  - **Why needed here:** The SNS module relies entirely on vector embeddings to determine which neighbors are structurally significant. Without understanding how CompGCN combines entity and relation vectors, one cannot debug the selection logic.
  - **Quick check question:** How does the composition operator in CompGCN differ from a simple sum of entity and relation embeddings?

- **Concept: Text-to-Image Alignment (CLIP/ImageReward)**
  - **Why needed here:** The entire VNS module and the final evaluation depend on scoring how well text aligns with images. You must understand that these models project text and images into a shared latent space to measure cosine similarity.
  - **Quick check question:** Why might a high CLIPscore not always guarantee "correct" entity generation (e.g., wrong person but similar attributes)?

- **Concept: Diffusion Models (Stable Diffusion)**
  - **Why needed here:** This is the generation engine. Understanding the role of the prompt as a conditioning signal is crucial for engineering the "Semantics-enriched Prompts."
  - **Quick check question:** How does the classifier-free guidance scale affect the trade-off between image diversity and adherence to the text prompt?

## Architecture Onboarding

- **Component map:** Input KG Triples -> VNS Filter (samples triples -> calculates ImageReward score -> filters Relations) -> SNS Filter (CompGCN Embeddings -> Cosine Similarity -> filters Neighbors) -> Prompt Generator (LLM + Template -> Text Description) -> Image Generator (Stable Diffusion -> Output Image).

- **Critical path:** The VNS -> SNS -> Prompt pipeline. If VNS is too aggressive, SNS has no data. If SNS selects irrelevant neighbors, the LLM hallucinates context, and Stable Diffusion generates an off-target image.

- **Design tradeoffs:**
  - **VNS Threshold ($\mu$):** Set too high (>0.5), and you lose potentially useful abstract context; set too low, and noise enters the prompt.
  - **LLM vs. Template:** Using an LLM for prompt generation adds latency and cost but significantly boosts semantic richness (as shown in Figure 3).
  - **Evaluation limitation:** Relying on FID/CLIPscore for tuning is imperfect; human evaluation is required for true entity relevance (CIE).

- **Failure signatures:**
  - **"Generic" Generation:** Images look like the category (e.g., "actor") but not the specific entity. *Likely cause:* SNS failed to select distinctive neighbors, or VNS filtered them out.
  - **Anatomical Distortion:** Missing limbs/blurring. *Likely cause:* Stable Diffusion limitations, potentially exacerbated by complex prompts (noted in [section V.D] human evaluation).
  - **Hallucination:** Image contains elements not in the KG. *Likely cause:* LLM "distilled knowledge" introduced facts not grounded in the graph.

- **First 3 experiments:**
  1. **VNS Threshold Sweep:** Run VNS on a validation slice with varying $\mu$ (e.g., 0.3 to 0.7) and measure the drop in unique relations. Target the "elbow" where visualizability stabilizes.
  2. **SNS Embedding Quality Check:** Visualize the selected neighbors for 10 diverse entities using t-SNE plots of their CompGCN embeddings. Verify that selected neighbors cluster near the target entity.
  3. **Prompt Ablation:** Generate images using (A) raw triples vs. (B) LLM-enriched prompts. Compare CLIPscore specifically for "entity relevance" to quantify the LLM's contribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework be adapted to generate meaningful visual representations for abstract entities, such as emotions or events, which lack concrete physical forms?
- Basis in paper: [explicit] The authors explicitly identify "handling abstract entities (e.g., emotions, events)" as a current limitation in the Conclusion.
- Why unresolved: The Visualizable Neighbor Selection (VNS) module relies on scoring visualizability based on human preferences for concrete concepts, which may not apply to abstract terms.
- What evidence would resolve it: Successful application of a modified VNS on a KG of abstract concepts, validated through human evaluation of semantic relevance.

### Open Question 2
- Question: Does the integration of VSNS-generated images improve performance in downstream tasks beyond link prediction, such as multi-modal question answering or recommendation systems?
- Basis in paper: [explicit] The paper states that downstream performance for tasks like "QA" and "recommendations" remains "unvalidated."
- Why unresolved: The current evaluation focuses strictly on Multi-modal Knowledge Graph Completion (MMKGC) and direct image quality metrics (FID/CLIPscore).
- What evidence would resolve it: Benchmarking a VSNS-enriched MMKG on standard multi-modal QA or recommendation datasets to measure task-specific accuracy gains.

### Open Question 3
- Question: Can the Visualizable Neighbor Selection (VNS) module be refined to assess visualizability at the triple level rather than the relation level?
- Basis in paper: [inferred] The VNS calculates a visualizability score per relation $r$ using sampled triples, potentially ignoring context-specific variations where a relation is only sometimes visualizable.
- Why unresolved: A relation like "has part" might be visualizable for machines but not for organizations; a relation-level threshold may filter valid visual context or retain noise.
- What evidence would resolve it: Ablation studies showing improved CLIPscores when filtering is applied based on specific $(h, r, t)$ semantics rather than $r$ alone.

## Limitations

- The VNS threshold (0.5) may not generalize across datasets or entity types, and the paper does not address how this was chosen or validated.
- Evaluation focuses on image generation and MMKGC tasks, leaving downstream performance on QA or recommendation systems unvalidated.
- Computational cost and scalability are not reported, making real-world deployment feasibility unclear.

## Confidence

**Low:** On the long-term robustness of VNS filtering. The method relies on a static threshold (0.5) determined on a sample of 10 triples per relation. This may not generalize to relations with sparse or highly contextual data. The paper does not address how the threshold was chosen or whether it was validated across diverse entity types.

**Medium:** On the claim that VSNS improves MMKGC tasks. While the paper reports improvements in entity prediction and relation prediction metrics, these results are based on MKG-Y only. The DB15K results focus solely on image generation metrics, leaving a gap in validation across tasks and datasets.

**Medium:** On the scalability of the approach. The method requires multiple forward passes (VNS sampling, SNS embedding, LLM prompting, image generation) for each entity. The paper does not report computational costs or timing, making it difficult to assess real-world deployment feasibility.

## Next Checks

1. **Cross-dataset Validation:** Apply VSNS to a third KG dataset (e.g., YAGO or Freebase) to test whether the 0.5 threshold generalizes or requires dataset-specific tuning.

2. **Failure Mode Analysis:** Systematically generate images for entities known to cause SD 2.0 to fail (e.g., "a person riding a bicycle") and measure whether VSNS mitigates or exacerbates anatomical distortions compared to baselines.

3. **Ablation on VNS Threshold:** Create a sweep of VNS thresholds (0.3, 0.4, 0.5, 0.6, 0.7) and plot the trade-off between the number of relations retained and the final CLIPscore/FID. Identify the point where additional filtering stops providing benefits.