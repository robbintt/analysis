---
ver: rpa2
title: 'AudioRole: An Audio Dataset for Character Role-Playing in Large Language Models'
arxiv_id: '2509.23435'
source_url: https://arxiv.org/abs/2509.23435
tags:
- audio
- speaker
- role-playing
- acoustic
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces AudioRole, a dataset and framework for audio-grounded
  character role-playing in large language models. The core challenge addressed is
  enabling AI to not only mimic character-specific dialogue content but also replicate
  unique vocal characteristics like speech patterns and emotional tone.
---

# AudioRole: An Audio Dataset for Character Role-Playing in Large Language Models

## Quick Facts
- arXiv ID: 2509.23435
- Source URL: https://arxiv.org/abs/2509.23435
- Authors: Wenyu Li; Xiaoqi Jiao; Yi Chang; Guangyan Zhang; Yiwen Guo
- Reference count: 11
- Primary result: ARP-Model significantly outperforms baselines in both acoustic and content personalization metrics for character role-playing

## Executive Summary
AudioRole introduces a novel dataset and framework for audio-grounded character role-playing in large language models. The work addresses the challenge of enabling AI to replicate not just character dialogue content but also unique vocal characteristics like speech patterns and emotional tone. By providing over 1 million audio-text pairs from 13 TV series and training a specialized model (ARP-Model) through semantic-acoustic decoupling, the authors demonstrate significant improvements in both acoustic and content personalization metrics compared to baseline models like GPT-4o-Audio.

## Method Summary
The AudioRole dataset is constructed from TV series dialogue using speaker diarization to isolate per-character audio segments, followed by transcription and postprocessing to create audio-text pairs. The ARP-Model architecture decouples training into two stages: GLM-4-Voice-9B is fine-tuned on formatted text-audio token pairs to learn character-specific dialogue patterns, while GLM-4-Voice-Decoder is trained from scratch on extracted character audio to capture vocal identity. The model is evaluated using a four-metric framework (AQ, CQ, AP, CP) that separately assesses acoustic quality, content quality, acoustic personalization, and content personalization.

## Key Results
- ARP-Model achieves Acoustic Personalization score of 0.31 versus baseline 0.02
- ARP-Model achieves Content Personalization score of 0.36 versus baseline 0.24
- Character with more training data (Sheldon: 12.5h) shows significantly higher AP (0.42) than character with less data (Tyrion: 1.34h, AP=0.25)

## Why This Works (Mechanism)

### Mechanism 1
Separate fine-tuning of semantic and acoustic components enables synchronized character alignment without catastrophic forgetting. The ARP-Model decouples training into two stages: (1) GLM-4-Voice-9B is fine-tuned on formatted text-audio token pairs to learn character-specific dialogue patterns; (2) GLM-4-Voice-Decoder is trained from scratch on extracted character audio as "high-quality unsupervised speech data from a single speaker" to capture vocal identity. The components are then recombined.

### Mechanism 2
Character-specific training data quantity directly correlates with acoustic personalization performance. The diarization pipeline isolates individual speakers from TV audio, creating per-character datasets. Fine-tuning on these datasets injects vocal traits (speaking rate, pitch patterns, emotional tone) into the model's decoder.

### Mechanism 3
Four-metric evaluation (AQ/CQ/AP/CP) captures the dual-alignment requirement better than single-metric approaches. ARP-Eval decomposes evaluation into: Acoustic Quality (signal properties), Content Quality (semantic appropriateness), Acoustic Personalization (voice similarity via cosine distance on embeddings), Content Personalization (style consistency via GPT-4o-audio).

## Foundational Learning

- Concept: **Speaker Diarization**
  - Why needed here: The dataset construction requires separating mixed TV audio into per-character clips before any training can occur.
  - Quick check question: Given a 5-minute clip with 3 speakers, can you explain how diarization produces speaker-labeled segments?

- Concept: **End-to-End Speech Models (e.g., GLM-4-Voice architecture)**
  - Why needed here: ARP-Model builds on GLM-4-Voice's tokenizer→LLM→decoder pipeline; understanding tokenization is critical for debugging.
  - Quick check question: How does discrete audio tokenization differ from continuous spectrogram features in terms of gradient flow?

- Concept: **Embedding Similarity for Voice Verification**
  - Why needed here: AP metric uses cosine distance on speaker embeddings; you must understand what these embeddings encode.
  - Quick check question: Would two recordings of the same person with different emotional states have high or low cosine similarity in speaker embeddings?

## Architecture Onboarding

- Component map:
Raw TV Audio → [Speaker Diarization] → Per-Speaker Segments → [Dialogue Scene Extraction] → [Postprocessing: Filter/Transcribe/Enhance] → AudioRole Dataset (Audio-Text Pairs) → [GLM-4-Voice-9B] → [GLM-4-Voice-Decoder] → [ARP-Model] → Audio Output → [ARP-Eval] → AQ/CQ/AP/CP Scores

- Critical path: Diarization accuracy → Character data quality → Decoder vocal learning → AP scores. Errors in early stages compound.

- Design tradeoffs:
  - Audio quality vs. personalization: Fine-tuning degrades AQ (7.6→6.5) but dramatically improves AP (0.05→0.31)
  - Data quantity vs. diversity: More character audio improves AP but may overfit to specific contexts
  - Single-turn vs. multi-turn evaluation: Current framework only tests single-turn; multi-turn consistency is unmeasured

- Failure signatures:
  - Low AP with high AQ: Model generates clean audio but wrong voice → Decoder not fine-tuned or insufficient character data
  - High CQ but low CP: Content is accurate but style is generic → 9B model fine-tuning failed or training data lacks style markers
  - High AP but low CQ: Voice sounds right but content is incoherent → Decoder trained but 9B model not fine-tuned

- First 3 experiments:
  1. Reproduce diarization on a single episode: Verify speaker segmentation accuracy by manually checking RTTM outputs against ground truth subtitles.
  2. Train decoder only (freeze 9B): Isolate acoustic learning by training only the decoder on one character; expect high AP, unchanged CP.
  3. Ablation on postprocessing: Train with/without acoustic enhancement (DeepFilterNet) to quantify impact on AQ vs. AP tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
How can evaluation frameworks effectively assess a model's ability to maintain consistent role embodiment across extended, multi-turn conversational exchanges? The current framework only assesses single-turn responses, excluding critical evaluation of the model's ability to maintain consistent role embodiment across extended, multi-turn conversational exchanges.

### Open Question 2
What methodologies can mitigate speaker attribution errors in scenes with conversational overlap or non-stationary environmental noise? Despite diarization efforts, speaker attribution errors continue to occur specifically in scenes with conversational overlap and where non-stationary environmental noise can still compromise precise voiceprint extraction.

### Open Question 3
Can the trade-off between Acoustic Quality (AQ) and Acoustic Personalization (AP) be resolved to prevent audio degradation during character fine-tuning? The authors identify an "Acoustic Content Tradeoff" where the ARP-Model lowers AQ (6.5) compared to the base model (7.6) due to conflicts between character patterns and default prosodic templates.

## Limitations

- Semantic-acoustic decoupling mechanism lacks ablation studies proving that separate training is necessary or optimal
- Character voice consistency across TV episodes is assumed but not empirically validated
- Four-metric evaluation framework conflates multiple factors, with AP scores heavily dependent on diarization accuracy

## Confidence

- **High confidence**: The dataset construction pipeline is reproducible and follows established practices; general improvement over baselines is clearly demonstrated
- **Medium confidence**: The quantity-performance correlation is supported by Sheldon vs. Tyrion comparison, but lacks systematic ablation across multiple characters
- **Low confidence**: Mechanism claims about why separate semantic-acoustic training works are speculative; no experiments test whether joint training would perform worse

## Next Checks

1. Run ablation on diarization accuracy: Manually annotate speaker turns in 3 test episodes and measure how pyannote-3.1 errors correlate with final AP scores.
2. Test semantic-acoustic coupling: Train a joint model (fine-tune entire ARP-Model end-to-end) and compare against the decoupled approach on all four metrics.
3. Validate voice consistency: Measure intra-character cosine similarity across different seasons/episodes to quantify how much character voices actually vary in the source data.