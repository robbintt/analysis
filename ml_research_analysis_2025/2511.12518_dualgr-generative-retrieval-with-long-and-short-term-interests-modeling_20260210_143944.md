---
ver: rpa2
title: 'DualGR: Generative Retrieval with Long and Short-Term Interests Modeling'
arxiv_id: '2511.12518'
source_url: https://arxiv.org/abs/2511.12518
tags:
- retrieval
- long
- generative
- short-term
- dualgr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DualGR, a generative retrieval method for short-video
  recommendation systems that explicitly models long-term and short-term user interests
  while addressing exposure bias. The method uses a Dual-Branch Long/Short-Term Router
  (DBR) to decompose user history into two windows, Search-based SID Decoding (S2D)
  to constrain fine-grained decoding within relevant coarse buckets, and an Exposure-aware
  Next-Token Prediction Loss (ENTP-Loss) to penalize unclicked exposures.
---

# DualGR: Generative Retrieval with Long and Short-Term Interests Modeling

## Quick Facts
- arXiv ID: 2511.12518
- Source URL: https://arxiv.org/abs/2511.12518
- Reference count: 14
- Key outcome: DualGR achieves +0.527% video views and +0.432% watch time lift in online A/B testing compared to strong baselines

## Executive Summary
DualGR is a generative retrieval method for short-video recommendation systems that explicitly models long-term and short-term user interests while addressing exposure bias. The method uses a Dual-Branch Long/Short-Term Router (DBR) to decompose user history into two windows, Search-based SID Decoding (S2D) to constrain fine-grained decoding within relevant coarse buckets, and an Exposure-aware Next-Token Prediction Loss (ENTP-Loss) to penalize unclicked exposures. Experiments on Kuaishou's short-video recommendation system with hundreds of millions of users show DualGR achieves +0.527% video views and +0.432% watch time lift in online A/B testing compared to strong baselines, demonstrating its effectiveness as a practical paradigm for industrial generative retrieval.

## Method Summary
DualGR is an encoder-free generative retrieval framework for short-video recommendation that models user history through dual temporal windows and hierarchical Semantic ID (SID) quantization. The method pre-quantizes videos into 3-level SIDs using RQ-KMeans, then uses a 4-layer Transformer decoder to autoregressively predict SIDs conditioned on user features and action history. A Dual-Branch Long/Short-Term Router (DBR) separates history into long (1000 actions) and short (64 actions) windows with selective routing based on target-aware similarity. Search-based SID Decoding (S2D) constrains fine-level prediction to actions within the same coarse bucket for efficiency, while Exposure-aware Next-Token Prediction Loss (ENTP-Loss) treats unclicked exposures as coarse-level hard negatives. At inference, both branches run independently with beam search, merge SID sets, deduplicate, and map to video IDs.

## Key Results
- Online A/B testing shows +0.527% video views and +0.432% watch time lift compared to strong baselines
- Offline HR@100/500/1000 metrics demonstrate consistent improvements over ablated variants
- Sensitivity analysis confirms optimal L_short=64 and α=0.1 for the Kuaishou setting

## Why This Works (Mechanism)

### Mechanism 1: Dual-Branch Long/Short-Term Router (DBR)
- **Claim**: Separating user history into long-term and short-term branches with selective routing prevents interest dilution and improves retrieval quality in double-column feeds that demand both stable preference coverage and rapid intent tracking.
- **Mechanism**: DBR defines two windows (H_long with L_long=1000 actions and H_short with L_short=64 actions), computes target-aware similarities via cosine distance between the ground-truth level-1 embedding and pooled summary embeddings, and applies a hard gate to select the more relevant window during training. At inference, both branches run independently and their candidate sets are merged and deduplicated to cover both temporal scales.
- **Core assumption**: User interests exhibit distinct temporal scales—stable preferences persist over longer histories while transient intents emerge in recent actions—and these require explicit separation rather than passive emergence through multi-beam decoding. The hard gate assumes the target's coarse category is predominantly aligned with one temporal scale.
- **Evidence anchors**:
  - [abstract]: "Dual-Branch Long/Short-Term Router (DBR) with selective activation"
  - [section 2.3]: "DBR computes target-aware similarities: γ_long = cos(e★^(1), r_long), γ_short = cos(e★^(1), r_short), applying a hard gate to select the window with the larger similarity as the coarse-step user history H★_t... avoids long-short term dilution and yields a cleaner supervision signal for the coarse category"
  - [corpus]: GemiRec and SPARC address multi-interest modeling via codebooks and quantization but don't explicitly separate temporal scales with hard routing; related work on long-term interest modeling (User Long-Term Multi-Interest Retrieval) focuses on scaling behavior sequences but doesn't decompose by recency windows.

### Mechanism 2: Search-based SID Decoding (S2D)
- **Claim**: Constraining fine-grained token prediction to actions within the same coarse bucket reduces context noise and enables longer usable histories under latency constraints by shrinking the effective search space.
- **Mechanism**: Once a coarse (level-1) SID s★^(1) is predicted, S2D searches the full action history H_t for actions with matching level-1 SIDs: Ĥ_t(s★^(1)) = Search({a ∈ H_t : s^(1)(a) = s★^(1)}). This filtered subset conditions the decoder for levels 2-3 via cross-attention. The in-bucket history size scales as |Ĥ_t(s★^(1))| ≈ |H_t| / |V^(1)|, which is much smaller than |H_t| when codebooks are large (|C^(ℓ)| = 8192 in experiments).
- **Core assumption**: Coarse semantic categories (level-1 SIDs) provide sufficient granularity to isolate relevant historical context for fine-grained prediction, and intra-bucket consistency exists—actions in the same bucket share predictive signals for the target's fine-level tokens.
- **Evidence anchors**:
  - [abstract]: "Search-based SID Decoding (S2D) that constrains fine-level decoding within the current coarse bucket for efficiency and noise control"
  - [section 2.4]: "S2D realizes the decoding with searching, controlled noise and latency... |Ĥ_t(s★^(1))| ≈ |H_t|/|V^(1)|. In practice, the codebook per level is typically large (e.g., ≥256 and up to 8192), hence |Ĥ_t(s★^(1))| ≪ |H_t|. This shrinkage enables longer usable histories under a fixed latency budget."
  - [corpus]: Weak corpus support—related papers (TIGER, OneRec) discuss hierarchical SID generation but don't specifically address search-space reduction via bucket-constrained conditioning; the "Lost in the Middle" reference cited suggests context noise is a known LLM issue but doesn't validate S2D's specific approach.

### Mechanism 3: Exposure-aware Next-Token Prediction Loss (ENTP-Loss)
- **Claim**: Treating unclicked exposed items as coarse-level hard negatives accelerates non-interest fade-out and improves decoding quality without introducing fine-grained label noise.
- **Mechanism**: Training tuples are organized as (x_i, s_i^(1:L), c_i) where c_i ∈ {0,1} indicates positive (clicked) or negative (unclicked exposure). The loss is L_ENTP = (1/N) Σ_i [c_i Σ_ℓ (-log p_i^(ℓ)) + (1-c_i)(-α log(1 - p_i^(1)))]. Positives contribute standard NTP loss at all levels; negatives contribute only at level-1 with weight α=0.1, explicitly penalizing coarse categories associated with unclicked exposures.
- **Core assumption**: Unclicked exposures indicate user disinterest at the coarse category level (level-1), but not necessarily at fine levels—the user may have rejected specific videos within a relevant category. Penalizing only at level-1 avoids overfitting to fine-grained negative signals that may be noisy or situational.
- **Evidence anchors**:
  - [abstract]: "Exposure-aware Next-Token Prediction Loss (ENTP-Loss) that treats unclicked exposures as coarse-level hard negatives to promote timely interest fade-out"
  - [section 2.5]: "This objective keeps the standard NTP likelihood on positives at all levels while adding a coarse-level (level-1) penalty for negatives, promoting timely non-interest fade-out without introducing fine-level label noise."
  - [corpus]: Related work (ComiRec, sampling-bias correction papers) identifies exposure bias as a challenge in embedding-based retrieval but doesn't propose coarse-level negative sampling in generative retrieval contexts; this appears to be a novel contribution specific to DualGR.

## Foundational Learning

- **Concept: Hierarchical Semantic ID (SID) Quantization via Residual K-Means**
  - **Why needed here**: DualGR operates on discrete token sequences rather than continuous embeddings. Videos are pre-quantized into L=3 hierarchical SIDs via RQ-KMeans before training. Understanding this mapping is foundational—every other component (DBR routing, S2D filtering, decoding targets) depends on SID structure.
  - **Quick check question**: Given |V|=1B videos, L=3 levels, and |C^(ℓ)|=8192 per level, what's the theoretical SID space size? Why does the paper still require a "Map: SIDs → Video IDs" post-processing step?

- **Concept: Autoregressive Decoding with Beam Search**
  - **Why needed here**: The core retrieval operation is beam search over conditional distributions p(s^(ℓ) | s^(1:ℓ-1), x_t). Understanding how beam size B balances exploration vs. exploitation, and how two branches are merged at inference, is critical for tuning.
  - **Quick check question**: If beam size B=10 and both long/short branches run, how many SID candidates are produced before merging? What determines the final |R_θ(x_t)| after Map projection?

- **Concept: Cross-Attention for Target-Aware History Conditioning**
  - **Why needed here**: The decoder conditions on user history via cross-attention (k,v from history, q from decoder state), enabling explicit interaction between target tokens and historical actions. This distinguishes generative retrieval from embedding-based retrieval's late-interaction (ANN search).
  - **Quick check question**: In DBR, target-aware similarity uses cos(e★^(1), r_long/short) computed during training. How is this different from the cross-attention mechanism during decoding? Why does the paper need both?

## Architecture Onboarding

- **Component map**:
  1. Preprocessing (Offline): RQ-KMeans quantizes all videos → hierarchical SIDs (L=3, |C^(ℓ)|=8192). Builds lookup table Map: SIDs → {video_ids}.
  2. Input Encoding (Encoder-free Frontend): User static features E_u(u) + action embeddings E_a(actions) → concatenate → LayerNorm. No transformer encoder; pooling for summary vectors r_long, r_short.
  3. Dual-Branch Router (DBR): Splits H_t → (H_long=1000, H_short=64). Computes summaries, similarity, hard gate. Training: selects one branch. Inference: both active.
  4. Transformer Decoder: 4 blocks, d=512, 8 heads, FFN=1024. Causal self-attention + cross-attention over filtered history + FFN.
  5. S2D Search Module: Filters H_t by predicted level-1 SID after coarse prediction. Constructs Ĥ_t(s★^(1)) for fine-level conditioning.
  6. ENTP-Loss Layer: Combines positive NTP (all levels) + negative coarse penalty (level-1 only) with α=0.1.
  7. Inference Engine: Beam search per branch → merge SID sets → deduplicate → Map → top-K video IDs.

- **Critical path**:
  1. Request: user u, history H_t (|H_t|≤1000)
  2. DBR: x_long = LN([E_u(u); E_a(H_long)]), x_short = LN([E_u(u); E_a(H_short)])
  3. Branch Long: decode level-1 SIDs → for each, S2D filters H_t → decode s^(2), s^(3)
  4. Branch Short: same process with x_short
  5. Merge & dedup SID sets from both branches
  6. Map: SIDs → video IDs, return R_θ(x_t)

- **Design tradeoffs**:
  - **Encoder-free vs. Full Encoder**: Removes encoder latency (~10-50ms in large-scale systems) but may sacrifice representation quality. Paper claims step-wise cross-attention achieves "SIM-like target awareness without redundant re-encoding"—assumes decoder cross-attention compensates.
  - **Hard Gate vs. Soft Routing**: Hard gate yields cleaner gradients for coarse prediction but discards non-selected branch info during training. Soft routing could smooth gradients but risks dilution—the paper chose hard gates explicitly to "suppress cross-branch dilution."
  - **Level-1 Only Negatives vs. Multi-level**: Coarse-only negative penalty avoids fine-level noise but may be too blunt. Multi-level could be more precise but risks overfitting to position/timing bias in unclicked exposures.
  - **Window Sizes**: L_long=1000, L_short=64. Larger L_long increases compute (cross-attention scales with sequence length); smaller L_short may miss short-term context beyond 64 actions.

- **Failure signatures**:
  - **Low HR@K with high branch divergence**: Long and short branches produce nearly disjoint candidate sets—check if window sizes are appropriate for your user distribution (e.g., new users may have |H_t| < L_short).
  - **High latency in S2D phase**: Level-1 buckets too large → |Ĥ_t(s★^(1))| ≈ |H_t|. Verify |C^(1)| is sufficiently large (≥8192 as in paper).
  - **Over-penalization of valid categories**: α too high or negative sampling too aggressive → model avoids categories with historical unclicks. Monitor per-category recall and consider reducing α.
  - **Empty Ĥ_t(s★^(1)) for predicted bucket**: User has no history in predicted coarse category. Need fallback: use H_short or full H_t with warning flag.

- **First 3 experiments**:
  1. **Ablate DBR (long-only vs. short-only vs. both)**: Run three configs—(a) long branch only (L_long=1000), (b) short branch only (L_short=64), (c) both merged. Measure HR@100/500/1000 and p99 latency. Quantify contribution of temporal decomposition vs. single-branch baselines.
  2. **Sweep L_short and α**: Grid search L_short ∈ {16, 32, 64, 128, 256} and α ∈ {0.001, 0.01, 0.1, 0.5}. Plot HR@K surfaces to identify optimal settings for your traffic. Check if optima vary by user segment (new vs. power users, high vs. low activity).
  3. **Validate S2D efficiency vs. quality**: Compare S2D (bucket-constrained) vs. unconstrained decoding (use full H_t at all levels). Measure latency reduction, HR@K delta, and distribution of |Ĥ_t(s★^(1))| across requests. Confirm efficiency gains don't degrade quality—expect <1% HR drop with >30% latency reduction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would soft routing or attention-based combination of long/short-term branches outperform the hard gating mechanism in DBR?
- Basis in paper: [inferred] The DBR uses a hard gate selecting only one branch per training example ("applying a hard gate to select the window with the larger similarity"), which may discard useful signals from the non-selected branch and prevent cross-branch interaction learning.
- Why unresolved: The paper does not compare hard gating against soft routing alternatives; ablation only tests removing DBR entirely, not alternative fusion strategies.
- What evidence would resolve it: Ablation experiments comparing hard gating against weighted/attention-based branch fusion on the same benchmark.

### Open Question 2
- Question: Can extending ENTP-Loss to penalize finer-grained (level-2/3) tokens for unclicked exposures further improve fade-out precision?
- Basis in paper: [inferred] ENTP-Loss applies penalties only at level-1 to "avoid introducing fine-level label noise," but this design choice is not empirically validated against multi-level negative signals.
- Why unresolved: The paper restricts negatives to coarse level explicitly but offers no comparison to fine-level negative strategies.
- What evidence would resolve it: Experiments with hierarchical negative penalties (levels 1-3) showing whether fine-grained signals help or hurt performance.

### Open Question 3
- Question: How sensitive is DualGR to the number of hierarchical SID levels and codebook sizes across different corpus scales?
- Basis in paper: [inferred] The paper fixes L=3 levels with |C|=8192 per level, noting only that codebooks are "typically large (e.g., ≥256 and up to 8192)" without systematic exploration.
- Why unresolved: Optimal hierarchy depth and codebook granularity remain unspecified; these may vary with corpus size (billions of items) and item diversity.
- What evidence would resolve it: Sweep experiments varying L and |C| on multiple corpus scales, reporting HR@K and latency tradeoffs.

## Limitations

- **Temporal window design assumptions**: The DBR mechanism assumes user interests decompose cleanly into long-term (L_long=1000) and short-term (L_short=64) windows with distinct temporal characteristics, which may not generalize to other platforms with different content dynamics.
- **Exposure bias generalization**: The ENTP-Loss treats unclicked exposures as negative signals only at the coarse level, assuming fine-grained negative signals are too noisy, but this assumption isn't validated across different user segments or content categories.
- **Hierarchical SID quality dependency**: The entire method depends on high-quality hierarchical SID quantization through RQ-KMeans, but the paper doesn't report SID quantization quality metrics or demonstrate sensitivity to SID quality degradation.

## Confidence

- **High Confidence**: The architectural components (DBR, S2D, ENTP-Loss) are technically sound and implementable as described; the dual-branch approach with temporal window separation is effective for Kuaishou's specific dataset and user behavior patterns; the exposure-aware loss function with coarse-level negative penalty is a valid approach to handling exposure bias.
- **Medium Confidence**: The specific hyperparameter choices (L_long=1000, L_short=64, α=0.1, |C^(ℓ)|=8192) are optimal for the described setting; the performance improvements (+0.527% video views, +0.432% watch time) generalize beyond Kuaishou's specific implementation and user base; the latency benefits claimed from S2D efficiency scaling hold under different codebook sizes and user behavior distributions.
- **Low Confidence**: The method generalizes to recommendation domains beyond short-video (e-commerce, news, music); the performance holds for platforms with significantly different user engagement patterns or content consumption dynamics; the approach remains effective when scaled to much larger user bases (>hundreds of millions) or content catalogs (>billions of videos).

## Next Checks

1. **Temporal Window Sensitivity Analysis**: Conduct systematic ablation studies varying L_long ∈ {500, 1000, 2000} and L_short ∈ {32, 64, 128, 256} across different user segments (new users, power users, casual users). Measure HR@K, branch divergence metrics (Jaccard similarity between branch outputs), and latency impact to validate whether the 1000:64 ratio is robust or requires platform-specific tuning.

2. **Exposure Negative Quality Assessment**: Implement an A/B test comparing three negative sampling strategies: (a) ENTP-Loss as described (coarse-only negatives), (b) Fine-level negatives (all levels), (c) No negative exposure penalty. Measure not only HR@K but also per-category recall and diversity metrics to detect over-penalization of valid interests and validate the coarse-only assumption.

3. **SID Quality and Robustness Evaluation**: Measure SID quantization quality using standard metrics (purity, normalized mutual information, reconstruction error) and conduct controlled experiments where SID quality is degraded (e.g., reduced codebook sizes, merged clusters). Measure impact on DBR routing accuracy, S2D efficiency gains, and final retrieval quality to validate the dependency chain from SID quality to final performance.