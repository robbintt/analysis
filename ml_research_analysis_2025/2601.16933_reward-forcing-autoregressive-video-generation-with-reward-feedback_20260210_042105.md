---
ver: rpa2
title: 'Reward-Forcing: Autoregressive Video Generation with Reward Feedback'
arxiv_id: '2601.16933'
source_url: https://arxiv.org/abs/2601.16933
tags:
- video
- diffusion
- generation
- reward
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Reward-Forcing, an autoregressive video generation
  method that uses reward signals instead of distillation from bidirectional teachers.
  The key insight is that diffusion models generate coarse global structures before
  refining texture details, so motion can be learned through ODE trajectories while
  reward models handle texture refinement.
---

# Reward-Forcing: Autoregressive Video Generation with Reward Feedback

## Quick Facts
- arXiv ID: 2601.16933
- Source URL: https://arxiv.org/abs/2601.16933
- Reference count: 40
- Key outcome: Achieves 84.92 VBench total score, matching state-of-the-art autoregressive methods while avoiding extensive teacher distillation

## Executive Summary
This paper introduces Reward-Forcing, an autoregressive video generation method that uses reward signals instead of distillation from bidirectional teachers. The key insight is that diffusion models generate coarse global structures before refining texture details, allowing motion to be learned through ODE trajectories while reward models handle texture refinement. By first learning motion dynamics from teacher model trajectories and then applying reward feedback for texture enhancement, the method achieves competitive performance on VBench (84.92 total score) without being constrained by teacher model performance. The approach demonstrates that autoregressive models can achieve high-quality results through reward optimization alone, with last-frame supervision preserving motion quality while enhancing visual fidelity.

## Method Summary
Reward-Forcing converts bidirectional video diffusion models into autoregressive models through a two-phase training approach. First, the method distills a Wan2.1-T2V-1.3B bidirectional teacher to a 4-step model using Distribution Matching Distillation, then samples 1.4K ODE trajectories to initialize motion learning in the student autoregressive model via L_ode loss. In the second phase, the model is fine-tuned using reward loss L_reward = -E[R(ĉx_T)] applied only to the last frame, with Self-Rollout during training. The approach uses ImageReward for differentiable scalar feedback on final generated frames, with batch size 8, AdamW optimizer (β1=0, β2=0.999, weight_decay=0.01), learning rate 2e-6, and EMA decay 0.99. Training requires 8× H100 GPUs (80GB each) and is data-free after ODE initialization.

## Key Results
- Achieves 84.92 VBench total score, closely matching state-of-the-art autoregressive methods (84.31)
- Individual metrics: Aesthetic Quality 70.51, Dynamic Degree 81.94, Quality Score 85.91, Semantic Score 80.97
- Demonstrates that reward-only optimization (84.92) outperforms combined distillation+reward (82.55) due to objective conflicts
- Shows student performance is not strictly limited by teacher quality, potentially exceeding teacher performance through reward guidance

## Why This Works (Mechanism)

### Mechanism 1: Temporal Decoupling via ODE-First Training
- Claim: Motion dynamics can be learned independently from texture details through sequential training phases
- Mechanism: ODE trajectories from the bidirectional teacher provide coarse motion priors; reward models subsequently refine appearance without disrupting temporal coherence
- Core assumption: Diffusion models' coarse-to-fine generative ordering generalizes from images to video
- Evidence anchors: [abstract] diffusion models generate coarse global structures before refining texture details; [section 3.2] empirical observation of learned consistent motions without texture info
- Break condition: If reward optimization gradients destabilize learned motion patterns

### Mechanism 2: Last-Frame Reward Anchoring
- Claim: Applying reward supervision only to the final frame preserves motion while enhancing visual quality
- Mechanism: Last-frame supervision maintains temporal dynamics because proximity to generation end reduces interference with earlier motion decisions, and ImageReward lacks temporal awareness so random-frame supervision encourages static content
- Core assumption: ImageReward's texture focus does not generalize to motion quality assessment
- Evidence anchors: [section 5.2] random-frame supervision degrades motion quality by >10 percentage points; [section 3.2] last-frame supervision preserves motion better
- Break condition: If video tasks require frame-level consistency rather than endpoint quality

### Mechanism 3: Teacher-Boundary Escape via Reward Optimization
- Claim: Reward-guided models can exceed teacher performance by not being constrained to match teacher distribution
- Mechanism: Standard distillation aligns student with teacher distribution via KL divergence, but reward optimization incentivizes divergence toward higher-scoring outputs, potentially capturing quality dimensions the teacher missed
- Core assumption: Reward model captures perceptual quality dimensions that the teacher's training objective underweighted
- Evidence anchors: [section 4.5] final model's performance not strictly limited by teacher; [section 5.1] combining distillation + reward degrades performance due to objective conflict
- Break condition: If reward model has systematic biases causing distribution collapse or reward hacking

## Foundational Learning

- Concept: **Flow Matching / Diffusion ODEs**
  - Why needed here: The method samples ODE trajectories from teacher models to initialize student motion learning; understanding velocity fields v(x,t) is essential for interpreting how trajectories capture dynamics without stochasticity
  - Quick check question: Can you explain why DDIM enables deterministic sampling from models trained with stochastic diffusion objectives?

- Concept: **Distribution Matching Distillation (DMD)**
  - Why needed here: The baseline compresses multi-step diffusion into few-step generation via KL divergence minimization; the paper explicitly contrasts this constraint with reward-only optimization
  - Quick check question: Why does minimizing KL(p_student || p_teacher) mathematically limit student performance to teacher's ceiling?

- Concept: **Reward Model Limitations for Temporal Data**
  - Why needed here: ImageReward provides scalar feedback on visual-textual alignment but lacks temporal modeling; understanding this gap explains why frame selection strategy critically affects motion preservation
  - Quick check question: What failure mode would you expect if applying an image-only reward model to all video frames during training?

## Architecture Onboarding

- Component map: Teacher Model (Wan2.1-1.3B bidirectional) -> ODE Sampler (extracts trajectories {x_t^i}) -> Student Model (Autoregressive variant) -> Reward Model (ImageReward) -> Training Pipeline (Phase 1 → Phase 2)

- Critical path: 1) Distill teacher to 4-step model via DMD 2) Sample 1.4K ODE trajectories for motion prior initialization 3) Train student with L_ode until motion emerges 4) Switch to L_reward optimization on last frame exclusively 5) Enable EMA (decay=0.99) throughout training

- Design tradeoffs:
  - ODE trajectory count vs motion coverage: Paper uses only 1.4K trajectories; insufficient samples may yield incomplete motion priors
  - Last-frame vs multi-frame supervision: Random frames improve texture but hurt motion (>10% dynamic degree drop)
  - Teacher size vs student capacity: Ablation with 14B teacher shows no improvement over 1.3B—student capacity may be ceiling
  - Reward-only vs combined loss: Combining distillation + reward degrades performance (82.55 vs 84.92) due to objective conflict

- Failure signatures:
  - Static video outputs: Over-applying reward to multiple frames; switch to last-frame-only supervision
  - Motion inconsistency or flicker: ODE initialization insufficient; increase trajectory count or verify optical flow quality before Phase 2
  - Style divergence from teacher: Expected behavior—reward model guides toward different aesthetic preferences
  - Training instability with reward loss: Check EMA implementation; normalize reward gradients to match distillation loss scale

- First 3 experiments:
  1. Ablate ODE trajectory count: Train with {500, 1.4K, 5K} trajectories; measure Dynamic Degree and Motion Smoothness to identify minimum viable motion prior
  2. Frame selection sweep: Compare last-frame, first-frame, middle-frame, and random-frame reward supervision; quantify motion degradation pattern and identify threshold
  3. Reward model substitution: Replace ImageReward with video-aware reward (VisionReward or VideoAlign); assess whether temporal rewards enable multi-frame supervision without motion collapse

## Open Questions the Paper Calls Out
None

## Limitations
- The claim that reward models enable teacher-boundary escape is under-validated - the paper shows combined distillation+reward performs worse but doesn't definitively prove student outperformed teacher's ceiling
- The 1.4K trajectory initialization count appears arbitrary without ablation showing motion coverage sufficiency
- The critical last-frame supervision strategy, while empirically validated, lacks theoretical grounding for why temporal coherence is preserved
- ImageReward's limitations for temporal data remain untested with alternative video-aware reward functions

## Confidence

- **High confidence**: ODE-first training successfully learns motion (supported by optical flow observations and Dynamic Degree metrics)
- **Medium confidence**: Reward-only optimization matches or exceeds distillation performance (strong VBench results but limited ablation on teacher quality ceiling)
- **Low confidence**: Last-frame supervision preserves motion while enhancing quality (empirical observation but lacks theoretical explanation of temporal coherence preservation)

## Next Checks

1. **Reward Model Substitution**: Replace ImageReward with VisionReward or VideoAlign to test whether temporal-aware rewards enable multi-frame supervision without motion collapse, validating the hypothesis that ImageReward's lack of temporal modeling necessitates last-frame-only supervision.

2. **Teacher Capacity Ceiling Test**: Train with varying teacher sizes (0.7B, 1.3B, 14B) and student capacities to identify whether the observed "no improvement with larger teachers" pattern holds across architectures, confirming the student capacity bottleneck claim.

3. **Trajectory Coverage Analysis**: Systematically vary ODE trajectory count (500, 1.4K, 5K, 10K) and measure motion quality degradation to establish minimum viable initialization requirements and test the arbitrary 1.4K choice.