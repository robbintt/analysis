---
ver: rpa2
title: 'Scam Shield: Multi-Model Voting and Fine-Tuned LLMs Against Adversarial Attacks'
arxiv_id: '2511.01746'
source_url: https://arxiv.org/abs/2511.01746
tags:
- scam
- adversarial
- detection
- voting
- fine-tuned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of detecting adversarial scams\u2014\
  fraudulent messages deliberately modified to evade automated filters. The proposed\
  \ Hierarchical Scam Detection System (HSDS) combines a multi-model voting ensemble\
  \ of four classifiers (XGBoost, Decision Tree, Random Forest, KNN) with a fine-tuned\
  \ LLaMA 3.1 8B Instruct model."
---

# Scam Shield: Multi-Model Voting and Fine-Tuned LLMs Against Adversarial Attacks

## Quick Facts
- arXiv ID: 2511.01746
- Source URL: https://arxiv.org/abs/2511.01746
- Reference count: 28
- Primary result: HSDS achieves 0.90 accuracy, outperforming GPT-3.5 Turbo (0.78) on adversarial scam detection.

## Executive Summary
This paper introduces a Hierarchical Scam Detection System (HSDS) that combines multi-model voting with a fine-tuned LLM to detect adversarial scams—fraudulent messages deliberately modified to evade filters. The system uses four traditional classifiers to vote on most messages, escalating only ambiguous cases to a fine-tuned LLaMA 3.1 8B Instruct model. Experiments show HSDS achieves 90% accuracy while reducing inference time by 56.7% compared to direct LLM classification, outperforming both individual models and proprietary LLM baselines.

## Method Summary
The approach trains four traditional classifiers (XGBoost, Decision Tree, Random Forest, KNN) on TF-IDF features from a 20,000-sample dataset augmented with adversarial transformations. A fine-tuned LLaMA 3.1 8B model using LoRA (rank=16, α=32) handles ambiguous cases escalated from the voting ensemble. The system routes messages through the cheap ensemble first, only invoking the expensive LLM when classifiers disagree, with KNN serving as a tiebreaker. Inference uses strict yes/no parsing with temperature=0 and top_k=1 to maintain determinism.

## Key Results
- HSDS achieves 0.90 accuracy on adversarial scam detection, outperforming GPT-3.5 Turbo (0.78) and standalone models
- Inference time reduced by 56.7% compared to direct LLM classification (2296s → 995s on 1200 messages)
- Precision improves from 0.89 to 0.95 when voting is added, though recall gains are modest (0.82 → 0.85)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical routing reduces computational overhead while preserving detection accuracy.
- Mechanism: Four traditional classifiers vote on each message; only cases without unanimous agreement escalate to the fine-tuned LLM. Since most cases achieve consensus at the cheap layer, total inference time drops by 56.7%.
- Core assumption: The four ML models agree on the majority of non-adversarial messages, and their disagreement correlates with ambiguity rather than random noise.
- Evidence anchors:
  - [abstract] "reduces inference time by 56.7% compared to direct LLM classification"
  - [section IV-G] Table VII shows per-message time drops from 1.91s to 0.83s
  - [corpus] Weak direct evidence; no corpus neighbor quantifies hierarchical routing efficiency.
- Break condition: If ensemble disagreement rate exceeds ~40-50%, latency advantage erodes and LLM becomes the bottleneck.

### Mechanism 2
- Claim: LoRA-based adversarial fine-tuning improves robustness to synonym substitution, deletion, and reordering attacks.
- Mechanism: Low-rank adapters inject trainable matrices into q_proj and v_proj layers; training on 20K samples including adversarial variants teaches the model to recognize obfuscated scams without full parameter updates.
- Core assumption: Adversarial transformations in training generalize to real-world evasion tactics.
- Evidence anchors:
  - [abstract] "fine-tuned LLaMA 3.1 8B Instruct model... optimized via LoRA-based adversarial training"
  - [section III-D] Eq. 1-4 shows 8,388,608 trainable parameters out of 8B total
  - [corpus] Corpus neighbors discuss adversarial robustness but not LoRA specifically for scam text.
- Break condition: If attackers use transformations outside the training distribution, robustness gains may not transfer.

### Mechanism 3
- Claim: Majority voting with LLM adjudication and KNN fallback improves precision over any single model.
- Mechanism: Agreement among all four classifiers → accept; disagreement → LLM decides; LLM uncertain → majority vote among four → tie → KNN. Precision rises from 0.89 to 0.95 when voting is added.
- Core assumption: KNN's superior standalone performance makes it a reliable tiebreaker, and LLM uncertainty is rare enough not to dominate fallback path.
- Evidence anchors:
  - [abstract] "ambiguous cases are escalated to the fine-tuned model"
  - [section III-E] Algorithm 1 formalizes the decision hierarchy
  - [corpus] No corpus neighbor validates this specific voting-fallback pattern.
- Break condition: If LLM uncertainty rate spikes, fallback dominates and precision regresses toward KNN baseline.

## Foundational Learning

- Concept: **Ensemble diversity requirement**
  - Why needed here: Voting only helps if models make uncorrelated errors. If all four classifiers share the same blind spots, voting adds no signal.
  - Quick check question: On a held-out set, what is the pairwise error correlation between RF, DT, XGB, and KNN on adversarial samples?

- Concept: **LoRA rank-capacity tradeoff**
  - Why needed here: Rank=16 may be insufficient to capture complex adversarial patterns; higher rank improves expressiveness but increases memory and may overfit.
  - Quick check question: Does increasing rank to 32 or 64 improve recall on romance scams without degrading precision on finance scams?

- Concept: **Calibration of LLM uncertainty**
  - Why needed here: The fallback triggers when LLM output is unparsable, but this is a binary heuristic. A well-calibrated probability threshold could route more intelligently.
  - Quick check question: Can we replace the yes/no parsing rule with a softmax over token probabilities and set a confidence threshold (e.g., 0.7) for escalation?

## Architecture Onboarding

- Component map:
  Message → Feature extraction → Four classifiers (RF, DT, XGB, KNN) → Voting router → (Unanimous? → Return prediction : LLM inference) → Parse yes/no → (Parsable? → Return prediction : Majority vote → KNN tiebreak)

- Critical path:
  1. Message arrives → feature extraction for four classifiers → predictions collected.
  2. If all four agree → return prediction (fast path).
  3. If disagreement → LLM inference with structured prompt → parse output.
  4. If LLM output unparsable → majority vote → if tie, KNN prediction.

- Design tradeoffs:
  - **Accuracy vs. latency**: LLM escalation improves accuracy (+3.4%) but adds ~1s per escalated message.
  - **Precision vs. recall**: Majority voting boosts precision (0.89 → 0.95) but recall gains are smaller (0.82 → 0.85).
  - **Parameter efficiency vs. capacity**: LoRA at 0.1% parameters enables fine-tuning on consumer hardware but may limit adaptation to novel attack types.

- Failure signatures:
  - **High LLM escalation rate (>40%)**: Indicates ensemble is undertrained or feature extraction fails; check feature pipeline.
  - **LLM unparsable output spike**: Prompt format drift or model hallucination; validate prompt template and decoding config.
  - **Romance/lottery scam recall drops below 0.75**: These categories rely on implicit cues; may need category-specific fine-tuning.

- First 3 experiments:
  1. **Ablate the voting layer**: Run inference using only the fine-tuned LLM (no ensemble) on the same 1200-message test set; measure accuracy delta and latency increase to quantify the voting contribution.
  2. **Stress-test adversarial generalization**: Generate adversarial samples using transformations NOT in training (e.g., LLM-based paraphrasing, emoji injection) and evaluate accuracy degradation across all components.
  3. **Calibrate LLM confidence threshold**: Replace binary yes/no parsing with token probability thresholding; sweep thresholds from 0.5 to 0.9 and plot precision-recall to find optimal operating point.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does LoRA-based adversarial fine-tuning generalize across diverse adversarial perturbation strategies beyond synonym replacement, random deletion, and sentence reordering?
- Basis in paper: [explicit] The paper states "LoRA has been proposed as a more efficient fine-tuning approach, but its effectiveness in adversarial settings remains an open question." The adversarial generation is bounded to three transformation types.
- Why unresolved: The operational definition limits adversarial scams to specific text transformations; more sophisticated attacks were not evaluated.
- What evidence would resolve it: Benchmark LoRA fine-tuned models against broader adversarial attack suites.

### Open Question 2
- Question: How does the hierarchical system perform against state-of-the-art proprietary LLMs (e.g., GPT-4o, Claude 3.5 Sonnet) under adversarial conditions?
- Basis in paper: [explicit] "Due to computational and resource constraints, we adopt GPT-3.5 Turbo and Claude 3 Haiku as representative baselines... systematic evaluation of their robustness under adversarial scam detection remains scarce."
- Why unresolved: Newer models with improved reasoning capabilities were not tested; performance gaps may narrow or reverse with stronger baselines.
- What evidence would resolve it: Comparative evaluation of HSDS against GPT-4o and Claude 3.5 Sonnet on the same adversarial scam benchmark.

### Open Question 3
- Question: Can category-specific fine-tuning or adaptive weighting improve detection of emotionally manipulative scams (romance, lottery) where current recall is substantially lower?
- Basis in paper: [explicit] Romance scams show recall of 0.7308 vs. 0.9701+ for finance/loan scams. The paper states "future steps such as more focused training... and category-specific fine-tuning could further enhance its ability to detect diverse scam types."
- Why unresolved: Current unified model may not capture implicit persuasion patterns; weighted voting underperformed majority voting, suggesting weighting requires refinement.
- What evidence would resolve it: Train specialized adapters per scam category and evaluate adaptive weighting schemes based on message features or model confidence.

## Limitations

- The testing dataset composition and provenance from reference [3] remain unspecified, making independent validation difficult
- LoRA-based adversarial fine-tuning may not generalize to sophisticated transformations like LLM-based paraphrasing or emoji injection
- The 56.7% latency reduction depends on low ensemble disagreement rates; if disagreement exceeds ~40%, the computational advantage erodes

## Confidence

- **High Confidence**: The hierarchical voting architecture and LLM fallback logic are clearly specified and reproducible
- **Medium Confidence**: The reported accuracy improvements over baselines are credible but contingent on undisclosed testing dataset and feature extraction pipeline
- **Low Confidence**: Generalization to real-world adversarial attacks outside the training distribution is not empirically validated

## Next Checks

1. **Ablation of Voting Layer**: Run inference using only the fine-tuned LLM (no ensemble) on the same 1200-message test set; measure accuracy delta and latency increase to quantify the voting contribution
2. **Stress-Test Adversarial Generalization**: Generate adversarial samples using transformations NOT in training (e.g., LLM-based paraphrasing, emoji injection) and evaluate accuracy degradation across all components
3. **Calibrate LLM Confidence Threshold**: Replace binary yes/no parsing with token probability thresholding; sweep thresholds from 0.5 to 0.9 and plot precision-recall to find optimal operating point