---
ver: rpa2
title: 'Learning to Reason: Training LLMs with GPT-OSS or DeepSeek R1 Reasoning Traces'
arxiv_id: '2511.19333'
source_url: https://arxiv.org/abs/2511.19333
tags:
- reasoning
- traces
- gpt-oss
- deepseek-r1
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work compares the impact of training medium-sized language\
  \ models with two distinct reasoning styles generated by DeepSeek-R1 and gpt-oss\
  \ on math problem-solving tasks. Both reasoning styles achieve comparable accuracy\
  \ on math benchmarks, but gpt-oss traces are 4\xD7 more token-efficient, generating\
  \ significantly fewer tokens per response."
---

# Learning to Reason: Training LLMs with GPT-OSS or DeepSeek R1 Reasoning Traces

## Quick Facts
- arXiv ID: 2511.19333
- Source URL: https://arxiv.org/abs/2511.19333
- Authors: Shaltiel Shmidman; Asher Fredman; Oleg Sudakov; Meriem Bendris
- Reference count: 9
- Primary result: gpt-oss reasoning traces achieve 4× token efficiency with comparable accuracy to DeepSeek-R1 traces in math problem-solving

## Executive Summary
This work compares the impact of training medium-sized language models with two distinct reasoning styles generated by DeepSeek-R1 and gpt-oss on math problem-solving tasks. Both reasoning styles achieve comparable accuracy on math benchmarks, but gpt-oss traces are 4× more token-efficient, generating significantly fewer tokens per response. This demonstrates that verbose reasoning does not necessarily lead to better performance, and models can be effectively trained to adopt more efficient reasoning patterns. The reduction in token usage directly translates to lower latency and operational costs in real-world deployments. These findings suggest that optimizing reasoning trace verbosity can balance accuracy and efficiency, encouraging further research into efficient reasoning techniques for LLMs.

## Method Summary
The study fine-tunes 12B parameter models (Mistral-Nemo-Base-2407 and NVIDIA-Nemotron-Nano-12B-v2-Base) on curated math problems paired with reasoning traces from either DeepSeek-R1 or gpt-oss. The training dataset consists of 242,000 samples from the Nemotron-Post-Training-Dataset-v1, filtered to include only problems where both teacher models produced correct answers (using Qwen3-30B-A3B-Thinking-2507 as judge). Models are trained for 3,000 steps with a batch size of 64, using AdamW optimization with cosine annealing. Evaluation measures Pass@8 accuracy on GSM8K, AIME 2025, and MATH-500 benchmarks, along with average tokens generated.

## Key Results
- gpt-oss-trained models generate ~4× fewer tokens than DeepSeek-R1-trained models while maintaining comparable Pass@8 accuracy
- Pass@8 accuracy is nearly identical between models (e.g., MATH-500: 88.0 vs 88.0)
- Nemotron-Nano-12B-v2-Base achieves higher accuracy than Mistral-Nemo-Base-2407 on all benchmarks
- The 4× token reduction directly translates to lower inference latency and operational costs

## Why This Works (Mechanism)

### Mechanism 1: Reasoning Trace Distillation
Fine-tuning medium-sized models on synthetic reasoning traces from frontier models transfers reasoning capabilities through supervised learning of token-level patterns. The student model learns to predict intermediate reasoning steps, internalizing goal decomposition and self-verification patterns. This works when the student has sufficient capacity (>1-2B params) to generalize the multi-step reasoning structures.

### Mechanism 2: Efficiency vs. Verbosity Trade-off
Reasoning accuracy doesn't linearly correlate with reasoning trace verbosity. Both verbose (DeepSeek-R1) and concise (gpt-oss) models can arrive at correct solutions, as the core reasoning algorithm (plan, execute, verify) can be represented with different token budgets. The student learns this algorithmic pattern rather than a specific length, though this trade-off may break down for extremely complex problems requiring extensive exploration.

### Mechanism 3: Inference Cost Reduction via Training
Training on shorter reasoning traces produces models that generate fewer tokens during inference, directly reducing latency and cost. This works when the learned efficient style is stable and the model doesn't revert to longer traces during inference, though overly aggressive compression could omit crucial self-correction steps for harder problems.

## Foundational Learning

- **Test-Time Scaling / Chain-of-Thought (CoT)**: Why needed: This is the core paradigm being explored—teaching models to perform "thinking" processes. Quick check: Why does generating more tokens at inference time often improve answer accuracy on complex problems?

- **Knowledge Distillation**: Why needed: The entire experimental setup is an application of distillation. A large "teacher" model's outputs are used to train a smaller "student" model. Quick check: In this paper, what is the role of DeepSeek-R1 and gpt-oss? What is the role of the 12B parameter models?

- **Pass@N Metric**: Why needed: Results are reported using Pass@8. This metric is crucial for evaluating generative models on problems with single correct answers, accounting for the fact a solution might not appear on the first try. Quick check: If a model has a Pass@8 score of 30% on a benchmark, what does that mean in practical terms?

## Architecture Onboarding

- **Component map**: DeepSeek-R1-2508, gpt-oss-120b (Teacher Models) -> Qwen3-30B-A3B-Thinking-2507 (Judge Model) -> 242K curated math problems -> Mistral-Nemo-Base-2407, NVIDIA-Nemotron-Nano-12B-v2-Base (Student Models)

- **Critical path**: The data curation pipeline. Model training is straightforward fine-tuning; final model quality depends on creating the set of 242k correct reasoning traces.

- **Design tradeoffs**: Accuracy vs. Efficiency (central tradeoff), Base Model Selection (Nemotron pre-exposed to DeepSeek traces, affecting training dynamics)

- **Failure signatures**: Catastrophic Forgetting (loss of general knowledge), Omission of Self-Correction (overly concise traces missing checking steps), Style Mismatch (underperformance when prompted with different reasoning styles)

- **First 3 experiments**:
  1. Train a single student model on only DeepSeek-R1 traces to establish "verbose" baseline
  2. Train the same student model on only gpt-oss traces to confirm 4x reduction and comparable accuracy
  3. Train a third model on 50/50 mix of both traces to investigate whether model adapts verbosity based on problem difficulty

## Open Questions the Paper Calls Out

- Does the efficiency-accuracy trade-off observed in mathematical reasoning generalize to complex domains like coding or creative writing? The authors explicitly state this may not generalize to domains where verbosity plays a different role, based on experiments restricted to math datasets.

- Can models learn to dynamically adjust reasoning verbosity based on problem difficulty through hybrid training? The authors propose exploring mixed training to investigate whether models can apply optimal verbosity for problem difficulty.

- Do the benefits of token-efficient reasoning traces persist when scaling model size beyond the 12B parameter limit tested? The authors note expanding to larger model scales would be valuable to confirm if findings generalize.

## Limitations

- Data Quality Control: Limited detail on judge model accuracy or potential bias that could skew training distribution
- Style Stability Under Transfer: No testing whether models retain efficiency when few-shot prompted with verbose examples
- Limited Generalization Beyond Math: All experiments focus on mathematical problem-solving, which may not transfer to other reasoning domains

## Confidence

- **High Confidence**: The core empirical finding that gpt-oss-trained models generate ~4× fewer tokens while maintaining comparable accuracy on math benchmarks
- **Medium Confidence**: The mechanism explanation that reasoning accuracy doesn't require verbosity because the core algorithmic pattern is what matters
- **Low Confidence**: The claim about direct translation to "≈4× reduction in latency and cost" in real-world applications

## Next Checks

1. **Few-shot Style Transfer Test**: Take the gpt-oss-trained model and evaluate it with few-shot prompts containing verbose DeepSeek-R1-style reasoning traces. Measure whether the model adapts its verbosity or maintains its efficient generation pattern.

2. **Cross-Domain Efficiency Validation**: Replicate the experiment on non-math reasoning tasks (coding problems, logical reasoning, multi-step QA) to measure whether the verbosity-efficiency trade-off holds or breaks in these domains.

3. **Judge Model Bias Analysis**: Conduct an ablation study using different judge models or no filtering to compare resulting model behaviors and determine whether judge preferences influenced training distribution and efficiency outcomes.