---
ver: rpa2
title: Fitzpatrick Thresholding for Skin Image Segmentation
arxiv_id: '2510.06655'
source_url: https://arxiv.org/abs/2510.06655
tags:
- skin
- fitzpatrick
- segmentation
- https
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that applying Fitzpatrick skin-type-specific
  decision thresholds to semantic segmentation models significantly improves performance
  on darker skin tones, particularly Fitzpatrick VI. Using a curated psoriasis dataset
  with detailed manual annotations and Fitzpatrick labels, the authors show that models
  trained without tone information perform suboptimally on darker skin due to shifted
  probability distributions.
---

# Fitzpatrick Thresholding for Skin Image Segmentation

## Quick Facts
- arXiv ID: 2510.06655
- Source URL: https://arxiv.org/abs/2510.06655
- Reference count: 31
- One-line result: Applying Fitzpatrick-specific thresholds to segmentation outputs improves performance on darker skin tones, with up to +31% bIoU and +24% Dice gains for Fitzpatrick VI.

## Executive Summary
This study addresses systematic under-segmentation of darker skin tones in dermatology AI by introducing a simple post-hoc calibration method. Using a curated psoriasis dataset with manual Fitzpatrick labels, the authors demonstrate that segmentation models trained without skin-tone information produce shifted probability distributions for darker skin. By sweeping decision thresholds per Fitzpatrick type on a held-out tuning split, they achieve substantial fairness gains—up to +31% bIoU and +24% Dice for Fitzpatrick VI—without retraining models. The approach leverages recent high-accuracy Fitzpatrick classifiers, making it a practical, cost-free intervention for clinical deployment.

## Method Summary
The authors train three segmentation architectures (U-Net, ResU-Net, SETR-small) on psoriasis images with manual pixel-level annotations but no skin-tone information. On a tuning split, they exhaustively sweep thresholds from 0.001 to 0.99, recording optimal values per Fitzpatrick group (I-VI). At inference, a Fitzpatrick classifier predicts skin type, and the corresponding precomputed threshold is applied to the segmentation probability map. This post-hoc calibration corrects the systematic under-segmentation of darker skin tones caused by distribution shifts in model outputs.

## Key Results
- Fitzpatrick VI shows up to +31% bIoU and +24% Dice improvement on UNet using tone-specific thresholds
- Gains persist across all three architectures tested, with smaller but consistent improvements for Fitzpatrick V and IV
- Global metrics mask subgroup failures; per-tone optimization reveals fairness issues hidden in aggregate scores
- The technique requires no retraining and adds minimal inference overhead via lightweight Fitzpatrick classifiers

## Why This Works (Mechanism)

### Mechanism 1: Probability Distribution Shift by Skin Tone
Models trained predominantly on lighter skin tones produce systematically lower probability outputs for darker skin, causing under-segmentation when a uniform threshold is applied. The entire output distribution shifts leftward for Fitzpatrick V–VI, and a global threshold optimized for the majority falls in a region where darker skin predictions are disproportionately filtered out. This mechanism assumes consistent distribution shifts within each Fitzpatrick group.

### Mechanism 2: Per-Group Optimal Operating Points Exist for Segmentation Tasks
Unlike binary classification with FPR–TPR trade-offs, segmentation permits per-group thresholds that improve subgroup metrics without necessarily harming others. Each image yields a dense probability map, and there exists, in principle, an optimal threshold for each subgroup that maximizes Dice or bIoU. This assumes the tuning split contains sufficient examples per Fitzpatrick type to estimate reliable optima.

### Mechanism 3: Practical Deployment via High-Accuracy Pre-Trained Fitzpatrick Classifiers
The operational cost of skin-tone labeling has dropped dramatically due to classifiers trained on Fitzpatrick-17k, making per-group thresholding deployable without manual annotation. A lightweight classifier predicts Fitzpatrick type at inference time; the precomputed optimal threshold for that type is applied to the segmentation probability map. This assumes classifier accuracy (>95%) is sufficient that misclassification does not introduce more error than the thresholding corrects.

## Foundational Learning

### Concept: Decision Threshold Calibration in Semantic Segmentation
Why needed: The entire method hinges on understanding that segmentation models output probability maps, not binary masks, and the choice of threshold directly determines what is classified as lesion vs. healthy skin.
Quick check: Why must thresholds be swept on a held-out tuning split rather than optimized during training? (Answer: To avoid overfitting to training data; thresholds are post-hoc operating points on model outputs, not learned parameters.)

### Concept: Fitzpatrick Skin Type Classification Scale (I–VI)
Why needed: The Fitzpatrick scale is the clinical standard for skin tone, and the paper demonstrates that optimal thresholds vary systematically across this scale—particularly that Fitz VI requires lower thresholds.
Quick check: If a model under-segments darker skin, would you expect the optimal threshold for Fitz VI to be higher or lower than for Fitz I? Explain why. (Answer: Lower—because probability outputs are shifted left, a lower threshold captures more true positives.)

### Concept: Group-Stratified vs. Global Performance Metrics
Why needed: The paper's fairness contribution is showing that global metrics hide subgroup failures; practitioners must learn to report and optimize per-group metrics.
Quick check: Given the dataset distribution (85 Fitz I, 350 Fitz II, 145 Fitz III, 96 Fitz IV, 62 Fitz V, 16 Fitz VI), why might a global threshold systematically disadvantage Fitz VI even if the model architecture is unbiased?

## Architecture Onboarding

### Component Map
Data Pipeline -> Manual Annotation -> Patient-Stratified Split -> Training -> Threshold Search -> Inference Pipeline
- Data Pipeline: 754 psoriasis images from 6 public atlases → manual Fitzpatrick labeling by dermatologist → pixel-level segmentation masks → 30/30/40 train/tune/test split (patient-stratified)
- Training: Three architectures (U-Net, ResU-Net, SETR-small) trained identically with weighted BCE+Dice loss
- Threshold Search Module: Exhaustive sweep τ∈[0.001, 0.99] on tuning split → compute metrics per Fitzpatrick group → record optimal thresholds
- Inference Pipeline: Input image → Fitzpatrick classifier → segmentation model → probability map → apply τ_g for predicted skin type → binary mask

### Critical Path
1. Train segmentation model on training split (no Fitzpatrick information provided during training)
2. On tuning split, perform exhaustive threshold sweep; store optimal thresholds per Fitzpatrick type
3. At inference, first predict Fitzpatrick type using external classifier, then apply corresponding threshold to segmentation output

### Design Tradeoffs
- **Simplicity vs. optimality**: Thresholding is architecturally trivial but cannot correct learned representation bias—only output calibration. More complex interventions (balanced resampling, adversarial debiasing) may achieve better fairness but require retraining.
- **No retraining vs. dataset limitations**: The method works with existing models but is constrained by whatever biases were learned during training.
- **Small subgroup sizes**: Only 16 Fitz VI images in total; tuning/test splits contain ~5–7 each. Per-group optima may have high variance.

### Failure Signatures
- **Minimal threshold separation**: If optimal thresholds across groups cluster tightly (e.g., all between 0.45–0.50), the method provides negligible benefit.
- **Classifier misclassification cascade**: If the Fitzpatrick classifier errors on ambiguous images, wrong thresholds are applied—potentially worse than global threshold.
- **Non-translational distribution shifts**: If probability distributions differ in shape (not just location), single thresholds cannot fully correct.

### First 3 Experiments
1. **Threshold generalization test**: Apply Fitzpatrick-specific thresholds learned on tuning split to held-out test set; verify improvements persist and are not tuning artifacts.
2. **End-to-end pipeline validation**: Replace ground-truth Fitzpatrick labels with automated classifier predictions; measure performance degradation to quantify real-world effectiveness.
3. **Ablation against alternative fairness interventions**: Compare Fitzpatrick thresholding vs. data balancing (oversample Fitz V–VI) vs. fairness-aware training (group-adaptive batch norm) on same test set to understand tradeoffs.

## Open Questions the Paper Calls Out

### Open Question 1
Does Fitzpatrick thresholding generalize to other inflammatory skin conditions beyond psoriasis?
The paper recommends this technique "for any clinical deployment of dermatology segmentation models" but validates only on psoriasis. The probability distribution shifts observed for psoriasis may manifest differently for conditions with distinct morphological presentations (e.g., eczema, vitiligo, lupus). Replication on curated datasets of other skin diseases with Fitzpatrick labels would resolve this.

### Open Question 2
How does Fitzpatrick thresholding interact with training-time fairness interventions?
The authors state the method is "orthogonal to, and composable with, data balancing, representation alignment, FairAdaBN, or FairPrune" but do not test combinations. It remains unknown whether thresholding provides additive gains when models are already debiased during training, or if the approaches target redundant error sources. Factorial experiments comparing baseline, training-only, thresholding-only, and combined approaches would resolve this.

### Open Question 3
How robust is Fitzpatrick thresholding when the skin-type classifier makes errors?
The method relies on >95% Fitzpatrick classification accuracy cited but not independently validated on psoriasis data. Misclassification would apply wrong thresholds—potentially harming performance. Sensitivity analysis injecting controlled classification noise into the tuning split and measuring downstream segmentation degradation across subgroups would quantify real-world robustness.

## Limitations
- Small subgroup sizes (only 16 Fitzpatrick VI images) limit reliability of tone-specific thresholds
- Method depends on high-accuracy Fitzpatrick classifiers not validated on psoriasis-specific data
- Thresholds tuned on one dataset may not transfer to images with different lighting or camera settings

## Confidence
- **High**: The core observation that uniform thresholds under-segment darker skin due to probability distribution shifts
- **Medium**: The claim that Fitzpatrick-17k classifiers exceed 95% accuracy (cited but not independently validated on psoriasis data)
- **Medium**: Generalization of per-group thresholds to new datasets (plausible but unverified beyond the curated psoriasis sample)

## Next Checks
1. **Cross-dataset threshold transferability**: Apply Fitzpatrick-specific thresholds trained on this psoriasis dataset to external datasets (e.g., ISIC skin lesion archive) and measure subgroup performance retention
2. **End-to-end pipeline stress test**: Replace ground-truth Fitzpatrick labels with automated predictions at inference; quantify degradation to establish real-world robustness
3. **Alternative fairness intervention comparison**: Benchmark Fitzpatrick thresholding against data balancing (oversampling darker tones) and fairness-aware training (group-adaptive batch norm) on the same test set to quantify relative effectiveness