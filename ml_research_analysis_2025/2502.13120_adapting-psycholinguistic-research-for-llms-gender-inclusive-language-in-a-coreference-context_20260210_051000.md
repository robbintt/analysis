---
ver: rpa2
title: 'Adapting Psycholinguistic Research for LLMs: Gender-inclusive Language in
  a Coreference Context'
arxiv_id: '2502.13120'
source_url: https://arxiv.org/abs/2502.13120
tags:
- gender
- coreferent
- language
- antecedent
- masculine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study adapts psycholinguistic methods to investigate how Large
  Language Models (LLMs) process gender-inclusive language in English and German.
  Using coreference contexts, the research examines whether LLMs align generated terms
  with given gender expressions or reflect underlying biases.
---

# Adapting Psycholinguistic Research for LLMs: Gender-inclusive Language in a Coreference Context

## Quick Facts
- **arXiv ID**: 2502.13120
- **Source URL**: https://arxiv.org/abs/2502.13120
- **Reference count**: 37
- **Key outcome**: This study adapts psycholinguistic methods to investigate how Large Language Models (LLMs) process gender-inclusive language in English and German. Using coreference contexts, the research examines whether LLMs align generated terms with given gender expressions or reflect underlying biases. Experiments on six English models and one German model measured coreferent probabilities and analyzed generated continuations. Results show that English models generally maintain antecedent-coreferent gender consistency but exhibit masculine bias, particularly struggling with singular "they." German models displayed stronger masculine bias, overriding gender-neutral strategies. However, gender-inclusive language in German increased the likelihood of feminine and neutral coreferents, partially achieving their intended effect. These findings highlight the importance of gender-inclusive language in LLMs, especially for underrepresented languages like German. Limitations include smaller model sizes and limited coreferent testing. Future work should explore larger models and longer contexts.

## Executive Summary
This research adapts psycholinguistic methodologies to evaluate how Large Language Models process gender-inclusive language in coreference contexts. The study investigates whether models maintain gender consistency between antecedents and coreferents while revealing underlying masculine biases that persist even with gender-neutral strategies. Through experiments measuring coreferent probabilities and analyzing generated continuations across English and German models, the research demonstrates that while English models generally maintain gender consistency, they struggle with singular "they" and exhibit masculine default bias. German models show even stronger masculine bias that overrides gender-neutral strategies, though inclusive language partially achieves its intended effect of increasing feminine and neutral coreferent probabilities.

## Method Summary
The study employed 44 sentence-pair templates from psycholinguistic research, adapted for LLM evaluation. For English, 13,464 phrase-level and 14,652 sentence-level instances tested coreferent probabilities across 34-37 antecedent triplets with men/women/people (or he/she/they) as coreferent options. German experiments used 10,560 instances testing 10 antecedents with 8 gender-inclusive strategies against Männer/Frauen/Personen coreferents. Models were evaluated through forced-choice probability measurement (extracting log-probabilities of specific coreferent tokens) and free generation experiments (sampling 8-10 tokens followed by human annotation for gender and coreference presence). Statistical analysis employed two-way ANOVA for probability data and χ² tests for generation results.

## Key Results
- English models maintain antecedent-coreferent gender consistency in plural contexts with large effect sizes (η² = 0.19)
- Masculine bias persists across all models, with men having ~3× higher probability than women for neutral antecedents
- German models exhibit stronger masculine bias that overrides gender-neutral strategies, making coreferent gender the primary predictor
- Gender-inclusive German strategies partially achieve their intended effect by increasing feminine and neutral coreferent probabilities
- Singular "they" shows extremely low probability across all models, indicating fundamental difficulty with singular interpretation

## Why This Works (Mechanism)

### Mechanism 1: Antecedent-Coreferent Gender Consistency
- Claim: LLMs tend to produce coreferents whose gender matches the antecedent's gender, particularly in English plural contexts.
- Mechanism: The model conditions token probabilities on the gender information encoded in the antecedent phrase (e.g., "sportsmen" vs. "athletes"). When antecedent and coreferent gender correspond, the interaction term in a two-way ANOVA dominates coreferent probability, showing the model has learned to maintain gender across sentence boundaries.
- Core assumption: Training data contains sufficient co-occurrences of gendered antecedents with matching gendered coreferents for this pattern to be learnable.
- Evidence anchors:
  - [abstract] "in English, LLMs generally maintain the antecedent's gender"
  - [section 4.1] "the interaction between antecedent and coreferent gender is statistically significant and large (F(4, 13455) = 809.94, p < .001; η² = 0.19)"
  - [corpus] "Implicit Causality-biases in humans and LLMs" explores similar discourse-level consistency patterns
- Break condition: Consistency degrades for neutral antecedents and singular "they" contexts where masculine bias interferes.

### Mechanism 2: Masculine Default Bias in Probability Distributions
- Claim: Even when maintaining gender consistency, models exhibit underlying masculine bias that elevates masculine coreferent probabilities above neutral alternatives.
- Mechanism: Masculine forms are overrepresented in training corpora, creating higher baseline log probabilities. For neutral antecedents, masculine coreferents receive ~3× higher probability than feminine (N:M/N:F = e^1.107 ≈ 3.03). In German, this bias dominates entirely, making coreferent gender the main predictor rather than the antecedent-coreferent interaction.
- Core assumption: The masculine generic convention in training data creates persistent prior probabilities that partially override prompt context.
- Evidence anchors:
  - [abstract] "In German, this bias is much stronger, overriding all tested gender-neutralization strategies"
  - [section 4.1] "For both feminine and neutral antecedents, masculine coreferents are second-most likely"
  - [section 4.1] "the main effect of coreferent gender is statistically significant and large (F(2, 10536) = 2601.35, η² = 0.33)" for German
  - [corpus] "Gender Bias in English-to-Greek Machine Translation" documents similar cross-linguistic bias patterns
- Break condition: Fine-tuning with gender-neutral data can shift priors (Figure 6b shows singular "they" dominance after intervention).

### Mechanism 3: Gender-Inclusive Strategy Partial Efficacy
- Claim: German gender-inclusive strategies (asterisk, colon, underscore forms) increase feminine and neutral coreferent probabilities but cannot fully overcome masculine baseline bias.
- Mechanism: Forms like "Tierärzt*innen" contain feminine suffixes ("-innen") that activate feminine associations, partially achieving inclusive intent. However, masculine bias remains strong enough that "Männer" still has highest probability across all conditions.
- Core assumption: The feminine suffix in inclusive forms provides a sufficient gender signal to shift probability distributions, though not enough to overcome masculine priors.
- Evidence anchors:
  - [section 4.1] "all German gender-inclusive language strategies lead to an increase in the probability of feminine and gender-neutral coreferents"
  - [section 4.2] "gender-inclusive antecedents invoke gender-neutral coreferents" in generation experiments
  - [corpus] "Gender-Neutral Machine Translation Strategies in Practice" addresses similar strategy effectiveness questions
- Break condition: Strategy efficacy is context-dependent; generation experiments show more neutral outputs than probability measurements, suggesting model repetition behavior.

## Foundational Learning

- **Concept: Coreference Resolution (CR)**
  - Why needed here: The entire methodology depends on understanding how LLMs resolve references across sentences—antecedent (first mention) to coreferent (subsequent reference).
  - Quick check question: Given "The doctors arrived. Some of ___ were tired," would you expect the model to prefer "they," "he," or "she" as the coreferent?

- **Concept: Log Probability Measurement**
  - Why needed here: The paper measures bias via log(p) of specific coreferent tokens, requiring understanding of how LLM probability distributions work.
  - Quick check question: If log(p) for "men" = -2.0 and log(p) for "women" = -3.0, which token has higher raw probability and by what factor?

- **Concept: Grammatical vs. Notional Gender Languages**
  - Why needed here: English (notional) and German (grammatical) differ fundamentally in gender marking, explaining why bias manifests differently across languages.
  - Quick check question: In German, why does "die Tierärzt*innen" require gender marking on the article while English "the veterinarians" does not?

## Architecture Onboarding

- **Component map:**
  - Template dataset -> Model inference -> Log probability extraction -> Statistical analysis -> Generation annotation

- **Critical path:**
  1. Create template dataset (44 templates × antecedent variants × coreferent variants)
  2. Run inference to extract log probabilities at coreferent position
  3. Conduct ANOVA to quantify effect sizes (antecedent, coreferent, interaction)
  4. Run generation experiments with human annotation for validation
  5. Compare across model sizes and languages

- **Design tradeoffs:**
  - Smaller model range (1.5–32B) vs. SOTA scale (671B+): limits generalizability
  - Fixed coreferent vocabulary (men/women/people; he/she/they) vs. open vocabulary: follows psycholinguistic protocol but may miss model behavior
  - Single German model vs. suite: underpowered for grammatical-gender language conclusions
  - Author annotation for German pilot: potential annotator bias

- **Failure signatures:**
  - Masculine coreferent probability remains highest across ALL antecedent types → masculine bias dominates
  - Singular "they" probability near zero for neutral antecedents → model rejects singular interpretation
  - Generation shows more neutral outputs than probability measurement → model relies on repetition, not true gender-neutral understanding
  - Low inter-annotator agreement on coreference labels (κ < 0.5) → ambiguous generations

- **First 3 experiments:**
  1. **Replicate English plural coreferent probability with a larger model (>70B):** Does masculine bias attenuate at scale? Measure log(p) for men/women/people across 34 antecedent triplets in Qwen2.5 32B pattern.
  2. **Test additional German models (at least 3):** Current single-model finding (coreferent gender dominates interaction) needs validation across model families and sizes.
  3. **Expand coreferent vocabulary:** Replace forced-choice (men/women/people) with open generation, then measure probability of the model's top-k tokens to capture behavior beyond protocol constraints.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do masculine bias patterns and difficulties with singular "they" persist in models significantly larger than 32B parameters?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section that "Future research is needed to determine whether our findings hold for these larger models."
- Why unresolved: Hardware constraints restricted the study to models with 1.5–32 billion parameters, while state-of-the-art models (e.g., DeepSeek-V3) exceed 600 billion parameters.
- What evidence would resolve it: Replicating the coreferent probability and generation experiments on models with 70B+ parameters to see if masculine bias diminishes with scale.

### Open Question 2
- Question: How does extending the context window beyond sentence pairs influence the stability of gender-inclusive coreference resolution?
- Basis in paper: [explicit] The authors note that "LLMs often handle longer contexts" and conclude that "future research should be conducted in a setting with a longer context."
- Why unresolved: The current methodology relied on short, two-sentence templates adapted from psycholinguistics, which may not reflect the temporal dependencies of longer text generation.
- What evidence would resolve it: Evaluating gender consistency in coreference chains within extended narrative contexts rather than isolated sentence pairs.

### Open Question 3
- Question: Does testing a broader range of coreferent candidates reveal different gender association probabilities than the fixed set used in this study?
- Basis in paper: [explicit] The authors acknowledge the limitation that they "used the same coreferents" but suggest it "would have been possible to measure the probability of several coreferent candidates."
- Why unresolved: Restricting output options to specific nouns (men/women/people) may artificially constrain the model's natural probabilistic distribution of gendered terms.
- What evidence would resolve it: Measuring the probability distribution over the full vocabulary for gendered associations following gender-inclusive antecedents.

## Limitations
- Limited model diversity with only six English models (1.5–32B) and one German model (7B)
- Restricted coreferent vocabulary to men/women/people (English) and Männer/Frauen/Personen (German)
- Single German model finding needs validation across multiple model families and sizes
- Forced-choice probability measurement protocol may miss model behavior with other coreferents

## Confidence

**High Confidence**: The finding that English models maintain antecedent-coreferent gender consistency in plural contexts is well-supported by large effect sizes (η² = 0.19) and statistically significant interactions across multiple models. The masculine bias finding (men having ~3× higher probability than women for neutral antecedents) shows consistent patterns across the English model suite.

**Medium Confidence**: The claim about German models exhibiting stronger masculine bias that overrides gender-neutral strategies is based primarily on a single model, though the effect sizes are large. The partial efficacy of gender-inclusive strategies in German (increasing feminine/neutral coreferent probabilities) shows promise but requires validation across more models and languages.

**Low Confidence**: The singular "they" findings are particularly uncertain due to extremely low probabilities across all models, suggesting the protocol may not effectively elicit this form or that models fundamentally struggle with singular interpretation. The generation experiment findings showing more neutral outputs than probability measurements need further investigation to understand whether this reflects true understanding or repetition behavior.

## Next Checks

1. **Scale validation experiment**: Replicate the English plural coreferent probability measurements with a >70B parameter model (e.g., Qwen2.5 32B or larger) to test whether masculine bias attenuates at scale. This would test whether the consistency mechanism holds across the full parameter range.

2. **German model family expansion**: Test at least three additional German models across different architectures and sizes to validate whether coreferent gender truly dominates antecedent-coreferent interaction, or if this finding is model-specific. Include both decoder-only and encoder-decoder architectures.

3. **Coreferent vocabulary expansion**: Replace the forced-choice protocol with open generation followed by probability measurement of the model's top-5 tokens at the coreferent position. This would capture whether models generate other coreferents (like "folks," "individuals," or German "Personen") that the current protocol misses, providing a more complete picture of gender-inclusive language processing.