---
ver: rpa2
title: On Finding Inconsistencies in Documents
arxiv_id: '2512.18601'
source_url: https://arxiv.org/abs/2512.18601
tags:
- evidence
- document
- inconsistencies
- documents
- description
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce FIND, a benchmark for detecting inconsistencies in
  long, complex documents. We evaluate 11 models on FIND, finding that the best model
  (gpt-5) recovers 64% of inserted inconsistencies, with high precision on undiscovered
  inconsistencies.
---

# On Finding Inconsistencies in Documents

## Quick Facts
- arXiv ID: 2512.18601
- Source URL: https://arxiv.org/abs/2512.18601
- Reference count: 40
- Primary result: Best model (gpt-5) recovers 64% of inserted inconsistencies with high precision

## Executive Summary
This paper introduces FIND, a benchmark designed to evaluate models' ability to detect inconsistencies in long, complex documents. The authors test 11 different models on this benchmark, finding that even the best-performing model (gpt-5) only recovers 64% of inserted inconsistencies. Performance notably decreases as document length increases and varies by inconsistency type, highlighting the ongoing challenge of automated inconsistency detection despite promising results.

## Method Summary
The authors constructed the FIND benchmark to systematically evaluate inconsistency detection capabilities across multiple models. They inserted controlled inconsistencies into documents of varying lengths and complexity, then measured model performance using precision and recall metrics. The evaluation included 11 different models, with particular attention to how performance varied with document length and inconsistency type.

## Key Results
- gpt-5 achieved the highest performance, recovering 64% of inserted inconsistencies
- High precision was maintained on undiscovered inconsistencies
- Performance decreased significantly with increasing document length and certain inconsistency types

## Why This Works (Mechanism)
The benchmark approach works by creating controlled test conditions that isolate the inconsistency detection capability from other document understanding tasks. By systematically varying document length and inconsistency type while measuring both precision and recall, the evaluation provides clear insights into model strengths and weaknesses in identifying contradictions, factual errors, and logical inconsistencies.

## Foundational Learning
- **Inconsistency detection**: Identifying contradictions and errors within text - needed to understand what the benchmark measures; quick check: can identify simple logical contradictions
- **Document complexity**: How structure and length affect comprehension - needed to interpret performance degradation; quick check: can parse nested document structures
- **Benchmark methodology**: Controlled testing with inserted errors - needed to evaluate results validity; quick check: understands synthetic vs. natural error distributions
- **Precision-recall tradeoff**: Balance between false positives and false negatives - needed to interpret model performance; quick check: can calculate and interpret these metrics

## Architecture Onboarding
Component map: Document preprocessing -> Inconsistency insertion -> Model evaluation -> Performance analysis
Critical path: Model receives document -> Processes content -> Identifies potential inconsistencies -> Outputs flagged locations
Design tradeoffs: Synthetic inconsistencies provide control but may not reflect real-world patterns; model diversity provides breadth but makes direct comparisons difficult
Failure signatures: Performance drops with document length suggest comprehension limits rather than pattern matching; type-specific failures indicate model knowledge gaps
First experiments:
1. Test on professionally authored documents with natural inconsistencies
2. Vary inconsistency density while holding document length constant
3. Compare cross-model agreement on same documents

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions, which represents a limitation in contextualizing the research within the broader field. Critical questions that remain include: How do models generalize from synthetic to natural inconsistencies? What architectural features most improve inconsistency detection? How does human performance compare to the best models?

## Limitations
- Synthetic inconsistencies may not reflect real-world document error patterns
- Performance degradation with length suggests surface-level processing rather than true understanding
- Single best-model focus may obscure architectural differences in handling inconsistencies
- Lack of human baseline comparison makes it difficult to assess absolute model capability

## Confidence
High confidence: Benchmark construction and evaluation framework appear methodologically sound
Medium confidence: Results support claim that inconsistency detection remains challenging
Low confidence: Generalizability to real-world professional document review is uncertain due to synthetic inconsistency approach

## Next Checks
1. Test model performance on professionally authored documents with naturally occurring inconsistencies to assess real-world applicability
2. Conduct ablation studies varying document length, inconsistency type distribution, and document domain
3. Compare model performance against human annotators on identical documents to establish baseline human performance
4. Investigate model architectures that could better handle longer documents and complex inconsistency types