---
ver: rpa2
title: Enhancing Control Policy Smoothness by Aligning Actions with Predictions from
  Preceding States
arxiv_id: '2601.18479'
source_url: https://arxiv.org/abs/2601.18479
tags:
- action
- asap
- lipschitz
- state
- states
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of high-frequency action oscillations
  in deep reinforcement learning policies, which can cause wear and safety issues
  in real-world deployments. The authors propose ASAP (Action Smoothing by Aligning
  Actions with Predictions from Preceding States), a novel loss-based method that
  defines transition-induced similar states as the distribution of next states from
  the same previous state, better capturing system dynamics than synthetic alternatives.
---

# Enhancing Control Policy Smoothness by Aligning Actions with Predictions from Preceding States

## Quick Facts
- **arXiv ID**: 2601.18479
- **Source URL**: https://arxiv.org/abs/2601.18479
- **Reference count**: 7
- **Primary result**: ASAP achieves up to 89.5% improvement in smoothness score while maintaining or improving policy performance across Gymnasium and Isaac-Lab environments

## Executive Summary
This paper addresses high-frequency action oscillations in deep reinforcement learning policies that cause wear and safety issues in real-world deployments. The authors propose ASAP (Action Smoothing by Aligning Actions with Predictions from Preceding States), a novel loss-based method that defines transition-induced similar states as the distribution of next states from the same previous state. ASAP combines spatial regularization that aligns current actions with predicted actions from previous states, and temporal regularization that penalizes second-order action differences to suppress high-frequency oscillations. Experiments demonstrate significant reductions in action oscillations while maintaining or improving policy performance compared to existing methods.

## Method Summary
ASAP adds three loss terms to base RL loss: (1) Spatial loss L_S = ||π_ϕ(s_t) - stopgrad(π_P(s_{t-1}))||² aligning current action with predicted action from previous state; (2) Prediction loss L_P = ||π_P(s_{t-1}) - stopgrad(π_ϕ(s_t))||² training predictor head; (3) Temporal loss L_T = ||(a_{t+1} - 2a_t + a_{t-1})/(tanh(a_{t+1} - a_{t-1}) + ε)||² penalizing second-order differences. The method uses transition-induced similar states (next states from same predecessor) rather than synthetic alternatives, better capturing system dynamics. Actor MLP has both action head and prediction head sharing a common backbone.

## Key Results
- ASAP achieves up to 89.5% improvement in smoothness score (sm↓) across multiple environments
- Maintains or improves cumulative return (re↑) compared to baseline methods
- Compatible with architectural approaches like LipsNet while providing superior smoothness
- Effective in both Gymnasium (MuJoCo) and Isaac-Lab (Franka, Allegro, Anymal) environments

## Why This Works (Mechanism)

### Mechanism 1: Transition-Induced Similar State Definition
Defining similar states as the distribution of next states from the same predecessor captures system dynamics more accurately than synthetic alternatives. Rather than sampling from arbitrary Gaussians (CAPS) or constructing synthetic neighbors (L2C2), this approach uses only empirically observed transitions. Under assumptions of Lipschitz-continuous transition dynamics and bounded noise, these states form a bounded neighborhood suitable for local Lipschitz constraints.

### Mechanism 2: Spatial Loss via Predicted Action Alignment
Aligning current actions with predicted actions from preceding states enforces action consistency without requiring synthetic state generation. A prediction head π_P trained to predict expected action E[π_ϕ(s_t) | s_{t-1}] provides a target. The spatial loss L_S minimizes ||π_ϕ(s_t) - stopgrad(π_P(s_{t-1}))||², propagating the transition function's Lipschitz property through the composite function f∘T.

### Mechanism 3: Temporal Regularization via Second-Order Differences
Penalizing second-order action differences suppresses high-frequency oscillations while permitting flexible action changes. L_T = ||(a_{t+1} - 2a_t + a_{t-1}) / (tanh(a_{t+1} - a_{t-1}) + ε)||² constrains acceleration in action space. The tanh denominator normalizes across action scales.

## Foundational Learning

- **Lipschitz Continuity (Global vs. Local)**: The entire theoretical foundation relies on Lipschitz constants bounding output changes. Understanding K_glob vs. K_loc explains why local constraints adapt better to varying state-space regions. Quick check: Given f(x) = x², what is the local Lipschitz constant in neighborhood [a, b]?

- **Transition Dynamics and Noise Models**: ASAP's similar-state definition requires understanding how transition functions propagate noise into state distributions. The bounded noise assumption (Assumption 2) is critical but not universally true. Quick check: In a stochastic environment with unbounded noise (e.g., Gaussian), does Lemma 1's bounded neighborhood guarantee hold?

- **Stop-Gradient Operations**: The spatial loss uses stopgrad on predictor output while temporal loss uses stopgrad on current policy. This asymmetric gradient blocking prevents circular training dynamics. Quick check: If stopgrad were removed from Eq. 14, how would gradients flow between π_ϕ and π_P?

## Architecture Onboarding

- **Component map**: Shared MLP Backbone -> Action Head (outputs π_ϕ(s_t)) and Prediction Head (outputs π_P(s_{t-1}) in parallel)

- **Critical path**: 1) Forward pass current state s_t through shared MLP → Action Head → a_t; 2) Forward pass previous state s_{t-1} through same MLP → Prediction Head → â_t; 3) Compute L_S = ||a_t - stopgrad(â_t)||²; 4) Compute L_T using three consecutive action tuples; 5) Combine: J_total = J_RL + λ_S·L_S + λ_P·L_P + λ_T·L_T

- **Design tradeoffs**: λ_S vs. λ_P balance (stronger λ_P speeds predictor learning but creates moving-target instability); shared vs. separate backbones (sharing reduces parameters but couples representations); on-policy (PPO) requires more parallel environments than off-policy (SAC) due to predictor update frequency

- **Failure signatures**: High action variance with low reward (λ_S too low); low variance but poor reward (λ_S or λ_T too high); predictor loss diverges (moving-target problem); large neighborhood in high-noise environments (spatial regularization weakens)

- **First 3 experiments**: 1) Ablation on spatial term: Train PPO+ASAP with only L_S (λ_T=0), compare smoothness vs. CAPS spatial term on Hopper; 2) Hyperparameter sweep: Grid search λ_S ∈ {0.01, 0.1, 1.0} and λ_T ∈ {0.001, 0.01, 0.1} on Walker; 3) Noise sensitivity test: Inject increasing observation noise (σ ∈ {0.01, 0.05, 0.1}) into Reacher; measure when spatial regularization effectiveness degrades

## Open Questions the Paper Calls Out

- **Adaptive λ_S for High-Noise Environments**: How can the spatial regularization weight λ_S be adapted automatically in environments with high observation noise to prevent the "enlarged neighborhood" from degrading the local Lipschitz constraint? The paper currently relies on manual adjustment, lacking an adaptive mechanism for robust deployment in noisy real-world settings.

- **Exploration Efficiency in Sparse Rewards**: To what extent does the spatial smoothing constraint hinder exploration efficiency in sparse reward settings, and can this be mitigated without sacrificing smoothness? The Isaac-Lab "Lift-Cube" results show variance increase attributed to oscillation suppression constraining exploration.

- **Moving Target Bottleneck in On-Policy Methods**: Is the "moving target" issue inherent to the prediction head training a significant bottleneck for on-policy algorithms compared to off-policy methods? The paper proposes increasing parallel environments as a workaround but doesn't fundamentally resolve the instability caused by rapidly shifting policy distribution.

## Limitations
- The bounded noise assumption required for Lemma 1's neighborhood guarantee may not hold in many real-world systems with unbounded or heavy-tailed noise distributions
- The predictor "moving target" problem during training is acknowledged but not fully resolved—proposed mitigation through parallel environments may not scale to all practical scenarios
- Interaction between λ_S and λ_P hyperparameters is not fully characterized, with instability noted but comprehensive guidelines lacking

## Confidence

- **High Confidence**: The temporal regularization mechanism (L_T) is well-established from prior work (Grad-CAPS) and directly addresses high-frequency oscillations through second-order differences. The smoothness metric calculation and experimental results are reproducible given the stated methodology.

- **Medium Confidence**: The spatial regularization's theoretical foundation via Lipschitz constraints is sound, but empirical validation of the predictor's accuracy in learning E[π_ϕ(s_t)|s_{t-1}] across diverse environments is limited. The bounded neighborhood claim depends on assumptions that may not hold universally.

- **Low Confidence**: The interaction between λ_S and λ_P hyperparameters is not fully characterized. The paper notes instability but doesn't provide comprehensive guidelines for hyperparameter tuning across different environment families.

## Next Checks

1. **Noise Sensitivity Analysis**: Systematically vary observation noise levels (σ ∈ {0.01, 0.05, 0.1}) across multiple environments to quantify when spatial regularization effectiveness degrades.

2. **Predictor Accuracy Evaluation**: Measure the predictor's ability to accurately estimate E[π_ϕ(s_t)|s_{t-1}] by comparing predicted vs. actual actions across state distributions, not just during training.

3. **Adaptive Regularization**: Implement environment-dependent scaling of λ_S based on estimated neighborhood size to maintain effectiveness in varying noise conditions.