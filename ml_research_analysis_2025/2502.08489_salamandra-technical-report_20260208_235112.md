---
ver: rpa2
title: Salamandra Technical Report
arxiv_id: '2502.08489'
source_url: https://arxiv.org/abs/2502.08489
tags:
- data
- language
- arxiv
- dataset
- salamandra
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Salamandra is a suite of open-source decoder-only large language
  models available in 2B, 7B, and 40B parameters, trained from scratch on multilingual
  data in 35 European languages and code. The models achieve competitive performance
  on multilingual benchmarks compared to similarly sized open-source models, with
  Salamandra 40B outperforming many baselines in tasks like reading comprehension
  and translation for Catalan and Basque.
---

# Salamandra Technical Report

## Quick Facts
- arXiv ID: 2502.08489
- Source URL: https://arxiv.org/abs/2502.08489
- Reference count: 40
- Salamandra achieves competitive multilingual performance with 35 European languages and code

## Executive Summary
Salamandra is a suite of open-source decoder-only large language models (2B, 7B, 40B parameters) trained from scratch on 35 European languages and code. The models demonstrate competitive performance on multilingual benchmarks compared to similarly sized open-source models, with particular strength in Catalan and Basque tasks. Instruction-tuned variants show improved task-following capabilities and reduced harmful content generation. The project emphasizes reproducibility through open-source training and evaluation scripts, with models released under Apache 2.0 license.

## Method Summary
Salamandra models use decoder-only transformer architectures with RoPE positional embeddings and SwiGLU activation. The 7B and 40B models employ Grouped-Query Attention for improved inference efficiency, while the 2B model uses Multi-Head Attention. Pre-training involves ~12.9T tokens (2B/7B) or 9T tokens (40B) with factor sampling to balance language representation. A final "continued training" phase uses high-quality curated data including Wikipedia and Aya Collection. Instruction tuning follows using FastChat with NEFTune noise. A custom SentencePiece BPE tokenizer with 256k vocabulary is trained on uniform language distribution to improve low-resource language representation.

## Key Results
- Salamandra 40B outperforms many baselines in reading comprehension and translation for Catalan and Basque
- Instruction-tuned models show improved task-following and reduced harmful content generation
- Preliminary vision experiments demonstrate image understanding and grounding capabilities
- Models achieve competitive multilingual performance compared to similarly sized open-source models

## Why This Works (Mechanism)

### Mechanism 1: Uniform Tokenizer Distribution for Low-Resource Efficiency
If a tokenizer is trained on a uniform distribution of languages rather than the natural pre-training distribution, it reduces the token fertility (tokens-per-word) for low-resource languages, potentially improving their representation within the fixed context window. By exposing the tokenizer vocabulary learning algorithm to equal amounts of all languages, rare sub-words in low-resource languages (like Basque or Galician) are prioritized over redundant sub-words in high-resource languages (like English). This prevents "over-segmentation" of low-resource text, allowing the model to "see" more actual content per context token.

### Mechanism 2: Data Factor Sampling to Mitigate Dominance
If dominant languages (English/Code) are undersampled and target languages are oversampled during pre-training, the model develops stronger multilingual capabilities than training on raw web-distributions. Raw web data is heavily skewed toward English (>50-60%). Without intervention, gradient updates are dominated by English patterns. By applying a "factor sampling" (undersampling dominant data by 0.5x and oversampling others), the optimizer is forced to allocate capacity to non-dominant linguistic structures, balancing the loss landscape across languages.

### Mechanism 3: High-Quality "Annealing" Phase
A final, short pre-training phase ("continued training") using exclusively high-quality data and instruction-formatted data improves downstream performance more efficiently than training on the same mixture for longer. The learning rate schedule is typically decaying. By introducing a curated set of high-quality tokens (Wikipedia, educational web text, code) right before convergence, the model fine-tunes its weights on "cleaner" reasoning patterns. The inclusion of instruction-formatted data (Aya Collection) in this phase acts as a soft warm-start for the subsequent instruction-tuning stage.

## Foundational Learning

- **Concept: BPE Tokenization & Fertility**
  - Why needed here: Salamandra uses a custom BPE tokenizer with a massive 256k vocabulary size. Understanding "fertility" (tokens per word) is critical to evaluating why they chose this size and the uniform training strategy.
  - Quick check question: How does a 256k vocabulary size affect the embedding layer size vs. the context length efficiency for a 7B parameter model?

- **Concept: Multilingual Interference ("Curse of Multilinguality")**
  - Why needed here: The paper explicitly discusses the trade-off between adding languages and maintaining performance. Understanding that capacity is finite helps explain the need for careful data balancing.
  - Quick check question: Does increasing the number of languages in a fixed-parameter model usually improve or degrade performance on the highest-resource language?

- **Concept: Grouped-Query Attention (GQA)**
  - Why needed here: The 7B and 40B models use GQA while the 2B model uses MHA. This architectural choice impacts inference speed and memory usage, which is a key system consideration.
  - Quick check question: How does GQA differ from Multi-Head Attention (MHA) in terms of KV-cache memory requirements during inference?

## Architecture Onboarding

- **Component map:** Text -> Tokenizer (SentencePiece BPE, 256k vocab) -> Transformer Decoder (RMSNorm, SwiGLU) -> RoPE -> FlashAttention 2 (2B: MHA; 7B/40B: GQA with 8 groups) -> Linear output layer

- **Critical path:** Data Curation (aggregate 35 languages → Filter/Curate → Apply Factor Sampling) → Tokenizer Training (train on uniform distribution) → Pre-training (7T/9T tokens with Cosine Decay) → Continued Training (315B high-quality tokens) → Post-training (SFT on specific languages)

- **Design tradeoffs:**
  - Uniform vs. Natural Tokenizer: Uniform training lowers fertility for low-resource languages (good) but slightly raises it for English (bad for efficiency). They prioritize minority languages.
  - Vocab Size (256k): A large vocabulary increases the non-embedding parameter count (significant for the 2B model) but improves multilingual encoding efficiency.
  - GQA vs MHA: GQA (used in 7B/40B) sacrifices a theoretical tiny amount of quality for significantly faster inference (KV-cache reduction) compared to MHA.

- **Failure signatures:**
  - High Perplexity on Low-Resource Languages: Indicates the "Uniform Tokenizer" was trained on skewed data or the vocabulary was too small (over-segmentation).
  - Instruction Hallucination: Indicates the SFT data was low quality or the "Continued Training" phase did not sufficiently prime the model for instruction following.
  - Catastrophic Forgetting: If the "Continued Training" phase was too aggressive or long, the model might lose general knowledge from the initial pre-training.

- **First 3 experiments:**
  1. Tokenizer Fertility Benchmark: Measure tokens-per-word for Basque/Galician on your target data using the Salamandra tokenizer vs. a standard Llama tokenizer to quantify the "Uniform Distribution" benefit.
  2. Language Ablation: Evaluate the base model on a specific task (e.g., translation) using 5-shot vs. 0-shot to gauge how well the "Continued Training" phase primed the model without explicit instructions.
  3. Inference Throughput Test: Compare inference tokens/second between the 7B (GQA) and 2B (MHA) models to validate the efficiency gains claimed by the GQA architecture.

## Open Questions the Paper Calls Out

### Open Question 1
Why does the Salamandra 40B base model underperform the smaller 7B variant on the Basque WNLI benchmark? Section 5.2.3 states that "anomalies, such as Salamandra 40B underperforming its smaller 7B counterpart in Basque WNLI, warrant further investigation." The authors present the performance gap as an unexpected observation without providing a root cause analysis for this scaling failure in the report.

### Open Question 2
Is the low summarization performance observed in Salamandra models a limitation of the evaluation metric (BLEU) or a genuine deficiency in generation capability? Section 5.2.3 notes that "Summarization results are surprisingly low in BLEU scores, indicating potential issues with either the task or the metric that we plan to revise in the future." BLEU scores often correlate poorly with human judgment in abstractive summarization, leaving the actual semantic quality of the summaries ambiguous.

### Open Question 3
How does Salamandra's resistance to adversarial attacks change when evaluated on multi-turn conversations rather than single-turn prompts? Section 6.4.2 notes that "research shows that multi-turn conversations increment the probability of harmful answers," and explicitly lists expanding to multi-turn conversations as future work. The current safety evaluation relies on single-turn "prompt-answer" pairs, which may not capture vulnerabilities exposed through incremental context manipulation.

## Limitations

- Data quality and representativeness uncertainties due to reliance on heuristic filtering thresholds and unspecified continued training phase composition
- Low-resource language coverage limitations despite uniform tokenizer strategy, with some languages having extremely limited training data
- Evaluation scope constraints primarily focusing on multilingual benchmarks with emphasis on European languages rather than broader generalization
- Training configuration reproducibility challenges due to unspecified exact factor sampling ratios and specific software versions

## Confidence

**High Confidence:**
- Technical architecture specifications (transformer decoder, GQA, RoPE base 10,000)
- Training procedure steps (pre-training → continued training → instruction tuning)
- Base model multilingual performance claims (competitive with similar-sized models)

**Medium Confidence:**
- Instruction-tuned model improvements in task-following and harm reduction
- Specific performance gains for Catalan and Basque languages
- Vision modality capabilities (preliminary nature acknowledged)

**Low Confidence:**
- Long-term stability of instruction-tuned models in production settings
- Cross-domain generalization beyond evaluated benchmarks
- True efficiency gains of uniform tokenizer strategy vs. natural distribution

## Next Checks

1. **Tokenizer Fertility Validation** - Measure tokens-per-word for Basque, Galician, and Irish using the Salamandra tokenizer versus a standard Llama tokenizer on representative text samples to empirically validate the uniform distribution strategy's impact on low-resource language efficiency.

2. **Language Capacity Abation Study** - Train smaller ablation models (1B-3B) with varying numbers of languages (5, 10, 20, 35) to quantify the "curse of multilinguality" and determine whether Salamandra's 7B model has sufficient capacity to support all 35 languages effectively.

3. **Continued Training Distribution Analysis** - Reconstruct the exact composition of the 315B token continued training phase using the percentage breakdowns provided, then evaluate model performance with and without this phase to isolate its contribution to final benchmark results.