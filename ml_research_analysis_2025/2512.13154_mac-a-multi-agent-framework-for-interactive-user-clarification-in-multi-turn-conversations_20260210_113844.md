---
ver: rpa2
title: 'MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn
  Conversations'
arxiv_id: '2512.13154'
source_url: https://arxiv.org/abs/2512.13154
tags:
- user
- clarification
- domain
- agent
- multi-agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MAC is a multi-agent framework for interactive user clarification
  in multi-turn conversations. It enables distributed agents to autonomously coordinate
  clarification strategies, addressing ambiguity resolution challenges in complex
  conversational systems.
---

# MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations

## Quick Facts
- **arXiv ID**: 2512.13154
- **Source URL**: https://arxiv.org/abs/2512.13154
- **Reference count**: 37
- **Primary result**: Hierarchical clarification framework increases task success rate by 7.8% and reduces dialogue turns from 6.53 to 4.86 on MultiWOZ 2.4

## Executive Summary
MAC is a multi-agent framework that addresses ambiguity resolution in task-oriented dialogue systems through distributed clarification. The framework introduces a novel taxonomy for categorizing user ambiguities and assigns clarification responsibilities between a manager (Supervisor) and domain-specific experts. By enabling both levels to ask clarifying questions before executing API calls, MAC proactively elicits complete user information upfront, minimizing repetition and improving task completion rates. Evaluations demonstrate consistent performance gains across different LLM models, with open-source variants showing particularly large relative improvements from clarification.

## Method Summary
MAC implements a hierarchical multi-agent system where a Supervisor agent routes queries to domain-specific Expert agents, with both levels capable of asking clarification questions based on defined taxonomies. The Supervisor handles domain-agnostic ambiguities (7 categories) while Experts handle domain-specific ambiguities (5 categories). Agents use structured outputs (`<clarify>`, `<route>`, `<domain>`) to coordinate behavior, with only one clarification issued per turn to limit latency. The framework uses MultiWOZ 2.4 dataset with GPT-4o-based user simulator and judge, operating without fine-tuning through prompt engineering alone.

## Key Results
- Clarification at both Supervisor and Expert levels increases task success rate by 7.8% (54.5% to 62.3%) on MultiWOZ 2.4
- Average dialogue turns reduced from 6.53 to 4.86 through proactive information elicitation
- MAC outperforms previous state-of-the-art task-oriented dialogue systems by 11.50%
- Open-source model Qwen3-235B-A22B shows larger relative gains (+7.28%) from clarification than GPT-4o

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical clarification with role-separated responsibilities improves both task success and conversational efficiency
- Mechanism: The Supervisor resolves domain-agnostic ambiguities (e.g., "What domain?") using commonsense reasoning before routing, while Domain Experts resolve specialized ambiguities (e.g., "Which cuisine?") using database access. This separation prevents redundant clarification and ensures questions come from the agent with appropriate context.
- Core assumption: Users will provide coherent responses to clarification questions within a reasonable number of turns; the taxonomy covers the majority of ambiguity types encountered in practice.
- Evidence anchors: [abstract] "enabling clarification at both levels increases task success rate by 7.8% (54.5% to 62.3%) and reduces average dialogue turns from 6.53 to 4.86"; [section 5.3] "high-level disambiguation with supervisor is particularly crucial for robust multi-agent dialogue" (ablation shows -6.20 drop when removed)

### Mechanism 2
- Claim: Proactive upfront information elicitation reduces conversational bloat without sacrificing task completion
- Mechanism: By enabling both agent levels to ask clarifying questions before executing API calls, the system collects complete slot values early rather than discovering missing parameters mid-task. The structured `<clarify>` tag enforces explicit clarification decisions at each turn.
- Core assumption: Asking targeted questions does not frustrate users; the clarification questions generated by LLMs are semantically appropriate.
- Evidence anchors: [abstract] "eliciting all required user information up front and minimizing repetition"; [section 4.1] "Only one clarification is issued per turn to limit latency"

### Mechanism 3
- Claim: Multi-agent clarification outperforms single-agent clarification due to focused context per agent
- Mechanism: Each agent operates with a narrower, domain-relevant context window rather than internalizing all domain knowledge. The Supervisor maintains only routing-level context; Experts maintain domain-specific schemas and API constraints. This modularity reduces reasoning complexity per agent.
- Core assumption: The routing decision is reliable; errors at the Supervisor level propagate to Experts without recovery mechanisms.
- Evidence anchors: [section 5.5] "Multi-Agent Clarification (MAC) system outperforms Single-Agent Clarification (SAC) by 6% (52.4%→58.4%)"; [section 5.5] "multi-agent setup consistently achieves higher success rates than the single-agent approach even in scenarios without clarification"

## Foundational Learning

- Concept: **Belief State Tracking in Task-Oriented Dialogue**
  - Why needed here: MAC agents must track which slots are filled vs. underspecified across turns to decide when clarification is needed. The `is_ambiguous(u)` function depends on maintaining accurate belief states.
  - Quick check question: Given a user request "Book a table for two somewhere nice," can you identify which slots are known, unknown, and ambiguous?

- Concept: **Hierarchical Multi-Agent Coordination**
  - Why needed here: MAC's Supervisor-Expert structure requires understanding when to centralize decisions (routing) vs. delegate (domain-specific reasoning). The taxonomy explicitly partitions responsibilities.
  - Quick check question: If a user says "I need to get from Cambridge to London tomorrow morning," should the Supervisor clarify intent or immediately route? Why?

- Concept: **LLM Structured Output Control**
  - Why needed here: MAC relies on parsed outputs (`<clarify>`, `<route>`, `<domain>`) to control agent behavior. Understanding prompt engineering for reliable structured generation is critical.
  - Quick check question: What failure modes might occur if an LLM generates malformed tags or mixed clarification/response outputs?

## Architecture Onboarding

- Component map: User Query -> Supervisor (domain-agnostic ambiguity check) -> [Clarify] -> User Response -> [Clear] -> Route to Domain Expert -> [Clarify] -> User Response -> [Clear] -> Execute API Call -> Return Response

- Critical path:
  1. User query enters → Supervisor evaluates `is_ambiguous(u)` against supervisor taxonomy
  2. If ambiguous → Supervisor issues `<clarify>` → User responds → Return to step 1
  3. If clear → Supervisor outputs `<domain>X</domain>` → Route to Expert X
  4. Expert evaluates `is_ambiguous(u)` against expert taxonomy
  5. If ambiguous → Expert issues `<clarify>` → User responds → Return to step 4 (max 20 turns)
  6. If clear → Expert executes API call → Returns response

- Design tradeoffs:
  - **Clarification placement (Supervisor vs. Expert vs. Both)**: Both-level clarification yields highest success (62.3%) but requires coordinating two clarification-capable agents. Expert-only reduces overhead but misses high-level ambiguities. Supervisor-only creates routing bottlenecks.
  - **Model selection**: GPT-4o provides best performance; open-source (Qwen3-235B-A22B) shows larger relative gains (+7.28%) from clarification, narrowing the gap, but requires sufficient model scale.
  - **Latency vs. completeness**: Only one clarification per turn limits latency but may extend total turns if multiple clarifications are needed.

- Failure signatures:
  - **Clarification loops**: ~1% of cases show agent repeatedly asking same clarification without progress (noted in Limitations).
  - **Routing errors**: Supervisor misclassifies domain, sending query to wrong Expert with no recovery path.
  - **Hallucinated success**: LLM Judge may assign positive scores to incorrect trajectories (rare but acknowledged).
  - **Taxonomy gaps**: Ambiguity types outside the 12-category taxonomy are not systematically handled.

- First 3 experiments:
  1. **Baseline ablation**: Run MAC without clarification, with Supervisor-only clarification, with Expert-only clarification, and with Both. Compare Success Rate (Max@5 and Avg@5) and Avg. Turns. Expect Both > Supervisor > Expert > None.
  2. **Model backbone swap**: Replace GPT-4o with GPT-4o-mini and Qwen3-235B-A22B in both Supervisor and Expert roles. Measure clarification gain delta per model to verify architecture is model-agnostic.
  3. **Taxonomy ablation**: Remove "Ambiguity & Vagueness Handling" from Supervisor prompts and measure success drop (expect ~-6.2 based on paper). Then remove "Slot/Parameter-Blocking" from Expert prompts (expect ~-2.2). Confirm which taxonomy components are most critical.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can reinforcement learning enable agents to autonomously learn optimal clarification timing?
- **Basis in paper:** [explicit] The Future Work section suggests agents could learn timing by monitoring environmental signals via RL to "continuously self-update."
- **Why unresolved:** The current framework relies on pre-defined taxonomies and static prompting rather than dynamic, experience-based learning.
- **What evidence would resolve it:** Integrating an RL loop that improves clarification success rates over multiple episodes without manual tuning.

### Open Question 2
- **Question:** How can user satisfaction be quantified beyond task success and turn counts?
- **Basis in paper:** [explicit] The Future Work section states quantifying user satisfaction remains open, noting "dialogue naturalness" is currently unmeasured.
- **Why unresolved:** The study proxies efficiency solely through average dialogue turns and success rates.
- **What evidence would resolve it:** A new evaluation protocol incorporating subjective metrics for naturalness and user-centric interaction quality.

### Open Question 3
- **Question:** Can smaller LLMs be effectively trained or distilled to perform MAC's complex coordination tasks?
- **Basis in paper:** [explicit] The Limitations section states that "teaching these capabilities to smaller LLMs... remains an open challenge."
- **Why unresolved:** The experiments relied primarily on large, high-capability API-based models (GPT-4o, Qwen3).
- **What evidence would resolve it:** Benchmarks demonstrating sub-10B parameter models achieving comparable task success rates in the MAC framework.

## Limitations
- **Implementation gaps**: Key implementation details are underspecified, particularly regarding the user simulator's response generation logic and exact prompt templates for domain experts beyond the hotel example
- **Evaluation bias**: Reliance on GPT-4o as both simulator and judge may introduce simulator bias or judge hallucination in the success rate metric
- **Taxonomy completeness**: The clarification taxonomy, while comprehensive, may not cover all real-world ambiguity types, with no validation provided for its completeness

## Confidence

- **High confidence**: The core mechanism of hierarchical clarification (Supervisor vs. Expert) improving task success and reducing turns is well-supported by ablation results (62.3% vs 54.5% success rate, 4.86 vs 6.53 turns)
- **Medium confidence**: The model-agnostic nature of MAC's gains is plausible but based on limited model diversity (only 3 models tested, with open-source results showing larger relative improvements)
- **Low confidence**: The long-term user satisfaction implications of clarification-based interactions, given that high success rates might come at the cost of perceived verbosity or repeated questioning

## Next Checks
1. **Taxonomy Coverage Validation**: Systematically sample clarification failures and classify whether they stem from taxonomy gaps or implementation issues to quantify the taxonomy's real-world coverage
2. **User Satisfaction Measurement**: Replace the LLM judge with human evaluations measuring both task success AND user satisfaction ratings to detect potential trade-offs between completion rates and conversational quality
3. **Cross-Domain Coordination Testing**: Design multi-domain scenarios requiring Expert-to-Expert coordination (e.g., "Book a train to a city where I have a hotel reservation") to test whether the current Supervisor routing can handle complex, cross-domain reasoning