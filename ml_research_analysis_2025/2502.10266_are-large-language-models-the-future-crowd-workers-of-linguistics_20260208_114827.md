---
ver: rpa2
title: Are Large Language Models the future crowd workers of Linguistics?
arxiv_id: '2502.10266'
source_url: https://arxiv.org/abs/2502.10266
tags:
- human
- llms
- participants
- language
- crowd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether Large Language Models (LLMs) can\
  \ replace human participants in empirical linguistic research, addressing challenges\
  \ like participant engagement and data quality control. Two forced-choice elicitation\
  \ tasks\u2014one on Spanish-English code-switching and one on French neologism detection\u2014\
  were replicated using GPT-4o-mini with zero-shot prompting."
---

# Are Large Language Models the future crowd workers of Linguistics?

## Quick Facts
- arXiv ID: 2502.10266
- Source URL: https://arxiv.org/abs/2502.10266
- Reference count: 17
- Primary result: GPT-4o-mini achieved >97% accuracy on critical linguistic stimuli, outperforming human participants in gender assignment and neologism detection tasks.

## Executive Summary
This paper investigates whether Large Language Models (LLMs) can replace human participants in empirical linguistic research, addressing challenges like participant engagement and data quality control. Two forced-choice elicitation tasks—one on Spanish-English code-switching and one on French neologism detection—were replicated using GPT-4o-mini with zero-shot prompting. GPT-4o-mini outperformed human participants across all conditions, demonstrating high accuracy in gender assignment (mean gender congruency >0.96) and neologism detection (>97%). However, the model showed lower accuracy on filler items, prompting follow-up experiments. Chain-of-Thought prompting improved alignment with human performance, particularly for fillers, while the zero-shot-role approach had limited impact. Results suggest LLMs can effectively replicate linguistic tasks but may require tailored prompting strategies for nuanced alignment with human judgment.

## Method Summary
The study replicated two forced-choice linguistic elicitation tasks using GPT-4o-mini via OpenAI API with zero-shot prompting. The pipeline involved: (1) loading stimuli from original studies, (2) constructing prompts with persona information and task instructions, (3) running independent API queries for each simulated participant, and (4) comparing synthetic responses to human baselines. The baseline used zero-shot prompting where the model received participant profile information without examples. Follow-up experiments tested Chain-of-Thought prompting with worked examples and modified role prompts. Accuracy metrics included gender congruency for code-switching tasks and binary detection accuracy for neologism identification, with comparisons made across multiple experimental conditions.

## Key Results
- GPT-4o-mini achieved mean gender congruency >0.96 across all conditions in the Spanish-English code-switching task
- LLM outperformed humans in French neologism detection with >97% accuracy versus human performance of 0.87-0.95
- Zero-shot prompting showed significant assistant bias, with filler accuracy dropping to 0.77 compared to 0.97+ on critical items
- Chain-of-Thought prompting improved filler accuracy to 0.99 while maintaining high performance on critical stimuli

## Why This Works (Mechanism)

### Mechanism 1
Zero-shot prompting with persona assignment enables LLMs to simulate human participant behavior in forced-choice linguistic tasks. The model receives task instructions and participant profile information (e.g., native language) without examples, relying on pre-trained linguistic knowledge to generate contextually appropriate responses. Independent prompting runs simulate separate informants, preventing contamination between responses. Core assumption: LLMs encode sufficient linguistic knowledge from pre-training to approximate native speaker intuitions without task-specific fine-tuning.

### Mechanism 2
Chain-of-Thought (CoT) prompting improves alignment with human judgment patterns, particularly for edge cases like filler items. CoT elicits intermediate reasoning steps through worked examples, slowing model responses and introducing deliberation that better approximates human decision-making patterns rather than optimized assistant behavior. Core assumption: Human-like performance on filler items requires explicit reasoning scaffolding that counteracts training toward helpfulness over task fidelity.

### Mechanism 3
LLMs exhibit "over-performance" relative to humans due to training objectives that prioritize accuracy over naturalistic uncertainty. Models trained as helpful assistants avoid negative responses and uncertainty expressions, leading to systematic biases (e.g., preferring "yes" over "no" responses) that diverge from human judgment distributions. Core assumption: Training for assistant behavior creates response patterns that may conflict with accurate simulation of human experimental behavior.

## Foundational Learning

- **Concept: Zero-shot vs. few-shot prompting**
  - Why needed here: The baseline methodology relies on zero-shot prompting; understanding when to add examples (CoT) requires knowing the distinction.
  - Quick check question: Can you explain why zero-shot prompting was chosen as the baseline rather than providing the model with example responses?

- **Concept: Experimental design in linguistics (critical items vs. fillers)**
  - Why needed here: The filler item accuracy discrepancy reveals model behavior that diverges from human patterns; understanding experimental validity is essential for interpreting results.
  - Quick check question: Why might a model that achieves 97%+ accuracy on critical stimuli still be problematic as a human substitute?

- **Concept: API-based LLM interaction (roles, system prompts, independent runs)**
  - Why needed here: The pipeline requires structuring messages with system/user roles and running independent queries to simulate separate participants.
  - Quick check question: How does independent prompting prevent contamination between simulated "informants"?

## Architecture Onboarding

- **Component map**: Stimuli preprocessing -> Single-informant trial -> Scale replication -> Evaluation
- **Critical path**: Stimuli → Prompt construction (including persona + task) → API call → Response collection → Accuracy computation against human benchmark
- **Design tradeoffs**: Closed-source (GPT-4o-mini) vs. open-source offers accessibility and performance but limits transparency and incurs API costs; Zero-shot vs. CoT balances simplicity against potential assistant bias; Reaction time measurement not viable due to hardware variance
- **Failure signatures**: Consistent "yes" bias on binary choice tasks → assistant training interference; Near-perfect accuracy where humans show variation → over-performance unsuitable for error pattern studies; High variance across runs → prompt instability
- **First 3 experiments**: (1) Replicate Cruz_23 zero-shot baseline, verifying gender congruency patterns match human proportions within 0.05 margin; (2) Run Lombard_21 zero-shot baseline; identify filler accuracy drop as diagnostic of assistant bias; (3) Apply CoT prompting to Lombard_21; confirm filler accuracy improvement (target: >0.95) while monitoring critical item accuracy tradeoff

## Open Questions the Paper Calls Out

- **Open Question 1**: Can open-source LLMs replicate human performance in linguistic tasks with comparable alignment while ensuring better data transparency? Basis: The Conclusion states future studies could test replication design with open-source LLMs. Why unresolved: This study relied exclusively on closed-source GPT-4o-mini. What evidence would resolve it: Replication using open-source models (e.g., Llama, Mistral) yielding statistically similar alignment scores to human baselines.

- **Open Question 2**: Does the effectiveness of LLMs as crowd workers generalize to linguistic subfields and languages beyond the Romance family? Basis: Authors acknowledge "limited scale of this study" and restricted number of linguistic subfields tested. Why unresolved: Experiments limited to Spanish-English code-switching and French neologism detection. What evidence would resolve it: Successful application to typologically diverse languages and distinct linguistic domains (e.g., phonetics, syntax).

- **Open Question 3**: What computational metric can serve as a valid proxy for human reaction times to measure processing effort in LLMs? Basis: Section 5.2 notes API computation time is influenced by external factors and "cannot be considered a meaningful evaluation parameter." Why unresolved: Human reaction times measure cognitive load, but corresponding metric for LLM processing complexity is currently undefined. What evidence would resolve it: Identification of model-derived metric (e.g., token probability entropy) correlating significantly with human reaction time data across similar tasks.

## Limitations
- Closed-source GPT-4o-mini limits transparency in model behavior and prevents fine-tuning adjustments
- Zero-shot baseline shows significant assistant bias toward affirmative responses, particularly evident in 0.77 filler accuracy versus 0.97+ on critical items
- Study scope limited to two specific linguistic tasks with binary forced-choice formats, leaving open questions about applicability to other experimental paradigms

## Confidence
- **High confidence**: LLM capability to achieve >97% accuracy on critical stimuli in both linguistic tasks when using zero-shot prompting with persona specification
- **Medium confidence**: Chain-of-Thought prompting effectively improves filler item performance (from 0.77 to 0.99) by introducing deliberation that better approximates human decision patterns
- **Low confidence**: Generalization of these findings to other linguistic tasks requiring embodied cognition or real-world referential experience, given the zero-shot approach's reliance on pre-trained knowledge alone

## Next Checks
1. Test the zero-shot approach on a linguistic task requiring semantic reference to physical objects or spatial reasoning to determine break conditions for the mechanism claiming LLMs encode sufficient linguistic knowledge from pre-training
2. Apply the zero-shot and CoT methodologies to a different experimental paradigm (e.g., acceptability judgment scales or reaction time measurements) to assess generalizability beyond binary forced-choice tasks
3. Compare results using an open-source model (e.g., Llama-3-8B) with similar prompting strategies to evaluate whether the observed performance patterns are model-specific or more general to current LLMs