---
ver: rpa2
title: 'Beyond the Hype: Comparing Lightweight and Deep Learning Models for Air Quality
  Forecasting'
arxiv_id: '2512.09076'
source_url: https://arxiv.org/abs/2512.09076
tags:
- forecasting
- were
- data
- prophet
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study demonstrates that lightweight additive models\u2014\
  Facebook Prophet and NeuralProphet\u2014can deliver highly accurate air quality\
  \ forecasts for PM2.5 and PM10 in Beijing, achieving test R\xB2 above 0.94. By using\
  \ systematic feature selection, leakage-safe preprocessing, and a realistic 7-day\
  \ holdout evaluation, the models outperformed traditional baselines (SARIMAX) and\
  \ complex approaches (LSTM, LightGBM)."
---

# Beyond the Hype: Comparing Lightweight and Deep Learning Models for Air Quality Forecasting

## Quick Facts
- **arXiv ID:** 2512.09076
- **Source URL:** https://arxiv.org/abs/2512.09076
- **Reference count:** 36
- **Primary result:** Facebook Prophet and NeuralProphet achieved test R² above 0.94 for PM2.5 and PM10 forecasting in Beijing

## Executive Summary
This study systematically evaluates both lightweight and deep learning models for air quality forecasting, focusing on PM2.5 and PM10 concentrations in Beijing. Through rigorous methodology including leakage-safe preprocessing, systematic feature selection, and a realistic 7-day holdout evaluation, the research demonstrates that Facebook Prophet and NeuralProphet outperform traditional statistical models (SARIMAX) and complex approaches (LSTM, LightGBM). The findings challenge the assumption that more complex models necessarily yield better results, showing that interpretable additive models can deliver state-of-the-art accuracy while offering practical advantages in deployment and interpretability.

## Method Summary
The research employed a comprehensive evaluation framework using Beijing's air quality dataset spanning 2013-2020. Models were assessed using five-fold cross-validation with a 7-day holdout period, ensuring realistic forecasting conditions. Feature selection was conducted through backward elimination, and preprocessing incorporated leakage-safe techniques to prevent information contamination. The study compared five model categories: Facebook Prophet and NeuralProphet (additive models), SARIMAX (traditional statistical), LSTM (deep learning), and LightGBM (tree-based ensemble). Performance metrics included R², RMSE, MAE, and MAPE, with statistical significance testing via t-tests.

## Key Results
- Facebook Prophet achieved the highest consistency across all evaluation metrics with test R² above 0.94
- Lightweight models outperformed both traditional baselines (SARIMAX) and complex approaches (LSTM, LightGBM)
- NeuralProphet provided competitive accuracy while maintaining interpretability advantages
- All five model categories showed statistically significant differences in performance

## Why This Works (Mechanism)
The additive nature of Prophet and NeuralProphet models allows them to effectively capture multiple temporal patterns (trend, seasonality, holidays) through decomposable components. This structure aligns well with air quality data's inherent periodicity and temporal dependencies, while avoiding overfitting that often plagues more complex models. The systematic preprocessing and feature selection further enhance their ability to extract meaningful patterns without introducing leakage.

## Foundational Learning
- **Time series decomposition**: Understanding how additive models separate trend, seasonality, and residual components is essential for interpreting model behavior and feature importance.
- **Feature selection methodology**: Backward elimination and leakage-safe preprocessing are critical for preventing information contamination and ensuring valid model evaluation.
- **Cross-validation strategies**: Five-fold with holdout periods ensures robust performance assessment while maintaining temporal integrity of the data.
- **Performance metrics**: R², RMSE, MAE, and MAPE each capture different aspects of model accuracy, requiring understanding of their relative strengths for comprehensive evaluation.

## Architecture Onboarding

**Component Map:** Raw Data -> Preprocessing -> Feature Selection -> Model Training -> Cross-Validation -> Performance Evaluation

**Critical Path:** Data preprocessing and feature selection represent the most critical path, as leakage-safe techniques and systematic feature selection directly impact model validity and performance comparisons.

**Design Tradeoffs:** The study prioritizes interpretability and deployment efficiency over marginal accuracy gains, choosing lightweight models despite the availability of more complex alternatives. This tradeoff enables easier model interpretation and maintenance while maintaining competitive accuracy.

**Failure Signatures:** Models may underperform if preprocessing introduces temporal leakage or if feature selection fails to capture relevant meteorological patterns. Deep learning approaches like LSTM may overfit on relatively small air quality datasets.

**First Experiments:**
1. Replicate the feature selection process using backward elimination on a subset of the data
2. Compare Prophet's performance on detrended vs. raw data to understand trend component importance
3. Test cross-validation with different holdout periods to assess temporal sensitivity

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to Beijing's air quality data from 2013-2020, restricting generalizability
- Models relied solely on meteorological and temporal features, excluding potential confounders like traffic or industrial emissions
- Only tested 7-day forecasting horizon without evaluating longer-term predictions

## Confidence

**Major Claims Confidence:**
- Comparative model performance (High): Robust statistical evaluation across multiple metrics with clear performance gaps
- Lightweight models' superiority (Medium): Strong results but limited to specific dataset and conditions
- Interpretability advantages (Medium): Methodologically sound but not empirically validated through stakeholder feedback

## Next Checks
1. External validation on diverse air quality datasets from different cities and countries
2. A/B testing with domain experts to quantify interpretability benefits in operational settings
3. Extended forecasting horizon evaluation (14-30 days) to assess temporal degradation patterns