---
ver: rpa2
title: 'DISCODE: Distribution-Aware Score Decoder for Robust Automatic Evaluation
  of Image Captioning'
arxiv_id: '2512.14420'
source_url: https://arxiv.org/abs/2512.14420
tags:
- discode
- score
- evaluation
- image
- fleur
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DISCODE is a test-time adaptive decoder that generates robust evaluation
  scores for image captioning by minimizing the Adaptive Test-Time loss with a Gaussian
  prior. This approach addresses the challenge of aligning LVLM-generated scores with
  human judgments, especially under domain shift.
---

# DISCODE: Distribution-Aware Score Decoder for Robust Automatic Evaluation of Image Captioning

## Quick Facts
- **arXiv ID:** 2512.14420
- **Source URL:** https://arxiv.org/abs/2512.14420
- **Reference count:** 28
- **Key outcome:** DISCODE achieves 83.6% mean accuracy on MCEval and 84.5% on Pascal-50S by minimizing Adaptive Test-Time loss with Gaussian prior

## Executive Summary
DISCODE addresses the challenge of aligning LVLM-generated scores with human judgments in image captioning evaluation, particularly under domain shift. The method introduces a test-time adaptive decoder that generates robust evaluation scores by minimizing an Adaptive Test-Time (ATT) loss with a Gaussian prior distribution. This approach leverages an analytical solution for efficient computation and is applied to multiple LVLMs to achieve state-of-the-art performance. DISCODE demonstrates robustness across diverse visual domains, achieving 83.6% mean accuracy on the new MCEval benchmark covering six domains and four representative real-image benchmarks.

## Method Summary
DISCODE is a test-time adaptive decoder for image captioning evaluation that minimizes ATT loss: L_ATT = H(ψ_θ(h_T), p_LVLM) + D_α(ψ_θ(h_T)∥q), where q is a Gaussian prior centered on the raw score s_raw, and D_α is a weighted KL divergence with adaptive α. The method extracts decoder features h_T and token probabilities p_LVLM from a frozen LVLM, then computes optimal decoder weights using an analytical solution based on the LVLM's linear projection head. The final score combines the integer part from raw prediction with the decimal part from the adapted distribution. This approach addresses symbolic bias in LVLM token generation and aligns outputs closer to human evaluation distributions without requiring model retraining.

## Key Results
- DISCODE achieves 83.6% mean accuracy on MCEval benchmark across six diverse visual domains
- Performance reaches 84.5% accuracy on Pascal-50S benchmark, demonstrating strong real-image performance
- Outperforms existing metrics on reference-free evaluation tasks while maintaining computational efficiency through analytical solution

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Enforcing a unimodal (Gaussian) structure on the score distribution mitigates "symbolic bias" in LVLM token generation, aligning outputs closer to human evaluation distributions.
- **Mechanism:** The method minimizes ATT loss that includes divergence term pushing score distribution p toward Gaussian prior q, counteracting multi-modal or skewed token probability distributions.
- **Core assumption:** Human evaluation scores naturally follow Gaussian distribution (unimodal), whereas raw LVLM token probabilities do not due to tokenization artifacts.
- **Evidence anchors:** [abstract] "leveraging a Gaussian prior distribution to improve robustness in evaluation score estimation"; [section 3.2] "human evaluation scores naturally tend to follow a Gaussian distribution... token probability distributions typically do not."
- **Break condition:** If human evaluations for specific task are intentionally multi-modal (polarized) rather than Gaussian, regularization may force scores into incorrect mean value.

### Mechanism 2
- **Claim:** Proposed loss function permits closed-form analytical solution for decoder parameters, allowing efficient test-time adaptation without iterative gradient descent.
- **Mechanism:** By assuming linear projection head in base LVLM, optimization of ATT loss reduces to direct calculation of weights using Eq. 6, transforming slow iterative process into single-step algebraic operation.
- **Core assumption:** LVLM's output probability p_LVLM is generated via linear projection (softmax(V^T h_T + c)).
- **Evidence anchors:** [abstract] "This loss is efficiently minimized at test time using an analytical solution that we derive"; [section 3.3] "The analytical solution of the minimization problem exists."
- **Break condition:** If applied to LVLM with non-linear MLP head or complex adaptive biasing at output layer, analytical solution is invalid and iterative optimization (slower) would be required.

### Mechanism 3
- **Claim:** Dynamically adjusting balance between LVLM's raw likelihood and Gaussian prior based on score value improves robustness at extremes of rating scale.
- **Mechanism:** Adaptive parameter α (derived from Gaussian function centered on mean score) modulates weighted KL divergence, allowing model to rely more on raw scores or prior regularization depending on whether prediction is near mean or at extremes.
- **Core assumption:** Intermediate scores require different calibration than extreme scores (0 or 9).
- **Evidence anchors:** [section 3.2] "When LVLMs predict scores near minimum or maximum values, we can rely more on raw scores... by assigning greater weight to unimodal prior"; [section 5.3] Ablation study shows performance drops when adaptive definition for α is removed.
- **Break condition:** If relationship between score position and required regularization is not monotonic or Gaussian-shaped, fixed formula for α may maladapt.

## Foundational Learning

- **Concept:** **KL Divergence (KLD)**
  - **Why needed here:** Core ATT loss uses weighted KLD to measure "distance" between predicted score distribution and Gaussian prior.
  - **Quick check question:** Does minimizing KLD make target distribution p look more like reference distribution q, or vice versa? (Answer: It makes p resemble q).

- **Concept:** **Test-Time Adaptation (TTA)**
  - **Why needed here:** DISCODE adapts decoder head ψ at inference time for every single image-caption pair without finetuning LVLM weights.
  - **Quick check question:** Why is analytical solution crucial for method that operates at test time? (Answer: Iterative optimization per sample would be too computationally expensive for real-time evaluation).

- **Concept:** **Vision-Language Models (LVLMs) Tokenization Bias**
  - **Why needed here:** Paper explicitly argues LVLMs suffer from "symbolic bias" where certain tokens (like '0') are over-represented in probability space, necessitating decoder correction.
  - **Quick check question:** If LVLM always predicts token "5" with 90% confidence regardless of image input, would "score smoothing" technique alone fix this, or is prior-based correction needed? (Answer: Smoothing would still output 5; prior correction needed to pull distribution).

## Architecture Onboarding

- **Component map:** Frozen LVLM -> Feature Extractor (h_T, p_LVLM) -> Prior Generator (Gaussian q) -> Analytical Solver (Ŵ, ˆb) -> DISCODE Head (final score distribution p)

- **Critical path:** Implementation relies entirely on correctly extracting h_T and V (LVLM's output projection weights). If LVLM architecture does not expose latent feature h_T or unembedding matrix V, analytical solution cannot be computed.

- **Design tradeoffs:**
  - **Speed vs. Black-box Compatibility:** Method is fast and robust but requires "white-box" LVLM (access to internal weights/features), unlike API-only approaches (GPT-4o).
  - **Reference-free vs. Reference-based:** DISCODE is reference-free by default, saving annotation costs, but may lose precision compared to reference-based metrics if ground truth references are actually available.

- **Failure signatures:**
  - **Constant Scoring:** If Gaussian variance σ² is too small or solver logic fails, outputs might collapse to mean.
  - **API Incompatibility:** Attempting to run on proprietary models (e.g., GPT-4o via API) will fail immediately due to missing h_T and V.
  - **Scale Mismatch:** If prompt is changed from "0.0 to 1.0" to "1 to 10" without adjusting set S, digit probability extraction will be misaligned.

- **First 3 experiments:**
  1. **Bias Visualization:** Run vanilla LLaVA on random captions; plot histogram of predicted digits. Verify "symbolic bias" (spike at 0/5) exists as motivation.
  2. **Analytical Validation:** For single sample, compare score from Analytical Solution (Eq. 6) vs. running 10 steps Adam optimization on ATT loss. They should converge.
  3. **Ablation on Prior:** Run DISCODE on MCEval with α=0 (no prior). Compare accuracy against full model to quantify impact of Gaussian regularization.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can DISCODE framework be adapted to function with proprietary LVLMs that restrict access to internal decoder features?
- **Basis:** [explicit] Authors explicitly state in "Limitations and Future Work" that method cannot be applied to models like GPT-4o because they do not support feature extraction.
- **Why unresolved:** Analytical solution relies on accessing latent decoder feature (h_T), which is unavailable in closed-source APIs.
- **What evidence would resolve it:** Variant of method using only output probabilities or API-accessible data that achieves comparable performance on closed-source models.

### Open Question 2
- **Question:** Can test-time adaptive bias mitigation techniques be effectively generalized to other vision-language tasks beyond image caption evaluation?
- **Basis:** [explicit] Authors identify extending bias mitigation techniques to tasks beyond image captioning evaluation as "promising future research direction."
- **Why unresolved:** Current experimental validation is strictly confined to domain of caption scoring.
- **What evidence would resolve it:** Application of ATT loss to tasks like Visual Question Answering (VQA) or text-to-image generation evaluation showing improved robustness.

### Open Question 3
- **Question:** Does assumption of Gaussian prior hinder performance on highly ambiguous visual domains where human judgments may not be unimodal?
- **Basis:** [inferred] Paper notes human scores "rarely" become non-unimodal but acknowledges ambiguous images (e.g., Quickdraw) remain challenging in qualitative examples.
- **Why unresolved:** Enforcing unimodal Gaussian prior may conflict with subjective, polarized human interpretations of abstract content.
- **What evidence would resolve it:** Analysis of DISCODE's performance on dataset specifically constructed to have bimodal human score distributions.

## Limitations
- **White-box requirement:** Method requires access to internal LVLM components (h_T, V, c), preventing application to proprietary models like GPT-4o
- **Gaussian assumption:** Assumes human evaluation scores follow Gaussian distribution, which may not hold for polarized or multi-modal judgments
- **Computational overhead:** While analytical solution is efficient, per-sample adaptation for large-scale evaluation could be computationally significant

## Confidence
- **High Confidence:** Mathematical derivation of analytical solution and experimental results on MCEval benchmarks are well-founded and reproducible given access to dataset and open LVLMs
- **Medium Confidence:** Mechanism explanation for why Gaussian priors help is logically sound but relies on assumptions about human evaluation distributions not empirically validated across all domains
- **Low Confidence:** Claim of "state-of-the-art performance" across all evaluated benchmarks may be overstated; improvements on existing benchmarks don't represent dramatic breakthroughs

## Next Checks
1. **Cross-Domain Distribution Validation:** Conduct statistical analysis of human evaluation score distributions across all six MCEval domains to verify Gaussian assumption using goodness-of-fit tests (e.g., Kolmogorov-Smirnov).

2. **API Compatibility Benchmark:** Implement version of DISCODE that approximates analytical solution using only API-accessible outputs (token probabilities) without internal feature extraction; compare performance against full white-box implementation.

3. **Extreme Score Robustness Test:** Design controlled experiment using synthetic image-caption pairs with clearly binary ground truth quality; test whether DISCODE's Gaussian prior prevents assigning extreme scores when appropriate or artificially pulls extreme cases toward mean.