---
ver: rpa2
title: 'Judge a Book by its Cover: Investigating Multi-Modal LLMs for Multi-Page Handwritten
  Document Transcription'
arxiv_id: '2502.20295'
source_url: https://arxiv.org/abs/2502.20295
tags:
- page
- first
- only
- text
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of transcribing multi-page handwritten
  documents in a zero-shot setting, where labeled training data is unavailable or
  impractical to obtain. The authors propose a novel method called "+FIRST PAGE,"
  which leverages the OCR output of an entire document along with just the first page
  image to improve transcription accuracy.
---

# Judge a Book by its Cover: Investigating Multi-Modal LLMs for Multi-Page Handwritten Document Transcription

## Quick Facts
- arXiv ID: 2502.20295
- Source URL: https://arxiv.org/abs/2502.20295
- Reference count: 15
- Multi-modal LLM approach for zero-shot multi-page handwritten document transcription

## Executive Summary
This paper addresses the challenge of transcribing multi-page handwritten documents in a zero-shot setting where labeled training data is unavailable. The authors propose a novel method called "+FIRST PAGE" that combines the OCR output of an entire document with just the first page image to improve transcription accuracy. This approach leverages shared document features such as formatting and handwriting style without incurring the high cost of processing all images. Experiments on a multi-page version of the IAM Handwriting Database demonstrate that "+FIRST PAGE" improves transcription accuracy while balancing cost with performance.

## Method Summary
The "+FIRST PAGE" method leverages a multi-modal large language model to combine OCR output from all pages of a document with the image of just the first page. This approach exploits shared document features such as formatting and handwriting style across pages. The method operates in a zero-shot setting, requiring no labeled training data. By using only the first page image alongside full OCR output, the approach achieves a balance between computational cost and transcription accuracy. The multi-modal LLM processes the combined input to produce enhanced transcriptions that benefit from the contextual information provided by the first page.

## Key Results
- Achieved up to 58% reduction in character error rate (CER) compared to baseline OCR engines
- Improved transcription accuracy while remaining cost-effective
- Enhanced results on out-of-sample text by extrapolating formatting and OCR error patterns from a single page

## Why This Works (Mechanism)
The method works by leveraging the consistency of formatting and handwriting style across multi-page documents. When a document shares these features across pages, the first page provides crucial contextual information that can be used to correct OCR errors on subsequent pages. The multi-modal LLM can understand the relationship between the visual layout (from the first page image) and the textual content (from full OCR output), allowing it to identify and correct systematic errors that occur throughout the document.

## Foundational Learning
- Multi-modal LLMs: Why needed - to process both visual and textual information simultaneously; Quick check - verify the model can accept image and text inputs together
- Zero-shot learning: Why needed - to work without labeled training data; Quick check - confirm no fine-tuning occurs on target domain
- Character Error Rate (CER): Why needed - standard metric for transcription accuracy; Quick check - verify CER calculation matches standard definition
- OCR error patterns: Why needed - understanding systematic errors helps in correction; Quick check - analyze common error types in baseline OCR output
- Document layout consistency: Why needed - assumption that first page represents overall document structure; Quick check - measure layout similarity across pages
- Handwriting style consistency: Why needed - assumption that handwriting is consistent across pages; Quick check - compare handwriting features across pages

## Architecture Onboarding

**Component Map:**
Multi-modal LLM -> First Page Image + Full OCR Output -> Enhanced Transcriptions

**Critical Path:**
First page image → Multi-modal LLM → OCR output from all pages → Enhanced transcription output

**Design Tradeoffs:**
- Cost vs accuracy: Processing only one image versus all images
- Zero-shot approach vs fine-tuning: Avoiding labeled data requirements
- Single page representation vs multiple pages: Balancing information sufficiency with efficiency

**Failure Signatures:**
- Poor performance when handwriting style varies significantly across pages
- Reduced effectiveness when document layout changes between pages
- Limited improvement when OCR quality is extremely low across all pages

**First Experiments:**
1. Compare CER between baseline OCR and +FIRST PAGE on documents with consistent formatting
2. Test performance degradation when first page has different handwriting style than subsequent pages
3. Evaluate cost-benefit ratio by measuring computational overhead versus accuracy gains

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness primarily validated on IAM Handwriting Database, which may not represent real-world archival diversity
- Assumes consistent formatting and handwriting style across all pages, which may not hold for documents with significant variation
- Cost-benefit analysis doesn't account for potential computational overhead from multi-modal LLM processing
- Assumes availability of high-quality OCR output for the first page, which may not always be guaranteed

## Confidence

**Major Claims Confidence Assessment:**
- CER reduction of up to 58% compared to baseline OCR engines: **High**
- Cost-effectiveness of the "+FIRST PAGE" approach: **Medium** (limited cost analysis scope)
- Extrapolation of formatting and OCR error patterns from a single page: **Medium** (assumes consistency across pages)

## Next Checks
1. Evaluate the method on additional handwriting datasets with greater variability in writing styles, document layouts, and degradation levels to assess generalizability.
2. Conduct a comprehensive cost analysis including computational overhead of multi-modal LLM processing to validate claimed efficiency improvements.
3. Test the approach on documents with known inter-page variations in formatting or handwriting to quantify performance degradation when the consistency assumption is violated.