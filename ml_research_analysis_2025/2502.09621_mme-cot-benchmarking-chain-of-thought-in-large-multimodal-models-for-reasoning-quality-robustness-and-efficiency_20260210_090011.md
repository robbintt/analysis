---
ver: rpa2
title: 'MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning
  Quality, Robustness, and Efficiency'
arxiv_id: '2502.09621'
source_url: https://arxiv.org/abs/2502.09621
tags:
- step
- answer
- reasoning
- triangle
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces MME-CoT, a comprehensive benchmark for evaluating\
  \ Chain-of-Thought reasoning in Large Multimodal Models across six domains: math,\
  \ science, OCR, logic, space-time, and general scenes. It proposes a fine-grained\
  \ evaluation suite with three novel metrics\u2014reasoning quality (precision and\
  \ recall), robustness (stability and efficacy), and efficiency (relevance rate and\
  \ reflection quality)\u2014to assess the correctness, impact, and cost-effectiveness\
  \ of CoT processes."
---

# MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency

## Quick Facts
- arXiv ID: 2502.09621
- Source URL: https://arxiv.org/abs/2502.09621
- Reference count: 40
- Primary result: Comprehensive benchmark reveals CoT reasoning degrades perception tasks (-6.8% stability) while improving reasoning tasks, with reflection steps often ineffective (30-40% invalid)

## Executive Summary
MME-CoT introduces a comprehensive benchmark for evaluating Chain-of-Thought (CoT) reasoning in Large Multimodal Models (LMMs) across six domains. The benchmark proposes three novel evaluation dimensions: reasoning quality (precision/recall/F1), robustness (stability/efficacy across perception vs reasoning tasks), and efficiency (relevance rate/reflection quality). Experiments on 15 leading LMMs reveal that while reflection-enabled models achieve superior CoT quality, they suffer from significant inefficiency and often degrade performance on perception-heavy tasks through "overthinking" behavior.

## Method Summary
The benchmark uses 1,130 questions across six domains with 3,865 annotated key reasoning steps. Models are evaluated using two prompting strategies: direct prompt (immediate answer) and CoT prompt (step-by-step reasoning). GPT-4o serves as the evaluation judge for step partition, matching, relevance classification, and reflection validity. Three parallel evaluation tracks assess quality (precision/recall via step-level matching), robustness (stability/efficacy via accuracy differences), and efficiency (relevance rate via step-level classification, reflection quality via indicator detection).

## Key Results
- Reflection-enabled models achieve superior CoT quality (QVQ-72B F1=62.0%, Kimi k1.5 outperforms GPT-4o)
- CoT reasoning degrades perception task performance (InternVL2.5-8B: -6.8% stability)
- 30-40% of reflection steps are ineffective or invalid across evaluated models
- GPT-4o and Kimi k1.5 outperform open-source models in multimodal reasoning
- Long CoT models often achieve correct answers while skipping intermediate key steps

## Why This Works (Mechanism)

### Mechanism 1: Reflection-enabled models achieve higher CoT quality through iterative self-correction
Models like QVQ-72B and Kimi k1.5 generate extended reasoning chains with explicit reflection steps (marked by indicators like "Wait", "Alternatively"), enabling verification and correction of intermediate conclusions before final answers. Assumption: Reflection steps genuinely correct errors rather than introduce noise. Evidence: QVQ surpasses its base model by 5.8% precision; reflection quality formula identifies ~40% invalid reflections when this mechanism breaks.

### Mechanism 2: CoT reasoning interferes with perception tasks through "overthinking" behavior
When models apply deliberative reasoning to tasks requiring minimal inference, the extended chain introduces opportunities for error propagation, second-guessing correct perceptual judgments, or generating irrelevant descriptions that distract from direct answers. Assumption: Perception tasks should be solvable without explicit reasoning chains. Evidence: InternVL2.5-8B shows 6.8% degradation when applying CoT on perception tasks; models with stronger instruction-following resist this degradation.

### Mechanism 3: Long CoT models lose efficiency through irrelevant image descriptions and ineffective reflection
Extended reasoning chains include exhaustive visual descriptions unrelated to the question, plus reflection steps that either repeat conclusions, propose unexecuted approaches, or introduce new errors—consuming tokens without advancing toward answers. Assumption: Relevance can be judged independently of correctness. Evidence: 30-40% of reflection steps fail to help answer questions; relevance rate formula scales raw rates to amplify model differences (α=0.8).

## Foundational Learning

### Concept: Precision vs. Recall in reasoning chains
Why needed: The benchmark distinguishes between faithful steps (precision—all generated steps correct) and informative steps (recall—all required steps present). High precision with low recall indicates skipping; low precision with high recall indicates hallucination. Quick check: If a model answers correctly but skips intermediate calculations, which metric suffers?

### Concept: Task classification (perception vs. reasoning)
Why needed: The robustness evaluation depends on correctly categorizing questions. Perception tasks require minimal reasoning; reasoning tasks require multi-step inference. Misclassification invalidates stability/efficacy measurements. Quick check: Does "count objects in image" require CoT reasoning?

### Concept: Reflection validity criteria
Why needed: Not all self-correction is productive. Valid reflection must either correct an error OR verify with new insights. Repetition, incompleteness, and interference are failure modes. Quick check: A model says "Wait, let me reconsider" then restates the same conclusion—is this valid reflection?

## Architecture Onboarding

### Component map
Question → Model response → Step partition → Per-step judgment → Metric aggregation

### Critical path
Question → Model response → Step partition (GPT-4o) → Per-step correctness judgment → Precision/Recall calculation → Relevance classification → Reflection validity judgment → Metric aggregation

### Design tradeoffs
Using GPT-4o as judge introduces model dependency; complex visual questions may exceed judge capability. Key step annotation requires human verification; multiple solution paths complicate recall calculation. Reflection indicators ("Wait", "Alternatively") may miss valid reflections or catch false positives.

### Failure signatures
Models refusing direct prompts (Mulberry, LLaVA-CoT, Virgo, QVQ, Kimi k1.5) produce unreliable robustness scores. Repetitive outputs inflate step counts without adding value. Long CoT models show recall misalignment with final accuracy (correct answer, skipped steps).

### First 3 experiments
1. Baseline calibration: Run GPT-4o and Qwen2-VL-7B on 50-sample subset with both direct and CoT prompts to verify stability/efficacy calculation pipeline.
2. Judge agreement test: Compare human vs. GPT-4o annotations on 20 questions for step partition and relevance judgment; target >80% agreement before scaling.
3. Reflection error profiling: Analyze 50 QVQ responses to quantify the four error types (ineffective reflection, incompleteness, repetition, interference) and validate the reflection quality formula.

## Open Questions the Paper Calls Out

### Open Question 1: Why do long CoT models frequently achieve correct final answers while omitting key intermediate logical steps?
Basis: Page 9 states long CoT models sometimes reach correct answers while skipping intermediate steps, contradicting stepwise reasoning principles. Unresolved because current metrics like Recall fail to explain how models maintain high accuracy despite low Recall scores, suggesting hidden or heuristic reasoning paths. Evidence needed: Mechanistic interpretability studies or ablation studies identifying parallel implicit reasoning pathways versus strictly serial verbal reasoning.

### Open Question 2: How can models be trained to reduce the high incidence of ineffective reflection steps?
Basis: Page 10 notes future research should explore methods to reduce ineffective reflections to improve both efficiency and quality. Unresolved because the benchmark reveals 30-40% of reflection steps fail to contribute to correctness, often introducing interference or incompleteness. Evidence needed: Demonstrating a training paradigm (e.g., fine-grained RL rewards) that specifically penalizes redundant or misleading self-correction steps.

### Open Question 3: What defines a predictive criterion to determine if a visual task requires CoT reasoning or direct perception?
Basis: Page 6 states there exists no golden standard to determine which question can benefit from CoT. Unresolved because authors observe CoT often harms perception tasks (negative Stability), yet models lack capability to dynamically switch between direct answering and CoT reasoning. Evidence needed: Development of a "router" model capable of classifying queries as perception-heavy or reasoning-heavy with high accuracy.

## Limitations
- Benchmark depends on GPT-4o as evaluation judge, introducing potential model bias
- Robustness evaluation compromised for models that refuse direct prompts (Mulberry, LLaVA-CoT, Virgo, QVQ, Kimi k1.5)
- Relevance rate formula (scaling by α=0.8) amplifies differences without clear justification
- Reflection quality metrics rely on explicit textual indicators that may miss valid reflections

## Confidence

### High confidence
- CoT reasoning degrades perception task performance (InternVL2.5-8B: -6.8% stability)
- Reflection-enabled models achieve higher CoT quality (QVQ-72B F1=62.0%)

### Medium confidence
- 30-40% reflection steps being invalid requires manual validation of error taxonomy
- "Overthinking" mechanism explanation for perception degradation is plausible but not directly tested

### Low confidence
- Specific scaling parameter α=0.8 in relevance rate calculation lacks empirical justification
- Claim that long CoT models systematically lose efficiency requires more nuanced analysis

## Next Checks
1. Judge agreement validation: Compare human expert annotations with GPT-4o judgments on 50 randomly selected questions for step partition, relevance classification, and reflection validity. Target >80% agreement.
2. Direct prompt compliance testing: For models that refuse direct prompts, systematically test whether model behavior changes with explicit "no reasoning chain" instructions or temperature adjustments to obtain reliable robustness scores.
3. Error type validation: Manually analyze 100 reflection steps from QVQ responses to verify the four error types (ineffective, incompleteness, repetition, interference) and assess whether the reflection quality formula accurately captures the true cost-benefit ratio.