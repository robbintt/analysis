---
ver: rpa2
title: Enhanced Denoising and Convergent Regularisation Using Tweedie Scaling
arxiv_id: '2503.05956'
source_url: https://arxiv.org/abs/2503.05956
tags:
- regularisation
- convergent
- scaling
- convergence
- denoiser
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of designing convergent regularisation
  for Plug-and-Play (PnP) methods in image reconstruction, particularly when using
  deep learning-based denoisers. The key issue is the lack of a control parameter
  to modulate regularisation strength in pre-trained denoisers, which complicates
  achieving both interpretability and provable convergence.
---

# Enhanced Denoising and Convergent Regularisation Using Tweedie Scaling

## Quick Facts
- arXiv ID: 2503.05956
- Source URL: https://arxiv.org/abs/2503.05956
- Reference count: 28
- Primary result: Introduces Tweedie scaling to provide interpretable control over regularisation strength in Plug-and-Play methods, ensuring provable convergence while maintaining practical deep learning denoiser performance.

## Executive Summary
This paper addresses a fundamental challenge in Plug-and-Play (PnP) image reconstruction: achieving both convergent regularisation and interpretable control over regularisation strength when using deep learning-based denoisers. Pre-trained denoisers lack explicit parameters for modulating regularisation intensity, complicating theoretical guarantees. The authors propose a Tweedie scaling method that introduces a scaling parameter δ to adjust regularisation strength while maintaining provable convergence properties. This approach bridges the gap between practical deep learning denoisers and theoretical convergence guarantees, with δ_opt = 1 indicating perfect denoiser training quality.

## Method Summary
The method introduces Tweedie scaling through the transformation D_δ = Id + (D - Id)/δ², where D is a pre-trained denoiser. For image reconstruction, the PnP-PGD operator becomes T_δ(x) = D_δ(x - τA^⊤(Ax - y)). The optimal scaling parameter δ_opt is computed via Proposition 1: δ_opt² = -E[||D(X+σξ)-(X+σξ)||²]/E[⟨σξ,D(X+σξ)-(X+σξ)⟩]. Convergence is guaranteed when δ² > 1 and D is non-expansive. The approach is validated on denoising (A = I_n) and inpainting (20% mask) tasks using DRUNet denoisers trained at noise level σ = 0.1 on augmented CBSD68 dataset.

## Key Results
- Tweedie scaling provides explicit control over regularisation strength through parameter δ, derived from Tweedie's identity and Taylor expansions
- δ_opt reveals denoiser training quality, with δ_opt = 1 indicating perfect L₂ training; experimental validation shows ordered δ_opt values across denoisers trained at different noise levels
- The method ensures provable convergence: for δ² > 1 and non-expansive D, T_δ is δ²/(2δ²-1)-averaged, guaranteeing fixed-point convergence
- Empirically demonstrates superior convergence and reconstruction quality compared to homogeneous scaling on denoising and inpainting tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Tweedie scaling transformation D_δ = Id + (D - Id)/δ² provides explicit control over regularisation strength.
- Mechanism: Starting from Tweedie's identity (D_MMSE = Id + σ²∇ log p_σ), the authors derive a scaling by examining the behaviour as σ → 0. Taylor expansion of p_σ around σ² = 0 (satisfying the heat equation) yields ∇ log p_σ → ∇ log p_X. Introducing δ² as a divisor on the gradient term creates a family of denoisers with tunable regularisation intensity.
- Core assumption: The denoiser D approximates D_MMSE through proper L₂ training; higher-order terms in the Taylor expansion become negligible as σ → 0.
- Evidence anchors:
  - [Section 2, pages 4-5]: Full derivation using heat equation and first-order Taylor expansion
  - [Corpus]: Limited direct corroboration; related work "From the Gradient-Step Denoiser to the Proximal Denoiser" discusses convergent PnP but via different constructions
- Break condition: If the denoiser D is poorly trained (far from D_MMSE) or σ is not small enough for Taylor approximation validity, the theoretical justification weakens—though the method may still work empirically.

### Mechanism 2
- Claim: The optimal scaling parameter δ_opt reveals training quality, with δ_opt = 1 indicating perfect L₂ training.
- Mechanism: By analysing L₂(D_δ) as a function of δ, Proposition 1 shows that a specific δ_opt minimises the denoising error. If the original denoiser D = D_MMSE (perfect training), then δ_opt = 1. Deviations from 1 indicate suboptimal training; the scaled denoiser D_δ_opt has lower MSE than D.
- Core assumption: The L₂ loss properly captures denoiser quality; the expectation formula for δ_opt² is computable from denoiser outputs.
- Evidence anchors:
  - [Section 3, page 5]: Proposition 1 with the inequality chain L₂(D_MMSE) ≤ L₂(D_δ_opt) ≤ L₂(D)
  - [Section 5.1, page 8-9]: Experimental validation showing 1 < δ_opt,4 < δ_opt,3 < δ_opt,2 < δ_opt,1 for denoisers trained at increasing noise levels
  - [Corpus]: No direct corroboration found for this specific δ interpretation
- Break condition: If δ_opt computation is noisy or unstable, or if the L₂ metric doesn't align with perceptual quality, the interpretation may be misleading.

### Mechanism 3
- Claim: The family (D_δ)_δ defines a convergent regularisation satisfying stability and convergence conditions.
- Mechanism: For convergence of iterations: if D is non-expansive and δ² > 1, then T_δ = D_δ ∘ G is averaged, guaranteeing fixed-point convergence. For convergent regularisation: conditions (A1)-(A4) from [6] are verified—D_δ is contractive (assuming rescaling via γ(1/δ²)), converges to Identity as δ → ∞ both pointwise and uniformly on bounded sets.
- Core assumption: D is non-expansive (empirically verified in Table 1 for noise levels σ ≥ 0.03); contractivity can be enforced via rescaling γ(1/δ²)D_δ.
- Evidence anchors:
  - [Section 4.1, page 6]: Proposition 2 proving δ²/(2δ²-1)-averagedness
  - [Section 4.2, pages 7-8]: Verification of admissibility conditions (A2)-(A4)
  - [Section 5.3, pages 9-10]: Empirical demonstration of convergence for Tweedie scaling vs. failure of homogeneous scaling
  - [Corpus]: "Solving Imaging Inverse Problems Using Plug-and-Play Denoisers" provides broader PnP convergence context
- Break condition: If D is expansive (Lip(D) > 1) and rescaling isn't applied, or if δ² ≤ 1, convergence guarantees break. Low noise levels (σ < 0.03) show slight expansiveness in experiments.

## Foundational Learning

- Concept: **Proximal operators and averaged mappings**
  - Why needed here: The convergence proof relies on D_δ being 1/δ²-averaged and T_δ being δ²/(2δ²-1)-averaged. Without understanding that averaged operators have fixed-point convergence properties, the theoretical contribution is opaque.
  - Quick check question: Given an operator O = θR + (1-θ)Id with R non-expansive, what range of θ guarantees averaging? (Answer: θ ∈ (0,1))

- Concept: **Tweedie's identity and score functions**
  - Why needed here: The entire scaling derivation starts from D_MMSE = Id + σ²∇ log p_σ. Understanding that ∇ log p_σ is the score function (gradient of log-density) connects denoising to probabilistic modelling.
  - Quick check question: For a Gaussian distribution N(μ, σ²), what is ∇_x log p(x)? (Answer: -(x-μ)/σ², pointing toward the mean)

- Concept: **Convergent regularisation vs. convergent iterations**
  - Why needed here: The paper distinguishes these—convergent iterations mean the algorithm reaches a fixed point; convergent regularisation means the solution family converges to a meaningful limit as regularisation strength varies. Both are needed for theoretical soundness.
  - Quick check question: If PnP iterations converge to a fixed point, does this guarantee convergent regularisation? (Answer: No—convergent iterations are necessary but not sufficient)

## Architecture Onboarding

- Component map:
  - **Denoiser D**: DRUNet (5 blocks), trained on augmented CBSD68 at noise level σ
  - **Scaling transform**: D_δ(x) = (1 - 1/δ²)x + D(x)/δ² = x + (D(x) - x)/δ²
  - **PnP-PGD operator**: T_δ(x) = D_δ(x - τA^⊤(Ax - y))
  - **Rescaling function γ**: γ(1/δ²) = δ²/(1 + δ²) to enforce contractivity if needed

- Critical path:
  1. Train D on Gaussian denoising at noise level σ (or load pre-trained)
  2. Verify non-expansiveness empirically for your σ (Table 1 approach)
  3. Choose δ: either compute δ_opt via Proposition 1 formula, or sweep δ for your task
  4. Run PnP-PGD: x^(i+1) = D_δ(x^i - τA^⊤(Ax^i - y)) for ~300 iterations
  5. If convergence issues: apply rescaling γ(1/δ²)D_δ

- Design tradeoffs:
  - **δ² > 1 requirement**: Guarantees convergence but limits regularisation strength; smaller δ = stronger regularisation but no theoretical guarantee
  - **Non-expansiveness vs. performance**: Enforcing strict contractivity via γ may reduce denoising quality
  - **Training noise level**: Lower σ = harder training = δ_opt further from 1; higher σ = easier training but less precise denoiser

- Failure signatures:
  - Diverging iterations: δ² ≤ 1 or D is expansive (check Lip(D) > 1)
  - Poor reconstruction quality: δ far from δ_opt, or mismatch between training σ and inference noise level
  - No convergence to true image: expected when ker(A) ≠ {0}—regularisation prior may not match true image characteristics

- First 3 experiments:
  1. **Validate δ_opt interpretation**: Train denoisers at σ ∈ {0.01, 0.07, 0.10, 0.12}, compute δ_opt for each, verify ordering δ_opt,1 > δ_opt,2 > δ_opt,3 > δ_opt,4 > 1
  2. **Test convergence on inpainting**: Set A as 20% mask operator, run PnP-PGD with δ sweep, plot ‖Ax_δ - Ax†‖/‖Ax†‖ vs. δ to verify data consistency convergence
  3. **Compare scaling methods**: Run identical PnP-PGD with Tweedie scaling vs. homogeneous scaling on denoising (A = I_n), observe that only Tweedie should show convergent regularisation behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Tweedie scaling framework guarantee convergent regularisation for Plug-and-Play algorithms other than Proximal Gradient Descent (PGD), such as ADMM or Half-Quadratic Splitting (HQS)?
- Basis in paper: [explicit] The conclusion states a future direction is "...investigating whether the property of convergent regularisation holds for other PnP algorithms..."
- Why unresolved: The paper’s theoretical proofs and empirical validation are restricted to the PnP-PGD iteration $x^{(i+1)} = D_\delta(x^{(i)} - \tau A^\top(Ax^{(i)} - y))$.
- What evidence would resolve it: Extending the fixed-point and admissibility proofs in Section 4 to the operator splitting structures inherent in ADMM or HQS.

### Open Question 2
- Question: What is the theoretical rate of convergence for the regularisation error as the scaling parameter $\delta$ increases?
- Basis in paper: [explicit] The conclusion lists "...a study of the rate of convergence of the regularisation..." as a specific future direction.
- Why unresolved: The paper proves that a limit exists (convergence) but does not characterize the speed at which the sequence converges to the solution.
- What evidence would resolve it: Theoretical bounds on the regularisation error $\|x^\delta - x^\dagger\|$ relative to $\delta$ or empirical plots showing error decay rates on standard datasets.

### Open Question 3
- Question: Can the stability and convergence guarantees be maintained if the assumption that the denoiser $D$ is non-expansive is relaxed or removed?
- Basis in paper: [explicit] The conclusion suggests "...relaxing the assumption of non-expansiveness of the initial denoiser..." as a potential research avenue.
- Why unresolved: The proof of convergent iterations (Proposition 2) and the stability analysis (Section 4.2) fundamentally rely on the non-expansiveness (or contractivity) of $D$.
- What evidence would resolve it: A modified proof framework that allows for $\|D(y_1) - D(y_2)\| > \|y_1 - y_2\|$ in certain regions, or a new scaling method that enforces stability despite an expansive raw denoiser.

### Open Question 4
- Question: Does the Tweedie scaling method hold for the infinite-dimensional setting typically studied in the theory of inverse problems?
- Basis in paper: [explicit] The authors explicitly mention "...considering the infinite-dimensional setting, as is usually the case in the theory of inverse problems" in the conclusion.
- Why unresolved: The current analysis assumes a finite-dimensional setting $\mathbb{R}^n$ where weak and strong convergence coincide, simplifying the proof of condition (A3).
- What evidence would resolve it: A theoretical extension of the Tweedie scaling definitions and admissibility proofs to function spaces (Hilbert spaces), accounting for the nuances of weak convergence.

## Limitations

- **Theoretical derivation scope**: The Tweedie scaling derivation relies heavily on first-order Taylor expansion and Gaussian assumptions, potentially losing accuracy for denoisers trained at higher noise levels where linear approximation breaks down.
- **Empirical validation breadth**: Experiments focus on denoising and inpainting with σ = 0.1, lacking extensive testing across different forward operators, higher noise levels, or perceptual quality metrics beyond PSNR.
- **Interpretation uncertainty**: The δ_opt interpretation as a training quality metric, while theoretically justified, requires more diverse empirical validation across different denoiser architectures and training regimes.

## Confidence

- **High confidence**: The theoretical framework for convergent regularisation is mathematically rigorous. The Tweedie scaling formula D_δ = Id + (D - Id)/δ² is well-derived from Tweedie's identity, and the convergence proofs (Proposition 2 and admissibility verification) are sound under stated assumptions.
- **Medium confidence**: The practical implementation details are mostly specified, though some architectural specifics (exact DRUNet configuration, training hyperparameters) are omitted. The experimental results showing superior convergence compared to homogeneous scaling are compelling but limited in scope.
- **Low confidence**: The interpretation of δ_opt as a training quality metric, while theoretically justified, requires more extensive empirical validation across different denoiser architectures and training regimes to be considered robust.

## Next Checks

1. **Generalization test**: Apply Tweedie scaling to a different denoiser architecture (e.g., DnCNN or U-Net) and verify that δ_opt interpretation holds across architectures.

2. **Noise level sensitivity**: Repeat δ_opt experiments across a broader range of noise levels (σ ∈ {0.02, 0.05, 0.15, 0.20}) to verify the relationship between training difficulty and δ_opt deviation from 1.

3. **Operator diversity**: Test Tweedie scaling on other inverse problems (compressed sensing, deblurring) to validate convergence properties beyond denoising and inpainting tasks.