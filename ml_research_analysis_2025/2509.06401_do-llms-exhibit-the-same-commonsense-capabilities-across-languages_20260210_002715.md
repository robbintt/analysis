---
ver: rpa2
title: Do LLMs exhibit the same commonsense capabilities across languages?
arxiv_id: '2509.06401'
source_url: https://arxiv.org/abs/2509.06401
tags:
- languages
- commonsense
- sentence
- evaluation
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates whether large language models (LLMs) exhibit
  the same commonsense generation capabilities across languages. The authors introduce
  MULTICOM, a novel benchmark extending the COCOTEROS dataset to four languages: English,
  Spanish, Dutch, and Valencian.'
---

# Do LLMs exhibit the same commonsense capabilities across languages?

## Quick Facts
- **arXiv ID**: 2509.06401
- **Source URL**: https://arxiv.org/abs/2509.06401
- **Authors**: Ivan Martínez-Murillo; Elena Lloret; Paloma Moreda; Albert Gatt
- **Reference count**: 22
- **Primary result**: LLMs consistently generate more commonsensical sentences in English than in Spanish, Dutch, and Valencian

## Executive Summary
This paper evaluates whether large language models exhibit consistent commonsense generation capabilities across languages. The authors introduce MULTICOM, extending COCOTEROS to English, Spanish, Dutch, and Valencian, testing five LLM families across multiple sizes with automatic metrics, LLM-as-judge evaluations, and human annotations. Results consistently show superior performance in English compared to other languages, with under-represented languages benefiting more from contextual support. The study confirms that LLMs generate more commonsensical sentences in English than in other languages.

## Method Summary
The study evaluates five LLM families (LLaMA, Qwen, Gemma, EuroLLM, Salamandra) on commonsense generation across four languages using the MULTICOM benchmark. Models generate sentences from word triplets with and without context, evaluated via automatic metrics (BERTScore, USE similarity, dependency parsing), LLM-as-judge systems (Prometheus, JudgeLM), and human annotation on a subset. The task involves producing grammatically correct, commonsensical sentences incorporating all keywords, with performance measured against both target and counterfactual references to assess commonsense alignment.

## Key Results
- English consistently outperforms other languages in commonsense generation across all evaluation methods
- Under-represented languages (Dutch, Valencian) benefit more from contextual support than English
- Automatic metrics and LLM-as-judge evaluations align with human judgments
- Context injection yields mixed results, being detrimental for some models like Gemma
- Models specifically trained with multilingual curation (EuroLLM, Salamandra) show more balanced performance

## Why This Works (Mechanism)

### Mechanism 1: Training Data Imbalance Creates Representation Gaps
English-centric pre-training data produces systematic commonsense generation disparities across languages. Models develop denser, higher-quality semantic representations for high-resource languages during pre-training, enabling more accurate commonsense inference in English compared to under-represented languages. This mechanism assumes commonsense generation quality correlates with representation richness in parameter space.

### Mechanism 2: Contextual Support Compensates for Weak Representations
External context benefits under-represented languages more than high-resource languages. When models lack robust internal representations for low-resource languages, external context provides grounding that partially compensates for knowledge gaps. For high-resource languages, models already possess sufficient internal knowledge, making context redundant or potentially noisy.

### Mechanism 3: Multilingual-Curated Training Reduces Language Disparities
Models specifically trained with multilingual data curation show more balanced cross-lingual commonsense capabilities. Intentional data curation during training creates more equitable representations across target languages, reducing English-centric bias. This mechanism assumes training data composition directly affects cross-lingual transfer quality.

## Foundational Learning

- **Commonsense Generation vs. Commonsense Reasoning**: The paper tests generation (producing coherent sentences from concept triplets) rather than multiple-choice reasoning. Quick check: Can you explain why generating a commonsensical sentence from "dog, bark, meow" is different from classifying whether "dogs can meow" is true?

- **Cross-Lingual Transfer and Language Representation**: Performance gaps relate to how models represent knowledge across languages. English dominance suggests knowledge may be language-bound rather than language-agnostic. Quick check: If a model learns "dogs bark" in English, should it automatically know this in Spanish? What evidence would support or refute automatic transfer?

- **Evaluation Validity for Multilingual NLG**: The paper uses three evaluation approaches precisely because multilingual evaluation is notoriously unreliable. Understanding their limitations prevents overclaiming. Quick check: Why might BERTScore show smaller gaps than USE similarity? What does this suggest about metric sensitivity?

## Architecture Onboarding

- **Component map**: MULTICOM Dataset -> Model Layer (5 families × 2 sizes × 2 variants) -> Evaluation Layer (Automatic metrics + LLM judges + human annotation) -> Context Conditions

- **Critical path**: 1) Translate/augment COCOTEROS → English, Dutch, Valencian; 2) Generate sentences for all model configurations × languages × context conditions; 3) Evaluate via automatic metrics (compare to target vs. counterfactual); 4) Validate with LLM-as-judge and human annotation on subset; 5) Analyze score differentials

- **Design tradeoffs**: Translation vs. native creation (dataset translated from Spanish); Judge language bias (English-only rubrics for all languages); Open-source only scope (excluded GPT-4/Gemini); Valencian proxy (used Catalan models/speakers)

- **Failure signatures**: BERTScore shows smaller cross-lingual gaps than USE; context backfire (Gemma models performed worse with context); Judge inconsistency (Prometheus preferred Dutch over Spanish); Task misunderstanding (Gemma-9B generated pseudo-code)

- **First 3 experiments**: 1) Baseline language comparison (LLaMA-3.2-3B-Instruct on all 4 languages without context, compute (target-counterfactual) score gaps); 2) Context sensitivity test (Valencian and Dutch with/without context using LLaMA and Salamandra); 3) Judge alignment validation (compare Prometheus, JudgeLM, and human scores on 20-instance subset)

## Open Questions the Paper Calls Out

- **Open Question 1**: Do proprietary LLMs (e.g., GPT-4, Gemini) exhibit the same degree of cross-linguistic disparity in commonsense generation as the open-weight models evaluated? Basis: The authors explicitly state in the "Future Work" section that it would be useful to compare their performance to proprietary ones such as GPT-4, Grok, and Gemini.

- **Open Question 2**: Can fine-tuning an LLM-as-a-judge model improve its alignment with human evaluations regarding nuanced multilingual commonsense reasoning? Basis: The "Future Work" section identifies the need to fine-tune an LLM-as-a-judge model to better align with the nuanced demands of multilingual commonsense reasoning.

- **Open Question 3**: Does the reliance on English-centric automatic metrics artificially inflate the perceived performance gap between English and low-resource languages? Basis: The "Limitations" section notes that automatic tools are typically developed and benchmarked primarily on English, which may lead to more favourable performance scores for English texts.

## Limitations

- Dataset creation artifacts: The MULTICOM benchmark was translated from Spanish rather than natively created in each target language, potentially introducing translation artifacts that could systematically affect model performance across languages.

- Valencian evaluation proxy: Valencian was evaluated using Catalan models and speakers due to resource constraints, creating uncertainty about whether results generalize to true Valencian language processing.

- Evaluation rubric language bias: Both LLM-as-judge systems used English-language rubrics regardless of the target language being evaluated, creating potential bias where commonsense quality judgments may not transfer equivalently across linguistic contexts.

## Confidence

- **High confidence**: English superiority in commonsense generation is consistently observed across all evaluation methods and model families. The magnitude and direction of cross-lingual gaps are robust to different evaluation approaches.

- **Medium confidence**: The contextual support compensation mechanism for under-represented languages is well-supported by quantitative results showing larger benefit magnitudes, though the underlying cognitive explanation remains speculative.

- **Low confidence**: Claims about EuroLLM and Salamandra showing more balanced performance due to multilingual data curation are based on limited model comparisons without systematic ablation studies controlling for other architectural differences.

## Next Checks

1. **Native content validation**: Create a parallel validation set where test instances are natively generated in each language rather than translated, then re-run the core experiments to determine if translation artifacts explain any observed performance gaps.

2. **Cross-linguistic judge adaptation**: Train separate Prometheus/JudgeLM instances with language-specific rubrics for each target language, then evaluate whether English-centric rubrics systematically underestimate commonsense quality in non-English outputs.

3. **Proxy language comparison**: Evaluate true Catalan models on the Valencian subset and compare results with the original Catalan-model evaluation to quantify the uncertainty introduced by the Valencian-Catalan proxy approach.