---
ver: rpa2
title: Large Language Models for Depression Recognition in Spoken Language Integrating
  Psychological Knowledge
arxiv_id: '2505.22863'
source_url: https://arxiv.org/abs/2505.22863
tags:
- depression
- knowledge
- audio
- llms
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study presents the first application of large language models
  (LLMs) to multimodal depression detection using the DAIC-WOZ dataset. The method
  integrates audio features extracted via Wav2Vec with text transcripts, and introduces
  a novel strategy for incorporating psychological knowledge into LLMs through structured
  question-answer pairs derived from authoritative sources.
---

# Large Language Models for Depression Recognition in Spoken Language Integrating Psychological Knowledge

## Quick Facts
- arXiv ID: 2505.22863
- Source URL: https://arxiv.org/abs/2505.22863
- Reference count: 6
- The study applies LLMs to multimodal depression detection, integrating audio features with text transcripts and psychological knowledge to significantly improve accuracy.

## Executive Summary
This study presents the first application of large language models to multimodal depression detection using the DAIC-WOZ dataset. The approach integrates audio features extracted via Wav2Vec with text transcripts, and introduces a novel strategy for incorporating psychological knowledge into LLMs through structured question-answer pairs derived from authoritative sources. By fine-tuning LLaMA with both audio-text modality fusion and knowledge injection, the method significantly improves depression recognition accuracy, achieving a Mean Absolute Error of 5.356 and outperforming the baseline audio-only model from AVEC 2016.

## Method Summary
The method processes the DAIC-WOZ dataset by segmenting audio and transcripts into 5-utterance chunks, extracting audio features using Wav2Vec 2.0, and projecting them into LLaMA's embedding space via a feedforward network. The model is pre-trained with psychological knowledge injection through 4,920 WHO-derived Q&A pairs, then fine-tuned on the multimodal dataset using LoRA. Segment-level predictions are averaged to produce participant-level PHQ-8 scores. The approach leverages LoRA for resource-efficient fine-tuning while maintaining the original model weights.

## Key Results
- Achieved MAE of 5.356 and RMSE of 6.713 on DAIC-WOZ test set
- Outperformed baseline audio-only model (MAE 5.72, RMSE 7.78) from AVEC 2016
- Knowledge injection improved text-based predictions (MAE dropped from 6.342 to 5.354)
- Multimodal fusion did not significantly outperform audio-only baseline (MAE 5.373)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Projecting audio embeddings into LLM hidden space enables text-based models to process acoustic depression markers.
- Mechanism: Wav2Vec 2.0 extracts audio features (shape R^{s×d_a}), then a feedforward network projects them to LLaMA's embedding dimension (shape R^{s×d_t}), allowing the LLM to treat audio tokens analogously to text tokens for PHQ-8 regression.
- Core assumption: Acoustic features relevant to depression (e.g., pitch variability, pauses, speech rate) are captured by Wav2Vec and remain informative after projection.
- Evidence anchors:
  - [abstract]: "We extract the audio features using the pre-trained model Wav2Vec, and mapped it to text-based LLMs for further processing."
  - [section 2.3.2]: "we aim to project the audio representations into a shared latent space of the LLM... instead of using Whisper... we utilise Wav2Vec 2.0 due to its stronger capacity for capturing a broader range of audio features"
  - [corpus]: Weak direct evidence; neighbor papers (MLlm-DR, Multi-Modal LLM) explore similar multimodal fusion but do not validate Wav2Vec-to-LLaMA projection specifically.
- Break condition: If audio features lose discriminative signal during projection (e.g., feedforward network undertrained), text-audio fusion may degrade to text-only performance.

### Mechanism 2
- Claim: Structured Q&A knowledge injection improves LLM understanding of depression-related content, particularly for text-based predictions.
- Mechanism: Extract 4,920 Q&A pairs from WHO ICD entries using DeepSeek-V3 with 5 question types (definition, rationale, symptoms, extended knowledge, critical thinking), then fine-tune LLaMA via supervised learning to generate authoritative answers.
- Core assumption: Structured Q&A training transfers to improved PHQ-8 score prediction; knowledge in textual form aligns better with text modality than audio.
- Evidence anchors:
  - [abstract]: "introduces a novel strategy for incorporating psychological knowledge into LLMs through structured question-answer pairs"
  - [section 3]: "knowledge injection has a noticeable impact... The performance improves more for text than for audio... MAE drops from 6.342 to 5.354 for text"
  - [corpus]: PsychoLexLLaMA (Abbasi et al., cited in paper) demonstrates Q&A-based knowledge injection for psychological assessment, supporting plausibility.
- Break condition: If Q&A content lacks coverage of depression manifestations in speech (acoustic markers), text-knowledge transfer to audio modality will remain limited.

### Mechanism 3
- Claim: Segment-level prediction aggregation produces participant-level PHQ-8 estimates with lower variance.
- Mechanism: Split each participant's interview into merged 5-utterance segments (6,556 total files), predict PHQ-8 per segment, then average predictions for final participant score.
- Core assumption: Depression severity is consistent across utterances within a participant; segment-level predictions are i.i.d. samples around true score.
- Evidence anchors:
  - [section 2.2.2]: "the dataset only provides a single sentence-independent PHQ-8 score per participant, which we therefore take as the target for all sentences"
  - [section 2.2.2]: "These segment-level predictions are then averaged to produce the overall PHQ-8 score"
  - [corpus]: No direct validation found; aggregation strategy is dataset-specific.
- Break condition: If segments have heterogeneous emotional content (e.g., neutral questions vs. emotional disclosures), averaging may dilute signal.

## Foundational Learning

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: Fine-tunes LLaMA-2-7B with limited GPU resources (V100 32GB) by training only low-rank decomposition matrices (r=8) instead of full weights.
  - Quick check question: Can you explain why LoRA reduces trainable parameters while preserving original model weights?

- Concept: **Wav2Vec 2.0 Pre-training**
  - Why needed here: Extracts self-supervised speech representations that capture prosodic and paralinguistic features relevant to depression, without requiring ASR transcription.
  - Quick check question: How does Wav2Vec 2.0 differ from Whisper in terms of training objective and feature type?

- Concept: **Multimodal Projection Alignment**
  - Why needed here: Maps audio embeddings to LLM text embedding space so a single transformer can process both modalities.
  - Quick check question: What is the shape transformation required between Wav2Vec output and LLaMA hidden dimension?

## Architecture Onboarding

- Component map:
  - **Input**: DAIC-WOZ audio + transcripts → segmented into 5-utterance chunks
  - **Audio encoder**: Wav2Vec 2.0 (frozen) → feedforward projector
  - **Text encoder**: LLaMA tokenizer + embeddings
  - **Knowledge injector**: LLaMA fine-tuned on 4,920 WHO-derived Q&A pairs
  - **Fusion**: Projected audio embeddings concatenated/mixed with text embeddings in LLaMA forward pass
  - **Output**: Linear head on last hidden layer → PHQ-8 regression

- Critical path:
  1. Knowledge injection pre-training (Q&A pairs → LLaMA)
  2. Text-only fine-tuning (transcripts → PHQ-8)
  3. Audio projector training (Wav2Vec → LLaMA space → PHQ-8)
  4. Audio+text joint inference with segment averaging

- Design tradeoffs:
  - **LoRA vs. full fine-tuning**: LoRA enables resource-constrained training but limits model capacity for substantial behavior change (acknowledged in paper section 4)
  - **Wav2Vec vs. Whisper**: Wav2Vec captures broader acoustic features; Whisper optimized for ASR may miss prosodic cues
  - **Segment size (5 utterances)**: Balances context window limits vs. temporal resolution

- Failure signatures:
  - Text+Audio performs same as Audio-only → text modality not contributing (observed in results)
  - Knowledge injection improves text but not audio → modality gap in knowledge transfer
  - High RMSE variance across participants → segment heterogeneity

- First 3 experiments:
  1. **Baseline replication**: Run audio-only model without knowledge injection; verify MAE ≈ 5.37
  2. **Ablation: projector capacity**: Test larger feedforward network (e.g., 2-layer MLP) for audio projection
  3. **Ablation: segment size**: Compare 3-utterance vs. 5-utterance vs. 10-utterance segmentation on validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can psychological knowledge be effectively injected into the audio modality to bridge the gap with text-based knowledge?
- Basis in paper: [explicit] The authors state that while the model is equipped with theoretical knowledge, "there is a notable scarcity of psychologically-informed audio data," causing the model to "struggle to effectively transfer this understanding to audio-based applications."
- Why unresolved: The current method injects knowledge via text-based Q&A pairs. Translating these semantic clinical concepts into acoustic embeddings without paired audio-text clinical datasets remains an unaddressed challenge.
- What evidence would resolve it: Experiments utilizing "clinically annotated audio datasets that pair spoken examples with corresponding textual explanations" to demonstrate if audio-specific knowledge injection significantly lowers MAE.

### Open Question 2
- Question: Does full-scale training of the LLM yield significantly better performance than the Low-Rank Adaptation (LoRA) approach used in this study?
- Basis in paper: [explicit] The authors acknowledge that "LoRA offers a lightweight... method, its capacity to induce substantial changes in model behaviour is limited compared to full-scale training," noting that fully training demands resources not always feasible.
- Why unresolved: The study relied on LoRA due to GPU resource limitations, so the potential performance ceiling of the proposed architecture under full parameter fine-tuning remains unknown.
- What evidence would resolve it: A comparative analysis of MAE and RMSE between the proposed LoRA-based model and a fully fine-tuned version on the DAIC-WOZ dataset.

### Open Question 3
- Question: Why does the multimodal fusion of audio and text fail to outperform the audio-only baseline significantly in the proposed architecture?
- Basis in paper: [inferred] The results show that the multimodal model (Audio+Text+Knowledge) achieved an MAE of 5.356, which is nearly identical to the audio-only model (5.373) and showed no additive benefit from the text improvements.
- Why unresolved: The authors suggest the model's learning may be "biased towards audio" or limited by LoRA, but the specific failure of the fusion mechanism to leverage the text improvements is not empirically determined.
- What evidence would resolve it: Ablation studies or gradient analyses showing whether the feedforward projector successfully aligns text embeddings with the LLM's hidden space or if they are overshadowed by audio features.

## Limitations

- The specific architecture of the feedforward projector mapping Wav2Vec embeddings to LLaMA's hidden space is not detailed, risking loss of acoustic depression markers during transformation.
- Knowledge injection improves text-based predictions but the degree to which WHO-derived depression knowledge transfers to acoustic depression recognition remains uncertain.
- With only 47 participants in the test set, the reported MAE of 5.356 and RMSE of 6.713 may not generalize reliably to broader populations.

## Confidence

- **High Confidence**: The multimodal fusion architecture and knowledge injection approach are technically sound and well-motivated by prior research. The performance improvement over audio-only baseline is substantial and measurable.
- **Medium Confidence**: The specific implementation details for audio projection and knowledge injection are plausible but not fully specified. The observed performance gains are consistent with the proposed mechanisms but require independent verification.
- **Low Confidence**: Generalization claims to broader populations and the effectiveness of knowledge transfer to acoustic depression markers require additional validation beyond the DAIC-WOZ dataset.

## Next Checks

1. **Ablation Study on Projector Architecture**: Systematically test different projector configurations (varying depth, width, activation functions) to identify the minimum viable architecture that preserves audio information during embedding transformation.

2. **Cross-Dataset Validation**: Evaluate the trained model on an independent depression detection dataset (e.g., AVEC 2019-2020) to assess generalization beyond DAIC-WOZ and validate the robustness of reported performance metrics.

3. **Knowledge Transfer Analysis**: Conduct a modality-specific ablation where the knowledge-injected model is tested separately on text-only, audio-only, and multimodal inputs to quantify the exact contribution of knowledge injection to each modality's performance.