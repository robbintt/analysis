---
ver: rpa2
title: 'SA-IQA: Redefining Image Quality Assessment for Spatial Aesthetics with Multi-Dimensional
  Rewards'
arxiv_id: '2512.05098'
source_url: https://arxiv.org/abs/2512.05098
tags:
- quality
- image
- spatial
- sa-iqa
- lighting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces SA-IQA, a new Image Quality Assessment (IQA)\
  \ framework tailored for interior spatial aesthetics. It addresses the lack of systematic\
  \ evaluation tools for AI-generated interior scenes by defining four key dimensions\u2014\
  layout, harmony, lighting, and distortion\u2014and constructing SA-BENCH, a large-scale\
  \ benchmark of 18,000 images with 50,000 precise human annotations."
---

# SA-IQA: Redefining Image Quality Assessment for Spatial Aesthetics with Multi-Dimensional Rewards

## Quick Facts
- arXiv ID: 2512.05098
- Source URL: https://arxiv.org/abs/2512.05098
- Authors: Yuan Gao; Jin Song
- Reference count: 40
- Primary result: New IQA framework achieving state-of-the-art PLCC/SRCC correlations on spatial aesthetic dimensions for interior scene generation

## Executive Summary
This work introduces SA-IQA, a novel Image Quality Assessment framework specifically designed for evaluating AI-generated interior scenes. The framework addresses the lack of systematic evaluation tools for spatial aesthetics by defining four key dimensions—layout, harmony, lighting, and distortion—and constructing SA-BENCH, a large-scale benchmark with 18,000 images and 50,000 human annotations. SA-IQA leverages MLLM fine-tuning with expert-aware prompts and a multi-dimensional fusion approach to produce calibrated, domain-specific scores that significantly outperform existing IQA methods.

## Method Summary
SA-IQA is developed through supervised fine-tuning of the Ovis2.5-9B MLLM using a frozen visual encoder (ViT and Aligner) and fine-tuned LLM component. The framework uses expert-aware prompts with dimension tags (e.g., "<Layout>") to guide evaluation, converts categorical text outputs to expected-value scores via softmax-weighted summation over rating words, and employs Bradley-Terry optimized fusion weights to combine dimension scores into a single reward signal. Training uses AdamW optimizer with specific hyperparameters on H20 GPUs, with three epochs and careful data preparation including prompt corruption and annotation filtering.

## Key Results
- Achieves state-of-the-art PLCC/SRCC correlations across all four dimensions: Overall (0.864/0.860), Layout (0.843/0.848), Harmony (0.896/0.894), Lighting (0.817/0.818), Distortion (0.657/0.596)
- Expert-aware prompts significantly outperform concise prompts in correlation metrics
- Bradley-Terry optimized fusion weights achieve 0.567 rank accuracy versus 0.503 for equal weighting
- Validated in two downstream tasks: GRPO-based prompt optimization and Best-of-N selection, both substantially improving AIGC quality

## Why This Works (Mechanism)

### Mechanism 1
Expert-aware prompts with domain-specific terminology substantially improve MLLM-based IQA correlation over concise prompts. Detailed evaluation criteria embedded in prompts activate specialized visual attention patterns in the MLLM, enabling finer-grained discrimination of spatial aesthetic flaws. Core assumption: pre-trained MLLM has latent spatial reasoning capabilities requiring explicit semantic scaffolding. Evidence: Type 4 Expert-Aware prompts achieve best Overall (0.864/0.860) and Distortion (0.657/0.596) vs Type 1 Concise (0.857/0.855 and 0.575/0.552).

### Mechanism 2
Probabilistic conversion of categorical text outputs to expected-value scores yields calibrated continuous MOS predictions. Extracting top-5 logits for rating words and computing softmax-weighted expected value preserves model uncertainty. Core assumption: MLLM's logit distribution over rating words meaningfully encodes confidence. Evidence: Softmax normalization formula provided with weighted sum calculation, demonstrated in Q-Align [27].

### Mechanism 3
Bradley-Terry optimized fusion weights outperform equal-weight aggregation for combining multi-dimensional scores into single reward signal. Pairwise preference labels directly optimize relative contribution of each dimension to human overall judgments. Core assumption: human overall preferences can be approximated as linear combination of dimension scores. Evidence: BT-loss formula with optimal weights achieving 0.567 rank accuracy vs 0.503 for equal weighting.

## Foundational Learning

- **Concept: Bradley-Terry Models**
  - Why needed here: Core mechanism for learning fusion weights from pairwise preferences; foundational for reward model training
  - Quick check question: Can you explain why BT loss uses sigmoid((xA - xB)ᵀw) rather than direct regression on individual scores?

- **Concept: MLLM Supervised Fine-Tuning (SFT) with Frozen Vision Encoder**
  - Why needed here: SA-IQA freezes ViT/Aligner and only fine-tunes LLM—understanding this split is critical for efficient training
  - Quick check question: Why might freezing the visual encoder preserve general perceptual capabilities while LLM fine-tuning adapts evaluation criteria?

- **Concept: PLCC vs SRCC Correlation Metrics**
  - Why needed here: Paper reports both; they capture different alignment properties (linear vs rank-order)
  - Quick check question: If PLCC improves but SRCC degrades, what does this indicate about score distribution changes?

## Architecture Onboarding

- **Component map**: Input Image → Ovis2.5-9B Backbone (ViT frozen, LLM fine-tuned) → Dimension-Conditioned Query (e.g., "<Layout> expert-aware prompt") → Text Output Token ("excellent" | "good" | "fair" | "poor" | "bad") → Logit Extraction (top-5 rating words) → Softmax → Expected Value → Per-Dimension Score (4D vector) → Bradley-Terry Weighted Fusion → Final Scalar Reward

- **Critical path**: Prompt template design → SFT data quality (MOS aggregation) → Logit extraction correctness → Fusion weight optimization. Errors in logit extraction (wrong token indexing) cascade silently.

- **Design tradeoffs**: Full fine-tuning vs LoRA: Table 8 shows full fine-tuning (0.864 Overall) dramatically outperforms LoRA (0.642). Model scale: 4B→8B shows saturation (0.853→0.849 for Qwen3-VL); suggest 4-9B sweet spot. Threshold selection in fusion: Dynamic thresholds for tie-breaking affect ranking accuracy significantly.

- **Failure signatures**: Near-zero or negative correlations on specific dimensions indicates systematic bias or misunderstanding of evaluation criteria. Large gap between PLCC and SRCC suggests score miscalibration despite correct ranking. Distortion consistently lowest-performing dimension may indicate inherent task difficulty or annotation noise.

- **First 3 experiments**: 1) Baseline validation: Run pretrained Ovis2.5 on SA-BENCH test split with Type 1 prompts to establish zero-shot correlation floor. 2) Ablate prompt types on held-out subset: Compare Type 3 vs Type 4 prompts on 500-image subset. 3) Fusion weight sanity check: Compare equal-weight vs BT-optimized fusion on pairwise preference task.

## Open Questions the Paper Calls Out

### Open Question 1
Would scaling the model backbone beyond 9B parameters (e.g., to 32B or 72B) specifically narrow the performance gap observed in the "Distortion" dimension? The authors state computational constraints prevented exploring larger model scales, and Table 3 shows "Distortion" correlation (0.657) lags significantly behind "Harmony" (0.896).

### Open Question 2
Can the SA-IQA reward signal effectively penalize or guide generation for complex semantic reasoning tasks, such as strict functional furniture arrangement? Section G notes that the model's applicability is limited for tasks needing deeper semantic reasoning because fine-tuning focused primarily on visual perception.

### Open Question 3
How robust is the Bradley-Terry fusion mechanism against distribution shifts when applied to generative models not represented in the SA-BENCH training set? The SA-BENCH dataset is constructed using only four specific inpainting models, and fusion weights are learned on this specific data distribution.

## Limitations
- Lack of public access to SA-BENCH dataset prevents independent validation
- Expert-aware prompts and exact corruption strategies for prompt generation are unspecified
- Reliance on human annotations introduces potential subjective bias, particularly for harmony and lighting dimensions
- Bradley-Terry fusion assumes linear combination of dimension scores may not capture nonlinear interactions

## Confidence
- **High confidence**: MLLM fine-tuning methodology with frozen visual encoder, softmax-based score calibration from logits, and overall experimental methodology
- **Medium confidence**: Specific improvements from expert-aware prompts and effectiveness of Bradley-Terry fusion
- **Low confidence**: Generalization beyond interior design to other spatial aesthetic domains and assumption of linear fusion for pairwise preferences

## Next Checks
1. **Dataset Accessibility Validation**: Attempt to obtain SA-BENCH through authors' channels to verify claimed 17,753 images and 50,476 annotations match paper's description
2. **Prompt Ablation Reproduction**: Replicate prompt type comparison (Type 1 vs Type 4) on small subset of SA-BENCH (500 images)
3. **Fusion Weight Stability Test**: Train Bradley-Terry fusion on 750 pairwise preferences, evaluate on independent held-out set of 250 pairs to confirm learned weights are stable