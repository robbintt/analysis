---
ver: rpa2
title: 'MOTGNN: Interpretable Graph Neural Networks for Multi-Omics Disease Classification'
arxiv_id: '2508.07465'
source_url: https://arxiv.org/abs/2508.07465
tags:
- graph
- data
- motgnn
- omics
- xgboost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MOTGNN addresses multi-omics integration for disease classification
  by combining XGBoost-based supervised graph construction with modality-specific
  graph neural networks and a deep feedforward network for cross-omics integration.
  The model constructs sparse, task-specific feature graphs using XGBoost decision
  paths, applies graph neural networks to learn modality-specific embeddings, and
  integrates them through a deep network.
---

# MOTGNN: Interpretable Graph Neural Networks for Multi-Omics Disease Classification

## Quick Facts
- arXiv ID: 2508.07465
- Source URL: https://arxiv.org/abs/2508.07465
- Reference count: 40
- Key outcome: MOTGNN outperforms baselines by 5-10% in accuracy, ROC-AUC, and F1-score on cancer datasets while providing interpretable feature importance scores.

## Executive Summary
MOTGNN addresses multi-omics integration for disease classification by combining XGBoost-based supervised graph construction with modality-specific graph neural networks and a deep feedforward network for cross-omics integration. The model constructs sparse, task-specific feature graphs using XGBoost decision paths, applies graph neural networks to learn modality-specific embeddings, and integrates them through a deep network. Evaluated on three cancer datasets (COADREAD, LGG, STAD), MOTGNN consistently outperforms baselines (XGBoost, RF, DFN, GCN) with 5-10% improvements in accuracy, ROC-AUC, and F1-score. It maintains robustness under severe class imbalance, achieving 87.2% F1-score versus 33.4% for RF on imbalanced data. The approach produces sparse graphs (2.1-2.8 edges per node) and provides interpretable feature and omics-level importance scores. Results demonstrate superior predictive accuracy and built-in interpretability for multi-omics disease modeling.

## Method Summary
MOTGNN integrates multi-omics data through a three-stage pipeline: supervised graph construction using XGBoost decision paths to create task-specific feature graphs, modality-specific graph neural networks to learn embeddings within each omics layer, and a deep feedforward network to integrate cross-omics information. The graph construction identifies biologically relevant feature interactions by extracting decision paths from XGBoost trees, creating sparse graphs that balance interpretability with predictive power. Each omics modality is processed independently through a GCN to learn local representations, then combined through a DFN for final classification. The model is trained end-to-end with cross-entropy loss, allowing the GNN embeddings to be optimized for the downstream classification task while maintaining the interpretable graph structure.

## Key Results
- MOTGNN achieves 5-10% higher accuracy, ROC-AUC, and F1-score compared to baselines (XGBoost, RF, DFN, GCN) across three cancer datasets
- Maintains strong performance under severe class imbalance (88.9% accuracy vs 64.8% for XGBoost on 10% minority class)
- Produces sparse, interpretable graphs with 2.1-2.8 edges per node while preserving predictive performance
- Identifies biologically relevant biomarkers including SFRP4 and TFF3 with computational importance scores

## Why This Works (Mechanism)
The effectiveness stems from leveraging XGBoost's decision trees to construct biologically meaningful feature graphs that capture task-specific interactions, then using GNNs to learn local representations within each omics modality before integrating them through a deep network. The static graph construction based on XGBoost paths ensures interpretability while the GNN layers learn optimal embeddings for classification. The deep feedforward network effectively combines cross-omics information by learning weighted combinations of modality-specific features, allowing the model to capture complex interactions between different molecular layers.

## Foundational Learning
- **Graph Neural Networks**: Why needed - learn node representations by aggregating information from local graph neighborhoods; Quick check - can propagate information through connected features
- **Multi-omics integration**: Why needed - capture complementary information across molecular layers (genomics, transcriptomics, epigenomics); Quick check - improved predictive performance vs single-omics approaches
- **Supervised graph construction**: Why needed - create task-specific feature relationships rather than using generic similarity metrics; Quick check - edges correspond to biologically meaningful interactions
- **Class imbalance handling**: Why needed - real-world disease datasets often have skewed class distributions; Quick check - balanced accuracy and F1-score remain high under imbalance
- **Interpretability in deep learning**: Why needed - enable identification of relevant biomarkers for clinical validation; Quick check - feature importance scores align with biological knowledge

## Architecture Onboarding

Component map: Omics Data -> XGBoost Graph Construction -> Modality-Specific GNN -> Cross-Omics DFN -> Classification Output

Critical path: Feature selection through XGBoost → Graph construction → GNN embedding learning → Cross-omics integration → Final classification

Design tradeoffs: Static vs dynamic graph structures (interpretability vs potential performance), single vs multiple GNN layers per modality (computational efficiency vs representation power), simple vs complex cross-omics integration (transparency vs modeling capacity)

Failure signatures: Poor performance may indicate suboptimal graph construction thresholds, insufficient GNN depth, or ineffective cross-omics integration weights

First experiments: 1) Vary XGBoost tree depth and number of trees to optimize graph quality, 2) Test different GNN layer counts and aggregation functions per modality, 3) Experiment with alternative cross-omics integration architectures (attention vs weighted sum)

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can MOTGNN be adapted for multi-class classification or survival analysis while maintaining its current level of interpretability?
- Basis in paper: [explicit] The Conclusion states that future extensions may include "adapting the architecture for more complex prediction tasks, such as multi-class classification or survival analysis."
- Why unresolved: The current architecture is specifically designed for binary classification using a DFN with a softmax output; extending this to censored data (survival) or multiple classes requires fundamental changes to the loss function and output layers.
- What evidence would resolve it: Successful implementation of a Cox proportional hazards loss layer or multi-class classification head within the MOTGNN framework, evaluated on suitable benchmarks.

### Open Question 2
- Question: Do the top-ranked biomarkers identified by MOTGNN (e.g., SFRP4) correlate with actual biological mechanisms when subjected to wet-lab validation?
- Basis in paper: [explicit] The authors state in the Results section that "biological validation remains an important next step" after computationally identifying biomarkers like SFRP4.
- Why unresolved: Computational importance scores indicate predictive power but do not guarantee that the identified features are biologically causal drivers of the disease phenotypes.
- What evidence would resolve it: Experimental studies (e.g., gene knockdown or methylation interference) confirming that perturbing the top-ranked MOTGNN features induces the expected phenotypic changes in disease models.

### Open Question 3
- Question: Does the static graph construction method limit the discovery of latent interactions compared to fully dynamic, learnable graph structures?
- Basis in paper: [inferred] The graph topology is fixed based on XGBoost decision paths prior to GNN training; the edges are not updated or refined during the backpropagation of the GNN.
- Why unresolved: A static graph enforces a specific topology based on tree splits, which might overlook complex, non-linear feature interactions that could be discovered through end-to-end structure learning.
- What evidence would resolve it: A comparative study integrating a learnable adjacency matrix (e.g., via attention mechanisms) to see if allowing the graph structure to evolve during training improves performance over the static XGBoost graph.

## Limitations
- Focuses exclusively on TCGA cancer datasets, limiting applicability to other disease types or non-cancer multi-omics applications
- Graph construction relies on XGBoost decision paths, which may not capture all relevant biological relationships and could be sensitive to hyperparameter choices
- Performance comparisons lack recent state-of-the-art multi-omics integration methods beyond cited related works
- Interpretability claims lack biological validation or pathway analysis to confirm clinical relevance of identified features

## Confidence
- **High confidence**: Predictive performance improvements over baselines, class imbalance robustness, and graph sparsity metrics
- **Medium confidence**: Interpretability claims without biological validation, generalization beyond TCGA datasets
- **Low confidence**: Optimal graph construction parameters, sensitivity to omics modality scaling

## Next Checks
1. Test MOTGNN on non-cancer multi-omics datasets (e.g., neurological disorders, metabolic diseases) to assess cross-disease applicability
2. Perform ablation studies varying XGBoost hyperparameters and graph construction thresholds to determine sensitivity
3. Conduct biological validation of top-ranked features through pathway enrichment analysis and comparison with known disease mechanisms