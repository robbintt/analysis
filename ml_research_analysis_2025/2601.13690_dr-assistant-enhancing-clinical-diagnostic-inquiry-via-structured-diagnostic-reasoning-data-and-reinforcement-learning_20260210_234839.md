---
ver: rpa2
title: 'Dr. Assistant: Enhancing Clinical Diagnostic Inquiry via Structured Diagnostic
  Reasoning Data and Reinforcement Learning'
arxiv_id: '2601.13690'
source_url: https://arxiv.org/abs/2601.13690
tags:
- inquiry
- clinical
- diagnostic
- reasoning
- patient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enhancing clinical diagnostic
  reasoning and inquiry capabilities in Large Language Models (LLMs) for healthcare
  applications. The authors propose a structured Clinical Diagnostic Reasoning Data
  (CDRD) format to capture abstract diagnostic logic, along with a three-stage pipeline
  for constructing CDRD from clinical guidelines.
---

# Dr. Assistant: Enhancing Clinical Diagnostic Inquiry via Structured Diagnostic Reasoning Data and Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2601.13690
- **Source URL:** https://arxiv.org/abs/2601.13690
- **Reference count:** 40
- **Primary result:** 13.59% improvement in ICD-Recall over HuatuoGPT-o1-72B on 242 real clinical cases

## Executive Summary
This paper addresses the challenge of enhancing clinical diagnostic reasoning and inquiry capabilities in Large Language Models (LLMs) for healthcare applications. The authors propose a structured Clinical Diagnostic Reasoning Data (CDRD) format to capture abstract diagnostic logic, along with a three-stage pipeline for constructing CDRD from clinical guidelines. They develop Dr. Assistant, a clinical diagnostic model trained through Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) with a tailored reward function that evaluates clinical reasoning, inquiry skills, and fidelity to CDRD. The model is evaluated on a benchmark of 242 real clinical cases across 8 secondary departments and 147 real inquiry rounds, demonstrating state-of-the-art performance: 13.59% improvement in ICD-Recall over HuatuoGPT-o1-72B, achieving competitive performance to closed-source models like GPT-5, and receiving higher physician satisfaction ratings for inquiry quality.

## Method Summary
The authors develop Dr. Assistant using a two-stage training pipeline: Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL). First, they construct Clinical Diagnostic Reasoning Data (CDRD) from internal clinical guidelines using a three-stage pipeline (symptom extraction, disease matching, logic completion) with LLM synthesis and physician refinement. This structured data captures abstract clinical reasoning logic as triple-tuples (Symptom, Evidence, Diagnoses). From CDRD, they synthesize 4,400 QA pairs for SFT and 36,688 multi-turn inquiry dialogues for RL. The RL stage uses DAPO optimization with a composite reward function that evaluates clinical reasoning, inquiry skills, and fidelity to CDRD. The model is based on Qwen3-14B and trained on 32 A800 GPUs for SFT and 8 H800 GPUs for RL.

## Key Results
- 13.59% improvement in ICD-Recall over HuatuoGPT-o1-72B
- Competitive performance to closed-source models like GPT-5
- Higher physician satisfaction ratings for inquiry quality compared to baseline models
- Ablation studies show RL contributes 13.39% ICD-Recall improvement and 18.82% satisfaction improvement

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structuring diagnostic logic into CDRD triples (Symptom, Evidence, Diagnoses) facilitates learning by consolidating scattered guideline knowledge.
- **Mechanism:** Clinical guidelines scatter logic across chapters. The proposed pipeline extracts and re-structures this into a triple-tuple (S, E, D), reducing the burden on the model to "retrieve and connect" logic implicitly.
- **Core assumption:** The LLM + Physician pipeline successfully captures abstract reasoning logic without significant information loss.
- **Evidence anchors:** Abstract mentions CDRD structure; Section 3.1 defines the triple-tuple; Section 3.2 describes the pipeline to resolve "scattered reasoning logic."
- **Break condition:** If CDRD is too rigid or fails to capture comorbidities, the model may struggle with complex patients.

### Mechanism 2
- **Claim:** Sequential SFT and RL elicit superior inquiry skills compared to SFT alone.
- **Mechanism:** SFT initializes the model with static diagnostic knowledge. RL then uses a composite reward function to optimize dynamic, multi-turn inquiry trajectories, moving the model from "knowing facts" to "acting strategically."
- **Core assumption:** The reward function accurately proxies clinical utility, and DAPO optimizes this policy effectively.
- **Evidence anchors:** Abstract mentions two-stage process; Table 3 shows removing RL drops ICD-Recall significantly (from 0.5066 to 0.3927).
- **Break condition:** If SFT and RL data distributions differ significantly, the model may suffer from catastrophic forgetting.

### Mechanism 3
- **Claim:** Penalizing logical deviations from source CDRD during RL reduces diagnostic hallucination.
- **Mechanism:** The reward function includes Rdiv penalty for "Fidelity to CDRD." If the model generates diagnoses or information not present in source CDRD, it is penalized, constraining the policy search space to valid clinical logic paths.
- **Core assumption:** CDRD contains a sufficiently comprehensive set of valid diagnostic paths.
- **Evidence anchors:** Section 4 defines Rdiv as penalty λn; Table 3 shows removing Rdiv drops ICD-Recall by ~4%.
- **Break condition:** Over-constraining via Rdiv might suppress novel or correct clinical hypotheses missing from initial CDRD.

## Foundational Learning

### Concept: Hypothetico-Deductive Reasoning
- **Why needed here:** The CDRD structure (S→E→D) mirrors this clinical reasoning pattern. Understanding this explains why "Diagnoses" come before "Inquiry" in the template.
- **Quick check question:** In the model's output template, does the "Inquiry" step come before or after the "Diagnoses" step, and why?

### Concept: LLM-as-a-Judge
- **Why needed here:** The reward function (Rcomp) relies on a separate LLM (Qwen3-32B) to score reasoning similarity to ground truth. You must understand limitations and biases of using a model to grade a model.
- **Quick check question:** What model is used to score the 7 reasoning and inquiry parts in the reward function Rcomp?

### Concept: DAPO (Dynamic Advantage Policy Optimization)
- **Why needed here:** The paper uses DAPO for RL stage. Familiarity with policy gradients and advantage calculation is required to debug the training loop.
- **Quick check question:** How is the advantage (Â) calculated in the DAPO objective?

## Architecture Onboarding

### Component map
Data Construction Pipeline (LLM Synthesis + Physician Refinement of CDRD) -> Data Synthesis (QA & Dialogue Generators) -> Training Pipeline (SFT module -> RL VeRL framework with LLM-Judge) -> Evaluation (Patient Simulator + Physician Review)

### Critical path
The most critical dependency is the quality of the CDRD. If CDRD is incomplete, SFT learns flawed logic, and RL's "Fidelity" penalty will reinforce errors.

### Design tradeoffs
The authors traded scalability for fidelity by using "LLM synthesis + Physician refinement" pipeline, ensuring high data quality but limiting volume. Another tradeoff is using a fixed template for reasoning which enforces structure but might limit natural expression.

### Failure signatures
- **Reward Hacking:** Model generates correct "Reasoning" text to appease LLM-Judge but outputs poor or irrelevant "Inquiry"
- **Over-fitting to Template:** Model gets stuck in repetitive loops of the 6-step reasoning template without progressing diagnosis

### First 3 experiments
1. **Verify Data Quality:** Randomly sample 20 CDRD entries and trace them back to original guidelines to confirm "LLM + Physician" pipeline correctly consolidated scattered logic.
2. **Reward Sensitivity Analysis:** Run RL loop with Rdiv weight (λ) set to 0 to observe if model hallucinates diagnoses not present in CDRD.
3. **Judge Consistency Check:** Evaluate consistency of LLM-Judge (Qwen3-32B) by scoring same model response 3 times to check for variance.

## Open Questions the Paper Calls Out
- Can the CDRD construction pipeline be fully automated to eliminate manual physician refinement while maintaining data quality?
- Does replacing the heuristic RL reward function with data-driven reward modeling or direct clinical outcome feedback improve diagnostic nuance capture?
- Does the model maintain efficacy and safety when deployed in fully open-ended clinical conversations or integrated with live EHR systems?
- How does the model generalize to specialized or broader clinical settings outside the 8 secondary departments covered in the benchmark?

## Limitations
- Relies on physician refinement for CDRD construction, introducing manual effort and scalability limitations
- Benchmark covers limited number of secondary departments (8), potentially affecting generalizability
- Experiments focus on diagnostic inquiry within controlled dialogue framework, not fully open-ended clinical conversations

## Confidence

### High confidence
- Methodology for structuring diagnostic reasoning (CDRD format)
- General training pipeline (SFT + RL)
- Ablation studies provide strong evidence for component effectiveness

### Medium confidence
- Specific performance claims due to proprietary evaluation data
- Lack of comparison to recent clinical diagnostic models on public benchmarks

### Low confidence
- Generalizability to clinical settings outside specific departments and disease categories covered in CDRD

## Next Checks
1. **Data validation:** Randomly sample 20 CDRD entries and trace them back to original guidelines to confirm "LLM + Physician" pipeline correctly consolidated scattered logic.
2. **Reward function analysis:** Run RL loop with Rdiv weight (λ) set to 0 to observe if model hallucinates diagnoses not present in CDRD.
3. **Judge consistency verification:** Evaluate consistency of LLM-Judge (Qwen3-32B) by scoring same model response 3 times to check for variance.