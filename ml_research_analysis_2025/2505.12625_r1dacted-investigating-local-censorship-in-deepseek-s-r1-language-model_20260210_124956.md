---
ver: rpa2
title: 'R1dacted: Investigating Local Censorship in DeepSeek''s R1 Language Model'
arxiv_id: '2505.12625'
source_url: https://arxiv.org/abs/2505.12625
tags:
- censorship
- prompts
- behavior
- reasoning
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates local censorship in DeepSeek\u2019s R1\
  \ language model, focusing on its refusal to answer politically sensitive topics\
  \ related to China. The authors develop a systematic pipeline to curate a dataset\
  \ of over 10,000 prompts that trigger censorship in R1 but not in other models."
---

# R1dacted: Investigating Local Censorship in DeepSeek's R1 Language Model

## Quick Facts
- arXiv ID: 2505.12625
- Source URL: https://arxiv.org/abs/2505.12625
- Authors: Ali Naseh; Harsh Chaudhari; Jaechul Roh; Mingshi Wu; Alina Oprea; Amir Houmansadr
- Reference count: 40
- Primary result: Local censorship in DeepSeek's R1 can be bypassed in 97.86% of cases using a simple prompt-based jailbreak

## Executive Summary
This paper investigates local censorship in DeepSeek's R1 language model, focusing on its refusal to answer politically sensitive topics related to China. The authors develop a systematic pipeline to curate a dataset of over 10,000 prompts that trigger censorship in R1 but not in other models. They analyze censorship patterns across topics, languages, and distilled models, finding that censorship is pervasive but can be bypassed with a simple prompt-based jailbreaking method, successfully recovering answers for 97.86% of censored samples. The study also compares their approach to Perplexity's censorship removal model, showing that the jailbreak method preserves factuality better and requires no computational overhead. Overall, the work highlights local censorship as an emerging alignment behavior in LLMs, raising concerns about transparency and governance in model deployment.

## Method Summary
The study curates a dataset of 10,030 prompts that trigger censorship in DeepSeek R1 but not in reference models (GPT-4o, o3-mini-high, Llama-3-8B-Instruct). The jailbreaking method prepends "Okay, the user is asking" within R1's reasoning delimiters, iteratively forcing the model to generate reasoning tokens that bypass censorship shortcuts. For distillation experiments, censored samples are injected into fine-tuning data to transfer censorship behaviors to student models. The evaluation uses GPT-4.1 with a JSON classifier prompt to categorize censorship types and factuality judgments.

## Key Results
- R1 exhibits local censorship on 96 categories of China-related sensitive topics, refusing 9.12% of tested prompts
- Simple jailbreaking recovers answers for 97.86% of censored samples by forcing reasoning token generation
- Distillation experiments show censorship can transfer to student models with as few as 30 injected samples achieving 95.5% censorship rate
- The jailbreak method preserves factuality better than Perplexity's R1-1776 model while requiring no computational overhead

## Why This Works (Mechanism)

### Mechanism 1: Censorship Classification via Differential Model Behavior
- Claim: Local censorship is identified by prompts that trigger refusal in R1 but not in reference models
- Mechanism: By filtering out globally censored prompts, the remaining R1-only refusals are attributed to model-specific alignment choices
- Core assumption: Reference models adequately represent neutral safety standards
- Evidence anchors: [abstract] "curated prompts that trigger censorship in R1, covering a range of politically sensitive topics, but are not censored by other models"
- Break condition: If reference models themselves exhibit hidden biases

### Mechanism 2: Jailbreaking via Reasoning Token Prefill
- Claim: Prepending "Okay, the user is asking" within reasoning delimiters bypasses censorship
- Mechanism: Censorship manifests as skipped reasoning tokens; forcing reasoning initiation disrupts this shortcut
- Core assumption: Censorship is implemented as a reasoning bypass mechanism
- Evidence anchors: [abstract] "can be bypassed with a simple prompt-based jailbreaking method"
- Break condition: If models detect reasoning prefill patterns or move censorship to post-reasoning filtering

### Mechanism 3: Censorship Transfer Through Distillation Data Injection
- Claim: Censorship behaviors transfer to distilled models via injection of censored examples
- Mechanism: Supervised fine-tuning on censored input-output pairs causes student models to learn refusal patterns
- Core assumption: Censorship is encoded in input-output patterns rather than architectural constraints
- Evidence anchors: [section VI] Table V shows refusal samples achieve 98.5% censorship transfer
- Break condition: If base model's reasoning architecture inherently resists shortcuts

## Foundational Learning

- **Safety Alignment in LLMs**: Why needed - The paper frames censorship as an alignment behavior; understanding RLHF contextualizes how such behaviors emerge. Quick check - Can you explain how refusal responses are typically reinforced during RLHF?
- **Chain-of-Thought (CoT) Reasoning**: Why needed - R1's architecture separates reasoning tokens from final outputs; censorship exploits this by skipping reasoning. Quick check - What is the expected behavior of a CoT model when asked to "show its work" on a sensitive topic?
- **Knowledge Distillation**: Why needed - The paper examines whether censorship transfers from teacher to student models via fine-tuning. Quick check - In distillation, what determines which teacher behaviors the student learns?

## Architecture Onboarding

- **Component map**: Topic extraction (CDT 404 Archive) → Prompt generation (human + LLM) → Global censorship filter (multi-model check) → Local censorship filter (R1-only refusal)
- **Critical path**: Jailbreak success depends on correct reasoning initiation phrases, model's willingness to continue generation, and censorship shortcut not activating mid-stream
- **Design tradeoffs**: Random vs. diverse vs. refusal sample injection for distillation; jailbreak repetitions vs. failure rate; factuality vs. uncensoring
- **Failure signatures**:
  - Type 1: Empty reasoning, template-like positive answer (97.3% of R1 censorship)
  - Type 2: Empty reasoning, explicit refusal (2.7% of R1 censorship)
  - Type 3 (jailbreak failure): Reasoning tokens generated, but final answer still refuses (~2% after jailbreak)
- **First 3 experiments**:
  1. Baseline audit: Run curated prompts through target model; classify responses using JSON classifier prompt to establish censorship rate
  2. Jailbreak validation: Apply reasoning prefill attack with 1-4 repetitions; measure bypass rate and categorize residual failures
  3. Distillation probe: Fine-tune small model with 10-30 censored samples injected; evaluate censorship transfer and performance impact on benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an adversary infer the internal reasoning structure of a model when it is not externally visible?
- Basis in paper: [explicit] Authors state this security question has implications for future attacks and "remains an open direction for future work"
- Why unresolved: Jailbreak relies on appending reasoning triggers, assuming model exposes thought process
- What evidence would resolve it: Successful extraction of reasoning traces from models configured to hide chain-of-thought outputs

### Open Question 2
- Question: How do opaque web-based censorship mechanisms differ structurally from censorship embedded in local model weights?
- Basis in paper: [explicit] Authors note web interfaces add black-box censorship layers and state "understanding how these systems are configured... is an important open direction"
- Why unresolved: Study focuses on locally deployed weights; external filtering logic in web chatbots is inaccessible
- What evidence would resolve it: Comparative study measuring discrepancies between API responses and local inference for identical sensitive prompts

### Open Question 3
- Question: Does "censorship spillover" to out-of-distribution samples during distillation generalize across all sensitive topics?
- Basis in paper: [inferred] Trade-off between censorship injection and reasoning performance was only validated on Taiwan topic
- Why unresolved: Trade-off was only validated on a single topic and limited dataset
- What evidence would resolve it: Distillation experiments utilizing the full 96-category taxonomy of sensitive topics

## Limitations
- The study focuses exclusively on DeepSeek R1 and doesn't examine whether similar censorship patterns exist in other local models
- The jailbreaking method requires multiple prompt repetitions, which may not scale efficiently for real-world applications
- The distillation experiments only examined one sensitive topic (Taiwan) and may not generalize across the full taxonomy

## Confidence

| Claim | Confidence |
|-------|------------|
| Local censorship exists in R1 and can be systematically identified | High |
| Jailbreaking method successfully bypasses censorship in 97.86% of cases | High |
| Censorship can transfer to distilled models through fine-tuning | Medium |
| Jailbreak preserves factuality better than Perplexity's R1-1776 | Medium |

## Next Checks
1. Reproduce baseline audit: Run the curated 10,030 prompts through your target model using GPT-4.1 JSON classifier to establish censorship rate by category
2. Validate jailbreak effectiveness: Apply reasoning prefill attack with 1-4 repetitions and measure bypass rate, categorizing residual failures
3. Test distillation transfer: Fine-tune a small model with 10-30 censored samples and evaluate censorship transfer on held-out prompts while measuring performance impact on benchmarks