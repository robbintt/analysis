---
ver: rpa2
title: 'DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation'
arxiv_id: '2512.05112'
source_url: https://arxiv.org/abs/2512.05112
tags:
- image
- arxiv
- draft
- generation
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces DraCo, a novel approach for text-to-image
  generation that leverages interleaved reasoning by incorporating both visual and
  textual chain-of-thought. The method generates a low-resolution draft image as visual
  planning, uses the model's understanding capability to verify potential semantic
  misalignments between the draft and input prompt, and refines the draft through
  selective corrections with super-resolution.
---

# DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation

## Quick Facts
- arXiv ID: 2512.05112
- Source URL: https://arxiv.org/abs/2512.05112
- Reference count: 40
- Primary result: 8% improvement on GenEval, 0.91 point increase on ImagineBench, 3% improvement on GenEval++

## Executive Summary
DraCo introduces a novel text-to-image generation method that uses low-resolution draft images as visual chain-of-thought (CoT) reasoning. Unlike previous text-only planning approaches, DraCo generates a 384×384 draft image first, uses the model's understanding capability to verify potential semantic misalignments between the draft and input prompt, and refines the draft through selective corrections with super-resolution. This approach addresses the limitations of abstract textual planning and the difficulty in generating rare attribute combinations. The method significantly outperforms existing approaches on standard benchmarks.

## Method Summary
DraCo operates in three sequential stages: first generating a low-resolution draft image via Rectified Flow, then using the model's understanding capability to verify potential semantic misalignments between the draft and input prompt, and finally refining the draft through selective corrections with super-resolution. The approach leverages unified multimodal LLMs (Bagel) that handle both understanding (ViT tokens) and generation (VAE tokens) in one model. A specialized DraCo-CFG classifier-free guidance strategy is used to strengthen visual-semantic preservation from draft while enforcing prompt/correction alignment. The method is trained on DraCo-240K, a curated dataset targeting three atomic capabilities: general correction, instance manipulation, and layout reorganization.

## Key Results
- Achieves 8% improvement on GenEval benchmark
- Improves ImagineBench rare-concept generation by 0.91 points
- Shows 3% improvement on GenEval++ evaluation
- Outperforms existing methods including Auto1111, FLUX-dev, and Kandinsky 2.2

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low-resolution visual drafts provide more concrete planning guidance than abstract textual reasoning alone.
- Mechanism: DraCo generates a 384×384 draft image first, which encodes spatial layouts, object attributes, and structural composition into explicit visual tokens. This allows the model to "preview" its planned output before committing to high-resolution generation.
- Core assumption: Textual descriptions underspecify visual details (especially low-level features like appearance and style), and models struggle to "imagine" correct outputs in one forward pass.
- Evidence anchors: [abstract] "Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance." [Section 1] "For generating a dense modality like an image, only planning with text offers too vague and coarse guidance."
- Break condition: If draft resolution is too low (128×128), semantic information is lost; if too high (1024×1024), token length explodes and training efficiency drops. Paper reports scores of 0.76, 0.86, 0.75 respectively.

### Mechanism 2
- Claim: Unified MLLMs can self-verify draft-prompt misalignments when prompted to compare visual content against textual intent.
- Mechanism: After draft generation, the model encodes the draft via ViT (without VAE features to avoid low-level constraints), then generates textual verification that identifies mismatches and proposes corrections.
- Core assumption: The model's understanding and generation capabilities can productively conflict—understanding can detect what generation got wrong.
- Evidence anchors: [abstract] "We employ the model's inherent understanding capability to verify potential semantic misalignments between the draft and input prompt." [Section 2.2] "We propose inputting only the ViT features of the draft image without the VAE features... Dropping the VAE feature could eliminate the constraints from the low-level feature and facilitate more substantial changes."
- Break condition: Including VAE features during verification causes the model to over-adhere to draft details, producing artifacts. Ablation shows 2% score drop (0.84 vs 0.86).

### Mechanism 3
- Claim: Decomposed classifier-free guidance with separate draft and correction condition scales improves final image quality.
- Mechanism: DraCo-CFG uses three guidance forms—unconditional m(ϕ, ϕ, ϕ), draft-only m(ϕ, vit, ϕ), and full-condition m(p, vit, v)—with separate scales (s_draft, s_text).
- Core assumption: Standard CFG formulations couple multiple conditions incorrectly, over-emphasizing some signals.
- Evidence anchors: [Section 2.2] Formula showing decomposed guidance with separate draft and text condition terms. [Section 3.3] "DraCo-CFG surpasses the original CFG by 3% on the overall score of GenEval."
- Break condition: Original Bagel CFG uses sequential computation that couples scales non-linearly, causing blurry outputs and condition over-emphasis.

## Foundational Learning

- Concept: Unified Multimodal LLMs (e.g., Bagel, Transfusion)
  - Why needed here: DraCo builds on architectures that handle both understanding (ViT tokens) and generation (VAE tokens) in one model. Understanding the MoT (Mixture-of-Transformers) design is prerequisite.
  - Quick check question: Can you explain why Bagel uses separate transformer branches for ViT and VAE tokens?

- Concept: Rectified Flow for Image Generation
  - Why needed here: Bagel generates images via Rectified Flow, not standard diffusion. Training loss uses MSE on flow prediction (Eq. 3).
  - Quick check question: How does Rectified Flow differ from DDPM denoising?

- Concept: Classifier-Free Guidance (CFG)
  - Why needed here: DraCo-CFG is a modified CFG strategy. You must understand standard CFG before appreciating why decomposition helps.
  - Quick check question: What is the standard CFG formula for conditional vs unconditional generation?

## Architecture Onboarding

- Component map: Input text prompt -> Draft Sketching (384×384 generation) -> Verification (ViT encoding + textual comparison) -> Corrective Refinement (1024×1024 generation with DraCo-CFG)

- Critical path: Draft resolution choice (384×384 optimal) -> ViT-only verification encoding (dropping VAE prevents artifacts) -> DraCo-CFG scale tuning (s_draft, s_text in [2, 6])

- Design tradeoffs:
  - Draft resolution: 384×384 balances semantic expressiveness vs token efficiency
  - ViT-only vs ViT+VAE for verification: Dropping VAE enables larger corrections but loses low-level detail constraints
  - Sequential vs parallel CFG: DraCo-CFG uses single-step decomposition to avoid scale coupling

- Failure signatures:
  - Draft too small (128px): Model cannot identify objects for verification (score: 0.76)
  - Including VAE in verification: Artifacts from draft over-adherence (unnatural lighting, disconnected structures)
  - Using original Bagel CFG: Blurry outputs, 3% score degradation

- First 3 experiments:
  1. Reproduce draft resolution ablation (128 vs 384 vs 1024) on a subset of GenEval to validate the 384×384 choice on your compute budget.
  2. Compare ViT-only vs ViT+VAE verification on 20 manually inspected examples to confirm artifact patterns match paper description.
  3. Implement DraCo-CFG formula and run head-to-head against original CFG on ImagineBench rare-concept prompts to verify +0.91 improvement signal.

## Open Questions the Paper Calls Out

- Question: Can the draft-verify-correct paradigm be effectively adapted for video or 3D generation where temporal consistency or geometry are the primary constraints?
  - Basis in paper: [explicit] Appendix E states the current draft design "cannot be directly or optimally used" for video or 3D assets and requires capturing modal-specific difficulties like consistency.
  - Why unresolved: The current DraCo implementation focuses on 2D image semantics; video requires handling temporal coherence across frames, while 3D requires geometric consistency, which a 2D low-resolution draft does not inherently capture.
  - What evidence would resolve it: A successful extension of DraCo to video generation demonstrating that a draft (e.g., low frame rate or skeleton) improves temporal consistency metrics compared to direct generation.

- Question: How would incorporating human-in-the-loop feedback during data curation or the training cycle impact the model's correction accuracy and alignment?
  - Basis in paper: [explicit] Appendix E notes that "the role of humans... is not well studied" in the current pipeline but suggests it could help better align generation and correction methods.
  - Why unresolved: The current DraCo-240K dataset relies entirely on automated pipelines using models like Qwen3-VL and FLUX-Kontext, which may propagate synthetic artifacts or limit the diversity of "reasoning" patterns.
  - What evidence would resolve it: A comparative study measuring alignment improvements (e.g., on GenEval++) when human annotators replace or augment the automated verification step in the training data.

- Question: How susceptible is the draft verification step to the model's own "blind spots," where it fails to detect semantic misalignments that it is prone to generating?
  - Basis in paper: [inferred] The method assumes the model can successfully "verify potential semantic misalignments" (Section 1). However, if the model generates a rare concept error (e.g., a blue orange) due to training bias, it may also fail to identify it as an error during verification.
  - Why unresolved: The paper evaluates the end-to-end performance but does not isolate the failure rate of the verification step itself (false negatives where correction was needed but not triggered).
  - What evidence would resolve it: An ablation study analyzing the "missed detection" rate of the verification module on drafts containing known rare concept errors versus its performance on common errors.

## Limitations

- DraCo-240K dataset availability is unclear—the GitHub link exists but full dataset or synthesis pipeline details may not be released yet, blocking exact reproduction.
- Some architectural choices (e.g., ViT-only vs ViT+VAE for verification) lack direct corpus evidence, relying on ablation results instead.
- Guidance scale tuning (s_draft, s_text in [2, 6]) and exact data mixture ratios between correction types are underspecified.

## Confidence

- High: Draft resolution ablation (384×384 optimal), ViT-only verification improving over ViT+VAE, DraCo-CFG improving over original CFG.
- Medium: Effectiveness of interleaved visual-textual reasoning vs text-only CoT; dataset construction pipeline reliability.
- Low: Generalization of rare-concept generation improvements beyond curated benchmarks.

## Next Checks

1. Reproduce draft resolution ablation (128 vs 384 vs 1024) on a subset of GenEval to confirm the 384×384 choice on available compute.
2. Compare ViT-only vs ViT+VAE verification on 20 manually inspected examples to verify artifact patterns match paper description.
3. Implement DraCo-CFG formula and run head-to-head against original CFG on ImagineBench rare-concept prompts to validate +0.91 improvement signal.