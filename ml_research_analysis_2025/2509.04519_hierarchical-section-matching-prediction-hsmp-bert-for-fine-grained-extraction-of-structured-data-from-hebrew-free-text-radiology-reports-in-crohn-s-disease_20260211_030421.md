---
ver: rpa2
title: Hierarchical Section Matching Prediction (HSMP) BERT for Fine-Grained Extraction
  of Structured Data from Hebrew Free-Text Radiology Reports in Crohn's Disease
arxiv_id: '2509.04519'
source_url: https://arxiv.org/abs/2509.04519
tags:
- wall
- ileum
- radiology
- reports
- hsmp-bert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Hierarchical Section Matching Prediction (HSMP) BERT was developed
  for structured extraction of organ-finding labels from Hebrew radiology reports
  in Crohn's disease. The model combines domain-adaptive pretraining on 9,683 radiology
  reports with a Section Matching Prediction (SMP) objective that leverages Findings-Impression
  coherence, followed by hierarchical prompt-based inference.
---

# Hierarchical Section Matching Prediction (HSMP) BERT for Fine-Grained Extraction of Structured Data from Hebrew Free-Text Radiology Reports in Crohn's Disease

## Quick Facts
- arXiv ID: 2509.04519
- Source URL: https://arxiv.org/abs/2509.04519
- Reference count: 40
- HSMP-BERT achieves mean F1 0.83±0.08 and Cohen's κ 0.65±0.17 on structured extraction from Hebrew Crohn's disease radiology reports

## Executive Summary
This work introduces Hierarchical Section Matching Prediction (HSMP) BERT, a framework for structured extraction of organ-finding labels from Hebrew free-text radiology reports in Crohn's disease. The model leverages domain-adaptive pretraining on 9,683 radiology reports and a Section Matching Prediction (SMP) objective that exploits the semantic coherence between Findings and Impression sections. Evaluated on 512 expert-annotated reports, HSMP-BERT significantly outperforms zero-shot SMP and standard fine-tuning baselines. Hierarchical prompt-based inference reduces runtime by 5.1× while maintaining high predictive performance.

## Method Summary
HSMP-BERT extends Hebrew RoBERTa (HeRo) through three pretraining stages: domain-adaptive pretraining via MLM on radiology reports, SMP pretraining to classify Findings-Impression section coherence, and prompt-based tuning using verbalizer templates. Inference follows a hierarchical tree (Scan → Organ → Finding) to efficiently route prompts and skip unnecessary evaluations. The approach is designed for low-resource settings where structured labels are scarce but report structure is consistent.

## Key Results
- HSMP-BERT achieves mean F1 score 0.83±0.08 and Cohen's κ 0.65±0.17
- Outperforms zero-shot SMP (F1 0.49±0.07, κ 0.06±0.07) and standard fine-tuning (F1 0.30±0.27, κ 0.27±0.34; p < 10^-7)
- Hierarchical inference reduces runtime by 5.1× versus flat inference
- Applied at scale, identifies clinically meaningful associations (e.g., ileal wall thickening with stenosis and pre-stenotic dilatation)

## Why This Works (Mechanism)

### Mechanism 1: Section Matching Prediction (SMP) for Structural Alignment
- **Claim:** Leveraging the intrinsic logical coherence between "Findings" (observations) and "Impression" (summary) sections creates domain-aware representations that outperform standard fine-tuning in low-resource settings.
- **Mechanism:** The model is pretrained to classify whether a pair of Findings and Impression sections originate from the same report (Match vs. NotMatch). This forces the encoder to learn semantic consistency and medical logic without manual labels.
- **Core assumption:** Radiology reports follow a consistent semi-structured logic where the Impression is a faithful summary of the Findings.
- **Evidence anchors:**
  - [abstract] "...SMP objective that leverages Findings-Impression coherence..."
  - [section 2.2.1] "...enables HSMP-BERT to internalize the logical structure of radiology reports, a critical prerequisite for label-efficient extraction..."
  - [corpus] Neighbor paper *Comparative Analysis of Abstractive Summarization...* validates the strong semantic link between Findings and Impression sections, supporting the viability of this signal.
- **Break condition:** Performance degrades if reports are unstructured (no distinct sections) or if the Impression sections are copy-pasted boilerplate unrelated to specific Findings.

### Mechanism 2: Prompt-Based Verbalization for Zero-Shot Generalization
- **Claim:** Mapping extraction labels to natural language prompts (verbalizers) allows the model to reuse the SMP-learned knowledge directly, minimizing the gap between pretraining and downstream tasks.
- **Mechanism:** Instead of training a random classifier head, the system uses templates (e.g., "There is {finding} in the {organ}"). The model predicts the compatibility of this sentence with the report content. This treats extraction as a text-matching problem similar to the SMP pretraining task.
- **Core assumption:** The pretrained model has encoded sufficient semantic knowledge to associate natural language descriptions with specific text spans in the report.
- **Evidence anchors:**
  - [abstract] "...prompt-based model... outperforming zero-shot SMP... and standard fine-tuning..."
  - [section 2.2.2] "...prompts were designed to mirror clinical phrasing and support inference without additional layers."
  - [corpus] *Structuring Radiology Reports: Challenging LLMs with Lightweight Models* supports the efficacy of lightweight models for structuring but highlights LLM complexity; this mechanism bridges that gap using prompts on a lighter BERT model.
- **Break condition:** Fails if the prompt phrasing is ambiguous or conflicts with the linguistic patterns seen during pretraining (e.g., using lay terms for medical findings).

### Mechanism 3: Hierarchical Inference for Conditional Routing
- **Claim:** Structuring inference as a decision tree (Scan → Organ → Finding) significantly reduces computational cost while maintaining predictive validity.
- **Mechanism:** The model queries general states first (e.g., "Is the scan normal?"). If the answer is negative, it drills down to specific organs. This prunes the search space by skipping entire subtrees of prompts for normal regions.
- **Core assumption:** Pathological findings are hierarchically conditional; a finding cannot be present in an organ if the scan (or organ) is deemed normal at a higher level.
- **Evidence anchors:**
  - [abstract] "...Hierarchical inference reduced runtime by 5.1× versus flat inference."
  - [section 2.2.3] "...if a higher-level prompt yields a negative prediction, lower-level prompts for that subtree are skipped."
  - [corpus] *Radiology Workflow-Guided Hierarchical Reinforcement Fine-Tuning* supports the general validity of mimicking radiology workflows hierarchically.
- **Break condition:** Error propagation; if a high-level query (e.g., "Is organ X abnormal?") yields a false negative, all specific findings for that organ are missed, potentially harming recall on rare subtleties.

## Foundational Learning

- **Concept: Domain-Adaptive Pretraining (DAPT)**
  - **Why needed here:** General Hebrew models (like HeRo) struggle with medical jargon. DAPT on 9,683 in-domain reports is required to align the vocabulary embedding space before task-specific tuning.
  - **Quick check question:** Does the model tokenize "stenosis" as a single medical concept or fragmented sub-words before the SMP phase?

- **Concept: Verbalizer Functions**
  - **Why needed here:** Standard BERT outputs a class index. A verbalizer is needed to map the model's output space (tokens/logits) to the specific clinical labels (e.g., mapping "positive" → "wall thickening present").
  - **Quick check question:** How does the system handle a label that has no obvious natural language phrasing in the training distribution?

- **Concept: Multi-Label Stratification**
  - **Why needed here:** The dataset has high class imbalance (some findings <5%).
  - **Quick check question:** Did the split ensure that rare classes (e.g., "Sigmoid Comb sign") appear in both the validation and test sets with similar prevalence?

## Architecture Onboarding

- **Component map:** HeRo -> MLM head (Domain pretraining) -> SMP Head (Binary Classifier) -> Verbalizer Templates (Prompt construction) -> Hierarchical Router (Decision logic)
- **Critical path:** The SMP Pretraining phase is the most sensitive step. If the Findings/Impression pairs are not correctly parsed or shuffled (for negative samples), the model fails to learn the structural coherence required for the prompt-based approach.
- **Design tradeoffs:**
  - **Speed vs. Recall:** The hierarchical inference offers 5.1× speedup but risks "early exit" errors where a subtle finding is missed because the parent node (Organ-level) was predicted as normal.
  - **Label granularity:** Collapsing labels 2 (not visible) and 9 (resected) into "negative" simplifies the task but may lose clinically relevant nuance regarding patient history.
- **Failure signatures:**
  - **Low Kappa on rare classes:** The paper reports lower Kappa (0.27) on rare findings like "Sigmoid Comb sign," suggesting the hierarchical routing or data scarcity limits fine-grained learning.
  - **Zero-shot collapse:** If SMP zero-shot performance is near random (F1 ~0.49), it indicates the prompt templates are not aligned with the model's latent space.
- **First 3 experiments:**
  1. **Sanity Check (Flat vs. Hierarchical):** Run inference on a subset using flat prompts vs. the hierarchical tree. Measure the divergence in F1 score for rare findings to quantify the "early exit" error rate.
  2. **Ablation on SMP:** Train a model without the SMP objective (only standard fine-tuning) and compare Kappa scores to isolate the contribution of the section-matching logic.
  3. **Prompt Sensitivity:** Test variations of the prompt template (e.g., "Patient has {finding}" vs. "There is {finding}") to see if the model is brittle to phrasing.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the hierarchical inference tree be learned directly from data rather than manually designed?
- Basis in paper: [explicit] The authors state in Section 4.1, "Automating tree construction or learning task hierarchies from data may improve both scalability and performance."
- Why unresolved: The current study relies on a fixed, manually crafted three-level decision tree (Scan → Organ → Finding) to route prompts.
- What evidence would resolve it: A comparison of performance and inference speed between the manual hierarchy and an automated hierarchy derived via reinforcement learning or gradient-based search on the same dataset.

### Open Question 2
- Question: Does the Section Matching Prediction (SMP) objective function effectively generalize to radiology reports lacking distinct "Findings" and "Impression" sections?
- Basis in paper: [explicit] Section 4.1 identifies the reliance on semi-structured formatting as a limitation, noting, "Its performance may degrade on reports that deviate from this convention," and suggests extending the framework to "free-text narratives."
- Why unresolved: The SMP pretraining explicitly trains on the coherence between the separated "Findings" and "Impression" sections, which may not exist in all clinical documents.
- What evidence would resolve it: Benchmarking HSMP-BERT performance on a corpus of unstructured narrative reports that lack explicit section headers.

### Open Question 3
- Question: Is the HSMP-BERT framework transferable to other low-resource languages and clinical domains outside of Hebrew Crohn's disease radiology?
- Basis in paper: [explicit] The conclusion and limitations section (4.1) states that "further validation is required in other low-resource languages and clinical domains."
- Why unresolved: The model was exclusively validated on Hebrew MRE/CTE reports for Crohn's disease, utilizing specific anatomical prompts.
- What evidence would resolve it: Zero-shot or few-shot evaluation results when applying the model to different languages (e.g., Arabic) or different specialties (e.g., musculoskeletal radiology) without retraining the core architecture.

## Limitations

- The epi-IIRN dataset is private, making direct replication challenging without equivalent Hebrew radiology corpora
- Performance metrics may be inflated by filtering labels to those with >15 positive instances, excluding more challenging rare findings
- Hierarchical inference introduces error propagation where false negatives at higher nodes cascade to miss all specific findings in that subtree

## Confidence

- **High Confidence:** The hierarchical inference mechanism and its computational benefits are well-supported by the experimental design. The claim that SMP pretraining leverages Findings-Impression coherence is credible given the reported performance improvements over standard fine-tuning.
- **Medium Confidence:** The comparative performance gains against zero-shot SMP and standard fine-tuning are valid within the reported experimental conditions, but the exact contribution of each component (DAPT, SMP, prompting) is not fully isolated through ablation studies.
- **Low Confidence:** The clinical interpretability of the extracted associations (e.g., "ileal wall thickening with stenosis and pre-stenotic dilatation") and their statistical significance for population-level analysis cannot be independently verified without access to the underlying data.

## Next Checks

1. **Ablation Study:** Train and evaluate models with: (a) DAPT only, (b) DAPT + SMP pretraining, (c) DAPT + SMP + prompting. Compare F1 and κ scores to quantify the isolated contribution of each component.
2. **Hierarchical vs. Flat Inference:** Run the model on a subset using flat prompt evaluation for all 24 labels and compare the F1 scores for rare findings (<10 positive instances) to assess the recall loss from early-exit errors.
3. **Prompt Sensitivity Analysis:** Systematically vary the verbalizer templates (e.g., "Patient has {finding}" vs. "There is {finding}") and measure the variance in F1 score to evaluate the model's robustness to phrasing.