---
ver: rpa2
title: Enhancing Text-Based Hierarchical Multilabel Classification for Mobile Applications
  via Contrastive Learning
arxiv_id: '2507.04413'
source_url: https://arxiv.org/abs/2507.04413
tags:
- label
- labels
- level
- negative
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of hierarchical multilabel classification
  of mobile applications using only their textual descriptions. The proposed solution,
  HMCN (Hierarchical Multilabel Classification Network) combined with HMCL (Hierarchical
  Multilabel Contrastive Learning), pretrains the text encoder to produce more discriminative
  embeddings.
---

# Enhancing Text-Based Hierarchical Multilabel Classification for Mobile Applications via Contrastive Learning

## Quick Facts
- **arXiv ID:** 2507.04413
- **Source URL:** https://arxiv.org/abs/2507.04413
- **Reference count:** 40
- **Primary result:** Proposed HMCN + HMCL achieves 80.75% micro-F1 and 48.62% macro-F1 on Tencent App Store dataset, with 10.70% KS improvement in credit risk management.

## Executive Summary
This paper addresses the challenge of classifying mobile applications into hierarchical categories using only their textual descriptions. The authors propose a novel framework combining a Hierarchical Multilabel Classification Network (HMCN) with Hierarchical Multilabel Contrastive Learning (HMCL). HMCN integrates global and local classification strategies to capture both hierarchical dependencies and label co-occurrence patterns, while HMCL pre-trains the text encoder to produce more discriminative embeddings. The approach is evaluated on a private Tencent App Store dataset and two public datasets, demonstrating significant improvements over state-of-the-art methods, particularly for datasets with multiple text fields.

## Method Summary
The method employs a two-stage approach: first, HMCL pre-trains a RoFormer encoder using hierarchical label-aware contrastive learning with a "Level" negative sampling strategy to create semantically structured embeddings. Then, HMCN performs hierarchical multilabel classification using both global (flat label set) and local (level-by-level with cross-attention) perspectives, integrating their predictions via an MLP. A path regularization term ensures hierarchical consistency. The model processes multi-field app text (name, description, comments) through separate encoding and self-attention pooling to produce a unified embedding.

## Key Results
- Achieved 80.75% micro-F1 and 48.62% macro-F1 on Tencent App Store dataset
- Demonstrated 10.70% improvement in Kolmogorov-Smirnov metric for credit risk management
- Showed 8-10% macro-F1 improvement over state-of-the-art methods on RCV1 dataset
- HMCL pre-training improved model performance by creating more separable app embeddings

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical Multilabel Contrastive Learning (HMCL) improves app classification by creating more semantically structured and separable app embeddings. The HMCL scheme pre-trains the text encoder using a level-wise sampling strategy, constructing positive pairs from apps sharing labels at the same hierarchy level and negative pairs from apps with different labels at that level. This process explicitly optimizes the embedding space to group similar apps and separate dissimilar ones.

### Mechanism 2
The HMCN's dual-perspective architecture captures both hierarchical dependencies and global label co-occurrence patterns. HMCN operates two concurrent classification flows: the local manner processes the hierarchy level-by-level using cross-attention to pass information from parent to children, while the global manner treats all labels as a flat set. The final prediction is a learned ensemble of both, with path regularization preventing hierarchy violations.

### Mechanism 3
Field-specific encoding with self-attention pooling preserves information from multi-field text better than simple concatenation. Instead of concatenating all app text fields, the model encodes each field separately with special tokens and uses multi-head self-attention on the resulting embeddings to produce a single weighted root embedding, allowing dynamic focus on the most informative fields.

## Foundational Learning

### Concept: Contrastive Learning & Representation Quality
- **Why needed:** This is the key innovation (HMCL) for improving the model's input features. Understanding alignment and uniformity is crucial for diagnosing performance.
- **Quick check question:** How does the "Level" negative sampling strategy differ from the "Sibling" strategy, and why does it empirically perform better?

### Concept: Hierarchical Multi-label Classification (HMC)
- **Why needed:** This is the core problem formulation. Understanding the difference between flat, local, and global HMC approaches is essential to grasp HMCN's design.
- **Quick check question:** What is "path violation" in hierarchical classification, and which component of the loss function is designed to prevent it?

### Concept: Transformers & Attention Mechanisms
- **Why needed:** The entire architecture is built on Transformers (RoFormer encoder) and various attention mechanisms (self-attention for pooling, cross-attention for hierarchical information transfer).
- **Quick check question:** In the HMCN's local manner, how is multi-head cross-attention used to pass information from one hierarchy level to the next?

## Architecture Onboarding

### Component map:
- **Data Encoder:** RoFormer model taking tokenized text fields (Name, Description, Comments) to produce initial embeddings
- **Pooling Layer:** Multi-head self-attention module aggregating three field embeddings into single root embedding (h0)
- **Contrastive Pre-trainer (HMCL):** Level-wise sampler and contrastive loss module pre-training the Data Encoder
- **HMCN Classifier:**
  - **Local Branch:** Series of cross-attention layers and MLPs, one per hierarchy level, generating level-specific predictions
  - **Global Branch:** Single MLP taking h0 to generate predictions for all labels simultaneously
  - **Integrator:** Final MLP fusing local and global prediction logits

### Critical path:
High-quality labeled app data → HMCL pre-training of the encoder → End-to-end training of the full HMCN (encoder + classifier). The HMCL step is the primary performance driver.

### Design tradeoffs:
- **HMCL Sampling:** Evaluates three negative sampling strategies; "Level" chosen for best performance, trading computational simplicity for better embedding structure
- **Global vs. Local:** Dual-branch design increases model size and complexity but shown necessary for best results, avoiding underfitting of pure global models and error propagation of pure local models

### Failure signatures:
- **Low KS in downstream task:** Model not learning features useful for user profiling
- **High path violation rate:** Model predicts child labels without parents, indicating path regularization term too weak or local branch failing
- **Poor embedding separation:** t-SNE plots show no clustering by category, indicating HMCL pre-training failed

### First 3 experiments:
1. **Baseline Check:** Train HMCN without HMCL pre-training step; compare macro-F1 scores on validation set to quantify HMCL contribution
2. **Ablation Study:** Ablate global branch, then local branch, from final model; measure drop in Micro-F1 and Macro-F1 on test set to understand each branch's contribution
3. **Hyperparameter Sensitivity:** Test effect of path regularization coefficient (λ); start with λ=0 (no constraint) and increase to find point where validation loss is minimized without causing model to underfit deeper labels

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent does the proposed method improve performance in downstream business applications other than credit risk management?
- **Basis in paper:** The Conclusion states: "For future work, additional downstream business applications will be explored to leverage the hierarchical multi-labels of apps."
- **Why unresolved:** The deployment section only quantifies impact on the Kolmogorov-Smirnov (KS) metric for the specific task of credit risk management (10.70% improvement).
- **What evidence would resolve it:** Empirical results from online A/B testing the classification labels in alternative tasks, such as personalized app recommendations or advertisement targeting campaigns.

### Open Question 2
- **Question:** Can the HMCN architecture be modified to achieve competitive performance on datasets containing only a single text field?
- **Basis in paper:** In Section 5.4, the authors note the HMCN performed worse on the single-field WOS dataset, hypothesizing it is because the model is "designed to cope with data containing multiple fields" and cross-attention might be less effective in that context.
- **Why unresolved:** The paper did not propose or test a variant of HMCN tailored for single-field inputs, leaving a performance gap compared to state-of-the-art methods like HILL on datasets like WOS.
- **What evidence would resolve it:** An ablation study modifying the cross-attention mechanism for single-field inputs or testing the model on a multi-field dataset with fields selectively masked to simulate single-field constraints.

### Open Question 3
- **Question:** Does a hierarchy-aware negative sampling strategy that explicitly accounts for class imbalance outperform the static "Level" strategy?
- **Basis in paper:** Section 5.2 notes that the "Sibling" strategy underperformed on the imbalanced RCV1 dataset regarding macro-F1, while "All" introduced bias towards leaf nodes.
- **Why unresolved:** The implemented strategies (All, Level, Sibling) are static; "Sibling" failed specifically on minority labels, indicating the current "Level" strategy might not be optimal for all hierarchy types.
- **What evidence would resolve it:** Experimentation with a dynamic sampling weight based on label frequency or node depth on the RCV1 dataset to observe if macro-F1 improves beyond the "Level" baseline.

## Limitations
- The core HMCL mechanism's contribution is difficult to fully assess without access to the private Tencent dataset
- The paper does not specify exact dimensions or architectural details of the MLPs used in local and global branches
- The long pre-training time for HMCL (52 hours) is a practical limitation with no exploration of potential reductions

## Confidence
- **High Confidence:** Overall problem formulation (HMC for apps) and general HMCN architecture (dual-branch, path regularization) are well-specified and logically sound
- **Medium Confidence:** Specific implementation of HMCL pre-training, particularly "Level" sampling strategy and its integration with the encoder, is well-described but isolated impact is harder to verify without private data
- **Medium Confidence:** Reported quantitative results are impressive, but inability to access primary dataset or comparable benchmark means full validation is not possible from paper alone

## Next Checks
1. **Ablation of HMCL:** Reproduce HMCN model without HMCL pre-training step on public RCV1 dataset; measure and compare Macro-F1 score to reported results to isolate contribution of contrastive learning phase
2. **Evaluation of Sampling Strategies:** Implement and test all three negative sampling strategies described for HMCL ("Level," "Parent," "Sibling") on a public dataset; compare their performance to confirm "Level" strategy is optimal choice
3. **Path Violation Analysis:** On validation set, calculate rate of path violations (child labels predicted without their parent) for final model; directly test if path regularization term is effectively enforcing hierarchical constraints