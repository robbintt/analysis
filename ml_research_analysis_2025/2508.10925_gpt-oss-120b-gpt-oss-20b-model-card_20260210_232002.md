---
ver: rpa2
title: gpt-oss-120b & gpt-oss-20b Model Card
arxiv_id: '2508.10925'
source_url: https://arxiv.org/abs/2508.10925
tags:
- openai
- gpt-oss-120b
- reasoning
- tool
- evaluations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The gpt-oss-120b and gpt-oss-20b are open-weight reasoning models
  released under Apache 2.0, built using an efficient mixture-of-experts transformer
  architecture. They are trained with large-scale distillation and reinforcement learning,
  optimized for agentic capabilities such as deep research browsing, Python tool use,
  and custom function calling, all using a structured "harmony chat format" that supports
  clear instruction following.
---

# gpt-oss-120b & gpt-oss-20b Model Card

## Quick Facts
- arXiv ID: 2508.10925
- Source URL: https://arxiv.org/abs/2508.10925
- Reference count: 35
- Performance: Strong on AIME, GPQA, SWE-bench; approaches o4-mini on key benchmarks

## Executive Summary
The gpt-oss-120b and gpt-oss-20b are open-weight reasoning models released under Apache 2.0, built using an efficient mixture-of-experts transformer architecture. They are trained with large-scale distillation and reinforcement learning, optimized for agentic capabilities such as deep research browsing, Python tool use, and custom function calling, all using a structured "harmony chat format" that supports clear instruction following. Both models perform strongly on benchmarks spanning mathematics, coding, and safety, with gpt-oss-120b approaching or exceeding the performance of leading closed models like OpenAI o4-mini on tasks such as AIME, GPQA, and HealthBench. Safety testing and adversarial fine-tuning showed that even enhanced versions did not meet high capability thresholds in biological, chemical, or cyber risk categories, and the models include layered safeguards to mitigate misuse. The models are customizable, support full chain-of-thought reasoning, and are accompanied by open-source inference tools and documentation to enable broad use and further research.

## Method Summary
The models use a mixture-of-experts transformer architecture with 36 layers and 128 experts for gpt-oss-120b (5.1B active parameters) and 24 layers with 32 experts for gpt-oss-20b (3.6B active parameters). Training involved pretraining on trillions of STEM-focused tokens with CBRN safety filtering, followed by post-training with distillation and chain-of-thought reinforcement learning. The harmony chat format enables structured agentic tool use through role-based hierarchies and visibility channels. Variable-effort reasoning is supported via system prompt keywords that control chain-of-thought length and depth. Safety measures include adversarial fine-tuning and layered safeguards, though open-weight deployment introduces additional uncertainty.

## Key Results
- Strong performance on AIME (98%), GPQA Diamond (28%), and SWE-bench Verified (44%)
- Approaches or exceeds OpenAI o4-mini on MMLU, HLE, and HealthBench benchmarks
- Safety evaluations show models fail to reach high capability thresholds even after adversarial enhancement
- Supports variable-effort reasoning with log-linear accuracy scaling from 2k to 16k chain-of-thought tokens

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Mixture-of-Experts architecture enables frontier-level reasoning performance at reduced inference cost by activating only a sparse subset of parameters per token.
- **Mechanism:** Each MoE block contains 128 (120b) or 32 (20b) expert networks. A learned router projects residual activations to expert scores, selecting top-4 experts per token. The gated SwiGLU activation with clamping and residual connections produces weighted expert outputs. This allows 116.8B total parameters but only ~5.1B active per forward pass.
- **Core assumption:** Expert specialization emerges during training such that routing decisions meaningfully partition the computation graph across semantic domains.
- **Evidence anchors:** [abstract] "efficient mixture-of-experts transformer architecture...optimized for agentic capabilities"; [section 2.2] "select the top-4 experts for each token given by the router, and weight the output...by the softmax"; [corpus] Weak direct validation; neighbor papers focus on deployment benchmarks rather than MoE mechanics.
- **Break condition:** If expert utilization becomes highly imbalanced (e.g., few experts handle most tokens), the effective model capacity collapses toward a smaller dense model.

### Mechanism 2
- **Claim:** Variable-effort reasoning training produces controllable test-time compute scaling, where longer chain-of-thought correlates with higher accuracy on complex tasks.
- **Mechanism:** Models are trained with three reasoning levels (low/medium/high) configured via system prompt keywords. The RL post-training teaches the model to allocate more CoT tokens when higher reasoning effort is requested. Figure 3 shows log-linear returns: AIME 2025 accuracy scales from ~65% (2k tokens) to ~98% (16k tokens) at high reasoning.
- **Core assumption:** The model learns a generalizeable "effort allocation" policy rather than memorizing fixed-length responses per reasoning level.
- **Evidence anchors:** [section 2.5.2] "train the models to support three reasoning levels...Increasing the reasoning level will cause the model's average CoT length to increase"; [section 2.6.1] "gpt-oss-20b use over 20k CoT tokens per problem on average for AIME"; [corpus] No direct corpus validation of this specific mechanism yet.
- **Break condition:** If CoT length increases without corresponding quality improvements (e.g., repetitive loops), the mechanism degrades to waste without accuracy gains.

### Mechanism 3
- **Claim:** The Harmony chat format enables agentic tool use through structured role hierarchy and visibility channels that separate internal reasoning from user-facing outputs.
- **Mechanism:** Special tokens delineate message boundaries with keyword arguments for roles (System > Developer > User > Assistant > Tool). "Channels" (analysis, commentary, final) indicate visibility levels. The model learns to route CoT to `analysis`, tool calls to `commentary`, and final answers to `final`. This supports interleaved tool calls within CoT.
- **Core assumption:** Developers correctly implement the format parser and respect channel visibility (e.g., not showing raw CoT to end users without filtering).
- **Evidence anchors:** [section 2.5.1] "role-based information hierarchy...channels to indicate intended visibility...critical to deploy properly"; [section 4.4] "Developers should not directly show chains of thought to users...without further filtering"; [corpus] Neighbor papers (e.g., "Probing GPT-OSS-20B") examine security implications of the Harmony format but don't validate the mechanism itself.
- **Break condition:** If downstream systems ignore channel semantics or expose raw CoT, safety monitoring becomes unreliable and user experience degrades.

## Foundational Learning

- **Concept: Mixture-of-Experts routing**
  - **Why needed here:** Understanding how top-k expert selection works is essential for debugging load balancing, diagnosing expert collapse, and optimizing inference throughput.
  - **Quick check question:** If all tokens route to the same 2 experts, what happens to effective model capacity?

- **Concept: Chain-of-thought reinforcement learning**
  - **Why needed here:** The post-training pipeline uses RL to teach reasoning patterns; understanding this helps interpret why models produce long CoTs and how effort levels influence behavior.
  - **Quick check question:** Why might directly penalizing "bad thoughts" in CoT cause models to hide misbehavior rather than correct it?

- **Concept: Instruction hierarchy**
  - **Why needed here:** Deploying these models safely requires understanding how System > Developer > User precedence prevents prompt injection attacks and guards against override attempts.
  - **Quick check question:** If a user message attempts to extract the system prompt, which evaluation metric measures success of the defense?

## Architecture Onboarding

- **Component map:** Input tokenizer (o200k_harmony BPE, 201K tokens) -> 36/24 transformer layers with alternating banded/dense attention -> Each layer: RMSNorm -> GQA attention (64 query heads, 8 KV heads) -> MoE block (128/32 experts, top-4 routing) -> RoPE + YaRN for 131K context extension -> Output: Harmony-formatted response with channel-tagged spans

- **Critical path:** Router scores -> Top-4 expert selection -> Weighted expert outputs -> Residual merge. This path executes per-token and dominates inference latency.

- **Design tradeoffs:** MXFP4 quantization on MoE weights (90%+ of params) enables single-GPU deployment but may degrade numerical precision on sensitive tasks. Banded attention (128-token window) reduces memory but limits long-range dependency modeling to dense layers. Unconventional SwiGLU (clamping + residual) may improve stability but deviates from standard implementations.

- **Failure signatures:** Expert collapse: Repeated routing to small expert subset; monitor via router entropy. CoT loops: Excessive token count without progress; detect via repetition metrics. Channel leakage: `analysis` content appearing in `final` output; validate parser correctness. Instruction hierarchy bypass: User messages overriding system constraints; check Table 7-8 evals.

- **First 3 experiments:**
  1. **Reasoning level sweep:** Run identical prompts at low/medium/high reasoning; measure CoT length, latency, and accuracy on a held-out task (e.g., GPQA subset). Verify log-linear scaling holds for your domain.
  2. **Tool use integration test:** Configure a Python tool harness; send multi-turn requests requiring interleaved CoT, code execution, and final answers. Confirm channel parsing correctly separates `analysis`, `commentary`, and `final`.
  3. **Instruction hierarchy stress test:** Attempt prompt injection via user messages trying to extract system prompts or override developer constraints. Compare refusal/override rates against Table 7-8 baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why do gpt-oss-120b and gpt-oss-20b underperform OpenAI o4-mini on instruction hierarchy evaluations despite achieving parity on jailbreak robustness (StrongReject)?
- **Basis in paper:** [explicit] "We observed that gpt-oss-120b and gpt-oss-20b generally underperform OpenAI o4-mini on our instruction hierarchy evaluations. More research is needed to understand why this is the case."
- **Why unresolved:** The authors note the gap but do not investigate causes or propose hypotheses beyond observing the disparity.
- **What evidence would resolve it:** Ablation studies isolating training data composition, model scale effects, or architecture-specific factors that differentiate instruction hierarchy learning from jailbreak robustness.

### Open Question 2
- **Question:** Can chain-of-thought monitoring systems reliably detect misbehavior in open-weight reasoning models without direct optimization pressure on CoT?
- **Basis in paper:** [explicit] "We hope that this gives developers the opportunity to implement CoT monitoring systems in their projects and enables the research community to further study CoT monitorability."
- **Why unresolved:** The authors intentionally avoided CoT optimization to preserve monitorability, but empirical validation of monitoring effectiveness remains unstudied.
- **What evidence would resolve it:** Benchmarks measuring detection rates of adversarial behavior in unconstrained CoTs across varied attack strategies.

### Open Question 3
- **Question:** What is the true upper bound of adversarial capability elicitation through improved scaffolding, given current results likely represent lower bounds?
- **Basis in paper:** [explicit] "As always, we note that these evaluation results likely represent lower bounds on model capability, because additional scaffolding or improved capability elicitation could substantially increase observed performance."
- **Why unresolved:** The paper acknowledges uncertainty about maximal elicitation but does not systematically explore the scaffolding design space.
- **What evidence would resolve it:** Systematic sweeps over scaffolding complexity levels on preparedness benchmarks to establish capability ceilings.

## Limitations

- **Safety Generalization:** Open-weight deployment allows adversaries to access model weights, potentially enabling custom jailbreaks or capability amplification beyond evaluated scenarios.
- **Performance Robustness:** Limited evaluation on adversarial robustness, out-of-distribution generalization, and long-horizon agentic tasks; real-world deployment may reveal capability gaps or safety failures.
- **Reproducibility Gaps:** Critical training details (exact data composition, RL reward structures, distillation sources) are unspecified, making faithful reproduction difficult.

## Confidence

- **High Confidence:** Mixture-of-Experts architecture specification, benchmark performance claims, and safety evaluation methodology are well-documented and internally consistent.
- **Medium Confidence:** Claims about variable-effort reasoning scaling and agentic tool use are supported by provided evidence but lack external validation or ablation studies.
- **Low Confidence:** Safety claims regarding CBRN risk mitigation and instruction hierarchy robustness are based on limited adversarial testing; open-weight nature introduces additional uncertainty.

## Next Checks

1. **Instruction Hierarchy Robustness:** Systematically attempt prompt injection attacks to override system constraints, measuring success rates against Table 7-8 baselines. Document any bypass techniques discovered.
2. **Expert Load Balancing Validation:** Monitor router entropy and expert utilization during inference across diverse prompts. Verify that expert selection remains balanced and no collapse occurs toward a small expert subset.
3. **Channel Privacy Enforcement:** Test harmony format parsers by injecting malicious payloads designed to leak `analysis` or `commentary` content into `final` outputs. Confirm channel visibility controls function as specified under adversarial conditions.