---
ver: rpa2
title: 'Slm-mux: Orchestrating small language models for reasoning'
arxiv_id: '2510.05077'
source_url: https://arxiv.org/abs/2510.05077
tags:
- accuracy
- wang
- slm-mux
- zhang
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether small language models (SLMs) can\
  \ be orchestrated into a system more accurate than any single model. Existing orchestration\
  \ methods\u2014such as debate, verification, and agent-based aggregation\u2014are\
  \ designed for frontier models and perform poorly when applied to SLMs, often reducing\
  \ accuracy due to error amplification and groupthink."
---

# Slm-mux: Orchestrating small language models for reasoning

## Quick Facts
- arXiv ID: 2510.05077
- Source URL: https://arxiv.org/abs/2510.05077
- Reference count: 28
- Key outcome: SLM-MUX achieves up to 13.4% improvement on MATH, 8.8% on GPQA, and 7.0% on GSM8K by orchestrating small language models

## Executive Summary
This paper investigates whether small language models (SLMs) can be orchestrated into a system more accurate than any single model. Existing orchestration methods—such as debate, verification, and agent-based aggregation—are designed for frontier models and perform poorly when applied to SLMs, often reducing accuracy due to error amplification and groupthink. To address this, the authors propose SLM-MUX, a three-stage approach: (1) a novel orchestration method that avoids discussion and instead selects outputs based on model confidence scores, (2) a model selection search that identifies complementary model subsets, and (3) compute scaling strategies to further boost performance. SLM-MUX consistently outperforms existing methods, achieving up to 13.4% improvement on MATH, 8.8% on GPQA, and 7.0% on GSM8K. With just two SLMs, SLM-MUX surpasses the much larger Qwen 2.5 72B on GPQA and GSM8K, and matches its performance on MATH. The results demonstrate that orchestrating SLMs can yield highly accurate and efficient systems, offering a promising alternative to scaling monolithic models.

## Method Summary
SLM-MUX orchestrates multiple small language models through three key stages: (1) Independent generation where each model generates multiple samples at temperature >0, (2) Confidence-based selection using intra-model consistency as the confidence score, and (3) Model selection search that finds complementary model subsets using a union accuracy minus contradiction penalty objective. The system generates k samples per model, computes confidence as the frequency of the most common answer, and selects the highest-confidence answer with tie-breaking by validation accuracy. A two-objective search optimizes for coverage while minimizing contradiction across candidate model subsets.

## Key Results
- SLM-MUX achieves 61.8% accuracy on MATH (13.4% improvement over best single model)
- 42.1% accuracy on GPQA (8.8% improvement) and 87.8% on GSM8K (7.0% improvement)
- Outperforms existing orchestration methods including MoA, LLM-Debate, Multi-Agent Verification, and Agent Forest
- With just two SLMs, surpasses Qwen 2.5 72B on GPQA and GSM8K, matches on MATH

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Independent generation with consistency-based selection avoids the error amplification that discussion-based methods cause in SLMs.
- Mechanism: Each SLM generates k samples independently at temperature >0. The most frequent answer from each model becomes its candidate, and its frequency (0–1) serves as a confidence score. The system selects the answer with highest confidence, breaking ties via validation-set accuracy.
- Core assumption: Intra-model consistency correlates with correctness more reliably than inter-model persuasion for SLMs.
- Evidence anchors:
  - [abstract]: "SLM-MUX avoids this by having each model independently generate multiple outputs, then selecting the most consistent answer across samples."
  - [Section 3.1 + Algorithm 1]: "We evaluate the confidence of each SLM's outputs by measuring their consistency across their own outputs... if SLM A produces three identical answers while model B produces three different ones, the answer from model A is selected."
  - [corpus]: Weak direct corpus support; related work (EASE, RoseRAG) targets SLM alignment/robustness but not orchestration selection protocols.
- Break condition: If all models are confidently wrong (high consistency but incorrect), the method selects an incorrect answer; no external ground-truth signal exists to override.

### Mechanism 2
- Claim: Model selection search improves composed accuracy by maximizing complementarity rather than standalone strength.
- Mechanism: A two-objective search over candidate subsets: (1) Union Accuracy—fraction of questions any model in S answers correctly; (2) Contradiction Penalty—fraction where one model is consistently wrong while another is correct. The objective O(S) = Acc(S) − λ·Contradiction(S) balances coverage vs. conflicting confidence.
- Core assumption: Complementary capability (e.g., algebra vs. geometry strength) matters more than simply including the highest-accuracy single model.
- Evidence anchors:
  - [Section 3.2]: "Qwen2.5-7B consistently outperforms Llama3.2-3B across all subjects, so combining them offers no capability beyond what Qwen2.5-7B already provides. In contrast, Mistral Small 24B and Qwen2.5-7B show complementary strengths."
  - [Figure 7]: Shows union accuracy and contradiction penalty both increase as more models are added.
  - [corpus]: No direct corpus evidence for this specific search objective; related SLM work focuses on distillation or alignment, not orchestration search.
- Break condition: If validation set is unrepresentative of test distribution, searched combinations may not transfer.

### Mechanism 3
- Claim: Drawing more samples per model improves confidence estimation; adding more model types shows diminishing returns dependent on contradiction patterns.
- Mechanism: More samples → tighter confidence estimates via frequency; more models → higher union accuracy but also higher contradiction penalty. The optimal K varies by benchmark (GPQA peaks at K=2, MATH keeps improving to K=5).
- Core assumption: Confidence estimates stabilize with more samples; the marginal gain from additional models depends on their error correlation.
- Evidence anchors:
  - [Section 4.4 + Figure 8]: Accuracy improves monotonically with samples per model (2→9) across benchmarks.
  - [Section 4.4 + Figure 9]: "On GPQA, accuracy peaks when combining two models and declines thereafter. On GSM8K, accuracy quickly saturates at two models. On MATH, accuracy continues to improve."
  - [corpus]: "Efficient Long CoT Reasoning in Small Language Models" discusses test-time compute scaling but not multi-model orchestration.
- Break condition: If models have highly correlated errors, adding more provides no diversity benefit; if latency budget is tight, sample scaling may be infeasible.

## Foundational Learning

- Concept: Self-consistency for LLMs
  - Why needed here: SLM-MUX builds on self-consistency but extends it to multi-model selection; understanding the single-model case clarifies why p>0.5 is required for self-consistency to help (Section 5).
  - Quick check question: If a model has p=0.4 accuracy on a question type, does self-consistency with N samples improve or harm accuracy?

- Concept: Mixture-of-Agents / Discussion-based Orchestration
  - Why needed here: The paper positions SLM-MUX as an alternative that avoids the groupthink failure mode of discussion-based methods; understanding MoA clarifies what SLM-MUX replaces.
  - Quick check question: Why does peer critique work for frontier LLMs but cause groupthink for SLMs?

- Concept: Ensemble Diversity and Error Correlation
  - Why needed here: The model selection search implicitly optimizes for low error correlation; grasping this explains why "more models" doesn't always help.
  - Quick check question: If two SLMs make mistakes on the same 80% of questions, will combining them provide significant gains?

## Architecture Onboarding

- Component map: Query → parallel generation across K models (k samples each) → per-model confidence scoring → selection → final answer
- Critical path: Query → parallel generation across K models (k samples each) → per-model confidence scoring → selection → final answer. Latency dominated by slowest model's k generations.
- Design tradeoffs:
  - Higher k improves confidence estimation but increases latency linearly
  - Higher K increases union accuracy but raises contradiction penalty and compute
  - Temperature >0 required for diversity; too high reduces consistency signal quality
  - λ in search objective trades coverage vs. conflict; default λ=1 used in experiments
- Failure signatures:
  - All models confidently wrong: high si for incorrect answers, no recovery path
  - High contradiction rate: multiple models with high confidence on different answers; tie-break unreliable
  - Validation–test drift: selected model combination underperforms on test distribution
  - Correlated errors: union accuracy plateaus despite adding more models
- First 3 experiments:
  1. Replicate the single-baseline: Run each SLM (Llama-3.1-8B, Gemma-2-27B, Mixtral-8×7B) with k=3, temperature=0.3 on MATH/GPQA/GSM8K; verify reported single-model accuracies (Table 6)
  2. Implement SLM-MUX with K=3, k=3: Confirm 61.8% on MATH, 42.1% on GPQA, 87.8% on GSM8K (Table 1); log confidence distributions per model
  3. Run model selection search on validation set: With 5 SLM pool, compute UnionAcc(S) and Contradiction(S) for all K∈{2,3,4,5}; verify the selected pair (e.g., Mistral-Small-24B + Qwen2.5-7B for MATH) matches reported gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can confidence estimation in SLM-MUX be improved to handle models that are consistently incorrect?
- Basis in paper: [explicit] The authors identify the limitation that "a model can be very consistent while still being incorrect" because the framework relies solely on self-consistency for confidence scoring.
- Why unresolved: SLM-MUX currently lacks a mechanism to distinguish between a model that is confidently correct and one that is confidently hallucinating.
- What evidence would resolve it: An ablation study integrating alternative signals (e.g., token log-probabilities or learned verifiers) into the confidence score to filter out consistently wrong outputs.

### Open Question 2
- Question: Can a dynamic, query-aware router outperform the static model selection strategy used in SLM-MUX?
- Basis in paper: [inferred] The paper notes that the framework's "design is static and does not adapt to specific questions," relying on fixed groups and validation accuracy for all queries.
- Why unresolved: It is unclear if routing specific queries to specific "expert" SLMs dynamically would yield higher accuracy than the current method of pre-selecting a static group.
- What evidence would resolve it: A comparison between the static exhaustive search and a dynamic router that selects model subsets based on the input query's characteristics.

### Open Question 3
- Question: Is the exhaustive search algorithm feasible for orchestrating large pools of 50+ candidate models?
- Basis in paper: [inferred] The authors note that the exhaustive search method is "slow and costly when there are many models to choose from."
- Why unresolved: The paper only validates the search on a small pool of 5 models; scalability to the growing universe of SLMs is untested.
- What evidence would resolve it: Benchmarking the performance and computational cost of approximate search algorithms (e.g., greedy or evolutionary) against the exhaustive baseline as the model pool size increases.

## Limitations

- The method relies on the assumption that intra-model consistency correlates with correctness, which may not hold for all model types or domains
- The orchestration approach lacks a mechanism to handle cases where all models are confidently wrong
- The model selection search is computationally expensive and may not scale well to larger model pools

## Confidence

- High Confidence: Experimental results showing SLM-MUX outperforming existing orchestration methods on MATH, GPQA, and GSM8K benchmarks
- Medium Confidence: The claim that SLM-MUX avoids error amplification inherent in discussion-based methods for SLMs
- Medium Confidence: The assertion that model selection search meaningfully improves performance by finding complementary subsets

## Next Checks

1. **Ablation Study on Confidence Estimation**: Run SLM-MUX with varying k (samples per model) from 1 to 10 on a held-out validation set to quantify the relationship between sample count and confidence estimation quality. Measure whether confidence scores correlate with actual accuracy across different model types.

2. **Error Correlation Analysis**: Compute pairwise error correlation matrices for the SLM pool on the validation set. Verify whether the model selection search actually identifies low-correlation pairs (e.g., by comparing selected pairs against random pairs with similar individual accuracies). This would validate the core assumption about complementarity.

3. **Robustness to Misalignment**: Construct test cases where all models are confidently wrong (e.g., by modifying questions to have clearly incorrect "popular" answers that models might consistently generate). Measure whether SLM-MUX's confidence-based selection amplifies these errors compared to simple majority voting or random selection.