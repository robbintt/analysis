---
ver: rpa2
title: 'Leave It to the Experts: Detecting Knowledge Distillation via MoE Expert Signatures'
arxiv_id: '2510.16968'
source_url: https://arxiv.org/abs/2510.16968
tags:
- distillation
- expert
- knowledge
- arxiv
- routing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting whether a large language
  model (LLM) has been distilled from another, which is important for intellectual
  property protection and maintaining model diversity. The key innovation is exploiting
  the transfer of structural habits in MoE models, specifically internal routing patterns,
  as signatures of distillation.
---

# Leave It to the Experts: Detecting Knowledge Distillation via MoE Expert Signatures

## Quick Facts
- arXiv ID: 2510.16968
- Source URL: https://arxiv.org/abs/2510.16968
- Authors: Pingzhi Li; Morris Yu-Chao Huang; Zhen Tan; Qingquan Song; Jie Peng; Kai Zou; Yu Cheng; Kaidi Xu; Tianlong Chen
- Reference count: 23
- Primary result: >94% detection accuracy for identifying whether an LLM was distilled from another model via MoE expert routing signatures

## Executive Summary
This paper addresses the challenge of detecting knowledge distillation between large language models by exploiting an overlooked signal: the transfer of MoE "structural habits," particularly internal routing patterns. The key insight is that knowledge distillation transfers not just input-output mappings but also the internal computational patterns learned by the teacher model. The method, called Shadow-MoE, analyzes expert specialization (task-specific activation profiles) and expert collaboration (co-activation patterns between experts) to create distinctive fingerprints that persist through the distillation process.

## Method Summary
The method constructs proxy MoE representations of models (either directly for white-box MoE models or through auxiliary distillation for black-box models) and analyzes two routing signatures: expert specialization and expert collaboration. For white-box access, the method extracts expert activation patterns directly from MoE layers. For black-box access, it trains a lightweight Shadow-MoE proxy to mimic the target model's behavior through text-level distillation. The detection uses permutation-invariant Wasserstein distances between specialization and collaboration profiles to determine if models share routing signatures indicative of distillation.

## Key Results
- Achieves >94% detection accuracy across various scenarios in both white-box and black-box settings
- Perfect 100% accuracy in the most challenging black-box scenario (detecting distillation when both teacher and student are black-box)
- Outperforms existing baselines that rely on self-identity or output similarity
- Robust to prompt-based evasion, maintaining high accuracy even under adversarial conditions

## Why This Works (Mechanism)

### Mechanism 1: Structural Habits Transfer Through Distillation
Knowledge distillation transfers internal computational patterns (routing habits), not just input-output mappings. When a student learns from a teacher's outputs, it inherits decision-making pathways. In MoE architectures, this manifests as similar expert routing distributions for semantically similar inputs—even when the student model has different architecture or parameter count.

### Mechanism 2: Expert Specialization Captures Domain-Partitioning Fingerprints
The normalized frequency with which each expert activates for a given domain forms a permutation-invariant signature. Distilled students show lower Wasserstein-1 distance to the teacher's profile than scratch-trained models, indicating that independent training on similar data does NOT converge to identical expert-to-domain mappings.

### Mechanism 3: Expert Collaboration Provides Orthogonal Verification
The normalized co-activation matrix between expert pairs provides a complementary signal that is less sensitive to domain-specific syntax than specialization. This captures which experts work together and reveals structural computation habits beyond which expert is "responsible" for handling specific domains.

### Mechanism 4: Shadow-MoE Proxies Preserve Routing Signatures for Black-box Access
Lightweight text-level distillation from any LLM to a sparse MoE proxy preserves the original model's routing habits. The proxy's routing—specialization and collaboration—is then compared to another proxy, enabling detection when internals are inaccessible.

## Foundational Learning

- **Mixture-of-Experts (MoE) Routing**
  - Why needed here: The entire detection method relies on interpreting routing distributions (which experts activate, how often, with whom) as signatures.
  - Quick check question: Given an MoE layer with 64 experts and top-2 routing, how would you compute the specialization profile for a "math" domain from 100 prompts?

- **Wasserstein Distance (Earth Mover's Distance)**
  - Why needed here: The core metric for comparing routing distributions is permutation-invariant W1 distance, not KL or cosine similarity.
  - Quick check question: Why is W1 distance preferred over KL divergence when comparing two expert specialization profiles that may differ by an expert index permutation?

- **Knowledge Distillation (KD) Taxonomy**
  - Why needed here: The method must distinguish text-level KD (black-box) from logit/hidden-state KD (white-box), and understand what each transfers.
  - Quick check question: If a student model is trained only on teacher-generated text (no logit matching), would you expect specialization signatures to transfer? Why or why not?

## Architecture Onboarding

- **Component map:**
  - Calibration Dataset -> Shadow-MoE Proxy (Moonlight-16B-A3B) -> Signature Extractor -> Permutation Solver -> Detection Scorer
  - DeepSeek-R1 API/OLMoE-1B-7B weights -> Text-level distillation training -> Last MoE layer routing extraction -> Hungarian algorithm optimization -> Wasserstein distance aggregation

- **Critical path:**
  1. Query both models on calibration prompts → collect responses
  2. For each model, either (a) directly extract routing if white-box MoE, or (b) train Shadow-MoE proxy via text-level distillation (3 epochs, LR=5e-6)
  3. For each proxy, compute specialization profiles per domain and overall collaboration matrix at the last MoE layer
  4. Solve for optimal permutation Π via Hungarian algorithm on each metric
  5. Compute W1 distance under Π; aggregate into final detection score

- **Design tradeoffs:**
  - Proxy vs. direct access: Training proxies adds computational cost but enables black-box detection; direct routing extraction is cheap but requires white-box access
  - Last layer vs. earlier layers: Deeper layers show more discriminative routing (94% vs. 46% for first layer), but are more expensive to profile if model is large
  - Calibration domain diversity: General instruction-following prompts yield stronger detection than domain-specific prompts
  - Number of experts: More experts (E=64 vs. E=8) provide finer-grained signatures but increase Hungarian algorithm cost

- **Failure signatures:**
  - Random-guess accuracy (~50%): May indicate insufficient calibration prompts, proxy under-training, or that the student was NOT distilled
  - High false positives: May occur if both models trained on identical data with strong domain-induced routing similarity
  - Signature inversion: Observed in Code Contest, likely due to domain-specific routing convergence
  - Proxy collapse (single expert dominates): Load-balancing regularizer Ω(g) should prevent this

- **First 3 experiments:**
  1. Reproduce white-box student, black-box teacher result (Table 2): Train Shadow-MoE proxy for DeepSeek-R1 (teacher), extract routing directly from OLMoE-1B-7B checkpoints (9 domains, distilled vs. scratch)
  2. Ablate calibration set composition: Test the 9 training tasks against 28 calibration subsets (Figure 4) to confirm that general instruction-following prompts yield strongest detection
  3. Stress-test with evasion: Attempt prompt-based evasion (jailbreaking, paraphrasing) on distilled student to verify robustness claim (>94% detection maintained)

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the structural habit detection framework be adapted for dense, non-MoE model architectures where explicit expert routing does not exist?
  - Basis in paper: The authors explicitly state in the Limitations section the need to extend signatures to "dense model and incorporate additional structure cues."
  - Why unresolved: The current method relies entirely on calculating Wasserstein distances between expert specialization and collaboration profiles, which are undefined in standard dense transformers.
  - What evidence would resolve it: A successful adaptation of the framework that utilizes alternative internal structures, such as attention head patterns, to detect distillation in dense models.

- **Open Question 2:** Does the transfer of structural habits persist when distillation involves Reinforcement Learning (RL) or reward-model-mediated channels?
  - Basis in paper: The paper lists "Alternative distillation channels for detecting reward-model-mediated or RL-based distillation" as a specific limitation.
  - Why unresolved: The current study focuses on output text-level KD; RL-based methods might optimize for different objectives that could decouple the student's routing behavior from the teacher's structural habits.
  - What evidence would resolve it: Experiments evaluating Shadow-MoE detection accuracy on student models trained via RLHF or reward modeling.

- **Open Question 3:** Can defensive mechanisms such as structural watermarks or routing randomization be implemented to effectively evade this detection method?
  - Basis in paper: The authors identify exploring "defensive mechanisms (e.g. structural watermarks or routing randomization)" as a future direction.
  - Why unresolved: While the method is robust to prompt-based evasion, it is unclear if actively perturbing the routing mechanism during training would destroy the structural habits required for detection.
  - What evidence would resolve it: A study analyzing detection accuracy when the teacher model employs specific routing noise injection or watermarking techniques during the distillation process.

## Limitations
- The method's reliance on MoE routing patterns raises questions about applicability to non-MoE architectures
- Performance appears sensitive to calibration prompt selection, with general instruction-following prompts showing stronger detection than domain-specific ones
- Limited testing of sophisticated evasion techniques that target routing patterns directly

## Confidence
- **High Confidence:** The core detection mechanism leveraging expert specialization and collaboration signatures shows robust performance across multiple experimental conditions
- **Medium Confidence:** The Shadow-MoE proxy construction reliably preserves routing signatures for black-box detection
- **Low Confidence:** The claim that instruction-following prompts provide stronger signatures than domain-specific prompts requires further investigation

## Next Checks
1. **Cross-Architecture Transfer Test:** Apply the detection method to dense transformer architectures trained with knowledge distillation to verify whether the routing-based signatures generalize beyond MoE models
2. **Adversarial Routing Manipulation:** Implement targeted attacks that modify routing patterns during or after distillation (e.g., adversarial regularization on gating weights) to assess whether detection accuracy degrades
3. **Calibration Set Ablation Study:** Systematically vary calibration prompt diversity, domain coverage, and prompt complexity to quantify their impact on detection accuracy