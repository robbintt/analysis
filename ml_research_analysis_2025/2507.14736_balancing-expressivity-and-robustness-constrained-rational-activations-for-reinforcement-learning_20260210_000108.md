---
ver: rpa2
title: 'Balancing Expressivity and Robustness: Constrained Rational Activations for
  Reinforcement Learning'
arxiv_id: '2507.14736'
source_url: https://arxiv.org/abs/2507.14736
tags:
- learning
- activation
- rational
- functions
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Trainable rational activation functions enhance neural network
  expressivity in reinforcement learning but can introduce instability, particularly
  in continuous control tasks under high-update-to-data regimes. This instability
  manifests as activation explosion and overestimation errors due to coefficient imbalance.
---

# Balancing Expressivity and Robustness: Constrained Rational Activations for Reinforcement Learning

## Quick Facts
- arXiv ID: 2507.14736
- Source URL: https://arxiv.org/abs/2507.14736
- Reference count: 21
- Primary result: Constrained rational activations significantly improve training stability and performance in RL compared to original rational activations and ReLU, while revealing a trade-off between stability and plasticity in continual learning.

## Executive Summary
Trainable rational activation functions enhance neural network expressivity in reinforcement learning but can introduce instability, particularly in continuous control tasks under high-update-to-data regimes. This instability manifests as activation explosion and overestimation errors due to coefficient imbalance. To address this, the authors propose constrained rational activations with structural modifications that limit excessive output scaling while preserving adaptability. Experiments on MetaWorld and DeepMind Control Suite show that these constrained variants significantly improve training stability and performance compared to both original rational activations and ReLU. However, the constraints reduce plasticity in continual learning scenarios, revealing a fundamental trade-off between stability and adaptability. The study also highlights the importance of coefficient initialization and weight decay in balancing these competing objectives.

## Method Summary
The authors propose constrained rational activation functions that address instability in reinforcement learning by modifying the denominator structure. The key modification adds an internal regularization term |x/c|^d (where d > n, the numerator degree) to the denominator and removes the constant term a₀. This forces asymptotic decay to zero as |x| → ∞, preventing uncontrolled growth while maintaining flexibility within operational input ranges. The method is evaluated across continuous control tasks in DeepMind Control Suite and MetaWorld environments using Soft Actor-Critic, comparing original rational activations, constrained variants, and standard ReLU.

## Key Results
- Unconstrained rational activations cause activation explosion and overestimation errors in high UTD regimes due to coefficient imbalance between numerator and denominator
- Constrained rational activations significantly improve training stability and performance in MetaWorld and DeepMind Control Suite environments
- There is a fundamental trade-off between expressivity and plasticity: constraints that stabilize RL training reduce plasticity in continual learning scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unconstrained rational activations cause activation explosion under high UTD regimes in continuous control, leading to severe overestimation.
- Mechanism: When numerator coefficients grow disproportionately large relative to denominator coefficients (or denominator → 0), the rational function produces unbounded outputs for large pre-activations. In high UTD settings with dynamic RL input distributions, this amplifies into runaway overestimation in critic networks and gradient explosion.
- Core assumption: The instability is driven primarily by coefficient imbalance rather than inherent optimization dynamics.
- Evidence anchors:
  - "instability manifests as activation explosion and overestimation errors due to coefficient imbalance"
  - "parameters in the numerator often grew significantly larger than those in the denominator... This imbalance caused the rational activation function to output extremely large values"
- Break condition: If coefficient growth is externally constrained (e.g., weight decay, spectral normalization on weights), or if pre-activation distributions remain narrowly bounded near zero.

### Mechanism 2
- Claim: Adding a structural internal regularization term |x/c|^d (where d > n) to the denominator prevents uncontrolled growth while preserving adaptability.
- Mechanism: Forcing the highest power in the denominator to exceed the numerator forces asymptotic decay to zero as |x| → ∞. Combined with removing the constant term a₀ (ensuring f(0) = 0), this limits extreme outputs while maintaining flexibility within operational input ranges.
- Core assumption: The operational input range stays within bounds where the rational function remains expressive.
- Evidence anchors:
  - "propose constrained rational activations with structural modifications that limit excessive output scaling while preserving adaptability"
  - "This modification limits extreme outputs while preserving flexibility within a controlled input range"
- Break condition: If pre-activations drift far from zero during training, the regularization term causes activations to decay, reducing neuron responsiveness (mitigated by weight decay on coefficients).

### Mechanism 3
- Claim: Constraints that stabilize RL training reduce plasticity in continual learning, creating a fundamental expressivity–plasticity trade-off.
- Mechanism: Constrained Rationals maintain higher NTK rank regardless of initialization, which preserves feature separation and expressivity but accelerates feature collapse over multiple tasks. Original Rationals with low initialization produce lower NTK rank, constraining the function space and better preserving plasticity.
- Core assumption: NTK rank correlates with the plasticity–stability trade-off in this architecture.
- Evidence anchors:
  - "demonstrating a trade-off between expressivity and plasticity in rational activations"
  - "smaller coefficient values correspond to a reduced NTK rank, suggesting a more constrained function space that better preserves plasticity"
- Break condition: When weight decay is applied specifically to activation coefficients, it prevents coefficient drift and partially restores plasticity in CL without severely harming RL performance.

## Foundational Learning

- Concept: **Rational functions (ratios of polynomials)**
  - Why needed here: The entire paper builds on rational activations f(x) = P(x)/Q(x) where coefficients are trainable. You need to understand how polynomial degrees (n, m) affect approximation capacity and why denominator → 0 causes numerical explosion.
  - Quick check question: Given f(x) = (a₁x + a₀)/(|b₁x| + 1), what happens to f(x) as x → 100 if a₁ = 10 and b₁ = 0.01?

- Concept: **High update-to-data (UTD) regimes in RL**
  - Why needed here: The instability only manifests under high UTD (e.g., 10+ gradient updates per environment sample). This regime amplifies coefficient drift and overestimation.
  - Quick check question: If your agent takes 1 step per update vs. 10 gradient updates per step, how does this affect the speed of coefficient drift?

- Concept: **Actor-critic overestimation**
  - Why needed here: The paper's primary failure mode is critic overestimation caused by large activation outputs. You need to understand why Q-value overestimation destabilizes policy learning.
  - Quick check question: In SAC, if the critic systematically overestimates Q-values by 100×, what happens to the actor's policy gradient?

## Architecture Onboarding

- Component map:
  Input → Linear Layer → [Rational Activation: (numerator poly aₙ...a₁) / (denominator poly |bₘ...| + 1 + |x/c|^d)] → Output
                                    ↑
                          Trainable coefficients {aᵢ, bⱼ, c}

- Critical path:
  1. Initialize rational coefficients to approximate Leaky ReLU over [-5, 5] range
  2. Forward pass computes f(x) = P(x)/(Q(x) + |x/c|^d)
  3. Backprop updates both network weights and activation coefficients jointly
  4. Apply weight decay to activation coefficients to prevent drift

- Design tradeoffs:
  - **Expressivity vs. stability**: OR provides maximum adaptability but risks explosion; CR is stable but may underfit in rapidly changing tasks
  - **Initialization scale**: High coefficients stabilize RL (via higher NTK rank) but accelerate plasticity loss in CL
  - **Weight decay on coefficients**: Helps CL plasticity and prevents coefficient drift; excessive decay harms RL performance
  - **LayerNorm interaction**: Unexpectedly, LayerNorm + OR increases instability in MetaWorld (pre-activations stay near zero but outputs still explode)

- Failure signatures:
  - **Activation explosion**: Gradient norms of activation coefficients grow unbounded; critic Q-values diverge to 10³–10⁴ range
  - **Overestimation spikes**: Critic loss suddenly increases; IQM performance collapses after ~500K steps
  - **Plasticity loss**: In CL, accuracy on new tasks degrades progressively; denominator coefficients grow while numerator shrinks (activation range compresses)
  - **LayerNorm + OR failure**: Training becomes unstable specifically in MetaWorld environments even though DMC appears stable

- First 3 experiments:
  1. **Sanity check on simple RL task**: Run SAC with CR (n=3, m=2, d=4) on a single DMC environment (e.g., cheetah-run) with UTD=10. Monitor gradient norms of activation coefficients and critic Q-values. Expect stable training with Q-values in reasonable range.
  2. **Ablate the internal regularization**: Compare OR vs. CR vs. CR-without-|x/c|^d term on MetaWorld Sweep task. Quantify overestimation (predicted Q vs. actual returns) at 200K, 400K, 600K steps. Expect OR to show exploding overestimation, CR to remain stable.
  3. **Test plasticity in CL setting**: Train on MNIST with reshuffled labels (10 tasks) using OR-low-init, CR-high-init, and CR+weight-decay. Plot accuracy per task to measure plasticity retention. Expect CR+WD to best balance stability and adaptability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can dynamic mechanisms be developed to adjust activation function expressivity throughout training to achieve an adaptive balance between stability and plasticity?
- Basis in paper: The conclusion states that future work should explore dynamic mechanisms for adjusting activation function expressivity throughout training, potentially enabling a more adaptive balance between stability and plasticity.
- Why unresolved: The current study employs static structural constraints, which successfully stabilize RL but inherently reduce plasticity in continual learning scenarios.
- What evidence would resolve it: A method that modulates constraint strength based on the training phase, demonstrating improved retention in continual learning without suffering from activation explosion in high-UTD RL.

### Open Question 2
- Question: How do constrained rational activations theoretically influence the optimization landscape and representation learning in deep reinforcement learning?
- Basis in paper: The conclusion calls for further theoretical analysis to understand the impact of constrained rational activations on optimization landscapes and representation learning.
- Why unresolved: The study provides empirical evidence of stability but lacks a formal theoretical derivation of how the denominator constraint alters the loss surface geometry.
- What evidence would resolve it: A theoretical framework linking the structural constraints to the curvature of the loss landscape and gradient flow properties.

### Open Question 3
- Question: Why does the instability of rational activations manifest in continuous control tasks but remain absent in discrete action domains like Atari?
- Basis in paper: The paper notes that preliminary experiments in discrete domains (Atari) did not show similar instability, suggesting the trade-off is particularly relevant for continuous control.
- Why unresolved: The authors identify the discrepancy but do not isolate whether the cause is the input distribution dynamics, reward scale, or architecture-specific interactions.
- What evidence would resolve it: An ablation study analyzing gradient norms and activation distributions across hybrid environments that isolate variables like action space continuity and observation variance.

## Limitations
- The coefficient-imbalance mechanism is primarily supported by indirect evidence, with alternative explanations (LayerNorm pathology, critic network depth) not fully ruled out
- The plasticity-stability trade-off based on NTK rank correlation is heuristic and lacks mechanistic justification for why smaller coefficient initialization preserves plasticity
- The proposed regularization depends on maintaining bounded pre-activation distributions, which is not guaranteed in all RL environments

## Confidence
- Coefficient-imbalance mechanism: Medium
- Regularization efficacy: High
- Plasticity-stability trade-off: Low

## Next Checks
1. **Pre-activation distribution analysis**: Measure pre-activation histograms during training for SAC+CR vs. SAC+OR in DMC environments to verify that bounded input distributions are maintained under the proposed regularization.
2. **LayerNorm ablation study**: Compare SAC performance with and without LayerNorm on rational activation layers to isolate whether LayerNorm is a confounding factor in MetaWorld instability.
3. **Continual learning plasticity test**: Run SAC with CR + weight decay on a multi-task RL benchmark (e.g., MetaWorld with task sequences) to quantify whether coefficient decay restores plasticity without sacrificing RL stability.