---
ver: rpa2
title: 'Agreement Between Large Language Models and Human Raters in Essay Scoring:
  A Research Synthesis'
arxiv_id: '2512.14561'
source_url: https://arxiv.org/abs/2512.14561
tags:
- https
- essay
- scoring
- studies
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This synthesis examined agreement between large language models
  (LLMs) and human raters in automated essay scoring. Among 65 studies published from
  2022 to 2025, reported agreement indices such as Quadratic Weighted Kappa, Pearson
  correlation, and Spearman's rho generally ranged from 0.30 to 0.80, indicating moderate
  to good agreement.
---

# Agreement Between Large Language Models and Human Raters in Essay Scoring: A Research Synthesis

## Quick Facts
- arXiv ID: 2512.14561
- Source URL: https://arxiv.org/abs/2512.14561
- Reference count: 16
- Among 65 studies, LLM-human agreement ranged from 0.30 to 0.80, indicating moderate to good agreement

## Executive Summary
This systematic synthesis examined agreement between large language models and human raters in automated essay scoring. Analyzing 65 studies published from 2022 to 2025, the research found moderate to good agreement indices (0.30-0.80) across various metrics including Quadratic Weighted Kappa, Pearson correlation, and Spearman's rho. GPT-4 was the most frequently used model, with studies predominantly focusing on English-language writing and L2 contexts. While findings suggest LLMs can serve as useful scoring tools, significant variability exists due to differences in study design, scoring rubrics, and reporting practices. The synthesis identifies important research gaps, particularly regarding younger learners and non-English language contexts.

## Method Summary
Following PRISMA 2020 guidelines, the synthesis screened 248 abstracts from multiple databases (ERIC, PsycINFO, Web of Science, ProQuest Central, IEEE Xplore, arXiv, Google Scholar) using the search query `(essay or writing) AND ("language model*" or LLM or GPT or ChatGPT or "Generative AI") AND (agreement or correlation or consistency or reliability or concordance)`. From an initial pool, 106 full-text articles were reviewed, yielding 65 included studies. Multiple coders conducted the screening and extraction process, with pilot coding forms and calibration meetings established. The first author double-coded all studies to ensure reliability.

## Key Results
- Agreement indices ranged from 0.30 to 0.80 across studies, indicating moderate to good agreement between LLMs and human raters
- GPT-4 was the most frequently used model and typically outperformed earlier versions like GPT-3.5
- Studies using standardized datasets (e.g., ASAP, TOEFL11) reported higher agreement values compared to classroom-based writing tasks
- Analytic scoring rubrics were more common (n=37) than holistic approaches (n=26)

## Why This Works (Mechanism)

### Mechanism 1: Prompt-Rubric Alignment
Agreement improves when prompts explicitly include detailed scoring rubrics and scored examples through few-shot prompting. LLMs approximate assessment by mapping input essays against the context provided in the prompt. Providing descriptive criteria and exemplars grounds probabilistic generation, reducing drift in scoring standards.

### Mechanism 2: Model Scale and Semantic Capacity
Advanced models like GPT-4 achieve higher agreement due to superior instruction following and semantic nuance detection. Larger parameter counts and training data volumes improve latent representation of "quality" writing, capturing subtle linguistic features rather than surface proxies like word count.

### Mechanism 3: Domain Standardization Effect
LLMs exhibit higher agreement on standardized, high-stakes datasets compared to classroom-collected essays. Standardized prompts and responses likely resemble the data distribution in models' pre-training corpora, performing better "in-distribution" than on novel classroom tasks.

## Foundational Learning
- **Concept:** Quadratic Weighted Kappa (QWK)
  - Why needed: Primary metric used across 65 studies to measure agreement, accounting for ordinal scoring and penalizing large disagreements more than small ones
  - Quick check: If human rates essay 6 and LLM rates it 1, does QWK penalize this more than 6 vs 5 disagreement?

- **Concept:** Holistic vs. Analytic Scoring
  - Why needed: Synthesis distinguishes studies based on these rubric types, with analytic (n=37) more common than holistic (n=26)
  - Quick check: Does holistic rubric evaluate "grammar" as separate score or part of overall impression?

- **Concept:** L1 vs. L2 Writing Context
  - Why needed: Significant portion of studies (n=31) involved L2 writers, and models may process interlanguage errors differently than native writing
  - Quick check: Why might model trained on L1 text inappropriately penalize L2 writer for distinct phrasing that is actually communicative?

## Architecture Onboarding
- **Component map:** Student Essay + Scoring Rubric + (Optional) Benchmark Examples -> Generative LLM (GPT-4, Claude) -> Agreement Metric Calculator (QWK/Pearson) comparing LLM output against Human Ground Truth
- **Critical path:** Define rubric clearly -> Construct prompt (zero-shot or few-shot) -> Batch process essays -> Calculate agreement indices -> Analyze variance
- **Design tradeoffs:**
  - Standardization vs. Realism: Standardized datasets yield higher agreement but may not reflect real-world classroom writing
  - Cost vs. Accuracy: GPT-4 shows best agreement but is computationally expensive; open-source models offer lower cost but higher variance
- **Failure signatures:**
  - Score Collapse: Model avoids extreme scores (1 or 6), clustering outputs around mean (3-4)
  - Length Bias: Model's score highly correlated with essay length rather than content quality
- **First 3 experiments:**
  1. Baseline Calibration: Run zero-shot prompt on subset to establish lower bound of agreement
  2. Few-Shot Ablation: Add specific examples to prompt and measure delta in QWK
  3. Demographic Stress Test: Evaluate performance on "Elementary" (n=3) or "Non-English" (n=12) subsets to identify edge cases where agreement drops below 0.30

## Open Questions the Paper Calls Out
- How does LLM-based automated essay scoring perform with younger writers (e.g., K-12) compared to adult populations?
- To what extent do factors such as model version, prompting strategies, and dataset type impact LLM-human agreement when examined simultaneously?
- How does the reliability of the human "ground truth" rater affect the measured agreement with LLMs?
- How reliably do LLMs score essays written in non-English languages?

## Limitations
- Limited representation of younger learners (only 3 studies focused on elementary students)
- Significant variability in measurement practices and reporting methods across studies
- Most research focused on English-language writing, with limited validation in non-English contexts

## Confidence
- **High Confidence:** GPT-4 achieving highest agreement rates is well-supported across multiple studies
- **Medium Confidence:** Prompt design impact shows high variability despite some studies demonstrating improvement
- **Low Confidence:** Generalizability to non-English languages and elementary learners remains questionable due to limited sample size

## Next Checks
1. Replicate Masikisiki et al. (2023) study design comparing zero-shot, few-shot, and rubric-only prompts on same dataset to quantify exact improvement delta
2. Systematically evaluate LLM performance on the three elementary-level studies and twelve non-English language studies to identify specific failure patterns
3. Compare agreement rates across model tiers (GPT-4, GPT-3.5, open-source alternatives) while tracking computational costs to establish optimal accuracy-to-expense ratio