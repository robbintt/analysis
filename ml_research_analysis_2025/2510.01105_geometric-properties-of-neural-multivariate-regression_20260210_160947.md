---
ver: rpa2
title: Geometric Properties of Neural Multivariate Regression
arxiv_id: '2510.01105'
source_url: https://arxiv.org/abs/2510.01105
tags:
- dimension
- intrinsic
- neural
- generalization
- regression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the geometry of neural multivariate regression
  through intrinsic dimension analysis, revealing why collapse behaviors differ from
  classification. The key finding is that collapsed regression models (IDH < IDY)
  consistently underperform because their lower-dimensional feature manifolds cannot
  fully reconstruct the target manifold, leading to an unavoidable error.
---

# Geometric Properties of Neural Multivariate Regression

## Quick Facts
- **arXiv ID**: 2510.01105
- **Source URL**: https://arxiv.org/abs/2510.01105
- **Reference count**: 40
- **Primary result**: ID_H < ID_Y collapse causes unavoidable error; non-collapsed models show data-regime-dependent optimal ID_H.

## Executive Summary
This paper investigates the geometry of neural multivariate regression through intrinsic dimension analysis, revealing why collapse behaviors differ from classification. The key finding is that collapsed regression models (ID_H < ID_Y) consistently underperform because their lower-dimensional feature manifolds cannot fully reconstruct the target manifold, leading to an unavoidable error. In contrast, non-collapsed models (ID_H ≥ ID_Y) show varying generalization based on data size and noise: for low-data/noisy tasks, reducing ID_H improves performance, while for high-data/low-noise tasks, increasing ID_H enhances generalization. This contrasts with classification, where lower intrinsic dimension typically correlates with better generalization. The work provides a geometric framework for understanding regression collapse and offers practical strategies: increasing feature dimension in over-compressed regimes and decreasing it in under-compressed, noisy, or low-data settings.

## Method Summary
The study uses multivariate regression tasks with MLPs (1-5 layers, 64-1024 width) trained on MuJoCo locomotion data (20K samples) and synthetic vision tasks (MNIST, CIFAR-10 with 50K samples). Training employs SGD (MuJoCo) and Adam (vision) with weight decay sweeps. The core analysis uses the 2-Nearest Neighbor estimator to compute intrinsic dimensions of last-layer features (ID_H) and targets (ID_Y), comparing these to test MSE across different weight decay and architecture configurations.

## Key Results
- Collapsed models exhibit ID_H < ID_Y, leading to over-compression and poor generalization due to Sard's Theorem
- For low-data/noisy tasks, reducing ID_H improves performance; for high-data/low-noise tasks, increasing ID_H enhances generalization
- NRC1 metric is inadequate for detecting nonlinear collapse—ID_H often clusters near ID_Y even when NRC1 suggests full collapse to n dimensions
- Intrinsic dimension decreases monotonically across layers during training for both collapsed and non-collapsed models

## Why This Works (Mechanism)

### Mechanism 1: Over-Compression Creates Irreducible Reconstruction Error
- Claim: When ID_H < ID_Y, the model cannot fully reconstruct the target manifold, leading to unavoidable error
- Mechanism: Sard's Theorem shows smooth maps from lower-dimensional to higher-dimensional manifolds cannot be surjective, meaning some target manifold points are outside the model's predictive reach
- Core assumption: Learned features lie on a smooth manifold of dimension ID_H; targets lie on a smooth manifold of dimension ID_Y
- Evidence anchors: Theorem 1 proof via Sard's Theorem; abstract states collapsed models exhibit ID_H < ID_Y leading to over-compression and poor generalization
- Break condition: If target manifold is embeddable in lower dimension or if the map is non-smooth, the claim may weaken

### Mechanism 2: Data Regime Determines Optimal Feature Dimension in Non-Collapsed Models
- Claim: For non-collapsed models (ID_H ≥ ID_Y), optimal ID_H depends non-monotonically on data quantity and noise
- Mechanism: In low-data regimes, higher ID_H captures training-specific noise/outliers—extra dimensions overfit spurious correlations; abundant clean data enables richer representations that transfer better
- Core assumption: Outliers and noise disproportionately inflate estimated ID_H when data is scarce
- Evidence anchors: Abstract states low-data/noisy tasks benefit from reducing ID_H while high-data/low-noise tasks benefit from increasing it; Figures 6(b,e,h) show U-shaped curves for small/noisy datasets
- Break condition: If noise is structured and learnable, reducing ID_H may discard signal; if test distribution shifts significantly, higher ID_H may not help

### Mechanism 3: Intrinsic Dimension Captures Nonlinear Collapse Missed by PCA-Based NRC
- Claim: 2-NN intrinsic dimension estimator reveals nonlinear manifold structure missed by linear PCA-based NRC metrics
- Mechanism: NRC1 measures linear subspace concentration, missing nonlinear curvature; 2-NN estimator captures manifold dimensionality through local neighborhood ratios
- Core assumption: Data is locally uniform in density within the range of the second nearest neighbor
- Evidence anchors: Section 5, Figure 4 shows collapsed models with ID_H values scattered below ID_Y, not at n; NRC1 is inadequate at uncovering this refined geometric structure
- Break condition: If local uniformity assumption fails, 2-NN estimates become unreliable

## Foundational Learning

- **Concept: Intrinsic Dimension (ID)**
  - Why needed here: Core diagnostic metric distinguishing over-compressed from under-compressed regimes; determines whether feature manifold can theoretically cover target manifold
  - Quick check question: Given a dataset, can you explain why 2-NN estimation relies on local uniformity and how it differs from PCA-based dimensionality?

- **Concept: Neural Collapse (Classification vs. Regression)**
  - Why needed here: Central contrast—classification benefits from collapse (simplex ETF structure improves generalization), while regression suffers (over-compression loses target information)
  - Quick check question: Why does collapse to a simplex ETF help classification but collapse to an n-dimensional subspace harm regression?

- **Concept: Manifold Reconstruction and Smooth Maps**
  - Why needed here: Theoretical foundation for why ID_H < ID_Y causes unavoidable error; connects differential geometry (Sard's Theorem) to practical model failure
  - Quick check question: If a linear map W projects from a 3D feature manifold to a 5D target space, can it be surjective? Explain using rank constraints.

## Architecture Onboarding

- **Component map**: h_θ (feature extractor MLP) -> W (final linear layer) -> Output
- **Critical path**:
  1. Estimate ID_Y on target data before training (establishes threshold)
  2. Train with varying weight decay and architecture sizes
  3. Compute ID_H on last-layer features using 2-NN estimator
  4. Classify regime: over-compressed (ID_H < ID_Y) vs. under-compressed (ID_H > ID_Y)
  5. If over-compressed: reduce weight decay or increase architecture width
  6. If under-compressed with low/noisy data: increase weight decay or reduce width

- **Design tradeoffs**:
  - High weight decay → lower ID_H → risk of over-compression but reduces overfitting in noisy/low-data regimes
  - Large architecture width → higher ID_H → avoids over-compression but may overfit with limited/noisy data
  - Deeper networks → progressive ID reduction through layers

- **Failure signatures**:
  - **Over-compressed failure**: High train AND test MSE, small generalization gap, ID_H < ID_Y, NRC1 near zero
  - **Under-compressed overfitting**: Low train MSE, high test MSE, large generalization gap, ID_H >> ID_Y in low-data/noisy settings
  - **Diagnostic check**: If output layer ID_P is significantly below ID_Y, model is likely in over-compressed regime

- **First 3 experiments**:
  1. **Establish baseline ID_Y**: Apply 2-NN estimator to your target dataset to determine ID_Y; this is your critical threshold
  2. **Weight decay sweep**: Train models across λ_WD ∈ {0, 1e-5, 1e-4, ..., 1e-2, 3e-2} with fixed architecture (e.g., 3-layer, 256-width). Plot ID_H vs. test MSE
  3. **Architecture width sweep**: At a fixed weight decay that produces ID_H ≈ ID_Y, vary width ∈ {64, 128, 256, 512, 1024} to test whether increasing ID_H improves or harms test MSE based on your data regime

## Open Questions the Paper Calls Out

- **Open Question 1**: Can ID_Y be estimated a priori from input statistics or task structure, without access to ground-truth targets?
- **Open Question 2**: How can practitioners precisely control ID_H during training beyond varying weight decay and architecture width?
- **Open Question 3**: Do these findings generalize to high-dimensional regression tasks where ID_Y > 20?
- **Open Question 4**: What mechanisms drive the monotonic decrease of ID_H across layers during training, and can this trajectory predict final generalization?

## Limitations
- Geometric framework relies on targets and features residing on smooth manifolds, which may not hold for real-world data
- 2-NN estimator's absolute values depend on local density assumptions that may not hold in practice
- Results are primarily demonstrated on synthetic vision targets and MuJoCo regression, requiring validation on broader real-world datasets

## Confidence
- **High**: The over-compression mechanism (ID_H < ID_Y → irreducible error) is well-supported by Sard's Theorem and consistent experimental observations
- **Medium**: The regime-split mechanism (low-data/noisy favors lower ID_H, high-data/low-noise favors higher ID_H) is empirically demonstrated but lacks theoretical grounding for exact thresholds
- **Medium**: The 2-NN estimator's ability to detect nonlinear collapse missed by NRC1 is validated but relies on local uniformity assumptions

## Next Checks
1. **Estimator Robustness**: Repeat experiments using alternative ID estimators (e.g., MLE-based) to verify regime classification consistency across methods
2. **Distribution Shift**: Test whether the ID_H < ID_Y over-compression error persists when training and test data distributions differ (e.g., covariate shift)
3. **Real-World Targets**: Apply the framework to real-world regression tasks (e.g., tabular datasets with inherent lower-dimensional structure) to test generalizability beyond synthetic targets