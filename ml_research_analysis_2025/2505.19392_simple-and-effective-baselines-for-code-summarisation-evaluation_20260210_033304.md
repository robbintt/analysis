---
ver: rpa2
title: Simple and Effective Baselines for Code Summarisation Evaluation
arxiv_id: '2505.19392'
source_url: https://arxiv.org/abs/2505.19392
tags:
- summary
- code
- metrics
- reference
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new method for evaluating code summarization
  by directly asking a large language model (LLM) to rate the quality of a summary.
  Unlike traditional metrics that only compare text similarity, this approach can
  also consider the code context.
---

# Simple and Effective Baselines for Code Summarisation Evaluation

## Quick Facts
- arXiv ID: 2505.19392
- Source URL: https://arxiv.org/abs/2505.19392
- Authors: Jade Robinson; Jonathan K. Kummerfeld
- Reference count: 40
- Key outcome: LLM-based evaluation consistently outperforms or matches traditional metrics like BLEU, METEOR, ROUGE-L, SIDE, and embedding-based approaches for code summarization quality assessment

## Executive Summary
This paper introduces a new method for evaluating code summarization by directly asking a large language model (LLM) to rate the quality of a summary. Unlike traditional metrics that only compare text similarity, this approach can also consider the code context. The authors test their method on two standard human-annotated datasets and find that it consistently outperforms or matches existing metrics. They also introduce a reference-free variant that works just as well and could be used for other tasks, such as flagging poor-quality documentation in codebases. However, they observe that the LLM may favor its own outputs, so they recommend using their method alongside embedding-based metrics to avoid bias.

## Method Summary
The authors propose evaluating code summaries by prompting an LLM with the code snippet and generated summary (optionally including a reference summary), using a chain-of-thought approach with a software engineer persona. The LLM rates the summary on a 1-5 Likert scale based on accuracy and adequacy. They test this approach against traditional n-gram metrics (BLEU, METEOR, ROUGE-L), embedding-based methods (SentenceBERT, voyage-code-3), and specialized code metrics (SIDE) across three human-annotated datasets measuring Overall Score, Similarity, and Adequacy. The method includes a reference-free variant that omits the reference summary from the prompt.

## Key Results
- LLM-based evaluation achieves Spearman correlation of 0.47 with human Overall Score judgments, outperforming best embedding baseline (0.43) and traditional n-gram methods (0.35-0.42)
- Reference-free variant performs equivalently to reference-based version (0.46 vs 0.47, p=0.6779), enabling new use cases
- LLM metrics are less sensitive to low-quality reference summaries compared to traditional metrics
- Self-preference bias: Claude rated its own generated summaries highest in 92.7% of cases

## Why This Works (Mechanism)

### Mechanism 1: Code-Contextual Evaluation via LLM Prompting
- Claim: Directly prompting an LLM to rate a code summary produces quality assessments that correlate with human judgments as well as or better than existing metrics.
- Mechanism: Unlike n-gram and embedding-based metrics that compare only the generated summary to a reference, the LLM receives the actual source code as context. This allows it to judge whether the summary accurately reflects the code's functionality, not just its similarity to reference text.
- Core assumption: LLMs have sufficient code comprehension capabilities to detect factual inconsistencies between summaries and implementation.
- Evidence anchors: [abstract] "Unlike n-gram and embedding-based baselines, our approach is able to consider the code when giving a score." [section 4, p.2] "One benefit is that this approach can consider the relevant code as well as the reference summary."

### Mechanism 2: Reference-Free Quality Estimation
- Claim: Removing the reference summary from the prompt does not significantly degrade evaluation quality for overall score prediction.
- Mechanism: The LLM grounds its evaluation directly in the code-summary relationship rather than requiring a human-written proxy. The reference-free variant (ask-claude-no-ref) achieved correlation of 0.46 vs. 0.47 for the reference-based version on Overall Score—no statistically significant difference (p=0.6779).
- Core assumption: The LLM's internal representation of "good documentation" aligns with human annotators' implicit quality criteria.
- Evidence anchors: [abstract] "This allows us to also make a variant that does not consider the reference summary at all." [section 6, p.3] "ask-LLM-no-ref is just as effective... with no statistically significant difference between the two."

### Mechanism 3: Self-Output Bias in LLM Evaluators
- Claim: LLMs exhibit systematic bias toward rating outputs they generated more highly.
- Mechanism: When Claude evaluated summaries it had generated, it assigned the highest possible rating in 92.7% of cases. This self-preference bias could inflate scores for certain models in comparative evaluations.
- Core assumption: The bias is model-specific rather than an artifact of the generation prompt.
- Evidence anchors: [abstract] "LLMs tend to rate their own output more highly, so they recommend using this method alongside embedding-based metrics." [section 6.1, p.4] "While Claude did find some issues... in 92.7% of cases it gives its summaries the highest possible rating."

## Foundational Learning

- **Spearman's Rank Correlation**
  - Why needed here: The paper uses this non-parametric measure to evaluate how well metric scores align with human ratings. Unlike Pearson's, it doesn't assume normal distribution—critical for human judgment data.
  - Quick check question: If Metric A assigns scores [1, 2, 3] and humans rate the same summaries [10, 50, 100], what's the Spearman correlation? (Answer: 1.0—perfect rank alignment)

- **Reference-Free vs. Reference-Based Metrics**
  - Why needed here: Understanding this distinction is essential for grasping why the no-ref variant enables new use cases (flagging bad documentation in codebases) that traditional metrics cannot support.
  - Quick check question: You have a legacy codebase with outdated comments. Which metric variant would you use to identify low-quality documentation? Why?

- **N-gram vs. Embedding-Based Similarity**
  - Why needed here: The paper positions its approach against these two families. BLEU/METEOR/ROUGE count token overlaps; embedding methods measure semantic similarity via vector cosine. Both ignore code entirely.
  - Quick check question: A generated summary says "sorts items" while the reference says "orders elements." Would BLEU or SentenceBERT give a higher similarity score? Why?

## Architecture Onboarding

- **Component map:**
  [Code Snippet] + [Generated Summary] + (Optional: [Reference Summary]) -> Prompt Template (role + task + response format) -> LLM API Call (Claude/GPT/OLMo) -> Structured Response Extraction -> Score (1-5 Likert or 0-100 scale)

- **Critical path:** Start with the final prompt (Appendix E, Figure 3)—chain-of-thought + software engineer role + agree/disagree format. The prompt asks the LLM to show reasoning steps before providing its rating. The reference-free variant simply omits the reference summary line.

- **Design tradeoffs:**
  - Cost: Embedding metrics (voyage-code-3) cost $0.000002/query vs. $0.024/query for ask-claude—10,000× more expensive
  - Latency: LLM calls add 1-3 seconds per evaluation vs. milliseconds for embeddings
  - Bias vs. Accuracy: LLM metrics correlate better with human overall quality (0.47 vs. 0.43 for best embedding) but introduce self-bias risk
  - Recommendation (p.4): Use LLM + embedding metrics together during development; human evaluation for final benchmarks

- **Failure signatures:**
  - LLM struggles to evaluate specific quality dimensions (conciseness, fluency) separately—scores tend to collapse into adequacy assessments (Table 2, p.4)
  - Low-quality reference summaries degrade embedding/n-gram metrics more than LLM metrics (Figure 1, p.9)
  - Rare programming languages may produce unreliable scores due to weaker LLM understanding

- **First 3 experiments:**
  1. Baseline replication: Run ask-claude prompt on the Roy et al. dataset excerpts to verify Spearman correlation ~0.42-0.47 for Overall Score
  2. Self-bias probe: Generate summaries with your target LLM, then evaluate with the same model vs. a different LLM. Measure rating differential
  3. Ensemble test: Combine ask-LLM score with voyage-code-3 embedding similarity using weighted average. Test whether this reduces variance in model rankings compared to either metric alone

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can debiasing techniques mitigate the self-preference bias where LLMs rate their own generated summaries more highly?
- Basis in paper: [explicit] The authors found Claude gave its own summaries the highest rating in 92.7% of cases, and recommend using embedding-based methods alongside LLM metrics to avoid this bias.
- Why unresolved: The paper identifies the bias but does not test interventions; the root cause (whether stylistic familiarity or content) remains unclear.

### Open Question 2
- Question: How well do LLM-based metrics perform on less common programming languages with limited representation in training data?
- Basis in paper: [explicit] The authors state that due to data availability, they could not evaluate on languages other than Python and Java, and note "our method may be less effective on languages that are less widely used."
- Why unresolved: The correlation between language representation in LLM pre-training data and metric reliability is unknown.

### Open Question 3
- Question: Can prompt engineering enable LLM-based metrics to reliably evaluate specific quality dimensions (conciseness, fluency) rather than defaulting to overall adequacy?
- Basis in paper: [explicit] The authors attempted to adapt prompts for different dimensions but found the LLM "surprisingly difficult to override" and scores "mainly reflect Adequacy."
- Why unresolved: It is unclear whether this is a fundamental limitation of LLM judges or a prompt design problem.

## Limitations

- Self-preference bias: LLMs systematically rate their own generated summaries more highly (92.7% top ratings for Claude), requiring complementary embedding-based metrics
- Language coverage: Method may be less effective on less common programming languages with limited representation in LLM training data
- Dimension specificity: LLMs struggle to evaluate specific quality dimensions (conciseness, fluency) separately, defaulting to overall adequacy judgments

## Confidence

- **High confidence**: Reference-free variant performs equivalently to reference-based (p=0.6779); traditional metrics perform worse on Overall Score; self-bias exists and is substantial
- **Medium confidence**: LLM metrics improve correlation with human Similarity/Adequacy judgments; ensemble approach with embeddings is beneficial
- **Low confidence**: Claims about applicability to other documentation tasks; generalizability to rare programming languages; whether correlation improvements translate to better downstream model selection

## Next Checks

1. **Cross-LLM evaluation test**: Generate summaries using GPT-4o, then evaluate with Claude, GPT-4o, and OLMo. Measure rating variance to quantify self-bias impact on relative model rankings.

2. **Cost-benefit analysis**: For a typical code review workflow (100 summaries/day), calculate total evaluation cost difference between LLM metrics ($2.40/day) and embedding metrics ($0.0002/day), then assess whether correlation improvements justify the 10,000× cost increase.

3. **Domain adaptation probe**: Apply the reference-free variant to documentation quality scoring in a large codebase with known documentation issues. Compare flagged summaries against human-identified low-quality documentation to validate practical utility beyond research datasets.