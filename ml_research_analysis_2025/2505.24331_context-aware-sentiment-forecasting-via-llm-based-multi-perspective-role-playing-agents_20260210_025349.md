---
ver: rpa2
title: Context-Aware Sentiment Forecasting via LLM-based Multi-Perspective Role-Playing
  Agents
arxiv_id: '2505.24331'
source_url: https://arxiv.org/abs/2505.24331
tags:
- sentiment
- social
- media
- user
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-perspective role-playing framework
  for sentiment forecasting on social media, predicting future user sentiments in
  response to evolving real-world events. The method extracts sentiment-centered features
  like textual tone and event attitude from user comments, then simulates future social
  media responses using a subjective role-playing agent.
---

# Context-Aware Sentiment Forecasting via LLM-based Multi-Perspective Role-Playing Agents

## Quick Facts
- **arXiv ID**: 2505.24331
- **Source URL**: https://arxiv.org/abs/2505.24331
- **Reference count**: 33
- **Key outcome**: Multi-perspective role-playing agents achieve up to 45% individual-level accuracy and 10x JSD reduction in forecasting social media sentiment responses to real-world events

## Executive Summary
This paper introduces a multi-perspective role-playing framework for sentiment forecasting on social media, predicting future user sentiments in response to evolving real-world events. The method extracts sentiment-centered features like textual tone and event attitude from user comments, then simulates future social media responses using a subjective role-playing agent. An objective role-playing agent—trained as a behavioral psychologist—ensures consistency in tone and attitude, iteratively refining the predictions. Experiments on Hurricane Sandy and the 2020 U.S. Election datasets show significant improvements over state-of-the-art baselines in both microscopic (accuracy, Macro F1) and macroscopic (Jensen-Shannon divergence) evaluations.

## Method Summary
The framework combines feature extraction, subjective role-playing, and objective consistency checking to forecast future sentiment. It extracts textual tone and event attitude from historical comments, uses a subjective agent to simulate future responses, and employs a fine-tuned psychologist agent to detect inconsistencies and refine predictions through iterative feedback. The approach leverages LLMs for reasoning and simulation while maintaining computational efficiency through LoRA fine-tuning.

## Key Results
- Achieves up to 45% individual-level sentiment forecasting accuracy
- Reduces Jensen-Shannon divergence by 10-fold compared to baselines
- Ablation study shows each component (feature extraction, role-playing, objective agent) contributes significantly to performance

## Why This Works (Mechanism)

### Mechanism 1: Implicit Feature Extraction for User Persona Modeling
- **Claim**: Extracting implicit sentiment-centered features (textual tone of voice and event attitude) from historical user comments enables more accurate simulation of user behavior than relying on explicit user attributes alone.
- **Mechanism**: LLMs analyze user comments Cu_t before time t to extract: (1) textual tone of voice νu_t (lexical, syntactic, paralinguistic patterns reflecting persona), output as three descriptive adjectives; (2) attitude αu_t toward the event, inferred from event context and expression patterns. These features condition the role-playing agent.
- **Core assumption**: Users maintain relatively consistent textual tone of voice on social media, and attitude shifts can be reasonably inferred from event context and prior expression patterns.
- **Evidence anchors**: [abstract]: "extract sentiment-centered features like textual tone and event attitude from user comments"; [section 3.3]: "people tend to maintain a consistent textual tone of voice according to their social image on social media"
- **Break condition**: If users exhibit highly stochastic or context-independent expression patterns, extracted features would not generalize to future comments.

### Mechanism 2: Subjective Role-Playing with Contextual Reasoning
- **Claim**: LLMs with common-sense knowledge can simulate user response processes by integrating event context, historical comments, and followee interactions to generate future comments.
- **Mechanism**: The subjective agent LLMs_t role-plays user u using extracted features (νu_t, αu_t), self-reported attributes Au_t, historical comments Cu_t, and event context Eu_t. The agent "browses" followee comments Fu_t and generates a predicted comment φu_t' at future time t', treating sentiment forecasting as a reasoning problem.
- **Core assumption**: LLMs possess sufficient common-sense and social reasoning to simulate human-like responses to evolving events based on limited context windows.
- **Evidence anchors**: [abstract]: "simulates future social media responses using a subjective role-playing agent"; [section 1]: "LLMs understand complex external event contexts with embedded common-sense knowledge"
- **Break condition**: If the event involves highly specialized knowledge not captured in LLM training, or if user responses are driven by private information (offline experiences), simulation accuracy degrades.

### Mechanism 3: Objective Psychologist Agent with Iterative Rectification
- **Claim**: A fine-tuned "psychologist" LLM can detect behavioral inconsistencies in generated comments and provide corrective feedback, reducing stochasticity and improving prediction consistency.
- **Mechanism**: Llama 3 8B is fine-tuned via LoRA on 25,000 expert-annotated Q&A pairs where behavioral psychologists assessed tone/attitude consistency. The objective agent analyzes generated comment φu_t' against historical patterns and outputs analysis θu_t'. If inconsistent, θu_t' is fed back to the subjective agent for regeneration (max 3 iterations).
- **Core assumption**: Expert behavioral psychology knowledge can be distilled into an LLM to reliably detect inconsistencies in synthetic social media comments.
- **Evidence anchors**: [abstract]: "An objective role-playing agent—trained as a behavioral psychologist—ensures consistency in tone and attitude, iteratively refining the predictions"; [section 4.3, ablation]: MPR-OB (removing objective agent) shows performance degradation vs. full MPR on sentiment forecasting
- **Break condition**: If the fine-tuned psychologist over-constrains generation, it may prevent legitimate sentiment shifts; if under-constrained, stochasticity remains high.

## Foundational Learning

- **Concept: Role-Playing as Prompt Engineering Paradigm**
  - Why needed here: The framework relies on instructing LLMs to adopt specific personas (subjective users, objective psychologist) rather than treating them as generic text generators.
  - Quick check question: Can you explain the difference between asking an LLM "What would a user say?" versus "You are [user with features X, Y, Z]. Given context, what do you say?"

- **Concept: LoRA (Low-Rank Adaptation) Fine-Tuning**
  - Why needed here: The objective psychologist agent uses LoRA to embed expert behavioral knowledge while keeping base model weights frozen—critical for parameter efficiency.
  - Quick check question: What is the key advantage of LoRA over full fine-tuning when adapting an 8B parameter model to a specialized task?

- **Concept: Jensen-Shannon Divergence for Distribution Comparison**
  - Why needed here: Macroscopic evaluation uses JSD to measure how closely forecasted sentiment distribution matches ground truth. Understanding JSD properties is necessary for interpreting results.
  - Quick check question: Why might JSD be preferred over KL divergence when comparing predicted vs. actual sentiment distributions across a population?

## Architecture Onboarding

- **Component map**: Historical comments + event context → Feature extraction → Subjective role-play → Comment generation → Objective consistency check → (if fail) Regeneration with feedback → Sentiment classification → Final forecast
- **Critical path**: Historical comments + event context → Feature extraction → Subjective role-playing agent generates predicted comment → Objective psychologist agent analyzes consistency → Iterative rectification (max 3 iterations) → Sentiment analysis → Final forecast
- **Design tradeoffs**:
  - Model choice for subjective agent: Gemma/Mistral chosen over GPT/Llama-main to avoid content censorship (social media comments often contain aggressive language); tradeoff is performance variation
  - Iteration limit (n=3): Balances computational cost against refinement quality
  - Ground truth aggregation: Averaging user comments over 24h periods (vs. single next-comment) for stability; verified via K-S test (p=0.9999)
- **Failure signatures**:
  - Content censorship: Using aligned models (GPT, Llama) causes 15-51% of generations to fail due to refusal to process aggressive content
  - Stochastic user behavior: ~15% of Hurricane Sandy users posted about circumstances without social media input, making prediction inherently difficult
  - Sparse historical data: Feature extraction fails if comments are insufficient to characterize tone/attitude
- **First 3 experiments**:
  1. Reproduce ablation study (MPR-RP, MPR-FE, MPR-OB, full MPR) on Hurricane Sandy subset; expect MPR-RP near random baseline and full MPR achieving best accuracy
  2. Swap subjective agent model (Gemma 2 vs. Mistral NeMo) to assess sensitivity; paper shows Mistral slightly outperforms on some metrics but not consistently
  3. Test iteration limits (n=1, 3, 5) to determine marginal benefit of additional rectification rounds; current n=3 may be conservative

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the integration of multimodal data (e.g., images, videos) impact the forecasting accuracy compared to the text-only approach?
- **Basis in paper**: [explicit] The authors explicitly state in the Limitations section that the current framework "merely includes textual data" and suggest future work explore "multimodal integration."
- **Why unresolved**: Social media communication increasingly relies on non-textual cues, which the current LLM agents cannot process or incorporate into their role-playing simulations.
- **What evidence would resolve it**: A comparative study evaluating the MPR framework on datasets containing image/text pairs versus text-only baselines, specifically analyzing performance differences in sentiment polarity.

### Open Question 2
- **Question**: To what degree does augmenting agents with diverse external information sources (e.g., local news, offline networks) improve accuracy?
- **Basis in paper**: [explicit] The Discussion section notes that agents currently access limited information, whereas real users have "diverse sources like friends, families, local news," implying this gap limits performance.
- **Why unresolved**: The paper does not quantify how much the lack of "real-world" context (outside social media) contributes to the prediction error, particularly for the ~15% of stochastic users.
- **What evidence would resolve it**: Experiments enriching the agent's context window with localized news feeds or inferred offline event data, followed by a measurement of the reduction in Jensen-Shannon divergence.

### Open Question 3
- **Question**: Can the performance degradation in safety-aligned LLMs be mitigated to allow accurate simulation of aggressive or negative sentiments?
- **Basis in paper**: [explicit] The authors report in the Limitations and Appendix that models like Llama 3.1 fail to process "inappropriate" content, resulting in significantly lower accuracy (e.g., 17.3% vs 44.5%).
- **Why unresolved**: Standard safety alignment filters conflict with the necessity to simulate raw, sometimes toxic, human emotional expression found in disaster or political datasets.
- **What evidence would resolve it**: Developing a specialized fine-tuning strategy that allows the model to analyze and generate aggressive content within a "psychologist" persona without triggering refusal behaviors.

## Limitations

- The framework's reliance on subjective feature extraction from limited historical comments introduces potential bias when user expression patterns change rapidly or historical data is sparse
- The psychologist agent's effectiveness depends entirely on the quality and coverage of the 25,000 fine-tuning pairs, which may not represent all behavioral scenarios
- Geographic and demographic bias may affect results since experiments focus specifically on 3,000 users each from New Jersey and New York counties during specific events

## Confidence

- **High Confidence**: The multi-perspective role-playing architecture design and the iterative rectification mechanism (proven by ablation study showing MPR-OB degradation). The approach of using uncensored models for subjective generation is well-justified.
- **Medium Confidence**: The quantitative improvements over baselines (accuracy up to 45%, JSD reduction 10x) given the limited evaluation on only two datasets and the specific geographic/cultural context.
- **Low Confidence**: The robustness of feature extraction across diverse user populations and events, since the paper provides minimal analysis of feature extraction failures or edge cases where tone/attitude inference breaks down.

## Next Checks

1. **Stress-test feature extraction** by evaluating performance on users with highly variable expression patterns or limited historical data (e.g., new accounts, users who change personas) to quantify the 15% failure rate threshold
2. **Cross-dataset generalization** by applying the framework to a third event dataset from a different cultural/geographic context (e.g., non-US political event or natural disaster) to assess scalability beyond the New Jersey/New York user base
3. **Psychologist agent calibration** by conducting a human evaluation study where independent behavioral psychologists rate the consistency analyses θu_t' against ground truth, measuring alignment between automated and expert assessments