---
ver: rpa2
title: 'pFedFair: Towards Optimal Group Fairness-Accuracy Trade-off in Heterogeneous
  Federated Learning'
arxiv_id: '2503.14925'
source_url: https://arxiv.org/abs/2503.14925
tags:
- learning
- fairness
- clients
- cited
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of achieving group fairness
  in heterogeneous federated learning (FL) settings where clients have different distributions
  of sensitive attributes. The authors identify that standard FL approaches optimizing
  a global model can lead to suboptimal fairness-accuracy trade-offs, particularly
  disadvantaging underrepresented clients.
---

# pFedFair: Towards Optimal Group Fairness-Accuracy Trade-off in Heterogeneous Federated Learning

## Quick Facts
- arXiv ID: 2503.14925
- Source URL: https://arxiv.org/abs/2503.14925
- Reference count: 40
- Authors achieve superior fairness-accuracy trade-offs in federated learning with heterogeneous sensitive attribute distributions

## Executive Summary
This paper addresses group fairness in federated learning where clients have different distributions of sensitive attributes. Standard global model optimization leads to suboptimal fairness-accuracy trade-offs, particularly disadvantaging underrepresented clients. The authors propose pFedFair, a personalized federated learning approach that combines global model optimization with local fairness-aware personalization at each client through Moreau envelope regularization.

The method achieves state-of-the-art fairness-accuracy trade-offs on both tabular and image datasets, with up to 5-6% accuracy improvements while maintaining lower demographic disparity parity (DDP) values. The approach effectively addresses the bias introduced by global model aggregation that favors majority sensitive attribute distributions.

## Method Summary
pFedFair combines global model optimization with client-level personalized fairness optimization. Each client optimizes both prediction loss and group fairness violations while maintaining proximity to the global model through Moreau envelope regularization. For vision tasks, the method uses pre-trained embeddings (DINOv2, CLIP) with linear classifiers for computational efficiency. The personalization is controlled by parameters λ (balancing global vs local updates), η (fairness regularization strength), and γ (proximity to global model).

## Key Results
- pFedFair consistently outperforms FedAvg+KDE and pFedMe+KDE baselines in worst-case accuracy-DDP trade-offs across heterogeneous clients
- On Adult dataset, achieves ~84.7% accuracy with DDP ~0.031 for underrepresented Client 1
- Using DINOv2/CLIP embeddings with linear classifiers reduces training time from 331.2s to 15.3s with 11.71M to 0.0008M parameters
- Optimal λ=0.4 parameter value achieves best fairness-accuracy trade-off across experiments

## Why This Works (Mechanism)

### Mechanism 1: Targeted Fairness Personalization
- Claim: Client-level demographic parity requires different optimal classifiers per client when sensitive attribute distributions vary
- Mechanism: Decouples optimization into global utility model and personalized fair models via Moreau envelope regularization
- Core assumption: Shared Bayes optimal classifier (P(Y|X,S) consistent) across clients with varying P(S)
- Evidence: Proposition 4.1 proves optimal decision rules differ across clients with performance gap bounded by (2P(S=s_max) - 1) × |P(Y|S=s_max) - P(Y|S=s^(i)_max)|

### Mechanism 2: Moreau Envelope as Fairness-Controlled Deviation
- Claim: Moreau envelope enables personalized models to deviate from global model proportionally to local fairness needs
- Mechanism: Each client solves min_{w_i}[L̂_i(w_i) + η·ρ(w_i(X), S) + γ/2||w_i - w||²] with gradient combination w̃_i ← w - α(g_w + λg_w^fair)
- Core assumption: λ value exists that balances global utility preservation against local fairness optimization
- Evidence: Figure 4(c) validates λ=0.4 achieves optimal trade-off

### Mechanism 3: Embedding-Space Fairness for Visual Recognition
- Claim: Pre-trained embeddings enable efficient fairness-aware learning by reducing optimization to linear classifier
- Mechanism: Freezes pre-trained φ(x) and trains only linear layer with fairness regularization, enabling larger batch sizes for KDE estimation
- Core assumption: Pre-trained embeddings capture task-relevant features sufficiently for linear fair classifier
- Evidence: Figure 4(a) shows linear classifier on DINOv2/CLIP embeddings achieves superior fairness-accuracy trade-offs vs end-to-end training

## Foundational Learning

- **Concept: Moreau Envelope Regularization**
  - Why needed: Mathematical tool enabling targeted personalization through smoothed objective
  - Quick check: Can you explain why the Moreau envelope gradient points toward the minimizer of the inner optimization problem?

- **Concept: Demographic Parity and DDP**
  - Why needed: Fairness metric being optimized (DDP(f) = Σ_y|P(f(X)=y|S=0) - P(f(X)=y|S=1)|)
  - Quick check: If DDP=0.05 for a binary classifier, what does this imply about prediction rates between protected groups?

- **Concept: Kernel Density Estimation (KDE) for Fairness**
  - Why needed: KDE-based dependence measurement between predictions and sensitive attributes as fairness regularizer
  - Quick check: Why would KDE-based fairness estimation struggle with small batch sizes in standard end-to-end training?

## Architecture Onboarding

- **Component map**: Server -> Global model w -> Clients (Client i: Local dataset, Personalized weights w_i^γ, Fairness regularizer ρ) -> Server aggregation

- **Critical path**: 
  1. Initialize global weights w
  2. For each round t: clients download w, compute local gradients, solve inner Moreau minimization, upload updates
  3. Server aggregates: w ← (1/m)Σw̃_i
  4. At inference: Client i uses personalized weights w_i^γ

- **Design tradeoffs**:
  - λ (global vs. local balance): λ=0.4 optimal; higher values prioritize local fairness at cost of global utility
  - η (fairness regularization strength): Range [0, 0.9]; higher values reduce DDP but may harm accuracy
  - γ (Moreau envelope parameter): Controls personalized model deviation; too low prevents fairness adaptation, too high causes overfitting
  - Embedding choice: DINOv2 vs CLIP; DINOv2 generally shows better fairness-accuracy trade-offs

- **Failure signatures**:
  - NPR alignment to global majority: If personalized models show negative prediction rates mirroring global majority, personalization is not working—check λ value
  - Catastrophic accuracy drop on underrepresented clients: If Client 1 shows >5% accuracy drop vs local-only training, global model dominates—increase λ
  - Non-convergence in vision tasks: If training loss oscillates with complex architectures, switch to embedding-based approach with linear classifier

- **First 3 experiments**:
  1. Reproduce tabular baseline on Adult dataset with 5 clients (Client 1 underrepresented: 2000 male/400 female; Clients 2-5 overrepresented: 400 male/2000 female). Compare pFedFair against FedAvg+KDE and pFedMe+KDE using worst-case accuracy and DDP.
  2. Ablation on λ parameter using CelebA dataset with DINOv2 embeddings. Test λ ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6} and plot worst-case test error vs. DDP trade-off.
  3. Embedding comparison on UTKFace with 10 clients. Compare DINOv2 vs CLIP embeddings for ethnicity classification with gender as sensitive attribute. Measure training time and parameter count.

## Open Questions the Paper Calls Out

- **Extension to complex vision tasks**: Future work includes extending pFedFair to object detection and generative models (explicit in conclusion)
- **Adaptive fairness constraints for multimodal models**: Exploring adaptive fairness constraints tailored to multimodal vision-language models (explicit in conclusion)
- **Non-shared conditional distributions**: Performance when P(Y|X,S) varies across clients, not just P(X,S) (inferred from theoretical assumption)
- **Multi-valued sensitive attributes**: Handling multi-valued or continuous sensitive attributes beyond binary categories (inferred from binary-only focus)

## Limitations

- The Moreau envelope-based personalization assumes a shared global utility model across heterogeneous clients, which may not hold when clients have fundamentally different label distributions
- The fairness metric (DDP) is sensitive to estimation quality, particularly for KDE-based regularizers, with no sensitivity analysis provided
- The embedding-based approach relies heavily on pre-trained model's ability to capture task-relevant features without encoding biases, with no validation on fairness-trained embeddings

## Confidence

- **High Confidence**: The mechanism of combining global utility optimization with local fairness personalization via Moreau envelope is well-supported by theoretical analysis and experimental validation across four datasets
- **Medium Confidence**: The assumption of shared Bayes optimal classifier across clients is theoretically justified but not empirically validated beyond the specific heterogeneous sensitive attribute distribution scenario
- **Low Confidence**: The claim that embedding-based approaches with frozen pre-trained models consistently outperform end-to-end training lacks corpus validation and appears to be a novel empirical observation

## Next Checks

1. **Shared Utility Assumption Test**: Implement federated learning scenario where clients have different label distributions and measure whether pFedFair's assumption of shared global utility model still holds or leads to degraded performance

2. **KDE Sensitivity Analysis**: Systematically vary KDE bandwidth parameters and batch sizes in the fairness regularizer, then measure impact on DDP estimation accuracy and final fairness-accuracy trade-offs across different dataset sizes and client distributions

3. **Embedding Bias Transfer Test**: Train embeddings with explicit fairness constraints (e.g., adversarial debiasing) and compare their performance in pFedFair's embedding-based approach against standard pre-trained embeddings to quantify how much fairness improvements depend on embedding quality