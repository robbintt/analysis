---
ver: rpa2
title: 'Beyond Semantics: How Temporal Biases Shape Retrieval in Transformer and State-Space
  Models'
arxiv_id: '2510.22752'
source_url: https://arxiv.org/abs/2510.22752
tags:
- probability
- position
- temporal
- token
- heads
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how Large Language Models (LLMs) retrieve
  information based on temporal position within prompts, analogous to human episodic
  memory. The authors design two experiments using prompts with repeated tokens and
  varying contexts to isolate temporal effects from semantic content.
---

# Beyond Semantics: How Temporal Biases Shape Retrieval in Transformer and State-Space Models

## Quick Facts
- arXiv ID: 2510.22752
- Source URL: https://arxiv.org/abs/2510.22752
- Reference count: 25
- Primary result: LLMs exhibit temporal biases (serial recall, primacy, recency) analogous to human episodic memory, with induction heads linked to serial recall in transformers.

## Executive Summary
This paper investigates how Large Language Models (LLMs) retrieve information based on temporal position within prompts, analogous to human episodic memory. The authors design two experiments using prompts with repeated tokens and varying contexts to isolate temporal effects from semantic content. Across seven diverse LLMs (transformer and state-space models), they find consistent temporal biases: models preferentially predict the token immediately following repeated instances ("serial recall") and show strong primacy and recency effects, favoring information near the prompt's beginning or end. An ablation study links the serial recall phenomenon in transformers to induction heads, while state-space models exhibit similar biases despite architectural differences. These findings demonstrate that temporal structure significantly shapes LLM retrieval, enabling a form of episodic separation within prompts.

## Method Summary
The authors designed two experimental paradigms to isolate temporal retrieval biases from semantic content. In the first experiment, they used prompts with repeated tokens and varying contexts to measure serial recall - the tendency to predict tokens immediately following repeated instances. The second experiment manipulated token positions to test primacy and recency effects by placing target information at different locations within prompts. They tested seven diverse LLMs including both transformer architectures (GPT-2, BERT variants) and state-space models (Mamba variants). The experiments measured token prediction probabilities and compared them against baseline semantic expectations. An ablation study was conducted on transformer models by removing induction heads to test their contribution to serial recall effects.

## Key Results
- LLMs show consistent serial recall, preferentially predicting tokens immediately following repeated instances across all tested architectures
- Strong primacy and recency effects observed, with models favoring information near the beginning or end of prompts
- Ablation study demonstrates induction heads in transformers are responsible for serial recall phenomenon
- State-space models exhibit similar temporal biases despite fundamental architectural differences from transformers

## Why This Works (Mechanism)
The temporal biases arise from the models' training on sequential data, where position-dependent patterns become embedded in attention mechanisms and recurrence structures. In transformers, induction heads learn to detect and complete repeated patterns, explaining the serial recall effect. The primacy and recency effects likely stem from attention patterns that naturally emphasize positions near sequence boundaries, possibly due to positional encoding schemes or training dynamics that create position-specific feature representations.

## Foundational Learning
- **Episodic memory**: Memory for specific events in temporal context - needed to establish the conceptual framework for comparing LLM retrieval to human memory systems; quick check: can be validated through behavioral experiments measuring recall accuracy based on temporal position
- **Induction heads**: Transformer attention heads that complete repeated patterns - needed to understand the mechanistic basis of serial recall in transformers; quick check: can be verified through ablation studies removing specific attention heads
- **Primacy/recency effects**: Cognitive biases favoring information at sequence beginning/end - needed to frame the experimental design and interpret results in terms of established memory research; quick check: can be measured by systematically varying token positions in prompts
- **State-space models**: Alternative architecture using selective state updating instead of attention - needed for architectural comparison to understand whether temporal biases are architecture-specific or universal; quick check: can be validated by testing multiple state-space architectures on the same temporal bias paradigms

## Architecture Onboarding

Component map: Token embeddings -> Positional encoding -> Attention layers/Recurrence -> Feed-forward networks -> Output projection

Critical path: Input token sequence flows through positional encoding, then through multiple layers of attention (transformers) or state updates (state-space), finally reaching output projection that generates next-token probabilities

Design tradeoffs: Transformers use quadratic attention complexity for global context modeling vs state-space models' linear complexity with selective state updates; positional encodings in transformers are fixed vs learned positional embeddings in some architectures

Failure signatures: Serial recall failure indicates induction head malfunction; primacy/recency effect absence suggests positional encoding issues; cross-architecture inconsistency may indicate training instability or architectural constraints

First experiments:
1. Test induction head ablation on multiple transformer variants to confirm serial recall mechanism
2. Apply identical temporal bias experiments to additional state-space architectures for broader validation
3. Vary prompt length systematically to measure how primacy/recency effects scale with context size

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental design focuses on simple token repetition paradigms that may not capture full complexity of real-world episodic-like memory retrieval
- Temporal biases could be influenced by tokenization artifacts or specific choice of repeated tokens
- Comparison between transformer and state-space models limited by small sample size (seven models total)

## Confidence

High confidence:
- Core finding that LLMs exhibit serial recall, primacy, and recency effects is supported by consistent results across seven diverse models
- Ablation study linking induction heads to serial recall in transformers provides mechanistic insight with strong empirical support

Medium confidence:
- Claim that state-space models exhibit "similar biases despite architectural differences" is supported but requires caution due to limited sample size
- Assertion that temporal structure enables "episodic separation within prompts" is conceptually compelling but somewhat speculative

## Next Checks
1. Cross-linguistic validation: Replicate experiments with non-English prompts and tokens to verify whether temporal biases are language-independent
2. Extended context windows: Test the same temporal bias paradigms with significantly longer prompts (4K-8K tokens) to examine whether primacy and recency effects persist
3. Mechanistic ablation in state-space models: Perform targeted ablation studies on state-space model components to identify specific architectural elements responsible for temporal biases