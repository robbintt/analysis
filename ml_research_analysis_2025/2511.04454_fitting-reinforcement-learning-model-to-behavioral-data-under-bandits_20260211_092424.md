---
ver: rpa2
title: Fitting Reinforcement Learning Model to Behavioral Data under Bandits
arxiv_id: '2511.04454'
source_url: https://arxiv.org/abs/2511.04454
tags:
- problem
- fitting
- cvx-loc
- d-loc
- cvx-loc-t
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of fitting reinforcement learning
  (RL) models to behavioral data from multi-armed bandit tasks, a common paradigm
  in neuroscience and psychology. The authors provide a generic mathematical optimization
  formulation for a wide range of RL models and analyze their convexity properties,
  finding them non-convex in general.
---

# Fitting Reinforcement Learning Model to Behavioral Data under Bandits

## Quick Facts
- arXiv ID: 2511.04454
- Source URL: https://arxiv.org/abs/2511.04454
- Authors: Hao Zhu; Jasper Hoffmann; Baohe Zhang; Joschka Boedecker
- Reference count: 13
- Primary result: Novel convex relaxation method for fitting RL models to bandit behavioral data, achieving significant computational speedup while maintaining accuracy.

## Executive Summary
This paper tackles the computational challenge of fitting reinforcement learning models to behavioral data from multi-armed bandit tasks. The authors demonstrate that standard RL model fitting problems are generally non-convex and propose a convex relaxation approach that transforms these into tractable convex programs. Through extensive evaluation on simulated datasets, they show their method achieves comparable accuracy to existing approaches while offering dramatic computational improvements. The work is accompanied by an open-source Python package, making the method accessible to researchers without specialized optimization expertise.

## Method Summary
The authors formulate RL model fitting as a mathematical optimization problem and analyze its convexity properties, finding it non-convex in general. They then develop a convex relaxation technique that replaces the original problem with a tractable convex surrogate. This approach uses geometric constraints relaxed through convex approximation, enabling efficient computation while preserving solution quality. The method is specifically designed for multi-armed bandit environments and is implemented in an open-source Python package for broad accessibility.

## Key Results
- Convex relaxation achieves comparable accuracy in recovering value functions and model parameters versus direct local minimization and Monte Carlo inference
- Significant computational speedup, often orders of magnitude faster than existing methods on 1000-episode datasets with 200 time steps each
- Open-source Python package enables direct application without requiring convex optimization expertise
- Theoretical analysis confirms non-convexity of standard RL model fitting problems

## Why This Works (Mechanism)
The convex relaxation approach works by transforming the inherently non-convex optimization landscape of RL model fitting into a convex surrogate problem. By relaxing geometric constraints through convex approximation, the method enables the use of efficient convex optimization algorithms that can find global optima or high-quality solutions. The key insight is that while the original problem is intractable due to non-convexity, carefully constructed convex relaxations can preserve solution quality while dramatically reducing computational complexity.

## Foundational Learning
- **Convex optimization**: Why needed - Provides tractable alternatives to non-convex problems; Quick check - Can verify convexity using second-order conditions
- **Multi-armed bandit theory**: Why needed - Defines the specific RL setting being addressed; Quick check - Understands exploration-exploitation tradeoff
- **Geometric constraints in value iteration**: Why needed - Forms the basis for relaxation approach; Quick check - Can identify when constraints become problematic
- **Behavioral data analysis**: Why needed - Connects computational methods to real-world applications; Quick check - Knows how to evaluate model fit quality
- **Numerical stability**: Why needed - Critical for implementing log-space formulations; Quick check - Can identify gradient explosion issues
- **Model parameter recovery**: Why needed - Core evaluation metric for method effectiveness; Quick check - Can compare recovered vs true parameters

## Architecture Onboarding
- **Component map**: Behavioral data → Convex relaxation formulation → Optimization solver → Recovered parameters → Model evaluation
- **Critical path**: Data preprocessing → Constraint relaxation → Convex optimization → Parameter recovery → Validation
- **Design tradeoffs**: Computational efficiency vs. solution accuracy; Generality vs. problem-specific optimization
- **Failure signatures**: Large bound gaps (J - J_lb), numerical instability in log-space formulations, poor convergence on complex models
- **First experiment**: Test on simple Bernoulli bandit with known parameters
- **Second experiment**: Compare computation time vs. accuracy on medium-sized dataset
- **Third experiment**: Validate on real behavioral data from published study

## Open Questions the Paper Calls Out
### Open Question 1
Can the log-space convex formulation for recovering RL model parameters be stabilized to overcome the numerical issues associated with the geometric decay of matrix rows?
- Basis in paper: Appendix A explicitly discusses this formulation (Eq A.1) but states it is rejected in practice due to "severe numerical issues" and gradient explosion near zero.
- Why unresolved: The logarithmic penalty creates unstable gradients for small entries, and the strict inequality constraint v^j_1 < 0 is unsupported by standard conic solvers.
- Evidence would resolve it: A robust regularization method or a solver capable of handling the strict constraints without fitting the non-informative tails of the vectors.

### Open Question 2
How can researchers definitively distinguish between a poor heuristic solution and a loose convex relaxation bound when the gap J - J_lb is large?
- Basis in paper: Section 7.2 notes that a large gap implies "either the fitting is poor, or, the bound is poor (or both)," leaving the interpretation ambiguous.
- Why unresolved: The paper provides the lower bound J_lb as a diagnostic tool but does not offer a method to quantify the tightness of the bound itself relative to the ground truth.
- Evidence would resolve it: A theoretical analysis deriving the maximum possible gap for specific model structures or noise levels.

### Open Question 3
Can this convex relaxation approach be generalized to reinforcement learning settings with state dependencies, such as Markov Decision Processes (MDPs)?
- Basis in paper: The problem formulation in Section 1 and 2 is strictly restricted to "multi-armed bandit environments," implying the method does not currently handle state-transition dynamics.
- Why unresolved: The transformation F and the relaxation of geometric constraints rely on the specific recursive structure of bandit value updates.
- Evidence would resolve it: A derivation showing the constraints for an MDP value function can be similarly relaxed to a convex surrogate without exploding in variable complexity.

## Limitations
- Method currently restricted to multi-armed bandit environments without state transitions
- Computational savings demonstrated only on specific dataset sizes (1000 episodes × 200 time steps)
- Limited validation on real behavioral data versus simulated datasets
- Uncertainty about performance on more complex RL models with function approximation

## Confidence
- High: Core mathematical claims about non-convexity and convex relaxation validity
- Medium: Empirical performance comparisons on simulated data
- Low: Claims about robustness across different RL model variants not explicitly tested

## Next Checks
1. Test the method on real behavioral datasets from published neuroscience studies to verify performance on noisy, heterogeneous human data
2. Extend the framework to handle contextual bandits and more complex state representations beyond simple multi-armed bandits
3. Benchmark against additional inference methods including variational approaches and expectation-maximization algorithms to establish relative performance across broader methodological landscape