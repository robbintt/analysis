---
ver: rpa2
title: Large Language Models and Futures Price Factors in China
arxiv_id: '2509.23609'
source_url: https://arxiv.org/abs/2509.23609
tags:
- factor
- factors
- futures
- price
- market
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies large language models (LLMs) to generate factors
  for Chinese futures markets, addressing the challenge of limited data availability
  and underdeveloped factor research in emerging markets. The authors use GPT-4 to
  generate 40 novel factors from market features like basis, spot price, futures premium,
  open-high-low-close prices, and volume.
---

# Large Language Models and Futures Price Factors in China

## Quick Facts
- arXiv ID: 2509.23609
- Source URL: https://arxiv.org/abs/2509.23609
- Authors: Yuhan Cheng; Heyang Zhou; Yanchu Liu
- Reference count: 27
- Primary result: GPT-4 generated factors achieved annualized returns up to 22.13% with Sharpe ratios as high as 13.58

## Executive Summary
This paper introduces a novel methodology using large language models to generate predictive factors for Chinese futures markets. The authors employ GPT-4 to create 40 novel factors from market features like basis, spot price, futures premium, and volume. The approach addresses the challenge of limited data availability in emerging markets by leveraging zero-shot learning, allowing the model to synthesize factors without relying on extensive historical return data. The generated factors are tested using long-short and long-only strategies, demonstrating statistically significant outperformance versus traditional IPCA benchmark models.

## Method Summary
The authors use prompt engineering to constrain GPT-4 to act as a "fund manager" and generate Python code for factor calculations. They provide the model with a data architecture framework and restrict its access to market features like basis, spot price, futures premium, and OHLC prices. GPT-4 outputs executable code that the authors manually review to prevent forward-looking bias. The 40 generated factors are then used to construct single-factor and multi-factor portfolios, which are tested over in-sample (2018-2023) and out-of-sample (2023-2024) periods.

## Key Results
- GPT-generated factors achieved annualized returns up to 22.13% with Sharpe ratios as high as 13.58
- Factors demonstrated statistically significant alphas (p<0.01) versus IPCA benchmark model
- Performance remained robust across different strategies and prompt variations
- Methodology showed particular effectiveness after GPT's training data cutoff date in April 2023

## Why This Works (Mechanism)

### Mechanism 1: Zero-Shot Factor Synthesis via Semantic Reasoning
GPT-4 generates predictive factors by synthesizing financial theory and code logic without exposure to historical return data. The model leverages its pre-trained knowledge of asset pricing theories to propose hypotheses that map market features to factor values, outputting executable Python code rather than text descriptions.

### Mechanism 2: Multi-Dimensional Feature Aggregation
High-performing factors result from non-linear combinations of liquidity, volatility, and sentiment proxies derived from raw market data. GPT-4 constructs composite indicators by normalizing disparate data streams and applying transformations like Z-scores and rolling windows.

### Mechanism 3: Temporal Generalization via Training Cutoff
Since GPT-4's training data ends in April 2023, performance in the 2023-2024 testing period suggests the factors capture durable market structures rather than overfitting to past data. This demonstrates genuine predictive power.

## Foundational Learning

- **Zero-Shot Learning in Finance:** Why needed: Unlike traditional ML models that require extensive backtesting data, this approach generates logic rules instantly based on prompt context. Quick check: Can you explain why GPT-4 generating a Python function differs from a Random Forest fitting on a dataset?

- **Factor Construction & Alpha Generation:** Why needed: Understanding the difference between raw features and constructed factors is essential for evaluating the methodology. Quick check: How does the "basis" serve as a sentiment indicator in this context?

- **Forward-Looking Bias (Data Snooping):** Why needed: The authors manually review GPT code to prevent using future data. Quick check: If GPT generates code using `shift(-1)` in a backtest, why is that invalid?

## Architecture Onboarding

- **Component map:** Market Data (Basis, OHLC, Volume, Spot) -> Prompt Engineering Interface -> GPT-4 API -> Manual Code Review -> Factor Database (40 factors) -> Backtesting Engine -> Performance Metrics
- **Critical path:** The "Prompt Engineering" and "Manual Review" steps are critical. Prompt quality dictates factor novelty, and manual review ensures factors are tradeable without data leakage.
- **Design tradeoffs:** Prompt Complexity (simple yields known indicators, complex yields novel but potentially unstable factors) vs. LLM Selection (GPT-4o vs. Claude/Copilot tradeoffs between stability and creativity)
- **Failure signatures:** Syntax/Runtime Errors (invalid column references), Look-Ahead Bias (using future window statistics), Low Variance (constant factor values)
- **First 3 experiments:** 1) Prompt Sensitivity Test: Compare provided prompt vs. simplified prompt to verify originality impact. 2) Bias Validation: Audit Factor 22 code for future date usage. 3) IPCA Benchmark Replication: Replicate IPCA-5 model to establish baseline alpha.

## Open Questions the Paper Calls Out

### Open Question 1
Can the GPT-based factor generation methodology be successfully extended to other capital markets, such as options and bonds, where data structures differ significantly from futures? The current study is restricted to Chinese futures markets.

### Open Question 2
How does the performance of GPT-generated factors hold up across diverse market conditions (e.g., financial crises) and global financial contexts outside of China? The paper focuses on Chinese futures during a specific timeframe.

### Open Question 3
Can prompt engineering or fine-tuning improve GPT's utilization of futures-specific indicators, such as the "basis," which the model currently appears to overlook or misunderstand? Section 6.2 notes GPT may not fully grasp futures market dynamics.

### Open Question 4
To what extent can the manual "forward-looking bias" check be automated or eliminated through stricter prompt constraints or automated code validation? The current methodology relies on human intervention.

## Limitations

- Study focuses exclusively on Chinese commodity futures markets with distinct structural characteristics
- Transaction cost treatment assumes fixed costs of 0.01% which may understate real-world friction
- LLM consistency varies across different GPT variants and prompt variations

## Confidence

**High Confidence:** Methodology successfully generates executable Python code, backtesting framework is correctly implemented, performance metrics are accurately calculated

**Medium Confidence:** LLM genuinely synthesizes novel factors, factors demonstrate genuine predictive power, outperformance reflects superior factor construction

**Low Confidence:** Specific mechanisms by which GPT-4 identifies profitable relationships, robustness across different market regimes, extent of generalization to other markets

## Next Checks

1. **Cross-Market Validation:** Apply methodology to US commodity futures or European equity index futures using same prompt engineering to compare factor performance distributions.

2. **Robustness to Transaction Costs:** Conduct sensitivity analysis varying transaction costs from 0.001% to 0.1% and incorporate realistic bid-ask spreads and volume constraints.

3. **Factor Stability Analysis:** Generate factors using same prompt across 10 independent runs with different random seeds, analyze correlation structure and conduct principal component analysis.