---
ver: rpa2
title: Uncertainty Quantification for Large Language Model Reward Learning under Heterogeneous
  Human Feedback
arxiv_id: '2512.03208'
source_url: https://arxiv.org/abs/2512.03208
tags:
- have
- reward
- page
- theorem
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning reward models from
  heterogeneous human feedback in large language model alignment. The authors develop
  a framework that jointly models both the latent reward function and human annotator
  rationality, accounting for varying expertise levels across different annotators.
---

# Uncertainty Quantification for Large Language Model Reward Learning under Heterogeneous Human Feedback

## Quick Facts
- **arXiv ID**: 2512.03208
- **Source URL**: https://arxiv.org/abs/2512.03208
- **Reference count**: 40
- **Primary result**: Method jointly models latent reward and annotator rationality with uncertainty quantification for large language model reward learning

## Executive Summary
This paper develops a framework for learning reward models from heterogeneous human feedback in large language model alignment. The authors introduce a heterogeneous preference model that jointly captures both the latent reward function and human annotator rationality, accounting for varying expertise levels across different annotators. The method uses alternating gradient descent to solve the resulting biconvex optimization problem and provides theoretical guarantees for convergence and asymptotic normality, enabling uncertainty quantification through confidence intervals. Practically, this allows for statistically valid comparisons between different LLM outputs and introduces a pessimistic best-of-N policy that incorporates reward uncertainty into decision-making, improving robustness under uncertainty.

## Method Summary
The approach models annotator preferences using a scale heterogeneity framework where U(x,s,a) = σ_γ(x)·r_θ(s,a) + ε, with σ_γ(x) = ψ_0(x) + γ^Tψ(x) capturing annotator rationality as a function of contextual features. The method employs alternating gradient descent to jointly estimate the reward parameters θ and rationality parameters γ, solving a biconvex optimization problem. Theoretical analysis establishes convergence guarantees and derives asymptotic normality for the estimator, enabling construction of confidence intervals. The framework supports pessimistic best-of-N policies that select actions based on lower confidence bounds rather than point estimates, providing robustness under uncertainty.

## Key Results
- Alternating gradient descent converges to statistically consistent estimates with error decaying as O(ρ^T b² + c_1 log(1/δ)/((1-ρ)n)) under appropriate initialization
- Asymptotic normality enables construction of confidence intervals for reward estimates with coverage rates approaching nominal levels as n increases
- Pessimistic best-of-N policy achieves higher gold-model rewards than standard BoN policies, particularly for larger N where uncertainty matters more
- Method successfully captures heterogeneity in human preferences across demographic factors in real LLM datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modeling annotator rationality as a scale function enables disentanglement of true reward quality from annotator expertise variation.
- Mechanism: The scale heterogeneity model U(x,s,a) = σ_γ(x)·r_θ(s,a) + ε multiplies the latent reward by annotator-specific rationality σ_γ(x). When σ_γ(x) > 0, annotators prefer higher-reward responses; when ≤ 0, they exhibit non-expert or adversarial behavior. This multiplicative structure allows the likelihood to distinguish between "this response is actually better" versus "this annotator is more reliable."
- Core assumption: Annotator rationality can be expressed as a linear combination ψ_0(x) + γ^Tψ(x) of contextual features, and ψ_0(x) ≠ 0 ensures identifiability between reward and rationality parameters.
- Evidence anchors:
  - [abstract]: "We adopt a heterogeneous preference framework that jointly models the latent reward of answers and human rationality."
  - [section 2]: "The value of σ_γ*(x) reflects the rationality or expertise of the teacher. If σ_γ*(x) > 0, the teacher is more likely to select the response with a higher true reward."
  - [corpus]: Related work on heterogeneous preferences exists (Deng et al. 2014, Jin et al. 2020) but focuses on fixed-item ranking, not unbounded LLM outputs. Corpus evidence for this specific scale heterogeneity mechanism in RLHF is limited.
- Break condition: If annotator contextual features are uninformative or ψ_0 ≡ 0, reward and rationality become unidentifiable, collapsing the mechanism.

### Mechanism 2
- Claim: Alternating gradient descent converges to statistically consistent estimates despite nonconvex joint optimization.
- Mechanism: The negative log-likelihood L_n(θ,γ) is biconvex—convex in θ given γ, convex in γ given θ, but jointly nonconvex. Algorithm 1 alternates: update θ_t via gradient step on ∇_θL_n, then update γ_t via gradient step on ∇_γL_n. Theoretical analysis shows local contraction: each block descends while cross-block error propagation remains controlled under Assumption 4's bound on mixed derivatives.
- Core assumption: Initialization satisfies ||θ_0 - θ*||_2, ||γ_0 - γ*||_2 ≤ b/√2 for sufficiently small b, and the interaction strength M between contextual and response features is bounded: M < min{λ_φ, λ_ψ}/(6c_0).
- Evidence anchors:
  - [section 3]: "Lemma 1...Ln(θ,γ) is convex in θ when γ is fixed, and convex in γ when θ is fixed. However, Ln(θ,γ) is not necessarily jointly convex."
  - [section 4.1]: Theorem 1 establishes ||θ_T - θ*||²_2 + ||γ_T - γ*||²_2 ≤ ρ^T b² + c_1 log(1/δ)/((1-ρ)n).
  - [corpus]: Corpus does not provide comparative evidence for alternating descent vs. other biconvex solvers in this RLHF setting.
- Break condition: If learning rates exceed threshold η_0, or initialization is too far from true parameters, the contraction property fails and error accumulates across blocks.

### Mechanism 3
- Claim: Selecting actions via reward lower confidence bounds yields provably robust best-of-N policies with suboptimality decaying as O(1/√n).
- Mechanism: The pessimistic BoN policy (pBoN) maximizes ĝ(s,a) = r_θT(s,a) - q_{1-α/2}√(φ^T S²_θ φ/n) rather than point estimates. This lower bound accounts for estimation uncertainty: when confidence intervals are wide (high uncertainty), pBoN favors actions with tighter bounds or higher guaranteed minimum rewards. Theorem 6 proves expected suboptimality ≤ 3q_{1-α/2}||Ĩ^{-1/2} E_s[φ(s,a*(s))]||_2 / √n.
- Core assumption: The asymptotic normality in Theorem 3 holds, and the dataset provides sufficient coverage along optimal trajectories (not uniform coverage over all state-action pairs).
- Evidence anchors:
  - [section 5.2]: "The pessimistic BoN policy selects the action that maximizes this conservative estimate... SubOpt(a^pBoN) ≤ 3q_{1-α/2}||Ĩ^{-1/2} E_s φ(s,a*(s))||_2 / √n."
  - [section 6.2.3, Table 5]: pBoN variants achieve higher gold-model rewards than non-pessimistic counterparts across N ∈ {20,40,60,80,128}.
  - [corpus]: Related work (Zhang et al. 2024, Liu et al. 2024) uses finite-sample lower bounds; this approach uses asymptotic bounds. Direct corpus comparison is unavailable.
- Break condition: If sample size n is insufficient for asymptotic approximation validity, or if optimal trajectories are poorly covered by training data, suboptimality guarantees degrade.

## Foundational Learning

- Concept: **Bradley-Terry-Luce (BTL) preference model**
  - Why needed here: The heterogeneous preference model extends BTL by adding the scale function σ_γ(x). Without understanding the baseline BTL probability P(Y=1) = 1/(1+e^{-(r(s,a^(1))-r(s,a^(0)))}), the heterogeneous extension is opaque.
  - Quick check question: Given two responses with rewards 0.8 and 0.5, what is the probability a standard BTL model predicts the first is preferred?

- Concept: **Biconvex optimization and block coordinate descent**
  - Why needed here: The joint estimation problem is nonconvex but biconvex. Understanding why alternating updates work (each subproblem is convex) and when they fail (cross-dependency too strong) is essential for debugging convergence.
  - Quick check question: If L(x,y) is convex in x given y and convex in y given x, does alternating gradient descent always converge to the global minimum? Why or why not?

- Concept: **Fisher information and asymptotic normality**
  - Why needed here: Confidence intervals derive from the asymptotic distribution √n(τ_T - τ*) → N(0, I^{-1}(τ*)). Understanding how the inverse Fisher information determines estimation variance enables proper interpretation of uncertainty quantification results.
  - Quick check question: If the Fisher information matrix has small eigenvalues in certain directions, what does this imply about estimation uncertainty for parameters along those directions?

## Architecture Onboarding

- Component map: Feature extraction (φ(s,a) from pre-trained model, ψ(x) for annotator contexts) -> Parameter estimation (alternating gradient descent) -> Uncertainty quantification (empirical Fisher information matrices) -> Policy deployment (pessimistic or standard BoN)

- Critical path:
  1. Extract features φ(s_i, a_i) for all prompt-response pairs and ψ(x_i) for annotator contexts
  2. Initialize θ₀, γ₀ (paper uses uniform U(-1,1) in experiments; recommend small random values near zero)
  3. Run alternating GD with learning rates η₁, η₂ (paper: 5×10⁻⁵ for real data) for T iterations until convergence
  4. Compute empirical Fisher matrices using final θ_T, γ_T
  5. For each test (s,a), compute confidence interval via Eq. (8)

- Design tradeoffs:
  - **Linear vs. nonlinear reward**: Assumption 1 restricts reward to linear r_θ(s,a) = θ^Tφ(s,a). Trade-off: tractable inference vs. representational capacity. Nonlinear rewards would break asymptotic normality guarantees.
  - **Sample size vs. iteration count**: Theorem 1 requires both n→∞ and T→∞, with nρ₁^T → 0. Practically, prioritize more samples over more iterations once optimization error is negligible.
  - **Confidence level α**: Tighter intervals (smaller α) yield more conservative pBoN policies but may over-penalize genuinely good actions. Paper uses α=0.05.

- Failure signatures:
  - **Diverging gradients**: Check if learning rates exceed η_0 or if initialization violates ||θ_0 - θ*|| ≤ b/√2. Symptom: loss increases rather than decreases.
  - **Non-identifiability**: If ψ_0 ≡ 0 or contextual features are uninformative, θ and γ become confounded. Symptom: large confidence intervals, erratic estimates across runs.
  - **Poor coverage**: If training data lacks diversity in prompts/responses, λ_φ or λ_ψ may be near zero. Symptom: singular or ill-conditioned Fisher matrices.

- First 3 experiments:
  1. **Convergence validation on synthetic data**: Generate data with known θ*, γ* per Section 6.1 setup. Verify ||θ_T - θ*||² and ||γ_T - γ*||² decrease with n and T. Plot against theoretical rate from Theorem 1.
  2. **Coverage rate calibration**: For n ∈ {200, 400, 600}, compute 95% confidence intervals for reward parameters. Check if empirical coverage matches nominal 0.95 (Table 1 benchmark). If undercoverage, increase T or check Fisher matrix conditioning.
  3. **pBoN vs. BoN comparison**: On held-out prompts, generate N candidate responses, rank via both standard BoN (Eq. 2) and pBoN (Eq. 3). Evaluate using independent gold reward model (paper uses Eurus-RM-7B). Confirm pBoN achieves higher or equal gold rewards, especially for larger N where uncertainty matters more.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the uncertainty quantification framework be extended to non-linear reward models (e.g., neural networks) while preserving theoretical convergence guarantees and asymptotic normality?
- Basis in paper: [inferred] Assumption 1 restricts reward functions to linear models: "The reward function lies within a family of linear models rθ(s,a) = θ⊤ϕ(s,a)." The authors acknowledge this is a "standard" assumption but do not address non-linear settings.
- Why unresolved: The theoretical analysis (Lemmas S4–S6, Theorems 1–2) heavily exploits the linear structure for Lipschitz continuity, strong convexity bounds, and explicit gradient/Hessian computations. Non-linear models introduce additional non-convexity beyond the biconvex structure already analyzed.
- What evidence would resolve it: (1) Extension of Theorems 1–2 to neural network reward functions with appropriate smoothness/regularity conditions; (2) empirical demonstration that asymptotic normality holds for non-linear models with comparable or acceptable coverage rates.

### Open Question 2
- Question: How can the framework be generalized from pairwise to K-wise comparisons, where annotators rank or select among multiple candidate responses simultaneously?
- Basis in paper: [inferred] The model in Equation (5) and all subsequent analysis exclusively considers binary comparisons between two responses a(0) and a(1). The related work section references Zhu et al. (2023a) who studied "pairwise or K-wise comparisons," but the current method does not extend to K-wise settings.
- Why unresolved: The likelihood formulation (Equation 6), gradient computations, and Fisher information matrix derivations all assume binary outcomes. K-wise comparisons require modeling multinomial choice probabilities and would change the structure of the Hessian matrix and uncertainty quantification.
- What evidence would resolve it: (1) Derivation of the K-wise likelihood and modified alternating descent algorithm; (2) extension of Theorem 3 to provide confidence intervals for K-wise reward differences.

### Open Question 3
- Question: Can finite-sample (non-asymptotic) confidence intervals be derived that maintain valid coverage for small sample sizes n?
- Basis in paper: [inferred] Theorem 2 establishes asymptotic normality requiring "n → ∞ and T → ∞," while Theorem 4 constructs confidence intervals using asymptotic quantiles. The related work section notes that "recent works... primarily rely on finite-sample lower bounds" while "our approach leverages an asymptotic lower bound."
- Why unresolved: The proof of Theorem 2 relies on classical MLE asymptotic theory (Slutsky's theorem, central limit theorem) which provides no guarantees for small n. Finite-sample validity would require different concentration inequalities or distribution-free methods.
- What evidence would resolve it: (1) Derivation of finite-sample confidence intervals with explicit coverage guarantees for any n ≥ n_min; (2) empirical coverage analysis showing the asymptotic intervals remain valid for practical sample sizes (n < 1000).

## Limitations

- The linear reward model assumption (r_θ(s,a) = θ^Tφ(s,a)) may not capture complex non-linear preferences in real-world LLM outputs
- Convergence guarantees depend on specific initialization conditions and bounded interaction strength M that may not hold in practice
- Asymptotic normality results require large sample sizes with unclear minimum requirements for valid inference
- Pessimistic best-of-N policy's suboptimality guarantees assume sufficient coverage of optimal trajectories in training data

## Confidence

- **High Confidence**: The heterogeneous preference model structure (scale heterogeneity) and alternating gradient descent implementation are well-specified and theoretically grounded. The experimental results on synthetic data demonstrate the method's effectiveness under controlled conditions.
- **Medium Confidence**: The asymptotic normality results and uncertainty quantification framework are mathematically sound, but their practical applicability depends on meeting sample size requirements and feature quality assumptions that may not be verifiable in real applications.
- **Low Confidence**: The pessimistic best-of-N policy's suboptimality guarantees assume sufficient trajectory coverage and large n for asymptotic approximations. These conditions are difficult to verify in practice and may not hold for sparse or biased training data.

## Next Checks

1. **Non-asymptotic performance verification**: Run experiments with varying sample sizes n ∈ {100, 200, 400, 800} to empirically measure how quickly the coverage rates and parameter estimation error approach theoretical bounds, rather than relying solely on asymptotic results.

2. **Robustness to initialization and hyperparameters**: Systematically vary initialization radius b, learning rates η₁, η₂, and M constraints to identify failure modes where the alternating descent algorithm diverges or converges to suboptimal solutions, particularly with realistic (non-synthetic) data.

3. **Transferability testing**: Evaluate the learned reward model on prompts and response types that were excluded or underrepresented in training data to assess whether uncertainty quantification remains valid under distribution shift, and whether the pessimistic policy provides meaningful protection against poor out-of-distribution decisions.