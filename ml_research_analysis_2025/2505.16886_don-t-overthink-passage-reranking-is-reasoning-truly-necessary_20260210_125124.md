---
ver: rpa2
title: 'Don''t "Overthink" Passage Reranking: Is Reasoning Truly Necessary?'
arxiv_id: '2505.16886'
source_url: https://arxiv.org/abs/2505.16886
tags:
- reasonrr
- reasoning
- standardrr
- relevance
- rerankers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies whether explicit reasoning processes improve
  pointwise passage reranking accuracy. The authors compare three variants: StandardRR
  (direct relevance prediction), ReasonRR (adds reasoning chain), and ReasonRR-NoReason
  (reasoning disabled).'
---

# Don't "Overthink" Passage Reranking: Is Reasoning Truly Necessary?

## Quick Facts
- arXiv ID: 2505.16886
- Source URL: https://arxiv.org/abs/2505.16886
- Reference count: 16
- This paper studies whether explicit reasoning processes improve pointwise passage reranking accuracy.

## Executive Summary
This paper studies whether explicit reasoning processes improve pointwise passage reranking accuracy. The authors compare three variants: StandardRR (direct relevance prediction), ReasonRR (adds reasoning chain), and ReasonRR-NoReason (reasoning disabled). Across in-domain (MS MARCO) and out-of-domain (BRIGHT) datasets with Qwen2.5 models (1.5B–7B), StandardRR consistently outperforms ReasonRR. Surprisingly, ReasonRR-NoReason often beats ReasonRR, especially at larger model sizes. Analysis reveals that reasoning pushes models toward polarized scores, limiting their ability to assign partial relevance—a key factor for reranking accuracy. While self-consistency improves ReasonRR, it still lags behind StandardRR. The results suggest reasoning is unnecessary for pointwise reranking, and simpler methods are more effective and cost-efficient.

## Method Summary
The study compares three pointwise reranking variants using Qwen2.5 models (1.5B, 3B, 7B) fine-tuned with LoRA (rank=32, alpha=64) on Rank1 training data (~386K quadruples). StandardRR predicts binary relevance directly from query-passage pairs. ReasonRR generates reasoning chains before predicting relevance. ReasonRR-NoReason pre-fills reasoning with placeholder text before prediction. Evaluation uses NDCG@10 on TREC DL19–DL23 (MS MARCO) and BRIGHT benchmark, reranking BM25 top-100 passages.

## Key Results
- StandardRR consistently outperforms ReasonRR across all model sizes and datasets
- ReasonRR-NoReason surprisingly beats ReasonRR on larger models (7B), suggesting reasoning induces polarization
- Self-consistency improves ReasonRR by 1.8-2.9 NDCG@10 but still underperforms StandardRR
- Score polarization analysis shows ReasonRR concentrates 29% of predictions in extreme bins vs StandardRR's 19.7%

## Why This Works (Mechanism)

### Mechanism 1: Score Polarization from Reasoning
- **Claim:** Explicit reasoning chains push models toward binary relevance decisions, eliminating intermediate scores needed for accurate ranking.
- **Mechanism:** When a model generates reasoning before predicting relevance, the concluding statement ("Therefore, the answer is true/false") creates high confidence in the final token. The softmax over true/false logits becomes polarized toward 0 or 1, with few scores in the 0.1–0.9 range.
- **Core assumption:** Accurate pointwise reranking requires the ability to distinguish between "somewhat relevant" and "highly relevant" passages through graduated scores.
- **Evidence anchors:**
  - [abstract] "reasoning-based rerankers are limited by the LLM's reasoning process, which pushes it toward polarized relevance scores"
  - [Section 4.2] Figure 2 shows ReasonRR places 29% of scores in 0.9–1.0 bin vs StandardRR's 19.7%, with almost no intermediate scores
  - [corpus] Related work on "Rethinking Reasoning in Document Ranking" reports similar findings about CoT in reranking

### Mechanism 2: Implicit Partial Relevance Learning
- **Claim:** Standard pointwise rerankers trained only on binary labels nevertheless learn to output calibrated scores reflecting partial relevance.
- **Mechanism:** Without reasoning tokens, the model must internalize relevance signals. The logit distribution over true/false implicitly encodes confidence levels, naturally producing intermediate probabilities for partially relevant passages.
- **Core assumption:** The model learns to map query-passage similarity features to a continuous confidence signal during fine-tuning.
- **Evidence anchors:**
  - [abstract] "StandardRR generally outperforms ReasonRR"
  - [Section 4.4] "StandardRR having a stronger ability to capture partial relevance"
  - [Section 4.2] StandardRR distributes 11.4% of predictions in partial-relevance bins (0.1–0.9) vs ReasonRR's near-zero

### Mechanism 3: Self-Consistency as Partial Recovery
- **Claim:** Averaging relevance scores across multiple reasoning samples reintroduces score diversity, partially mitigating polarization.
- **Mechanism:** Each reasoning chain may conclude differently for borderline passages. Averaging softmax probabilities across 8 samples produces smoother distributions—20% in partial-relevance bins vs near-zero for single-sample ReasonRR.
- **Core assumption:** The variance across reasoning paths reflects underlying uncertainty that should be preserved.
- **Evidence anchors:**
  - [Section 4.4] Self-consistency improves ReasonRR by 1.8 NDCG@10 on MS MARCO, 2.9 on BRIGHT
  - [Section 4.2] Figure 3 shows ReasonRR+Self-Consistency shifts 20% of scores to intermediate bins

## Foundational Learning

- **Concept: Pointwise vs Listwise Reranking**
  - **Why needed here:** The paper studies pointwise methods that score each query-passage pair independently. Understanding this distinction is critical for grasping why partial relevance matters—pointwise scores must be comparable across passages.
  - **Quick check question:** Can a pointwise reranker directly compare two passages, or must it score them separately?

- **Concept: Relevance Score from Softmax Logits**
  - **Why needed here:** All variants extract relevance R = softmax(z_true, z_false)[true]. The mechanism of polarization operates through this specific extraction method.
  - **Quick check question:** If z_true = 5.0 and z_false = -2.0, what is R?

- **Concept: Chain-of-Thought Prompting**
  - **Why needed here:** ReasonRR generates reasoning tokens before the final answer. The forced reasoning variant ("Okay, I have finished thinking.") shows that the presence of reasoning tokens—not their content—triggers the polarization effect.
  - **Quick check question:** What happens if you pre-fill the reasoning buffer with placeholder text instead of generated reasoning?

## Architecture Onboarding

- **Component map:** Query + Passage → [LLM Backbone: Qwen2.5-1.5B/3B/7B] → [Reasoning Layer: optional CoT generation] → [Scoring Layer: softmax over "true"/"false" logits] → R (relevance probability)
- **Critical path:** The comparison hinges on identical training data (MS MARCO + R1 reasoning chains, 386K examples). Training uses LoRA (rank 32, alpha 64) for 1 epoch. Evaluation re-ranks BM25 top-100 passages.
- **Design tradeoffs:**
  - Inference cost: ReasonRR generates ~100-300 extra tokens per passage
  - Accuracy vs efficiency: StandardRR achieves 62.4 vs ReasonRR's 57.4 NDCG@10 on MS MARCO avg (7B model)
  - Out-of-domain robustness: ReasonRR shows mixed results—better on Psychology/StackOverflow at smaller scales, worse overall
- **Failure signatures:**
  - High-confidence wrong predictions: ReasonRR assigns R > 0.99 to passages it explicitly called "somewhat relevant"
  - Score clustering: If >90% of predictions fall in 0–0.1 or 0.9–1.0 bins, polarization is occurring
  - Classification-retrieval mismatch: Better F1 as classifier but worse NDCG@10 indicates poor partial relevance modeling
- **First 3 experiments:**
  1. **Reproduce the score distribution analysis:** Plot histograms of R values for StandardRR vs ReasonRR on a held-out set. Confirm polarization pattern (should match Figure 2).
  2. **Test ReasonRR-NoReason with your model:** Implement forced reasoning prefill. If NDCG improves, the issue is reasoning-induced polarization, not training quality.
  3. **Pilot self-consistency with n=3 vs n=8:** Table 10 shows no gain beyond 3 samples. Verify this finding to save inference cost before deploying.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does reasoning improve accuracy for listwise or setwise reranking approaches, or does the polarization effect generalize beyond pointwise methods?
- **Basis in paper:** [explicit] "While we demonstrate that reasoning hurts pointwise rerankers, it remains an open question what influence reasoning may have for other reranking approaches such as listwise and setwise."
- **Why unresolved:** The study only evaluated pointwise rerankers; listwise and setwise methods use different scoring mechanisms (comparing passages jointly rather than independently) that may interact differently with reasoning processes.
- **What evidence would resolve it:** Train and evaluate reasoning-based variants of listwise and setwise rerankers using the same methodology, comparing against non-reasoning baselines on MS MARCO and BRIGHT benchmarks.

### Open Question 2
- **Question:** Can training ReasonRR with graded relevance scores (e.g., 1–5) instead of binary labels enable it to capture partial relevance and outperform StandardRR?
- **Basis in paper:** [explicit] "Instead of predicting binary relevance, ReasonRR can be trained to generate graded scores... however, the current Rank1 training data only provides binary labels, so it will be necessary to develop methods to synthesize realistic data."
- **Why unresolved:** Current training data lacks fine-grained relevance labels; it is unclear whether synthetic graded labels would be sufficiently accurate to improve reranking.
- **What evidence would resolve it:** Create or obtain training data with graded relevance judgments, train ReasonRR on this data, and compare its NDCG@10 against StandardRR and the binary ReasonRR.

### Open Question 3
- **Question:** Do the findings generalize across different LLM families (e.g., LLaMA, Mistral, Gemma) beyond Qwen2.5?
- **Basis in paper:** [explicit] "We limit our study to the Qwen2.5 family of models... as future work, it would be interesting to study the impact of reasoning across different model families."
- **Why unresolved:** Qwen2.5 may have architecture or training characteristics that interact specifically with reasoning in reranking; the polarization effect may be stronger or weaker in other models.
- **What evidence would resolve it:** Replicate the StandardRR vs. ReasonRR comparison using identical training data and conditions on alternative LLM families (1.5B–7B scale) and report performance gaps.

### Open Question 4
- **Question:** Does the negative impact of reasoning on reranking accuracy persist, diminish, or reverse at model scales beyond 7B parameters?
- **Basis in paper:** [explicit] "Our experiments were also limited to LLMs with ≤ 7B parameters... the influence at larger scales remains an open question."
- **Why unresolved:** Larger models may have better calibration or reasoning capabilities that mitigate the polarization issue; the trend observed (larger models benefit StandardRR more than ReasonRR) may or may not continue.
- **What evidence would resolve it:** Train and evaluate StandardRR and ReasonRR on larger backbone models (e.g., 14B, 32B, 70B) using the same training setup and report performance differences on MS MARCO and BRIGHT.

## Limitations

- Dataset Coverage Limitation: The analysis relies on MS MARCO and BRIGHT, both using binary relevance judgments. The mechanisms described may not generalize to datasets with graded relevance scales (1-5 ratings).
- Architecture Specificity: All experiments use Qwen2.5 models with specific LoRA configurations. The polarization effect and performance patterns may not transfer to other architectures or fine-tuning approaches.
- Inference Protocol Ambiguity: Self-consistency sampling parameters are not fully specified. The paper reports no gain beyond 3 samples, but optimal settings may vary by model size and task domain.

## Confidence

**High Confidence:** StandardRR consistently outperforms ReasonRR across model sizes and datasets (MS MARCO NDCG@10: 62.4 vs 57.4 for 7B models). The score polarization mechanism is well-supported by empirical distributions.

**Medium Confidence:** The reasoning-induced polarization hypothesis is supported by the ReasonRR-NoReason variant outperforming ReasonRR on larger models, but the mechanism is indirect. The claim that implicit partial relevance learning drives StandardRR's superiority is plausible but requires datasets with graded relevance to fully validate.

**Low Confidence:** The assertion that self-consistency partially recovers from polarization is weakly supported. While self-consistency improves ReasonRR by 1.8-2.9 NDCG@10, this benefit may stem from variance reduction rather than reasoning quality.

## Next Checks

1. **Test ReasonRR-NoReason with your model:** Implement the forced reasoning prefill ("Okay, I have finished thinking.") before true/false prediction. If NDCG@10 improves at 7B scale while degrading at 1.5B/3B, this confirms the polarization mechanism is model-size dependent and not an artifact of training data quality.

2. **Replicate score distribution analysis:** Generate histograms of relevance scores (R values) for StandardRR vs ReasonRR on a held-out validation set. Verify the polarization pattern—ReasonRR should show >25% of scores in extreme bins (0-0.1 and 0.9-1.0) with minimal intermediate scores, while StandardRR should show a more uniform distribution with ~10% in partial-relevance ranges.

3. **Pilot self-consistency with variable sample sizes:** Test self-consistency with n=3, 5, and 8 reasoning samples. Confirm the paper's finding that gains plateau at 3 samples to optimize inference cost. Additionally, test with temperature=0.7 vs greedy decoding to determine if sampling strategy affects the variance reduction benefit.