---
ver: rpa2
title: 'ProMRVL-CAD: Proactive Dialogue System with Multi-Round Vision-Language Interactions
  for Computer-Aided Diagnosis'
arxiv_id: '2502.10620'
source_url: https://arxiv.org/abs/2502.10620
tags:
- medical
- dataset
- dialogue
- report
- proactive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ProMRVL-CAD introduces a proactive medical dialogue system that
  integrates knowledge graphs and multi-view image analysis to generate high-quality
  diagnostic reports. Unlike passive systems, it actively collects patient information
  through synthesized dialogues and uses multi-modal inputs (text and multiple X-ray
  views) to improve diagnosis.
---

# ProMRVL-CAD: Proactive Dialogue System with Multi-Round Vision-Language Interactions for Computer-Aided Diagnosis

## Quick Facts
- **arXiv ID**: 2502.10620
- **Source URL**: https://arxiv.org/abs/2502.10620
- **Reference count**: 40
- **Primary result**: Proactive dialogue with knowledge graphs and multi-view image analysis outperforms state-of-the-art CAD systems in BLEU/ROUGE metrics and clinical efficacy

## Executive Summary
ProMRVL-CAD introduces a proactive medical dialogue system that integrates knowledge graphs and multi-view image analysis to generate high-quality diagnostic reports. Unlike passive systems, it actively collects patient information through synthesized dialogues and uses multi-modal inputs (text and multiple X-ray views) to improve diagnosis. Evaluated on MIMIC-CXR and IU-Xray datasets, ProMRVL-CAD outperforms state-of-the-art models in BLEU and ROUGE metrics, demonstrating superior text quality and clinical efficacy (higher recall and F1 scores). The system also shows robustness to low-resolution and noisy images and achieves scalability with parameter reduction via LoRA. A synthetic proactive dialogue dataset, ProDial, supports model training. Overall, ProMRVL-CAD advances automated medical diagnosis by bridging the gap between LLM interactions and real-world clinical consultations.

## Method Summary
ProMRVL-CAD consists of two core components: Pro-Q Gen (proactive question generation) and MVP-DR Gen (multi-view proactive diagnostic report generation). Pro-Q Gen uses a fine-tuned LLM to generate candidate questions, which are ranked by a clinical concept knowledge graph to ensure relevance and avoid redundancy. MVP-DR Gen extracts features from multiple X-ray views using Swin Transformer, averages them, and combines with text embeddings through alignment layers to generate diagnostic reports via a frozen Llama-2-7B. The system is trained on a hybrid dataset of real (Huatuo-26M, CMtMedQA) and synthetic (ProDial) dialogues, with LoRA adapters reducing parameter count while maintaining performance.

## Key Results
- ProMRVL-CAD achieves BLEU1=0.430 and ROUGE-1=0.375, outperforming state-of-the-art models
- Multi-view + textual input consistently outperforms single-view approaches (BLEU1: 0.430 vs. 0.361)
- Knowledge graph ranking improves question generation quality by ~10% (Table 5)
- System shows robustness to low-resolution and noisy images (86-88% BLEU1 retention)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Proactive question generation via knowledge graph-guided ranking improves diagnostic information collection compared to passive QA systems.
- Mechanism: Pro-Q Gen first generates N candidate questions using an LLM fine-tuned on synthetic dialogue data. A clinical concept knowledge graph—encoding disease-symptom relationships extracted from real clinical records—then ranks candidates by relevance to the patient's accumulated symptom base. The system rejects redundant queries and terminates when key symptoms have been checked.
- Core assumption: Disease-symptom correlations in the knowledge graph meaningfully reflect clinical reasoning patterns that guide effective questioning.
- Evidence anchors:
  - [abstract] "proactive dialogue to provide patients with constant and reliable medical access via an integration of knowledge graph into a recommendation system"
  - [section 1.2] "introducing the clinical concept knowledge graph can significantly enhance clinical professionalism in query generation, with around 10% performance improvements"
  - [corpus] Related work (DoPI, arXiv 2507.04877) similarly combines multi-turn dialogue with knowledge graphs for TCM diagnosis, suggesting convergent validity of this approach.
- Break condition: If the knowledge graph has incomplete or biased disease-symptom mappings, ranking will surface irrelevant questions, reducing dialogue efficiency.

### Mechanism 2
- Claim: Multi-view image averaging combined with textual dialogue/history inputs yields higher-quality diagnostic reports than single-view, image-only approaches.
- Mechanism: MVP-DR Gen uses a Swin Transformer to extract 1024-dim features from each image view, then averages embeddings across views for fixed-size input. Textual inputs (medical history, dialogue) are encoded via MiniLM-L6-v2 (384-dim) and similarly averaged. Both modalities pass through alignment layers that serve as soft prompts to a frozen Llama-2-7B, which generates the report.
- Core assumption: Averaging multi-view features preserves diagnostically relevant information without requiring architectural modifications to the LLM.
- Evidence anchors:
  - [abstract] "multi-view image analysis to generate high-quality diagnostic reports"
  - [section 1.3] "features extracted from different images/views are directly averaged to ensure a fixed input size"
  - [table 3] Multi-view + textual input achieves BLEU1=0.430 vs. single-view 0.361 and multi-view-only 0.370
  - [corpus] Evidence is mixed—related work (3MDBench, arXiv 2504.13861) explores multi-agent multimodal dialogue but doesn't validate the averaging approach specifically.
- Break condition: If critical diagnostic features are view-specific and localized, averaging may dilute their signal.

### Mechanism 3
- Claim: Synthetic proactive dialogue data (ProDial) enables training LLMs to lead diagnostic conversations even without real proactive dialogue datasets.
- Mechanism: The authors generate 66,149 synthetic dialogues using ChatGPT conditioned on medical history and findings from MIMIC-CXR. Dialogues are structured as doctor-patient exchanges where clinical concepts remain consistent with medical visuals. This hybrid dataset (synthetic + 12,250 real conversations from Huatuo-26M/CMtMedQA) fine-tunes Pro-Q Gen via LoRA.
- Core assumption: Synthetic dialogues generated from medical records capture sufficient conversational dynamics to transfer to real interactions.
- Evidence anchors:
  - [abstract] "synthetic medical dialogue dataset that simulates proactive diagnostic interactions"
  - [section 1.2] "ProDial... has similar professionalism and conciseness properties as natural dialogue datasets" (Table 4: Professionalism=0.875, Conciseness=0.860)
  - [corpus] Corpus evidence is limited—no neighbor papers validate synthetic dialogue quality for medical LLM training specifically.
- Break condition: If synthetic dialogues exhibit distributional drift from real clinical conversations, model may generate plausible but clinically inappropriate questions.

## Foundational Learning

- Concept: **Knowledge Graphs for Clinical Reasoning**
  - Why needed here: Pro-Q Gen's ranking system relies on understanding disease-symptom relationships. Without grasping how knowledge graphs encode structured medical knowledge, the ranking logic is opaque.
  - Quick check question: Given symptoms A and B connected to disease X with edge weights 0.8 and 0.3 respectively, which symptom should be queried first?

- Concept: **Vision-Language Alignment via Soft Prompts**
  - Why needed here: MVP-DR Gen uses alignment layers to bridge vision embeddings to a frozen LLM's embedding space. Understanding this is critical for debugging modality fusion failures.
  - Quick check question: Why freeze the LLM and train only the alignment layer rather than fine-tuning the entire model?

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: Both Pro-Q Gen and the simplified MVP-DR Gen use LoRA to reduce trainable parameters (90.9M → 5M with <4% performance drop). Essential for understanding the training pipeline.
  - Quick check question: If LoRA rank N=16 and the original weight matrix is 4096×4096, what's the parameter count for the LoRA adaptation?

## Architecture Onboarding

- Component map:
  - Pro-Q Gen: Llama-3-8B Instruct (backbone) + LoRA adapters → generates N candidate questions → Knowledge Graph ranking module → selected question
  - MVP-DR Gen: Swin Transformer (visual encoder, 1024-dim) + MiniLM-L6-v2 (text encoder, 384-dim) → alignment layers → frozen Llama-2-7B → diagnostic report
  - Supporting: ProDial dataset (78,399 dialogues), Clinical concept knowledge graph (disease-symptom edges with correlation weights)

- Critical path:
  1. Patient initiates dialogue with symptom description
  2. Pro-Q Gen generates candidate questions using fine-tuned LLM
  3. Knowledge graph ranks candidates by disease-symptom relevance
  4. Top-ranked question posed to patient; response added to symptom base
  5. Steps 2-4 repeat until confidence threshold met
  6. All collected inputs (multi-view images + dialogue + history) fed to MVP-DR Gen
  7. Report generated with disease classification appended

- Design tradeoffs:
  - **Averaging vs. attention-based multi-view fusion**: Averaging is simpler but may lose view-specific features. Paper doesn't compare against attention mechanisms.
  - **Frozen LLM vs. full fine-tuning**: Freezing preserves reasoning capability and reduces training cost, but limits adaptation to medical domain-specific language patterns.
  - **Synthetic vs. real dialogue data**: Synthetic enables scale but risks distributional mismatch. Paper uses hybrid approach to balance.

- Failure signatures:
  - **Repetitive questions**: Knowledge graph failing to track checked symptoms; check symptom base update logic
  - **Low BLEU/ROUGE on reports**: Vision-language alignment layer not converging; verify embedding dimensions and learning rate
  - **Hallucinated symptoms in dialogue**: LoRA rank too low or synthetic data quality issue; inspect ProDial samples
  - **Poor recall on specific diseases**: Knowledge graph missing relevant symptom edges for those conditions

- First 3 experiments:
  1. **Ablation on knowledge graph ranking**: Compare Pro-Q Gen with vs. without KG ranking on BLEU/ROUGE for generated questions (Table 5 shows 10%+ improvement; replicate to validate).
  2. **Multi-view vs. single-view comparison**: Run MVP-DR Gen on MIMIC-V2 subset with single-view input vs. multi-view averaging (Table 3 shows ~7% BLEU1 gain; confirm on held-out test).
  3. **Robustness test**: Evaluate on degraded images (half resolution, Gaussian noise) to validate claimed robustness (Table 8 shows 86-88% BLEU1 retention; reproduce as baseline for future modifications).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can federated learning be implemented to scale ProMRVL-CAD across distributed medical institutions while preserving data privacy?
- Basis in paper: [explicit] The conclusion states, "Future work will focus on implementing the system across multiple datasets empowered by federated learning."
- Why unresolved: The current model is trained and evaluated on centralized, publicly available datasets (MIMIC-CXR, IU-Xray). Real-world deployment requires distributed training to comply with patient privacy regulations (e.g., HIPAA).
- Evidence: Demonstrating model convergence and maintenance of BLEU/F1 scores when training on decentralized data shards without raw data exchange.

### Open Question 2
- Question: Does the simple averaging of multi-view visual features limit the model's ability to capture spatial relationships compared to attention-based fusion?
- Basis in paper: [inferred] The methodology states visual features are "directly averaged to ensure a fixed input size," a heuristic that assigns equal weight to all views regardless of diagnostic relevance.
- Why unresolved: Averaging may discard view-specific anomalies (e.g., subtle lesions visible only in lateral views) that require distinct feature highlighting rather than smoothing.
- Evidence: Ablation studies comparing the current averaging layer against cross-attention mechanisms on the MIMIC-V2 (multi-view) dataset.

### Open Question 3
- Question: To what extent does training on ChatGPT-generated synthetic dialogues introduce bias or hallucinations not present in real clinical consultations?
- Basis in paper: [inferred] The Pro-Q Gen is fine-tuned on synthetic data (ProDial) and evaluated using ChatGPT, creating a closed loop where LLMs assess LLMs without human validation.
- Why unresolved: Synthetic dialogues may lack the nuance, uncertainty, or logical errors characteristic of actual patients, potentially leading the system to learn "idealized" but unrealistic interaction patterns.
- Evidence: A "Turing test" evaluation where medical professionals blind-score the naturalness and diagnostic utility of ProMRVL dialogues against human-generated transcripts.

## Limitations

- The synthetic dialogue generation relies entirely on ChatGPT without validation against real clinical conversation data beyond limited real datasets
- The averaging approach for multi-view image fusion may not be optimal for cases where diagnostic features are view-specific and localized
- Long-term clinical efficacy in real-world settings remains unproven due to lack of human validation studies

## Confidence

- **High Confidence**: Multi-view averaging + textual input consistently outperforms single-view approaches (BLEU1: 0.430 vs. 0.361). Knowledge graph ranking improves question generation quality by ~10% (validated in Table 5). Synthetic dialogue generation produces professional conversations (Table 4 scores).

- **Medium Confidence**: Robustness claims for low-resolution and noisy images (86-88% BLEU1 retention) need independent validation. The hybrid synthetic/real dialogue training approach shows promise but lacks comparison to fully real-data alternatives.

- **Low Confidence**: Long-term clinical efficacy in real-world settings remains unproven. The knowledge graph's completeness for rare diseases is not assessed. The averaging mechanism's limitations for view-specific diagnostics are acknowledged but not empirically tested against alternatives.

## Next Checks

1. **Knowledge Graph Completeness Audit**: Evaluate the clinical concept knowledge graph's coverage for rare and complex diseases by measuring symptom-disease edge density across different disease categories. Identify gaps where ranking performance might degrade.

2. **Multi-View Fusion Comparison**: Implement and compare attention-based multi-view fusion against the current averaging approach on the same datasets. Measure whether attention mechanisms preserve view-specific features and improve diagnostic accuracy for diseases requiring bilateral comparison.

3. **Real-World Dialogue Validation**: Conduct a small-scale study where Pro-Q Gen interacts with actual patients or medical professionals using the same clinical cases from MIMIC-CXR. Compare question relevance and diagnostic accuracy against human physicians' questioning patterns.