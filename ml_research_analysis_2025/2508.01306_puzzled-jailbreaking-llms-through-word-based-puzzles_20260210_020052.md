---
ver: rpa2
title: 'PUZZLED: Jailbreaking LLMs through Word-Based Puzzles'
arxiv_id: '2508.01306'
source_url: https://arxiv.org/abs/2508.01306
tags:
- words
- word
- puzzled
- masked
- harmful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces PUZZLED, a novel jailbreak attack that leverages\
  \ word-based puzzles to bypass LLM safety filters. The method masks keywords in\
  \ harmful instructions and embeds them into familiar puzzle formats\u2014word search,\
  \ anagram, and crossword\u2014requiring the model to solve the puzzle to reconstruct\
  \ and respond to the original harmful prompt."
---

# PUZZLED: Jailbreaking LLMs through Word-Based Puzzles

## Quick Facts
- arXiv ID: 2508.01306
- Source URL: https://arxiv.org/abs/2508.01306
- Reference count: 40
- Primary result: PUZZLED achieves 88.8% average attack success rate across five state-of-the-art LLMs using word puzzles to mask harmful keywords

## Executive Summary
This paper introduces PUZZLED, a novel jailbreak attack that leverages word-based puzzles to bypass LLM safety filters. The method masks keywords in harmful instructions and embeds them into familiar puzzle formats—word search, anagram, and crossword—requiring the model to solve the puzzle to reconstruct and respond to the original harmful prompt. Evaluated on five state-of-the-art LLMs using standardized benchmarks, PUZZLED achieves an average attack success rate of 88.8%, with peak performance of 96.5% on GPT-4.1 and 92.3% on Claude 3.7 Sonnet. The approach demonstrates both high effectiveness and efficiency, using minimal model calls while consistently outperforming baseline jailbreak methods across diverse models and categories.

## Method Summary
PUZZLED operates by masking 3-6 keywords in harmful instructions (prioritizing explicitly harmful words, sensitive terms, and high-impact verbs/nouns) and embedding them as puzzle answers in word search, anagram, or crossword formats. The method uses spaCy for POS tagging to identify keywords, applies algorithmic puzzle generation (word search grids, character shuffling, or shared-letter symbol substitution), and generates euphemistic semantic clues using GPT-4o with caching. The final attack prompt combines the masked instruction, puzzle board, and clues, requiring the model to solve the puzzle to reconstruct the harmful instruction and respond. This approach exploits LLM reasoning capabilities while evading surface-level safety pattern detection.

## Key Results
- Achieves 88.8% average attack success rate across five state-of-the-art LLMs
- Outperforms baseline methods while using significantly fewer LLM calls
- Shows peak performance of 96.5% on GPT-4.1 and 92.3% on Claude 3.7 Sonnet
- Demonstrates effectiveness across diverse puzzle types (word search, anagram, crossword)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Puzzle-based prompts exploit LLM reasoning capabilities to bypass safety filters through multi-step reconstruction pathways.
- Mechanism: Word puzzles require sequential reasoning—spatial pattern recognition for word search, character recombination for anagrams, and transitive inference for crosswords. This cognitive load engages problem-solving pathways that appear to override or circumvent safety filter activation during direct content evaluation.
- Core assumption: Safety filters are primarily triggered by surface-level pattern matching rather than multi-step reasoning processes; reasoning mode and safety evaluation mode compete for model capacity.
- Evidence anchors: [abstract] "leverages the LLM's reasoning capabilities" and "The model must solve the puzzle to uncover the masked words"; [Page 5] "PUZZLED forces the LLM to perform multi-step reasoning"; [Page 6] "larger models leverage the word puzzle structure to perform more refined reconstruction"; [corpus] "Large Reasoning Models Are Autonomous Jailbreak Agents."

### Mechanism 2
- Claim: Keyword masking combined with puzzle encoding creates semantic obfuscation that evades token-level safety detection while preserving reconstructability.
- Mechanism: By masking 3-6 keywords and embedding them as puzzle answers, the harmful instruction's surface form becomes unrecognizable to pattern-matching filters. The model receives sufficient clues (word length, POS tags, indirect semantic hints) to reconstruct the instruction, but safety filters never see the harmful keywords in plaintext during input processing.
- Core assumption: Safety filters rely primarily on detecting harmful keywords or phrases in the input prompt rather than evaluating the model's planned output trajectory; the reconstruction process itself is not monitored.
- Evidence anchors: [abstract] "masks keywords in a harmful instruction and presents them as word puzzles"; [Page 3] "obscures toxicity and also prevents the LLM from recognizing the thematic context of the instruction"; [Page 3] "semantic hints are carefully crafted to be euphemistic and indirect."

### Mechanism 3
- Claim: Task framing as puzzle-solving shifts the model into a "completion mode" with different safety activation thresholds than direct instruction mode.
- Mechanism: Using indexed placeholders ([WORD1], [WORD2]) and framing the request as a puzzle task positions the model as a helper completing a game rather than an agent executing harmful instructions. This exploits a gap between "refusing to generate harmful content" and "solving a puzzle that happens to reconstruct harmful content."
- Core assumption: Models have inconsistent safety coverage across different task types; puzzle-solving and instruction-following may invoke different evaluation pathways with different refusal thresholds.
- Evidence anchors: [Page 3] "Since `<MASK>` may prompt the LLM to suspect censorship or harmful content, PUZZLED uses neutral placeholders to frame the task as a puzzle"; [Page 5] "The prompt proposes detailed strategies to achieve the reconstruction objective" without triggering refusal.

## Foundational Learning

- Concept: **Attack Success Rate (ASR) evaluation methodology**
  - Why needed here: The paper reports ASR as the primary metric (88.8% average), calculated via GPT-4o scoring responses on a 1-10 harm scale with threshold ≥7 for "successful jailbreak." Understanding this evaluation protocol is essential for interpreting results and reproducing experiments.
  - Quick check question: Given a response scored 6/10 for harmfulness, would it count toward the ASR?

- Concept: **Keyword masking strategies and priority rules**
  - Why needed here: PUZZLED's effectiveness depends on which words are masked. The paper uses two lists (essential: explicitly harmful words; recommended: contextually sensitive words) plus fallback to longest nouns/verbs. Masking count scales with instruction length (3-6 words). Understanding this hierarchy is critical for implementing the attack pipeline.
  - Quick check question: If an instruction contains "exploit," "database," and "passwords," which gets masked first?

- Concept: **Puzzle-specific constraints and affordances**
  - Why needed here: Each puzzle type has distinct properties affecting both attack success and implementation complexity. Word search requires spatial reasoning but is straightforward to generate; anagrams are simplest but combine all words (increasing difficulty); crosswords provide the strongest inter-word inference but require overlap detection. The paper shows different puzzle types excel on different models (e.g., word search on Claude: 87.7%, anagram on GPT-4.1: 96.5%).
  - Quick check question: Which puzzle type would fail if only one word were masked, and why?

## Architecture Onboarding

- Component map: Keyword Masking Module -> Puzzle Generator -> Clue Generator -> Attack Prompt Assembler
- Critical path:
  1. Input harmful instruction -> keyword masking (rule-based, no LLM calls)
  2. Masked words -> puzzle generation (algorithmic, no LLM calls)
  3. Masked words -> clue lookup (cache hit = no call, cache miss = GPT-4o call)
  4. Assembled prompt -> target LLM -> response
  5. Response -> GPT-4o evaluator -> harm score -> ASR calculation

- Design tradeoffs:
  - **Masking count vs. reconstructability**: More masked words increase obfuscation but decrease model's ability to reconstruct (LLaMA performance drops sharply with >4 masked words; Claude improves with more masking)
  - **Clue directness vs. stealth**: More specific clues aid reconstruction but risk triggering safety filters; euphemistic clues are safer but may fail on complex instructions
  - **Puzzle type vs. model compatibility**: No single puzzle type dominates across all models; crossword requires minimum 2 masked words with shared letters

- Failure signatures:
  - **Low ASR on small models**: LLaMA 3.1 8B shows 74.2% vs. 96.5% on GPT-4.1, suggesting reasoning-based attacks require sufficient model capacity
  - **Performance cliff on crossword with many masked words**: LLaMA crossword ASR drops sharply with increased masking count (Figure 6)
  - **Clue generation failures**: If semantic hints inadvertently expose harmful terms, safety filters may activate

- First 3 experiments:
  1. **Puzzle type ablation**: Run PUZZLED on GPT-4o with all three puzzle types (word search, anagram, crossword) on 20 AdvBench samples each, compare ASR to identify which puzzle type works best on your target model. Expected result: Anagram typically achieves highest ASR on GPT models per Figure 2.
  2. **Masking count sensitivity**: Fix puzzle type to anagram, vary masking count from 1-6 on 20 samples, plot ASR curve. Expected result: Performance varies by model—Claude improves with more masking, LLaMA degrades (Figure 6).
  3. **Baseline comparison**: Implement ArtPrompt and SATA-MLM as baselines, run on same 20 samples, compare ASR and LLM call count to verify PUZZLED's efficiency advantage (Figure 5). Expected result: PUZZLED achieves >80% ASR with <5 LLM calls, while ReNeLLM requires >10 calls for similar ASR.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an adaptive masking strategy that dynamically adjusts the number and selection of masked words based on target model characteristics improve PUZZLED's effectiveness beyond the current length-proportional heuristic?
- Basis in paper: [explicit] The authors state: "The results indicate that setting the number of masked words solely in proportion to instruction length may not be optimal... This supports the need for an adaptive masking strategy that adjusts dynamically to the characteristics of the target model when applying PUZZLED."
- Why unresolved: The current fixed mapping (3-6 masked words based on token count) ignores model-specific reconstruction capabilities; Figure 6 shows Claude improves with more masked words while LLaMA degrades sharply, suggesting no single policy is optimal across models.
- What evidence would resolve it: Experiments comparing adaptive masking (e.g., learning optimal masking counts per model via probing or gradient-based optimization) against the current heuristic, measuring ASR across all target models.

### Open Question 2
- Question: What defense mechanisms can specifically detect or mitigate puzzle-based jailbreak attacks without degrading legitimate puzzle-solving capabilities?
- Basis in paper: [inferred] The paper focuses solely on attack effectiveness and efficiency, with no discussion of defenses against PUZZLED despite reviewing existing defense literature (SmoothLLM, SafeDecoding, etc.).
- Why unresolved: Standard defenses like perturbation-based methods (SmoothLLM) may not detect semantic obfuscation through puzzles; re-writing approaches may preserve the puzzle structure that triggers reasoning pathways.
- What evidence would resolve it: Evaluation of existing defenses against PUZZLED attacks, plus development of specialized detectors that identify puzzle-structured prompts containing masked harmful content.

### Open Question 3
- Question: Why do models with stronger safety filters (e.g., Claude 3.7 Sonnet) show higher vulnerability to PUZZLED compared to simpler baseline attacks?
- Basis in paper: [explicit] The authors note: "These results suggest that LLMs with stronger safety filters may, paradoxically, be more vulnerable to indirect attacks that exploit their reasoning capabilities" (Page 2).
- Why unresolved: The paper demonstrates the phenomenon empirically but does not investigate the underlying mechanism—whether it stems from enhanced reasoning capabilities that aid puzzle-solving, or from safety training that is biased toward surface-form detection.
- What evidence would resolve it: Layer-wise analysis of model activations during puzzle-based attacks, comparing attention patterns and safety-related neuron activations between successful PUZZLED attacks and failed baseline attacks on the same model.

## Limitations

- **Dataset generalizability**: Evaluation is limited to AdvBench (520 harmful instructions) and JBB-Behaviors (100 instructions), which may not represent real-world harmful prompts or novel attack patterns.
- **Clue generation reliability**: PUZZLED depends on GPT-4o for generating euphemistic semantic hints, with potential brittleness if clue quality varies or if GPT-4o generates overly direct hints.
- **Puzzle generation algorithmic completeness**: The puzzle generation algorithms contain underspecified implementation details (grid sizes, retry counts, symbol mapping rules) that could affect reproducibility and attack consistency.

## Confidence

- **High confidence**: The claim that PUZZLED achieves high ASR (88.8% average) across five state-of-the-art LLMs is well-supported by experimental results with clear methodology and consistent performance across models and puzzle types.
- **Medium confidence**: The claim that PUZZLED exploits LLM reasoning capabilities to bypass safety filters is supported by mechanism description and performance patterns, but the exact interaction between reasoning pathways and safety filters remains somewhat speculative.
- **Medium confidence**: The claim about semantic obfuscation through keyword masking is supported by attack design and performance, but the paper doesn't directly measure how safety filters process masked vs. unmasked inputs.

## Next Checks

1. **Cross-dataset generalization test**: Evaluate PUZZLED on 100 harmful prompts from real-world sources (e.g., recent toxic social media content or custom-crafted prompts outside AdvBench/JBB) to assess whether the essential/recommended keyword lists and puzzle-based obfuscation generalize beyond academic benchmarks. Compare ASR drop between AdvBench and real-world prompts.

2. **Safety filter monitoring experiment**: Use a fine-tuned safety classifier to analyze intermediate model states during PUZZLED attacks. Compare token-level safety scores of the original harmful instruction, the masked puzzle prompt, and the model's reconstruction attempts to empirically validate whether safety filters are indeed bypassed through semantic obfuscation rather than just task framing.

3. **Model capacity threshold analysis**: Systematically test PUZZLED across a broader range of model sizes (e.g., 1B, 7B, 13B, 33B parameters) on a fixed set of 50 AdvBench prompts to identify the minimum model capacity required for effective puzzle-based jailbreaking. This would validate the claim that reasoning capabilities are essential for the attack and identify at what scale the attack becomes viable.