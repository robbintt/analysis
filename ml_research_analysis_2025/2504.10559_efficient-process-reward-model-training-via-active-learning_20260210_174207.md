---
ver: rpa2
title: Efficient Process Reward Model Training via Active Learning
arxiv_id: '2504.10559'
source_url: https://arxiv.org/abs/2504.10559
tags:
- data
- process
- training
- zhang
- actprm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of expensive step-level annotation
  costs in training Process Reward Models (PRMs) for mathematical reasoning. The authors
  propose ActPRM, an active learning approach that uses an ensemble PRM to estimate
  uncertainty and selectively annotate only the most informative reasoning steps.
---

# Efficient Process Reward Model Training via Active Learning

## Quick Facts
- arXiv ID: 2504.10559
- Source URL: https://arxiv.org/abs/2504.10559
- Reference count: 8
- Achieves comparable performance to full-data training while reducing annotation costs by 50%

## Executive Summary
This paper addresses the challenge of expensive step-level annotation costs in training Process Reward Models (PRMs) for mathematical reasoning. The authors propose ActPRM, an active learning approach that uses an ensemble PRM to estimate uncertainty and selectively annotate only the most informative reasoning steps. By identifying uncertain data points based on ensemble prediction disagreement and confidence levels, ActPRM significantly reduces annotation costs while maintaining performance. The approach is validated on large-scale mathematical reasoning datasets, demonstrating state-of-the-art results with substantially reduced computational resources.

## Method Summary
The paper introduces ActPRM, an active learning framework for training Process Reward Models that reduces annotation costs by selectively labeling only the most informative reasoning steps. The method uses an ensemble of PRMs to estimate uncertainty through prediction disagreement and confidence scores. During training, the ensemble identifies data points with high uncertainty, which are then labeled by a powerful reasoning model (GPT-4/4o-mini). The authors apply this approach to filter and annotate over 1M+ math reasoning trajectories, retaining 60% of the data while achieving state-of-the-art performance on ProcessBench and PRMBench. The framework balances exploration and exploitation by focusing annotation efforts on steps where the ensemble disagrees or shows low confidence.

## Key Results
- Achieves 75.0% performance on ProcessBench and 65.5% on PRMBench
- Reduces annotation costs by 50% compared to full-data training
- Processes 1M+ math reasoning trajectories while retaining 60% of data
- Outperforms prior models using only 20% of the annotation budget

## Why This Works (Mechanism)
The approach leverages ensemble-based uncertainty estimation to identify the most informative data points for annotation. When an ensemble of PRMs shows high disagreement or low confidence on certain reasoning steps, these points are likely to be most valuable for improving the model. By focusing annotation resources on these uncertain cases rather than uniformly annotating all data, the method achieves better sample efficiency. The use of a powerful reasoning model (GPT-4/4o-mini) for labeling ensures high-quality annotations for the selected data points, while the ensemble structure provides robust uncertainty estimates that guide the active learning process.

## Foundational Learning
- Process Reward Models (PRMs): Learn to evaluate intermediate reasoning steps in mathematical problem-solving. Why needed: Traditional PRMs require expensive step-level annotations; understanding PRMs is crucial for grasping the annotation cost challenge.
- Active Learning: Machine learning paradigm that selectively queries the most informative samples for labeling. Why needed: The core innovation relies on active learning principles to reduce annotation costs while maintaining performance.
- Ensemble Methods: Using multiple models to improve prediction robustness and uncertainty estimation. Why needed: The ensemble PRM is central to the uncertainty estimation mechanism that drives selective annotation.
- Uncertainty Estimation: Techniques for quantifying prediction confidence and disagreement. Why needed: The method's effectiveness depends on accurately identifying uncertain data points through ensemble disagreement metrics.
- Mathematical Reasoning Trajectories: Sequences of reasoning steps used to solve mathematical problems. Why needed: The approach is specifically designed for and evaluated on mathematical reasoning tasks.

## Architecture Onboarding

Component Map:
ActPRM -> Ensemble PRM -> Uncertainty Estimation -> Selective Annotation -> Filtered Dataset -> Final PRM Training

Critical Path:
1. Initial PRM ensemble training on available annotated data
2. Uncertainty estimation through ensemble disagreement and confidence scores
3. Selective annotation of high-uncertainty reasoning steps
4. Training final PRM on filtered, high-quality dataset

Design Tradeoffs:
- Ensemble size vs. computational cost: Larger ensembles provide better uncertainty estimates but increase computation
- Uncertainty threshold selection: Stricter thresholds reduce annotation costs but may miss valuable data
- Model capacity vs. efficiency: More powerful PRMs may require fewer annotations but increase per-annotation cost

Failure Signatures:
- Poor uncertainty estimation leading to selection of uninformative samples
- Overfitting to the filtered dataset due to reduced diversity
- Computational bottleneck during ensemble uncertainty estimation

First Experiments:
1. Compare single PRM vs. ensemble PRM uncertainty estimation quality
2. Ablation study on different uncertainty metrics (disagreement vs. confidence)
3. Test different annotation budget allocations and their impact on final performance

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on proprietary models (GPT-4, GPT-4o-mini) for uncertainty estimation and data generation, limiting reproducibility
- The 50% annotation reduction claim depends on specific ensemble configuration assumptions
- The 60% retention rate may not generalize across different reasoning domains beyond mathematics
- "State-of-the-art" performance is benchmark-specific and may not reflect real-world mathematical reasoning tasks

## Confidence
- Performance claims (75.0% ProcessBench, 65.5% PRMBench): **High**
- 50% annotation cost reduction: **Medium**
- 60% data retention at scale: **Medium**
- Outperformance of prior models: **High**

## Next Checks
1. Test ActPRM with open-source PRM alternatives (e.g., LLaMA, Mistral) to verify if proprietary model dependency is essential for the performance gains
2. Conduct ablation studies removing the ensemble component to quantify its specific contribution to uncertainty estimation quality
3. Evaluate the approach on mathematical reasoning datasets outside ProcessBench and PRMBench to test domain generalization