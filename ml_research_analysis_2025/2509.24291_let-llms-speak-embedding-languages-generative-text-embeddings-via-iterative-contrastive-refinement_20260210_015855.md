---
ver: rpa2
title: 'Let LLMs Speak Embedding Languages: Generative Text Embeddings via Iterative
  Contrastive Refinement'
arxiv_id: '2509.24291'
source_url: https://arxiv.org/abs/2509.24291
tags:
- embedding
- gircse
- question
- user
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GIRCSE introduces a generative embedding framework that leverages
  autoregressive generation to iteratively refine semantic representations. By producing
  soft tokens optimized under a contrastive objective, GIRCSE captures latent concepts
  and implicit semantics that encoder-only methods miss.
---

# Let LLMs Speak Embedding Languages: Generative Text Embeddings via Iterative Contrastive Refinement

## Quick Facts
- arXiv ID: 2509.24291
- Source URL: https://arxiv.org/abs/2509.24291
- Reference count: 30
- Primary result: GIRCSE achieves top 5–6 on MTEB and top 2–3 on instruction-following tasks through iterative contrastive refinement

## Executive Summary
GIRCSE introduces a generative embedding framework that leverages autoregressive generation to iteratively refine semantic representations. By producing soft tokens optimized under a contrastive objective, GIRCSE captures latent concepts and implicit semantics that encoder-only methods miss. The method employs an Iterative Contrastive Refinement (ICR) objective to guide each generation step toward better representations. Experiments show that GIRCSE outperforms strong LLM-based embedding baselines on the MTEB benchmark and instruction-following tasks, achieving top 5–6 on MTEB and top 2–3 on instruction-following. Additionally, GIRCSE exhibits an emergent test-time scaling property: generating more tokens at inference steadily improves embedding quality. These results establish generative iterative refinement as a new paradigm for representation learning.

## Method Summary
GIRCSE repurposes a decoder-only LLM to generate text embeddings through iterative soft token generation. Starting from an input text embedding, the model autoregressively generates K soft tokens, where each token is a probability-weighted combination of vocabulary embeddings rather than a discrete sample. At each step k, the model produces a probability distribution over the vocabulary, computes a soft embedding as a weighted sum, and applies contrastive loss to intermediate representations. The total loss combines stepwise contrastive objectives with a regularization term that encourages monotonic improvement across generation steps. During inference, the number of generated tokens can be extended beyond the training setting (K=5→K=10→K=20) to improve embedding quality.

## Key Results
- GIRCSE achieves 67.83 average score on MTEB, placing in top 5-6 overall
- Outperforms Causal-EOS baseline by +2.4 to +5.7 points across MTEB tasks
- Demonstrates test-time scaling property: embedding quality improves with more generated tokens at inference
- Ranks top 2-3 on instruction-following benchmarks (IntentEmotion, NYTClustering)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Soft token generation preserves gradient flow for end-to-end contrastive training while capturing richer semantics than discrete token sampling.
- **Mechanism**: At each generation step k, the model produces a probability distribution s_k over the vocabulary via softmax, then computes a soft embedding as a convex combination: d_k = Σ s_k,i × e_i. This weighted sum avoids the non-differentiable argmax operation that discrete tokenization would require.
- **Core assumption**: The full probability distribution over vocabulary tokens contains more semantic information than any single sampled token.
- **Evidence anchors**:
  - [section 3.2] "This soft token generation approach offers two advantages: (1) Differentiability: The weighted combination preserves gradients throughout the generation process... (2) Semantic Richness: Rather than collapse the next-token distribution into a single token, soft tokens capture the semantic diversity of the full probability distribution."
  - [section 3.2] Equation 6: d_k = Σ s_k,i × e_i shows the convex combination formulation
  - [corpus] Related work on autoregressive LLM embeddings notes inherent tension between generative token prediction and contrastive semantics, but GIRCSE's soft approach may resolve this through differentiable optimization.
- **Break condition**: If the vocabulary lacks semantic coverage for the target embedding space (e.g., highly specialized domains), soft token combinations may not span the necessary semantic dimensions.

### Mechanism 2
- **Claim**: Stepwise contrastive supervision at every generation step prevents intermediate representations from collapsing into trivial or noisy states.
- **Mechanism**: Rather than supervising only the final embedding after K steps, ICR applies contrastive loss L_k at each step k using intermediate embeddings z_k = P(G_{1:k}). The total loss L_contrast = Σ L_k forces early tokens to capture useful semantics rather than deferring all learning to later steps.
- **Core assumption**: Intermediate representations in an autoregressive process will not automatically become semantically meaningful without explicit supervision.
- **Evidence anchors**:
  - [section 3.3] "supervising only the final embedding (i.e., K-th generation step) might collapse intermediate steps into trivial or noisy representations"
  - [Table 3] Ablation shows stepwise loss (SL) provides gains particularly for classification (+0.6) and summarization (+1.42) tasks
  - [corpus] Explicit corpus evidence for stepwise supervision in generative embeddings is weak; this appears to be a novel contribution of GIRCSE.
- **Break condition**: If generation steps K is very small (e.g., K=1), stepwise supervision provides no additional signal beyond single-pass contrastive learning.

### Mechanism 3
- **Claim**: Iterative refinement regularization enforces monotonic improvement across generation steps, preventing redundant token generation.
- **Mechanism**: The regularization term L_reg = (1/(K-1)) × Σ max(log L_{k+1} - log L_k, 0) penalizes cases where later steps fail to outperform earlier ones. This encourages each generated token to add new semantic information rather than repeat previous signals.
- **Core assumption**: LLMs without such regularization tend to produce highly similar tokens across steps, leading to diminishing returns.
- **Evidence anchors**:
  - [section 3.3] "We empirically observe that simply increasing the number of generation steps does not guarantee improved embedding quality, as LLMs often produce highly similar tokens"
  - [Table 3] Adding iterative refinement (IR) on top of stepwise loss improves average MTEB from 65.69 to 66.27 and instruction following from 60.13 to 62.97
  - [corpus] No direct corpus comparison; regularization for generative embeddings is not well-explored in prior work.
- **Break condition**: If the contrastive task is extremely easy or hard negatives are poorly constructed, the loss surface may become too flat for meaningful stepwise improvement signals.

## Foundational Learning

- **Concept: Contrastive Learning for Embeddings**
  - Why needed here: GIRCSE builds on standard contrastive objectives (InfoNCE-style loss) but applies them stepwise. Understanding how contrastive learning pulls similar pairs closer and pushes dissimilar pairs apart in embedding space is essential before extending it across multiple generation steps.
  - Quick check question: Given a batch with query q, positive document d+, and negative documents {d-}, can you sketch why the loss -log[exp(sim(q,d+)/τ) / Σ exp(sim(q,d_j)/τ)] encourages the model to increase sim(q,d+) relative to sim(q,d-)?

- **Concept: Autoregressive Generation in Decoder-Only LLMs**
  - Why needed here: GIRCSE repurposes the next-token prediction mechanism of LLMs for embedding refinement. Understanding how decoder attention masks work—each position attends only to previous positions—is critical for following the soft token generation loop.
  - Quick check question: In a decoder-only model generating tokens t_1, t_2, t_3, which positions does the attention mask allow t_3 to attend to?

- **Concept: Soft/Differentiable Token Embeddings**
  - Why needed here: The core innovation is avoiding discrete token sampling by computing weighted embeddings. This requires understanding how to maintain gradients through what would normally be a non-differentiable indexing operation.
  - Quick check question: Why does taking argmax over a softmax distribution break gradient flow, and how does computing Σ p_i × e_i instead preserve it?

## Architecture Onboarding

- **Component map**:
  1. Input Processing: Tokenize input text T into tokens, retrieve embeddings X from embedding matrix E
  2. Autoregressive Soft Token Generator (loops K times):
     - Forward pass through transformer decoder f_θ on [X; D] where D is list of previously generated soft embeddings
     - Extract last hidden state h'_{k-1} for next-token prediction
     - Compute soft token distribution s_k via LM head (softmax)
     - Compute soft embedding d_k = Σ s_k,i × e_i
     - Append d_k to D
  3. Pooling: After K steps, collect hidden states G for generated tokens, apply mean pooling to produce final embedding z
  4. Loss Computation: Compute stepwise contrastive loss for each intermediate z_k, plus regularization term

- **Critical path**:
  1. Input tokenization and embedding lookup (standard)
  2. **Soft token generation loop** (novel): K autoregressive forward passes where each step conditions on all previous soft embeddings
  3. Final pooling over generated token hidden states
  4. **ICR loss computation** (novel): Must cache intermediate embeddings z_1...z_K for stepwise contrastive loss
  5. Backpropagation through entire soft generation chain

- **Design tradeoffs**:
  - **Generation length K**: Training with K=5 balances cost and performance; inference can extend to K=20 for quality gains. More tokens improve quality but increase latency ~linearly without KV caching.
  - **Pooling strategy**: Paper uses mean pooling over generated token hidden states; alternative pooling (attention-weighted, last-token) is unexplored.
  - **Training data scale**: 0.2M examples is sufficient; scaling laws for generative embeddings remain unclear.
  - **LoRA vs full fine-tuning**: Paper uses LoRA (rank 64, α=32) for efficiency; full fine-tuning might yield different performance-compute tradeoffs.

- **Failure signatures**:
  1. **Gradient instability**: Monitor gradient norms during training; the soft token chain can accumulate numerical issues. Paper shows stable training (Section G), but this should be verified on new datasets.
  2. **No improvement with more tokens**: If test-time scaling fails (K=10 doesn't beat K=5), check if ICR regularization was properly applied—without it, tokens become redundant.
  3. **Degraded vs. non-generative baseline**: If GIRCSE underperforms Causal-EOS, verify stepwise contrastive loss is applied to all K steps, not just the final embedding.
  4. **Memory overflow**: Autoregressive generation without KV caching requires K forward passes over growing sequences; implement KV caching to reduce FLOPs to ~1.0-1.1× baseline.

- **First 3 experiments**:
  1. **Reproduce single-backbone result**: Train GIRCSE on Mistral-7B with 0.2M samples, K=5 training tokens, evaluate on MTEB. Target: match or exceed reported 67.83 average score. Verify test-time scaling improves from K=5 to K=10 to K=20.
  2. **Ablate ICR components**: Train three variants—(1) generation only without stepwise loss, (2) generation with stepwise loss only, (3) full GIRCSE. Compare on both MTEB and instruction-following benchmarks to quantify contribution of each component.
  3. **Test data efficiency**: Train GIRCSE vs. Causal-EOS baseline on 50K samples only. Paper reports +2.4 to +5.7 point improvements; verify this efficiency gain holds across your target domain.

## Open Questions the Paper Calls Out
None

## Limitations
- Soft token generation assumes vocabulary embeddings span necessary semantic dimensions, which may fail for specialized domains
- Test-time scaling property lacks mechanistic explanation for why more tokens improve quality
- O(K) computational overhead for K generation steps, though KV caching mitigates this
- Novel stepwise contrastive and iterative refinement mechanisms lack extensive external validation

## Confidence
**High Confidence**: MTEB benchmark performance claims relative to established baselines (Causal-EOS, GTE). The methodology is standard and results are reproducible with provided implementation details.

**Medium Confidence**: Instruction-following task results and relative rankings. While the evaluation setup appears sound, the dataset composition and task definitions are less standardized than MTEB.

**Low Confidence**: Test-time scaling property and the specific contribution of ICR regularization. These represent novel empirical observations that require additional validation across different domains and model scales.

## Next Checks
1. **Cross-domain generalization test**: Train GIRCSE on a domain-specific dataset (e.g., biomedical literature or legal documents) and evaluate whether soft token generation maintains semantic coverage when vocabulary is domain-constrained. Compare against discrete token generation baseline.

2. **Component ablation with statistical significance**: Conduct a more rigorous ablation study with multiple random seeds (n=5) to establish statistical significance of stepwise contrastive loss and iterative refinement contributions. Report 95% confidence intervals on performance differences.

3. **Scaling analysis beyond K=20**: Systematically evaluate test-time scaling up to K=50 or until diminishing returns are observed. Plot embedding quality vs. computational cost to establish the practical tradeoff boundary and test whether the monotonic improvement trend continues.