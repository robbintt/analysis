---
ver: rpa2
title: 'ToolMem: Enhancing Multimodal Agents with Learnable Tool Capability Memory'
arxiv_id: '2510.06664'
source_url: https://arxiv.org/abs/2510.06664
tags:
- tool
- memory
- prompt
- tools
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ToolMem, a framework that enables multimodal
  agents to learn and leverage structured memories of tool capabilities through interaction.
  By summarizing tool strengths and weaknesses into proficiency-based categories and
  updating these memories dynamically with feedback, agents can retrieve relevant
  entries to make more accurate tool selections.
---

# ToolMem: Enhancing Multimodal Agents with Learnable Tool Capability Memory

## Quick Facts
- **arXiv ID**: 2510.06664
- **Source URL**: https://arxiv.org/abs/2510.06664
- **Reference count**: 40
- **Primary result**: Agents predict tool performance 14.8–28.7% more accurately and improve optimal tool selection by 21–24% over generic agents.

## Executive Summary
This paper introduces ToolMem, a framework that enables multimodal agents to learn and leverage structured memories of tool capabilities through interaction. By summarizing tool strengths and weaknesses into proficiency-based categories and updating these memories dynamically with feedback, agents can retrieve relevant entries to make more accurate tool selections. Evaluated on text generation and text-to-image generation tasks, ToolMem-augmented agents predict tool performance 14.8–28.7% more accurately and improve optimal tool selection by 21–24% over generic agents. The approach bridges the gap between static tool usage and adaptive, experience-driven tool reasoning, advancing more autonomous and efficient multi-tool agents.

## Method Summary
ToolMem operates by first running a set of tools on training tasks to collect feedback scores. This feedback is then passed through an induction module (GPT-4o) which summarizes each interaction into a capability statement (e.g., "Proficient at +2: Translating French"). These statements are stored in a vector database with embeddings. At runtime, when a new task arrives, the system retrieves the top-k most relevant capability entries and appends them to the agent's prompt. The agent then makes tool selection or performance predictions based on this augmented context. The memory is continuously updated through a refinement process that merges, updates, or deletes entries based on new feedback, preventing redundancy and contradiction.

## Key Results
- ToolMem-augmented agents predict tool performance 14.8% and 28.7% more accurately than generic agents on text generation and text-to-image generation tasks respectively
- Optimal tool selection accuracy improves by 21% and 24% over generic agents across the two task domains
- The structured taxonomy approach outperforms raw few-shot examples by isolating specific behavioral traits rather than storing all interaction logs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Structured memory taxonomies improve tool selection accuracy over raw few-shot examples by isolating specific behavioral traits.
- **Mechanism**: The system categorizes natural language capabilities into proficiency levels (e.g., `proficient at`, `good at`, `bad at`, `weak at`) rather than storing raw interaction logs. When a new task arrives, the agent retrieves relevant entries from these specific categories. This reduces the noise inherent in raw examples (which contain both successes and failures) and focuses the context window on generalized behavioral patterns.
- **Core assumption**: The Language Model (LM) acting as the "Induction Module" can accurately generalize a single interaction into a reliable, standing capability rule, and that these rules transfer to unseen task variations.
- **Evidence anchors**: [abstract] "...summarizing their strengths and weaknesses and storing them in memory..." [section 2.1] "...categorize memory entries based on the varied proficiency levels the agent possesses, namely C = {proficient at(p), good at(g), bad at(b), weak at(w)}."
- **Break condition**: If the Induction Module produces vague summaries (e.g., "Good at art") that fail to distinguish performance on fine-grained tasks (e.g., "Spatial reasoning").

### Mechanism 2
- **Claim**: Dynamic memory updates via Retrieval-Augmented Generation (RAG) prevent redundancy and contradiction.
- **Mechanism**: When new feedback is received, the system retrieves the top-$k$ most semantically similar existing entries. The LM then *refines* this subset—merging, updating, or deleting entries—rather than simply appending new data. This maintains a compact, non-redundant knowledge base, which is critical for context-limited inference.
- **Core assumption**: The refinement prompt successfully resolves logical contradictions between old and new memories (e.g., if a tool improves via an update, the "bad at" memory is removed).
- **Evidence anchors**: [section 2.2] "The refinement process involves adding novel insights, updating incomplete entries, removing redundant information, and merging semantically related entries..." [section 3.2] "...TOOLMEM delivers robust improvements without such regressions, underscoring the value of a compact, induced capability memory over raw example reference."
- **Break condition**: If the retrieval step fails to surface conflicting memories, the update step may hallucinate inconsistencies or retain outdated "facts."

### Mechanism 3
- **Claim**: Performance prediction improves by conditioning the agent on learned constraints rather than parametric knowledge.
- **Mechanism**: Standard agents rely on static descriptions. ToolMem injects the induced capability memory ("Proficient at +2: Translating French") into the prompt. This allows the agent to predict a score or select a tool based on *empirical past failures* rather than the tool's advertised description.
- **Core assumption**: The quality of the feedback signal (human or LLM-judge) is high; garbage feedback leads to garbage memory.
- **Evidence anchors**: [section 3] "TOOLMEM-augmented agents predict tool performance 14.8% and 28.7% more accurately..." [section 4] "...accuracy on unequal pairs climbs from 0.09 (GENERIC) ... to 0.33 with TOOLMEM..."
- **Break condition**: If the agent retrieves irrelevant capabilities for a task (e.g., retrieving "good at text rendering" for a task about object counting), the prediction accuracy degrades.

## Foundational Learning

- **Concept**: Retrieval-Augmented Generation (RAG)
  - **Why needed here**: ToolMem is essentially a specialized RAG system where the "documents" are capability statements. Understanding chunking, embedding (ada-002), and cosine similarity is required to debug why certain memories are retrieved.
  - **Quick check question**: Given a prompt "Draw a red car," would you expect a retrieval system to prioritize "Good at red" or "Bad at cars"?

- **Concept**: LLM-as-a-Judge
  - **Why needed here**: The system relies on automated feedback ($r_t$) to build memory. If the judge (e.g., GPT-4) systematically underrates a specific tool, ToolMem will learn an incorrect bias against it.
  - **Quick check question**: How might you detect if the "Judge" used for training has a bias toward specific output styles?

- **Concept**: Inductive Reasoning
  - **Why needed here**: The core innovation is moving from specific instances (deduction) to general rules (induction). You must understand how LLMs summarize datasets to verify the quality of the generated memory.
  - **Quick check question**: If a tool fails once due to a server timeout, how should the Induction Module frame that in memory to avoid over-generalizing that the tool is "unreliable"?

## Architecture Onboarding

- **Component map**: Vector Store (ChromaDB) -> Induction Module (GPT-4o) -> Task-Solving Agent -> Feedback Loop
- **Critical path**: 1. Initialization: Create proficiency categories (+2 to -2) 2. Batch Ingest: Run training set through tools, get feedback, call Induction Module to populate Vector Store 3. Runtime: Receive Query -> Retrieve top-$k$ entries -> Append to Agent Prompt -> Predict/Select
- **Design tradeoffs**: Granularity vs. Noise: Section A analyzes top-$k$ retrieval. Higher $k$ adds context but eventually adds noise ($k > 14$ degrades performance). The paper finds $k=12$ optimal. Staleness: The system assumes tool capabilities are static. If a tool is updated (e.g., GPT-3.5 → GPT-4), the memory must be flushed or re-induced.
- **Failure signatures**: The "Generic" Drift: If the Induction Module outputs vague statements like "It is a helpful assistant," the memory provides no signal, effectively reverting to the GENERIC baseline. Catastrophic Retrieval: If the retrieval vector space is dense with "Bad at" entries for a capable tool, the agent may refuse to use a valid tool.
- **First 3 experiments**: 1. Verify Baseline Delta: Replicate Table 1 using the provided datasets. Specifically, confirm that GENERIC outperforms FEW-SHOT on smaller models to validate that raw examples confuse small LLMs. 2. Ablate the Taxonomy: Remove the `proficient`/`bad at` categories and store all memories in a flat list. Measure the drop in MAE to quantify the value of the structured taxonomy. 3. Test Memory Editing: Inject a deliberate error into the memory (e.g., "Midjourney is bad at [correct capability]"). Verify if the system self-corrects after processing 5 contradictory positive examples.

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions, suggesting confidence in the approach or focusing on empirical validation over theoretical exploration.

## Limitations
- Assumes tool capabilities remain static over time, which may not hold for frequently updated models like GPT-4 or commercial APIs
- Performance is bounded by the quality of feedback signals—biased or noisy judgments will propagate incorrect memories
- The memory induction process may struggle with ambiguous cases where a single failure doesn't represent a systematic weakness

## Confidence
- **High confidence**: The core mechanism of structured memory taxonomy improving tool selection accuracy is well-supported by experimental results (14.8–28.7% improvement in prediction accuracy)
- **Medium confidence**: The dynamic memory update mechanism via RAG shows promise but relies on assumptions about contradiction resolution that weren't extensively validated
- **Medium confidence**: The performance prediction improvements are demonstrated but may be sensitive to the specific task distributions used in evaluation

## Next Checks
1. **Temporal Stability Test**: Evaluate whether ToolMem maintains accuracy when tool capabilities change over time by simulating tool updates and measuring memory drift
2. **Feedback Quality Sensitivity**: Systematically introduce varying levels of noise in the feedback signal and measure degradation in tool selection performance
3. **Generalization Cross-Domain**: Test ToolMem on completely different tool types (e.g., coding tools, data analysis tools) to verify if the learned capability memory transfers beyond text/image generation domains