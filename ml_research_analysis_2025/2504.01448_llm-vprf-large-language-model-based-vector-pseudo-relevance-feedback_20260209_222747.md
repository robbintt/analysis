---
ver: rpa2
title: 'LLM-VPRF: Large Language Model Based Vector Pseudo Relevance Feedback'
arxiv_id: '2504.01448'
source_url: https://arxiv.org/abs/2504.01448
tags:
- retrieval
- vprf
- feedback
- query
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper extends Vector Pseudo Relevance Feedback (VPRF) from\
  \ BERT-based to LLM-based dense retrievers, introducing LLM-VPRF. The authors adapt\
  \ two VPRF approaches\u2014Average and Rocchio\u2014for use with LLM-generated embeddings\
  \ from PromptReps, RepLLaMa, and LLM2Vec models."
---

# LLM-VPRF: Large Language Model Based Vector Pseudo Relevance Feedback

## Quick Facts
- arXiv ID: 2504.01448
- Source URL: https://arxiv.org/abs/2504.01448
- Reference count: 40
- This paper extends Vector Pseudo Relevance Feedback (VPRF) from BERT-based to LLM-based dense retrievers, introducing LLM-VPRF.

## Executive Summary
This paper extends Vector Pseudo Relevance Feedback (VPRF) from BERT-based to LLM-based dense retrievers, introducing LLM-VPRF. The authors adapt two VPRF approaches—Average and Rocchio—for use with LLM-generated embeddings from PromptReps, RepLLaMa, and LLM2Vec models. Evaluations on BEIR and TREC DL datasets show consistent improvements across models: RepLLaMa improves by 1.6% (R@100) and 1.0% (nDCG@10) on BEIR; LLM2Vec shows stronger gains especially on TREC DL with 6.2% (R@100) and 13.4% (nDCG@10). Efficiency analysis reveals minimal computational overhead, maintaining practical applicability. Results demonstrate that VPRF benefits successfully extend to LLM architectures, establishing it as a robust technique for enhancing dense retrieval regardless of underlying models.

## Method Summary
The paper adapts VPRF to LLM-based dense retrievers by applying vector arithmetic (Average or Rocchio) to the original query embedding using top-$k$ passage embeddings from initial retrieval. The method works with three LLM models: PromptReps (zero-shot), RepLLaMa (supervised), and LLM2Vec (unsupervised). VPRF operates on vector indices rather than generating new tokens, maintaining low computational overhead. The study evaluates performance on BEIR and TREC DL datasets using nDCG@10 and Recall@100 metrics, with hyperparameters including feedback passage count $\kappa$ and combination weights $\alpha$ and $\beta$.

## Key Results
- RepLLaMa improves by 1.6% (R@100) and 1.0% (nDCG@10) on BEIR datasets
- LLM2Vec shows strongest gains on TREC DL with 6.2% (R@100) and 13.4% (nDCG@10)
- VPRF adds only ~0.005s per query, maintaining remarkably low computational overhead
- Zero-shot PromptReps shows performance degradation (-0.4%) with VPRF, highlighting model dependency

## Why This Works (Mechanism)

### Mechanism 1: Centroid-Based Query Drift Correction
- **Claim:** Aggregating the embeddings of top-ranked passages into the query vector shifts the query representation toward dense clusters of relevant information, compensating for vocabulary mismatch.
- **Mechanism:** LLM-VPRF applies vector arithmetic (Average or Rocchio) to the original query embedding $E(Q)$ using the top-$k$ passage embeddings. This effectively "moves" the query vector in the semantic space closer to the centroids of relevant topics identified in the first pass.
- **Core assumption:** The top-$k$ retrieved passages from the initial LLM-based search contain a sufficient signal-to-noise ratio such that their averaged semantic vectors approximate the relevance criteria more accurately than the original query alone.
- **Evidence anchors:** Section 3, Equations 1 and 2 explicitly define the mathematical combination of $E(Q_{original})$ and feedback passage embeddings. Abstract states the technique leverages "initial retrieval results to modify the query embeddings in vector space."
- **Break condition:** If the initial retrieval is poor (low precision in top-$k$), the "centroid" will drift toward non-relevant topics, degrading performance (Query Drift).

### Mechanism 2: Semantic Density Calibration via LLM Embeddings
- **Claim:** The richer semantic representations inherent in LLMs (compared to BERT) provide a more robust vector space where relevance feedback operations are more effective.
- **Mechanism:** Because LLM-based retrievers capture deeper contextual relationships, the vector operations performed during VPRF have higher quality "raw material" to work with. The paper suggests that the "semantic depth" of LLMs translates into better VPRF effectiveness, particularly for supervised models.
- **Core assumption:** The geometry of the LLM embedding space preserves semantic relationships in a way that linear combinations (Rocchio) result in meaningful semantic interpolations rather than vector noise.
- **Evidence anchors:** Section 2.2 posits that LLM embeddings "surpass the capabilities of traditional BERT-based word embeddings," offering "rich semantic representations."
- **Break condition:** If the LLM embeds queries and passages in a non-linear or highly sparse manifold where simple averaging destroys semantic meaning, this mechanism fails.

### Mechanism 3: Non-Generative Efficiency Preservation
- **Claim:** VPRF achieves performance gains comparable to generative query expansion but with significantly lower latency by operating purely on vector indices rather than generating new tokens.
- **Mechanism:** Unlike prompting an LLM to generate expansion terms (which requires autoregressive decoding), VPRF performs a fixed number of matrix additions/scalar multiplications. This bypasses the computational bottleneck of text generation while still refining the query.
- **Core assumption:** Vector operations in high-dimensional space are sufficient to capture relevance signals that would otherwise require explicit text term expansion.
- **Evidence anchors:** Table 3 shows VPRF adds only ~0.005s per query, compared to the baseline cost, maintaining "remarkably low per query computational overhead."
- **Break condition:** If the retrieval system is not compute-bound, the marginal gain in efficiency might not justify the engineering complexity of managing vector indices and feedback loops.

## Foundational Learning

- **Concept: Dense Retrieval & Bi-Encoders**
  - **Why needed here:** LLM-VPRF operates on *dense embeddings* (vectors), not sparse keywords (BM25). You must understand that "retrieval" here is Approximate Nearest Neighbor (ANN) search in vector space, not inverted index lookups.
  - **Quick check question:** Does the system retrieve documents by matching exact keywords or by calculating cosine similarity between the query vector and passage vectors?

- **Concept: Rocchio Algorithm**
  - **Why needed here:** This is the mathematical core of the paper (Eq. 2). Understanding that $\alpha$ keeps the original query intact while $\beta$ injects "relevant" feedback is crucial for tuning.
  - **Quick check question:** If you set $\alpha=1$ and $\beta=0.9$, are you discarding the original query or adding to it? (Answer: Adding to it).

- **Concept: Zero-Shot vs. Supervised Retrievers**
  - **Why needed here:** The paper tests PromptReps (Zero-Shot) vs. RepLLaMa (Supervised). The results differ significantly (Table 2). Understanding the training provenance of the backbone helps explain why one benefits more from VPRF than the other.
  - **Quick check question:** Which model in the study had no task-specific fine-tuning, and how did that impact its stability with VPRF?

## Architecture Onboarding

- **Component map:** Input Query -> LLM Encoder (generates $E(Q)$) -> Faiss Index (stores $E(P)$) -> Retriever (Stage 1: fetches top-$k$ passages) -> VPRF Module (computes $E(Q_{new}) = \alpha E(Q) + \beta \text{Avg}(E(P_{1...k}))$) -> Retriever (Stage 2: fetches final ranked list using $E(Q_{new})$)
- **Critical path:** The generation of the initial embeddings. If the LLM encoder produces poor representations (e.g., via truncation or poor prompting), the VPRF module will simply amplify noise.
- **Design tradeoffs:** BIA vs. Oracle gap is large (up to 8.1%); dataset-specific hyperparameter tuning may be needed. Depth ($k$) higher $k$ introduces more noise but potentially more recall signal.
- **Failure signatures:** Performance Degradation on Zero-Shot Models (Table 2 shows PromptReps BIA scores *decreased* (-0.4%)). Drift on Short Queries: Short, ambiguous queries might be pulled toward dominant but irrelevant topics in the top-$k$.
- **First 3 experiments:** 1) Baseline Establishment: Run RepLLaMa or LLM2Vec on a validation set without VPRF to establish $E(Q)$ quality. 2) Parameter Sweep ($k$): Fix $\alpha=1, \beta=0.5$ and vary $k \in \{1, 3, 5, 10\}$. Plot Recall@100. 3) Efficiency Budgeting: Measure the latency of the vector addition operation (Eq 1/2) against your latency SLA.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can feedback passage selection strategies be optimized to close the performance gap between Best-In-Average (BIA) and Oracle results?
- **Basis in paper:** [explicit] The Conclusion explicitly states that "Future research should focus on developing better feedback selection strategies." Additionally, Section 5.2 highlights that PromptReps exhibits a large gap (up to 8.1%) between BIA and Oracle performance.
- **Why unresolved:** The current study relies on standard pseudo-relevance feedback (top-k passages) which likely includes non-relevant documents that degrade the "average" performance case, whereas Oracle results prove better vectors exist if selection were perfect.
- **What evidence would resolve it:** A study introducing a mechanism to filter or weight feedback passages before vector averaging, resulting in BIA scores that statistically significantly approach Oracle levels.

### Open Question 2
- **Question:** Does LLM-VPRF effectiveness generalize to a broader range of LLM architectures and real-world deployment scenarios?
- **Basis in paper:** [explicit] The Conclusion identifies the limitation that the study is restricted to "evaluation of only three LLM-based retrievers and controlled environment testing," suggesting the need to evaluate "more diverse LLM architectures and deployment scenarios."
- **Why unresolved:** The current results are specific to RepLLaMa, LLM2Vec, and PromptReps on BEIR/TREC datasets; behavior on other architectures (e.g., encoder-decoder) or in production environments remains unknown.
- **What evidence would resolve it:** Experiments applying VPRF to alternative model families (e.g., GPT-based or Mistral variants) and diverse, noisy real-world data distributions.

### Open Question 3
- **Question:** Can incorporating sparse representations alongside dense embeddings improve VPRF performance for models capable of generating both?
- **Basis in paper:** [inferred] Section 4.3 notes that for PromptReps, "we exclusively utilize the dense representation component to maintain consistency," despite the model generating both dense and sparse representations.
- **Why unresolved:** It is unclear if the discarded sparse lexical information could complement the dense semantic vectors to mitigate "query drift" or improve precision in specific domains.
- **What evidence would resolve it:** A hybrid VPRF approach that fuses weighted sparse and dense feedback vectors, showing improvements over the dense-only baseline.

## Limitations

- Performance degradation observed with PromptReps (zero-shot) suggests VPRF stability depends heavily on the quality of initial embeddings, which varies by model architecture.
- The study relies on brute-force Faiss indexing rather than approximate methods, potentially limiting scalability to production workloads.
- Evaluation focuses primarily on English retrieval tasks, with limited discussion of multilingual or domain-specific applications.

## Confidence

- **High Confidence**: Claims about VPRF's computational efficiency (negligible latency overhead) and its consistent positive impact on RepLLaMa and LLM2Vec models are well-supported by the experimental data.
- **Medium Confidence**: The assertion that LLM embeddings provide "richer semantic representations" benefiting VPRF is supported by observed performance gains but lacks direct geometric or semantic analysis.
- **Low Confidence**: The generalizability of optimal hyperparameters across different domains or retrieval tasks is uncertain, given the large BIA vs. Oracle gaps (up to 8.1%).

## Next Checks

1. **Cross-Domain Testing**: Evaluate VPRF performance on non-English or specialized domain datasets to assess generalizability beyond BEIR and TREC DL.
2. **Approximate Index Scaling**: Replace brute-force Faiss with HNSW indexing to measure VPRF's practical scalability while monitoring recall degradation.
3. **Prompt Sensitivity Analysis**: Systematically vary the encoding prompts for LLM retrievers to quantify how prompt quality affects VPRF stability and performance.