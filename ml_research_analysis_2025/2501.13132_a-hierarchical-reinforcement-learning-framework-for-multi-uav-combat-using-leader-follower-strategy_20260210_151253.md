---
ver: rpa2
title: A Hierarchical Reinforcement Learning Framework for Multi-UAV Combat Using
  Leader-Follower Strategy
arxiv_id: '2501.13132'
source_url: https://arxiv.org/abs/2501.13132
tags:
- combat
- aircraft
- target
- uni00000033
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses high-dimensional action space and cooperation
  challenges in multi-UAV air combat using a hierarchical reinforcement learning framework
  with a leader-follower strategy. The method decomposes the decision-making process
  into three levels: a top-level policy selector that assigns roles and optimizes
  value functions for cooperation, a middle level that generates desired action angles,
  and a bottom level that translates these into specific commands.'
---

# A Hierarchical Reinforcement Learning Framework for Multi-UAV Combat Using Leader-Follower Strategy

## Quick Facts
- arXiv ID: 2501.13132
- Source URL: https://arxiv.org/abs/2501.13132
- Reference count: 40
- Key outcome: Hierarchical reinforcement learning framework for 6v6 multi-UAV air combat using leader-follower strategy, achieving superior performance over MAPPO, PPO, QMIX, and VDN baselines.

## Executive Summary
This paper presents a hierarchical reinforcement learning framework for multi-UAV air combat that addresses the challenges of high-dimensional action spaces and agent cooperation. The framework decomposes decision-making into three levels: a top-level policy selector for role assignment, a middle level for generating desired action angles, and a bottom level for translating angles into specific 6-DOF control commands. The method incorporates a leader-follower strategy where value functions are optimized asymmetrically, and includes a posture-aware target selector. Evaluated in 6v6 combat simulations, the approach demonstrates superior performance with higher rewards and win rates compared to multiple baselines, while enabling emergent cooperative behaviors such as bait-and-strike strategies.

## Method Summary
The method addresses multi-UAV combat by decomposing the problem into a three-level hierarchical framework. The top level is a policy selector that assigns roles and optimizes value functions using a leader-follower strategy. The middle level generates desired action angles based on target-approaching, offensive, or defensive sub-policies. The bottom level translates these angles into specific 6-DOF commands. A target selector evaluates threat levels using posture and distance metrics. The entire system is trained using a modified MAPPO algorithm within a JSBSim simulation environment. The approach reduces the complexity of learning in high-dimensional continuous action spaces while promoting coordinated team behaviors.

## Key Results
- Hierarchical framework achieved higher average rewards and win rates than MAPPO, PPO, QMIX, and VDN in 6v6 combat simulations
- Demonstrated effective emergent cooperative behaviors including bait-and-strike strategies
- Showed improved mission effectiveness compared to non-hierarchical approaches with superior tactical positioning

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical decomposition of the action space enables effective learning in high-dimensional 6-DOF environments. The framework divides decision-making into three levels: a top-level policy selector for macro-strategy, a middle level generating desired action angles, and a bottom level translating angles into specific 6-DOF control commands. This abstraction reduces the complexity of the action space that any single policy network must learn. The core assumption is that the task can be effectively decomposed into high-level tactical decisions and low-level control execution without significant loss of optimality. Evidence includes the framework description and corpus confirmation that HRL is standard for high-dimensional action spaces. Break condition: If the middle-level action space is still too complex or if translation from angles to 6-DOF controls is lossy, the hierarchy will fail to produce coordinated behavior.

### Mechanism 2
The leader-follower value function optimization promotes emergent cooperative strategies. The method assigns distinct roles where the leader's value function is updated based on its own reward, while the follower's value function is updated based on its own reward and conditioned on the leader's action. This asymmetric value estimation, optimized via a modified MAPPO objective, incentivizes followers to take actions that complement the leader's strategy. The core assumption is that followers can reliably infer or are provided with the leader's intended action. Evidence includes the abstract description and the specific value update equations. Break condition: If the leader's policy is unstable or if communication/action inference fails, followers cannot effectively estimate leader utility.

### Mechanism 3
A posture-aware target selector improves tactical decision-making. Instead of simply targeting the nearest enemy, the target selector scores threats using a function that combines position attributes (distance) and posture attributes (angle off/target angle). This provides the policy with a more informative state representation regarding threats. The core assumption is that threat level is a function of both distance and relative posture, and a weighted combination of these factors is a sufficient heuristic for threat assessment. Evidence includes the abstract description and section detailing the threat scoring mechanism. Break condition: If the threat assessment heuristic is poor, the policy will be guided by incorrect target priorities.

## Foundational Learning

- **Hierarchical Reinforcement Learning (HRL)**: Needed to address the high-dimensional action space for multi-UAV combat in 6-DOF. HRL decomposes the problem into simpler sub-tasks (policy selection, angle generation, control execution). Quick check: Can you explain why decomposing a policy into high-level goals (angles) and low-level execution (commands) makes the learning problem more tractable?

- **Multi-Agent Proximal Policy Optimization (MAPPO)**: The base learning algorithm for the cooperative team. PPO provides stable policy updates, and the multi-agent variant allows agents to learn coordinated policies using global information during training. Quick check: How does MAPPO balance the need for global information during training with decentralized execution during deployment?

- **Leader-Follower Coordination**: A coordination paradigm where one agent guides the team's strategy, creating structured dependency that reduces multi-agent credit assignment complexity. Quick check: In the paper's formulation, how is the follower's value function update different from the leader's, and what is the intended effect of this difference?

## Architecture Onboarding

- **Component map**: State -> Target Selector (Threat Score) -> Policy Selector (Leader/Follower Sub-Policy Choice) -> Sub-Policy (Desired Angle Generation) -> Bottom Level (6-DOF Command) -> JSBSim Environment -> Next State & Reward

- **Critical path**: The complete decision-making pipeline from raw state observations through target threat assessment, role assignment, tactical angle generation, and low-level control execution to environment interaction.

- **Design tradeoffs**:
  - Hierarchy Depth vs. End-to-End Learning: Three-level hierarchy simplifies each level's task but introduces potential sub-optimality from abstraction
  - Discrete Policies vs. Continuous Actions: Policy selector chooses discrete sub-policies while action space is continuous, simplifying high-level decisions but limiting tactical flexibility
  - Leader-Follower Symmetry: Asymmetric value updates promote cooperation but make the system dependent on a competent leader policy

- **Failure signatures**:
  - Looping/Circling Behavior: Agents fly in loops without engaging, indicating policy selector failure to choose aggressive sub-policies
  - "Laziness" in Followers: Followers over-rely on the leader and fail to act when leader guidance is unclear
  - Sub-optimal Target Selection: UAVs consistently ignore high-threat targets in favor of closer, low-threat ones

- **First 3 experiments**:
  1. Baseline Comparison in 6v6 Combat: Replicate experiment comparing LFMAPPO against MAPPO, PPO, VDN, and QMIX, tracking average return and win rate over 150x100 episodes
  2. Ablation on Key Modules: Run ablations removing policy selector and leader-follower value updates, comparing their average returns to full LFMAPPO
  3. Target Selector Analysis: Compare proposed posture-aware target selector against "random" and "nearest-distance" selectors, measuring impact on average return

## Open Questions the Paper Calls Out
- How can agent laziness resulting from relative positioning control be effectively mitigated within the hierarchical framework? The conclusion states that future work must address "the issue of agent laziness caused by relative positioning control."
- Does the LFMAPPO framework maintain superior performance when scaled to combat scenarios involving significantly more than six agents per side? The evaluation is restricted to 6v6 simulations.
- Can the framework effectively generalize to physically heterogeneous UAV teams where aerodynamic dynamics differ between leaders and followers? While the problem description mentions "heterogeneous UAVs," the simulation relies on a homogeneous F-16 model for all agents.

## Limitations
- Implementation details remain unspecified, particularly regarding network architecture, exact reward function coefficients, and hierarchical execution frequency
- Leader-follower value function modification lacks direct comparison to alternative multi-agent credit assignment methods in the literature
- Performance evaluation is limited to 6v6 scenarios, leaving scalability to larger teams uncertain

## Confidence

- **High Confidence**: The hierarchical decomposition approach effectively addresses the high-dimensional action space problem, as supported by both theoretical justification and comparative results against flat RL baselines
- **Medium Confidence**: The leader-follower value function optimization promotes cooperation, though the specific mechanism's effectiveness relative to other multi-agent coordination approaches remains unclear
- **Medium Confidence**: The posture-aware target selector improves tactical decision-making, though the threat assessment heuristic's optimality is not rigorously validated

## Next Checks

1. Baseline Ablation Study: Run the same 6v6 combat scenarios with LFMAPPO, MAPPO, PPO, VDN, and QMIX baselines, measuring both average reward and win rate across 150Ã—100 episodes to confirm the reported performance advantages

2. Target Selector Impact Analysis: Implement ablations comparing the proposed posture-aware target selector against "random" and "nearest-distance" selectors in isolation, measuring the impact on average return to quantify the contribution of this specific component

3. Leader-Follower Value Update Verification: Implement ablations removing the leader-follower asymmetric value function updates (Eq. 3) while keeping the hierarchical structure intact, to isolate the specific contribution of this mechanism to cooperative behavior