---
ver: rpa2
title: 'DialectalArabicMMLU: Benchmarking Dialectal Capabilities in Arabic and Multilingual
  Language Models'
arxiv_id: '2510.27543'
source_url: https://arxiv.org/abs/2510.27543
tags:
- arabic
- dialectal
- language
- dialects
- dialect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DialectalArabicMMLU, a new benchmark for
  evaluating large language models (LLMs) across Arabic dialects. While recent Arabic
  benchmarks have advanced evaluation for Modern Standard Arabic (MSA), dialectal
  varieties remain underrepresented despite their prevalence in everyday communication.
---

# DialectalArabicMMLU: Benchmarking Dialectal Capabilities in Arabic and Multilingual Language Models

## Quick Facts
- arXiv ID: 2510.27543
- Source URL: https://arxiv.org/abs/2510.27543
- Reference count: 0
- Primary result: Introduces DialectalArabicMMLU benchmark with 15K manually translated Arabic dialect QA pairs across 32 domains, revealing substantial performance gaps between MSA and dialects in 19 evaluated LLMs

## Executive Summary
This paper introduces DialectalArabicMMLU, a comprehensive benchmark for evaluating large language models across Arabic dialects. While Modern Standard Arabic (MSA) has been well-represented in recent Arabic benchmarks, dialectal varieties remain underrepresented despite their prevalence in everyday communication. The benchmark extends the MMLU-Redux framework through manual translation of 3K multiple-choice question-answer pairs into five major dialects (Syrian, Egyptian, Emirati, Saudi, and Moroccan), enabling systematic assessment of LLM reasoning and comprehension beyond MSA. Evaluation of 19 open-weight Arabic and multilingual LLMs reveals substantial performance variation across dialects, with models showing significant accuracy drops when handling dialectal Arabic compared to MSA and English.

## Method Summary
The benchmark translates 3,135 English MCQA pairs from MMLU-Redux-v2 across 32 domains into five Arabic dialects through a manual pipeline involving native speaker translation, review, and adjudication. The resulting 15K QA pairs enable evaluation using log-likelihood scoring (LM-Eval-Harness) in three settings: default (no dialect cue), oracle (dialect specified), and dialect identification (6-way classification). The study evaluates 19 open-weight models (1B-13B parameters) across these configurations, reporting accuracy and statistical significance across multiple runs.

## Key Results
- Models show 4-15 percentage point accuracy drops across dialects compared to MSA (51.9%) and English (62.8%)
- Oracle dialect conditioning consistently degrades performance across all dialects (-1.9 to -2.3 average)
- Translation to English yields +5.6 average accuracy gain (statistically significant, p=0.002); translation to MSA yields only +0.3 (not significant, p=0.24)
- Dialect identification performance remains near-random chance for most models (e.g., UAE at 0.0-5.8%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dialectal performance gaps stem from training data distribution, not architectural limitations.
- Mechanism: Models trained predominantly on MSA and high-resource languages lack sufficient dialectal exposure; the 4-15 percentage point drops reflect lexical and morphological coverage gaps rather than reasoning failures.
- Core assumption: The parallel benchmark structure isolates linguistic competence from domain knowledge.
- Evidence anchors:
  - [abstract]: "Models show significant performance drops when handling dialectal Arabic compared to MSA and English, with accuracy ranging from 34.5% to 58.7% across dialects versus 51.9% for MSA and 62.8% for English on average."
  - [Section 5.1]: "The performance consistently declines across all dialects compared to MSA and English, and this trend holds consistently across all Arabic-enabled LLMs we evaluated."
  - [corpus]: Weak direct corpus evidence on training data composition; assumption based on performance patterns.
- Break condition: If models with explicit dialectal pre-training show similar drops, the mechanism shifts to representation capacity rather than data distribution.

### Mechanism 2
- Claim: Explicit dialect conditioning (oracle prompts) degrades rather than improves performance.
- Mechanism: Models cannot effectively use dialect labels because their internal representations do not maintain distinct dialectal subspaces; priming introduces noise without adding actionable signal.
- Core assumption: Dialect labels provide no latent benefit without corresponding fine-grained dialectal representations.
- Evidence anchors:
  - [Section 5.2]: "Interestingly, the performance on the oracle was worse for all dialects and this difference was statistically significant."
  - [Table 7]: All dialects show negative accuracy differences (-1.9 to -2.3 average) in oracle vs. default setting.
  - [corpus]: Corpus shows related work on dialectal ASR struggles with similar generalization issues, supporting representation gaps.
- Break condition: If instruction-tuned models with dialect-aware fine-tuning show positive oracle gains, the mechanism would indicate training-alignment failure rather than representation failure.

### Mechanism 3
- Claim: Translating dialectal content to English yields larger gains than translating to MSA.
- Mechanism: English serves as a stronger pivot because models have richer English representations; MSA translation introduces compounding errors (dialect→English→MSA pipeline) without equivalent representational benefits.
- Core assumption: Translation quality is sufficiently high that gains reflect target-language competence rather than translation artifacts.
- Evidence anchors:
  - [Section 5.3, Table 9]: Translation to English yields +5.6 average accuracy gain (statistically significant, p=0.002); translation to MSA yields only +0.3 (not significant, p=0.24).
  - [Section 5.3]: "One potential explanation is that translation errors that occur when translating to English cause more errors when translating to MSA."
  - [corpus]: "Advancing Dialectal Arabic to Modern Standard Arabic Machine Translation" notes significant lexical/syntactic divergences between DA and MSA, supporting pivot-language fragility.
- Break condition: If direct dialect→MSA translation (without English pivot) shows larger gains, the mechanism would implicate pipeline error accumulation rather than representation strength.

## Foundational Learning

- Concept: **Diglossia and Arabic linguistic structure**
  - Why needed here: Understanding why MSA benchmarks inadequately represent real-world Arabic usage; dialects diverge in morphology, syntax, and lexicon despite shared script.
  - Quick check question: Why does MSA performance not predict dialectal performance?

- Concept: **Log-likelihood evaluation for MCQA**
  - Why needed here: The benchmark uses log-likelihood scoring rather than generation; each answer option is appended and scored by probability, requiring understanding of discriminative vs. generative evaluation.
  - Quick check question: How does log-likelihood evaluation differ from prompting for generated answers?

- Concept: **Jaccard similarity for lexical variation**
  - Why needed here: Table 1's similarity metrics reveal dialect distances (MAG most distant from MSA at 0.41; KSA closest at 0.70), contextualizing why performance varies by dialect.
  - Quick check question: Why might Moroccan Arabic (MAG) show different error patterns than Saudi (KSA)?

## Architecture Onboarding

- Component map: MMLU-Redux-v2 (3,135 English QA pairs) -> LSP manual translation (native review/adjudication) -> 5 dialects (21,945 total pairs) -> LM-Eval-Harness with custom MMLU config -> 3 evaluation modes (default/oracle/dialect-ID) -> log-likelihood scoring

- Critical path:
  1. Load QA pairs for target dialect from DialectalArabicMMLU
  2. Format prompt per evaluation setting (default/oracle/dialect-ID)
  3. Compute log-likelihood for each answer option
  4. Select highest-scoring option; compare to ground truth
  5. Average accuracy across 5 runs × 32 domains

- Design tradeoffs:
  - Human translation (high quality, slow, expensive) vs. machine translation with post-editing (faster, lower fidelity)
  - 5-dialect coverage (manageable scale) vs. full dialectal spectrum (comprehensive but resource-intensive)
  - MCQA format (comparable to MMLU, limited to recognition) vs. generative tasks (broader capability assessment)

- Failure signatures:
  - Oracle setting causes accuracy drops: indicates model lacks dialect-specific representations
  - Near-random dialect ID performance (e.g., UAE at 0.0-5.8% in Table 6): indicates dialect confusion in training data
  - Large variance between dialects on same model: indicates uneven dialectal exposure

- First 3 experiments:
  1. **Baseline replication**: Run default evaluation on your model across all 5 dialects + MSA + English; compute accuracy gaps to establish dialect sensitivity.
  2. **Dialect ablation**: Test whether few-shot prompting with dialect examples closes the gap better than oracle conditioning.
  3. **Translation robustness**: Compare direct dialect→English translation vs. dialect→MSA→English cascade to quantify pivot-language error accumulation.

## Open Questions the Paper Calls Out

- **Question**: How do state-of-the-art proprietary models (e.g., GPT-4) perform on dialectal reasoning tasks compared to the open-weight models evaluated?
  - Basis in paper: [explicit] The authors note that experiments are "limited to open-weight models" and "Results for larger proprietary models... remain to be explored."
  - Why unresolved: Access restrictions and high API costs often prevent the systematic academic benchmarking of closed-source frontier models on specialized linguistic datasets.
  - What evidence would resolve it: Evaluation of frontier models (e.g., GPT-4, Claude 3.5) on the DialectalArabicMMLU dataset using the described log-likelihood or generative protocols.

- **Question**: Can fine-tuning on DialectalArabicMMLU effectively improve LLM reasoning capabilities across diverse Arabic varieties?
  - Basis in paper: [explicit] The authors state they "envision the benchmark serving as a foundation for fine-tuning and adaptation, encouraging the development of LLMs that can reason and communicate effectively."
  - Why unresolved: The current study focuses exclusively on zero-shot or few-shot evaluation of pre-trained models, without testing model plasticity or adaptation via training.
  - What evidence would resolve it: Experiments fine-tuning Arabic LLMs on the dataset and measuring performance deltas on both dialectal QA tasks and general MSA benchmarks to check for transfer learning.

- **Question**: Does explicit dialect conditioning (priming) harm performance due to training data imbalances or attention mechanism failures?
  - Basis in paper: [explicit] The authors report that "priming the model on the dialect label caused degradation" in the Oracle setting, contrary to the intuition that it should help.
  - Why unresolved: The paper reports the phenomenon but does not investigate the underlying model mechanisms causing the negative impact of dialect labels.
  - What evidence would resolve it: Mechanistic interpretability studies or ablation studies analyzing attention weights when dialect tags are injected into the prompt.

## Limitations

- The dialectal translations themselves are not publicly available, preventing independent validation of translation quality across the 15K QA pairs
- The evaluation methodology relies on log-likelihood scoring rather than generation, which may not capture all aspects of language understanding
- The corpus analysis reveals limited direct evidence for training data composition claims; performance gaps are primarily inferred from pattern observation

## Confidence

- **High Confidence**: The existence of significant performance gaps between MSA and dialectal varieties (accuracy ranges from 34.5% to 58.7% across dialects vs. 51.9% for MSA and 62.8% for English). This claim is directly supported by Table 5 data and the consistent patterns across 19 evaluated models.
- **Medium Confidence**: The interpretation that these gaps primarily reflect training data distribution rather than architectural limitations. While the parallel benchmark structure supports this inference, the lack of training corpus analysis means this remains an educated inference rather than a proven mechanism.
- **Low Confidence**: The claim that oracle dialect conditioning degrades performance due to lack of distinct dialectal representations. This interpretation assumes the negative effects are not due to prompt formatting issues, tokenization artifacts, or other experimental confounds not controlled for in the study.

## Next Checks

1. **Translation Quality Validation**: Obtain access to the dialectal translation corpus and conduct independent quality assessment through native speaker evaluation and comparison with machine translation baselines. This would validate whether the 4-15 percentage point performance gaps reflect genuine linguistic challenges or translation artifacts.

2. **Training Data Analysis**: Analyze the training corpora of the evaluated models to quantify actual dialectal content exposure. This would directly test whether performance gaps correlate with training data distribution or other factors like model architecture, fine-tuning approaches, or representation capacity.

3. **Direct Dialect-to-MSA Translation Experiment**: Conduct head-to-head comparison of direct dialect→MSA translation versus dialect→English→MSA pipelining to isolate whether the +0.3 gain (p=0.24) reflects fundamental representation differences or compounding translation errors in the pivot approach.