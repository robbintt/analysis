---
ver: rpa2
title: Divergence-Based Similarity Function for Multi-View Contrastive Learning
arxiv_id: '2507.06560'
source_url: https://arxiv.org/abs/2507.06560
tags:
- similarity
- learning
- views
- loss
- moco
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a divergence-based similarity function (DSF)
  for multi-view contrastive learning that explicitly captures joint structure among
  augmented views by modeling each set of views as a probability distribution on the
  unit hypersphere and measuring similarity via negative KL divergence between distributions.
  Unlike existing methods that rely on pairwise relationships, DSF integrates multi-views
  into a single collective loss formulation without additional computational overhead.
---

# Divergence-Based Similarity Function for Multi-View Contrastive Learning

## Quick Facts
- arXiv ID: 2507.06560
- Source URL: https://arxiv.org/abs/2507.06560
- Reference count: 19
- Primary result: DSF achieves 2-3× faster convergence than baselines while improving kNN accuracy by 1.7-2.6% on CIFAR-10/ImageNet-100

## Executive Summary
This paper introduces a divergence-based similarity function (DSF) for multi-view contrastive learning that models each set of augmented views as a von Mises-Fisher distribution on the unit hypersphere. By measuring similarity via negative KL divergence between distributions, DSF captures joint structure across all views simultaneously without decomposing into pairwise terms. The method eliminates the need for temperature hyperparameter tuning while achieving consistent performance improvements across diverse tasks including kNN classification (ImageNet-100: 79.00%, CIFAR-10: 90.04%), linear evaluation (ImageNet-100: 85.80%, CIFAR-10: 91.21%), transfer learning, and robustness to distribution shifts (CIFAR-10-C: 16.20% mCE, ImageNet-100-C: 33.15% mCE).

## Method Summary
The method represents each set of m augmented views as a von Mises-Fisher (vMF) distribution parameterized by mean direction μ and concentration κ. Similarity between two view sets is computed as the negative KL divergence between their vMF distributions. The approach is integrated into the MoCo framework with M=8 total views (m=4 per encoder branch), using InfoNCE loss with the divergence-based similarity. Key technical innovations include κ stabilization (dividing by feature dimension and scaling resultant length) to prevent numerical instability, and the elimination of temperature tuning through the unbounded nature of KL divergence.

## Key Results
- kNN accuracy: CIFAR-10 (+2.6%), ImageNet-100 (+1.7%) improvement over baselines
- Linear evaluation: CIFAR-10 (+1.21%), ImageNet-100 (+0.80%) improvement
- Robustness: CIFAR-10-C 16.20% mCE, ImageNet-100-C 33.15% mCE
- Convergence: 2-3× faster than baselines while requiring no temperature tuning

## Why This Works (Mechanism)

### Mechanism 1: Distribution-Based Joint Structure Capture
Representing multiple augmented views as a von Mises-Fisher distribution captures joint structural relationships across all views simultaneously, rather than decomposing into pairwise terms. Each set of m augmented views is mapped to a vMF distribution on the unit hypersphere, parameterized by mean direction μ (central tendency) and concentration κ (joint structure), allowing the similarity function to reason about the entire collection of views as a coherent whole.

### Mechanism 2: Information-Theoretic Similarity via KL Divergence
Using negative KL divergence between vMF distributions provides a principled similarity measure that captures both alignment (μ) and compactness (κ) differences between view sets. The KL divergence D_KL(D_i || D_j) decomposes into three terms involving log ratios of concentrations, Bessel functions, and the interaction between concentration and mean direction dot product, providing a comprehensive distributional comparison.

### Mechanism 3: Inherent Scaling via Concentration Parameter
DSF eliminates temperature tuning because the concentration parameter κ provides automatic, adaptive scaling of similarity margins as training progresses. Unlike cosine similarity bounded to [-1, 1], the KL divergence is unbounded, and as features align better during training, κ increases naturally, amplifying the positive-negative margin without manual temperature adjustment.

## Foundational Learning

- **von Mises-Fisher (vMF) distribution**: Core representation for multi-view features; understanding μ and κ is essential for debugging. *Quick check*: Given 4 unit vectors on a hypersphere with average resultant length Ŕ=0.8, what does a high κ value indicate about their clustering?

- **Kullback-Leibler divergence**: Mathematical foundation of the similarity function; must understand asymmetry and bounds. *Quick check*: If D_KL(D_i || D_j) = 0, what can you conclude about μ_i vs. μ_j and κ_i vs. κ_j?

- **InfoNCE loss with temperature**: Understanding why cosine similarity requires temperature helps motivate DSF's design. *Quick check*: With 4096 negative pairs and τ=1, why does InfoNCE with cosine similarity produce suboptimal loss values even at optimal s+=1, s-=-1?

## Architecture Onboarding

- **Component map**: Augmentation pipeline → Encoder + projection head → vMF estimator → κ stabilizer → KL divergence module → InfoNCE loss
- **Critical path**: Augmentation (M views) → Encoder (m features per branch) → vMF estimation (μ, κ per branch) → κ stabilization → KL divergence → InfoNCE
- **Design tradeoffs**: 
  1. View count vs. batch size: Fixed GPU memory constraint; M=8 requires 4× smaller batch than M=2
  2. Convergence speed vs. per-epoch cost: DSF converges 2-3× faster but each epoch processes fewer unique images
  3. Numerical precision: Bessel function approximation (k=5 truncation) trades some accuracy for speed
- **Failure signatures**:
  1. NaN/Inf losses: κ exploding → check stabilization; ensure λ_R̄=0.95 applied
  2. Performance plateaus below baseline: Likely temperature accidentally introduced (τ≠1)
  3. kNN good but linear eval poor: vMF estimation working but features not discriminative
- **First 3 experiments**:
  1. Two-view equivalence test: Set M=2, verify DSF matches MoCo baseline within 0.5% accuracy
  2. Stabilization ablation: Train with M=8 on CIFAR-10, compare stabilization variants
  3. View scaling sweep: Train M∈{2,4,6,8} with matched compute, plot kNN accuracy vs. epochs

## Open Questions the Paper Calls Out

1. **Alternative f-divergences**: Can Rényi, χα, or Jensen–Shannon divergences outperform the KL divergence used in DSF? The paper identifies these as promising directions for future research.

2. **Theoretical generalization bounds**: What are the theoretical generalization bounds and convergence guarantees for DSF in high-dimensional regimes? The authors call for deeper theoretical study of DSF's generalization behavior and convergence.

3. **Robustness mechanisms**: What specific theoretical properties explain DSF's superior robustness and transferability compared to cosine similarity? The paper suggests theoretical analysis may clarify why DSF exhibits superior robustness.

## Limitations

- **Computational complexity**: While claiming "no additional computational overhead," vMF estimation and KL divergence computation introduce non-trivial complexity compared to single-view methods.
- **View count sensitivity**: DSF requires M≥4 views for consistent gains, limiting applicability when computational resources constrain view generation.
- **Distribution assumption fragility**: The vMF distribution assumption breaks down with aggressive augmentations that destroy semantic content, potentially limiting robustness in extreme cases.

## Confidence

- **Confidence: Medium** in the theoretical robustness of the vMF assumption
- **Confidence: Medium** in temperature-free claims
- **Confidence: Low** in computational efficiency claims

## Next Checks

1. **View Consistency Test**: Systematically vary augmentation strength and measure vMF parameter stability to establish operational bounds for DSF.

2. **Temperature Sensitivity Validation**: Run controlled experiments with τ∈{0.1, 0.5, 1.0, 2.0, 5.0} on CIFAR-10 to verify DSF's performance degrades gracefully compared to cosine similarity.

3. **Extreme View Count Analysis**: Train with M∈{2,4,6,8,12,16} on CIFAR-10 while holding total images processed constant to establish the optimal view count frontier and identify when vMF estimation becomes unreliable.