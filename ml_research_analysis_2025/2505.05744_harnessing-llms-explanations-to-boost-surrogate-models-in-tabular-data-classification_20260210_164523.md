---
ver: rpa2
title: Harnessing LLMs Explanations to Boost Surrogate Models in Tabular Data Classification
arxiv_id: '2505.05744'
source_url: https://arxiv.org/abs/2505.05744
tags:
- tabular
- learning
- llms
- data
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a novel in-context learning framework for
  tabular prediction that leverages large language model (LLM) explanations to guide
  a smaller, locally deployable surrogate language model (SLM). The framework addresses
  three key challenges in existing LLM-based methods: high resource requirements,
  suboptimal demonstration selection, and limited interpretability.'
---

# Harnessing LLMs Explanations to Boost Surrogate Models in Tabular Data Classification

## Quick Facts
- **arXiv ID**: 2505.05744
- **Source URL**: https://arxiv.org/abs/2505.05744
- **Reference count**: 39
- **Key outcome**: Proposes explanation-guided demonstration selection framework that improves tabular classification accuracy by 5.31% on average

## Executive Summary
This paper introduces a novel framework for few-shot tabular classification that leverages LLM-generated explanations to guide demonstration selection for a smaller surrogate model. The approach addresses three key challenges in existing LLM-based methods: high resource requirements, suboptimal demonstration selection, and limited interpretability. By generating post-hoc explanations for candidate demonstrations, filtering based on feature importance, and using these explanations as rationales in few-shot prompts, the framework achieves significant accuracy improvements while reducing dependency on expensive LLM inference.

## Method Summary
The framework operates in three stages: (1) generate post-hoc explanations for candidate demonstrations using an LLM, identifying salient features that explain each prediction; (2) filter and select demonstrations based on feature importance scores, using cosine similarity on SentenceBERT embeddings of filtered text; (3) prompt a smaller language model (Llama2-7B) with the selected demonstrations plus their explanations as rationales to make predictions on test samples. The approach is validated on four tabular datasets using a 4-shot setting, showing improved accuracy compared to baseline methods while reducing resource consumption.

## Key Results
- Average accuracy improvement of 5.31% across four tabular datasets compared to baseline methods
- Significant performance gains in ablation studies, with up to 15% accuracy drop when removing explanation-guided filtering
- Demonstrates reduced dependency on resource-intensive LLMs while maintaining competitive performance
- Shows effectiveness of rationale-augmented in-context learning for improving SLM prediction quality

## Why This Works (Mechanism)

### Mechanism 1: Explanation-Guided Noise Filtration
Filtering candidate demonstrations to retain only LLM-identified salient features likely improves semantic matching quality compared to using raw tabular embeddings. Tabular data often contains redundant or noisy features that degrade embedding quality. By prompting an LLM to identify the top-n salient features for candidates, the framework filters out non-salient attributes before computing embeddings, aligning similarity search on a cleaned semantic space rather than a noisy raw feature space.

### Mechanism 2: Rationale-Augmented In-Context Learning
Augmenting few-shot prompts with post-hoc explanations enhances the SLM's classification accuracy and output interpretability. The framework appends LLM-generated explanations to question-answer pairs, serving as reasoning chains that condition the SLM to utilize provided logic rather than relying solely on pattern matching from Qâ†’A mappings.

### Mechanism 3: Offline Knowledge Distillation via Prompts
Offloading explanation generation to a one-time "warm-up" phase allows a local SLM to approximate LLM performance with significantly reduced inference costs. The resource-intensive LLM is only queried during offline candidate generation, while the SLM reuses these static explanations during live inference, acting as a surrogate that has "learned" the LLM's reasoning style via prompt context.

## Foundational Learning

- **Concept: Tabular Data Serialization**
  - Why needed here: The framework relies on converting structured database rows into natural language text strings to be processed by LLMs/SLMs
  - Quick check question: Can you explain how a categorical variable (e.g., "Credit History") and a numerical variable (e.g., "Age") are differently represented in the prompt text?

- **Concept: Post-hoc Explanation (Feature Attribution)**
  - Why needed here: The core driver is using an LLM to explain why an answer is correct after the fact by selecting the top-n most important words
  - Quick check question: How does the paper ensure the LLM outputs the explanation in a parseable format, and why is the number of words (n) a critical hyperparameter?

- **Concept: Surrogate Modeling**
  - Why needed here: The goal is to replace the expensive "Teacher" (LLM) with a cheaper "Student" (SLM) for final deployment
  - Quick check question: In this architecture, is the SLM "fine-tuned" on the LLM's outputs, or does it learn purely from the context window?

## Architecture Onboarding

- **Component map**: Serializer -> Explainer (LLM) -> Filter -> Encoder (BERT) -> Selector -> Predictor (SLM)
- **Critical path**: The most sensitive component is the Feature Importance Threshold (p) and Explanation Count (n). If n is too low, the filter strips too much info; if p is too high, noise remains. The ablation study confirms that removing explanation-guided selection causes up to 15% accuracy drop in specific datasets.
- **Design tradeoffs**:
  - Latency vs. Performance: Increasing candidate pool size M improves match quality but linearly increases one-time LLM API cost
  - SLM Size: The paper uses Llama2-7B. A smaller model (e.g., 1B parameters) might fail to utilize complex rationales in the prompt
- **Failure signatures**:
  - Selection Collapse: If feature importance calculation results in flat distributions, filtering fails to distinguish signal from noise
  - Format Drift: If LLM generates explanations violating the expected JSON-like format, the parser fails
- **First 3 experiments**:
  1. Run framework with and without filter step on Bank dataset to verify ~15% accuracy delta
  2. Sweep n (explanation words) from 1 to 10 on Heart dataset to reproduce volatility in Figure 4
  3. Compare Cosine Similarity vs. Manhattan Distance on unfiltered data to confirm distance metrics are unstable without explanation-guided filter

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the framework be effectively generalized to non-tabular data structures or extended to handle multi-class classification tasks?
- Basis in paper: The authors state "Future work will focus on extending these methods to other data types," and experiments were limited to binary classification
- Why unresolved: Current validation covers only binary tabular classification, leaving method's efficacy on complex, multi-class, or non-structured data unproven
- What evidence would resolve it: Successful application and evaluation on multi-class tabular datasets and non-tabular domains (e.g., text or image classification)

### Open Question 2
- Question: How robust is the surrogate model when the teacher LLM provides noisy, incorrect, or hallucinated post-hoc explanations?
- Basis in paper: The method relies on the assumption that GPT-3.5 generates accurate feature attributions, yet LLMs are known to hallucinate or produce inconsistent reasoning
- Why unresolved: The paper does not evaluate performance degradation when LLM explanation quality is low or misleading
- What evidence would resolve it: Sensitivity analysis measuring SLM accuracy when synthetic noise is injected into LLM-generated explanations

### Open Question 3
- Question: Can the hyperparameters for feature importance thresholds (p) and explanation count (n) be dynamically adapted rather than manually tuned?
- Basis in paper: The hyper-parameter analysis shows significant variance in performance across datasets based on static values of p and n
- Why unresolved: A static configuration may not be optimal for all datasets, suggesting need for adaptive mechanisms
- What evidence would resolve it: Comparative study showing adaptive tuning strategy outperforms fixed hyperparameter settings across diverse datasets

## Limitations

- Framework's reliance on LLM-generated explanations introduces sensitivity to prompt design, with exact task description text missing from paper
- Fixed filtering threshold (p=0.85) and explanation word count (n=5) appear dataset-dependent, showing volatile performance when hyperparameters vary
- SentenceBERT checkpoint is unspecified beyond dimension size, creating reproducibility gaps
- Explanation format dependency creates single point of failure if GPT-3.5 deviates from expected JSON-like structure

## Confidence

- **High confidence**: Core ablation results showing 5.31% average accuracy improvement over baselines, and general mechanism of explanation-guided filtering improving semantic matching quality
- **Medium confidence**: Generalizability to datasets with different class distributions and feature types, particularly given volatility in Figure 4
- **Low confidence**: Robustness to LLM hallucination in explanations and claim that approach significantly reduces resource dependency when one-time explanation generation cost is considered

## Next Checks

1. **Format robustness test**: Systematically corrupt explanation format (missing braces, reordered keys) to measure pipeline failure rates and quantify brittleness of JSON-like parsing requirement

2. **Distribution shift evaluation**: Hold out 20% of each dataset as "out-of-distribution" test set to verify whether pre-generated explanations from training pool remain effective when feature distributions change

3. **Cost-benefit analysis**: Measure total resource consumption (API calls, latency, memory) for full pipeline including one-time explanation generation phase, comparing against continuous LLM inference to validate claimed efficiency gains