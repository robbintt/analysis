---
ver: rpa2
title: 'Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic
  Data for LLM Reasoning'
arxiv_id: '2505.10182'
source_url: https://arxiv.org/abs/2505.10182
tags:
- reasoning
- thoughts
- hidden
- thought
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates Reasoning CPT, a continual pretraining method
  that augments expert texts with synthetically generated hidden thoughts (simulating
  the reasoning process behind the text). By training Gemma2-9B on synthetic data
  from STEM and Law domains, the method improves MMLU performance by up to 3.3% overall,
  with gains of up to 8 points on the most difficult problems.
---

# Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning

## Quick Facts
- arXiv ID: 2505.10182
- Source URL: https://arxiv.org/abs/2505.10182
- Reference count: 40
- Primary result: Reasoning CPT improves MMLU performance by up to 3.3% overall, with gains of up to 8 points on hardest problems

## Executive Summary
This paper introduces Reasoning CPT, a continual pretraining method that augments expert texts with synthetically generated hidden thoughts simulating the reasoning process behind the text. By training Gemma2-9B on synthetic reasoning traces from STEM and Law domains, the method improves reasoning performance across domains. The approach demonstrates both domain-specific gains (up to +5.4 points in STEM) and cross-domain transfer (Law-trained models improve STEM by +4.3 points), while also learning to adjust reasoning depth according to problem difficulty.

## Method Summary
Reasoning CPT continually pretrains Gemma2-9B by augmenting expert texts with synthetically generated hidden thoughts (reasoning traces) using Gemma2-9B-it as the generator. The method formats data as `<start_of_thought> H <end_of_thought> S` where H represents hidden thoughts and S is the original text. Training uses LoRA (rank 64) with autoregressive loss over the full sequence. The approach is evaluated on MMLU across domains and difficulty levels, with hidden-thought-style few-shot prompts for assessment.

## Key Results
- Reasoning CPT consistently improves MMLU performance across all evaluated domains
- Gains of up to 8 points on the most challenging problems compared to baseline
- Cross-domain benefits observed: Law-trained models improve STEM performance by 4.3 points
- Models learn to adjust reasoning length according to problem difficulty

## Why This Works (Mechanism)

### Mechanism 1
Augmenting texts with synthetic hidden thoughts during continual pretraining improves reasoning performance across domains. The model learns to predict both thinking process and final output, internalizing reasoning patterns without requiring task-specific rewards. Evidence: MMLU improvements of +5.4 points in STEM and +2.9 in humanities; cross-domain gains of +4.3 points from Law to STEM. Break condition: Low-quality synthetic thoughts may teach spurious patterns.

### Mechanism 2
Reasoning skills acquired in one domain transfer to others, even without domain-specific training data. Hidden thoughts capture domain-agnostic reasoning patterns (hypothesis verification, multi-perspective analysis) that generalize beyond surface content. Evidence: Law-trained models improve MMLU-STEM domains by 4.3 points. Break condition: Highly specialized reasoning styles may not share transferable patterns with target domains.

### Mechanism 3
Models trained with hidden thoughts learn to scale reasoning length adaptively based on problem difficulty. Training data exhibits positive correlation between original text length and hidden thought length (Spearman ρ = 0.348 for STEM, 0.486 for Law), teaching a heuristic to continue reasoning until sufficient evidence accumulates. Evidence: Models generate shorter thoughts (~80-100 tokens) for easy problems and longer thoughts (~180-200 tokens) for hard problems. Break condition: If inference problems don't correlate length with difficulty like training texts, the heuristic may misfire.

## Foundational Learning

- **Continual Pretraining (CPT) vs. Fine-Tuning**: CPT uses autoregressive language modeling on raw (or augmented) text without task-specific labels, unlike SFT with instruction-response pairs. Quick check: What loss function does CPT optimize, and how does it differ from supervised fine-tuning?

- **Chain-of-Thought (CoT) Reasoning**: Hidden thoughts are structured as CoT-style reasoning traces. Familiarity with CoT prompts and their role in eliciting reasoning is prerequisite. Quick check: How does a CoT prompt differ from a standard few-shot prompt, and why might it improve performance on multi-step reasoning tasks?

- **Synthetic Data Generation for LLM Training**: The method relies on using an instruction-tuned LLM to generate hidden thoughts. Understanding tradeoffs of synthetic vs. human-authored training data is critical. Quick check: What are two risks of training on synthetically generated reasoning traces, and how might they affect downstream performance?

## Architecture Onboarding

- **Component map**: OpenWebMath/Law texts → preprocessed to 64-512 tokens → Gemma2-9B-it hidden thought generator → `<start_of_thought> H <end_of_thought> S` format → Gemma2-9B with LoRA → MMLU/GSM8k evaluation

- **Critical path**: 1) Preprocess domain texts with sentence-boundary truncation and quality filtering; 2) Generate hidden thoughts using structured prompt, temp 0.3, max 512 tokens; 3) Concatenate thoughts + original text within 1024-token limit; 4) Train with LoRA on autoregressive loss; 5) Evaluate with hidden-thought-style few-shot prompts

- **Design tradeoffs**: Thought length vs. coverage (longer thoughts reduce original-text visibility); domain specificity vs. transfer (single-domain training simplifies collection but may limit gains); generator quality vs. cost (stronger generators increase synthesis cost)

- **Failure signatures**: Low cross-domain transfer (thoughts too domain-specific); inefficient reasoning (weak correlation between text length and thought length); catastrophic forgetting (monitor base-model benchmarks)

- **First 3 experiments**: 1) Ablation: train with hidden thoughts vs. standard CoT vs. no thoughts; measure MMLU and GSM8k accuracy; 2) Cross-domain probe: train on Law only, evaluate on MMLU-STEM subsets; compare to STEM-only training; 3) Difficulty-stratified analysis: classify MMLU by difficulty, plot accuracy vs. difficulty for Reasoning CPT vs. standard CPT

## Open Questions the Paper Calls Out

- **Q1**: Does combining Reasoning CPT with subsequent RL or SFT training yield greater improvements than either method alone? Basis: Paper suggests post-training with RL/SFT could refine strong reasoning foundations built by Reasoning CPT.

- **Q2**: Does the capability of the thought-generator model affect the quality of acquired reasoning skills? Basis: Paper uses Gemma2-9B-it but notes stronger models like Gemini 2.5 Pro or DeepSeek-R1 could be used, without comparing generator quality effects.

- **Q3**: Can Reasoning CPT applied to synthetic LLM-generated texts produce comparable or better reasoning improvements than human-authored texts? Basis: Paper generated hidden thoughts for human-written texts but suggests the method could also apply to synthetic data.

- **Q4**: What mechanisms cause the reversal in thinking-token behavior between STEM and Law-trained models? Basis: Paper observes this reversal and offers hypotheses about domain characteristics and regularizing effects, but provides no definitive explanation.

## Limitations

- Domain coverage remains narrow, validating only on STEM and Law domains without testing transfer to medicine, humanities, or code
- Synthetic data quality is unverified, with no human evaluation of generated hidden thoughts that could contain hallucinated reasoning traces
- Correlation vs causation uncertainty in adaptive reasoning, as inference problems may not follow the same length-difficulty relationship as training data

## Confidence

- **High confidence**: Domain-specific reasoning gains (STEM +5.4, Law +2.9) with direct training-evaluation alignment
- **Medium confidence**: Cross-domain transfer (Law→STEM +4.3) shows evidence but lacks robustness testing across multiple domain pairs
- **Medium confidence**: Adaptive reasoning length demonstrated on MMLU but depends on training data's length-difficulty correlation
- **Low confidence**: Claims about "reasoning skills" acquisition versus pattern matching cannot be distinguished with current methodology

## Next Checks

1. **Human evaluation of synthetic thoughts**: Have domain experts rate 100 randomly sampled hidden thoughts from each domain on correctness, coherence, and relevance; calculate inter-rater reliability and correlate quality scores with downstream performance gains.

2. **Transfer robustness test**: Train Reasoning CPT on Law, then systematically evaluate on STEM subdomains (Mathematics, Computer Science, Engineering separately); compare transfer patterns to training STEM→STEM transfer to identify which reasoning skills generalize versus domain-specific knowledge.

3. **Length correlation stress test**: Create a controlled MMLU subset where easy problems have long solutions and hard problems have short ones; evaluate whether Reasoning CPT maintains accuracy or if the length heuristic breaks down when the training correlation reverses.