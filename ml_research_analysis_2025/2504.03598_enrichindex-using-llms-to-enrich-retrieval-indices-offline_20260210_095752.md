---
ver: rpa2
title: 'EnrichIndex: Using LLMs to Enrich Retrieval Indices Offline'
arxiv_id: '2504.03598'
source_url: https://arxiv.org/abs/2504.03598
tags:
- retrieval
- query
- table
- object
- name
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EnrichIndex addresses the challenge of retrieving documents and
  tables that require implicit reasoning by using LLMs offline to generate semantically-enriched
  indices during ingestion. It enriches each object with a purpose, summary, and QA
  pairs, which are then indexed alongside the original content.
---

# EnrichIndex: Using LLMs to Enrich Retrieval Indices Offline

## Quick Facts
- arXiv ID: 2504.03598
- Source URL: https://arxiv.org/abs/2504.03598
- Reference count: 23
- Key outcome: Improves recall@10 by 11.7 points and NDCG@10 by 10.6 points over strong baselines while reducing LLM token processing by 293.3x

## Executive Summary
EnrichIndex addresses the challenge of retrieving documents and tables that require implicit reasoning by using LLMs offline to generate semantically-enriched indices during ingestion. It enriches each object with a purpose, summary, and QA pairs, which are then indexed alongside the original content. During retrieval, a weighted similarity score across all enriched representations is computed. Evaluated on five retrieval tasks, EnrichIndex improves recall @10 by 11.7 points and NDCG @10 by 10.6 points over strong online re-ranking baselines while reducing LLM token processing by 293.3x, significantly lowering online latency and cost. It also boosts the performance of existing re-rankers when used as a stage-one retriever.

## Method Summary
EnrichIndex performs a one-time offline pass over a corpus to enrich each object (document or table) with three LLM-generated representations: purpose (explaining relevance in layman's terms), summary (condensing key points), and 20 QA pairs. These enrichments are indexed alongside the original content, creating four parallel indices per object. At retrieval time, a query is compared against all representations using embedding or BM25 similarity, with results combined via weighted fusion. Weights are tuned per domain using a validation set. The method targets implicit reasoning tasks where lexical overlap is insufficient, and demonstrates significant efficiency gains by amortizing LLM processing across the corpus.

## Key Results
- Achieves 11.7 point improvement in recall@10 and 10.6 point improvement in NDCG@10 over strong online re-ranking baselines
- Reduces LLM token processing by 293.3x, significantly lowering online latency and cost
- Improves the performance of existing re-rankers when used as a stage-one retriever

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-representation enrichment bridges semantic gaps between queries and objects requiring implicit reasoning.
- **Mechanism:** LLM-generated purpose, summary, and QA pairs create alternative lexical and semantic signals that align with query language even when original content uses jargon, sparse columns, or implicit structures. The Wasserstein distance between gold/non-gold similarity distributions increases (e.g., +5.3% for Bright, +58.1% for Spider2), indicating improved separability.
- **Core assumption:** Queries and objects share latent intent that LLMs can surface via paraphrasing and question generation.
- **Evidence anchors:**
  - [abstract] "enriches each object with a purpose, summary, and QA pairs, which are then indexed alongside the original content"
  - [section 4.1, Table 6] Wasserstein distance increases across all datasets after enrichment
  - [corpus] AR-Med (2503.14887) similarly uses LLM-driven information augmentation for semantic matching, suggesting cross-domain validity of enrichment patterns
- **Break condition:** When enrichment generates generic content that doesn't discriminate (e.g., "This is a table about data"), or when QA pairs overlap across many objects, diluting specificity.

### Mechanism 2
- **Claim:** Weighted fusion over multiple indices outperforms single-index retrieval.
- **Mechanism:** Final score `S(q,o) = α₁S(q,oᵦ) + α₂S(q,oₚ) + α₃S(q,oₛ) + α₄S(q,o_qa)` combines signals tuned per domain. Purpose dominates for implicit reasoning (Bright: +2.0 recall vs +1.5 QA), while summary dominates for tables (+5.77 recall vs +4.77 purpose).
- **Core assumption:** Different retrieval contexts benefit from different representation priorities, capturable via validation-set tuning.
- **Evidence anchors:**
  - [section 2.2, Eq. 1] Explicit weighted sum formulation with tunable coefficients
  - [section 4.2, Figure 2] Per-enrichment-type ablation shows purpose > QA > summary for implicit reasoning; summary > purpose > QA for tables
  - [corpus] LightRetriever (2505.12260) uses LLMs for efficient query encoding but focuses on speed; doesn't address multi-index fusion
- **Break condition:** When weights overfit validation sets that don't reflect query distribution shift, or when one enrichment type is noisy and dominates via poorly tuned α.

### Mechanism 3
- **Claim:** Offline amortization enables LLM-quality retrieval at 293.3× lower online token cost.
- **Mechanism:** One-time corpus pass during ingestion generates enriched indices; online retrieval uses only embedding/BM25 lookups plus optional lightweight query expansion. Compared to Judgerank's per-query document expansion, EnrichIndex reduces input tokens by 1158.5× (Bright) to 1998.8× (tables).
- **Core assumption:** Corpus is relatively static; enrichment remains valid across query distributions.
- **Evidence anchors:**
  - [abstract] "reducing LLM token processing by 293.3x, significantly lowering online latency and cost"
  - [section 3.4, Table 5] Token reduction factors: 348.4× total tokens (Bright avg), 235.5× (tables avg)
  - [corpus] AdNanny (2602.01563) applies similar offline LLM reasoning pattern for ads recommendation, validating the amortization strategy in production systems
- **Break condition:** When corpus updates frequently (e.g., real-time feeds) and re-enrichment latency/cost becomes prohibitive, or when enrichment quality degrades for new object types not seen during prompt design.

## Foundational Learning

- **Concept: Implicit retrieval** — retrieving documents where relevance requires inference, not lexical overlap.
  - **Why needed here:** The paper targets this explicitly (Bright benchmark, technical tables); standard dense/BM25 fail when queries use different terminology than content.
  - **Quick check question:** Can you identify a query-document pair where BM25 returns zero lexical matches but a human would correctly judge relevance?

- **Concept: Stage-one vs. re-ranking** — coarse retrieval over full corpus vs. fine-grained scoring over top-k.
  - **Why needed here:** EnrichIndex improves stage-one, which lifts re-ranker ceilings; poor stage-one means re-rankers never see relevant objects.
  - **Quick check question:** If stage-one recall@100 is 60%, what's the maximum possible recall@10 after re-ranking?

- **Concept: Document/query expansion** — augmenting representations with generated text.
  - **Why needed here:** EnrichIndex is a form of document expansion; understanding this pattern helps contextualize against query expansion and hybrid approaches.
  - **Quick check question:** What's the tradeoff between expanding documents offline vs. expanding queries online?

## Architecture Onboarding

- **Component map:** Ingestion pipeline: Object serializer → LLM enrichment (purpose/summary/QA) → Index writer (4 indices per object) → Online retrieval: Query encoder → Parallel similarity computation across 4 indices → Weighted fusion → Top-k selection → Optional re-ranker

- **Critical path:**
  1. Validate enrichment prompts on sample objects before full corpus pass (prompts in Appendix B.1)
  2. Serialize tables correctly (database name, table name, 5 sample rows as markdown)
  3. Tune α weights on held-out validation split (80/20 test/val used in paper)
  4. Monitor for enrichment failures (LLM returns "None" for non-meaningful content)

- **Design tradeoffs:**
  - **Enrichment depth vs. cost:** 20 QA pairs per object improves recall but increases storage/indexing cost; reduce for large corpora
  - **Summary vs. purpose:** Summary helps tables (modality translation); purpose helps implicit reasoning; choose based on domain
  - **Weight tuning:** Requires validation set; if unavailable, paper suggests α₁=α₂=α₃=α₄ as neutral starting point (not explicitly tested)

- **Failure signatures:**
  - **Enrichment collapse:** Generated purpose/summary/QA all look similar across objects → Wasserstein distance doesn't increase → check prompt diversity and LLM temperature
  - **Weight overfitting:** Validation performance good, test poor → use cross-validation for α tuning
  - **Stale enrichment:** Corpus updated but indices not re-built → retrieval quality degrades for new objects → implement incremental enrichment or periodic full rebuilds

- **First 3 experiments:**
  1. **Baseline comparison:** Run BM25/GTE on your corpus without enrichment; measure recall@10 and NDCG@10 on held-out queries
  2. **Ablation by enrichment type:** Enable purpose-only, then summary-only, then QA-only; identify which contributes most for your domain
  3. **Weight sensitivity:** Grid search α weights on validation set; compare uniform weights vs. tuned weights to assess tuning necessity

## Open Questions the Paper Calls Out
- How does EnrichIndex perform in highly dynamic corpora where documents are frequently added or updated, potentially negating the "single pass" efficiency gains?
- Can an adaptive system selectively generate specific enrichment types (e.g., Purpose vs. Summary) based on object characteristics to optimize the cost-performance trade-off?
- To what extent is the quality of the enriched index dependent on the proprietary GPT-4o-mini model, and can smaller, open-source models achieve comparable results?

## Limitations
- Weight tuning stability across domains is not analyzed; poor α choices could nullify gains
- Corpus staleness impact not evaluated; rapid update environments may see degraded enrichment quality
- Token savings calculation assumes uniform cost-per-token across all enrichment representations

## Confidence
- **High confidence:** Token reduction claims (293.3×), basic enrichment effectiveness (recall/NDCG improvements), offline amortization mechanism
- **Medium confidence:** Weight tuning benefits, domain transferability of enrichment types, real-world cost savings
- **Low confidence:** Long-term enrichment quality maintenance, handling of frequent corpus updates, optimal enrichment depth for large-scale applications

## Next Checks
1. **Weight sensitivity test:** Compare retrieval performance using tuned α weights vs. uniform weights (α=0.25 each) on a held-out validation set to quantify tuning necessity.
2. **Corpus update simulation:** Measure retrieval quality degradation when enriching only 20% of corpus and leaving 80% un-enriched, then retrieving queries that may target either subset.
3. **Enrichment quality audit:** Manually sample 50 enriched objects and evaluate whether generated purpose/summary/QA pairs are meaningful and distinct across objects.