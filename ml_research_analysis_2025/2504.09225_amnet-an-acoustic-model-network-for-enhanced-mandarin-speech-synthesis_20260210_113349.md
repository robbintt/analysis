---
ver: rpa2
title: 'AMNet: An Acoustic Model Network for Enhanced Mandarin Speech Synthesis'
arxiv_id: '2504.09225'
source_url: https://arxiv.org/abs/2504.09225
tags:
- speech
- tone
- local
- synthesis
- phrase
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AMNet addresses the challenge of local context modeling in Mandarin
  speech synthesis by integrating phrase structure annotation and local convolution
  mechanisms into the FastSpeech 2 architecture. The method enhances local context
  sensitivity using a phrase structure parser for linguistic information and a local
  convolution module to capture semantic correlations between adjacent words or characters.
---

# AMNet: An Acoustic Model Network for Enhanced Mandarin Speech Synthesis

## Quick Facts
- **arXiv ID:** 2504.09225
- **Source URL:** https://arxiv.org/abs/2504.09225
- **Reference count:** 40
- **Primary result:** AMNet achieves MOS 4.11±0.12 on BIAOBEI dataset, outperforming FastSpeech 2 baseline with improved MCD and F0 fitting.

## Executive Summary
AMNet enhances Mandarin speech synthesis by addressing local context modeling challenges through three key innovations: phrase structure annotation, local convolution mechanisms, and tone-phoneme decoupling. The method integrates a Phrase Structure Parser that assigns SBME labels to characters, a multi-scale local convolution module that captures adjacent-character dependencies, and explicit tone embeddings as conditioning signals. Experimental results demonstrate significant improvements over FastSpeech 2 baseline, with MOS increasing from 3.72 to 4.11 and MCD decreasing from 6.32 to 5.54. The model achieves better fundamental frequency fitting (R²=0.874) while maintaining competitive performance on other acoustic metrics.

## Method Summary
AMNet modifies FastSpeech 2 by adding a local convolution module before self-attention, using multi-scale 1D-CNN kernels (9, 5, 3) to capture local context. A Phrase Structure Parser (HMM/Viterbi) segments text into phrases and assigns SBME labels (Single, Begin, Middle, End) that are embedded as auxiliary features. Tone information is extracted separately from phonemes (labels 0-5) and provided as explicit conditioning. The model is trained on BIAOBEI dataset (10k sentences, 22.05 kHz) using Adam optimizer with gradient clipping every 30k steps, batch size 64, and 100k total steps. HiFi-GAN vocoder is used for final waveform generation.

## Key Results
- **MOS improvement:** 4.11±0.12 vs baseline 3.72, representing 10.6% absolute gain
- **MCD reduction:** 5.54 vs baseline 6.32, 12.4% relative improvement
- **F0 fitting:** R² of 0.874 vs baseline 0.811, 7.8% relative improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Phrase structure annotation enhances local boundary perception in Mandarin speech synthesis.
- Mechanism: The Phrase Structure Parser (PSP) assigns SBME labels to characters based on HMM/Viterbi phrase segmentation. These labels are embedded as auxiliary features alongside phoneme embeddings, explicitly signaling phrase boundaries to the model. Phrase-level duration vectors additionally constrain training.
- Core assumption: Explicit boundary markers improve the model's ability to predict appropriate pauses and prosodic phrasing better than implicit learning from phonemes alone.
- Evidence anchors:
  - [abstract] "embedding a phrase structure parser into the model... enhances the model's sensitivity to local information"
  - [section III.B] Describes SBME annotation mapping to numerical sequences, phrase-level duration summation
  - [corpus] Weak direct evidence; related work (GSA-TTS) captures local style per semantic unit, but no direct validation of SBME annotation for TTS
- Break condition: If phrase boundaries are already implicitly learned by large pre-trained language models, explicit annotation may yield diminishing returns; annotation errors from the parser could introduce noise.

### Mechanism 2
- Claim: Local convolution integrated with self-attention captures adjacent-character dependencies that pure global attention may miss.
- Mechanism: A two-layer 1D-CNN with multi-scale kernels (sizes 9, 5, 3) processes input embeddings before attention, expanding dimensions to 1024 then compressing to 256. Features are averaged across kernel scales. This generates locality-aware query/value representations.
- Core assumption: Mandarin pronunciation depends heavily on adjacent characters (co-articulation effects), and convolution's inductive bias for local patterns complements global attention.
- Evidence anchors:
  - [abstract] "local convolution module to capture semantic correlations between adjacent words or characters"
  - [section III.C] Details multi-scale kernel design, padding strategy, feature fusion formula
  - [corpus] UMA-Split uses unimodal aggregation for local acoustic segment grouping in ASR, suggesting locality matters for Mandarin broadly, but no direct TTS validation
- Break condition: If local context is already well-modeled by modern Transformer variants (e.g., local attention windows), additional convolution adds complexity without proportional gain.

### Mechanism 3
- Claim: Decoupling tone from phonemes as independent conditional inputs improves tone accuracy and pronunciation naturalness.
- Mechanism: Vowel tones are extracted and labeled {1,2,3,4,5} (plus 0 for consonants), separated from base phonemes. The tone sequence is fed as an explicit conditioning signal, enabling the model to learn tone-specific prosodic patterns independently.
- Core assumption: Tonal information, when entangled with phoneme embeddings, is insufficiently disentangled for accurate tone synthesis; explicit conditioning forces better representation.
- Evidence anchors:
  - [abstract] "decouples tonal characteristics from phonemes, providing explicit guidance for tone modeling"
  - [section III.D] Describes tone label extraction and recombination process
  - [corpus] ToxicTone dataset annotates Mandarin tonality for prosodic cues; suggests tone modeling is underexplored, but no direct mechanism validation
- Break condition: If the base phoneme representation already captures sufficient tonal information (e.g., tone-specific phoneme tokens), explicit decoupling is redundant.

## Foundational Learning

- Concept: **Self-attention and locality in Transformers**
  - Why needed here: AMNet modifies standard self-attention to address its weakness in local context modeling.
  - Quick check question: Can you explain why pure self-attention may struggle with local dependencies in sequence modeling?

- Concept: **Mandarin tonal system (4 tones + neutral)**
  - Why needed here: The paper's tone decoupling mechanism assumes familiarity with Mandarin's tonal structure.
  - Quick check question: What are the four main Mandarin tones, and why might they affect synthesis differently from non-tonal languages?

- Concept: **Phrase segmentation (CWS, SBME tagging)**
  - Why needed here: The PSP module builds on Chinese word segmentation principles.
  - Quick check question: What does the SBME labeling scheme represent, and how does it differ from part-of-speech tagging?

## Architecture Onboarding

- Component map:
  Text -> Phoneme sequence + Tone extraction + Phrase annotation -> Concatenate phoneme embeddings + phrase embeddings -> Local convolution -> Encoder -> Tone embeddings injection -> Variance adaptor -> Decoder -> Mel-spectrogram -> HiFi-GAN vocoder

- Critical path:
  1. Text → Phoneme sequence + Tone extraction + Phrase annotation
  2. Concatenate phoneme embeddings + phrase embeddings; apply local convolution
  3. Encoder processes locality-enhanced representations
  4. Tone embeddings injected as conditioning
  5. Variance adaptor predicts duration/pitch/energy
  6. Decoder generates Mel-spectrogram → HiFi-GAN vocoder

- Design tradeoffs:
  - PSP vs. POST: PSP provides better boundary signals (per ablation, MOS 4.11 vs. 3.97 without), but requires additional parsing pipeline
  - Multi-scale kernels (9,5,3) vs. single kernel: Multi-scale captures varied context lengths but increases parameters
  - Tone decoupling adds input complexity but improves MOS (4.11 vs. 4.02 without tone per ablation)

- Failure signatures:
  - Incorrect phrase parsing → misplaced pauses, unnatural prosody
  - Local convolution misconfiguration (wrong padding) → sequence length mismatch
  - Missing tone labels → tonal confusion, semantic errors in output speech

- First 3 experiments:
  1. **Baseline replication**: Train vanilla FastSpeech 2 on BIAOBEI to confirm baseline MOS (~3.72) before modifications.
  2. **Ablation by component**: Train AMNet variants removing PSP, local convolution, and tone embedding separately to isolate each contribution (target: validate ablation MOS deltas from Table IV).
  3. **Spectrogram/F0 visualization**: Compare Mel-spectrograms and F0 curves of AMNet vs. FastSpeech 2 on held-out test sentences to verify improved pause placement and tone accuracy (following Figure 3/4 methodology).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the deeper connection between tone features and pronunciation be modeled beyond simple feature decoupling?
- Basis in paper: [explicit] The introduction states that "The deeper connection between tone features and pronunciation remains an area for further exploration," despite the method treating them as independent inputs.
- Why unresolved: AMNet decouples tones as explicit conditional inputs to guide prediction, but this may not fully capture the complex, non-linear co-articulation effects where tonal variations actively alter the spectral characteristics of adjacent phonemes.
- What evidence would resolve it: A comparative study against architectures that use cross-attention mechanisms between tone and phoneme streams, rather than simple concatenation, to see if modeling the interaction improves tone accuracy metrics.

### Open Question 2
- Question: To what extent does the performance of AMNet depend on the accuracy of the external Phrase Structure Parser (PSP)?
- Basis in paper: [inferred] Section III.B describes the use of an external HMM and Viterbi algorithm for phrase segmentation, but the paper does not analyze how segmentation errors impact the downstream duration constraint.
- Why unresolved: The pipeline architecture assumes correct phrase boundary annotations (SBME tags). If the external parser misidentifies a phrase boundary, the duration summation and local convolution modules may receive misaligned context, potentially causing prosodic errors.
- What evidence would resolve it: An ablation study introducing synthetic noise into the SBME labels during inference to measure the degradation in Mean Opinion Score (MOS) and Mel Cepstral Distortion (MCD).

### Open Question 3
- Question: Does the local convolution module generalize effectively to multi-speaker or cross-lingual datasets?
- Basis in paper: [inferred] The experimental validation is limited to the single-speaker BIAOBEI dataset, leaving the model's generalization capabilities unverified.
- Why unresolved: The specific kernel sizes (e.g., 9×256) and local feature extraction were optimized for a single speaker's prosody. It is unclear if these fixed local windows are flexible enough to capture the diverse prosodic patterns of different speakers or languages.
- What evidence would resolve it: Experimental results from training AMNet on a multi-speaker corpus (e.g., AISHELL-3) or a non-tonal language to verify if the local context improvements persist across diverse voice timbres and phonetic structures.

## Limitations

- **Single dataset validation**: Results only validated on BIAOBEI corpus (12 hours, single speaker), limiting generalizability claims.
- **Implementation ambiguity**: Critical details missing for Phrase Structure Parser alignment and tone embedding integration, creating reproducibility barriers.
- **Statistical significance**: MOS improvements reported without confidence intervals or significance testing across evaluation sessions.

## Confidence

**High Confidence** (4.11±0.12 MOS, 5.54 MCD, 0.874 R²): The core architectural modifications - phrase structure annotation, local convolution, and tone decoupling - are clearly described and their individual contributions are demonstrated through systematic ablation experiments. The improvements over baseline FastSpeech 2 are substantial and well-documented.

**Medium Confidence** (Local convolution mechanism): While the multi-scale CNN design is specified, the exact feature fusion method and its integration with self-attention are not fully detailed. The assumption that convolution's locality bias meaningfully complements global attention in this specific context, though plausible, lacks comprehensive theoretical justification.

**Low Confidence** (Phrase Structure Parser implementation): The PSP methodology references HMM/Viterbi segmentation but provides insufficient detail on the segmentation algorithm parameters, phrase unit definition, or the SBME-to-phoneme alignment procedure. This represents the most significant reproducibility gap.

## Next Checks

1. **Statistical validation of MOS improvements**: Perform paired t-tests or Wilcoxon signed-rank tests on MOS ratings between AMNet and baseline variants across multiple evaluation sessions to establish statistical significance of the reported 0.11-0.47 MOS improvements.

2. **Cross-corpus generalization test**: Evaluate AMNet on an independent Mandarin TTS dataset (e.g., AISHELL-2 or THCHS-30) to verify that phrase structure and tone decoupling benefits transfer beyond the BIAOBEI corpus.

3. **Ablation with statistical power**: Replicate the ablation study with increased sample sizes (n≥200) and report confidence intervals for each variant's MOS to confirm that individual component contributions are statistically robust rather than artifacts of small sample sizes.