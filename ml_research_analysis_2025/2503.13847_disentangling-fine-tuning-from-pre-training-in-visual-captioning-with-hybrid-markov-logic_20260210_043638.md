---
ver: rpa2
title: Disentangling Fine-Tuning from Pre-Training in Visual Captioning with Hybrid
  Markov Logic
arxiv_id: '2503.13847'
source_url: https://arxiv.org/abs/2503.13847
tags:
- image
- training
- caption
- examples
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to disentangle what a visual
  captioning model learns during fine-tuning from what it has already learned during
  pre-training, using Hybrid Markov Logic Networks (HMLNs). The core idea is to model
  symbolic knowledge extracted from captions alongside visual features using a probabilistic
  framework, allowing for inference that quantifies the influence of training examples
  on generated captions.
---

# Disentangling Fine-Tuning from Pre-Training in Visual Captioning with Hybrid Markov Logic

## Quick Facts
- arXiv ID: 2503.13847
- Source URL: https://arxiv.org/abs/2503.13847
- Reference count: 40
- Primary result: Introduces HMLN framework to quantify influence of fine-tuning vs pre-training in image captioning models

## Executive Summary
This paper addresses the fundamental question of what image captioning models learn during fine-tuning versus what they inherit from pre-training. The authors propose using Hybrid Markov Logic Networks (HMLNs) to model the relationship between symbolic knowledge extracted from captions and visual features from images. By performing abductive inference and back-tracing captions to training examples, the framework quantifies the influence of fine-tuning data on caption generation. Experiments comparing traditional captioning models to BLIP2 (a VLLM-based model) reveal that BLIP2's generations are less explainable by fine-tuning data alone, suggesting greater reliance on pre-trained LLM knowledge.

## Method Summary
The method extracts ground predicates from captions using a scene graph parser, then builds an HMLN with Conjunctive and XOR feature templates slot-filled from training data. Parameter learning uses contrastive divergence with Gibbs sampling, while MAP inference is performed via MILP optimization with Gurobi. Marginal inference employs importance weighting with Gibbs sampling to trace captions back to training examples. The framework uses CLIP embeddings to compute real-valued terms representing visual-semantic similarity, and SpaCy embeddings for semantic equivalence in soft evidence.

## Key Results
- BLIP2 achieves lower MAP scores than traditional captioning models, indicating its generations are less explainable by fine-tuning data
- User studies via AMT confirm contrastive training examples are less interpretable for BLIP2 compared to non-VLLM models
- The framework successfully quantifies the influence of fine-tuning examples on caption generation across multiple model architectures
- Results support the hypothesis that VLLM-based models like BLIP2 rely more heavily on pre-trained knowledge than fine-tuning data

## Why This Works (Mechanism)
The approach works by creating a probabilistic graphical model that explicitly represents the relationship between visual features and symbolic caption knowledge. By learning weights for these relationships and performing MAP inference, the framework can determine which training examples best explain a given caption. The marginal inference with importance weighting then quantifies how much each training example contributes to the generated caption. This disentanglement is possible because the HMLN structure forces the model to choose between explaining captions through fine-tuning examples versus relying on pre-trained knowledge implicitly encoded in the weights.

## Foundational Learning
- **Hybrid Markov Logic Networks**: Combines first-order logic with probabilistic graphical models to handle both discrete and continuous variables; needed for modeling complex relationships between visual and textual modalities; quick check: verify the HMLN correctly handles both logical predicates and real-valued similarity terms.
- **Contrastive Divergence**: Parameter learning technique for Markov Random Fields that avoids expensive partition function computation; needed for scalable weight learning in HMLN; quick check: monitor weight convergence during training iterations.
- **MAP Inference via MILP**: Exact optimization approach for finding most probable explanation in probabilistic models; needed for precise attribution of captions to training examples; quick check: verify MILP solver finds optimal solutions within reasonable time.
- **Importance Weighting in Gibbs Sampling**: Technique for estimating marginal probabilities from MCMC samples; needed for back-tracing captions to training examples; quick check: verify importance weights remain stable across multiple sampling chains.

## Architecture Onboarding

**Component Map:**
Preprocessing (CLIP + SpaCy embeddings) -> HMLN Construction (ground predicates + feature templates) -> Parameter Learning (contrastive divergence) -> MAP Inference (MILP) -> Marginal Inference (importance weighting) -> Evaluation (metrics + AMT)

**Critical Path:**
The critical path is HMLN Construction → Parameter Learning → MAP Inference. Without properly constructed HMLNs with appropriate feature templates, learning cannot proceed. Without learned weights, MAP inference cannot attribute captions to training examples. This sequence must complete successfully for the framework to work.

**Design Tradeoffs:**
- Exact MILP vs. approximate MAP inference: Exact gives precise attribution but doesn't scale; approximation would scale better but sacrifice precision
- Full vs. restricted grounding: Full grounding captures all relationships but is computationally expensive; restricted grounding improves efficiency but may miss important connections
- Hard vs. soft evidence: Hard evidence is cleaner but less flexible; soft evidence handles semantic similarity but adds complexity

**Failure Signatures:**
- Poor MAP scores across all models: Likely indicates incorrect HMLN construction or feature template design
- Extremely high variance in importance weights: Suggests poor mixing of Gibbs sampler or need for more samples
- MILP solver fails to find solutions: May indicate numerical instability or need for constraint reformulation
- MAP scores don't correlate with caption quality: Suggests the attribution framework isn't capturing relevant relationships

**Exactly 3 First Experiments:**
1. Run HMLN construction on a small dataset subset and manually verify ground predicates and feature templates match expectations
2. Test MAP inference on synthetic data with known ground truth to verify attribution accuracy
3. Perform sensitivity analysis on semantic similarity threshold to understand its impact on MAP scores

## Open Questions the Paper Calls Out
### Open Question 1
Does the finding that fine-tuning has less influence on models using LLMs (like BLIP2) generalize to other Visual Large Language Models (VLLMs) with different architectures? The current study is limited to a single VLLM (BLIP2), making it difficult to distinguish between architecture-specific behaviors and general characteristics of the VLLM model class. Applying the HMLN inference methods to a diverse set of VLLMs (e.g., LLaVA, Flamingo) and observing if the lower MAP scores and importance weights persist would resolve this.

### Open Question 2
Can this HMLN-based disentanglement framework be effectively adapted to separate pre-training knowledge from fine-tuning in other multimodal tasks, such as Visual Question Answering (VQA)? The current approach relies on caption-specific structures (scene graphs from captions); VQA requires modeling different relationships (questions, answers, and visual evidence) which may require different HMLN templates. Demonstrating that marginal and MAP inference can successfully trace VQA answers back to fine-tuning examples or attribute them to pre-training would resolve this.

### Open Question 3
Is the "unexplained" portion of BLIP2's generations truly attributable to abstract "general knowledge," or is it simply a result of memorization from massive pre-training datasets not present in the fine-tuning set? The paper establishes that the knowledge is not in the fine-tuning data, but lacks a mechanism to distinguish between reasoning/generalization and memorization from external pre-training data. A method that can trace the "unexplained" generated predicates back to specific samples or concepts within the massive pre-training corpora (e.g., LAION, CC3M) would resolve this.

## Limitations
- Experimental conclusions depend on unspecified model versions and initialization states for pre-trained captioning models
- Key hyperparameters (semantic similarity thresholds, training example selection) are left unspecified, potentially affecting results
- MAP inference relies on exact MILP optimization without characterization of computational scaling
- Interpretation that MAP score differences directly quantify "pre-trained knowledge influence" is somewhat speculative

## Confidence
- **High confidence**: The HMLN framework correctly implements the stated inference procedures (MAP via MILP, marginal via importance weighting); the experimental setup follows standard practices for captioning evaluation and AMT studies.
- **Medium confidence**: The comparative conclusions about BLIP2's reliance on pre-trained knowledge are supported by the evidence but could be influenced by unspecified hyperparameters and model version differences.
- **Low confidence**: The interpretation that MAP score differences directly quantify "fine-tuning influence" versus "pre-trained knowledge influence" is somewhat speculative without additional controls or ablation studies.

## Next Checks
1. Reproduce MAP inference results on a small held-out subset of MSCOCO with known ground truth to verify the MILP implementation produces reasonable and stable explanations.
2. Perform sensitivity analysis by varying the semantic similarity threshold ε and the training example selection threshold C to determine how robust the MAP score differences are across parameter settings.
3. Conduct an ablation study where BLIP2 is fine-tuned on a synthetic dataset with controlled vocabulary to test whether the low MAP scores persist when the model cannot rely on pre-trained knowledge.