---
ver: rpa2
title: 'Trick or Neat: Adversarial Ambiguity and Language Model Evaluation'
arxiv_id: '2506.01205'
source_url: https://arxiv.org/abs/2506.01205
tags:
- telescope
- random
- woman
- synonym
- subj
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AmbAdv, the first adversarial ambiguity dataset
  designed to evaluate how well language models (LMs) detect ambiguity under adversarial
  variations. The dataset includes syntactically, lexically, and phonologically ambiguous
  sentences with perturbations such as synonym replacements, random word substitutions,
  and rhyme-based alterations.
---

# Trick or Neat: Adversarial Ambiguity and Language Model Evaluation

## Quick Facts
- arXiv ID: 2506.01205
- Source URL: https://arxiv.org/abs/2506.01205
- Reference count: 40
- Key outcome: Models encode ambiguity in representations but fail to leverage it in outputs; linear probes achieve >90% accuracy while prompting shows "yes bias"

## Executive Summary
This paper introduces AmbAdv, the first adversarial ambiguity dataset designed to evaluate how well language models detect ambiguity under adversarial variations. The dataset includes syntactically, lexically, and phonologically ambiguous sentences with perturbations such as synonym replacements, random word substitutions, and rhyme-based alterations. Experiments with four open-access 7B-parameter LMs show that direct prompting fails to reliably identify ambiguity, often exhibiting a "yes bias." However, linear probes trained on model representations can decode ambiguity with high accuracy, sometimes exceeding 90%, especially for last-noun and end-of-prompt tokens. The findings reveal that while LMs encode ambiguity-related information in their representations, they struggle to leverage it in their outputs, suggesting partial reliance on memorization rather than ambiguity understanding.

## Method Summary
The AmbAdv dataset contains 1,529 sentences across syntactic (1,097), lexical (224), and phonological (208) ambiguity types with four manipulation types each: word order, synonym, random, and rhyme substitutions. The study evaluates four 7B-parameter instruction-tuned models (Qwen-2.5, Mistral-v0.3, Llama-3, and Gemma) using zero-shot prompting with 8 Jinja2-formatted templates. For probing analysis, linear logistic regression classifiers are trained on layer-wise hidden states extracted at five token positions (first verb, first noun, last noun, last punctuation, eos) with 5-fold cross-validation (80/20 split).

## Key Results
- Direct prompting exhibits "yes bias" with accuracy often below 50% on balanced sets
- Linear probes achieve >90% accuracy on last-noun and eos tokens at middle-to-late layers
- Models cluster adversarial variants near original sentences in representational space, suggesting memorization effects
- Gemma shows particularly strong "yes bias" while other models exhibit varied failure modes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Language models encode ambiguity information in their internal residual stream representations (observable via linear probes) but fail to reliably access or utilize this information when generating outputs via direct prompting.
- **Mechanism**: Information about syntactic/semantic ambiguity is aggregated into the hidden states of specific tokens (e.g., the last noun) at middle-to-late layers. However, the instruction-following circuitry or the language modeling head does not effectively route this encoded signal to the final output token, leading to a "representational decoupling" where the model "knows" (representationally) but cannot "say" (behaviorally).
- **Core assumption**: High accuracy in a linear probe trained on hidden states implies that the information is explicitly linearly decodable and therefore "present" in the model's processing, distinct from the model's ability to verbally report it.
- **Evidence anchors**:
  - [abstract]: "linear probes trained on model representations can decode ambiguity with high accuracy... while direct prompting fails."
  - [section 4, Representational Analysis]: "averaged probe accuracies are consistently higher compared to the performance of the model's generated responses through prompting."
  - [corpus]: Corpus neighbors discuss balancing performance and ambiguity but do not address this specific probe-vs-prompt decoupling mechanism.

### Mechanism 2
- **Claim**: Models rely partially on memorized patterns of specific sentences rather than a generalized mechanism for understanding ambiguity, evidenced by high error rates on adversarial variants and clustering of variants around original sentences in representational space.
- **Mechanism**: When faced with adversarial variations, the model fails to robustly update its internal representation of plausibility. Instead, it appears to map the input to the nearest "anchor" (the original canonical sentence found in training data), leading to hallucinations or incorrect disambiguation based on the original sentence's properties.
- **Core assumption**: The clustering of adversarial examples near the original sentence in PCA projections indicates a failure to distinguish the semantic differences introduced by the adversarial perturbation, suggesting memorization of the original instance.
- **Evidence anchors**:
  - [section 4, Representational Analysis]: "We observe groups of both ambiguous and unambiguous sentences clustering around the original sentence... offering representational insight into repeated misclassifications, which may stem from memorized patterns."
  - [section 4, Disambiguation analysis]: "In some cases, the model repeated the disambiguation of the original sentence in synonym substitutions, indicating a memorization effect."
  - [corpus]: Weak/missing. Neighbors discuss "Deceptive Agents" but not this specific clustering/memorization mechanism.

### Mechanism 3
- **Claim**: Ambiguity signals are most concentrated in the representations of the last noun and the end-of-sequence (EOS) token, rather than the first verb or subject, suggesting a backward-looking synthesis process.
- **Mechanism**: As the transformer processes the sentence, the ambiguity (specifically in PP-attachment) is resolved or flagged by the time the final noun (the object of the preposition) is encoded. The EOS token accumulates this context in later layers (12+), achieving >90% probe accuracy, whereas the "decision" is less visible in earlier tokens or layers.
- **Core assumption**: The specific token role (last noun) is causally significant for this specific type of syntactic ambiguity (PP-attachment), and the high probe accuracy reflects a "special role" in disambiguation.
- **Evidence anchors**:
  - [section 4, Representational Analysis]: "probe accuracies of 0.9 and higher for token representations related to the last punctuation and last noun token, starting from layer 5 on."
  - [figure 3 description]: "An analysis of PCA projections... highlights that ambiguous/unambiguous... samples appear clustered... [probe accuracies] consistently higher... for last-noun."
  - [corpus]: Weak/missing. No specific evidence in corpus neighbors regarding token-specific encoding of ambiguity.

## Foundational Learning

- **Concept**: Linear Probing (Probing Classifiers)
  - **Why needed here**: To distinguish between what a model *encodes* (internal state) vs. what it *outputs* (behavior). The paper relies on the gap between probe accuracy and prompting accuracy to prove the "decoupling" mechanism.
  - **Quick check question**: If a linear probe on Layer 12 achieves 90% accuracy on a task, but the model's next-token prediction is random, does the model "know" the concept? (The paper argues yes).

- **Concept**: Prepositional Phrase (PP) Attachment Ambiguity
  - **Why needed here**: This is the core syntactic structure of the AmbAdv dataset (e.g., "saw the woman with the telescope"). Understanding the high/low attachment distinction is necessary to interpret the "random word" adversarial substitutions (which often rule out one reading via world knowledge).
  - **Quick check question**: In "The man saw the woman with the dress," which attachment (high: instrument of seeing, or low: attribute of woman) is semantically implausible due to world knowledge?

- **Concept**: Instruction-Tuning "Yes" Bias
  - **Why needed here**: To understand the failure mode of models like Gemma in the paper. The paper identifies a "yes bias" where models default to affirmative answers ("Yes, it is ambiguous") regardless of the input, which is a known artifact of RLHF/Instruction Tuning.
  - **Quick check question**: Why might a model output "Yes" to "Is this sentence ambiguous?" even for a clearly unambiguous sentence like "The cat sat on the mat"?

## Architecture Onboarding

- **Component map**: Input Processor -> Tokenizer -> Model Backbone (7B Llama/Qwen/etc.) -> Extraction Hook -> Linear Probe; Parallel path: Input Processor -> Prompt Evaluator
- **Critical path**: Extracting the correct token index for the "last noun" or "EOS" token from the tokenizer output and mapping it to the residual stream of the correct layer. If the index is misaligned, the probe accuracy drops significantly.
- **Design tradeoffs**:
  - Prompting vs. Probing: Prompting requires no training but suffers from "yes bias" and low accuracy. Probing requires labeled data and training a classifier but reveals the model's internal capacity.
  - Token Selection: Probing the EOS token is easier (late layers) but probing intermediate tokens offers earlier mechanistic insight.
- **Failure signatures**:
  - Hallucinated Agency: When substituting random words, models hallucinate agency rather than flagging the sentence as nonsensical.
  - Yes Bias: High accuracy on balanced sets but failure on unbalanced sets because the model answers "Yes" to everything.
  - Representational Collapse: In PCA plots, adversarial examples clustering directly on top of original examples.
- **First 3 experiments**:
  1. Verify the Probe Gap: Train a linear probe on the `last_noun` token at Layer 12 for Llama-3 on the AmbAdv syntactic set. Verify that probe accuracy (>90%) significantly exceeds the prompting accuracy (~45%).
  2. Test the "Yes Bias": Run Gemma-7b on the Lexical Ambiguity set using the 8 prompt templates. Confirm that the model predicts "Ambiguous" (True/Yes) for >90% of inputs, regardless of the ground truth.
  3. Adversarial Stress Test: Take the "Original" sentence "The man saw the woman with the telescope" and the "Random" variant "The man saw the woman with the book." Visualize the cosine similarity of the `last_noun` hidden states at Layer 12. Confirm they are distinct.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Why do language models encode ambiguity information in their representations but fail to leverage it in their generated outputs?
- **Basis in paper**: The paper concludes: "While ambiguity-related information seems present in the models' representations, they fail to leverage it effectively in their outputs," with probe accuracies exceeding 90% while prompting accuracy remains low.
- **Why unresolved**: The paper demonstrates the gap exists but does not investigate the causal mechanismsâ€”whether this is an architectural limitation, a training objective problem, or an alignment failure.
- **What evidence would resolve it**: Ablation studies intervening on specific layers or attention heads during generation, or training approaches that explicitly connect representations to output behavior.

### Open Question 2
- **Question**: Can models be trained or fine-tuned to better leverage their internal ambiguity representations for behavioral tasks?
- **Basis in paper**: The paper states the findings "suggest that LLMs may partially rely on memorization rather than ambiguity understanding" and shows the disconnect between probing and prompting performance.
- **Why unresolved**: The paper evaluates only pre-trained instruction-tuned models without exploring intervention methods that could bridge the encoding-expression gap.
- **What evidence would resolve it**: Experiments with targeted fine-tuning, reinforcement learning from probe-based rewards, or architectural modifications that channel ambiguity representations into output tokens.

### Open Question 3
- **Question**: How does ambiguity detection performance differ when using log-likelihood comparisons versus prompting?
- **Basis in paper**: The limitations section states: "a direct comparison of log-likelihood and prompting measures shows that prompting may systematically underestimate the model's true linguistic capabilities," noting that prompting requires interpreting the prompt and formatting output correctly.
- **Why unresolved**: The paper uses prompting as the primary evaluation paradigm and does not conduct log-likelihood comparisons on the AmbAdv dataset.
- **What evidence would resolve it**: Parallel experiments comparing log-likelihood scores for minimal pairs against prompting accuracy on the same sentences, controlling for prompt interpretation and output formatting.

### Open Question 4
- **Question**: Does the observed gap between encoded and expressed ambiguity generalize across languages beyond English?
- **Basis in paper**: The limitations section explicitly states: "a major limitation is that our dataset only includes English sentences, which limits its applicability to other languages."
- **Why unresolved**: AmbAdv contains only English sentences, so cross-linguistic generalization of the encoding-behavior gap remains untested.
- **What evidence would resolve it**: Replication of the probing and prompting experiments on translated AmbAdv variants in typologically diverse languages.

## Limitations
- Dataset restricted to English sentences, limiting cross-linguistic generalizability
- Focus exclusively on PP-attachment ambiguity, not other ambiguity types
- Linear probing assumes linear decodability of semantic information
- Does not account for potential confounding factors like sentence length or syntactic complexity

## Confidence

**High Confidence**: The finding that direct prompting exhibits "yes bias" across multiple models and that linear probes achieve higher accuracy than prompting. This is directly observable from the reported results and is a straightforward empirical claim.

**Medium Confidence**: The claim that models rely on memorization rather than generalized ambiguity understanding. While the clustering analysis and repeated disambiguation errors support this, alternative explanations exist (e.g., robust syntactic generalization or sensitivity to specific word order patterns that coincidentally match training data).

**Low Confidence**: The specific claim about the "last noun" and "EOS" tokens being uniquely important for ambiguity encoding. While probe accuracies are higher for these positions, the paper does not establish causal mechanisms or rule out dataset artifacts that might artificially elevate these positions as predictors.

## Next Checks

1. **Probe Architecture Ablation**: Replicate the probing experiments using different classifier architectures (e.g., small MLP vs. logistic regression) and feature sets (e.g., using attention weights or intermediate layer representations). This would test whether the linear probe results are specific to logistic regression or represent a more general property of the representations.

2. **Cross-Linguistic Transfer Test**: Apply the AmbAdv methodology to a translated version of the dataset in a morphologically rich language (e.g., German or Russian) where PP-attachment ambiguity manifests differently. This would test whether the token-specific encoding patterns (last noun, EOS) generalize beyond English syntax.

3. **Adversarial Perturbation Analysis**: Generate a new adversarial set where perturbations systematically vary the semantic plausibility of PP-attachment readings (e.g., keeping the "with" phrase instrumentally plausible vs. implausible) and measure whether models can distinguish these cases based on world knowledge rather than memorized patterns. This would test whether models are truly understanding the semantic consequences of attachment choices or merely pattern-matching.