---
ver: rpa2
title: 'Freeze and Cluster: A Simple Baseline for Rehearsal-Free Continual Category
  Discovery'
arxiv_id: '2503.09106'
source_url: https://arxiv.org/abs/2503.09106
tags:
- learning
- novel
- continual
- classes
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Freeze and Cluster: A Simple Baseline for Rehearsal-Free Continual Category Discovery

## Quick Facts
- **arXiv ID:** 2503.09106
- **Source URL:** https://arxiv.org/abs/2503.09106
- **Authors:** Chuyu Zhang, Xueyang Yu, Peiyan Gu, Xuming He
- **Reference count:** 19
- **Primary result:** Achieves state-of-the-art performance on multiple continual category discovery benchmarks using a simple freeze-and-cluster approach

## Executive Summary
This paper proposes FAC (Freeze And Cluster), a simple baseline for Rehearsal-Free Continual Category Discovery (RF-CCD). The method learns labeled classes in session 0, then discovers novel classes in unlabeled sessions 1 to T without storing raw data. The key insight is to freeze a pre-trained backbone, use K-means clustering on extracted features, and update the classifier via generative replay. This approach achieves competitive performance while being conceptually simpler than existing methods that rely on complex pseudo-label refinement or memory replay.

## Method Summary
FAC operates in two phases: supervised adaptation and continual learning. In session 0, the last transformer block is fine-tuned on labeled data while the rest of the backbone is frozen. For sessions t>0, the entire backbone remains frozen, and novel classes are discovered through K-means clustering on frozen features. The classifier is updated using generative replay, where features are sampled from Gaussian distributions fitted to each cluster. Logit normalization with temperature 0.1 is applied to prevent overconfidence. The method estimates the number of novel classes by over-clustering and merging sub-clusters based on a distance threshold d_min.

## Key Results
- Achieves state-of-the-art performance on CUB200, StanfordCars196, Tiny-ImageNet200, and iNat550 datasets
- Simple implementation with minimal hyperparameters compared to complex pseudo-label refinement methods
- Demonstrates that freezing the backbone and using generative replay is sufficient for effective continual category discovery

## Why This Works (Mechanism)
The method works by leveraging the strong pre-trained features from DINO while preventing catastrophic forgetting through backbone freezing. K-means clustering effectively groups similar features without requiring labeled data, and generative replay maintains knowledge of previous classes without storing raw examples. The Logit Normalization ensures balanced learning across sessions by preventing the classifier from becoming overconfident about frequently sampled classes.

## Foundational Learning
- **K-means Clustering:** Groups unlabeled data into clusters based on feature similarity; needed to discover novel classes without labels
- **Generative Replay:** Samples synthetic features from learned distributions to maintain knowledge of old classes; needed to prevent catastrophic forgetting without storing raw data
- **Logit Normalization:** Applies temperature scaling to logits to prevent overconfidence; needed to ensure balanced learning across multiple sessions
- **Hungarian Matching:** Used for evaluation to handle class index permutation across sessions; needed for fair comparison of discovered classes
- **DINO Pre-training:** Self-supervised learning method that provides strong visual features; needed as foundation for effective clustering

## Architecture Onboarding
- **Component Map:** DINO ViT-B/16 Backbone -> Feature Extractor -> K-means Clustering -> Gaussian Distribution Fitting -> Generative Replay -> Classifier Update
- **Critical Path:** Backbone freezing -> K-means clustering -> Generative replay sampling -> Classifier training
- **Design Tradeoffs:** Simpler than pseudo-label refinement methods but may be less accurate in noisy scenarios; trades implementation complexity for potentially lower precision
- **Failure Signatures:** 
  - Representation degradation if backbone is unfrozen
  - Classifier bias toward novel classes without Logit Normalization
  - Cluster collapse if K-means parameters are poorly chosen
- **3 First Experiments:**
  1. Implement complete pipeline on CUB200 using ground truth class counts for K-means validation
  2. Compare performance with and without backbone freezing to verify its necessity
  3. Test different Logit Normalization temperatures to find optimal balance

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Critical hyperparameters for supervised adaptation stage are not specified
- Merging threshold derivation algorithm lacks sufficient detail
- Evaluation protocol details for handling class imbalance are not fully described

## Confidence
- **High:** Core conceptual framework (freeze backbone, K-means clustering, generative replay) is clearly described and experimentally validated
- **Medium:** Implementation details for generative replay and classifier training are adequately specified, though hyperparameter values remain unknown
- **Low:** Pseudo-label estimation pipeline (over-clustering, merging threshold, cluster validation) lacks sufficient detail for exact replication

## Next Checks
1. Implement the complete pipeline on a single dataset (CUB200) using ground truth class counts for K-means to validate the overall framework before implementing the pseudo-label estimation component
2. Conduct ablation studies on the supervised adaptation stage to determine optimal fine-tuning duration and learning rate for establishing the merging threshold d_min
3. Compare the proposed method against standard rehearsal-free baselines (e.g., fine-tuning without replay) on the same datasets to verify the claimed performance improvements are attributable to the proposed components rather than dataset-specific factors