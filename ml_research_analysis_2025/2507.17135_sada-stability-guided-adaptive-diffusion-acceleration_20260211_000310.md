---
ver: rpa2
title: 'SADA: Stability-guided Adaptive Diffusion Acceleration'
arxiv_id: '2507.17135'
source_url: https://arxiv.org/abs/2507.17135
tags:
- diffusion
- sada
- acceleration
- uni00000013
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high computational cost of diffusion models,
  which stems from their iterative sampling process and quadratic attention mechanisms.
  The authors propose SADA (Stability-guided Adaptive Diffusion Acceleration), a training-free
  framework that dynamically allocates sparsity decisions (step-wise and token-wise)
  via a unified stability criterion derived from the underlying ODE formulation.
---

# SADA: Stability-guided Adaptive Diffusion Acceleration

## Quick Facts
- arXiv ID: 2507.17135
- Source URL: https://arxiv.org/abs/2507.17135
- Reference count: 40
- Key outcome: SADA achieves ≥1.8× speedup with LPIPS ≤0.10 and FID ≤4.5 compared to unmodified diffusion baselines.

## Executive Summary
This paper addresses the high computational cost of diffusion models by introducing SADA (Stability-guided Adaptive Diffusion Acceleration), a training-free framework that dynamically allocates sparsity decisions during sampling. SADA uses a stability criterion derived from the underlying ODE formulation to make principled approximation schemes, correcting for errors that previous methods suffer from. Comprehensive evaluations on SD-2, SDXL, and Flux show consistent speedups with minimal fidelity degradation.

## Method Summary
SADA is a training-free framework that accelerates diffusion models by dynamically exploiting step-wise and token-wise sparsity based on a unified stability criterion. The method computes a stability criterion using the second-order difference of the ODE velocity and extrapolation error to classify timesteps as stable or unstable. When stable, SADA applies step-wise approximation using Adams-Moulton methods; when unstable, it uses token-wise cache-assisted pruning. The framework integrates with existing ODE solvers (EDM, DPM++) and adapts to different modalities without modification.

## Key Results
- Achieves ≥1.8× speedup across SD-2, SDXL, and Flux models
- Maintains fidelity with LPIPS ≤0.10 and FID ≤4.5 compared to baselines
- Outperforms prior methods like DeepCache and AdaptiveDiffusion
- Adapts to other pipelines (ControlNet) and modalities (MusicLDM) without modifications

## Why This Works (Mechanism)

### Mechanism 1: Stability Criterion for Adaptive Sparsity
SADA computes a stability criterion using the second-order difference of ODE velocity (∆(2)yt) and extrapolation error (xt−1 − x̂t−1). When these are anti-aligned ((xt−1 − x̂t−1) · ∆(2)yt < 0), the timestep is deemed stable and eligible for acceleration. This leverages precise gradient information from the numerical ODE solver rather than heuristic thresholds.

### Mechanism 2: Principled Step-wise Approximation
SADA uses third-order Adams-Moulton methods to estimate xt−1 when stable, with error bounds of O(∆t²). This allows skipping denoising steps while maintaining fidelity through higher-order numerical approximation.

### Mechanism 3: Token-wise Cache-Assisted Pruning
When step-wise acceleration is unsafe, SADA partitions tokens into unstable (full compute) and stable (cache-reconstructed) sets. This exploits redundancy in spatial tokens without the low-pass filtering effects of token merging.

## Foundational Learning

- **Concept: Probability Flow ODE (PF-ODE) and Numerical Solvers**
  - Why needed here: Diffusion sampling is reformulated as solving dx/dt = f(t)xt + g²(t)/(2σt)ϵθ(xt,t). SADA leverages the velocity yt = dx/dt from solvers like EDM or DPM++ for stability decisions.
  - Quick check question: Can you explain why the velocity yt is central to SADA's stability criterion rather than the noise prediction ϵt alone?

- **Concept: Adams-Moulton Methods and Lagrange Interpolation**
  - Why needed here: SADA uses third-order Adams-Moulton for step-wise approximation and Lagrange interpolation for multistep reconstruction. Understanding local truncation error O(∆t²) vs. O(hᵏ⁺¹) is critical.
  - Quick check question: Given a 50-step schedule with stable regime after step 30, how would you choose k for Lagrange interpolation to balance error vs. cache size?

- **Concept: Token Pruning vs. Token Merging**
  - Why needed here: SADA uses pruning over merging to avoid low-pass filtering effects. Merging averages tokens, suppressing high-frequency details.
  - Quick check question: Why does SADA reconstruct pruned tokens from cache instead of merging similar tokens? What artifact would merging introduce?

## Architecture Onboarding

- **Component map:** Criterion Module → Approximation Engine → Token Pruner → Cache Manager
- **Critical path:** At each timestep t, compute forward pass → extract yt from solver → evaluate Criterion 3.4 → branch: (True) skip/prune step with approximation; (False) apply token-wise pruning with cache reconstruction.
- **Design tradeoffs:**
  - Speedup vs. Fidelity: Higher pruning ratios increase speedup but raise LPIPS/FID
  - Cache Size vs. Memory: Larger k for Lagrange improves accuracy but increases memory
  - Token Pruning Threshold: Aggressive reduction speeds up attention but risks losing fine details
- **Failure signatures:**
  - Over-pruning in unstable regimes causes FID spikes with visible artifacts
  - Cache staleness produces blurry regions when attention patterns drift
  - Solver mismatch yields incorrect yt, breaking the stability criterion
- **First 3 experiments:**
  1. Reproduce Table 1 baseline on SD-2 with DPM++ 50 steps on MS-COCO prompts
  2. Ablation on criterion sensitivity by varying stability threshold for SDXL
  3. Cross-modal transfer to MusicLDM or ControlNet without modification

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the abstract or conclusion sections.

## Limitations
- Unknown thresholds for token partitioning based on stability metric
- Unspecified caching parameters (start timestep T* and interval i) for different models
- Dependency on specific ODE solvers (EDM, DPM++) for gradient extraction

## Confidence
- High Confidence: Core mechanism of stability criterion for adaptive sparsity is well-specified and theoretically grounded
- Medium Confidence: Generalizability to other modalities demonstrated but with less implementation detail
- Low Confidence: Robustness in extreme conditions (few steps, high-curvature trajectories, noisy gradients) not fully characterized

## Next Checks
1. Reproduce Table 1 baseline on SD-2 with DPM++ 50 steps on MS-COCO prompts
2. Ablation on criterion sensitivity by varying stability threshold for SDXL
3. Cross-modal transfer to MusicLDM or ControlNet without modification