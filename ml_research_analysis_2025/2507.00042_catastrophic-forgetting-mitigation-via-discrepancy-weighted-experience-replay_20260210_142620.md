---
ver: rpa2
title: Catastrophic Forgetting Mitigation via Discrepancy-Weighted Experience Replay
arxiv_id: '2507.00042'
source_url: https://arxiv.org/abs/2507.00042
tags:
- er-emu
- edge
- experience
- data
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses catastrophic forgetting in cloud-edge collaborative
  object detection for traffic monitoring, where edge models lose previously learned
  knowledge when adapting to new data distributions, particularly problematic in dynamic
  traffic environments with cyclical patterns like day/night transitions. The proposed
  ER-EMU algorithm introduces adaptive experience replay to mitigate this issue.
---

# Catastrophic Forgetting Mitigation via Discrepancy-Weighted Experience Replay

## Quick Facts
- arXiv ID: 2507.00042
- Source URL: https://arxiv.org/abs/2507.00042
- Reference count: 26
- One-line primary result: ER-EMU improves state-of-the-art cloud-edge collaborative object detection performance with adaptive experience replay, achieving significant gains in repeated day/night traffic scenarios.

## Executive Summary
This paper addresses catastrophic forgetting in cloud-edge collaborative object detection for traffic monitoring, where edge models lose previously learned knowledge when adapting to new data distributions. The proposed ER-EMU algorithm introduces adaptive experience replay with a novel Domain Distance Metric-based Experience Selection (DDM-ES) algorithm that uses multi-kernel maximum mean discrepancy (MK-MMD) to quantify domain dissimilarity and prioritize historical data most dissimilar to the current target domain. Experiments on the Bellevue traffic video dataset with repeated day/night cycles show that ER-EMU consistently improves performance of several state-of-the-art frameworks, with enhanced adaptation to repeated scenarios and significant performance gains over baselines.

## Method Summary
The ER-EMU algorithm mitigates catastrophic forgetting through adaptive experience replay in cloud-edge collaborative object detection. It employs a limited-size experience buffer managed with FIFO principle and uses DDM-ES to select historical domains most dissimilar to the current target domain based on MK-MMD. The algorithm combines current domain data with weighted historical data using a combined loss function, where weights are computed via sigmoid(distance). Buffer updates use random sampling to maintain balanced representation across domains. The method targets edge models that receive pseudo-labels from cloud models and must adapt to evolving traffic conditions while preserving knowledge of previously encountered scenarios.

## Key Results
- ER-EMU consistently improves state-of-the-art cloud-edge collaborative object detection frameworks on Bellevue traffic dataset
- Enhanced adaptation to repeated day/night scenarios with significant performance gains over baselines
- DDM-ES component shown to be crucial for effectiveness through ablation studies
- Algorithm demonstrates robustness to hyperparameter variations across different settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prioritizing historically dissimilar domains during replay mitigates catastrophic forgetting more effectively than uniform sampling.
- Mechanism: The DDM-ES algorithm computes Multi-Kernel Maximum Mean Discrepancy (MK-MMD) between current target domain and each historical domain in buffer. The l most distant domains are selected and weighted via sigmoid(distance), ensuring domains with larger distributional shift contribute more to training. This enforces training diversity and prevents overfitting to current domain.
- Core assumption: Domains with higher MK-MMD distance contain knowledge more at risk of being forgotten.
- Evidence anchors: [abstract] DDM-ES employs MK-MMD to quantify dissimilarity between target domains; [section 4.1] Equation 3 defines MK-MMD; [corpus] Related work confirms replay buffer effectiveness for domain retention.

### Mechanism 2
- Claim: Combining current domain data with small, diversity-maximizing subset of historical data preserves prior knowledge without requiring full data replay.
- Mechanism: Loss function combines current domain loss L_t with weighted sum over selected historical domain losses: L_Det = L_t + Σ w_hj * L_t(D_hj). Sigmoid-weighted replay term acts as regularizer that anchors model to dissimilar past distributions while adapting to new data.
- Core assumption: Small buffer (30 domains × 200 samples) captures sufficient representational diversity.
- Evidence anchors: [section 4.3] Equation 5 formalizes combined loss; [Table 1] M_size=30, h=200, l=5; [section 5.2] ER-EMU variants show consistent mAP improvements across repeated day/night cycles.

### Mechanism 3
- Claim: Random sampling per domain combined with FIFO eviction maintains balanced, representative buffer without requiring domain importance scoring at insertion time.
- Mechanism: RS-EBU stores h randomly selected samples per domain. When full, evicts oldest h samples (FIFO) before inserting new random samples. Ensures each encountered domain has equal representation regardless of downstream utility.
- Core assumption: Within-domain samples are IID and equally informative.
- Evidence anchors: [section 4.2] Algorithm 2 specifies random sampling and FIFO eviction procedure; [section 5.4] Ablation shows random sampling ER-EMU still improves over baselines but underperforms full ER-EMU.

## Foundational Learning

- Concept: **Maximum Mean Discrepancy (MMD)**
  - Why needed here: Core metric for quantifying distributional distance between domains; MK-MMD extends single-kernel MMD for better expressiveness.
  - Quick check question: Can you explain why a multi-kernel approach captures more complex distributional differences than a single RBF kernel?

- Concept: **Experience Replay in Continual Learning**
  - Why needed here: ER-EMU is fundamentally a replay-based method; understanding why interleaving past data prevents weight drift is essential.
  - Quick check question: Why does replay mitigate catastrophic forgetting at the optimization level (think: gradient interference)?

- Concept: **Cloud-Edge Collaborative Architectures**
  - Why needed here: The algorithm targets edge model updates where cloud provides pseudo-labels; resource constraints motivate limited buffer sizes.
  - Quick check question: What constraints differentiate edge model updates from centralized retraining, and how does this influence buffer design?

## Architecture Onboarding

- Component map: [New Target Domain Dt_i] → [DDM-ES] ←→ [Experience Buffer M (FIFO, size=30 domains × 200 samples)] → [Combined Training Batch: current data + l=5 weighted historical domains] → [Edge Model θ_edge update via L_Det] → [RS-EBU] → Update Buffer M

- Critical path: DDM-ES selection → weighted loss computation → model update. The MK-MMD computation per historical domain is the computational bottleneck at O(n_a × n_b) per comparison.

- Design tradeoffs:
  - Buffer size (M_size) vs. edge memory constraints: Paper uses 30 domains × 200 samples; larger buffers improve coverage but may be infeasible.
  - l (selected domains) vs. training overhead: l=5 default; more domains increase computation but ablation shows low sensitivity.
  - MK-MMD kernel selection: Paper does not specify exact kernels; Assumption: standard RBF family with learned weights.

- Failure signatures:
  - Performance plateaus despite ER-EMU: Likely buffer saturation with redundant domains; consider increasing M_size or diversity-aware insertion.
  - Large variance across repeated cycles: May indicate random sampling instability; verify seed consistency and consider stratified sampling.
  - Minimal gain over baseline: Check if DDM-ES is actually selecting dissimilar domains (log distance distributions); MK-MMD may need kernel tuning.

- First 3 experiments:
  1. Implement ER-EMU on Shoggoth backbone with Bellevue day/night split; reproduce Table 2 gains to validate implementation.
  2. Run random-sampling ER-EMU vs. full ER-EMU vs. no replay; confirm DDM-ES contribution matches Figure 4a patterns.
  3. Vary l ∈ {1, 3, 5, 7, 10} and M_size ∈ {10, 20, 30, 50}; verify robustness claims and identify breaking points for your target hardware.

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- MK-MMD kernel specification remains undefined, limiting exact reproduction
- Exact training hyperparameters (learning rate, batch size, optimizer, epochs) are not provided
- Performance comparisons rely on baselines that may have unreported implementation variations

## Confidence
- High: The FIFO buffer management and random sampling procedures are clearly specified and reproducible.
- Medium: The DDM-ES mechanism using MK-MMD is well-described conceptually, but implementation details (kernel selection, feature extraction) require assumptions.
- Low: Performance comparisons rely on baselines (DCC, LVACCL, Shoggoth) that may have unreported implementation variations affecting absolute numbers.

## Next Checks
1. Implement MK-MMD with standard RBF kernels and verify it produces meaningful domain distance scores on sample Bellevue domains.
2. Run ablation experiments varying l (selected domains) and M_size to confirm the reported robustness and identify breaking points.
3. Test ER-EMU on a simpler dataset (e.g., CIFAR-100 with sequential domain splits) to isolate whether gains are due to algorithm or specific dataset characteristics.