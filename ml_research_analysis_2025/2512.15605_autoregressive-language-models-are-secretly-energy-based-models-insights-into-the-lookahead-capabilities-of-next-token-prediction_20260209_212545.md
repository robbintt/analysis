---
ver: rpa2
title: 'Autoregressive Language Models are Secretly Energy-Based Models: Insights
  into the Lookahead Capabilities of Next-Token Prediction'
arxiv_id: '2512.15605'
source_url: https://arxiv.org/abs/2512.15605
tags:
- learning
- language
- arms
- ebms
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a bijection between autoregressive models
  (ARMs) and energy-based models (EBMs) through the chain rule of probability, showing
  that ARMs can represent any EBM distribution. The key insight is that the bijection
  corresponds to a soft Bellman equation from maximum entropy reinforcement learning.
---

# Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction

## Quick Facts
- **arXiv ID:** 2512.15605
- **Source URL:** https://arxiv.org/abs/2512.15605
- **Reference count:** 40
- **Primary result:** ARMs and EBMs are theoretically equivalent in function space, with teacher forcing implicitly computing the ARM's partition function along observed paths

## Executive Summary
This paper establishes a formal equivalence between autoregressive language models (ARMs) and energy-based models (EBMs) through a bijection that corresponds to a soft Bellman equation from maximum entropy reinforcement learning. The key insight is that ARMs can represent any EBM distribution by implicitly encoding a soft value function that marginalizes over all future continuations. This theoretical framework provides justification for the next-token prediction paradigm and teacher forcing, showing they are optimal in function space. The work also derives error bounds for distilling EBMs into ARMs and validates the theory through numerical experiments with tiny language models.

## Method Summary
The paper proves that any sequence distribution can be factorized via the chain rule into next-token conditionals (ARM form), and conversely that next-token conditionals uniquely reconstruct the sequence distribution (EBM form). This establishes a bijection M between ARM logits q and EBM energy function R, corresponding to a soft Bellman equation. In function space, minimizing negative log-likelihood yields identical optima for both architectures. The paper derives KL bounds for EBM-to-ARM distillation and validates the theory by training both architectures on synthetic tiny language models with V=8, T=4, showing convergence to the same minimum and validating the bijection mapping.

## Key Results
- ARMs and EBMs are equivalent in function space via a bijection corresponding to a soft Bellman equation
- Teacher forcing is optimal for ARM training, implicitly computing the partition function along observed paths
- Error bounds exist for distilling EBMs into ARMs, proportional to sequence length and logits error
- Numerical experiments confirm theoretical predictions with tiny language models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The chain rule of probability induces a bijective mapping between ARMs and EBMs, enabling ARMs to represent globally normalized distributions.
- **Mechanism:** Any sequence distribution can be factorized via the chain rule into next-token conditionals (ARM form). Conversely, next-token conditionals uniquely reconstruct the sequence distribution. This bijection corresponds to a soft Bellman equation, where the ARM's logits embed a "soft value function" that marginalizes over all future continuations.
- **Core assumption:** The function space is tabular (no parameterization constraints), and the chain rule decomposition holds exactly for the target distribution.
- **Evidence anchors:**
  - [abstract] "establish an explicit bijection between ARMs and EBMs in function space, which we show to correspond to a special case of the soft Bellman equation"
  - [Section 3.1–3.2] Derives the mapping q=M(r) and its inverse, showing equivalence of distributions.
  - [corpus] Related work on EBMs (e.g., "Joint Learning of Energy-based Models") supports the tractability challenges EBMs face, indirectly motivating the ARM-EBM connection.
- **Break condition:** If the model capacity is insufficient to represent the exact bijection in weight space (function approximation), or if the chain rule cannot decompose the true data distribution (e.g., non-Markovian dependencies beyond context window).

### Mechanism 2
- **Claim:** In function space, minimizing the negative log-likelihood of ARMs and EBMs yields identical optima, providing theoretical justification for teacher forcing.
- **Mechanism:** The bijection ensures that the ARM's loss landscape (teacher forcing on observed sequences) and the EBM's loss (global normalization) have the same global minimum. Teacher forcing implicitly computes the ARM's partition function along the observed path rather than summing over all paths.
- **Core assumption:** Optimization reaches the global minimum in function space, and the data distribution lies within the model's representational capacity.
- **Evidence anchors:**
  - [abstract] "derive the equivalence between supervised learning of ARMs and EBMs"
  - [Section 4.1, Proposition 2] Proves equivalence of minima and optimality of teacher forcing in function space.
  - [corpus] Corpus evidence is weak on this specific equivalence; no direct citations.
- **Break condition:** Under function approximation, optimization may converge to different local minima; empirical gaps may arise from architecture differences (causal vs. non-causal Transformers).

### Mechanism 3
- **Claim:** Distilling an EBM into an ARM (as in MaxEnt RL) incurs bounded approximation error, linking RL alignment to the ARM-EBM bijection.
- **Mechanism:** The KL divergence between an EBM and an ARM is bounded by the L∞ error between the ARM's logits and the optimal logits from the bijection. This provides a quantitative guarantee for distillation.
- **Core assumption:** The soft Bellman fixed point can be approximated by a causal Transformer with sufficient capacity.
- **Evidence anchors:**
  - [abstract] "analyze the distillation of EBMs into ARMs by providing theoretical error bounds"
  - [Section 4.2, Proposition 3] Derives KL bound proportional to sequence length and maximum logits error.
  - [corpus] Work on "EBT-Policy" and energy-based models in RL supports the broader relevance of energy formulations for policy learning.
- **Break condition:** If the Transformer's embedding dimension must scale with vocabulary size (as noted in the paper), practical implementations may violate the approximation guarantee.

## Foundational Learning

- **Concept: Energy-based models (EBMs)**
  - Why needed here: EBMs define sequence-level distributions via a global energy function; understanding their normalization challenges clarifies why ARMs are preferable in practice despite theoretical equivalence.
  - Quick check question: Can you explain why EBMs require computing a partition function and how this differs from ARMs' local normalization?

- **Concept: Maximum entropy reinforcement learning (MaxEnt RL)**
  - Why needed here: The paper shows the ARM-EBM bijection corresponds to a soft Bellman equation; MaxEnt RL provides the RL framework where this connection emerges.
  - Quick check question: How does entropy regularization in RL relate to the soft value function in the bijection?

- **Concept: Chain rule of probability**
  - Why needed here: It is the mathematical foundation for the bijection, enabling decomposition of sequence distributions into next-token conditionals.
  - Quick check question: If you have a sequence distribution p(y|x), how would you compute the next-token conditionals π(yt|st) via the chain rule?

## Architecture Onboarding

- **Component map:**
  - EBM: Non-causal (bidirectional) Transformer + global pooling (e.g., mean or CLS token) to scalar energy
  - ARM: Causal Transformer with autoregressive mask + per-position projection to logits over vocabulary

- **Critical path:**
  1. Implement both architectures with identical backbones (differing only in masking/pooling)
  2. Train on the same data distribution via maximum likelihood
  3. Verify convergence to the same loss minimum (Proposition 2)
  4. Optionally, compute the bijection mapping M to convert EBM logits to ARM logits and compare

- **Design tradeoffs:**
  - EBM: Tractable for small vocabularies/sequence lengths (exact partition function), but scales poorly (O(V^T)). Enables lookahead but sampling requires MCMC.
  - ARM: Scales efficiently (O(T)), supports parallel training and exact sampling, but logits must implicitly encode soft value function (harder to learn).
  - Equivalence holds in function space; in weight space, causal constraints may limit ARM expressivity compared to non-causal EBM.

- **Failure signatures:**
  - ARM and EBM losses do not converge to similar values → likely optimization or capacity issues.
  - Logits distance between ARM and optimal EBM does not decrease → bijection not learned; check architecture capacity or training stability.
  - KL bound violated → logits error may be high; verify L∞ distance and sequence length scaling.

- **First 3 experiments:**
  1. **Toy-scale validation:** Use tiny vocabulary (V=8) and short sequences (T=4) to exactly compute EBM partition function; train both models and verify loss convergence (replicate Figure 2).
  2. **Logits mapping test:** After training EBM, compute the bijection M to obtain ARM logits; compare with trained ARM's logits (L∞ distance before/after mapping).
  3. **KL bound check:** Measure KL divergence between EBM and ARM distributions and compare against the theoretical bound from Proposition 3; plot against logits error to validate scaling.

## Open Questions the Paper Calls Out

- **Question:** What is the formal complexity comparison between learning the EBM reward function R versus learning the ARM scoring function q, and under what conditions does one approach have a computational or statistical advantage?
  - Basis in paper: [explicit] "Formally comparing the complexity of learning R versus that of learning q is a promising future direction."
  - Why unresolved: The paper establishes equivalence in function space but notes that R can use non-causal Transformers while q must be causal, and q must simultaneously learn immediate local scores and future-looking value functions—suggesting potentially different learning dynamics.
  - What evidence would resolve it: Theoretical bounds on sample complexity and optimization dynamics for each approach, or empirical comparisons measuring convergence speed and sample efficiency across varying sequence lengths and vocabulary sizes.

- **Question:** How do latent variables (e.g., "thinking traces" or chain-of-thought) enhance the expressivity of ARMs in representing EBM distributions, and can they bridge the gap between theory and practice?
  - Basis in paper: [explicit] "Another important direction is to study the impact of latent variables (thinking traces) in enhancing the expressivity of ARMs."
  - Why unresolved: The bijection requires q to implicitly compute Vq (marginalizing over all futures), which may be computationally difficult for standard ARMs; latent variables could provide intermediate computation steps.
  - What evidence would resolve it: Experiments comparing standard ARMs versus ARMs with latent reasoning tokens on tasks requiring lookahead, measuring whether latent variables enable better approximation of EBM-optimal distributions.

- **Question:** What specific optimization and approximation errors prevent practical ARMs from achieving the theoretical equivalence to EBMs, and can modified training objectives reduce these gaps?
  - Basis in paper: [inferred] "Proposition 2 does not account for optimization or approximation error" and "the limitation of teacher forcing is not in the objective itself but in the difficulty of reaching its minimum, due to optimization or approximation error."
  - Why unresolved: The paper validates theory only on tiny synthetic models; real LLMs face function approximation constraints and optimization challenges that may prevent convergence to the EBM-equivalent minimum.
  - What evidence would resolve it: Systematic analysis of approximation error as model size and data scale vary, or development of training objectives that better approximate the soft Bellman equation structure inherent in the bijection.

## Limitations

- The theoretical equivalence holds in function space but may not translate to weight space due to architectural differences between causal and non-causal Transformers
- The bijection requires Transformer embedding dimensions that scale with vocabulary size, making it impractical for real-world applications
- Numerical experiments are limited to extremely small-scale models (V=8, T=4, ~3.6K parameters) that may not capture real language modeling complexity

## Confidence

**High Confidence:** The bijection between ARMs and EBMs in function space (Proposition 1) and the equivalence of their expected risk minima (Proposition 2) are mathematically rigorous and well-supported by derivations.

**Medium Confidence:** The error bounds for EBM-to-ARM distillation (Proposition 3) are theoretically sound but may be loose in practice.

**Low Confidence:** The claim that teacher forcing is optimal for ARM training may not fully capture empirical advantages in practice, as the paper doesn't address differences in training stability, convergence speed, or final performance on downstream tasks.

## Next Checks

1. **Scale-up validation**: Replicate experiments with larger vocabularies (V=32-64) and longer sequences (T=8-16) to test whether theoretical equivalence holds when computational advantages of ARMs become more significant.

2. **Weight space analysis**: Compare learned weights of ARMs and EBMs trained on same data to identify architectural constraints preventing perfect function space equivalence in practice.

3. **Generalization study**: Evaluate both ARM and EBM architectures on held-out sequences and downstream tasks to determine whether theoretical equivalence translates to similar generalization performance.