---
ver: rpa2
title: 'QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code'
arxiv_id: '2511.01183'
source_url: https://arxiv.org/abs/2511.01183
tags:
- code
- prompt
- assembly
- optimization
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of using large language models
  (LLMs) for IR-to-assembly compilation, which requires both functional correctness
  and competitive performance. The authors introduce NeuComBack, a benchmark dataset
  for IR-to-assembly compilation, and propose a self-evolving prompt optimization
  method that enables LLMs to iteratively improve their compilation strategies by
  learning from self-debugging traces.
---

# QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code

## Quick Facts
- **arXiv ID:** 2511.01183
- **Source URL:** https://arxiv.org/abs/2511.01183
- **Reference count:** 40
- **Primary result:** Functional correctness increased from 44% to 64% on x86_64 and from 36% to 58% on aarch64, with 87.5% of correctly generated x86_64 programs surpassing clang-O3 performance

## Executive Summary
This paper addresses the challenge of using large language models (LLMs) for IR-to-assembly compilation, which requires both functional correctness and competitive performance. The authors introduce NeuComBack, a benchmark dataset for IR-to-assembly compilation, and propose a self-evolving prompt optimization method that enables LLMs to iteratively improve their compilation strategies by learning from self-debugging traces. Experiments show significant improvements: functional correctness increased from 44% to 64% on x86_64 and from 36% to 58% on aarch64, with 87.5% of correctly generated x86_64 programs surpassing clang-O3 performance. These consistent gains across multiple architectures and benchmarks validate the effectiveness of the approach for low-level neural compilation.

## Method Summary
The approach uses a self-evolving prompt optimization framework where LLMs iteratively improve their compilation capabilities by learning from self-debugging traces. The method operates in two stages: offline prompt learning and online inference. In the offline phase, the LLM generates assembly from LLVM IR, self-debugging fails, and the LLM analyzes its own error patterns to evolve the prompt. This process repeats for multiple epochs. In the online phase, the evolved prompt guides generation with optional iterative optimization rounds, each followed by self-debugging to ensure correctness. The NeuComBack benchmark provides curated test programs from ExeBench and TSVC, focusing on kernel-like functions that are challenging for traditional neural translation methods.

## Key Results
- Functional correctness improved from 44% to 64% on x86_64 and from 36% to 58% on aarch64
- 87.5% of correctly generated x86_64 programs outperformed clang-O3 in performance
- Cross-architecture generalization achieved with evolved prompts transferring between x86_64 and aarch64
- Significant reduction in common assembly generation errors through prompt evolution

## Why This Works (Mechanism)

### Mechanism 1
Prompt optimization based on debugging traces improves functional correctness by reducing syntax and semantic errors. The offline prompt learning phase uses self-debugging traces to identify recurring error patterns (e.g., incorrect section usage, stack misalignment). The LLM then synthesizes rules from these failures and explicitly encodes them in the prompt. This creates a feedback loop: error → diagnosis → rule → prompt refinement.

### Mechanism 2
A staged chain-of-generation process, where an initial assembly draft is iteratively optimized, enables higher performance than single-pass generation. The online inference stage first prioritizes functional correctness through self-debugging. Once a correct baseline is established, further optimization rounds are run using the evolved prompt. This separates the objectives, allowing the LLM to first "get it right" and then "get it fast," reducing the cognitive load of simultaneous correctness and optimization.

### Mechanism 3
LLMs can discover non-obvious optimizations, such as instruction reduction or alternative vectorization strategies, that surpass standard compiler flags like `-O3`. LLMs treat code as a flexible sequence generation problem, not a set of rigid compiler passes. This allows them to find "creative" solutions, like reducing two `paddd` instructions to one via pre-calculation or using `cmpps` + `bsfl` for faster conditional loops, which a traditional compiler might miss due to its predefined heuristics.

## Foundational Learning

- **Concept:** **LLVM Intermediate Representation (IR)**
  - **Why needed here:** This is the input format the LLM must translate. Unlike C source code, IR is a lower-level, typed, SSA-form representation that is closer to assembly but still architecture-independent. Understanding its structure (phis, basic blocks, types) is critical for the translation task.
  - **Quick check question:** What is Static Single Assignment (SSA) form, and why does it simplify data flow analysis for both traditional compilers and LLMs?

- **Concept:** **Instruction Set Architecture (ISA)**
  - **Why needed here:** The paper targets x86_64 and aarch64. Each has different register names, instruction mnemonics (AT&T syntax vs. ARM), addressing modes, and calling conventions. The LLM must switch between these contexts based on the target architecture `M`.
  - **Quick check question:** What is the fundamental difference in how x86_64 and aarch64 handle function arguments and return values at the assembly level?

- **Concept:** **Prompt Engineering and Evolution**
  - **Why needed here:** The core method is not fine-tuning model weights, but rather fine-tuning the *prompt*. Understanding how instructions, examples, and constraints are formatted to guide the LLM is central to reproducing this work.
  - **Quick check question:** What is the difference between "zero-shot" prompting and a "few-shot" prompting approach, and which does this paper's method more closely resemble after prompt evolution?

## Architecture Onboarding

- **Component map:** NeuComBack Benchmark -> Offline Prompt Learning Loop -> Online Inference Engine
- **Critical path:**
  1. **Data Prep:** Build NeuComBack by compiling C source from ExeBench/TSVC to LLVM IR. This IR becomes the model input.
  2. **Offline Learning:** Run the prompt evolution loop (Figure 1, Stage 1) on the training split. The LLM analyzes its own failures. **Crucial Step:** The comparison and confirmation of prompt edits adds stability.
  3. **Evaluation:** Apply the final evolved prompt to the held-out test set (Figure 1, Stage 2). Run the generated assembly through the correctness and performance tests.

- **Design tradeoffs:**
  - **Data Scope vs. Realism:** The benchmark focuses on kernel-like functions (TSVC) and embedded code (ExeBench). It does not cover OS-level code, heavy pointer arithmetic, or complex dynamic memory. Gains here may not transfer.
  - **Cost vs. Correctness:** The method relies on DeepSeek-R1, a reasoning model with higher inference cost. A cost-controlled experiment (Appendix F) showed a cheaper model (DeepSeek-V3) with 14x sampling could solve *more* problems. This raises a key tradeoff: is a single expensive reasoning pass better than multiple cheap samples?
  - **Generalization:** The learned prompt is shown to transfer reasonably well from L2 to L1 datasets, but its effectiveness on completely different ISAs (e.g., RISC-V) or programming domains is untested.

- **Failure signatures:**
  - **Format Errors:** The model generates assembly that violates the requested template (e.g., including commentary outside the code block). The evolved prompt's formatting rules should reduce this.
  - **Subtle Semantic Bugs:** The self-debugging loop may pass if the test inputs are insufficient. A program might crash on edge cases not covered by the benchmark's test harness. The ACC metric only guarantees correctness on *provided* inputs.
  - **Optimization Regression:** An optimization round might introduce a subtle logic change while trying to make the code faster, breaking correctness. The "test & self-debug" step after each optimization round is the safeguard.

- **First 3 experiments:**
  1. **Baseline Establishment:** Run the raw prompt (Appendix B) with your chosen LLM on the NeuComBack-L2 test set. Measure ACC (correctness). This confirms your setup matches the paper's initial performance (around 44% for DeepSeek-R1).
  2. **Prompt Ablation:** Take a simple error-correction prompt (not the full evolved one) and see if correctness improves at all. This validates the core hypothesis that structured feedback helps.
  3. **Architecture Transfer:** Train the prompt on x86_64 (as in the paper) and then apply it *without further training* to aarch64. Compare the results to the paper's aarch64-specific prompt results to understand cross-architecture transferability.

## Open Questions the Paper Calls Out

- **Can the self-evolving prompt optimization framework effectively transfer to smaller or weaker Large Language Models while maintaining efficiency?** The current method demonstrates success primarily on advanced reasoning models like DeepSeek-R1, leaving the performance gap for non-reasoning or distilled models unknown.

- **Can neural compilation techniques maintain functional correctness and performance when applied to programs featuring recursion, dynamic memory allocation, and irregular control flow?** The study focused on loop-centric kernels (TSVC) and basic correctness (ExeBench), avoiding complex memory hierarchies and control structures that traditionally challenge compilers.

- **Would incorporating evolutionary algorithms or other search heuristics improve the prompt discovery process compared to the current LLM-as-optimizer approach?** The current method relies on a single LLM to propose and review prompt edits, which may limit the search space diversity compared to population-based search methods.

## Limitations

- The benchmark focuses on kernel-like functions and embedded code, not full applications or OS-level code, limiting real-world applicability
- The method requires significant computational resources, particularly DeepSeek-R1 for prompt evolution, though cheaper alternatives with sampling may be viable
- The approach assumes the LLM can meaningfully interpret its own debug traces and generate generalizable correction rules, which may not hold for more complex error patterns or different programming domains

## Confidence

- **High Confidence:** The core mechanism of using self-debugging traces to evolve prompts is well-supported by the results, with clear quantitative improvements in both correctness and performance metrics
- **Medium Confidence:** The generalization of the evolved prompt to different architectures (x86_64 to aarch64) appears reasonable but would benefit from testing on additional ISAs like RISC-V
- **Low Confidence:** The claim that LLMs can discover non-obvious optimizations that surpass clang-O3 is intriguing but needs more validation on diverse codebases beyond the specific benchmarks used

## Next Checks

1. **Architecture Transferability Test:** Apply the x86_64-evolved prompt to RISC-V assembly generation without further training and measure correctness/performance degradation compared to architecture-specific evolution

2. **Cost-Effectiveness Analysis:** Implement the DeepSeek-V3 sampling approach from Appendix F on the full test suite to quantify the tradeoff between inference cost and problem-solving capability

3. **Robustness to Edge Cases:** Design adversarial test cases with subtle semantic bugs (e.g., integer overflow conditions, edge-case array indexing) to evaluate whether the self-debugging mechanism catches failures beyond the provided test harness inputs