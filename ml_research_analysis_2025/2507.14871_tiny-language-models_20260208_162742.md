---
ver: rpa2
title: Tiny language models
arxiv_id: '2507.14871'
source_url: https://arxiv.org/abs/2507.14871
tags:
- table
- accuracy
- dataset
- classification
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether tiny language models (TLMs), which\
  \ are pre-trained on datasets that are 10\u22123 \u2212 10\u22124 times smaller\
  \ than those used for large language models (LLMs), can exhibit the same key qualitative\
  \ features as LLMs. The study focuses on pre-training BERT-6 and variants of BERT-1\
  \ on a subset of the Wikipedia dataset and evaluating their performance on FewRel,\
  \ AGNews, and DBPedia classification tasks."
---

# Tiny language models

## Quick Facts
- arXiv ID: 2507.14871
- Source URL: https://arxiv.org/abs/2507.14871
- Authors: Ronit D. Gross; Yarden Tzach; Tal Halevi; Ella Koresh; Ido Kanter
- Reference count: 40
- Primary result: Pre-training tiny language models on dramatically reduced datasets (10⁻³–10⁻⁴ of standard LLM scales) still yields measurable performance gains and can be replicated by ensembles of shallow architectures.

## Executive Summary
This paper investigates whether tiny language models (TLMs), pre-trained on datasets 10⁻³–10⁻⁴ times smaller than standard LLMs, can exhibit the same qualitative features as their larger counterparts. The study focuses on pre-training BERT-6 and BERT-1 variants on small Wikipedia subsets and evaluating their performance on FewRel, AGNews, and DBPedia classification tasks. The primary results demonstrate that TLMs show a clear performance gap between pre-trained and non-pre-trained models across classification tasks, indicating the effectiveness of pre-training even at a tiny scale. Furthermore, the classification accuracy achieved by a pre-trained deep TLM architecture can be replicated through a soft committee of multiple, independently pre-trained shallow architectures, enabling low-latency TLMs without affecting classification accuracy.

## Method Summary
The study pre-trains BERT-6 (6-layer transformer) and BERT-1 variants (1-layer with different configurations) on Wikipedia subsets ranging from 2,000 to 90,000 paragraphs using Masked Language Modeling (MLM). Models are then fine-tuned on FewRel, AGNews, and DBPedia classification tasks. A "soft committee" approach combines outputs from multiple independently pre-trained shallow BERT-1 variants to match the accuracy of a single deep BERT-6 model while reducing inference latency. The research examines how pre-training dataset size and token overlap with downstream tasks affect performance, with experiments using the bert-base-uncased tokenizer and max sequence length of 128 tokens.

## Key Results
- TLMs show a clear performance gap between pre-trained and non-pre-trained models on downstream classification tasks
- Performance gap increases with pre-training dataset size and token overlap with classification datasets
- Classification accuracy of a deep pre-trained model (BERT-6) can be replicated by a soft committee of independently pre-trained shallow architectures (BERT-1 variants)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training effectiveness is retained at dramatically reduced dataset scales (10⁻³–10⁻⁴ of standard LLM datasets).
- Mechanism: A small pre-training corpus containing a substantial fraction of the target task's token space enables useful representations to be learned, creating a measurable performance gap between pre-trained and non-pre-trained models.
- Core assumption: Token overlap between pre-training and fine-tuning datasets is a primary driver of transferability.
- Evidence anchors: Table 3 shows that with Ws = 90,000 paragraphs (≈1.5% of full Wikipedia), models achieve 50% to 90% of the maximal performance gap observed with full pre-training.

### Mechanism 2
- Claim: The accuracy of a single deep model (BERT-6) can be replicated by an ensemble of shallower, independently pre-trained models (BERT-1 variants).
- Mechanism: A "soft committee" aggregates outputs of multiple diverse shallow architectures, where diversity in learned features compensates for reduced depth, achieving comparable accuracy with lower inference latency.
- Core assumption: Diverse inductive biases of different shallow architectural variants, when combined, can approximate the function space of a deeper model.
- Evidence anchors: Table 8 demonstrates that a soft committee of five BERT-1 variants achieves accuracy of 0.866 on FewRel, matching the 0.866 accuracy of a single BERT-6 model.

### Mechanism 3
- Claim: Classification accuracy is strongly and directly influenced by the lexical overlap between pre-training and downstream datasets.
- Mechanism: Performance gains from pre-training degrade as the number of tokens present in the classification task but missing from the pre-training corpus increases, indicating pre-training efficiency is maximized when data is lexically aligned with the target task.
- Core assumption: The utility of pre-training is primarily derived from learning representations for specific tokens encountered during fine-tuning.
- Evidence anchors: Table 4 demonstrates this by artificially increasing missing tokens while holding other variables constant, resulting in decreased accuracy comparable to models with much smaller pre-training datasets.

## Foundational Learning

- **Pre-training/Fine-tuning Paradigm**
  - Why needed here: The entire paper's argument is built on demonstrating a performance "gap" between models that undergo this two-stage process and those that do not.
  - Quick check question: Can you explain why a model might perform better on a classification task if it has first been trained on a large, unlabeled text corpus?

- **Transfer Learning**
  - Why needed here: The study frames its central question around whether representations learned by TLMs "transfer" qualitatively to downstream tasks in a manner similar to LLMs.
  - Quick check question: What does it mean for a model to "transfer" knowledge from one task to another, and what is one way to measure this transfer?

- **Model Latency vs. Depth**
  - Why needed here: A key proposed solution for deploying TLMs is the "soft committee" approach, which trades a single deep model for multiple shallow ones to achieve lower inference latency.
  - Quick check question: Why is a model with fewer layers generally considered to have lower inference latency, and what architectural technique does the paper propose to exploit this property?

## Architecture Onboarding

- **Component map**: Wikipedia subset (Ws) → BERT-6/BERT-1 pre-training (MLM) → Fine-tuning on classification task (FewRel/AGNews/DBPedia) → Soft committee ensemble (optional) → Classification accuracy evaluation

- **Critical path**:
  1. Dataset Curation: Select pre-training corpus (Ws) via random Wikipedia subset or custom filtering
  2. Pre-training (Optional): Train BERT-6 or BERT-1 variants on Ws using MLM objective
  3. Fine-tuning: Initialize model and train on labeled downstream classification task
  4. Ensembling (for Low Latency): Combine outputs of multiple independently pre-trained shallow models using soft committee

- **Design tradeoffs**:
  - Depth vs. Latency: Single BERT-6 provides good accuracy but higher latency; soft committee of 5 BERT-1s can match accuracy with lower per-model latency
  - Pre-training Dataset Size vs. Cost: Increasing Ws improves performance gap but with diminishing returns
  - Custom vs. Random Pre-training Data: Custom-made dataset maximizes token overlap but requires prior knowledge of target task vocabulary

- **Failure signatures**:
  - Negligible Performance Gap: Check if Ws is sufficiently large (>2,000 paragraphs) and token overlap is not artificially low
  - Ensemble Underperformance: Ensure BERT-1 variants are sufficiently diverse in architecture (heads, dimensions, CLs)
  - Task Mismatch: Benefits are less pronounced on simpler tasks (e.g., AGNews)

- **First 3 experiments**:
  1. Replicate core finding by training BERT-6 on random Ws of ~90,000 paragraphs, fine-tuning on FewRel subset, and comparing accuracy to non-pre-trained baseline
  2. Test overlap hypothesis by taking fixed Ws and artificially increasing missing tokens, then measuring resulting drop in accuracy
  3. Build low-latency ensemble by creating 3-5 BERT-1 variants with different configurations, pre-training independently, and combining outputs via soft committee

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does a frequency-weighted token overlap metric better predict pre-training efficiency than simple overlap?
- Basis in paper: The authors state "the impact of this frequency-weighted overlap on pre-training efficiency remains unclear."
- Why unresolved: Current metrics treat all missing tokens equally, ignoring skewed distribution where some tokens appear frequently and others rarely
- What evidence would resolve it: Experiments correlating classification performance with weighted overlap score accounting for token appearance frequency

### Open Question 2
- Question: How does the presence of pre-training tokens absent from the classification dataset affect model accuracy?
- Basis in paper: The paper notes that "the effect of the reverse case (tokens in $W_S$ absent from $T_C$) has not been fully explored."
- Why unresolved: The study focused on minimizing missing tokens in classification set but did not isolate noise introduced by irrelevant tokens in pre-training set
- What evidence would resolve it: Ablation studies varying ratio of irrelevant tokens in pre-training set while keeping classification target tokens constant

### Open Question 3
- Question: Are the soft committee and scaling efficiency findings for TLMs directly applicable to Large Language Models?
- Basis in paper: The authors conclude "The direct applicability of the presented results to LLMs remains uncertain."
- Why unresolved: Resource constraints limited this study to tiny scales ($10^{-3}$ to $10^{-4}$ smaller), and scaling laws may not remain linear
- What evidence would resolve it: Replicating experiments on standard LLM architectures and larger datasets

## Limitations

- The study primarily uses Wikipedia-derived datasets, which may not generalize to domains with significantly different lexical characteristics like social media or code
- Experiments focus on BERT-style architectures, and findings may not transfer directly to other architectures like GPT or T5
- The study does not extensively evaluate robustness to adversarial examples, out-of-distribution generalization, or energy efficiency during training

## Confidence

- **High Confidence**: Core empirical finding that pre-training on tiny datasets creates measurable performance gap between pre-trained and non-pre-trained models on downstream classification tasks
- **Medium Confidence**: Claim that soft committee of shallow architectures can replicate accuracy of single deep model, though effectiveness appears task-dependent
- **Low Confidence**: Broader implication that TLMs may be sufficient for developing language analogous to children's language acquisition extends beyond empirical scope

## Next Checks

- **Check 1**: Validate token overlap hypothesis by conducting experiments on a domain with minimal lexical overlap to Wikipedia, such as biomedical text or legal documents
- **Check 2**: Test whether soft committee approach generalizes beyond BERT-style architectures by implementing similar ensemble strategy with GPT-2 variants
- **Check 3**: Systematically vary task complexity from FewRel (64 labels) to AGNews (4 labels) to simpler binary classification problems to measure how performance gap and soft committee effectiveness scale with task complexity