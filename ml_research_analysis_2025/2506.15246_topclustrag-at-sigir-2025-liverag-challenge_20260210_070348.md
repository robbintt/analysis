---
ver: rpa2
title: TopClustRAG at SIGIR 2025 LiveRAG Challenge
arxiv_id: '2506.15246'
source_url: https://arxiv.org/abs/2506.15246
tags:
- passages
- system
- retrieval
- cluster
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents TopClustRAG, a retrieval-augmented generation
  system developed for the LiveRAG Challenge, which evaluates end-to-end question
  answering over large-scale web corpora. The system employs a hybrid retrieval strategy
  combining sparse (BM25) and dense (embedding-based) indices, followed by K-Means
  clustering to group semantically similar passages.
---

# TopClustRAG at SIGIR 2025 LiveRAG Challenge

## Quick Facts
- arXiv ID: 2506.15246
- Source URL: https://arxiv.org/abs/2506.15246
- Reference count: 4
- Ranked 2nd in faithfulness and 7th in correctness on official LiveRAG Challenge leaderboard

## Executive Summary
This paper presents TopClustRAG, a retrieval-augmented generation system developed for the LiveRAG Challenge, which evaluates end-to-end question answering over large-scale web corpora. The system employs a hybrid retrieval strategy combining sparse (BM25) and dense (embedding-based) indices, followed by K-Means clustering to group semantically similar passages. Representative passages from each cluster are used to construct cluster-specific prompts for a large language model (LLM), generating intermediate answers that are filtered, reranked, and synthesized into a final response. This multi-stage pipeline enhances answer diversity, relevance, and faithfulness to retrieved evidence.

## Method Summary
TopClustRAG implements a multi-stage retrieval-augmented generation pipeline that combines hybrid retrieval with semantic clustering. The system first retrieves passages using both BM25 and dense embedding indices, then applies K-Means clustering to group semantically similar passages. From each cluster, representative passages are selected to create cluster-specific prompts for an LLM, which generates intermediate answers. These intermediate responses undergo filtering and reranking before being synthesized into a final answer. The clustering approach aims to improve answer diversity and faithfulness by ensuring multiple perspectives are considered during generation.

## Key Results
- Ranked 2nd in faithfulness metric on FineWeb Sample-10BT dataset
- Ranked 7th in correctness metric on official leaderboard
- Demonstrates effectiveness of clustering-based context filtering in RAG systems
- Shows multi-stage pipeline improves answer quality through enhanced diversity

## Why This Works (Mechanism)
The system works by combining complementary retrieval methods (sparse and dense) to capture both keyword and semantic relevance, then using K-Means clustering to organize retrieved passages into semantically coherent groups. This clustering enables the generation of multiple intermediate answers from different perspectives, which are then filtered and reranked to produce a final response that balances diversity with relevance. The approach addresses the challenge of information overload in large-scale web corpora by systematically organizing and synthesizing evidence.

## Foundational Learning
- **Hybrid Retrieval (BM25 + Dense Embeddings)**: Combines exact keyword matching with semantic similarity to capture both lexical and conceptual relevance. Why needed: Web corpora contain diverse language patterns requiring multiple retrieval strategies. Quick check: Verify retrieval recall and precision on sample queries.
- **K-Means Clustering**: Groups semantically similar passages to enable multi-perspective answer generation. Why needed: Large-scale retrieval produces many redundant passages that need organization. Quick check: Evaluate cluster coherence using silhouette score or manual inspection.
- **Prompt Engineering for Cluster-specific Queries**: Creates tailored prompts for each cluster to generate diverse intermediate answers. Why needed: Different semantic clusters require different question framing to elicit relevant responses. Quick check: Compare intermediate answers across clusters for diversity.
- **Answer Filtering and Reranking**: Processes intermediate answers to select the most relevant and faithful responses. Why needed: LLM-generated answers vary in quality and may contain hallucinations. Quick check: Measure faithfulness scores before and after filtering.

## Architecture Onboarding
Component Map: Web Corpus -> Hybrid Retrieval (BM25 + Dense) -> K-Means Clustering -> Cluster-specific Prompt Generation -> LLM (Intermediate Answers) -> Filtering/Reranking -> Final Answer Synthesis

Critical Path: The most time-consuming operations are the hybrid retrieval step and K-Means clustering, as these process the entire web corpus. The LLM inference is also computationally expensive but operates on a smaller set of cluster-specific prompts.

Design Tradeoffs: The system trades computational overhead (clustering and multiple LLM calls) for improved answer quality and diversity. The choice of cluster number affects both performance and runtime, with more clusters potentially improving diversity but increasing computational cost.

Failure Signatures: Poor clustering may result in redundant or irrelevant intermediate answers. Insufficient passage filtering could allow low-quality or hallucinated content to influence the final answer. Over-aggressive clustering might miss important connections between semantically related but distinct passages.

Three First Experiments:
1. Test retrieval recall with varying numbers of passages per query to find optimal balance between coverage and computational cost.
2. Evaluate clustering performance with different numbers of clusters (k values) to optimize answer diversity.
3. Measure faithfulness scores with different passage filtering thresholds to find optimal balance between recall and precision.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance metrics are limited to official leaderboard rankings without detailed ablation studies
- System relies on FineWeb Sample-10BT dataset, limiting generalizability to other web corpora
- Lacks specification of exact LLM model and hyperparameter details for clustering and prompt generation
- Faithfulness and correctness metrics are not fully defined, making performance assessment difficult

## Confidence
- **High**: Hybrid retrieval strategy combining BM25 and dense embeddings is well-established and clustering-based prompt aggregation is logically sound
- **Medium**: Claim about enhanced answer diversity and relevance through clustering lacks quantitative evidence from controlled experiments
- **Medium**: Leaderboard rankings are factual but relative performance is not contextualized with competing approaches

## Next Checks
1. Conduct ablation studies to measure individual impact of K-Means clustering, passage filtering, and reranking on faithfulness and correctness scores
2. Test system on multiple datasets beyond FineWeb Sample-10BT to evaluate generalizability across different web corpora
3. Implement controlled experiments varying clustering parameters (number of clusters, distance metrics) and passage selection strategies to optimize balance between answer diversity and relevance