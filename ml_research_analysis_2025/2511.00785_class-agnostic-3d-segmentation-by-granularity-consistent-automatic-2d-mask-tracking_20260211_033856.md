---
ver: rpa2
title: Class-agnostic 3D Segmentation by Granularity-Consistent Automatic 2D Mask
  Tracking
arxiv_id: '2511.00785'
source_url: https://arxiv.org/abs/2511.00785
tags:
- segmentation
- object
- masks
- stage
- mask
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of class-agnostic 3D instance
  segmentation without manual annotations, which suffers from inconsistent segmentation
  granularity and conflicting 3D pseudo labels when transferring 2D masks to 3D space.
  The authors propose a Granularity-Consistent automatic 2D Mask Tracking approach
  that maintains temporal correspondences across video frames, eliminating conflicting
  pseudo labels.
---

# Class-agnostic 3D Segmentation by Granularity-Consistent Automatic 2D Mask Tracking

## Quick Facts
- **arXiv ID:** 2511.00785
- **Source URL:** https://arxiv.org/abs/2511.00785
- **Reference count:** 40
- **Primary result:** State-of-the-art performance on ScanNet200 (30.2 AP) and ScanNet++ (17.7 AP) for class-agnostic 3D instance segmentation

## Executive Summary
This paper introduces a Granularity-Consistent automatic 2D Mask Tracking approach for class-agnostic 3D instance segmentation without manual annotations. The method addresses inconsistent segmentation granularity and conflicting 3D pseudo labels when transferring 2D masks to 3D space by maintaining temporal correspondences across video frames. Combined with a three-stage curriculum learning framework, the approach progressively trains from fragmented single-view data to unified multi-view annotations, achieving globally coherent full-scene supervision. Experimental results demonstrate state-of-the-art performance on standard benchmarks while showing superior open-vocabulary capabilities in fine-grained object recognition and long-tail categories.

## Method Summary
The method employs a three-stage curriculum learning pipeline: Stage 1 samples keyframes and uses SAM for automatic mask generation, filtering redundant masks with a containment threshold before projecting to 3D and training an initial model; Stage 2 employs SAM2 for temporal mask propagation with object state management (Active/Dormant/Terminated) to generate consistent masks across all frames, which are then projected to 3D and used to fine-tune the model; Stage 3 generates pseudo-labels on full point clouds using the Stage 2 model, applies confidence-based filtering, and performs final fine-tuning. The Granularity-Consistent Segmentation Policy maintains temporal correspondences across frames to eliminate conflicting pseudo labels, ensuring consistent segmentation granularity before projection to 3D space.

## Key Results
- Achieves 30.2 AP on ScanNet200 validation set
- Achieves 17.7 AP on ScanNet++ validation set
- Outperforms baseline Segment3D by 4.8 AP on ScanNet200

## Why This Works (Mechanism)

### Mechanism 1: Temporal Resolution of Semantic Conflicts
Establishing cross-frame correspondences reduces conflicting 3D pseudo-labels caused by independent frame processing. The method tracks masks across frames, enforcing unified segmentation granularity before projection, ensuring that 3D points accumulating from different views receive consistent labels. Core assumption: SAM2 maintains sufficient identity stability to prevent drift. Evidence: [abstract] "introduce a Granularity-Consistent automatic 2D Mask Tracking approach that maintains temporal correspondences... eliminating conflicting pseudo labels."

### Mechanism 2: Curriculum-Based Noise Tolerance
A three-stage training pipeline allows the model to distill robust 3D geometry from noisy, fragmented 2D priors. The system progressively exposes the model to data of increasing quality: first learning local feature representations from fragmented single-view data, then refining geometric understanding using consistent multi-view tracks, and finally enforcing global coherence. Core assumption: Fragmented single-view data contains sufficient signal to initialize feature extractors. Evidence: [table 4] Shows performance drop when stages are skipped; Stage 1+2+3 yields 16.3 AP vs 11.4 for Stage 1 alone.

### Mechanism 3: Geometric Confidence Filtering
Filtering Stage 3 predictions via confidence thresholds refines boundaries better than raw 2D-to-3D projection. After training on projected masks, the model predicts on the full 3D point cloud, and by back-propagating only on high-confidence predictions, it self-corrects "bleeding" artifacts common in 3D lifting. Core assumption: High-confidence predictions correlate strongly with geometric correctness. Evidence: [section 3.3] "confidence-based filtering... ensures the quality of pseudo labels."

## Foundational Learning

- **Concept: 3D Lifting (2D-to-3D Projection)**
  - Why needed: The entire method relies on converting 2D pixel masks into 3D point clouds using depth maps and camera extrinsics. Errors here propagate directly to the 3D segmenter.
  - Quick check: If a 2D mask is erroneous, does the projection logic discard invalid depth values (where $D \le 0$)?

- **Concept: Object State Management (Tracking)**
  - Why needed: The system must handle objects leaving and re-entering the frame (Active → Dormant → Active). Without this logic, the system might generate new instance IDs for previously seen objects, fragmenting the 3D pseudo-labels.
  - Quick check: How does the system differentiate between an object that has moved vs. one that is merely occluded (Dormant state)?

- **Concept: Pseudo-Labeling & Self-Training**
  - Why needed: The model learns from generated labels (pseudo-labels) rather than human annotations. Understanding the loss calculation on these "soft" targets is critical for debugging convergence.
  - Quick check: Does the loss function weight the objectness loss differently than the mask loss to handle potential false positives in pseudo-labels?

## Architecture Onboarding

- **Component map:** SAM/SAM2 (2D Mask Generation & Tracking) -> Projection Layer (Depth + Pose math) -> MinkowskiUNet (Sparse 3D Convs) -> Transformer Decoder with Learned Queries (Instance Mask Prediction)

- **Critical path:** The Granularity-Consistent Segmentation Policy (Algorithm 1) is the critical novelty. Failure in the matching step (IoU < τIoU) causes ID fragmentation. If the state transition logic is buggy, the 3D pseudo-labels will be contradictory, rendering the curriculum learning ineffective.

- **Design tradeoffs:** Computational Cost: Using SAM2 on all frames improves consistency but significantly increases pre-processing time compared to keyframe-only methods. Recall vs. Precision: The dormancy threshold trades off tracking stability vs. memory/stability.

- **Failure signatures:**
  - "Shattered" Objects: A single chair appearing as 5 separate instances in 3D. Diagnostic: Check the IoU threshold in Algorithm 1; it may be too low to match the object across view changes.
  - Bleeding Boundaries: Segmentation masks spilling into walls. Diagnostic: Stage 3 confidence filtering may be too low.

- **First 3 experiments:**
  1. Sanity Check (Overfit): Train only Stage 1 on a single scene. Verify the model can overfit the fragmented pseudo-labels (loss should approach zero).
  2. Ablation (Consistency): Run the full pipeline but skip the "Matching" step in Algorithm 1 (treat all frames as independent). Compare AP on ScanNet200 to prove the value of consistency.
  3. Visual Debug (Tracking): Visualize the 2D masks on a video sequence, specifically looking for frame-to-frame ID switching or "Terminated" states triggering prematurely on visible objects.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the framework be improved to handle precise boundary localization for large-scale structural objects (Head categories) without compromising its advantage in fine-grained long-tail recognition?
- **Basis in paper:** [explicit] Section 4.4.1 states that the "relative underperformance on Head categories... suggests that our method has room for improvement in the precise boundary localization of large-scale objects."
- **Why unresolved:** The authors attribute the issue to the limitations of zero-shot methods with common indoor structures but do not propose a mechanism to enforce the necessary geometric constraints for large planar surfaces.
- **What evidence would resolve it:** Integrating geometric priors (e.g., plane fitting constraints) into the loss function or mask tracking stage and demonstrating improved AP on "Head" categories (e.g., walls, floors) in ScanNet200.

### Open Question 2
- **Question:** Does the Granularity-Consistent Segmentation Policy transfer effectively to outdoor autonomous driving scenarios where depth sensing differs from the RGB-D setup used in ScanNet?
- **Basis in paper:** [inferred] The Introduction explicitly lists "autonomous driving" as a target domain for real-world applications, but the experimental validation is restricted to indoor datasets (ScanNet, ScanNet++) which rely on dense RGB-D video.
- **Why unresolved:** It is unclear if the 2D mask tracking and 3D projection pipeline remains robust when depth data is sparse (LiDAR) or when outdoor lighting conditions degrade the reliability of the SAM/SAM2 foundation models.
- **What evidence would resolve it:** Evaluation results on outdoor 3D datasets (e.g., SemanticKITTI or nuScenes) showing that the cross-frame tracking maintains consistency despite dynamic motion and sparse depth inputs.

### Open Question 3
- **Question:** To what extent is the performance sensitive to the fixed containment threshold (τcontain=0.8) used for filtering redundant SAM masks?
- **Basis in paper:** [inferred] Section 3.1.1 details the filtering of redundant masks using a fixed threshold τcontain to resolve conflicting granularity, but provides no ablation study on how varying this threshold affects the final segmentation quality.
- **Why unresolved:** A fixed threshold may fail to generalize to objects with complex self-similarity or fractal structures, potentially over-filtering valid parts of an object or under-filtering noise.
- **What evidence would resolve it:** A sensitivity analysis showing the correlation between different threshold values and AP scores across diverse object categories with varying complexity.

## Limitations
- **Hyperparameter sensitivity:** Performance depends on multiple unspecified thresholds (IoU matching, dormancy, confidence filtering) that significantly impact granularity consistency
- **Scalability concerns:** Computational overhead from SAM2-based temporal tracking across all frames may limit real-world applicability to larger datasets
- **Generalization boundaries:** Strong results on standard benchmarks but lacks validation on datasets with significantly different object distributions or environmental conditions

## Confidence

- **High confidence:** Three-stage curriculum learning framework and its effectiveness in progressively improving performance (evidenced by Table 4's clear ablation results)
- **Medium confidence:** Granularity-Consistent Segmentation Policy's ability to resolve semantic conflicts through temporal tracking, as direct validation is limited in the paper
- **Medium confidence:** Open-vocabulary capabilities, as the paper shows quantitative results but lacks detailed qualitative analysis of fine-grained object recognition

## Next Checks
1. **Ablation of tracking thresholds:** Systematically vary τIoU, τdorm, and τconf to quantify their impact on AP performance and identify optimal values for different object categories
2. **Cross-dataset generalization test:** Evaluate the trained model on a non-ScanNet dataset (e.g., Matterport3D) to assess robustness to different environments and object distributions
3. **Computational complexity analysis:** Measure wall-clock time and memory usage for SAM2-based temporal tracking versus keyframe-only approaches to quantify the trade-off between consistency and efficiency