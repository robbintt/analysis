---
ver: rpa2
title: Lossless Acceleration of Large Language Models with Hierarchical Drafting based
  on Temporal Locality in Speculative Decoding
arxiv_id: '2502.05609'
source_url: https://arxiv.org/abs/2502.05609
tags:
- drafting
- database
- tokens
- draft
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel lossless acceleration method for large
  language model (LLM) inference called Hierarchy Drafting (HD), which organizes diverse
  token sources into multiple databases based on temporal locality and accesses them
  hierarchically. The method addresses the limitations of existing database drafting
  approaches that rely on single sources, which lead to inconsistent performance across
  tasks.
---

# Lossless Acceleration of Large Language Models with Hierarchical Drafting based on Temporal Locality in Speculative Decoding

## Quick Facts
- arXiv ID: 2502.05609
- Source URL: https://arxiv.org/abs/2502.05609
- Reference count: 38
- Primary result: Hierarchical Drafting achieves 1.5-1.7x speedup over autoregressive decoding while maintaining >70% acceptance ratios across diverse tasks

## Executive Summary
This paper introduces Hierarchy Drafting (HD), a novel lossless acceleration method for LLM inference that organizes draft tokens from multiple sources into three databases accessed hierarchically by temporal locality. The method addresses the inconsistency problem of single-source drafting approaches by combining context-dependent, model-dependent, and statistics-dependent databases. HD achieves consistent acceleration (1.5-1.7x speedup) across diverse tasks on Spec-Bench while maintaining lossless verification through speculative sampling. The hierarchical access pattern ensures minimal drafting latency while maximizing acceptance ratios.

## Method Summary
HD is a database-based speculative decoding approach that retrieves draft tokens from three specialized databases (Dc, Dm, Ds) ordered by temporal locality. The method uses context-dependent database (Dc) for recent token patterns, model-dependent database (Dm) for model-specific frequent n-grams, and statistics-dependent database (Ds) for broader corpus patterns. Draft tokens are collected hierarchically—Dc first, then Dm, then Ds—until reaching the draft set size N. The combined draft set undergoes parallel verification against the target LLM using speculative sampling. Dc is updated per forward step using parallel decoded tokens, while Dm and Ds remain static. The approach is plug-and-play, requiring no model training or fine-tuning.

## Key Results
- HD achieves 1.5-1.7x speedup compared to autoregressive decoding across all tested tasks
- Maintains high acceptance ratios (>70%) while reducing drafting latency through temporal locality ordering
- Context-dependent database excels at summarization and RAG tasks (32.3% verify success ratio)
- Model-dependent and statistics-dependent databases compensate for Dc's weaknesses in QA and translation
- Performance is consistent across diverse tasks unlike single-source methods (PLD, REST) which show concave regions

## Why This Works (Mechanism)

### Mechanism 1: Temporal Locality-Ordered Database Access
- Prioritizing databases by temporal locality (context → model → statistics) improves acceptance ratio while reducing drafting latency
- Tokens with higher temporal locality are rarer and stored in smaller databases, yielding faster lookups and higher verify-success rates (Dc: 32.3%, Dm: 15.5%, Ds: 6.5%)
- Core assumption: Tokens that repeat within or across generations follow predictable locality patterns that can be exploited hierarchically
- Evidence anchors: Figure 5 shows Dc achieves highest verify success ratio over 30% with lowest draft latency; hierarchical ordering ensures consistent acceleration

### Mechanism 2: Multi-Source Token Coverage for Task Robustness
- Combining three token sources provides consistent speedup across diverse tasks, unlike single-source methods that excel only in specific scenarios
- Each database compensates for others' weaknesses—Dc excels at summarization/RAG while Dm and Ds compensate for QA/translation
- Core assumption: No single token source covers all generation patterns; task diversity requires source diversity
- Evidence anchors: Figure 4 shows HD maintains high speedup across all six tasks while PLD/REST show concave regions; single-source methods show task-specific limitations

### Mechanism 3: Lossless Verification via Speculative Sampling
- Draft tokens verified against target LLM maintain identical output distribution to autoregressive decoding
- Speculative sampling accepts/rejects draft tokens based on probability ratios, ensuring mathematical equivalence
- Core assumption: Verification algorithm preserves distributional correctness regardless of draft source
- Evidence anchors: §3.1 confirms speculative sampling maintains identical output distributions; HD uses draft probability of 1.0 for this purpose

## Foundational Learning

- **Concept: Speculative Decoding Draft-then-Verify Paradigm**
  - Why needed here: HD is a specific drafting strategy within this framework; understanding the two-step process (draft generation → parallel verification) is prerequisite
  - Quick check question: Can you explain why verification must happen in a single forward pass for speedup to occur?

- **Concept: Temporal Locality in Memory Hierarchies**
  - Why needed here: The paper's core insight borrows from CPU cache design—data accessed recently is likely accessed again soon. HD maps this to token repetition patterns
  - Quick check question: Why does LRU eviction make sense for Dc but not for Ds?

- **Concept: LLM Inference Memory-Bound Bottleneck**
  - Why needed here: Speculative decoding addresses memory bandwidth limits (not compute limits). Understanding why generating multiple tokens per forward pass improves throughput is essential
  - Quick check question: Why does autoregressive decoding underutilize GPU memory bandwidth?

## Architecture Onboarding

- **Component map:** Dc (context-dependent DB) → Dm (model-dependent DB) → Ds (statistics-dependent DB) → Parallel verification → Accept/reject tokens → Update Dc

- **Critical path:**
  1. Receive previous tokens (xt−l:t), query Dc first
  2. If draft set size |X̃| < N, query Dm; if still insufficient, query Ds
  3. Pass combined draft set to target LLM for parallel verification
  4. Update Dc with accepted/rejected tokens (parallel decoding + recycling)
  5. Return accepted tokens, repeat until EOS or max length

- **Design tradeoffs:**
  - Larger N increases acceptance probability but raises verification compute
  - Including Ds improves coverage but adds 2-3ms latency; may hurt speedup for smaller models
  - Ablation shows (Dc, Dm) alone achieves 1.71x speedup (faster than full HD's 1.64x on 7B) due to lower latency

- **Failure signatures:**
  - High draft latency (>10ms) with low acceptance: Likely Ds accessed too frequently (Dc/Dm miss rates high)
  - Acceptance ratio <50%: Database sources may not match task domain
  - Non-deterministic outputs at T>0: Dc updates with sampled tokens cause mismatches

- **First 3 experiments:**
  1. Baseline replication: Run HD vs. AR on Spec-Bench subset with Vicuna-7B; verify 1.5x+ speedup and acceptance >70% at T=0
  2. Ablation by database: Measure speedup with (Dc only), (Dc+Dm), (Dc+Ds), (full HD) on translation vs. summarization tasks
  3. Latency sensitivity: Test HD on 13B vs. 7B models; verify that Ds contribution increases for larger models

## Open Questions the Paper Calls Out
- How does HD perform on LLMs significantly larger than 13B parameters (e.g., 70B+ models)?
- Can drafting latency be further reduced to optimize the trade-off between acceptance ratio and retrieval speed?
- What performance gains result from integrating previously omitted drafting sources, such as token recycling, into the hierarchical framework?

## Limitations
- Limited testing on LLMs larger than 13B parameters, with performance on 70B+ models remaining unverified
- Database construction details underspecified, particularly n-gram extraction process for Dm and LRU eviction policy for Dc
- Hierarchical ordering by temporal locality is theoretically optimal but not empirically validated against alternative orderings

## Confidence
- **High Confidence**: Hierarchical database access reducing drafting latency is well-supported by analysis showing Dc's superior performance; lossless property through speculative sampling is theoretically sound
- **Medium Confidence**: Task-robustness claims are supported by Spec-Bench experiments but need more extensive ablation studies across different model sizes and domains
- **Low Confidence**: Claim that hierarchical ordering by temporal locality is optimal across all scenarios is largely theoretical; impact of database size scaling on performance curves remains unexplored

## Next Checks
1. Test HD on non-transformer architectures (e.g., Mamba, RWKV) and non-English language models to verify temporal locality patterns across different model families
2. Systematically vary database sizes (Dc: 100-10K entries, Dm: 10K-1M n-grams, Ds: different corpus sizes) to quantify coverage vs latency tradeoffs
3. Implement and evaluate runtime reordering of databases based on task-specific acceptance patterns (e.g., switching to Dm→Ds→Dc for translation tasks)