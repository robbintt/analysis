---
ver: rpa2
title: On the Limitations of Vision-Language Models in Understanding Image Transforms
arxiv_id: '2503.09837'
source_url: https://arxiv.org/abs/2503.09837
tags:
- image
- clip
- augmented
- augmentation
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the fundamental limitations of Vision-Language
  Models (VLMs) like CLIP and SigLIP in understanding basic image transformations.
  Through systematic evaluation using an augmented Flickr8k dataset with 24 transformation
  types, the study reveals that VLMs struggle with comprehending simple image modifications
  such as rotation, brightness/contrast adjustments, and geometric distortions.
---

# On the Limitations of Vision-Language Models in Understanding Image Transforms

## Quick Facts
- arXiv ID: 2503.09837
- Source URL: https://arxiv.org/abs/2503.09837
- Reference count: 40
- Primary result: VLMs achieve high semantic association accuracy (99.57%) but fail at direct transformation classification (<4% top-1 accuracy)

## Executive Summary
This paper investigates fundamental limitations of Vision-Language Models (VLMs) like CLIP and SigLIP in understanding basic image transformations. Through systematic evaluation using an augmented Flickr8k dataset with 24 transformation types, the study reveals that VLMs struggle with comprehending simple image modifications such as rotation, brightness/contrast adjustments, and geometric distortions. The evaluation demonstrates that while these models show reasonable performance in matching augmented images with their descriptions (achieving 98-99% accuracy for CLIP variants), they fail significantly in direct classification tasks, with top-1 accuracy below 4% across all models. This deficiency has critical implications for downstream applications like image editing, where VLMs fail to execute basic transformation instructions. The findings suggest that the invariance property of VLMs, while beneficial for semantic understanding, comes at the cost of explicit spatial reasoning capabilities, highlighting the need for new training paradigms that balance invariance with transformation awareness.

## Method Summary
The study evaluates CLIP and SigLIP models using an augmented Flickr8k dataset with 24 transformation types across six categories (geometric, color, clarity, distortion, size, processing). Three experiments assess VLM performance: (1) associating augmented prompts with augmented vs original images, (2) matching augmented images with augmented vs original prompts, and (3) direct multi-class classification of transformation types. The evaluation uses zero-shot cosine similarity between image and text embeddings, with no training involved. Transformations include rotations, flips, brightness/contrast adjustments, Gaussian blur, perspective distortion, cropping, and various image processing effects. Captions are augmented with transformation descriptions to create paired prompts for evaluation.

## Key Results
- VLMs achieve 98-99% accuracy in associating augmented images with augmented captions
- Direct transformation classification fails with <4% top-1 accuracy across all models
- CLIP variants show modest accuracy gains with larger ViT models (43.10% vs 40.87%)
- SigLIP outperforms CLIP on mean accuracy in classification (47.21% vs 43.10%) but underperforms in association tasks (47.41% vs 99.57%)
- Downstream image editing models (DALL·E, Instruct Pix2Pix, IP Adapter) fail on simple rotation instructions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive pre-training creates embedding associations without explicit transformation encoding
- Mechanism: CLIP and SigLIP align image-text pairs via contrastive learning, producing high similarity scores for matched content. This allows augmented images to associate with augmented captions (99.57% accuracy in Experiment 2) because the text provides semantic context the visual encoder cannot derive alone.
- Core assumption: The model leverages textual cues rather than visual transformation features when making associations.
- Evidence anchors: [abstract] "while CLIP models can generally associate augmented images with augmented prompts (99.57% accuracy), they struggle with direct classification of transformations (0% top-1 accuracy)"; [section 4.2.2] "there is a very small difference in the similarity score indicating that even though CLIP models perform very well, they can not differentiate between the normal prompt and augmented prompt really well"; [corpus] Related work on CLIP robustness shows strong shape bias and adversarial vulnerability, suggesting embedding structure prioritizes semantic over pixel-level features.
- Break condition: If explicit transformation tokens were added to pre-training data with geometric/augmentation labels, association-only behavior should shift toward classification capability.

### Mechanism 2
- Claim: Invariance training objectives suppress transformation-specific feature learning
- Mechanism: CLIP-style models are trained for robustness to augmentation (cropping, color jitter, etc.) so that semantically equivalent images produce similar embeddings. This architectural bias causes transformation details to be discarded in service of semantic stability.
- Core assumption: Invariance is learned during contrastive pre-training through data augmentation pipelines that treat transforms as noise rather than signal.
- Evidence anchors: [section 1] "these models are designed to exhibit robustness and invariant behavior to standard image transforms, we argue that this invariance might come at the cost of explicit understanding"; [section 5] "current CLIP-based models lack a comprehensive understanding of image structure and spatial relationships due to their invariant nature which comes at the cost of explicit spatial understanding"; [corpus] Laroudie et al. (LP-CLIP) show CLIP is overconfident in incorrect predictions and vulnerable to domain shift, consistent with invariance-induced blindness to low-level changes.
- Break condition: If pre-training included transformation classification as an auxiliary task alongside contrastive learning, invariance would coexist with transformation awareness.

### Mechanism 3
- Claim: Vision encoder lacks spatial structure representation required for geometric reasoning
- Mechanism: ViT-based image encoders tokenize images into patches and process them with attention, but without explicit positional/structural inductive biases for geometric transforms. The model cannot decompose an image into structural elements and their spatial relationships.
- Core assumption: Patch-based representations flatten spatial structure, and self-attention alone is insufficient for learning explicit geometric invariances/equivariances.
- Evidence anchors: [section 5] "none of these CLIP-powered models was able to understand this basic instruction and failed to generate an image with the requested transformation applied [rotation]"; [section 2] "Lewis et al. show that CLIP models perform poorly on compositional visual reasoning tasks and cannot encode compositional concepts or bind variables in a structure-sensitive way"; [corpus] "Why Vision Language Models Struggle with Visual Arithmetic" paper confirms VLMs fail at visual arithmetic (counting, length comparison), reinforcing structural reasoning deficits.
- Break condition: If encoder architecture incorporated explicit spatial feature maps (CNN backbones) or geometric attention mechanisms, structural reasoning should improve for transformations like rotation.

## Foundational Learning

- **Contrastive Learning (CLIP-style pre-training)**
  - Why needed here: The entire paper hinges on understanding how image-text alignment via contrastive loss creates embedding spaces that privilege semantic similarity over structural detail.
  - Quick check question: Given an image of a rotated dog and the caption "a dog rotated 90 degrees," would a contrastive model embed this pair closer together than the same image with the caption "a dog"? (Answer: likely yes due to semantic overlap, but the rotation detail may be weakly encoded.)

- **Image Augmentations and Transforms**
  - Why needed here: The paper evaluates 24 transforms across 6 categories (geometric, color, clarity, distortion, size, processing). Understanding what each transform does to pixel-level structure is essential for interpreting the results.
  - Quick check question: What is the difference between a geometric transform (e.g., rotation) and a processing transform (e.g., solarization) in terms of what features they affect?

- **Vision Transformer (ViT) Patch Embeddings**
  - Why needed here: All evaluated models use ViT backbones (ViT-B/32, ViT-B/16, ViT-L/14). Understanding how ViT processes images as sequences of patches explains why spatial structure is weakened.
  - Quick check question: How does a ViT encode the spatial position of patches, and why might this representation fail to capture rotation-equivariant features?

## Architecture Onboarding

- **Component map:** Image encoder (ViT-B/32, ViT-B/16, ViT-L/14) -> Text encoder (Transformer) -> Projection heads -> Similarity function (cosine similarity)
- **Critical path:**
  1. Load pre-trained CLIP/SigLIP model (do NOT train from scratch)
  2. For each image, apply transformation T from the 24 defined transforms
  3. Generate augmented caption: original_caption + ", " + transformation_description
  4. Compute similarity scores: sim(I_aug, C_aug), sim(I_orig, C_aug), sim(I_aug, C_orig)
  5. For classification (Experiment 3), compute sim(I_aug, desc_T) for all 24 transforms and rank
- **Design tradeoffs:**
  - Larger ViT variants (L/14) show modest accuracy gains in Experiment 1 (43.10% vs 40.87% for B/16), but all models fail at direct classification (<4% top-1)
  - SigLIP outperforms CLIP on mean accuracy in Experiment 1 (47.21% vs 43.10%), but underperforms dramatically in Experiment 2 (47.41% vs 99.57%), suggesting sigmoid loss changes embedding structure differently
  - Assumption: Trade-off between association strength and discrimination granularity
- **Failure signatures:**
  - Near-zero top-1 accuracy on transformation classification (Experiment 3) even though association tasks (Experiment 2) show high accuracy
  - Very small similarity score differences between correct and incorrect captions (Figure 6 shows mean differences <0.06)
  - Downstream image editing models (DALL·E, Instruct Pix2Pix, IP Adapter) fail on simple rotation instructions
- **First 3 experiments:**
  1. **Replicate Experiment 1:** Augment 100 images with random transforms, create augmented captions, compute accuracy of sim(I_aug, C_aug) > sim(I_orig, C_aug). Expect ~40-48% accuracy.
  2. **Replicate Experiment 2:** For same augmented images, compute accuracy of sim(I_aug, C_aug) > sim(I_aug, C_orig). Expect >95% for CLIP, <65% for SigLIP.
  3. **Replicate Experiment 3:** For each augmented image, compute similarity against all 24 transform descriptions, report top-1 and top-5 accuracy. Expect <5% top-1.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What training paradigms can balance invariance to transformations (for robustness) with explicit transformation awareness (for controllable editing)?
- Basis in paper: [explicit] The authors state that "invariance might come at the cost of explicit understanding" and explicitly call for "newer training paradigms for Vision Language Models that balance invariance with explicit transformation awareness."
- Why unresolved: Current contrastive learning objectives optimize for invariance, making it unclear how to simultaneously preserve transformation sensitivity without degrading semantic matching performance.
- What evidence would resolve it: A training framework that achieves both high semantic matching accuracy and non-trivial transformation classification accuracy (>50% top-1) on the same model.

### Open Question 2
- Question: Why do VLMs achieve 99.57% accuracy on matching augmented images to augmented prompts but 0% top-1 accuracy on direct transformation classification?
- Basis in paper: [inferred] Experiment 2 shows near-perfect association between augmented images and augmented descriptions, while Experiment 3 shows complete failure to classify transformations directly. This dramatic gap is presented without explanation.
- Why unresolved: The paper demonstrates this paradox but does not investigate whether the issue lies in the embedding space structure, the prompting methodology, or fundamental architectural limitations.
- What evidence would resolve it: Probing studies that analyze the embedding space geometry to determine whether transformation information is present but inaccessible, or genuinely absent from the representations.

### Open Question 3
- Question: Can explicit spatial and structural understanding be incorporated into VLMs without sacrificing their existing semantic capabilities?
- Basis in paper: [explicit] The authors conclude that "despite their impressive semantic capabilities, current CLIP-based models lack a comprehensive understanding of image structure and spatial relationships due to their invariant nature."
- Why unresolved: It remains unknown whether the semantic strengths of VLMs are fundamentally incompatible with transformation awareness, or if both can coexist with appropriate architectural innovations.
- What evidence would resolve it: Demonstrating a model that maintains competitive performance on standard VLM benchmarks (e.g., ImageNet zero-shot) while achieving meaningful transformation classification accuracy.

## Limitations
- The study uses a fixed Flickr8k subset without reporting exact sample sizes or distribution across transformation types
- The paper does not experimentally isolate whether transformation blindness stems from ViT architecture versus training objective
- Evaluation uses zero-shot similarity scoring without exploring whether fine-tuning on transformation classification would resolve limitations
- Downstream image editing failure demonstrations are observational rather than systematic, lacking controlled prompts and quantitative metrics

## Confidence
- **High Confidence**: Experiment 2 results showing CLIP achieves 99.57% accuracy in associating augmented images with augmented captions. This finding is directly reproducible with the specified methodology and aligns with established contrastive learning theory.
- **Medium Confidence**: The core claim that VLMs lack explicit transformation understanding. While Experiment 3 convincingly shows <4% top-1 accuracy on classification, the study does not control for whether this reflects architectural limitations versus training data gaps.
- **Low Confidence**: The downstream image editing failure analysis. The paper demonstrates that VLM-powered models fail on rotation instructions, but provides no systematic evaluation of prompt engineering, model selection, or quantitative success metrics.

## Next Checks
1. **Architectural ablation study**: Replicate Experiments 1-3 using CNN-based vision encoders (e.g., ResNet) instead of ViT to determine whether the transformation blindness is architecture-dependent or inherent to contrastive training.

2. **Fine-tuning intervention**: Take the top-5 transformation predictions from Experiment 3 and fine-tune the model on this binary classification task (correct vs. incorrect transformations) to measure whether explicit supervision can overcome the invariance-induced blindness.

3. **Controlled downstream testing**: Systematically evaluate VLM-powered image editing models on a benchmark of transformation instructions with quantitative metrics (pixel-level error, structural similarity) and controlled prompt variations to determine whether the failure is fundamental or addressable through prompt engineering.