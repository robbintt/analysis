---
ver: rpa2
title: 'Heartificial Intelligence: Exploring Empathy in Language Models'
arxiv_id: '2508.08271'
source_url: https://arxiv.org/abs/2508.08271
tags:
- empathy
- human
- cognitive
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examined cognitive and affective empathy in small and
  large language models using standardized psychological tests. LLMs significantly
  outperformed humans, including psychology students, on cognitive empathy tasks,
  with GPT-4o achieving the highest scores on situational and image-based assessments.
---

# Heartificial Intelligence: Exploring Empathy in Language Models

## Quick Facts
- arXiv ID: 2508.08271
- Source URL: https://arxiv.org/abs/2508.08271
- Reference count: 26
- Primary result: LLMs significantly outperformed humans on cognitive empathy tasks but scored lower on affective empathy measures

## Executive Summary
This study examined cognitive and affective empathy in both small and large language models using standardized psychological tests. The results showed that LLMs consistently outperformed humans—including psychology students—on cognitive empathy tasks, with GPT-4o achieving the highest scores on situational and image-based assessments. However, both small and large language models demonstrated significantly lower affective empathy compared to human participants. These findings reveal that while LLMs excel at understanding and interpreting complex emotional and social cues, they lack genuine emotional resonance. The research suggests potential applications in virtual companionship and emotional support, while highlighting limitations in simulating authentic human-like empathy.

## Method Summary
The study evaluated 6 small language models (SLMs) and 5 large language models (LLMs) using four standardized psychological assessments: Strange Stories Revised (SSR), Situational Test of Emotional Understanding (STEU), Seeing Emotions in the Eyes-48 (SEE-48), and Toronto Empathy Questionnaire (TEQ). Each test was administered in separate, isolated chat sessions to prevent cross-session learning, with prompts emulating human test administration. Models were tested across June-November 2024, with multimodal LLMs also evaluated on the SEE-48 image-based assessment. Performance was statistically compared against human benchmarks using Mann-Whitney U tests (p<0.05).

## Key Results
- LLMs significantly outperformed humans on cognitive empathy tasks, with GPT-4o achieving near-perfect scores on text-based assessments
- Both SLMs and LLMs scored significantly lower than humans on affective empathy measures
- Parameter scale strongly correlated with cognitive empathy performance, but multimodal capability did not consistently improve results across all emotional categories

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Parameter scale strongly correlates with performance on cognitive empathy tasks, enabling LLMs to outperform humans in interpreting complex social cues.
- **Mechanism:** As model size increases, the capacity to process and retrieve nuanced social scripts and theory of mind patterns improves, allowing for near-perfect scores on text-based cognitive assessments like the Strange Stories Revised test.
- **Core assumption:** Standardized psychological tests validly measure "understanding" in machines just as they do in humans, and high scores reflect genuine inference rather than pattern matching.
- **Evidence anchors:**
  - [abstract]: "LLMs consistently outperformed humans – including psychology students – on cognitive empathy tasks."
  - [PAGE 8]: "Results... show that there is a positive correlation between model parameter size and cognitive empathy performance."
  - [corpus]: Related work confirms this gap, noting that while LLMs excel at logical reasoning, "emotional intelligence (EQ) still lags far behind their cognitive prowess."
- **Break condition:** Performance drops when tasks require "affective resonance" rather than just "cognitive inference," or when models are small (SLMs).

### Mechanism 2
- **Claim:** Affective empathy deficits in LLMs stem from a lack of embodied emotional experience, limiting them to simulating empathy through linguistic proxies.
- **Mechanism:** Models generate responses based on statistical associations of "empathetic language" found in training data, but cannot access the physiological or subjective states required for genuine affective sharing.
- **Core assumption:** Self-report questionnaires like the TEQ effectively probe the "emotional resonance" gap between humans and disembodied systems.
- **Evidence anchors:**
  - [abstract]: "Both small and large language models showed significantly lower affective empathy compared to human participants."
  - [PAGE 8]: "This gap highlights a fundamental limitation of language models – they lack genuine emotional experiences, constraining their ability to authentically simulate affective empathy."
  - [corpus]: Neighbors suggest this is a known boundary; systems often struggle to bridge the gap between recognizing emotion and genuinely experiencing or "evoking" it in a human-like way.
- **Break condition:** In scenarios requiring deep emotional bonding or where "feeling" the user's pain is a prerequisite for trust, the mechanism of simulated empathy may be insufficient.

### Mechanism 3
- **Claim:** Multimodal capability (vision) allows for human-level emotion recognition in specific high-performing architectures, but performance is uneven across emotional categories.
- **Mechanism:** Vision-enabled models process visual stimuli and map them to emotional concepts, outperforming humans on clear emotions (happiness, surprise) but failing on subtle cues like fear.
- **Core assumption:** The image-based assessments accurately reflect visual emotional intelligence rather than simple pattern recognition of exaggerated facial features.
- **Evidence anchors:**
  - [PAGE 6]: "GPT-4o achieved human-like performance levels in emotion recognition tasks... demonstrating exceptional proficiency in recognizing emotions such as happiness... and surprise."
  - [PAGE 7]: "GPT-4o (M = 0.83) significantly outperformed Claude 3.5 Sonnet (M = 0.40)... highlighting a clear advantage of GPT-4o in recognizing emotional states."
  - [corpus]: No direct corpus evidence explains the specific variance in visual emotion recognition; related papers focus on text-based empathy.
- **Break condition:** This mechanism varies drastically by model architecture (Claude 3.5 Sonnet performed poorly on image tasks) and emotion type (fear was harder for models than humans).

## Foundational Learning

- **Concept:** **Cognitive vs. Affective Empathy**
  - **Why needed here:** The study relies on this distinction to explain why LLMs can act as excellent "analysts" (high cognitive) but poor "companions" (low affective).
  - **Quick check question:** Does the model understand *why* the user is sad (cognitive), or does it *feel* sadness in response (affective)?

- **Concept:** **Theory of Mind (ToM)**
  - **Why needed here:** The "Strange Stories" test effectively measures ToM—the ability to attribute mental states to others—which is the foundation of the cognitive empathy observed in the paper.
  - **Quick check question:** Can the model predict that a character believes something false (false belief), or is it only tracking the literal text?

- **Concept:** **Psychometric Validity for AI**
  - **Why needed here:** The paper applies human tests (TEQ, SSR) to machines. Understanding the limitations of these proxies is critical for interpreting the results.
  - **Quick check question:** Does scoring high on a multiple-choice empathy test guarantee empathetic *behavior* in open-ended conversation?

## Architecture Onboarding

- **Component map:** Input Layer (Text/ Image stimuli) -> Processing Core (SLMs/LLMs with parameter size and multimodal integration) -> Evaluation Layer (Standardized psychological rubrics)

- **Critical path:**
  1. **Isolate Sessions:** Ensure no chat history leaks between tests (models must start as "blank slates").
  2. **Prompt Engineering:** Emulate human test administration exactly (e.g., open-ended for SSR, multiple-choice for STEU).
  3. **Scoring:** Apply strict rubrics; do not rely on model self-assessment.

- **Design tradeoffs:**
  - **High-Cognitive / Low-Affective:** The paper suggests using LLMs for objective support (mental health triage, education) where emotional fatigue is a risk, rather than deep therapy.
  - **Scale vs. Cost:** High cognitive empathy requires massive parameters (175B+), making SLMs unsuitable for complex emotional understanding tasks.

- **Failure signatures:**
  - **Hallucinated Resonance:** The model claims to "feel" emotions (affective hallucination) despite the paper showing it lacks this capacity.
  - **Visual Blindness:** Non-multimodal models or lower-tier multimodal models fail to interpret image-based emotional cues.

- **First 3 experiments:**
  1. **Baseline Cognitive Test:** Run the STEU on your target model to establish a cognitive empathy benchmark (compare against GPT-4o's 0.80 score).
  2. **Affective Check:** Administer the TEQ to verify the "affective gap" described in the paper (expect scores <15 for models vs ~45 for humans).
  3. **Modality Stress Test:** Compare model performance on text-based vs. image-based emotion recognition to determine if visual emotional processing is robust or architecture-dependent.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does high performance on standardized cognitive empathy tests translate to effective empathy in dynamic, real-world social interactions?
- **Basis in paper:** The authors note that existing research focuses on structured, domain-specific applications and lacks investigation into "diverse... situational contexts," leaving a gap in understanding real-world applicability.
- **Why unresolved:** This study relied on static, validated psychological assessments rather than measuring performance in fluid, evolving conversations.
- **What evidence would resolve it:** Longitudinal studies comparing LLM performance on standardized tests against human satisfaction ratings in unscripted, open-ended dialogue.

### Open Question 2
- **Question:** How can affective empathy be accurately assessed in language models without relying on self-report questionnaires designed for humans?
- **Basis in paper:** The paper states that "methodological constraints of affective empathy assessments, particularly the subjective nature of self-report measures, complicate the accurate evaluation of affective empathy capacities" in LLMs.
- **Why unresolved:** Current measures force models to simulate human emotional experiences (feelings they biologically lack), potentially measuring simulation capability rather than emotional resonance.
- **What evidence would resolve it:** The development and validation of behavioral or objective metrics for AI affective empathy that do not depend on self-reported internal states.

### Open Question 3
- **Question:** Does integrating LLMs with high cognitive empathy into human workflows improve outcomes in mental health and conflict resolution compared to human-only teams?
- **Basis in paper:** The authors suggest that "distinct strengths of humans and AI suggest a collaborative approach could effectively address societal challenges," proposing a hybrid model.
- **Why unresolved:** The study evaluated models in isolation against human benchmarks but did not experimentally test the efficacy of human-AI collaboration.
- **What evidence would resolve it:** Empirical trials in clinical or mediation settings comparing success rates of human-only teams against AI-assisted teams.

## Limitations

- The study's reliance on standardized human psychological tests for evaluating machine empathy introduces fundamental validity concerns, as these tests may not accurately measure AI understanding versus sophisticated pattern matching
- Affective empathy measurements using the Toronto Empathy Questionnaire may not capture genuine emotional resonance in systems lacking embodied experience
- The generalizability of findings to real-world applications and the specific performance of individual models is uncertain due to unknown number of test iterations and potential variations in prompt engineering

## Confidence

- **High Confidence:** The comparative performance results showing LLMs outperforming humans on cognitive empathy tasks and underperforming on affective measures. These findings are directly supported by the experimental data and statistical analyses presented.
- **Medium Confidence:** The mechanisms proposed linking parameter scale to cognitive performance and the interpretation of affective deficits as stemming from lack of embodied experience. While consistent with observed results, these explanations rely on assumptions about what the tests actually measure in AI systems.
- **Low Confidence:** The generalizability of findings to real-world applications and the specific performance of individual models, given the unknown number of test iterations and potential variations in prompt engineering.

## Next Checks

1. **Prompt Sensitivity Analysis:** Systematically vary the exact wording of test prompts across a subset of models to quantify how much performance depends on prompt engineering versus genuine capability differences.

2. **Alternative Measurement Framework:** Develop and test a complementary evaluation framework using LLM-as-a-judge with human-validated rubrics, comparing results against the current standardized test approach to assess measurement validity.

3. **Longitudinal Stability Test:** Run each model through the full battery of tests multiple times across different days to measure consistency and rule out session-specific anomalies or learning effects between tests.