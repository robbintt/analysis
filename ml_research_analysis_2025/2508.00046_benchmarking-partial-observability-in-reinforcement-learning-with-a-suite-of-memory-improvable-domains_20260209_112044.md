---
ver: rpa2
title: Benchmarking Partial Observability in Reinforcement Learning with a Suite of
  Memory-Improvable Domains
arxiv_id: '2508.00046'
source_url: https://arxiv.org/abs/2508.00046
tags:
- observability
- partial
- environments
- learning
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces POBAX, a benchmark suite for evaluating\
  \ reinforcement learning algorithms under partial observability. The authors argue\
  \ that effective benchmarks must cover diverse forms of partial observability and\
  \ be memory improvable\u2014exhibiting a performance gap between agents with more\
  \ or less state information."
---

# Benchmarking Partial Observability in Reinforcement Learning with a Suite of Memory-Improvable Domains

## Quick Facts
- arXiv ID: 2508.00046
- Source URL: https://arxiv.org/abs/2508.00046
- Reference count: 16
- Primary result: POBAX benchmark suite for evaluating RL algorithms under partial observability, featuring 8 categories of observability challenges and memory-improvable domains

## Executive Summary
This paper introduces POBAX, a benchmark suite for evaluating reinforcement learning algorithms under partial observability. The authors argue that effective benchmarks must cover diverse forms of partial observability and be memory improvable—exhibiting a performance gap between agents with more or less state information. POBAX includes tasks spanning localization, visual control, games, and more, all implemented in JAX for GPU scalability. The environments are characterized as requiring hard-to-learn memory functions and are shown to be memory improvable via controlled hyperparameter studies. Evaluation of three popular memory-learning algorithms (recurrent PPO, λ-discrepancy, and transformer-XL) on POBAX domains confirms that performance gains arise from mitigating partial observability, validating POBAX as a robust testbed for this research area.

## Method Summary
POBAX is implemented as a suite of 12 memory-improvable environments in JAX, covering eight categories of partial observability. The benchmark defines memory improvability as a performance gap between agents with partial observations (floor) and those with full state information (ceiling). Three memory-learning algorithms—recurrent PPO, λ-discrepancy, and transformer-XL—are evaluated alongside memoryless PPO baselines. Hyperparameters are swept per environment and fixed across algorithms to isolate memory mechanism differences. Performance is measured via area under the learning curve over 30 seeds, with statistical significance assessed through confidence intervals.

## Key Results
- POBAX domains exhibit measurable memory improvability gaps, confirming they require history-based policies to overcome partial observability
- Memory-learning algorithms (recurrent PPO, λ-discrepancy, transformer-XL) consistently outperform memoryless PPO, with gains attributable to partial observability mitigation
- The three algorithms show comparable overall performance, though transformer-XL excels in visual domains while recurrent approaches perform better in structured symbolic tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Memory improvability provides a reliable signal that performance gains arise from mitigating partial observability rather than confounding factors.
- Mechanism: By establishing both a performance floor (memoryless agent on partial observations) and ceiling (agent with more state information), the gap between them quantifies how much improvement is available from better memory. When an algorithm closes this gap, progress is attributable to partial observability mitigation.
- Core assumption: Other factors (network capacity, hyperparameters, learning dynamics) are roughly equal between the floor and ceiling agents, so the performance difference isolates information availability.
- Evidence anchors:
  - [abstract] "This gap implies that an environment is memory-improvable: where performance gains in a domain are from an algorithm's ability to cope with partial observability as opposed to other factors."
  - [section 4] "An environment is memory-improvable if there exists a gap between the performance of agents with less or more state information."
  - [corpus] Weak direct support; neighbor papers address partial observability but not the memory improvability property specifically.
- Break condition: If ceiling agents outperform floor agents due to different network architectures, hyperparameters, or training regimes (not just observation content), the signal is confounded.

### Mechanism 2
- Claim: Coverage across diverse partial observability categories ensures algorithms generalize beyond narrow benchmarks.
- Mechanism: The eight categories (noisy features, visual occlusion, object tracking, spatial uncertainty, moment features, unknown opposition, episode nonstationarity, needle-in-haystack) represent distinct structural challenges. Testing across them reveals whether an algorithm learns transferable memory strategies.
- Core assumption: The selected categories span the practically relevant forms of partial observability; unrepresented forms (e.g., adversarial observation corruption) may exhibit different failure modes.
- Evidence anchors:
  - [section 5] "We define eight categories popular in partial observability and example problems for each. Note that environments may fall into multiple categories."
  - [section 1] "Current benchmarks are narrow in their scope of state aliasing, bringing into question whether performance on the benchmark translates to other forms of partial observability."
  - [corpus] Neighbor papers (e.g., "Belief States for Cooperative Multi-Agent Reinforcement Learning") address multi-agent partial observability—a category the paper notes is out of scope.
- Break condition: If a new partial observability type emerges that doesn't fit these categories, generalization claims may not transfer.

### Mechanism 3
- Claim: Fixing general hyperparameters (parallel environments, network width) across algorithms isolates memory mechanism differences.
- Mechanism: Number of parallel environments affects minibatch composition and gradient update frequency; network width affects capacity. By sweeping these per environment then fixing them, the benchmark reduces variance from implementation details, making algorithm comparisons meaningful.
- Core assumption: The recommended hyperparameters are near-optimal for all tested algorithms; if an algorithm requires fundamentally different settings, comparisons may be unfair.
- Evidence anchors:
  - [section 3.1] "Modifying the number of parallel copies of environments can drastically change the performance of a given featurization and algorithm."
  - [section 3.2] "Network width is another general hyperparameter... with a sizable but diminishing effect as width increases."
  - [corpus] No direct corpus evidence on hyperparameter confounding in partial observability benchmarks.
- Break condition: If an algorithm's memory mechanism requires different optimal hyperparameters (e.g., transformer vs. RNN), fixing shared settings may disadvantage one approach.

## Foundational Learning

- Concept: POMDPs (Partially Observable Markov Decision Processes)
  - Why needed here: The entire benchmark assumes agents receive observations o_t from an observation function Φ(s_t) rather than true states, requiring history-based policies.
  - Quick check question: Can you explain why P(ot+1, rt | ot, at) ≠ P(ot+1, rt | ot, at, ..., o0, a0) in partially observable settings?

- Concept: Memory functions (RNNs, GRUs, Transformers)
  - Why needed here: The tested algorithms use recurrent memory (GRU cells) or attention-based memory (Transformer-XL) to summarize history into a compact state.
  - Quick check question: What is the difference between a recurrent memory function mt = μ(ot, at, mt−1) and a fixed-context transformer approach?

- Concept: PPO (Proximal Policy Optimization)
  - Why needed here: All baseline algorithms use PPO as the underlying optimizer; understanding its clipping objective and GAE λ parameter is necessary to interpret hyperparameter sweeps.
  - Quick check question: How does the GAE λ parameter trade off bias vs. variance in advantage estimation?

## Architecture Onboarding

- Component map:
  - Environments: JAX-based reimplementations of T-Maze, RockSample, Battleship, Masked Mujoco (via Brax), DMLab MiniGrid (via NAVIX), Visual Mujoco (via Madrona MJX), No-inventory Crafter (via Craftax)
  - Algorithms: Memoryless PPO, Recurrent PPO (GRU), λ-discrepancy (extends Recurrent PPO), Transformer-XL with PPO
  - Training loop: Parallel environment sampling → trajectory collection → PPO update with optional memory state management

- Critical path:
  1. Select environment and load recommended hyperparameters (num_envs, hidden_size, etc.)
  2. Choose observation mode (partial vs. full state) to establish floor/ceiling
  3. Run hyperparameter sweep for algorithm-specific settings (lr, λ0, λ1 for λ-discrepancy)
  4. Evaluate over 30 seeds; compute AUC and confidence intervals

- Design tradeoffs:
  - GPU parallelization vs. memory: More parallel environments (256–512) improve stability but require GPU memory; visual environments use Madrona MJX which supports only single batched environment
  - Ceiling agent design: Some environments (RockSample, masked Mujoco) use RNN even for "fully observable" ceiling because RNN function approximation outperforms MLP—this is a methodological choice, not pure observability
  - Transformer context window vs. recurrent state: Transformer-XL uses segment recurrence; window_mem=128 caches hidden states but increases memory

- Failure signatures:
  - No memory improvability gap: If partial and full observation agents perform similarly, the environment doesn't test memory (seen in some masked control tasks in prior work)
  - Ceiling unreachable: If even the ceiling agent fails to learn, exploration or reward shaping may be the bottleneck, not partial observability
  - Hyperparameter sensitivity: Large variance across seeds suggests hyperparameters weren't adequately swept

- First 3 experiments:
  1. Reproduce memory improvability gap on T-Maze (simplest diagnostic): Train memoryless PPO vs. Recurrent PPO vs. full-state MLP; verify RNN closes the gap.
  2. Ablate number of parallel environments on DMLab Minigrid: Compare num_envs=64 vs. 256 to confirm the recommended setting is necessary.
  3. Compare Recurrent PPO vs. Transformer-XL on No-inventory Crafter: Test whether attention-based memory handles object tracking and inventory recall differently than recurrent memory.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can existing memory-learning algorithms solve the DeepMind Lab MiniGrid `maze_id = 03` environment, which was excluded from the main benchmark due to its high difficulty?
- Basis in paper: [explicit] The paper notes that `maze_id = 03` requires complex localization and poses a hard exploration challenge, leaving it as "a difficult, unsolved challenge" (Section 6.2).
- Why unresolved: The authors successfully benchmarked `maze_id = 01` and `02` but found the third maze intractable for the tested algorithms (RNN, Transformer-XL) even with high parallelism.
- What evidence would resolve it: A study showing an agent achieving the "full state" performance ceiling or significantly closing the memory improvability gap specifically on the `maze_id = 03` task.

### Open Question 2
- Question: How would the ranking or evaluation of memory-learning algorithms change if the POBAX suite included "Needle in a Haystack" environments that test pure sequence memorization?
- Basis in paper: [explicit] The authors categorize this form of partial observability but explicitly exclude it because such environments are "diagnostic and purely meant to test memory length" rather than general partial observability (Section 5).
- Why unresolved: The benchmark focuses on domains where information accumulates meaningfully, leaving open the question of how well these algorithms handle tasks relying solely on rote memorization of random sequences.
- What evidence would resolve it: Integrating a diagnostic "Needle in a Haystack" task (like the Autoencode task mentioned) into POBAX and comparing the performance gaps of RNNs versus Transformers.

### Open Question 3
- Question: Does the heterogeneous definition of the performance "ceiling" (skyline) across different POBAX domains affect the comparability of "memory improvability" gaps?
- Basis in paper: [inferred] The paper notes that the "full state" agent varies by task: it is an optimal belief policy in Battleship, a symbolic observation in Crafter, and a memoryless agent in other tasks (Section 6.2, Appendices C.3, C.8).
- Why unresolved: Because the upper bound is computed using different methods (programmatic optimal policies vs. trained agents with privileged info), a "large gap" in one domain may not represent the same algorithmic difficulty as a large gap in another.
- What evidence would resolve it: A standardized analysis applying a single method for defining the state-ceiling (e.g., using oracle value functions) across all domains to normalize the memory improvability metric.

## Limitations
- Memory improvability depends on ceiling agents being trained with optimal hyperparameters and architecture; if these differ systematically from floor agents, the gap may conflate observation informativeness with training artifacts
- While eight categories of partial observability are covered, emerging forms (e.g., adversarial observation corruption) may not be well-represented
- The benchmark assumes memory agents benefit from richer observations, but in some cases (e.g., RockSample) ceiling agents still use RNNs, complicating interpretation of the improvability gap

## Confidence
- High confidence in the core design principle of memory improvability as a metric for partial observability benchmarks
- Medium confidence in the generalization claims across the eight categories, given that some domains may exhibit multiple observability types simultaneously
- Low confidence in hyperparameter stability across all algorithms, as different memory mechanisms (RNN vs. Transformer-XL) may have fundamentally different optimal settings

## Next Checks
1. Perform ablation studies comparing RNN vs. MLP ceiling agents across all domains to isolate the effect of observation informativeness vs. architectural bias
2. Test the suite on a domain with adversarial observation corruption (e.g., randomly flipped observations) to evaluate robustness to categories not explicitly covered
3. Conduct hyperparameter sensitivity analysis for Transformer-XL vs. recurrent PPO to determine if fixed settings disadvantage any algorithm