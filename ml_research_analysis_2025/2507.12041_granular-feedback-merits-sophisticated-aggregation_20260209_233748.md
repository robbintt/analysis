---
ver: rpa2
title: Granular feedback merits sophisticated aggregation
arxiv_id: '2507.12041'
source_url: https://arxiv.org/abs/2507.12041
tags:
- feedback
- individuals
- loss
- point
- regavg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether more sophisticated aggregation
  methods can outperform simple regularized averaging when predicting population distributions
  from limited individual feedback. The authors show that the advantage of sophisticated
  methods like supervised learning increases with feedback granularity.
---

# Granular feedback merits sophisticated aggregation

## Quick Facts
- arXiv ID: 2507.12041
- Source URL: https://arxiv.org/abs/2507.12041
- Reference count: 40
- More granular feedback enables sophisticated aggregation methods to significantly outperform simple averaging

## Executive Summary
This paper demonstrates that the advantage of sophisticated aggregation methods over regularized averaging increases with feedback granularity. Using a dataset of social attitude questions with 11-point feedback from 39 Thai crowdworkers, the authors show that while sophisticated methods barely improve upon regularized averaging with binary feedback, they require only about half as many individuals to match regularized averaging's performance with 5-point feedback. The key insight is that regularized averaging binarizes granular feedback before aggregation, losing information that sophisticated methods can exploit. This finding has practical implications for data efficiency in RLHF pipelines, where sophisticated aggregation could reduce the number of scores needed by approximately 20% while maintaining comparable quality.

## Method Summary
The paper compares regularized averaging (RegAvg) against supervised learning (SL) for predicting population distributions from limited individual feedback. RegAvg predicts the CDF using a weighted combination of a prior and empirical CDF, while SL uses a modified MLP that outputs valid CDFs through softmax and cumulative sum operations. The experiments use a custom dataset of 1,020 task units with 11-point feedback from 39 Thai crowdworkers, evaluating performance across binary, 5-point, and 11-point feedback scales. The evaluation measures ordinal log loss and preference signal losses, with hyperparameter tuning through 5-fold cross-validation and permutation-based environment sampling.

## Key Results
- Sophisticated methods require only ~50% as many individuals as RegAvg to achieve equivalent performance with 5-point feedback
- With binary feedback, sophisticated methods show minimal improvement over RegAvg
- The advantage of sophisticated methods scales with feedback granularity
- These findings suggest potential 20% data efficiency improvements in RLHF pipelines

## Why This Works (Mechanism)
The advantage of sophisticated aggregation methods stems from their ability to process granular feedback without binarization. While RegAvg treats all non-zero feedback as equivalent by converting to binary signals before aggregation, sophisticated methods can exploit the full ordinal information in granular feedback. This allows them to better capture the true distribution of preferences across the population, particularly when feedback scales provide meaningful distinctions between different levels of preference.

## Foundational Learning
- **Cumulative Distribution Functions (CDFs)**: Why needed - to represent population-level preference distributions; Quick check - verify output layer produces monotonically increasing values
- **Ordinal Log Loss**: Why needed - appropriate metric for ordinal prediction tasks; Quick check - confirm implementation applies log loss to cumulative probabilities
- **Probabilistic Binning**: Why needed - mapping 11-point to lower granularities while preserving information; Quick check - verify -3 maps to -2 (0.2) and -1 (0.8) in 5-point scale
- **Permutation-based Environment Sampling**: Why needed - to create train/eval splits for hyperparameter tuning; Quick check - confirm seed 42 split with 30/30 permutation split
- **Modified MLP for CDF Output**: Why needed - standard regression heads can produce invalid CDFs; Quick check - ensure softmax followed by cumulative sum

## Architecture Onboarding

**Component Map**: Data Preprocessing -> Aggregation Method -> Loss Function -> Evaluation

**Critical Path**: The modified MLP architecture is critical - it must output valid CDFs through softmax + cumulative sum rather than standard regression

**Design Tradeoffs**: RegAvg trades information preservation for simplicity (binarization loses granularity), while SL trades computational complexity for better information utilization

**Failure Signatures**: Non-monotonic CDF outputs indicate incorrect final layer implementation; Poor performance with binary data suggests overfitting to granular patterns

**First Experiments**:
1. Verify the probabilistic mapping from 11-point to 5-point scales by checking that a value of -3 maps to -2 with probability 0.2 and -1 with probability 0.8
2. Confirm the cumulative sum operation on the MLP's softmax output to ensure valid CDF predictions
3. Test that the prior Q₀ is computed exclusively from the held-out output-set workers to prevent data leakage

## Open Questions the Paper Calls Out
1. To what extent do sophisticated unsupervised aggregation algorithms improve prediction performance over regularized averaging when processing granular feedback? The paper only evaluated supervised learning and leaves demonstrating the advantage for unsupervised algorithms to future work.

2. How does the frequency of missing values—where not all individuals provide feedback on all task units—affect the relative advantage of sophisticated aggregation methods? The experimental design assumed complete feedback, but the advantage is "unclear when there are missing values."

3. Is there a granularity ceiling beyond which increasing the number of scale points fails to improve aggregation performance because users cannot meaningfully differentiate options? The study only tested scales up to 11 points, and the advantage "may not keep increasing with granularity."

## Limitations
- Dataset accessibility not directly provided, creating potential barrier to exact replication
- MLP architecture specifics (activation functions, normalization) not fully detailed
- 20% data efficiency improvement in RLHF is an extrapolation rather than empirically validated claim

## Confidence
- Granular feedback provides information that regularized averaging cannot exploit: Medium confidence
- Sophisticated methods require ~50% as many individuals for 5-point feedback: High confidence (directly supported by Figure 2b)
- 20% data efficiency improvement in RLHF pipelines: Low confidence (reasonable extrapolation but not empirically validated)

## Next Checks
1. Verify the probabilistic mapping from 11-point to 5-point scales by checking that a value of -3 maps to -2 with probability 0.2 and -1 with probability 0.8
2. Confirm the cumulative sum operation on the MLP's softmax output to ensure valid CDF predictions
3. Test that the prior Q₀ is computed exclusively from the held-out output-set workers to prevent data leakage