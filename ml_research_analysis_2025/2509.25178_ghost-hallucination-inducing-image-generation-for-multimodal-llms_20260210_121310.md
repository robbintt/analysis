---
ver: rpa2
title: 'GHOST: Hallucination-Inducing Image Generation for Multimodal LLMs'
arxiv_id: '2509.25178'
source_url: https://arxiv.org/abs/2509.25178
tags:
- ghost
- image
- object
- images
- clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GHOST is a fully automated method for generating images that induce
  object hallucinations in multimodal large language models (MLLMs). It optimizes
  in CLIP embedding space to mislead models while preserving visual naturalness, then
  uses diffusion to generate natural-looking images.
---

# GHOST: Hallucination-Inducing Image Generation for Multimodal LLMs

## Quick Facts
- **arXiv ID**: 2509.25178
- **Source URL**: https://arxiv.org/abs/2509.25178
- **Reference count**: 40
- **Key outcome**: GHOST achieves hallucination success rates exceeding 28% in open-source MLLMs through automated image generation optimized in CLIP embedding space

## Executive Summary
GHOST introduces a fully automated method for generating images that induce object hallucinations in multimodal large language models (MLLMs). The approach optimizes in CLIP embedding space to mislead models while preserving visual naturalness, then uses diffusion to generate natural-looking images. Across three open-source models, GHOST achieves hallucination success rates exceeding 28%, compared to 1% in prior methods. Images are high-quality and object-free as confirmed by human evaluation (89% agreement). GHOST also uncovers transferable vulnerabilities: images optimized for Qwen2.5-VL induce hallucinations in GPT-4o at 66.5%. Fine-tuning on GHOST-generated images improves robustness to hallucination while preserving general capabilities.

## Method Summary
GHOST operates through a two-stage optimization process. First, it optimizes in CLIP embedding space to create images that mislead MLLMs into hallucinating objects that don't exist in the image. This optimization balances two objectives: maximizing hallucination success while preserving visual naturalness to avoid detection. The second stage uses diffusion models to convert these optimized embeddings into high-quality, natural-looking images. The method is fully automated and doesn't require manual intervention or handcrafted adversarial patterns. GHOST can generate images that cause MLLMs to confidently describe non-existent objects, with success rates significantly higher than previous methods that relied on less sophisticated attack vectors.

## Key Results
- GHOST achieves hallucination success rates exceeding 28% across three open-source MLLMs, compared to 1% in prior methods
- Human evaluation confirms images are high-quality and object-free (89% agreement)
- Cross-model transferability demonstrated: Qwen2.5-VL-optimized images induce hallucinations in GPT-4o at 66.5%
- Fine-tuning on GHOST-generated images improves robustness while preserving general capabilities

## Why This Works (Mechanism)
GHOST exploits the fact that MLLMs rely on CLIP-like embeddings that capture semantic relationships between images and text. By optimizing in this embedding space, GHOST can create subtle perturbations that cause the model to associate non-existent objects with the image's semantic content. The diffusion-based generation ensures these optimizations translate into natural-looking images rather than obvious adversarial patterns. The method leverages the inherent vulnerability of MLLMs to make confident predictions about objects that aren't present, exploiting the gap between visual appearance and semantic interpretation.

## Foundational Learning
- **CLIP embedding space optimization**: Understanding how CLIP models represent semantic relationships between images and text is crucial for creating effective hallucinations. Quick check: Verify that small perturbations in CLIP space can significantly alter model predictions.
- **Diffusion model inversion**: The ability to convert optimized embeddings back into natural images while preserving the adversarial properties requires understanding diffusion model mechanics. Quick check: Ensure the generated images maintain high FID scores while preserving hallucination capability.
- **Multimodal model vulnerabilities**: Recognizing that MLLMs can be misled by semantic-level attacks rather than just pixel-level perturbations is key to understanding GHOST's effectiveness. Quick check: Compare success rates against pixel-level vs semantic-level attacks.

## Architecture Onboarding

**Component Map**: CLIP embedding optimizer -> Diffusion generator -> MLLM hallucination detector -> Human evaluation pipeline

**Critical Path**: The optimization loop (CLIP space → MLLM hallucination detection → embedding update) represents the core pipeline where success is determined.

**Design Tradeoffs**: 
- Visual naturalness vs hallucination strength (optimized jointly)
- Computational cost of optimization vs generation quality
- Transferability across models vs attack specificity

**Failure Signatures**: 
- Low hallucination rates despite successful optimization
- Generated images fail human naturalness evaluation
- Cross-model transferability not achieved

**First Experiments**:
1. Test optimization success rate on a single MLLM before diffusion generation
2. Validate that diffusion generation preserves optimization objectives
3. Measure hallucination success on held-out images from the same optimization

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to open-source models with only limited testing on GPT-4o, raising questions about generalizability to other commercial MLLMs
- Human evaluation of image naturalness involved only three annotators per image, potentially insufficient for capturing subjective judgments
- Computational cost and optimization time not discussed, which could impact practical deployment

## Confidence
**High confidence**: The core technical contribution of the optimization framework is well-documented and reproducible. Quantitative improvements over baselines (28%+ success rate vs 1%) are clearly demonstrated.

**Medium confidence**: Cross-model transferability results are promising but based on limited testing. Human evaluation of image naturalness, while showing high agreement, involves a small sample size.

**Low confidence**: Long-term effectiveness of the fine-tuning defense and its impact on model capabilities in non-targeted scenarios requires further investigation. Generalizability to the full spectrum of commercial MLLMs remains uncertain.

## Next Checks
1. **Cross-model generalization study**: Test GHOST-generated images against a broader range of commercial MLLMs (Claude-3, Gemini, etc.) to establish the prevalence and extent of transferable vulnerabilities across the MLLM ecosystem.

2. **Longitudinal effectiveness analysis**: Evaluate whether GHOST-generated images maintain their hallucination-inducing capability across different model versions and after standard fine-tuning procedures, establishing temporal stability of the attack vectors.

3. **Comprehensive robustness evaluation**: Compare the fine-tuning defense approach against alternative mitigation strategies (adversarial training, input preprocessing, architectural modifications) while measuring performance across diverse downstream tasks to assess potential trade-offs.