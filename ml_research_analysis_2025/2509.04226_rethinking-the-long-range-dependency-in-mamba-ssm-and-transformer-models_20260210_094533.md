---
ver: rpa2
title: Rethinking the long-range dependency in Mamba/SSM and transformer models
arxiv_id: '2509.04226'
source_url: https://arxiv.org/abs/2509.04226
tags:
- hidden
- mamba
- arxiv
- state
- dependency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new theoretical framework for understanding
  long-range dependency (LRD) in sequence models, specifically comparing state-space
  models (SSMs) like Mamba with transformer architectures. The authors define LRD
  using the derivative of hidden states with respect to past inputs and prove that
  SSMs exhibit exponential decay in LRD with sequence length, similar to RNNs, while
  transformers do not have this constraint.
---

# Rethinking the long-range dependency in Mamba/SSM and transformer models

## Quick Facts
- arXiv ID: 2509.04226
- Source URL: https://arxiv.org/abs/2509.04226
- Authors: Cong Ma; Kayvan Najarian
- Reference count: 40
- Key outcome: Introduces a new theoretical framework for understanding long-range dependency (LRD) in sequence models, proving SSMs exhibit exponential decay in LRD while transformers do not have this constraint, and proposes a new interaction-based hidden state formulation to overcome SSM limitations.

## Executive Summary
This paper provides a novel theoretical framework for understanding long-range dependency in sequence models by analyzing the sensitivity of hidden states to past inputs through gradient-based analysis. The authors prove that traditional state-space models like Mamba suffer from exponential decay in their ability to maintain information from distant past inputs, similar to RNNs, while transformers avoid this limitation through their flexible attention mechanism. To address this fundamental limitation of SSMs while preserving their computational efficiency, the paper proposes a new hidden state formulation that introduces an interaction term between past hidden states and current inputs, allowing for more flexible dependency patterns that avoid strict exponential decay.

## Method Summary
The paper analyzes long-range dependency through the lens of sensitivity analysis, defining LRD as the derivative of future hidden states with respect to past inputs. The authors prove exponential decay in SSMs by showing that the product of transition matrices with eigenvalues less than 1 inevitably decays over time. They then propose a new formulation that adds an interaction term between the previous hidden state and current input, effectively making the transition matrix input-dependent. The stability of this new formulation is proven under standard Gaussian input distributions using sub-exponential tail bounds.

## Key Results
- SSMs exhibit exponential decay in LRD due to the product of stable transition matrices, similar to RNNs
- Transformers do not have fixed exponential decay constraints due to dynamic attention weight computation
- The proposed interaction-based SSM formulation avoids exponential decay while maintaining stability under Gaussian inputs
- Theoretical proof shows the new model's hidden states remain bounded despite the non-linear interaction term

## Why This Works (Mechanism)

### Mechanism 1
State Space Models (SSMs), including Mamba, theoretically exhibit exponential decay in long-range dependency (LRD) due to the recursive multiplication of the state transition matrix. The paper defines LRD as the derivative of a future hidden state $h_{t+k}$ with respect to a past input $x_t$. In SSMs, this derivative relies on a product of transition matrices $\prod \bar{A}_j$. If eigenvalues of $\bar{A}$ are less than 1 (required for stability), the norm of this product inevitably decays exponentially as the time gap $k$ increases. Core assumption: The system is stable, meaning the eigenvalues of the transition matrix $\bar{A}$ are non-positive or strictly less than 1 in magnitude. Evidence anchors: [abstract] "We showed that the long-range dependency of SSM decays exponentially with the sequence length, which aligns with the exponential decay of memory function in RNN." [section 3.1] Corollary 1 proves that the L2 norm of LRD is bounded by $exp(\lambda_1 \Delta t)$. Break condition: If the transition matrix eigenvalues are not strictly stable (e.g., $\ge 1$), the hidden states may explode, breaking the utility of the model despite potentially higher LRD.

### Mechanism 2
The attention mechanism in Transformers is not constrained by fixed exponential decay, allowing dependency strength to increase or fluctuate based on content relevance rather than distance. In Transformers, the derivative of the hidden state with respect to past inputs depends on the weights $w(i, t)$ and their derivatives. Since weights are computed dynamically via softmax($QK^T$), the interaction strength between time $t$ and $t+k$ is not a fixed function of $k$. Theoretically, $w(t, t+k+1)$ can be larger than $w(t, t+k)$, allowing LRD to grow or persist non-exponentially. Core assumption: Sufficient training data and compute exist to learn the optimal attention matrices $W_Q, W_K, W_V$; without this, the flexibility may lead to overfitting or "artificial interactions." Evidence anchors: [section 3.2] Theorem 1 shows the LRD formula includes a summation of weight derivatives and weights, allowing for non-monotonic behavior. [section 1] Notes Transformers are "more flexible and is not constrained to exponential decay." Break condition: In practice, limited context windows or attention sink issues may practically truncate LRD even if the theoretical mechanism allows it.

### Mechanism 3
Introducing a non-linear "interaction term" between the previous hidden state and current input modifies the state transition dynamics, allowing the model to escape strict exponential decay while maintaining stability. The proposed formulation $h_t = (\bar{A}_t + G x_t x_t^T W^T) h_{t-1} + \bar{B}_t x_t$ adds a rank-1 update $G x_t x_t^T W^T$ to the transition matrix. This makes the effective transition matrix input-dependent. If the interaction is high, it shifts the eigenvalues dynamically, preventing the monotonic decay seen in standard SSMs. Core assumption: Inputs follow a distribution (e.g., Standard Gaussian) where the product of eigenvalue perturbations over time remains bounded. Evidence anchors: [section 4] Equation 13 introduces the interaction term $(h_{t-1}^T W x_t)(G x_t)$. [section 4.2] Theorem 4 provides a probabilistic bound ensuring the hidden state does not explode under Gaussian input assumptions. Break condition: If inputs consistently have large magnitudes such that the perturbed eigenvalue $\lambda_H + \gamma x_i^2 > 1$ occurs frequently, the system may become unstable.

## Foundational Learning

- **Concept: Sensitivity Analysis (The Gradient Perspective)**
  - Why needed here: The paper redefines "memory" not as a storage bin, but as sensitivity ($\partial h_{t+k} / \partial x_t$). Understanding LRD requires thinking about how much a small change in the past ripples forward to the present.
  - Quick check question: If $\partial h_{t+k} / \partial x_t \approx 0$, what does that imply about the model's ability to use information from time $t$ to inform time $t+k$? (Answer: It implies the model has effectively "forgotten" the input at $t$).

- **Concept: Eigenvalues of Recurrence Matrices**
  - Why needed here: The exponential decay in SSMs is a direct consequence of linear algebra rules regarding repeated matrix multiplication. The spectral radius (largest eigenvalue magnitude) dictates whether signals decay (fade out) or explode (accumulate noise).
  - Quick check question: In a stable SSM, why must the eigenvalues of the discretized matrix $\bar{A}$ typically be less than 1? (Answer: To ensure the signal magnitude decreases over time to prevent unbounded state growth).

- **Concept: Sub-exponential Distributions**
  - Why needed here: The stability proof for the new architecture relies on "sub-exponential tail bounds." This statistical concept ensures that while extreme inputs are possible, they are rare enough that they don't destabilize the infinite recursion of the model.
  - Quick check question: Why is a "tail bound" necessary to prove stability in the proposed interaction model? (Answer: Because the interaction term introduces randomness into the transition matrix eigenvalues; we need to prove these random shifts won't systematically push the system toward infinity).

## Architecture Onboarding

- **Component map:** Standard SSM: $h_t = \bar{A} h_{t-1} + \bar{B} x_t$ (Linear recurrence) -> Proposed Interaction SSM: $h_t = (\bar{A} + \text{Interaction}_t) h_{t-1} + \bar{B} x_t$
- **Critical path:** 1. Project current input $x_t$ and previous state $h_{t-1}$ using $W$. 2. Compute scalar interaction strength $\alpha = h_{t-1}^T W x_t$. 3. Compute state shift direction $v = G x_t$. 4. Update effective transition matrix or add term directly to $h_{t-1}$ before applying $\bar{A}$.
- **Design tradeoffs:** Efficiency: The proposed interaction term likely introduces overhead compared to the pure linear scan of Mamba, though it aims to remain more efficient than quadratic attention. Stability: The model trades the guaranteed stability of fixed-eigenvalue SSMs for flexible LRD. Stability is now probabilistic (Theorem 4), relying on data distribution properties (Gaussian assumption) rather than architectural hard-coding.
- **Failure signatures:** Exploding States: If $G$ and $W$ are scaled too aggressively, or if inputs are not normalized, the term $\gamma x_i^2$ will push eigenvalues $> 1$, causing $h_t$ to diverge to infinity (NaNs). No Improvement: If the interaction term weights are small or effectively learned to be zero, the model collapses back to standard SSM behavior (exponential decay).
- **First 3 experiments:** 1. Synthetic Validation (Figure 1 Reproduction): Generate random matrices and Gaussian inputs. Plot the L2 norm of $\partial h_{t+k} / \partial x_t$ over time gap $k$. Verify that the standard SSM curve decays to 0, while the proposed model shows non-monotonic or sustained magnitude. 2. Stability Boundary Testing: Input sequences with increasing variance (non-Gaussian tails). Identify the input variance threshold where the proposed model's hidden states begin to diverge (testing Theorem 4's limits). 3. Copy-Paste Benchmark: Run a simple sequence copying task where the model must retrieve a token from $N$ steps back. Compare the standard SSM (performance drops as $N$ grows) vs. the Interaction SSM (should maintain performance longer if interaction acts like attention).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the stability conditions for the proposed hidden state formulation when the interaction term weight vectors ($G$ and $W$) do not align with the eigenvectors of the transition matrix $\bar{A}$?
- Basis in paper: [explicit] The authors state, "The stability condition only handles the special case when the weight vectors in the interaction term are scaling of the same eigenvector... but the stability conditions of the model for other cases... are unknown."
- Why unresolved: The current mathematical proof for stability (Theorem 4) relies on the assumption of simultaneous diagonalization to manage eigenvalues, which does not hold for general matrix configurations.
- What evidence would resolve it: A formal proof establishing bounded hidden states for the general case, or identification of specific constraints required for stability when matrices are not simultaneously diagonalizable.

### Open Question 2
- Question: Can an efficient parallel algorithm, similar to the prefix sum approach used in standard Mamba, be derived for the proposed interaction-based hidden state update?
- Basis in paper: [explicit] The authors note that "the time complexity and parallelizability of the proposed model require further investigation."
- Why unresolved: While standard SSMs allow for parallelization via FFT or prefix scans, the introduction of the input-dependent interaction term ($h_{t-1}^T W x_t$) changes the recurrence structure, potentially breaking the associative property required for parallel scans.
- What evidence would resolve it: Derivation of a parallel algorithm with associated time complexity analysis, or a proof showing that the recurrence can be computed in linear time with logarithmic parallel depth.

### Open Question 3
- Question: Does the theoretical avoidance of exponential decay in the new formulation translate to improved performance on real-world benchmark datasets compared to hybrid models?
- Basis in paper: [explicit] The authors state, "Evaluation using existing benchmarking datasets is needed to verify the improvement in prediction... Comparing the accuracy... with the hybrid models combing Mamba and transformers is of particular interest."
- Why unresolved: The paper validates the model only through simulations (Figure 1) demonstrating non-monotonic LRD decay, but provides no empirical results on downstream tasks like language modeling or classification.
- What evidence would resolve it: Experimental results on standard long-range sequence benchmarks (e.g., Long Range Arena, language modeling perplexity) comparing the new model against Mamba, Transformers, and hybrid architectures.

## Limitations

- Theoretical stability proof relies heavily on Gaussian input assumptions that may not hold in real-world scenarios
- Significant empirical validation deficit - lacks comprehensive benchmarking on real-world tasks
- Potential efficiency overheads from interaction term may offset SSM computational advantages

## Confidence

- High Confidence: The proof that standard SSMs exhibit exponential decay in LRD (Section 3.1)
- Medium Confidence: The characterization of transformer LRD as non-exponential (Section 3.2)
- Low Confidence: The stability guarantees for the proposed interaction model (Theorem 4)

## Next Checks

1. **Synthetic LRD Decay Curves:** Replicate Figure 1 by implementing both standard SSM and interaction SSM formulations, then plot the L2 norm of ∂h_{t+k}/∂x_t over increasing time gaps k. Verify that the standard SSM shows exponential decay while the interaction model demonstrates sustained or non-monotonic behavior.

2. **Stability Stress Testing:** Create a benchmark that inputs sequences with systematically increasing variance (from Gaussian to heavy-tailed distributions). Measure the point at which the proposed model's hidden states begin to diverge, testing the practical limits of Theorem 4's stability guarantees.

3. **Copy-Paste Task Evaluation:** Implement a controlled sequence copying task where a token from N steps back must be retrieved. Compare standard SSM, interaction SSM, and transformer performance as N increases to quantify the practical LRD improvements claimed by the paper.