---
ver: rpa2
title: 'GORACS: Group-level Optimal Transport-guided Coreset Selection for LLM-based
  Recommender Systems'
arxiv_id: '2506.04015'
source_url: https://arxiv.org/abs/2506.04015
tags:
- uni00000013
- selection
- uni00000011
- goracs
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently fine-tuning large
  language models (LLMs) for recommender systems by developing a novel coreset selection
  framework called GORACS. The core method combines optimal transport distance with
  gradient information to create a proxy optimization objective that bounds test loss
  without requiring repeated LLM retraining.
---

# GORACS: Group-level Optimal Transport-guided Coreset Selection for LLM-based Recommender Systems

## Quick Facts
- arXiv ID: 2506.04015
- Source URL: https://arxiv.org/abs/2506.04015
- Authors: Tiehua Mei; Hengrui Chen; Peng Yu; Jiaqing Liang; Deqing Yang
- Reference count: 40
- Primary result: Achieves 5-9% improvements in recommendation metrics while using only 15-20% of computational resources compared to full dataset training

## Executive Summary
This paper addresses the challenge of efficiently fine-tuning large language models (LLMs) for recommender systems by developing a novel coreset selection framework called GORACS. The core method combines optimal transport distance with gradient information to create a proxy optimization objective that bounds test loss without requiring repeated LLM retraining. The framework employs a two-stage Initialization-Then-Refinement Algorithm (ITRA) that first uses greedy search to generate an initial solution, then refines it through sample exchanges with pruning strategies. Experiments on generative and discriminative recommendation tasks across multiple datasets demonstrate that GORACS significantly reduces fine-tuning costs while achieving superior performance compared to state-of-the-art baselines and full data training.

## Method Summary
GORACS introduces a Proxy Optimization Objective (POO) that combines optimal transport distance with gradient norms to create a computationally efficient upper bound for test loss. The framework uses a two-stage Initialization-Then-Refinement Algorithm (ITRA) where greedy search generates an initial coreset based on a relaxed p-median problem, followed by exchange refinement accelerated by marginal improvement estimation using optimal transport dual variables. The method selects representative samples at the group level, capturing collective structures that individual-level importance metrics miss. The selected coreset is then used to fine-tune LLMs (specifically LLaMA-7B with LoRA) for both generative sequential recommendation and discriminative click-through rate prediction tasks.

## Key Results
- Achieves 5-9% improvements in recommendation metrics (HR@10, NDCG@5) compared to full dataset training
- Reduces computational resources by 80-85% (using only 15-20% of the data)
- Outperforms state-of-the-art baselines including importance-based methods (GraNd, EL2N) and diversity-based approaches
- Demonstrates strong generalization capability with test loss well-bounded by the Proxy Optimization Objective
- Shows robustness across multiple Amazon datasets (Games, Food, Movies) with varying dataset characteristics

## Why This Works (Mechanism)

### Mechanism 1
The Proxy Optimization Objective (POO) functions as a computationally efficient upper bound for test loss by combining optimal transport distance with gradient norms. It uses Kantorovich-Rubinstein duality to bound the generalization gap between training and test distributions, while gradient analysis bounds the training loss under smoothness assumptions. By minimizing the weighted sum of these terms, the framework theoretically minimizes expected test error without repeated LLM retraining.

### Mechanism 2
The Initialization-Then-Refinement Algorithm (ITRA) solves the combinatorial explosion of group-level selection by separating search into coarse initialization and fine-grained refinement. Greedy search first establishes a baseline coreset by solving a relaxed p-median problem, then exchange-based optimization refines it through sample swaps. Crucially, pruning accelerates this by estimating marginal improvement using OT dual variables, avoiding full OT distance calculations for every exchange.

### Mechanism 3
Group-level selection via Optimal Transport captures inter-sample correlations and collective structures that individual-level importance metrics miss. Unlike methods that score samples independently, OT distance is calculated over the distribution of the subset, enforcing structural matching between the coreset and validation distribution. This preserves the "geometry" of the data and prevents selection of only "hard" samples that might be outliers or unrepresentative.

## Foundational Learning

### Optimal Transport (Wasserstein Distance)
**Why needed here:** This is the mathematical engine of GORACS, quantifying the "cost" of morphing one probability distribution into another. Understanding this is required to grasp why POO bounds the generalization gap.
**Quick check question:** Can you explain why Wasserstein distance is preferred over KL-Divergence when measuring distances between distributions that might have non-overlapping support?

### Lipschitz Continuity & Smoothness
**Why needed here:** These mathematical properties are preconditions for the theoretical bounds. The proofs rely on the loss function not changing too abruptly (Lipschitz) and gradients being well-behaved (Smoothness) to guarantee the bounds hold.
**Quick check question:** If a model's loss landscape is extremely sharp (high curvature), does the gradient-based bound on training loss become looser or tighter?

### Coreset Selection vs. Active Learning
**Why needed here:** Both involve selecting subsets, but Coreset Selection is typically a one-shot preprocessing step for efficiency, whereas Active Learning is iterative and query-based. GORACS is a coreset method designed to reduce fine-tuning costs.
**Quick check question:** Does GORACS require feedback from the fine-tuned model to select the next batch of data, or does it select data before fine-tuning begins?

## Architecture Onboarding

### Component map:
Embedding Encoder (RoBERTa) -> Gradient Norm Cache -> Cost Matrix Constructor (combines embedding distances and gradient norms) -> ITRA Solver (Greedy Init -> Exchange Refinement) -> Label-Aware Splitter (for discriminative tasks)

### Critical path:
Encoding (Batch) -> Gradient Computation (One-time) -> Cost Matrix Calculation -> Greedy Init -> Refinement Loop (Estimate MI -> Prune -> Exchange -> Verify)

### Design tradeoffs:
- **Encoder strength:** Stronger encoders (BGE vs. BERT) improve performance but increase initialization overhead. Decision: Use pre-trained encoder aligned with LLM's tokenizer if possible.
- **Gradient vs. OT ($\lambda$):** High $\lambda$ prioritizes "hard" samples (gradient), potentially hurting generalization. Low $\lambda$ prioritizes coverage (OT), potentially retaining noise. Decision: Treat $\lambda$ as a hyperparameter tuned on validation proxy.
- **Budget size ($n$):** Smaller budgets reduce training cost but risk dropping below critical mass needed for LLM adaptation. Decision: Ensure $n$ exceeds minimum threshold (~256 samples).

### Failure signatures:
- "Pass@1 is 0%": If Refinement stage accepts no exchanges, greedy initialization likely converged to poor local minimum or MI estimator is miscalibrated.
- High Popularity Bias: If coreset over-represents popular items, OT distance minimization may be failing to capture long-tail of validation set.
- Computational Bottleneck: If selection time exceeds training time, exchange candidates $k$ or total iterations $T$ are set too high.

### First 3 experiments:
1. **Proxy Correlation Check:** Generate 10 random subsets of varying sizes. Compute POO score vs. Actual Test Loss. Verify linear correlation to ensure Proxy is valid for your specific LLM backbone.
2. **Ablation on $\lambda$:** Run GORACS with $\lambda=0$ (pure OT) vs. high $\lambda$ (pure Gradient). Determine sensitivity of recommendation task to "coverage" vs. "difficulty."
3. **Scalability Timing:** Measure wall-clock time for "Cost Matrix Construction" vs. "ITRA Refinement" on 10% sample of your dataset to estimate total selection overhead.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Can exploiting explicit temporal dependencies in user interaction sequences improve the approximation of the test distribution compared to the current static validation set approach?
**Basis in paper:** Section 4.1.1, Footnote 4 states, "We leave this promising direction [exploiting temporal information] as future work."
**Why unresolved:** Current method approximates test distribution using held-out validation set, which may fail to capture evolving user preferences in sequential data.
**What evidence would resolve it:** Experiments integrating time-decay weighting or temporal embeddings into Optimal Transport distance calculation.

### Open Question 2
**Question:** How does GORACS perform on complex recommendation paradigms beyond generative sequential and discriminative tasks, such as conversational or cross-domain recommendation?
**Basis in paper:** Section 6 (Conclusion) states, "In the future work, we will explore applying GORACS to more complex recommendation tasks."
**Why unresolved:** Paper currently validates framework only on SeqRec (generative) and CTR prediction (discriminative) tasks using specific datasets.
**What evidence would resolve it:** Evaluation results on multi-turn conversational datasets or cross-domain transfer tasks to demonstrate generalizability.

### Open Question 3
**Question:** Do the theoretical assumptions of smoothness and learning rate bounds hold when applying GORACS with standard adaptive optimizers like AdamW?
**Basis in paper:** Theorem 4.2 assumes $G$-smoothness and specific learning rate constraints for full-batch Gradient Descent, whereas experiments utilize LoRA and likely Adam-based optimizers.
**Why unresolved:** Theoretical gap exists between analysis based on GD and practical implementation using adaptive gradient methods that don't adhere to fixed learning rate assumptions.
**What evidence would resolve it:** Convergence analysis of POO specifically under dynamics of Adam or AdamW optimization.

## Limitations

- Theoretical bounds rely on strong assumptions about Lipschitz continuity and smoothness that may not hold for highly non-convex LLM fine-tuning landscapes
- Computational overhead of two-stage ITRA algorithm may become prohibitive for very large-scale recommendation datasets despite claimed efficiency gains
- Study uses only Amazon datasets with relatively small item vocabularies (Food: 1,300, Games: 5,000, Movies: 3,000), limiting generalizability to industrial-scale systems

## Confidence

- **High confidence**: Experimental results showing GORACS outperforms baselines on tested datasets (HR@10 improvements of 5-9%)
- **Medium confidence**: Theoretical claims about POO bounding test loss, as these depend on assumptions requiring careful validation in practice
- **Low confidence**: Scalability claims for industrial applications given limited dataset scope in experiments

## Next Checks

1. **Distribution Shift Test**: Evaluate GORACS on held-out test set with temporal or covariate shift relative to validation set to verify generalization bound holds under realistic conditions
2. **Large-Scale Scalability**: Benchmark GORACS selection time and downstream performance on dataset with 100K+ items to assess practical deployment viability
3. **Loss Landscape Analysis**: Profile actual loss surface of LLM during fine-tuning to measure gap between theoretical smoothness assumptions and empirical behavior