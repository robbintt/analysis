---
ver: rpa2
title: User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale
arxiv_id: '2601.08225'
source_url: https://arxiv.org/abs/2601.08225
tags:
- tool
- data
- task
- generation
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the limitation of existing multi-turn dialogue
  generation frameworks that rely on static, predefined toolsets, which fail to capture
  the complexity of open-ended human-agent collaboration. To overcome this, we introduce
  a user-oriented simulation paradigm that decouples task generation from a dedicated
  user simulator governed by human behavioral rules, enabling more authentic, extended
  multi-turn dialogues.
---

# User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale

## Quick Facts
- arXiv ID: 2601.08225
- Source URL: https://arxiv.org/abs/2601.08225
- Authors: Jungho Cho; Minbyul Jeong; Sungrae Park
- Reference count: 40
- Primary result: Introduces user-oriented simulation paradigm decoupling task generation from dedicated user simulator to produce high-density, extended multi-turn dialogues; improves agentic benchmark performance and tool usage reliability.

## Executive Summary
This work addresses the limitation of existing multi-turn dialogue generation frameworks that rely on static, predefined toolsets, which fail to capture the complexity of open-ended human-agent collaboration. To overcome this, we introduce a user-oriented simulation paradigm that decouples task generation from a dedicated user simulator governed by human behavioral rules, enabling more authentic, extended multi-turn dialogues. Our generation pipeline dynamically synthesizes domain-specific tools, maintains state consistency across turns, and supports high-density trajectories where multiple tasks are completed within a single session. Empirical results show that models trained on our data achieve consistently stronger performance on agentic benchmarks (BFCL and τ2), with improved multi-turn coherence and tool usage reliability, especially in stateful domains.

## Method Summary
The method introduces a three-stage pipeline: (1) Tool Preparation generates domain-specific tools from seed tools using question-to-spec generation and complementary tool expansion; (2) Tool Preprocessing predicts JSON schemas for tool outputs ensuring type consistency; (3) Conversation Generation uses descriptive tasks, rubrics, and a dedicated user simulator to produce high-density, multi-turn dialogues. The user simulator follows behavioral rules like incremental requests (1-2 subtasks per turn) and turn-by-turn feedback. Optional SQL-backed tool execution provides grounded outputs. The resulting data trains reasoning models via full fine-tuning with DeepSpeed ZeRO-3, FlashAttention-2, and AdamW.

## Key Results
- User-oriented data averages 2.48 tasks per sample vs. 1.63 for task-oriented, enabling high-density trajectories.
- Models trained on user-oriented data outperform baselines on BFCL (Multi-turn/Agentic/Live/Non-Live/Hallucination) and τ2 (Airline/Retail/Telecom) benchmarks.
- Execution-grounded tool outputs improve tool usage reliability, with User-oriented + Tool Execution outperforming User-oriented alone on τ2 Telecom (35.1 vs 30.7 for 4B model; 40.4 vs 34.2 for 30B model).

## Why This Works (Mechanism)

### Mechanism 1: Task-User Decoupling Reduces Efficiency Bias
- **Claim:** Separating task definition from user interaction modeling produces higher-turn, more realistic dialogues than unified task-solving simulation.
- **Mechanism:** A dedicated user simulator governed by behavioral rules (e.g., asking one subtask per turn) prevents the "efficiency trap" where optimal task solvers complete objectives in minimal turns without realistic back-and-forth.
- **Core assumption:** Human-agent collaboration is inherently incremental and iterative, not optimally efficient.
- **Evidence anchors:**
  - [abstract] "By decoupling task generation from a dedicated user simulator that mimics human behavioral rules—such as incremental request-making and turn-by-turn feedback—we facilitate more authentic, extended multi-turn dialogues"
  - [Section 4] "the simulator identifies the required sub-tasks but deliberately issues requests in a piecemeal fashion, typically asking for only one or two subtasks per turn"
  - [corpus] Related work on stateful tool use (Rethinking Stateful Tool Use) identifies similar gaps but focuses on benchmark design rather than data generation
- **Break condition:** If the user simulator's behavioral rules do not accurately reflect target user populations, generated dialogues may be realistic but misaligned with deployment needs.

### Mechanism 2: Execution-Grounded Tool Outputs Improve Fidelity
- **Claim:** Grounding tool calls in executable environments (e.g., SQL databases) yields more reliable training signals than simulated tool outputs.
- **Mechanism:** Actual execution provides verifiable, factually accurate tool outputs that persist state changes across turns, reducing hallucination risk and enabling stateful consistency.
- **Core assumption:** Synthetic tool output simulators introduce systematic errors that propagate through training.
- **Evidence anchors:**
  - [Section 4] "the simulator conditions its behavior on tool outputs that are produced through actual execution... returned verbatim to the dialogue"
  - [Table 3] User-oriented + Tool Execution outperforms User-oriented alone on τ2 Telecom (35.1 vs 30.7 for 4B model; 40.4 vs 34.2 for 30B model)
  - [corpus] Weak direct evidence; related work (AURA, OrchDAG) emphasizes integration but not necessarily execution-grounding for training data
- **Break condition:** Execution environments must accurately reflect deployment schemas; database-view mismatches degrade quality.

### Mechanism 3: High-Density Trajectories Improve Long-Horizon Coherence
- **Claim:** Multiple task completions within single conversation threads improve multi-turn state tracking and intent maintenance.
- **Mechanism:** Models trained on sessions containing interdependent tasks (query, update, summarize) learn to maintain context across goal shifts rather than treating each turn independently.
- **Core assumption:** Real-world human-agent sessions are multi-objective; training on single-task trajectories under-specifies stateful reasoning.
- **Evidence anchors:**
  - [abstract] "high-density trajectories where multiple tasks are completed within a single session"
  - [Table 1] User-oriented data averages 2.48 tasks per sample vs. 1.63 for task-oriented
  - [Section 6.2] Pass@k consistency analysis shows models "sustain correct tool-use behavior across multiple trials"
  - [corpus] AgentChangeBench explicitly evaluates goal-shift robustness, suggesting community recognition of this problem
- **Break condition:** Task interdependencies must be coherent; random task concatenation may introduce noise.

## Foundational Learning

- **Concept: Stateful vs. Stateless Tool Interactions**
  - **Why needed here:** The paper's core contribution targets stateful domains (τ2 Telecom, SQL operations) where tool outputs modify persistent state that must be tracked across turns.
  - **Quick check question:** Can you explain why a "read then update" sequence requires different model capabilities than two independent read operations?

- **Concept: Synthetic Data Generation Pipelines**
  - **Why needed here:** Understanding how the pipeline generates tools, tasks, and dialogues is prerequisite to modifying or extending it.
  - **Quick check question:** What is the role of the validation module, and what happens if it accepts low-quality trajectories?

- **Concept: Behavioral Rules for User Simulation**
  - **Why needed here:** The user simulator's rules (incremental requests, turn-by-turn feedback) determine dialogue realism; understanding them enables customization.
  - **Quick check question:** If you wanted to simulate an expert user who provides all requirements upfront, which rules would you modify?

## Architecture Onboarding

**Component map:**
Seed Tools → Tool Preparation → Tool Preprocessing → Conversation Generation
    ↓              ↓                    ↓                     ↓
Questions   Tool Expansion      JSON Schema         Task Generation
                ↓               Prediction                ↓
           Domain-Specific                         User Simulator
           Toolset                                     ↓
                                                   Response Gen
                                                        ↓
                                                   Validation → Dataset

**Critical path:** Tool Preparation → User Simulator → Validation. The user simulator is the key differentiator; if behavioral rules misalign, downstream training data degrades.

**Design tradeoffs:**
- **Latency vs. Realism:** User-oriented pipeline has 4.11s latency vs. 0.64s for task-oriented (Table 4) due to multi-turn simulation complexity
- **Scalability vs. Fidelity:** Execution-grounding improves quality but requires database schema alignment and state consistency management
- **Diversity vs. Coherence:** High-density trajectories increase task count but require careful task-interdependency design

**Failure signatures:**
- Low average turns (<10) suggest user simulator is not enforcing incremental behavior
- Validation rejection rate >50% indicates task-tool misalignment or schema conflicts
- τ2 Telecom underperformance suggests insufficient stateful training signal

**First 3 experiments:**
1. **Behavioral rule ablation:** Remove incremental request constraint; measure turn count change and benchmark impact
2. **Execution-grounding toggle:** Compare simulated tool outputs vs. SQL execution on τ2 Telecom domain
3. **Task density scaling:** Vary tasks-per-trajectory (1, 2, 4, 8) and measure Pass@k consistency and BFCL multi-turn scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the SQL-based executable pipeline enhance cross-domain generalization in agents compared to purely simulated environments?
- Basis in paper: [explicit] The conclusion states that while the SQL-based pipeline is promising, "its impact on cross-domain generalization remains an open question."
- Why unresolved: The paper evaluates performance on specific benchmarks (BFCL and $\tau^2$) but does not conduct experiments to measure how well the resulting agents transfer reasoning or tool-use skills to entirely unseen domains.
- What evidence would resolve it: A comparative study measuring zero-shot performance on out-of-domain tasks for models trained on SQL-grounded data versus simulated data.

### Open Question 2
- Question: Can robust state-recovery mechanisms be developed to mitigate error propagation when agents encounter partial or ambiguous database views?
- Basis in paper: [inferred] The Limitations section notes the model exhibits "brittleness when presented with partial or ambiguous database views" and that discrepancies can lead to "error propagation" in long-horizon interactions.
- Why unresolved: The current simulation loop lacks sophisticated error-handling strategies to maintain factual accuracy when the initial context is incomplete.
- What evidence would resolve it: Demonstrating improved Pass@k scores and trajectory coherence in tasks intentionally designed with obscured or partial database schemas.

### Open Question 3
- Question: Is it computationally feasible to scale the user-oriented pipeline to millions of trajectories without sacrificing the interaction realism?
- Basis in paper: [explicit] The Limitations section highlights that scaling to millions of trajectories poses a "practical challenge" due to "higher cumulative token consumption and extended processing time" compared to single-shot methods.
- Why unresolved: The paper identifies a trade-off between the high cost of multi-turn interaction and the efficiency of single-shot data synthesis, leaving the solution for large-scale adoption unresolved.
- What evidence would resolve it: Presenting an optimized pipeline architecture that reduces latency and token usage while maintaining the high-density trajectory characteristics.

## Limitations
- The incremental request mechanism is a strong assumption about human behavior that may not generalize across user populations or domains.
- Execution-grounding introduces dependency on execution environment fidelity; schema mismatches or permission errors can degrade dataset quality.
- High-density trajectories require meaningful task interdependence; arbitrary task concatenation may introduce noise rather than improve learning signals.

## Confidence

**High Confidence Claims:**
- The user-oriented pipeline generates longer dialogues (avg 2.48 tasks vs 1.63 for task-oriented) with demonstrable performance improvements on BFCL and τ2 benchmarks
- Execution-grounding improves tool output reliability compared to pure simulation
- The three-stage pipeline architecture is implementable and produces usable training data

**Medium Confidence Claims:**
- The behavioral rules accurately simulate human-agent interaction patterns
- High-density trajectories provide superior learning signals for stateful reasoning
- Performance improvements on benchmarks translate to real-world deployment robustness

**Low Confidence Claims:**
- The incremental request mechanism is the optimal way to simulate human behavior
- Generated dialogues will generalize across diverse user populations and domains
- The trade-off between simulation latency and realism is appropriately calibrated

## Next Checks

1. **Behavioral Rule Validation:** Conduct ablation studies removing the incremental request constraint to measure the magnitude of efficiency bias and benchmark impact. Compare generated dialogues against real human-agent interaction datasets to assess behavioral fidelity.

2. **Execution Environment Robustness:** Systematically introduce controlled schema mismatches and permission errors into the SQL execution environment. Measure impact on dialogue generation quality, training data rejection rates, and downstream model performance to quantify the execution-grounding dependency.

3. **Task Interdependence Analysis:** Implement a task dependency scoring system that evaluates semantic coherence between consecutive tasks in high-density trajectories. Compare model performance on coherently interdependent task sequences versus randomly concatenated tasks to isolate the effect of meaningful task complexity.