---
ver: rpa2
title: 'Overview of the Sensemaking Task at the ELOQUENT 2025 Lab: LLMs as Teachers,
  Students and Evaluators'
arxiv_id: '2507.12143'
source_url: https://arxiv.org/abs/2507.12143
tags:
- questions
- systems
- evaluation
- answers
- material
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Sensemaking shared task at ELOQUENT 2025 evaluated how well
  generative language models can create questions, answer them, and evaluate responses
  based on input materials. Four teams participated, using 7 sources of test materials
  spanning English, German, Ukrainian, and Czech languages.
---

# Overview of the Sensemaking Task at the ELOQUENT 2025 Lab: LLMs as Teachers, Students and Evaluators

## Quick Facts
- arXiv ID: 2507.12143
- Source URL: https://arxiv.org/abs/2507.12143
- Reference count: 38
- Primary result: LLM-as-a-Judge systems show significant unreliability, conflating semantic similarity with correctness

## Executive Summary
The Sensemaking shared task at ELOQUENT 2025 evaluated generative language models across three roles: creating questions from materials (Teachers), answering questions using only provided context (Students), and evaluating responses (Evaluators). Four teams participated using 7 multilingual material sources spanning English, German, Ukrainian, and Czech. While Teacher systems performed reasonably at question generation and Student systems achieved moderate success answering questions, Evaluator systems showed critical unreliability, often rating garbled or mismatched question-answer pairs as acceptable. Adversarial testing revealed that LLM-as-a-Judge approaches struggle with semantic similarity conflation and shallow text-level relationships, highlighting the need for more robust automatic evaluation methods for text understanding tasks.

## Method Summary
The task evaluated LLMs across three tracks using 7 material sources in 4 languages, all translated to English. Teacher systems generated questions and reference answers from material sections. Student systems answered questions using only provided materials. Evaluator systems scored answers on a 0-100 scale. The evaluation used automatic metrics (sBERT embeddings for Teachers, ROUGE-L Recall for Students) plus LLM-based ranking. Adversarial tests included word shuffling and question/answer swapping to test Evaluator robustness. GPT-4.1-nano served as baseline for all tracks with structured output prompts.

## Key Results
- Teacher systems performed reasonably well at question generation, with expert-made questions scoring among the best
- Student systems achieved moderate success with ROUGE-L Recall scores averaging 40-57% on expert-made questions
- Evaluator systems showed significant unreliability, often rating garbled or mismatched question-answer pairs as acceptable
- Adversarial tests revealed LLM-as-Judge systems struggle with semantic similarity conflation and shallow text relationships

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate reasonable-quality questions from source text via extraction and reformulation, even with limited semantic processing
- Mechanism: Teacher systems extract text spans and prompt the LLM to reformulate them into questions, leveraging the LLM's strong pattern-matching capabilities
- Core assumption: The LLM's ability to generate questions answerable specifically from the provided text indicates some level of comprehension
- Evidence anchors: Teacher systems performed reasonably well at question generation, with expert-made questions scoring among the best; DeepSeek plus LLaMA system rated close to baselines

### Mechanism 2
- Claim: LLM-as-a-Judge unreliability stems from conflation of semantic similarity with correctness
- Mechanism: Evaluator LLM calculates scores based on semantic relationship but overweights surface-level similarity (shared vocabulary, topic overlap) and underweights logical validity
- Core assumption: LLM's internal representation of "correctness" is heavily biased toward embedding similarity or token overlap metrics
- Evidence anchors: Adversarial tests showed evaluators rated garbled text much higher than expected (e.g., Gemma ratings dropped from ~83 to ~59, not near 0)

### Mechanism 3
- Claim: Student performance is constrained by difficulty forcing LLMs to rely exclusively on provided context
- Mechanism: Student system prompts LLM with material and question, but model must inhibit parametric knowledge to prioritize provided context
- Core assumption: Instruction "answer based on input materials" is insufficient to fully suppress retrieval of general knowledge
- Evidence anchors: Moderate ROUGE-L scores (40-57%) and negative correlation (-0.31) between baseline and participant scores suggest different approaches succeed/fail on different question types

## Foundational Learning

- Concept: ROUGE-L Recall
  - Why needed here: Primary automatic metric used to evaluate Student systems' answers against reference answers
  - Quick check question: What does a ROUGE-L Recall score of 0.57 mean for a student answer? (It means the longest common subsequence of tokens between the generated answer and the reference covers 57% of the reference answer's tokens.)

- Concept: LLM-as-a-Judge
  - Why needed here: Core paradigm tested in the Evaluator track, using an LLM to assign quality scores to text
  - Quick check question: According to the paper, what is the primary failure mode of the LLM-as-a-Judge approach in this task? (It tends to conflate semantic similarity with correctness, giving high scores to topically related but incorrect or mismatched answers.)

- Concept: Adversarial Testing
  - Why needed here: Key method used to expose fragility of Evaluator systems by creating deliberately flawed inputs
  - Quick check question: How did adversarial tests reveal the weaknesses in the Evaluator systems? (By showing that evaluators assigned surprisingly high scores to nonsensical or mismatched question-answer pairs, rather than correctly identifying them as failures.)

## Architecture Onboarding

- Component map:
  Data Ingestion & Preprocessing -> Teacher System -> Student System -> Evaluator System -> Evaluation Pipeline
  (Preprocess 7 sources in 4 languages -> Generate questions -> Generate answers -> Score answers -> Compute metrics)

- Critical path:
  1. Pre-process material -> Provide to Teacher -> Generate Questions
  2. Provide Material + Questions to Student -> Generate Answers
  3. Provide Material + Question + Answer to Evaluator -> Generate Score
  4. Run Adversarial Tests (e.g., shuffle words, swap questions) on Evaluator to validate robustness

- Design tradeoffs:
  - Teacher: Simple automatic metrics (sBERT-based) are low-variance but potentially less accurate; LLM-based evaluation is more nuanced but may suffer from model bias
  - Student: ROUGE-L Recall is standard but imperfect for abstractive answers; reference answers provided by Teachers were often low quality
  - Prompt Engineering: Using constrained scale (0-5) for Evaluator baselines helped anchor outputs compared to 0-100 scale used by participants

- Failure signatures:
  - Teacher: Generates questions unrelated to material section or provides low-quality reference answers
  - Student: Answers based on general knowledge rather than text, or has formatting issues that break automated parsing
  - Evaluator: Assigns high scores to garbled text or mismatched question-answer pairs, indicating failure to assess logical consistency

- First 3 experiments:
  1. Implement and Test the Adversarial Evaluator Suite: Build pipeline to generate adversarial inputs and run chosen LLM-as-a-Judge model against them; target near-zero scores on corrupted inputs
  2. Compare Student Evaluation Metrics: For fixed materials/questions, generate answers and compare scores from ROUGE-L Recall, LLM-as-a-Judge, and manual review; investigate discrepancies
  3. Analyze Teacher Question Quality: Generate questions using baseline model; manually review sample for answerability and relevance; compare with automatic coverage/diversity scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can better calibrated automatic metrics be developed to reliably discern the quality of generated question sets?
- Basis in paper: Section 4.3 states that "better evaluation strategies will still have to be devised because it is difficult to discern the quality of the various candidate question sets"
- Why unresolved: Current simple automatic metrics showed high variance and failed to effectively distinguish between quality of different systems' outputs
- What evidence would resolve it: A new metric demonstrating high correlation with human rankings across diverse material kinds

### Open Question 2
- Question: How can Student systems be improved to rely on deep semantic understanding rather than shallow vocabulary overlap?
- Basis in paper: Section 5.4 notes that "models are using the similarity in terms and names and other shallow expressions instead of understanding the texts"
- Why unresolved: Systems performed significantly better on questions with high lexical overlap, failing when answers required reasoning over obfuscated text
- What evidence would resolve it: High accuracy on adversarial datasets where correct answers lack explicit word overlap with source text

### Open Question 3
- Question: To what extent does using relative ranking or constrained rating scales improve reliability of LLM-as-a-Judge systems?
- Basis in paper: Section 7 notes that "we have not yet conducted any experiments to verify these preliminary observations [regarding relative judgements and rating ranges]"
- Why unresolved: Current Evaluator systems showed high unreliability, often rating garbled or mismatched text as acceptable
- What evidence would resolve it: Comparative experiments showing statistically significant reductions in error rates on adversarial inputs using these strategies

## Limitations
- ROUGE-L Recall is known to be imperfect for abstractive answers and was compromised by low-quality reference answers from Teacher systems
- Automatic evaluation metrics for Teacher systems (sBERT embeddings) showed only weak correlation with LLM-based rankings, indicating fundamental measurement instability
- Copyright restrictions on source materials prevent full reproducibility, and several critical implementation details were not specified

## Confidence
- High confidence: The core finding that LLM-as-a-Judge systems conflate semantic similarity with correctness and fail adversarial tests
- Medium confidence: The observation that Student systems achieve only moderate performance (40-57% ROUGE-L) due to difficulty constraining LLMs to use only provided context
- Low confidence: The specific mechanisms by which Teacher systems generate questions (whether through shallow pattern-matching vs. semantic processing)

## Next Checks
1. Implement adversarial evaluation suite: Create and test exact adversarial inputs (word shuffling, question/answer swapping) against your chosen LLM-as-a-Judge model to verify if it achieves near-zero scores on corrupted inputs
2. Cross-metric Student evaluation comparison: Generate student answers and compare scores across ROUGE-L Recall, LLM-as-a-Judge scoring, and manual human evaluation on the same dataset
3. Teacher question quality audit: Manually evaluate a random sample of questions generated by baseline models for answerability from source text and relevance to key concepts, comparing results with automatic coverage and diversity metrics