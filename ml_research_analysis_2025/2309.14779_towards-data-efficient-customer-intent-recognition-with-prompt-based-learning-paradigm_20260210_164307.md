---
ver: rpa2
title: Towards Data-efficient Customer Intent Recognition with Prompt-based Learning
  Paradigm
arxiv_id: '2309.14779'
source_url: https://arxiv.org/abs/2309.14779
tags:
- data
- learning
- training
- language
- prompt-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates prompt-based learning for customer intent
  recognition in conversational data, addressing the challenge of limited labeled
  data in customer service domains. The authors propose a pipeline combining prompted
  training, answer mapping, active sampling, and ensemble learning to enhance intent
  recognition performance with small language models.
---

# Towards Data-efficient Customer Intent Recognition with Prompt-based Learning Paradigm

## Quick Facts
- arXiv ID: 2309.14779
- Source URL: https://arxiv.org/abs/2309.14779
- Authors: Hengyu Luo; Peng Liu; Stefan Esping
- Reference count: 6
- Primary result: Prompt-based learning achieves 59% accuracy on customer intent recognition using only 5% of labeled data

## Executive Summary
This paper investigates prompt-based learning for customer intent recognition in conversational data, addressing the challenge of limited labeled data in customer service domains. The authors propose a pipeline combining prompted training, answer mapping, active sampling, and ensemble learning to enhance intent recognition performance with small language models. They demonstrate that this approach achieves competitive results compared to traditional fine-tuning while using only 5% of labeled data. Additionally, zero-shot experiments show that well-crafted prompts enable small models to perform effectively without training data.

## Method Summary
The paper presents a prompt-based learning pipeline for customer intent recognition using small language models. The approach reformulates intent classification as a masked language modeling task by appending prompt templates to conversational text and mapping predicted tokens to intent labels via verbalizers. The pipeline includes stratified 5% data sampling, active sampling based on BERT embedding centroids, and ensemble learning across multiple prompt-verbalizer combinations. The method is evaluated on a proprietary customer service dataset with 14 intent classes, comparing performance against traditional fine-tuning approaches.

## Key Results
- Prompt-based learning with 5% labeled data achieves 59% accuracy vs 33% for traditional fine-tuning
- Active sampling improves accuracy by 3-7% over random sampling with the same data budget
- Ensemble learning across 4 templates and 4 verbalizers achieves 68.70% accuracy
- Zero-shot performance with detailed prompts reaches 31.35% accuracy on small models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt-based learning reduces the objective gap between pre-training and downstream tasks, enabling data-efficient adaptation.
- Mechanism: Traditional fine-tuning introduces new parameters (e.g., classification heads) and changes the training objective, requiring substantial labeled data to avoid overfitting. Prompt-based learning reformulates downstream tasks into natural language formats that align with pre-training objectives (masked language modeling or seq2seq generation), allowing the model to leverage pre-existing knowledge without architectural changes.
- Core assumption: The pre-trained model has captured relevant domain knowledge that can be accessed through appropriate prompting; the verbalizer mapping from predicted tokens to labels preserves semantic meaning.
- Evidence anchors:
  - [abstract] "Utilizing prompted training combined with answer mapping techniques, this approach allows small language models to achieve competitive intent recognition performance with only a minimal amount of training data."
  - [section 5.1.1] "According to Liu et al. (2023), this is because there's a significant 'gap' between training objectives... of pre-training and the downstream tasks... prompt-based learning seeks to bridge this gap."
  - [corpus] Limited direct corpus validation; neighbor papers focus on broader customer service NLU without testing prompt-based vs. fine-tuning efficiency claims.
- Break condition: If downstream task labels cannot be meaningfully mapped to tokens the model has learned during pre-training, or if the task requires reasoning beyond surface-level patterns captured in pre-training data.

### Mechanism 2
- Claim: Active sampling by selecting centroid-proximal samples improves data efficiency compared to random sampling.
- Mechanism: BERT embeddings represent samples in a 512-dimensional vector space. Samples near the centroid of each class cluster are assumed to be more representative of that class's core features. Training on these representative samples may provide more consistent gradient signals and reduce noise from outliers.
- Core assumption: Centroid proximity in embedding space correlates with label-predictive quality; BERT embeddings capture task-relevant semantic structure.
- Evidence anchors:
  - [abstract] "We enhance the performance by integrating active sampling and ensemble learning strategies in the prompted training pipeline."
  - [section 5.1.2] "active sampling notably boosts accuracy and macro F1-score outperforming random sampling (7% performance gain with 3% data sampled, and 3% performance gain with 5% data sampled)."
  - [corpus] No direct corpus validation; related papers do not test centroid-based active sampling for intent classification.
- Break condition: If embedding space structure does not align with label semantics (e.g., multi-label or fine-grained intents with overlapping embeddings), or if minority classes have sparse, irregular clusters.

### Mechanism 3
- Claim: Ensembling multiple prompts and verbalizers improves robustness by aggregating complementary predictions.
- Mechanism: Different prompt templates and verbalizers may capture different aspects of the task. Some templates may better align with certain input patterns. By summing softmaxed logits across multiple model configurations before prediction, the ensemble reduces variance and mitigates individual prompt weaknesses.
- Core assumption: Individual prompt/verbalizer configurations make uncorrelated errors; aggregation improves signal-to-noise ratio.
- Evidence anchors:
  - [abstract] "We enhance the performance by integrating active sampling and ensemble learning strategies in the prompted training pipeline."
  - [section 5.1.3] "by aggregating templates and/or verbalizers, the performance of the pipeline improves greatly and surpasses that of any individual model alone... 68.70% accuracy vs. ~62% for best single template-verbalizer pair."
  - [corpus] No corpus papers directly test prompt/verbalizer ensembling; ensemble learning for NLU is mentioned tangentially but not validated for this mechanism.
- Break condition: If prompts are highly correlated in their error patterns, or if computational budget prevents running multiple model configurations at inference time.

## Foundational Learning

- Concept: Pre-training objectives (masked language modeling, seq2seq)
  - Why needed here: Understanding why prompt-based learning works requires grasping how models like T5 or BERT are trained, and how prompts align downstream tasks with those objectives.
  - Quick check question: Can you explain why predicting a masked token is different from adding a classification layer, and how prompt-based learning bridges that gap?

- Concept: Verbalizer design (token-to-label mapping)
  - Why needed here: The verbalizer determines how model outputs translate to intent labels. Poor verbalizer choices can break the pipeline regardless of prompt quality.
  - Quick check question: Given an intent label "Issue Handling," what tokens might a verbalizer map to this label, and why might multiple tokens be beneficial?

- Concept: Embedding space and centroid-based sampling
  - Why needed here: Active sampling relies on the assumption that BERT embeddings cluster by semantic similarity, and centroids represent class prototypes.
  - Quick check question: How would you verify that your embedding space has meaningful cluster structure before relying on centroid sampling?

## Architecture Onboarding

- Component map: Input conversation -> Prompt template -> Language model (T5/FLAN-T5) -> Verbalizer mapping -> Ensemble aggregation (optional) -> Output intent label

- Critical path:
  1. Design or select prompt templates (short templates for training, detailed template for zero-shot)
  2. Define verbalizer mappings (token sets per label)
  3. Sample training data (random or active sampling)
  4. Run prompted training using OpenPrompt framework (20 epochs, select best checkpoint on dev set)
  5. If ensembling: aggregate logits across configurations
  6. Evaluate on held-out test set (accuracy, macro F1)

- Design tradeoffs:
  - Data efficiency vs. complexity: Prompt-based learning reduces data needs but requires careful prompt/verbalizer engineering; ensemble improves performance but multiplies inference cost.
  - Zero-shot vs. few-shot: Detailed prompts enable zero-shot (no training data) but underperform few-shot prompted training (31.35% vs. 68.70% accuracy); choose based on data availability.
  - Model size vs. deployment: FLAN-T5-large (770M) outperforms smaller models in zero-shot; T5-base (220M) is baseline for prompted training with 5% data.

- Failure signatures:
  - Low accuracy with specific templates/verbalizers: Indicates misalignment between prompt wording and model's pre-training distribution; try alternative templates or ensemble.
  - Active sampling underperforms random: Embedding space may not reflect task structure; verify clustering quality or fall back to random sampling.
  - Zero-shot near-random performance (e.g., T5-large at 3.31%): Prompt lacks task-specific detail; use detailed prompts with label descriptions (Template 5).
  - Macro F1 substantially lower than accuracy: Model biased toward majority classes; consider class-weighted sampling or rebalancing.

- First 3 experiments:
  1. Baseline replication: Use T5-base with 5% random-sampled data, Template 1, Verbalizer 1. Target: ~59% accuracy per paper. Purpose: Validate pipeline setup.
  2. Active sampling test: Replace random sampling with centroid-based active sampling at 5% data. Target: ~3% accuracy improvement. Purpose: Confirm active sampling benefit.
  3. Zero-shot probe: Use FLAN-T5-large with detailed prompt (Template 5), no training data. Target: ~31% accuracy. Purpose: Establish zero-shot baseline before investing in labeling.

## Open Questions the Paper Calls Out

- Question: Can multimodal data integration (e.g., images, voice characteristics) meaningfully improve customer intent recognition performance compared to text-only prompt-based approaches?
  - Basis in paper: [explicit] The authors state in Section 6: "future work could explore the integration of multimodal data inputs to enrich the context and effectiveness of customer intent recognition systems."
  - Why unresolved: The current study evaluates only text-based conversational data; the marginal benefit of multimodal signals in prompt-based learning pipelines remains unquantified.
  - What evidence would resolve it: Comparative experiments on datasets containing both text and auxiliary modalities (product images, audio logs), measuring accuracy/F1 gains from multimodal integration versus text-only baselines.

- Question: How can verbalizer design be systematically optimized or automated to reduce manual engineering while maintaining or improving performance across domains?
  - Basis in paper: [explicit] The authors mention in Section 6: "One potential area involves refining the methodologies (e.g. a more strategic verbalizer design, etc.) used in our prompt-based learning pipeline to enhance the adaptability and accuracy of SLMs across different domains."
  - Why unresolved: The paper manually designed four verbalizers using heuristics (label phrasing, topic modeling, LM-BFF generation), but Table 8 shows substantial performance variance (macro F1 from 43.76% to 58.40%), indicating verbalizer quality is inconsistent and not well-understood.
  - What evidence would resolve it: An automated verbalizer optimization framework tested across multiple customer service domains, demonstrating stable performance without manual tuning.

## Limitations
- Proprietary dataset prevents independent validation and generalizability assessment
- Performance heavily dependent on prompt engineering quality and domain expertise
- Zero-shot accuracy (31.35%) may not be sufficient for production deployment
- Active sampling assumes BERT embeddings meaningfully cluster by intent class

## Confidence
- High confidence (8/10): The core claim that prompt-based learning with 5% labeled data achieves competitive accuracy (59%) compared to traditional fine-tuning is well-supported by the methodology and consistent with established literature on prompt-based learning's data efficiency benefits.
- Medium confidence (6/10): The active sampling improvement claim (3-7% gains over random sampling) is plausible given the mechanism, but requires validation on non-proprietary datasets to confirm the embedding-space assumptions hold across domains.
- Medium confidence (6/10): Zero-shot performance claims (31.35% accuracy) are valid for the specific detailed prompt template used, but the performance ceiling suggests this approach may not replace data collection in production settings where higher accuracy is required.
- Low confidence (4/10): The ensemble learning benefit (68.70% vs 62% single model) is demonstrated but the computational trade-offs are not fully explored. The practical value depends on inference-time constraints not addressed in the paper.

## Next Checks
1. **Dataset generalization test**: Replicate the pipeline on a publicly available conversational intent dataset (e.g., Banking77 or CLINC150) using 5% stratified sampling. Compare prompted training accuracy against traditional fine-tuning with the same data subset to validate the 59% vs 33% performance differential.

2. **Embedding space validation**: Extract BERT embeddings from a sample of the training data and perform t-SNE or UMAP visualization to verify that samples cluster by intent class. Compute silhouette scores to quantify cluster quality before and after active sampling to confirm the centroid selection strategy captures meaningful semantic structure.

3. **Prompt sensitivity analysis**: Systematically vary prompt templates (short vs. detailed) and verbalizer mappings on a held-out subset while keeping the training data constant at 5%. Measure performance variance to quantify how much the accuracy gains depend on prompt engineering quality versus the underlying prompt-based learning mechanism.