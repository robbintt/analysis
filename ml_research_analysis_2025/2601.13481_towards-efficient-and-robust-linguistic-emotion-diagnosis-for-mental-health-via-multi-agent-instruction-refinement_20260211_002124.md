---
ver: rpa2
title: Towards Efficient and Robust Linguistic Emotion Diagnosis for Mental Health
  via Multi-Agent Instruction Refinement
arxiv_id: '2601.13481'
source_url: https://arxiv.org/abs/2601.13481
tags:
- emotion
- apolo
- prompt
- optimization
- diagnosis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: APOLO introduces a POMDP-based multi-agent framework to address
  challenges in linguistic emotion diagnosis, including emotional comorbidity and
  inefficient exploration of clinically relevant cues. By integrating Planner, Teacher,
  Critic, Student, and Target agents, it enables systematic, risk-aware, and cost-efficient
  prompt optimization.
---

# Towards Efficient and Robust Linguistic Emotion Diagnosis for Mental Health via Multi-Agent Instruction Refinement

## Quick Facts
- arXiv ID: 2601.13481
- Source URL: https://arxiv.org/abs/2601.13481
- Reference count: 40
- APOLO achieves state-of-the-art performance across six benchmarks, with average improvements of 2.96% in Macro F1 and 3.40% in Micro F1 over the strongest baseline.

## Executive Summary
APOLO introduces a POMDP-based multi-agent framework to address challenges in linguistic emotion diagnosis, including emotional comorbidity and inefficient exploration of clinically relevant cues. By integrating Planner, Teacher, Critic, Student, and Target agents, it enables systematic, risk-aware, and cost-efficient prompt optimization. Experiments show APOLO achieves state-of-the-art performance across six benchmarks, with average improvements of 2.96% in Macro F1 and 3.40% in Micro F1 over the strongest baseline. It also delivers faster convergence and greater computational efficiency, particularly in multi-label emotion diagnosis.

## Method Summary
APOLO formulates instruction refinement as a Partially Observable Markov Decision Process, where a Planner agent generates risk/cost-aware trajectories, a Socratic triad (Teacher-Critic-Student) iteratively refines prompts, and a Target agent evaluates final prompts with adaptive early stopping. The framework balances semantic plausibility, clinical safety, and computational efficiency through regularized objective functions that incorporate risk and cost penalties.

## Key Results
- APOLO achieves average improvements of 2.96% in Macro F1 and 3.40% in Micro F1 over the strongest baseline (OPRO).
- Ablation studies show the Socratic module's contribution is most significant, with average performance drops of 7.19% Macro F1 and 8.69% Micro F1 when removed.
- APOLO demonstrates consistent rapid early improvement followed by smooth stabilization, with average iteration counts of 6.8 on DailyDialog and 7.3 on DepressionEmo.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Risk-aware and cost-constrained trajectory planning guides prompt optimization toward clinically safe and computationally efficient regions of the search space.
- Mechanism: The Planner agent generates candidate trajectories by maximizing a regularized objective combining semantic likelihood, diagnostic risk penalty (R_emo + R_safety), and computational cost penalty (trajectory length, model calls, latency). This restricts exploration away from unsafe reasoning paths and unnecessarily expensive refinements.
- Core assumption: Risk and cost can be meaningfully decomposed into additive penalty terms that correlate with downstream diagnostic utility.
- Evidence anchors:
  - [abstract] "APOLO formulates instruction refinement as a Partially Observable Markov Decision Process... enables systematic, risk-aware, and cost-efficient prompt optimization."
  - [section 3.2] "By combining the likelihood term with the risk and cost penalties defined in Eqs. (3)–(4), the planner optimizes a regularized objective that balances semantic plausibility, clinical safety, and computational efficiency."
- Break condition: If risk penalties are poorly calibrated, the planner may generate trajectories that appear safe but miss high-stakes emotional signals.

### Mechanism 2
- Claim: Socratic-style Teacher–Critic–Student dialogue provides structured pseudo-gradient feedback that progressively improves prompt quality for multi-label emotion diagnosis.
- Mechanism: At each refinement step, the Teacher proposes emotion-aware probing queries based on sub-goals; the Critic evaluates proposals along clarity, diagnostic relevance, and safety; the Student incorporates feedback to update the working prompt. Historical interaction sequences maintain coherence across steps.
- Core assumption: Iterative question-answer refinement yields prompts that capture comorbid emotional states better than single-pass generation.
- Evidence anchors:
  - [abstract] "Teacher–Critic–Student agents iteratively refine prompts to enhance reasoning stability and effectiveness."
  - [section 5.1] "Removing [the Socratic module] leads to the most significant performance degradation. On average, Macro F1 decreases by 7.19%, Micro F1 by 8.69%."
- Break condition: If Critic feedback is noisy or inconsistent, pseudo-gradient updates may introduce oscillations rather than stable improvement.

### Mechanism 3
- Claim: Closed-loop evaluation with adaptive early stopping prevents over-refinement while ensuring convergence to high-performing prompts.
- Mechanism: The Target agent evaluates the final prompt on a held-out test set after each outer iteration. Optimization halts when marginal reward gain ΔR^{(t)} ≤ δ or maximum iterations I is reached. This balances performance gains against computational cost.
- Core assumption: Validation-set performance tracks true diagnostic utility and does not overfit to benchmark-specific artifacts.
- Evidence anchors:
  - [section 3.4] "APOLO employs an adaptive early-stopping rule based on marginal reward gain... The optimization halts when ΔR^{(t)} ≤ δ or t ≥ I."
  - [section 5.2] "APOLO consistently exhibits a clear pattern of rapid early improvement followed by smooth stabilization."
- Break condition: If δ is set too high, optimization may terminate before reaching effective prompts; if too low, unnecessary iterations increase cost without meaningful gains.

## Foundational Learning

- Concept: **Partially Observable Markov Decision Process (POMDP)**
  - Why needed here: APOLO models prompt optimization as a POMDP where latent diagnostic cues and uncertainty evolve through state transitions, with prompts as observations.
  - Quick check question: Can you explain why a POMDP (vs. a standard MDP) is appropriate when the system cannot directly observe a patient's true emotional state?

- Concept: **Multi-label Classification with Label Co-occurrence**
  - Why needed here: Emotional comorbidity means single utterances can express multiple intertwined states; standard single-label formulations fail.
  - Quick check question: How does Macro F1 differ from Micro F1 in multi-label settings, and why does the paper emphasize Macro F1 improvements?

- Concept: **Prompt Optimization as Policy Search**
  - Why needed here: APOLO treats prompt refinement as learning a joint policy over Teacher–Critic–Student actions, rather than discrete token-level search.
  - Quick check question: What is the advantage of framing prompt optimization as policy improvement rather than pure gradient-based search?

## Architecture Onboarding

- Component map:
  - Planner -> Generates trajectory ST = [st_1, ..., st_n] given goal g, input x, initial prompt p_0
  - Teacher -> Proposes probing queries q_i based on sub-goals and historical context
  - Critic -> Evaluates queries along clarity, relevance, safety; returns binary judgment with feedback
  - Student -> Updates prompts p_i based on (q_i, c_i) pairs
  - Target -> Evaluates final prompt on test set, provides reward signal R^{(t)}

- Critical path:
  1. Planner generates trajectory ST using Eq. (5) objective
  2. For each sub-goal st_i, Teacher–Critic–Student executes Socratic refinement loop
  3. Target evaluates final prompt p_ℓ, computes reward
  4. Outer loop continues until early-stopping criterion satisfied

- Design tradeoffs:
  - Risk coefficient γ_r: Higher values enforce conservative exploration but may miss subtle emotional cues; lower values risk unsafe outputs
  - Cost coefficient γ_c: Higher values produce shorter trajectories with fewer model calls but potentially less thorough refinement
  - Early-stopping threshold δ: Controls convergence speed vs. final performance

- Failure signatures:
  - Sudden drop in Macro F1 with stable Micro F1 → likely overfitting to frequent emotion classes
  - High variance across runs with same initial prompt → Critic feedback instability
  - Convergence without improvement → Planner generating low-utility trajectories

- First 3 experiments:
  1. Reproduce Table 1 results on DailyDialog and DepressionEmo with GPT-5-mini backbone; verify reported Macro F1 improvements
  2. Ablate the Planner (w/o Plan) and compare trajectory lengths, token consumption, and final performance to validate risk/cost-aware planning contribution
  3. Test early-stopping sensitivity by varying δ ∈ {0.005, 0.01, 0.02} and plot convergence curves against total token cost

## Open Questions the Paper Calls Out

- Can the APOLO framework be extended to handle multimodal emotion diagnosis where linguistic cues must be integrated with acoustic or physiological signals?
  - Basis: Section 2.1 distinguishes textual methods from multimodal approaches
  - Why unresolved: Current POMDP formulation designed for text only
  - What evidence would resolve: Extension to multimodal dataset like CMU-MOSEI

- How sensitive is the optimization trajectory to the specific risk and cost coefficients (γ_r, γ_c) defined in the Planner's objective function?
  - Basis: Equation 5 introduces trade-off coefficients without sensitivity analysis
  - Why unresolved: Fixed coefficients may not generalize across diverse clinical contexts
  - What evidence would resolve: Ablation study varying γ_r and γ_c to observe changes in False Negative Rate for high-risk emotions

- Can instruction prompts optimized by APOLO on a high-capacity model transfer effectively to smaller, open-source models without re-optimization?
  - Basis: Section 4.1 states all agents powered by same backbone model
  - Why unresolved: Prompts often overfit to specific LLM reasoning patterns
  - What evidence would resolve: Evaluating prompts optimized on GPT-5-mini directly on smaller model (e.g., Llama-3-8B)

## Limitations

- Risk and cost estimation functions are mathematically defined but not concretely specified, introducing ambiguity in reproducing the Planner's decision-making.
- Computational expense is high due to full test set evaluation within each outer iteration, with no discussion of sub-sampling strategies for intermediate checks.
- Results depend on proprietary "GPT-5-mini" model, requiring careful substitution with available alternatives for exact reproduction.

## Confidence

- High Confidence: Core POMDP formulation and multi-agent Socratic refinement loop are well-specified and logically consistent.
- Medium Confidence: Reported 2.96% Macro F1 and 3.40% Micro F1 improvements are plausible but exact reproduction depends on unspecified risk/cost estimators and model substitution.
- Low Confidence: Adaptive early-stopping mechanism's effectiveness is asserted but not empirically validated against alternative criteria.

## Next Checks

1. Ablation of Risk/Cost Estimation: Implement APOLO with and without Planner's risk/cost penalties to compare performance, token usage, and safety compliance.
2. Early-Stopping Sensitivity Analysis: Run APOLO across δ ∈ {0.005, 0.01, 0.02} and plot convergence curves against total token cost to determine optimal trade-off.
3. Multi-Label Robustness Test: Evaluate APOLO on held-out multi-label subset of DepressionEmo with different co-occurrence patterns to assess performance in unseen comorbidity configurations.