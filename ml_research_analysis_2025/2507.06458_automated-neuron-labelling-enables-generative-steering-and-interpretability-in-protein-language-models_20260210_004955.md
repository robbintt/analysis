---
ver: rpa2
title: Automated Neuron Labelling Enables Generative Steering and Interpretability
  in Protein Language Models
arxiv_id: '2507.06458'
source_url: https://arxiv.org/abs/2507.06458
tags:
- protein
- neuron
- features
- neurons
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces the first automated framework for labeling
  every neuron in a protein language model with biologically grounded natural language
  descriptions. Unlike prior approaches relying on sparse autoencoders or manual annotation,
  the method scales to hundreds of thousands of neurons, revealing that individual
  neurons are selectively sensitive to diverse biochemical and structural properties.
---

# Automated Neuron Labelling Enables Generative Steering and Interpretability in Protein Language Models

## Quick Facts
- **arXiv ID:** 2507.06458
- **Source URL:** https://arxiv.org/abs/2507.06458
- **Reference count:** 30
- **Key outcome:** First automated framework to label every neuron in protein language models with biologically grounded natural language descriptions, enabling targeted protein generation.

## Executive Summary
This work introduces the first automated framework for labeling every neuron in a protein language model (PLM) with biologically grounded natural language descriptions. Unlike prior approaches relying on sparse autoencoders or manual annotation, the method scales to hundreds of thousands of neurons, revealing that individual neurons are selectively sensitive to diverse biochemical and structural properties. The authors then develop a novel neuron activation-guided steering method to generate proteins with desired traits, enabling convergence to target biochemical properties like molecular weight and instability index as well as secondary and tertiary structural motifs, including alpha helices and canonical Zinc Fingers. Analysis of labeled neurons across different model sizes reveals PLM scaling laws and a structured neuron space distribution.

## Method Summary
The method consists of two main components: automated neuron labeling and generative steering. For labeling, the framework records neuron activations for 500,000 protein sequences, then uses an LLM (GPT-4.1-nano) to generate candidate natural language descriptions for each neuron based on its top-activating sequences. A simulator model (fine-tuned Longformer) validates these descriptions by predicting neuron activations from the text descriptions, retaining only those with high Pearson correlation. For steering, the method identifies neurons associated with target properties and applies affine transformations to their activations during masked sequence generation, biasing the model toward desired biochemical and structural properties.

## Key Results
- Automated labeling framework successfully labels hundreds of thousands of neurons with biologically meaningful descriptions
- Generative steering achieves monotonic divergence for controlled properties (molecular weight, GRAVY, instability index)
- Structural motifs like alpha helices and zinc fingers can be generated through targeted neuron activation steering
- Neuron space analysis reveals scaling laws across different PLM sizes, with functional features distributed differently in small vs. large models

## Why This Works (Mechanism)

### Mechanism 1: Simulation-Based Hypothesis Verification
The labeling framework filters explainer hallucinations through a predictive simulator. An LLM generates candidate descriptions for a neuron based on top-activating sequences, and a fine-tuned simulator model predicts neuron activation levels using only that description. Descriptions yielding high correlation between predicted and actual activations are retained. This works because neuron behavior is sufficiently described by linearly separable or text-encodable features in the dataset.

### Mechanism 2: Activation Steering via Affine Intervention
The steering method modifies neuron activation magnitudes during inference to bias output probability distributions toward desired properties. During masked sequence generation, hidden states are intercepted and affine transformations (z* = A·z + B) are applied to neurons linked to target properties. This forces downstream layers to compute logits as if the property were present, assuming individual neurons function as linear control levers for complex properties.

### Mechanism 3: Late-Specialization in Smaller Models
Analysis reveals that smaller PLMs (8M params) require deeper layers to distinguish functional features, while larger models (3B params) distribute them hierarchically. In small models, early layers act as generic encoders, pushing functional discrimination to final layers. This distribution pattern enables more efficient steering in larger models where functional features are accessible across multiple layers.

## Foundational Learning

- **Concept: Masked Language Modeling (MLM) Inpainting**
  - **Why needed here:** The generative steering mechanism relies on iteratively masking and resampling tokens (inpainting) rather than autoregressive decoding.
  - **Quick check question:** How does the model fill in a masked token `[MASK]`—does it look at tokens to the left, right, or both?

- **Concept: Pearson Correlation Coefficient**
  - **Why needed here:** This is the primary metric for the "Simulator" to validate if a text description accurately explains a neuron's behavior.
  - **Quick check question:** If a neuron activates strongly for "hydrophobic" sequences, but the Simulator predicts low activation for them, will the correlation score be high or low?

- **Concept: Affine Transformations**
  - **Why needed here:** The steering mechanism applies a linear transformation (z* = Az + B) to neuron activations.
  - **Quick check question:** If you want to *suppress* a feature, should the scaling factor A be greater than 1, or negative?

## Architecture Onboarding

- **Component map:** Dataset -> Record Top-k Activations -> Generate Hypotheses (Explainer) -> Score Hypotheses (Simulator) -> Select Neurons via Semantic Search -> Apply Affine Transform during Inpainting

- **Critical path:** Construct Dataset → Extract Top-k Activations → Generate Hypotheses (Explainer) → Score Hypotheses (Simulator) → Select Neurons via Semantic Search → Apply Affine Transform during Inpainting

- **Design tradeoffs:**
  - **Interpretability vs. Viability:** The method successfully controls properties (low/high weight) and motifs (zinc fingers), but generated proteins often have high ΔG (low folding stability).
  - **Direct Labeling vs. SAE:** This approach avoids training unstable SAEs but depends heavily on LLM Explainer quality and dataset features.

- **Failure signatures:**
  - **High ΔG scores:** Generated sequences match target patterns but are energetically unlikely to fold.
  - **Semantic Entanglement:** Steering for "Alpha" produces "Beta" strands due to polysemantic neurons.
  - **Low Simulator Correlations:** Indicates poor hypothesis quality or features not present in sequence annotations.

- **First 3 experiments:**
  1. **Validation Run:** Take 100 random neurons; verify Simulator's predicted activations correlate >0.5 with ground truth on held-out sequences.
  2. **Binary Steering:** Generate two sets of proteins steering for "High GRAVY" vs. "Low GRAVY"; plot distributions to confirm non-overlapping results.
  3. **Structural Check:** Generate a Zinc Finger protein and calculate ΔG score to verify viability (anticipating failure per Appendix C).

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the generative steering framework be modified to ensure structural stability and foldability of generated protein sequences? The current method optimizes for specific motifs but neglects global thermodynamic stability, resulting in energetically unfavorable sequences. Resolution requires integrating feedback loops (e.g., AlphaFold2) that reward sequences with ΔG scores within the viable 5–15 kCal/mol range.

- **Open Question 2:** Can this framework enable simultaneous control over multiple distinct biological properties in a single protein sequence? Current results focus on single-feature steering, and it's unclear if activating neurons for multiple traits results in coherent proteins or interference. Resolution requires demonstrating a protein converging to target values for two independent properties without degrading either.

- **Open Question 3:** How can the fidelity and accuracy of automated natural language neuron descriptions be rigorously evaluated against human biological knowledge? The method relies on simulator validation but lacks standardized, human-verified ground truth to assess if labels are hallucinated or oversimplified. Resolution requires creating a benchmark dataset where human experts rate the accuracy of generated neuron labels compared to known biological functions.

- **Open Question 4:** Can neurons identified as having low specificity or high redundancy be pruned to create smaller, interpretable PLM variants without loss of performance? While the authors demonstrate neurons encode specific features, they haven't tested if removing "less important" neurons maintains predictive capabilities. Resolution requires a comparative study showing pruned ESM models retain performance on standard benchmarks while reducing parameter count.

## Limitations
- Generated proteins frequently exhibit high folding energy (ΔG), suggesting steered sequences may not be biologically viable despite matching target properties
- The framework depends heavily on LLM quality for hypothesis generation and may produce oversimplified or hallucinated labels
- Affine steering assumes linear controllability of complex protein properties, which may not hold for distributed or non-linear feature representations
- No experimental validation of generated proteins' biological functionality beyond computational predictions

## Confidence

- **High Confidence:** The labeling framework produces consistent neuron descriptions correlated with activation patterns. Steering method shows monotonic divergence for controlled properties like molecular weight and GRAVY.
- **Medium Confidence:** Neuron space analysis reveals meaningful scaling differences between model sizes. Structural motif generation (alpha helices, zinc fingers) is achieved but with questionable folding stability.
- **Low Confidence:** Claims about biological interpretability of individual neuron functions require experimental validation beyond computational predictions.

## Next Checks
1. **Label Verification:** Select 50 labeled neurons and manually verify whether top-activating sequences actually exhibit the described properties beyond computed features used in training.
2. **Steering Ablation:** Compare steered protein generation against random-neuron steering controls to confirm observed property changes result from targeted intervention rather than general optimization pressure.
3. **Viability Assessment:** Generate proteins steered for multiple properties simultaneously and evaluate their predicted folding stability (ΔG) and structural consistency using independent protein structure prediction tools.