---
ver: rpa2
title: 'Balancing Knowledge Updates: Toward Unified Modular Editing in LLMs'
arxiv_id: '2510.27400'
source_url: https://arxiv.org/abs/2510.27400
tags:
- knowledge
- attn
- editing
- uni00000013
- modules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a significant limitation in current knowledge
  editing methods for large language models (LLMs): they predominantly update only
  MLP modules, leaving residual outdated knowledge in attention (Attn) modules. The
  authors conduct comprehensive causal tracing experiments on Qwen2.5-7B and find
  that Attn modules play a substantial role in factual knowledge storage, especially
  in earlier layers.'
---

# Balancing Knowledge Updates: Toward Unified Modular Editing in LLMs

## Quick Facts
- **arXiv ID**: 2510.27400
- **Source URL**: https://arxiv.org/abs/2510.27400
- **Reference count**: 5
- **Primary result**: IntAttn-Edit achieves higher edit success, improved generalization, and stronger knowledge preservation by jointly updating MLP and Attn modules with proportional update magnitudes

## Executive Summary
This paper addresses a critical limitation in current knowledge editing methods for LLMs: they predominantly update only MLP modules, leaving residual outdated knowledge in attention modules. Through comprehensive causal tracing experiments on Qwen2.5-7B, the authors demonstrate that attention modules play a substantial role in factual knowledge storage, especially in earlier layers. Based on these findings, they propose IntAttn-Edit, a novel method that extends the associative memory paradigm to jointly update both MLP and Attn modules. The method employs a knowledge balancing strategy that proportionally allocates update magnitudes based on each module's measured contribution to knowledge storage, consistently outperforming existing methods across ZsRE and WikiData counterfact benchmarks.

## Method Summary
IntAttn-Edit extends linear associative memory from MLP to attention modules, reformulating knowledge storage as key-value associations in both pathways. The method uses causal tracing to identify which layers store target knowledge, then computes a balance factor α that determines the proportional allocation of updates between MLP and Attn modules. The update magnitude for each module type is scaled by (1-α) and α respectively, where α is derived from the ratio of causal contributions. Closed-form least-squares solutions update both W_out (MLP) and W_o (Attn) while preserving existing associations through a sampled preserve set of 100K Wikipedia triplets. The method is evaluated on Qwen2.5-7B and Mistral-7B using Edit Success, Portability, Locality, and Fluency metrics across batch sizes of 100, 300, and 500.

## Key Results
- IntAttn-Edit consistently outperforms existing methods across ZsRE and WikiData counterfact benchmarks
- Achieves higher edit success rates by addressing knowledge residuals in attention modules
- Improves generalization and knowledge preservation through proportional update magnitudes
- Knowledge balancing strategy maintains editing performance within optimal range across different settings

## Why This Works (Mechanism)

### Mechanism 1: Dual-Pathway Knowledge Storage in Attn and MLP
- Claim: Attention modules store factual knowledge alongside MLP modules, particularly in early layers
- Mechanism: Causal tracing with activation patching measures the restoration of correct factual output when clean activations are reintroduced to corrupted runs. Attn modules in layers 1-5 show peak causal influence before MLP contributions dominate in middle layers
- Core assumption: The causal effect measured by logit difference restoration accurately reflects a module's role in knowledge storage rather than merely contextual aggregation
- Evidence anchors: [abstract] "Attn modules play a substantial role in factual knowledge storage and retrieval, especially in earlier layers"; [Section 4.1, Figure 2] "The contribution of Attn to factual recall peaks within the first five layers and then quickly diminishes"
- Break condition: If causal tracing on different model architectures shows Attn contributions negligible across layers, dual-pathway assumption fails

### Mechanism 2: Proportional Update Magnitude via Balance Factor α
- Claim: Allocating update magnitudes proportionally to measured causal contributions reduces knowledge residuals and improves editing efficacy
- Mechanism: Balance factor α = Σ(l∈Attn) LD(l) / [Σ(l∈MLP) LD(l) + Σ(l∈Attn) LD(l)] determines proportional allocation. Updates applied as: Ŵ_mlp = W_mlp + (1-α)Δ_mlp, Ŵ_attn = W_attn + αΔ_attn
- Core assumption: Measured causal contributions at edit time generalize to the specific knowledge being updated; the proportional relationship holds across different factual domains
- Evidence anchors: [abstract] "A knowledge balancing strategy that proportionally allocates update magnitudes based on each module's measured contribution"; [Section 4.2, Eq. 12-13] Formal definition of α and scaling equations
- Break condition: If α values derived from causal tracing produce worse performance than uniform allocation or single-module editing, the proportionality assumption is flawed

### Mechanism 3: Associative Memory Extension to Attention Output Projections
- Claim: Attention output projection matrices (W_o^l) function as linear associative memories analogous to MLP output weights
- Mechanism: Attn module reformulated as key-value store where keys = ATTN(γ(h^{l-1})) and values = W_o · k. Knowledge updates computed via closed-form least-squares solution matching Eq. (6) for MLPs
- Core assumption: The linear associative memory interpretation valid for MLP extends to Attn output projections without architectural modification
- Evidence anchors: [Section 4.1, Eq. 9] Explicit formulation of Attn as associative memory with key-value structure; [Section 4.2] "Deriving Attn Knowledge K-V Pairs" constructs keys and values analogously to MLP
- Break condition: If closed-form updates on W_o degrade attention patterns or cause attention head collapse, the associative memory assumption for Attn is invalid

## Foundational Learning

- Concept: **Causal Tracing / Activation Patching**
  - Why needed here: Core diagnostic tool for identifying which modules store target knowledge. Without understanding causal tracing, cannot determine α or select critical layers
  - Quick check question: Given a corrupted run where noise is added to token embeddings, if restoring layer-3 Attn activations recovers the correct answer, what does this imply about layer-3 Attn's role?

- Concept: **Linear Associative Memory in Transformers**
  - Why needed here: IntAttn-Edit extends this paradigm from MLP to Attn. Must understand how weight matrices encode key-value mappings and how least-squares updates preserve existing associations
  - Quick check question: If W·k ≈ v encodes factual associations, what constraint must Δ satisfy to insert new knowledge (k*, v*) while minimizing disturbance to existing (K₀, V₀)?

- Concept: **Moore-Penrose Pseudoinverse for Closed-Form Editing**
  - Why needed here: Both MLP and Attn updates rely on Eq. (6) closed-form solution. Understanding numerical stability and rank conditions is critical for implementation
  - Quick check question: What happens to the closed-form solution if K₁K₁^T + K₀K₀^T is ill-conditioned?

## Architecture Onboarding

- Component map: Input (s, r, o_new) -> Causal Tracing Module -> Balance Factor Calculator -> Key-Value Extractor -> Dual Update Computer -> Scaled Parameter Applicator -> Modified parameters Ŵ_mlp, Ŵ_attn

- Critical path:
  1. Run causal tracing on target fact → identify R_mlp and R_attn layer sets
  2. Compute cumulative LD for each module type → derive α
  3. Extract K-V pairs at critical layers for (s, r, o_new)
  4. Sample preserve set K₀, V₀ from external corpus
  5. Compute Δ via closed-form solution
  6. Scale and apply updates to both pathways

- Design tradeoffs:
  - **α selection**: Paper uses fixed values (0.3 for Qwen2.5, 0.1 for Mistral) from empirical tracing; per-fact dynamic α could improve but adds compute cost
  - **Preserve set size**: 100K triples balances knowledge retention vs. memory/compute; smaller sets risk overwriting unrelated facts
  - **Layer selection**: Early Attn (layers 2-4) + middle MLP (layers 4-8) chosen from heatmaps; broader selection increases edit magnitude and interference risk

- Failure signatures:
  - **Knowledge residual**: Original fact still retrieved → α too low or wrong Attn layers targeted
  - **Catastrophic forgetting**: Unrelated facts degraded → preserve set insufficient or Δ magnitude too large
  - **Attention collapse**: Generated text incoherent → α too high, Attn update destabilizes attention patterns
  - **Portability failure**: Edit succeeds but logical implications fail → only surface-level associations updated, not relational pathways

- First 3 experiments:
  1. **Causal tracing validation**: Replicate Figure 1-2 on target model; verify Attn shows early-layer causal peaks before proceeding with IntAttn-Edit. If Attn contribution negligible, method reduces to MEMIT
  2. **α sensitivity sweep**: Fix batch size T=100, vary α ∈ {0, 0.1, 0.2, ..., 1.0}; plot Edit Success, Portability, Locality. Confirm optimal α matches paper-reported values; identify failure boundaries
  3. **Layer ablation**: Test R_attn ∈ {{1,2}, {2,3}, {3,4}, {2,3,4}, {1,2,3,4,5}} while holding R_mlp fixed; determine if early-layer constraint is strict or if broader Attn layer selection improves performance

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions beyond the scope of the current work.

## Limitations
- The balance factor α is set empirically per model rather than computed dynamically per edit, potentially limiting optimization
- Knowledge preservation relies on sampling 100K Wikipedia triplets without clear diversity guarantees, risking underrepresentation of certain domains
- Causal tracing methodology assumptions about knowledge localization are not independently validated across different model architectures

## Confidence
- **High confidence**: The empirical performance improvements (Edit Success, Portability, Locality) are well-documented across benchmarks and batch sizes
- **Medium confidence**: The causal tracing results showing Attn module contributions are compelling but could benefit from independent replication across different model architectures
- **Low confidence**: The optimal balance factor selection methodology is unclear and lacks rigorous ablation studies

## Next Checks
1. Replicate the causal tracing experiments on a different LLM architecture (e.g., LLaMA or Falcon) to verify that attention modules consistently show early-layer causal contributions across models
2. Implement dynamic α computation per factual triple by running causal tracing at edit time rather than using pre-set model-specific values
3. Conduct a layer ablation study varying the number of attention layers updated (R_attn ∈ {2,3}, {2,3,4}, {2,3,4,5}, {2,3,4,5,6}) while holding MLP layers constant