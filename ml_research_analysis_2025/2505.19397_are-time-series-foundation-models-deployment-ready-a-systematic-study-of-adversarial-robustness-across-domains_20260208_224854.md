---
ver: rpa2
title: Are Time-Series Foundation Models Deployment-Ready? A Systematic Study of Adversarial
  Robustness Across Domains
arxiv_id: '2505.19397'
source_url: https://arxiv.org/abs/2505.19397
tags:
- adversarial
- robustness
- attacks
- attack
- tsfms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates the adversarial robustness
  of Time-Series Foundation Models (TSFMs) and finds they are highly vulnerable to
  small, imperceptible perturbations, which can reliably steer forecasts toward malicious
  outcomes such as trend reversals or drifts. The evaluation framework introduced
  uses variance-normalized perturbation budgets and unified, scale-invariant metrics
  across white-box and black-box attacks.
---

# Are Time-Series Foundation Models Deployment-Ready? A Systematic Study of Adversarial Robustness Across Domains

## Quick Facts
- **arXiv ID:** 2505.19397
- **Source URL:** https://arxiv.org/abs/2505.19397
- **Reference count:** 40
- **Primary result:** TSFMs are highly vulnerable to small, imperceptible perturbations, with forecast errors up to 50× higher than clean performance.

## Executive Summary
This study systematically evaluates the adversarial robustness of Time-Series Foundation Models (TSFMs) across six representative architectures and eight real-world datasets. The findings reveal that even minor, imperceptible perturbations can reliably steer forecasts toward malicious outcomes such as trend reversals or drifts. The evaluation framework introduces variance-normalized perturbation budgets and unified, scale-invariant metrics across white-box and black-box attacks. A novel defense, Latent Adversarial Fine-Tuning (LAT), substantially improves worst-case robustness and retains gains even with out-of-domain training data. These results highlight critical safety gaps in current TSFMs and point toward practical paths for hardening them for real-world deployment.

## Method Summary
The study evaluates six TSFMs (TimesFM 200M, TimeMoE base, UniTS x128, Moirai base, Chronos small, TabPFN-TS) on eight datasets from the GIFT-Eval benchmark. Zero-shot frozen inference is used with context length 128. Adversarial attacks include PGD (300 iterations, α=0.05) and black-box methods (SimBA, ZOO) with variance-normalized perturbation budgets (ε ∈ {0.25, 0.5, 0.75, 1}, r ∈ {0.25, 0.5, 0.75, 1}). The defense, Latent Adversarial Training (LAT), perturbs latent representations during fine-tuning (5 epochs, Adam lr=1e-4, ℓ∞ budget ε=0.5, 5 inner steps, batch size 64). Robustness is measured using Relative Error Deviation (RED) and standard forecasting metrics (NMAE, NRMSE, CRPS).

## Key Results
- Small, imperceptible perturbations led to forecast errors up to 50 times higher than clean performance.
- TSFMs exhibit heightened sensitivity to perturbations at time steps immediately preceding the forecast horizon, allowing efficient targeted attacks.
- Increasing input context length improves clean accuracy but amplifies adversarial vulnerability, creating a trade-off between utility and robustness.
- Latent adversarial fine-tuning emerged as an effective defense, substantially improving worst-case robustness and retaining gains even with out-of-domain training data.

## Why This Works (Mechanism)

### Mechanism 1: Horizon-Proximal Brittleness
The paper suggests that TSFMs exhibit heightened sensitivity to perturbations at time steps immediately preceding the forecast horizon, allowing efficient targeted attacks with limited budgets. The gradient magnitude relative to the loss is maximized when concentrating the perturbation budget on these specific indices, forcing the model to deviate from the clean trajectory more efficiently than distributing noise uniformly. This sensitivity is intrinsic to the architectural attention mechanisms or recurrent dependencies rather than an artifact of the specific data distribution.

### Mechanism 2: Context Paradox (Context-Length Amplification)
Increasing the input context length improves clean accuracy but amplifies adversarial vulnerability, creating a trade-off between utility and robustness. A longer context window expands the dimensionality of the input space, and even with a fixed perturbation ratio, the absolute number of perturbed steps increases, effectively widening the feasible attack surface. This allows perturbations to propagate through richer temporal dependencies, with the model lacking sufficient internal regularization to smooth out accumulated noise over long sequences.

### Mechanism 3: Latent Adversarial Training (LAT) as a Defense
Perturbing the latent representation during fine-tuning improves worst-case robustness more effectively than input-space training, with gains generalizing to out-of-domain data. LAT optimizes the model parameters to minimize loss against worst-case internal perturbations, explicitly regularizing the learned manifold and forcing the prediction head to be invariant to small shifts in the latent space. This addresses "unforeseen failure modes" by smoothing the internal decision boundaries rather than just the input-output mapping.

## Foundational Learning

- **Concept: Adversarial Perturbation Constraints ($\ell_p$ norms & Sparsity)**
  - **Why needed here:** The paper introduces a hybrid constraint combining magnitude ($\ell_\infty$) and sparsity ($\ell_0$). Understanding that $\ell_\infty$ bounds the size of a single change while $\ell_0$ bounds the number of changes is critical for grasping why attacks are "imperceptible" yet effective.
  - **Quick check question:** Why would a defense that only checks for large magnitude changes fail against a sparse attack with small $\epsilon$ but high $r$?

- **Concept: Gradient Obfuscation**
  - **Why needed here:** Section 4.1 highlights that TimeMoE appears robust to PGD but fails against simple black-box attacks or single-step FGSM. This is a "false sense of security" caused by non-differentiable components disrupting gradient flow, not true robustness.
  - **Quick check question:** If a model is resistant to iterative white-box attacks but fails instantly to random search or single-step attacks, is it robust? Why not?

- **Concept: Variance-Normalized Budgets**
  - **Why needed here:** Time series datasets have vastly different scales. The paper uses $\epsilon^* = \epsilon \cdot \text{var}(x)$ to ensure an attack of strength $\epsilon=0.5$ represents the same relative noise level across all datasets.
  - **Quick check question:** Without variance normalization, would an attack with $\epsilon=0.5$ be more effective on a dataset with mean 1,000,000 and variance 10, or a dataset with mean 10 and variance 100?

## Architecture Onboarding

- **Component map:** Data → Variance Normalization → [Attacker applies $\delta$ within budget $S$] → TSFM Backbone → Prediction → [Relative Error Deviation (RED) Evaluation]
- **Critical path:** Data → Variance Normalization → [Attacker applies $\delta$ within budget $S$] → TSFM Backbone → Prediction → [Relative Error Deviation (RED) Evaluation]
- **Design tradeoffs:**
  - **Context Length:** Longer context boosts clean accuracy but significantly increases the attack surface (Context Paradox).
  - **Architecture:** Decoder-only models (like TimesFM) showed high vulnerability (50x errors), whereas Encoder-only (UniTS) demonstrated higher robustness in some settings, potentially due to multi-task pretraining effects.
  - **Defense:** Inference-time smoothing is cheap but unreliable and harms black-box robustness; Adversarial Training (LAT) is effective but computationally expensive during fine-tuning.
- **Failure signatures:**
  - **Gradient Obfuscation:** Model resists PGD but fails to FGSM/SimBA (observed in TimeMoE).
  - **Horizon Drift:** Targeted attacks successfully induce linear drifts or trend flips even when local structure is preserved.
  - **Defense Regression:** Smoothing or IAT improves white-box robustness but degrades black-box robustness on specific datasets (e.g., Hier. Sales).
- **First 3 experiments:**
  1. **Baseline Robustness Check:** Run untargeted PGD vs. SimBA on the target TSFM with $\epsilon=0.5, r=1$ across varied datasets to determine if robustness is genuine or obfuscated (check for PGD > SimBA disparity).
  2. **Sensitivity Mapping:** Implement the gradient-magnitude analysis described in Section 4.2 to verify if the specific model architecture exhibits "horizon-proximal brittleness" or if vulnerability is distributed uniformly.
  3. **Defense Validation (LAT):** Apply Latent Adversarial Training (LAT) using an out-of-domain dataset and measure the reduction in RED scores to verify the transferability of the defense claim.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What architectural components mechanistically drive the distinct vulnerability patterns observed (e.g., horizon-proximal brittleness, patch-based sensitivity in Moirai)?
- **Basis in paper:** The distinct vulnerability patterns warrant deeper mechanistic analysis to isolate how architectural components, such as patching schemes, drive these failures.
- **Why unresolved:** The paper identifies correlations but does not perform causal ablations to isolate which design choices cause which failures.
- **What evidence would resolve it:** Controlled ablation studies varying individual architectural choices (patch size, gating mechanisms, normalization) while holding other factors constant, coupled with gradient flow analysis.

### Open Question 2
- **Question:** Can adversarial pretraining from scratch, rather than post-hoc fine-tuning, fundamentally close the robustness gap between TSFMs and supervised models?
- **Basis in paper:** The defense analysis focused on lightweight methods, leaving resource-intensive techniques like adversarial pretraining as a direction to clarify robustness limits.
- **Why unresolved:** The paper only evaluates inference-time smoothing and adversarial fine-tuning; whether robustness can be baked into foundation models during large-scale pretraining remains untested.
- **What evidence would resolve it:** Train TSFMs with adversarial objectives during pretraining and compare worst-case robustness against standard pretrained models and supervised baselines.

### Open Question 3
- **Question:** Why do gradient-based adversarial defenses fail to transfer to query-based black-box attacks, and can this misalignment be corrected?
- **Basis in paper:** Black-box robustness remains elusive, suggesting a misalignment between the gradient-based optimization used in defenses and the query-based geometry of black-box attacks.
- **Why unresolved:** The paper documents the phenomenon but does not investigate its causes or propose remedies for the SimBA vulnerability persistence.
- **What evidence would resolve it:** Analyze the loss landscape geometry differences between PGD and query-based attacks; propose and test defense objectives that incorporate query-based threat models.

## Limitations
- The robustness claims are empirical and derived from specific model architectures and datasets, without theoretical bounds for the general TSFM class.
- The defense mechanism (Latent Adversarial Training) requires fine-tuning, which may not be feasible for all deployment scenarios.
- Variance-normalized perturbation budgets may not reflect real-world attack constraints where adversaries have access to absolute measurement units.

## Confidence

- **High Confidence:** The core finding that TSFMs are vulnerable to small perturbations (observed 50x error increases) is well-supported by systematic experimentation across multiple models and datasets.
- **Medium Confidence:** The mechanism explanations (horizon-proximal brittleness, context paradox) are plausible but lack formal theoretical justification. The defense effectiveness claims are supported but may not generalize beyond the tested fine-tuning regime.
- **Low Confidence:** The transferability analysis showing "model-specific rather than transferable attack effects" is based on pairwise comparisons that may not capture the full space of possible attack strategies or model vulnerabilities.

## Next Checks

1. **Architecture Ablation Study:** Test the same attack framework on a TSFM variant with uniform positional attention to verify if horizon-proximal brittleness is an intrinsic vulnerability or architecture-specific artifact.
2. **Theoretical Robustness Bounds:** Derive formal bounds on adversarial vulnerability for TSFMs based on their architectural components to complement the empirical findings.
3. **Real-World Deployment Simulation:** Implement the defense mechanisms in a constrained-resource setting to evaluate the practical feasibility of LAT versus inference-time smoothing under realistic computational budgets.