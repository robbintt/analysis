---
ver: rpa2
title: 'Model Inversion in Split Learning for Personalized LLMs: New Insights from
  Information Bottleneck Theory'
arxiv_id: '2501.05965'
source_url: https://arxiv.org/abs/2501.05965
tags:
- inversion
- information
- attacks
- attack
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work identifies a critical privacy vulnerability in split
  learning for personalized LLMs, where intermediate representations transmitted between
  devices and servers can be exploited through model inversion attacks. To address
  the challenge of sparse and information-poor representations compared to embeddings,
  the authors propose a two-stage attack system: first, an information purification
  module projects representations into the embedding space, then a generative decoder
  recovers the original text.'
---

# Model Inversion in Split Learning for Personalized LLMs: New Insights from Information Bottleneck Theory

## Quick Facts
- arXiv ID: 2501.05965
- Source URL: https://arxiv.org/abs/2501.05965
- Reference count: 27
- Over 60% improvement over state-of-the-art model inversion methods for recovering original text from intermediate representations

## Executive Summary
This work identifies a critical privacy vulnerability in split learning for personalized LLMs, where intermediate representations transmitted between devices and servers can be exploited through model inversion attacks. To address the challenge of sparse and information-poor representations compared to embeddings, the authors propose a two-stage attack system: first, an information purification module projects representations into the embedding space, then a generative decoder recovers the original text. This approach significantly improves attack performance, achieving ROUGE-L scores of 38%-75% across various scenarios, representing over 60% improvement over state-of-the-art methods. The research also provides novel insights into information propagation in Transformer-based LLMs through mutual information analysis, demonstrating that intermediate representations pose substantial privacy risks comparable to raw data and must be protected with robust security measures.

## Method Summary
The authors propose RevertLM, a two-stage model inversion attack for split learning in personalized LLMs. The attack first uses an information purification module to project sparse intermediate representations into a semantically richer embedding space, then employs a generative decoder (GPT2-XL or T5) to recover the original text autoregressively. The system is trained in three steps: pretraining the purification module separately to avoid disturbing generation capacity, training the adversarial decoder with SequenceCrossEntropy loss, and finally joint fine-tuning all components. The approach addresses the challenge that intermediate representations are more compressed and information-poor than embeddings, which previous single-stage inversion methods struggled to handle effectively.

## Key Results
- ROUGE-L scores of 38%-75% across various split points and datasets, demonstrating successful text recovery from intermediate representations
- Over 60% improvement compared to state-of-the-art methods (GEIA, Vec2Text) on PersonaChat and Wiki datasets
- Mutual information analysis reveals that attention blocks retain significantly more input information than FFN layers, with attack success rates directly correlating to I(x;h) values
- Earlier split points (near embeddings) show higher vulnerability but the attack remains effective even at deeper layers, challenging assumptions about privacy protection through model depth

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Inversion (Information Purification + Generative Decoding)
Decomposing representation-to-text inversion into two subproblems (representation→embedding→text) improves recovery performance when intermediate representations are sparse. An information purification module projects sparse intermediate representations into a semantically richer embedding space. A generative decoder (GPT2-XL or T5) then reconstructs text autoregressively from these purified embeddings. This bypasses the difficulty of directly inverting representations that have passed through multiple transformer blocks.

### Mechanism 2: Mutual Information Analysis for Layer Vulnerability Assessment
Mutual information I(x;h) between input x and intermediate representation h predicts inversion susceptibility and does not monotonically decrease with depth in transformers. The authors compute discretized mutual information across transformer blocks, finding that attention layers preserve more input information than FFN layers. I(x;h) and I(h;y) remain positively correlated, guiding which split points are most vulnerable.

### Mechanism 3: Generative Decoder Superiority for Sequence Recovery
Decoder-only models (GPT2-XL) outperform optimization-based inversion for text recovery from embeddings because autoregressive generation leverages learned language priors. The attack generator is trained with SequenceCrossEntropy loss using teacher-forcing, learning to map embeddings to token sequences. The pre-trained language model provides strong priors for coherent text generation, enabling >60% improvement over SOTA.

## Foundational Learning

- **Split Learning Architecture**: Understanding where the model is partitioned (cut layer) determines what representations are transmitted and thus what an attacker can access.
- **Information Bottleneck Theory**: Provides theoretical grounding for why deeper layers might retain less input information, though transformers violate simple IB predictions.
- **Autoregressive Language Modeling**: The generative decoder uses autoregressive training (predict next token given previous tokens), which is core to how the attack recovers sequences.

## Architecture Onboarding

- **Component map**: Victim Model (layers 0 to split point i) → Information Purification Module → Attack Generator (GPT2-XL/T5) → Generated Text
- **Critical path**: 1) Intercept intermediate representations at split point, 2) Apply pre-trained purification projection, 3) Feed to attack generator, 4) Generate text autoregressively, 5) Evaluate with ROUGE-L, BLEU, cosine similarity
- **Design tradeoffs**: Linear projection vs autoencoder for purification (linear projection achieved best results but may not generalize); GPT2-XL vs T5 (decoder-only vs encoder-decoder); earlier split points easier to invert but less realistic attack scenario
- **Failure signatures**: Very low ROUGE (<0.15) suggests split point is after FFN layer or at very deep block; high perplexity in generated text indicates purification module not properly trained; semantic drift (high ROUGE but wrong meaning) suggests generator priors dominate over representation information
- **First 3 experiments**: 1) Replicate Table 3: Measure attack ROUGE at attention blocks vs FFN layers at blocks 1, 20, 45 on GPT2-XL to confirm MI-attack correlation, 2) Ablate purification module: Compare linear projection, trained autoencoder, and direct inversion to quantify purification contribution, 3) Cross-dataset generalization: Train attacker on PersonaChat auxiliary data, test on Wiki to measure distribution shift sensitivity

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but raises several important implications:
1. Why do Feed-Forward Network (FFN) layers exhibit significantly lower susceptibility to inversion attacks compared to multi-head attention layers?
2. What specific architectural components drive the non-monotonic variation of mutual information observed in deep Transformer layers?
3. How can defense mechanisms be optimized to prevent model inversion in split learning without significantly degrading the utility of the personalized LLM?

## Limitations

- The attack relies on having access to an auxiliary dataset with similar distribution to train the generative decoder, which may not be available in all scenarios
- Mutual information estimates are based on discretized approximations that may not fully capture privacy-relevant information in continuous representations
- The two-stage approach introduces additional complexity and may be vulnerable to distribution shifts between auxiliary training data and target scenarios

## Confidence

- **High Confidence**: The basic mechanism of two-stage inversion (purification followed by generative decoding) is well-established in related work and the empirical results showing improved performance are convincing
- **Medium Confidence**: The claim about information bottleneck theory violation in transformers is interesting but the MI analysis is limited to discrete approximations that may not capture all relevant information
- **Low Confidence**: The generalizability of the linear projection purification approach across different model architectures and domains is uncertain

## Next Checks

1. **Distribution Shift Robustness**: Train the attacker on PersonaChat auxiliary data, then test on Wiki dataset to measure performance degradation and validate whether the purification module generalizes beyond the training distribution

2. **Architectural Transferability**: Apply the same purification module architecture (linear projection) to a different transformer architecture (e.g., BERT instead of GPT2-XL) to test if the approach is architecture-specific or generalizes

3. **Information Content Correlation**: Systematically vary the amount of information in representations (by selecting different split points) and measure correlation between mutual information estimates and actual inversion success rate to validate the MI-attack relationship claimed in the paper