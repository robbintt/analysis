---
ver: rpa2
title: Ask a Strong LLM Judge when Your Reward Model is Uncertain
arxiv_id: '2510.20369'
source_url: https://arxiv.org/abs/2510.20369
tags:
- uncertainty
- arxiv
- reward
- routing
- judge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an uncertainty-based routing framework that
  complements a fast reward model (RM) with a strong but costly LLM judge in reinforcement
  learning with human feedback (RLHF). The approach formulates advantage estimation
  as pairwise preference classification, enabling principled uncertainty quantification
  to guide routing decisions.
---

# Ask a Strong LLM Judge when Your Reward Model is Uncertain

## Quick Facts
- arXiv ID: 2510.20369
- Source URL: https://arxiv.org/abs/2510.20369
- Reference count: 40
- Primary result: Uncertainty-based routing significantly outperforms random routing at same cost in RLHF

## Executive Summary
This paper introduces an uncertainty-based routing framework that complements a fast reward model (RM) with a strong but costly LLM judge in reinforcement learning with human feedback (RLHF). The approach formulates advantage estimation as pairwise preference classification, enabling principled uncertainty quantification to guide routing decisions. Uncertain pairs are forwarded to the LLM judge while confident ones are evaluated by the RM. Experiments show that this method significantly outperforms random routing at the same cost, improving both reward benchmark accuracy and downstream alignment performance. The framework achieves higher accuracy gains with fewer LLM judge calls, striking an effective balance between evaluation quality and computational efficiency in online RLHF.

## Method Summary
The framework trains a pairwise reward model (PM) with Spectral-Normalized Gaussian Process (SNGP) to output both preference scores and epistemic uncertainty. During RLHF training, the system routes uncertain preference pairs to a strong LLM judge (DeepSeek-R1) while confident pairs are evaluated by the PM. The pairwise formulation enables well-defined uncertainty quantification that pointwise RMs cannot provide. Advantages are computed from preference differences using group-based policy gradient methods like RLOO. The system maintains training efficiency by limiting LLM judge calls to pairs with uncertainty scores exceeding a threshold.

## Key Results
- Uncertainty routing achieves 1.7% absolute accuracy gain over random routing at same LLM judge cost
- System significantly improves downstream alignment performance (Arena-Hard, AlpacaEval 2.0, MT-Bench)
- Routing efficiency allows better accuracy with fewer LLM judge calls
- SNGP-PM maintains performance parity with standard PM while enabling uncertainty quantification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Routing based on epistemic uncertainty selectively targets samples where the reward model's limited training coverage causes unreliability.
- Mechanism: The framework applies Spectral-Normalized Gaussian Process (SNGP) to a pairwise preference model, which outputs both a preference logit (aleatoric uncertainty from inherent preference ambiguity) and a variance-based uncertainty score (epistemic uncertainty from distance to training data). Only pairs exceeding an uncertainty threshold are forwarded to the LLM judge.
- Core assumption: Epistemic uncertainty correlates with out-of-distribution inputs where reward models generalize poorly, and LLM judges with reasoning capabilities generalize better on such inputs.
- Evidence anchors:
  - [abstract] "Uncertain pairs are forwarded to the LLM judge, while confident ones are evaluated by the RM."
  - [Section 3.1] "The variance-induced uncertainty u quantifies the epistemic uncertainty due to limited training data... a strong LLM judge with good generalization may help improve the prediction on epistemic uncertain samples."
  - [corpus] Related work on probabilistic uncertain reward models (arxiv 2503.22480) similarly addresses overconfidence on OOD samples, suggesting convergent concern about this failure mode.
- Break condition: If LLM judges do not generalize better than RMs on high-uncertainty inputs, or if uncertainty scores fail to correlate with actual prediction errors, routing gains diminish.

### Mechanism 2
- Claim: Pairwise preference formulation enables principled uncertainty quantification that pointwise reward models cannot support.
- Mechanism: Under the Bradley-Terry model, pointwise RMs are invariant to prompt-dependent bias terms, making the UQ problem ill-posed (different but equivalent solutions yield different uncertainty estimates). Pairwise preference classification is well-defined with a unique solution, allowing distance-aware uncertainty methods like SNGP to function correctly.
- Core assumption: The classification formulation captures sufficient information for RL advantage estimation while maintaining well-defined uncertainty.
- Evidence anchors:
  - [Section 2.3] "Pointwise RMs are inherently indefinite under the Bradley-Terry preference model—adding a prompt-dependent-only bias term yields the same preference distribution, making the UQ problem ill-defined."
  - [Section 3.1, Remark 1] "PM is well defined within the data support and is unique in the population sense. Therefore, one can measure the distance from the data to the support of the training set for epistemic uncertainty quantification."
  - [corpus] Weak corpus signal; related papers focus on multi-judge systems rather than pairwise vs. pointwise UQ distinction.
- Break condition: If preference data deviates significantly from pairwise comparison assumptions, or if pointwise UQ methods can be made well-posed through alternative formulations.

### Mechanism 3
- Claim: Reward differences suffice for advantage estimation in group-based policy gradient methods, enabling direct use of pairwise preferences without scalar rewards.
- Mechanism: Methods like RLOO and GRPO estimate advantages via within-group reward comparisons. The advantage for response yi is computed as the average difference between r(xi, yi) and all other responses in the group. Since only differences matter, the pairwise PM output directly substitutes for implicit scalar rewards.
- Core assumption: The policy gradient methods used can operate effectively with preference-based advantages without requiring absolute reward calibration.
- Evidence anchors:
  - [Section 3.2] "In view of (10), estimating the advantage only requires reward differences between responses within each group, and thus our SNGP-PM with an uncertainty router is applicable."
  - [Section 3.2, Equation 10-11] Shows the RLOO loss formulation using pairwise preference differences directly.
  - [corpus] "Alignment as Distribution Learning" paper notes preference models can explicitly function as language models, supporting the theoretical coherence of preference-based objectives.
- Break condition: If downstream tasks require well-calibrated scalar rewards for other purposes (e.g., best-of-n selection, rejection sampling), the pairwise formulation may need extension.

## Foundational Learning

- Concept: **Bradley-Terry preference model**
  - Why needed here: The entire reward modeling pipeline assumes preferences follow the BT model, where preference probability is a sigmoid function of reward difference. Understanding this clarifies why pairwise PM outputs are logits and why pointwise RM identifiability is problematic.
  - Quick check question: Given two responses with reward scores 2.0 and 1.5, what is the predicted probability that the first is preferred?

- Concept: **Epistemic vs Aleatoric Uncertainty**
  - Why needed here: The routing mechanism explicitly separates these—aleatoric uncertainty (inherent preference ambiguity) is considered irreducible and not worth escalating to LLM judges, while epistemic uncertainty (knowledge gap from limited training data) is the routing trigger.
  - Quick check question: A preference pair has high disagreement among human annotators. Is this epistemic or aleatoric uncertainty, and should it trigger LLM judge routing?

- Concept: **Policy Gradient with Baseline/Variance Reduction**
  - Why needed here: The advantage estimation formula depends on understanding why baselines reduce variance without introducing bias, and how group-based methods like RLOO approximate the value function without a learned critic.
  - Quick check question: In RLOO with K=4 samples per prompt, how is the baseline computed for the first response's advantage estimate?

## Architecture Onboarding

- Component map: Llama-3.1-8B-Instruct base -> SNGP-PM with spectral normalization and GP layer -> Uncertainty Router (threshold comparator) -> LLM Judge (DeepSeek-R1) or PM -> RLOO Trainer (policy gradient with KL regularization)

- Critical path:
  1. Prompt batch -> policy model -> K responses per prompt
  2. All K(K-1) ordered pairs -> SNGP-PM -> preference scores + uncertainty scores
  3. Pairs above uncertainty threshold -> LLM judge -> override preference scores
  4. Assemble preference matrix -> compute advantages via leave-one-out averaging
  5. Policy gradient step with KL regularization

- Design tradeoffs:
  - **Threshold selection**: Lower thresholds increase accuracy gains but raise LLM judge costs non-linearly (harder instances take longer). Paper shows diminishing returns above ~25% routing rates on some benchmarks.
  - **Group size K**: Larger K provides better variance reduction but quadratically increases pairwise comparisons. Paper uses K=4 due to resource constraints.
  - **Judge choice**: Stronger judges (DeepSeek-R1) give better accuracy but higher latency; smaller generative RMs are suggested as future work.

- Failure signatures:
  - **Uncertainty miscalibration**: If SNGP uncertainty doesn't correlate with actual errors, routing becomes near-random. Monitor: accuracy vs. uncertainty quantile plots (Figure 1 shows expected negative correlation).
  - **Judge bottleneck**: If routing rate exceeds judge throughput, training pipeline stalls. Paper's 200 RPM limit required careful threshold tuning.
  - **Position bias in PM**: Without response swapping augmentation, PM may learn spurious position preferences. Paper augments with both orderings.

- First 3 experiments:
  1. **SNGP-PM accuracy parity**: Train standard PM and SNGP-PM on HelpSteer2-Preference; verify <1% accuracy difference on validation/RewardBench/RM-Bench (Table 1). Confirms UQ head doesn't degrade base performance.
  2. **Uncertainty-correlation check**: Bin test pairs by uncertainty quantile; plot accuracy per bin. Confirm negative Spearman correlation (Figure 1). If correlation is weak, reconsider UQ method.
  3. **Routing ablation at fixed budget**: At identical LLM judge call counts (e.g., 9.2% of pairs), compare uncertainty-based vs. random routing on RewardBench. Expect ~1.7% absolute accuracy gain (Table 2). If gains disappear, uncertainty signal is uninformative.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a smaller-scale generative reward model effectively replace a large proprietary LLM judge (like DeepSeek-R1) to maintain inference efficiency while preserving the accuracy gains of the uncertainty-based routing framework?
- **Basis in paper:** [explicit] The conclusion states one can "replace it with a smaller-scale generative RM to further improve the judge quality and inference efficiency" rather than relying on a resource-intensive model like DeepSeek-R1.
- **Why unresolved:** The current experiments exclusively utilize DeepSeek-R1, which requires huge computational resources and is not specifically fine-tuned for the judge task.
- **What evidence would resolve it:** Experiments substituting DeepSeek-R1 with a distilled or fine-tuned smaller generative model, demonstrating comparable RewardBench accuracy and downstream alignment improvements with lower latency.

### Open Question 2
- **Question:** How can the uncertainty-based router be extended to quantify the reliability or "hardness" of the LLM judge itself, enabling a hierarchical routing strategy?
- **Basis in paper:** [explicit] The authors identify the need for "quantifying the hardness of the sample and the uncertainty of the LLM judge and enabling a hierarchical routing strategy" to allocate inference budgets more efficiently.
- **Why unresolved:** The current framework treats the LLM judge as a strong oracle, assuming its verdict is always superior to the RM, without accounting for cases where the judge itself might be uncertain or unreliable.
- **What evidence would resolve it:** A hierarchical system that filters samples based on both RM uncertainty and LLM judge confidence, showing improved error detection over the single-stage routing approach.

### Open Question 3
- **Question:** Does using the uncertainty router to select samples for human annotation (active learning) significantly close the distribution gap between the reward model and the policy model?
- **Basis in paper:** [explicit] The paper suggests the router "can also send the most uncertain samples to human annotators... This could help close the gap between reward training and actual policy model distributions."
- **Why unresolved:** The current work uses the router only for real-time inference intervention (switching to an AI judge) rather than for data selection to iteratively improve the reward model.
- **What evidence would resolve it:** A study where the RM is retrained on routed samples, showing a reduction in OOD uncertainty scores and improved stability in online RLHF.

## Limitations

- The approach critically depends on SNGP's ability to produce well-calibrated epistemic uncertainty scores; if this correlation degrades under different data distributions or PM architectures, routing effectiveness could collapse
- The cost-benefit tradeoff assumes stable judge latency; if uncertain samples systematically take longer than confident ones, the routing system could become bottlenecked
- The current framework is specialized to RLHF pipelines using pairwise preference differences; extension to pointwise reward calibration tasks requires additional mechanisms

## Confidence

- **High confidence**: The core uncertainty routing mechanism and experimental results showing superior performance over random routing at fixed cost. The pairwise preference formulation enabling principled UQ is well-supported by theory.
- **Medium confidence**: The assumption that SNGP uncertainty will remain well-calibrated when scaled to larger PMs or different preference datasets. The cost-benefit analysis assumes stable judge performance characteristics.
- **Low confidence**: Generalization to reward calibration tasks beyond RLHF, and performance when applied to significantly different preference model architectures.

## Next Checks

1. **Uncertainty calibration test**: Evaluate SNGP-PM on out-of-distribution preference pairs from HelpSteer2-Instruction (responses vs human-written answers) to verify uncertainty scores maintain correlation with actual errors beyond the training distribution.

2. **Pairwise assumption stress test**: Introduce controlled noise into preference comparisons (e.g., synthetic ties, inconsistent pairwise comparisons) and measure degradation in both PM accuracy and routing performance.

3. **Scalability boundary test**: Train SNGP-PM on 5× larger preference dataset and measure whether routing gains scale proportionally or if uncertainty calibration degrades with model scale.