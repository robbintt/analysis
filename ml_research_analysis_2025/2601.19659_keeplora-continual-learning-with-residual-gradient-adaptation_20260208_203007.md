---
ver: rpa2
title: 'KeepLoRA: Continual Learning with Residual Gradient Adaptation'
arxiv_id: '2601.19659'
source_url: https://arxiv.org/abs/2601.19659
tags:
- learning
- keeplora
- wang
- subspace
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes KeepLoRA, a continual learning method for
  vision-language models that addresses the challenge of balancing three competing
  objectives: retaining pre-trained knowledge, preserving knowledge from previously
  learned tasks, and maintaining plasticity to acquire new knowledge. The core idea
  is that general knowledge is encoded in the principal subspace of model parameters
  while task-specific knowledge resides in the residual subspace.'
---

# KeepLoRA: Continual Learning with Residual Gradient Adaptation

## Quick Facts
- **arXiv ID:** 2601.19659
- **Source URL:** https://arxiv.org/abs/2601.19659
- **Reference count:** 31
- **Primary result:** Proposes KeepLoRA, achieving 86.1% Last score and 77.5% Average score on MTIL, and 64.4% Last score and 54.2% Average score on MLLM-DCL

## Executive Summary
This paper introduces KeepLoRA, a continual learning method for vision-language models that addresses the fundamental challenge of balancing three competing objectives: retaining pre-trained knowledge, preserving knowledge from previously learned tasks, and maintaining plasticity to acquire new knowledge. The core insight is that general knowledge is encoded in the principal subspace of model parameters while task-specific knowledge resides in the residual subspace. By projecting gradients onto a subspace orthogonal to both the principal subspace of pre-trained parameters and the dominant feature directions of previous tasks, while initializing updates in a gradient-informed direction within the residual subspace, KeepLoRA effectively preserves both pre-trained and previously learned knowledge while maintaining strong adaptation capability to new tasks.

## Method Summary
KeepLoRA operates by first extracting the principal subspace from pre-trained weight matrices using SVD, then for each new task, projecting gradients onto the residual subspace orthogonal to both pre-trained principal directions and accumulated feature directions from previous tasks. The method initializes LoRA matrices using the projected gradient, freezes the down-projection matrix A after initialization, and optimizes only the up-projection matrix B. After task completion, the LoRA weights are merged into the main model, and dominant feature directions from the current task are extracted and stored. This process repeats for each new task, maintaining a unified subspace that captures both pre-trained knowledge and accumulated task-specific directions.

## Key Results
- Achieves 86.1% Last score and 77.5% Average score on MTIL benchmark (11 datasets)
- Achieves 64.4% Last score and 54.2% Average score on MLLM-DCL benchmark
- Demonstrates state-of-the-art performance while preserving both pre-trained and previously learned knowledge
- Maintains strong plasticity for adapting to new tasks across diverse vision-language scenarios

## Why This Works (Mechanism)

### Mechanism 1
The method constrains parameter updates to the residual subspace (small singular values) while preserving pre-trained general knowledge encoded in the principal subspace (large singular values). SVD analysis shows that general-domain datasets remain robust to alterations in the principal subspace, while task-specific datasets are highly sensitive to residual subspace changes. By projecting gradients orthogonal to the principal subspace, updates avoid corrupting general capabilities while allowing domain-specific adaptation.

### Mechanism 2
Freezing the down-projection matrix A initialized in the orthogonal residual subspace, while optimizing only B, is mathematically equivalent to gradient descent constrained to that subspace. Proposition 3.1 proves that when A is frozen and orthonormal, updating B produces weight changes confined to span(A). The projection operator A·A^T restricts all updates, ensuring stability while maintaining plasticity within the constrained subspace.

### Mechanism 3
Maintaining a unified principal subspace combining pre-trained weight principal directions and dominant feature directions from previous tasks prevents both forward and backward forgetting. After each task, dominant singular vectors of task features are extracted and appended to a growing subspace. The next task's gradient is projected to be orthogonal to this accumulated knowledge, with total vectors bounded by the input dimension, ensuring a compact representation that captures essential task-specific information without storing samples.

## Foundational Learning

- **Concept: Singular Value Decomposition (SVD) for Subspace Analysis**
  - Why needed here: The entire method relies on decomposing weight matrices and feature matrices to extract principal directions vs. residual directions
  - Quick check question: Given a matrix W, can you explain what the columns of U represent and how selecting the first k columns defines a subspace?

- **Concept: Orthogonal Projection Operators**
  - Why needed here: The core operation is projecting gradients onto residual subspaces using P = I - W_p·W_p^T - M·M^T, which requires understanding how orthonormal bases define projection operators
  - Quick check question: If P = I - AA^T where A has orthonormal columns, what happens to any vector v when multiplied by P if v lies entirely within span(A)?

- **Concept: Low-Rank Adaptation (LoRA) Parameterization**
  - Why needed here: KeepLoRA modifies standard LoRA by freezing A with gradient-informed initialization while optimizing only B; understanding the standard LoRA formulation W + α/r·AB is prerequisite
  - Quick check question: In standard LoRA, why is B initialized to zero and A initialized randomly? How does KeepLoRA's initialization differ?

## Architecture Onboarding

- **Component map:** W_p extraction → Gradient projection → LoRA initialization → Training B → Feature direction extraction → Weight merging
- **Critical path:** 1) Extract W_p before any training (requires one SVD pass) 2) For each new task: compute first-step gradient → project to residual → SVD for initialization → train B → extract and store feature directions 3) Ensure ε_w and ε_f thresholds leave sufficient residual subspace capacity
- **Design tradeoffs:** ε_w (energy retention for W_p) - higher preserves more pre-trained knowledge but shrinks residual subspace; ε_f (energy retention for features) - controls how many previous task directions to store; Rank r - affects approximation quality of gradient in residual subspace
- **Failure signatures:** Plasticity collapse (Last metric drops → residual subspace exhausted); Forward forgetting (Transfer metric degrades → W_p not adequately preserved); Backward forgetting (Previous task accuracy drops → ε_f too low)
- **First 3 experiments:** 1) Subspace capacity check: compute dim(residual) = d_in - dim(W_p) - dim(M_{t-1}) before training; 2) Gradient alignment verification: compute ||A·A^T·G_t||_F / ||G_t||_F to confirm gradient is captured; 3) Interference heatmap: replicate Figure 2 visualization showing LoRA output norms across task pairs

## Open Questions the Paper Calls Out

- **Open Question 1:** How does text encoder hyperparameter sensitivity differ between classification tasks (using only class names) and more text-rich tasks like VQA or image captioning? The paper notes that datasets with substantial text data warrant further study regarding ϵ_w(text) sensitivity, as experiments primarily use CLIP with limited text content.

- **Open Question 2:** What happens when the unified principal subspace approaches its theoretical capacity limit of d_in dimensions with very long task sequences? The paper notes the unified subspace is "upper-bounded by d_in" but experiments only cover 11-6 tasks, leaving scenarios with 50-100+ tasks unexplored.

- **Open Question 3:** Can gradient-informed initialization be improved by incorporating information beyond the first training step? KeepLoRA uses only the first-step gradient G_t for initialization without justifying why this single step is optimal versus multi-step or adaptive approaches.

## Limitations

- **Subspace capacity ceiling:** The method's effectiveness depends critically on having sufficient residual subspace, which shrinks as M_t accumulates, potentially causing plasticity collapse on long task sequences without recovery mechanisms.
- **Feature direction extraction:** The exact procedure for extracting M_t from projected features is underspecified, making faithful reproduction challenging without clear details on accumulation and orthogonalization.
- **Layer selection:** The paper doesn't specify which layers receive LoRA adaptation, and applying to all layers vs. selective attention projections could significantly impact performance.

## Confidence

- **High confidence:** The core mechanism of gradient projection onto residual subspaces is mathematically sound and well-supported by SVD analysis of weight matrices.
- **Medium confidence:** The subspace separation assumption (principal vs residual knowledge) holds for attention weights in CLIP models, but generalizability to other architectures requires validation.
- **Medium confidence:** State-of-the-art benchmark results, though direct comparison requires access to the same experimental setup and hyperparameters.

## Next Checks

1. **Subspace capacity analysis:** Before each task, compute residual subspace dimension and track how it shrinks across tasks to determine if plasticity degradation correlates with subspace exhaustion.
2. **Gradient capture verification:** After A initialization, measure the fraction of gradient energy captured by the LoRA subspace, targeting >70% capture rate for effective adaptation.
3. **Interference matrix visualization:** Generate task-pair interference heatmaps similar to Figure 2 to verify that off-diagonal entries (cross-task interference) remain low, confirming stability preservation.