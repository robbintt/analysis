---
ver: rpa2
title: Discourse-Aware Scientific Paper Recommendation via QA-Style Summarization
  and Multi-Level Contrastive Learning
arxiv_id: '2511.03330'
source_url: https://arxiv.org/abs/2511.03330
tags:
- uni00000013
- uni00000011
- summarization
- recommendation
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents OMRC-MR, a content-based scientific paper recommendation
  framework that addresses the challenge of recommending relevant papers without relying
  on user interaction data or citation graphs. The key innovation is integrating QA-style
  OMRC (Objective, Method, Result, Conclusion) summarization with multi-level contrastive
  learning and structure-aware re-ranking to capture the discourse organization of
  scientific papers.
---

# Discourse-Aware Scientific Paper Recommendation via QA-Style Summarization and Multi-Level Contrastive Learning

## Quick Facts
- **arXiv ID:** 2511.03330
- **Source URL:** https://arxiv.org/abs/2511.03330
- **Reference count:** 40
- **Key outcome:** Content-based paper recommendation achieving Precision@10 up to 68.92% and Recall@10 up to 55.14% on Sci-OMRC dataset

## Executive Summary
This paper introduces OMRC-MR, a framework for recommending scientific papers without relying on user interaction data or citation graphs. The method transforms papers into structured OMRC (Objective, Method, Result, Conclusion) summaries via evidence-constrained QA-style generation, then learns document-level and role-specific embeddings through multi-level contrastive learning. A two-stage retrieval with role-aware re-ranking achieves state-of-the-art performance across three datasets, improving precision by up to 7.2% over strong baselines while maintaining cross-disciplinary robustness.

## Method Summary
The system operates in three stages: (1) QA-style OMRC summarization using LLM-based evidence-constrained generation with role-specific questions and evidence anchors; (2) multilingual BERT encoder with role-specific projection heads trained via multi-level contrastive learning balancing document-level metadata alignment and role-level discrimination; (3) hierarchical retrieval with coarse metadata-based candidate selection followed by role-aware re-ranking using a composite similarity score. The approach achieves strong performance without requiring citation graphs or user interaction data.

## Key Results
- **Precision@10:** 68.92% on Sci-OMRC (7.2% improvement over best baseline)
- **Recall@10:** 55.14% on Sci-OMRC (3.8% improvement over best baseline)
- **Cross-disciplinary robustness:** Maintains strong performance across DBLP, S2ORC, and Sci-OMRC datasets with different domains

## Why This Works (Mechanism)

### Mechanism 1: QA-Style Structured Summarization Decomposes Discourse Semantics
Evidence-constrained QA-style decomposition into OMRC components yields more semantically complete and factually grounded representations than single-pass summarization. The method transforms papers via role-specific question templates with evidence anchors, generating candidate answers that are clustered and aggregated to produce final role summaries. This constrains generation to verifiable textual spans rather than hallucinated content.

### Mechanism 2: Multi-level Contrastive Learning Enforces Cross-granular Alignment
Jointly optimizing document-level alignment (via metadata) and role-level discrimination (via OMRC components) produces embeddings that balance thematic coherence with fine-grained structural awareness. Document-level InfoNCE loss aligns papers in similar research clusters; role-level InfoNCE loss aligns identical roles across related papers while separating different roles.

### Mechanism 3: Hierarchical Retrieval with Role-aware Re-ranking Refines Precision
Two-stage retrieval (coarse metadata matching → fine-grained role similarity scoring) improves top-k precision over single-stage retrieval by leveraging both global thematic and local methodological alignment. Initial retrieval uses metadata embeddings to select top-K candidates; re-ranking computes a composite score blending document-level and role-level similarities.

## Foundational Learning

- **Contrastive Learning (InfoNCE):**
  - Why needed here: Core representation learning uses InfoNCE loss at two levels (document and role). Understanding positive/negative sampling, temperature τ, and batch size is essential for debugging convergence.
  - Quick check question: Given a batch of 16 papers, can you identify which pairs would serve as positives vs. negatives for document-level vs. role-level contrastive objectives?

- **Evidence-constrained LLM Generation:**
  - Why needed here: Stage I relies on LLM-based QA with evidence anchors. Understanding prompt engineering and grounding mechanisms helps diagnose summarization quality issues.
  - Quick check question: If OMRC summaries for a paper lack citation to any figure or table, what does this suggest about the evidence-constrained generation pipeline?

- **Two-stage Retrieval Architecture:**
  - Why needed here: The system separates coarse retrieval from fine-grained re-ranking. Understanding vector similarity search, retrieval depth tradeoffs, and score fusion is critical for system tuning.
  - Quick check question: If Precision@10 improves but Recall@10 drops when increasing λ from 0.6 to 0.8, what aspect of the re-ranking is being over-emphasized?

## Architecture Onboarding

- **Component map:** Raw paper → QA-style OMRC extraction → Encoding + projection → Contrastive training (offline) / Embedding storage → Query encoding → Coarse retrieval → Role-aware re-ranking → Ranked recommendations

- **Critical path:** Raw paper → QA-style OMRC extraction → Encoding + projection → Contrastive training (offline) / Embedding storage → Query encoding → Coarse retrieval → Role-aware re-ranking → Ranked recommendations

- **Design tradeoffs:**
  - λ (re-ranking weight): 0.6 optimal; higher emphasizes metadata similarity, lower emphasizes role-level matching
  - α/β (contrastive balance): 0.4/0.6 optimal; higher β prioritizes role discrimination
  - K (retrieval depth) and N (candidate pool): 600/100 optimal; larger values introduce noise
  - Question template count K∈[5,8] per role with similarity threshold δ=0.85

- **Failure signatures:**
  - Low Precision@10 despite high Recall@10: Re-ranking weight λ too low, or role embeddings insufficiently discriminative
  - High cross-lingual performance gap: mBERT encoder not adequately fine-tuned on target language domain
  - OMRC summaries factually inconsistent or missing evidence anchors: LLM generation pipeline not properly constrained by section markers
  - Training divergence: Temperature τ too low (over-confident negatives) or positive sampling strategy flawed

- **First 3 experiments:**
  1. Summarization quality audit: Manually inspect 20-30 OMRC summaries for factual consistency and evidence anchor presence; compare against single-pass baselines to validate gains.
  2. Ablation replication: Remove each module (QA-Sum, MCL, Re-rank) one at a time on a held-out subset to confirm contribution magnitudes match Table 3; prioritize MCL given largest drop.
  3. Hyperparameter sensitivity sweep: Vary λ∈{0.3, 0.4, 0.5, 0.6, 0.7}, α/β∈{(0.3,0.7), (0.4,0.6), (0.5,0.5), (0.6,0.4)}, and K∈{400, 600, 800} on a validation split to identify dataset-specific optima.

## Open Questions the Paper Calls Out

- **Open Question 1:** How would extending the rhetorical schema beyond the standard four OMRC components affect retrieval precision in specialized scientific domains? The authors state future work will explore enhanced discourse decomposition beyond OMRC to strengthen scientific knowledge understanding.

- **Open Question 2:** To what extent can adaptive contrastive objectives improve the modeling of heterogeneous document elements compared to the current static weighting approach? The paper identifies adaptive contrastive objectives for heterogeneous document elements as a specific direction for future work.

- **Open Question 3:** How robust is the recommendation pipeline to the propagation of factual errors or hallucinations from the initial LLM-based summarization stage? While using "evidence-constrained" prompts, the paper does not quantify how residual summarization errors impact final embedding quality or ranking accuracy.

## Limitations

- The OMRC summarization pipeline's factual consistency and error propagation remain incompletely characterized, with limited analysis of generation failures.
- The positive sampling strategy for contrastive learning (same research cluster) is not fully specified, potentially affecting generalization.
- Cross-disciplinary robustness claims are based on three datasets but don't explore edge cases like highly interdisciplinary papers or emerging research areas.

## Confidence

- **High confidence:** The ablation study showing multi-level contrastive learning provides the largest performance gain (+6.8 points Precision@10) is well-supported by Table 3 and consistent with established contrastive learning literature.
- **Medium confidence:** Cross-disciplinary robustness claims are based on three datasets with varying domains, but analysis doesn't explore edge cases.
- **Medium confidence:** The superiority of QA-style summarization over single-pass approaches is demonstrated empirically, but evaluation metrics focus on information overlap rather than factual accuracy verification.

## Next Checks

1. **Factual consistency audit:** Manually verify 50 randomly sampled OMRC summaries against source papers to quantify hallucination rates and identify systematic error patterns in the LLM generation pipeline.

2. **Cluster sampling validation:** Reproduce the contrastive learning performance with different cluster definitions (venue-based vs. semantic clustering) to test sensitivity to the positive sampling strategy.

3. **Long-tail performance analysis:** Analyze Precision@10/Recall@10 separately for papers in well-represented vs. emerging research areas to validate cross-disciplinary robustness claims beyond aggregate metrics.