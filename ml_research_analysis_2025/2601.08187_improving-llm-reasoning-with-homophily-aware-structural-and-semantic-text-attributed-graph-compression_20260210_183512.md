---
ver: rpa2
title: Improving LLM Reasoning with Homophily-aware Structural and Semantic Text-Attributed
  Graph Compression
arxiv_id: '2601.08187'
source_url: https://arxiv.org/abs/2601.08187
tags:
- graph
- nodes
- node
- target
- structural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a homophily-aware framework for text-attributed
  graph compression to improve LLM reasoning. The method performs global hierarchical
  partitioning based on structural entropy minimization to detect naturally cohesive
  communities, then applies community-variant semantic aggregation using LLMs to condense
  redundant context into concise summaries.
---

# Improving LLM Reasoning with Homophily-aware Structural and Semantic Text-Attributed Graph Compression

## Quick Facts
- arXiv ID: 2601.08187
- Source URL: https://arxiv.org/abs/2601.08187
- Reference count: 40
- Key outcome: Achieves up to 94.98% graph compression while improving accuracy by 3.06%-4.92% on 10 node-level and 7 graph-level benchmarks.

## Executive Summary
This paper proposes HS2C, a homophily-aware framework for compressing text-attributed graphs to improve LLM reasoning performance. The method performs global hierarchical partitioning guided by structural entropy minimization to detect cohesive communities, then applies community-variant semantic aggregation using LLMs to condense redundant context into concise summaries. Extensive experiments demonstrate superior compression rates and accuracy compared to baseline methods, with strong scalability across different graph types and LLM sizes.

## Method Summary
HS2C operates in three main stages: Graph Structure Enhancement (GSE) augments the original graph with KNN edges based on BERT embeddings to recover latent relationships; Hierarchical Community Partition (HCP) uses structural entropy minimization to construct a coding tree and identify homophilic communities; and Community-variant Semantics Aggregation (CSA) applies type-specific LLM prompts to compress background nodes within each community into semantic summaries. The compressed graph retains target nodes and aggregated communities, enabling efficient LLM inference while preserving task-relevant information.

## Key Results
- Achieves up to 94.98% graph compression on Instagram dataset while maintaining or improving accuracy
- Improves accuracy by 3.06%-4.92% compared to original graphs across multiple benchmarks
- Demonstrates strong performance across 10 node-level and 7 graph-level benchmarks with different LLM sizes (3B-8B)

## Why This Works (Mechanism)

### Mechanism 1: Structural Entropy-Guided Community Detection
- **Claim:** Minimizing structural entropy reveals natural homophilic communities while discarding stochastic noise in the graph topology.
- **Mechanism:** The algorithm constructs a coding tree T* that minimizes H_T(G) = -Σ (g_α/vol(G)) × log(vol(α)/vol(α⁻)), representing the uncertainty of node codewords during biased random walks. Greedy MERGE operations combine communities that maximize entropy reduction, followed by DROP operations to enforce a fixed tree height, yielding communities C_homo where intra-community connectivity is high and inter-community connectivity is low.
- **Core assumption:** Observed graph topology contains noise (missing edges, spurious connections) that obscures true community structure; homophily (similar nodes connect) holds for the target domain.
- **Evidence anchors:**
  - [abstract]: "guided by the principle of Structural Entropy minimization, we perform a global hierarchical partition that decodes the graph's essential topology. This partition identifies naturally cohesive, homophilic communities, while discarding stochastic connectivity noise."
  - [Section 4.1.2]: "SE measures the uncertainty embedded within graph topologies by performing a biased random walk throughout the entire graph... minimizing the SE... decodes an optimal coding tree T*"
  - [corpus]: BiGTex (2504.12474) confirms "text-attributed graphs present unique challenges... requiring models to capture both semantic richness... and structural dependencies"

### Mechanism 2: Context-Aware Semantic Aggregation
- **Claim:** LLM-generated summaries conditioned on target node context preserve homophilic semantics while compressing redundant background information.
- **Mechanism:** For each community c_i, background node texts are summarized via an LLM: èR⁽ⁱ⁾_b = f_LLM({R_b | v_b ∈ T_i ∩ V_bg}, {R_t | v_t ∈ T_i ∩ V_tg}; θ). Four community types (Pure Target, Specific Target, Common-Shared Target, Mixed Target) receive distinct prompt templates that instruct the LLM to extract content semantically aligned with target nodes.
- **Core assumption:** LLMs can reliably identify and extract semantically relevant content when given explicit context; semantic alignment between background and target nodes correlates with downstream task performance.
- **Evidence anchors:**
  - [abstract]: "empowering it to perform differentiated semantic aggregation based on predefined community type... compresses redundant background contexts into concise community-level consensus"
  - [Section 4.2]: "the LLM generates a condensed summary of the background nodes that is semantically consistent with the target nodes within each homophilic structure"
  - [corpus]: Related work (2511.16767) cautions that "LLMs Do Not Read Text-Attributed Graphs as Effectively as We Expected," suggesting compression quality depends heavily on prompt design and LLM capability

### Mechanism 3: KNN-Augmented Graph Enhancement
- **Claim:** Adding similarity-based edges recovers latent relationships missing from observed topology, improving community detection.
- **Mechanism:** Compute node embeddings via BERT (x_i = f_BERT(R_i; ζ)), calculate Pearson correlation similarity matrix S, add k-nearest neighbor edges E_k; optimal k_m selected by detecting plateau in 1D structural entropy H¹(Ĝ⁽ᵏ⁾).
- **Core assumption:** Observed edges are incomplete due to temporal evolution, data collection gaps, or task-specific splits; semantic similarity predicts missing structural links.
- **Evidence anchors:**
  - [Section 4.1.1]: "recover and reinforce latent relationships and mitigate the information loss arising from the incompleteness of the original graph structure"
  - [Table 4]: HS2C achieves homophily scores of 0.987 (OGBN) vs. 0.338 (Original), 0.957 (DBLP-Homo) vs. 0.244 (Original), validating improved community detection

## Foundational Learning

- **Concept: Structural Entropy (SE)**
  - Why needed here: Core mathematical foundation for the hierarchical partitioning algorithm; quantifies uncertainty in graph topology via random walk codeword lengths.
  - Quick check question: Explain why minimizing H_T(G) naturally produces communities with high internal connectivity and low external connectivity.

- **Concept: Homophily in Graphs**
  - Why needed here: The entire compression framework assumes nodes with similar attributes/labels form connected communities; preserving homophilic structure retains task-relevant information.
  - Quick check question: Given a citation network, why would homophily-based compression outperform random sampling for paper classification?

- **Concept: Text-Attributed Graphs (TAGs)**
  - Why needed here: The target data structure has both topological (edges) and semantic (text) signals that must be jointly compressed.
  - Quick check question: What two complementary signals must HS2C preserve during compression, and why does random sampling fail at both?

## Architecture Onboarding

- **Component map:** BERT encodings → Pearson similarity → KNN edges → enhanced graph → MERGE operations → coding tree → DROP operations → communities → community typing → LLM summarization → compressed graph

- **Critical path:** GSE (k selection) → HCP (coding tree quality) → CSA (summary relevance) → Reconstruction → LLM inference. Errors in coding tree construction (HCP) propagate downstream; poor community partitions cannot be fixed by better summarization.

- **Design tradeoffs:**
  - Tree height h: Lower height = higher compression but risks merging semantically distinct communities
  - KNN parameter k: Larger k = denser enhancement but introduces more noise edges
  - LLM backbone for CSA: Larger models (8B vs 3B) yield better summaries but increase latency/cost
  - Community type granularity: More types = more targeted prompts but more template maintenance

- **Failure signatures:**
  - Homophily score H_S (Table 4) remains near baseline → GSE/HCP failing to detect structure; check k selection or SE computation
  - Accuracy drops despite high compression → CSA losing task-relevant information; inspect summary quality
  - High variance across random seeds → MERGE greediness sensitive to initialization; consider deterministic ordering
  - GCR stagnates around 80%+ (Instagram Table 11) → communities too granular; increase tree height or adjust DROP threshold

- **First 3 experiments:**
  1. **GSE ablation:** Run HCP on original graph G vs. KNN-enhanced Ĝ on OGBN-ArXiv; report homophily score H_S and final accuracy to quantify enhancement benefit.
  2. **Tree height sweep:** Vary h ∈ {2, 3, 4, 5, 6} on TAPE dataset; plot GCR vs. ACC curve and identify knee point where accuracy degrades significantly.
  3. **CSA prompt validation:** Manually inspect 20 community summaries across all 4 types; score each for semantic alignment with target nodes (1-5 scale) to catch systematic prompt failures before full pipeline runs.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the HS2C framework be adapted to handle dynamic temporal graphs where node attributes and structural connectivity evolve over time?
  - Basis in paper: [explicit] The conclusion explicitly identifies the extension to temporal graphs as a key area for future work.
  - Why unresolved: The current methodology relies on static structural entropy minimization and one-time semantic aggregation, which cannot capture time-varying dependencies or concept drift.
  - What evidence would resolve it: A dynamic variant of HS2C incorporating incremental coding tree updates or time-aware attention mechanisms, validated on temporal graph benchmarks.

- **Open Question 2:** Can the compression strategy be further optimized to handle industrial-scale Text-Attributed Graphs (TAGs) while maintaining computational efficiency?
  - Basis in paper: [explicit] The authors list "investigating the scalability of HS2C to industrial-scale TAGs" as a primary goal for future research.
  - Why unresolved: While the method shows strong performance on datasets like DBLP (2M nodes), industrial graphs may contain billions of nodes, potentially straining the hierarchical partitioning and LLM inference steps.
  - What evidence would resolve it: Complexity analysis and runtime benchmarks on graphs with >10^8 nodes, potentially utilizing distributed computing or sampling approximations.

- **Open Question 3:** To what extent does the strict homophily assumption limit the framework's effectiveness on heterophilic graphs where dissimilar nodes connect?
  - Basis in paper: [inferred] The method is explicitly designed to detect "naturally cohesive, homophilic communities" and discard "stochastic connectivity noise."
  - Why unresolved: In heterophilic networks, edges connecting dissimilar nodes are often informative signals rather than noise; removing them to enforce homophily could degrade reasoning accuracy.
  - What evidence would resolve it: Ablation studies on specific heterophilic benchmarks (e.g., Chameleon, Squirrel) analyzing the retention of inter-class edges and resulting classification performance.

## Limitations

- The method's performance depends heavily on the homophily assumption, which may not hold for all real-world graph domains.
- LLM-based semantic aggregation introduces variability and potential hallucination risks that aren't fully characterized.
- The KNN enhancement mechanism lacks robust validation in the text-attributed graph compression literature.

## Confidence

**High Confidence:** The structural entropy minimization framework for community detection is mathematically well-founded, with clear algorithmic specifications for MERGE/DROP operations. The reported homophily score improvements (0.338→0.987 for OGBN) provide strong empirical validation of the coding tree construction quality.

**Medium Confidence:** The LLM-based semantic aggregation mechanism shows promise but lacks detailed ablation studies on prompt quality and summary consistency. The paper reports accuracy improvements but doesn't validate whether compressed graphs maintain semantic fidelity to original texts.

**Low Confidence:** The KNN augmentation's effectiveness for text-attributed graphs is weakly supported. While the method claims to recover latent relationships, there's minimal discussion of false positive edges or the impact of poor BERT embeddings on community detection quality.

## Next Checks

1. **Heterophily Stress Test:** Run HS2C on datasets with known low/moderate homophily (e.g., actor-movie networks, protein interaction networks) and compare against random sampling baselines. Measure accuracy degradation and homophily score changes to establish performance boundaries.

2. **Community Type Ablation:** Perform detailed analysis of the four community types by measuring compression quality and accuracy per type. Identify which community categories benefit most from compression and which show degradation, then correlate with prompt template effectiveness.

3. **Semantic Fidelity Audit:** Sample 50 compressed communities and manually evaluate summary quality against original texts using semantic similarity metrics (BERTScore, BLEU). Establish whether LLM hallucinations or over-compression occur more frequently in specific community types or datasets.