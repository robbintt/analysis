---
ver: rpa2
title: 'EchoFake: A Replay-Aware Dataset for Practical Speech Deepfake Detection'
arxiv_id: '2510.19414'
source_url: https://arxiv.org/abs/2510.19414
tags:
- speech
- echofake
- detection
- replay
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EchoFake addresses the vulnerability of existing speech deepfake
  detection systems to replay attacks by introducing a comprehensive dataset featuring
  both zero-shot TTS-generated speech and physical replay recordings under varied
  real-world conditions. The dataset includes over 120 hours of audio from more than
  13,000 speakers, capturing diverse spoofing scenarios including replayed bona fide
  and replayed fake speech.
---

# EchoFake: A Replay-Aware Dataset for Practical Speech Deepfake Detection

## Quick Facts
- **arXiv ID:** 2510.19414
- **Source URL:** https://arxiv.org/abs/2510.19414
- **Reference count:** 0
- **Primary result:** EchoFake dataset enables more robust speech deepfake detection by including replayed audio, achieving lower EERs than models trained on existing datasets

## Executive Summary
EchoFake addresses a critical vulnerability in speech deepfake detection systems: their failure to detect replayed audio. Current detection models, trained primarily on clean synthetic speech, experience severe performance degradation (dropping to 59.6% accuracy) when tested on physically replayed audio. EchoFake introduces a comprehensive dataset featuring both zero-shot TTS-generated speech and physical replay recordings under varied real-world conditions, including over 120 hours of audio from more than 13,000 speakers across diverse spoofing scenarios.

## Method Summary
The EchoFake dataset contains 81,890 utterances (126.4 hours) with four categories: bona fide (B), replayed bona fide (RB), fake (F), and replayed fake (RF). It uses CommonVoice 17.0 for genuine speech and 11 zero-shot TTS systems to generate synthetic speech. Physical replay is automated using WebRTC with 16 closed-set and 4 open-set replay configurations varying playback device, recording device, room, and distance. The dataset is split into train (39,926 utt), dev (3,973 utt), eval-closed (5,991 utt), and eval-open (32,000 utt) sets. Three baseline models (RawNet2, AASIST, Wav2Vec2+MLP) are evaluated using EER for binary detection and F1-score for four-class classification.

## Key Results
- Models trained on EchoFake achieve significantly lower average EERs (16.79-32.49%) across multiple benchmarks compared to those trained on existing datasets
- The dataset demonstrates improved generalization, with reduced performance degradation when evaluated on replayed audio
- Replayed bona fide speech proves particularly challenging to detect, highlighting the need for replay-aware training data

## Why This Works (Mechanism)

### Mechanism 1
Physical replay introduces channel distortions that mask or corrupt the spectral artifacts models use to detect synthetic speech. Replay through speakers and re-recording via microphones adds reverberation, background noise, and hardware-specific frequency responses that alter the acoustic fingerprint of both bona fide and synthetic speech, reducing the signal-to-noise ratio of discriminative cues.

### Mechanism 2
Training on diverse replay conditions improves cross-dataset generalization without sacrificing performance on conventional synthesis-only benchmarks. EchoFake's 16 closed-set and 4 open-set replay configurations expose models to a wider distribution of acoustic conditions, acting as data augmentation that reduces overfitting to specific channel characteristics.

### Mechanism 3
Replayed bona fide speech is misclassified at higher rates than direct bona fide because models conflate channel distortion with spoofing evidence. Current detectors learn to associate "clean" acoustics with bona fide speech, so when bona fide audio is replayed and acquires distortions resembling spoofing artifacts, it triggers false alarms.

## Foundational Learning

- **Concept:** Equal Error Rate (EER)
  - **Why needed here:** Primary evaluation metric across all experiments; reports the threshold where false acceptance rate equals false rejection rate
  - **Quick check question:** If a model has EER = 14.88% on EchoFake-open (AASIST), what does this mean in terms of error trade-off?

- **Concept:** Zero-shot Text-to-Speech (TTS)
  - **Why needed here:** EchoFake uses 11 zero-shot TTS systems that clone voices from reference audio, representing the modern threat model the dataset addresses
  - **Quick check question:** How does zero-shot voice cloning differ from conventional speaker-specific TTS training, and why does this matter for detection?

- **Concept:** Physical Access vs. Logical Access Attack Modes
  - **Why needed here:** ASVspoof distinguishes LA (direct synthesis injection) from PA (replay through physical channel); EchoFake extends PA by including replayed fake speech
  - **Quick check question:** Why would a replayed deepfake be harder to detect than a directly injected deepfake, even when the underlying synthesis is the same?

## Architecture Onboarding

- **Component map:**
  EchoFake Dataset -> Training Set (39,926 utt) -> Bona fide (10,000), Replayed Bona fide (9,955), Fake (10,004), Replayed Fake (9,967) -> Development Set (3,973 utt) -> Eval-Closed (5,991 utt) -> Eval-Open (32,000 utt)

- **Critical path:**
  1. Acquire CommonVoice 17.0 samples → sample bona fide
  2. Generate fake speech via 6 zero-shot TTS models using sampled text + reference audio
  3. Replay 50% of B and F through WebRTC-based automated pipeline
  4. Post-process: volume normalization (−23 LUFS), MP3 compression (64 kbps, 16 kHz)
  5. Train detector, select checkpoint on dev set, evaluate on Eval-C and Eval-O

- **Design tradeoffs:**
  - Fewer utterances (81,890) than ASVspoof 5 (1.2M) but higher speaker diversity (13,005 vs. 1,922)
  - Binary vs. 4-class classification: binary treats B as genuine and RB/F/RF as spoofed; 4-class enables fine-grained analysis but harder task
  - Open-set includes unseen TTS systems to stress-test generalization; expect 30–50% EER degradation vs. closed-set

- **Failure signatures:**
  - High RB false alarm rate: model flags replayed genuine speech as spoofed (indicates channel sensitivity)
  - Low F EER but high RF EER: model detects synthesis artifacts but cannot handle replay-corrupted fakes
  - Large closed-to-open gap (>15% EER increase): overfitting to training TTS systems or replay conditions

- **First 3 experiments:**
  1. **Baseline reproduction:** Train RawNet2, AASIST, and Wav2Vec2 on EchoFake-train; verify EER on Eval-C matches paper (≈0.27–3.95%) and identify which model class performs best on RB detection
  2. **Ablation on replay data:** Retrain models on EchoFake-train with RB/RF removed (downsampled to match size); compare EER on Eval-O and external benchmarks to quantify replay data contribution
  3. **Cross-dataset transfer test:** Train on ASVspoof 2019 LA only, evaluate on EchoFake-open; confirm the 40–50% EER degradation reported, then train on EchoFake and evaluate on ASVspoof 2021 LA/DF to verify generalization claims

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can detection frameworks effectively distinguish replayed bona fide speech from genuine utterances without relying on the absence of synthetic artifacts?
- **Basis in paper:** The authors note that Replayed Bona fide (RB) is "particularly difficult to detect" due to its lack of synthetic traces, resulting in significantly lower accuracy (e.g., 65.89% for RawNet2) compared to other classes
- **Why unresolved:** Current models appear to rely heavily on spectral artifacts present in fake or replayed-fake speech; a method to detect the physical channel noise of a replay without falsely flagging genuine channel noise is not established
- **What evidence would resolve it:** Development of a model that achieves >90% accuracy on the RB class in the open-set evaluation

### Open Question 2
- **Question:** What architectural modifications are required to improve generalization to the simultaneous novelty of unseen TTS generators and physical replay conditions?
- **Basis in paper:** The paper reports a sharp performance drop in the "open-set" evaluation (e.g., Wav2Vec2 F1 drops from 98.81% to 60.99%) where speakers, generators, and replay environments are all unseen
- **Why unresolved:** The study confirms the vulnerability but only tests existing baselines, leaving the solution for handling this combined domain shift as an open challenge
- **What evidence would resolve it:** A learning strategy or architecture that maintains single-digit EER degradation when moving from closed-set to open-set conditions

### Open Question 3
- **Question:** Can specialized physical-access anti-spoofing architectures (e.g., those modeling room impulse responses) outperform the general deepfake detection baselines provided?
- **Basis in paper:** The paper evaluates general Audio Deepfake Detection (ADD) models which fail against physical replay; however, it does not test against architectures specifically designed for the ASVspoof Physical Access (PA) task
- **Why unresolved:** The distinct nature of EchoFake (combining zero-shot TTS with physical replay) may require hybrid approaches not explored in the baseline experiments
- **What evidence would resolve it:** Benchmarking PA-specific countermeasures on EchoFake to see if they outperform the reported AASIST/RawNet2 baselines

## Limitations
- Limited architectural specifications for baseline models make exact reproduction challenging
- Specific replay configurations may not capture all real-world deployment scenarios, particularly telephony environments
- Relatively small number of baseline models tested limits generalizability of conclusions

## Confidence
- **High:** Replay attack vulnerability demonstration (supported by clear EER degradation evidence)
- **Medium:** Generalization claims (supported by cross-dataset evaluations but limited by small number of baseline models)
- **Medium:** Replay configuration coverage (real-world deployment scenarios may differ)

## Next Checks
1. Reproduce the baseline model training and evaluation pipeline using the provided dataset to verify the reported EER and F1-score results
2. Conduct ablation studies removing replay data to quantify its specific contribution to performance improvements
3. Test model performance on additional real-world replay scenarios not captured in the EchoFake dataset to assess true deployment robustness