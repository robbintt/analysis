---
ver: rpa2
title: Group Fairness in Multi-Task Reinforcement Learning
arxiv_id: '2503.07817'
source_url: https://arxiv.org/abs/2503.07817
tags:
- fairness
- group
- multi-task
- learning
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles group fairness in multi-task reinforcement learning,
  where different demographic groups should experience equitable outcomes across multiple
  tasks. The authors formulate a constrained optimization problem ensuring demographic
  parity (equal expected returns) across groups for all tasks, introducing a relaxed
  constraint with a tolerance parameter.
---

# Group Fairness in Multi-Task Reinforcement Learning

## Quick Facts
- arXiv ID: 2503.07817
- Source URL: https://arxiv.org/abs/2503.07817
- Reference count: 40
- Primary result: Multi-task RL algorithm achieving zero demographic parity violations with sublinear regret

## Executive Summary
This paper addresses group fairness in multi-task reinforcement learning by ensuring demographic parity (equal expected returns) across demographic groups for all tasks simultaneously. The authors formulate this as a constrained optimization problem with relaxed fairness constraints and introduce a model-based algorithm for finite-horizon settings that guarantees zero constraint violations with high probability. For infinite-horizon settings, they extend FOCOPS to handle multiple fairness constraints across groups and tasks. Experiments on modified RiverSwim and MuJoCo environments demonstrate the approach consistently achieves smaller fairness gaps across tasks compared to single-task baselines while maintaining comparable returns.

## Method Summary
The method formulates group fairness in multi-task RL as a constrained optimization problem ensuring demographic parity across groups for all tasks. For finite-horizon tabular settings, it uses a model-based algorithm with optimistic/pessimistic reward estimates to construct a conservative policy set guaranteeing zero fairness violations with high probability, achieving sublinear regret. For infinite-horizon continuous control settings, it extends FOCOPS to handle multiple constraints through block coordinate descent, updating each group's policy separately. The approach requires access to an initial strictly fair policy to bootstrap safe exploration and uses exploration bonuses scaled to the fairness tolerance gap.

## Key Results
- Finite-horizon algorithm achieves zero fairness violations with high probability and sublinear regret bounds
- Multi-task approach consistently outperforms single-task fairness baselines on fairness gap across all tasks
- Modified MuJoCo experiments (Ant, Hopper, Humanoid, Half-Cheetah variants) demonstrate practical effectiveness with 1:3 forward:backward task ratios

## Why This Works (Mechanism)

### Mechanism 1: Conservative Policy Set Construction via Uncertainty Bounding
- Claim: Constructing a policy set using optimistic and pessimistic reward bounds ensures zero fairness violations with high probability during training.
- Mechanism: The algorithm maintains confidence-based bounds on reward estimates. For any policy π, it computes an optimistic return using upper-bounded rewards for one group and a pessimistic return using lower-bounded rewards for another. If the optimistic fairness gap (difference between optimistic return of one group and pessimistic return of another) stays within tolerance ε, the policy is deemed "safe" and added to the feasible set Πk_F. When uncertainty is too high (no policies satisfy the bound), the algorithm falls back to a known strictly fair policy π₀.
- Core assumption: Assumption 1.1 requires access to an initial strictly fair policy π₀ with fairness gap ε₀ < ε, whose value is known. This bootstraps safe exploration.
- Evidence anchors:
  - [Section 3.3, Eq. 7-8]: Defines Πk_F and the fallback to π₀ when bounds are violated
  - [Theorem 1.1]: Proves Pr(|J(π_i) - J(π_j)| ≤ ε) ≥ 1 - δ for all episodes
  - [corpus]: Related work on single-task fairness RL (Satija et al., 2023) provides the foundational uncertainty-bounding approach this extends
- Break condition: If ε₀ is not sufficiently smaller than ε, or if the initial policy π₀ does not actually satisfy strict fairness, the fallback mechanism fails and fairness violations can occur.

### Mechanism 2: Exploration Bonus for Regret Minimization
- Claim: Adding uncertainty-proportional exploration bonuses to rewards enables sublinear regret while maintaining fairness guarantees.
- Mechanism: The algorithm augments the estimated reward with an exploration bonus: r^opt(s,a) = r̂(s,a) + αβ(s,a), where β is the confidence radius from transition uncertainty and α scales the bonus. This incentivizes visiting uncertain state-action pairs, accelerating learning while the conservative policy set ensures exploration stays within fair regions.
- Core assumption: The exploration bonus coefficient α = |S|H + 4|S|H/(ε-ε₀) is tuned specifically for the fairness threshold gap; assumes the gap (ε-ε₀) is not too small, otherwise bonus explodes.
- Evidence anchors:
  - [Section 3.3, Eq. 9]: Defines the exploration-augmented reward
  - [Theorem 1.2]: Establishes sublinear regret bound Õ(|Z|H³/(ε-ε₀)√(|S|³|A|K))
  - [corpus]: No directly comparable multi-task fairness regret analysis found in corpus; this appears novel
- Break condition: When (ε - ε₀) → 0, the exploration bonus diverges, potentially causing numerical instability or overly aggressive exploration that could destabilize training.

### Mechanism 3: Multi-Constraint FOCOPS Extension for Infinite-Horizon Setting
- Claim: Extending First-Order Constrained Optimization in Policy Space (FOCOPS) to handle multiple constraints enables multi-task fairness in continuous control settings.
- Mechanism: The infinite-horizon formulation converts fairness constraints into CMDP cost constraints. For each pair of groups (i,j) and each task m, two constraints are created: J(π_i) - J̄_j ≤ ε and J̄_j - J(π_i) ≤ ε, where J̄_j is the (fixed) estimated return of group j's current policy. FOCOPS solves this using a first-order trust-region method with Lagrange multipliers for each constraint, updating each group's policy via block coordinate descent.
- Core assumption: Block coordinate descent (updating one group's policy at a time while holding others fixed) may not find the global optimum of the joint constrained problem.
- Evidence anchors:
  - [Section 4.3, Eq. 13-15]: Formulates the per-group CMDP with fairness constraints
  - [Appendix B, Algorithm 2-3]: Details the multi-constraint FOCOPS and multi-task fairness RL procedure
  - [corpus]: Corpus contains no prior work on multi-task fairness in RL; single-task FOCOPS (Zhang et al., 2020) is the base
- Break condition: If group policies are highly coupled (one group's optimal policy depends heavily on another's), block coordinate descent may oscillate or converge to poor local optima.

## Foundational Learning

- **Constrained Markov Decision Processes (CMDPs)**
  - Why needed here: The entire approach formulates fairness as a constraint optimization problem. Understanding how cost constraints, Lagrange multipliers, and feasible policy sets interact is essential.
  - Quick check question: Can you explain why standard reward shaping cannot guarantee constraint satisfaction, whereas CMDP methods can?

- **Demographic Parity / Group Fairness**
  - Why needed here: The paper adopts demographic parity (equal expected returns across groups) as its fairness definition. Alternative fairness notions (equalized odds, individual fairness) would require different formulations.
  - Quick check question: Why does demographic parity focus on expected returns rather than per-episode outcomes, and what are the limitations of this choice?

- **Optimism-in-the-Face-of-Uncertainty (OFU) Principle**
  - Why needed here: The finite-horizon algorithm relies on optimistic/pessimistic bounds to construct safe policy sets and drive exploration. Understanding confidence bounds and their concentration is critical.
  - Quick check question: How does the confidence radius β^k(s,a) scale with visitation count, and why does this enable both safety and exploration?

## Architecture Onboarding

- **Component map**:
  - Finite-Horizon Tabular Algorithm (Algorithm 1): Empirical transition estimator → Optimistic/Pessimistic reward computation → Conservative policy set Πk → LP-based policy selection → Environment interaction → Counter update
  - Infinite-Horizon Deep RL Algorithm (Algorithms 2-3): Trajectory sampling for all groups → Return estimation J̄ for fixed groups → Constraint formulation (2M constraints per group pair) → Multi-constraint FOCOPS update → Policy/value network gradient steps

- **Critical path**:
  1. Obtain or construct initial strictly fair policy π₀ (required by Assumption 1.1)
  2. For tabular setting: verify ε₀ < ε and initialize counters
  3. For deep RL setting: initialize separate policy networks per group
  4. Main loop: estimate returns, formulate constraints, solve constrained optimization

- **Design tradeoffs**:
  - **Strict vs. relaxed fairness (ε₀ vs. ε)**: Smaller gap allows tighter guarantees but requires better initialization and causes larger exploration bonuses
  - **Joint vs. block coordinate optimization**: Joint optimization of all group policies is more principled but computationally intractable; block coordinate is practical but suboptimal
  - **Model-based vs. model-free**: Finite-horizon uses model-based uncertainty bounds (better guarantees); infinite-horizon uses model-free FOCOPS (scales to continuous spaces but weaker guarantees)

- **Failure signatures**:
  - Empty policy set Πk_F in early episodes despite fallback → π₀ initialization is invalid or ε₀ mis-specified
  - Fairness gap oscillating rather than converging → block coordinate descent instability; consider reducing learning rate or increasing constraint slack
  - Regret not improving → exploration bonus may be too small or confidence bounds too tight

- **First 3 experiments**:
  1. **RiverSwim Multi-Task Validation**: Replicate the 7-state, 2-group, 2-task experiment to verify zero fairness violations and sublinear regret; compare against single-task GFRL baseline on Task 2
  2. **Ablation on (ε - ε₀) gap**: Test how the fairness slack difference affects both regret bound tightness and practical convergence; expect slower learning with smaller gaps
  3. **HalfCheetah Variant Pair**: Pick one group pair (e.g., Original vs. HugeGravity) from Table 1 and run forward/backward tasks with 1:3 imbalance; verify MTGF achieves lower max fairness violation than IHGF alternating baseline

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- Requires access to an initial strictly fair policy π₀, which may be as hard to obtain as solving the original problem
- Infinite-horizon approach lacks formal theoretical guarantees analogous to the finite-horizon setting
- Performance highly sensitive to the fairness tolerance gap (ε-ε₀), with small gaps causing numerical instability

## Confidence
- Finite-horizon theoretical guarantees: High
- Infinite-horizon algorithm effectiveness: Medium
- Experimental results validity: Medium

## Next Checks
1. **Zero-violation guarantee test**: Implement the RiverSwim multi-task experiment and verify that MTGF maintains zero fairness violations throughout training while achieving sublinear regret, compared to single-task GFRL baseline.
2. **ε-ε₀ gap sensitivity analysis**: Systematically vary the fairness tolerance gap and measure its impact on regret bounds and practical convergence speed to quantify the tradeoff between theoretical guarantees and empirical performance.
3. **Block coordinate descent stability**: Run the HalfCheetah variant experiment with different learning rates and constraint slacks to identify conditions where policy oscillations occur, testing the robustness of the multi-task fairness approach.