---
ver: rpa2
title: 'LLaMEA-SAGE: Guiding Automated Algorithm Design with Structural Feedback from
  Explainable AI'
arxiv_id: '2601.21511'
source_url: https://arxiv.org/abs/2601.21511
tags:
- code
- algorithm
- llamea
- llamea-sage
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLaMEA-SAGE, a method that augments LLM-driven
  automated algorithm design with structural feedback derived from code analysis.
  By extracting graph-theoretic and complexity features from Abstract Syntax Trees
  (ASTs) of generated algorithms, learning a surrogate model, and using SHAP to identify
  influential code properties, the approach translates structural insights into natural-language
  mutation prompts that guide subsequent LLM-based modifications.
---

# LLaMEA-SAGE: Guiding Automated Algorithm Design with Structural Feedback from Explainable AI

## Quick Facts
- arXiv ID: 2601.21511
- Source URL: https://arxiv.org/abs/2601.21511
- Reference count: 24
- LLaMEA-SAGE achieves same or better performance than vanilla LLaMEA faster, with reduced variance across runs

## Executive Summary
LLaMEA-SAGE introduces a novel approach to automated algorithm design by augmenting LLM-driven evolution with structural feedback derived from code analysis. The method extracts graph-theoretic and complexity features from Abstract Syntax Trees of generated algorithms, uses a surrogate model to learn performance correlations, and applies SHAP to identify influential code properties. These insights are translated into natural-language mutation prompts that guide subsequent LLM-based modifications. Evaluated on black-box optimization benchmarks, LLaMEA-SAGE demonstrates improved efficiency, robustness, and performance compared to state-of-the-art baselines while maintaining low token costs.

## Method Summary
LLaMEA-SAGE operates within an evolutionary strategy framework, using an LLM to generate algorithm variants that are evaluated on optimization benchmarks. The key innovation is the structural feedback loop: generated code is parsed into ASTs, from which graph-theoretic features (node/edge counts, degree statistics, tree depth, clustering coefficients) and complexity metrics (cyclomatic complexity, token counts) are extracted. These features are used to train a surrogate model (XGBoost) that predicts performance. SHAP analysis identifies the most influential features, which are then translated into natural-language guidance prompts that steer subsequent LLM mutations toward beneficial structural changes without restricting expressivity.

## Key Results
- LLaMEA-SAGE achieves same or better performance than vanilla LLaMEA faster, with reduced variance across runs
- In large-scale comparison on MA-BBOB suite, outperforms state-of-the-art baselines including MCTS-AHD and LHNS
- Validation on unseen instances confirms strong generalization, with best discovered algorithms outperforming competition winners in higher dimensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structural code features extracted from ASTs provide useful inductive bias for LLM-driven algorithm evolution
- Mechanism: Parse generated Python code into Abstract Syntax Trees → construct directed graphs → compute graph-theoretic statistics (node/edge counts, degree statistics, tree depth, clustering coefficients) and complexity indicators (cyclomatic complexity, token counts, parameter counts). These static features are then used to learn performance correlations
- Core assumption: Static structural properties of code correlate with runtime optimization performance in ways the LLM cannot infer from fitness feedback alone
- Evidence anchors: [abstract]: "feedback constructed from graph-theoretic and complexity features extracted from the abstract syntax trees of the generated algorithms"; [section 3.1]: Feature vector x_s ∈ R^d computed from AST graph G_c = (V, E); features include degree statistics, tree depth, clustering coefficients, cyclomatic complexity
- Break condition: If code structure has no systematic relationship to optimization behavior (e.g., similar performance from syntactically diverse implementations), surrogate model learns noise and guidance becomes random

### Mechanism 2
- Claim: SHAP-based attribution on a surrogate model identifies actionable code features to increase/decrease
- Mechanism: Train gradient-boosted tree (XGBoost) surrogate ˆf(cf) ≈ fitness on archive of evaluated solutions → apply SHAP to get attribution values φ_j per feature → select feature with largest |φ_j| → sign determines "increase" or "decrease" direction
- Core assumption: The surrogate model captures meaningful feature-performance relationships; SHAP attributions generalize beyond training archive
- Evidence anchors: [abstract]: "Using explainable AI techniques, we identify features that substantially affect performance and translate them into natural-language mutation instructions"; [section 3.3]: "For each feature j, SHAP provides an attribution value φ_j quantifying its contribution to the predicted fitness"
- Break condition: If surrogate model overfits to archive (low diversity, small N), SHAP attributions reflect spurious correlations and misguide mutations

### Mechanism 3
- Claim: Natural-language guidance prompts effectively steer LLM mutations toward beneficial structural changes without constraining expressivity
- Mechanism: Translate SHAP-derived direction into prompt augmentation: "Based on archive analysis, try to <increase/decrease> the <feature> of the solution." → LLM generates modified code incorporating this guidance while retaining full creative freedom
- Core assumption: LLMs reliably interpret and act on structural guidance prompts; the mapping from "increase cyclomatic complexity" to actual code changes is learnable
- Evidence anchors: [abstract]: "translate them into natural-language mutation instructions that steer subsequent LLM-based code generation without restricting expressivity"; [section 3.4]: "This guidance does not constrain the LLM syntactically but biases the generated mutation toward structural regions that are empirically correlated with improved performance"; [section 5.1, Figure 5]: When using "refine" prompt, LLM follows guidance in majority of cases; "random new" prompt shows LLM largely ignores guidance
- Break condition: If LLM cannot reliably implement structural changes (e.g., "increase parameter count" produces unpredictable code), guidance adds noise without benefit

## Foundational Learning

- **Abstract Syntax Trees (ASTs) as structural code representation**
  - Why needed here: Core to SAGE's feature extraction pipeline; must understand how code maps to tree/graph structures and what graph metrics mean
  - Quick check question: Given a Python function with nested conditionals and loops, can you sketch its AST and identify where cyclomatic complexity would increase?

- **SHAP values for feature attribution in tree ensembles**
  - Why needed here: Explains how SAGE selects which code feature to target; requires understanding additive feature attribution and why |φ_j| matters
  - Quick check question: If a surrogate model predicts fitness from 10 code features and SHAP gives φ_3 = 0.45 while φ_7 = -0.02, which feature should guidance target and in what direction?

- **Evolutionary strategies with elitist selection (μ+λ)**
  - Why needed here: LLaMEA-SAGE operates within this evolutionary loop; understanding parent/offspring dynamics clarifies where guidance is injected
  - Quick check question: In a (4+16) ES with elitism, after generating 16 offspring from 4 parents, how many individuals survive to the next generation?

## Architecture Onboarding

- **Component map**:
  LLM (GPT-5-mini) -> AST Feature Extractor -> Archive -> Surrogate Model (XGBoost) -> SHAP Analyzer -> Guidance Translator -> LLM (for next generation)

- **Critical path**:
  1. Initialize population with LLM (no guidance yet)
  2. Evaluate → extract features → populate archive
  3. Once archive size ≥ μ: train surrogate → SHAP analysis → select guidance
  4. For each offspring: select parent → sample base prompt → augment with guidance → generate → evaluate → add to archive
  5. Elitist selection → repeat until budget exhausted

- **Design tradeoffs**:
  - Single-feature vs. multi-feature guidance: Paper uses single top-SHAP feature; multi-feature could capture interactions but increases prompt complexity
  - Archive size threshold: Training surrogate too early risks overfitting; too late delays guidance benefits (paper uses initial population size)
  - Prompt type ("refine" vs. "random new"): Refine prompts follow guidance well; random new largely ignores it—choose based on exploration needs
  - Static vs. dynamic features: Current approach uses static code features; runtime features (e.g., convergence rate) could add signal but require execution tracing

- **Failure signatures**:
  - Stagnant surrogate accuracy: If surrogate R² remains low, SHAP attributions are unreliable—check archive diversity
  - Guidance ignored by LLM: If >70% of mutations show feature change opposite to guidance, prompt formulation may need adjustment or switch from "random new" to "refine"
  - No performance improvement over vanilla: May indicate code features have weak correlation with fitness on current benchmark—inspect SHAP value distributions
  - Token explosion: If generated code becomes excessively complex (runaway "increase" guidance), add length/complexity constraints to prompts

- **First 3 experiments**:
  1. **Ablate guidance entirely**: Run vanilla LLaMEA vs. SAGE with identical seeds on SBOX-COST (5 functions, d=10); plot convergence curves and compute AUC difference. Confirm ~11 AUC improvement shown in paper.
  2. **Analyze feature-action consistency**: For 100+ mutations, compute match rate between guidance direction (increase/decrease) and actual feature change in offspring code. Stratify by prompt type to validate Figure 5 findings.
  3. **Surrogate quality check**: Train surrogate on archive after 50, 100, 150 evaluations; measure R² and top-3 SHAP feature stability across retraining. Identify minimum viable archive size for your benchmark.

## Open Questions the Paper Calls Out

- Can LLaMEA-SAGE's structural guidance mechanism effectively generalize to fundamentally different problem classes, such as combinatorial optimization or high-dimensional noisy environments? (The experiments were restricted to continuous black-box optimization benchmarks at moderate dimensionalities.)
- Does incorporating dynamic execution behavior (runtime features) improve the guidance quality compared to purely static structural analysis? (The surrogate model relies exclusively on AST features and complexity metrics, ignoring how the code actually behaves when executed.)
- Can adaptive or multi-feature guidance strategies outperform the current single-feature attribution approach? (The current implementation selects only the single feature with the largest absolute SHAP attribution for generating mutation prompts.)

## Limitations

- **Surrogate model generalization**: While SHAP attributions guide structural changes, the surrogate's predictive accuracy on out-of-distribution code may degrade when encountering novel algorithmic constructs.
- **Code-to-performance mapping stability**: The assumed correlation between static AST features and runtime optimization performance may not hold across diverse problem domains or higher-dimensional instances.
- **Guidance mechanism generalizability**: The single-top-feature SHAP approach may miss important feature interactions that multi-feature guidance could capture.

## Confidence

- **High confidence**: The empirical performance improvements over vanilla LLaMEA and competitive baselines on MA-BBOB are well-supported by statistical tests and convergence curves.
- **Medium confidence**: The claim that AST-derived features capture meaningful algorithmic properties relies on demonstrated correlations within the paper's evaluation but lacks external validation on truly novel problem classes.
- **Low confidence**: The assertion that the approach maintains "low token costs" compared to alternatives is supported by token counts but lacks comparison to token-optimized variants of competing methods.

## Next Checks

1. **Cross-domain transferability test**: Apply SAGE-discovered algorithms to a distinct optimization domain (e.g., discrete combinatorial problems or reinforcement learning) to verify whether structural guidance generalizes beyond continuous optimization benchmarks.
2. **Surrogate model ablation study**: Systematically compare guidance quality when using 1-feature vs. 3-feature SHAP combinations, and when training surrogates at different archive sizes (25, 50, 100 individuals) to identify optimal guidance timing and complexity.
3. **Code feature sensitivity analysis**: Conduct a controlled experiment where the same algorithmic logic is expressed with varying AST structures (different code styles, parameter orderings) to quantify how much structural diversity is tolerated before performance correlations break down.