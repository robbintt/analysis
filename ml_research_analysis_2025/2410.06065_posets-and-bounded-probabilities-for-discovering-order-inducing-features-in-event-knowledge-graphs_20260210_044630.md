---
ver: rpa2
title: Posets and Bounded Probabilities for Discovering Order-inducing Features in
  Event Knowledge Graphs
arxiv_id: '2410.06065'
source_url: https://arxiv.org/abs/2410.06065
tags:
- event
- order
- linear
- bound
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of automating event knowledge
  graph (EKG) discovery from uncurated event data by formulating it as a Bayesian
  model selection problem over features that induce partial orders. The core method
  uses maximum likelihood with a model prior based on feature informativeness (normalized
  Shannon entropy) to select feature sets that define df-paths in the EKG.
---

# Posets and Bounded Probabilities for Discovering Order-inducing Features in Event Knowledge Graphs

## Quick Facts
- **arXiv ID**: 2410.06065
- **Source URL**: https://arxiv.org/abs/2410.06065
- **Reference count**: 36
- **Primary result**: The paper introduces a principled probabilistic method for automating EKG discovery by formulating it as Bayesian model selection over features that induce partial orders, using bounding and pruning to make the exponential search space tractable.

## Executive Summary
This paper addresses the challenge of automating event knowledge graph (EKG) discovery from uncurated event data. The authors formulate EKG discovery as a Bayesian model selection problem over features that induce partial orders on events. The core method uses maximum likelihood with a model prior based on feature informativeness (normalized Shannon entropy) to select feature sets that define df-paths in the EKG. The approach makes the exponential search space tractable through bounding and pruning strategies based on monotonicity properties of the objective function.

## Method Summary
The method uses a branch-and-bound search over the power set of features, where each feature induces relations between events sharing values. The algorithm calculates an upper bound on the achievable score using the "most restrictive reachable poset" rather than exact linear extension counts, which would be #P-complete. The search prunes branches where the bound falls below the current best score. The objective function combines a normalized entropy prior (to penalize trivial and unnecessarily complex models) with a likelihood term based on the inverse of linear extension counts across samples.

## Key Results
- The algorithm successfully discovers models consistent with manually built EKGs on the BPIC 2017 event log within reasonable runtime
- The method identifies key entities like EventOrigin, ApplicationType, and LoanGoal as important order-inducing features
- One clear discrepancy is that the algorithm fails to select the OfferID feature, resulting in two offers being combined into one df-path

## Why This Works (Mechanism)

### Mechanism 1
Features define relations between events sharing values, combined with temporal ordering to create a poset. The set of linear extensions forms an outcome space, enabling likelihood computation as P(D|M) = 1/|E_P| under uniform distribution. Core assumption: observed sequences are uniformly sampled from linear extensions permitted by the true underlying model.

### Mechanism 2
Normalized Shannon entropy as a model prior prevents degenerate solutions and penalizes unnecessary complexity. Each feature's normalized entropy η(X) ∈ [0,1) measures informativeness, penalizing trivial features with deterministic outcomes (η≈0) and model complexity through multiplicative penalties.

### Mechanism 3
Bounds on linear extension counts, combined with antitonic pruning, make the exponential search space tractable. Two decomposition theorems enable recursive bounding without exact counts. The bounding function P(M)/|E_M*| is antitonic w.r.t. model inclusion: adding features can only reduce the upper bound on achievable scores.

## Foundational Learning

- **Concept: Partial orders and linear extensions**
  - Why needed here: Core data structure; understanding how features constrain event ordering and why counting extensions is #P-complete is essential.
  - Quick check question: Given events {a, b, c} with constraints a < b and a < c, how many linear extensions exist? (Answer: 3: abc, acb, but NOT bac since a must precede b)

- **Concept: Bayesian model comparison and posterior odds**
  - Why needed here: The objective function combines likelihood and prior; understanding why P(D) cancels in posterior odds comparisons is necessary.
  - Quick check question: If P(M_1|D)/P(M_2|D) = 2, what does this mean? (Answer: M_1 is twice as probable as M_2 given the data)

- **Concept: Branch-and-bound with monotonic bounding functions**
  - Why needed here: Understanding why antitonicity enables pruning—once a bound falls below current best, all children in that subtree can be skipped.
  - Quick check question: If bounding function b(M) ≥ f(M) for all models in subtree rooted at M, and b(M) < best_score, can we prune? (Answer: Yes, no model in subtree can exceed best_score)

## Architecture Onboarding

- **Component map**: Event table parser → Feature relation builder → Poset constructor → Bound estimator → Branch-and-bound search → Entropy prior calculator
- **Critical path**: Bound estimator is the bottleneck—every model evaluation requires poset construction + bound estimation. Focus optimization here first.
- **Design tradeoffs**:
  - Stopping condition time vs. bound precision: longer estimation improves pruning but slows per-model evaluation
  - Breadth-first vs. depth-first search: BFS with prefix-based branching preserves monotonicity properties required for antitonic pruning
  - Atomic-only vs. including derived features: derived features double search space; monotonicity properties need careful handling
- **Failure signatures**:
  - Slow convergence: Stopping condition too short; bounds never tighten. Fix: exponential backoff on revisit.
  - Degenerate models selected: Entropy prior not penalizing trivial features. Check: verify η(X) ≈ 0 for single-value features.
  - Pruning too aggressive: Bounds incorrect. Check: validate decomposition theorems implementation against hand-computed examples.
- **First 3 experiments**:
  1. Sanity check: On Tab. I data with M={{Invoice}}, manually compute g_M(D) and verify |E_P| bounds match Algorithm 1 output. Confirm pruning logic with trivial search space.
  2. Scalability test: Run on BPIC 2017 subset with increasing sample sizes (N=5,10,20,40) and poset sizes (|D|=16,32,64). Plot convergence time; identify where bound estimation becomes dominant cost.
  3. Model quality validation: Compare discovered models against hand-built EKGs from [13]. Specifically check: does algorithm recover EventOrigin, ApplicationType, LoanGoal? Investigate why OfferID was missed.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can branching rules be designed that preserve monotonicity properties while incorporating derived features (pairs of atomic features) into the model selection algorithm?
- **Open Question 2**: What quantitative metrics (e.g., graph matching, edit distance) can effectively measure the closeness between a discovered EKG model and a reference model?
- **Open Question 3**: Can discovered EKGs be evaluated without reference to handbuilt models, such as through held-out event prediction?
- **Open Question 4**: Why does the algorithm consistently fail to select the OfferID feature despite its importance in handbuilt EKGs?

## Limitations
- The entropy-based prior may inappropriately penalize features that are highly discriminative despite having skewed distributions
- The branching rules for derived features are deferred to future work, leaving the complete theoretical model incompletely specified
- Performance degrades significantly on highly connected posets where decomposition is limited

## Confidence
- **High confidence**: The core mechanism linking features to partial orders via df-paths is well-founded and empirically validated
- **Medium confidence**: The entropy prior effectively prevents degenerate solutions, though its universal applicability across domains requires further testing
- **Medium confidence**: The branch-and-bound approach with antitonic pruning makes the exponential search tractable, but scalability remains an issue for dense posets

## Next Checks
1. **Prior sensitivity analysis**: Test the algorithm with synthetic data where features have known entropy distributions to verify the prior correctly penalizes uninformative features while preserving discriminative ones.
2. **Uniformity assumption validation**: Apply the method to controlled process logs with known non-uniform ordering constraints to assess how violations of the uniformity assumption affect model selection quality.
3. **Scalability stress test**: Systematically evaluate the branch-and-bound algorithm on increasing poset sizes and densities to quantify the impact of decomposition limitations on pruning effectiveness and runtime.