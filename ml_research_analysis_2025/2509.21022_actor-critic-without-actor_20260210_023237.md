---
ver: rpa2
title: Actor-Critic without Actor
arxiv_id: '2509.21022'
source_url: https://arxiv.org/abs/2509.21022
tags:
- step
- diffusion
- critic
- learning
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Actor-Critic without Actor (ACA), a lightweight
  reinforcement learning framework that eliminates the explicit actor network and
  generates actions directly from the gradient field of a noise-level critic. By removing
  the actor, ACA avoids policy lag, reduces computational overhead, and ensures actions
  remain immediately aligned with up-to-date value estimates.
---

# Actor-Critic without Actor

## Quick Facts
- arXiv ID: 2509.21022
- Source URL: https://arxiv.org/abs/2509.21022
- Reference count: 34
- Key outcome: Achieves 10.2% higher returns than SAC and maintains 99.3% sample coverage across high-value modes in a 2D bandit task

## Executive Summary
Actor-Critic without Actor (ACA) introduces a novel lightweight reinforcement learning framework that eliminates the explicit actor network entirely. Instead of maintaining a separate policy network, ACA generates actions directly from the gradient field of a noise-level conditioned critic. This approach removes policy lag, reduces computational overhead, and ensures actions remain immediately aligned with up-to-date value estimates. The framework achieves competitive or superior performance compared to standard actor-critic and diffusion-based methods while using fewer parameters.

## Method Summary
ACA replaces the traditional actor-critic architecture by conditioning the critic on both the noised action and diffusion timestep, enabling stable gradient computation across different noise levels. The framework uses a noised action space to maintain multi-modal action distributions and employs soft clipping to ensure stable gradient updates. By removing the actor network, ACA eliminates policy lag issues and reduces the number of learnable parameters. The noise-level critic generates action gradients directly, which are then used to produce actions without requiring a separate policy network. This design simplifies the architecture while maintaining the ability to capture complex action distributions.

## Key Results
- Achieves 10.2% higher returns than SAC on average across benchmarks
- Maintains 99.3% sample coverage across high-value modes in 2D bandit tasks
- Uses fewer parameters than standard actor-critic methods while achieving superior or competitive performance

## Why This Works (Mechanism)
ACA works by eliminating the policy lag inherent in traditional actor-critic methods where the actor network can become outdated relative to the critic. By directly generating actions from the critic's gradient field, ACA ensures immediate alignment between actions and the most recent value estimates. The noise-level conditioning allows the critic to provide stable gradients across different noise scales, which is crucial for maintaining exploration and capturing multi-modal action distributions. The soft clipping mechanism prevents gradient explosion during training, ensuring stable learning dynamics.

## Foundational Learning
- **Diffusion-based RL**: Understanding how diffusion processes can be used for action generation is crucial for grasping ACA's approach. Quick check: Can you explain how diffusion timestep conditioning affects action generation stability?
- **Policy lag in actor-critic methods**: Recognizing why policy networks can become misaligned with value estimates is key to understanding ACA's motivation. Quick check: What causes policy lag in standard actor-critic architectures?
- **Gradient-based action generation**: The shift from network-based to gradient-based action generation represents a fundamental change in RL methodology. Quick check: How does gradient-based action generation differ from policy network sampling?

## Architecture Onboarding

**Component map**: State -> Noised Action + Timestep -> Noise-level Critic -> Action Gradients -> Clipped Actions

**Critical path**: The core execution path flows from the current state through noise injection and timestep conditioning to the critic, which then generates action gradients. These gradients are clipped and used to produce the final actions.

**Design tradeoffs**: ACA trades the flexibility of a separate policy network for reduced computational overhead and eliminated policy lag. The noise-level conditioning adds complexity to the critic but enables stable gradient computation across noise scales.

**Failure signatures**: Potential failure modes include gradient instability if clipping parameters are poorly tuned, exploration collapse if noise levels are insufficient, and critic overfitting if the timestep conditioning is not properly regularized.

**First experiments**:
1. Verify gradient stability across different noise levels on a simple continuous control task
2. Test multi-modality preservation in a controlled 2D action space
3. Compare parameter counts and training time against standard SAC baseline

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Theoretical analysis relies heavily on distributional assumptions that may not hold in high-dimensional control tasks
- Claims about eliminating policy lag assume perfect critic accuracy, which may not be realistic
- Experimental scope is limited to MuJoCo benchmarks without extensive testing on sparse reward or long-horizon tasks

## Confidence
- High confidence in the core algorithmic contribution and its implementation
- Medium confidence in the multi-modality preservation claims due to limited task diversity
- Medium confidence in the parameter efficiency benefits across different domains

## Next Checks
1. Test ACA's performance and parameter efficiency across diverse continuous control benchmarks beyond MuJoCo, including tasks with sparse rewards and long time horizons
2. Conduct ablation studies varying diffusion timestep sampling strategies and noise schedules to identify optimal configurations for different task types
3. Evaluate ACA's behavior in high-dimensional multi-modal action spaces to verify the claims about preserving action distribution diversity under complex dynamics