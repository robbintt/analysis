---
ver: rpa2
title: A Double-Norm Aggregated Tensor Latent Factorization Model for Temporal-Aware
  Traffic Speed Imputation
arxiv_id: '2504.17196'
source_url: https://arxiv.org/abs/2504.17196
tags:
- data
- ieee
- traffic
- yijk
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TATSI, a tensor-based method for imputing missing
  traffic speed data in intelligent transportation systems. TATSI combines L2-norm
  and smooth L1 (SL1)-norm losses in its objective function to achieve both accuracy
  and robustness against outliers in high-dimensional incomplete data.
---

# A Double-Norm Aggregated Tensor Latent Factorization Model for Temporal-Aware Traffic Speed Imputation

## Quick Facts
- arXiv ID: 2504.17196
- Source URL: https://arxiv.org/abs/2504.17196
- Authors: Jiawen Hou; Hao Wu
- Reference count: 40
- This paper proposes TATSI, a tensor-based method for imputing missing traffic speed data in intelligent transportation systems.

## Executive Summary
This paper addresses the challenge of imputing missing traffic speed data in intelligent transportation systems by proposing TATSI, a tensor-based method that combines L2-norm and smooth L1 (SL1)-norm losses in its objective function. TATSI uses a single latent factor-dependent, nonnegative, and multiplicative update (SLF-NMU) approach to perform nonnegative latent factor analysis on traffic speed tensors. Experimental results on three real-world datasets demonstrate that TATSI achieves superior imputation accuracy compared to state-of-the-art models, with 0.82%-7.74% lower RMSE and 1.22%-9.78% lower MAE than baseline methods.

## Method Summary
TATSI performs tensor factorization using CP decomposition with nonnegative constraints on the latent factors. The model uses a hybrid loss function combining L2-norm for stable gradients and smooth L1-norm for robustness against outliers. The SLF-NMU algorithm employs carefully designed multiplicative update rules that guarantee non-negativity of the factors while accelerating convergence. The method factorizes the traffic speed tensor into three factor matrices representing sensor, day, and time components, which are then combined via outer products to reconstruct the complete tensor.

## Key Results
- TATSI achieves 0.82%-7.74% lower RMSE than baseline methods across different datasets
- TATSI shows 1.22%-9.78% lower MAE compared to state-of-the-art models
- Superior performance is maintained across varying data split ratios (10%, 20%, and 50% training data)

## Why This Works (Mechanism)

### Mechanism 1: Double-Norm Loss Aggregation
Combining L2-norm and smooth L1-norm yields both accurate and robust imputation for missing traffic speed data. The SL1-norm applies quadratic loss when absolute error ≤ 1 and linear loss when error > 1, while L2-norm contributes stable gradients across all error magnitudes. This combination balances generality and robustness by allowing SL1 to dominate for large errors while L2 stabilizes small-error regions. The core assumption is that traffic speed residuals follow a mixed distribution with both small Gaussian-like noise and occasional large outliers from sensor anomalies.

### Mechanism 2: Nonnegative Latent Factor Analysis via CP Decomposition
Factorizing the traffic speed tensor into nonnegative latent factors preserves interpretability and aligns with physical constraints of speed data. The observed tensor Y ∈ R^(I×J×K) is approximated as a sum of R rank-one tensors via outer products of factor matrices S, D, T. Non-negativity constraints ensure all factors represent additive contributions to speed, preventing physically meaningless negative speed components. The core assumption is that traffic speed patterns decompose into additive latent factors that are interpretable as sensor-specific, day-specific, and time-specific contributions.

### Mechanism 3: Single Latent Factor-Dependent Multiplicative Update (SLF-NMU)
Carefully designed learning rates enable multiplicative updates that guarantee non-negativity while accelerating convergence. Standard gradient descent can produce negative factors, but SLF-NMU sets learning rates ηir = sir / denominator, transforming the additive update into a multiplicative ratio. This ensures factors remain nonnegative if initialized properly. The piecewise gradient and corresponding η handle SL1's conditional derivatives. The core assumption is that initial factor values are nonnegative and the loss landscape permits multiplicative optimization without saddle points trapping convergence.

## Foundational Learning

- **Concept: Canonical Polyadic (CP) Tensor Decomposition**
  - Why needed here: TATSI represents the 3-way traffic tensor as a sum of rank-one tensors. Understanding CP decomposition is prerequisite to comprehending how latent factors S, D, T interact to reconstruct missing entries.
  - Quick check question: Given a 3×2×2 tensor, can you manually compute its CP decomposition with R=2 rank-one components?

- **Concept: L1 vs L2 vs Smooth L1 (Huber-like) Loss Functions**
  - Why needed here: The core innovation is hybridizing SL1 and L2 losses. You must understand why L2 is sensitive to outliers (quadratic growth), why L1 is robust but non-smooth (non-differentiable at 0), and how SL1 bridges both.
  - Quick check question: For errors e = [0.5, 1.5, 5.0], compute L2, L1, and SL1 losses. Which loss penalizes the outlier (e=5) least severely?

- **Concept: Multiplicative Update Rules for Nonnegative Matrix Factorization**
  - Why needed here: SLF-NMU extends NMF's multiplicative updates to tensors. Understanding why multiplicative updates preserve non-negativity (ratio form, positive terms) is essential for debugging convergence issues.
  - Quick check question: In standard NMF with multiplicative update H ← H × (W^T V) / (W^T W H), prove that if H ≥ 0 initially and all terms are nonnegative, H remains nonnegative after one iteration.

## Architecture Onboarding

- **Component map:**
  Input: Sparse traffic speed tensor Y (sensor × day × time) with missing entries Γ
  ↓
  Initialization: Random nonnegative factor matrices S, D, T with dimension R
  ↓
  [Loop until convergence]
  ↓
  Forward pass: Compute reconstructed tensor Ŷ via CP outer products
  ↓
  Residual computation: Δijk = yijk - ŷijk for observed entries Λ only
  ↓
  Piecewise loss evaluation: Apply SL1 (|Δ|≤1 or |Δ|>1) + L2 + regularization
  ↓
  Gradient computation: Piecewise derivatives ∂ε/∂sir, ∂ε/∂djr, ∂ε/∂tkr
  ↓
  Learning rate calculation: ηir, ηjr, ηkr via SLF-NMU formulas
  ↓
  Multiplicative update: sir ← sir × (numerator/denominator), similarly for D, T
  ↓
  Convergence check: |loss_t - loss_{t-1}| < 10^-5 or iterations > 1000
  ↓
  Output: Imputed tensor Ŷ with all missing entries filled

- **Critical path:** The piecewise gradient computation (Eq. 8) and corresponding learning rate selection (Eq. 9) are the most error-prone steps. Each observed entry yijk must be classified into one of three error regimes (Δijk < -1, |Δijk| ≤ 1, Δijk > 1), and the correct gradient branch applied. Misclassification here propagates incorrect updates.

- **Design tradeoffs:**
  - **Rank R selection:** Higher R captures more complex patterns but risks overfitting and slower convergence. Paper uses R=20 universally; tuning per dataset may improve accuracy.
  - **Regularization λ:** Controls factor magnitude suppression. Too high → underfitting; too low → numerical instability. Paper tunes λ per dataset (9.77e-4 for D1/D2, 1.0 for D3).
  - **Training/validation split:** 10%/20% training sets test low-data regimes; 20%/20% tests moderate data. Trade-off between imputation accuracy and available ground truth for validation.

- **Failure signatures:**
  - Negative factor values appearing after updates → learning rate formula error or incorrect gradient sign
  - Loss increasing across iterations → learning rate too large or regularization term sign error
  - Convergence to constant imputation (all ŷijk ≈ mean(Y)) → rank R too low or initialization collapsed to near-equal values
  - Divergence to infinity → denominator in learning rate approaching zero (add small ε stabilizer)
  - Poor accuracy on outliers only → SL1 piecewise threshold incorrectly implemented (check |Δijk| vs 1 boundary)

- **First 3 experiments:**
  1. **Reproduce baseline comparison on D1.1:** Implement TATSI with R=20, λ=9.765625e-4, 10%/20%/70% split. Target RMSE ≈ 5.55 (Table 2, M1 on D1.1). Verify convergence curve matches expected behavior.
  2. **Ablation: SL1-only vs L2-only vs double-norm:** Disable one norm at a time to isolate contribution. Expect L2-only to degrade on outlier-heavy subsets; SL1-only to show slower convergence on small-error regions.
  3. **Synthetic outlier injection test:** Add 5% Gaussian noise with σ=3×std to training data. Compare TATSI vs L2-only baseline robustness. Expect TATSI's RMSE gap to widen in favor of double-norm under outlier contamination.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can an adaptive strategy be developed for the regularization hyperparameter λ to eliminate the need for manual tuning?
  - Basis in paper: The conclusion explicitly states, "developing an adaptive strategy for the hyperparameter λ presents an interesting challenge, which we plan to investigate in future work."
  - Why unresolved: The current methodology requires searching a fixed range [2^-20, 2^0] to manually set λ (e.g., 9.765625 × 10^-4), which may not be optimal or efficient for new datasets.
  - Evidence: A dynamic adjustment mechanism for λ that converges automatically or a heuristic function that sets λ based on input tensor statistics without grid search.

- **Open Question 2:** Does the SL1-norm component effectively provide robustness against non-Gaussian noise and outliers in traffic data?
  - Basis in paper: The paper claims the model achieves "robustness against outliers" by integrating the Smooth L_1-norm, but the experiments utilize standard traffic datasets with random missing masks rather than datasets explicitly corrupted by outliers or heavy noise.
  - Why unresolved: While the loss function is theoretically robust, the empirical evaluation relies on RMSE and MAE on randomly split data, which does not specifically test the model's ability to ignore extreme anomalies compared to baselines.
  - Evidence: Comparative results on datasets where a specific percentage of observed values are artificially replaced with outliers (e.g., speed values of 0 or 200) to demonstrate resistance to corruption.

- **Open Question 3:** How does TATSI perform when data is Missing Not at Random (MNAR), such as during prolonged sensor blackouts?
  - Basis in paper: The study simulates missing data using random splits (e.g., 10%:20%:70%), but acknowledges that real-world data is incomplete due to sensor failures.
  - Why unresolved: Randomly distributed missing values are easier to impute using temporal patterns than contiguous missing blocks (e.g., a sensor failing for a full day); the current evaluation does not validate performance on such structural missingness.
  - Evidence: Performance metrics (MAE/RMSE) evaluated on test sets containing contiguous missing time intervals rather than randomly scattered entries.

## Limitations
- The exact initialization strategy for latent factors is not specified, which significantly affects convergence speed and local minima in non-convex problems.
- It is unclear if input speeds were normalized before applying the SL1-norm loss, as the threshold |x| ≤ 1 may behave differently depending on input scaling.
- The exact source or download link for the "Urban road segments" (D3) dataset is not explicitly provided, potentially hindering reproduction on that specific set.

## Confidence

- **High Confidence**: The double-norm loss aggregation mechanism combining L2 and SL1 is well-defined mathematically and its theoretical benefits are sound.
- **Medium Confidence**: The nonnegative CP decomposition approach is valid but depends heavily on proper initialization and hyperparameter tuning that aren't fully specified.
- **Medium Confidence**: The SLF-NMU multiplicative update method is theoretically sound but may be sensitive to numerical instabilities in practice.

## Next Checks
1. **Reproduce baseline comparison on D1.1**: Implement TATSI with specified hyperparameters and verify RMSE ≈ 5.55 matches reported results.
2. **Ablation study**: Test SL1-only vs L2-only vs double-norm configurations to isolate each component's contribution to overall performance.
3. **Outlier robustness test**: Inject synthetic outliers into training data and compare TATSI's performance degradation against L2-only baseline to verify robustness claims.