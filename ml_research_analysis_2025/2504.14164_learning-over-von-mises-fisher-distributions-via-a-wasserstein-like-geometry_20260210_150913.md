---
ver: rpa2
title: Learning over von Mises-Fisher Distributions via a Wasserstein-like Geometry
arxiv_id: '2504.14164'
source_url: https://arxiv.org/abs/2504.14164
tags:
- distributions
- distance
- distribution
- which
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a Wasserstein-like distance metric for comparing
  von Mises-Fisher (vMF) distributions on the unit hypersphere. Motivated by optimal
  transport theory, the proposed metric decomposes the discrepancy between two vMF
  distributions into a geodesic term (capturing angular separation) and a variance-like
  term (quantifying differences in concentration).
---

# Learning over von Mises-Fisher Distributions via a Wasserstein-like Geometry

## Quick Facts
- arXiv ID: 2504.14164
- Source URL: https://arxiv.org/abs/2504.14164
- Authors: Kisung You; Dennis Shung; Mauro Giuffrè
- Reference count: 7
- Primary result: Introduces a Wasserstein-like distance metric for comparing von Mises-Fisher (vMF) distributions on the unit hypersphere, enabling principled statistical operations such as mixture model reduction.

## Executive Summary
This paper introduces a Wasserstein-like distance metric (WL) for comparing von Mises-Fisher distributions on the unit hypersphere. The metric decomposes distributional discrepancy into geodesic (angular) and variance (concentration) components, enabling principled operations like mixture model reduction. The method is demonstrated through simulated and real-world experiments, including biomedical sentence classification and image clustering, showing improved performance over standard L2-based approaches.

## Method Summary
The method constructs a Wasserstein-like distance by projecting vMF distributions onto tangent spaces and applying Gaussian approximations. The WL distance decomposes into a geodesic term (arccos²(μ₁·μ₂)) capturing angular separation and a variance-like term (derived from Bures-Wasserstein distance) quantifying concentration differences. This enables operations like mixture model reduction via weighted Fréchet means (barycenters) computed through Riemannian gradient descent. The approach is validated on synthetic vMF data and real-world datasets (medical abstracts and CIFAR-10 images) using KNN classification and clustering.

## Key Results
- The WL distance successfully separates high-concentration vMF classes in synthetic experiments while L2 merges low-concentration groups
- Classification performance improves on biomedical abstracts and CIFAR-10 when using WL distance versus cosine similarity
- Mixture model reduction via WL-barycenters preserves geometric structure better than independent re-fitting

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The proposed WL distance isolates distributional discrepancy into directional alignment and dispersion tightness, enabling more meaningful comparisons than density overlap.
- **Mechanism:** The metric decomposes the distance into arccos²(μ₁·μ₂) (geodesic transport) and a variance transport term derived from Bures-Wasserstein distance on concentration parameters.
- **Core assumption:** The manifold curvature is locally negligible enough to treat tangent space covariances as Euclidean for the variance term.
- **Evidence anchors:** Abstract states "decomposes the discrepancy... into a geodesic term... and a variance-like term"; Section 3.1 defines closed-form WL distance; related work S2WTM validates difficulty of modeling vMF priors.
- **Break condition:** Fails if applied to uniform distributions (κ → 0) where the variance term diverges, or if manifold curvature significantly distorts the tangent space approximation.

### Mechanism 2
- **Claim:** High-concentration vMF distributions can be approximated as isotropic Gaussians in the tangent space, allowing tractable transport geometry.
- **Mechanism:** Projects vMF distributions onto the tangent hyperplane orthogonal to the mean direction μ. This transforms intractable spherical integration into a standard Bures-Wasserstein distance problem on covariance matrices Σ = 1/κI.
- **Core assumption:** The data lies in a "high-concentration regime" (κ sufficiently large) such that probability mass does not wrap significantly around the sphere.
- **Evidence anchors:** Section 3.1.1 employs "Gaussian approximation on the tangent space"; Page 6 details the projection M = 1/κI_{d-1} in tangent coordinates.
- **Break condition:** Low concentration values where the "Gaussian blob" on the tangent plane no longer represents the spherical distribution accurately.

### Mechanism 3
- **Claim:** Mixture model reduction via WL-barycenters preserves the geometric support of the original distribution better than independent re-fitting.
- **Mechanism:** Uses the Riemannian structure induced by WL to compute weighted Fréchet means (barycenters). Merging components involves minimizing the sum of squared WL distances rather than likelihoods, which maintains the angular and dispersion structure of the cluster.
- **Core assumption:** The component mean directions lie within an open geodesic ball of radius π/2 to guarantee a unique barycenter.
- **Evidence anchors:** Section 3.2 Theorem 3.4 proves uniqueness of the barycenter under the π/2 radius condition; Section 4.1 describes the greedy merging algorithm.
- **Break condition:** If components are antipodal (separated by ≥ π/2), the barycenter becomes non-unique or ambiguous, potentially causing oscillation in optimization.

## Foundational Learning

- **Concept:** von Mises-Fisher (vMF) Distribution
  - **Why needed here:** This is the fundamental data model. You cannot interpret the distance metric without understanding that vMF is a probability distribution on a sphere defined by a mean direction μ and concentration κ.
  - **Quick check question:** If κ increases, does the distribution become tighter or looser around μ?

- **Concept:** Tangent Space & Geodesics
  - **Why needed here:** The core mathematical trick involves projecting spherical data onto a "flat" tangent plane to apply Euclidean tools (like Gaussian approximations).
  - **Quick check question:** What is the geodesic distance between two points x, y on a unit sphere?

- **Concept:** Wasserstein / Optimal Transport
  - **Why needed here:** The paper constructs a "Wasserstein-like" metric. Understanding that Wasserstein distance measures the "cost" of moving mass (earth mover's distance) helps explain why this metric is better for distributions than simple L2 density difference.
  - **Quick check question:** Why does Wasserstein distance often perform better than KL divergence for overlapping distributions with different supports?

## Architecture Onboarding

- **Component map:** Raw vector data (embeddings) → Normalizes to unit sphere → Parameter Estimator (fits vMF parameters μ, κ) → Distance Engine (implements WL distance) → Reduction Engine (optional: greedy or partitional clustering using WL barycenters)

- **Critical path:** The calculation of the variance transport term BW²(Σ₁, Σ₂) correctly handles the parallel transport of covariance matrices. Ensuring the orthogonal transformation Q maps tangent spaces correctly is crucial for theoretical soundness, though the final formula is simplified.

- **Design tradeoffs:**
  - **Accuracy vs. Tractability:** The method trades exact spherical integration for a closed-form Gaussian approximation. This is fast but theoretically valid only for high κ.
  - **Metric Choice:** Using WL (geometry-aware) vs. L2 (density-based). L2 is cheaper but fails to separate low-concentration distributions (Fig 3).

- **Failure signatures:**
  - **Low Concentration Collapse:** If inputs have very low κ (near uniform), the distance calculation may behave unpredictably or diverge as 1/√κ grows.
  - **Antipodal ambiguity:** If clustering tries to merge two opposing clusters (e.g., North and South poles), the barycenter optimization may fail to converge or return a trivial result.

- **First 3 experiments:**
  1. **Unit Test:** Generate synthetic vMF samples with known κ=10 (high) and κ=0.5 (low). Verify that WL separates them while L2 merges the low-concentration groups.
  2. **Ablation on Approximation:** Compare the WL distance result against a numerically integrated exact Wasserstein distance for various κ to find the "break point" where the Gaussian approximation fails.
  3. **Downstream Task:** Implement K-Nearest Neighbors on a small dataset (e.g., the biomedical abstracts) using WL vs. Cosine Similarity. Check if WL improves F1 scores by utilizing the concentration κ as a signal.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the theoretical relationship between the proposed closed-form WL metric and the exact Wasserstein distance for von Mises-Fisher distributions?
- **Basis in paper:** [explicit] The conclusion explicitly states a need for "a deeper theoretical investigation into the relationship between the proposed WL metric and the exact Wasserstein distance."
- **Why unresolved:** The WL metric is derived via a Gaussian approximation in the tangent space (high-concentration regime), rather than solving the optimal transport problem exactly on the sphere.
- **What evidence would resolve it:** Deriving bounds on the approximation error between WL and the true Wasserstein distance, or proving asymptotic convergence properties as the approximation assumptions weaken.

### Open Question 2
- **Question:** Can the WL geometry be generalized to anisotropic directional distributions, such as the Kent distribution, to handle complex covariance structures in deep embeddings?
- **Basis in paper:** [explicit] The authors identify the "extension of our approach to broader families of directional distributions, especially those exhibiting anisotropy" as a primary direction for future inquiry.
- **Why unresolved:** The current WL formulation relies on the isotropic nature of the vMF distribution to simplify the Bures-Wasserstein covariance term to a scalar difference.
- **What evidence would resolve it:** A reformulation of the variance transport term that accounts for distinct eigenvalues in the covariance matrix, validated on datasets where the paper notes isotropic models are "overly restrictive" (e.g., CIFAR-10).

### Open Question 3
- **Question:** How does the WL distance behave in low-concentration regimes where the Gaussian approximation on the tangent space breaks down?
- **Basis in paper:** [inferred] The method explicitly excludes uniform distributions (κ=0) and relies on a "high-concentration approximation." The paper notes that as κ → 0, the variance transport term diverges, limiting applicability to broad distributions.
- **Why unresolved:** The derivation assumes the distribution is concentrated enough to be approximated by a Gaussian in the tangent space; this assumption fails for small κ.
- **What evidence would resolve it:** A modification of the metric or a separate derivation that remains finite and geometrically consistent for κ ≈ 0, or empirical analysis showing the error bounds in this regime.

### Open Question 4
- **Question:** Does the barycenter remain unique for collections of vMF distributions where mean directions are distributed globally (e.g., antipodal points) rather than within a small geodesic ball?
- **Basis in paper:** [inferred] Theorem 3.4 guarantees a unique barycenter only if mean directions lie within an open geodesic ball of radius π/2. The paper does not address the existence or uniqueness of solutions outside this local convexity region.
- **Why unresolved:** Standard Riemannian optimization guarantees often fail outside local convexity neighborhoods (the "cut locus"), potentially leading to multiple local minima for barycenters on the full sphere.
- **What evidence would resolve it:** Simulation results showing convergence behavior for antipodal data clusters, or a theoretical proof of existence/uniqueness for the global minimizer on the complete manifold.

## Limitations
- The Gaussian tangent space approximation for the variance transport term remains the primary theoretical weakness, with no explicit characterization of the concentration threshold beyond which the approximation breaks down
- The method explicitly excludes uniform distributions (κ=0) and relies on a "high-concentration approximation," limiting applicability to broad distributions
- The choice of gradient descent parameters for barycenter optimization (step size 0.25, stopping threshold ε) is heuristic without convergence guarantees for the non-convex problem

## Confidence

- **High Confidence:** The closed-form expression for the geodesic term and its implementation in Equation (6). The synthetic experiments showing clear separation of high-concentration classes versus L2 merging.
- **Medium Confidence:** The overall pipeline performance on real-world datasets. While results show improvements, the lack of standardized baselines (S2WTM) and ablation studies on the tangent approximation makes it difficult to isolate the contribution of each component.
- **Low Confidence:** The theoretical justification for the tangent space approximation's validity across all concentration regimes, particularly for the low-concentration biomedical text data where κ values are likely small.

## Next Checks

1. **Approximation Error Analysis:** Implement numerical integration of the exact spherical Wasserstein distance for various κ values and compare against the proposed WL distance to empirically determine the concentration threshold where the tangent approximation introduces >5% error.

2. **Antipodal Case Robustness:** Systematically test the barycenter computation when merging nearly antipodal components (μ₁·μ₂ ≈ -1) to verify the algorithm's behavior and whether the π/2 radius condition is practically enforceable.

3. **Ablation on Parameter Estimation:** Compare the downstream classification performance using WL distance when κ is estimated via MLE versus the approximation method, to isolate whether improvements come from better geometry handling or simply more accurate concentration estimates.