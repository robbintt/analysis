---
ver: rpa2
title: 'KAIO: A Collection of More Challenging Korean Questions'
arxiv_id: '2509.14752'
source_url: https://arxiv.org/abs/2509.14752
tags:
- arxiv
- korean
- kaio
- accuracy
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KAIO is a Korean math benchmark introduced to address the saturation
  of existing Korean-language benchmarks by modern LLMs. The benchmark contains 44
  manually curated, high-difficulty problems from math contests, filtered through
  adversarial testing to ensure they remain challenging for top models.
---

# KAIO: A Collection of More Challenging Korean Questions

## Quick Facts
- arXiv ID: 2509.14752
- Source URL: https://arxiv.org/abs/2509.14752
- Reference count: 9
- Top model (GPT-5) achieves 62.8% accuracy on KAIO

## Executive Summary
KAIO introduces a Korean math benchmark designed to address the saturation of existing Korean-language benchmarks by modern LLMs. The benchmark contains 44 manually curated, high-difficulty problems from math contests, filtered through adversarial testing to ensure they remain challenging for top models. KAIO remains far from saturation, with the best model (GPT-5) achieving 62.8% accuracy, and most open models falling below 30%. To prevent contamination, KAIO will remain private until performance surpasses 80% accuracy. The dataset is designed to enable robust tracking of frontier progress in Korean mathematical reasoning.

## Method Summary
KAIO was created through a multi-stage process: 331 candidate problems were collected from math contests and olympiad sites, digitized via YOLO-based OCR, and validated by five human annotators. These candidates underwent adversarial filtering using fixed snapshots of earlier state-of-the-art models (GPT-4.1, Gemini-2.0-Pro, Qwen2.5-72B, Llama-3.1-70B), retaining only problems that consistently defeated these models. The final 44-problem set spans linear algebra, analysis, probability, optimization, and logical reasoning, with problems averaging 216 tokens in length. Evaluation uses temperature 0.9, top_p 0.95, three independent runs, and GPT-4.1 as answer extractor/normalizer.

## Key Results
- KAIO remains far from saturation: best model (GPT-5) achieves 62.8% accuracy
- Most open models score below 30% accuracy on KAIO
- Output length correlates with correctness only for GPT-5 and Qwen3-32B
- KAIO demonstrates strong discriminative power between closed-source (>50%) and open-source (<30%) models

## Why This Works (Mechanism)

### Mechanism 1
Adversarial filtering against prior SOTA models yields problems that remain unsolved by current frontier systems, preserving discriminative power. 331 candidate problems were screened using fixed snapshots of earlier state-of-the-art models, retaining only problems that consistently defeated these models across diverse architectures. This approach targets the upper difficulty boundary where progress is trackable.

### Mechanism 2
Private, held-out evaluation slows contamination and extends benchmark lifetime for tracking frontier progress. KAIO is not publicly released but served through a held-out evaluator, with public release gated at 80% accuracy. This limits exposure of problem texts to training corpora and reduces memorization-driven gains.

### Mechanism 3
Depth-over-breadth curation with long-chain math problems exposes gaps in multi-step reasoning that broad, shallow benchmarks miss once saturated. The 44 problems averaging 216 tokens stress long-chain reasoning across multiple mathematical domains, maintaining discriminative power at the performance ceiling.

## Foundational Learning

- **Benchmark saturation dynamics**: Understanding why legacy benchmarks saturate quickly is essential to contextualize KAIO's design choices. Quick check: Can you explain why an 80% accuracy threshold is used to define saturation, and what types of errors likely dominate above this threshold?

- **Contamination and memorization risks in evaluation**: KAIO explicitly addresses contamination through private serving. Quick check: What are two ways a model could achieve high accuracy on a benchmark without genuine capability gains, and how does KAIO mitigate each?

- **Long-chain reasoning and token efficiency tradeoffs**: The paper reports that output length does not generally predict correctness (except for GPT-5 and Qwen3-32B). Quick check: For most evaluated models, longer outputs did not correlate with higher accuracy. What does this suggest about the relationship between "thinking more" and "thinking correctly" on KAIO?

## Architecture Onboarding

- **Component map**: Problem collection → OCR digitization → human validation → adversarial filtering → final 44-problem set → private evaluator → periodic evaluation → release at 80% threshold

- **Critical path**: 1) Problem collection from contests/olympiad sites → OCR digitization; 2) Human validation of mathematical notation; 3) Adversarial filtering with older SOTA snapshots; 4) Final 44-problem set deployment via private evaluator; 5) Periodic evaluation; release when best model exceeds 80%

- **Design tradeoffs**: Depth vs. breadth (44 problems enable meticulous curation but limit domain coverage); Privacy vs. accessibility (private serving reduces contamination but limits reproducibility); Older SOTA filtering (reduces adversarial tailoring but may retain items solvable by newer reasoning techniques)

- **Failure signatures**: Contamination (sudden accuracy jumps across models without architectural changes); High variance (inconsistent rankings across three runs); Evaluator leakage (models generating outputs matching gold labels without coherent reasoning)

- **First 3 experiments**: 1) Baseline diagnostic: Run your model on KAIO via held-out evaluator with temperature 0.9, top_p 0.95; compare to reported clusters; 2) Ablation on reasoning length: Correlate output token count with correctness for your model; 3) Contamination check: Compare performance on KAIO vs. public Korean math benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
Can the adversarial filtering and curation methodology of KAIO be effectively generalized to non-mathematical domains or culturally nuanced tasks in other low-resource languages? The current study focuses exclusively on mathematical reasoning, which relies on universal logic. It is unclear if "model-in-the-loop" screening works for subjective topics without introducing the specific biases of the filter model into the dataset.

### Open Question 2
Why does output length positively correlate with correctness for some models on KAIO, contrary to the "overthinking" hypothesis found in recent literature? The results section notes that GPT-5 and Qwen3-32B generate significantly longer outputs for correct answers, which is "contrary to recent works (Chen et al., 2024)." It is undetermined if the increased token count reflects necessary computational steps for hard problems or indicates a specific interaction between the model's reinforcement learning and Korean language structure.

### Open Question 3
Does the use of a strong LLM (GPT-4.1) as an answer extractor artificially suppress the scores of weaker models with non-canonical answer formatting? While the authors claim the extractor is "purely mechanical," complex math answers often vary in structure. If the extractor struggles to parse the disordered outputs of weaker models, the performance gap may be exaggerated.

## Limitations
- Small problem set size (44 items) may lack statistical power and domain coverage
- Private serving limits independent verification and broader community engagement
- Adversarial filtering assumes problems defeating older SOTA will challenge newer models, which may not hold for all reasoning strategies

## Confidence

- **Benchmark creation methodology**: High confidence
- **Benchmark difficulty relative to existing suites**: High confidence
- **Contamination prevention through private serving**: Medium confidence
- **Adversarial filtering transferability**: Low confidence

## Next Checks

1. **Statistical power analysis**: Evaluate whether the 44-item benchmark provides sufficient discrimination between models by computing confidence intervals for accuracy differences and testing for significance across three evaluation runs.

2. **Contamination risk assessment**: Compare model performance distributions on KAIO versus public Korean math benchmarks to quantify the expected accuracy drop if KAIO is genuinely avoiding contamination. Analyze whether performance improvements correlate with architectural changes or training data releases.

3. **Adversarial filtering validation**: Test whether problems that defeated older SOTA models in the filtering phase maintain their difficulty against a broader set of current models not used in the original evaluation.