---
ver: rpa2
title: 'Adoption of Explainable Natural Language Processing: Perspectives from Industry
  and Academia on Practices and Challenges'
arxiv_id: '2508.09786'
source_url: https://arxiv.org/abs/2508.09786
tags:
- methods
- explainability
- explainable
- explanations
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the current state of explainable NLP in
  practice through qualitative interviews with industry practitioners and academic
  researchers. The study identifies a lack of consensus on explainability definitions
  and concepts, with practitioners favoring simplified terminology compared to researchers.
---

# Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges

## Quick Facts
- arXiv ID: 2508.09786
- Source URL: https://arxiv.org/abs/2508.09786
- Reference count: 14
- Most common explainability methods: SHAP and LIME

## Executive Summary
This paper investigates the current state of explainable NLP in practice through qualitative interviews with industry practitioners and academic researchers. The study identifies a lack of consensus on explainability definitions and concepts, with practitioners favoring simplified terminology compared to researchers. SHAP and LIME methods are the most commonly used explainability approaches in NLP applications, but satisfaction levels are low (73% of interviewees expressed dissatisfaction). The research highlights the need for clear definitions, improved evaluation frameworks, and stakeholder-specific explanations to enhance adoption of explainable NLP methods in real-world applications.

## Method Summary
The study employed semi-structured interviews with 15 participants (10 industry practitioners, 5 academic researchers) from 9 countries, each with 2-10 years of experience in NLP and explainability. Interviews were conducted via Zoom between December 2023 and September 2024, recorded and transcribed using otter.ai. Thematic analysis was performed using Reflexive Thematic Analysis (RTA) following Braun and Clarke (2019), with two researchers present per interview for observer triangulation and theoretical saturation as the stopping criterion.

## Key Results
- 73% of interviewees expressed dissatisfaction with current explainability methods
- SHAP and LIME are the most commonly used explainability approaches in NLP applications
- Faithfulness and simplicity are the most desired properties of explanations
- Lack of consensus on explainability definitions and concepts between practitioners and researchers

## Why This Works (Mechanism)
The study works by capturing practitioner perspectives on what makes explanations useful in practice, rather than relying solely on academic metrics. By interviewing both industry practitioners and academic researchers, the research identifies the gap between theoretical explanations and practical needs. The thematic analysis reveals that current methods fail to meet user expectations because they prioritize technical faithfulness over stakeholder-specific needs and comprehensibility.

## Foundational Learning
- **Stakeholder-specific explanations**: Different audiences need different explanation formats - technical users need detailed feature importance while end-users need natural language summaries. Why needed: The study shows practitioners create different explanations for developers vs. end-users. Quick check: Compare user satisfaction scores for generic vs. stakeholder-tailored explanations.
- **Faithfulness vs. simplicity tradeoff**: More faithful explanations tend to be more complex and harder to understand. Why needed: The study identifies both as highly desired but conflicting properties. Quick check: Measure explanation complexity (e.g., token count) against user comprehension scores.
- **Post-hoc explanation methods**: Techniques like SHAP and LIME that explain black-box models after training. Why needed: These are the most commonly used methods according to the study. Quick check: Implement SHAP and LIME on the same model and compare their explanations for consistency.

## Architecture Onboarding

- **Component map**:
    1. NLP Model (Black Box) -> Explanation Generator (SHAP/LIME) -> Explanation Renderer -> Evaluation & Monitoring Module

- **Critical path**:
    The primary bottleneck is the Explanation Generator, which can be computationally expensive and may produce low-quality or unfaithful results if not properly configured. The Explanation Renderer is the second critical point; a poorly rendered explanation breaks adoption by failing to meet the "simplicity" requirement.

- **Design tradeoffs**:
    - Faithfulness vs. Comprehensibility (Simplicity): A perfectly faithful explanation of a complex model may be incomprehensible
    - Performance vs. Explainability: Generating explanations in real-time adds latency
    - Automation vs. Human-in-the-Loop: Fully automated explanation pipelines are scalable but may produce nonsensical results for edge cases

- **Failure signatures**:
    - Contradictory Explanations: Different methods give conflicting importance scores for the same prediction
    - Fragility to Perturbation: Slight changes in input text cause drastic changes in explanation
    - User Disregard: Users consistently ignore or override the system's explanations
    - Performance Degradation: Explanation generation causes unacceptable latency

- **First 3 experiments**:
    1. Method Comparison & Trust Test: Generate explanations using both SHAP and LIME for fixed dataset, conduct user study on trustworthiness
    2. Faithfulness Stress Test: Perturb input texts that don't change predictions, measure explanation stability
    3. Stakeholder Rendering Prototype: Build prototypes with raw weights vs. natural language summary, gather feedback on actionability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the adoption of explanation techniques change when practitioners transition from traditional NLP models to Large Language Models (LLMs)?
- Basis in paper: [explicit] The authors state they "plan to study how practitioners engage with LLMs" and anticipate "reduced reliance on traditional post-hoc methods such as SHAP and LIME."
- Why unresolved: The current study focused on established methods like SHAP/LIME used for smaller models; LLMs introduce computational constraints that make these traditional methods infeasible.
- What evidence would resolve it: A longitudinal study or new interview series specifically analyzing the toolbox of practitioners working with LLMs to see which new techniques have replaced perturbation-based methods.

### Open Question 2
- Question: What are the specific explanation requirements and satisfaction criteria for distinct stakeholder groups (e.g., end-users vs. developers) in high-stakes domains?
- Basis in paper: [explicit] The authors suggest future research should focus on "specific stakeholder subgroups in high-stakes domains... to first identify their expectations and needs."
- Why unresolved: The study identified a gap between technical experts and consumers but lacks granular data necessary to design explanations for specific non-technical audiences.
- What evidence would resolve it: Qualitative data from end-users in sectors like healthcare or finance identifying which explanation properties they prioritize for trust-building.

### Open Question 3
- Question: Can standardized benchmarks and human-annotated explanation datasets effectively resolve the challenges practitioners face in evaluating explanation quality?
- Basis in paper: [explicit] The authors identify "evaluating the quality and effectiveness of explanations" as a key challenge and propose "construction of standardized... benchmarks."
- Why unresolved: While proposed as a solution, the paper notes the current lack of consensus on metrics and the high cost of human annotation, leaving the efficacy of these specific solutions unverified in practice.
- What evidence would resolve it: The development and industry adoption of a standardized evaluation framework that correlates high benchmark scores with practitioner satisfaction ratings.

## Limitations
- Small sample size of 15 interviews limits generalizability across all NLP domains and regions
- Interview transcripts cannot be fully published due to anonymization constraints, preventing independent verification
- 73% dissatisfaction rate is based on subjective self-reporting rather than objective performance metrics

## Confidence
- High confidence: Practitioners use simplified terminology vs. academic definitions; SHAP/LIME are dominant methods; faithfulness and simplicity are key desired properties
- Medium confidence: 73% dissatisfaction rate; thematic findings about challenges and solutions
- Low confidence: Specific prevalence rates of different explainability methods; relative importance of different properties across stakeholders

## Next Checks
1. Conduct a follow-up quantitative survey with a larger sample (n>50) to validate the dissatisfaction rate and method preferences
2. Implement a controlled experiment comparing SHAP vs LIME explanations for the same NLP model to measure faithfulness and user comprehension differences
3. Design a user study testing whether stakeholder-specific explanations (developer vs end-user) actually improve trust and decision-making compared to generic explanations