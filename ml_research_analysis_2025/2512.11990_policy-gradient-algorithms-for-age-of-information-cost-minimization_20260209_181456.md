---
ver: rpa2
title: Policy Gradient Algorithms for Age-of-Information Cost Minimization
arxiv_id: '2512.11990'
source_url: https://arxiv.org/abs/2512.11990
tags:
- cost
- policy
- algorithm
- function
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents two policy gradient algorithms for minimizing
  age-of-information (AoI) cost in IoT systems under a generate-at-will model. The
  algorithms learn online without prior knowledge of transmission delay distributions
  or cost functions, making them applicable to broad scenarios.
---

# Policy Gradient Algorithms for Age-of-Information Cost Minimization

## Quick Facts
- arXiv ID: 2512.11990
- Source URL: https://arxiv.org/abs/2512.11990
- Reference count: 34
- Primary result: Two policy gradient algorithms achieve AoI cost within 3% of optimal, outperforming state-of-the-art methods in cost, applicability, or computational efficiency.

## Executive Summary
This paper presents two policy gradient algorithms for minimizing age-of-information (AoI) cost in IoT systems using a generate-at-will model. The algorithms learn online without prior knowledge of transmission delay distributions or cost functions, making them broadly applicable. The work addresses two update strategies: waiting before sending new updates and discarding ongoing transmissions exceeding a threshold. Both algorithms handle continuous state and action spaces and can be combined to implement multiple strategies simultaneously. Experimental results show the algorithms achieve time-average cost within 3% of optimal values when computable, outperforming state-of-the-art methods in either lower cost, broader applicability, or computational efficiency (at least one order of magnitude faster).

## Method Summary
The method employs policy gradient techniques within model-free reinforcement learning, using average-reward formulation rather than discounted return to handle continuing problems. For the wait strategy, a vanilla policy gradient algorithm updates parameters using differential return without requiring a state-value function. For the discard strategy, an actor-critic algorithm is used since the next state depends on the action. Both algorithms use parameterized stochastic policies based on transformed lognormal distributions mapped to bounded continuous action spaces, with Fourier cosine basis functions for function approximation. The algorithms estimate average cost online as accumulated cost divided by elapsed time, enabling learning without prior knowledge of delay distributions.

## Key Results
- Algorithms achieve time-average AoI cost within 3% of optimal values when computable
- Combined wait and discard strategy achieves lower cost than either strategy alone
- Algorithm 1 (wait) outperforms Benchmark 2 at high channel correlation (ρ > 0.5)
- Computational efficiency is at least one order of magnitude faster than optimal offline methods

## Why This Works (Mechanism)

### Mechanism 1: Average-Reward Policy Gradient with Differential Return
The algorithm minimizes long-term time-average AoI cost using an average-reward formulation rather than discounted return, enabling online learning without knowledge of delay distributions or cost functions. Instead of discounted cumulative reward, the algorithm uses differential return G_i = lim_{n→∞} Σ(R_{i+j} - r_{i+j}(π)), where the average reward r_i(π) = -β(π)(D_i - D_{i-1}) is estimated online as C/D (accumulated cost divided by elapsed time). Policy parameters update via θ_{i+1} = θ_i + α_θ δ_i ∇log π(A_i|Y_{i-1}, θ_i), where δ_i captures the deviation from average performance. Core assumption: The cost rate C_i/(D_i - D_{i-1}) is upper-bounded and transmission delays form a stationary, ergodic Markov chain with finite mean.

### Mechanism 2: Transformed Lognormal Distribution for Bounded Continuous Actions
Parameterized stochastic policies using transformed lognormal distributions enable learning in continuous action spaces while maintaining bounded outputs and supporting deterministic policy approximation. The policy π(X|Y,θ) maps state Y to a probability distribution over action X in [X_min, X_max] via X = X_max - (X_max - X_min)/(1 + X'), where log X' ~ N(μ, σ²). The mean μ(Y,θ) = θ^T f(Y) uses Fourier cosine basis functions. As learning progresses and σ is small, the distribution can approximate deterministic policies. Core assumption: The optimal policy is approximately stationary and deterministic (proven for wait strategy with AoI penalty cost and F=0).

### Mechanism 3: Wait and Discard Strategies with Conditional State-Value Requirement
The wait strategy requires only policy gradient (no state-value function) because the next state doesn't depend on the action, while discard strategy requires actor-critic because the next state depends on when transmissions are discarded. For wait strategy, δ_i = -C_i + β_i(π)(D_i - D_{i-1}) (simplified from TD error) since Y_i doesn't depend on Z_i. For discard strategy, δ_i = -C_i + β_i(π)(D_i - D_{i-1}) + ν̂(Y_i, ω) - ν̂(Y_{i-1}, ω), requiring learned state-value function ν̂ to account for state-transition dependency on action X_i. Core assumption: Wait strategy: Y_i is independent of Z_i. Discard strategy: Action X_i affects both the number of transmissions k_i and which packet gets delivered.

## Foundational Learning

- **Concept**: Age of Information (AoI) metric
  - Why needed here: The core optimization target. AoI Δ(t) = t - max{R_i : D_i ≤ t} measures time elapsed since the currently-held information was generated.
  - Quick check question: If a packet generated at t=2 is delivered at t=5, and no other packets arrive, what is AoI at t=7? (Answer: 5)

- **Concept**: Average-reward vs discounted reinforcement learning
  - Why needed here: The paper uses average-reward formulation because AoI minimization is a continuing (non-episodic) problem with no natural discount horizon.
  - Quick check question: Why would discounting harm AoI optimization? (Hint: Think about what happens to future freshness costs under high discount factors.)

- **Concept**: Policy gradient theorem
  - Why needed here: Understanding why ∇log π(A|s,θ) appears in the update rule. The theorem shows that ∇J(θ) ∝ E[∇log π(A|S,θ) · G].
  - Quick check question: In the update θ ← θ + α·δ·∇log π(A|s,θ), what happens to the update magnitude when the policy becomes very confident (π→1 for one action)?

## Architecture Onboarding

- **Component map**: 
  - State Y_{i-1} → Fourier basis → Policy Network → Action A_i
  - State Y_{i-1} → Fourier basis → State-Value Network (discard/combined only) → Value estimate
  - Action A_i + State Y_i → Cost C_u → TD error δ → Parameter updates
  - Accumulated cost C + Elapsed time D → Average cost estimate

- **Critical path**:
  1. Observe Y_{i-1} (previous transmission delay)
  2. Sample action A_i ~ π(·|Y_{i-1}, θ) (Z_i for wait, X_i for discard)
  3. Execute action, observe Y_i and cost C_u
  4. Compute δ_i and update parameters
  5. Update running average C ← C + k_i·F + C_u

- **Design tradeoffs**:
  - **σ (exploration variance)**: Lower σ → faster convergence to deterministic policy but risk premature convergence. Paper uses σ=0.5.
  - **Fourier basis dimension d**: Higher d → better policy approximation but more samples needed. Paper uses d=n=10.
  - **Learning rate α_θ**: Paper uses 10^{-4}. Too high causes instability; too low causes slow convergence.

- **Failure signatures**:
  - Oscillating cost with no convergence → learning rate too high
  - Policy collapses to Z=0 or X=X_max → exploration insufficient (increase σ) or reward scaling wrong
  - Divergent parameters θ → Fourier basis may be overfitting rare Y values; consider Y_max adjustment

- **First 3 experiments**:
  1. **Baseline replication (identity penalty, ρ=0.5)**: Run Algorithm 1 with log-normal Markov delays, identity penalty p(t)=t, F=0. Compare learned Z(Y) against Figure 9 optimal policy. Target: cost within 3% of optimal.
  2. **Ablation on correlation ρ**: Vary ρ ∈ {0.1, 0.3, 0.5, 0.7, 0.9}. Verify that Algorithm 1 outperforms Benchmark 2 at high correlation (per Figure 8).
  3. **Combined strategy test**: Run Algorithm 3 with F=4, γ=1.5. Verify that combined wait+discard achieves lower cost than either strategy alone (per Figure 16).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does a stationary and deterministic optimal policy exist for the discard strategy, and what are its characteristics?
- Basis: [explicit] Section III-B states, "We have no theoretical evidence, not even for the AoI penalty cost, of the existence of an optimal policy which solves (14), nor the characteristics of this policy."
- Why unresolved: The mathematical optimization structure for the discard strategy differs significantly from the wait strategy, preventing the application of existing optimality proofs found in prior literature.
- What evidence would resolve it: A formal proof establishing the existence or non-existence of an optimal stationary policy for the discard strategy under specific delay distributions.

### Open Question 2
- Question: Can rigorous convergence guarantees be derived for the proposed average-reward policy gradient algorithms with function approximation?
- Basis: [inferred] The paper demonstrates convergence empirically through simulations (Section V) but does not provide a theoretical analysis proving the stability of the learning updates.
- Why unresolved: Establishing convergence for actor-critic methods with linear function approximation in continuous, average-reward settings is mathematically complex and was not addressed in this work.
- What evidence would resolve it: A theoretical derivation specifying the conditions (e.g., learning rates, feature selection) under which the algorithm is guaranteed to converge to a local optimum.

### Open Question 3
- Question: How does the algorithm scale to multi-source environments involving interference or scheduling constraints?
- Basis: [inferred] The paper focuses exclusively on a single source-destination pair, while the related work (e.g., [8], [11]) highlights the distinct challenges of multi-source AoI minimization.
- Why unresolved: Adding multiple sources transforms the state and action spaces from scalar to vector/matrix forms, potentially increasing computational complexity non-linearly.
- What evidence would resolve it: An extension of the proposed algorithms to a multi-agent or multi-source simulation environment with an analysis of convergence speed and computational cost.

## Limitations

- The transformed lognormal distribution assumption may fail for multimodal optimal policies, limiting applicability to scenarios requiring discrete action choices
- Stationary ergodic assumption for transmission delays is critical but may not hold in dynamic wireless environments with time-varying conditions
- The paper doesn't address policy stability under non-stationary conditions or provide convergence guarantees for combined strategies
- Exact hyperparameter values (Y_max, action bounds, critic learning rate) are unspecified, affecting reproducibility

## Confidence

- **High confidence**: Core policy gradient framework and algorithm mechanics are well-specified and theoretically grounded
- **Medium confidence**: Performance claims relative to benchmarks are credible but depend on unreported implementation details
- **Low confidence**: Generalization to non-stationary environments and scenarios requiring multimodal policies

## Next Checks

1. Test algorithm robustness under non-stationary channel conditions (e.g., time-varying η in the AR process) to validate the ergodic assumption
2. Evaluate performance when optimal policy requires multimodal action distributions (e.g., two distinct preferred wait times)
3. Conduct ablation studies on Fourier basis dimension d and exploration variance σ to quantify sensitivity to these hyperparameters