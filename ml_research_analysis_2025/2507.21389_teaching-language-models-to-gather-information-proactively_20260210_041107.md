---
ver: rpa2
title: Teaching Language Models To Gather Information Proactively
arxiv_id: '2507.21389'
source_url: https://arxiv.org/abs/2507.21389
tags:
- information
- questions
- proactive
- task
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces proactive information gathering as a new
  task for language models, where models must identify and strategically elicit missing
  contextual information through targeted questions. The authors design a synthetic
  conversation engine using the DOLOMITES dataset to generate partially specified
  real-world tasks with masked information, then train a Qwen-2.5-7B model using reinforcement
  learning with a reward signal that encourages questions reaching beyond the provided
  context.
---

# Teaching Language Models To Gather Information Proactively

## Quick Facts
- arXiv ID: 2507.21389
- Source URL: https://arxiv.org/abs/2507.21389
- Reference count: 7
- Primary result: Proactive information gathering trained with RLHF achieves 18% improvement over o3-mini on automatic metrics

## Executive Summary
This paper introduces proactive information gathering as a new task for language models, where models must identify and strategically elicit missing contextual information through targeted questions. The authors design a synthetic conversation engine using the DOLOMITES dataset to generate partially specified real-world tasks with masked information, then train a Qwen-2.5-7B model using reinforcement learning with a reward signal that encourages questions reaching beyond the provided context. Experiments show the trained model significantly outperforms o3-mini by 18% on automatic evaluation metrics, while human evaluation reveals a 42% preference for clarification questions and 28% preference for final outlines generated by their approach.

## Method Summary
The approach involves creating synthetic conversations from the DOLOMITES dataset, where information is deliberately masked to simulate incomplete context. A Qwen-2.5-7B model is trained using reinforcement learning with a reward function that encourages asking clarifying questions about missing information. The training pipeline includes a conversation engine that generates partially specified tasks, a reward signal that measures how effectively questions gather information beyond the initial context, and iterative refinement through RLHF. The model learns to balance asking appropriate clarification questions while avoiding unnecessary queries, ultimately producing more accurate and contextually appropriate responses.

## Key Results
- Trained model outperforms o3-mini by 18% on automatic evaluation metrics (RPF, U-Score)
- Human evaluation shows 42% preference for clarification questions generated by the proposed method
- Human evaluation shows 28% preference for final outlines generated by the trained model
- Model demonstrates ability to function as collaborative thought partner rather than passive responder

## Why This Works (Mechanism)
The approach works by explicitly training language models to recognize and address information gaps rather than assuming complete context. By using synthetic data with masked information, the model learns to identify what's missing and formulate appropriate questions. The reinforcement learning reward signal reinforces this behavior by measuring how effectively questions gather information beyond the initial context. This creates a feedback loop where the model becomes increasingly skilled at determining when clarification is needed and how to ask for it effectively.

## Foundational Learning
- **Reinforcement Learning from Human Feedback (RLHF)**: Used to train the model to ask effective clarification questions; needed because standard supervised learning doesn't capture the nuanced decision-making required for proactive information gathering; quick check: model asks fewer unnecessary questions over training iterations
- **Synthetic Data Generation**: DOLOMITES dataset is used to create conversations with masked information; needed because real-world datasets with incomplete information are scarce; quick check: generated conversations maintain coherence despite masked information
- **Automatic Evaluation Metrics (RPF, U-Score)**: Used to measure question quality and information gain; needed because manual evaluation is expensive and slow; quick check: metrics correlate with human judgment in validation studies

## Architecture Onboarding
Component Map: DOLOMITES Dataset -> Synthetic Conversation Engine -> Masked Information Generator -> Qwen-2.5-7B Model -> RLHF Training Loop -> Reward Function -> Output

Critical Path: Synthetic conversation generation → Question formulation → Information gathering → Final response generation

Design Tradeoffs: Synthetic vs. real-world data (controlled vs. authentic), model size (computational cost vs. performance), question frequency (completeness vs. user fatigue)

Failure Signatures: Asking irrelevant questions, failing to identify clear information gaps, over-questioning leading to user fatigue, missing critical information despite opportunities

First Experiments:
1. Baseline comparison without RLHF training
2. A/B test with different reward function formulations
3. Cross-domain validation on held-out task types

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic training data may not fully capture real-world information-gathering complexity
- Evaluation relies heavily on automatic metrics that may not fully capture question quality
- Human evaluation sample sizes are relatively limited and may have selection bias

## Confidence
High confidence: The core methodology of using synthetic data generation and RLHF for training information-gathering capabilities
Medium confidence: The comparative performance improvements against o3-mini and the effectiveness of the reward function design
Medium confidence: The human evaluation results showing preference for clarification questions and final outlines

## Next Checks
1. Conduct a longitudinal study with real users across multiple domains to validate the synthetic training approach against actual information-gathering scenarios
2. Test the trained model's performance across different model sizes and architectures to assess generalizability
3. Evaluate the model's behavior in edge cases where clarification questions may be unnecessary or counterproductive