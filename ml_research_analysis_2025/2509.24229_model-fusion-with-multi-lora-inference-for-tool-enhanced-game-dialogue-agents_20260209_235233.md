---
ver: rpa2
title: Model Fusion with Multi-LoRA Inference for Tool-Enhanced Game Dialogue Agents
arxiv_id: '2509.24229'
source_url: https://arxiv.org/abs/2509.24229
tags:
- task
- dialogue
- function
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the opdainlp team's solution for the GPU track
  of the CPDC 2025 challenge, achieving first place in Task 1 and Task 3, and second
  place in Task 2. The challenge required building an in-game conversational AI that
  adheres to character personas, aligns with the game's worldview, and supports function
  calling.
---

# Model Fusion with Multi-LoRA Inference for Tool-Enhanced Game Dialogue Agents

## Quick Facts
- arXiv ID: 2509.24229
- Source URL: https://arxiv.org/abs/2509.24229
- Reference count: 7
- Achieved first place in Task 1 and Task 3, second place in Task 2 of CPDC 2025 GPU track

## Executive Summary
This paper presents the opdainlp team's solution for the CPDC 2025 challenge, achieving first place in Task 1 and Task 3, and second place in Task 2. The challenge required building an in-game conversational AI that adheres to character personas, aligns with the game's worldview, and supports function calling. The team employed Qwen3-14B with LoRA fine-tuning and model fusion, using a base model integrated with multiple LoRA adapters during inference. Specifically, they used three distinct LoRA adapters to handle tool calling, response generation with tool call results, and response generation without tool call results, respectively. MultiLoRA inference was implemented using vLLM. To address data limitations, they synthesized additional training data using commercial LLM APIs and employed model fusion techniques. Their approach achieved an automatic score of 0.635 on Task 3, securing first place in that task and demonstrating the effectiveness of their MultiLoRA framework for tool-enhanced game dialogue agents.

## Method Summary
The team used Qwen3-14B as the base model with three specialized LoRA adapters: one for tool-calling prediction, one for dialogue generation with tool results, and one for dialogue generation without tool results. During inference, the system first attempts tool-calling; if a tool is invoked, the "with results" adapter generates the response; otherwise, the "without results" adapter is used. They trained for 3 epochs with checkpoint averaging and used synthetic data from commercial APIs (GPT-4.1, Claude-Sonnet-4, Qwen-Max) to augment limited training data. The adapters were deployed using vLLM's MultiLoRA inference capability, with input-aware context gating that selectively includes or excludes worldview, weapon info, and dialogue history based on the scenario.

## Key Results
- Achieved first place in Task 1 (task-oriented dialogue) and Task 3 (combined evaluation)
- Second place in Task 2 (context-aware dialogue) among GPU track submissions
- Automatic score of 0.635 on Task 3, demonstrating superior performance in the combined evaluation
- Tool-calling accuracy improved from 0.536 to 0.682 by adding weapon information to the input context

## Why This Works (Mechanism)

### Mechanism 1: Task-Decomposed Multi-LoRA Routing
- Claim: Routing inputs through scenario-specific LoRA adapters improves performance over a single unified adapter in low-data, multi-objective settings.
- Mechanism: Three specialized adapters are trained separately—(1) tool-calling prediction, (2) dialogue generation with tool results, (3) dialogue generation without tool results. During inference, the system first attempts tool-calling; if a tool is invoked, the "with results" adapter generates the response; otherwise, the "without results" adapter is used.
- Core assumption: Task interference is higher than the benefit of shared representation when training data is scarce.
- Evidence anchors:
  - [abstract]: "we used three distinct LoRA adapters to handle tool calling, response generation with tool call results, and response generation without tool call results, respectively"
  - [section 4.3, p.5]: "we attempted to fine-tune a single LoRA adapter for NPC responses, shared across both cases with and without function call results. However, this approach performed worse than the non-fine-tuned Qwen3-14b model"
  - [corpus]: LRAgent (arXiv:2602.01053) provides indirect support for Multi-LoRA efficiency in multi-agent settings, but does not validate task decomposition claims for dialogue

### Mechanism 2: LoRA Checkpoint Averaging (Model Fusion)
- Claim: Averaging LoRA weights across training epochs and across synthetic data sources improves generalization over any single checkpoint.
- Mechanism: For each training run (3 epochs), LoRA checkpoints are saved per epoch and then averaged. Additionally, when training on synthetic data from multiple commercial APIs (GPT-4.1, Claude-Sonnet-4, Qwen-Max), the resulting LoRA weights from each source are further averaged.
- Core assumption: Epoch-to-epoch and source-to-source variance captures complementary generalization directions; averaging smooths overfitting to any single trajectory.
- Evidence anchors:
  - [section 3, p.3]: "After training, we performed parameter averaging over the LoRA checkpoints from each epoch, which further improved performance in our experiments"
  - [table 5, p.5]: "Average LoRA Weights" achieves 0.615 vs. best single-source (Qwen-Max SD) at 0.611
  - [corpus]: Limited direct validation in neighbors; fusion gains reported in general LLM fusion work (Kim et al., 2024) but not specifically for LoRA averaging in dialogue

### Mechanism 3: Input-Aware Context Gating
- Claim: Selectively including or excluding context fields (worldview, weapon info, dialogue history) per scenario improves both tool-calling accuracy and response relevance.
- Mechanism: For tool-calling, include dialogue history and weapon knowledge but exclude worldview/lore (reduces distraction). For dialogue with tool results, include weapon info and function results but exclude worldview. For dialogue without tool calls, include worldview/lore but exclude weapon info.
- Core assumption: Each scenario has a distinct "signal-to-noise" profile in its input; irrelevant context degrades performance more than it helps.
- Evidence anchors:
  - [section 3, p.3]: "our experimental observations showed that including redundant information such as game worldview or world background degraded model performance"
  - [table 4, p.5]: Adding weapon info during function call improves Task 1 from 0.536 to 0.682 (large gain)
  - [table 2, p.4]: Adding dialogue history improves Task 3 from 0.526 to 0.536
  - [corpus]: No direct validation in neighbors; context engineering for game dialogue remains underexplored

## Foundational Learning

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: Core technique enabling multiple specialized adapters to coexist on a single 14B base model within 48GB GPU memory constraints.
  - Quick check question: Can you explain why LoRA allows multiple adapters to share a frozen base model while only storing small rank-decomposed weight deltas?

- Concept: **vLLM Multi-LoRA Inference**
  - Why needed here: Production system must serve multiple adapters with low latency (7-second timeout per turn); vLLM provides efficient batching and KV-cache management.
  - Quick check question: How does vLLM handle concurrent requests to different LoRA adapters without reloading the base model?

- Concept: **Hermes Function-Calling Format**
  - Why needed here: Standardized prompt structure for tool-calling that the model was fine-tuned on; essential for correct function invocation.
  - Quick check question: Can you reconstruct the expected XML-style output format for a function call given the Hermes schema?

## Architecture Onboarding

- Component map:
  - Base Model: Qwen3-14B (frozen weights)
  - Adapter 1: Tool-calling LoRA (input: history + weapon info; output: function JSON)
  - Adapter 2: Dialogue-with-tools LoRA (input: function results + weapon info; output: NPC response)
  - Adapter 3: Dialogue-without-tools LoRA (input: worldview + lore; output: NPC response)
  - Router: Checks if Adapter 1 returns non-empty function call; selects Adapter 2 or 3 accordingly
  - Inference Engine: vLLM serving all adapters
  - Training Pipeline: ms-swift with DeepSpeed ZeRO-3 + FlashAttention

- Critical path:
  1. Receive dialogue turn with full context (worldview, persona, weapon info, history)
  2. Router invokes Adapter 1 with gated input (history + weapons only)
  3. If function returned: execute function, invoke Adapter 2 with results
  4. If no function: invoke Adapter 3 with worldview/lore
  5. Return NPC response within 7-second timeout

- Design tradeoffs:
  - Decoupled vs. unified adapters: Decoupled improves per-task performance but increases routing complexity and serving overhead
  - Checkpoint averaging vs. single best epoch: Averaging improves robustness but requires storing multiple checkpoints and re-running averaging after each training run
  - Synthetic data quality vs. quantity: Commercial APIs produce higher-quality synthetic data but at cost; sequential turn-by-turn generation outperformed bulk generation

- Failure signatures:
  - Tool-calling timeout: Large models (Qwen3-30B-A3B) exceeded 7-second limit on Task 2
  - Unified adapter degradation: Single adapter for both dialogue scenarios performed worse than no fine-tuning
  - Context overload: Including worldview during function-calling reduced accuracy

- First 3 experiments:
  1. Baseline routing test: Deploy base Qwen3-14B with Adapter 1 only; measure function-calling accuracy with and without dialogue history to validate context gating hypothesis
  2. Checkpoint averaging ablation: Train Adapter 3 for 3 epochs; compare single-best-epoch vs. averaged checkpoints on held-out Task 2 data
  3. Synthetic data source comparison: Generate synthetic Task 2 data using GPT-4.1, Claude-Sonnet-4, and Qwen-Max separately; train individual LoRAs and compare before/after averaging weights

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the negative interference observed when merging tool-calling and dialogue datasets persist in high-resource training regimes, or is it an artifact of the low-sample setting?
- Basis in paper: [explicit] The authors state, "We adopted this approach because we found that merging datasets from different scenarios (e.g., tool calling and dialogue) affected performance in this low-sample setting."
- Why unresolved: The paper demonstrates the efficacy of separate adapters only within the constraints of the competition's limited dataset (40 conversations). It is not tested whether a single unified model would eventually outperform the MultiLoRA approach if trained on thousands of examples.
- What evidence would resolve it: A comparison of single-adapter vs. MultiLoRA performance curves as the training dataset size is scaled up by orders of magnitude.

### Open Question 2
- Question: Why does the inclusion of game worldview and background information degrade tool-calling performance, and does this apply to other LLM architectures?
- Basis in paper: [explicit] "Our experiments showed that including redundant information such as game worldview or world background degraded model performance, while incorporating dialogue history improved the model’s ability..."
- Why unresolved: The paper identifies that "redundant" context hurts performance but does not isolate whether this is due to the specific Qwen3-14B attention mechanism, the noise level of the specific text provided, or a general failure of LLMs to filter irrelevant context during function calling.
- What evidence would resolve it: An ablation study measuring tool-calling accuracy across different model families (e.g., Llama, Mistral) while systematically varying the amount of irrelevant background text included in the prompt.

### Open Question 3
- Question: Is the performance gain from averaging LoRA weights derived from different synthetic data sources dependent on the diversity of the teacher models?
- Basis in paper: [inferred] Table 5 shows that averaging LoRA weights trained on data from GPT-4.1, Claude-Sonnet-4, and Qwen-Max (Score: 0.615) outperformed using data from any single commercial API (Best single: 0.611).
- Why unresolved: While the fusion strategy worked, the paper does not determine if the improvement is due to the diversity of reasoning styles in the synthetic data or simply the result of effectively increasing the total dataset size by combining them.
- What evidence would resolve it: A comparison of "model fusion" (averaging weights from diverse teachers) against "data pooling" (training a single adapter on the combined dataset from all teachers) to isolate the benefit of weight fusion.

## Limitations

- Data Availability: The CPDC 2025 challenge dataset is not publicly accessible, making exact reproduction impossible without access to competition organizers.
- Synthetic Data Quality: The paper claims sequential turn-by-turn generation outperformed bulk generation, but provides no quantitative comparison or ablation study.
- Multi-LoRA Serving Complexity: The paper doesn't detail the exact routing logic implementation or measure the overhead introduced by managing three concurrent adapters.

## Confidence

- High Confidence: The basic Multi-LoRA architecture with task-decomposed adapters is technically sound and supported by direct experimental evidence in the paper (Section 4.3).
- Medium Confidence: The checkpoint averaging technique shows promise in Table 5, but the improvement is modest (0.611 to 0.615) and lacks ablation studies.
- Low Confidence: The context gating strategy relies heavily on qualitative observations rather than systematic ablation studies.

## Next Checks

1. Ablation on Synthetic Data Sources: Train three separate LoRA adapters using synthetic data from only one commercial API each (GPT-4.1, Claude-Sonnet-4, Qwen-Max), then compare their individual performance against the averaged model to quantify fusion benefits.

2. Unified vs. Decoupled Adapter Benchmark: Systematically compare the three-adapter approach against a single unified adapter trained on combined data, controlling for training steps and data distribution to validate the decomposition hypothesis.

3. Context Gating Sensitivity Analysis: Implement a systematic grid search over context inclusion/exclusion combinations for each scenario, measuring both tool-calling accuracy and response quality to identify optimal gating strategies beyond the authors' empirical observations.