---
ver: rpa2
title: 'Context Matters: Comparison of commercial large language tools in veterinary
  medicine'
arxiv_id: '2510.01224'
source_url: https://arxiv.org/abs/2510.01224
tags:
- clinical
- product
- veterinary
- across
- commercial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study compared three commercially available LLM-powered summarization
  tools in veterinary medicine using a rubric-guided LLM-as-a-judge framework. Product
  1 (Hachiko), a veterinary-specific model, achieved the highest performance with
  a median average score of 4.61 (IQR: 0.73), compared to 2.55 (IQR: 0.78) for Product
  2 and 2.45 (IQR: 0.92) for Product 3.'
---

# Context Matters: Comparison of commercial large language tools in veterinary medicine

## Quick Facts
- arXiv ID: 2510.01224
- Source URL: https://arxiv.org/abs/2510.01224
- Reference count: 1
- Primary result: Veterinary-specific LLM summarization tool (Product 1) significantly outperformed general-purpose alternatives in clinical accuracy and consistency

## Executive Summary
This study compared three commercial LLM-powered veterinary summarization tools using a rubric-guided LLM-as-a-judge framework. The veterinary-specific model (Product 1) achieved the highest performance with a median average score of 4.61 (IQR: 0.73), receiving perfect scores in factual accuracy and chronological order. The evaluation method demonstrated high reproducibility, with standard deviations of 0.015 (Product 1), 0.088 (Product 2), and 0.034 (Product 3) across triplicate runs. These results highlight the importance of domain-specific training for clinical NLP applications and validate the LLM-as-a-judge methodology as a scalable evaluation approach for veterinary medicine.

## Method Summary
The study evaluated three commercial LLM summarization tools using 42 de-identified veterinary oncology records and a rubric-guided LLM-as-a-judge framework. Gemini 2.5 Pro served as the automated grader, assessing summaries across five weighted criteria (Factual Accuracy: 2.5, Completeness: 1.2, Chronological Order: 1.0, Clinical Relevance: 1.5, Organization: 0.8) using a temperature of 0.1, 16,384-token reasoning budget, and JSON schema validation. Each summary received three independent evaluations to measure reproducibility, with median scores and IQRs reported. The veterinary-specific pipeline (Product 1) was compared against two general-purpose models with veterinary wrappers (Products 2-3).

## Key Results
- Product 1 (veterinary-specific) achieved highest median average score of 4.61 (IQR: 0.73)
- Product 1 received perfect median scores in Factual Accuracy and Chronological Order
- Reproducibility demonstrated by low standard deviations: 0.015 (Product 1), 0.088 (Product 2), 0.034 (Product 3)
- Significant performance gap between veterinary-specific and general-purpose models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific training improves factual accuracy and consistency in veterinary summarization tasks.
- Mechanism: Veterinary-specific pipelines reduce hallucinations by aligning model representations with specialized terminology, disease patterns, and clinical entity relationships (dates, diagnoses, treatments) that general-purpose LLMs may misinterpret without targeted training or robust RAG integration.
- Core assumption: The observed performance gap stems primarily from domain-specific training/pipeline differences rather than unreported architectural advantages, prompt optimization, or evaluator alignment with Product 1's output style.
- Evidence anchors:
  - [abstract] "Product 1 (Hachiko), a veterinary-specific model, achieved the highest performance... received perfect median scores in Factual Accuracy and Chronological Order"
  - [section] "Product 1: A proprietary veterinary-specific language model pipeline developed in-house with domain-specific training on veterinary medical data"
  - [corpus] Limited direct corpus support; PetBERT reference (ref 19) suggests domain-specific pretraining benefits veterinary coding tasks, but doesn't validate the specific mechanism here.
- Break condition: If Products 2-3 also use veterinary-specific training but were misclassified; if the LLM judge systematically favors writing styles that match Product 1's training distribution, observed gains reflect evaluator bias rather than true accuracy improvements.

### Mechanism 2
- Claim: LLM-as-a-Judge with structured rubrics and low temperature produces reproducible evaluations across repeated runs.
- Mechanism: Explicit scoring criteria with weights, JSON schema validation, and a large reasoning budget (16,384 tokens) force systematic evaluation before scoring, while temperature=0.1 reduces stochasticity—yielding stable assessments for identical inputs.
- Core assumption: Low variance across triplicate runs indicates evaluation reliability, not consistent systematic bias; the rubric criteria capture clinically meaningful dimensions.
- Evidence anchors:
  - [abstract] "LLM grader demonstrated high reproducibility, with Average Score standard deviations of 0.015 (Product 1), 0.088 (Product 2), and 0.034 (Product 3)"
  - [section] "configured with a temperature of 0.1, JSON response formatting with Pydantic schema validation, and a reasoning budget of 16,384 tokens"
  - [corpus] Weak corpus support; no direct validation of this specific judge methodology in related papers.
- Break condition: If the judge exhibits "self-preference" bias favoring outputs similar to its own training distribution; if different LLM judges (e.g., GPT-4, Claude) produce systematically different rankings.

### Mechanism 3
- Claim: Weighted multi-criteria scoring aligns aggregate evaluation with clinical priorities.
- Mechanism: Clinician-consulted weighting (Factual Accuracy=2.5, Clinical Relevance=1.5 vs Organization=0.8) ensures high-stakes dimensions dominate the final score, reducing the influence of stylistic differences that minimally affect clinical utility.
- Core assumption: The chosen weights accurately reflect veterinary clinical priorities across contexts and specialties.
- Evidence anchors:
  - [section] "five weighted criteria that were developed in consultation with a board-certified veterinary clinician... Factual accuracy (weight: 2.5)... Organization (weight: 0.8)"
  - [abstract] "summaries were scored across five domains: Factual Accuracy, Completeness, Chronological Order, Clinical Relevance, and Organization"
  - [corpus] No corpus evidence on optimal weighting schemes for clinical NLP evaluation.
- Break condition: If different clinical contexts require different weightings; if weights were post-hoc optimized to favor Product 1.

## Foundational Learning

### Concept: LLM-as-a-Judge Evaluation
- Why needed here: The study's methodology relies on using one LLM to evaluate others, which offers scalability but has documented limitations—pattern matching vs true comprehension, potential bias toward certain output styles.
- Quick check question: Does low intra-rater variance guarantee clinical validity, or only consistency of whatever bias the judge holds?

### Concept: Domain Adaptation for Clinical NLP
- Why needed here: The central claim is that veterinary-specific training produces better outputs than general-purpose models, but the paper doesn't isolate training data from other pipeline components (prompt engineering, post-processing, RAG).
- Quick check question: What additional evidence would distinguish "domain training improved performance" from "better prompt engineering or post-processing improved performance"?

### Concept: Intra-rater vs Inter-rater Reliability
- Why needed here: The study demonstrates reproducibility (same grader, repeated runs) but explicitly acknowledges the need for human expert validation, which has not yet been performed.
- Quick check question: Why is high intra-rater reproducibility insufficient to claim the framework is clinically valid?

## Architecture Onboarding

- Component map: Input (42 veterinary oncology records) -> Summarization Layer (3 commercial platforms) -> Evaluation Layer (Gemini 2.5 Pro judge with 5-category rubric) -> Validation (triplicate runs) -> Output (median scores, IQRs)
- Critical path: Record ingestion → Platform summarization → Judge reasoning (per criterion) → Weighted aggregation → Triplicate validation → Statistical reporting
- Design tradeoffs:
  - Median vs mean: Median chosen for ordinal scales and outlier resistance—appropriate for small sample sizes
  - Large reasoning budget: Reduces snap judgments but increases cost/latency; not validated against smaller budgets
  - Single specialty (oncology): Controls domain variance but limits generalizability
  - Author affiliation: Product 1 developers conducted the evaluation—potential confirmation bias acknowledged in competing interests
- Failure signatures:
  - Judge style bias: LLM may favor outputs matching its training distribution (section: "may exhibit inherent biases... favoring certain writing styles")
  - Prompt sensitivity: Products 2-3 evaluated with single prompt; different prompts may change rankings
  - Missing human validation: No inter-rater reliability with veterinary clinicians yet established
- First 3 experiments:
  1. Human inter-rater validation: Have 3+ board-certified veterinary clinicians independently score a sample using the same rubric; calculate Cohen's kappa between human and LLM judge.
  2. Prompt optimization ablation: Re-test Products 2-3 with multiple prompt variants to isolate training effects from prompt engineering effects.
  3. Cross-specialty generalization: Expand evaluation to non-oncology records (internal medicine, surgery) to test whether Product 1's advantage transfers or is specialty-specific.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent do LLM-as-a-judge scores correlate with expert veterinary clinician evaluations?
- Basis in paper: [explicit] The authors state that future work should "validate these automated assessments against expert veterinary clinician evaluations to establish inter-rater reliability."
- Why unresolved: The study relied entirely on an automated model (Gemini 2.5 Pro) for grading, with no human ground truth or clinician review to confirm the validity of the scores.
- What evidence would resolve it: A comparative study measuring the agreement rate (e.g., Cohen's kappa) between the LLM grader and a panel of board-certified veterinary specialists.

### Open Question 2
- Question: Do the performance differences observed in oncology records generalize to other veterinary specialties?
- Basis in paper: [explicit] The authors note the evaluation was "limited to veterinary oncology records" and suggest future work should "expand beyond oncology" as this "may not fully represent the broader spectrum of veterinary specialties."
- Why unresolved: Oncology records possess specific terminology, chronological structures, and data densities that may differ significantly from general practice, surgery, or internal medicine records.
- What evidence would resolve it: Replication of the comparative framework using datasets from diverse veterinary domains (e.g., cardiology, dermatology, emergency medicine).

### Open Question 3
- Question: Does the LLM-judge fail to penalize "hallucinations" that maintain linguistic fluency but lack clinical accuracy?
- Basis in paper: [explicit] The discussion highlights the limitation that the LLM evaluation relies on "pattern matching rather than true clinical comprehension," which could lead to it "overlooking subtle but clinically significant errors."
- Why unresolved: High reproducibility scores confirm the judge is consistent, but consistency does not guarantee that the judge is correctly identifying factual fabrications versus fluent but incorrect text.
- What evidence would resolve it: An error analysis of high-scoring summaries reviewed by humans to determine if the LLM judge systematically overlooks specific types of clinical hallucinations.

## Limitations
- Evaluation relied entirely on LLM judge without human validation
- Exact mechanisms behind Product 1's superiority remain unclear (training vs. prompt engineering vs. post-processing)
- Single-oncology specialty dataset limits generalizability to broader veterinary practice

## Confidence
- Medium: Core claim that domain-specific tools perform better than general-purpose alternatives
- Low: Claims about superiority of veterinary-specific training absent prompt effects
- Low: Generalizability of results beyond oncology specialty

## Next Checks
1. Conduct blinded human expert validation with 3+ board-certified veterinary clinicians using the same rubric
2. Perform prompt optimization ablation studies on Products 2-3 to isolate training effects
3. Expand evaluation to non-oncology specialties (internal medicine, surgery) to test generalizability