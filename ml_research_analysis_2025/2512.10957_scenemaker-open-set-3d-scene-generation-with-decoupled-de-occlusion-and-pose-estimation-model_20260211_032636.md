---
ver: rpa2
title: 'SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose
  Estimation Model'
arxiv_id: '2512.10957'
source_url: https://arxiv.org/abs/2512.10957
tags:
- scene
- object
- open-set
- generation
- pose
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of open-set 3D scene generation
  from a single image, where existing methods struggle with high-quality geometry
  and accurate pose estimation under severe occlusion. The authors propose a decoupled
  framework that separately addresses three tasks: de-occlusion, 3D object generation,
  and pose estimation.'
---

# SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model

## Quick Facts
- **arXiv ID:** 2512.10957
- **Source URL:** https://arxiv.org/abs/2512.10957
- **Reference count:** 40
- **Primary result:** Proposes a decoupled framework that achieves state-of-the-art performance on open-set 3D scene generation from single images, with significant improvements in object geometry quality and pose accuracy.

## Executive Summary
This paper addresses the challenge of open-set 3D scene generation from single images, where existing methods struggle with high-quality geometry and accurate pose estimation under severe occlusion. The authors propose a decoupled framework that separately tackles de-occlusion, 3D object generation, and pose estimation. By enhancing the de-occlusion model with image datasets and a curated 10K dataset, and introducing a unified diffusion-based pose estimation model with global and local attention mechanisms, they achieve state-of-the-art performance on both indoor and open-set test sets. The method also shows strong generalization to varying object counts and potential for further improvement with richer scene information.

## Method Summary
The method decouples 3D scene generation into three tasks: de-occlusion, 3D object generation, and pose estimation. For de-occlusion, a pretrained image editing model (Flux Kontext) is finetuned on a curated 10K dataset with synthetic mask types to handle severe occlusion. For pose estimation, a unified diffusion model with global and local self-attention mechanisms is introduced, along with a 200K synthesized scene dataset to improve generalization. The framework uses Grounded-SAM for segmentation, MoGe for depth and point clouds, and an off-the-shelf image-to-3D model for object generation, followed by pose estimation to composite objects into the scene.

## Key Results
- Achieves state-of-the-art performance on both indoor and open-set test sets for 3D scene generation
- Significant improvements in object geometry quality and pose accuracy under severe occlusion
- Strong generalization to varying object counts and potential for further improvement with richer scene information

## Why This Works (Mechanism)

### Mechanism 1: Decoupled De-occlusion Leverages Image-Scale Priors
Separating de-occlusion from 3D generation and training on image datasets provides richer open-set occlusion patterns, improving geometry under severe occlusion. The authors freeze a pretrained image editing model (Flux Kontext), then finetune on a curated 10K de-occlusion dataset with three synthetic mask types. This transfers 2D inpainting priors to amodal completion without coupling to 3D geometry losses.

### Mechanism 2: Global-Local Attention Disentangles Pose Variables by Spatial Scope
A unified diffusion pose estimator with global self-attention (all objects interact), local self-attention (per-object rotation/translation/size/geometry tokens interact), and decoupled cross-attention (rotation attends only to canonical object conditions; translation/size attend to scene-level conditions) improves accuracy over monolithic attention. Each object is represented as four tokens (R, T, S, geometry), with local self-attention binding each object's internal pose variables and global self-attention enforcing coherent inter-object relationships.

### Mechanism 3: Synthetic 200K Scene Dataset Provides Open-Set Pose Priors
A large-scale synthetic dataset (200K scenes, 2-5 Objaverse objects each, 20 views/scene) fills the missing open-set prior for pose estimation, enabling generalization beyond indoor datasets. Objects are curated from Objaverse, composed with random environment maps and ground planes, and rendered with Blender Cycles to teach the pose model diverse object geometries and spatial configurations.

## Foundational Learning

- **Concept: Diffusion Models for Conditional Generation**
  - **Why needed here:** Both de-occlusion and pose estimation use diffusion (flow matching); understanding denoising objectives and conditioning is essential.
  - **Quick check question:** Can you explain how a diffusion model conditions on multi-modal inputs (image + point cloud + geometry tokens) in a DiT architecture?

- **Concept: Amodal Completion / De-occlusion**
  - **Why needed here:** The core bottleneck addressed is inferring hidden object parts from occluded views before 3D generation.
  - **Quick check question:** What is the difference between inpainting and amodal completion, and why might inpainting priors transfer to amodal tasks?

- **Concept: Attention Disentanglement in Transformers**
  - **Why needed here:** Global-local and decoupled cross-attention are non-standard; understanding token grouping and attention masks is critical for implementation.
  - **Quick check question:** How would you implement local self-attention over per-object quadruple tokens while allowing global self-attention across all objects?

## Architecture Onboarding

- **Component map:**
  - Scene Perception: Grounded-SAM (segmentation) → MoGe (depth) → point clouds
  - De-occlusion: Flux Kontext (frozen init) → finetune on 10K triplet dataset (masked image, prompt, target) → produces de-occluded images
  - 3D Object Generation: Off-the-shelf image-to-3D model (e.g., Step1X-3D) on de-occluded images → produces canonical meshes
  - Pose Estimation: DiT with GSA/LSA/GCA/LCA → predicts R, T, S per object → composites into scene

- **Critical path:**
  1. Segmentation/depth quality directly affects point cloud inputs to pose model.
  2. De-occlusion quality determines 3D geometry fidelity; severe errors propagate.
  3. Pose estimation relies on both point cloud features and canonical geometry; missing size prediction (in prior methods) caused failures in scene composition.

- **Design tradeoffs:**
  - Decoupling de-occlusion improves quality but adds inference overhead vs. end-to-end.
  - Synthetic dataset scales open-set coverage but may introduce sim-to-real gap.
  - Local vs. global attention balance: more global improves coherence but risks over-smoothing relative poses.

- **Failure signatures:**
  - **Occlusion failure:** Plausible texture but hollow or truncated 3D geometry; check de-occlusion output vs. ground truth.
  - **Pose failure:** Objects float, intersect, or misalign; check if rotation is correct but translation/size wrong → suggests cross-attention issue.
  - **Open-set generalization failure:** Good indoor, poor wild images; likely dataset coverage or domain shift.

- **First 3 experiments:**
  1. **Reproduce de-occlusion ablation:** Train Flux Kontext finetune with only one mask type at a time (object cutout, corner, brush) on 1K subset; measure PSNR/SSIM/CLIP to isolate contribution.
  2. **Attention mechanism ablation on pose:** Using ground-truth meshes (to isolate pose), train pose model with GSA-only, LSA-only, and full GSA+LSA+LCA; report CD-S and IoU-B.
  3. **Synthetic data scaling:** Train pose model on 20K, 50K, 100K, 200K synthetic scenes; evaluate on held-out open-set test to characterize data scaling curve.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can 3D scene generation frameworks be refined to ensure physical plausibility regarding object interpenetration and force interactions?
- **Basis in paper:** [explicit] The authors explicitly state in "Limitations and future work" that "real-world arrangement of objects is often much more complex... a key future research topic is how to construct or refine 3D scenes more accurately in a physically plausible manner."
- **Why unresolved:** The current method relies on synthetic datasets where bounding boxes do not intersect, but it lacks a physics engine or force-interaction modeling to handle complex stability or support relationships.
- **Evidence:** A modified framework that integrates physical simulation constraints to eliminate floating objects and validate mechanical stability in generated scenes.

### Open Question 2
- **Question:** How can the control mechanisms be expanded beyond simple captions to allow for more complex natural language interactions and localized editing?
- **Basis in paper:** [explicit] The "Limitations and future work" section notes that "existing methods can only control scene generation through images or simple captions, and further development is needed for more control signals and natural language interactions."
- **Why unresolved:** The current architecture relies on global conditioning from images or text prompts and lacks the interface for iterative, fine-grained user instructions.
- **Evidence:** A model extension that accepts multi-turn dialogue or spatial constraints (e.g., "move the chair to the left of the table") to modify specific objects without regenerating the entire scene.

### Open Question 3
- **Question:** What architectural adaptations are required to utilize generated high-quality 3D scenes effectively for embodied AI decision-making and in-depth understanding tasks?
- **Basis in paper:** [explicit] The authors conclude that "how to perform more in-depth understanding tasks and adapt embodied decision-making based on generated high-quality 3D scenes is also an unsolved challenge."
- **Why unresolved:** While the paper improves geometry and pose accuracy, it does not address the semantic gap between static scene assets and the dynamic, actionable representations needed for robotics or agent interaction.
- **Evidence:** Benchmarks demonstrating that an embodied agent trained solely on scenes generated by this method can successfully perform navigation or manipulation tasks in real-world environments.

## Limitations
- The primary limitation lies in the sim-to-real gap for the synthetic pose dataset: while the authors report improved open-set performance, the real-world distribution of occlusions, lighting, and object arrangements may differ substantially from the curated synthetic scenes.
- The 10K de-occlusion dataset, while showing strong PSNR gains, remains small relative to the diversity of open-set occlusions; further scaling may be necessary for robust generalization.
- The decoupled design adds inference overhead and may introduce error accumulation across stages.

## Confidence
- **High Confidence:** Decoupled de-occlusion mechanism (image-scale priors transfer to 3D reasoning) - supported by quantitative gains in PSNR and qualitative improvements in severe occlusion cases.
- **Medium Confidence:** Global-local attention design for pose estimation - ablative evidence shows benefit, but lacks comparison to other attention designs and direct evaluation on symmetric objects.
- **Medium Confidence:** Synthetic 200K dataset improves open-set pose generalization - reported gains are substantial, but the sim-to-real transfer is assumed rather than directly validated with wild data.

## Next Checks
1. **Wild Image Test:** Evaluate the full pipeline on in-the-wild images (e.g., CO3D, RealFusion) to measure actual open-set generalization beyond curated test sets.
2. **Symmetry Robustness Test:** Construct a test set of symmetric objects (chairs, bottles) and measure pose accuracy specifically for rotation estimation; compare local-only vs. global+local attention.
3. **De-occlusion Scaling Study:** Train de-occlusion models on 10K, 50K, and 100K curated datasets (using synthetic augmentation if needed) to characterize quality scaling and identify saturation points.