---
ver: rpa2
title: A Large-Language Model Framework for Relative Timeline Extraction from PubMed
  Case Reports
arxiv_id: '2504.12350'
source_url: https://arxiv.org/abs/2504.12350
tags:
- time
- events
- case
- temporal
- event
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a framework for extracting clinical event
  timelines from PubMed case reports using large language models (LLMs). The system
  converts case reports into structured event-time pairs, enabling temporal analytics
  like forecasting and process tracing.
---

# A Large-Language Model Framework for Relative Timeline Extraction from PubMed Case Reports

## Quick Facts
- arXiv ID: 2504.12350
- Source URL: https://arxiv.org/abs/2504.12350
- Reference count: 0
- Primary result: LLMs extract clinical event timelines from case reports with 0.80 event recall and 0.95 temporal concordance

## Executive Summary
This work introduces a framework for extracting clinical event timelines from PubMed case reports using large language models (LLMs). The system converts case reports into structured event-time pairs, enabling temporal analytics like forecasting and process tracing. Manual annotations by a clinical expert were compared with LLM outputs (GPT-4 and O1-preview) on a subset of case reports, showing moderate event recall (0.80) but high temporal concordance (0.95) among identified events. Inter-LLM agreement was also high (0.97), validating the reliability of the annotations. The approach demonstrates the potential of LLMs to recover clinical timelines from free text, with room for improvement in event completeness and timing accuracy. The study establishes a benchmark for leveraging the PubMed Open Access corpus for temporal analytics.

## Method Summary
The framework ingests PubMed Open Access case reports identified via regex filters, extracts body text between designated markers, and processes them through LLMs (GPT-4 and O1-preview) using a carefully engineered prompt that defines time zero as admission and requests relative timestamps in hours. The LLMs output structured event-time pairs which are then evaluated against manual clinical annotations using semantic embeddings (S-PubMedBert-MS-MARCO) with cosine distance thresholds. Event matching employs a recursive best-match algorithm to handle paraphrasing and avoid many-to-one mapping errors. Performance is measured through event recall, temporal concordance, and absolute time error calculations.

## Key Results
- Event recall of 0.80 for O1-preview annotations compared to manual expert annotations
- Temporal concordance of 0.95 among identified events, indicating high reliability in event ordering
- Inter-LLM agreement of 0.97 validates consistency across different LLM outputs
- Time errors increase significantly for events distant from presentation (years vs. hours)

## Why This Works (Mechanism)

### Mechanism 1: Relative Temporal Anchoring
Converting absolute or vague temporal text into standardized "hours relative to admission" format improves structural usability of clinical narratives. The framework assigns "time 0" to the admission/encounter event and uses LLM pre-trained clinical knowledge to resolve expressions like "3-day history" or "4 weeks ago" into negative hour offsets. This mechanism fails if case reports lack clear admission anchors or if LLMs confuse time directionality.

### Mechanism 2: Semantic Similarity for Event Matching
Rigid string matching fails when LLMs paraphrase clinical events, so the system relies on semantic embeddings to identify equivalent events. Using S-PubMedBert-MS-MARCO embeddings with cosine distance (threshold 0.1) allows detection of conceptual matches even with different wording. This mechanism fails if LLMs combine multiple distinct events into single summary statements, confusing the recursive matcher.

### Mechanism 3: High Concordance via Sequence Learning
While exact timestamp accuracy is noisy, LLMs demonstrate high reliability in recovering correct ordering of clinical events. The LLM likely leverages narrative flow of case reports (history -> presentation -> treatment -> outcome) to sequence events correctly, even with imprecise hour calculations. This mechanism fails in complex reports with non-linear narratives or when presentation isn't the chronological center.

## Foundational Learning

- **ISO-TimeML Specification**: Defines what constitutes an "event" (happened/holds true). Required to interpret why certain text spans were extracted and others ignored. Quick check: Does the framework treat a patient's "history of diabetes" as an event occurring at a single timestamp or an ongoing state?

- **Recursive Best Match Algorithm**: Prevents "many-to-one" mapping errors where LLM might split one expert annotation into two, or merge two into one. Quick check: If LLM outputs "fever and rash" as one event but expert lists them separately, does recursive algorithm match them as one entity or fail?

- **Concordance vs. Absolute Error**: Distinguishes between getting order right (concordance) and getting time right (absolute error). One is high (0.95), other is variable. Quick check: If event is placed at -100 hours instead of -200 hours, does it hurt concordance or absolute error?

## Architecture Onboarding

- **Component map**: Ingestion -> Extraction -> Evaluation (PubMed corpus -> LLM + Prompt -> Semantic Embedder + Recursive Matcher -> Concordance/Error Calculator)
- **Critical path**: Prompt Engineering step. Definition of "t=0" and instruction to output "hours" is the singular point of failure for timestamp accuracy.
- **Design tradeoffs**: Timestamp vs. Interval (single time points for stability vs. temporal richness), Exact vs. Semantic Matching (cosine distance allows flexibility but introduces false positives)
- **Failure signatures**: Temporal Drift (errors increase for events further from presentation), Rephrasing Drift (O1-preview paraphrases heavily), Token Truncation (GPT-4's 8k limit potentially cuts off discharge/follow-up events)
- **First 3 experiments**: 
  1. Threshold Sweep: Run matcher with cosine thresholds 0.05-0.25 to visualize precision/recall trade-off curve
  2. Time-Delta Stress Test: Feed synthetic reports with ambiguous temporal logic to measure error propagation
  3. Model Comparison on Truncation: Compare GPT-4 vs. O1-preview on events in final paragraphs to quantify information loss

## Open Questions the Paper Calls Out

- **Can the framework be adapted to extract relative time intervals rather than single time points without degrading performance?** The authors note that requesting intervals "often degraded query responses substantially" in initial tests, but suggest "future work could consider expansion of time points into intervals." This remains unresolved as current prompting strategies failed to consistently output valid interval formats compared to single timestamps.

- **Does the method maintain high temporal concordance when applied to clinical domains requiring minute or hour-level resolution?** The authors identify a limitation that "none of the 10 reports... had events at the minute or hour resolution," necessitating validation in fields like critical care. This remains unverified as the current benchmark relies on case reports with low temporal granularity.

- **Can event recall be significantly improved through techniques like LLM ensembling or active learning?** The authors report moderate recall (0.80) and suggest "LLM annotation ensembling [and] active learning-based fine tuning" as future directions to address this. The single-model approach currently misses a non-trivial portion of events listed in manual annotations.

## Limitations
- Small sample size (10 manually annotated case reports) may introduce bias
- Implementation details of recursive matching algorithm not fully specified
- Reliance on semantic embeddings for event matching may introduce false positives
- Time errors increase significantly for events distant from presentation
- Performance unverified on clinical domains requiring minute/hour-level resolution

## Confidence
- **High Confidence**: Temporal concordance results (0.95) and inter-LLM agreement (0.97) - directly measurable and consistent
- **Medium Confidence**: Event recall metrics (0.80) - based on small sample size and semantic matching thresholds
- **Low Confidence**: Claims about temporal accuracy for events distant from presentation - acknowledged as problematic but not quantified

## Next Checks
1. **Temporal Reasoning Stress Test**: Create synthetic case reports with nested temporal references to measure error propagation and validate multi-hop temporal reasoning capabilities.

2. **Embedding Threshold Analysis**: Systematically sweep cosine distance thresholds from 0.05 to 0.25 on development set to quantify precision-recall trade-off and identify optimal threshold.

3. **Narrative Structure Impact Study**: Test framework on case reports with non-linear narratives to quantify how narrative structure affects event ordering accuracy and identify failure patterns.