---
ver: rpa2
title: 'Agentic AutoSurvey: Let LLMs Survey LLMs'
arxiv_id: '2509.18661'
source_url: https://arxiv.org/abs/2509.18661
tags:
- survey
- papers
- agents
- quality
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Agentic AutoSurvey addresses automated literature survey generation
  through a multi-agent framework with specialized Paper Search, Topic Mining & Clustering,
  Academic Survey Writer, and Quality Evaluator agents. The system processes 75-443
  papers per topic (847 total across six LLM research topics), generating comprehensive
  surveys in 15-20 minutes.
---

# Agentic AutoSurvey: Let LLMs Survey LLMs

## Quick Facts
- arXiv ID: 2509.18661
- Source URL: https://arxiv.org/abs/2509.18661
- Authors: Yixin Liu; Yonghui Wu; Denghui Zhang; Lichao Sun
- Reference count: 40
- Primary result: 71% improvement in survey quality over baseline (8.18/10 vs 4.77/10)

## Executive Summary
Agentic AutoSurvey introduces a multi-agent framework for automated literature survey generation using specialized LLMs for distinct tasks. The system processes 75-443 papers per topic, generating comprehensive surveys in 15-20 minutes through coordinated Paper Search, Topic Mining & Clustering, Academic Survey Writer, and Quality Evaluator agents. The framework achieves significant quality improvements over existing approaches, with average scores of 8.18/10 compared to 4.77/10 for the baseline, demonstrating strong performance in citation coverage, synthesis, and critical analysis.

## Method Summary
The system employs a four-agent architecture where each agent specializes in a specific task: Paper Search Agent retrieves relevant papers using hybrid keyword and semantic queries, Topic Mining & Clustering Agent performs semantic clustering to identify subtopics, Academic Survey Writer Agent generates survey drafts synthesizing information across clusters, and Quality Evaluator Agent provides feedback for iterative refinement. The framework processes 847 papers across six LLM research topics, using a 12-dimensional evaluation framework to assess survey quality across dimensions including organization, synthesis, and critical analysis. The approach achieves comprehensive citation coverage (often ≥80%) through semantic clustering and cross-cluster synthesis.

## Key Results
- Achieved 71% improvement in survey quality (8.18/10 vs 4.77/10 baseline)
- Generated comprehensive surveys in 15-20 minutes across 75-443 papers per topic
- Achieved citation coverage often ≥80% on paper sets of 75-100 papers

## Why This Works (Mechanism)
The multi-agent architecture enables specialized LLMs to focus on distinct aspects of survey generation, reducing the complexity burden on any single model. Semantic clustering identifies natural groupings in the literature, enabling more coherent organization than keyword-based approaches. The iterative refinement process with Quality Evaluator feedback allows continuous improvement without manual intervention. Cross-cluster synthesis ensures comprehensive coverage by connecting related concepts across different subtopics, creating more integrated surveys than sequential processing approaches.

## Foundational Learning

**Semantic Clustering**
- Why needed: Groups related papers into coherent subtopics for organized survey structure
- Quick check: Visualize cluster distributions and verify papers within clusters share semantic themes

**Multi-Agent Orchestration**
- Why needed: Enables specialized LLMs to handle distinct tasks without overwhelming any single model
- Quick check: Trace agent outputs to ensure proper handoff and information flow between agents

**Cross-Cluster Synthesis**
- Why needed: Connects related concepts across different clusters for comprehensive coverage
- Quick check: Verify survey sections reference concepts from multiple clusters when appropriate

**Iterative Quality Evaluation**
- Why needed: Provides continuous feedback for improvement without manual intervention
- Quick check: Compare survey versions before and after evaluation to measure quality improvements

## Architecture Onboarding

**Component Map**
Paper Search Agent -> Topic Mining & Clustering Agent -> Academic Survey Writer Agent -> Quality Evaluator Agent -> Refinement Loop

**Critical Path**
1. Paper Search retrieves relevant papers using hybrid queries
2. Topic Mining clusters papers semantically and identifies subtopics
3. Survey Writer generates draft synthesizing across clusters
4. Quality Evaluator provides feedback for refinement
5. Iterative loop improves survey until quality threshold met

**Design Tradeoffs**
- Multiple specialized agents vs single general-purpose LLM: Specialized agents achieve better quality but increase system complexity
- Semantic vs keyword-based clustering: Semantic clustering provides better organization but requires more computational resources
- Iterative refinement vs single-pass generation: Iterative approach achieves higher quality but increases processing time

**Failure Signatures**
- Poor citation coverage: Paper Search Agent fails to retrieve relevant papers or clustering misses important connections
- Disorganized structure: Topic Mining Agent fails to identify coherent subtopics or Survey Writer doesn't properly organize content
- Weak synthesis: Survey Writer doesn't effectively connect concepts across clusters or Quality Evaluator provides insufficient feedback

**First 3 Experiments**
1. Test Paper Search Agent with known paper sets to verify retrieval accuracy and coverage
2. Evaluate Topic Mining Agent's clustering quality on synthetic datasets with known groupings
3. Run end-to-end survey generation on small paper sets (25-50 papers) to validate workflow integration

## Open Questions the Paper Calls Out
None identified in provided content.

## Limitations
- Human evaluation relies on three researchers with varying expertise, introducing potential subjectivity and bias
- Comparison only includes AutoSurvey as baseline, limiting understanding of relative performance against other approaches
- Quality score of 8.18/10 indicates room for improvement, particularly in originality and critical analysis

## Confidence
- **High**: Multi-agent architecture design and implementation feasibility
- **Medium**: Quality improvements over AutoSurvey given controlled comparison but limited external validation
- **Medium**: 12-dimensional evaluation framework's ability to capture comprehensive quality metrics

## Next Checks
1. Conduct cross-domain validation by applying Agentic AutoSurvey to literature surveys in non-LLM domains (e.g., materials science, biology) to assess generalizability
2. Perform inter-rater reliability analysis with independent expert panels to establish consistency of human assessments
3. Test scalability by evaluating system performance on larger paper sets (500-1000 papers) and analyzing whether quality metrics degrade or processing times significantly exceed 15-20 minutes