---
ver: rpa2
title: An Agile Method for Implementing Retrieval Augmented Generation Tools in Industrial
  SMEs
arxiv_id: '2508.21024'
source_url: https://arxiv.org/abs/2508.21024
tags:
- data
- retrieval
- language
- arxiv
- chunking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EASI-RAG, a structured, agile method for
  deploying Retrieval-Augmented Generation (RAG) tools in industrial SMEs. Based on
  method engineering principles, EASI-RAG defines roles, activities, and techniques
  to facilitate RAG adoption in resource-constrained environments.
---

# An Agile Method for Implementing Retrieval Augmented Generation Tools in Industrial SMEs

## Quick Facts
- arXiv ID: 2508.21024
- Source URL: https://arxiv.org/abs/2508.21024
- Reference count: 0
- Primary result: Structured, agile method for deploying RAG tools in resource-constrained industrial SMEs

## Executive Summary
This paper introduces EASI-RAG, a structured, agile method for deploying Retrieval-Augmented Generation (RAG) tools in industrial SMEs. Based on method engineering principles, EASI-RAG defines roles, activities, and techniques to facilitate RAG adoption in resource-constrained environments. The method was validated through a real-world case in an environmental testing laboratory, where a RAG tool was implemented to answer operator queries using operational procedures. The system was deployed in under a month by a team with no prior RAG experience and iteratively improved based on user feedback. Results showed 86% correct answers, zero incorrect responses, and high user adoption. The study highlights EASI-RAG's potential for enabling fast, reliable RAG deployment in industrial SMEs.

## Method Summary
EASI-RAG is a 5-block agile method for RAG deployment in industrial SMEs. Block 1 establishes a baseline pipeline with simple chunking and single vectorization. Block 2 defines evaluation criteria and creates a test query set. Block 3 analyzes errors using Pareto analysis to identify the most frequent failure categories and applies corrections in descending order. Block 4 integrates the system with minimal interface and feedback mechanisms. Block 5 collects weekly user feedback and updates the test set. The method was validated in an environmental testing laboratory using operational procedure documents, achieving 86% correct responses and zero incorrect answers through iterative improvement.

## Key Results
- RAG system deployed in under one month by team with no prior RAG experience
- Achieved 86% correct answers and zero incorrect responses in industrial SME context
- High user adoption demonstrated through positive feedback and continued usage
- Iterative improvement process corrected issues from most frequent to least frequent errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative error-driven improvement with Pareto prioritization accelerates RAG deployment for non-expert teams.
- Mechanism: Deploy a minimal viable pipeline quickly, then systematically analyze failures using diagnostic questions (Table 4). Apply Pareto analysis to identify the most frequent error categories. Correct issues in descending order of frequency, avoiding premature optimization of components that already perform adequately.
- Core assumption: Most RAG failures cluster around a small number of root causes; addressing top issues yields disproportionate performance gains.
- Evidence anchors:
  - [abstract] "The system was deployed in under a month by a team with no prior RAG experience and was later iteratively improved based on user feedback."
  - [section 4.5] "Identified errors have been corrected through six successive modifications, addressing the issues from the most frequent to the least frequent" with results improving from 34% to 86% correct answers.
  - [corpus] Related paper "Pursuing Best Industrial Practices for Retrieval-Augmented Generation in the Medical Domain" similarly emphasizes component organization and implementation order, though without Pareto-specific validation.
- Break condition: If errors are uniformly distributed across many categories with no clear Pareto pattern, the efficiency advantage diminishes.

### Mechanism 2
- Claim: Hybrid vectorization (dense + sparse) handles unknown technical vocabulary more reliably than either method alone.
- Mechanism: Dense embeddings (e.g., Sentence-BERT) capture semantic similarity but may "ignore" rare keywords by blending representations with surrounding terms. Sparse methods (BM25/TF-IDF) provide exact keyword matching for technical terminology. Combining both—retrieving top-k chunks from each method—improves recall for both conceptual and keyword-driven queries.
- Core assumption: Domain-specific vocabulary appears in queries but may be underrepresented in embedding model training data.
- Evidence anchors:
  - [section 3.1.2] "Sparse vectors enable precise keyword-based matching, particularly beneficial in industrial contexts where technical terminology is often crucial."
  - [section 4.5] "Some user queries contained rare keywords that the embedder effectively 'ignored'... To mitigate this, a hybrid vectorization approach combining dense embeddings with sparse BM25 was implemented."
  - [corpus] "PIKE-RAG" notes that retrieval alone is insufficient for complex industrial needs, suggesting hybrid approaches may be necessary but not universally sufficient.
- Break condition: If queries are predominantly conceptual (few technical keywords) or all vocabulary is well-represented in embedding training, sparse addition adds computational overhead without benefit.

### Mechanism 3
- Claim: Role-centric workflow with explicit responsibility assignments enables non-specialist teams to deploy RAG systems.
- Mechanism: Define four roles (User, Data Expert, Process Owner, Developer) and map each to specific activities. Data experts handle chunking and validation; developers manage vectorization and model selection; users provide test queries and feedback; process owners govern integration decisions. This distributes cognitive load and ensures domain knowledge informs each stage.
- Core assumption: SMEs have access to individuals who can fulfill each role, even without RAG-specific expertise.
- Evidence anchors:
  - [abstract] "EASI-RAG is based on method engineering principles and comprises well-defined roles, activities, and techniques."
  - [section 3] "This role-centric design addresses a gap in computer science research, which often focuses mainly on technical aspects while overlooking user involvement."
  - [corpus] Corpus evidence for role-centric methodologies in RAG is limited; most related papers focus on technical architectures rather than organizational workflows.
- Break condition: If roles cannot be staffed (e.g., no data expert available) or communication overhead between roles exceeds efficiency gains.

## Foundational Learning

- **Chunking strategies and tradeoffs**
  - Why needed here: The paper demonstrates that chunking choice (fixed-size vs. hierarchical vs. semantic) directly affects retrieval quality, particularly for structured documents like Excel files.
  - Quick check question: Given a 95-page technical manual with sections and subsections, would fixed-size 1000-token chunks or hierarchical chunking preserve more semantic coherence?

- **Dense vs. sparse vectorization**
  - Why needed here: Hybrid retrieval is presented as a key technique for handling unknown vocabulary; understanding when each approach excels is essential for error diagnosis.
  - Quick check question: If user queries frequently include part numbers like "XK-4720," which vectorization method would be more reliable for retrieval?

- **Agile iteration and feedback loops**
  - Why needed here: The entire EASI-RAG method is structured around rapid deployment followed by iterative improvement; understanding agile principles helps contextualize why the method avoids upfront optimization.
  - Quick check question: What is the risk of spending two weeks optimizing chunking strategy before deploying any version to users?

## Architecture Onboarding

- **Component map:**
  Documents → [Data Retrieval + Chunking] → [Vectorization (Dense + Sparse)] → [Chunk Retrieval (top-k from each)] → [Prompt Engineering] → [LLM] → Response

- **Critical path:**
  1. Initial design (Block 1): Establish baseline pipeline with simple chunking and single vectorizer
  2. Evaluation (Block 2): Define 3-5 performance criteria and create test query set with ~50 queries
  3. Error analysis (Block 3): Run Pareto analysis on failures; apply corrections from Table 4 in order
  4. Integration (Block 4): Deploy with minimal interface and "Report Incorrect Answer" mechanism
  5. Feedback (Block 5): Weekly reviews; new failure cases added to test set

- **Design tradeoffs:**
  - Simple chunking (hierarchical/fixed-size) vs. semantic chunking: Paper found simple sufficient; semantic adds computational cost
  - Cloud LLM vs. local deployment: Cloud is cheaper (~$0.001/query) but requires non-sensitive data; local needs hardware investment
  - Context size (3 chunks vs. 5+): More context can help recall but increases inference time and may introduce noise

- **Failure signatures:**
  - "Relevant chunks retrieved but answer wrong" → Check prompt grounding instructions
  - "Technical queries return off-topic results" → Vocabulary unknown to embedder; add sparse retrieval
  - "Excel data returns nonsense" → Preprocessing needed; add header prefixes per cell
  - "Answers contradict documentation" → Model using prior knowledge; add explicit instruction against this

- **First 3 experiments:**
  1. Deploy baseline pipeline with fixed-size chunking (1000 tokens), dense embeddings only, and simple prompt. Test against 20 queries to establish baseline accuracy.
  2. Classify all errors using Table 4 diagnostic questions. Identify whether failures cluster in retrieval (wrong chunks) or generation (wrong synthesis).
  3. Apply single highest-impact correction based on Pareto analysis (e.g., add hybrid vectorization if vocabulary errors dominate). Re-test and measure delta.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the EASI-RAG method generalizable to diverse industrial sectors and organizational structures while maintaining its low resource requirements?
- Basis in paper: [explicit] The authors state that "Future research could broaden validation of the EASI-RAG method across diverse SMEs to assess generalisability and robustness," noting the current study is limited to a single case.
- Why unresolved: The method was validated in only one environmental testing laboratory with specific stakeholders; its effectiveness across varied industries or team profiles remains unproven.
- What evidence would resolve it: Successful replication of the deployment process in multiple SMEs across different sectors (e.g., manufacturing, logistics) with varying data complexities.

### Open Question 2
- Question: What are the long-term effects of deploying EASI-RAG tools on employee communication and manual information retrieval skills?
- Basis in paper: [explicit] The conclusion suggests that "Longitudinal studies should examine long-term effects on human factors, including communication and information-retrieval skills."
- Why unresolved: The study focused on immediate implementation and adoption metrics (under one month) rather than the long-term sociotechnical impacts or potential skill erosion mentioned in the discussion.
- What evidence would resolve it: Longitudinal data tracking changes in employee search behaviors and interpersonal communication frequency over extended periods of tool usage.

### Open Question 3
- Question: Can the method be adapted to support hybrid approaches involving fine-tuning or integration with industrial systems like ERP and MES?
- Basis in paper: [explicit] The authors identify "future works" to include "further integration with fine-tuned models" and "integration with industrial systems (ERP, MES...)" to enhance interoperability.
- Why unresolved: The current iteration explicitly excludes fine-tuning to fit SME resource constraints and has not yet addressed the technical challenges of linking the RAG tool to dynamic industrial databases.
- What evidence would resolve it: A modified version of the EASI-RAG framework that successfully incorporates domain-specific fine-tuning or real-time data retrieval from an MES.

## Limitations

- Method's generalizability to domains beyond environmental testing remains unverified; all validation occurred within a single industrial laboratory context
- Without access to exact 50 test queries and full document corpus, independent replication of claimed 86% accuracy cannot be performed
- Role-based workflow assumes stable team composition and clear communication channels, which may not hold in all SME environments

## Confidence

- High: Core claim that structured role assignments and iterative error analysis enable non-expert teams to deploy RAG systems
- Medium: Specific claim that hybrid vectorization is necessary for technical vocabulary; evidence is correlational and domain-specific
- Medium: Claim that Pareto analysis accelerates convergence; supported by single case study but mechanism not universally validated

## Next Checks

1. Apply EASI-RAG to a different industrial domain (e.g., manufacturing or healthcare) and measure whether the same iterative improvement pattern emerges
2. Test the hypothesis that errors follow Pareto distribution by analyzing error frequencies in multiple deployments
3. Validate the necessity of hybrid vectorization by comparing performance with dense-only vs. hybrid retrieval on technical vs. general vocabulary queries