---
ver: rpa2
title: Uncertainty quantification with approximate variational learning for wearable
  photoplethysmography prediction tasks
arxiv_id: '2505.11412'
source_url: https://arxiv.org/abs/2505.11412
tags:
- uncertainty
- dropout
- calibration
- each
- uncertainties
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work compares Monte Carlo Dropout and Improved Variational
  Online Newton for uncertainty quantification in deep learning models predicting
  atrial fibrillation and blood pressure from photoplethysmography signals. Both techniques
  model epistemic and aleatoric uncertainty to assess prediction trustworthiness.
---

# Uncertainty quantification with approximate variational learning for wearable photoplethysmography prediction tasks

## Quick Facts
- arXiv ID: 2505.11412
- Source URL: https://arxiv.org/abs/2505.11412
- Reference count: 40
- Primary result: MCD and IVON quantify uncertainty in PPG-based AF and BP prediction, but hyperparameter choices significantly affect calibration and performance tradeoffs

## Executive Summary
This work evaluates Monte Carlo Dropout and Improved Variational Online Newton for uncertainty quantification in deep learning models predicting atrial fibrillation and blood pressure from photoplethysmography signals. Both techniques model epistemic and aleatoric uncertainty to assess prediction trustworthiness. Key findings show that hyperparameter choices significantly affect both calibration quality and predictive performance. For example, dropout rate and Hessian initialization influence the proportion of epistemic uncertainty and per-class calibration accuracy. Models with higher stochasticity generally exhibit better calibrated probabilities but may also overpredict uncertainty. Uncertainty disentanglement results reveal high correlation between aleatoric and epistemic components, cautioning against direct interpretation of disentangled estimates. The study emphasizes the need for rigorous evaluation protocols assessing both global and class-specific calibration, as well as individual prediction reliability, to ensure clinical applicability of uncertainty estimates.

## Method Summary
The study compares Monte Carlo Dropout and Improved Variational Online Newton for uncertainty quantification in PPG-based AF classification and BP regression. Models use 1D CNNs or 1D ResNet architectures with heteroscedastic output heads predicting both mean and variance. Uncertainty is quantified through K stochastic forward passes during inference, with aleatoric uncertainty estimated from predicted variances and epistemic uncertainty from variance of predicted means. The study evaluates calibration using ECE, UCE, and ENCE metrics, and assesses disentanglement quality through Pearson correlation between aleatoric and epistemic components. Hyperparameter sweeps examine dropout rates (5%, 10%, 40%) and Hessian initialization (0.001, 0.01, 0.5) to understand their impact on uncertainty estimates.

## Key Results
- Dropout rate and Hessian initialization significantly influence the proportion of epistemic uncertainty attributed to predictions
- Higher stochasticity generally improves probability calibration but may degrade predictive performance
- Aleatoric and epistemic uncertainty components show high correlation (r=0.75-0.81), suggesting poor disentanglement
- Per-class calibration reveals significant discrepancies between majority and minority classes
- Individual prediction calibration remains poor despite adequate population-level calibration metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Monte Carlo Dropout approximates Bayesian variational inference by treating dropout as posterior sampling over weight subnetworks.
- Mechanism: Dropout randomly zeroes activations during both training and inference. Each forward pass with active dropout samples a different thinned network, producing a distribution of predictions. Aggregating K passes approximates sampling from a posterior over model parameters, enabling uncertainty estimation without explicit Bayesian computation.
- Core assumption: The dropout distribution provides a meaningful approximation to the true posterior; K samples sufficiently characterize the posterior predictive distribution.
- Evidence anchors:
  - [abstract] "Monte Carlo Dropout and the recently proposed Improved Variational Online Newton...assess the trustworthiness of models"
  - [section 3.2] "each 'dropout model' can be interpreted as a sample from the model's posterior distribution"
  - [corpus] Related work "Understanding the Trade-offs in Accuracy and Uncertainty Quantification" examines similar BNN inference choices
- Break Condition: If dropout rate is too low (<1%), posterior approximation collapses to near-deterministic; if too high (>40%), model capacity degrades and predictive performance drops (Tables 3-4 show MAE increases from 5.39 to 7.47 mmHg for DBP).

### Mechanism 2
- Claim: Aleatoric and epistemic uncertainties can be disentangled using the law of total variance applied across stochastic forward passes.
- Mechanism: For regression (Equations 3-4): epistemic uncertainty = variance of predicted means across K passes; aleatoric uncertainty = mean of predicted variances across K passes. The model outputs both mean and variance predictions; heteroscedastic aleatoric uncertainty is learned via Gaussian NLL loss.
- Core assumption: Aleatoric and epistemic components are statistically separable; the model correctly learns to predict per-example variance.
- Evidence anchors:
  - [abstract] "uncertainty disentanglement results reveal high correlation between aleatoric and epistemic components, cautioning against direct interpretation"
  - [section 7.1, Table 5] Pearson correlation r=0.75-0.81 between aleatoric and epistemic estimates
  - [corpus] "Benchmarking uncertainty disentanglement" (Mucsányi et al.) cited for challenges in disentanglement
- Break Condition: High observed correlation (r>0.7) suggests disentanglement fails; components likely conflate rather than represent distinct uncertainty sources.

### Mechanism 3
- Claim: Hyperparameters controlling sampling stochasticity (dropout rate for MCD, Hessian initialization h₀ for IVON) directly determine the proportion of total uncertainty attributed as epistemic.
- Mechanism: Higher dropout rates produce broader posterior approximations with more weight variability across samples. Lower h₀ in IVON reduces posterior concentration at initialization, increasing sampling diversity. Both increase epistemic uncertainty estimates relative to aleatoric.
- Core assumption: Increased sampling variance genuinely reflects model uncertainty rather than artifact.
- Evidence anchors:
  - [abstract] "dropout rate and Hessian initialization influence the proportion of epistemic uncertainty"
  - [section 7.3, Figure 4 g-i] Shows epistemic proportion increases with dropout rate (5%→40%)
  - [section 7.4, Figure 5 g-i] Shows epistemic proportion increases with lower h₀ (0.5→0.001)
  - [corpus] Limited direct validation; corpus focuses on UQ techniques rather than hyperparameter effects
- Break Condition: Excessive stochasticity overpredicts uncertainty and degrades calibration; optimal values task-dependent.

## Foundational Learning

- Concept: **Aleatoric vs. Epistemic Uncertainty**
  - Why needed here: The paper's core contribution requires distinguishing irreducible data noise (aleatoric) from reducible model uncertainty (epistemic). Clinical decisions depend on knowing which source dominates.
  - Quick check question: If adding more training data would reduce uncertainty, which type is it?

- Concept: **Calibration (ECE, UCE, ENCE)**
  - Why needed here: The paper evaluates UQ quality through multiple calibration metrics. ECE measures probability calibration; UCE measures entropy calibration; ENCE measures regression uncertainty calibration. Without understanding these, results interpretation is impossible.
  - Quick check question: A model predicts 80% confidence on 100 examples—is it calibrated if 75 are correct?

- Concept: **Variational Inference Approximation**
  - Why needed here: Both MCD and IVON are justified as approximations to intractable Bayesian inference. Understanding what's being approximated (posterior over weights) clarifies why these methods work and their limitations.
  - Quick check question: Why is computing the true Bayesian posterior intractable for deep networks?

## Architecture Onboarding

- Component map:
  - 1D CNN or 1D ResNet-19 backbone → Dropout layers (MCD) or Variational weight sampling (IVON) → Mean and variance prediction heads → Gaussian NLL loss → K stochastic forward passes for inference

- Critical path:
  1. Design backbone without batch norm (IVON incompatibility noted in Section 5.1)
  2. Add variance prediction heads with Softplus activation (ensures positive variance)
  3. Implement stochastic sampling loop (K=50-100 passes at inference)
  4. Compute disentangled uncertainties using variance decomposition
  5. Evaluate with both global (ECE/UCE) and per-class calibration metrics

- Design tradeoffs:
  - **Dropout rate**: Lower (1-5%) → better predictive performance, potentially underconfident; Higher (10-40%) → better probability calibration, lower accuracy
  - **Sample count K**: Higher K → more stable uncertainty estimates, slower inference
  - **Architecture capacity**: Small models enable IVON prototyping; larger models may require MCD only
  - **Calibration target**: Optimizing ECE vs. UCE yields different optimal hyperparameters (Section 7.3)

- Failure signatures:
  - **ECE improves but UCE degrades**: Model confident but entropy uninformative (Figure 4 shows this tradeoff)
  - **High aleatoric-epistemic correlation (r>0.7)**: Disentanglement failed; don't interpret components as independent
  - **Per-class calibration discrepancy**: Good global calibration masks poor minority-class calibration (AF UCE=0.176-0.212 vs. non-AF UCE=0.024-0.055 at dropout=10%)
  - **Overpredicted uncertainty at individual level**: Bivariate histograms (Figure 2 g-l) show predicted uncertainty exceeds actual error

- First 3 experiments:
  1. **Hyperparameter sweep**: Train MCD models with dropout rates [0.01, 0.05, 0.1, 0.2, 0.4] on validation set; evaluate both ECE and UCE to identify optimal calibration-performance tradeoff point.
  2. **Per-class calibration audit**: For chosen hyperparameters, compute calibration curves separately for each class (AF vs. non-AF; or by BP range bins) to detect hidden miscalibration in clinically important subgroups.
  3. **Disentanglement validation**: Compute Pearson correlation between estimated aleatoric and epistemic uncertainties; if r>0.7, report total uncertainty only and avoid component-level claims in downstream use.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can aleatoric and epistemic uncertainty be reliably disentangled using standard variance decomposition methods for PPG-based prediction tasks?
- Basis in paper: [explicit] "A Pearson's correlation coefficient was computed comparing the aleatoric and epistemic uncertainties... indicate that the aleatoric and epistemic uncertainty exhibit high correlation... These results suggest that our own estimates are likely not completely disentangled."
- Why unresolved: The high correlation between disentangled components contradicts the assumption that they represent distinct uncertainty sources, but the paper does not validate whether this is an artifact of the decomposition method or a fundamental limitation.
- What evidence would resolve it: Comparison of disentanglement methods on synthetic data with known ground-truth uncertainty compositions, or development of task-specific validation protocols for disentanglement quality.

### Open Question 2
- Question: How can individual calibration be achieved for PPG prediction models when population-level calibration metrics show adequate performance but per-prediction reliability remains poor?
- Basis in paper: [explicit] "For the regression models, global and local calibration metrics indicate some degree of calibration for most models, but the bivariate histograms qualitatively suggest poor individual calibration across all models. This is concerning, as individual calibration is the most practically relevant metric, given clinical decisions may be derived from single estimates."
- Why unresolved: Standard calibration metrics aggregate over populations and cannot guarantee reliability for any single prediction, yet clinical decisions require exactly such per-instance trustworthiness.
- What evidence would resolve it: Development and validation of metrics that assess individual-level calibration, potentially through reliability diagrams stratified by input features or prediction confidence ranges.

### Open Question 3
- Question: What systematic protocol should be used to select hyperparameters that optimize both predictive performance and uncertainty calibration when the optimal configuration varies with the chosen uncertainty expression?
- Basis in paper: [explicit] "The optimal balance between calibration quality and predictive performance may be assessed with a rigorous evaluation protocol... we advise that a range of model parameterisations should be investigated to acquire the values that provide the best combination of uncertainty calibration and predictive performance needed for a given task... the optimal model parameterisation depends on the chosen expression of uncertainty."
- Why unresolved: Current practice relies on ad hoc grid search over dropout rates or Hessian initializations without established guidelines for balancing the tradeoff between calibration quality (which varies by metric choice) and task performance.
- What evidence would resolve it: Multi-objective optimization studies across diverse PPG tasks identifying Pareto-optimal hyperparameter configurations, or automated methods like Concrete Dropout adapted for variational learning approaches.

## Limitations

- High correlation (r=0.75-0.81) between aleatoric and epistemic uncertainty components suggests poor disentanglement quality
- Individual prediction calibration remains poor despite adequate population-level calibration metrics
- Single-source PPG datasets limit generalizability to different acquisition conditions and patient populations

## Confidence

- **High**: Core mechanism linking hyperparameter stochasticity to epistemic uncertainty proportion is robustly demonstrated through systematic ablation studies
- **Medium**: Practical clinical interpretation of disentangled uncertainties is questionable given high correlation between components
- **Low**: Cross-dataset generalization remains unverified due to single-source PPG data without external validation

## Next Checks

1. Evaluate per-class calibration separately for minority classes (non-AF, abnormal BP ranges) to detect hidden miscalibration that global metrics may mask

2. Compute aleatoric-epistemic correlation across multiple datasets to verify if high correlation is dataset-specific or a general limitation of variance decomposition methods

3. Test whether uncertainty estimates transfer to external PPG datasets with different acquisition conditions and patient populations to assess real-world applicability