---
ver: rpa2
title: 'Reasoning Is Not All You Need: Examining LLMs for Multi-Turn Mental Health
  Conversations'
arxiv_id: '2505.20201'
source_url: https://arxiv.org/abs/2505.20201
tags:
- patient
- sensemaker
- health
- prompt
- stage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines the multi-turn mental health conversation capabilities
  of large language models (LLMs), addressing the gap between their single-turn clinical
  performance and the complexities of real-world, iterative mental health sensemaking.
  To address this, the authors introduce MedAgent, a novel framework for synthetically
  generating realistic, multi-turn mental health conversations grounded in clinical
  literature, producing the Mental Health Sensemaking Dialogue (MHSD) dataset with
  over 2,200 patient-LLM dialogues.
---

# Reasoning Is Not All You Need: Examining LLMs for Multi-Turn Mental Health Conversations

## Quick Facts
- arXiv ID: 2505.20201
- Source URL: https://arxiv.org/abs/2505.20201
- Reference count: 40
- Frontier reasoning models achieve below-par patient-centric communication scores (~2.55–2.77/4) and struggle with diagnostic accuracy (~31% exact match) in multi-turn mental health dialogues

## Executive Summary
This study addresses the gap between single-turn LLM performance on clinical tasks and their capabilities in realistic, multi-turn mental health conversations. The authors introduce MedAgent, a framework for synthetically generating grounded, multi-turn mental health dialogues, producing the MHSD dataset with over 2,200 conversations. They also present MultiSenseEval, a holistic evaluation framework using human-centric criteria across six axes. Results show that even frontier reasoning models like OpenAI o1 and DeepSeek-R1 perform suboptimally for patient-centric communication and diagnostic accuracy, with performance varying by patient persona and declining with increased conversation turns. The study highlights the need for improved multi-turn reasoning and personalized engagement in mental health applications.

## Method Summary
The study develops a two-agent framework where a patient actor LLM generates responses based on atomic medical facts and persona attributes, while a sensemaker actor LLM (using reasoning models o1/R1) conducts structured clinical conversations through five progressive stages. The MedAgent framework generates the MHSD dataset by creating 1,142 conversations per model from 181 mental health cases in MedQA, each case augmented with five personality variants and two literacy levels. Evaluation uses MultiSenseEval, an LLM-as-judge framework validated against human annotators across six axes including patient-centric communication, diagnostic accuracy, and conversation flow metrics.

## Key Results
- Frontier reasoning models achieve average patient-centric communication scores of 2.55–2.77 on a 4-point scale
- Diagnostic accuracy shows ~31% exact match performance, declining by 29.6% from 5 to 10-15 sensemaker messages
- Performance varies significantly by patient persona, with better results for agreeable personalities compared to neurotic ones
- Models maintain conversation flow correctness (≥3/4) while degrading on patient-centric metrics as conversations lengthen

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured stage progression with explicit state tracking enables longer, more coherent multi-turn mental health conversations than unstructured dialogue.
- Mechanism: The sensemaker maintains a discrete stage variable (Fostering → Gathering → Providing → Decision → Responding → Exit). At each turn, a next-stage determination module evaluates patient message, accumulated facts, and conversation history to decide whether to remain or advance. Candidate messages are generated conditioned on the current stage goal, then filtered against history for redundancy before selection.
- Core assumption: Medical conversations naturally decompose into sequential communicative functions that can be modeled as discrete states with explicit transition criteria.
- Evidence anchors:
  - [section] Algorithm 2 (p.5) shows explicit stage variable gt updated via GetNextStage; candidate generation conditioned on current stage; redundancy filtering against message history H.
  - [section] Table 1 shows stage-wise message distributions; o1 averages 7.64 messages in Gathering vs. R1's 3.88, suggesting stage duration is model-dependent.
  - [corpus] MindEval and Psy-Insight (related papers) also adopt staged/structured evaluation for multi-turn mental health support, corroborating stage-based design as a community pattern.
- Break condition: If stage determination becomes noisy (e.g., patient provides mixed signals) or redundancy filtering is too aggressive, the conversation may stagnate or skip critical stages.

### Mechanism 2
- Claim: Atomic fact accumulation with closed-world patient responses grounds sensemaker reasoning and prevents hallucination of patient details.
- Mechanism: Patient background is decomposed into atomic facts F. At each turn, if the sensemaker asks about known facts, the patient actor retrieves matches (MatchFacts) and replies using those facts with persona-based stylistic variation. If no match and the query is background-related, a closed-world "I'm not sure" response is generated, preventing fabricated details.
- Core assumption: Decomposing clinical cases into fine-grained atomic facts enables accurate retrieval and prevents information leakage or hallucination in simulated patient responses.
- Evidence anchors:
  - [section] Algorithm 1 (p.3-4) describes Fmatch ← MatchFacts(st−1, F); closed-world assumption for unmatched background queries.
  - [section] Appendix Table 10-11 (p.18) shows explicit fact extraction and fact-matching prompts used to maintain consistency.
  - [corpus] Limited direct corpus evidence on atomic fact decomposition; related works focus more on dialogue structure than fact-level grounding.
- Break condition: If atomic facts are too coarse or incomplete, patient responses may become implausibly sparse or fail to reflect realistic patient knowledge gradients.

### Mechanism 3
- Claim: Multi-axis human-centric evaluation (MultiSenseEval) reveals performance gaps that single-metric accuracy or win-rate evaluations miss.
- Mechanism: Conversations are evaluated across six axes derived from clinical communication theory: Perceived Susceptibility, Perceived Severity, Perceived Benefits (from Health Belief Model), Diagnostic Accuracy (hard/soft match), Conversation Flow/Correctness, and Readability (SMOG). LLM-as-judge scores are validated against human annotators.
- Core assumption: Patient-centered communication quality can be operationalized via Health Belief Model constructs and evaluated reliably with LLM judges calibrated to human ratings.
- Evidence anchors:
  - [abstract] Reports average patient-centric scores ~2.55–2.77/4 and ~31% hard diagnostic accuracy.
  - [section] p.6-7: Both models exceed "Good" threshold for Flow/Correctness (≥3/4) but fall below for patient-centric metrics, exposing a gap invisible to single-metric evaluation.
  - [corpus] MindEval and MindGuard similarly advocate multi-dimensional evaluation for mental health dialogue, supporting multi-axis assessment as emerging practice.
- Break condition: If LLM-as-judge rubrics are misaligned with human judgment or fail to capture cultural/contextual nuance, evaluation scores may be misleading despite multi-axis coverage.

## Foundational Learning

- Concept: Multi-turn degradation in LLMs
  - Why needed here: Performance on patient-centric metrics and diagnostic accuracy declines as conversation length increases (R1 drops 12.83% on Susceptibility, 29.6% on Hard Accuracy from 5 to 10-15 sensemaker messages). Understanding this phenomenon is prerequisite to diagnosing and mitigating context drift.
  - Quick check question: Can you explain why a model might maintain conversation flow correctness while degrading on patient-centric communication as turns increase?

- Concept: Health Belief Model (HBM) constructs
  - Why needed here: Evaluation uses Susceptibility, Severity, and Benefits as core patient-centric metrics. Without understanding these constructs, engineers may misinterpret low scores as model failures rather than communication-alignment gaps.
  - Quick check question: How would you distinguish a failure in communicating Perceived Severity versus Perceived Susceptibility in a mental health dialogue?

- Concept: Persona-conditioned dialogue simulation
  - Why needed here: Patient personas vary along Big Five traits, health literacy, intentions, and goals. Model performance varies significantly (e.g., better for Agreeable personalities), so understanding persona conditioning is essential for debugging disparate performance.
  - Quick check question: Why might a model perform better with an "Agreeable" persona than a "Neurotic" one, and how would you test this systematically?

## Architecture Onboarding

- Component map:
  - Patient Actor LLM: GPT-4o; given persona Π (personality, literacy, intent, goal) and atomic facts F; generates responses via fact-matching or closed-world fallback.
  - Sensemaker Actor LLM: Reasoning models (o1, R1); modules for Next Stage Determination → Fact Extraction/Matching → Message Generation (3 candidates) → Redundancy Filtering → Selection.
  - MultiSenseEval: LLM-as-judge evaluation across 6 axes; human validation subset (100 conversations); prompts in Appendix J (Tables 24-29).
  - MHSD Dataset: 2,284 conversations (1,142 per model); derived from 181 MedQA mental health cases × 5 personality variants × 2 literacy levels.

- Critical path:
  1. Seed case selection (MedQA filtering) → atomic fact extraction
  2. Persona augmentation (personality × literacy × intent × goal)
  3. Conversation generation (Patient ↔ Sensemaker loop with stage progression)
  4. Post-processing (remove errors/incomplete)
  5. MultiSenseEval scoring (automated + human validation)

- Design tradeoffs:
  - Reasoning models for sensemaker vs. cost/latency: Authors chose o1/R1 for advanced reasoning in stage transitions and hypothesis validation, but these are slower and more expensive.
  - Closed-world assumption vs. naturalness: Prevents hallucination but may yield overly sparse patient responses for non-matching queries.
  - LLM-as-judge vs. scalability: Enables large-scale evaluation but requires human validation; inter-annotator agreement varies by metric (79–93.5% per Appendix K).

- Failure signatures:
  - Stage stagnation: If redundancy filter rejects all candidates, conversation advances prematurely or stalls (Algorithm 2 line 29: gt ← gt + 1).
  - Persona collapse: If patient persona prompts are underspecified, stylistic variation diminishes, reducing behavioral diversity.
  - Evaluation misalignment: Low human-LLM judge agreement on Susceptibility (79%) suggests this metric may be subjective or under-specified.

- First 3 experiments:
  1. Ablate stage progression: Replace explicit stage tracking with implicit end-to-end generation; compare Flow/Correctness and Diagnostic Accuracy to baseline.
  2. Persona sensitivity analysis: Run conversations across all 5 personality traits at fixed literacy; quantify performance variance (e.g., Agreeable vs. Neurotic delta on patient-centric scores).
  3. Conversation length stratification: Bin conversations by sensemaker message count (5, 5-10, 10-15); plot degradation curves for each MultiSenseEval axis to identify which capabilities decay fastest.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop evaluation metrics for multi-turn mental health conversations that go beyond accuracy-based measures while maintaining reliability and scalability across diverse clinical settings?
- Basis in paper: [explicit] Authors state "an open question remains, how can we develop evaluation metrics and frameworks that go beyond simplistic accuracy based metrics while maintaining the reliability, scalability across diverse settings?"
- Why unresolved: Current objective measures (accuracy, win-rates) overlook subjective aspects like patient-centeredness, while subjective measures lack generalizability.
- What evidence would resolve it: A validated evaluation framework demonstrating high inter-rater reliability, correlation with patient outcomes, and consistent performance across varied clinical contexts.

### Open Question 2
- Question: Why do frontier reasoning models perform significantly better with patients exhibiting high "Agreeableness" compared to high "Neuroticism" (7.1–7.5% performance gap)?
- Basis in paper: [inferred] The paper documents performance disparity across personality traits but does not identify mechanisms causing models to favor cooperative patients over anxious/emotionally reactive ones.
- Why unresolved: The finding reveals potential algorithmic bias toward certain interaction styles without explaining whether this stems from training data, prompting, or model architecture.
- What evidence would resolve it: Controlled experiments isolating the contribution of training data distribution, prompt design, and model reasoning processes to personality-based performance gaps.

### Open Question 3
- Question: Why does model performance decline substantially with increasing conversation length (up to 29.6% drop in hard diagnostic accuracy from 5 to 10-15 sensemaker messages)?
- Basis in paper: [inferred] The paper documents the performance drop but does not explain whether it stems from context window limitations, error accumulation, stage transition failures, or reasoning degradation.
- Why unresolved: Understanding the mechanism is critical for designing interventions to sustain multi-turn performance.
- What evidence would resolve it: Ablation studies examining context memory, error propagation patterns across turns, and per-stage performance trajectories in extended conversations.

### Open Question 4
- Question: How can synthetic data generation frameworks capture emerging or under-represented mental health experiences when LLMs themselves are trained on existing web data?
- Basis in paper: [explicit] Authors ask "How do we capture emerging or under-represented experiences when LLMs themselves are trained on existing web data?"
- Why unresolved: LLM-based synthetic data risks reproducing existing biases and gaps in mental health representation, limiting dataset diversity.
- What evidence would resolve it: Demonstrated methods for incorporating novel or marginalized patient narratives that are not well-represented in pre-training corpora, validated against real patient populations.

## Limitations

- Synthetic conversations may not fully capture the complexity and variability of real patient-provider interactions
- Closed-world assumption for patient responses may produce unrealistically sparse or constrained dialogue in some scenarios
- Evaluation framework relies heavily on LLM-as-judge with limited human validation (100 conversations), and inter-annotator agreement varies by metric (79-93.5% per Appendix K)

## Confidence

**Confidence Level: Medium** - The study demonstrates systematic multi-turn conversation evaluation but faces several key limitations. The synthetic nature of conversations, while controlled, may not fully capture the complexity and variability of real patient-provider interactions. The closed-world assumption for patient responses, while preventing hallucination, may produce unrealistically sparse or constrained dialogue in some scenarios. Additionally, the evaluation framework, though comprehensive, relies heavily on LLM-as-judge with limited human validation (100 conversations), and inter-annotator agreement varies by metric (79-93.5% per Appendix K).

**Confidence Level: High** - The multi-turn degradation phenomenon is well-documented and the methodology for stage progression and fact grounding is clearly specified. The persona conditioning approach and its impact on performance (better for Agreeable personalities) is robust and reproducible. The comparison between reasoning models (o1 vs R1) across multiple evaluation axes is methodologically sound.

**Confidence Level: Low** - Several critical components lack full specification for exact reproduction. The specific MedQA filtering criteria and atomic fact decomposition process are not fully detailed. The persona assignment process for the 181 cases is described but the exact prompts and decision logic are not provided. The threshold values for redundancy filtering and stage transition criteria are not explicitly stated.

## Next Checks

1. **Stage Progression Ablation Study**: Remove the explicit stage tracking mechanism and replace with end-to-end generation. Compare conversation flow correctness and diagnostic accuracy to the baseline to quantify the contribution of structured stage progression.

2. **Persona Performance Sensitivity Analysis**: Systematically vary all five personality traits while holding other factors constant. Quantify performance variance across all evaluation metrics to identify which persona characteristics most strongly impact model performance.

3. **Conversation Length Degradation Analysis**: Stratify conversations by sensemaker message count (5, 5-10, 10-15) and plot degradation curves for each MultiSenseEval axis. Identify which capabilities (patient-centric communication, diagnostic accuracy, or conversation flow) decay fastest with increased conversation turns.