---
ver: rpa2
title: Are language models aware of the road not taken? Token-level uncertainty and
  hidden state dynamics
arxiv_id: '2511.04527'
source_url: https://arxiv.org/abs/2511.04527
tags:
- token
- outcome
- steering
- distribution
- hidden
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores whether language models represent alternate
  reasoning paths during generation, using hidden states to quantify and control token-level
  uncertainty. The authors estimate uncertainty via Forking Path Analysis, which samples
  alternate tokens and re-generates completions to reveal outcome distributions at
  each token.
---

# Are language models aware of the road not taken? Token-level uncertainty and hidden state dynamics

## Quick Facts
- arXiv ID: 2511.04527
- Source URL: https://arxiv.org/abs/2511.04527
- Authors: Amir Zur; Atticus Geiger; Ekdeep Singh Lubana; Eric Bigelow
- Reference count: 18
- Primary result: Language model hidden states encode uncertainty and alternate reasoning paths, enabling effective steering when models are uncertain

## Executive Summary
This work explores whether language models represent alternate reasoning paths during generation by quantifying token-level uncertainty through hidden states. The authors use Forking Path Analysis to estimate outcome distributions at each token by sampling alternate tokens and re-generating completions, then test if these uncertainty estimates can predict steering effectiveness. They find that steering success correlates strongly with the model's uncertainty (R=0.57 to 0.64), peaking when the model is most uncertain, and that linear probes can predict outcome distributions from hidden states (KL loss 0.11 for same model vs 0.19 for different model). These findings indicate that hidden activations encode uncertainty and decision paths, enabling efficient uncertainty estimation and effective intervention.

## Method Summary
The authors investigate token-level uncertainty dynamics using three main techniques. Forking Path Analysis samples alternate tokens along a base generation path, re-samples completions from each alternate, and computes outcome distributions at each position. Difference-in-means steering vectors are computed from n=500 generations leading to different final answers, then applied at specific token positions to test steering effectiveness. Linear probes are trained on hidden states (layers 6-10, best at layer 8) to predict outcome distributions using KL divergence loss, with probes trained on the same model showing better performance than probes trained on a different model (Gemma-2 2B). The experiments use Llama-3.2 3B Instruct on reasoning datasets (GSM8k, AQuA, GPQA) with zero-shot CoT prompting.

## Key Results
- Steering success correlates strongly with uncertainty (R=0.57 reported, R=0.64 average), peaking when models are most uncertain
- Linear probes predict outcome distributions from hidden states with KL loss of 0.11 for same model vs 0.19 for different model
- Steering vector classification accuracy reaches ~80% on held-out data
- Steering is most effective when the model has not yet committed to a particular final answer

## Why This Works (Mechanism)
The findings suggest that language models maintain representations of multiple possible reasoning paths during generation, with hidden states encoding uncertainty about which path to follow. When a model is uncertain between alternate answers, the hidden states contain decision-relevant information that can be manipulated through activation steering. The correlation between steering success and uncertainty indicates that models are most amenable to intervention when they haven't yet committed to a specific reasoning trajectory. This mechanism allows for both efficient uncertainty estimation (via linear probes) and targeted interventions that can guide the model toward desired outcomes.

## Foundational Learning
- Forking Path Analysis: Method for estimating token-level uncertainty by sampling alternate tokens and re-generating completions. Why needed: Provides ground truth uncertainty estimates for training and validating probes. Quick check: Verify outcome distributions vary meaningfully across token positions in sample base paths.
- Difference-in-means steering: Technique for creating activation interventions by averaging hidden states from generations leading to different outcomes. Why needed: Enables targeted manipulation of model behavior based on uncertainty estimates. Quick check: Confirm steering vectors transfer across token positions with held-out accuracy >70%.
- Linear probe training with KL loss: Method for learning to predict outcome distributions from hidden states. Why needed: Provides efficient alternative to expensive Forking Path Analysis for uncertainty estimation. Quick check: Compare probe performance on same vs different model embeddings.

## Architecture Onboarding
Component map: Forking Path Analysis -> Steering Vector Creation -> Linear Probe Training -> Steering Evaluation
Critical path: Forking Path Analysis provides uncertainty ground truth → Steering vectors test intervention effectiveness → Linear probes learn to predict uncertainty → Steering success validates probe predictions
Design tradeoffs: Forking Path Analysis is accurate but computationally expensive (millions of tokens per example) vs. linear probes are efficient but potentially less accurate
Failure signatures: Steering vectors fail with near-zero success → Check classification accuracy; Linear probes overfit → Check KL loss on validation set; Forking Path Analysis computational explosion → Start with small S samples
First experiments: 1) Implement Forking Path Analysis on single example with S=10-20 samples, 2) Train linear probe on middle layers for single token, 3) Create steering vector from 50 generations and test on single example

## Open Questions the Paper Calls Out
Do non-linear probes or steering methods reveal stronger relationships between hidden states and uncertainty dynamics than the linear methods used in this work? The moderate correlation (R=0.57) suggests linear subspaces may not capture the full relationship.

Do the findings generalize across model scales, architectures, and reasoning domains beyond the single 3B-parameter Llama model and mathematical/scientific reasoning tasks tested? Limited experimental scope prevents conclusions about whether the relationship is general.

Can hidden-state probing be developed into a practical, computationally efficient method for real-time uncertainty estimation that avoids the expensive re-sampling required by Forking Paths Analysis? The paper demonstrates feasibility but doesn't develop an efficient deployment-ready method.

What are the mechanistic details of how and when a model "commits" to a particular reasoning path, and can this commitment point be precisely predicted or intervened upon? The paper identifies correlation but doesn't isolate specific mechanisms or circuit-level representations.

## Limitations
- Computational constraints of Forking Path Analysis limit testing to only 4-10 examples across three datasets
- Small sample sizes (4-10 examples) limit generalizability of steering and probing results
- Uncertainty correlation explains only ~30-40% of steering variance, leaving substantial unexplained variance
- Model-specific probe performance suggests potential overfitting given limited training data

## Confidence
High: Correlation between uncertainty and steering success; Forking Path Analysis methodology
Medium: Linear probes capturing decision information; steering vector effectiveness
Low: Generalizability across models/datasets; computational efficiency claims

## Next Checks
1. Scale Forking Path Analysis to 50-100 examples across all three datasets, measuring how correlation coefficients stabilize with sample size
2. Cross-validate linear probes using k-fold splits on the existing examples to assess overfitting risk
3. Test steering vector transfer across different reasoning tasks to evaluate task-specific vs. general uncertainty representations