---
ver: rpa2
title: 'HARBOR: Holistic Adaptive Risk assessment model for BehaviORal healthcare'
arxiv_id: '2512.18829'
source_url: https://arxiv.org/abs/2512.18829
tags:
- risk
- harbor
- behavioral
- clinical
- health
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces HARBOR, a behavioral health\u2013aware language\
  \ model designed to predict discrete mood and risk scores (HARBOR Risk Score, HRS)\
  \ on an integer scale from -3 (severe depression) to +3 (mania). The authors also\
  \ release PEARL, a longitudinal behavioral healthcare dataset containing four years\
  \ of monthly observations from three patients, with physiological, behavioral, and\
  \ self-reported mental health signals."
---

# HARBOR: Holistic Adaptive Risk assessment model for BehaviORal healthcare

## Quick Facts
- arXiv ID: 2512.18829
- Source URL: https://arxiv.org/abs/2512.18829
- Reference count: 6
- HARBOR achieves 69% accuracy in predicting discrete mood scores on a -3 to +3 scale from multimodal behavioral health data

## Executive Summary
This paper introduces HARBOR, a behavioral health–aware language model designed to predict discrete mood and risk scores (HARBOR Risk Score, HRS) on an integer scale from -3 (severe depression) to +3 (mania). The authors also release PEARL, a longitudinal behavioral healthcare dataset containing four years of monthly observations from three patients, with physiological, behavioral, and self-reported mental health signals. HARBOR is trained using a three-stage process involving mid-training, supervised fine-tuning, and reinforcement learning, and is benchmarked against traditional machine learning models and proprietary LLMs. Results show that HARBOR achieves 69% accuracy, significantly outperforming logistic regression (54%) and the strongest proprietary LLM baseline (29%). The model demonstrates improved temporal abstraction, calibration, and interpretability, making it a promising decision-support tool in behavioral healthcare.

## Method Summary
HARBOR uses a 20B-parameter GPT-style model trained through three stages: mid-training on psychiatry textbooks, supervised fine-tuning on clinical guidelines, and reinforcement learning for scale adherence. The model takes 16 multimodal features (sleep, activity, PHQ-9, etc.) and outputs a discrete risk score with confidence estimates. PEARL, the accompanying dataset, contains monthly observations from three patients over four years. The model is evaluated on accuracy, calibration (ECE), and temporal generalization, showing significant improvements over traditional ML models and baseline LLMs.

## Key Results
- HARBOR achieves 69% accuracy in predicting discrete mood scores, outperforming logistic regression (54%) and proprietary LLMs (29%)
- The model demonstrates strong calibration with ECE of 0.07, significantly better than baseline LLMs (0.18-0.21)
- Temporal generalization shows some degradation (0.69 to 0.52 accuracy) but remains competitive
- HARBOR handles missing data effectively, with MICE imputation maintaining performance within 5% of baseline

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Domain-specific mid-training enables the model to internalize clinical reasoning patterns that general-purpose LLMs lack.
- **Mechanism:** Mid-training on psychiatry textbooks, clinical guidelines, and behavioral health literature adapts the base 20B-parameter model's latent representations toward clinical concepts before task-specific fine-tuning. This creates priors that map behavioral signals (sleep, activity, PHQ-9) to mood states.
- **Core assumption:** The textbook corpus contains structured knowledge about symptom-to-diagnosis relationships that transfers to numerical risk scoring.
- **Evidence anchors:**
  - [abstract] "HARBOR is trained using a three-stage process involving mid-training, supervised fine-tuning, and reinforcement learning"
  - [Section 2.1] "We perform mid-training on a curated corpus of psychiatry, psychology, and therapy textbooks"
  - [corpus] No direct corpus evidence on mid-training effectiveness; related papers focus on LLM evaluation, not training methodology
- **Break condition:** If the pre-training corpus lacks representation of the specific patient populations or feature distributions in deployment, domain adaptation may not transfer.

### Mechanism 2
- **Claim:** Reinforcement learning with scale-adherence rewards produces calibrated, temporally consistent predictions.
- **Mechanism:** RL optimizes for agreement with expert-aligned reasoning while penalizing extreme or inconsistent predictions. This shapes the output distribution toward the discrete HRS scale rather than unconstrained text generation.
- **Core assumption:** Expert-aligned reward signals capture clinically valid reasoning patterns that generalize across patients.
- **Evidence anchors:**
  - [Section 2.2] "Rewards emphasize agreement with expert-aligned reasoning and penalize extreme or inconsistent predictions"
  - [Table 7] HARBOR achieves ECE of 0.07 (token likelihood) vs 0.18–0.21 for proprietary LLMs, indicating better calibration
  - [corpus] "Evaluating Large Language Models in Crisis Detection" notes LLM struggles in emotionally sensitive clinical settings, supporting the need for structured training
- **Break condition:** If reward design over-constrains outputs, the model may avoid clinically appropriate uncertainty expression.

### Mechanism 3
- **Claim:** Constraining outputs to discrete risk scores with explicit confidence improves clinical interpretability over free-form generation.
- **Mechanism:** The HRS scale maps directly to functional impairment categories (-3 to +3). The model outputs both a discrete score and confidence estimate, enabling clinicians to weight predictions by certainty.
- **Core assumption:** Clinicians prefer discrete, calibrated outputs over narrative explanations for triage decisions.
- **Evidence anchors:**
  - [Section 6] "HARBOR is designed as an interpretability-first system, prioritizing clinically meaningful outputs over opaque latent representations"
  - [Section 10] "Off-the-shelf LLMs perform poorly despite strong general reasoning capabilities, suggesting that structured clinical risk scoring requires domain-specific adaptation"
  - [corpus] "A Survey of Large Language Models in Psychotherapy" notes traditional NLP struggles with context-sensitive interactions
- **Break condition:** If the discrete scale fails to capture clinically meaningful gradations (e.g., mixed states), the model may force inappropriate categorization.

## Foundational Learning

- **Concept: Expected Calibration Error (ECE)**
  - Why needed here: HARBOR's safety claims depend on well-calibrated confidence. You must understand how to measure whether high-confidence predictions are actually more accurate.
  - Quick check question: If a model outputs 0.8 confidence on average for predictions that are correct 60% of the time, is it overconfident or underconfident?

- **Concept: Reinforcement Learning from Human Feedback (RLHF)**
  - Why needed here: HARBOR uses RL for scale adherence and consistency. Understanding reward shaping helps diagnose why the model avoids extreme predictions.
  - Quick check question: What failure mode occurs if the reward penalizes all uncertainty expression?

- **Concept: Temporal Generalization in Clinical Data**
  - Why needed here: HARBOR claims improved performance on time-based splits. Understanding distribution shift in longitudinal health data is critical for evaluating robustness.
  - Quick check question: Why might a model trained on months 1–24 of patient data fail on months 25–48 even if features appear similar?

## Architecture Onboarding

- **Component map:** Base LLM (20B GPT-style) -> Mid-training (psychiatry textbooks) -> SFT (clinical guidelines) -> STaR (reasoning traces) -> RL (scale adherence) -> Inference (16 features → HRS score + confidence)

- **Critical path:** The RL stage is where calibration and scale adherence are enforced. If rewards are misspecified, all downstream safety guarantees break.

- **Design tradeoffs:**
  - Discrete 7-point scale vs continuous: Improves interpretability but may lose granularity for mixed or atypical presentations
  - Small validated dataset (N=3) vs larger weakly-labeled data: Authors argue for depth over breadth; this limits population-level generalization claims
  - Single prediction vs majority voting: Voting adds 0.02–0.03 accuracy but increases latency

- **Failure signatures:**
  - Systematic overconfidence on out-of-distribution patients (check via patient-based split performance: 0.56 vs 0.69 random split)
  - Collapse to mode prediction (check class distribution of outputs)
  - Temporal drift: accuracy drops from 0.69 (t₀) to 0.52 (t₋₃)

- **First 3 experiments:**
  1. **Reproduce calibration curves:** Plot reliability diagrams using token likelihoods vs self-reported confidence on the test split. Verify ECE < 0.10.
  2. **Ablate training stages:** Run inference with checkpoints after mid-training only, SFT only, and full RL. Quantify each stage's contribution to accuracy and calibration.
  3. **Test missingness robustness:** Mask 25% of features randomly, apply MICE imputation (best-performing method per Table 8), and measure accuracy degradation. Target: < 5% drop from 0.69 baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Will HARBOR's predictive accuracy and calibration generalize to larger, more diverse patient populations beyond the three-patient PEARL cohort?
- Basis in paper: [explicit] Future work states "expanding PEARL to more patients"; Section 12 explicitly notes the dataset "is not intended to support population-level generalization claims."
- Why unresolved: PEARL contains only 3 patients (144 samples), limiting statistical power for assessing generalizability across demographics, diagnoses, and care settings.
- What evidence would resolve it: Evaluation on an expanded dataset with hundreds of patients spanning diverse demographics and clinical profiles, with performance reported across stratified subgroups.

### Open Question 2
- Question: Can HARBOR maintain accuracy and calibration when predicting mood at daily or hourly granularity rather than monthly intervals?
- Basis in paper: [explicit] Future work explicitly proposes "increasing temporal resolution to daily or hourly predictions."
- Why unresolved: Current features (e.g., monthly expenses, lab values) are aggregated monthly; higher-frequency prediction may require new feature engineering and could increase noise.
- What evidence would resolve it: Training and evaluation on a version of PEARL with daily/hourly features and ground-truth labels, comparing performance metrics to monthly baselines.

### Open Question 3
- Question: What evaluation standards should the field adopt for calibrated, discrete behavioral health risk scoring?
- Basis in paper: [explicit] Section 12.2 calls for "evaluation standards that emphasize calibration, robustness to missingness, and temporal and patient-level generalization."
- Why unresolved: No consensus benchmark exists; prior work emphasizes free-text outputs rather than discrete, clinically grounded risk scales.
- What evidence would resolve it: Community-driven benchmark creation with standardized splits, calibration metrics (e.g., ECE), missingness protocols, and reporting requirements for temporal/patient-level generalization.

### Open Question 4
- Question: How does HARBOR perform when deployed as a real-time clinical decision-support tool in active psychiatric practice?
- Basis in paper: [inferred] The paper positions HARBOR as a "decision-support tool" and describes safety guardrails, but all experiments are offline retrospective evaluations.
- Why unresolved: No prospective or clinician-in-the-loop study has assessed workflow integration, trust calibration, or impact on clinical decision-making.
- What evidence would resolve it: A pilot deployment study where clinicians use HARBOR during routine care, measuring changes in decision patterns, time-to-intervention, and clinician trust via surveys and interaction logs.

## Limitations

- Small sample size: PEARL contains only 3 patients (144 samples), limiting population-level generalizability claims
- Unclear RL reward design: The exact reward formulation for scale adherence and consistency is not fully specified
- Unvalidated mid-training impact: No empirical validation that psychiatry textbook mid-training actually contributes to performance gains

## Confidence

- **High confidence:** Three-stage training architecture is clearly specified; accuracy improvements over logistic regression are statistically significant; calibration results demonstrate measurable improvement
- **Medium confidence:** Claims about temporal abstraction and consistency; interpretability benefits from discrete scoring; clinical utility of HRS scale
- **Low confidence:** Generalization to broader patient populations; long-term reliability of predictions; safety in real-world deployment

## Next Checks

1. **Population diversity test:** Apply HARBOR to a dataset with 10+ patients from different demographic backgrounds. Measure accuracy variance and calibration stability across groups to assess generalizability limits.

2. **Temporal drift analysis:** Train on months 1-24 and test on months 25-48 separately. Quantify accuracy drop and calibration degradation to establish temporal generalization bounds.

3. **Missing data robustness:** Systematically mask 10%, 25%, and 50% of features to evaluate imputation requirements. Compare MICE performance against mean imputation and zero-filling to determine minimum data quality thresholds.