---
ver: rpa2
title: Finite-Time Bounds for Average-Reward Fitted Q-Iteration
arxiv_id: '2510.17391'
source_url: https://arxiv.org/abs/2510.17391
tags:
- function
- learning
- sample
- complexity
- bellman
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Anchored Fitted Q-Iteration (Anc-F-QI) for
  offline RL in average-reward MDPs. The key innovation is combining Fitted Q-Iteration
  with an anchor mechanism, which can be interpreted as weight decay, to enable finite-time
  analysis in this setting.
---

# Finite-Time Bounds for Average-Reward Fitted Q-Iteration

## Quick Facts
- arXiv ID: 2510.17391
- Source URL: https://arxiv.org/abs/2510.17391
- Reference count: 40
- Primary result: First sample complexity bounds for average-reward offline RL with function approximation under weak communication assumptions

## Executive Summary
This paper introduces Anchored Fitted Q-Iteration (Anc-F-QI) for offline reinforcement learning in average-reward Markov Decision Processes. The key innovation is combining Fitted Q-Iteration with an anchor mechanism that acts as weight decay, enabling finite-time analysis in this challenging setting. The authors establish the first sample complexity results for average-reward offline RL with function approximation for weakly communicating MDPs, significantly relaxing structural assumptions compared to prior work. The algorithm achieves sample complexities of $\tilde{O}(1/\epsilon^6)$ to $\tilde{O}(1/\epsilon^4)$ for IID datasets and $\tilde{O}(1/\epsilon^{12})$ to $\tilde{O}(1/\epsilon^8)$ for single-trajectory datasets, depending on normalization techniques.

## Method Summary
The paper proposes Anchored Fitted Q-Iteration (Anc-F-QI) for offline RL in average-reward MDPs. The algorithm iteratively performs regression to fit the Bellman operator target, followed by an anchor step that combines the new estimate with the initial point (zero) using a schedule $\lambda_k = \frac{k}{k+2}$. This anchoring acts as weight decay, stabilizing the iteration and enabling finite-time convergence rates. For relative normalization, the algorithm subtracts the midpoint of the value range at each iteration. The method works with both IID datasets and single-trajectory data satisfying $\beta$-mixing conditions, achieving sample complexity bounds that depend on the inherent Bellman error, function class complexity, and coverage coefficients.

## Key Results
- First sample complexity bounds for average-reward offline RL with function approximation under weak communication assumptions
- Sample complexity of $\tilde{O}(1/\epsilon^6)$ for IID datasets, improving to $\tilde{O}(1/\epsilon^4)$ with relative normalization
- Sample complexity of $\tilde{O}(1/\epsilon^{12})$ for single-trajectory datasets, improving to $\tilde{O}(1/\epsilon^8)$ with relative normalization
- Results require only weak communication rather than ergodicity or linearity assumptions

## Why This Works (Mechanism)

### Mechanism 1: Anchoring (Halpern Iteration)
The anchor mechanism enables explicit finite-time convergence rates for value iteration in average-reward MDPs, where standard updates may diverge. By computing $f_{k+1} = (1-\lambda_{k+1})f_0 + \lambda_{k+1}\hat{T}f_k$ with $f_0=0$, the $(1-\lambda_{k+1})f_0$ term acts as weight decay, pulling iterates back toward the origin to stabilize value estimates. This works for weakly communicating MDPs satisfying the Bellman optimality equation.

### Mechanism 2: Relative Normalization
Subtracting the midpoint $(\max + \min)/2$ at each iteration improves sample complexity from $\tilde{O}(1/\epsilon^6)$ to $\tilde{O}(1/\epsilon^4)$. This shrinks the range of the function approximator, preventing the need for increasing function ranges and keeping approximation error bounded by the optimal bias span rather than iteration count.

### Mechanism 3: $\beta$-Mixing for Single-Trajectory Data
The analysis extends finite-time bounds from IID data to single-trajectory data by quantifying dependence decay rates. The $\beta$-mixing coefficient bounds dependence between past and future blocks, allowing standard concentration inequalities to be applied despite temporal correlation.

## Foundational Learning

- **Weakly Communicating MDPs**: The most general MDP class where optimal gain is constant and independent of initial state. Required as the structural foundation for Bellman optimality equation.
  - Quick check: Does your environment allow reaching any recurrent state from any other under some policy?

- **Inherent Bellman Error (IBE) & Completeness**: The error when projecting Bellman operator output back into function class. Zero IBE (Bellman completeness) is required for main theoretical guarantees.
  - Quick check: Is your function class closed under Bellman operator (can it perfectly represent target Q-values for any policy)?

- **Coverage Coefficient ($C_\mu$)**: Measures "density ratio" between optimal policy and offline data distribution. Appears as multiplicative factor in sample complexity bounds.
  - Quick check: Does your offline dataset cover state-action regions visited by optimal policy with sufficient probability density?

## Architecture Onboarding

- **Component map**: Dataset $D$ -> Regressors $\mathcal{F}_k$ -> Core Loop (Algorithm 2) -> Operations (Regression, Normalization, Anchoring)
- **Critical path**: The exact choice of $\lambda_k = \frac{k}{k+2}$ is mathematically coupled to $O(1/K)$ convergence rate. Changing schedule requires re-deriving bound in Proposition 1.
- **Design tradeoffs**:
  - IID vs. Single-Trajectory: Single-trajectory relaxes data requirements but inflates complexity from $\tilde{O}(\epsilon^{-4})$ to $\tilde{O}(\epsilon^{-8})$
  - Relative vs. Standard: Relative normalization requires function class to handle arbitrary shifts but yields strictly better bounds
- **Failure signatures**:
  - Exploding Q-values: Likely without anchoring in standard algorithm
  - Slow Convergence: If $\lambda_k$ grows too fast or mixing time misestimated
  - Suboptimal Policy: If function approximator lacks capacity for Bellman completeness
- **First 3 experiments**:
  1. Implement Algorithm 2 on 2-state "weakly communicating" MDP to verify bounded Q-values vs standard VI
  2. Ablate $\lambda_k$: Run with $\lambda_k=1$ vs $\lambda_k=\frac{k}{k+2}$ on dataset with distribution shift
  3. Generate slow-mixing chain data and compare IID vs single-trajectory bounds

## Open Questions the Paper Calls Out
- Extending analysis to relax full coverage coefficient requirements
- Utilizing variance reduction techniques to further improve sample complexity
- Generalizing convergence guarantees to general multichain MDP settings
- Determining if derived sample complexity bounds are tight

## Limitations
- Strong assumptions on Bellman completeness (IBE=0) rarely satisfied in practice
- Coverage coefficient $C_\mu$ is problem-specific and may be very large
- $\beta$-mixing assumptions for single-trajectory data are difficult to verify empirically
- Large polynomial dependencies on error tolerance $\epsilon$ in sample complexity bounds

## Confidence
- **High**: Anchoring mechanism enabling finite-time convergence rates for average-reward MDPs
- **Medium**: Sample complexity bounds for both IID and single-trajectory settings
- **Low**: Practical applicability of Bellman completeness and coverage coefficient assumptions

## Next Checks
1. **Bellman Completeness Verification**: Test Anc-F-QI on tabular MDP where completeness can be verified, comparing convergence rates with/without anchor
2. **Coverage Sensitivity Analysis**: Generate datasets with varying coverage qualities and measure policy error scaling with sample size
3. **Mixing Time Impact**: Create single-trajectory datasets with controlled mixing coefficients and verify $\tilde{O}(1/\epsilon^8)$ degradation with slow mixing