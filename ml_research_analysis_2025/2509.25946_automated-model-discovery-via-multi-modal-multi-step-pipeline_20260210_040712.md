---
ver: rpa2
title: Automated Model Discovery via Multi-modal & Multi-step Pipeline
arxiv_id: '2509.25946'
source_url: https://arxiv.org/abs/2509.25946
tags:
- data
- analyzervlm
- kernel
- discovery
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a multi-modal and multi-step automated model
  discovery pipeline that leverages vision-language models for effective model proposal
  and evaluation. The core method introduces AnalyzerVLM for iterative multi-step
  analysis of data and models, and EvaluatorVLM for visual-based model assessment
  using a proposed Visual Information Criterion (VIC).
---

# Automated Model Discovery via Multi-modal & Multi-step Pipeline

## Quick Facts
- **arXiv ID**: 2509.25946
- **Source URL**: https://arxiv.org/abs/2509.25946
- **Reference count**: 40
- **Primary result**: Multi-modal, multi-step automated model discovery pipeline using vision-language models outperforms five competing methods on real-world univariate datasets with consistently lower RMSE values.

## Executive Summary
This paper introduces a novel automated model discovery pipeline that leverages vision-language models (VLMs) for both proposing and evaluating model structures. The system combines AnalyzerVLM for iterative multi-step data analysis with EvaluatorVLM for visual-based model assessment, using a proposed Visual Information Criterion (VIC). The approach demonstrates superior performance over five competing methods on real-world univariate datasets, achieving consistently lower RMSE values in both training and test regions. The pipeline effectively captures fine-grained details and ensures strong generalizability by combining visual fitness and generalizability scores with traditional Bayesian Information Criterion (BIC).

## Method Summary
The pipeline employs two VLM modules: AnalyzerVLM autonomously plans and executes multi-step analyses to propose effective candidate models through iterative reasoning and code execution, while EvaluatorVLM assesses model fitness and generalizability using visual criteria combined with Bayesian Information Criterion (BIC). The method operates through an agentic proposal-evaluation loop where one agent proposes structures and another evaluates them, creating a gradient of improvement over a combinatorial search space. The system uses standard GP hyperparameter fitting (L-BFGS-B with 10 random restarts) and maintains a model pool through iterative rounds, selecting models based on the Visual Information Criterion (VIC) that balances complexity and generalization.

## Key Results
- The multi-modal pipeline achieves consistently lower RMSE values than five competing methods on real-world univariate datasets
- VIC effectively penalizes models that fit training data well but exhibit implausible behavior in extrapolation regions, unlike standard BIC
- Multi-step analysis and visual modality significantly improve model proposal accuracy compared to single-step or text-only approaches

## Why This Works (Mechanism)

### Mechanism 1: Multi-step Analysis via Code Execution
Performing analysis in iterative steps (reason → code → observe) yields better model proposals than single-step inference by allowing the agent to refine hypotheses based on intermediate quantitative evidence. AnalyzerVLM operates within a policy where it alternates between natural language reasoning and generating Python code, executing this code to produce observations and updating its context until sufficient analysis is complete.

### Mechanism 2: Visual Information Criterion (VIC) for Generalization
Augmenting quantitative metrics (BIC) with VLM-derived visual scores enables selection of models that maintain structural consistency in extrapolation regions where traditional metrics fail. The EvaluatorVLM scores "Visual Fitness" (uncertainty size, data match) and "Visual Generalizability" (structural consistency in extrapolated regions), combined with BIC to penalize models with physically implausible behavior.

### Mechanism 3: Agentic Proposal-Evaluation Loop
An iterative loop where one agent proposes structures and another evaluates them creates gradient improvement over a combinatorial search space. The pipeline runs for R rounds, with AnalyzerVLM proposing structures based on top models from previous rounds, which are then parameter-fitted, scored by EvaluatorVLM, and added to a pool.

## Foundational Learning

- **Concept: Gaussian Process (GP) Kernels**
  - **Why needed here**: The primary search space is composition of GP kernels (Linear, Periodic, SE). Understanding how these base kernels combine to model trends and seasonality is required to interpret model discovery results.
  - **Quick check question**: How does multiplying a Linear kernel by a Periodic kernel change the resulting function shape compared to adding them?

- **Concept: Bayesian Information Criterion (BIC)**
  - **Why needed here**: The paper explicitly modifies BIC to create VIC. Understanding that BIC penalizes model complexity is required to grasp why authors add a visual term to handle generalization gaps.
  - **Quick check question**: Does a lower BIC score indicate a better or worse model, and how does the proposed VIC formula modify this directionality?

- **Concept: Tool-Use / Code-Act Agents**
  - **Why needed here**: AnalyzerVLM is not just a text generator; it's an agent that writes and executes Python code to analyze data.
  - **Quick check question**: In the AnalyzerVLM loop, what serves as the "observation" that updates the agent's context for the next step?

## Architecture Onboarding

- **Component map**: Data → AnalyzerVLM (reasoning/code) → Execution Sandbox → Observations → Model Structure Proposal → Optimizer → Fitted Model → Visualizations → EvaluatorVLM → Visual Scores → VIC Calculation → Model Pool

- **Critical path**: 1. Load dataset, 2. AnalyzerVLM generates code -> Sandbox executes -> Result feeds back to VLM -> VLM proposes model structure, 3. Optimizer finds parameters, 4. Controller generates posterior prediction plots, 5. EvaluatorVLM inputs plots -> outputs Visual Score, 6. Compute VIC = α·Visual Score - BIC, 7. Update pool

- **Design tradeoffs**: Latency vs. Robustness (multi-step analysis increases API costs and latency), Flexibility vs. Constraints (arbitrary analysis code is flexible but risks errors vs. predefined library ensures stability)

- **Failure signatures**: Code Execution Errors (AnalyzerVLM generates crashing code), Visual Hallucination (EvaluatorVLM gives high scores to clearly diverging plots), Overfitting to Visuals (model learns to generate plots that "look good" rather than fitting data)

- **First 3 experiments**:
  1. Ablation on Modality: Run pipeline on Airline dataset using only text inputs vs. multi-modal inputs to validate performance drop
  2. BIC vs. VIC Comparison: Generate candidate models and rank using standard BIC vs. VIC, plot top-ranked models to confirm VIC rejects bad extrapolators
  3. Step Analysis: Run AnalyzerVLM on dataset with known ground truth and log steps required to correctly identify components, comparing LLM vs. VLM

## Open Questions the Paper Calls Out

- How can the pipeline be generalized to multivariate datasets to effectively capture complex relationships between variables? (The current pipeline focuses on 1D datasets and extending to multivariate data is identified as necessary future work)

- How can the system autonomously identify or generate optimal visualizations to maximize VLM analysis accuracy? (Pipeline performance depends on input visualization quality, suggesting searching for good visualization quality as a distinct future direction)

- What are the computational and financial trade-offs of the multi-step VLM pipeline compared to traditional model discovery algorithms? (While demonstrating superior model quality, the approach relies on iterative reasoning via large commercial models, implying high latency and token costs not analyzed)

## Limitations

- The exact system prompts for AnalyzerVLM and EvaluatorVLM are not fully disclosed, affecting reproducibility of the multi-step reasoning process
- Visual scoring consistency from EvaluatorVLM may vary based on visualization format and resolution
- The method currently focuses on 1D univariate datasets, limiting applicability to multivariate data

## Confidence

- **High confidence**: The core multi-modal pipeline architecture and experimental results showing superior RMSE performance
- **Medium confidence**: The effectiveness of VIC in improving generalization, as this relies heavily on VLM visual assessment quality
- **Medium confidence**: The multi-step analysis mechanism, given lack of complete prompt details and termination criteria

## Next Checks

1. **Ablation on Modality**: Run the pipeline on a benchmark dataset using only text inputs versus multi-modal inputs to quantify the performance impact of visual information

2. **VIC vs. BIC Comparison**: Generate candidate models and rank them using standard BIC versus VIC, then visually inspect top-ranked models to verify VIC's ability to reject poor extrapolators

3. **Step Analysis**: Apply AnalyzerVLM to a synthetic dataset with known ground truth structure and measure the number of reasoning steps required for accurate identification, comparing multi-step versus single-step performance