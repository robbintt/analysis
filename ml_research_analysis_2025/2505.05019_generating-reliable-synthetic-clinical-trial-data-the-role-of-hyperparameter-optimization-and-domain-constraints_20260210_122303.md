---
ver: rpa2
title: 'Generating Reliable Synthetic Clinical Trial Data: The Role of Hyperparameter
  Optimization and Domain Constraints'
arxiv_id: '2505.05019'
source_url: https://arxiv.org/abs/2505.05019
tags:
- data
- synthetic
- metrics
- survival
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates the effectiveness of hyperparameter optimization\
  \ (HPO) in improving synthetic clinical trial data quality. Nine generative models\
  \ are systematically evaluated across two clinical datasets (AML and ACTG) using\
  \ four HPO objectives\u2014single-metric and compound-metric approaches."
---

# Generating Reliable Synthetic Clinical Trial Data: The Role of Hyperparameter Optimization and Domain Constraints

## Quick Facts
- arXiv ID: 2505.05019
- Source URL: https://arxiv.org/abs/2505.05019
- Reference count: 40
- Primary result: Compound metric HPO outperforms single-metric objectives in synthetic clinical data quality, with Tab DDPM showing largest gains.

## Executive Summary
This paper systematically evaluates nine generative models for synthetic clinical trial data, comparing the effectiveness of single-metric versus compound-metric hyperparameter optimization (HPO). The study uses two clinical datasets (AML and ACTG) and finds that compound metric optimization consistently produces more generalizable datasets by balancing multiple evaluation criteria. While HPO significantly improves data quality, the research reveals that preprocessing transformations and model architecture play crucial roles in maintaining clinical validity and privacy. The findings demonstrate that Survival CTGAN emerges as the most balanced model, combining high data quality with domain constraint adherence.

## Method Summary
The study compares nine generative models (TVAE, CTGAN, Tab Transformer, Tab DDPM, and four Synthcity models) across two clinical datasets using Optuna's TPE sampler for hyperparameter optimization. Four HPO objectives are evaluated: ML Efficiency, Survival, Four Metrics, and Full Objective (equally weighted average of seven evaluation metrics). Each model undergoes 30 HPO trials with 5-fold cross-validation, generating 25 synthetic datasets per configuration. Domain constraints are enforced through preprocessing (transforming survival times to differences) and postprocessing validation. Privacy is assessed through Authenticity and Adversarial Accuracy metrics.

## Key Results
- Compound metric optimization (Full Objective) outperformed single-metric approaches, producing more generalizable synthetic datasets
- Tab DDPM achieved the largest relative gains from HPO (up to 335% improvement) due to poor baseline performance
- Privacy risks are primarily determined by model architecture rather than optimization objectives, with TVAE and Tab DDPM showing highest memorization risks
- Without preprocessing transformations, models generated invalid clinical data in up to 61% of cases

## Why This Works (Mechanism)

### Mechanism 1: Compound Metric Optimization
- **Claim:** Compound metric optimization consistently produces more generalizable synthetic datasets than single-metric objectives
- **Mechanism:** Single metrics encourage overfitting to specific tasks, while composite objectives force navigation of trade-offs between utility and fidelity
- **Core assumption:** Chosen metrics adequately represent multidimensional data quality, and equal weighting balances dimensions
- **Evidence anchors:** Abstract confirms compound optimization outperformed single metrics; Section 3.1 shows Four Metrics and Full Objective yielded best results
- **Break condition:** Highly correlated metrics would make compound objective redundant

### Mechanism 2: Domain Constraint Preprocessing
- **Claim:** Explicit preprocessing transformations are necessary to enforce domain-specific logical constraints
- **Mechanism:** Logical dependencies (e.g., Event-Free Survival â‰¤ Overall Survival) are preserved by transforming variables before synthesis
- **Core assumption:** Logical rules are hard constraints, and transformations preserve statistical utility
- **Evidence anchors:** Abstract notes HPO alone doesn't prevent constraint violations; Section 3.3 shows fault rates jump from 10% to 48% without transformation
- **Break condition:** Complex transformations may degrade synthesis quality if models cannot capture resulting distributions

### Mechanism 3: Architecture-Driven Privacy
- **Claim:** Privacy risks are determined more by model architecture than hyperparameter optimization
- **Mechanism:** Different architectures have inherent privacy profiles that HPO cannot fundamentally alter
- **Core assumption:** Authenticity and Adversarial Accuracy are valid privacy proxies
- **Evidence anchors:** Abstract states architecture has stronger privacy impact; Section 3.1 shows model variability was 1.62x higher than objective variability
- **Break condition:** Different hyperparameter search spaces could conflate architecture effects with tuning flexibility

## Foundational Learning

- **Concept: Survival Analysis Logic (Censoring & Time-to-Event)**
  - **Why needed here:** Clinical trial data requires logical constraints between survival times
  - **Quick check question:** If a patient has Event-Free Survival Time of 100 days and Overall Survival Time of 90 days, why is this clinically impossible?

- **Concept: Generative Model Architectures (GANs vs. VAEs vs. Diffusion)**
  - **Why needed here:** Different architectures have varying capabilities for constraint adherence and privacy
  - **Quick check question:** Which architecture iteratively denoises data, and which relies on an encoder-decoder bottleneck?

- **Concept: Tree-structured Parzen Estimator (TPE)**
  - **Why needed here:** TPE is the specific HPO algorithm used via Optuna
  - **Quick check question:** How does TPE differ from standard Grid Search in computational efficiency?

## Architecture Onboarding

- **Component map:** Clinical Data -> Preprocessing (EFSTMdif transformation + encoding) -> HPO Optimizer (Optuna TPE) -> Generative Models (9 candidates) -> Postprocessing (constraint validation) -> Evaluation (Fidelity, Utility, Privacy metrics)
- **Critical path:** Preprocessing Transformation is most fragile step; without EFSTMdif transformation, fault rates jump from ~10% to ~48%
- **Design tradeoffs:**
  - TVAE and Tab DDPM offer highest utility but worst privacy
  - Survival CTGAN provides best balance with slightly lower peak utility
  - Compound objectives take ~5x longer than single-metric optimization
- **Failure signatures:**
  - High Violation Rate (>40%): Missing preprocessing or using models without robust post-processing
  - Low Authenticity (< Test Baseline): Privacy leakage/memorization, especially risky with TVAE
  - Metric Instability: Large standard deviations suggest unstable hyperparameters
- **First 3 experiments:**
  1. Baseline Validation: Compare CTGAN vs Survival CTGAN on ACTG dataset to confirm preprocessing impact on fault rates
  2. Objective Ablation: Test Tab DDPM with Single-Metric vs Full Objective to verify compound optimization prevents utility collapse
  3. Privacy Stress Test: Generate data using TVAE and calculate Too-Close 5% rate to identify training data leakage

## Open Questions the Paper Calls Out
None

## Limitations
- Results based on two specific clinical datasets may not generalize to other domains
- Privacy analysis relies on limited metrics that may not capture all privacy risks
- Equal weighting in compound objectives may not be optimal for all use cases
- Computational cost of compound HPO (5x longer) limits practical applicability

## Confidence
- Compound metric HPO improves data quality: High
- Architecture drives privacy more than optimization: High
- Preprocessing is crucial for domain validity: High
- Privacy metrics capture all risks: Low

## Next Checks
1. Replicate results using alternative privacy metrics (membership inference, attribute inference) to verify architectural effects
2. Test compound objective performance with weighted rather than equal metric combinations
3. Validate findings on additional clinical datasets with different sizes and variable types