---
ver: rpa2
title: Aligned Textual Scoring Rules
arxiv_id: '2507.06221'
source_url: https://arxiv.org/abs/2507.06221
tags:
- scoring
- score
- review
- rule
- instructor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Aligned Scoring Rule (ASR) to optimize proper
  scoring rules for text elicitation by minimizing mean squared error with reference
  scores. ASR reduces textual elicitation to numerical elicitation using language
  models for summarization and question-answering, then optimizes over separate scoring
  rules to align with human preferences while maintaining properness.
---

# Aligned Textual Scoring Rules

## Quick Facts
- arXiv ID: 2507.06221
- Source URL: https://arxiv.org/abs/2507.06221
- Reference count: 40
- Primary result: Reduces textual elicitation to numerical elicitation using LLM oracles, then optimizes proper scoring rules to align with human preferences while maintaining properness

## Executive Summary
The paper proposes Aligned Scoring Rules (ASR) to optimize proper scoring rules for text elicitation by minimizing mean squared error with reference scores. The method reduces textual elicitation to numerical elicitation using language models for summarization and question-answering, then optimizes over separate scoring rules to align with human preferences while maintaining properness. Experiments on peer grading datasets show ASR achieves nearly identity linear fit with reference scores, outperforming baselines in MSE and Pearson correlation.

## Method Summary
The method reduces textual elicitation to numerical elicitation using LLM oracles for summarization (identifying rubric dimensions) and question-answering (mapping reports to binary states). It then formulates a convex optimization problem to minimize MSE between the scoring rule output and reference scores, subject to properness constraints. The optimization finds a "separate scoring rule" - a weighted average of single-dimensional proper scoring rules - that balances alignment with reference scores against theoretical guarantees of truthfulness.

## Key Results
- ASR achieves MSE of 1.73 vs. 3.74 for constant baseline on instructor score
- Nearly identity linear fit with reference scores (slope close to 1, intercept near 0)
- Outperforms baselines in both MSE and Pearson correlation metrics
- Identifies important rubric dimensions through interpretable V-shaped scoring rules

## Why This Works (Mechanism)

### Mechanism 1: Text-to-Numerical Reduction via LLM Oracles
The paper posits that textual elicitation can be reduced to numerical probabilistic elicitation to inherit provable properness guarantees. The system uses a language model as an oracle to perform two tasks: 1) Summarization, which identifies a finite set of binary rubric dimensions from ground truth text, and 2) Question-Answering, which maps agent text reports to these states (0, 1, or ⊥ for uncertain). Once text is mapped to vectors, standard numerical proper scoring rules apply. The core assumption is that the QA oracle is "non-inverting," meaning the probability of mapping a report to the wrong state is strictly less than 0.5.

### Mechanism 2: Alignment via Convex Optimization
Proper scoring rules can be optimized to align with potentially non-proper reference scores (e.g., human instructor grades) without sacrificing theoretical properness. The method defines a "separate scoring rule" as a weighted average of single-dimensional proper scoring rules. It formulates a convex optimization problem to minimize Mean Squared Error (MSE) between the ASR score and the reference score, subject to the constraints of properness. The core assumption is that the reference score is a valid, albeit noisy or non-proper, signal of quality that can be approximated by a linear combination of rubric scores.

### Mechanism 3: Interpretable Importance Weighting
The optimization process automatically identifies important rubric dimensions by shaping the "convexity" of the scoring rule for those dimensions. The optimization assigns weights and shapes to the V-shaped scoring rules for each dimension. A "flatter" (linear) shape implies the dimension is less important for matching the reference score, while a "sharper" V-shape implies higher importance (higher penalty/reward for deviation from the state).

## Foundational Learning

- **Concept: Proper Scoring Rules**
  - Why needed here: This is the mathematical definition of "truthfulness." You cannot understand ASR without understanding that a "proper" rule makes truth-telling the utility-maximizing strategy for a rational agent.
  - Quick check question: If an agent believes the probability of an event is 80%, does a proper scoring rule incentivize them to report 80% or something else?

- **Concept: Separate Scoring Rules (Aggregation)**
  - Why needed here: The paper restricts the hypothesis space to "separate" rules (weighted averages) to ensure the optimization problem remains convex. Understanding this aggregation is key to interpreting the results.
  - Quick check question: Is the final score calculated by taking the max of all dimension scores, or the weighted average? (Answer: Weighted average, specifically "separate").

- **Concept: Language Model as Oracle**
  - Why needed here: The mechanism relies on the LLM not just generating text, but functioning as a structured "function" that maps unstructured text to discrete states (0/1/⊥).
  - Quick check question: In this architecture, does the LLM generate the rubric dynamically, or is the rubric hard-coded by the engineer? (Answer: Generated dynamically via the Summarization Oracle).

## Architecture Onboarding

- **Component map:** Ground Truth Text + Agent Report -> LLM Summarization + QA Oracles -> Binary Vectors -> Convex Solver -> ASR Output
- **Critical path:** The Summarization Oracle is the highest risk point. If the LLM fails to generate distinct, relevant rubric points, the subsequent optimization will fit noise.
- **Design tradeoffs:** Separate vs. Max-Over-Separate (Separate chosen for convexity and interpretability), Reference Score (LLM-Judge is scalable but noisy; Instructor Score is high-quality but expensive)
- **Failure signatures:** Trivial Solution (optimizer converges to constant score - diagnosis: QA Oracle failing to correlate reports with ground truth states), High Variance (ASR scores swing wildly - diagnosis: Summarization Oracle generating inconsistent rubric dimensions)
- **First 3 experiments:**
  1. Oracle Unit Test: Manually label 20 peer reviews. Run the QA Oracle against them and calculate accuracy. If < 60%, the "non-inverting" assumption is at risk.
  2. Toy Alignment: Create a synthetic dataset where "Reference Score" = 0.5 × Rubric₁ + 0.5 × Rubric₂. Verify the ASR optimization recovers these exact weights (w₁=0.5, w₂=0.5).
  3. Robustness Check: Run ASR with a "shuffled" reference score (labels randomized). Confirm the resulting MSE is high and the rule fails to find structure, proving it isn't just memorizing the dataset size.

## Open Questions the Paper Calls Out
1. Can the optimization framework be extended using neural network function approximators (differentiable economics) to improve performance on larger datasets without sacrificing properness?
2. How does the Aligned Scoring Rule (ASR) perform in domains where the "know-it-or-not" assumption (binary beliefs) does not hold?
3. What is the sensitivity of the mechanism's properness to errors in the summarization and question-answering oracles?

## Limitations
- Requires high-quality LLM oracles; performance degrades if "non-inverting" assumption is violated (oracle error > 0.5)
- Properness guarantees depend on the know-it-or-not model assumption, which may not hold for real-world uncertainty patterns
- Method requires a reference scoring mechanism, which may not be available for all domains or could introduce bias

## Confidence
- **High Confidence:** The mathematical framework for aligned scoring rules is sound, and the convex optimization approach is theoretically valid
- **Medium Confidence:** Empirical results show strong performance on the specific peer grading dataset, but generalization to other domains requires validation
- **Medium Confidence:** The interpretability claims regarding importance weighting are supported by the results, though the relationship between convexity and importance is heuristic

## Next Checks
1. **Oracle Reliability Test:** Manually evaluate the LLM QA oracle accuracy on a held-out sample to verify the non-inverting assumption holds across the dataset
2. **Domain Transfer Test:** Apply ASR to a different text elicitation task (e.g., product review scoring) to assess generalization beyond peer grading
3. **Robustness Test:** Systematically vary the reference score quality (e.g., using different LLM judges vs. human raters) to measure impact on alignment performance