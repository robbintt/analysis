---
ver: rpa2
title: Automatic Pronunciation Error Detection and Correction of the Holy Quran's
  Learners Using Deep Learning
arxiv_id: '2509.00094'
source_url: https://arxiv.org/abs/2509.00094
tags:
- quran
- default
- madd
- more
- rules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a novel ASR-based approach for detecting Quranic
  pronunciation errors by encoding Tajweed rules into a custom Quran Phonetic Script
  (QPS). The system uses a 98% automated pipeline to generate 850+ hours of annotated
  Quranic audio data, with segmentation via fine-tuned wav2vec2-BERT and transcription
  using Tarteel ASR.
---

# Automatic Pronunciation Error Detection and Correction of the Holy Quran's Learners Using Deep Learning

## Quick Facts
- arXiv ID: 2509.00094
- Source URL: https://arxiv.org/abs/2509.00094
- Reference count: 40
- Primary result: 0.16% average Phoneme Error Rate (PER) on test data using a multi-level CTC model trained on 850+ hours of automated Quran recitation data

## Executive Summary
This work introduces a novel ASR-based approach for detecting Quranic pronunciation errors by encoding Tajweed rules into a custom Quran Phonetic Script (QPS). The system uses a 98% automated pipeline to generate 850+ hours of annotated Quranic audio data, with segmentation via fine-tuned wav2vec2-BERT and transcription using Tarteel ASR. A multi-level CTC model achieves 0.16% average Phoneme Error Rate (PER) on test data, demonstrating that the QPS is learnable and enabling automated pronunciation error detection. The approach bridges the gap in high-quality annotated Quranic datasets and provides a comprehensive framework for assessing learners' pronunciation accuracy.

## Method Summary
The method employs a 26-rule phonetization process to convert Uthmani Quranic text into a two-level QPS representation (phonemes and sifat attributes). Audio is segmented using a fine-tuned wav2vec2-BERT model with TimeStretch augmentation, then transcribed via Tarteel ASR and verified using a Tasmeea algorithm. The resulting dataset is used to train a multi-level CTC model with 11 parallel heads (1 phoneme, 10 sifat) attached to a shared wav2vec2-BERT encoder. The model is trained for one epoch with weighted CTC loss, filtering samples longer than 30 seconds.

## Key Results
- Multi-level CTC model achieves 0.16% average Phoneme Error Rate on test data
- Automated pipeline generates 850+ hours of annotated Quranic audio data with 98% automation ratio
- System successfully detects madd, ghunnah, and qalqalah errors in informal testing on actual learner samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Encoding Tajweed rules into a custom script makes pronunciation error patterns machine-detectable.
- Mechanism: The Quran Phonetic Script (QPS) replaces standard orthography with a two-level representation (phoneme + sifa/articulation attributes). This forces the model to learn disentangled representations of both what is said and how it is articulated, transforming a subjective assessment problem into a multi-label sequence learning task.
- Core assumption: The 43 phoneme symbols and 10 sifa attributes defined in QPS provide a complete and unambiguous acoustic mapping for Hafs recitation.
- Evidence anchors:
  - [abstract] "utilizing our custom Quran Phonetic Script (QPS) to encode Tajweed rules (unlike the IPA standard...)"
  - [Section III] "QPS uses a two-level script: (Phoneme level): Encodes Arabic letters with short/long vowels. (Sifa level): Encodes articulation characteristics of every phoneme."
  - [corpus] Corpus evidence for this specific script design is weak; related work uses standard phonemes (GOP, MDD). The novelty here is domain-specific.
- Break condition: If a critical Tajweed rule cannot be represented by the 26 phonetization operations, the script will systematically fail to detect errors for that rule.

### Mechanism 2
- Claim: A multi-level CTC architecture can jointly learn phonetic and articulatory features from speech.
- Mechanism: Instead of a single output head, the model uses 11 parallel heads (1 phoneme, 10 sifa) attached to a shared wav2vec2-BERT encoder. The loss is a weighted average of the CTC loss for each head. This architectural choice compels the encoder to develop internal representations useful for all defined attributes simultaneously.
- Core assumption: The shared speech encoder can capture sufficient information to distinguish fine-grained articulatory characteristics (e.g., Ghunna, Qalqala) without explicit duration modeling.
- Evidence anchors:
  - [abstract] "A multi-level CTC Model which achieved 0.16% average Phoneme Error Rate (PER) on the testset."
  - [Section V] "We compute the loss by averaging all CTC losses for the 11 levels, assigning a weight of 0.4 to the phonemes level..."
  - [corpus] CTC is a standard loss for alignment (arXiv:2509.05399), but its use in a multi-head, multi-task setup for articulation is the key adaptation here.
- Break condition: If the 0.4 weight assigned to the phoneme head is too high, the model may prioritize lexical correctness over articulatory nuance, failing to detect subtle sifa errors.

### Mechanism 3
- Claim: High-quality automated dataset creation can emerge from a pipeline of specialized, fine-tuned components.
- Mechanism: The pipeline chains a fine-tuned wav2vec2-BERT segmenter with a Tarteel ASR model and a Tasmeea verification algorithm. This modular approach replaces manual annotation with a validation/correction loop, creating the 850+ hour dataset necessary for the multi-level model to converge.
- Core assumption: The "golden" recitations used are indeed error-free, and errors introduced by the automated transcription/verification steps are negligible.
- Evidence anchors:
  - [abstract] "A 98% automated pipeline... encompassing: Collection... Segmentation... Transcription... Transcript verification..."
  - [Section IV] "We manually annotated 5400 samples out of 286,537 utterances, resulting for the automation ratio of 98%."
  - [corpus] The corpus shows a trend toward complex pipelines for data generation (e.g., MuFFIN), but the use of a dedicated "Tasmeea" algorithm for transcript verification is a domain-specific innovation.
- Break condition: If the initial segmentation model fails at a pause point (waqf), subsequent transcription and alignment steps will cascade errors, corrupting the final dataset.

## Foundational Learning

- Concept: **Tajweed Rules and Sifat al-Huruf**
  - Why needed here: This is the target domain knowledge. Understanding that Quranic recitation is governed by specific, codified rules (e.g., Madd for elongation, Ghunna for nasalization) and that each letter has inherent attributes (Sifat) is essential to interpret the model's 11 output levels.
  - Quick check question: Can you explain the difference between a phoneme-level error (mispronouncing a letter) and a sifa-level error (mispronouncing an attribute like emphasis/Tafkheem)?

- Concept: **Self-Supervised Speech Learning (SSL)**
  - Why needed here: The core of both the segmenter and the multi-level model is wav2vec2-BERT, an SSL model. Understanding that it learns powerful speech representations from vast unlabeled audio before being fine-tuned is key to grasping why the system works with limited labeled data.
  - Quick check question: What is the primary advantage of using a pre-trained wav2vec2-BERT model over training a speech model from scratch on the 850-hour dataset?

- Concept: **Connectionist Temporal Classification (CTC)**
  - Why needed here: The multi-level model uses CTC loss. This loss function is designed for sequence-to-sequence problems where the alignment between input audio frames and output tokens is unknown, which is critical for mapping continuous speech to the discrete QPS symbols.
  - Quick check question: Why does the paper state they use CTC "without language model integration" for pronunciation error detection?

## Architecture Onboarding

- Component map:
  1. Data Pipeline: Raw Audio -> wav2vec2-BERT Segmenter -> Tarteel ASR -> Tasmeea Algorithm -> QPS Annotated Audio
  2. Phonetizer: Imlaey Text -> Uthmani Text Converter -> 26-rule Phonetizer -> (QPS Phonemes, Sifat)
  3. Multi-Level Model: wav2vec2-BERT Encoder -> 11 Parallel Linear Heads -> 11 CTC Losses -> Weighted Average Loss

- Critical path: The Data Pipeline. The entire system's viability rests on the QPS-annotated dataset. Without the automated segmenter and Tasmeea algorithm producing high-quality training pairs, the multi-level model cannot learn the complex QPS representations.

- Design tradeoffs:
  - QPS vs. IPA: Trading the universality of IPA for the domain-specific precision of QPS. This optimizes for Quranic accuracy but reduces portability to general Arabic.
  - Multi-head CTC vs. Single-head: Using multiple heads increases complexity and training time but forces the model to learn disentangled articulatory features, which is the core goal.
  - No Language Model (LM): An LM would bias the transcript toward fluent speech, which is exactly what must be avoided when the goal is to detect errors. The tradeoff is slightly lower raw accuracy on fluent speech for the ability to detect mistakes.

- Failure signatures:
  1. Silent Failure: The model outputs a "perfect" QPS sequence for a recitation with clear errors. This likely means the training data contained similar errors (contamination from imperfect "golden" recitations).
  2. Cascading Segmentation Errors: If audio is cut mid-word, the Tasmeea algorithm may misalign text, producing garbage training examples and leading to high training loss.
  3. Attribute Blindness: The model cannot learn Sifat that apply to only a single letter (e.g., Istitala for Ø¶), as it may never see enough examples to generalize.

- First 3 experiments:
  1. Phonetizer Validation: Select 20 diverse Quranic verses, manually compute their QPS representation, and compare with the software output. Verify all 26 operations are applied correctly.
  2. Segmenter Boundary Test: Run the fine-tuned wav2vec2-BERT segmenter on unseen Mushafs. Manually inspect 50 random boundaries against the ground truth to confirm the reported 99.35% accuracy.
  3. Multi-level Model Ablation: Train the model with only the phoneme head, then with only a few sifa heads. Compare the PER and convergence speed to the full 11-head model to understand the contribution of the multi-task setup.

## Open Questions the Paper Calls Out

- Question: How does the multi-level CTC model perform on recitations containing actual pronunciation errors when trained exclusively on error-free expert recitations?
  - Basis in paper: [explicit] "Our primary limitation is that our dataset consists of golden recitations with no errors, limiting our ability to evaluate performance on real-world data. Although we tested on a few actual samples and successfully detected madd, ghunnah, and qalqalah errors, we need to develop a comprehensive dataset containing error-containing recitations transcribed with our Quran Phonetic Script."
  - Why unresolved: The 850+ hour dataset contains only expert recitations; evaluation on erroneous samples was limited to informal testing on "a few actual samples" without systematic benchmarking.
  - What evidence would resolve it: Creation and benchmarking of a labeled dataset containing learner recitations with annotated pronunciation errors across all Tajweed rule categories.

- Question: Can the model reliably detect errors in infrequent Tajweed rules (Imala, Rawm, Tasheel) given their rarity in the training data?
  - Basis in paper: [explicit] "This limitation similarly applies to Tajweed rules that occur less frequently in the Holy Quran, such as Imala, Rawm, and Tasheel."
  - Why unresolved: Rare rules provide insufficient training examples for the model to learn robust error detection patterns.
  - What evidence would resolve it: Targeted evaluation on verses containing these rare rules, comparing detection accuracy against high-frequency rules like Ghunna and Madd.

- Question: How can the framework be extended to support non-Hafs Qira'at (recitation methodologies)?
  - Basis in paper: [explicit] "In our study, we considered only Hafs riwayah as it's the most popular recitation method globally."
  - Why unresolved: The phonetizer, dataset criteria, and Moshaf attributes are all designed specifically for Hafs; other Qira'at have different Tajweed rules, variant pronunciations, and Madd lengths.
  - What evidence would resolve it: Adapting the QPS and pipeline for a second Qira'ah (e.g., Warsh) and demonstrating comparable PER performance on its recitations.

## Limitations
- The quality of the automated dataset pipeline is critical; errors in Tarteel ASR or Tasmeea algorithm will propagate into training data
- The QPS system may not fully capture all Tajweed rules, leading to systematic blind spots in error detection
- The model has only been tested on error-free expert recitations, limiting validation of real-world error detection capabilities

## Confidence
- High Confidence: The overall architecture (multi-level CTC with shared encoder) is technically sound and the PER of 0.16% demonstrates the model can learn the QPS representation
- Medium Confidence: The specific design of the QPS script and the completeness of its 26 phonetization operations for all Tajweed rules
- Medium Confidence: The automated data pipeline's quality control and the actual error rate in the final dataset

## Next Checks
1. QPS Completeness Test: Take 100 diverse Quranic verses representing all major Tajweed rules and manually verify that the 26 phonetization operations produce correct QPS representations
2. Dataset Quality Audit: Manually audit 100 randomly selected segments from the final dataset, comparing the Tasmeea-aligned QPS labels against ground truth
3. Generalization Test: Evaluate the trained model on recitations from a different riwayah (e.g., Warsh) or a different Arabic dialect to test whether the model has learned Tajweed-specific patterns or is overfitting to Hafs recitation patterns