---
ver: rpa2
title: Unstable Grounds for Beautiful Trees? Testing the Robustness of Concept Translations
  in the Compilation of Multilingual Wordlists
arxiv_id: '2503.00464'
source_url: https://arxiv.org/abs/2503.00464
tags:
- language
- list
- concept
- data
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates robustness of concept translation in multilingual
  wordlists used for phylogenetic reconstruction in historical linguistics. By comparing
  independently compiled wordlists for 70 language pairs across 9 families, it finds
  that on average only 83% of concept translations yield the same word form, while
  identical phonetic transcriptions occur in just 23% of cases.
---

# Unstable Grounds for Beautiful Trees? Testing the Robustness of Concept Translations in the Compilation of Multilingual Wordlists

## Quick Facts
- arXiv ID: 2503.00464
- Source URL: https://arxiv.org/abs/2503.00464
- Reference count: 19
- Primary result: Concept translation in multilingual wordlists yields only 83% agreement on word forms, with 23% identical phonetic transcriptions

## Executive Summary
This study investigates the robustness of concept translation in multilingual wordlists used for phylogenetic reconstruction in historical linguistics. By comparing independently compiled wordlists for 70 language pairs across 9 language families, the authors find substantial variation in how the same concepts are translated into word forms. Using Sound-Class Based Phonetic Alignments (SCA) with a threshold of 0.5 proves effective for distinguishing phonetic variation from true lexical differences, achieving 98% F-score on a test set. The results suggest that concept translation introduces significant variation that may impact phylogenetic analyses, calling for additional robustness measures such as inter-annotator agreement testing and sampling error assessment.

## Method Summary
The study compares independently compiled multilingual wordlists from the Lexibank repository (v2.0), covering 10 dataset pairs and yielding 70 language pairs across 9 families. Languages are matched using Glottocodes with manual verification, and concepts are matched via Concepticon IDs. Phonetic strings are preprocessed by removing morpheme boundary markers and ignoring tone markers for Southeast Asian languages. SCA distances are computed using LingPy with a 0.5 threshold to distinguish phonetic variation from true lexical differences. Manual annotations using EDICTOR serve as gold standard for evaluating the SCA-based approach on Indo-European subsets, measuring precision, recall, and F-scores.

## Key Results
- Only 83% of concept translations yield the same word form on average across language pairs
- Identical phonetic transcriptions occur in just 23% of cases
- SCA distance thresholding with 0.5 achieves 98% F-score on test set
- Translation variation rates vary significantly across language families
- The study highlights the need for additional robustness measures in comparative linguistic datasets

## Why This Works (Mechanism)

### Mechanism 1 - SCA Distance Thresholding for Phonetic vs. Lexical Distinction
- Claim: Sound-Class Based Phonetic Alignment (SCA) distances with a 0.5 threshold can reliably distinguish transcription variation from genuine lexical differences.
- Mechanism: SCA computes phonetic alignments and produces distance scores from 0 (near identity) to 1 (very low similarity). The authors assume scores <0.5 reflect transcription differences while scores >0.5 reflect different word forms. This avoids false negatives from minor transcription inconsistencies.
- Core assumption: Phonetic transcription variations cluster below 0.5, while genuinely different lexical items cluster above—this distribution holds across diverse language families.
- Evidence anchors:
  - [abstract]: "Using Sound-Class Based Phonetic Alignments (SCA) with a threshold of 0.5 proves effective for distinguishing phonetic variation from true lexical differences, achieving 98% F-score on a test set"
  - [section 3.3]: "SCA distance scores range between 0 (near identity of phonetic sequences) and 1 (very low similarity). Assuming that a score below 0.5 points to differences resulting from phonetic transcriptions, while scores higher than 0.5 result from differences stemming from different translations"
  - [corpus]: Weak direct corpus evidence—no neighboring papers validate this specific threshold approach
- Break condition: If transcription practices vary dramatically across families, a single threshold may not generalize.

### Mechanism 2 - Independent Compilation Introduces Systematic Translation Variation
- Claim: Concept translation in multilingual wordlist compilation is inherently variable because scholars weight evidence differently when selecting among multiple valid candidates or handling concepts lacking direct equivalents.
- Mechanism: Compilers consult informants, published resources, and archives, making subjective choices when concepts have multiple appropriate translations. This creates idiosyncratic variation across independently compiled wordlists for the same language.
- Core assumption: The 17% average translation difference rate reflects genuine ambiguity in concept-to-lexeme mapping, not random errors or dataset quality issues.
- Evidence anchors:
  - [abstract]: "on average only 83% of concept translations yield the same word form"
  - [section 1]: "scholars who compile a wordlist must weight their evidence carefully... A given concept may lack a direct translational equivalent in a given language, or there may be several good candidates from which scholars must select the most appropriate ones"
  - [corpus]: Limited corpus validation—related work on multilingual wordlists exists but doesn't directly test inter-annotator variation rates
- Break condition: If variation primarily reflects dataset errors rather than inherent translation ambiguity.

### Mechanism 3 - Manual Annotation as Validation Ground Truth
- Claim: Expert manual annotation of Indo-European translation pairs provides reliable ground truth for validating automated comparison methods.
- Mechanism: Using EDICTOR, linguists manually annotate whether each translation pair represents identical or different word forms. These annotations serve as gold standard for computing precision, recall, and F-scores of the automated SCA-based approach.
- Core assumption: Expert annotations accurately capture lexical identity ground truth and generalize beyond Indo-European.
- Evidence anchors:
  - [section 3.5]: "translation differences were annotated for all language and concept pairs in the Indo-European datasets of our sample. This allows us to use the manual annotations as a gold standard"
  - [section 4.1, Table 2]: "F-Scores of 0.98 on average for all 15 language pairs"
  - [corpus]: No corpus papers validate this specific annotation methodology
- Break condition: If annotator disagreement is high or Indo-European is unrepresentative of other families.

## Foundational Learning

- Concept: **Phylogenetic reconstruction workflow in historical linguistics**
  - Why needed here: The paper's significance depends on understanding how wordlists feed into cognate identification → cognate coding → phylogeny inference. Translation variation at stage 2 propagates through all downstream stages.
  - Quick check question: Can you trace how a translation difference for "MEAT" (French viande vs. chair/chair humaine) would affect cognate judgments and ultimately tree topology?

- Concept: **Sound-Class Based Phonetic Alignment (SCA) vs. Edit Distance**
  - Why needed here: Understanding why SCA outperforms raw string identity (23%) for detecting same-word pairs requires knowing that SCA groups sounds into classes (e.g., stops, nasals) rather than treating IPA symbols as atomic.
  - Quick check question: Why would SCA correctly identify [vj˜A:d] and [vj˜Ad] as similar when string identity fails?

- Concept: **CLDF/Lexibank/Concepticon/GLottolog infrastructure**
  - Why needed here: This study is only possible because standardized cross-linguistic data formats enable comparison across independently compiled datasets. The three dimensions (language, meaning, form) and reference catalogs (Glottolog for languages, Concepticon for concepts) provide the linkage infrastructure.
  - Quick check question: Why did Glottocode matching alone produce false language-pair matches in Chadic, Japonic, and Koreanic datasets?

## Architecture Onboarding

- Component map: Lexibank repository (18 datasets → 10 language groups → 70 language pairs) -> CLDF format, CLTS phonetic transcription unification, Glottolog language identifiers, Concepticon concept mappings -> Preprocessing: strip morpheme boundary markers (+), ignore tone markers (Southeast Asian languages) -> Matching layer: Glottocode-based language matching → manual verification; Concepticon-based concept matching -> Comparison engine: LingPy SCA distance computation + normalized/traditional edit distance -> Threshold classifier: SCA < 0.5 → similar (same word); SCA ≥ 0.5 → different word -> Evaluation layer: EDICTOR manual annotation → precision/recall/F-score -> Visualization: CLDFViz geographic maps

- Critical path: 1. Extract matching languages via Glottocode, then manually filter to true variety pairs 2. Identify shared concepts via Concepticon intersection 3. Preprocess: remove + markers, strip tones 4. Compute SCA distances for all concept-translation pairs 5. Apply 0.5 threshold → classify similar/different 6. Aggregate: identical rate, similar rate, mean SCA/edit distances per family

- Design tradeoffs:
  - **Threshold 0.5**: Higher threshold increases false negatives (misses genuine differences); lower threshold increases false positives (flags transcription noise as lexical difference). Validated on Indo-European only.
  - **Tone stripping**: Reduces noise for tone languages but may obscure meaningful tonal distinctions in some datasets.
  - **Indo-European validation set**: Well-studied, high-quality data yields reliable ground truth but may not generalize to under-resourced families.
  - **Glottocode vs. manual matching**: Automated Glottocode matching scales but conflates subvarieties; manual matching is accurate but labor-intensive.

- Failure signatures:
  - Identical rate ~23% + similar rate ~83% = expected baseline; similar rate <70% suggests severe dataset incompatibility
  - F-score <0.90 on validation set → threshold or preprocessing misconfigured
  - Glottocode-only matching showing dramatically different results from manual matching (Chadic, Japonic, Koreanic) → language variety conflation

- First 3 experiments:
  1. **Reproduce validation**: Select 3 Indo-European language pairs from the paper, manually annotate 20 concept pairs each, compute SCA distances, verify F-score ≥0.95.
  2. **Threshold sensitivity analysis**: Sample 50 pairs with SCA distances 0.3–0.7; manually classify; plot precision/recall curves for thresholds 0.3–0.7 to confirm 0.5 is optimal.
  3. **Cross-family generalization test**: Apply pipeline to one language family not in the study; compute variation rates; compare to the 83% baseline to assess generalizability.

## Open Questions the Paper Calls Out
None

## Limitations
- The 0.5 SCA threshold was validated only on Indo-European data, raising questions about cross-family generalizability
- Tone stripping for Southeast Asian languages may obscure meaningful distinctions in some datasets
- Selection criteria for the 70 language pairs from initial Glottocode matches are not fully specified
- Manual annotation process lacks detail on inter-annotator agreement and resolution procedures

## Confidence

- High confidence: The existence of substantial variation (17% translation differences on average) and the basic methodology for detecting phonetic vs. lexical differences using SCA
- Medium confidence: The effectiveness of the 0.5 threshold across diverse language families and the impact of this variation on phylogenetic reconstruction accuracy
- Low confidence: The generalizability of findings to under-resourced language families and the optimal preprocessing strategies for different transcription traditions

## Next Checks

1. **Cross-family threshold validation**: Apply the SCA distance threshold approach to 3-5 language families not included in the original study and compare F-scores to the 0.98 baseline to assess generalizability.

2. **Annotation reliability assessment**: Replicate the manual annotation process on 10% of the data with multiple annotators to measure inter-annotator agreement and identify systematic disagreement patterns.

3. **Phylogenetic impact simulation**: Using a controlled dataset, artificially introduce translation variation at different rates (5%, 15%, 25%) and measure downstream effects on cognate identification accuracy and tree topology stability.