---
ver: rpa2
title: Multi-task Pretraining for Enhancing Interpretable L2 Pronunciation Assessment
arxiv_id: '2509.16876'
source_url: https://arxiv.org/abs/2509.16876
tags:
- pronunciation
- features
- assessment
- speech
- phoneme
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-task pretraining (MTP) framework
  for automatic pronunciation assessment (APA) to address the limitation of current
  models that rely solely on segmental-level features, neglecting supra-segmental
  pronunciation cues. The MTP approach jointly models phonetic and prosodic subtasks,
  including phoneme prediction, duration prediction, and pitch/energy prediction,
  during the pretraining of the phoneme encoder.
---

# Multi-task Pretraining for Enhancing Interpretable L2 Pronunciation Assessment

## Quick Facts
- arXiv ID: 2509.16876
- Source URL: https://arxiv.org/abs/2509.16876
- Reference count: 38
- Primary result: MTP improves APA with phoneme-level MSE of 0.077 and utterance-level PCC of 0.804 on speechocean762

## Executive Summary
This paper addresses the limitation of current automatic pronunciation assessment (APA) models that rely solely on segmental features by introducing a multi-task pretraining (MTP) framework. The MTP approach jointly models phonetic and prosodic subtasks during phoneme encoder pretraining, capturing both segmental and supra-segmental pronunciation cues. The framework integrates handcrafted features (HCFs) from automated speaking assessment (ASA) to provide interpretable proficiency scores. Experiments on the speechocean762 corpus demonstrate improved performance, with MTP achieving strong phoneme-level reconstruction and word/utterance-level correlation scores.

## Method Summary
The MTP framework addresses APA limitations by jointly modeling phonetic (phoneme, duration, articulation traits) and prosodic (pitch, energy) features during encoder pretraining. A 3-layer Transformer encoder is pretrained to reconstruct masked phonetic and prosodic features from input sequences. This pretrained encoder is then integrated into a hierarchical E-Branchformer architecture for the main APA model. Handcrafted features (112 selected dimensions) capturing fluency and rhythm metrics are fused at the utterance level to enhance interpretability. The approach combines ModernBERT embeddings for text input with jointly trained acoustic features, creating a comprehensive pronunciation assessment system.

## Key Results
- MTP achieves phoneme-level Mean Squared Error (MSE) of 0.077 on speechocean762
- Word-level Pearson Correlation Coefficient (PCC) reaches 0.794
- Utterance-level PCC improves to 0.804 with MTP, further enhanced to 0.795 with HCF fusion
- Ablation studies show phonetic subtasks contribute more than prosodic subtasks to overall performance

## Why This Works (Mechanism)
The MTP framework works by addressing the fundamental limitation of APA models that focus exclusively on segmental features. By jointly modeling phonetic and prosodic cues during pretraining, the encoder learns richer representations that capture both the discrete phonetic content and the continuous prosodic patterns essential for natural pronunciation. The hierarchical E-Branchformer architecture effectively processes these multi-level features, while the fusion of interpretable handcrafted features provides additional context about fluency and rhythm that correlates with human assessment criteria.

## Foundational Learning
- **Automatic Pronunciation Assessment (APA)**: The task of automatically evaluating L2 speakers' pronunciation quality across multiple granularities (phoneme, word, utterance levels)
  - *Why needed*: Traditional models fail to capture supra-segmental features crucial for natural pronunciation
  - *Quick check*: Verify the model outputs scores at all three levels for validation samples

- **Multi-task Pretraining (MTP)**: Pretraining strategy that jointly optimizes multiple related tasks to learn richer representations
  - *Why needed*: Single-task pretraining misses complementary information available across related tasks
  - *Quick check*: Compare representations learned with and without multi-task objectives

- **Handcrafted Features (HCFs)**: Human-designed metrics like fluency, rhythm, and stress patterns that correlate with pronunciation proficiency
  - *Why needed*: Provides interpretable assessment aligned with human evaluation criteria
  - *Quick check*: Correlate individual HCFs with human expert ratings

- **E-Branchformer Architecture**: Neural architecture designed for efficient sequence modeling with branch structures
  - *Why needed*: Handles variable-length input sequences while maintaining computational efficiency
  - *Quick check*: Monitor training/validation loss curves for convergence

- **Forced Alignment**: Process of aligning speech audio with phonetic transcripts using ASR systems
  - *Why needed*: Provides precise timing information for feature extraction at phoneme level
  - *Quick check*: Visualize alignment results against spectrograms for sample utterances

## Architecture Onboarding

**Component Map**: Input Audio -> TDNN-F ASR -> Feature Extraction -> MTP Pretraining -> E-Branchformer Encoder -> HCF Fusion -> Scoring Heads

**Critical Path**: Audio features → MTP encoder → E-Branchformer → HCF fusion → Final scoring

**Design Tradeoffs**: 
- Joint modeling of phonetic/prosodic features vs. task-specific specialization
- Interpretable handcrafted features vs. end-to-end learned representations
- Hierarchical architecture complexity vs. training efficiency

**Failure Signatures**:
- Poor alignment between forced alignment timestamps and actual speech signals
- Unstable gradients during MTP pretraining due to feature scale mismatches
- HCFs failing to improve performance when fused at finer-grained levels

**3 First Experiments**:
1. Verify TDNN-F forced alignment accuracy by visualizing alignment against spectrograms for 10 random samples
2. Test MTP pretraining with simplified loss weights to confirm basic functionality before full hyperparameter tuning
3. Evaluate HCF selection process by training with top 50, 100, and 150 features to determine optimal dimensionality

## Open Questions the Paper Calls Out
The paper explicitly identifies several open questions: (1) Whether the MTP framework generalizes to L2 learners with native languages other than Mandarin, given current evaluation is restricted to speechocean762 corpus; (2) If dynamic weighting strategies for MTP loss functions could further improve performance by better balancing phonetic and prosodic contributions; (3) Whether HCF fusion can be optimized for phoneme and word-level assessments or remains inherently limited to utterance-level holistic scoring.

## Limitations
- MTP hyperparameters (learning rate, batch size, loss weights) are not fully specified, creating reproducibility challenges
- The exact architecture of projection layers for HCF fusion is not detailed in the methodology
- Current evaluation is limited to Mandarin L1 speakers, raising questions about cross-linguistic generalization

## Confidence
- **High**: Technical soundness of MTP framework and its benefits for capturing segmental and supra-segmental features
- **Medium**: Effectiveness of HCF fusion approach, though implementation details are incomplete
- **Low to Medium**: Reproducibility of reported results due to unspecified hyperparameters and feature engineering steps

## Next Checks
1. **Hyperparameter Verification**: Confirm MTP training details (optimizer settings, loss weights, masking ratio) against reference [7] and ensure alignment with reported results
2. **Feature Extraction Validation**: Test the consistency of TDNN-F forced alignment and prosodic feature extraction across samples, visualizing alignment and F0 contours
3. **HCF Implementation Test**: Reproduce the Lasso-based selection of top 112 HCFs and validate their integration into the utterance-level regressor