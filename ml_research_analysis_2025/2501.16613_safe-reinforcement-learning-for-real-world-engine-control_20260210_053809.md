---
ver: rpa2
title: Safe Reinforcement Learning for Real-World Engine Control
arxiv_id: '2501.16613'
source_url: https://arxiv.org/abs/2501.16613
tags:
- control
- agent
- policy
- safety
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a toolchain for applying Reinforcement Learning
  (RL) in safety-critical real-world environments, using Deep Deterministic Policy
  Gradient (DDPG) for transient load control of a Homogeneous Charge Compression Ignition
  (HCCI) engine. HCCI offers high thermal efficiency and low emissions but poses control
  challenges due to its nonlinear, autoregressive, and stochastic nature.
---

# Safe Reinforcement Learning for Real-World Engine Control

## Quick Facts
- arXiv ID: 2501.16613
- Source URL: https://arxiv.org/abs/2501.16613
- Reference count: 12
- Result: Achieved 0.1374 bar RMSE for IMEP tracking in HCCI engine using DDPG with k-NN safety monitoring

## Executive Summary
This work presents a toolchain for applying Reinforcement Learning in safety-critical real-world environments, demonstrated through transient load control of a Homogeneous Charge Compression Ignition (HCCI) engine. The approach combines DDPG for continuous control with real-time k-nearest neighbor-based safety monitoring to prevent unsafe actions like excessive pressure rise rates and misfires. The system achieves competitive performance compared to traditional neural network controllers while enabling safe exploration of the action space. The toolchain's flexibility is demonstrated by adapting the learned policy to increase ethanol energy shares, promoting renewable fuel use while maintaining safety constraints.

## Method Summary
The method employs DDPG (Deep Deterministic Policy Gradient) for continuous action space control, using an actor-critic architecture with experience replay. The agent interacts with an HCCI testbench to learn transient load control policies, receiving a multi-component reward based on IMEP tracking, pressure gradient limits, and combustion timing. Real-time safety monitoring is implemented through a k-nearest neighbor algorithm that maintains a pre-mapped limitation matrix of safe action boundaries. During training, proposed actions are compared against these boundaries, with violations replaced by safe alternatives and penalized in the reward function. The toolchain is built on the LExCI framework, integrating Ray/RLlib for RL abstractions and TensorFlow for model training, with hardware communication via UDP between an FPGA, MABX processor, and Raspberry Pi.

## Key Results
- Achieved IMEP tracking RMSE of 0.1374 bar, comparable to neural network-based controllers
- Successfully prevented safety violations (pressure gradient limits and misfires) through k-NN safety monitoring during training
- Demonstrated policy adaptation capability by increasing ethanol energy share from 8.8% to 12.5% while maintaining safety
- Showed no increase in pressure gradient violations during adaptation phase, indicating safe transfer learning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A k-nearest neighbor (k-NN) safety monitor can prevent unsafe actions in real-time by interpolating from pre-mapped safe action boundaries.
- **Mechanism:** A dynamic measurement algorithm first explores the action space to populate a limitation matrix R_Lim, storing maximum safe distances from a starting point along predefined directions. During RL training, the agent's proposed action a_Norm is compared against these boundaries; if it exceeds the safe radius R_Safe (computed via weighted k-NN interpolation), the action is replaced with a_Safe,Norm before execution. A reward penalty r_ΔR_Safety scales with the violation distance, shaping the policy toward safe regions.
- **Core assumption:** The safe action space boundaries learned during the measurement phase generalize to the RL exploration phase; process stochasticity is adequately handled by the tolerance window ΔR_Tol = 0.15.
- **Evidence anchors:** [abstract] "real-time safety monitoring based on the k-nearest neighbor algorithm is implemented, enabling safe interaction with the testbench"; [Section 4.3] "Through penalization for exceeding safe boundaries, the agent implicitly learns to stay within the safe region. Thus, after 12,500 training combustion cycles (middle), the percentage of unsafe points decreases significantly"; [corpus] Limited direct corpus support; neighbor papers focus on theoretical safe RL frameworks (e.g., Control Barrier Functions, world model learning) rather than k-NN-based safety shielding.
- **Break condition:** If the agent explores regions not covered by the initial dynamic measurement, safety monitoring cannot guarantee protection (acknowledged in Section 5.2 where monitoring was disabled for policy adaptation).

### Mechanism 2
- **Claim:** DDPG enables sample-efficient learning in continuous action spaces by decoupling policy optimization from data collection via off-policy learning with experience replay.
- **Mechanism:** The actor network μ_θ_μ outputs deterministic actions; Gaussian noise enables exploration. The critic network Q̂_θ_Q approximates the action-value function, trained via TD learning with target networks updated via Polyak averaging (ρ = 10^-3). Experience tuples are stored in a replay buffer (50,000 capacity) and sampled for training batches (64 size), breaking temporal correlations and improving data efficiency.
- **Core assumption:** The HCCI process satisfies the Markov property in closed-loop operation—prior autocorrelation analysis suggests process memory spans only one combustion cycle.
- **Evidence anchors:** [Section 2] "DDPG is a model-free, off-policy, actor-critic algorithm... capable of leveraging existing data through its experience replay buffer"; [Section 4.1] "it is assumed that HCCI fulfills the Markov property in stabilized, closed-loop operation"; [corpus] Weak direct corpus linkage; neighbor papers discuss model-based RL and CBF-based safety but not DDPG specifically for real-world deployment.
- **Break condition:** If process memory exceeds one cycle or if reward sparsity prevents meaningful gradient signals, DDPG convergence may fail or require significantly more samples.

### Mechanism 3
- **Claim:** Transfer learning from a converged safe policy enables adaptation to new objectives (e.g., increased ethanol share) while maintaining safety, even when safety monitoring is disabled.
- **Mechanism:** Starting from the policy trained with safety monitoring, the agent continues learning with reduced exploration noise (σ = 0.3 vs. 0.5) and a new reward term r_Δx_E^Eth for ethanol share tracking. Slow policy updates prevent abrupt transitions into unsafe regions. The previously learned safety constraints persist in the policy's behavioral patterns.
- **Core assumption:** The safe policy learned with monitoring provides a sufficiently conservative starting point that gradual exploration will not encounter unsafe states.
- **Evidence anchors:** [Section 5.2] "Disabling safety monitoring is feasible only because a safe policy was learned before through its use"; [Section 5.2] "no increase in the reward component associated with pressure gradient limitations r_dpMax is observed"; [corpus] Neighbor paper "Safe Online Control-Informed Learning" touches on online adaptation with safety constraints but uses Kalman filtering rather than policy transfer.
- **Break condition:** If the new objective requires exploring fundamentally different action regions or if process dynamics shift significantly, the prior safe policy may not provide adequate coverage.

## Foundational Learning

- **Concept: Markov Decision Processes (MDPs)**
  - **Why needed here:** RL formulation requires the state to capture all relevant information for predicting future states. The paper assumes HCCI satisfies the Markov property with single-cycle memory.
  - **Quick check question:** Can you explain why the state vector s_i-1 must include combustion phasing, IMEP, heat release, and ion current features for the Markov assumption to hold?

- **Concept: Actor-Critic Methods with Experience Replay**
  - **Why needed here:** DDPG's architecture separates policy (actor) from value estimation (critic), enabling stable learning in continuous spaces. Replay buffers break sample correlation.
  - **Quick check question:** Why does Polyak averaging (θ' ← ρθ + (1-ρ)θ' with ρ ≪ 1) for target networks improve training stability compared to direct copying?

- **Concept: k-Nearest Neighbors for Boundary Interpolation**
  - **Why needed here:** Safety monitoring must evaluate actions in real-time (<3ms) against state-dependent boundaries; k-NN provides computationally lightweight interpolation.
  - **Quick check question:** How does weighting neighbors by both distance d_l and counter z_k,l,Lim (reliability measure) affect boundary estimation in stochastic systems?

## Architecture Onboarding

- **Component map:** FPGA (Xilinx Kintex-7) -> MABX Processor (ARM Cortex-A15) -> Raspberry Pi 400
- **Critical path:**
  1. FPGA computes state s_i-1 from pressure and ion current signals by ~50°CA (predicting remaining expansion stroke)
  2. MABX transmits state to RPI (1ms task)
  3. RPI executes policy with noise, returns action to MABX within 3ms (9ms with safety factor)
  4. MABX runs safety monitoring; if violation, replaces action with a_Safe
  5. FPGA applies safe action to actuators
  Total latency budget: 13ms

- **Design tradeoffs:**
  - **Tolerance window ΔR_Tol:** Larger values increase agent exploration but raise risk of minor violations. Paper found 0.15 as a compromise
  - **Noise decay factor λ = 0.95:** Balances early exploration with late-stage exploitation. Faster decay may miss optimal regions
  - **Policy on RPI vs. MABX:** Current implementation uses RPI for policy execution due to TensorFlow Lite constraints on older hardware; introduces latency but enables rapid prototyping. Future work could move policy to MABX III

- **Failure signatures:**
  - **High rate of action replacements (>50%):** Indicates insufficient initial measurement coverage or excessive exploration noise
  - **Pressure gradient violations despite monitoring:** Suggests tolerance window too large or limitation matrix outdated due to process drift
  - **Training instability (oscillating rewards):** May indicate reward function weighting conflicts or learning rate too high
  - **Timeout on RPI computation (>9ms):** Non-real-time system occasionally exceeds window; safety monitoring on MABX provides fallback

- **First 3 experiments:**
  1. **Validate safety monitoring coverage:** Run dynamic measurement algorithm across target load range; verify limitation matrices have sufficient coverage (counter z_k,l,Lim ≫ 1) for all state classes before enabling RL
  2. **Baseline policy convergence:** Train agent from scratch with safety monitoring enabled; confirm reward convergence within ~50,000 cycles and IMEP RMSE <0.2 bar before adaptation experiments
  3. **Controlled policy adaptation test:** Starting from converged policy, enable ethanol share reward with reduced noise (σ=0.3); monitor for safety violations and verify ethanol share increases without pressure gradient degradation

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the safety monitoring function be learned dynamically during the RL training process rather than relying on extensive prior measurements?
- **Basis in paper:** [explicit] The Conclusion states: "Future research could address this by integrating the learning of the safety monitoring function into the RL training process."
- **Why unresolved:** Dynamically learning safety limits alters the environment during training, creating stability challenges that the current static, pre-measured approach avoids.
- **What evidence would resolve it:** Demonstration of an agent learning safe control policies in real-time without needing the preliminary dynamic measurement algorithm phase.

### Open Question 2
- **Question:** How can continuous control RL agents be adapted to handle discrete physical actuation steps, such as minimum injector opening times?
- **Basis in paper:** [inferred] The results show the RL agent struggled with "discrete steps of the actions," specifically the ethanol injector's minimum opening time, resulting in static control offsets.
- **Why unresolved:** The DDPG algorithm assumes a continuous action space, creating difficulties when physical hardware imposes non-differentiable, discrete constraints.
- **What evidence would resolve it:** A modified policy architecture or action space representation that eliminates static IMEP offsets caused by hardware discretization.

### Open Question 3
- **Question:** Is there a systematic method to parameterize the reward function (constants C1-C5) to replace manual iterative tuning?
- **Basis in paper:** [inferred] The methodology states that the reward function parameters were "manually tuned through iterative adjustments during testbench experiments."
- **Why unresolved:** Manual tuning is resource-intensive and subjective, potentially limiting the optimality and reproducibility of the learned policy.
- **What evidence would resolve it:** An automated hyperparameter optimization or inverse reinforcement learning approach that converges to equivalent or superior performance without manual intervention.

## Limitations
- Safety monitoring requires extensive pre-measurement phase to map safe action boundaries, which may not generalize to process variations
- DDPG assumes Markov property which may not hold for all engine states, potentially limiting learning performance
- Manual reward function tuning through iterative experiments is time-consuming and may not yield globally optimal policies

## Confidence
- **Mechanism 1 (k-NN safety monitoring):** Medium - Well-specified and empirically validated, but generalization to arbitrary safety-critical systems remains unproven
- **Mechanism 2 (DDPG with experience replay):** Medium - Standard approach with established theory, but Markov property assumption for HCCI needs rigorous validation
- **Mechanism 3 (Policy adaptation):** Medium - Promising results for modest policy shifts, but slow exploration may not generalize to more aggressive objective changes

## Next Checks
1. **Safety boundary coverage validation:** Systematically evaluate the limitation matrix coverage across the full state-action space using Monte Carlo sampling before RL training begins
2. **Robustness to process drift:** Test the safety monitoring system under perturbed engine conditions (temperature, fuel quality) to assess boundary generalization
3. **Alternative safety methods comparison:** Implement Control Barrier Functions or world model-based safety methods on the same testbed to benchmark against k-NN monitoring performance and computational requirements