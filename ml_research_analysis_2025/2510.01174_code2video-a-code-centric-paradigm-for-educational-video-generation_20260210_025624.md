---
ver: rpa2
title: 'Code2Video: A Code-centric Paradigm for Educational Video Generation'
arxiv_id: '2510.01174'
source_url: https://arxiv.org/abs/2510.01174
tags:
- video
- visual
- videos
- generation
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Code2Video introduces a code-centric agent framework for generating\
  \ educational videos, leveraging executable Python code to achieve precise temporal\
  \ sequencing and spatial organization. The system uses three collaborative agents\u2014\
  Planner, Coder, and Critic\u2014to convert learning topics into structured, high-quality\
  \ tutorial videos."
---

# Code2Video: A Code-centric Paradigm for Educational Video Generation

## Quick Facts
- arXiv ID: 2510.01174
- Source URL: https://arxiv.org/abs/2510.01174
- Reference count: 40
- Code-centric agent framework achieves 40% improvement over direct code generation on educational video benchmark

## Executive Summary
Code2Video introduces a code-centric agent framework for generating educational videos, leveraging executable Python code to achieve precise temporal sequencing and spatial organization. The system uses three collaborative agents—Planner, Coder, and Critic—to convert learning topics into structured, high-quality tutorial videos. Planner structures lecture content and retrieves visual assets, Coder translates instructions into executable Manim code with scope-guided auto-fix, and Critic refines spatial layout using vision-language models with visual anchor prompts. Evaluated on the MMMC benchmark, Code2Video achieves a 40% improvement over direct code generation and produces videos comparable to human-crafted tutorials.

## Method Summary
The Code2Video system follows a three-agent pipeline: (1) Planner generates structured outlines and storyboards while retrieving visual assets from an external database, (2) Coder produces parallel Manim code sections with hierarchical scope-guided auto-fix debugging, and (3) Critic refines spatial layout using visual anchor prompts with VLM feedback. The system leverages Manim's deterministic rendering for precise control and uses a novel TeachQuiz metric for evaluating knowledge transfer through unlearning-relearning protocols.

## Key Results
- 40% improvement over direct code generation baselines on MMMC benchmark
- Comparable quality to human-crafted tutorial videos
- Strong performance across multi-dimensional evaluation including aesthetics, efficiency, and knowledge transfer metrics

## Why This Works (Mechanism)

### Mechanism 1: Code as a Controllable, Interpretable Substrate for Video Synthesis
Representing video generation as executable code (Manim) enables more precise temporal sequencing and spatial organization compared to direct pixel-space synthesis. This allows for symbolic layout, programmatic animation timing, and explicit control over every visual element's position and behavior.

### Mechanism 2: Hierarchical Multi-Agent Decomposition for Complex Generation
Decomposing the video generation task into specialized agents (Planner, Coder, Critic) improves output quality and reliability over end-to-end generation. Each agent focuses on a specific sub-problem—planning, coding, or visual quality control—allowing for more effective problem-solving.

### Mechanism 3: Scope-Guided Auto-Fix for Reliable Code Execution
A hierarchical, scope-guided debugging strategy efficiently repairs generated code errors. When execution fails, the Coder agent first attempts line-level repairs before escalating to block-level and finally full regeneration, minimizing token usage and latency.

## Foundational Learning

- **LLM as a Code Agent**: The Coder agent is an LLM that translates natural language storyboards into syntactically correct Manim code. Failure mode: Hallucinating non-existent functions or incorrect syntax/API usage.
- **Deterministic Rendering Engines (e.g., Manim)**: Manim code produces the same video every time it's run, unlike probabilistic diffusion models. Quick check: How does Manim's declarative nature differ from text-to-video diffusion models?
- **Multimodal Feedback / Vision-Language Models (VLM)**: The Critic agent uses VLMs to watch rendered videos and provide feedback. Quick check: What limitation does the "visual anchor prompt" address in current VLMs?

## Architecture Onboarding

- **Component map**: User Input → Planner (Storyboard) → Coder (Initial Code Generation) → Manim Render → Critic (VLM Feedback) → Refinement Loop → Updated Code
- **Critical path**: Planner (Storyboard) → Coder (Initial Code Generation) → Manim Render. Poor initial plan or code cascades through the entire pipeline.
- **Design tradeoffs**: Code-centric offers perfect text/formula rendering but is limited to Manim style. Parallel code generation speeds execution but reduces global context.
- **Failure signatures**: Infinite debug loops, visual hallucination in code, critic misalignment, asset retrieval failure.
- **First 3 experiments**: (1) Remove Planner to test temporal coherence, (2) Replace visual anchors with continuous coordinates to measure layout success, (3) Compare ScopeRefine efficiency against full debug strategy.

## Open Questions the Paper Calls Out

- **Asset selection efficiency**: Designing more efficient and aesthetic-aware asset selection pipelines remains an open research direction due to occasional retrieval of unusable items.
- **Human attention modeling**: Future work requires agent designs that explicitly account for human attention and patience based on user study results showing sensitivity to duration and layout errors.
- **Attractiveness and consistency improvements**: Limitations remain in AT and VC dimensions, pointing to opportunities for refinement despite overall improvements.
- **Lightweight scalability**: Developing more lightweight, scalable agent frameworks is needed to reduce generation time and token cost.

## Limitations

- Evaluation relies heavily on VLM-as-a-Judge, which may have inherent biases in assessing educational content quality
- TeachQuiz metric depends on specific question construction that could influence results
- Generalizability to educational domains beyond the 3Blue1Brown mathematical focus remains unclear

## Confidence

- **High confidence**: The core code-centric approach and multi-agent architecture are technically sound and well-documented
- **Medium confidence**: The 40% improvement claim requires careful interpretation given the VLM-based evaluation methodology
- **Medium confidence**: The TeachQuiz metric's effectiveness in measuring knowledge transfer needs broader validation

## Next Checks

1. **Cross-domain validation**: Test the system on educational content outside the 3Blue1Brown mathematical focus (e.g., history, science, literature) to assess generalizability
2. **Human evaluation comparison**: Conduct side-by-side human assessments of Code2Video outputs versus traditional methods to validate VLM-based metrics
3. **Scalability stress test**: Evaluate performance and quality consistency when scaling to longer videos (10+ minutes) or more complex topic hierarchies