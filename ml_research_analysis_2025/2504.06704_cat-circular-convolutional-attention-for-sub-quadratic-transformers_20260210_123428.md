---
ver: rpa2
title: 'CAT: Circular-Convolutional Attention for Sub-Quadratic Transformers'
arxiv_id: '2504.06704'
source_url: https://arxiv.org/abs/2504.06704
tags:
- attention
- should
- softmax
- standard
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CAT introduces circular-convolutional attention, which applies
  FFT-based circular convolutions to achieve O(N log N) complexity while preserving
  the global softmax weighting of standard transformers. It requires fewer learnable
  parameters by merging query and key projections and introduces no additional heavy
  operations.
---

# CAT: Circular-Convolutional Attention for Sub-Quadratic Transformers

## Quick Facts
- arXiv ID: 2504.06704
- Source URL: https://arxiv.org/abs/2504.06704
- Authors: Yoshihiro Yamada
- Reference count: 34
- Primary result: O(N log N) complexity via FFT-based circular convolutions while preserving softmax weighting

## Executive Summary
CAT introduces circular-convolutional attention, which applies FFT-based circular convolutions to achieve O(N log N) complexity while preserving the global softmax weighting of standard transformers. It requires fewer learnable parameters by merging query and key projections and introduces no additional heavy operations. Experiments on ImageNet-1k and WikiText-103 show consistent accuracy improvements and approximately 10% speedup over standard attention in naive PyTorch implementations. The method is grounded in an engineering-isomorphism framework and demonstrates that sub-quadratic transformers can maintain softmax-based weighting without sacrificing performance.

## Method Summary
CAT replaces standard quadratic attention with circular convolution computed via FFT. The method learns a single projection matrix that replaces separate query and key projections, applies softmax to create a kernel, and uses FFT/IFFT to compute the convolution efficiently. This achieves O(N log N) theoretical complexity while maintaining exact softmax weighting through circulant matrix properties.

## Key Results
- Achieves O(N log N) complexity through FFT-based circular convolutions
- Maintains exact global softmax weighting unlike kernel-based linear transformers
- Reduces parameters by merging query and key projections
- Shows ~10% speedup over standard attention in PyTorch implementations
- Demonstrates consistent accuracy improvements on ImageNet-1k and WikiText-103

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing quadratic attention matrix multiplication with circular convolution reduces computational complexity to O(N log N) while preserving output fidelity.
- **Mechanism:** Attention operation is mapped to frequency domain using FFT. Instead of calculating N×N attention map, method learns kernel Z* and computes circular convolution (circ(Z*)V) via FFT as IFFT(FFT(Z*) ⊙ FFT(V)).
- **Core assumption:** Token interactions can be effectively modeled by circulant (shift-invariant) structure without losing necessary semantic context compared to dense QK^T interaction.
- **Evidence anchors:** Abstract states "applies FFT-based circular convolutions to achieve O(N log N) complexity"; section 4 explains convolving matrix with XW_V via FFT to avoid naive N×N multiplication.
- **Break condition:** Performance degrades if task requires non-circular, absolute positional relationships that cannot be captured by shift-invariant kernel.

### Mechanism 2
- **Claim:** CAT maintains exact global softmax weighting, distinguishing it from kernel-based linear transformers which approximate softmax.
- **Mechanism:** Leverages algebraic property of circulant matrices: softmax(circ(Z)) ≡ circ(softmax(Z)). Applying softmax to projection vector Z before constructing circulant matrix maintains normalized, global distribution similar to standard attention.
- **Core assumption:** Row-wise normalization property of softmax is critical for training stability and performance, rather than specific query-key dot-product structure.
- **Evidence anchors:** Abstract states "preserving the global softmax weighting of standard transformers"; section 4.2 explains CAT maintains exact row-wise softmax normalization via circulant matrices.
- **Break condition:** If interaction between specific tokens i and j depends on content that cannot be decomposed into circular shift relationship, expressivity is lower than full attention.

### Mechanism 3
- **Claim:** Merging query and key projections reduces parameter count and compute without sacrificing accuracy.
- **Mechanism:** Instead of learning separate matrices W_Q and W_K, CAT learns single projection matrix W_A. This creates attention vector Z* directly, reducing number of linear layers and matrix multiplications in attention block.
- **Core assumption:** Distinct roles of Query and Key in standard attention can be collapsed into single projection space for generating attention weights, provided value stream remains independent.
- **Evidence anchors:** Abstract states "requires fewer learnable parameters by merging query and key projections"; section 6 explains qv approach provides practical balance emulating attention's capacity without quadratic parameter overhead.
- **Break condition:** Accuracy drops significantly in tasks requiring highly asymmetric query-key lookups where "search" key structure differs fundamentally from "document" key structure.

## Foundational Learning

- **Concept:** Fast Fourier Transform (FFT) for Convolution
  - **Why needed here:** Mathematical engine of CAT. Understanding multiplication in frequency domain equals convolution in time domain is required to grasp how paper avoids O(N^2) matrix multiplication.
  - **Quick check question:** If you have two vectors of length N, why is calculating their convolution via FFT generally faster (O(N log N)) than direct sliding window multiplication (O(N^2))?

- **Concept:** Circulant Matrices
  - **Why needed here:** Paper restricts attention matrix to be circulant. Must understand that circulant matrix is fully defined by its first row (the kernel), meaning relationship between tokens repeats or shifts cyclically.
  - **Quick check question:** In a circulant attention matrix, how does relationship between token t and t+1 compare to relationship between t+1 and t+2?

- **Concept:** Row-wise Softmax Normalization
  - **Why needed here:** Paper argues preserving this normalization is key to "Engineering-Isomorphism." Need to know this ensures attention weights for specific token sum to 1, maintaining interpretability and stability.
  - **Quick check question:** Why might model that approximates softmax (like Performers) suffer from training instability compared to one that computes exact softmax?

## Architecture Onboarding

- **Component map:** Input X -> Single linear layer W_A (merging Q/K) projects to Z -> Softmax applied to Z creating kernel Z* -> FFT(Z*) and FFT(V) -> Hadamard Product in frequency domain -> IFFT produces final output. Separate linear layer W_V projects X to V.

- **Critical path:** FFT/IFFT path is theoretical speedup mechanism. However, paper notes discrepancy: Gather-based approach (rolling V via indexing) is often faster in practice on current hardware for moderate N than FFT path.

- **Design tradeoffs:**
  - Gather vs. FFT: Paper implements "Gather" version which is theoretically O(N^2) but practically ~10% faster in PyTorch due to GPU memory access patterns. FFT version is theoretically O(N log N) but has kernel overhead.
  - Expressivity: Restricting attention to circular convolution enforces relative positional bias; paper notes this works well for vision/masked LM but struggles more with causal (autoregressive) masking where complexity can revert to quadratic.

- **Failure signatures:**
  - Causal Degradation: If standard causal masking applied to CAT, efficiency gains may disappear (reverting to O(N^2)) or accuracy may drop if circular wrap-around information not handled carefully.
  - FFT Overhead: On short sequences (e.g., N=256), overhead of launching FFT kernels may outweigh theoretical reduction in FLOPs, resulting in slower wall-clock time than standard attention.

- **First 3 experiments:**
  1. Baseline Speed Verification: Implement "Gather-based" version on ViT-B/16 backbone and verify claimed ~10% iteration speedup over standard nn.MultiheadAttention on V100/A100.
  2. Crossover Analysis: Vary sequence length N (256, 512, 1024, 2048) to find exact point where theoretical FFT implementation becomes faster than Gather implementation.
  3. Ablation on Projections: Retrain small model (e.g., on CIFAR or WikiText-2) comparing "qv" (merged) vs. "qkv" (separate) parameterization to validate claim that merged projections maintain accuracy with fewer parameters.

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical O(N log N) FFT implementation is often slower than O(N²) Gather-based approach on typical hardware due to kernel overhead
- Circular convolution mechanism assumes token interactions can be adequately modeled as shift-invariant relationships, which may not hold for tasks requiring precise absolute positional relationships
- Performance on causal autoregressive tasks remains unclear, as paper notes CAT "struggles more with causal (autoregressive) masking where complexity can revert to quadratic"

## Confidence
- **High** for the core mathematical framework (FFT-based circular convolution reduces complexity to O(N log N) when FFT is faster than Gather)
- **Medium** for the accuracy claims on ImageNet-1k and WikiText-103, as these are specific to tested architectures and datasets
- **Low** for the universal applicability claim, particularly regarding causal tasks

## Next Checks
1. Implement and benchmark the Gather-based CAT on a standard transformer backbone to verify the ~10% speedup claim
2. Conduct sequence length analysis to identify the crossover point where FFT implementation becomes faster than Gather
3. Perform ablation studies comparing merged vs. separate query-key projections to validate parameter efficiency claims