---
ver: rpa2
title: Maximal Update Parametrization and Zero-Shot Hyperparameter Transfer for Fourier
  Neural Operators
arxiv_id: '2506.19396'
source_url: https://arxiv.org/abs/2506.19396
tags:
- learning
- neural
- fourier
- parametrization
- operator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the computational bottleneck of hyperparameter\
  \ tuning for large Fourier Neural Operators (FNOs) by introducing \xB5Transfer-FNO,\
  \ a zero-shot hyperparameter transfer technique. The core idea leverages Maximal\
  \ Update Parametrization (\xB5P) theory to derive a parametrization scheme that\
  \ keeps optimal hyperparameters stable across models with different numbers of Fourier\
  \ modes."
---

# Maximal Update Parametrization and Zero-Shot Hyperparameter Transfer for Fourier Neural Operators

## Quick Facts
- arXiv ID: 2506.19396
- Source URL: https://arxiv.org/abs/2506.19396
- Authors: Shanda Li; Shinjae Yoo; Yiming Yang
- Reference count: 28
- Primary result: Achieves zero-shot hyperparameter transfer across FNO model scales using µP scaling, reducing training compute by ~3x while maintaining accuracy

## Executive Summary
This paper addresses the computational bottleneck of hyperparameter tuning for large Fourier Neural Operators (FNOs) by introducing µTransfer-FNO, a zero-shot hyperparameter transfer technique based on Maximal Update Parametrization (µP) theory. The method enables stable hyperparameter transfer across models with different numbers of Fourier modes by scaling initialization variances and learning rates by Θ(1/√(d log K)), where d is the PDE dimensionality and K is the number of Fourier modes. The approach is validated on Burgers' Equation, Darcy Flow, and Navier-Stokes Equation, demonstrating consistent optimal hyperparameters across model scales and achieving up to 3× reduction in training compute.

## Method Summary
µTransfer-FNO applies Maximal Update Parametrization specifically to FNOs by scaling the initialization variance of the kernel tensor R by Θ(1/√(d log K)) and the Adam learning rate for R by the same factor. This ensures spectral norm stability and consistent feature learning across different model scales. The method involves: (1) scaling the variance of R at initialization, (2) applying FFT → truncate to K modes → multiply by R → inverse FFT in the forward pass, and (3) using scaled learning rates for R during optimization. The approach generalizes to both standard FNO training and Physics Informed Neural Operators (PINO).

## Key Results
- Optimal learning rates, batch sizes, and optimizer configurations remain consistent across model scales under µTransfer-FNO
- On Navier-Stokes Equations, µTransfer-FNO achieves lower test error than naive tuning while using only 30% of the training compute
- The method enables zero-shot hyperparameter transfer across models ranging from 1.7M to 906M parameters
- Successfully validated on Burgers' Equation, Darcy Flow, and Navier-Stokes Equation with different dimensionalities

## Why This Works (Mechanism)

### Mechanism 1: Spectral Norm Stabilization via Initialization Scaling
Scaling the initialization variance of kernel tensor R by Θ(1/√(d log K)) prevents activation explosion in deep layers by keeping the spectral norm of the kernel integral operator at Θ(1) regardless of K. This ensures the forward pass starts in a stable regime. The log K term arises from the expected maximum value of Gaussian random variables, which grows with the logarithm of the set size.

### Mechanism 2: Consistent Feature Learning via Learning Rate Transfer
Scaling the Adam learning rate for R by Θ(1/√(d log K)) ensures the magnitude of weight updates remains proportional to the weight magnitude across different model scales. This correction prevents the network from entering the "lazy training" regime and enables "feature learning" where hidden representations move non-trivially.

### Mechanism 3: Hyperparameter Landscape Invariance
Under µTransfer, the loss landscape with respect to hyperparameters becomes invariant to the model scale, allowing a small proxy model to act as a perfect simulator for the large model. The optimal hyperparameters form a consistent "ridge" that persists across scales, enabling zero-shot transfer.

## Foundational Learning

- **Fourier Neural Operators (FNOs) & Spectral Convolution**: You cannot apply µP without knowing what is being scaled. Unlike standard ConvNets which scale by channel width, FNOs scale by the number of Fourier modes (K) in the spectral convolution layer, which dominates the parameter count (O(K^d)).
  - *Quick check*: If I double the number of Fourier modes K in a 2D FNO, by roughly what factor does the parameter count of the kernel integral operator increase? (Answer: ≈ 2^2 = 4, ignoring edge effects)

- **Maximal Update Parametrization (µP) vs. Standard Parametrization (SP)**: Standard parametrization assumes infinite width averages, while µP is designed for "maximal feature learning." This paper adapts µP specifically for the spectral domain.
  - *Quick check*: Does standard parameter transfer usually work when scaling model width? (Answer: No, optimal LRs typically shrink as width grows, which µP corrects)

- **Sub-Gaussian Random Variables & Extremes**: The theoretical derivation relies on the behavior of the maximum of random variables (spectral norm) rather than the mean. This is why the scaling factor involves √(log K) rather than polynomial terms.
  - *Quick check*: Why does the log K term appear in the scaling factor? (Answer: It arises from the expected maximum value of a set of Gaussian variables, which grows with the logarithm of the set size)

## Architecture Onboarding

- **Component map**: Input → Lifting (P) → Fourier Layers (W, K with trainable R) → Projection (Q) → Output
- **Critical path**: 
  1. Initialize R with variance ∝ 1/√(d log K)
  2. Forward: Apply FFT → Truncate to K modes → Multiply by R → Inverse FFT
  3. Optimizer: Scale Adam LR for R by ∝ 1/√(d log K)
- **Design tradeoffs**: Standard parametrization is easier to implement but requires retuning for every scale. µP requires custom initialization/optimizer hooks but enables zero-shot transfer.
- **Failure signatures**: 
  - Exploding Loss: Initialization variance scaling missing or incorrect
  - Stagnant Loss: Learning rate scaling missing; updates are too small
  - Shifted Optima: If optimal LR changes between K=12 and K=24, implementation is likely incorrect
- **First 3 experiments**:
  1. Sanity Check (1D Burgers): Train small proxy (K=3) and large target (K=24) with standard parametrization. Verify optimal LRs differ.
  2. Pilot Transfer (2D Darcy): Apply Algorithm 1. Tune LR on proxy (K_proxy=12), transfer to target (K=24) using derived scaling. Compare loss curves.
  3. Compute Budget Audit: Measure FLOPs for "Tune on Target" vs. "Tune Proxy + Train Target". Verify ≈3× reduction in compute.

## Open Questions the Paper Calls Out

- Can µTransfer be extended to other neural operator architectures beyond FNO, such as DeepONets, Multipole Graph Neural Operators, Multiwavelet Neural Operators, or Transformer-based foundation models?
- Why does µTransfer-FNO exhibit instability for small K values, and can this be theoretically characterized or remedied?
- How should µP for FNO be derived for optimizers other than Adam (e.g., SGD, AdamW, or second-order methods)?

## Limitations

- The scaling factor is derived specifically for Adam with sub-Gaussian gradients; results may not transfer to other optimizers without re-derivation
- Validation is limited to three PDE types with specific discretization patterns; broader PDE families remain untested
- The method requires tuning a sufficiently large proxy model; if K_proxy is too small, the optimal hyperparameter landscape may not transfer

## Confidence

- **High Confidence**: Mechanism 1 (initialization scaling for spectral stability) - backed by formal proof and clear empirical validation
- **Medium Confidence**: Mechanism 2 (learning rate scaling for consistent feature learning) - supported by theoretical definition but relies on gradient clipping assumption
- **Medium Confidence**: Mechanism 3 (hyperparameter landscape invariance) - empirically validated but theoretical foundation is less explicit

## Next Checks

1. **Gradient Clipping Robustness**: Test µTransfer-FNO without gradient clipping on Navier-Stokes. Measure if test error degrades or training diverges, quantifying the necessity of this component.

2. **Optimizer Generalization**: Apply µTransfer scaling to RMSProp instead of Adam. Compare if optimal LRs still align across K values, or if the scaling breaks.

3. **Scale Gap Sensitivity**: Systematically vary the ratio K_proxy/K_target (e.g., 1:2, 1:4, 1:8). Determine the maximum scale gap where zero-shot transfer still succeeds without retraining the proxy.