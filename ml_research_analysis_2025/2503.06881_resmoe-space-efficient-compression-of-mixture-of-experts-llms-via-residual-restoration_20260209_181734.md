---
ver: rpa2
title: 'ResMoE: Space-efficient Compression of Mixture of Experts LLMs via Residual
  Restoration'
arxiv_id: '2503.06881'
source_url: https://arxiv.org/abs/2503.06881
tags:
- expert
- experts
- resmoe
- pruning
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ResMoE, a method for compressing large-scale
  Mixture-of-Experts (MoE) language models to improve space efficiency during inference.
  The key idea is to extract a common "barycenter expert" from all experts using Wasserstein
  barycenter techniques, then approximate the residuals between this barycenter and
  each original expert using either unstructured pruning or SVD.
---

# ResMoE: Space-efficient Compression of Mixture of Experts LLMs via Residual Restoration

## Quick Facts
- arXiv ID: 2503.06881
- Source URL: https://arxiv.org/abs/2503.06881
- Reference count: 40
- Primary result: ResMoE reduces expert parameters by up to 75% while maintaining comparable performance on GLUE tasks

## Executive Summary
This paper introduces ResMoE, a method for compressing large-scale Mixture-of-Experts (MoE) language models to improve space efficiency during inference. The key idea is to extract a common "barycenter expert" from all experts using Wasserstein barycenter techniques, then approximate the residuals between this barycenter and each original expert using either unstructured pruning or SVD. This approach avoids the information loss that occurs when simply reducing the number of experts. Experiments on Switch Transformer, Mixtral, and DeepSeekMoE models show that ResMoE can reduce expert parameters by up to 75% while maintaining comparable performance. The method is data-agnostic, requires no retraining, and provides a practical solution for deploying large MoE models with reduced memory requirements.

## Method Summary
ResMoE operates by first constructing a design matrix for each expert that flattens the MLP weights into a single matrix where each row represents a sub-MLP "embedding." A free-support Wasserstein barycenter is then computed over these expert design matrices to extract a common barycenter expert that captures shared knowledge. Permutation matrices are derived to optimally align each expert to the barycenter. The residuals between aligned experts and the barycenter are computed and compressed using either unstructured pruning (75% sparsity) or truncated SVD. During inference, experts are restored as the sum of the barycenter and the compressed residuals. The method is applied to top MoE layers only and works in a data-agnostic manner without requiring retraining.

## Key Results
- ResMoE achieves up to 75% reduction in expert parameters while maintaining comparable performance on GLUE tasks
- Approximation error of ResMoE (UP) is 22.05 vs 34.27 for vanilla UP on Switch Transformer
- ResMoE maintains performance across Switch Transformer, Mixtral, and DeepSeekMoE architectures with zero-shot and fine-tuning evaluations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extracting a shared barycenter expert via Wasserstein barycenter preserves common expert knowledge better than expert merging.
- Mechanism: The method treats each expert MLP as an ensemble of bottleneck-1 sub-MLPs, formulates their weight rows as discrete distributions, and computes a free-support Wasserstein barycenter that minimizes the sum of squared Wasserstein distances to all expert distributions. This yields permutation matrices that align experts to a common reference without layer-by-layer propagation.
- Core assumption: Expert weights exhibit redundancy—much of their information is shared and can be captured in a single barycenter, leaving only low-information residuals to compress.
- Evidence anchors:
  - [abstract]: "utilizes Wasserstein barycenter to extract a common expert (barycenter expert) and approximate the residuals"
  - [Section 4.2, Proposition 4.1]: Mathematical proof that the WB solution yields optimal permutations for residual minimization
  - [corpus]: Related methods (Sub-MoE, PuzzleMoE) also exploit expert redundancy via merging but use different alignment strategies; direct comparison evidence is weak
- Break condition: If experts have near-orthogonal knowledge (low shared structure), the barycenter captures little, and residuals remain high-information—compression gains diminish.

### Mechanism 2
- Claim: Compressing residuals (differences from barycenter) achieves lower approximation error than compressing original weights directly.
- Mechanism: After alignment via permutation matrices T_k, the residual Δ_k = T_kW_k - W_ω is computed. These residuals are sparser/lower-rank than original weights, making unstructured pruning or SVD more effective. During inference, experts are restored as Ĵ_k = W_ω + Δ_k.
- Core assumption: Residuals have lower Frobenius norm and information density than original weights after optimal alignment.
- Evidence anchors:
  - [Section 4.3]: "We will store a compressed matrix Δ_k to approximate T_kW_k − W_ω"
  - [Table 1]: ResMoE (UP) achieves 22.05 approximation error vs. 34.27 for vanilla UP on Switch Transformer
  - [corpus]: HEAPr also prunes in output space but uses Hessian-based importance; no direct residual-compression comparison available
- Break condition: If alignment fails (poor permutations), residuals retain high variance and compression degrades to baseline pruning quality.

### Mechanism 3
- Claim: One-shot, data-agnostic compression without retraining is sufficient because expert knowledge is preserved structurally rather than through activation statistics.
- Mechanism: ResMoE operates solely on weight matrices using Wasserstein geometry. No calibration data or gradient computation is needed. The barycenter and residual compression are computed from weight distributions alone.
- Core assumption: Expert specialization is encoded in weight structure, not in data-dependent activation patterns.
- Evidence anchors:
  - [abstract]: "data-agnostic manner without retraining"
  - [Section 5.4]: Zero-shot results on Mixtral (WikiText PPL 5.38 vs. 3.87 original) with no fine-tuning
  - [corpus]: Camera and MoBE papers require data-dependent analysis; corpus evidence for purely weight-based MoE compression is sparse
- Break condition: If downstream tasks require activation-distribution-specific knowledge (e.g., domain-specific calibration), data-agnostic methods may underfit compared to calibration-aware approaches.

## Foundational Learning

- Concept: **Optimal Transport and Wasserstein Distance**
  - Why needed here: Core mathematical tool for computing barycenters that respect the geometry of weight distributions, not just their statistics.
  - Quick check question: Given two discrete distributions μ and ν with equal support size, does minimizing W_2^2(μ, ν) yield a permutation matrix when transport is constrained to uniform marginals?

- Concept: **Permutation Symmetry in Neural Networks**
  - Why needed here: MLPs are equivariant to neuron permutations—this allows alignment of experts to a common reference without changing their function, enabling meaningful barycenter computation.
  - Quick check question: If you permute the hidden neurons of a two-layer MLP via matrix T, how must the input and output weight matrices transform to preserve the input-output mapping?

- Concept: **Frobenius Norm as Proxy for Approximation Quality**
  - Why needed here: The paper uses ||T_kW_k - Ŵ_k||_F as the approximation error metric; understanding this connects weight reconstruction to functional preservation.
  - Quick check question: Why is Frobenius norm a reasonable proxy for function preservation in linear layers, and when might it fail to capture semantic degradation?

## Architecture Onboarding

- Component map:
  - Expert weight matrices -> Design matrix construction -> Barycenter computation -> Permutation alignment -> Residual computation -> Compression (UP/SVD) -> Storage of barycenter + compressed residuals -> Inference restoration

- Critical path:
  1. Flatten each expert into design matrix W_k
  2. Compute Wasserstein barycenter W_ω and transport matrices (use Cuturi & Doucet 2014 algorithm)
  3. Derive permutations T_k, compute residuals Δ_k
  4. Apply unstructured pruning (magnitude-based) or truncated SVD to Δ_k
  5. Store W_ω + {compressed Δ_k}; at inference, reconstruct Ĵ_k = W_ω + Δ_k on demand

- Design tradeoffs:
  - **UP vs. SVD**: Unstructured pruning preserves performance better (Table 2: 93.58 vs. 92.85 on SST-2) but requires sparse matrix storage; SVD yields structured compression with slightly worse accuracy but simpler deployment.
  - **Barycenter overhead**: Storing W_ω adds O(p_I × 2p) parameters; overhead diminishes as expert count N increases (irrelevant at N=64).
  - **Layer selection**: Paper applies to top layers only (24/32 for Mixtral); lower layers may have different redundancy profiles.

- Failure signatures:
  - High approximation error in Table 1 → alignment failed or compression too aggressive
  - Perplexity explosion (>10x original) → expert restoration unstable; check permutation correctness
  - Merge baselines outperforming ResMoE → experts may have near-uniform initialization (copy-paste in Mixtral); barycenter provides little gain

- First 3 experiments:
  1. **Ablation on barycenter method**: Compare Wasserstein barycenter vs. simple averaging vs. Git Re-Basin for center extraction on a single MoE layer; measure ||T_kW_k - W_center||_F.
  2. **Compression rate sweep**: On Mixtral top-8 layers, test 10%, 25%, 50%, 75% retention with UP and SVD; plot LAMBADA accuracy vs. parameter count to find Pareto frontier.
  3. **Expert count scaling**: Run ResMoE on Switch-base-8 (N=8) vs. Switch-base-16 (N=16) vs. DeepSeekMoE (N=64); measure overhead ratio (barycenter params / total params) and performance degradation to validate scalability claims.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ResMoE be effectively applied during the fine-tuning stage rather than only at inference?
- Basis in paper: [explicit] The authors state "ResMoE is currently applied during the model inference stage. The resulting performance of applying it to fine-tuning is an open question that requires further investigation."
- Why unresolved: The paper only validates ResMoE in zero-shot and fine-tuned-then-compressed settings, not when compression occurs before or during fine-tuning.
- What evidence would resolve it: Experiments comparing models fine-tuned with ResMoE applied beforehand versus the current inference-only approach, measuring downstream task performance.

### Open Question 2
- Question: Would heterogeneous compression rates across layers or individual experts improve the accuracy-compression tradeoff compared to uniform compression?
- Basis in paper: [explicit] The authors state "The future direction of this work can be the exploration of adopting different compression rates for each layer or even each expert."
- Why unresolved: All experiments use a fixed 25% retention rate uniformly; the potential benefit of adaptive compression based on layer/expert importance remains unexplored.
- What evidence would resolve it: Ablation studies with varying compression rates per layer or expert, guided by metrics like router activation frequency or layer-wise sensitivity analysis.

### Open Question 3
- Question: How can the practical storage overhead of sparse residual matrices from unstructured pruning be reduced?
- Basis in paper: [explicit] The authors acknowledge "the space efficiency of storing the sparse matrices obtained from unstructured pruning is limited" and note that COO format indices can consume more memory than saved (840MB vs 672MB original).
- Why unresolved: The paper suggests int16 indices or CSR format could help but does not implement or evaluate these solutions.
- What evidence would resolve it: Implementation and benchmarking of alternative sparse storage formats (CSR, int16 indices, block sparse) with actual memory measurements.

### Open Question 4
- Question: What performance gains or tradeoffs arise from combining ResMoE with hardware-oriented quantization methods?
- Basis in paper: [explicit] The authors mention "further combining our method with hardware quantization methods" as a future direction.
- Why unresolved: ResMoE focuses on parameter reduction through pruning/SVD; synergies or conflicts with quantization (INT8, INT4) have not been investigated.
- What evidence would resolve it: Experiments applying quantization to the barycenter expert and compressed residuals, reporting accuracy, memory footprint, and inference latency.

## Limitations

- Architecture-dependent effectiveness: ResMoE's compression gains depend on expert redundancy, which is artificially inflated in Mixtral due to copy-paste initialization; performance degrades on DeepSeekMoE where Sub-MoE outperforms ResMoE
- Memory overhead caveat: Unstructured pruning with COO sparse format uses 64-bit indices that can negate parameter savings on small expert counts (N=8), though this becomes negligible at larger N
- Limited validation scope: Experiments only validate on publicly available MoE models (Switch, Mixtral, DeepSeek) without testing on proprietary or task-specific MoE variants where expert specialization might be higher

## Confidence

- **High Confidence**: The mathematical framework (Wasserstein barycenter, permutation alignment, residual compression) is rigorously specified and the proof of optimal permutation alignment (Proposition 4.1) is sound. The space-efficiency mechanism is well-explained and the approximation error improvements over vanilla UP are clearly demonstrated.
- **Medium Confidence**: The performance preservation claims are supported by experimental results, but the comparison with other compression methods (MoBE, Sub-MoE, PuzzleMoE) is incomplete. The ablation study on layer selection (top-8 vs. bottom layers) is missing, and the effect of expert count scaling on compression gains needs more systematic evaluation.
- **Low Confidence**: The claim that ResMoE works "without retraining" for all downstream tasks is based on limited zero-shot and fine-tuning experiments. The paper does not test on domain-specific MoE models or examine whether activation statistics play a role in expert specialization that weight-based compression cannot capture.

## Next Checks

1. **Architecture Transfer Test**: Apply ResMoE to a proprietary MoE model with high expert specialization (e.g., domain-specific medical/legal MoE) and measure whether barycenter extraction still captures sufficient shared knowledge. Compare performance degradation against Sub-MoE and PuzzleMoE baselines.

2. **Memory Overhead Analysis**: Implement custom int16 CSR sparse storage for UP-compressed residuals and measure actual disk/RAM savings across Switch-base-8, Mixtral-8x7B, and DeepSeekMoE. Verify that the 75% compression claim holds when accounting for index storage overhead.

3. **Layer-wise Compression Efficacy**: Run ResMoE separately on bottom, middle, and top MoE layers of Mixtral. Measure approximation error and downstream task performance for each layer group to determine if top-layer compression alone is optimal or if a mixed-strategy (different compression per layer) yields better Pareto efficiency.