---
ver: rpa2
title: A Unified Speech LLM for Diarization and Speech Recognition in Multilingual
  Conversations
arxiv_id: '2507.02927'
source_url: https://arxiv.org/abs/2507.02927
tags:
- speech
- speaker
- diarization
- task
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of joint speaker diarization
  and multilingual speech recognition in conversational audio. The authors propose
  a unified speech LLM approach that reformulates the training data format to include
  speaker and timestamp tokens interleaved with transcribed text, enabling the model
  to jointly predict speaker identity and transcription in context.
---

# A Unified Speech LLM for Diarization and Speech Recognition in Multilingual Conversations

## Quick Facts
- arXiv ID: 2507.02927
- Source URL: https://arxiv.org/abs/2507.02927
- Reference count: 0
- Primary result: Unified speech LLM achieves 54.87% relative improvement in tcpWER/tcpCER over baseline (27.25 vs 60.39)

## Executive Summary
This paper addresses joint speaker diarization and multilingual speech recognition in conversational audio through a unified speech LLM approach. The authors reformulate training data to include speaker and timestamp tokens interleaved with transcribed text, enabling the model to jointly predict speaker identity and transcription in context. Using locally sliding inference windows with speaker context from prior turns, the model resolves ambiguities in turn-taking and maintains speaker coherence. Evaluated on the MLC-SLM Task II, their Llama-3.2-3B-instruct-based system achieved significant improvements over the baseline, ranking 8th overall with a 54.87% relative improvement in tcpWER/tcpCER.

## Method Summary
The method uses a unified token sequence format that interleaves special speaker tokens (`<|SPK0|>`, `<|SPK1|>`) and timestamp tokens with transcribed text during training. The architecture consists of a frozen Whisper-large-v3 encoder, a sub-sampling projector with 2D convolutional and linear layers, and a Llama-3.2-3B-instruct model with LoRA adaptation. Training involves 130k iterations with SpecAugment and speed perturbation augmentation on a single RTX 6000 GPU. Inference processes audio in 24-second segments using dynamic start times based on previous speaker turns, with a 4-turn sliding context window. Post-processing aligns local LLM outputs with global diarization results from 3D-Speaker to improve speaker consistency.

## Key Results
- Achieved 54.87% relative improvement in tcpWER/tcpCER over baseline (27.25 vs 60.39) on MLC-SLM Task II
- Reduced diarization error rate from 16.44% to 13.36% through global alignment with 3D-Speaker
- Ranked 8th overall in MLC-SLM Challenge Task II with 0.1491 WER/CER score
- Task I (oracle-segmented ASR) results showed 25.56% WER/CER, ranking 20th with extended training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Jointly modeling speaker identity and transcription in a unified token sequence enables context-aware disambiguation of speaker turns.
- Mechanism: The model interleaves special speaker tokens (`<|SPK0|>`, `<|SPK1|>`) and timestamp tokens with transcribed text during training. This unified format allows the LLM to condition transcription on speaker identity and vice versa, rather than treating diarization and ASR as separate pipeline stages.
- Core assumption: Ambiguous speaker transitions can be resolved when both acoustic and textual context are available to the model simultaneously.
- Evidence anchors:
  - [abstract] "By reformulating the training data format and modifying the inference procedure, our model addresses the ambiguity inherent in pre-segmented audio."
  - [Section 2.2] "This format interleaves speaker tokens and timestamp tokens with transcribed text, allowing the model to generate speaker-labeled utterances with temporal alignment."
  - [corpus] SpeakerLM (arXiv:2508.06372) similarly proposes end-to-end diarization and recognition with multimodal LLMs, supporting the joint modeling paradigm.
- Break condition: If conversations exceed 2 speakers (model only supports `<|SPK0|>` and `<|SPK1|>`), the unified format cannot represent additional speakers.

### Mechanism 2
- Claim: Iterative local window inference with sliding transcript context maintains speaker consistency across long conversations.
- Mechanism: Audio is processed in segments up to 24 seconds. Each segment's start time is dynamically set using the previous chunk's second-to-last speaker turn end time. The prompt includes the 4 most recent speaker turns and the predicted speaker token from the prior segment, enabling continuity.
- Core assumption: Speaker identity can be propagated across segments via textual context even when the model lacks access to the full audio.
- Evidence anchors:
  - [Section 2.4.1] "For all subsequent segments, the prompt includes a sliding context window consisting of the four most recent speaker turns from previous chunks, along with the second-to-last predicted speaker token."
  - [Section 2.4.1] "This mechanism enables the model to carry contextual information forward and resolve ambiguity in turn-taking."
  - [corpus] Corpus does not provide direct evidence for this specific inference strategy; mechanism remains unvalidated externally.
- Break condition: If silence or hallucination produces no valid turns, the model falls back to a 0.3s offset advancement, which may misalign subsequent segments.

### Mechanism 3
- Claim: Post-hoc alignment with external diarization output corrects local speaker assignment errors.
- Mechanism: A global diarization module (3D-Speaker) generates RTTM files with consistent speaker identities. The LLM's local STM outputs are aligned by overlapping timestamps, assigning the RTTM speaker label with greatest temporal overlap. Adjacent segments with matching speakers are merged.
- Core assumption: External diarization provides more globally consistent speaker clustering than the LLM can achieve locally.
- Evidence anchors:
  - [Section 2.4.2] "These inconsistencies degrade speaker coherence and contribute to a higher diarization error rate (DER) over full audio."
  - [Section 2.5] "These gains are attributed to the integration of diarization outputs from the 3D-Speaker system with our Speech LLM."
  - [corpus] SE-DiCoW (arXiv:2601.19194) similarly conditions ASR on diarization outputs, supporting hybrid approaches.
- Break condition: If RTTM and STM segments have poor temporal overlap, the original LLM label is retained, potentially preserving errors.

## Foundational Learning

- Concept: Speaker Diarization Error Rate (DER)
  - Why needed here: Primary metric for evaluating speaker attribution accuracy; the model reduced DER from 16.44% to 13.36% vs. baseline.
  - Quick check question: Given overlapping speech segments, how would you compute the proportion of time with incorrect speaker labels?

- Concept: time-constrained pitER calculation (tcpWER/tcpCER)
  - Why needed here: Task II evaluation metric combining diarization and transcription errors with a 5-second collar tolerance.
  - Quick check question: Why does tcpWER penalize both word errors and speaker attribution errors jointly rather than separately?

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: The LLM backbone uses LoRA adaptation to reduce trainable parameters while keeping the base model frozen.
  - Quick check question: If LoRA rank is too low, what symptom would you expect in multilingual fine-tuning?

## Architecture Onboarding

- Component map: Whisper-large-v3 encoder -> Sub-sampling projector -> Learnable prompt embeddings -> Llama-3.2-3B-instruct + LoRA -> Special tokens
- Critical path: Audio → Whisper encoder → Sub-sampling projector → Prompt + speech tokens → LLM → Speaker-labeled transcript with timestamps.
- Design tradeoffs:
  - 3B vs. 8B backbone: Smaller model reduces memory but may underperform on complex languages (Thai showed slight reverse trend).
  - Local window inference vs. full audio: Limits global context but enables processing long conversations without memory explosion.
  - External diarization alignment: Improves DER but introduces pipeline dependency and potential alignment failures.
- Failure signatures:
  - No valid turns returned: Silence or hallucination; triggers 0.3s offset fallback.
  - Cascading speaker assignment errors: Local context insufficient; requires global alignment correction.
  - Language-specific degradation: Thai showed 49.47% WER at 1 epoch; may indicate data scarcity or overfitting.
- First 3 experiments:
  1. Reproduce Task II baseline comparison on development set: train unified format model with Llama-3.2-3B-instruct, evaluate tcpWER and DER against baseline.
  2. Ablate sliding context window size: test 0, 2, 4, 8 previous turns to measure impact on speaker consistency.
  3. Validate global alignment contribution: compare DER and tcpWER with and without 3D-Speaker RTTM alignment post-processing.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What mechanisms cause performance degradation in specific languages (e.g., Spanish, Australian English) when extending training from 1 to 2.5 epochs in Task I?
- Basis: [explicit] The paper reports that while extending training improved many languages, Australian English and Spanish showed "notable degradation."
- Why unresolved: The authors acknowledge the results but do not isolate whether the cause is data imbalance, catastrophic forgetting, or hyperparameter sensitivity.
- What evidence would resolve it: Ablation studies analyzing validation loss curves per language and experiments with language-specific learning rate schedules.

### Open Question 2
- Question: Why does the smaller 1B parameter model outperform the larger 3B model on the Thai language subset?
- Basis: [explicit] The authors note that Thai was the only language where the 1B model achieved lower tcpWER than the 3B model, speculating on "domain variability or overfitting sensitivity."
- Why unresolved: This contradicts standard scaling laws observed in other languages and the paper provides no definitive causal analysis.
- What evidence would resolve it: Comparative analysis of the Thai training data distribution and regularization tests on the 3B model to assess overfitting.

### Open Question 3
- Question: Can the proposed Speech LLM maintain global speaker consistency without relying on the external 3D-Speaker diarization module for alignment?
- Basis: [inferred] The method uses a separate global diarization module to generate RTTM files because the LLM "lacks global context," leading to cascading errors if used in isolation.
- Why unresolved: It is undetermined if the unified training format can be extended to resolve global speaker identity independently of external toolkits.
- What evidence would resolve it: Evaluation of the system's DER and tcpWER when the external diarization alignment post-processing step is completely removed.

## Limitations
- Two-speaker assumption: The model only supports `<|SPK0|>` and `<|SPK1|>` tokens, making it fundamentally incapable of handling conversations with more than two speakers.
- Unvalidated inference strategy: The sliding window approach lacks external validation despite being described mechanistically.
- Pipeline dependencies: Reliance on external diarization (3D-Speaker) for global speaker alignment introduces a dependency that may not generalize.

## Confidence

- **High Confidence**: The unified training format using interleaved speaker and timestamp tokens is well-supported by the literature and authors' results.
- **Medium Confidence**: The sliding window inference with 4-turn context is logically sound but lacks external validation.
- **Low Confidence**: The global diarization alignment contribution is difficult to isolate from other improvements.

## Next Checks

1. **Ablation study of sliding context window size**: Systematically test inference with 0, 2, 4, and 8 previous turns to quantify the exact contribution of contextual information to speaker consistency.

2. **Isolation of global alignment contribution**: Train and evaluate the model with and without the 3D-Speaker RTTM alignment step to definitively measure how much DER and tcpWER improvement comes from this post-processing versus the core LLM architecture.

3. **Stress test with more than two speakers**: Create or find test data with 3+ speakers and evaluate whether the model degrades gracefully or catastrophically.