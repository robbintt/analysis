---
ver: rpa2
title: 'Hybrid Annotation for Propaganda Detection: Integrating LLM Pre-Annotations
  with Human Intelligence'
arxiv_id: '2507.18343'
source_url: https://arxiv.org/abs/2507.18343
tags:
- propaganda
- label
- labels
- annotation
- fine-grained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an LLM-assisted framework for propaganda
  detection that combines automated span extraction with human verification. A hierarchical
  taxonomy groups 14 fine-grained propaganda techniques into three broader categories
  based on intent.
---

# Hybrid Annotation for Propaganda Detection: Integrating LLM Pre-Annotations with Human Intelligence

## Quick Facts
- **arXiv ID**: 2507.18343
- **Source URL**: https://arxiv.org/abs/2507.18343
- **Reference count**: 19
- **Primary Result**: Hybrid LLM-human framework improves propaganda detection annotation efficiency and agreement

## Executive Summary
This paper introduces a hybrid annotation framework that combines automated LLM pre-annotations with human verification for propaganda detection tasks. The system addresses the challenge of low inter-annotator agreement in fine-grained propaganda detection by first using LLMs to extract propagandistic spans and assign labels, then having human annotators verify these outputs. The framework employs a hierarchical taxonomy that groups 14 fine-grained propaganda techniques into three broader categories based on intent. The approach demonstrates significant improvements in annotation efficiency and agreement while maintaining reasonable detection performance.

## Method Summary
The proposed framework follows a two-stage process: automated pre-annotation followed by human verification. Initially, LLMs extract propagandistic spans from text and assign both local (span-specific) and global (document-level) propaganda labels. Human annotators then verify these LLM outputs, correcting errors and resolving ambiguities. The framework incorporates a hierarchical taxonomy that simplifies the complex fine-grained propaganda techniques into three intent-based categories. Additionally, the paper explores knowledge distillation, where smaller language models are fine-tuned on the LLM-generated annotations to improve scalability and reduce computational costs.

## Key Results
- Inter-annotator agreement (Krippendorff's Alpha) increased from 0.1233 to 0.5941 for fine-grained labels
- Annotation time per tweet reduced from 151.70 seconds to 41.14 seconds
- Smaller LMs fine-tuned via knowledge distillation achieved reasonable performance on span detection and global-label prediction

## Why This Works (Mechanism)
The framework leverages LLMs as intelligent pre-annotation tools that provide initial span identification and labeling, reducing the cognitive load on human annotators. By presenting annotators with pre-identified spans and suggested labels, the system streamlines the verification process, allowing annotators to focus on validation rather than initial identification. The hierarchical taxonomy simplifies the complex decision space by grouping similar propaganda techniques under broader intent categories, making fine-grained annotation more manageable. The knowledge distillation approach transfers the learned patterns from large LLMs to smaller, more efficient models while maintaining performance.

## Foundational Learning

1. **Propaganda Detection Taxonomy**: Hierarchical grouping of 14 fine-grained techniques into 3 intent-based categories
   - *Why needed*: Simplifies complex labeling task and improves annotation consistency
   - *Quick check*: Verify taxonomy coverage matches all identified propaganda techniques

2. **LLM-Assisted Annotation**: Using pre-trained models to generate initial annotations
   - *Why needed*: Reduces human workload and provides consistent baseline annotations
   - *Quick check*: Compare LLM output quality against human baseline annotations

3. **Knowledge Distillation**: Transferring knowledge from large models to smaller ones
   - *Why needed*: Enables deployment of efficient models while maintaining performance
   - *Quick check*: Measure performance degradation between teacher and student models

4. **Inter-Annotator Agreement Metrics**: Using Krippendorff's Alpha to measure consistency
   - *Why needed*: Provides objective measure of annotation quality and reliability
   - *Quick check*: Calculate agreement both before and after human verification

## Architecture Onboarding

**Component Map**: Raw Text -> LLM Pre-annotation -> Human Verification -> Fine-tuned Model

**Critical Path**: The core workflow involves text input, LLM processing to generate spans and labels, human verification of these outputs, and finally knowledge distillation to smaller models. The human verification stage serves as the quality control mechanism.

**Design Tradeoffs**: The framework trades initial annotation accuracy for speed and scalability by using LLM pre-annotations. While this approach may introduce some errors, the human verification stage catches and corrects these issues. The hierarchical taxonomy sacrifices some granularity for improved agreement and efficiency.

**Failure Signatures**: The system may struggle with novel propaganda techniques not well-represented in training data, leading to systematic mislabeling. Local-label assignment remains particularly challenging, potentially due to the fine-grained nature of propaganda technique classification.

**First Experiments**:
1. Run the pre-annotation pipeline on a small validation set and measure LLM accuracy before human verification
2. Conduct a pilot human verification study with 10-20 samples to assess annotation agreement improvements
3. Compare performance of knowledge-distilled models against the original LLM on a held-out test set

## Open Questions the Paper Calls Out
None

## Limitations

- **Single LLM Dependency**: The framework relies on one LLM model, raising concerns about generalizability and potential model-specific biases
- **Moderate Agreement Levels**: Despite improvements, inter-annotator agreement remains at only moderate levels (0.5941), suggesting residual annotation challenges
- **Limited Performance Validation**: The evaluation focuses on annotation efficiency rather than downstream detection accuracy compared to traditional methods

## Confidence

- **High Confidence**: The observation that human verification improves annotation agreement and reduces time is well-supported by the reported metrics
- **Medium Confidence**: Claims about the hierarchical taxonomy's effectiveness in simplifying fine-grained propaganda techniques are plausible but not rigorously validated
- **Low Confidence**: The assertion that knowledge distillation from LLM annotations yields "reasonable performance" lacks comparative baselines against models trained on purely human-annotated data

## Next Checks

1. Conduct cross-model validation by replicating the pre-annotation pipeline with multiple LLMs to assess robustness and identify model-specific biases

2. Perform a comprehensive downstream evaluation comparing propaganda detection accuracy of models trained on LLM-assisted annotations versus those trained exclusively on human-annotated data

3. Expand the human verification study to include diverse annotator backgrounds and expertise levels to determine whether observed agreement improvements generalize across different annotation teams