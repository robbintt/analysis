---
ver: rpa2
title: 'Passing the Baton: High Throughput Distributed Disk-Based Vector Search with
  BatANN'
arxiv_id: '2512.09331'
source_url: https://arxiv.org/abs/2512.09331
tags:
- search
- graph
- vector
- query
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BatANN, a distributed disk-based vector search
  system designed to scale beyond the capacity of a single server while maintaining
  logarithmic search efficiency. The key innovation is a state-passing query procedure
  that sends the full query state to another server when a neighbor is stored remotely,
  enabling asynchronous execution and improved data locality compared to traditional
  request-reply patterns.
---

# Passing the Baton: High Throughput Distributed Disk-Based Vector Search with BatANN

## Quick Facts
- **arXiv ID:** 2512.09331
- **Source URL:** https://arxiv.org/abs/2512.09331
- **Reference count:** 38
- **Primary result:** BatANN achieves 6.21-6.49x throughput of scatter-gather baseline at 0.95 recall, maintaining sub-6ms latency on 100M-1B point datasets.

## Executive Summary
This paper introduces BatANN, a distributed disk-based vector search system designed to scale beyond the capacity of a single server while maintaining logarithmic search efficiency. The key innovation is a state-passing query procedure that sends the full query state to another server when a neighbor is stored remotely, enabling asynchronous execution and improved data locality compared to traditional request-reply patterns. This approach reduces communication overhead and avoids bottlenecks common in distributed search systems.

BatANN is evaluated on 100M- and 1B-point datasets, achieving 6.21–6.49x and 2.5–5.10x the throughput of a scatter-gather baseline at 0.95 recall, respectively, while maintaining mean latency below 6 ms on standard TCP. The system preserves the computational and I/O efficiency of single-server DiskANN while enabling near-linear scaling across multiple servers. To the authors' knowledge, BatANN is the first open-source distributed disk-based vector search system to operate over a single global graph.

## Method Summary
BatANN builds upon DiskANN's Vamana graph structure, partitioning it across multiple servers to scale capacity while preserving search efficiency. The system uses a state-passing query procedure where the full search state (beam, results, parameters) is serialized and sent to remote servers rather than using request-reply patterns. A modified I/O pipeline heuristic prioritizes local candidates to minimize disk reads and distance computations. The system employs ZeroMQ for communication, io_uring for asynchronous I/O, and an in-memory head index for initial routing. Graph partitioning is performed using an algorithm derived from Gottesburen et al. to maximize data locality and minimize inter-partition hops.

## Key Results
- BatANN achieves 6.21-6.49x throughput improvement over scatter-gather baseline on 100M-point dataset at 0.95 recall
- Maintains mean latency below 6ms on standard TCP while scaling to 1B-point datasets
- Inter-partition hops account for only 11.6-24.3% of total hops, validating the graph partitioning approach
- Preserves near-identical I/O efficiency and computational characteristics compared to single-server DiskANN

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Asynchronous state passing reduces communication latency compared to synchronous request-reply patterns.
- **Mechanism:** Instead of a coordinator requesting remote data and waiting (blocking), the system serializes the full query state (beam, results, parameters) and "passes the baton" to the server holding the relevant data. That server continues execution immediately.
- **Core assumption:** The network transfer and deserialization of the query state (approx. 4-8 KB) is faster than the round-trip latency of a request-reply cycle, and the remote server has immediate capacity to process the incoming state.
- **Evidence anchors:**
  - [abstract] "state-passing query procedure that sends the full query state to another server... enabling asynchronous execution."
  - [section 4] "latency penalty from communication is cut in half relative to requesting data... as soon as the query state reaches the other server, it can take over."
  - [corpus] The corpus entry for DISTRIBUTEDANN highlights a centralized orchestration layer, which BatANN explicitly avoids to prevent locking resources.
- **Break condition:** If the beam size ($L$) or result set grows excessively large (e.g., >400), the state size may exceed the network's ability to transfer it efficiently over standard TCP, negating the latency benefits.

### Mechanism 2
- **Claim:** Graph partitioning minimizes the frequency of expensive inter-server hops.
- **Mechanism:** The system uses a partitioning algorithm (derived from Gottesburen et.al) to place graph neighbors on the same physical server. This maximizes "data locality," ensuring that beam search traversal stays on a single machine for as many steps as possible.
- **Core assumption:** The dataset's vector distribution allows for spatial clustering where nearest neighbors in the graph are also close enough in vector space to be grouped onto the same partition without causing severe load imbalance.
- **Evidence anchors:**
  - [section 4.3] "inter-partition hops only account for 11.6-24.3% of total hops, validating the choice to use graph partitioning."
  - [figure 3] Shows that total hops remain constant while inter-partition hops are kept to a minority.
  - [corpus] Related work (HAKES, ARCADE) focuses on indexing/modality; this mechanism specifically targets network locality.
- **Break condition:** If the dataset is uniformly random or high-dimensional with no clusters, partitioning algorithms may fail to provide locality, causing the search to "ping-pong" between servers, degrading throughput.

### Mechanism 3
- **Claim:** A modified I/O pipeline heuristic preserves computational efficiency across servers.
- **Mechanism:** Algorithm 2 adapts the I/O pipeline width ($W$) for distributed environments. If the top candidates reside on the local server, it processes them locally. If top candidates are remote, it prefers local candidates if available, or transfers state only when necessary. This prevents wasted disk reads and distance computations.
- **Core assumption:** Prioritizing local candidates (even if slightly lower ranked than remote ones) for the I/O pipeline does not significantly degrade the convergence of the search or final recall.
- **Evidence anchors:**
  - [section 4.4] "BatANN performs nearly the same number of distance comparisons and Disk I/O as on a single server."
  - [figure 5] Shows overlapping lines for Distance Computations and Disk I/O between $W=1$ and $W=8$.
  - [corpus] No direct corpus evidence for this specific heuristic; anchored in paper evaluation.
- **Break condition:** If strict global ordering of candidate exploration is required for a specific query, this heuristic might introduce a slight recall penalty, though the paper notes this is minimal at 0.95 recall.

## Foundational Learning

- **Concept: Beam Search**
  - **Why needed here:** The "state" passed between servers is the beam (the list of candidate vectors). You must understand that this list changes dynamically as the graph is traversed to understand what is being serialized and moved.
  - **Quick check question:** If a beam has size $L=10$, and the algorithm finds a new neighbor closer than the current 10th closest vector, what happens to the beam?

- **Concept: Vamana Graph / DiskANN**
  - **Why needed here:** BatANN builds upon DiskANN and the Vamana graph structure. Understanding that this is a disk-resident graph index where neighbor lists are stored on SSD is crucial for realizing why minimizing disk reads (I/O efficiency) is a primary optimization goal.
  - **Quick check question:** Why does the system keep a PQ (Product Quantization) representation in memory while storing full vectors on disk?

- **Concept: Product Quantization (PQ)**
  - **Why needed here:** PQ allows for fast approximate distance comparisons using the in-memory index. The system relies on these fast comparisons to decide where to route the query state without reading the full vectors from disk first.
  - **Quick check question:** How does using PQ distances differ from using exact distances during the candidate pruning phase?

## Architecture Onboarding

- **Component map:** Client Query -> Head Index Lookup -> Worker Thread (Local Search) -> (Conditional) Serialization -> Network Transfer -> Remote Receiver -> Remote Worker -> Result Return
- **Critical path:** Client Query -> Head Index Lookup -> Worker Thread (Local Search) -> (Conditional) Serialization -> Network Transfer -> Remote Receiver -> Remote Worker -> Result Return
- **Design tradeoffs:**
  - **Latency vs. Throughput:** The batching thread optimizes for network throughput but can introduce slight latency; `io_uring` is used to overlap I/O and compute to mitigate this.
  - **Message Size vs. Locality:** Sending the full result list with every state transfer simplifies final reranking but increases message size (up to 3.2 KB). The system caches query embeddings to reduce this overhead.
- **Failure signatures:**
  - **High Tail Latency:** Likely caused by "ping-ponging" where a query hits multiple remote partitions sequentially.
  - **Queue Buildup:** If the Receiver Queue fills up, it indicates workers are disk-bound or network-bound.
  - **Recall Drop:** Might occur if the I/O pipeline heuristic ($W$) is too aggressive in filtering remote candidates (though the paper suggests this is stable).
- **First 3 experiments:**
  1.  **Baseline Single-Node Validation:** Run the single-server baseline to ensure the underlying PipeANN/DiskANN implementation matches expected QPS/Recall curves before introducing distribution.
  2.  **Partitioning Efficiency Test:** Measure the ratio of "inter-partition hops" vs. "total hops" on a 3-server setup. If this ratio is >40%, the graph partitioning has failed or the dataset is unsuitable.
  3.  **Scalability Limit Check:** Run the full system on 5 vs. 10 servers. If throughput does not scale near-linearly, profile the "Batching Thread" and network serialization to determine if the state-passing overhead is dominating the computation time.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can offloading final result aggregation to the client reduce inter-server message sizes enough to significantly improve throughput in high-recall regimes?
- Basis in paper: [explicit] Section 8 (Discussion) proposes sending result data directly to the client for local reranking instead of including it in every state transfer.
- Why unresolved: Currently, the result set grows with every hop (up to 3.2 KB), adding overhead to the state-passing mechanism.
- What evidence would resolve it: A modified implementation where the client aggregates partial results, benchmarked for throughput at large beam sizes ($L \geq 200$).

### Open Question 2
- Question: How can the system efficiently handle dynamic updates (insertions and deletions) across the distributed global graph?
- Basis in paper: [explicit] Section 8 notes that supporting in-place updates presents "unique challenges," particularly regarding updating in-neighbors of a deleted point that may reside on different servers.
- Why unresolved: The current system is designed for static datasets; maintaining graph connectivity and routing accuracy during distributed modifications is unsolved.
- What evidence would resolve it: An extension of BatANN that supports real-time updates without requiring a full index rebuild or halting query service.

### Open Question 3
- Question: Would utilizing specialized hardware like RDMA provide tangible performance benefits over the current TCP-based implementation?
- Basis in paper: [explicit] Section 8 states it is "unclear that RDMA would really bring much benefit" because TCP performs well on the small state objects (4–8 KB) used in BatANN.
- Why unresolved: While RDMA is faster, it is also more expensive; the efficiency of the state-passing design on standard networks makes the trade-off uncertain.
- What evidence would resolve it: Performance benchmarks comparing the current TCP implementation against a version ported to an RDMA-capable framework like Derecho.

## Limitations
- Performance heavily depends on quality of graph partitioning; non-clustered datasets may experience degraded locality and performance
- 1% head index replication may become bottleneck at larger scales or with different query distributions
- Evaluation focuses only on single recall target (0.95), leaving performance characteristics at other recall points unclear

## Confidence
- **High Confidence:** The state-passing mechanism's basic operation and its advantage over request-reply patterns are well-demonstrated through both theoretical explanation and empirical evidence.
- **Medium Confidence:** The I/O pipeline heuristic's effectiveness is supported by overlapping performance curves, but the specific conditions under which this holds across diverse datasets remain untested.
- **Medium Confidence:** The scalability claims are well-supported for the tested dataset sizes and server counts, but the lack of testing beyond 10 servers leaves questions about near-linear scaling at larger scales.

## Next Checks
1. **Partition Quality Validation:** Measure inter-partition hop ratios on a 3-server setup with a synthetic uniform dataset to verify the partitioning algorithm's robustness against non-clustered data distributions.
2. **State Transfer Overhead Measurement:** Instrument the system to measure the actual serialization/deserialization time and network transfer time of query states across different beam sizes (L=10, L=100, L=400) to identify the break condition for message size.
3. **Recall vs. Throughput Trade-off Analysis:** Run the system at recall targets 0.90, 0.95, and 0.99 on the 100M dataset to characterize the throughput penalty at higher recall requirements and validate the stability of the I/O pipeline heuristic.