---
ver: rpa2
title: 'CharBench: Evaluating the Role of Tokenization in Character-Level Tasks'
arxiv_id: '2508.02591'
source_url: https://arxiv.org/abs/2508.02591
tags:
- tasks
- character
- wang
- performance
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating character-level
  reasoning in large language models, focusing on how tokenization impacts performance.
  The authors introduce CharBench, a comprehensive benchmark for character-level tasks
  that is two orders of magnitude larger than existing alternatives.
---

# CharBench: Evaluating the Role of Tokenization in Character-Level Tasks

## Quick Facts
- arXiv ID: 2508.02591
- Source URL: https://arxiv.org/abs/2508.02591
- Authors: Omri Uzan; Yuval Pinter
- Reference count: 7
- Primary result: Modern LLMs achieve only 50.3% average accuracy on character-level tasks, with particularly poor performance (32.3%) on positional understanding tasks

## Executive Summary
This paper introduces CharBench, a comprehensive benchmark for evaluating character-level reasoning in large language models. The authors find that modern LLMs struggle significantly with fundamental character tasks, particularly those requiring positional understanding within tokens. Through detailed analysis, they demonstrate that performance is strongly correlated with the actual number of character occurrences in counting tasks, while for positional tasks, accuracy declines as the length of the token containing the queried character increases. The benchmark and evaluation methodology provide a foundation for future work to improve model performance on these fundamental tasks.

## Method Summary
The authors constructed CharBench by selecting 2,310 unique words from the OSCAR corpus, filtering for English vocabulary, length diversity, and character diversity. They created five task types: Find First Character, Find Last Character, Count Characters, Spell Out, and Which Token. Each task was evaluated across 12 diverse models including both open-weight and proprietary systems. The evaluation framework systematically analyzed performance correlations with intrinsic tokenizer properties including target token length, compression ratio, and word length, while controlling for gold truth character counts.

## Key Results
- Modern LLMs achieve only 50.3% average accuracy across all character-level tasks
- Positional understanding tasks show particularly poor performance at 32.3% accuracy
- Performance on counting tasks is strongly negatively correlated with the actual number of character occurrences
- For positional tasks, accuracy declines sharply as target token length increases

## Why This Works (Mechanism)

### Mechanism 1: The Token Length Positional Obfuscation Effect
- **Claim:** The length of the token containing a target character is the primary bottleneck for positional reasoning tasks.
- **Mechanism:** In BPE-based models, a single embedding vector represents a sequence of characters (e.g., "strawberry" → "straw", "berry"). When a model is asked to find a character inside a long token, it lacks explicit input-level access to the character's internal index. The model must rely on "latent" character information encoded in the token embedding. As token length increases, the entropy of the character position within that embedding increases, degrading the model's ability to resolve the exact index.
- **Core assumption:** The model does not utilize an internal "character-by-character" scratchpad or attention subroutine by default; it attempts to resolve the query directly against the token embedding.
- **Evidence anchors:**
  - [abstract] "For tasks requiring intra-word positional understanding, performance is negatively correlated with the length of the token containing the queried character."
  - [Page 6] "Figure 3 presents the average performance... performance declines sharply as the target token becomes longer."
  - [corpus] Related work "Spelling-out is not Straightforward" confirms LLMs struggle to identify compositional subcomponents within tokens despite being able to spell them out.
- **Break condition:** If the model employs an explicit "token-to-character" expansion layer or is trained/finetuned with character-level auxiliary objectives (e.g., character-level token classification heads).

### Mechanism 2: Frequency-Driven Counting Heuristics
- **Claim:** For counting tasks, models rely on word-level and character-level frequency statistics rather than distinct enumeration of tokens.
- **Mechanism:** Models appear to estimate counts based on the correlation between character occurrences and word identity (e.g., recognizing 'e' is common in "cheese") rather than counting explicitly. The paper finds that tokenization structure (how many tokens a word splits into) is weakly correlated with counting success, while the *actual* count (Gold Truth) is strongly negatively correlated. This suggests models fail as the complexity of the count exceeds their "statistical expectation" for that word/character pair.
- **Core assumption:** The failure mode is not an inability to "see" the token, but a failure of the logical mechanism to enumerate instances > 1 or 2, reverting to statistical priors.
- **Evidence anchors:**
  - [Page 5, Table 3] "For counting tasks... tokenization properties are weakly correlated... while the length of the queried word and the actual character count play a more significant part."
  - [Page 6] "Performance on this task for GPT-4o... shows a notably stronger negative correlation with the Gold Truth (GT) metric."
  - [corpus] "The Strawberry Problem" frames this as a low mutual information issue between tokenization and character counts.
- **Break condition:** If the model uses Chain-of-Thought prompting to explicitly write out characters before counting, bypassing the statistical heuristic.

### Mechanism 3: The Compression-Reasoning Tradeoff
- **Claim:** Optimizing tokenizers for compression (fewer tokens per word) directly degrades character-level reasoning capabilities.
- **Mechanism:** Efficient tokenizers (like BPE) merge frequent character sequences into single units to minimize sequence length. This creates a "resolution trap": high compression reduces computational cost but obfuscates the underlying character lattice. The paper notes a tradeoff between the compression efficiency that makes LLMs fast and the "myopia" regarding character positions.
- **Core assumption:** The tokenizer is fixed and its vocabulary determines the "resolution" of the model's input window.
- **Evidence anchors:**
  - [Page 6] "[This] reveals a tradeoff between token-level compression and the model’s ability to reason about character-level structure."
  - [Page 5] "Compression Ratio (CR)... exhibits weaker correlations with model performance compared to [Target Token Length]."
  - [corpus] "StochasTok" suggests stochastically breaking this compression to improve subword understanding.
- **Break condition:** Using character-level or byte-level models where compression is minimal or non-existent (though these have their own context-length limitations).

## Foundational Learning

- **Concept:** **Target Token Length (TTL)**
  - **Why needed here:** This is the specific intrinsic metric identified in the paper as the strongest predictor of failure for positional tasks. It measures the character count of the single token containing the query character.
  - **Quick check question:** If the word "unbelievable" is tokenized as `[un, believe, able]` and we ask for the index of 'v', is TTL 3 (for 'bel') or 9 (for 'believe')?

- **Concept:** **BPE (Byte Pair Encoding)**
  - **Why needed here:** All evaluated models use BPE. Understanding that BPE merges frequent pairs iteratively explains *why* long, common subwords become opaque "black boxes" (tokens) to the model.
  - **Quick check question:** Why does BPE tend to obscure the middle characters of the word "strawberry" more than the characters in the phrase "a b c d"?

- **Concept:** **Intrinsic vs. Extrinsic Evaluation**
  - **Why needed here:** The paper evaluates tokenizers via *intrinsic* properties (entropy, length, compression) correlated with *downstream* (extrinsic) task performance. Distinguishing these helps in understanding that a "good" tokenizer for translation (compression) may be "bad" for CharBench (reasoning).
  - **Quick check question:** If a tokenizer achieves 100% compression (1 token per word), why does its intrinsic score for "efficiency" conflict with its extrinsic score on "character indexing"?

## Architecture Onboarding

- **Component map:**
  Tokenizer (BPE) -> Embedding Layer -> Transformer Layers -> LM Head

- **Critical path:**
  The analysis pipeline depends on joining three data streams:
  1. **Model Predictions:** (Input Word, Query Char → Predicted Index).
  2. **Tokenizer Artifacts:** (Input Word → Token List, mapping Token → Char Indices).
  3. **Gold Truth:** (Input Word, Query Char → Actual Index).
  The correlation analysis computes P(Correct | TTL, WL, GT).

- **Design tradeoffs:**
  - **Efficiency vs. Granularity:** Increasing vocabulary size (more compression) lowers inference cost but increases TTL, which the paper shows hurts positional accuracy.
  - **Tokenizer Access:** Proprietary models often hide tokenizer outputs. The paper excludes models where tokenization cannot be verified (e.g., "state-of-the-art proprietary models were excluded due to restrictions").

- **Failure signatures:**
  - **Positional Myopia:** High error rates when Target Token Length > 4, specifically for "Find First" or "Find Last" tasks.
  - **Undercounting:** Models systematically predict lower counts than ground truth (e.g., saying "2" instead of "3" for 'r' in 'strawberry').
  - **Over-indexing:** Models tend to guess higher indices for "Find Last" tasks (predicting characters are closer to the end than they are).

- **First 3 experiments:**
  1. **TTL Correlation Scan:** Run CharBench on a target model and plot accuracy vs. TTL. Confirm if the negative linear trend exists for your specific architecture.
  2. **Ablation by Token Force-Split:** Manually modify the tokenizer to force-split long tokens into smaller units (or characters) for specific test words. Measure if positional accuracy recovers to isolate the "embedding bottleneck" vs. "reasoning capacity."
  3. **Error Overlap Analysis:** Compare the Intersection-over-Union (IoU) of errors between a large model (e.g., Llama-70B) and a smaller efficient model (e.g., Mistral-7B) on "Find Last" vs. "Count." The paper suggests "Find Last" errors overlap significantly across models, while counting errors are more random.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed effects of tokenization on character-level reasoning generalize to languages with diverse typologies and non-Latin scripts?
- Basis in paper: [explicit] The authors state, "Our evaluation is limited to character-level tasks in English... We encourage future work to replicate these experiments in additional languages, prioritizing diversity in typology and script."
- Why unresolved: The current study isolated English to minimize training data frequency confounders, leaving the impact of different tokenization strategies (e.g., on agglutinative or logographic languages) unknown.
- What evidence would resolve it: Replicating the CharBench evaluation protocol on multilingual models across varied languages to observe if the correlation between target token length and positional accuracy persists.

### Open Question 2
- Question: Can tokenizer training objectives be modified to preserve intra-word positional information without sacrificing subword compression efficiency?
- Basis in paper: [inferred] The analysis identifies a "tradeoff between token-level compression and the model’s ability to reason about character-level positions," suggesting current compression algorithms may obscure necessary character details.
- Why unresolved: The paper establishes the correlation but does not propose or test alternative tokenization algorithms that might mitigate the accuracy decline observed with longer tokens.
- What evidence would resolve it: Developing and testing new tokenizers that penalize the loss of character-level granularity and evaluating them on CharBench to see if the negative correlation with target token length is reduced.

### Open Question 3
- Question: What specific representational mechanisms fail during positional indexing tasks despite models ostensibly retaining character information in their embeddings?
- Basis in paper: [inferred] The authors note that while prior work shows embeddings can recover spelling (character occurrence), models fail significantly at absolute positional tasks (32.3% accuracy), suggesting a disconnect between storage and retrieval.
- Why unresolved: The paper identifies the performance gap and correlation with token length but does not investigate the internal mechanism preventing the model from locating characters within the identified token.
- What evidence would resolve it: Probing studies that visualize or measure how positional data degrades within the hidden states of long tokens versus short tokens during inference.

## Limitations

- Limited model coverage and generalizability: The evaluation includes 12 models but excludes several state-of-the-art proprietary models due to tokenization access restrictions.
- Correlation vs. causation ambiguity: While strong correlations are demonstrated, the causal mechanisms remain inferential rather than experimentally validated.
- Benchmark construction biases: CharBench was built using heuristic filtering of an English corpus that may favor certain morphological patterns.

## Confidence

**High Confidence (9/10):** The core finding that modern LLMs struggle with character-level tasks, achieving only 50.3% average accuracy, is robustly demonstrated across multiple model families and task types.

**Medium Confidence (7/10):** The identification of TTL as the primary bottleneck for positional tasks and actual character count for counting tasks is well-supported by correlation analysis, though the mechanistic explanation is inferred.

**Low Confidence (4/10):** The broader claim about a fundamental "compression-reasoning tradeoff" in tokenization design extends beyond what the current empirical results directly demonstrate.

## Next Checks

1. **TTL manipulation experiment:** Create a controlled experiment where the same words are tested with modified tokenizers that force-split long tokens into smaller units. Measure whether positional accuracy improves proportionally to TTL reduction, providing direct causal evidence for the TTL bottleneck hypothesis.

2. **Cross-linguistic generalizability test:** Evaluate CharBench-equivalent tasks on morphologically rich languages (e.g., Finnish, Turkish) where tokenization complexity differs substantially from English. This would test whether the observed patterns hold across different linguistic typologies or are English-specific artifacts.

3. **Tokenization access audit:** Systematically test a broader range of proprietary models using black-box prompting techniques to infer tokenization patterns (e.g., by asking models to spell out words or identify token boundaries). This would address the selection bias introduced by excluding models without accessible tokenizers and provide more complete coverage of the current LLM landscape.