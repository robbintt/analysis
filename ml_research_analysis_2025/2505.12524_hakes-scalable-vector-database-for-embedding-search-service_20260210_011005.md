---
ver: rpa2
title: 'HAKES: Scalable Vector Database for Embedding Search Service'
arxiv_id: '2505.12524'
source_url: https://arxiv.org/abs/2505.12524
tags:
- vectors
- search
- index
- vector
- recall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'HAKES addresses high-recall vector search in high-dimensional
  embeddings under concurrent read-write workloads. It uses a two-stage filter-refine
  design: aggressive compression (dimensionality reduction + 4-bit quantization) in
  the filter stage, followed by exact similarity computation in the refine stage.'
---

# HAKES: Scalable Vector Database for Embedding Search Service

## Quick Facts
- arXiv ID: 2505.12524
- Source URL: https://arxiv.org/abs/2505.12524
- Reference count: 40
- Primary result: HAKES achieves up to 16× higher throughput than three distributed vector database baselines under concurrent read-write workloads while maintaining high recall.

## Executive Summary
HAKES addresses the challenge of high-recall vector search in high-dimensional embeddings under concurrent read-write workloads. It introduces a two-stage filter-refine design that uses aggressive compression in the filter stage, followed by exact similarity computation in the refine stage. The system learns index parameters end-to-end to preserve local similarity distributions and decouples insert/search parameters to avoid recall degradation during updates. A distributed disaggregated architecture separates filter and refine stages to scale compute and memory independently. Experiments show HAKES-Index outperforms 12 state-of-the-art indexes in the high-recall region, and HAKES achieves up to 16× higher throughput than three distributed vector database baselines under concurrent workloads.

## Method Summary
HAKES uses a two-stage ANN index where the filter stage applies learned aggressive compression (dimensionality reduction + 4-bit quantization) to quickly identify candidate vectors, followed by a refine stage that computes exact similarities on full-precision vectors. The compression parameters are learned through self-supervised training to minimize KL divergence between similarity score distributions in original and compressed spaces for local neighborhoods. The system decouples parameters for inserting new vectors from those used for searching, enabling concurrent read-write workloads without contention. A distributed disaggregated architecture separates stateless IndexWorkers (filter stage) from stateful RefineWorkers (refine stage), allowing independent scaling of compute and memory resources.

## Key Results
- HAKES-Index outperforms 12 state-of-the-art indexes in the high-recall region on six deep embedding datasets
- HAKES achieves up to 16× higher throughput than three distributed vector database baselines under concurrent workloads
- The system demonstrates linear scalability, achieving 32× higher throughput than a single-node system with 32 IndexWorkers

## Why This Works (Mechanism)

### Mechanism 1: Learned Aggressive Compression for Filter Stage
HAKES-Index uses a two-stage design with highly compressed vectors in the filter stage (dimensionality reduction + 4-bit Product Quantization). The compression parameters are learned through a lightweight self-supervised process that minimizes KL divergence between similarity score distributions in original and compressed spaces for local neighborhoods. This allows the filter stage to quickly identify high-quality candidate sets from many vectors, with the refine stage correcting any remaining errors for the candidate set. The core assumption is that preserving relative ranking of similarity scores for local neighborhoods is sufficient for the filter stage to retrieve true nearest neighbors.

### Mechanism 2: Decoupled Index Parameters for Concurrent Operations
The system maintains two sets of compression parameters: "insert index parameters" (stable, initialized via OPQ and k-means) for compressing and assigning new vectors to IVF partitions, and "search index parameters" (learned, optimized) for searching all vectors. New vectors are indexed using stable insert parameters, while all searches use optimized search parameters. This avoids locking or updating the global search index structure for every insert, enabling high-recall search while supporting concurrent read-write workloads. The core assumption is that insert parameters are sufficiently good for partition assignment and search parameters will effectively rank new vectors alongside existing ones.

### Mechanism 3: Disaggregated Architecture for Scalability
HAKES separates query processing into two independent services: IndexWorkers (stateless, manage compressed filter indexes) and RefineWorkers (stateful, manage shards of full-precision vectors). A client coordinates queries, sending them to IndexWorkers to get candidate IDs, then dispatching those to relevant RefineWorkers for final reranking. This decouples memory and compute requirements, enabling independent horizontal scaling. The core assumption is that network overhead of sending candidate IDs is lower than computation and memory access overheads saved, and that the filter index is small enough to be replicated on IndexWorkers.

## Foundational Learning

- **Concept: Approximate Nearest Neighbor (ANN) Search & Recall**
  - Why needed: HAKES is fundamentally an ANN system; understanding the recall vs. latency/throughput trade-off is critical to its claims
  - Quick check: If an ANN search for top 10 nearest neighbors returns 8 true neighbors and 2 non-neighbors, what is the recall@10? (Answer: 0.8 or 80%)

- **Concept: Product Quantization (PQ)**
  - Why needed: The filter stage uses PQ as core compression technique; understanding how PQ compresses vectors into compact codes and enables fast similarity approximation is essential
  - Quick check: How does 4-bit Product Quantization reduce memory footprint and speed up distance calculations compared to 32-bit float vectors? (Answer: It compresses vectors into 4-bit codes and uses pre-computed lookup tables for distance approximation, avoiding expensive direct vector operations)

- **Concept: Inverted File Index (IVF)**
  - Why needed: HAKES-Index uses IVF for coarse-grained partitioning; understanding how IVF divides vector space into cells and how searches probe subsets of these cells is key
  - Quick check: In an IVF index with 4096 partitions and n_probe of 64, how many partitions are scanned during a search query? (Answer: 64)

## Architecture Onboarding

- **Component map:**
  - Client -> IndexWorker (filter stage) -> RefineWorker (refine stage) -> Client

- **Critical path:**
  1. Query Ingestion: Client receives query vector
  2. Filter Stage: Client sends query vector to IndexWorker, which applies dimensionality reduction, computes LUT, scans n_probe IVF partitions, returns top k' candidate IDs
  3. Refine Stage: Client sends candidate IDs to relevant RefineWorkers, which fetch full vectors, compute exact similarities, return top k results
  4. Result Aggregation: Client merges results from RefineWorkers and returns to user

- **Design tradeoffs:**
  - Throughput vs. Latency: Two network hops may increase tail latency but increase throughput by enabling independent scaling of filter (compute) and refine (memory) stages
  - Consistency vs. Performance: Decoupled parameters and asynchronous updates mean no strong consistency between writes and reads, favoring performance
  - Recall vs. Throughput: Controlled by n_probe, k', and k parameters; higher values increase recall but reduce throughput

- **Failure signatures:**
  - Sudden Recall Drop: Could indicate significant data drift making learned parameters obsolete or insert parameters mis-assigning new data
  - RefineWorker Hotspotting: Skewed data distribution may cause some RefineWorkers to become bottlenecks
  - Throughput Saturation with High Latency: Client coordination bottleneck preventing worker scaling benefits
  - IndexWorker Divergence: Inconsistent candidate lists from non-atomic parameter propagation to replicas

- **First 3 experiments:**
  1. Establish End-to-End Baseline: Run throughput vs. recall benchmark on single-node HAKES against HNSW baseline to confirm performance advantage
  2. Profile a Single Query: Trace single query through system, measuring time in Client, IndexWorker (filter), and RefineWorker (refine) to identify primary bottleneck
  3. Test Scalability: Deploy HAKES with 2, 4, and 8 nodes, run fixed read-only workload at target recall (95%), plot throughput against nodes to verify linear scalability

## Open Questions the Paper Calls Out

- **Open Question 1:** Can a new space partitioning scheme be devised to handle Out-of-Distribution (OOD) queries in cross-modal datasets efficiently? The paper states improving HAKES-Index for cross-modal settings requires a new space partitioning scheme, which presents a research challenge for future work.

- **Open Question 2:** How can the index design be adapted to maintain superiority over graph-based indexes on low-dimensional datasets? The paper notes HAKES-Index falls behind some baselines on SIFT-128 dataset because learning technique provides little improvement for low-dimensional vectors.

- **Open Question 3:** Is it possible to adapt to significant distribution shifts or embedding model updates without triggering a full index rebuild? The paper states rebuilding the index is necessary when embedding models are retrained or dataset distributions change significantly.

## Limitations

- Scalability in practice may degrade under real-world skewed data access patterns, with some RefineWorkers becoming hotspots
- The system lacks mechanisms for online adaptation to rapid distribution changes, potentially degrading recall over time
- Learned compression parameters may not generalize well to queries outside the training distribution

## Confidence

- **High confidence:** The two-stage filter-refine architecture with learned compression parameters is technically sound with supporting equations and ablation studies
- **Medium confidence:** Disaggregated architecture scalability claims are supported by controlled experiments but may not hold under real-world data skew or network contention
- **Low confidence:** System behavior under concurrent high-rate inserts combined with sustained query load is not thoroughly characterized

## Next Checks

1. **Skew resilience test:** Deploy HAKES with skewed query distribution (Zipfian) where some vectors are queried 100× more than others; measure throughput and latency variance across RefineWorkers to identify hotspotting

2. **Drift impact assessment:** Simulate distribution shift by inserting vectors from different distribution; measure recall degradation over time without rebuild to quantify decoupling's limits

3. **Concurrent workload stress test:** Run benchmark with simultaneous high-rate inserts and sustained queries at target recall; measure throughput, recall, and tail latency to assess real-world performance under contention