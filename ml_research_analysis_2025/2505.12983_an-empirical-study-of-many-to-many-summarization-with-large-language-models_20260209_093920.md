---
ver: rpa2
title: An Empirical Study of Many-to-Many Summarization with Large Language Models
arxiv_id: '2505.12983'
source_url: https://arxiv.org/abs/2505.12983
tags:
- llms
- language
- m2ms
- association
- summarization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the ability of large language models (LLMs)
  to perform many-to-many summarization (M2MS), which involves summarizing documents
  in any language into summaries in any target language. The authors reorganize eight
  existing multilingual summarization datasets spanning five domains and six languages
  to create a multi-domain M2MS benchmark with 47.8K samples.
---

# An Empirical Study of Many-to-Many Summarization with Large Language Models

## Quick Facts
- **arXiv ID:** 2505.12983
- **Source URL:** https://arxiv.org/abs/2505.12983
- **Reference count:** 40
- **One-line primary result:** Instruction-tuned LLMs significantly outperform both fine-tuned traditional models and zero-shot LLMs on M2MS tasks but face factuality challenges.

## Executive Summary
This paper investigates large language models' (LLMs) ability to perform many-to-many summarization (M2MS), where documents in any language can be summarized into any target language. The authors create a multi-domain M2MS benchmark by reorganizing eight existing multilingual summarization datasets spanning five domains and six languages, totaling 47.8K samples. They evaluate 18 LLMs in zero-shot and instruction-tuning settings against fine-tuned traditional models like mBART and PISCES. Results show that while zero-shot LLMs achieve competitive performance with fine-tuned traditional models, instruction-tuned open-source LLMs significantly outperform both baselines, though at the cost of increased factual errors. The study demonstrates LLMs' strong potential for M2MS applications while highlighting factuality control as a critical challenge.

## Method Summary
The study reorganizes eight existing multilingual summarization datasets (CrossSum, XWikis, XSAMSum, XMediaSum, DialogSumX, WikiLingua, Perseus, and Spektrum) into a unified M2MS benchmark with 47.8K samples across 5 domains and 6 languages. The authors evaluate 18 LLMs (both closed-source and open-source) in zero-shot settings with in-context examples and instruction-tuning settings with 19.5K training samples. Instruction tuning uses a learning rate of 1e-5, batch size 32, and 2 epochs on 8× NVIDIA A800 GPUs. Performance is measured using ROUGE-1/2/L, BERTScore, and human evaluation for factuality. The evaluation includes automatic metrics, LLM-as-judge for conciseness/coherence/relevance, and human factuality assessment.

## Key Results
- Zero-shot LLMs achieve competitive R1 scores (26.0) with fine-tuned traditional models (30.8) despite no task-specific training
- Instruction-tuned LLMs significantly outperform both baselines, with Vicuna-13B-16k achieving 38.0 R1 score (+7.2 over PISCES)
- Instruction tuning improves automatic metrics but intensifies factual errors, with hallucination rates increasing from 8-12% to 17-23%
- LLMs preserve general task-solving abilities on MMLU after instruction tuning, with scores remaining stable at ~54-55

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Zero-shot LLMs can perform M2MS competitively with fine-tuned traditional models without parameter updates.
- **Mechanism:** Pre-trained multilingual representations enable instruction-following via in-context learning. The model draws on language modeling across multiple languages from pre-training, and the prompt with task-specific instructions + few examples activates these capabilities without gradient updates.
- **Core assumption:** The LLM's pre-training distribution includes sufficient multilingual summarization-adjacent patterns that can be recombined via prompting.
- **Evidence anchors:**
  - [abstract]: "zero-shot LLMs achieve competitive results with fine-tuned traditional models"
  - [Table 3]: GPT-4o achieves 26.0 R1 vs. mBART-50's 27.4 and PISCES's 30.8—competitive despite no task-specific training
  - [corpus]: Limited direct corpus support for cross-lingual summarization mechanisms; related work focuses on less-resourced languages and video summarization, not M2MS specifically
- **Break condition:** If source-target language pair lacks representation in pre-training (e.g., Cs↔Uk pairs absent in data), zero-shot performance degrades. Also breaks when documents exceed context window truncation (3600 tokens here).

### Mechanism 2
- **Claim:** Instruction-tuned LLMs outperform both traditional models and zero-shot LLMs by large margins on automatic metrics.
- **Mechanism:** Task-specific instruction tuning on 19.5K samples enables: (1) parameter adaptation to the document→summary mapping across languages, (2) better fit than traditional models that need >100K samples for similar tasks, (3) longer context handling via RoPE positional encoding vs. mBART's 1K limit.
- **Core assumption:** Instruction tuning doesn't catastrophically interfere with multilingual representations learned during pre-training.
- **Evidence anchors:**
  - [abstract]: "open-source LLMs can significantly improve their M2MS ability, and outperform zero-shot LLMs (including GPT-4) in terms of automatic evaluations"
  - [Table 3]: Vicuna-13B-16k instruction-tuned achieves 38.0 R1 vs. PISCES 30.8 (+7.2) and zero-shot GPT-4o 26.0 (+12.0)
  - [Section 5]: "LLMs involve more parameters than traditional models, thus having a more powerful ability to fit the tasks-specific data with small-scale instruction samples"
  - [corpus]: Corpus lacks direct evidence; related papers focus on summarization approaches but not M2MS instruction tuning specifically
- **Break condition:** Training data quality issues propagate—human-written summaries with hallucinations may amplify factual errors (see Mechanism 3). Also breaks if training scale is too small; Appendix G shows traditional models degrade 21-23 R1 points when reducing from 19.5K to 2K samples, while LLMs degrade less (12.4 points).

### Mechanism 3
- **Claim:** Instruction tuning intensifies factual errors, particularly hallucinations, despite improving automatic metrics.
- **Mechanism:** Human-written reference summaries in training data contain information beyond source documents (background knowledge, implicit facts). The model learns to add such information during generation, producing hallucinations not grounded in input. This creates a metrics-factuality trade-off.
- **Core assumption:** Hallucinations in generated summaries stem from training signal rather than model architecture or decoding strategy.
- **Evidence anchors:**
  - [abstract]: "instruction tuning might intensify the [factuality] issue"
  - [Table 7 Human Evaluation]: GPT-4 has 8 hallucination errors; zero-shot LLaMa-2-13B has 17; instruction-tuned LLaMa-2-13B has 23. Tuned Vicuna-13B-16k: 17 vs. zero-shot 12.
  - [Section 6]: "summaries in training samples are written by humans, thus they might involve more information (like background information) beyond the given documents"
  - [corpus]: Weak corpus support; related work (REFLEX, log summarization) discusses reference-free evaluation challenges but not M2MS factuality specifically
- **Break condition:** If training references are factually consistent with source documents, this mechanism weakens. Breaks when evaluation uses only automatic metrics (ROUGE/BERTScore) that don't penalize hallucinations—human evaluation reveals the gap.

## Foundational Learning

- **Concept: Many-to-Many Summarization (M2MS) Task Definition**
  - **Why needed here:** M2MS generalizes cross-lingual summarization (different source→target language) and multilingual summarization (same language in/out) into a unified any→any paradigm. Understanding this helps interpret why the task requires both summarization and translation capabilities simultaneously.
  - **Quick check question:** Given a Chinese news article, what are all valid M2MS outputs? (Answer: Summaries in En, Cs, De, Fr, Zh, or Uk—any supported target language, not just Chinese or English)

- **Concept: Instruction Tuning vs. Fine-Tuning**
  - **Why needed here:** The paper uses instruction tuning (formatted prompt-response pairs) for LLMs but standard fine-tuning for mBART/PISCES. The difference explains why LLMs improve with 19.5K samples while traditional models would need >100K.
  - **Quick check question:** What formatting difference exists between mBART training and LLM instruction tuning? (Answer: mBART receives raw document+language tag; LLMs receive full instruction prompt with task description, domain info, and examples)

- **Concept: ROUGE/BERTScore Limitations for Factuality**
  - **Why needed here:** Automatic metrics improve with instruction tuning, but human evaluation reveals factuality degradation. Understanding this disconnect is critical for interpreting results and deployment decisions.
  - **Quick check question:** Why might a summary with higher ROUGE scores contain more hallucinations? (Answer: ROUGE measures lexical/semantic overlap with reference, not factual consistency with source; if reference itself contains hallucinations, matching it increases ROUGE but not factuality)

## Architecture Onboarding

- **Component map:**
  Data Layer: 8 source datasets → Quality filtering (coverage/redundancy/coherence) → 47.8K samples → Train/Val/Test split with contamination control
  ↓
  Prompt Layer: System prompt (persona, task description) + User prompt (document, target language request) + In-context examples
  ↓
  Model Layer: Closed-source (GPT-4o/GPT-4/GPT-3.5) OR Open-source (LLaMa/Vicuna/Baichuan/Qwen/InternLM) → Zero-shot OR Instruction-tuned
  ↓
  Evaluation Layer: Automatic (ROUGE-1/2/L, BERTScore) + LLM-as-judge (GPT-4o conciseness/coherence/relevance) + Human (factuality: hallucination/particulars/predicate/entity errors)

- **Critical path:** Data preparation with contamination filtering (test set <1% contaminated) → Zero-shot baseline evaluation across all 18 LLMs → Instruction tuning on 19.5K samples (2 epochs, lr=1e-5, batch=32) → MMLU general capability preservation check → Human factuality evaluation on 100-sample subset

- **Design tradeoffs:**
  - **Long context vs. model capacity:** Vicuna-16k variants outperform 4K variants (22.9 vs 22.4 R1 zero-shot) but require more memory; traditional mBART capped at 1K tokens
  - **Automatic metrics vs. factuality:** Instruction tuning improves R1 by ~15 points but increases hallucination errors by 6-7 percentage points
  - **Training scale sensitivity:** Traditional models lose 21-23 R1 when reducing 19.5K→2K; LLMs lose only 12.4—LLMs more sample-efficient but still benefit from scale

- **Failure signatures:**
  - **Wrong language generation:** Table 5 shows 2-20% of outputs in wrong language for zero-shot; instruction tuning reduces to <2%. Detect via `fastlangid`.
  - **Hallucination cascade:** Human evaluation shows 8-23% hallucination rate; particulars errors (12-18%) also common. Signal: summary contains named entities or events not in source.
  - **Length mismatch:** Zero-shot LLMs generate verbose summaries (low conciseness ~3.0/5); instruction tuning improves to ~4.6/5.
  - **Domain mismatch:** Technology domain shows highest R1 (30-39) while dialogue shows lowest for some models (Internlm2: 11.6 R1 zero-shot)—per-domain evaluation critical.

- **First 3 experiments:**
  1. **Zero-shot baseline with contamination check:** Run GPT-4o and Vicuna-13B-16k on full test set (14.15K samples) with the Figure 4 prompt. Calculate per-domain and per-language-pair R1/RL/BS. Use `fastlangid` to verify correct language rate. Expected: GPT-4o ~26 R1 overall; Vicuna-16k ~23 R1.
  2. **Instruction tuning with factuality tracking:** Fine-tune Vicuna-13B-16k on 19.5K training samples (8×A800, 2 epochs, lr=1e-5). Evaluate on test set AND run human factuality evaluation on 100 random English→Chinese samples. Expected: R1 improves to ~38, but hallucination rate increases from 12% to 17%.
  3. **General capability preservation (MMLU):** Run 5-shot MMLU on Vicuna-13B-16k before and after instruction tuning. Expected: MMLU score stable (~54-55) or slight improvement, confirming task-specific tuning doesn't sacrifice general abilities. If MMLU drops >2 points, reduce training epochs or learning rate.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can factual errors and hallucinations be effectively mitigated in Many-to-Many Summarization (M2MS) when instruction tuning appears to intensify these issues due to information gaps in human-written references?
- **Basis in paper:** [explicit] The abstract concludes that "how to control factual errors becomes the key," and Section 6 states that instruction tuning "might intensify factual errors" because "ground truth references... might involve more information... beyond the given documents."
- **Why unresolved:** The study identifies a correlation between instruction tuning and increased hallucination but does not propose or validate specific methods to filter training data or constrain generation to ensure factuality.
- **What evidence would resolve it:** Experiments comparing standard instruction tuning against methods utilizing source-aligned or synthetic references, showing a reduction in hallucination rates without compromising ROUGE scores.

### Open Question 2
- **Question:** Can advanced prompt engineering strategies improve the zero-shot performance of LLMs on M2MS tasks beyond the "lower threshold" baseline established in this study?
- **Basis in paper:** [explicit] The Limitations section states the authors "only evaluate the lower threshold of these models’ M2MS performance" and suggests "future work could explore better prompts to obtain better results."
- **Why unresolved:** The experiments utilized a fixed, standardized prompt to ensure fair comparison across models, leaving the potential of optimized prompting techniques unexplored.
- **What evidence would resolve it:** A study applying complex prompting techniques (e.g., Chain-of-Thought or Self-Consistency) to the same benchmark, demonstrating statistically significant gains over the reported zero-shot baselines.

### Open Question 3
- **Question:** How does the performance of LLMs on M2MS tasks generalize to real-world domains not covered by the reorganized academic datasets?
- **Basis in paper:** [explicit] The Limitations section notes the used data "only involves data from existing multi-lingual summarization datasets" and suggests "future work could extend it with more domains in the real scenes."
- **Why unresolved:** The current benchmark relies on specific domains (news, dialogue, etc.) that may not represent the noise, format, or complexity of industrial applications.
- **What evidence would resolve it:** Evaluations on a newly curated dataset from diverse industrial sources (e.g., legal or financial), analyzing performance degradation or domain shift relative to the current benchmark.

## Limitations
- **Data coverage limitations:** The reorganized dataset spans 5 domains and 6 languages but lacks coverage for certain language pairs (e.g., Cs↔Uk), potentially limiting generalization to truly zero-resource pairs.
- **Factuality trade-off:** Instruction tuning significantly improves automatic metrics but intensifies factual errors, with hallucination rates increasing by 6-7 percentage points, revealing a metrics-factuality trade-off.
- **Human evaluation sample size:** Factuality assessment conducted on only 100 samples, limiting statistical power for comparing hallucination rates across models and domains.

## Confidence

- **High confidence:** Zero-shot LLMs achieving competitive results with fine-tuned traditional models (supported by multiple R1 comparisons in Table 3); instruction tuning significantly improving LLM performance while preserving MMLU capabilities (demonstrated through before/after evaluation).
- **Medium confidence:** Mechanism linking human-written training references to increased hallucinations (plausible but requires deeper analysis of reference-source alignment); sample efficiency advantage of LLMs over traditional models (robust across Appendix G experiments but dependent on specific training setup).
- **Low confidence:** Claims about the mechanism behind factuality degradation—while the paper attributes it to reference content, the analysis lacks systematic investigation of whether it's driven by training signal, decoding strategy, or model architecture.

## Next Checks

1. **Factuality source attribution:** Analyze a stratified sample of hallucination instances to determine whether they originate from training references, model architecture, or decoding strategy. Compare outputs from models trained on factuality-filtered references versus original references.

2. **Zero-resource pair evaluation:** Test model performance on language pairs absent from the training corpus (e.g., Cs→Uk) to validate whether the "any-to-any" capability truly generalizes or depends on pre-training language overlap.

3. **Human reference analysis:** Conduct a systematic comparison between reference summaries and source documents to quantify the proportion of additional information (background knowledge, implicit facts) that may drive hallucination learning during instruction tuning.