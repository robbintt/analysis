---
ver: rpa2
title: 'Common Corpus: The Largest Collection of Ethical Data for LLM Pre-Training'
arxiv_id: '2506.01732'
source_url: https://arxiv.org/abs/2506.01732
tags:
- data
- open
- corpus
- language
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Common Corpus, the largest open dataset for
  pre-training large language models, addressing legal and ethical challenges posed
  by proprietary data. Comprising two trillion tokens, it includes uncopyrighted or
  permissively licensed data across multiple languages and domains, such as Open Government,
  Open Culture, Open Science, and Open Code.
---

# Common Corpus: The Largest Collection of Ethical Data for LLM Pre-Training

## Quick Facts
- arXiv ID: 2506.01732
- Source URL: https://arxiv.org/abs/2506.01732
- Reference count: 40
- The largest open dataset for LLM pre-training, addressing legal and ethical challenges with 2 trillion permissively licensed tokens.

## Executive Summary
Common Corpus addresses the legal and ethical challenges of LLM pre-training by assembling two trillion tokens from exclusively permissively licensed or public domain sources. The dataset spans government documents, cultural heritage, scientific literature, code, and web content across 10+ languages. Custom tools for OCR correction, text segmentation, and PII removal ensure data quality while maintaining compliance with regulations. Adopted by major AI projects including HPLT 3.0 and EMMA-500, Common Corpus represents critical infrastructure for open science in LLM development, though coverage remains limited for low-resource languages.

## Method Summary
The corpus aggregates data from six domains: Open Government (SEC, WTO, legal documents), Open Culture (historical books/newspapers), Open Science (OpenAlex, arXiv), Open Code (The Stack), Open Web (Wikipedia, YouTube Commons), and Open Semantic (Wikidata). Each source undergoes preprocessing including OCR error detection/correction (OCRerrcr/OCRonos models), text segmentation (Segmentext), PII removal (Microsoft Presidio + custom regex), and toxicity filtering (Celadon classifier). The pipeline transforms raw, often low-quality historical documents into training-suitable tokens while ensuring license compliance and removing harmful content.

## Key Results
- 2 trillion tokens from exclusively permissively licensed or public domain sources
- 10+ languages with ≥10B tokens each, supporting multilingual model development
- Custom preprocessing tools (OCR correction, toxicity filtering, PII removal) enable use of historical documents
- Adopted by major AI projects (HPLT 3.0, EMMA-500) as foundational training data

## Why This Works (Mechanism)

### Mechanism 1
Aggregating exclusively license-clean data enables legally compliant LLM pre-training at scale. By curating only public domain, CC-By, MIT, Apache-2.0, and other permissively licensed content, the corpus removes copyright barriers that have caused takedowns of other datasets (Books3, LAION, MATH benchmark). This allows model developers to release training artifacts without relying on fair use defenses. Legal risk scales with license ambiguity; clear provenance reduces downstream liability.

### Mechanism 2
Domain-specific curation tools transform low-quality historical and scanned documents into usable pre-training tokens. Three custom models address distinct quality issues: OCRerrcr (error detection), OCRonos (correction/rewriting), and Segmentext (text segmentation). For toxicity, Celadon filters harmful content across five bias dimensions. This preprocessing pipeline converts archival material—often with 41% non-recognized 7-grams—into training-suitable text. LLM-based correction preserves semantic content while fixing surface errors; synthetic rewriting does not introduce harmful hallucinations at scale.

### Mechanism 3
Multilingual, multi-domain diversity improves downstream task performance, particularly for European languages and specialized applications. The corpus spans government documents, cultural heritage, scientific papers, code, and structured semantic data across 10+ languages with ≥10B tokens each. This diversity supports both general-purpose models and domain-specific applications (RAG, legal, document processing). Domain and linguistic breadth in pre-training correlates with improved generalization; evaluation benchmarks capture this.

## Foundational Learning

- **Concept: Copyright licensing for ML training data**
  - Why needed here: The entire paper's value proposition rests on license compliance; understanding public domain, CC-By, MIT, and fair use is essential.
  - Quick check question: Why might a dataset be "publicly available" but not "permissively licensed"?

- **Concept: OCR error modes in historical documents**
  - Why needed here: ~886B tokens come from Open Culture (digitized books/newspapers), where OCR quality is a primary bottleneck.
  - Quick check question: What are three common OCR error types in 18th–19th century printed texts?

- **Concept: PII removal and GDPR compliance**
  - Why needed here: The corpus includes government and legal documents with potential personal data; GDPR imposes strict requirements.
  - Quick check question: Why does the paper replace PII with "fictitious but realistic values" rather than [REDACTED] tags?

## Architecture Onboarding

- **Component map:**
Common Corpus (~2T tokens, 10K parquet files on HuggingFace)
├── Open Government (407B tokens): Finance Commons, Legal Commons
├── Open Culture (886B tokens): historical books/newspapers, 13+ languages
├── Open Science (281B tokens): OpenAlex, arXiv, CC-By papers
├── Open Code (283B tokens): The Stack v1/v2, permissively licensed
├── Open Web (73B tokens): Wikipedia, YouTube Commons, StackExchange
└── Open Semantic (68B tokens): Wikidata in natural language format

- **Critical path:**
  1. **Filter by use case:** Select collections matching your domain (e.g., Open Code for coding models, Open Culture for creative writing).
  2. **Apply license sub-filtering:** Use metadata to exclude specific licenses if your application requires it (e.g., CC-By-SA may trigger copyleft obligations).
  3. **Run language filtering:** Use provided fastText language tags to isolate target languages.
  4. **Optional re-processing:** Apply OCRonos/Celadon if using older Open Culture subsets.

- **Design tradeoffs:**
  - **Size vs. licensing certainty:** 2T tokens is smaller than proprietary corpora (14–36T for DeepSeek/Llama 4) but has clear provenance.
  - **Historical depth vs. modern language:** Open Culture includes 18th–19th century texts; language patterns may not match modern usage.
  - **Multilingual breadth vs. per-language depth:** English dominates (~969B tokens); low-resource languages have shallow coverage.

- **Failure signatures:**
  - **OCR artifacts in historical texts:** Look for character-level noise (e.g., "Û", broken words) in Open Culture outputs.
  - **Language switching in OCR correction:** OCRonos may transcribe deteriorated text into wrong languages (mitigated but not eliminated).
  - **Toxicity false positives:** Celadon may flag period-appropriate historical language as harmful.

- **First 3 experiments:**
  1. **Subset benchmark:** Sample 10B tokens each from Open Culture, Open Science, and Open Code; train a 350M parameter model and compare downstream task performance.
  2. **OCR quality audit:** Run OCRerrcr on 1K documents from French PD Newspapers; measure error rate vs. OCRoscope baseline.
  3. **License compliance check:** Randomly sample 500 documents; manually verify license metadata matches source repository status.

## Open Questions the Paper Calls Out

### Open Question 1
Can a combination of web domain curation and language model-based annotation reliably identify permissive licenses in web archives at scale? Existing web archives lack normalized license metadata; pages may contain mixed or ambiguous licensing for different content elements. Development and benchmarking of a pipeline that correctly attributes licenses to >90% of web documents, validated against human annotation, would resolve this.

### Open Question 2
How can the open data paradox—the underrepresentation of permissively licensed content in standard pre-training pipelines—be systematically addressed? Web archiving infrastructure prioritizes general crawlability, not license clarity; open content sources often lack visibility in these pipelines. Documented expansion of open corpora through novel discovery methods that increase permissively-licensed token counts by measurable margins would resolve this.

### Open Question 3
What strategies can effectively expand ethically-sourced, license-compliant pre-training data for low-resource languages? Limited digital archives, poor OCR quality, and fewer cultural heritage institutions in relevant regions constrain available sources. Release of license-compliant datasets achieving ≥10B tokens for multiple African or other low-resource languages would resolve this.

## Limitations
- 2T tokens is smaller than proprietary corpora (14–36T for DeepSeek/Llama 4), limiting potential model scale
- Limited coverage for low-resource languages despite multilingual breadth
- OCR correction and toxicity filtering rely on internal benchmarks without independent validation

## Confidence

**High Confidence:** The corpus's scale (2T tokens), multilingual coverage (10+ languages with ≥10B tokens each), and domain diversity are verifiable through HuggingFace metadata and source repository snapshots. The license compliance framework is methodologically sound given current copyright frameworks.

**Medium Confidence:** The quality improvements from OCR correction and PII removal tools are supported by internal benchmarks but lack external validation. The claim that these tools make historical documents "usable" for pre-training assumes no systematic semantic drift, which remains unproven at scale.

**Low Confidence:** The assertion that Common Corpus will become "critical infrastructure for open science in LLM development" overstates current adoption. While projects like HPLT 3.0 and EMMA-500 pursue similar goals, the paper provides no comparative analysis of performance or adoption rates.

## Next Checks

1. **Legal Risk Validation:** Commission an independent copyright law review of Common Corpus's license compliance framework, focusing on potential challenges under jurisdictions with stricter "text and data mining" exceptions than the US.

2. **OCR Correction Audit:** Select 500 historical documents from Open Culture, apply OCRonos correction, and have domain experts (historians, linguists) evaluate semantic drift vs. surface error correction rates compared to original PDFs.

3. **Toxicity Filtering Precision:** Use Celadon's five-dimensional filtering on a balanced test set containing both harmful and legitimate historical content (e.g., period-appropriate racial terminology in historical contexts). Measure precision vs. recall trade-offs and false positive rates.