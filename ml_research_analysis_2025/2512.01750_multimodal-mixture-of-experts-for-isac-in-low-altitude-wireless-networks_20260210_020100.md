---
ver: rpa2
title: Multimodal Mixture-of-Experts for ISAC in Low-Altitude Wireless Networks
arxiv_id: '2512.01750'
source_url: https://arxiv.org/abs/2512.01750
tags:
- multimodal
- sensing
- fusion
- isac
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multimodal mixture-of-experts (MoE) framework
  for integrated sensing and communication (ISAC) in low-altitude wireless networks
  (LAWNs). The core method idea is to use modality-specific expert networks for processing
  heterogeneous sensing data (visual, radar, lidar, positional) and a gating network
  to dynamically assign fusion weights based on instantaneous informativeness and
  reliability of each modality.
---

# Multimodal Mixture-of-Experts for ISAC in Low-Altitude Wireless Networks

## Quick Facts
- arXiv ID: 2512.01750
- Source URL: https://arxiv.org/abs/2512.01750
- Reference count: 40
- Key outcome: A multimodal mixture-of-experts framework for integrated sensing and communication in low-altitude wireless networks, showing improved learning performance and training sample efficiency over conventional fusion baselines.

## Executive Summary
This paper introduces a multimodal mixture-of-experts (MoE) framework for integrated sensing and communication (ISAC) in low-altitude wireless networks (LAWNs). The core innovation is a modality-specific expert network architecture combined with a lightweight gating network that dynamically assigns fusion weights based on instantaneous informativeness and reliability of each modality. To address energy constraints of aerial platforms, a sparse MoE variant selectively activates only a subset of experts. The framework is evaluated on three typical ISAC tasks: sensing-aided beam prediction, sensing-aided path loss prediction, and communication-aided UAV trajectory tracking, consistently outperforming conventional multimodal fusion baselines.

## Method Summary
The framework processes five heterogeneous sensing modalities (visual, radar, lidar, positional, RF) through modality-specific expert networks, with a gating network dynamically assigning fusion weights. The sparse MoE variant uses top-N selection to activate only a subset of experts, reducing computation overhead while preserving adaptive fusion benefits. The approach is trained end-to-end using Adam optimizer on a public multimodal ISAC dataset, with task-specific prediction heads for each application.

## Key Results
- The multimodal MoE framework consistently outperforms conventional multimodal fusion baselines in learning performance and training sample efficiency across three ISAC tasks
- Sparse MoE achieves nearly on-par performance with dense MoE while using only 5 of 15 experts, demonstrating significant computational savings
- The adaptive gating mechanism improves robustness when individual modalities degrade, automatically suppressing unreliable inputs

## Why This Works (Mechanism)

### Mechanism 1
Adaptive gating improves robustness when individual modalities degrade. The learned gating network observes all modalities and produces softmax-normalized weights that amplify reliable experts while suppressing noisy or uninformative ones, creating input-dependent fusion rather than fixed weights. Core assumption: the gating network can learn to correlate input features with downstream task performance during training. Evidence: gating network learns to model uncertainty associated with each modality and dynamically generates probability distribution over experts. Break condition: if all modalities degrade simultaneously, gating may have no reliable reference to weight against.

### Mechanism 2
Sparse expert activation reduces computation with minimal accuracy loss. The sparse MoE variant uses top-N selection to activate only highest-scored experts, skipping forward passes through inactive branches. Gradients are approximated via straight-through estimator during backpropagation. Core assumption: marginal utility of additional modalities follows diminishing returns. Evidence: sparse MoE achieves nearly on par NMSE with dense MoE while using 5 of 15 experts. Break condition: if task-relevant information is evenly distributed across many modalities, aggressive sparsity may discard critical features.

### Mechanism 3
Modality-specific expert architectures capture domain-specific patterns better than shared backbones. Each expert uses architecture tailored to its modality—ResNet for images, PointNet for lidar, sequence models for radar. This preserves inductive biases rather than forcing heterogeneous inputs through unified encoder. Core assumption: domain-specific architectures extract more discriminative features than generic shared encoders. Evidence: neural architecture of each expert is specifically customized to align with characteristics of its corresponding modality. Break condition: if training data is insufficient for some modalities, specialized experts may overfit or fail to generalize.

## Foundational Learning

- **Mixture-of-Experts (MoE)**
  - Why needed here: Core architectural pattern; separate experts with learned routing is the primary contribution
  - Quick check question: Given 4 modalities with gating weights [0.1, 0.6, 0.2, 0.1], what is the effective contribution of modality 2 to the fused representation?

- **Multimodal Fusion Strategies**
  - Why needed here: Paper positions MoE against static fusion (concatenation, weighted sum) and monolithic architectures (transformers, LLMs)
  - Quick check question: Why does static weighted summation fail when vision is blocked but lidar remains reliable?

- **ISAC (Integrated Sensing and Communication)**
  - Why needed here: Application domain; tasks include beam prediction, path loss estimation, and trajectory tracking
  - Quick check question: How does sensing-aided beam prediction differ from traditional pilot-based beam selection?

## Architecture Onboarding

- **Component map:**
  Input modalities (vision, lidar, radar, position, RF) -> 15 parallel experts (3 per modality: ResNet-18 for images, PointNet for point clouds) -> 3-layer MLP gating network (128 hidden units) -> Top-N selector (N=5) -> Task-specific prediction heads

- **Critical path:**
  1. Synchronize multimodal inputs at time t
  2. Extract expert embeddings z_d(t) for active experts only (sparse) or all experts (dense)
  3. Compute gating weights via g_gate(X(t))
  4. Select top-N and renormalize weights (sparse variant)
  5. Fuse: z(t) = Σ w_d(t) × z_d(t)
  6. Predict: ô(t) = g_o(z(t))

- **Design tradeoffs:**
  - Dense vs. sparse: Dense guarantees all information used; sparse saves compute but risks missing modality-specific signals
  - Expert count per modality: More experts increase capacity but require more training data and compute
  - Gating complexity: Simpler gating (MLP) is fast but may miss complex cross-modal dependencies; attention-based gating is expressive but heavier

- **Failure signatures:**
  - Gating collapse: All weight assigned to one expert (check weight entropy during training)
  - Expert underutilization: Some experts never activated (check activation frequency per expert)
  - Sparsity too aggressive: Accuracy drops sharply when N reduced (run ablation on N)
  - Modality interference: Performance degrades when adding modalities (suggests gating not learning to suppress noise)

- **First 3 experiments:**
  1. Baseline comparison on single task: Implement dense MoE for beam prediction; compare top-1 accuracy against feature concatenation and transformer baselines
  2. Sparsity ablation: Vary N from 1 to 15 experts; plot accuracy vs. FLOPs to find knee point where marginal accuracy gain < 1%
  3. Modality dropout robustness: Systematically zero out each modality during inference; measure performance degradation to verify gating adapts correctly

## Open Questions the Paper Calls Out

- **Open Question 1**
  Can integrating explicit uncertainty-aware gating or hierarchical routing mechanisms significantly improve the robustness of the sparse MoE framework compared to the standard MLP gating network? While the authors suggest these extensions, the implemented framework utilizes a standard MLP for gating; the potential performance gains remain unquantified. Comparative simulations under high noise or partial sensor failure conditions would resolve this.

- **Open Question 2**
  How does the proposed framework perform when deployed on real-world aerial platforms compared to the synthetic simulation environment used for evaluation? The evaluation relies on simulated channels and sensing data, potentially creating a sim-to-real gap. Over-the-air experimental results using physical UAVs and sensors would provide validation.

- **Open Question 3**
  To what extent does the computational overhead of the modality-aware gating network negate the efficiency gains achieved by the sparse activation of experts? The gating network must still process high-dimensional inputs from all modalities, which could become a bottleneck on resource-constrained UAVs. A detailed breakdown of inference latency and energy consumption would clarify this tradeoff.

## Limitations
- Expert output dimensions and prediction head architecture not fully specified, affecting reproducibility
- Gating network expressiveness may be insufficient for complex cross-modal dependencies
- Sparse MoE routing stability may be compromised by straight-through estimator approximation during early training

## Confidence
- **High Confidence**: The core mechanism of adaptive modality fusion through learned gating weights is well-supported by described architecture and ablation studies
- **Medium Confidence**: The sparsity benefits claim is demonstrated through comparative experiments, but optimal N appears task-dependent
- **Low Confidence**: The superiority of modality-specific expert architectures over shared backbones lacks direct empirical comparison

## Next Checks
1. Apply the proposed MoE framework to an additional ISAC task using the same LAWN dataset to verify cross-task generalization
2. Simulate scenarios where multiple modalities fail simultaneously to test gating network adaptation to complex degradation patterns
3. Replace modality-specific experts with shared backbone architectures and measure performance gap to quantify benefits of specialized architectures