---
ver: rpa2
title: 'Exploring the Use of Contrastive Language-Image Pre-Training for Human Posture
  Classification: Insights from Yoga Pose Analysis'
arxiv_id: '2501.07221'
source_url: https://arxiv.org/abs/2501.07221
tags:
- clip
- images
- classification
- accuracy
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the use of Contrastive Language-Image Pre-Training
  (CLIP) for classifying human postures, focusing on yoga poses. CLIP, a multimodal
  learning technique, is adapted for yoga pose classification through transfer learning.
---

# Exploring the Use of Contrastive Language-Image Pre-Training for Human Posture Classification: Insights from Yoga Pose Analysis

## Quick Facts
- arXiv ID: 2501.07221
- Source URL: https://arxiv.org/abs/2501.07221
- Reference count: 0
- Primary result: CLIP fine-tuning achieves >85% accuracy on 82-class yoga pose classification, surpassing previous state-of-the-art by ~6%

## Executive Summary
This paper evaluates CLIP for yoga pose classification, demonstrating that fine-tuning the pre-trained multimodal model on the Yoga-82 dataset yields over 85% accuracy across 82 pose classes. The approach outperforms previous state-of-the-art by approximately 6% while requiring 3.5 times less training time than YOLOv8-based methods. The study also shows that with as few as 20 training images per pose, accuracy remains around 90% for a six-class subset, highlighting CLIP's strong few-shot capabilities for human posture classification tasks.

## Method Summary
The method employs CLIP's pre-trained ViT-B/32 visual encoder, fine-tuned on 15,301 yoga pose images from the Yoga-82 dataset. The fine-tuning uses a learning rate of 10⁻⁵, weight decay of 10⁻³, and runs for 5 epochs with a batch size of 82. Text prompts follow the template "Image of a person doing the yoga pose <category>". The model computes cosine similarities between image embeddings and class text embeddings, applying softmax to obtain classification probabilities. Evaluation includes both filtered and unfiltered training subsets across 78 random train/test splits.

## Key Results
- Fine-tuned CLIP achieves 85.9% accuracy on 82-class yoga pose classification
- Training time is 3.5× faster than YOLOv8-based approaches
- Inference time is approximately 7 ms per image
- With only 20 images per class, six-class accuracy reaches 89.9%

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning CLIP's visual encoder with contrastive loss improves pose classification accuracy from near-random (zero-shot) to 85%+ on 82 classes. CLIP jointly trains image and text encoders to maximize cosine similarity for correct image-text pairs while minimizing it for incorrect pairs. Fine-tuning adapts the pre-trained embedding space to the yoga domain by optimizing this contrastive objective on domain-specific pairs.

### Mechanism 2
Reducing the number of training images per class to as few as 20 still yields approximately 90% accuracy for a 6-class subset. CLIP's pre-training on 400M image-text pairs provides a rich prior over visual-linguistic concepts, enabling few-shot generalization. Fine-tuning with limited data adjusts the decision boundaries without catastrophic forgetting of pre-trained representations.

### Mechanism 3
Lower learning rates (10⁻⁵) and moderate weight decay (10⁻³) produce optimal fine-tuning results; higher rates cause divergence. Pre-trained models occupy a stable region in parameter space. Small learning rates enable incremental adaptation without disrupting learned representations, while weight decay regularizes against overfitting on small datasets.

## Foundational Learning

- **Contrastive Learning (InfoNCE-style objectives)**
  - Why needed here: Understanding how CLIP learns to align image-text pairs through similarity maximization/minimization explains why fine-tuning works and how to interpret cosine similarity matrices
  - Quick check question: Given an image of a yoga pose and three text descriptions, can you explain which similarity scores CLIP would maximize vs. minimize during training?

- **Vision Transformer (ViT) Architecture**
  - Why needed here: The paper selects ViT-B/32 as the visual encoder; understanding patch-based processing, attention mechanisms, and positional embeddings helps interpret activation maps and gScoreCAM visualizations
  - Quick check question: Why might a ViT-based encoder capture global pose relationships differently than a CNN-based encoder like ResNet?

- **Prompt Engineering for Vision-Language Models**
  - Why needed here: Table 5 shows that text description syntax affects accuracy (96.3%–100% range); understanding prompt design is critical for deployment and for extending to new pose categories
  - Quick check question: If adding a new pose class "Eka Pada Rajakapotasana," how would you construct the text prompt, and why might Sanskrit terms underperform compared to descriptive English?

## Architecture Onboarding

- **Component map:**
  - Image encoder: ViT-B/32 (151M parameters), 224×224 input resolution
  - Text encoder: Transformer-based, processes tokenized prompts
  - Projection heads: Map both modalities to shared embedding space
  - Classification head: Cosine similarity computation between image embedding and all class text embeddings; softmax over similarities for probabilities

- **Critical path:**
  1. Data preparation: Format images as 224×224 RGB; generate text prompts using template "Image of a person doing the yoga pose <category>"
  2. Forward pass: Encode image through ViT-B/32; encode all class descriptions through text encoder
  3. Loss computation: Cross-entropy on image-text similarity logits (Eq. 1-2)
  4. Inference: Compare image embedding to all 82 class text embeddings; return class with highest similarity

- **Design tradeoffs:**
  - ViT-B/32 vs. larger encoders (ViT-L/14): Paper chose B/32 for computational efficiency; larger encoders may improve accuracy but increase training/inference latency
  - Filtered vs. unfiltered training data: <1% accuracy difference suggests filtering is unnecessary for this task
  - Epochs: 5 epochs sufficient; Figure 5 shows diminishing returns after epoch 1

- **Failure signatures:**
  - Low accuracy on visually similar poses (e.g., Makara Adho Mukha Svanasana vs. Chaturanga Dandasana; Figure 7, Appendix B)
  - High standard deviation across random splits suggests sensitivity to class imbalance (Table 9)
  - Zero-shot accuracy near random (Table 4) indicates pre-training data has limited yoga-specific coverage

- **First 3 experiments:**
  1. Reproduce zero-shot baseline on Yoga-82-Subset II using pre-trained CLIP ViT-B/32 with "Yoga pose <category>" prompt to confirm Table 4 results
  2. Fine-tune CLIP on Subset I (unfiltered, 6 classes) with learning rate 10⁻⁵, weight decay 10⁻³, 5 epochs, batch size 6; verify ~98.8% accuracy matches Table 6
  3. Run data ablation: train on progressively smaller subsets (43→20→6 images/class per Table 11) to confirm few-shot degradation curve and identify the practical minimum for your deployment context

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the CLIP-based classification pipeline be effectively generalized to other human posture domains, such as ergonomics or physical rehabilitation, without significant loss of accuracy?
  - **Basis in paper:** The conclusion states that future work includes generalizing the pipeline for human posture classification in other domains
  - **Why unresolved:** The study restricted its validation to the Yoga-82 dataset, and the authors note that a "sufficiently strong generalization hypothesis" cannot yet be established for non-yoga postures
  - **What evidence would resolve it:** Successful benchmarking of the fine-tuned model on diverse datasets like workplace ergonomics or clinical rehabilitation datasets

- **Open Question 2:** How can generative AI be utilized to create individual-tailored datasets to improve the model's rapid adaptation to individual diversity?
  - **Basis in paper:** Section 7 proposes using generative AI and posterior augmentation to produce datasets tailored to individuals
  - **Why unresolved:** The current study utilized static datasets (Yoga-82) and data filtering, but did not implement dynamic, AI-generated synthetic data to address individual variability
  - **What evidence would resolve it:** Experiments showing that models trained on AI-generated, user-specific augmentation data outperform those trained on generic datasets in recognizing specific user variations

- **Open Question 3:** Can the classification model be extended to accurately assess the *quality* or correctness of a pose, rather than solely classifying its label?
  - **Basis in paper:** The abstract and conclusion describe the motivation of building a "personal yoga assistant for performance assessment," implying a need for qualitative evaluation beyond simple labeling
  - **Why unresolved:** The current methodology focuses on Top-1 accuracy for class labels (identifying *which* pose is performed), not the biomechanical correctness of the execution
  - **What evidence would resolve it:** The development of a scoring mechanism or regression output that correlates with expert human evaluation of pose quality

## Limitations
- Results are specific to the Yoga-82 dataset and may not generalize to more diverse human posture tasks
- Computational efficiency claims lack rigorous benchmarking details with exact YOLOv8 configuration and hardware specifications
- The study doesn't explore the theoretical minimum data requirements for stable fine-tuning

## Confidence

**High Confidence**: The fine-tuning methodology is well-established, and the reported accuracy improvements (>85% on 82 classes, 6% over previous state-of-the-art) are supported by multiple experimental conditions. The hyperparameter sensitivity (learning rate 10⁻⁵ optimal, 10⁻⁴ causing divergence) is systematically validated.

**Medium Confidence**: The few-shot learning claims (90% accuracy with 20 images/class) are supported by ablation studies, but the results may be specific to the yoga domain and the particular 6-class subset. The computational efficiency claims lack detailed benchmarking methodology.

**Low Confidence**: The assertion that CLIP can be "effectively used for human posture classification in general" extends beyond the empirical evidence, which is limited to yoga poses. The paper doesn't test CLIP on non-yoga posture datasets or investigate potential domain adaptation challenges.

## Next Checks

1. **Domain Transfer Validation**: Test the fine-tuned CLIP model on a non-yoga human posture dataset (e.g., Human3.6M or MPII) to assess generalizability beyond the yoga domain.

2. **Benchmark Reproducibility**: Replicate the computational efficiency comparison by implementing YOLOv8 with identical hardware specifications and dataset splits to verify the 3.5× training speed improvement and 7ms inference time claims.

3. **Few-Shot Robustness Analysis**: Conduct a systematic study of the learning curve with progressively fewer training samples (100→50→20→10→5 images/class) to identify the theoretical minimum for stable fine-tuning and investigate the impact of class imbalance on few-shot performance.