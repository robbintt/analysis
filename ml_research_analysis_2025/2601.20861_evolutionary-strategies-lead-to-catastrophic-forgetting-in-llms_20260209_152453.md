---
ver: rpa2
title: Evolutionary Strategies lead to Catastrophic Forgetting in LLMs
arxiv_id: '2601.20861'
source_url: https://arxiv.org/abs/2601.20861
tags:
- grpo
- task
- updates
- arxiv
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates catastrophic forgetting in evolutionary
  strategies (ES) when applied to fine-tune large language models (LLMs), comparing
  ES to the gradient-based GRPO algorithm. ES is explored as a memory-efficient alternative
  to GRPO, but the study finds that ES causes significant degradation in prior task
  performance while training on new tasks, even after new task convergence.
---

# Evolutionary Strategies lead to Catastrophic Forgetting in LLMs

## Quick Facts
- **arXiv ID**: 2601.20861
- **Source URL**: https://arxiv.org/abs/2601.20861
- **Reference count**: 8
- **Key outcome**: ES causes significant degradation in prior task performance while training on new tasks, even after new task convergence.

## Executive Summary
This paper investigates catastrophic forgetting when using Evolutionary Strategies (ES) for fine-tuning large language models, comparing ES to the gradient-based GRPO algorithm. ES is explored as a memory-efficient alternative to GRPO, but the study finds that ES causes significant degradation in prior task performance while training on new tasks, even after new task convergence. Analysis reveals that ES updates are less sparse and have much larger ℓ2 norms compared to GRPO, leading to large parameter drifts that interfere with previously learned capabilities. This behavior limits ES's applicability for continual learning, despite competitive performance on new tasks.

## Method Summary
The paper compares ES against GRPO for fine-tuning LLMs on new tasks while measuring retention of prior capabilities. Experiments use Qwen2.5-1.5B-Instruct and Llama-3.2-1B-Instruct models, training on Countdown, GSM8K, MATH, and OlympiadBench (200 examples each) while evaluating prior-task retention on HellaSwag. ES implementation follows Qiu et al. (2025) with population size 30, noise scale 0.001, and learning rate 0.0005. GRPO uses VERL library with 30 rollouts, batch size 200, mini-batch 32, and KL coefficient 0.001. The study tracks accuracy, Frobenius norm drift, and update sparsity across training iterations.

## Key Results
- ES causes catastrophic forgetting, with prior task accuracy dropping ~10% over 500 training iterations
- ES updates are significantly less sparse than GRPO (sparsity close to 95% for GRPO vs. very low for ES)
- ES update Frobenius norms are approximately 1000× larger than GRPO after equivalent training steps

## Why This Works (Mechanism)

### Mechanism 1: ES Update Density Destroys Prior Knowledge
ES constructs each update from high-variance, global perturbations applied uniformly across all parameters. Unlike backpropagation, which concentrates gradients in task-relevant subspaces, ES perturbs parameters without localized targeting. This dense update pattern modifies a much larger fraction of weights per step, interfering with previously learned capabilities.

### Mechanism 2: Large Update Norms Drive Parameter Drift
ES produces updates with Frobenius norms approximately 1000× larger than GRPO after equivalent training steps. Without gradient-based directional constraints, these updates accumulate in arbitrary directions, causing the model to drift far from its original capabilities. The paper shows monotonic Frobenius norm growth with training iterations, unlike GRPO's bounded drift.

### Mechanism 3: Absence of KL Regularization in ES
GRPO's objective includes -β·D_KL(π_θ || π_ref), actively penalizing divergence from the base model. ES lacks this safeguard—updates proceed purely based on reward-weighted noise, with no mechanism to anchor near the initialization. This absence of regularization enables uncontrolled deviation from prior capabilities.

## Foundational Learning

- **Concept: Catastrophic Forgetting**
  - Why needed here: The entire paper frames ES's limitation through this lens—you cannot evaluate the claims without understanding that "forgetting" refers to performance degradation on prior tasks when learning new ones.
  - Quick check question: If a model learns task B after task A, and its task A accuracy drops from 80% to 60%, is this catastrophic forgetting? (Yes, if the drop is due to parameter interference rather than task difficulty.)

- **Concept: Evolutionary Strategies (gradient-free optimization)**
  - Why needed here: ES is the core algorithm under study. You must understand it estimates gradients via population perturbations rather than backpropagation.
  - Quick check question: How does ES compute a parameter update without computing gradients? (Answer: It samples N perturbed models, evaluates their rewards, and updates parameters using reward-weighted noise vectors.)

- **Concept: Update Sparsity and Norm as Stability Indicators**
  - Why needed here: The paper's diagnostic analysis relies on these metrics to explain why ES forgets. Understanding them is prerequisite to interpreting Figures 3 and 4.
  - Quick check question: A sparsity of 95% means what fraction of parameters changed by more than 10⁻⁶? (Answer: 5% changed significantly; 95% are effectively unchanged.)

## Architecture Onboarding

- **Component map:**
  ```
  ES Pipeline: Base Model → N Random Seeds → N Perturbed Models → Reward Evaluation → Z-Score Weighting → Parameter Update
  GRPO Pipeline: Base Model → N Rollouts from Policy → Reward Evaluation → Advantage Calculation → KL-Regularized Gradient Update
  ```

- **Critical path:**
  1. Initialize base model (Qwen2.5-1.5B or Llama-3.2-1B)
  2. For ES: generate N=30 perturbations via noise sampling, evaluate rewards, compute z-score weighted update
  3. For GRPO: sample N=30 outputs per prompt, compute group-relative advantages, apply clipped objective with KL penalty (β=0.001)
  4. Track: new task accuracy (Countdown), prior task accuracy (HellaSwag), Frobenius norm drift, update sparsity

- **Design tradeoffs:**
  - Memory vs. Stability: ES avoids storing gradients/optimizer states (memory efficient) but produces unstable, dense updates. GRPO requires more memory but maintains prior capabilities.
  - Compute parity: Paper shows similar update steps to convergence, so compute is not the differentiator.
  - Assumption: Tradeoff assumes binary choice; hybrid approaches (sparse ES, KL-regularized ES) are unexplored.

- **Failure signatures:**
  - Prior task accuracy declines monotonically even after new task convergence
  - Frobenius norm grows without bound (no plateau)
  - KL divergence from base model increases linearly with iterations
  - Update sparsity remains low (<50%) across all layer types

- **First 3 experiments:**
  1. **Replicate forgetting curve:** Train Qwen2.5-1.5B on Countdown with ES, evaluate HellaSwag every 50 iterations. Confirm ~10% drop over 500 iterations.
  2. **Norm ablation:** Add explicit Frobenius norm clipping to ES (e.g., cap at 2× initial model norm). Measure impact on forgetting vs. new task performance.
  3. **Sparsification test:** Mask ES updates to retain only top-k% largest magnitude changes (k=5, 10, 20). Evaluate whether enforced sparsity reduces forgetting without sacrificing convergence speed.

## Open Questions the Paper Calls Out

### Open Question 1
Can ES be modified to produce sparser, lower-norm updates while preserving its gradient-free nature and memory efficiency?
Basis: The paper aims to highlight forgetting in gradient-free algorithms and inspire future work to mitigate these issues. Evidence: Demonstrating a modified ES variant with controlled update sparsity and bounded norms that reduces forgetting on prior tasks would resolve this.

### Open Question 2
Would combining ES with explicit regularization mechanisms (e.g., KL constraints similar to GRPO) mitigate catastrophic forgetting?
Basis: The paper attributes GRPO's stability partly to its KL-regularization factor, which ES lacks entirely. Evidence: Ablation studies comparing standard ES against ES with various regularization terms on forgetting metrics would resolve this.

### Open Question 3
How does population size in ES affect the trade-off between update variance, forgetting, and computational cost?
Basis: The limitations section states that "increased population size will decrease the variance and increase statistical stability" but this was not tested. Evidence: Systematic evaluation of ES forgetting curves across varying population sizes would resolve this.

### Open Question 4
Does the catastrophic forgetting observed in ES transfer across different prior-task evaluations beyond HellaSwag?
Basis: The limitations acknowledge that evaluating catastrophic forgetting by tracking performance on one task "does not fully capture multi-faceted loss of performance." Evidence: Multi-task prior capability evaluation (e.g., commonsense reasoning, knowledge, coding) during ES fine-tuning would resolve this.

## Limitations
- Study establishes correlations between update sparsity, norm magnitude, and forgetting, but does not prove causation through ablation studies
- Assumes sparse updates inherently preserve prior knowledge without directly testing whether artificially sparsifying ES updates would reduce forgetting
- Only uses HellaSwag to measure prior capability retention, leaving generality of forgetting unclear

## Confidence

- **High Confidence**: ES causes catastrophic forgetting while GRPO maintains prior capabilities (empirical observation)
- **Medium Confidence**: Dense updates and large norms are responsible for forgetting (mechanistic hypothesis supported by correlation but lacking ablation validation)
- **Low Confidence**: Sparsity-forgetting relationship (draws on related work but not independently validated)

## Next Checks

1. **Norm Regularization Test**: Apply explicit Frobenius norm clipping to ES updates (cap at 2× initial model norm) and measure impact on forgetting vs. new task performance
2. **Sparsification Experiment**: Mask ES updates to retain only top-k% largest magnitude changes (k=5, 10, 20) and evaluate whether enforced sparsity reduces forgetting without sacrificing convergence speed
3. **KL Regularization Addition**: Add reference-model KL regularization to ES updates and measure whether forgetting decreases while maintaining competitive new-task performance