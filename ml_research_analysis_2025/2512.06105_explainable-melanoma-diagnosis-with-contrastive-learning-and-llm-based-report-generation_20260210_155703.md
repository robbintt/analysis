---
ver: rpa2
title: Explainable Melanoma Diagnosis with Contrastive Learning and LLM-based Report
  Generation
arxiv_id: '2512.06105'
source_url: https://arxiv.org/abs/2512.06105
tags:
- clinical
- melanoma
- features
- classification
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the interpretability challenge in deep learning
  models for melanoma diagnosis, where black-box decision-making limits clinical adoption.
  The proposed Cross-modal Explainable Framework for Melanoma (CEFM) integrates a
  Vision Transformer for classification with a clinical explanation pipeline that
  quantifies ABC features (Asymmetry, Border, Color) from segmented lesions.
---

# Explainable Melanoma Diagnosis with Contrastive Learning and LLM-based Report Generation

## Quick Facts
- arXiv ID: 2512.06105
- Source URL: https://arxiv.org/abs/2512.06105
- Authors: Junwen Zheng; Xinran Xu; Li Rong Wang; Chang Cai; Lucinda Siyun Tan; Dingyuan Wang; Hong Liang Tey; Xiuyi Fan
- Reference count: 5
- Primary result: 92.79% accuracy and 0.961 AUC on ISIC 2020 melanoma classification with clinical interpretability

## Executive Summary
The paper addresses the interpretability challenge in deep learning models for melanoma diagnosis, where black-box decision-making limits clinical adoption. The proposed Cross-modal Explainable Framework for Melanoma (CEFM) integrates a Vision Transformer for classification with a clinical explanation pipeline that quantifies ABC features (Asymmetry, Border, Color) from segmented lesions. Contrastive learning aligns visual and clinical representations, while a CLIP-enhanced LLM generates structured diagnostic reports. Experiments on the ISIC dataset show 92.79% accuracy and 0.961 AUC. Qualitative analyses and expert dermatologist evaluations (mean 4.6/5) confirm the framework's clinical interpretability and trustworthiness, bridging the gap between high-performance classification and clinical decision support.

## Method Summary
CEFM employs a Vision Transformer as the backbone for melanoma classification, paired with a clinical explanation pipeline. Lesion segmentation uses a two-stage approach: UltraLight VM-UNet provides coarse masks (Dice: 0.8909), which serve as pseudo-labels for SAM2 to generate refined boundaries. ABC features are computed from these masks: Asymmetry via pixel differential between original and mirrored images; Border irregularity via mean curvature along the contour; Color variation via HSV channel standard deviations. A dual projection head maps ViT embeddings and clinical features into a shared latent space, optimized with normalized temperature-scaled cross-entropy loss for contrastive alignment. Discretized ABC scores and CLIP-retrieved visual descriptors are combined into prompts for DeepSeek to generate structured diagnostic reports.

## Key Results
- 92.79% ± 0.57% accuracy and 0.961 ± 0.004 AUC on ISIC 2020 test set
- Expert dermatologist interpretability score of 4.6/5 for generated reports
- Cosine similarity for positive image-clinical pairs clusters near 0.8, confirming effective cross-modal alignment

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Contrastive Alignment
Aligning visual embeddings with clinical ABC features through contrastive learning produces semantically meaningful representations that support both classification accuracy and interpretability. Dual projection heads map ViT image embeddings and clinical feature vectors into a shared latent space. Normalized temperature-scaled cross-entropy loss (NT-Xent) enforces alignment by maximizing cosine similarity for positive image-clinical pairs while minimizing it for negative pairs. The pretrained image encoder is frozen; only projection heads are optimized. Core assumption: Mathematical formulations of ABC features meaningfully capture dermatological diagnostic criteria, and alignment preserves clinical semantics in the embedding space. Evidence anchors: [abstract] "CEFM maps clinical criteria for melanoma diagnosis—namely Asymmetry, Border, and Color (ABC)—into the Vision Transformer embedding space using dual projection heads, thereby aligning clinical semantics with visual features"; [Feature Spaces Alignment] "positive pairs exhibit significantly higher similarity values, concentrated near 0.8, while negative pairs are distributed near zero. This clear separation demonstrates that contrastive learning effectively aligns cross-modal features"; [corpus] SLIMP (Skin Lesion Image-Metadata Pre-training) applies nested contrastive learning for skin lesion phenotyping, supporting the viability of contrastive approaches in dermatological imaging. Break condition: If ABC features are noisy or inconsistently computed (e.g., segmentation failures), contrastive alignment may learn spurious correlations that degrade both interpretability and classification.

### Mechanism 2: Coarse-to-Fine Segmentation for ABC Quantification
A two-stage segmentation pipeline enables robust extraction of clinically grounded ABC features that serve as interpretable diagnostic priors. UltraLight VM-UNet provides coarse lesion masks (Dice: 0.8909) as pseudo-labels. SAM2 is then prompted with foreground/background points sampled from coarse masks to generate refined boundaries. Quantitative ABC features are computed from refined masks: Asymmetry via pixel differential between original and mirrored images; Border irregularity via mean curvature along the contour; Color variation via HSV channel standard deviations. Core assumption: The mathematical definitions of A, B, C correlate with clinician perceptual assessment of these criteria. Evidence anchors: [Clinical Feature Extraction] Table 1 provides explicit formulas: A = Σ|I(x,y) - I_mirror(x,y)| / ΣM(x,y); B2 = (1/N)Σκi where κi = Δθi/Δsi; Color = σH, σS, σV; [User Study] "Experts confirmed that structured ABC quantification offers clinically relevant and interpretable outputs consistent with physician intuition"; [corpus] Weak direct evidence—related papers focus on classification or report generation rather than ABC quantification specifically. Break condition: Segmentation failures on lesions with fuzzy boundaries, artifacts, or rare morphologies propagate incorrect ABC values, undermining downstream alignment and reports.

### Mechanism 3: Hybrid Report Synthesis (CLIP + LLM)
Combining quantitative ABC discretization with CLIP-retrieved visual descriptors, then synthesizing via domain-adapted LLM, produces clinically coherent structured reports. ABC features are discretized into 5 severity levels. CLIP-ViT-B/16 retrieves top-ranked melanoma-related descriptors (e.g., "asymmetric shape," "blue-gray areas") based on image-text similarity. Both inputs are formatted into prompts for DeepSeek, which generates structured reports with quantitative analysis, visual features, and risk assessment sections. Core assumption: The LLM can resolve potential conflicts between quantitative ABC scores and CLIP-retrieved descriptors to produce internally consistent narratives. Evidence anchors: [Report Generation with LLM] "These quantitative and semantic cues are then combined into prompts and fed into DeepSeek, a domain-adapted large language model, to generate concise, medically grounded diagnostic reports"; [Ablation Study] "Removing the clinical explanation module eliminates critical quantification of ABC features, resulting in text that relies solely on visual descriptions without numerical interpretation. Without DeepSeek, the report becomes fragmented"; [corpus] "Eyes on the Image: Gaze Supervised Multimodal Learning" demonstrates a similar two-stage multimodal framework combining contrastive learning with report generation. Break condition: If CLIP descriptors contradict ABC quantification (e.g., CLIP retrieves "regular symmetric shape" while A score indicates severe asymmetry), reports may contain inconsistent recommendations.

## Foundational Learning

- Concept: **Contrastive Learning (SimCLR/BYOL paradigm)**
  - Why needed here: Core mechanism for cross-modal alignment; requires understanding of positive/negative pair construction, projection heads, and temperature-scaled loss functions.
  - Quick check question: Why does temperature scaling (τ) in NT-Xent affect the hardness of negative pair discrimination?

- Concept: **Vision Transformers (ViT) for Medical Imaging**
  - Why needed here: ViT serves as the backbone feature extractor; its patch-based tokenization and self-attention differ fundamentally from CNN locality biases.
  - Quick check question: How do positional embeddings in ViT encode spatial relationships, and why might this matter for lesion analysis?

- Concept: **Segmentation Architectures (U-Net family, SAM)**
  - Why needed here: Clinical pipeline depends on accurate lesion masks; understanding encoder-decoder skip connections and prompt-based segmentation is essential for debugging.
  - Quick check question: What role do skip connections play in preserving boundary detail during upsampling?

## Architecture Onboarding

- Component map:
  Input: Dermoscopic image
      │
      ├─► [Path A: Classification]
      │   ViT encoder (frozen) → MLP projector → L2 norm → z_v
      │                                   └─► Classification head → Melanoma/ Nevus
      │
      ├─► [Path B: Clinical Explanation]
      │   UltraLight VM-UNet → SAM2 (prompted) → Refined mask
      │                                   └─► ABC computation → MLP projector → L2 norm → z_c
      │
      ├─► [Fusion: Contrastive Alignment]
      │   NT-Xent loss between z_v and z_c (positive pairs from same image)
      │
      └─► [Output: Report Generation]
          ABC (discretized) + CLIP descriptors → DeepSeek LLM → Structured report

- Critical path:
  1. Segmentation accuracy → ABC feature quality → Contrastive alignment effectiveness
  2. Alignment quality → Embedding discriminability → Classification performance
  3. ABC + CLIP coherence → LLM prompt quality → Report clinical validity

- Design tradeoffs:
  - Frozen ViT encoder preserves pretrained semantic features but limits dermatology-specific adaptation
  - "Differential structure" (4th ABCD criterion) excluded due to annotation requirements—reduces completeness
  - DeepSeek selected over Claude/Gemini for cost-effectiveness; BERTScore showed comparable semantic similarity
  - Two-stage segmentation adds complexity but improves boundary precision over single-model approaches

- Failure signatures:
  - Cosine similarity distributions for positive pairs remain scattered (not clustered >0.75) → contrastive alignment failed
  - Segmentation IoU drops significantly on dark/irregular lesions → ABC values become unreliable
  - Reports contain contradictory statements (e.g., "severe asymmetry" + "symmetric shape") → CLIP-ABC conflict not resolved
  - Classification accuracy degrades after contrastive training → projection head may be disrupting discriminative features

- First 3 experiments:
  1. **Segmentation validation**: Run coarse-to-fine pipeline on held-out ISIC images with ground truth masks; report Dice/IoU by lesion subtype (melanoma vs. nevus) to identify failure modes.
  2. **Alignment verification**: Before/after contrastive training, plot cosine similarity histograms for positive and negative pairs; confirm positive pairs shift toward >0.75 clustering.
  3. **Ablation report coherence**: Generate reports with (a) ABC-only, (b) CLIP-only, (c) full pipeline; have a clinician rate clinical coherence and internal consistency on a 5-point Likert scale.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the differential structure (D) criterion be integrated into the computational pipeline without relying on extensive manual annotations?
- Basis in paper: [explicit] The authors state that the "Differential structure" feature was excluded due to the "lack of fine-grained expert annotations" and list its integration as a goal for future work.
- Why unresolved: Identifying specific dermoscopic structures (e.g., dots, globules, networks) typically requires precise, pixel-level labeling by specialists, which the current coarse-to-fine segmentation pipeline avoids using.
- What evidence would resolve it: A weakly supervised or zero-shot learning approach that successfully extracts differential structures and demonstrates improved diagnostic completeness in the final report.

### Open Question 2
- Question: Does the framework maintain high performance and clinical trust when applied to non-ideal imaging conditions and rare lesion subtypes?
- Basis in paper: [explicit] The conclusion notes the limitation that the system "assumes high-quality dermoscopic inputs" and may underperform on "poor imaging conditions or rare lesion subtypes not represented in the training data."
- Why unresolved: The current evaluation relies on the curated ISIC dataset, which may not adequately represent the variance found in chaotic real-world clinical environments or uncommon pathologies.
- What evidence would resolve it: A prospective clinical study showing that the model's accuracy and expert trust scores remain stable when evaluated on uncurated, heterogeneous clinical images.

### Open Question 3
- Question: How can the framework be adapted to support multi-temporal lesion tracking for monitoring disease progression over time?
- Basis in paper: [explicit] The authors explicitly identify "support multi-temporal lesion tracking" as a direction for future work in the conclusion.
- Why unresolved: The current architecture processes static images independently; it lacks mechanisms to encode temporal dependencies or compare feature evolution across patient visits.
- What evidence would resolve it: An extension of the framework that inputs sequential images and generates reports quantifying changes in ABC features (e.g., growing asymmetry), validated on longitudinal datasets.

## Limitations

- Clinical interpretability relies heavily on ABC quantification formulas whose dermatological validity is assumed but not empirically validated against physician scoring standards
- Two-stage segmentation introduces complexity and potential error propagation; performance on atypical or heavily pigmented lesions remains unclear
- CLIP-descriptor alignment with ABC scores is not guaranteed; report inconsistencies could arise if these modalities conflict
- Domain adaptation of DeepSeek is described qualitatively without metrics quantifying its dermatological performance versus general-purpose LLMs
- Frozen ViT backbone limits dermatology-specific feature learning, potentially capping accuracy improvements

## Confidence

- **High confidence**: Classification accuracy (92.79%) and AUC (0.961) metrics are clearly reported with standard deviations
- **Medium confidence**: Expert interpretability score (4.6/5) is supported but based on limited expert pool size not disclosed
- **Medium confidence**: Contrastive alignment mechanism is well-described mathematically but verification requires access to exact hyperparameters and data splits
- **Low confidence**: Clinical validity of automated ABC quantification against ground truth physician assessments is not demonstrated

## Next Checks

1. **Segmentation reliability**: Evaluate segmentation pipeline on a held-out ISIC 2018 test set stratified by lesion type; report Dice/IoU metrics and failure analysis on challenging cases (dark/irregular lesions)
2. **Alignment verification**: Before and after contrastive training, plot cosine similarity distributions for positive and negative pairs; confirm positive pairs cluster above 0.75 with clear separation from negatives
3. **Report consistency audit**: Generate reports using (a) ABC-only, (b) CLIP-only, (c) combined pipeline; have dermatologists rate clinical coherence and flag any internal contradictions between quantitative and semantic components