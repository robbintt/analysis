---
ver: rpa2
title: Generalization in Monitored Markov Decision Processes (Mon-MDPs)
arxiv_id: '2505.08988'
source_url: https://arxiv.org/abs/2505.08988
tags:
- reward
- environment
- learning
- agent
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends Monitored Markov Decision Processes (Mon-MDPs)
  to non-tabular settings using function approximation, addressing the gap between
  theoretical Mon-MDPs and real-world applications where rewards are not always observable.
  The authors demonstrate that combining function approximation with a learned reward
  model enables agents to generalize from monitored states with observable rewards
  to unmonitored states with unobservable rewards.
---

# Generalization in Monitored Markov Decision Processes (Mon-MDPs)

## Quick Facts
- arXiv ID: 2505.08988
- Source URL: https://arxiv.org/abs/2505.08988
- Reference count: 40
- Key outcome: Function approximation with learned reward models enables agents to generalize from monitored to unmonitored states in Mon-MDPs, achieving near-optimal policies where tabular methods fail.

## Executive Summary
This paper addresses the gap between theoretical Monitored Markov Decision Processes (Mon-MDPs) and real-world applications by extending the framework to non-tabular settings using function approximation. The authors demonstrate that neural network reward models trained on monitored states can predict rewards for unmonitored states by learning shared feature representations. This enables agents to achieve near-optimal policies in environments formally defined as unsolvable in the tabular setting. However, they identify overgeneralization as a critical limitation where agents incorrectly extrapolate rewards, leading to undesirable behaviors such as watering cacti. To mitigate this, they propose uncertainty-aware cautious policy optimization using k-of-N counterfactual regret minimization, showing that agents can learn to act cautiously in unmonitored states.

## Method Summary
The approach combines DQN with a learned reward model, where the reward model is trained only when proxy rewards are observable and predicts rewards for unmonitored states. The Q-network takes joint environment and monitor states as input, using predicted rewards when ground truth is unavailable. For robust policies, an ensemble of 500 reward models captures epistemic uncertainty, and k-of-N counterfactual regret minimization optimizes against the k worst predictions to encourage cautious behavior in uncertain states.

## Key Results
- Reward model generalization enables near-optimal policies in half-room environment (Zone 1 monitored, Zone 2 unmonitored)
- Overgeneralization causes agents to water cacti at same rate as plants when reward model incorrectly extrapolates
- Robust policies reduce watering of novel plant types by 5× compared to standard reward models
- Ensemble-based uncertainty quantification enables effective cautious policy optimization

## Why This Works (Mechanism)

### Mechanism 1: Reward Model Generalization via Function Approximation
- Claim: A neural network reward model trained on monitored states can predict rewards for unmonitored states by learning shared feature representations.
- Mechanism: The reward model R̂(sE, aE, θ) is trained only when proxy rewards are observable (r̂E = rE). Neural network layers learn to extract features from egocentric views (6×11×11 tensors) that correlate with rewards. These features generalize to unmonitored states because similar plant types and dryness levels produce similar visual patterns regardless of spatial location.
- Core assumption: Monitored and unmonitored states share reward-relevant features that convolutional layers can capture; the reward function is approximately regular with respect to these features.
- Evidence anchors:
  - [abstract] "combining function approximation with a learned reward model enables agents to generalize from monitored states with observable rewards, to unmonitored environment states with unobservable rewards"
  - [section 4.2] "the reward model generalizes from the monitored section (Zone 1) to the unmonitored section (Zone 2). This generalization is evidenced by the agent's ability to appropriately water plants in both zones"
  - [corpus] Related work on function approximation in non-Markov processes (arxiv:2601.00151) shows convergence under ergodicity conditions, but does not directly validate reward model generalization in Mon-MDPs.
- Break condition: When monitored and unmonitored states have structurally different reward mappings (e.g., cacti only in unmonitored zone with negative rewards), the model incorrectly extrapolates positive rewards → overgeneralization.

### Mechanism 2: Uncertainty-Aware Cautious Policy via k-of-N CFR
- Claim: Optimizing against the k worst predictions from an ensemble of reward models produces risk-averse behavior in states with high epistemic uncertainty.
- Mechanism: Train N=500 reward models with random initialization. For each state-action pair, sample predictions from all models; high variance indicates epistemic uncertainty. The k-of-N counterfactual regret minimization algorithm updates the policy based on the k worst predicted rewards. A low k/n ratio (e.g., 1-of-10) means the agent optimizes for highly pessimistic scenarios, avoiding actions where any model predicts bad outcomes.
- Core assumption: Ensemble variance correlates with epistemic uncertainty; model disagreement on novel states signals unreliable reward predictions.
- Evidence anchors:
  - [abstract] "cautious policy optimization method leveraging reward uncertainty and k-of-N counterfactual regret minimization, showing that agents can learn to act cautiously in unmonitored states"
  - [section 4.4 Table 1] "robust policies 5-of-10 and 1-of-10, maintain a higher watering frequency for standard plants while significantly reducing watering for novel plants up to 5 times less than the reward model"
  - [corpus] Corpus evidence is weak—the "learning to be cautious" framework (Mohammedalamen et al., 2021) is cited but not present in neighbor papers; no external validation of k-of-N CFR in Mon-MDP settings.
- Break condition: When ensemble members all make similar incorrect predictions (correlated errors), variance underestimates true uncertainty → false confidence.

### Mechanism 3: Monitor-Aware Q-Learning with Predicted Rewards
- Claim: Incorporating monitor state into the Q-network and using predicted rewards enables learning when monitoring is valuable versus avoidable.
- Mechanism: The Q-network takes joint state (sE, sM) as input—environment features pass through convolutional layers, then concatenate with one-hot monitor encoding before fully-connected layers. When r̂E = ⊥, the reward model's prediction substitutes for the missing signal. The agent learns that requesting monitoring (action aM) incurs penalty rM = -0.2 but provides ground-truth reward for training the reward model.
- Core assumption: The monitor state is Markovian and its value (information gain vs. penalty) can be learned through standard Q-learning.
- Evidence anchors:
  - [section 3] "DQN receives the joint state (sE, sM) as input to estimate the Q-value for all joint action pairs (aE, aM); instead of using the environment reward, DQN utilizes the predicted reward provided by the reward model"
  - [section 4.1 Figure 3b] "The reward model rapidly learns to stop requesting monitoring, whereas ⊥ = 0 continuously requests monitoring at all times"
  - [corpus] No direct corpus evidence for monitor-aware Q-learning in Mon-MDPs; related POMDP work (arxiv:2601.03132) addresses partial state observability but not partial reward observability.
- Break condition: When monitor costs don't align with information value, or when monitor state transitions depend on unobserved factors.

## Foundational Learning

- **Concept: Epistemic vs. Aleatoric Uncertainty**
  - Why needed here: The cautious policy optimization relies on epistemic uncertainty (reducible ignorance about rewards) captured by ensemble disagreement. Aleatoric uncertainty (inherent randomness in rewards) would not be reduced by more data and shouldn't drive cautious behavior.
  - Quick check question: If all 500 ensemble models predict similar rewards for a novel plant type but with high variance in individual predictions, is this epistemic or aleatoric uncertainty? How should the k-of-N algorithm respond?

- **Concept: Counterfactual Regret Minimization (CFR)**
  - Why needed here: The k-of-N CFR algorithm determines how risk-averse the policy becomes. Understanding regret accumulation across counterfactual reward scenarios is essential for tuning the robustness level.
  - Quick check question: With N=10 sampled models, what behavior difference would you expect between k=10 (risk-neutral) and k=1 (highly risk-averse) when encountering a plant type the ensemble has high disagreement about?

- **Concept: Partial Observability Distinctions (Mon-MDP vs. POMDP)**
  - Why needed here: Mon-MDPs have observable environment states but partially observable rewards; POMDPs have partially observable states. This distinction affects what information the agent has access to and how belief states should be formulated.
  - Quick check question: In a Mon-MDP, the agent observes (sE, sM, aE, aM, r̂E, rM) at each step. What does r̂E = ⊥ mean, and how does this differ from a POMDP where the agent receives an observation o instead of sE?

## Architecture Onboarding

- **Component map:**
  Input: (sE, sM) where sE is 6×11×11 egocentric view, sM is one-hot {0,1}
  
  Reward Model Branch:
    sE → Conv2D(32, 5×5) → ReLU → Conv2D(64, 3×3) → ReLU → Flatten → FC(512) → FC(|A|) → predicted reward per action
  
  Q-Network Branch:
    (sE, sM) → Conv2D(32, 5×5) → ReLU → Conv2D(64, 3×3) → ReLU → Flatten → 
    Concat(sM) → FC(512) → FC(|A_E| × |A_M|) → Q-values for joint actions
  
  Ensemble: 500 independent reward model instances
  
  k-of-N CFR: Sample N models → Sort predictions → Select k worst → Policy update

- **Critical path:**
  1. Collect experience: (sE, sM, aE, aM, r̂E, rM, s'E, s'M) → replay buffer
  2. If r̂E ≠ ⊥: train reward model on (sE, aE, rE) via MSE loss
  3. Sample batch from replay buffer; use reward model prediction when r̂E = ⊥
  4. Train Q-network via DQN loss with predicted rewards
  5. Every 50 episodes: update target network
  6. For robust policies: run k-of-N CFR optimization (CPU, <15 seconds for 50 iterations)

- **Design tradeoffs:**
  - **Ensemble size N=500**: Better uncertainty quantification but 83.3 GPU hours for botanical garden training. Start with N=50-100 for debugging.
  - **k/n ratio**: 10-of-10 = risk-neutral (average predictions); 1-of-10 = highly risk-averse. Paper shows 1-of-10 reduces novel plant watering by 5× but may miss valid opportunities.
  - **Monitor awareness**: One-hot encoding is simple but assumes binary monitor states. For continuous monitoring probability, would need different representation.
  - **Window size 11×11**: Agent sees ~30% of 10×10 grid. Walls at edges create visual differences between zones (noted in Section 4.2).

- **Failure signatures:**
  - **Overgeneralization**: Agent waters cacti (triangle symbols, [0,1,1] encoding) at same rate as plants → reward model learned "green things need water" instead of plant-specific features. Check: visualize convolutional feature maps for plants vs. cacti.
  - **Plasticity loss**: Training reward degrades after 6M+ timesteps (Appendix A.4, Figure 8) → network weights become committed. Mitigation: monitor gradient magnitudes, consider weight resets or smaller learning rates.
  - **Continuous monitoring**: Agent never reduces monitor action frequency → reward model not learning. Check: reward model loss curve, data coverage of monitored states.
  - **Ensemble collapse**: All 500 models predict identically → initialization variance lost during training. Check: prediction variance across ensemble on held-out states.

- **First 3 experiments:**
  1. **Binary monitor sanity check**: Run 10 seeds in binary environment for 10M timesteps. Target: Figure 3a training curve (discounted joint reward converges to ~20) and Figure 3b monitor frequency drops to <5% by 2M timesteps. If monitor frequency stays high, check reward model learning rate (ηR=10^-4) and exploration decay.
  2. **Half-room generalization**: Train on half-room environment. Measure per-zone rewards (Figure 4a). Target: Zone 1 and Zone 2 rewards within 10% of each other. If Zone 2 underperforms by >20%, verify that: (a) reward model sees diverse plant states in Zone 1, (b) visual features generalize across zones (check for wall artifacts).
  3. **Robustness ablation**: In botanical garden with novel plants [0,0,1], [0,1,0], [1,0,0], [1,0,1], [1,1,1], compare standard reward model vs. 5-of-10 vs. 1-of-10. Target: Table 5 patterns—1-of-10 should reduce watering of novel plants by 4-5× compared to reward model. If no difference, verify ensemble predictions have meaningful variance (std > 0.1 on novel states).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Mon-MDPs be effectively solved using policy-based or actor-critic methods to handle continuous action spaces?
- Basis in paper: [explicit] The authors state, "Moving forward, our research paves the way for... extending Mon-MDPs to policy-based and actor-critic methods to handle continuous action spaces, including REINFORCE, TRPO, PPO, and SAC."
- Why unresolved: The current work relies exclusively on DQN, which requires discrete action spaces and does not address the complexities of policy gradients in the context of unobservable rewards.
- What evidence would resolve it: Successful adaptation and convergence of an algorithm like PPO or SAC in a Mon-MDP environment with a continuous action dimension, showing performance comparable to the discrete case.

### Open Question 2
- Question: Can existing mitigation strategies for plasticity loss be effectively adapted to the dual-model architecture (reward model + Q-network) used in Mon-MDPs?
- Basis in paper: [explicit] Section 5 explicitly calls for "investigating the phenomenon of plasticity loss in Mon-MDPs... and exploring whether existing mitigation strategies... can be effectively adapted."
- Why unresolved: The authors observed performance deterioration after 6M timesteps, hypothesizing that the interaction between the learned reward model and the Q-network makes the agent susceptible to losing adaptability.
- What evidence would resolve it: Experiments demonstrating that techniques like plasticity injection or regenerative regularization prevent the observed performance drop in the binary and half-room environments during extended training.

### Open Question 3
- Question: Can computationally efficient methods like noisy networks replace large ensembles for capturing epistemic uncertainty without sacrificing robustness?
- Basis in paper: [explicit] The paper lists "adopting computationally efficient approaches for capturing epistemic uncertainty, e.g., noisy networks and epistemic neural networks" as a primary direction for future research.
- Why unresolved: The current robust policy optimization requires training an ensemble of 500 reward models to estimate uncertainty, which is computationally expensive.
- What evidence would resolve it: A comparative study showing that a single noisy network or epistemic neural network can achieve similar or better "cautious" behavior (e.g., avoiding watering novel plants) with a fraction of the training cost.

## Limitations

- Overgeneralization remains a fundamental challenge when monitored and unmonitored states have structurally different reward mappings, as evidenced by the agent watering cacti in unmonitored zones
- The approach requires substantial computational resources for ensemble-based uncertainty quantification (500 reward models, 83.3 GPU hours for botanical garden training)
- Plasticity loss emerges after 6M+ timesteps, with performance degradation potentially due to the interaction between the learned reward model and Q-network

## Confidence

- High confidence: Function approximation enables reward model generalization from monitored to unmonitored states in shared-feature environments
- Medium confidence: k-of-N CFR effectively reduces overgeneralization by 5× for novel plant types
- Medium confidence: Monitor-aware Q-learning learns optimal monitoring trade-offs

## Next Checks

1. Test ensemble prediction variance correlation with epistemic uncertainty on out-of-distribution states beyond the novel plant types
2. Vary ensemble size (N=50, 100, 500) to measure robustness-uncertainty trade-off curves
3. Evaluate performance when monitored and unmonitored states have fundamentally different reward structures to stress-test overgeneralization bounds