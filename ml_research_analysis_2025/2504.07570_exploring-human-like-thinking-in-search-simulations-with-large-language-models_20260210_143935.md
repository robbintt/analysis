---
ver: rpa2
title: Exploring Human-Like Thinking in Search Simulations with Large Language Models
arxiv_id: '2504.07570'
source_url: https://arxiv.org/abs/2504.07570
tags:
- user
- search
- behavior
- thinking
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of simulating realistic user
  search behavior by incorporating human-like thinking processes into large language
  models (LLMs). The authors propose a method that leverages LLMs to simulate users'
  hidden cognitive processes by prompting them to "think" like humans before executing
  search actions such as querying, browsing, and clicking.
---

# Exploring Human-Like Thinking in Search Simulations with Large Language Models

## Quick Facts
- arXiv ID: 2504.07570
- Source URL: https://arxiv.org/abs/2504.07570
- Reference count: 40
- Primary result: LLMs can simulate user search behavior by generating explicit thoughts before actions, with fine-tuning improving query alignment (BLEU up to 0.4766) and stopping behavior prediction (accuracy up to 0.6951)

## Executive Summary
This paper introduces a method for simulating realistic user search behavior by incorporating human-like thinking processes into large language models (LLMs). The approach leverages supervised fine-tuning on a novel dataset enriched with explicit user thoughts collected via think-aloud protocols. The framework prompts LLMs to "think" like humans before executing search actions, aiming to bridge the gap between observed actions and hidden cognitive processes. Experiments demonstrate that this cognitive modeling approach improves simulation fidelity, particularly for high-level decisions like query generation and stopping behavior, though performance gains on some metrics remain modest.

## Method Summary
The method involves collecting a user study dataset with explicit think-aloud data, then fine-tuning LLMs using supervised fine-tuning (SFT) with LoRA for 5 epochs on Llama3-8B-Chinese-Chat. The fine-tuning teaches models to generate reasoning traces before actions. The simulation framework iteratively predicts thoughts and actions, with explicit thinking for high-level decisions and simpler models for intuitive actions like clicking. Evaluation uses public datasets (KDD19, TianGong) and metrics including BLEU, BERTScore, MAUVE, FID for queries, and Accuracy/F1 for clicks and stopping.

## Key Results
- Fine-tuned models show improved query alignment with real user queries (BLEU scores up to 0.4766)
- Better stopping behavior prediction accuracy (up to 0.6951) with cognitive modeling
- Traditional probabilistic models still outperform LLMs on click prediction accuracy
- Performance gains on some metrics remain modest, indicating complexity of modeling human behavior

## Why This Works (Mechanism)

### Mechanism 1: Thought-Action Coupling via Supervised Fine-Tuning
Fine-tuning LLMs on a dataset that explicitly pairs user thoughts with subsequent actions enables the model to learn a conditional mapping from cognitive states to observable search behaviors. A supervised fine-tuning process uses a novel dataset of search logs enriched with think-aloud data, training the model to predict user reasoning and resulting action given a task and search history.

### Mechanism 2: Dual-Process Modeling for High-Level Decisions
Explicitly modeling a "thinking" process is more effective for high-level, deliberative user decisions like stopping a search than for low-level, intuitive actions like clicking. The architecture separates decisions based on cognitive load, using reasoning traces for complex decisions and simpler models for intuitive actions.

### Mechanism 3: Enhanced Query Alignment via Cognitive Grounding
Grounding query generation in an explicit thought process improves semantic alignment and similarity of simulated queries to real user queries. By forcing the model to articulate strategy or intent before generating a query, the process constrains the search space of possible queries, acting as prompt engineering to produce more contextually relevant queries.

## Foundational Learning

### Concept: Supervised Fine-Tuning (SFT)
- Why needed here: Primary method for teaching the LLM the specific relationship between search context, user's thought, and their action, moving beyond general-purpose capabilities.
- Quick check question: Given a prompt with Task, Search History, and desired Output Format, can you trace how a model is trained to produce target Reasoning and Output?

### Concept: Think-Aloud Protocol
- Why needed here: Data collection method that creates ground truth for the model's "thinking" module, serving as critical bridge between observable actions and hidden cognitive states.
- Quick check question: What are potential biases introduced by asking users to verbalize thoughts while performing a task?

### Concept: Evaluation Metrics for Generative Agents
- Why needed here: You cannot improve what you cannot measure. Understanding metrics like BLEU (for query similarity) and Accuracy/F1 (for stopping behavior) is essential to gauge simulation fidelity.
- Quick check question: Why might high BLEU score for generated queries not fully capture quality of user simulation?

## Architecture Onboarding

### Component map:
User Study Data -> Fine-Tuned LLM -> Prompting Module -> Action Simulator -> Evaluation Framework

### Critical path:
1. Data Collection: Conduct user study with think-aloud protocol to generate UserStudy dataset
2. Model Fine-Tuning: Train chosen base LLM on UserStudy data to learn thought-action sequence
3. Simulation: Run fine-tuned model on new tasks to generate simulated search session by iteratively predicting thoughts and actions
4. Evaluation: Measure fidelity by comparing generated actions to held-out real user sessions from KDD19 or TianGong datasets

### Design tradeoffs:
- Fidelity vs. Diversity: Models fine-tuned to closely match user queries may overfit and lose natural variability
- Resource Intensity vs. Realism: Explicit thought generation step doubles inference calls, increasing cost but potentially improving decision quality
- Cognitive Modeling Depth vs. Action Prediction Accuracy: Overhead of cognitive modeling may not yield accuracy gains for simple intuitive actions

### Failure signatures:
- Cascading Hallucination: Irrelevant or incorrect generated thought misleads subsequent action, derailing simulation
- Overfitting to Training Data: Simulation becomes repetitive and fails to generalize to new task types
- Think-Aloud Bias: Model mimics biases of think-aloud process rather than true cognitive process

### First 3 experiments:
1. Ablation Study: Compare performance of N-Llama vs. Llama-Llama on query generation to isolate cognitive module contribution
2. Cross-Dataset Evaluation: Evaluate model fine-tuned on UserStudy data on KDD19 and TianGong datasets to test generalizability
3. Manual Qualitative Analysis: Compare thoughts generated by GPT (zero-shot) vs. Llama (fine-tuned) vs. human thoughts to assess reasoning quality

## Open Questions the Paper Calls Out
1. How can simulation frameworks balance strict query-target alignment with distributional diversity characteristic of real user behavior?
2. Does explicit "thinking" hinder prediction of low-level intuitive behaviors like clicking compared to high-level decisions like stopping?
3. What specific model refinements are necessary to overcome modest performance gains observed in complex search scenarios?

## Limitations
- Think-aloud data fidelity remains untested as proxy for actual decision-making processes
- Cross-linguistic generalizability unknown due to Chinese-language models and data
- Computational overhead of explicit thinking step doubles inference calls without quantified cost-benefit tradeoff
- Click prediction limitations persist despite cognitive modeling approach

## Confidence
- High confidence: Supervised fine-tuning methodology and evaluation framework are sound
- Medium confidence: Dual-process modeling assumption lacks empirical validation
- Low confidence: Think-aloud data faithfully representing latent cognitive processes

## Next Checks
1. Conduct think-aloud validation study comparing verbalizations to eye-tracking, response time analysis, or post-task interviews
2. Replicate fine-tuning and evaluation pipeline with English-language model and dataset
3. Implement both thought-enabled and thought-disabled versions to measure inference time, computational cost, and performance differences across all metrics