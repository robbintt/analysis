---
ver: rpa2
title: 'RotBench: Evaluating Multimodal Large Language Models on Identifying Image
  Rotation'
arxiv_id: '2508.13968'
source_url: https://arxiv.org/abs/2508.13968
tags:
- image
- rotation
- images
- rotbench
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the ability of Multimodal Large Language
  Models (MLLMs) to identify the orientation of rotated images. The authors introduce
  RotBench, a benchmark consisting of 350 manually filtered images from the Spatial-MM
  dataset, divided into lifestyle, portrait, and landscape categories.
---

# RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation

## Quick Facts
- arXiv ID: 2508.13968
- Source URL: https://arxiv.org/abs/2508.13968
- Reference count: 40
- State-of-the-art MLLMs struggle to reliably identify image rotations, particularly distinguishing between 90° and 270° rotations

## Executive Summary
This paper investigates the spatial reasoning capabilities of Multimodal Large Language Models (MLLMs) through RotBench, a benchmark focused on identifying image rotation angles (0°, 90°, 180°, 270°). Despite the seemingly simple nature of the task, state-of-the-art models including GPT-5, o3, and Gemini-2.5-Pro show significant limitations. While models perform well on identifying upright (0°) and upside-down (180°) images, they consistently fail to distinguish between 90° and 270° rotations. The study reveals that auxiliary information like captions and depth maps, as well as chain-of-thought prompting, provide only minor improvements. Fine-tuning substantially improves performance on 180° images but fails to address the 90° vs 270° discrimination problem.

## Method Summary
The study introduces RotBench, a benchmark consisting of 350 manually filtered images from the Spatial-MM dataset, categorized into lifestyle, portrait, and landscape. The benchmark evaluates MLLMs' ability to identify four rotation angles: 0°, 90°, 180°, and 270°. The evaluation includes testing with raw images, images with auxiliary information (captions and depth maps), and chain-of-thought prompting. Additionally, the paper explores fine-tuning approaches using different objective functions to improve rotation identification performance. The methodology systematically tests various state-of-the-art models including GPT-4V, Gemini Pro, and Claude to assess their spatial reasoning capabilities.

## Key Results
- MLLMs consistently fail to distinguish between 90° and 270° rotations, despite performing well on 0° and 180° identifications
- Auxiliary information (captions, depth maps) and chain-of-thought prompting provide only minor and inconsistent improvements
- Fine-tuning substantially improves performance on 180° images but does not significantly improve 90° vs 270° discrimination
- The failure pattern is consistent across multiple state-of-the-art models including GPT-4V, Gemini Pro, and Claude

## Why This Works (Mechanism)
The paper demonstrates that MLLMs have fundamental limitations in processing rotational transformations, particularly for 90° and 270° rotations. The mechanism behind this failure appears to be related to how these models process spatial relationships and orientation cues. The consistent failure across different models and the limited improvement from auxiliary information suggest that the issue is not simply about missing visual cues but rather a structural limitation in how MLLMs represent and reason about rotational transformations.

## Foundational Learning
- Spatial-MM dataset: A collection of multimodal images used for spatial reasoning tasks; needed to provide diverse real-world images for rotation evaluation; quick check: verify dataset diversity across categories and rotation angles
- Auxiliary information processing: How MLLMs integrate additional visual or textual information; needed to understand whether supplemental data can compensate for rotation identification weaknesses; quick check: test models with various combinations of auxiliary inputs
- Chain-of-thought prompting: A prompting technique that encourages step-by-step reasoning; needed to determine if explicit reasoning helps with spatial tasks; quick check: compare performance with and without CoT across different rotation angles
- Fine-tuning methodology: Process of adapting pre-trained models to specific tasks; needed to assess whether additional training can overcome rotation identification limitations; quick check: measure performance improvement after fine-tuning on 180° images

## Architecture Onboarding

Component map: Image input -> Vision encoder -> Multimodal fusion -> Language model -> Output classification

Critical path: The vision encoder extracts visual features which are then fused with language representations before being processed by the language model to generate rotation angle predictions. The multimodal fusion layer is particularly critical for combining visual rotation cues with any auxiliary information.

Design tradeoffs: Models must balance between processing raw visual information directly versus relying on higher-level semantic understanding. The choice between end-to-end training versus frozen vision encoders affects how well rotation-specific features can be learned.

Failure signatures: The most prominent failure signature is the consistent inability to distinguish between 90° and 270° rotations, while maintaining high accuracy on 0° and 180° rotations. This suggests a systematic bias in how rotational transformations are processed.

First experiments:
1. Test model performance on synthetic images with clear geometric markers to isolate whether failure is due to lack of visual cues
2. Evaluate models with modified positional encodings that explicitly encode rotation information
3. Compare performance across different vision encoder architectures to identify if specific architectures are more rotation-robust

## Open Questions the Paper Calls Out
None

## Limitations
- The 90° vs 270° rotation distinction failure appears to be a fundamental limitation that persists across different models and intervention strategies
- The effectiveness of auxiliary information and chain-of-thought prompting is highly variable and model-dependent
- Fine-tuning shows promise for 180° images but fails to address the core 90° vs 270° discrimination problem
- The benchmark size (350 images) may limit the generalizability of findings to larger-scale applications

## Confidence

High: The consistent failure pattern across multiple state-of-the-art models to distinguish 90° from 270° rotations is well-demonstrated and reproducible.

Medium: The claim that auxiliary information and chain-of-thought prompting provide only minor improvements is supported by data but shows variability across models and image categories.

Low: The assertion that fine-tuning cannot significantly improve 90° vs 270° discrimination may be limited by the small dataset size (350 images) and requires further validation with larger training sets.

## Next Checks

1. **Expanded dataset validation**: Test the rotation identification task on a larger, more diverse dataset (1000+ images) with balanced representation across all rotation angles and image categories to confirm whether the 90° vs 270° failure pattern persists at scale.

2. **Controlled synthetic experiments**: Create synthetic images with clear geometric markers (text, grid patterns, or objects with directional features) to determine whether the failure is due to lack of visual cues or a fundamental limitation in rotation processing.

3. **Architectural ablation study**: Test whether models with explicit spatial reasoning modules (attention mechanisms with positional encoding modifications) or those trained on rotation-augmented data show improved performance on the 90° vs 270° distinction, helping isolate whether this is a data or architecture problem.