---
ver: rpa2
title: How much do LLMs learn from negative examples?
arxiv_id: '2503.14391'
source_url: https://arxiv.org/abs/2503.14391
tags:
- examples
- negative
- training
- answers
- incorrect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how large language models (LLMs) learn
  from negative examples during training. The authors introduce a likelihood-ratio
  (Likra) model that trains two separate heads - one on positive (correct) examples
  and one on negative (incorrect) examples - to quantify the impact of negative examples
  on model accuracy.
---

# How much do LLMs learn from negative examples?

## Quick Facts
- arXiv ID: 2503.14391
- Source URL: https://arxiv.org/abs/2503.14391
- Reference count: 7
- Primary result: Negative examples improve LLM accuracy 10× more than positive examples during critical training phase

## Executive Summary
This paper investigates how large language models learn from negative examples during training, introducing a likelihood-ratio (Likra) model that trains separate heads on positive and negative examples. The authors demonstrate that negative examples are far more effective than positive examples at improving model accuracy during a critical training phase, leading to sharp jumps in learning curves unlike the smooth improvement of supervised fine-tuning. Their experiments show that near-miss negative examples (plausible but incorrect answers) are significantly more effective than random incorrect answers, and models trained with negative examples better distinguish correct answers from plausible but incorrect ones.

## Method Summary
The authors introduce a likelihood-ratio (Likra) model that trains two separate LoRA adapters on a base model - one on positive (correct) examples and one on negative (incorrect) examples. Using multiple-choice benchmarks (ARC-Challenge and HellaSwag), they compare models trained with negative examples against standard supervised fine-tuning. Training uses 1 epoch, batch size 8, and Adam optimizer with learning rate 1e-4. Inference computes log-likelihood ratios (L+ - L-) to score answer candidates, achieving 80%+ accuracy compared to 60-66% for supervised fine-tuning alone.

## Key Results
- During a critical training phase, negative examples improve model accuracy 10× more than positive examples, creating sharp learning curve jumps unlike smooth SFT improvement
- Near-miss negative examples (plausible incorrect answers) are significantly more effective than random incorrect answers
- Models trained with negative examples achieve 80%+ accuracy versus 60-66% for supervised fine-tuning alone, better distinguishing correct from plausible incorrect answers

## Why This Works (Mechanism)
Unknown: The paper does not provide explicit theoretical justification for why negative examples are 10× more effective during the critical training phase. The mechanism behind the sharp learning curve jumps versus smooth SFT improvement is not explained.

## Foundational Learning
- Multiple-choice question answering: Task format where models select correct answer from options - needed to create controlled positive/negative example pairs for systematic study
- Likelihood-ratio scoring: Computing L+ - L- to compare answer candidates - needed to leverage separate positive/negative heads for final prediction
- LoRA adapters: Parameter-efficient fine-tuning method using low-rank matrix decomposition - needed to train separate heads without modifying base model parameters
- Near-miss negatives: Incorrect options that are plausible but wrong - needed to test hypothesis that realistic negative examples are more effective than random ones
- Critical training phase: Period where negative examples provide disproportionate learning benefit - needed to identify optimal timing for negative example incorporation

## Architecture Onboarding

Component map: Base model -> Positive LoRA head -> Negative LoRA head -> Log-likelihood computation -> Answer selection

Critical path: Training data preparation -> Separate LoRA training (positive/negative) -> Inference with likelihood ratio scoring -> Accuracy evaluation

Design tradeoffs: The paper chooses separate heads over single head with mixed data to enable explicit modeling of positive vs negative patterns, sacrificing parameter efficiency for interpretability and potentially better discrimination.

Failure signatures: Including test questions in training set, using random text instead of plausible incorrect options as negatives, training for multiple epochs beyond what helps

First experiments:
1. Train positive-only LoRA head on ARC-Challenge, evaluate accuracy baseline
2. Train negative-only LoRA head on same data, compare learning curves
3. Implement likelihood ratio scoring and verify it improves over individual heads

## Open Questions the Paper Calls Out
None

## Limitations
- Experiments limited to multiple-choice question answering tasks, not generalizable to open-ended generation or other domains
- Small base models (Mistral-7B, Llama-3.2-3B) used, unclear if findings scale to larger frontier models
- Very limited training (1 epoch, batch size 8) may not reflect production-scale training scenarios
- No exploration of computational efficiency or practical deployment considerations for maintaining two heads during inference

## Confidence
High: Negative examples improve accuracy 10× more than positive examples during critical training phase
Medium: Near-miss negatives are significantly more effective than random incorrect answers
Medium: Models trained with negatives better distinguish correct from plausible incorrect answers

## Next Checks
1. Replication across diverse task types: Test Likra approach on open-ended generation, summarization, and reasoning tasks beyond multiple-choice classification
2. Scale-up validation: Apply methodology to larger base models (Llama-3.1-70B, GPT-class) to verify 10× improvement ratio persists at scale
3. Long-term stability assessment: Conduct extended training with varying negative-to-positive ratios to evaluate whether sharp initial improvements translate to sustained performance gains