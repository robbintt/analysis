---
ver: rpa2
title: 'Co-DETECT: Collaborative Discovery of Edge Cases in Text Classification'
arxiv_id: '2507.05010'
source_url: https://arxiv.org/abs/2507.05010
tags:
- edge
- cases
- annotation
- case
- codebook
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Co-DETECT, a mixed-initiative annotation
  framework that integrates human expertise with LLM-guided annotation to discover
  and handle edge cases in text classification. The system starts with a codebook
  and dataset from a domain expert, uses LLMs to flag ambiguous examples, clusters
  them, and suggests high-level edge case descriptions with handling rules.
---

# Co-DETECT: Collaborative Discovery of Edge Cases in Text Classification

## Quick Facts
- **arXiv ID**: 2507.05010
- **Source URL**: https://arxiv.org/abs/2507.05010
- **Reference count**: 12
- **Primary result**: Co-DETECT improves classification F1 scores by up to 30 percentage points through iterative edge case discovery

## Executive Summary
Co-DETECT is a mixed-initiative annotation framework that integrates human expertise with LLM-guided annotation to discover and handle edge cases in text classification. The system starts with a codebook and dataset from a domain expert, uses LLMs to flag ambiguous examples, clusters them, and suggests high-level edge case descriptions with handling rules. Users iteratively refine the codebook based on these suggestions. A user study with 10 experts across diverse domains showed that Co-DETECT is easy to use, identifies relevant edge cases, and supports effective iterative refinement. Quantitative evaluation showed Co-DETECT improved classification F1 scores on two subjective datasets by up to 30 percentage points.

## Method Summary
Co-DETECT follows a three-step iterative process: (1) a non-reasoning LLM annotates the dataset and flags low-confidence examples as potential edge cases with item-level descriptions; (2) hierarchical clustering with a reasoning LLM (DeepSeek-R1) aggregates these into generalizable high-level rules; (3) domain experts review and refine these rules through a user interface, updating the codebook for subsequent iterations. The system requires an initial codebook and 500-1000 text samples, producing improved classification performance through collaborative refinement.

## Key Results
- 94.7% of suggested edge cases were deemed relevant by expert users
- 80% of participants found the iterative feature useful for refining annotation guidelines
- Classification F1 scores improved by up to 30 percentage points on GabHateCorpus and GoEmotions datasets
- 90% of participants found the user interface easy to use

## Why This Works (Mechanism)

### Mechanism 1: LLM Confidence-Guided Edge Case Detection
Co-DETECT uses an LLM's self-reported confidence to identify potential edge cases where the codebook is underspecified. A non-reasoning LLM annotates text and provides confidence scores; low confidence is treated as a proxy for ambiguous or underspecified regions in the codebook, triggering requests for explanations of the ambiguity. This assumes that LLM verbalized confidence correlates with annotation correctness and that low confidence indicates codebook gaps rather than model failures.

### Mechanism 2: Goal-Driven Clustering with Reasoning LLMs
Hierarchical clustering, guided by a reasoning LLM, aggregates item-level edge cases into generalizable, high-level rules. Item-level descriptions are embedded and clustered using constrained KMeans (10-20 samples per cluster). A reasoning LLM (DeepSeek-R1) synthesizes each cluster into a high-level edge case description and handling rule, with a final merging step to reduce redundancy.

### Mechanism 3: Expert-in-the-Loop Iterative Refinement
A user interface enables domain experts to validate, edit, and integrate suggested edge case rules back into the codebook, driving an iterative improvement cycle. Experts can approve, edit, or reject rules and append them to the codebook, creating a feedback loop where the augmented codebook is used in new rounds of annotation.

## Foundational Learning

- **Concept: Mixed-Initiative Interaction**
  - Why needed: This is the core paradigm of Co-DETECT, where AI proposes and humans decide
  - Quick check: Can you distinguish between a system that automates a task and one that collaborates on it?

- **Concept: Test-Time Compute / Reasoning LLMs**
  - Why needed: The paper differentiates between reasoning LLMs (e.g., DeepSeek-R1) for complex synthesis and non-reasoning LLMs (e.g., GPT-4.1) for faster initial annotation
  - Quick check: What is the difference in capability and cost profile between an LLM that answers immediately versus one that "thinks" first?

- **Concept: Constrained KMeans Clustering**
  - Why needed: This algorithm controls the size of edge case clusters fed to the reasoning LLM, balancing context size and reasoning load
  - Quick check: Why would you constrain the size of clusters in a dataset before feeding them into an LLM for summarization?

## Architecture Onboarding

- **Component map**: Input Page -> Initial Annotation (Non-reasoning LLM) -> Clustering & Abstraction Pipeline -> Analysis Dashboard -> Editing & Iteration Module -> Re-annotation
- **Critical path**: The pipeline flows from Input Page through Initial Annotation to Clustering, then to Dashboard Visualization, Expert Review/Editing, and back to Re-annotation
- **Design tradeoffs**: 
  - Cost vs. Detail: 500-1000 texts balance representativeness with API budget
  - Speed vs. Quality: Faster non-reasoning LLM for initial annotation, slower reasoning LLM for cluster synthesis
  - Automation vs. Control: Expert approval required to ensure validity and mitigate "Clever Hans" risk
- **Failure signatures**:
  - Miscalibrated Confidence: High-confidence wrong annotations or low-confidence correct ones
  - Hallucinated Rules: Reasoning LLM generates unsupported rules
  - Over-Specific Rules: Rules fail to generalize beyond cluster samples
  - Human-in-the-Loop Bottleneck: Experts reject valid rules or accept invalid ones due to fatigue
- **First 3 experiments**:
  1. Calibration Check: Compare model's verbalized confidence against actual annotation accuracy on held-out validation set
  2. A/B Test on Rule Quality: Have experts blindly evaluate rules from full pipeline vs. simpler baseline
  3. Downstream Performance: Train classifier with initial vs. Co-DETECT-refined codebook on separate test set

## Open Questions the Paper Calls Out

1. **Do inductively derived rules reinforce model-specific biases rather than capturing genuine domain regularities?**
   - Basis: Broader Impact Statement warns that rules may reinforce model-specific biases if the model identifies unintended cues
   - Why unresolved: User study focused on usability and accuracy, not qualitative audit of rule validity
   - Evidence: Counterfactual analysis of generated rules to determine if they correlate with stylistic artifacts rather than semantic content

2. **How can the system improve recall of edge cases to address user concerns about overlooking valid but isolated anomalies?**
   - Basis: 40% of participants expressed concern that Co-DETECT may overlook potential edge cases
   - Why unresolved: Constrained KMeans heuristic may filter out valid edge cases appearing as sparse outliers
   - Evidence: Ablation study varying minimum cluster size constraints or incorporating anomaly detection methods

3. **Is reliance on reasoning models necessary, or can standard LLMs achieve comparable performance?**
   - Basis: Authors use reasoning LLMs because the task is "challenging" and requires avoiding hallucination
   - Why unresolved: No ablation study comparing reasoning models versus standard instruction-tuned models
   - Evidence: Comparative evaluation of rule coherence when swapping reasoning model for standard model

## Limitations
- The core assumption that LLM verbalized confidence correlates with codebook ambiguity remains empirically untested
- Improvement claims are demonstrated on only two subjective datasets, not objective classification tasks
- The system's benefits may be domain-specific to nuanced, subjective text rather than clear-cut classification problems

## Confidence

**High Confidence**: System architecture and iterative workflow are well-specified and reproducible; user interface design and basic clustering methodology are clearly documented

**Medium Confidence**: Quantitative improvement claims (30 percentage point F1 gains) are supported by experiments but limited to two specific datasets; user study results are internally consistent but based on small sample

**Low Confidence**: Foundational assumption about LLM confidence correlation with codebook ambiguity is stated but not empirically validated; broader impact statement acknowledges this as an assumption rather than proven fact

## Next Checks

1. **Confidence Calibration Validation**: Run controlled experiment comparing LLM-annotated data against human gold labels to measure actual correlation between reported confidence and annotation accuracy

2. **Cross-Dataset Generalization Test**: Apply Co-DETECT to an objective classification task (e.g., spam detection) with clearer annotation boundaries to test if F1 improvement pattern holds

3. **Ablation Study on Clustering Method**: Compare full Co-DETECT pipeline against simpler baseline that identifies edge cases from low-confidence items without clustering or reasoning LLM synthesis