---
ver: rpa2
title: 'HMVLM: Multistage Reasoning-Enhanced Vision-Language Model for Long-Tailed
  Driving Scenarios'
arxiv_id: '2506.05883'
source_url: https://arxiv.org/abs/2506.05883
tags:
- driving
- hmvlm
- trajectory
- reasoning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces HMVLM, a vision-language model-based slow
  planner for autonomous driving in the Waymo Open Dataset challenge. HMVLM employs
  a multi-stage chain-of-thought reasoning approach to address complex, long-tailed
  driving scenarios.
---

# HMVLM: Multistage Reasoning-Enhanced Vision-Language Model for Long-Tailed Driving Scenarios

## Quick Facts
- arXiv ID: 2506.05883
- Source URL: https://arxiv.org/abs/2506.05883
- Authors: Daming Wang; Yuhao Song; Zijian He; Kangliang Chen; Xing Pan; Lu Deng; Weihao Gu
- Reference count: 14
- HMVLM achieves RFS 7.7367, placing second in the 2025 Waymo Vision-based End-to-End Driving Challenge

## Executive Summary
HMVLM is a vision-language model-based slow planner for autonomous driving that employs multi-stage chain-of-thought reasoning to handle complex, long-tailed driving scenarios. It uses selective five-view prompting with 4s of ego kinematics history, followed by structured reasoning through Scene Understanding, Driving Decision, and Trajectory Inference stages. A spline-based post-processing step smooths trajectories to reduce jitter. The model is fine-tuned on the Waymo dataset and demonstrates effectiveness in handling rare but safety-critical scenarios through interpretable, structured reasoning.

## Method Summary
HMVLM extends the Qwen2.5-VL-3B model with a multi-stage chain-of-thought prompting approach. The system takes five camera views (three front-facing plus left and right side views) and 4s of ego kinematics history as input. It employs structured reasoning through three stages: Scene Understanding (describing the environment), Driving Decision (determining high-level actions), and Trajectory Inference (generating 20 waypoints in bird's-eye view). A spline-based post-processing step applies Savitzky-Golay filtering with key-point preservation and outlier removal to ensure smooth, kinematically feasible trajectories. The model is fine-tuned on the Waymo Open Dataset and evaluated using the Rater Feedback Score metric in the Waymo Vision-based End-to-End Driving Challenge.

## Key Results
- Achieves Rater Feedback Score (RFS) of 7.7367, placing second in the 2025 Waymo Vision-based End-to-End Driving Challenge
- Outperforms baseline by 2.77% on RFS
- Demonstrates effective handling of long-tailed driving scenarios through interpretable, structured reasoning
- Shows particular strength in regular driving scenarios while facing challenges in highly dynamic situations (Spotlight: 6.7269, Cyclists: 7.3925)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selective five-view prompting with 4s ego kinematics reduces input bandwidth while improving motion-prediction fidelity.
- Mechanism: Three front-facing views plus left/right side views are selected (not all available cameras). Velocity and acceleration history is embedded directly in the prompt, compensating for limited visual temporal context without requiring video input.
- Core assumption: Not all camera views contribute equally to driving decisions; kinematic history can substitute for visual temporal reasoning in a single-frame VLM.
- Evidence anchors:
  - [abstract]: "selective five-view prompting with an embedded 4 s history of ego kinematics... reducing input bandwidth while improving motion-prediction fidelity"
  - [section 2.1]: "To balance computational efficiency and driving performance, we select three front-facing views along with left and right side views"
  - [corpus]: Corpus papers on VLM driving (ReasonDrive, RoboDriveVLM) do not specifically analyze view selection strategies; comparative evidence is limited.

### Mechanism 2
- Claim: Multi-stage Chain-of-Thought (CoT) prompting improves planning robustness and interpretability by enforcing sequential reasoning.
- Mechanism: Special tokens (`<DESC START/END>`, `<DECI START/END>`, `<TRAJ START/END>`) structurally decompose output into Scene Understanding → Driving Decision → Trajectory Inference. Each stage conditions on the previous, making intermediate reasoning explicit and debuggable.
- Core assumption: Decomposing the generation process into explicit stages reduces compounding errors and enables human inspection of reasoning traces.
- Evidence anchors:
  - [abstract]: "multi-stage chain-of-thought (CoT) prompting that enforces a Scene Understanding → Driving Decision → Trajectory Inference reasoning flow"
  - [section 2.2]: "Chain-of-Thought (CoT) reasoning significantly boosts the quality of model outputs... By explicitly disentangling visual understanding, intention, and motion generation, the model is better equipped to generalize"
  - [corpus]: ReasonDrive (arXiv:2504.10757) similarly investigates "whether explicitly modeling reasoning during fine-tuning enhances VLM performance on driving decision tasks"

### Mechanism 3
- Claim: Spline-based trajectory post-processing reduces collision events by removing kinematic discontinuities from VLM outputs.
- Mechanism: Adaptive Savitzky-Golay filtering with key-point preservation (directional changes >25°) plus z-score outlier removal. Endpoints are strictly maintained for route continuity.
- Core assumption: VLM trajectory outputs inherently contain irregularities (jitter, sharp kinks) that violate kinematic constraints; post-processing can safely correct these without altering strategic intent.
- Evidence anchors:
  - [abstract]: "spline-based trajectory post-processing that removes late-stage jitter and sharp turns"
  - [section 2.3]: "markedly reducing collision events... A smoother trajectory reduces unnecessary vehicle maneuvers and enhances safety"
  - [corpus]: Corpus papers do not emphasize post-processing as a primary contribution; this mechanism is underexplored in related work.

## Foundational Learning

- Concept: **Vision-Language Model (VLM) architecture**
  - Why needed here: HMVLM extends Qwen2.5-VL-3B, which combines a ViT vision encoder with an LLM via an MLP merger. Understanding modality fusion is essential for debugging perception-language misalignment.
  - Quick check question: Can you explain how image patches are tokenized and merged with text tokens in a typical VLM?

- Concept: **Chain-of-Thought (CoT) reasoning**
  - Why needed here: The entire planning pipeline relies on CoT decomposition. Without understanding why step-by-step reasoning improves LLM outputs, you cannot evaluate alternative prompting strategies.
  - Quick check question: What is the theoretical justification for why intermediate reasoning steps improve final output quality in LLMs?

- Concept: **Bird's-Eye View (BEV) trajectory representation**
  - Why needed here: The Trajectory Inference stage outputs waypoints in BEV coordinates. Interpreting and validating these outputs requires spatial reasoning.
  - Quick check question: Given a sequence of (x, y) waypoints in BEV, how would you detect kinematic infeasibility (e.g., excessive acceleration)?

## Architecture Onboarding

- Component map:
  - Multi-view images (5 cameras) → Vision encoder (ViT) → MLP merger → LLM backbone (Qwen2.5) → Structured output (DESC → DECI → TRAJ tokens) → Spline post-processing → BEV trajectory waypoints

- Critical path:
  1. Multi-view images encoded by ViT
  2. Kinematic history injected into text prompt
  3. LLM generates Scene Understanding description
  4. LLM conditions on description for Driving Decision
  5. LLM conditions on decision for Trajectory waypoints (20 points, BEV)
  6. Post-processing refines trajectory
  7. (External) Fast controller executes low-level commands

- Design tradeoffs:
  - **3B vs. larger VLMs**: 3B enables reasonable inference latency; larger models may improve reasoning but increase compute cost
  - **5 views vs. all views**: Balances coverage with token budget; side/rear exclusions may miss hazards
  - **CoT stages vs. direct output**: Trade-off between interpretability and latency
  - **Post-processing vs. model refinement**: Post-processing is a band-aid; better model training could reduce need for smoothing

- Failure signatures:
  - **Trajectory length mismatch**: VLM occasionally outputs ≠20 waypoints; requires trimming/padding
  - **Late-stage oscillations**: Sharp kinks in raw trajectory indicate VLM uncertainty
  - **Hallucination in Scene Understanding**: Non-existent obstacles described; may propagate to incorrect decisions
  - **Low RFS in Spotlight scenarios (6.7269)**: Suggests difficulty with adversarial/evaluator-focused situations

- First 3 experiments:
  1. **Baseline VLM comparison**: Evaluate Qwen2.5-VL-3B vs. other VLMs (as referenced in Section 2.1) on hallucination rate and reasoning quality using held-out Waymo segments
  2. **View ablation**: Systematically remove one view at a time to quantify contribution of each camera to RFS and ADE
  3. **CoT stage ablation**: Compare full 3-stage CoT vs. 2-stage (skip Scene Understanding) vs. direct trajectory prediction to measure interpretability-performance trade-off

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How will pairing HMVLM with a millisecond-latency fast controller affect overall system performance, safety, and latency in the full dual-system architecture?
- Basis in paper: [explicit] The conclusion states: "Future work will pair HMVLM with a millisecond-latency fast branch."
- Why unresolved: This submission evaluates only the slow planner in isolation; the fast branch integration and its interaction dynamics remain untested.
- What evidence would resolve it: A combined system evaluation measuring end-to-end latency, collision rates, and RFS across both branches operating together.

### Open Question 2
- Question: What is the optimal temporal window for ego kinematics history beyond the current 4-second horizon for handling extended long-tail scenarios?
- Basis in paper: [explicit] The conclusion lists "broaden temporal memory" as future work.
- Why unresolved: The paper uses a fixed 4s history but does not ablate this choice or explore longer contexts that may be needed for complex, multi-agent interactions.
- What evidence would resolve it: Ablation studies comparing 2s, 4s, 6s, and 8s kinematic histories on RFS and scenario-specific metrics.

### Open Question 3
- Question: How can HMVLM's relative weakness in highly dynamic scenarios (cyclists, spotlight events) be addressed through architectural or prompting improvements?
- Basis in paper: [inferred] The paper acknowledges the model "faces more substantial challenges in complex, dynamic situations such as Spotlight (6.7269) and interactions involving Cyclists (7.3925)."
- Why unresolved: No specific interventions are proposed; the authors only suggest "strategic enhancement in model prompting and targeted fine-tuning."
- What evidence would resolve it: Targeted experiments with cyclist/spotlight-specific prompts, augmented training data, or attention mechanisms, measuring improvement in these categories.

### Open Question 4
- Question: Can domain-adaptive self-supervision or model compression substantially reduce inference compute while preserving reasoning quality and RFS scores?
- Basis in paper: [explicit] The conclusion proposes incorporating "domain-adaptive self-supervision to curb compute cost."
- Why unresolved: The paper notes VLM inference is "computationally intensive" but provides no latency measurements or efficiency analysis.
- What evidence would resolve it: Benchmarks comparing inference time, FLOPs, and RFS before and after applying self-supervised pre-training or quantization techniques.

## Limitations

- Limited ablation on view selection strategy without systematic validation of marginal contributions
- Post-processing may mask fundamental limitations in the model's ability to generate kinematically feasible trajectories
- No comparative evaluation with video-capable VLMs to validate the optimality of selective view approach
- Lack of quantitative analysis showing individual contributions of each CoT stage

## Confidence

- **High Confidence**: HMVLM achieves RFS 7.7367 and second-place ranking in Waymo challenge (objective metrics)
- **Medium Confidence**: Selective five-view prompting with kinematic history improves performance (mechanism sound but limited ablation evidence)
- **Medium Confidence**: Multi-stage CoT reasoning improves interpretability and planning quality (theoretical justification but no quantitative ablation)
- **Low Confidence**: Spline-based post-processing is essential for collision reduction (assertion not empirically demonstrated)

## Next Checks

1. **View ablation study**: Systematically remove each camera view individually and measure the impact on RFS, collision rate, and specific scenario performance to quantify the marginal contribution of each view and validate the optimality of the five-view selection.

2. **Video vs. selective view comparison**: Implement a VLM variant that processes all available camera views as video input (using temporal modeling) and compare its performance against HMVLM's selective view approach to determine whether the kinematic history embedding truly compensates for limited visual temporal context.

3. **CoT stage contribution analysis**: Conduct ablation experiments removing each CoT stage (Scene Understanding, Driving Decision, or Trajectory Inference) individually and measure the impact on RFS, collision rate, and interpretability metrics to quantify the contribution of each reasoning stage and identify potential optimization opportunities.