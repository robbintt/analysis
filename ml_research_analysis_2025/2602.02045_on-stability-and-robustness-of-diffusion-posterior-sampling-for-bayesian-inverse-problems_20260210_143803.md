---
ver: rpa2
title: On Stability and Robustness of Diffusion Posterior Sampling for Bayesian Inverse
  Problems
arxiv_id: '2602.02045'
source_url: https://arxiv.org/abs/2602.02045
tags:
- posterior
- diffusion
- logp
- inverse
- likelihood
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes stability and robustness in diffusion-based
  solvers for Bayesian inverse problems (BIPs). Standard solvers rely on Tweedie-approximated
  likelihoods and are shown to be stable but non-robust to likelihood misspecification.
---

# On Stability and Robustness of Diffusion Posterior Sampling for Bayesian Inverse Problems

## Quick Facts
- arXiv ID: 2602.02045
- Source URL: https://arxiv.org/abs/2602.02045
- Reference count: 40
- Standard diffusion-based solvers are stable but non-robust to likelihood misspecification.

## Executive Summary
This paper analyzes stability and robustness in diffusion-based solvers for Bayesian inverse problems (BIPs). Standard solvers rely on Tweedie-approximated likelihoods and are shown to be stable but non-robust to likelihood misspecification. The authors prove posterior stability but identify vulnerability to measurement noise and outliers. To address this, they propose Robust Diffusion Posterior (RDP) sampling, a plug-and-play module using adaptively weighted losses for robust inference. RDP is theoretically guaranteed to be robust under mild conditions on the weight function. Experiments on scientific and natural image tasks show RDP improves performance under heavy-tailed noise and outliers while maintaining competitive results in well-specified settings. The method generalizes across different diffusion solvers (DPS, LGD, ΠGDM, PnPDM) and tasks, providing a principled solution for robust BIPs.

## Method Summary
The method addresses robustness in diffusion-based Bayesian inverse problem solvers by introducing a plug-and-play module that replaces standard likelihood gradients with adaptively weighted losses. The core approach uses inverse multi-quadratic (IMQ) weighting functions to down-weight the influence of large residuals during posterior sampling. The adaptive bandwidth parameter is set using quantiles of the residual distribution at each timestep. The method is compatible with multiple diffusion solvers including DPS, LGD, ΠGDM, and PnPDM, and uses Tweedie-approximated likelihoods with pretrained score networks from the InverseBench benchmark.

## Key Results
- Standard diffusion solvers are stable but vulnerable to outliers and heavy-tailed noise due to unbounded posterior influence functions.
- RDP provides theoretical robustness guarantees under mild conditions on the weight function.
- Experiments show RDP improves performance on inverse scattering and image reconstruction tasks under heavy-tailed noise and outlier contamination.
- RDP maintains competitive performance on well-specified Gaussian noise settings.

## Why This Works (Mechanism)

### Mechanism 1
Standard diffusion-based solvers (e.g., DPS) are stable but not robust to likelihood misspecification. The Tweedie-approximated likelihood yields a posterior mapping that is Lipschitz continuous, with total approximation error bounded by a quadratic term in the measurement noise. However, this error bound is radially unbounded, so large perturbations (e.g., outliers) can arbitrarily degrade the posterior. Core assumption: Gaussian likelihood model is well-specified; the measurement model F is twice differentiable; the score network is Lipschitz with bounded error. Evidence: Theorem 3.4 proves stability and shows error scales with ||y* - y||²; the discussion notes the bound is radially unbounded. Break condition: If the measurement noise deviates significantly from Gaussian (e.g., heavy-tailed or outliers), the error bound no longer constrains performance, and the posterior can diverge from the target.

### Mechanism 2
The non-robustness stems from the unbounded posterior influence function (PIF) under standard likelihood guidance. The PIF, defined as the KL divergence between posteriors under different measurements, is not uniformly bounded because the Gaussian likelihood score grows linearly with residuals. Thus, large corruptions (outliers or heavy-tailed noise) can exert unbounded influence on the posterior. Core assumption: The likelihood is Gaussian and misspecified; the weighting function in standard DPS does not limit residual influence. Evidence: Definition of PIF in Section 3.1; Theorem 3.4 implies unbounded PIF due to quadratic error term. Break condition: If the weighting function remains unbounded (e.g., standard Gaussian likelihood), the PIF remains unbounded, and robustness cannot be guaranteed.

### Mechanism 3
Robust Diffusion Posterior (RDP) sampling achieves bounded PIF and thus robustness via adaptively weighted losses. RDP replaces the standard likelihood with a generalized Bayes loss using component-wise adaptive weights. If the weight function satisfies boundedness conditions (|rw(r)| < ∞ and |r²w'(r)| < ∞), the guidance gradient is bounded, leading to a uniformly bounded PIF. Core assumption: Weight function w satisfies boundedness conditions; the measurement model F is Lipschitz; score network is regular. Evidence: Theorem 3.5 proves robustness under conditions on w; Remark 3.6 shows IMQ satisfies these. Break condition: If the chosen weight function does not satisfy the boundedness conditions (e.g., unbounded w), the PIF may remain unbounded, and robustness is not guaranteed.

## Foundational Learning

- **Bayesian Inverse Problems (BIPs)**: Why needed here: The paper formulates inverse problems as Bayesian inference with a prior and likelihood. Understanding BIPs is essential to grasp the goal: sampling from the posterior p(x|y). Quick check: Given noisy measurements y = F(x) + ε, what role does the prior p(x) play in regularizing the solution?

- **Diffusion Models and Score Functions**: Why needed here: Diffusion models provide the learned prior via a score network that estimates the prior score. The reverse SDE is guided by this score plus a likelihood term. Quick check: In the reverse SDE dx_t = -β(t)[x_t/2 + ∇_x_t log p_t(x_t|y)] dt + √β(t) dw_t, what does the term ∇_x_t log p_t(x_t|y) represent?

- **Tweedie's Formula and Likelihood Approximation**: Why needed here: The intractable likelihood p(y|x_t) is approximated via Tweedie's formula, yielding a point estimate. This approximation is central to standard DPS but introduces robustness issues. Quick check: How does Tweedie's formula relate the noisy sample x_t to an estimate of the clean data x_0? Why is this approximation critical for DPS?

## Architecture Onboarding

- **Component map**: Score Network -> Tweedie Approximator -> Residual Calculator -> Weight Function -> Robust Loss -> Reverse Sampler
- **Critical path**: 1. At each timestep t, denoise x_t to get x̂0|t via Tweedie and score network. 2. Compute residuals r_t and weights W = diag(w(r_t,1), ..., w(r_t,d_y)). 3. Form robust guidance ∇_x_t ℓ_y(x_t) = J_x̂0|t^T W ∇_x̂0|t log p(y|x̂0|t). 4. Update x_{t-1} using the reverse SDE with robust guided score.
- **Design tradeoffs**: Weight function choice: IMQ w(r) = (1 + r²/c²)^(-1/2) offers bounded gradients; parameter c controls the bias-robustness tradeoff (large c → low bias, low robustness). Adaptive vs. fixed c: Adaptive c_t = Quantile_q(|r_t|) adjusts to residual statistics but adds complexity; fixed c is simpler but may need tuning. Compatibility: RDP is modular and works with DPS, LGD, ΠGDM, PnPDM; integration requires modifying the likelihood gradient term.
- **Failure signatures**: Performance collapse under heavy-tailed noise: If w is not sufficiently down-weighting (e.g., c too large), outliers still dominate guidance. Bias in well-specified settings: If c is too small, clean measurements are overly down-weighted, leading to biased reconstructions. Numerical instability: If weight function gradients explode (e.g., improper w), the reverse process may diverge.
- **First 3 experiments**: 1. Validate stability under Gaussian noise: Compare standard DPS vs. RDP-DPS on FFHQ inpainting with Gaussian noise; expect competitive PSNR/SSIM. 2. Test robustness under heavy-tailed noise: Run inverse scattering with Student-t noise; measure NMAE and compare RDP variants against baselines. 3. Ablate weight functions: Test IMQ, Huber, and Mahalanobis weights on phase retrieval with outlier contamination; analyze the tradeoff between robustness and bias.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can Robust Diffusion Posterior (RDP) sampling effectively handle spatially varying or correlated noise structures using specific weighting functions? Basis: Section 5 states experiments primarily consider isotropic noise, whereas structured noise is common in practice and systematic evaluation is needed. Why unresolved: While Mahalanobis-based weighting might address this, theoretical and empirical analysis was restricted to isotropic cases. Evidence: Systematic evaluation of RDP on inverse problems with known spatial noise correlations, using the weighting functions proposed in Appendix E.2.

- **Open Question 2**: How can the theoretical robustness guarantees of RDP be extended to discrete measurement models, such as Poisson distributions found in high-photon count signals? Basis: Section 5 notes empirically the approach could be applied to discrete measurements, but extending theoretical analysis requires additional effort. Why unresolved: Current theoretical proofs rely on Gaussian likelihood assumptions and specific differentiability conditions that don't directly map to discrete probability models. Evidence: Derivation of robustness bounds for discrete likelihoods and empirical validation on tasks like low-dose CT or astronomical imaging.

- **Open Question 3**: Does the robustness of RDP hold when the forward measurement operator F is misspecified or only approximately known? Basis: Assumption 3.1 presumes the measurement model F is known and twice differentiable. The paper analyzes likelihood misspecification but doesn't address errors in the physics model F itself. Why unresolved: The adaptive weighting relies on residuals y - F(x̂0); if F is incorrect, residuals conflate noise with model error, potentially causing the weighting mechanism to suppress valid physics or fail to correct model bias. Evidence: Experiments applying RDP to inverse problems where the measurement operator is perturbed or learned from data, measuring the degradation of the Posterior Influence Function (PIF).

## Limitations
- Theoretical robustness guarantees depend on boundedness conditions that are not uniformly verified across all possible weight functions or residual distributions.
- The adaptive quantile-based choice of bandwidth parameter c_t introduces sensitivity to the quantile hyperparameter q, which is not fully explored.
- Error bounds for the Tweedie approximation are radially unbounded, implying that extreme outliers could still cause failure modes not captured in standard experiments.

## Confidence
- **Stability of Standard Diffusion Solvers (Theorem 3.4)**: High confidence - the proof structure is standard, and the quadratic error bound is explicitly derived.
- **Lack of Robustness in Standard Methods**: High confidence - the unbounded PIF argument is direct, and empirical results support the claim.
- **RDP Robustness Guarantee (Theorem 3.5)**: Medium confidence - proof conditions are clear, but practical verification of boundedness for adaptive IMQ weight across all timesteps is not shown.
- **Empirical Performance Claims**: Medium confidence - results on InverseBench and FFHQ are strong, but number of seeds and variance reporting are limited.

## Next Checks
1. **Quantify the sensitivity of RDP to the quantile parameter q**: Run a grid search over q ∈ {0.25, 0.5, 0.75, 0.9} on inverse scattering with outlier noise. Plot PSNR/NMAE vs. q to identify the sweet spot and test stability across a range of values.

2. **Verify boundedness of the adaptive IMQ PIF empirically**: For each timestep t in RDP sampling, record the maximum value of |rw(r)| and |r²w'(r)| across all data dimensions and samples. Plot these quantities vs. t and y to confirm they remain bounded, especially under heavy-tailed noise.

3. **Compare RDP to non-diffusion robust Bayesian methods**: Implement a robust variant of Stochastic Gradient Langevin Dynamics (SGLD) with IMQ weighting on inverse scattering. Compare NMAE and FID to RDP-DPS and standard SGLD to isolate the benefit of diffusion priors vs. robust weighting.