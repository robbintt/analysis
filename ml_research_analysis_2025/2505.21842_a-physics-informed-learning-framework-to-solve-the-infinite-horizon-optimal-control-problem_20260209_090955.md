---
ver: rpa2
title: A Physics-Informed Learning Framework to Solve the Infinite-Horizon Optimal
  Control Problem
arxiv_id: '2505.21842'
source_url: https://arxiv.org/abs/2505.21842
tags:
- u1d447
- u1d465
- u1d449
- u1d707
- uni2032
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the infinite-horizon optimal control problem
  for nonlinear systems by proposing a physics-informed neural network (PINN) framework
  that solves the steady-state Hamilton-Jacobi-Bellman (HJB) equation without requiring
  prior knowledge of stabilizing controllers or performing iterative policy evaluations.
  The core method involves applying PINNs to a finite-horizon variant of the HJB equation,
  which has a unique solution and uniformly approximates the infinite-horizon optimal
  value function as the horizon length increases.
---

# A Physics-Informed Learning Framework to Solve the Infinite-Horizon Optimal Control Problem

## Quick Facts
- arXiv ID: 2505.21842
- Source URL: https://arxiv.org/abs/2505.21842
- Reference count: 26
- Solves infinite-horizon optimal control using PINNs on finite-horizon HJB to avoid multiple-solution issues

## Executive Summary
This paper addresses the infinite-horizon optimal control problem for nonlinear systems by proposing a physics-informed neural network (PINN) framework that solves the steady-state Hamilton-Jacobi-Bellman (HJB) equation without requiring prior knowledge of stabilizing controllers or performing iterative policy evaluations. The core method involves applying PINNs to a finite-horizon variant of the HJB equation, which has a unique solution and uniformly approximates the infinite-horizon optimal value function as the horizon length increases. The approach provides an algorithm to verify if the chosen horizon is sufficiently large and a method to extend it with reduced computations when needed, while maintaining robustness to approximation errors.

## Method Summary
The method trains a neural network to approximate the value function of a finite-horizon optimal control problem, which converges to the infinite-horizon solution as the horizon increases. The network is trained using PINNs by minimizing a loss function composed of flow, boundary, and origin constraint residuals from the time-varying HJB equation. The quality of the learned approximation is evaluated by substituting it into the steady-state HJB equation and computing a residual. If the residual exceeds a threshold, the horizon is extended by solving a new PDE with the previous estimate as a terminal cost, avoiding retraining from scratch.

## Key Results
- Uniform convergence of the finite-horizon value function to the infinite-horizon optimal value function as the horizon length increases (Theorem 1)
- Ability to evaluate approximation quality through steady-state HJB residuals
- Effective horizon extension that prevents error accumulation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Directly solving the infinite-horizon optimal control problem by applying PINNs to the steady-state HJB is unreliable.
- Mechanism: The steady-state HJB equation is a nonlinear PDE that often admits multiple solutions. PINNs, lacking a unique solution target, can converge to a "spurious" solution unrelated to the optimal value function or stall during training.
- Core assumption: PINN convergence is sensitive to the uniqueness of the underlying PDE solution; the steady-state HJB has no inherent time-dependent term to regularize the solution path.
- Evidence anchors:
  - [abstract] "...the steady-state HJB equation generally yields multiple solutions; hence if PINNs are directly employed to it, they may end up approximating a solution that is different from the optimal value function."
  - [section 3] "...applying PINNs to directly solve (4) could cause the training procedure of PINNs to either stall or converge to a solution completely unrelated to the value function..."
  - [corpus] General corpus evidence on PINNs is sparse for this specific control failure mode, though convergence challenges are a known issue.
- Break condition: If the steady-state HJB for a specific system can be proven to have a unique solution.

### Mechanism 2
- Claim: Applying PINNs to a finite-horizon HJB variant guarantees convergence to a unique solution that uniformly approximates the infinite-horizon optimal value function.
- Mechanism: Introducing a time derivative and a terminal cost transforms the PDE into a form with a guaranteed unique solution. Theorem 1 proves that as the horizon length $T$ increases, the finite-horizon value function $V_T(\cdot, 0)$ converges uniformly to the infinite-horizon optimal $V^*$ on a compact set.
- Core assumption: The system dynamics are sufficiently regular (locally Lipschitz) and the terminal cost is positive semi-definite. The neural network is expressive enough to approximate the value function.
- Evidence anchors:
  - [abstract] "...applying PINNs to a finite-horizon variant of the steady-state HJB that has a unique solution, and which uniformly approximates the optimal value function as the horizon increases."
  - [section 4.3, Theorem 1] "Theorem 1. Let $\varphi$ be an arbitrary, positive semi-definite terminal cost... $V_T(\cdot, 0) \to V^*$ and $\mu_T(\cdot, 0) \to \mu^*$ uniformly on $\Omega$."
  - [corpus] No direct corpus evidence contradicts this, but related work on value function approximation exists.
- Break condition: If the system is not stabilizable or the cost functions do not meet required regularity conditions.

### Mechanism 3
- Claim: The quality of the learned finite-horizon approximation can be evaluated and improved using residuals from the steady-state HJB equation.
- Mechanism: The trained value function estimate $\hat{V}_T(\cdot, 0)$ is substituted into the steady-state HJB to compute a flow residual $\mathcal{E}_f$. A large residual signals that the horizon $T$ is too short. A method is provided to extend the horizon by solving a smaller PDE that uses the previously learned value function as a terminal cost, avoiding retraining from scratch.
- Core assumption: A low steady-state residual implies the finite-horizon solution is close to the true infinite-horizon optimum. Theorem 3 suggests approximation errors do not necessarily accumulate during horizon extension.
- Evidence anchors:
  - [abstract] "An algorithm to verify if the chosen horizon is large enough is also given, as well as a method to extend it..."
  - [section 5.1] Defines the evaluation residual $\mathcal{E}$ (Eq. 22) and describes the horizon extension procedure.
  - [corpus] Similar residual-based validation is not explicitly detailed in the provided corpus summaries.
- Break condition: If PINN training error is high, the residual check may be unreliable. Theorem 3's error bound depends on the approximation error being bounded.

## Foundational Learning

- **Concept: Hamilton-Jacobi-Bellman (HJB) Equation**
  - Why needed here: This PDE is the core of optimal control theory. The paper's entire method is built around solving its steady-state and time-varying forms with neural networks.
  - Quick check question: How does the optimal control policy $\mu^*$ relate to the gradient of the optimal value function $V^*$ in the HJB framework?

- **Concept: Physics-Informed Neural Networks (PINNs)**
  - Why needed here: This is the computational tool used. Understanding its loss function (based on PDE residuals) and training via backpropagation is essential to grasp the implementation.
  - Quick check question: In a PINN, what is the "physics-informed" component of the loss function minimizing?

- **Concept: Uniform Convergence**
  - Why needed here: The paper's main theoretical contribution is proving uniform convergence of the finite-horizon solution to the infinite-horizon one, which is a stronger and more useful guarantee than pointwise convergence.
  - Quick check question: Why is uniform convergence over a compact set $\Omega$ a more powerful result than pointwise convergence for guaranteeing controller performance?

## Architecture Onboarding

- **Component map:** Input (x, t) -> 3-layer MLP with tanh -> Output V_T(x, t; θ)

- **Critical path:**
  1. Initialization: Choose horizon T, terminal cost φ, sample collocation points in Ω × [0, T]
  2. Training: Train neural network by minimizing total MSE loss to obtain approximate finite-horizon value function V̂_T(·, 0)
  3. Validation: Compute steady-state HJB residual E using trained network
  4. Extension: If E > threshold, extend horizon to T' using V̂_T(·, 0) as terminal cost for new PDE

- **Design tradeoffs:**
  - Horizon Length (T): Small T reduces initial training time but may require multiple extensions; large T solves problem in one shot but with larger network
  - Network Capacity: Larger networks can approximate more complex value functions but are slower to train and may overfit (paper uses 3 layers, 100-500 nodes)
  - Collocation Points: More points improve accuracy but slow down each training epoch; sampling should cover Ω adequately

- **Failure signatures:**
  - High Training Loss: Network not expressive enough or learning rate poorly tuned; PDE may not be solved accurately
  - Large Steady-State Residual: Horizon T is too short for V_T(·, 0) to approximate V* well
  - Convergence to Spurious Solution: If horizon not "large enough" or problem is ill-conditioned, PINN may converge to poor local minimum

- **First 3 experiments:**
  1. Replicate Pendulum: Implement PINN framework for torsional pendulum example with T=1, compare learned policy and value function against paper's figures, compute steady-state HJB residual
  2. Horizon Study: For pendulum system, train PINNs with horizon lengths T ∈ {0.5, 1, 2, 4}, plot steady-state HJB residual E versus T to verify convergence trend
  3. Extension Workflow: Train PINN with T=0.5, then use horizon extension method to increase to T=2 in steps, compare total training time and final accuracy against single-shot training with T=2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed PINN-based framework for infinite-horizon optimal control be extended to solve differential games defined over nonlinear dynamics?
- Basis in paper: [explicit] The conclusion explicitly states "Future work includes extending this work to solve differential games defined over nonlinear dynamics."
- Why unresolved: Differential games involve multiple competing agents with coupled Hamilton-Jacobi-Isaacs equations rather than a single HJB equation, introducing additional mathematical complexity in proving uniqueness and convergence.
- What evidence would resolve it: A formal extension of the finite-horizon approximation framework to the HJI equations, with proofs of uniform convergence analogous to Theorem 1, and demonstrations on canonical differential game problems.

### Open Question 2
- Question: What theoretical guarantees can be established for the convergence of PINNs when applied to the finite-horizon HJB equation?
- Basis in paper: [explicit] The paper notes that "strict theoretical guarantees of convergence do not exist for the method of PINNs, they have been empirically shown to perform well when the solution to the underlying PDE is unique."
- Why unresolved: The convergence analysis of PINNs remains an active research area in deep learning theory, complicated by the non-convex nature of neural network training and the interaction between network architecture, sampling strategy, and PDE structure.
- What evidence would resolve it: Rigorous bounds relating network expressiveness, sample complexity, and approximation error for the specific class of parabolic PDEs that arise in optimal control.

### Open Question 3
- Question: How can one systematically distinguish between errors caused by insufficient horizon length versus inadequate neural network expressiveness when the steady-state HJB residual is non-zero?
- Basis in paper: [inferred] Remark 4 states that "a nonzero residual may not necessarily imply that T is not large enough, but it could mean that the underlying neural network architecture is not expressive enough."
- Why unresolved: Both sources of error manifest as violations of the steady-state HJB equation, and their effects compound in ways that are difficult to decouple without additional structural analysis.
- What evidence would resolve it: A diagnostic procedure or theoretical criterion that can provably attribute residual magnitude to one error source versus the other, validated across systems with known ground-truth solutions.

## Limitations

- The practical reliability of steady-state HJB residual as verification metric is not thoroughly quantified across diverse systems
- Performance on high-dimensional systems and those with complex dynamics remains unverified beyond three demonstrated examples
- Method's robustness to ill-conditioned dynamics or cost functions requires further empirical investigation

## Confidence

**High Confidence:** The convergence proof (Theorem 1) and the fundamental mechanism of using finite-horizon HJB to avoid the multiple-solution problem of steady-state HJB. The mathematical framework is rigorously established.

**Medium Confidence:** The effectiveness of the steady-state HJB residual as a practical verification tool and the computational efficiency of the horizon extension method. While theoretically justified, empirical validation across diverse systems is limited.

**Low Confidence:** The scalability of the approach to high-dimensional systems and the robustness of the method to ill-conditioned dynamics or cost functions. These aspects require further empirical investigation.

## Next Checks

1. **Residual Reliability Test:** Systematically evaluate the steady-state HJB residual's sensitivity to training noise by adding controlled perturbations to the learned value function and measuring the resulting residual changes across all three example systems.

2. **High-Dimensional Scalability:** Apply the framework to a 4-6 dimensional nonlinear system (e.g., a quadrotor or vehicle platoon) to assess computational scaling and identify potential bottlenecks in training or evaluation.

3. **Ill-Conditioned System Test:** Design a test case with singular or near-singular dynamics (e.g., very stiff ODEs) to evaluate the method's robustness and determine if the steady-state residual remains a reliable indicator of solution quality under challenging conditions.