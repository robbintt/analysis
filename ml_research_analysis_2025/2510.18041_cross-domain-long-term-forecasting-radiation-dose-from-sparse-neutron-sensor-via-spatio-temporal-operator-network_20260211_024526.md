---
ver: rpa2
title: 'Cross-Domain Long-Term Forecasting: Radiation Dose from Sparse Neutron Sensor
  via Spatio-Temporal Operator Network'
arxiv_id: '2510.18041'
source_url: https://arxiv.org/abs/2510.18041
tags:
- forecasting
- operator
- radiation
- stone
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: STONe introduces a non-autoregressive neural operator framework
  for long-term forecasting of physical fields from sparse, cross-domain sensor data.
  By learning a direct mapping between sensor time-series and target fields, STONe
  bypasses domain alignment and iterative error accumulation issues that limit existing
  models.
---

# Cross-Domain Long-Term Forecasting: Radiation Dose from Sparse Neutron Sensor via Spatio-Temporal Operator Network

## Quick Facts
- arXiv ID: 2510.18041
- Source URL: https://arxiv.org/abs/2510.18041
- Reference count: 40
- Primary result: GRU-based STONe achieves accurate 180-day radiation dose forecasts from sparse neutron sensors with millisecond inference

## Executive Summary
STONe introduces a non-autoregressive neural operator framework for long-term forecasting of physical fields from sparse, cross-domain sensor data. By learning a direct mapping between sensor time-series and target fields, STONe bypasses domain alignment and iterative error accumulation issues that limit existing models. Applied to radiation dose forecasting from ground-based neutron measurements, STONe achieves accurate 180-day predictions with millisecond inference latency. The GRU-based architecture outperforms alternatives, maintaining stable performance across the entire forecast horizon. This approach enables real-time prediction of complex spatiotemporal fields in physics, climate, and energy systems, establishing a universal framework for cross-domain operator inference.

## Method Summary
STONe learns a direct mapping from neutron monitor sensor time-series to global radiation dose fields using a branch-trunk operator architecture. The branch network (GRU, LSTM, Transformer, or FCN) encodes 180-day sensor histories into latent coefficients, while the trunk network maps spatiotemporal coordinates to basis functions. The inner product of these components reconstructs the radiation field without requiring spatial alignment between input sensors and output grid. Trained on NMDB neutron data and EXPACS dose fields (2001-2023), the model uses MSE loss with Adam optimization, ReduceLROnPlateau scheduling, and early stopping over 500 epochs.

## Key Results
- GRU-based STONe achieves lowest MAPE (<10%) at 180-day lead time in high-latitude critical areas
- Non-autoregressive prediction prevents error accumulation compared to autoregressive alternatives
- Inference latency remains in millisecond range despite long-horizon forecasting
- Model maintains stable performance across entire 180-day forecast horizon

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Non-autoregressive sequence-to-sequence prediction reduces long-horizon error accumulation compared to autoregressive approaches.
- Mechanism: The model predicts the full 180-step output trajectory in a single forward pass, eliminating recursive feedback where prediction errors at step t become input noise at step t+1.
- Core assumption: The target dynamics can be sufficiently captured by a functional mapping from input history directly to the full output sequence, rather than requiring explicit stepwise state propagation.
- Evidence anchors:
  - [abstract] "By directly inferring... STONe demonstrates that operator learning can generalize beyond shared-domain settings... remains stable over long forecasting horizons without iterative recurrence."
  - [section 1] "Autoregressive models... are highly susceptible to error accumulation over long horizons, where small approximation errors compound."
  - [corpus] Related work on turbulence forecasting (arXiv:2509.21196) similarly identifies autoregressive failure modes for long-term prediction.
- Break condition: If the underlying dynamics exhibit strong non-Markovian dependencies requiring explicit intermediate states, or if output horizons exceed the model's effective receptive field, single-pass prediction may underfit temporal structure.

### Mechanism 2
- Claim: The branch-trunk operator decomposition enables learning across physically disjoint manifolds without requiring spatial alignment.
- Mechanism: The branch network encodes sensor time-series into a latent coefficient vector b(u_hist) ∈ R^q. The trunk network maps spatiotemporal coordinates to basis functions T(r) ∈ R^(q×p×K). The inner product reconstructs the field without any shared spatial grid between input and output.
- Core assumption: The cross-domain relationship can be factorized into temporally-varying coefficients and spatially-varying basis functions that multiply linearly.
- Evidence anchors:
  - [section 2.3] "The branch network learns to compute the optimal temporal coefficients, while the trunk network learns the optimal spatiotemporal basis functions directly from data."
  - [section 2.2] "This architecture learns separate representations for the input function and the output evaluation coordinates... independent from the training data's resolution and domain."
  - [corpus] arXiv:2506.12045 ("From Proxies to Fields") uses similar sparse-to-dense reconstruction for radiation fields.
- Break condition: If the input-output relationship requires non-factorizable or highly non-linear spatial coupling, the inner product formulation may lack expressivity. Memory scaling becomes problematic at very long horizons due to trunk output dimension linear in K.

### Mechanism 3
- Claim: GRU-based temporal encoding outperforms alternatives for long-horizon stability in this domain.
- Mechanism: Gating mechanisms in GRUs selectively retain relevant historical information across the 180-day context window, better capturing mean-reverting and cyclical patterns in radiation dynamics.
- Core assumption: The task benefits from explicit memory states with learned forget/update dynamics, more so than from attention over flattened history (FCN) or attention over tokenized sequences (Transformer).
- Evidence anchors:
  - [section 3.2] "The GRU architecture consistently demonstrates superior performance, achieving the lowest average error across all metrics."
  - [section 3.1] "At the 180-day lead time, [GRU] is the only architecture to maintain a MAPE below 10% in [high-latitude] critical areas."
  - [corpus] Limited direct corpus evidence comparing GRU to Transformer for scientific operator learning; this appears underexplored.
- Break condition: GRU performance may degrade if temporal dependencies exceed effective memory capacity, or if parallelization requirements favor attention-based architectures at larger scale.

## Foundational Learning

- **Neural Operators (DeepONet)**:
  - Why needed here: STONe extends DeepONet's branch-trunk decomposition to spatiotemporal forecasting. Understanding operator learning as function-space mapping is prerequisite.
  - Quick check question: Can you explain why DeepONet can query predictions at arbitrary coordinates independent of training grid resolution?

- **Recurrent Sequence Modeling (GRU/LSTM gating)**:
  - Why needed here: The branch network uses GRU to encode 180-step sensor histories. Understanding how gates control information flow is essential for debugging temporal encoding failures.
  - Quick check question: What is the difference between how a GRU and a simple RNN handle long-range dependencies?

- **Autoregressive vs. Non-Autoregressive Decoding**:
  - Why needed here: The paper's central claim is that non-autoregressive prediction avoids error accumulation. Understanding this tradeoff is critical for architectural decisions.
  - Quick check question: In an autoregressive model, how does prediction error at step 50 affect step 100?

## Architecture Onboarding

- **Component map**: Neutron sensor time-series (12 sensors) → Branch GRU → Latent coefficients (128-dim) → Inner product with → Trunk MLP outputs → Global radiation dose fields (1°×1° resolution)

- **Critical path**:
  1. Input normalization for sensor time-series
  2. Branch forward pass: sequence → latent coefficients
  3. Trunk forward pass: coordinates → basis functions
  4. Tensor contraction: combine branch/trunk outputs
  5. MSE loss against ground-truth dose fields

- **Design tradeoffs**:
  - GRU vs. Transformer: GRU achieves lower error and faster convergence (150 vs. 350 epochs), but Transformer may scale better with larger datasets.
  - Latent dimension q=128: Controls expressivity vs. overfitting; not tuned in paper.
  - Non-autoregressive output dimension: Trunk output scales linearly with forecast horizon K; memory bottleneck at K >> 180.

- **Failure signatures**:
  - Oversmoothing in polar regions: FCN and LSTM show blurred reconstructions at Day 180 (MAPE > 10%).
  - Error accumulation in specific temporal bands: Heatmap shows elevated L2 error at certain lead times across all models.
  - Out-of-distribution generalization: No evaluation on extreme space weather events; extrapolation to Carrington-level storms is unknown.

- **First 3 experiments**:
  1. **Ablation on temporal encoder**: Replicate FCN vs. GRU vs. LSTM vs. Transformer comparison on held-out test split to verify GRU advantage and characterize error growth curves.
  2. **Horizon sweep**: Train at K=30, 60, 90, 180 and evaluate trunk memory scaling; identify practical horizon limits before GPU memory saturation.
  3. **Spatial resolution sensitivity**: Vary output grid resolution (1°, 0.5°, 0.25°) to test trunk network's resolution-agnostic claim and measure MAPE degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can STONe be extended to provide probabilistic forecasts with uncertainty quantification for safety-critical aviation applications?
- Basis in paper: [explicit] The authors state that "quantifying predictive uncertainty is paramount to risk assessment" and note the "current STONe implementation is deterministic."
- Why unresolved: The current architecture produces point estimates without confidence intervals, which limits utility in safety-critical contexts where decision-making relies on risk bounds.
- What evidence would resolve it: Integration of methods like deep ensembling or diffusion models that demonstrate calibrated confidence intervals alongside dose predictions.

### Open Question 2
- Question: Can the framework generalize effectively to extreme, out-of-distribution space weather events, such as Carrington-level solar storms, that are absent from the historical record?
- Basis in paper: [explicit] The authors list "generalization to rare, extreme space weather events... not seen in the historical data" as a critical open question for operational deployment.
- Why unresolved: Neural networks typically interpolate well but fail on inputs far outside the training distribution, and the 23-year dataset likely lacks sufficient extreme examples.
- What evidence would resolve it: Testing the model against physics-based simulations of extreme events or augmenting training data with synthetic rare scenarios to verify stability.

### Open Question 3
- Question: How can the trunk network architecture be redesigned to overcome the linear memory scaling bottleneck for extremely long forecast horizons or finer resolutions?
- Basis in paper: [explicit] The paper notes the "model faces a scalability limitation regarding memory consumption" because the trunk network’s output dimension scales linearly with the number of forecast steps.
- Why unresolved: This design constraint limits the feasibility of forecasting over horizons much longer than 180 days or at higher spatial densities without exceeding hardware memory.
- What evidence would resolve it: Development of continuous-time evolution operators or implicit neural representations that maintain constant memory usage regardless of temporal resolution.

### Open Question 4
- Question: Do the learned latent representations in the branch and trunk networks correspond to interpretable physical modes or governing dynamics of the radiation field?
- Basis in paper: [explicit] The authors suggest that "analyzing the activation patterns in the GRU’s hidden state" or "decomposing the trunk network’s basis functions" could yield physical insights.
- Why unresolved: While the model acts as an accurate surrogate, it currently functions as a black box, and the correspondence between latent features and physical variables remains unexplored.
- What evidence would resolve it: Analysis correlating specific latent dimensions with known physical drivers (e.g., solar cycles) or visualizing basis functions to identify dominant spatial modes.

## Limitations
- Unknown selection criteria for 12 neutron monitor stations from NMDB
- Performance at extreme space weather events remains untested
- GRU-based architecture may face scalability challenges for longer horizons
- Trunk memory scaling limits horizon extension beyond 180 days

## Confidence
- **High Confidence**: Stable 180-day predictions with millisecond inference, GRU outperforms other temporal encoders, non-autoregressive formulation prevents error accumulation
- **Medium Confidence**: Branch-trunk decomposition generalizes beyond domain alignment, model maintains accuracy across forecast horizon, latency claims hold across hardware
- **Low Confidence**: Extrapolation to extreme space weather events, performance with different sensor counts, behavior under distribution shift

## Next Checks
1. **Extreme Event Robustness Test**: Evaluate STONe on simulated Carrington-level solar storm scenarios to assess out-of-distribution generalization limits
2. **Sensor Network Sensitivity**: Vary the number and spatial distribution of neutron monitors (3, 6, 12, 24 sensors) to characterize scaling laws and identify minimum viable sensor density
3. **Cross-Resolution Transfer**: Train at 1° resolution and evaluate at 0.5° and 0.25° resolutions to validate the trunk network's resolution-agnostic claims quantitatively