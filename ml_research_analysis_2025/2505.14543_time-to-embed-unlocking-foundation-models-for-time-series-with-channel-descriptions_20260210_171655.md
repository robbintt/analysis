---
ver: rpa2
title: 'Time to Embed: Unlocking Foundation Models for Time Series with Channel Descriptions'
arxiv_id: '2505.14543'
source_url: https://arxiv.org/abs/2505.14543
tags:
- time
- series
- dataset
- learning
- channel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CHARM is a foundation embedding model for multivariate time series
  that learns shared, transferable representations by incorporating textual channel
  descriptions directly into its architecture. The model uses a description-aware
  temporal convolutional network and custom attention layers with inter-channel gating
  and time-offset mechanisms to capture complex dependencies while remaining invariant
  to channel order.
---

# Time to Embed: Unlocking Foundation Models for Time Series with Channel Descriptions

## Quick Facts
- arXiv ID: 2505.14543
- Source URL: https://arxiv.org/abs/2505.14543
- Reference count: 40
- Primary result: State-of-the-art foundation embedding model for multivariate time series using textual channel descriptions

## Executive Summary
CHARM introduces a novel foundation embedding model for multivariate time series that incorporates textual channel descriptions directly into its architecture. By using a description-aware temporal convolutional network with custom attention layers featuring inter-channel gating and time-offset mechanisms, CHARM learns shared, transferable representations that capture complex dependencies while remaining invariant to channel order. The model is trained via a Joint Embedding Predictive Architecture (JEPA) with novel augmentations and loss functions, avoiding the noise sensitivity associated with reconstruction-based objectives.

The approach achieves state-of-the-art performance across classification, forecasting, and anomaly detection tasks, setting new benchmarks for representation learning in time series. Notably, the interpretable gating patterns reveal causal channel relationships, demonstrating the model's ability to learn meaningful dependencies from both temporal and textual information.

## Method Summary
CHARM is a foundation embedding model that learns shared, transferable representations for multivariate time series by incorporating textual channel descriptions into its architecture. The model uses a description-aware temporal convolutional network with custom attention layers featuring inter-channel gating and time-offset mechanisms to capture complex dependencies while maintaining invariance to channel order. Trained via a Joint Embedding Predictive Architecture (JEPA) with novel augmentations and loss functions, CHARM avoids reconstruction-based objectives and their associated noise sensitivity. The approach demonstrates state-of-the-art performance across multiple downstream tasks including classification, forecasting, and anomaly detection.

## Key Results
- Achieves state-of-the-art performance on multivariate time series embedding tasks
- Sets new benchmarks for representation learning in classification, forecasting, and anomaly detection
- Demonstrates interpretable gating patterns revealing causal channel relationships

## Why This Works (Mechanism)
CHARM works by integrating textual channel descriptions into the embedding process through a description-aware temporal convolutional network. The key mechanism involves custom attention layers with inter-channel gating that learns which channels influence others based on their semantic descriptions, while time-offset mechanisms capture temporal dependencies. The JEPA training framework enables contrastive learning without reconstruction, reducing sensitivity to noise while learning robust representations. The model's invariance to channel order is achieved through permutation-equivariant operations that treat channel relationships as description-dependent rather than position-dependent.

## Foundational Learning
- **Joint Embedding Predictive Architecture (JEPA)**: Why needed - avoids reconstruction-based noise sensitivity; Quick check - contrastive loss performance vs reconstruction
- **Temporal Convolutional Networks**: Why needed - captures local temporal patterns efficiently; Quick check - receptive field size vs sequence length
- **Attention with Inter-channel Gating**: Why needed - models cross-channel dependencies based on descriptions; Quick check - gating sparsity and interpretability
- **Description Embeddings**: Why needed - provides semantic context for channel relationships; Quick check - embedding quality via downstream tasks
- **Time-offset Mechanisms**: Why needed - captures delayed temporal dependencies; Quick check - offset sensitivity analysis
- **Permutation Equivariance**: Why needed - ensures channel order invariance; Quick check - performance consistency under channel shuffling

## Architecture Onboarding

**Component Map**: Text Descriptions -> Embedding Layer -> TCN -> Attention with Gating -> JEPA Head -> Contrastive Loss

**Critical Path**: Channel descriptions are embedded and combined with temporal features from TCN, then processed through attention layers with inter-channel gating to produce representations for contrastive learning via JEPA.

**Design Tradeoffs**: The model trades computational complexity (multiple attention heads and gating mechanisms) for improved representation quality and interpretability. Reconstruction-free training via JEPA reduces noise sensitivity but requires careful negative sampling strategies.

**Failure Signatures**: Poor performance on datasets with low-quality or missing channel descriptions; degraded accuracy when channel relationships are primarily spatial rather than semantic; overfitting to description-channel correlations in synthetic datasets.

**First Experiments**:
1. Ablation study removing channel descriptions to assess contribution to performance
2. Channel shuffling test to verify order invariance claims
3. Noise injection in descriptions to evaluate robustness to description quality variations

## Open Questions the Paper Calls Out
None

## Limitations
- Potential overfitting to curated datasets with rich, well-aligned channel descriptions
- Uncertainty about performance on real-world scenarios with sparse or noisy metadata
- Limited validation of order invariance across all experimental settings

## Confidence

| Claim | Confidence |
|-------|------------|
| Technical validity of architectural innovations | High |
| Empirical performance claims | High |
| Causal interpretation of gating patterns | Medium |
| Generalization to real-world description quality | Medium |

## Next Checks

1. Evaluate CHARM on datasets with artificially degraded or missing channel descriptions to assess robustness to description quality variations
2. Conduct systematic ablation studies comparing JEPA-based training against reconstruction-based approaches under controlled noise conditions
3. Perform cross-domain transfer experiments where models trained on richly-described datasets are evaluated on datasets with minimal metadata to test true foundation model capabilities