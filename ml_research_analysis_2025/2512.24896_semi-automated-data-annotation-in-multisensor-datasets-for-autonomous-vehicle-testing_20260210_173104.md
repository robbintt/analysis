---
ver: rpa2
title: Semi-Automated Data Annotation in Multisensor Datasets for Autonomous Vehicle
  Testing
arxiv_id: '2512.24896'
source_url: https://arxiv.org/abs/2512.24896
tags:
- annotation
- data
- darts
- annotations
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This report presents the design and evaluation of a semi-automated
  data annotation pipeline for multimodal autonomous driving datasets developed within
  the DARTS project. The pipeline combines AI-based object detection with human-in-the-loop
  verification to address the challenge of manual annotation being costly and time-consuming.
---

# Semi-Automated Data Annotation in Multisensor Datasets for Autonomous Vehicle Testing

## Quick Facts
- arXiv ID: 2512.24896
- Source URL: https://arxiv.org/abs/2512.24896
- Reference count: 17
- Primary result: CAR=0.93 on Zenseact dataset, indicating 93% reduction in annotation effort vs. manual labeling

## Executive Summary
This paper presents a semi-automated data annotation pipeline for multimodal autonomous driving datasets that combines AI-based object detection with human-in-the-loop verification. The pipeline addresses the high cost of manual annotation through pre-annotations generated by 3D object detectors, with human annotators correcting rather than creating bounding boxes from scratch. A novel Correction Acceleration Ratio (CAR) metric quantifies annotation efficiency gains, achieving up to 93% time savings on Zenseact data. The system integrates preprocessing, automated annotation generation, anonymization, multi-object tracking, and external annotation platform support to enable scalable, high-quality annotation of Polish driving scenarios for the DARTS project.

## Method Summary
The pipeline uses pre-trained 3D object detectors (DSVT, PointPillars, CenterPoint) to generate initial bounding box annotations on point cloud data from multimodal AV sensors. DSVT was fine-tuned on Zenseact data to adapt to similar LiDAR sensor configurations before inference. Annotations are processed through a multi-object tracking module using Kalman filtering and polynomial trajectory fitting to ensure temporal consistency. Human annotators on Segments.ai correct pre-annotations rather than creating from scratch, with correction times categorized by error type. The system includes anonymization for privacy compliance and is orchestrated via Apache Airflow with PostgreSQL database storage.

## Key Results
- DSVT achieved CAR=0.93 on Zenseact dataset, indicating 93% annotation effort reduction
- Model fine-tuning on sensor-similar datasets (Zenseact) improved CAR from 0.87 to 0.93 vs. nuScenes baseline
- False negatives required ~23s to fix while positional corrections took 5-16s, highlighting error type impact on efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-annotations reduce manual effort proportionally to detection quality, measured by CAR
- Mechanism: Annotators correct existing boxes rather than create from scratch; false negatives cost ~23s while most corrections take 5-16s
- Core assumption: Correction times are consistent across datasets and annotators
- Evidence: CAR=0.93 on Zenseact; false negative correction ~23s in study
- Break condition: If annotators are significantly faster at manual creation or error distribution differs substantially

### Mechanism 2
- Claim: Domain adaptation via fine-tuning improves pre-annotation quality for target conditions
- Mechanism: DSVT fine-tuned on Zenseact due to similar LiDAR setup reduces domain gap for DARTS Polish data
- Core assumption: Zenseact sensor configuration is sufficiently similar to DARTS
- Evidence: CAR=0.93 on Zenseact vs. 0.87 on nuScenes; Zenseact chosen for similar LiDAR setup
- Break condition: If DARTS LiDAR differs significantly from Zenseact in resolution, mounting, or point density

### Mechanism 3
- Claim: Multi-object tracking reduces annotation effort by enforcing temporal consistency
- Mechanism: MOT uses Kalman filtering and polynomial fitting to provide persistent instance IDs across frames
- Core assumption: Objects move smoothly and maintain consistent dimensions over short sequences
- Evidence: MOT module incorporates size consistency correction and trajectory approximation
- Break condition: Highly dynamic scenes with occlusions or rapid appearance/disappearance may fragment tracks

## Foundational Learning

- Concept: **3D Object Detection from Point Clouds** (voxel-based, anchor-free, transformer architectures)
  - Why needed: Understanding detector error modes informs correction patterns and CAR calculations
  - Quick check: Can you explain why voxel-based detectors might miss small or distant objects compared to point-based methods?

- Concept: **Kalman Filtering for Object Tracking**
  - Why needed: MOT module relies on Kalman filter outputs for position smoothing between frames
  - Quick check: What assumptions does a constant-velocity Kalman filter make about object motion, and when would these fail in traffic scenarios?

- Concept: **Human-in-the-Loop ML Systems**
  - Why needed: Pipeline assumes iterative model retraining on corrected annotations to improve future pre-annotations
  - Quick check: What data quality risks emerge when annotators over-trust model suggestions (automation bias)?

## Architecture Onboarding

- Component map: Apache Airflow -> PostgreSQL database -> Docker microservices (Annotation Generator, Anonymization, MOT Tracker) -> Segments.ai interface -> PostgreSQL import

- Critical path: Raw data upload → preprocessing DAG → Annotation Generator produces 3D boxes → MOT adds tracking IDs → Anonymization detects faces/plates → upload to Segments.ai → human correction → manager approval → import back to PostgreSQL

- Design tradeoffs: Segments.ai dependency simplifies UI but creates external platform risk; Docker-based microservices provide modularity vs. orchestration complexity; CAR metric measures practical annotation time vs. pure detection accuracy

- Failure signatures: Preprocessing validation errors → check timestamp synchronization; low CAR despite high AP → likely high false negative rate; track ID switches in MOT → review Kalman parameters or detection thresholds

- First 3 experiments: 1) Measure CAR on held-out DARTS scenes to validate 0.93 claim; 2) Compare DSVT performance with vs. without Zenseact fine-tuning; 3) Perform MOT ablation by annotating scenes with and without tracking

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does annotation efficiency evolve across successive iterations of model retraining on corrected annotations?
- Basis: Pipeline design describes iterative retraining, but evaluation reports only single-pass performance
- Why unresolved: No experiments compare initial pre-annotation quality against models retrained on human-corrected data
- Evidence needed: Longitudinal CAR and AP measurements across 3-5 retraining iterations on DARTS dataset

### Open Question 2
- Question: Does CAR correlate with actual annotator time savings in large-scale real-world deployment?
- Basis: Correction times were estimated via manual study, not end-to-end timing comparisons
- Why unresolved: Real-world annotation involves context switching and fatigue not captured in controlled measurements
- Evidence needed: A/B comparison of total annotation time between pre-annotated and from-scratch workflows on full DARTS dataset

### Open Question 3
- Question: How robust is the pipeline's efficiency when applied to domain-shifted Polish driving scenarios?
- Basis: Polish conditions differ from Western European datasets, but evaluation used public datasets rather than Polish-specific scenarios
- Why unresolved: Evaluation performed on nuScenes, Zenseact, Waymo, KITTI rather than novel Polish scenarios
- Evidence needed: CAR and AP evaluation on held-out Polish driving sequences with unique infrastructure, signage, or weather patterns

### Open Question 4
- Question: How does annotation acceleration differ across object classes and distance ranges beyond Car/Vehicle?
- Basis: Correction time measurements covered Car, Pedestrian, and Cyclist, but results reported only for Car/Vehicle
- Why unresolved: Pedestrians and cyclists may have different error distributions affecting optimal model selection
- Evidence needed: Class-stratified and distance-stratified CAR analysis for Pedestrian and Cyclist categories

## Limitations

- CAR metric assumes correction times measured on nuScenes generalize to other datasets
- DSVT performance gains assume sensor similarity between Zenseact and DARTS, not independently validated
- MOT module effectiveness relies on smooth motion assumptions that may not hold in complex traffic
- No ablation studies quantify individual module contributions to overall annotation efficiency

## Confidence

- **High**: CAR methodology and general finding that pre-annotations reduce manual effort
- **Medium**: Specific CAR values (0.93 on Zenseact) due to potential domain transfer issues
- **Medium**: Relative model rankings (DSVT vs. PointPillars vs. CenterPoint) as this should generalize
- **Low**: Absolute annotation time savings without replication on target DARTS data

## Next Checks

1. Measure CAR on held-out DARTS scenes using DSVT inference to validate the 0.93 efficiency claim on actual deployment data
2. Compare DSVT performance with and without Zenseact fine-tuning on DARTS-style sequences to quantify domain adaptation benefits
3. Perform MOT ablation by annotating identical scenes with and without tracking to measure the tracking module's contribution to annotation efficiency