---
ver: rpa2
title: Multilingual Definition Modeling
arxiv_id: '2506.01489'
source_url: https://arxiv.org/abs/2506.01489
tags:
- language
- data
- definition
- llama
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first multilingual study on definition
  modeling, introducing high-quality dictionary datasets for Spanish, French, Portuguese,
  and German (460K terms, 730K definitions). It evaluates both fine-tuned multilingual
  language models and zero/few-shot large language models on monolingual definition
  generation tasks.
---

# Multilingual Definition Modeling

## Quick Facts
- arXiv ID: 2506.01489
- Source URL: https://arxiv.org/abs/2506.01489
- Reference count: 35
- Primary result: First multilingual definition modeling study with high-quality datasets for Spanish, French, Portuguese, and German

## Executive Summary
This paper introduces the first comprehensive study of multilingual definition modeling, creating high-quality dictionary datasets for Spanish, French, Portuguese, and German (460K terms, 730K definitions). The research evaluates both fine-tuned multilingual language models and zero/few-shot large language models on monolingual definition generation tasks. Results demonstrate that LLMs generally outperform fine-tuned models but struggle with language compliance and cross-lingual synergy. The study also reveals that BERTScore performance on definition modeling correlates strongly with multilingual LLM benchmarks, suggesting this task as a compute-efficient alternative for assessing language proficiency.

## Method Summary
The study employs two primary approaches: fine-tuning mT5-large with prefix-based prompts for each language, and zero/few-shot prompting with LLAMA-2-13b-chat, LLAMA-3.1-8B-Instruct, and Mistral-7B-Instruct-v0.1. Training uses AdamW with learning rate 5e-5, batch size 8, and up to 20 epochs. Evaluation metrics include BLEU, BERTScore, COMET, language compliance via FastText, and human Likert ratings. Datasets consist of monosemic terms from DRAE (Spanish), DICIO (Portuguese), LAROUSSE (French), DUDEN (German), and OXFORD (English) dictionaries, filtered to 80/10/10 train/valid/test splits.

## Key Results
- LLMs (Llama 3, Mistral) outperform fine-tuned mT5 models on multilingual definition generation
- BERTScore strongly correlates with multilingual LLM benchmark performance, validating definition modeling as a proficiency proxy
- Multilingual fine-tuning fails to leverage cross-lingual synergies despite training on related language families
- Language compliance remains a significant challenge, with models often generating English definitions for target language prompts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large Language Models (LLMs) leverage pre-trained parametric knowledge to outperform fine-tuned encoders
- Mechanism: LLMs access broad semantic associations learned during pre-training to define terms zero-shot, often yielding higher semantic similarity than training smaller models on limited dictionary data
- Core assumption: The model has encountered the target term or its semantic neighborhood during pre-training
- Evidence anchors: Abstract shows LLMs generally offering better performance; Section 5.4 questions why fine-tuned LLMs don't outperform base LLMs with ICL

### Mechanism 2
- Claim: Definition modeling acts as a stable proxy for general multilingual proficiency
- Mechanism: The ability to generate correct definitions correlates strongly with performance on complex multilingual benchmarks like MMLU and HellaSwag
- Core assumption: BERTScore captures semantic adequacy better than surface-level overlap metrics
- Evidence anchors: Abstract states BERTScore performance correlates with multilingual LLM benchmarks; Section 5.3 positions definition modeling as an efficient alternative

### Mechanism 3
- Claim: Multilingual fine-tuning fails to induce cross-lingual synergy for this task
- Mechanism: Training on combined data from related languages does not improve per-language performance, suggesting models treat definition generation as language-specific
- Core assumption: Lack of improvement is intrinsic to task architecture rather than optimization failure
- Evidence anchors: Abstract notes inability to leverage cross-lingual synergies; Section 5.1 shows exposing models to multilingual data didn't help generalization

## Foundational Learning

### Concept: Definition Modeling vs. Machine Translation
- Why needed here: To understand that the goal is generating a description in the target language based on a concept, not translating from English
- Quick check question: If I input "chat" (French for "cat"), should the model output "a small domesticated feline" or "a feline animal"?

### Concept: Monosemy vs. Polysemy
- Why needed here: The paper explicitly restricts experiments to monosemic words to remove the need for context-based disambiguation
- Quick check question: Why does the paper exclude the word "bank" (river vs. financial) from its primary evaluation dataset?

### Concept: Language Compliance
- Why needed here: A major failure mode is the model generating correct definitions but in the wrong language (usually English)
- Quick check question: If a model defines a Spanish word correctly but writes the definition in English, does it pass the "Compliance" metric?

## Architecture Onboarding

### Component map:
Lemma + Language Prefix -> mT5/LLM Backbone -> BERTScore/BLEU/COMET/FastText Output

### Critical path:
Data collection -> Lemmatization (spacy) -> Monosemic filtering -> Zero-shot Prompting or Fine-tuning -> Language Compliance Check

### Design tradeoffs:
- mT5 vs. LLMs: mT5 is computationally cheaper but requires fine-tuning and lags in semantic quality; LLMs offer better zero-shot definitions but struggle with language compliance and require more inference compute
- Metric Selection: BLEU is unreliable due to varying surface forms; BERTScore is preferred but implies higher compute cost for evaluation

### Failure signatures:
- Language Drift: Model defaults to English definitions when prompted for other languages (Low Compliance)
- Low Specificity: Model generates tautologies (e.g., defining "cat" as "a cat-like animal") rather than descriptive definitions

### First 3 experiments:
1. Establish Baseline: Run Llama-3-8B-Instruct on Spanish (DRAE) test set using "Define the Spanish word '{term}'. Use only Spanish to reply." Measure BERTScore and Compliance
2. Compliance Check: Implement FastText language identification on Experiment 1 outputs to quantify how often the model ignores the language constraint
3. Metric Correlation: Calculate BERTScore on generated definitions and compare per-language ranking against known multilingual benchmark scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do LLMs fine-tuned on dictionary data without context sentences underperform compared to base LLMs using in-context learning (ICL)?
- Basis in paper: [explicit] Section 5.4 states "it is still unclear why LLMs finetuned on dictionary data without context sentences do not perform better than base LLMs with ICL"
- Why unresolved: The paper observed fine-tuning often decreased performance relative to few-shot prompting, suggesting models may improve at "Task Recognition" without improving "Task Learning"
- What evidence would resolve it: Ablation studies distinguishing between style acquisition and knowledge retrieval, or analysis of pre-training data composition regarding lexical richness

### Open Question 2
- Question: How can the definition modeling task be effectively extended to tackle the disambiguation of polysemic words?
- Basis in paper: [explicit] The paper notes in Conclusion and Limitations that they "have not tackled polysemy" and "would also like to tackle the disambiguation problem by collecting usage examples"
- Why unresolved: The current study explicitly restricted experiments to monosemic words to isolate multilingual capabilities
- What evidence would resolve it: A study incorporating usage examples into training and inference pipeline for polysemic terms

### Open Question 3
- Question: Why do current multilingual models fail to leverage cross-lingual synergies even when trained on linguistically similar language families?
- Basis in paper: [explicit] The Abstract and Results section note that models "cannot leverage potential cross-lingual synergies" despite training on related languages
- Why unresolved: Authors hypothesized related languages would share term-definition pairs that aid generalization, but empirical results showed no significant benefit
- What evidence would resolve it: Analysis determining if lack of synergy is due to monosemic restriction or architectural limitations

## Limitations

- Data Representation Bias: Restricting evaluation to monosemic words limits real-world applicability and may artificially inflate performance metrics
- Cross-Lingual Synergy Failure: Finding that multilingual fine-tuning fails to leverage cross-lingual benefits remains unexplained with multiple alternative explanations possible
- Language Compliance Ambiguity: Paper identifies compliance as a major failure mode but provides limited diagnostic analysis of root causes

## Confidence

- High Confidence: Definition modeling can serve as a proxy for multilingual proficiency; LLMs generally outperform fine-tuned models; mT5 fine-tuning results are inferior to LLM performance
- Medium Confidence: Cross-lingual synergy is not achievable through multilingual fine-tuning; Definition modeling is more compute-efficient than translation-based benchmarks
- Low Confidence: LLMs can "easily learn to mimic the style" of dictionary definitions without actually improving recall ability

## Next Checks

1. **Polysemy Impact Assessment**: Re-run experiments on a subset of polysemic words with context sentences to quantify performance degradation. Compare BERTScore drop against the monosemic baseline to measure the true capability gap.

2. **Cross-Lingual Synergy Mechanism Investigation**: Design an experiment where models are trained on polysemic words with context sentences across related languages (e.g., Spanish/French/Portuguese). Measure whether semantic transfer improves when models must disambiguate meanings rather than generate from single definitions.

3. **Language Compliance Root Cause Analysis**: Implement a controlled experiment varying: (a) prompt language (target language vs. English), (b) target language representation in pre-training data, and (c) evaluation method (FastText vs. human annotation). Correlate compliance rates with these factors to identify primary failure modes.