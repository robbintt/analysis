---
ver: rpa2
title: Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks
arxiv_id: '2505.14212'
source_url: https://arxiv.org/abs/2505.14212
tags:
- shot
- dataset
- training
- answer
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach for enhancing large language
  models in knowledge-intensive question answering tasks through automated generation
  of context-based QA pairs. The method leverages LLMs to create synthetic training
  data, reducing reliance on human labelling while improving model comprehension and
  reasoning capabilities.
---

# Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks

## Quick Facts
- arXiv ID: 2505.14212
- Source URL: https://arxiv.org/abs/2505.14214
- Reference count: 40
- Primary result: LLM-generated QA pairs outperform human-annotated pairs in knowledge retention for QA tasks when fine-tuning 7-8B parameter models.

## Executive Summary
This paper introduces an automated pipeline for generating synthetic QA pairs from technical documents to fine-tune large language models for knowledge-intensive question answering tasks. The method addresses the high cost of human annotation by leveraging LLMs to create context-grounded question-answer pairs, which are then cleaned via automated filtering before being used for fine-tuning. Comprehensive experiments show that models trained on synthetic data achieve superior performance in knowledge retention (no-context setting) compared to those trained on human-annotated data, while human data remains better for in-context extraction. The approach significantly reduces annotation costs while maintaining or improving QA performance.

## Method Summary
The method consists of a two-stage pipeline: first, an LLM (Mistral-7b-instruct-v0.3) generates up to 10 question-answer pairs per document using carefully engineered prompts, with the option to mark questions as unanswerable. Second, the generated pairs undergo automated post-processing using BERT and RoBERTa models to filter out unanswerable, unfinished, or unrelated QA pairs. The cleaned synthetic dataset is then used to fine-tune target models (Llama-3-8b and Mistral-7b-v0.3) using QLoRA with 4-bit quantization. The entire pipeline is evaluated on the TechQA dataset using perplexity, ROUGE, BLEU, and BERTScore metrics, comparing performance with and without context.

## Key Results
- Mistral-7b-v0.3 outperforms Llama-3-8b with BERT F1 of 0.858, BLEU of 0.172, and ROUGE of 0.260 for LLM-generated QA pairs.
- Synthetic data shows superior performance in no-context settings compared to human-annotated data for knowledge retention.
- Human-annotated data remains superior for in-context extraction tasks when documents are provided during inference.

## Why This Works (Mechanism)

### Mechanism 1: Context-Grounded QA Generation as a Supervision Signal
The system uses a two-stage prompt engineering process where a generator LLM creates questions from source documents, then generates answers based only on those documents. This forces the creation of QA pairs intrinsically linked to the provided context, teaching the target LLM the relationship between questions, answers, and their supporting evidence within a specific domain. The core assumption is that the generator LLM can create factual, relevant, and logically coherent QA pairs that are transferable to the target model.

### Mechanism 2: Automatic Post-Processing for Data Quality Control
After generation, the system applies three automated checks: (1) Unanswered QA Check using BERT semantic similarity to remove unanswerable questions, (2) Unfinished QA Check using a RoBERTa model to label incomplete answers, and (3) Unrelated QA Pair Check using BERT similarity to remove irrelevant pairs. This acts as automatic quality assurance, with the core assumption that the auxiliary models are accurate enough at their tasks to effectively filter errors without discarding too much valid data.

### Mechanism 3: In-Context Learning and Knowledge Retention Trade-offs
The value of the synthetic dataset depends on the inference context. Human-annotated data optimizes models for in-context extraction (finding answers when the correct document is already provided), while synthetic data helps models better internalize information from documents during training, enabling them to answer questions when the document is not provided at inference time (knowledge retention). This trade-off suggests synthetic data is more effective for building internalized knowledge than for improving context-dependent reasoning.

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**: Why needed: The paper frames its contribution as addressing a core limitation of RAG—the difficulty of handling complex reasoning over multiple documents—by exploring supervised fine-tuning as an alternative. Quick check: What are the two main modules in a standard RAG system, and what does each do?
- **Supervised Fine-Tuning**: Why needed: This is the core training method used in the paper. Understanding fine-tuning, its benefits (domain adaptation), and its costs (need for labeled data) is essential to grasp the paper's motivation for creating synthetic data. Quick check: Why is supervised fine-tuning often considered costly for domain adaptation?
- **Parameter-Efficient Fine-Tuning (PEFT) & QLoRA**: Why needed: The authors use QLoRA for all their experiments to manage computational resources. Knowing what QLoRA is (4-bit quantization + LoRA adapters) is crucial for understanding the experimental setup and its limitations. Quick check: How does QLoRA reduce the memory requirements for fine-tuning a Large Language Model?

## Architecture Onboarding

- **Component map**: The system has two main stages. Data Generation Pipeline: [Source Documents] → [Question Generator LLM] → [Answer Generator LLM] → [Post-Processing Filters (BERT/RoBERTa)] → [Cleaned Synthetic QA Dataset]. Model Training Pipeline: [Synthetic QA Dataset] → [QLoRA Fine-Tuner] → [Fine-Tuned LLM] → [Evaluation].
- **Critical path**: The quality of the final model hinges on the Post-Processing Filters. If these filters fail to remove low-quality, unfinished, or unrelated QA pairs generated by the LLM, the fine-tuning process will be corrupted by noisy data, negating the benefits of the entire approach.
- **Design tradeoffs**: Human vs. Synthetic Data: Human data is superior for training a model to extract answers from a provided document. Synthetic data is shown to be better for training a model to retain knowledge to answer without a document. Cost vs. Quality: The system drastically reduces human annotation cost but requires significant compute for generation and filtering. Volume vs. Noise: Generating thousands of QA pairs is easy, but cleaning them is the critical challenge.
- **Failure signatures**: A key failure mode is Training Data Contamination. If the post-processing filters are too weak, the fine-tuned model may learn to hallucinate or give incomplete answers, mirroring the flaws in the synthetic data. Another failure is Overfitting to Generator Style, where the final model learns the quirks of the generator LLM rather than the underlying domain knowledge.
- **First 3 experiments**:
  1. **Ablation Study on Filters**: Train models using the synthetic dataset without the post-processing filters, with only one filter, and with all filters. Compare performance to quantify the impact of each cleaning step.
  2. **Human vs. Synthetic on Downstream Task**: Beyond the metrics in the paper (BLEU, ROUGE), evaluate the fine-tuned models on a more complex reasoning task to see if the "knowledge retention" from synthetic data translates to improved logical reasoning.
  3. **Cross-Domain Generalization**: Test the generation pipeline on a different domain dataset (e.g., legal contracts, scientific papers) to see if the filter thresholds and generation prompts need retuning, assessing the system's adaptability.

## Open Questions the Paper Calls Out

- **Question 1**: Can the automated QA generation and fine-tuning pipeline be effectively applied to non-technical, general-knowledge domains or other specialized fields beyond technical support documentation? Basis: The authors state it is not known how well the QA dataset generation and fine-tuning apply to other forms of data and documentation as the method was only tested on the domain-specific TechQA dataset.
- **Question 2**: Does the synthetic dataset fine-tuning approach scale effectively to larger parameter models (>8B) or closed-source architectures without resulting in performance degradation or overfitting? Basis: The authors note that larger models with >8B parameters and commercial closed-source models have not been tested.
- **Question 3**: What is the optimal ratio of human-annotated to synthetically generated QA pairs to maximize reasoning capabilities while minimizing labeling costs and potential bias? Basis: The discussion suggests that further studies on the balance of datasets curated by humans and synthetically generated datasets should be conducted.

## Limitations

- The reported performance improvements depend heavily on the quality of the synthetic QA generation and post-processing pipeline, which is not fully specified in the paper.
- The findings are based on a single technical domain (IBM TechQA) and two 7-8B parameter models, so performance on other domains, larger models, or different QA datasets remains unknown.
- While the paper reports strong results in the no-context setting, it does not explore whether synthetic data provides benefits for more complex reasoning or multi-hop QA tasks.

## Confidence

- **High confidence**: The general approach of using LLM-generated synthetic QA pairs for fine-tuning is methodologically sound and well-supported by the experimental results within the TechQA domain.
- **Medium confidence**: The specific superiority of Mistral-7b-v0.3 over Llama-3-8b for LLM-generated QA pairs, and the relative trade-off between human and synthetic data for in-context versus no-context performance, are credible but may be sensitive to implementation details not fully disclosed.
- **Low confidence**: Generalizability to other domains, model scales, or more complex QA tasks is speculative; no evidence is provided beyond the single TechQA experiment.

## Next Checks

1. **Filter Ablation Study**: Systematically remove or vary each post-processing filter (Unanswered, Unfinished, Unrelated checks) and measure the impact on final QA performance to quantify the contribution of each cleaning step.
2. **Cross-Domain Transfer**: Apply the entire pipeline (generation, filtering, fine-tuning) to a different domain (e.g., legal or biomedical documents) to test robustness and identify necessary adjustments in model checkpoints or thresholds.
3. **Complex Reasoning Evaluation**: Evaluate fine-tuned models on multi-hop or complex reasoning QA benchmarks (e.g., HotpotQA, QASC) to determine if the "knowledge retention" from synthetic data translates to improved logical reasoning beyond TechQA.