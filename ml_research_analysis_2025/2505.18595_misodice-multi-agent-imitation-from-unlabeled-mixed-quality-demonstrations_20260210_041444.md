---
ver: rpa2
title: 'MisoDICE: Multi-Agent Imitation from Unlabeled Mixed-Quality Demonstrations'
arxiv_id: '2505.18595'
source_url: https://arxiv.org/abs/2505.18595
tags:
- learning
- misodice
- multi-agent
- expert
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of offline multi-agent imitation
  learning from unlabeled demonstrations containing both expert and suboptimal trajectories.
  The authors propose a two-stage framework: (1) a progressive labeling pipeline using
  large language models and preference-based reinforcement learning to identify expert-quality
  trajectories, and (2) a novel multi-agent IL algorithm, MisoDICE, that leverages
  these labels to learn robust policies.'
---

# MisoDICE: Multi-Agent Imitation from Unlabeled Mixed-Quality Demonstrations

## Quick Facts
- **arXiv ID**: 2505.18595
- **Source URL**: https://arxiv.org/abs/2505.18595
- **Authors**: The Viet Bui; Tien Mai; Hong Thanh Nguyen
- **Reference count**: 40
- **Primary result**: Proposes MisoDICE, a multi-agent imitation learning algorithm that learns from unlabeled mixed-quality demonstrations using a two-stage framework combining LLM-based progressive labeling and value-decomposed policy optimization

## Executive Summary
This paper addresses the challenge of offline multi-agent imitation learning from unlabeled demonstrations containing both expert and suboptimal trajectories. The authors propose a two-stage framework: first, a progressive labeling pipeline using large language models and preference-based reinforcement learning to identify expert-quality trajectories; second, a novel multi-agent IL algorithm, MisoDICE, that leverages these labels to learn robust policies. MisoDICE extends the DICE framework with value decomposition and a mixing architecture, ensuring convex policy optimization and consistency between global and local policies. Extensive experiments on SMACv1 and SMACv2 benchmarks demonstrate that MisoDICE outperforms all baselines, especially when expert data is scarce, highlighting the effectiveness of its integrated approach.

## Method Summary
The proposed approach consists of two main stages. First, a progressive labeling pipeline uses large language models to generate labels for trajectories based on their similarity to expert demonstrations, with preference-based reinforcement learning (PBR) refining these labels through human feedback. Second, the MisoDICE algorithm leverages these expert-labeled trajectories to learn multi-agent policies. MisoDICE extends the DICE framework by introducing a value decomposition scheme where each agent's value function is decomposed into local and sharable components, combined through a mixing network. This ensures both consistency between global and local policies and enables convex optimization for improved stability and performance.

## Key Results
- MisoDICE achieves state-of-the-art performance on SMACv1 and SMACv2 benchmarks
- The approach is particularly effective when expert data is scarce
- MisoDICE outperforms all baselines in win rates and episode returns across various mixed-quality demonstration scenarios
- Ablation studies confirm the importance of both the labeling pipeline and the value decomposition scheme

## Why This Works (Mechanism)
The effectiveness of MisoDICE stems from its ability to leverage unlabeled mixed-quality demonstrations by first identifying expert trajectories and then learning from them using a principled decomposition approach. The progressive labeling pipeline reduces the need for manual labeling while maintaining quality through human verification on a small subset. The value decomposition in MisoDICE allows each agent to learn both local and sharable value components, which are combined through a mixing network. This structure ensures that the global policy is consistent with local policies while enabling convex optimization, leading to stable and efficient learning even from limited expert data.

## Foundational Learning
- **Value Decomposition**: Needed to separate individual agent contributions from shared team value in multi-agent settings; quick check: verify that individual Q-values sum appropriately to the joint Q-value
- **DICE Framework**: Provides the theoretical foundation for offline imitation learning; quick check: ensure KL divergence between policy and behavior policy is properly bounded
- **Preference-Based RL**: Enables iterative refinement of trajectory labels using human feedback; quick check: confirm preference queries are informative and converge to correct ranking
- **Large Language Models**: Used for initial trajectory labeling based on natural language descriptions; quick check: validate LLM labels against human-annotated subset
- **Convex Optimization in Policy Space**: Ensures stable policy updates; quick check: verify the objective remains convex after decomposition and mixing

## Architecture Onboarding

**Component Map:**
LLM-based Labeling Pipeline -> PBR Refinement -> MisoDICE Training
Environment -> MisoDICE (Local Policy + Sharable Value + Mixer -> Global Policy)

**Critical Path:**
1. Generate initial labels using LLM
2. Refine labels using PBR with human feedback
3. Train MisoDICE with labeled expert trajectories
4. Deploy learned policy in environment

**Design Tradeoffs:**
- Heavy reliance on LLM quality versus manual labeling cost
- Tradeoff between label accuracy and computational overhead in PBR refinement
- Balance between local autonomy and global coordination in value decomposition

**Failure Signatures:**
- Poor performance if LLM labeling is biased or inaccurate
- Suboptimal results if PBR refinement doesn't converge to correct labels
- Instability if value decomposition or mixing network is poorly designed

**First Experiments:**
1. Validate labeling pipeline accuracy on a small manually-labeled subset
2. Test MisoDICE with synthetic expert/non-expert trajectory mixtures
3. Compare MisoDICE against ablations (without labeling pipeline, without decomposition)

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on LLM-based labeling introduces potential biases and uncertainties
- Approach's scalability to larger state/action spaces remains untested
- Heavy dependence on LM-assistance for labeling may limit applicability in domains without suitable LLMs
- Limited ablation studies on the contribution of individual components

## Confidence
- **High Confidence**: Technical soundness of MisoDICE algorithm and convex optimization guarantees
- **Medium Confidence**: Effectiveness of progressive labeling pipeline given LLM quality dependence
- **Medium Confidence**: Superiority over baselines given limited scope of ablation studies

## Next Checks
1. Conduct a more comprehensive ablation study to isolate contributions of labeling pipeline versus MisoDICE algorithm
2. Test scalability on more complex multi-agent scenarios with larger state/action spaces
3. Evaluate approach's sensitivity to choice of LLM and proportion of human-verified trajectories required for reliable performance