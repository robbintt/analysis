---
ver: rpa2
title: Performance Evaluation of General Purpose Large Language Models for Basic Linear
  Algebra Subprograms Code Generation
arxiv_id: '2507.04697'
source_url: https://arxiv.org/abs/2507.04697
tags:
- code
- uplo
- trans
- diag
- side
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the code generation capabilities of two general-purpose
  Large Language Models (LLMs), GPT-4.1 and o4-mini, for Basic Linear Algebra Subprograms
  (BLAS) routines. The models were tested on generating C code from BLAS routine names
  alone, with and without basic optimizations (thread parallelization, SIMD vectorization,
  and cache blocking), and based on Fortran reference code.
---

# Performance Evaluation of General Purpose Large Language Models for Basic Linear Algebra Subprograms Code Generation

## Quick Facts
- arXiv ID: 2507.04697
- Source URL: https://arxiv.org/abs/2507.04697
- Reference count: 24
- Primary result: General-purpose LLMs (GPT-4.1, o4-mini) can generate correct BLAS implementations from routine names alone, with optimized code achieving significant speedups over reference BLAS implementations.

## Executive Summary
This study evaluates the code generation capabilities of two general-purpose Large Language Models (GPT-4.1 and o4-mini) for Basic Linear Algebra Subprograms (BLAS) routines. The models were tested on generating C code from BLAS routine names alone, with and without basic optimizations (thread parallelization, SIMD vectorization, and cache blocking), and based on Fortran reference code. Results showed that both models could generate correct code in many cases even when given only routine names, with o4-mini performing better overall. The generated optimized code achieved significant speedups compared to reference BLAS implementations, particularly for level-3 routines, demonstrating that general-purpose LLMs can produce practical numerical code with basic optimizations despite not being specifically trained for code generation.

## Method Summary
The study used OpenAI's GPT-4.1 and o4-mini models to generate C implementations of 20 BLAS routines across levels 1-3 (double-precision). Three prompt templates were used: generating unoptimized code from routine names, generating optimized code from routine names, and generating optimized code from Fortran reference implementations. For each prompt and model combination, 10 code samples were generated. Generated code was integrated into BLAS++ for validation using exhaustive parameter testing and performance benchmarking on an Intel Xeon Gold 6230 CPU with 40 threads, comparing against reference BLAS implementations.

## Key Results
- Both models achieved high correctness rates for level-1 and level-2 routines when given only routine names, with o4-mini outperforming GPT-4.1
- Level-3 routines showed lower correctness rates with name-only prompts but improved significantly when provided Fortran reference code
- Optimized code generated by both models achieved substantial speedups over reference BLAS implementations, particularly for level-3 routines (up to 28x for dtrsm with o4-mini)
- o4-mini consistently produced better results than GPT-4.1 across all evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** General-purpose LLMs can generate functional BLAS implementations from routine names alone by leveraging learned knowledge of API specifications from web-scale training data.
- **Mechanism:** The models map routine names (e.g., `dgemm`) to learned representations of their mathematical semantics and function signatures, acquired from documentation, tutorials, and open-source code during pre-training. This allows them to synthesize code without direct access to reference implementations.
- **Core assumption:** The training data contains sufficient high-quality documentation and code examples explaining BLAS routine specifications.
- **Evidence anchors:** [abstract] "correct code can be generated in many cases even when only routine name are given." [section] Section 4.1: "These results indicate that both models have an almost correct understanding of the specifications of the BLAS routines." [corpus] Weak direct evidence for BLAS specifically; corpus provides no directly comparable domain-specific generation studies.
- **Break condition:** Fails when routine names are ambiguous, rare, or absent from training data; also fails for routines with complex parameter combinations (e.g., `dtrsm` with multiple parameter permutations).

### Mechanism 2
- **Claim:** Providing Fortran reference code as context improves correctness by enabling code translation grounded in an explicit specification.
- **Mechanism:** The Fortran reference acts as an unambiguous specification. The LLM performs Fortran-to-C translation combined with optimization directives, grounding generation in the provided structure rather than relying solely on potentially incomplete internal knowledge.
- **Core assumption:** The LLM can accurately map Fortran constructs to C equivalents while preserving numerical semantics.
- **Evidence anchors:** [abstract] "C code with basic performance optimizations based on Fortran reference code." [section] Table 3: `FrtcodeToOptCcode` yields higher pass rates for complex routines (e.g., `dgemm` GPT-4.1: 5 vs 10 for `NameToOptCcode`). [corpus] Corpus neighbor "LLM-Assisted Translation of Legacy FORTRAN Codes to C++" supports cross-language translation capability.
- **Break condition:** Fails if Fortran code exceeds context window, uses unsupported language features, or if translation logic introduces semantic errors in edge cases.

### Mechanism 3
- **Claim:** LLMs can syntactically integrate standard HPC optimization patterns (OpenMP, SIMD, cache blocking) when explicitly instructed, yielding speedups over non-optimized reference code.
- **Mechanism:** When prompted to optimize, LLMs retrieve HPC patterns from training data (e.g., `#pragma omp parallel for`, SIMD intrinsics, loop tiling) and syntactically embed them into generated code without deriving optimizations from first principles.
- **Core assumption:** Models have been trained on sufficient HPC-related code to recognize and apply standard optimization templates.
- **Evidence anchors:** [abstract] "thread parallelization with OpenMP, SIMD vectorization, and cache blocking can be implemented to some extent, and that the code is faster than the reference code." [section] Table 6: Level-3 routines show significant speedups (e.g., `dgemm` at 7.8x for o4-mini, `dtrsm` at 28.0x). [corpus] Corpus neighbors "HPC-GPT" and "HPC-Coder" confirm active research in LLMs for HPC code generation.
- **Break condition:** Applied optimizations may be suboptimal for specific hardware; generated code is not guaranteed to match hand-tuned vendor libraries (e.g., Intel MKL). Some parameter combinations show degraded performance.

## Foundational Learning

- **Concept: BLAS (Basic Linear Algebra Subprograms) Levels**
  - **Why needed here:** Understanding the three levels—Level-1 (vector-vector), Level-2 (matrix-vector), Level-3 (matrix-matrix)—is essential to interpret the paper's performance results and complexity gradients.
  - **Quick check question:** Why do Level-3 routines (matrix-matrix operations) typically show higher speedups from cache blocking than Level-1 routines (vector operations)?

- **Concept: Reasoning Models vs. Standard GPT Models**
  - **Why needed here:** The paper compares GPT-4.1 (standard) with o4-mini (reasoning model). Understanding this distinction explains o4-mini's superior performance on complex code generation tasks.
  - **Quick check question:** According to the paper, what mechanism distinguishes o4-mini's output process from GPT-4.1's?

- **Concept: CPU Performance Optimization Techniques**
  - **Why needed here:** The paper evaluates LLMs' ability to apply thread parallelization (OpenMP), SIMD vectorization, and cache blocking. Readers must understand these concepts to evaluate generated code quality.
  - **Quick check question:** What hardware limitation does cache blocking specifically address, and how does blocking size relate to cache capacity?

## Architecture Onboarding

- **Component map:** Prompt Engineering Layer -> LLM Generation API -> Validation Harness
- **Critical path:** The `FrtcodeToOptCcode` path is most reliable for complex routines—Fortran reference code provides explicit specification, reducing reliance on potentially incomplete internal knowledge. This path tests the model's combined translation and optimization capabilities.
- **Design tradeoffs:**
  - **Name-only vs. code-based generation:** Name-only is faster and cheaper but less reliable for complex routines; code-based is more robust but consumes more context tokens.
  - **Model selection:** GPT-4.1 is cheaper/faster; o4-mini is more accurate but uses reasoning tokens and costs more.
  - **Optimization directive:** Requesting optimizations increases potential speedups but may introduce new bugs (e.g., incorrect parallelization for certain parameter combinations).
- **Failure signatures:**
  - **Compilation/runtime errors:** Syntax errors or segmentation faults, especially for complex routines with name-only prompts.
  - **Numerical errors:** Code runs but produces incorrect results, indicating algorithmic flaws.
  - **Parameter-specific failures:** Code works for standard parameters (e.g., `incx=1`) but fails for edge cases (e.g., `incx=2`, negative values).
  - **Performance regressions:** "Optimized" code slower than reference (e.g., `dgemv trans=N` at 0.6x for GPT-4.1).
- **First 3 experiments:**
  1. **Reproduce baseline correctness:** Select 3 routines across complexity levels (`daxpy`, `dgemv`, `dgemm`). Run `NameToCcode` with both models (10 samples each). Validate with `run_tests.py` to confirm Table 3 pass rates.
  2. **Evaluate optimization impact:** For `dgemm`, generate 10 optimized implementations using `NameToOptCcode` with o4-mini. Benchmark on multi-core CPU to reproduce Table 6 speedups. Analyze generated code for OpenMP/SIMD patterns used.
  3. **Test specification grounding:** For `dtrsm` (complex routine), compare `NameToOptCcode` vs. `FrtcodeToOptCcode` pass rates with o4-mini. Quantify improvement from explicit Fortran specification to isolate translation capability from specification inference.

## Open Questions the Paper Calls Out

- **Open Question 1:** To what extent can Retrieval-Augmented Generation (RAG) or iterative performance feedback loops improve the optimization quality of generated BLAS code compared to single-prompt methods?
  - **Basis in paper:** [explicit] The conclusion states that future work could "utilize RAGs to teach basic optimization methods... or iteratively modify prompts or feed back bugs or obtained performance to improve the code."
  - **Why unresolved:** The current study evaluated single-shot generation capabilities without external knowledge bases or iterative refinement.
  - **What evidence would resolve it:** A comparative study measuring the performance and correctness of BLAS routines generated via RAG or iterative feedback versus the baseline single-prompt approach.

- **Open Question 2:** Does providing specific processor architecture details in the prompt allow LLMs to select more optimal cache block sizes and SIMD instruction sets?
  - **Basis in paper:** [inferred] Section 4.2 notes that cache blocking often defaulted to a block size of 64 and SIMD usage varied, suggesting "it may be possible to set an appropriate block size by giving the processor specifications at the prompt."
  - **Why unresolved:** The experiments used generic prompts without specifying the target CPU architecture (Cascade Lake), potentially leading to sub-optimal hardware-specific tuning.
  - **What evidence would resolve it:** Performance benchmarks of code generated with architecture-aware prompts (e.g., specifying cache sizes, AVX-512 support) compared to the generic prompts used in the paper.

- **Open Question 3:** Can general-purpose LLMs maintain correctness and performance improvements when generating code for more complex HPC kernels, such as LAPACK routines?
  - **Basis in paper:** [inferred] The conclusion suggests the results apply to "numerical codes with BLAS-level complexity," implicitly leaving higher-complexity numerical kernels as an unexplored frontier.
  - **Why unresolved:** The study was limited to Level-1 to Level-3 BLAS routines; it is unclear if the observed success translates to more complex algorithms.
  - **What evidence would resolve it:** Evaluation of the same generation methodologies (NameToCcode, FrtcodeToOptCcode) applied to higher-level linear algebra routines found in LAPACK.

## Limitations

- **Generalization uncertainty:** The study's primary uncertainty stems from potential overfitting to BLAS routines due to their prevalence in training data, which may not generalize to other numerical computing domains.
- **Methodology constraints:** The evaluation methodology relies on correctness as binary pass/fail, which may mask subtle numerical inaccuracies in generated code.
- **Performance comparison limitation:** Performance comparisons are limited to a single CPU architecture (Intel Xeon Gold 6230), raising questions about cross-platform generalizability.

## Confidence

**High Confidence**: Both models can generate correct BLAS code from routine names alone for simpler routines (level-1 and level-2), with correctness rates exceeding 70% for most routines. The observed speedups from basic optimizations (OpenMP, SIMD, cache blocking) are reproducible and significant, particularly for level-3 routines.

**Medium Confidence**: o4-mini consistently outperforms GPT-4.1 across all evaluation metrics, including correctness and performance. The Fortran-to-C translation capability is demonstrated but may be specific to well-structured reference code rather than arbitrary Fortran implementations.

**Low Confidence**: The claim that general-purpose LLMs can produce "practical numerical code" is overstated. Generated code, while faster than reference BLAS, is not guaranteed to match the performance or numerical stability of hand-tuned vendor libraries. The optimization patterns applied are templated rather than derived from first principles, suggesting limited understanding of algorithmic optimization.

## Next Checks

1. **Cross-Domain Generalization:** Test the same prompt templates on a different numerical computing domain (e.g., FFTW routines or image processing kernels) to assess whether the demonstrated capabilities extend beyond BLAS.

2. **Numerical Accuracy Validation:** Implement comprehensive numerical accuracy testing beyond binary correctness, measuring relative errors against high-precision reference implementations for all generated routines across diverse parameter combinations.

3. **Hardware Portability Assessment:** Evaluate generated code performance on multiple CPU architectures (AMD, ARM) and GPU platforms to determine if optimization patterns are portable or architecture-specific.