---
ver: rpa2
title: 'Hope Speech Detection in Social Media English Corpora: Performance of Traditional
  and Transformer Models'
arxiv_id: '2510.23585'
source_url: https://arxiv.org/abs/2510.23585
tags:
- hope
- speech
- detection
- learning
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates traditional machine learning models and transformer
  architectures for detecting hope speech in social media English corpora. The dataset
  was split into train, development, and test sets.
---

# Hope Speech Detection in Social Media English Corpora: Performance of Traditional and Transformer Models

## Quick Facts
- arXiv ID: 2510.23585
- Source URL: https://arxiv.org/abs/2510.23585
- Reference count: 5
- Primary result: Transformers outperformed traditional ML (macro-F1 0.79-0.80 vs 0.75-0.78) on hope speech detection

## Executive Summary
This paper evaluates traditional machine learning models and transformer architectures for detecting hope speech in social media English corpora. The dataset was split into train, development, and test sets. On the development set, linear-kernel SVM and logistic regression achieved a macro-F1 of 0.78, SVM with RBF kernel reached 0.77, and Naïve Bayes hit 0.75. Transformer models outperformed these, with the best model achieving weighted precision of 0.82, weighted recall of 0.80, weighted F1 of 0.79, macro F1 of 0.79, and accuracy of 0.80. The results suggest that while traditional models remain effective, transformer architectures better capture the subtle semantics of hope speech, indicating potential for larger transformers and LLMs in small datasets.

## Method Summary
The study uses the PolyHope-M dataset from the RANLP 2025 shared task, containing 8,256 social media texts split into train (4,541), development (1,650), and test (2,065) sets. Preprocessing includes lemmatization with Spacy, removal of emojis/URLs/numbers/special characters, and stop word filtering using NLTK. Traditional models use TF-IDF or CountVectorizer with n-gram range (1,8), trained via SVM (linear/RBF), logistic regression, and Naïve Bayes. Transformer models use RoBERTa-Dynabench and XLM-RoBERTa-base, fine-tuned for 5 epochs with batch size 16, early stopping on F1. Evaluation uses macro-F1 as primary metric alongside weighted precision/recall/F1 and accuracy.

## Key Results
- Traditional models: Linear SVM and logistic regression achieved 0.78 macro-F1 on development set
- Transformer superiority: RoBERTa-Dynabench achieved 0.82 weighted precision and 0.80 weighted recall on test set
- N-gram effectiveness: Extended word n-grams (1-8 tokens) with TF-IDF outperformed standard unigram/bigram configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extended word n-grams (up to 8 tokens) capture phrasal patterns relevant to hope speech better than standard unigram/bigram configurations.
- Mechanism: TF-IDF vectorization with `ngram_range=(1,8)` and `analyzer='word'` preserves multi-word expressions that characterize hopeful language (e.g., "looking forward to," "can't wait to see").
- Core assumption: Hope speech contains distinctive phrasal constructions rather than isolated keywords.
- Evidence anchors:
  - [section 4.1.2]: "we observed the best performance setting ngram_range=(1,8) and analyzer='word' for both functions"
  - [abstract]: Linear SVM and logistic regression both reached macro-F1 of 0.78
  - [corpus]: Related papers (PolyHope, HopeEDI) similarly use n-gram features for hope detection with moderate success

### Mechanism 2
- Claim: Transformer self-attention captures subtle semantic distinctions between hope, optimism, and wishful thinking that sparse vector representations miss.
- Mechanism: RoBERTa-Dynabench's bidirectional attention models contextual relationships across the full sequence, distinguishing "I hope this works" (genuine hope) from sarcastic or non-hope contexts.
- Core assumption: Hope speech requires understanding pragmatic context, not just lexical content.
- Evidence anchors:
  - [abstract]: "transformer architectures detect some subtle semantics of hope to achieve higher precision and recall"
  - [section 6]: RoBERTa-Dynabench "nudges the numbers upward, hitting 0.82 for weighted precision and 0.80 for weighted recall"
  - [corpus]: Multiple related papers report transformers achieving 0.68-0.85 F1 for hope speech across languages

### Mechanism 3
- Claim: Transfer learning from a hate speech detection model provides useful semantic priors for hope speech classification.
- Mechanism: `facebook/roberta-hate-speech-dynabench-r4-target` has learned emotional and attitudinal text representations that transfer to related sentiment-adjacent tasks.
- Core assumption: Emotional intensity and goal-directed language patterns share representational structure across hate speech and hope speech domains.
- Evidence anchors:
  - [section 4.3]: Used "roberta-hate-speech-dynabench-r4-target which was trained for hate speech detection"
  - [section 5.2]: This model achieved best test performance with 0.80 accuracy and 0.82 weighted precision
  - [corpus]: Limited direct evidence for hate-to-hope transfer; related papers typically use general-purpose BERT/RoBERTa

## Foundational Learning

- Concept: TF-IDF Vectorization with N-grams
  - Why needed here: Understanding why `ngram_range=(1,8)` outperformed standard settings requires grasping how TF-IDF weights terms and how n-grams capture local context.
  - Quick check question: Why would an 8-gram capture hope speech patterns better than unigrams alone?

- Concept: Self-Attention in Transformer Encoders
  - Why needed here: The paper claims transformers capture "subtle semantics"—this requires understanding how attention weights model token relationships.
  - Quick check question: How does bidirectional self-attention differ from the sequential processing in TF-IDF?

- Concept: Transfer Learning and Fine-tuning
  - Why needed here: The study fine-tunes pre-trained models rather than training from scratch; understanding what transfers and what doesn't is critical.
  - Quick check question: Why might a hate speech model provide useful priors for hope speech detection?

## Architecture Onboarding

- Component map:
  Data Cleaning -> Feature Extraction -> Traditional Models / Transformer Models -> Evaluation

- Critical path:
  1. Clean text (lemmatize, remove noise)
  2. For traditional: vectorize with TF-IDF/CV → train classifier
  3. For transformers: tokenize with model-specific tokenizer → fine-tune with early stopping on F1
  4. Evaluate on held-out test set using macro-F1 as primary metric

- Design tradeoffs:
  - **Traditional ML vs. Transformers**: Traditional models are faster and more interpretable (0.78-0.79 F1) but plateau; transformers achieve higher precision (0.82) with more compute cost
  - **N-gram range**: Wider range (1,8) captures more phrasal context but increases feature sparsity
  - **Pre-trained checkpoint selection**: Hate-speech-trained RoBERTa outperformed general XLM-RoBERTa, suggesting task-adjacent pretraining matters

- Failure signatures:
  - Traditional models plateau around 0.78-0.79 macro-F1 regardless of vectorizer choice
  - XLM-RoBERTa underperforms RoBERTa-Dynabench (0.79 vs 0.82 precision) despite multilingual capacity
  - Small dataset (4,541 train samples) may cause overfitting in transformers beyond 5 epochs

- First 3 experiments:
  1. **Baseline traditional ML**: Train SVM-linear with TF-IDF (`ngram_range=(1,8)`) on train set, evaluate macro-F1 on development set (target: ~0.78)
  2. **Transformer fine-tuning**: Fine-tune RoBERTa-Dynabench for 5 epochs with batch size 16, early stopping on F1 (target: >0.79 macro-F1)
  3. **Ablation on n-gram range**: Compare `ngram_range=(1,2)` vs `(1,8)` to validate extended n-gram benefit for traditional models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can generative Large Language Models (LLMs) outperform fine-tuned encoder-only transformers on small hope speech datasets?
- Basis in paper: [explicit] The conclusion suggests that "larges transformers and LLMs could perform better in small datasets" than the models tested.
- Why unresolved: The study restricted its evaluation to BERT-based encoders (RoBERTa, XLM-Roberta) and traditional ML, excluding modern generative LLMs.
- What evidence would resolve it: A comparative benchmark of generative LLMs (e.g., GPT-4, Llama) against the RoBERTa-Dynabench baseline on the same corpus.

### Open Question 2
- Question: Does the aggressive text cleaning pipeline (removing emojis, URLs, and stopwords) hinder the ability of transformers to capture subtle hope semantics?
- Basis in paper: [inferred] The methodology describes a rigorous cleaning process using NLTK/Spacy, yet the paper concludes transformers excel at detecting "subtle semantics" often found in the removed features.
- Why unresolved: The paper does not conduct an ablation study to determine if standard preprocessing discards relevant semantic signals present in social media slang or symbols.
- What evidence would resolve it: A comparison of model performance on raw, uncleaned text versus the processed dataset.

### Open Question 3
- Question: Why does a transformer pre-trained on hate speech data (RoBERTa-Dynabench) provide superior feature extraction for hope speech compared to general-purpose models?
- Basis in paper: [inferred] The best-performing model was `roberta-hate-speech-dynabench-r4-target`, outperforming `xlm-roberta-base-language-detection`, but the paper offers no analysis of this transfer learning phenomenon.
- Why unresolved: It is unclear if the semantic structures of hate speech are inversely correlated with hope or if the domain-specific social media pre-training was the deciding factor.
- What evidence would resolve it: An analysis of attention weights or latent space clustering for hope/hate classes to identify shared feature representations.

## Limitations
- Dataset accessibility: PolyHope-M dataset tied to RANLP 2025 shared task with uncertain public release
- Missing hyperparameters: Critical transformer training parameters (learning rate, optimizer, warmup) not specified
- Speculative transfer mechanism: Hate-to-hope transfer claim lacks direct empirical evidence

## Confidence

**High Confidence**: Performance comparison between traditional ML models (macro-F1 ~0.78) and transformer models (macro-F1 ~0.79-0.80) is well-supported by reported metrics and standard evaluation methodology.

**Medium Confidence**: Extended n-grams (1-8) capturing hope speech patterns better than standard unigrams/bigrams is supported by development set results but lacks ablation studies. Transformer self-attention capturing "subtle semantics" is plausible but not empirically demonstrated.

**Low Confidence**: Hate speech transfer learning mechanism remains speculative without explicit analysis of what features transfer between hate and hope domains. Suggestion that larger transformers/LLMs could improve performance is theoretical rather than empirically validated.

## Next Checks
1. **Dataset Verification**: Obtain the PolyHope-M dataset from RANLP 2025 organizers and verify that the reported train/dev/test splits (4,541/1,650/2,065 samples with balanced classes) match the actual distribution. Check for any preprocessing discrepancies between the paper's description and the raw data format.

2. **Ablation Study on N-gram Range**: Systematically evaluate traditional ML models with n-gram ranges of (1,2), (1,4), and (1,8) on the development set to quantify the marginal benefit of extended n-grams and determine whether the performance plateau suggests diminishing returns beyond certain lengths.

3. **Transfer Learning Analysis**: Compare the hate speech model's performance against general-purpose RoBERTa and XLM-RoBERTa using the same fine-tuning protocol to isolate whether the pretraining domain (hate vs general) or model architecture differences drive the performance gap, and examine attention patterns to identify what semantic features transfer between these opposite emotional domains.