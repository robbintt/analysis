---
ver: rpa2
title: Don't Erase, Inform! Detecting and Contextualizing Harmful Language in Cultural
  Heritage Collections
arxiv_id: '2505.24538'
source_url: https://arxiv.org/abs/2505.24538
tags:
- terms
- language
- vocabulary
- tool
- contentious
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a multilingual vocabulary and AI-powered tool
  for detecting and contextualizing harmful language in Cultural Heritage metadata.
  The vocabulary, co-created with marginalized communities and experts, maps contentious
  terms to contextual explanations and usage recommendations, structured as a Knowledge
  Graph.
---

# Don't Erase, Inform! Detecting and Contextualizing Harmful Language in Cultural Heritage Collections

## Quick Facts
- arXiv ID: 2505.24538
- Source URL: https://arxiv.org/abs/2505.24538
- Reference count: 26
- Introduces a multilingual vocabulary and AI-powered tool for detecting and contextualizing harmful language in Cultural Heritage metadata

## Executive Summary
This work introduces a multilingual vocabulary and AI-powered tool for detecting and contextualizing harmful language in Cultural Heritage metadata. The vocabulary, co-created with marginalized communities and experts, maps contentious terms to contextual explanations and usage recommendations, structured as a Knowledge Graph. The tool combines lemmatization, string matching, NER, and LLM-based disambiguation to accurately identify derogatory usage while minimizing false positives. Evaluated on a niche-sourced dataset of over 3,700 texts, the system achieves precision up to 0.89, depending on language and model. Integrated with Europeana and MINT, it has processed over 7.9 million records, contextualizing more than 77,000 detected terms. The approach balances historical accuracy with sensitivity, supporting more inclusive and accessible cultural heritage collections.

## Method Summary
The method employs a hybrid NLP-LLM pipeline that first processes text through lemmatization and exact string matching against a 687-term multilingual vocabulary structured as a SKOS Knowledge Graph. Ambiguous terms are then passed to an LLM (Mixtral-8x7B, Llama-3.1-8B, or Ministral-8B) for context-aware disambiguation, determining whether usage is contentious or neutral. The vocabulary and contextual descriptions were co-created with marginalized communities through 12 sessions involving over 60 participants. Evaluation was conducted on a niche-sourced dataset of 3,700+ texts from 145 participants, achieving precision up to 0.89. The system is integrated with Europeana and MINT platforms, processing over 7.9 million records and contextualizing 77,000+ terms.

## Key Results
- Achieved precision up to 0.89 on niche-sourced evaluation dataset, with performance varying by language (0.42-0.97)
- Processed over 7.9 million CH records and contextualized more than 77,000 detected terms
- Demonstrated effective balance between historical accuracy and sensitivity through community co-creation
- Successfully integrated with Europeana and MINT platforms for real-world deployment

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Graph Vocabulary Grounding
Structuring contentious terms as a SKOS-compliant Knowledge Graph enables both computational detection and human-interpretable contextualization. The KG links `ContentiousTerm` entities to `ContentiousIssue` nodes containing etymology, historical usage, and contemporary perception, plus `SuggestionNotes` and `SuggestedTerms`. This allows the detection tool to retrieve rich context at inference time while maintaining semantic relationships across languages (e.g., "Third World" ↔ "Derde Wereld" ↔ "Dritte Welt"). The vocabulary's coverage of 687 terms across 5 languages is sufficient for CH metadata; terms not in the KG will be missed.

### Mechanism 2: Hybrid NLP-LLM Pipeline with Context-Aware Disambiguation
Combining traditional NLP (lemmatization, string matching, NER) with LLM-based disambiguation reduces false positives for ambiguous terms while maintaining throughput. The pipeline first identifies candidate terms via lemmatization + exact string matching, then filters out named entities (e.g., "Sordo" as surname), and finally applies LLM disambiguation only to terms flagged as ambiguous (~25% of vocabulary). The LLM receives the term, surrounding text, and KG context (contentious issue description) to classify usage as derogatory or neutral. LLMs can reliably distinguish contentious from neutral usage with zero-shot prompting; the prompt format elicits accurate binary judgments.

### Mechanism 3: Community Co-Creation + Niche-Sourcing Validation
Involving marginalized communities in vocabulary co-creation and expert annotators in evaluation produces culturally sensitive, contextually accurate resources that pure data-driven methods cannot achieve. Over 60 community members and allies contributed terms and descriptions through 12 co-creation sessions. The evaluation dataset was constructed via 8 niche-sourcing campaigns with 145 participants, yielding ~3,700 curated texts. Community input from limited sessions generalizes to broader CH contexts; participant perspectives are representative of affected communities.

## Foundational Learning

- **SKOS (Simple Knowledge Organization System)**
  - Why needed here: The vocabulary is structured as a SKOS-compliant KG, enabling interoperability with semantic web standards and CH platforms like Europeana.
  - Quick check question: Can you explain how SKOS `Concept` and `ConceptScheme` entities would represent a multilingual contentious term with language-specific contextual notes?

- **Lemmatization vs. Stemming**
  - Why needed here: The tool uses lemmatization (Stanza) to map inflected forms to canonical forms for matching; 162 custom rules handle vocabulary terms misprocessed by default models.
  - Quick check question: Why would stemming be insufficient for detecting antiquated slurs that may not appear in standard morphological lexicons?

- **Word Sense Disambiguation (WSD)**
  - Why needed here: The LLM module performs a WSD-adjacent task, determining whether an ambiguous term (e.g., "race") is used in contentious or neutral sense.
  - Quick check question: How does providing KG context (contentious issue description) to the LLM differ from traditional WSD approaches using contextual embeddings?

## Architecture Onboarding

- **Component map:**
  ```
  Input Text → Tokenization → Compound Splitting (DE/NL only)
           → Lemmatization (Stanza + 162 custom rules)
           → Parallel: [NER (Stanza)] + [String Matching (pre-lemmatized vocabulary)]
           → Filter: Remove detections inside named entities
           → LLM Disambiguation (only for ambiguous terms)
           → Output: Detected contentious terms + KG context
  ```

- **Critical path:** The LLM disambiguation module is the throughput bottleneck (813 vs. 15,000+ chars/sec). For high-volume processing, batch ambiguous-term detection and parallelize LLM calls.

- **Design tradeoffs:**
  - Exact string matching vs. fuzzy/similarity-based: Chosen because CH metadata is well-formed; avoids over-flagging substrings.
  - NER before LLM: Dedicated NER module is faster than delegating to LLM, maintaining throughput.
  - Language-specific subsets vs. unified vocabulary: Chosen to handle cross-linguistic differences (e.g., "queer" in English vs. Italian), increasing editorial complexity but improving cultural accuracy.

- **Failure signatures:**
  - High false positives for Dutch: Attributed to both high ambiguity in Dutch vocabulary terms and weaker LLM performance on Dutch (limited pretraining data).
  - Terms within named entities may still slip through if NER misses them (e.g., uncommon proper nouns).
  - Omission bias (missing contextual information rather than explicit slurs) cannot be detected by term-based approaches.

- **First 3 experiments:**
  1. **Baseline precision check:** Run the tool without LLM disambiguation on a sample of your CH metadata; measure false positive rate for ambiguous terms.
  2. **LLM ablation:** Compare precision/throughput across different LLMs (Llama-3.1-8B, Ministral-8B, Mixtral-8x7B) for your target language(s); the paper shows 0.42–0.89 precision variance.
  3. **Vocabulary coverage audit:** Check what percentage of your collection's potentially contentious terms exist in the 687-term vocabulary; identify gaps for domain-specific extension.

## Open Questions the Paper Calls Out

### Open Question 1
How can automated detection systems identify "bias by omission" (e.g., lack of LGBTQIA+ representation) in Cultural Heritage metadata, rather than just detecting the presence of contentious terms? The authors explicitly state in the Limitations section that their tool cannot address "information omissions" where descriptions are biased due to missing information or perspectives outside the community. This requires a fundamentally different semantic approach. A proof-of-concept system capable of analyzing collection descriptions to statistically identify under-represented narratives or gaps compared to historical records would resolve this.

### Open Question 2
What is the recall performance of the detection tool on a randomly sampled dataset, as opposed to the precision-focused evaluation presented? The authors state that their targeted dataset creation method "enabled effective precision assessment but, due to its non-random nature, did not provide meaningful insights into accuracy or recall." Recall is difficult to measure because contentious terms are sparse in CH metadata, making random sampling ineffective for creating a ground truth dataset containing enough positive examples. An evaluation study using a "needle in a haystack" approach or exhaustive manual annotation of a random sample to determine the rate of false negatives would resolve this.

### Open Question 3
Can larger, non-quantized LLMs resolve the precision gap observed in Dutch (0.76) compared to French/Italian (0.97), or is the gap inherent to the vocabulary's ambiguity in that language? The authors note lower precision in Dutch is likely due to "limited Dutch data used in LLM pretraining" combined with high term ambiguity, and explicitly cite computational constraints that prevented experimenting with larger models. It is undetermined if the performance ceiling for Dutch is caused by the model's capacity/size (which is fixable) or the linguistic nature of the Dutch vocabulary entries themselves (which requires editorial revision). A comparative benchmark using 70B+ parameter models on the Dutch evaluation set would resolve this.

### Open Question 4
How does the validity of the vocabulary hold when applied to themes and languages that were not covered by the specific community co-creation workshops? The authors acknowledge that due to budget constraints, they "could not apply this process [co-creation] to the entire vocabulary" and were "unable to validate and discuss the vocabulary terms for all themes across every language." The vocabulary's reliability depends on community consensus; terms derived strictly from literature review without community validation may lack the necessary nuance or may suggest alternatives that are not culturally appropriate. A follow-up validation campaign targeting the specific themes (e.g., Disability) and languages that were excluded from the initial workshops would resolve this.

## Limitations
- **Vocabulary Coverage Gaps**: The 687-term vocabulary may not cover domain-specific or emerging derogatory terms in CH metadata, and relevant communities may have been missed during co-creation sessions.
- **Language-Specific Performance Variance**: The system shows high precision in English (0.89) but significantly lower in Dutch (0.42), attributed to LLM performance differences on lower-resource languages.
- **Omission Bias**: The term-based approach cannot detect harmful content through what is *not* mentioned (e.g., exclusion of marginalized groups from narratives), which is an inherent limitation of this methodology.

## Confidence
- **High Confidence**: The hybrid NLP-LLM pipeline architecture (combining lemmatization, NER, and LLM disambiguation) is well-specified and achieves the reported precision/throughput metrics on the niche-sourced evaluation dataset.
- **Medium Confidence**: The community co-creation methodology produces culturally sensitive vocabularies that pure data-driven methods cannot achieve, though this relies on the representativeness of the consulted communities.
- **Medium Confidence**: The Knowledge Graph structure enables both computational detection and human-interpretable contextualization, though the semantic richness's practical impact on CH workflows is not fully demonstrated.

## Next Checks
1. **Vocabulary Coverage Audit**: Audit the 687-term vocabulary against a representative sample of your CH collection to identify coverage gaps for domain-specific or newly emerged terms not included in the current vocabulary.

2. **Language Performance Gap Analysis**: For collections containing Dutch or other lower-resource languages, conduct targeted testing to quantify the precision drop and evaluate whether the throughput benefits of skipping LLM disambiguation outweigh the precision loss.

3. **Community Representation Stress Test**: For terms identified as contentious in your collection, trace their inclusion back to the co-creation sessions and assess whether affected communities were adequately represented in the vocabulary development process.