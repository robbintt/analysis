---
ver: rpa2
title: Balancing Information Accuracy and Response Timeliness in Networked LLMs
arxiv_id: '2508.02209'
source_url: https://arxiv.org/abs/2508.02209
tags:
- accuracy
- llms
- response
- joint
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of balancing information accuracy
  and response timeliness in multi-user systems with networked large language models
  (LLMs). It introduces a centralized task processor that routes binary queries to
  specialized LLM clusters and aggregates responses.
---

# Balancing Information Accuracy and Response Timeliness in Networked LLMs

## Quick Facts
- **arXiv ID:** 2508.02209
- **Source URL:** https://arxiv.org/abs/2508.02209
- **Reference count:** 38
- **Primary result:** Centralized task processor routes binary queries to specialized LLM clusters and uses MAP-based adaptive majority voting to balance accuracy and latency.

## Executive Summary
This paper addresses the challenge of balancing information accuracy and response timeliness in multi-user systems with networked large language models (LLMs). The authors propose a centralized task processor that routes binary queries to specialized LLM clusters and aggregates responses using a MAP-based adaptive majority voting scheme. The approach accounts for query priors and LLM expertise to optimize the trade-off between accuracy and latency. Through mathematical modeling and empirical validation, the paper demonstrates that ensemble responses consistently outperform individual models, particularly when participating LLMs have similar standalone performance.

## Method Summary
The system processes binary queries by routing them to specialized LLM clusters, where m replicas process queries in parallel. The task processor uses a MAP-based adaptive majority voting scheme to aggregate responses, calculating an optimal threshold based on query priors and model expertise. The optimization problem minimizes a weighted sum of response time and error rate, parameterized by the number of LLMs queried per cluster. The authors validate their approach using four QA benchmarks and seven different LLM models, measuring both accuracy and processing time to derive closed-form expressions for system performance.

## Key Results
- Ensemble responses consistently outperform individual models across all tested benchmarks
- Accuracy gains are more significant when participating LLMs exhibit similar standalone performance
- The optimization framework provides a principled method to tune system performance based on accuracy and latency requirements
- The MAP-based adaptive voting scheme improves binary query accuracy by adjusting the decision threshold based on query priors and model expertise

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A Maximum a Posteriori (MAP) estimator improves binary query accuracy over simple majority voting by adjusting the decision threshold based on query priors and model expertise.
- **Mechanism:** The task processor aggregates responses from $m$ LLMs. Instead of counting votes and checking for $>50\%$, it calculates an adaptive threshold $k^* = \frac{m}{2} + \frac{\log(\frac{1-w_i}{w_i})}{2\log(\frac{1-p_i}{p_i})}$. If the number of positive votes $k \ge k^*$, the output is "true." This shifts the burden of proof based on how likely the query is true a priori ($w_i$) and how reliable the models are ($p_i$).
- **Core assumption:** LLM responses are statistically independent, and all LLMs in a cluster share identical accuracy ($p_i$) and processing time ($t_i$).
- **Evidence anchors:**
  - [abstract] Mentions a "MAP-based adaptive majority voting scheme that accounts for query priors and LLM expertise."
  - [Page 3] Derives the optimal MAP estimator $\hat{U}_{MAP}$ and the threshold $k^*$ in Equations (3) and (4).
  - [corpus] Corpus evidence for this specific mathematical derivation is weak; neighbors focus on general latency or multi-agent failure modes rather than MAP voting derivations.
- **Break condition:** If LLM errors are highly correlated (e.g., all models fail on the same adversarial prompt) or if the prior $w_i$ is misestimated, the threshold adjustment may bias the output incorrectly.

### Mechanism 2
- **Claim:** Increasing the number of LLMs ($m$) in a cluster creates a fundamental trade-off: it logarithmically increases response latency while asymptotically improving accuracy.
- **Mechanism:** The system time $E[S]$ includes the waiting time and the processing time of the slowest LLM in the cluster ($\max_j \{T_{i,j}\}$). As $m$ increases, the probability of accuracy saturates (approaching 1), but the expected max response time increases logarithmically with $m$. The optimization problem minimizes a weighted sum $\frac{1}{p_{joint}} + \theta E[S]$ to find an "optimal" $m$.
- **Core assumption:** Query arrivals follow a Poisson process and transmission times are exponentially distributed.
- **Evidence anchors:**
  - [abstract] Highlights the challenge of "balancing information accuracy and response timeliness" and the formulation of an optimization problem.
  - [Page 5] Equation (15) approximates the objective function showing the logarithmic growth of latency terms vs. the accuracy saturation.
  - [corpus] "Timeliness-Oriented Scheduling and Resource Allocation..." supports the general principle of optimizing for timeliness in collaborative networked systems.
- **Break condition:** If the weight $\theta$ is set without knowing the user's true latency sensitivity, the system may over-provision (high $m$, high delay) for a user who prefers a quick, "good enough" answer.

### Mechanism 3
- **Claim:** Ensembles yield the highest marginal accuracy gains when member LLMs have comparable standalone performance, rather than pairing one expert with several weak models.
- **Mechanism:** If weak models ($p \approx 0.5$) are aggregated with a strong expert ($p \gg 0.5$) using a simple majority (or weighted vote), the noise from the weak models can drown out the expert's signal. Homogeneous performance ensures that errors are random and cancelable, rather than systematic noise from weak agents.
- **Core assumption:** "Weak" models are random guessers rather than systematically wrong (adversarial).
- **Evidence anchors:**
  - [abstract] Notes improvement is "more significant when the participating LLMs exhibit similar standalone performance."
  - [Page 7] Observations section states: "when a single high performing expert model is paired with multiple lower accuracy agents, the weaker models sometimes act as noise."
  - [corpus] Corpus evidence is weak; neighbors do not explicitly validate the "similar performance" condition for binary ensembles.
- **Break condition:** If weak models are adversarial (consistently outputting the wrong answer with high confidence), they will degrade the ensemble performance regardless of the voting mechanism.

## Foundational Learning

- **Concept:** **Maximum A Posteriori (MAP) Estimation**
  - **Why needed here:** This is the mathematical engine of the router. It explains why the system might return "True" with only 40% "Yes" votes if the prior probability of the query being true is extremely high.
  - **Quick check question:** If the prior probability $w_i$ (chance query is true) increases from 0.5 to 0.9, does the required vote threshold $k^*$ for a "True" verdict go up or down?

- **Concept:** **Order Statistics (Max of Random Variables)**
  - **Why needed here:** The system's latency is determined by the *slowest* LLM in the cluster ($\max T_{i,j}$), not the average. Understanding why latency grows logarithmically with cluster size is key to the optimization.
  - **Quick check question:** Why does adding a second LLM to a cluster increase the expected max response time, even if the second LLM is statistically identical to the first?

- **Concept:** **Queueing Theory (M/M/1 Drop Policy)**
  - **Why needed here:** The paper assumes queries arriving while the processor is busy are "dropped." This defines the "System Time" metric and the stability of the throughput.
  - **Quick check question:** How does the "drop when busy" policy simplify the calculation of $E[S]$ compared to a standard queueing system where requests wait in a buffer?

## Architecture Onboarding

- **Component map:** Users -> Task Processor (Router) -> LLM Cluster (m replicas) -> Aggregator -> Final Response
- **Critical path:**
  1. Query arrives (or is dropped).
  2. Router identifies category and selects cluster $i$.
  3. Query broadcast to $m$ LLMs in parallel.
  4. **Latency Bottleneck:** Wait for *all* $m$ responses (specifically the slowest one).
  5. Router counts votes ($k$) and compares to adaptive threshold ($k^*$).
  6. Final aggregated response returned.

- **Design tradeoffs:**
  - **High $m$:** High accuracy, high latency (logarithmic cost), high compute cost.
  - **Low $m$:** Low latency, risk of error if models disagree.
  - **High Weight $\theta$:** System prefers fast, possibly incorrect answers (low $m$).
  - **Low Weight $\theta$:** System prefers accurate, slow answers (high $m$).

- **Failure signatures:**
  - **Latency spikes:** Occur if the cluster size $m$ is too large relative to the arrival rate $\lambda$, causing the expected wait time for the "max" response to dominate.
  - **Accuracy collapse:** Occurs if model accuracies ($p_i$) are overestimated in the router's configuration, causing the adaptive threshold $k^*$ to be set incorrectly.

- **First 3 experiments:**
  1. **Threshold Validation:** Implement the system with $m=3$ models. Run queries with known priors ($w=0.2$ vs $w=0.8$) and verify that the router correctly lowers or raises the vote count required for a "True" verdict.
  2. **Latency Stress Test:** Fix $m$ and measure the 99th percentile response time. Increase $m$ incrementally to validate the logarithmic growth of latency predicted in Equation (7).
  3. **Heterogeneity Ablation:** Mix one high-accuracy model with low-accuracy models. Measure if the ensemble accuracy drops compared to the standalone expert, validating the "similar performance" observation.

## Open Questions the Paper Calls Out
None

## Limitations
- The model independence assumption is critical but unverified; correlated errors could significantly degrade MAP estimator performance
- The exponential distribution assumption for transmission times may not reflect real network conditions with heavy-tailed delays
- The "drop when busy" queueing policy represents aggressive admission control that may not be practical in production systems

## Confidence

- **High Confidence:** The mathematical derivation of the MAP estimator (Equations 3-4) and the optimization framework are sound given the stated assumptions. The U-shaped relationship between cluster size and the weighted objective is analytically derived correctly.
- **Medium Confidence:** The empirical observations about ensemble performance (accuracy gains with similar-performing models, diminishing returns with heterogeneous ensembles) are supported by the presented data, though the corpus lacks independent verification.
- **Low Confidence:** The specific hyperparameter choices (Î¸ values, learning rate for optimization, exact prompt engineering for answer extraction) are not fully specified, making exact reproduction challenging.

## Next Checks

1. **Error Correlation Test:** Create synthetic binary query datasets where some models share correlated failure modes (e.g., all models fail on questions containing specific keywords). Measure how this affects ensemble accuracy compared to the independent error assumption.

2. **Latency Distribution Validation:** Replace the exponential transmission time assumption with a heavy-tailed distribution (e.g., Pareto) and re-compute the expected system time. Determine if the logarithmic growth pattern still holds and how it affects the optimal m.

3. **Admission Policy Comparison:** Implement a finite buffer queue instead of the drop policy. Compare the expected system time and accuracy under both policies to quantify the trade-off between throughput and response timeliness.