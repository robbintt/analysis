---
ver: rpa2
title: Document Screenshot Retrievers are Vulnerable to Pixel Poisoning Attacks
arxiv_id: '2501.16902'
source_url: https://arxiv.org/abs/2501.16902
tags:
- attack
- queries
- document
- attacks
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces three pixel-based poisoning attack methods\u2014\
  Direct Optimisation, Noise Optimisation, and Mask Direct Optimisation\u2014for compromising\
  \ vision-language model-based document screenshot retrievers such as DSE and ColPali.\
  \ By leveraging gradients to manipulate pixel values, these methods successfully\
  \ poison retrieval rankings, achieving top-10 success rates of 41.9% for DSE and\
  \ 26.4% for ColPali on unseen in-domain queries when injecting a single adversarial\
  \ screenshot."
---

# Document Screenshot Retrievers are Vulnerable to Pixel Poisoning Attacks

## Quick Facts
- arXiv ID: 2501.16902
- Source URL: https://arxiv.org/abs/2501.16902
- Reference count: 38
- Primary result: Pixel poisoning attacks achieve up to 41.9% success rate on unseen queries by manipulating document screenshots

## Executive Summary
This paper introduces three gradient-based pixel poisoning methods targeting vision-language model document screenshot retrievers. By leveraging white-box access to directly manipulate pixel values through ranking loss gradients, attackers can successfully poison retrieval rankings, achieving significant success rates on both targeted and in-domain queries. The attacks generalize across some out-of-domain datasets but show complete failure on cross-lingual and high-domain-gap corpora, highlighting both the vulnerability of VLM-based retrievers and the boundaries of attack transferability.

## Method Summary
The paper presents three pixel-based poisoning attack methods: Direct Optimisation (iteratively updating screenshot pixels via gradient descent), Noise Optimisation (adding Gaussian noise matrices to document images), and Mask Direct Optimisation (modifying only peripheral margins while preserving core content). All methods optimize pixel values to shift document embeddings toward target query representations using white-box gradient access. Attacks are evaluated on DSE and ColPali retrievers across in-domain and out-of-domain datasets, measuring success rates through various k-success metrics.

## Key Results
- Single adversarial screenshot injection achieves 41.9% success@10 for DSE and 26.4% success@10 for ColPali on unseen in-domain queries
- Targeted attacks on predefined query groups reach complete success@1 with minimal visual distortion (p=0.5-1%)
- Attacks generalize to some out-of-domain datasets (Docv, Infov) but fail on cross-lingual (ShiftPr) and high-domain-gap (Tatdqa) corpora
- ColPali shows greater robustness than DSE for in-domain attacks but higher vulnerability for targeted queries

## Why This Works (Mechanism)

### Mechanism 1: Gradient Propagation Through Continuous Pixel Space
VLM-based retrievers are susceptible to corpus poisoning because gradients can directly update pixel values, unlike text-based retrieval where tokenisation prevents direct input manipulation. The attack computes ∇_x L(x, C) and iteratively updates: x_{i+1} = Clip[x_i - α · sign(∇_x L / ||∇_x L||)], directly shifting document embeddings toward target query embeddings. This requires white-box access with frozen model parameters during optimization.

### Mechanism 2: Top-p Gradient Selection for Fidelity-Effectiveness Trade-off
Constraining updates to only the top-p% most influential pixels preserves document readability while maintaining attack effectiveness. By computing e∇_x = ∇_x ⊙ 1_{Top-p}(|∇_x|), modifications spread across the image rather than concentrating distortions. For targeted attacks, p = 0.5-1% achieves complete success@1 with minimal visual corruption; higher p values are required for broader query distributions.

### Mechanism 3: Mask-Based Optimization for Content Preservation
Modifying only a peripheral margin around a resized document preserves 100% of original content while enabling attack vectors. The method resizes the original image to h = aH, w = aW, adds white margin to restore original dimensions, and only updates margin pixels: e∇_x = ∇_x ⊙ 1_{(1-M)}. Original content remains fully readable while the margin encodes adversarial signal.

## Foundational Learning

- Concept: Dense Retrieval with Vision-Language Models
  - Why needed here: Understanding how DSE (single embedding) and ColPali (multi-vector ColBERT-style) encode screenshots is essential to grasp attack surfaces
  - Quick check question: Can you explain why ColPali shows greater robustness than DSE for in-domain attacks but higher vulnerability for targeted queries?

- Concept: Corpus Poisoning vs. SEO Attacks
  - Why needed here: Attack objectives differ — poisoning degrades system trust (fidelity secondary); SEO promotes specific content (fidelity critical for user engagement)
  - Quick check question: Which attack method produces "blurry" artifacts that users might mistake for buffering issues, and why is this relevant to SEO?

- Concept: White-Box Gradient-Based Adversarial Attacks (FGSM Family)
  - Why needed here: All three methods adapt I-FGSM; understanding momentum normalization and iterative refinement explains attack strength
  - Quick check question: What constraint does the Clip(·) function enforce, and what would happen without it?

## Architecture Onboarding

- Component map: Seed documents (bottom-100 RRF) -> Attack optimizer (gradient descent on pixels) -> Adversarial documents -> Retriever (DSE/ColPali) -> Retrieval results
- Critical path: 1) Sample low-relevance seed documents, 2) Initialize adversarial image, 3) Freeze retriever weights, compute gradient of ranking loss, 4) Apply top-p selection/mask constraint, update pixels, 5) Inject m adversarial documents, measure retrieval disruption
- Design tradeoffs: Effectiveness vs. fidelity (higher p/mask area increases success but reduces stealth), single vs. multiple injection (more injections boost success but raise detection risk), in-domain vs. out-of-domain (generalization boundaries vary)
- Failure signatures: ColPali + Noise Optimization drops to 8.9% success@10 vs. 26.4% for Direct; out-of-domain attacks on ShiftPr near-zero success even with m=100; API-only access prevents gradient computation
- First 3 experiments: 1) Reproduce target-query attack (10 groups, 1 seed, p=0.5%) to validate complete success@1 and establish baseline training time, 2) Run in-domain ablation across p values to map fidelity-effectiveness frontier, 3) Test cross-dataset transfer: train on Wiki-SS-NQ, evaluate on Vidore subsets with m=1,10,100

## Open Questions the Paper Calls Out
None

## Limitations
- White-box gradient access assumption is critical and not explored for black-box scenarios
- Attack effectiveness heavily depends on seed document selection quality and corpus representativeness
- Evaluation focuses on success@k metrics without comprehensive user studies or detection rate analysis
- Generalization results show complete failure on cross-lingual and high-domain-gap corpora

## Confidence

- **High Confidence**: Core gradient-based pixel manipulation mechanism is well-established in adversarial ML literature with clear implementation details
- **Medium Confidence**: Fidelity-effectiveness tradeoff claims supported by experimental results but lack comprehensive ablation studies
- **Low Confidence**: Generalization claims to out-of-domain datasets based on limited test corpora may not reflect real-world document diversity

## Next Checks

1. **Black-Box Transfer Validation**: Implement black-box attack variant using query-based optimization to evaluate success rates without white-box gradient access on API-accessible retrievers

2. **User Perception and Detection Study**: Conduct human evaluation with 100+ participants to measure detection rates of adversarial screenshots across different fidelity settings, including eye-tracking analysis

3. **Defense Mechanism Benchmarking**: Implement and evaluate JPEG compression preprocessing, adversarial training on poisoned samples, and margin-aware attention mechanisms to measure effectiveness in reducing success rates while maintaining clean query performance