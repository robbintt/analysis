---
ver: rpa2
title: 'ClinicalFMamba: Advancing Clinical Assessment using Mamba-based Multimodal
  Neuroimaging Fusion'
arxiv_id: '2508.03008'
source_url: https://arxiv.org/abs/2508.03008
tags:
- fusion
- image
- medical
- images
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multimodal medical image
  fusion by proposing ClinicalFMamba, an end-to-end CNN-Mamba hybrid architecture
  that effectively combines local and global feature modeling for both 2D and 3D medical
  images. The method integrates Dilated Gated Convolution Blocks for multiscale feature
  extraction, a latent Mamba model for long-range dependency modeling, and cross-modal
  channel attention for feature decoding.
---

# ClinicalFMamba: Advancing Clinical Assessment using Mamba-based Multimodal Neuroimaging Fusion

## Quick Facts
- arXiv ID: 2508.03008
- Source URL: https://arxiv.org/abs/2508.03008
- Authors: Meng Zhou; Farzad Khalvati
- Reference count: 33
- Superior fusion performance across multiple quantitative metrics on three datasets (MRI-CT, MRI-SPECT, and BraTS)

## Executive Summary
This paper addresses multimodal medical image fusion by proposing ClinicalFMamba, an end-to-end CNN-Mamba hybrid architecture that combines local feature extraction with global dependency modeling. The method integrates Dilated Gated Convolution Blocks for multiscale feature extraction, a latent Mamba model for long-range dependency modeling, and cross-modal channel attention for feature decoding. A novel tri-plane scanning strategy enables effective volumetric dependency learning in 3D images. The approach demonstrates superior fusion performance across multiple quantitative metrics on three datasets, achieving real-time fusion speeds and significant improvements in downstream brain tumor classification tasks.

## Method Summary
ClinicalFMamba is an end-to-end CNN-Mamba hybrid architecture designed for multimodal medical image fusion. It processes input modalities through Dilated Gated Convolution Blocks for local feature extraction, then applies either four-directional scanning (2D) or tri-plane scanning (3D) to capture global dependencies using latent Mamba models. The fused representations are decoded with cross-modal channel attention modules that preserve complementary information from both modalities. The architecture is trained with a weighted combination of pixel, gradient, and structural similarity losses, and validated through downstream brain tumor classification using a ResNet-50 classifier with focal loss.

## Key Results
- Achieved real-time fusion speeds (0.1s per 2D image pair, 7.3s per 3D volume)
- Superior fusion performance across MRI-CT, MRI-SPECT, and BraTS datasets
- Significant improvements in downstream brain tumor classification (2.1% AUC and 5.5% F1-Score gains)

## Why This Works (Mechanism)

### Mechanism 1
The CNN-Mamba hybrid architecture enables effective fusion by delegating local feature extraction to convolutional blocks while assigning global dependency modeling to the Mamba component. Dilated Gated Convolution Blocks extract multiscale local features through parallel 3×3 and 1×1 convolutions with element-wise multiplication for cross-region gating. These local features are then processed by the latent Mamba model, which uses selective scan mechanisms to capture long-range spatial relationships across the entire feature map. The decoder reconstructs from this combined representation.

### Mechanism 2
The tri-plane scanning strategy enables 3D volumetric dependency learning by processing feature maps along three orthogonal anatomical planes (axial, coronal, sagittal) and merging their representations. Rather than flattening 3D volumes into sequences along a single dimension, the model scans along each anatomical plane separately, processes each sequence through Mamba layers, and merges outputs via Fusion Mamba blocks. This preserves spatial coherence that would be lost in unidirectional scanning.

### Mechanism 3
Cross-modal channel attention (CMCA) preserves complementary information from both modalities by adaptively selecting informative channels from a reference modality and applying this weighting to a query modality. For each modality pair, CMCA applies average and max pooling to the reference modality to generate a channel-wise importance map. This map is applied to the query modality to emphasize channels containing complementary information. Two CMCA modules run bidirectionally (modality A→B and B→A), with outputs element-wise added to latent features before decoding.

## Foundational Learning

- **State Space Models (SSMs) / Mamba Architecture**: The paper's core innovation relies on Mamba's selective scan mechanism for linear-time long-range dependency modeling. Understanding SSM basics (hidden state evolution, selective scan, discretization) is necessary to understand why the architecture uses Mamba instead of Transformers.
  - Quick check: Can you explain why Mamba achieves O(N) complexity for sequence modeling while Transformers require O(N²)?

- **Dilated and Gated Convolutions**: The DGCB module combines dilated convolutions (for multiscale receptive fields without parameter explosion) with gating mechanisms (for controlled information flow). Understanding both is necessary to modify or debug the feature encoder.
  - Quick check: How does a dilation rate of 5 in a 3×3 convolution differ from a 15×15 standard convolution in terms of receptive field and parameter count?

- **Medical Imaging Modalities and Fusion Objectives**: The loss function combines SSIM, pixel intensity, and gradient terms based on what diagnostically matters. Understanding why MRI preserves soft tissue while CT captures bone structure clarifies why "max(x1, x2)" appears in the pixel loss formulation.
  - Quick check: Why would taking the element-wise maximum of two modalities be a reasonable reconstruction target for medical fusion?

## Architecture Onboarding

- **Component map**: Input (2 modalities) → DGCB × N (per modality) → Concatenation → Latent Mamba (2D: 4-direction / 3D: tri-plane) → Fusion Mamba → Decoder with CMCA (bidirectional) → Fused Output → Loss: L_pixel + L_grad + L_ssim (weighted sum)

- **Critical path**: The DGCB → Latent Mamba → CMCA sequence is the performance-critical pipeline. Ablation studies show CMCA removal causes measurable degradation; the tri-plane scanning contribution is substantial for 3D.

- **Design tradeoffs**:
  - **PSNR vs. Information Preservation**: The paper notes lower PSNR than EH-DRAN on MRI-CT but higher entropy/FSIM, indicating a tradeoff between pixel-level fidelity and feature richness. The loss weighting (λ_pixel=2, λ_grad=10, λ_ssim=5) reflects prioritizing structural and gradient preservation.
  - **2D vs. 3D Complexity**: 3D variant has 6.01M parameters vs. 4.05M for 2D, with 73× longer inference time (7.3s vs. 0.1s). For clinical deployment, this may determine which variant is feasible.
  - **Scanning Directions**: More scanning directions capture more spatial relationships but increase computation linearly. The paper uses 4 directions for 2D, 3 planes for 3D—empirical choices without systematic sweep.

- **Failure signatures**:
  - **Contrast degradation / washed-out outputs**: Indicates insufficient gradient loss weight or CMCA not activating correctly
  - **Artifact introduction**: May signal dilation rates too aggressive for image resolution
  - **Poor 3D performance relative to 2D baseline**: Suggests tri-plane scanning not converging—check dimension handling in Fusion Mamba blocks
  - **Slow convergence**: Loss weights may need adjustment; paper uses λ1=2, λ2=10, λ3=5

- **First 3 experiments**:
  1. **Sanity check**: Run fusion on a single MRI-CT pair with all loss terms enabled. Verify output visually preserves both bone (CT) and soft tissue (MRI) characteristics. Check that SSIM > 0.75 on training data within 50 epochs.
  2. **Component ablation**: Train three variants on MRI-CT: (a) full model, (b) without CMCA, (c) with single-direction scanning. Compare SSIM and FMI scores to reproduce paper's ablation deltas (~0.02 SSIM drop without CMCA).
  3. **3D tri-plane validation**: Train ClinicalFMamba-3D on BraTS with (a) tri-plane scanning, (b) single-axis scanning. Confirm tri-plane achieves >3 PSNR improvement as reported. Monitor for out-of-memory issues with 128³ volumes.

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but several limitations and directions for future work are implied:

- The architecture is evaluated exclusively on neuroimaging datasets, raising questions about generalizability to other anatomical structures or modalities
- The clinical validation is limited to a coarse classification task rather than dense prediction tasks like segmentation
- The fixed tri-plane scanning strategy may introduce computational redundancy compared to learnable scanning paths

## Limitations
- Several critical hyperparameters remain unspecified (learning rate schedules, batch sizes, optimizer choices)
- The ablation study showing tri-plane superiority over 2D scanning lacks statistical significance testing across multiple random seeds
- Clinical relevance claims based on downstream classification improvements don't necessarily translate to clinical utility without physician validation

## Confidence
- **High confidence**: The CNN-Mamba hybrid architecture combining local DGCB features with global Mamba dependency modeling
- **Medium confidence**: The tri-plane scanning strategy's superiority for 3D fusion
- **Low confidence**: The clinical relevance claim based on downstream classification improvements

## Next Checks
1. **Statistical validation**: Run 5 additional random seeds for the tri-plane vs. single-axis ablation on BraTS. Compute 95% confidence intervals for PSNR differences to confirm the ~3 point improvement is statistically significant.
2. **Clinical relevance verification**: Have a neuroradiologist blind-review 20 fused outputs (10 from ClinicalFMamba, 10 from best baseline). Rate image quality and diagnostic utility on Likert scales. Correlate ratings with quantitative metrics.
3. **Generalization testing**: Apply ClinicalFMamba to a held-out MRI-PET dataset not used in training. Compare fusion quality to baselines to verify architecture generalizes beyond the three specified datasets.