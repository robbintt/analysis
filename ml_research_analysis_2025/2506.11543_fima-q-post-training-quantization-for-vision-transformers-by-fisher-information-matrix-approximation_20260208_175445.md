---
ver: rpa2
title: 'FIMA-Q: Post-Training Quantization for Vision Transformers by Fisher Information
  Matrix Approximation'
arxiv_id: '2506.11543'
source_url: https://arxiv.org/abs/2506.11543
tags:
- quantization
- loss
- approximation
- matrix
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the accuracy degradation problem in post-training
  quantization (PTQ) of Vision Transformers (ViTs), particularly under low-bit quantization.
  The authors analyze the prevailing Hessian-guided quantization loss and identify
  limitations in conventional Fisher Information Matrix (FIM) approximations.
---

# FIMA-Q: Post-Training Quantization for Vision Transformers by Fisher Information Matrix Approximation

## Quick Facts
- arXiv ID: 2506.11543
- Source URL: https://arxiv.org/abs/2506.11543
- Reference count: 40
- Achieves 5.31% average accuracy improvement over state-of-the-art PTQ methods at low-bit quantization (W3/A3)

## Executive Summary
FIMA-Q addresses accuracy degradation in post-training quantization of Vision Transformers by establishing a precise connection between KL divergence and Fisher Information Matrix (FIM). The method corrects conventional approximations used in Hessian-guided PTQ, showing that FIM is linearly proportional to KL divergence gradients rather than squared gradients. Through an efficient diagonal-plus-low-rank (DPLR) approximation, FIMA-Q captures both per-dimension sensitivity and inter-token dependencies while maintaining computational efficiency. Extensive experiments demonstrate significant improvements, particularly at extreme low-bit settings, with 3-bit quantized ViTs achieving state-of-the-art performance using standard uniform quantizers.

## Method Summary
FIMA-Q operates within the block-wise reconstruction framework, replacing conventional MSE loss with a Fisher Information Matrix-based loss function. The method first computes output perturbations Δz between full-precision and quantized blocks, then calculates KL divergence between their outputs. Instead of using squared gradients as in prior approaches, FIMA-Q derives that FIM elements are linearly proportional to KL divergence gradients. The DPLR-FIM approximation combines diagonal elements (element-wise gradient-to-perturbation ratios) with a low-rank component capturing output dependencies. A progressive rank increase strategy accumulates perturbations and gradients across calibration samples, computing a weighted combination of diagonal and rank-k losses for quantization parameter updates.

## Key Results
- Achieves 64.09% accuracy on ViT-S with W3/A3 quantization, outperforming QDrop* (41.05%) by 23.04%
- Delivers 5.31% average accuracy improvement over state-of-the-art methods at 3-bit quantization
- Maintains competitive performance on COCO detection tasks when adapted for RPN classification heads
- Outperforms both diagonal-only and low-rank-only FIM approximations in comprehensive ablation studies

## Why This Works (Mechanism)

### Mechanism 1
The paper identifies that conventional diagonal FIM approximations incorrectly assume squared gradient relationships. The derivation shows L_KL(Δz^(b)) = 1/2·Δz^(b)^T·F(z^(b))·Δz^(b), yielding ∇L_KL(Δz^(b)) = F(z^(b))·Δz^(b). This linear relationship enables more accurate loss computation from calibration data alone, correcting BRECQ's squared-gradient approximation.

### Mechanism 2
FIMA-Q recognizes that FIM for ViTs contains non-negligible off-diagonal correlations that diagonal-only approximations discard. The DPLR-FIM combines F_diag = Diag(∇L_KL/Δz) with low-rank F_rank-k = ∇L_KL·(Δz^T·Δz)^(-1)·Δz^T via weighted sum. This captures both per-dimension sensitivity and collective output dependencies across tokens.

### Mechanism 3
The block-wise reconstruction with DPLR-FIM loss significantly improves low-bit quantization accuracy using standard uniform quantizers. The progressive strategy increases rank from 1 to r over iterations, recomputing averaged perturbations and gradients every x steps, allowing the method to capture increasingly complex output dependencies as optimization progresses.

## Foundational Learning

- **Concept: Fisher Information Matrix (FIM)**
  - Why needed here: Core mathematical object replacing Hessian for second-order quantization loss; understanding its relationship to KL divergence gradient is essential for implementing DPLR approximation
  - Quick check question: Given a perturbation Δz and gradient ∇L_KL, how would you compute a rank-1 approximation of FIM?

- **Concept: Block-wise Reconstruction in PTQ**
  - Why needed here: FIMA-Q operates within this framework; must understand how per-block loss optimization differs from end-to-end quantization
  - Quick check question: Why does block-wise reconstruction avoid labeled data requirements compared to QAT?

- **Concept: Low-rank Matrix Approximation**
  - Why needed here: DPLR-FIM combines diagonal and low-rank components; understanding Moore-Penrose inverse and rank-k decomposition is necessary for implementation
  - Quick check question: What condition must Δz^(b)^T·Δz^(b) satisfy for the low-rank FIM computation, and how does progressive rank increase address this?

## Architecture Onboarding

- **Component map:** Calibration data loader → Full-precision block B_full → Quantized block B_quant → Forward pass through remaining network → KL divergence loss → Backward pass for gradients → Diag-FIM/LR-FIM/DPLR-FIM loss computation → AdaRound weight updates + activation scaling factor updates

- **Critical path:**
  1. Generate Δz^(b) = z_quant - z_full for block output
  2. Compute KL divergence between full-precision and quantized model outputs
  3. Backpropagate to obtain ∇L_KL(Δz^(b))
  4. Accumulate perturbations and gradients across calibration samples (progressive rank increase)
  5. Compute L_DPLR = α·L_rank-k + (1-α)·L_diag and update quantization parameters
  6. Repeat for each transformer block

- **Design tradeoffs:**
  - Rank k: Higher k improves accuracy but linearly increases O(ak) computation; Table 4 shows 2-3x time increase for k=15 vs. k=1 on large models
  - α weighting: Controls diagonal vs. low-rank balance; paper doesn't disclose exact value
  - Calibration sample size: 1024 images for classification, 256 for detection; Table A shows reconstruction is more sensitive than FIMA step
  - Uniform vs. specialized quantizers: FIMA-Q uses standard uniform quantizers (hardware-friendly) but must compete with methods using custom log/twin quantizers

- **Failure signatures:**
  - Singular matrix in low-rank computation: Indicates insufficient linear independence in accumulated perturbations; increase iteration interval x or reduce rank k
  - Accuracy worse than MSE baseline: Suggests incorrect FIM-grad relationship implementation; verify linear (not squared) gradient scaling
  - Training time explosion on large models: Rank k=15 causes 2-3x slowdown; consider adaptive rank based on hidden dimension

- **First 3 experiments:**
  1. Reproduce Table 3 ablation on single ViT architecture: Compare MSE, BRECQ-FIM, Diag-FIM, LR-FIM, DPLR-FIM at W3/A3 to validate implementation correctness
  2. Rank sensitivity study: Sweep k ∈ {1, 5, 10, 15, 20} on DeiT-B at W4/A4, measuring both accuracy and GPU time to establish Pareto frontier
  3. Cross-architecture generalization test: Apply FIMA-Q to Swin-T on COCO detection task following section 4.1 adaptation (RPN classification head for FIM), comparing against QDrop baseline

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several limitations suggest areas for future work including the need for hyperparameter sensitivity analysis, extension to NLP Transformers, and investigation of extreme low-bit (2-bit) scenarios.

## Limitations
- The method relies on careful calibration data selection and sufficient sample coverage to accurately capture output distributions
- Fixed rank k=15 lacks systematic sensitivity analysis across different model architectures and bit-widths
- Progressive rank increase strategy requires careful tuning of iteration intervals and may not adapt well to all architectures

## Confidence
- **High Confidence**: Mathematical derivation connecting KL divergence gradients to FIM elements is rigorously proven with explicit equations and theoretical backing
- **Medium Confidence**: Experimental results showing consistent accuracy improvements across multiple architectures and tasks appear robust, but lack of hyperparameter ablation studies limits understanding of method sensitivity
- **Low Confidence**: Claim about DPLR-FIM computational efficiency compared to full FIM lacks direct runtime evidence; theoretical O(ak) advantage not validated empirically

## Next Checks
1. Perform rank sensitivity analysis by systematically varying k ∈ {1, 5, 10, 15, 20} on DeiT-B/W4/A4, measuring both accuracy and GPU memory/compute time to establish the Pareto frontier
2. Conduct hyperparameter robustness study for α (diagonal vs. low-rank weighting) and calibration iteration count across 3-4 ViT architectures to identify configuration sensitivities
3. Apply FIMA-Q to Swin-T backbone on COCO detection task, following the modified FIM computation for RPN classification heads, to verify cross-task effectiveness beyond ImageNet classification