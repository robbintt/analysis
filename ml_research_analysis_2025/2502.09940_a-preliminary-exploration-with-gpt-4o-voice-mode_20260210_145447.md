---
ver: rpa2
title: A Preliminary Exploration with GPT-4o Voice Mode
arxiv_id: '2502.09940'
source_url: https://arxiv.org/abs/2502.09940
tags:
- speech
- tasks
- audio
- classification
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This report presents a comprehensive evaluation of GPT-4o's audio
  understanding and reasoning capabilities through extensive experiments across diverse
  tasks. Using benchmarks like Dynamic-SUPERB, MMAU, and CMM, the study assesses GPT-4o's
  performance in audio, speech, and music domains, analyzing its ability to interpret
  acoustic information and generate responses.
---

# A Preliminary Exploration with GPT-4o Voice Mode

## Quick Facts
- arXiv ID: 2502.09940
- Source URL: https://arxiv.org/abs/2502.09940
- Reference count: 31
- Primary result: GPT-4o excels in audio understanding and reasoning but struggles with instrument classification and duration prediction, while showing high hallucination resistance but inconsistent refusal behaviors.

## Executive Summary
This report presents a comprehensive evaluation of GPT-4o's audio understanding and reasoning capabilities through extensive experiments across diverse tasks. Using benchmarks like Dynamic-SUPERB, MMAU, and CMM, the study assesses GPT-4o's performance in audio, speech, and music domains, analyzing its ability to interpret acoustic information and generate responses. While GPT-4o demonstrates strong knowledge in audio, speech, and music understanding—performing well in intent classification, spoken command classification, semantic and grammatical reasoning, multilingual speech recognition, and singing analysis—it struggles with tasks like audio duration prediction and instrument classification. Notably, GPT-4o shows greater robustness against hallucinations than other large audio-language models but exhibits a significantly different refusal rate for speaker verification tasks across datasets, likely due to variations in instructions or audio quality.

## Method Summary
The study evaluates GPT-4o using three benchmarks: Dynamic-SUPERB Phase 2 (180 tasks across 17 domains), MMAU (10,000 samples, 90% test/10% test-mini), and CMM (400 audio-text samples). GPT-4o (gpt-4o-audio-preview-2024-10-01) is compared against baseline models including Whisper-LLaMA, Qwen2-Audio-7B-Instruct, and others. The methodology involves querying GPT-4o for each benchmark task and using official evaluation scripts to compute relative scores versus Whisper-LLaMA baseline, refusal rates via string-matching and LLM judges, and specific metrics like Perception Accuracy (PA) and Hallucination Resistance (HR) for CMM. The study systematically analyzes performance across audio, speech, and music domains while documenting refusal behaviors and hallucination resistance.

## Key Results
- GPT-4o excels in intent classification, spoken command classification, semantic/grammatical reasoning, multilingual speech recognition, and singing analysis.
- GPT-4o struggles significantly with instrument classification and audio duration prediction, sometimes performing worse than random baselines.
- GPT-4o demonstrates much higher hallucination resistance than baselines but shows inconsistent refusal rates for speaker verification across datasets.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: End-to-end audio-language integration enables acoustic feature capture that cascaded systems lose during transcription.
- Mechanism: The model directly processes raw audio rather than first converting to text via ASR, preserving prosody, emotion, speaker characteristics, and environmental sounds that transcriptions omit.
- Core assumption: Acoustic information encoding is preserved through the audio encoder and not degraded before reaching the reasoning layers.
- Evidence anchors:
  - [abstract] "Unlike cascaded pipelines that integrate a speech recognition model with a text-based LLM, end-to-end LALMs excel at capturing rich information embedded in audio inputs that are often absent in ASR transcriptions, such as prosody, emotion, speaker information, and environmental sounds."
  - [section 4.7] "GPT-4o outperforms LALM baselines and Whisper-LLaMA in 'Stuttering Detection', showing its superior ability to handle speech characteristics and effectively identify speech disfluencies."
- Break condition: If audio encoder quantization discards fine-grained acoustic features needed for tasks like instrument classification (where GPT-4o struggles), the mechanism degrades.

### Mechanism 2
- Claim: Post-training alignment produces task-level refusal behaviors triggered by both safety categories and uncertainty-based avoidance.
- Mechanism: RLHF/RLAIF training teaches the model to refuse requests matching learned refusal patterns (speaker identification, age classification, deepfake detection) and also to refuse tasks where confidence is low, to avoid hallucination.
- Core assumption: The refusal classifier generalizes from training examples to semantically similar tasks, sometimes overgeneralizing.
- Evidence anchors:
  - [abstract] "GPT-4o's safety mechanisms cause it to decline tasks like speaker identification, age classification, MOS prediction, and audio deepfake detection."
  - [section 4.1] "The refusal rate of tasks in this domain is due to GPT-4o's low confidence in processing these tasks, leading it to refuse some samples to avoid misinformation."
- Break condition: If instruction phrasing differs significantly from training distribution, refusal triggers may not fire (explaining variable refusal rates across datasets for speaker verification).

### Mechanism 3
- Claim: Multimodal knowledge transfer from text pretraining compensates for weak acoustic processing in some tasks.
- Mechanism: The model leverages textual knowledge (e.g., song lyrics, language patterns) learned during pretraining to answer audio questions even when acoustic features are insufficient.
- Core assumption: Audio-text alignment during training creates shared representations where text knowledge can scaffold audio understanding.
- Evidence anchors:
  - [section 4.9] "We speculate that the knowledge stored in LLaMA acquired during the pretraining stage includes the textual content of music samples in this dataset. This enables Whisper-LLaMA to accurately generate pitch sequences according to the transcribed lyrics, even without relying on acoustic features."
- Break condition: When acoustic features are essential and no textual scaffolding exists (e.g., instrument classification without lyrics), performance drops significantly.

## Foundational Learning

- Concept: **Large Audio-Language Models (LALMs) vs. Cascaded Systems**
  - Why needed here: The paper evaluates GPT-4o as an end-to-end LALM and compares against cascaded baselines (Whisper-LLaMA); understanding this distinction is prerequisite to interpreting results.
  - Quick check question: Can you explain why stuttering detection would favor end-to-end over cascaded approaches?

- Concept: **Benchmark Taxonomy for Audio Tasks**
  - Why needed here: Dynamic-SUPERB organizes 180 tasks into 17 domains (Speech, Music, Audio); knowing this taxonomy helps navigate where GPT-4o excels vs. struggles.
  - Quick check question: Which three domains showed the highest refusal rates, and what safety category might explain each?

- Concept: **Hallucination Resistance Metrics**
  - Why needed here: CMM benchmark uses Perception Accuracy (PA) and Hallucination Resistance (HR); GPT-4o shows lower PA but much higher HR than baselines.
  - Quick check question: Why might higher hallucination resistance come at the cost of lower perception accuracy?

## Architecture Onboarding

- Component map: Audio Input → Audio Encoder → Shared Multimodal Representation → LLM Backbone → Safety/Refusal Classifier → Response → Text Pretraining Knowledge

- Critical path: Audio encoder quality determines acoustic feature preservation; safety classifier determines task acceptance; LLM backbone performs reasoning. Any break in audio encoding cascades to reasoning failures.

- Design tradeoffs:
  - **Safety vs. capability**: Aggressive refusal logic blocks potentially useful tasks (e.g., speaker count identification, which has no clear safety concern).
  - **End-to-end vs. interpretability**: Direct audio processing improves performance but makes error analysis harder than cascaded systems.
  - **Generalization vs. specialization**: Strong multilingual ASR but weak instrument classification suggests domain imbalance in training.

- Failure signatures:
  - High refusal rate with low accuracy when attempted → likely uncertainty-based refusal (task is genuinely hard).
  - High refusal rate with no attempts → likely safety trigger (task matches forbidden category).
  - Random baseline outperforming model → instruction-following failure or catastrophic forgetting.
  - Inconsistent refusal across datasets for same task → instruction phrasing or audio quality sensitivity.

- First 3 experiments:
  1. **Probe refusal sensitivity**: Run speaker verification task with systematically varied instruction phrasings across LibriSpeech vs. VCTK datasets to isolate whether refusal is instruction-driven or audio-quality-driven.
  2. **Ablate acoustic vs. text contribution**: Compare GPT-4o performance on music tasks with and without access to lyrics (via audio with intelligible vocals vs. instrumental only) to quantify text knowledge transfer.
  3. **Benchmark duration prediction failure**: Test audio duration prediction across short (<5s), medium (5-30s), and long (>30s) clips to identify if failure is systematic or scale-dependent.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How close are we to achieving a universal instruction-based speech model?
- Basis: [explicit] Section 1 states the report aims to address this specific question to explore development directions for future Large Audio-Language Models (LALMs).
- Why unresolved: While GPT-4o excels in semantic reasoning, it struggles with acoustic tasks like instrument classification, duration prediction, and spatial analysis, sometimes performing worse than random guessing.
- What evidence would resolve it: A single model achieving state-of-the-art performance across all Dynamic-SUPERB domains without requiring cascaded architectures.

### Open Question 2
- Question: How can safety alignment be achieved without causing over-refusal of benign acoustic tasks?
- Basis: [explicit] Section 7 notes that striking a balance between ethical considerations and advanced audio comprehension "remains a challenge for the speech research community."
- Why unresolved: The paper finds GPT-4o refuses non-sensitive tasks (e.g., speaker counting) and behaves inconsistently on identical tasks (e.g., Speaker Verification vs. SUPERB SV) depending on prompt nuances.
- What evidence would resolve it: A model that successfully performs acoustic analysis (e.g., deepfake detection) while maintaining strict refusal rates for genuinely harmful content or PII extraction.

### Open Question 3
- Question: Can end-to-end LALMs be optimized to effectively capture and reason about spatial audio information?
- Basis: [explicit] Section 4.17 concludes that current LALMs are "unable to effectively capture spatial information or respond accurately to related questions."
- Why unresolved: In spatial analysis tasks, GPT-4o and other LALMs performed significantly worse than cascaded systems and often worse than a random baseline.
- What evidence would resolve it: Significant performance improvements on "Multichannel Sound Event Understanding" and "Audio Spatial Distance Prediction" tasks, surpassing random baselines.

## Limitations

- The heterogeneous nature of audio benchmarks (180 tasks across 17 domains) makes cross-task comparisons difficult without unified evaluation protocols.
- Refusal detection relies on crude string-matching with limited keyword lists and LLM judges, potentially missing nuanced refusal patterns.
- The study does not systematically investigate why GPT-4o shows different refusal rates across datasets for identical speaker verification tasks.

## Confidence

**High confidence**: Claims about GPT-4o's strong performance on intent classification, spoken command classification, multilingual speech recognition, and semantic/grammatical reasoning are supported by multiple benchmarks with consistent results.

**Medium confidence**: Findings about refusal behaviors are moderately reliable but limited by crude detection methods; while refusal rates are accurately measured, underlying causes remain speculative.

**Low confidence**: Claims about knowledge transfer from text pretraining compensating for weak acoustic processing are highly speculative, based primarily on performance patterns rather than direct evidence of mechanism.

## Next Checks

1. **Controlled refusal ablation study**: Systematically vary instruction phrasing for speaker verification across identical audio samples from different datasets (LibriSpeech vs. VCTK) to determine whether refusal is driven by prompt sensitivity, audio quality differences, or safety model instability.

2. **Acoustic vs. text knowledge disentanglement**: Conduct controlled experiments comparing GPT-4o performance on music tasks with instrumental-only audio versus audio with intelligible vocals to quantify the contribution of textual knowledge versus acoustic processing.

3. **Refusal mechanism validation**: Implement a more sophisticated refusal detection pipeline using fine-tuned classifiers rather than string-matching, and validate against human judgments to establish baseline accuracy for detecting nuanced refusal behaviors.