---
ver: rpa2
title: 'VORTEX: Challenging CNNs at Texture Recognition by using Vision Transformers
  with Orderless and Randomized Token Encodings'
arxiv_id: '2503.06368'
source_url: https://arxiv.org/abs/2503.06368
tags:
- texture
- vortex
- recognition
- backbones
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of texture recognition, which
  has been dominated by CNNs but lacks exploration of Vision Transformers (ViTs).
  The proposed method, VORTEX, introduces a novel approach to enable effective use
  of ViTs for texture analysis by extracting multi-depth token embeddings from pre-trained
  ViT backbones and employing a lightweight module to aggregate hierarchical features
  and perform orderless encoding.
---

# VORTEX: Challenging CNNs at Texture Recognition by using Vision Transformers with Orderless and Randomized Token Encodings

## Quick Facts
- **arXiv ID:** 2503.06368
- **Source URL:** https://arxiv.org/abs/2503.06368
- **Reference count:** 40
- **Primary result:** VORTEX achieves state-of-the-art performance on nine diverse texture datasets using Vision Transformers with orderless and randomized token encodings.

## Executive Summary
VORTEX introduces a novel approach to texture recognition using Vision Transformers (ViTs) that addresses the challenge of extracting effective texture features from transformer architectures. The method extracts multi-depth token embeddings from all layers of pre-trained ViT backbones and employs a lightweight orderless encoding mechanism using Randomized Autoencoders (RAEs). This approach enables ViTs to capture both fine-grained local patterns and global semantic context while remaining invariant to spatial token ordering. VORTEX demonstrates superior performance across nine texture datasets, surpassing both traditional CNN-based methods and other ViT approaches.

## Method Summary
VORTEX uses a frozen pre-trained ViT backbone to extract spatial token embeddings from all transformer layers (excluding the CLS token). These tokens are concatenated and L2-normalized to form a multi-depth representation. The method then applies 16 parallel Randomized Autoencoders (RAEs) with analytically solved decoder weights to perform orderless encoding. Each RAE uses a random encoder with weights generated via Linear Congruential Generator, and the final representation is obtained by summing the decoder weights across all RAEs. A linear SVM classifier is trained on these frozen features to achieve state-of-the-art texture recognition performance.

## Key Results
- Achieves state-of-the-art performance on nine diverse texture datasets
- Outperforms both CNN-based methods and other ViT approaches
- Demonstrates robust computational efficiency with frozen backbone features
- Shows consistent gains across different texture types and dataset characteristics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aggregating spatial token embeddings from all transformer layers captures hierarchical texture features more effectively than using only the final layer representation.
- Mechanism: Shallow layers encode sparse, local texture patterns while deeper layers encode complex, semantic patterns biased toward object detection. By concatenating L2-normalized tokens from all layers, VORTEX preserves both low-level textural cues and high-level semantic context in a single representation χ ∈ R^(ln×d).
- Core assumption: Texture recognition benefits from both fine-grained local patterns (early layers) and global semantic context (deeper layers), similar to how CNN-based texture methods aggregate multi-scale features.
- Evidence anchors:
  - [abstract] "extracts multi-depth token embeddings from pre-trained ViT backbones"
  - [section 3.1] "attention in shallow layers is more sparse and global...at the last layers, the model tries to focus on specific regions"
  - [corpus] GraphTEN (arXiv:2503.13991) confirms modeling non-local context relations remains challenging for CNNs, motivating transformer approaches
- Break condition: If texture datasets primarily contain simple, homogeneous patterns without semantic context, shallow features alone may suffice and multi-depth aggregation adds unnecessary complexity.

### Mechanism 2
- Claim: Orderless encoding via Randomized Autoencoders (RAEs) produces texture representations invariant to spatial token ordering, improving generalization to rotated/scaled textures.
- Mechanism: RAEs project aggregated tokens χ through a random fixed encoder (LCG-generated, orthogonalized weights) with a single hidden neuron, then analytically solve for decoder weights f_k via least squares. Since the solution is invariant to input row order, the resulting decoder weights encode texture statistics without positional bias.
- Core assumption: Textures are characterized by stochastic/regular patterns rather than fixed spatial arrangements; removing positional sensitivity improves robustness to geometric transformations.
- Evidence anchors:
  - [abstract] "perform orderless encoding, obtaining a better image representation for texture recognition"
  - [section 3.2] "enforcing spatial order can limit the texture recognition ability...this process becomes an orderless encoding"
  - [corpus] No direct corpus validation of RAE-based orderless encoding for ViT textures found
- Break condition: If texture recognition tasks require precise spatial localization (e.g., texture boundary detection), orderless encoding may discard critical positional information.

### Mechanism 3
- Claim: Ensembling multiple RAEs with different random seeds ("soup") increases encoding capacity and representation stability without backpropagation.
- Mechanism: m parallel RAEs with different LCG seeds generate diverse random encodings. Their learned decoder weights are summed: φ_m = (Σf_k(ν_1), ..., Σf_k(ν_d)). This ensemble captures complementary aspects of the token distribution.
- Core assumption: Different random projections capture different statistical properties of the token space, and simple summation provides effective aggregation without learnable parameters.
- Evidence anchors:
  - [section 3.2] "A single RAE may have limited encoding capacity. Therefore, we use a model 'soup' built by combining the weights of m parallel RAEs"
  - [section 4.2.1] "VORTEX improves significantly with m > 1...we select m = 16"
  - [corpus] No corpus papers directly validate RAE ensemble strategies for ViT features
- Break condition: If m is set too high relative to dataset size, the ensemble may overfit to spurious correlations; if too low, representation capacity is insufficient.

## Foundational Learning

- Concept: **Vision Transformer (ViT) token structure**
  - Why needed here: VORTEX operates on spatial tokens from all layers, not the CLS token. Understanding that ViTs process images as sequences of patch embeddings (n tokens × d hidden dimension) is essential for implementing multi-depth extraction.
  - Quick check question: Can you explain why the CLS token is discarded in VORTEX and only spatial tokens are aggregated?

- Concept: **Randomized Neural Networks and analytical least-squares solutions**
  - Why needed here: The RAE decoder weights are computed via f_k = g_k^T(g_k g_k^T)^(-1)χ, not gradient descent. This closed-form solution enables efficient training without backpropagation.
  - Quick check question: Why does the RAE use a fixed random encoder and only learn the decoder weights?

- Concept: **Texture vs. object recognition inductive biases**
  - Why needed here: Understanding why orderless pooling helps texture recognition (textures are stationary patterns) while object recognition benefits from spatial structure clarifies the design rationale.
  - Quick check question: Would you expect orderless encoding to help or hurt performance on the ImageNet object classification task?

## Architecture Onboarding

- Component map:
  - **ViT Backbone (frozen)** -> **Multi-depth Aggregator** -> **RAE Ensemble (m=16)** -> **Soup Combiner** -> **Linear Classifier (SVM)**

- Critical path:
  1. Load pre-trained ViT (DeiT3-B/16 with IN-21k recommended as starting point)
  2. Forward pass input image, extract spatial tokens (exclude CLS) from all 12 layers
  3. Concatenate and L2-normalize → χ
  4. For k=1 to 16: Generate W_k via LCG, compute g_k = σ(χW_k), solve for f_k
  5. Sum f_k vectors → φ_16
  6. Train linear SVM on φ_16 features

- Design tradeoffs:
  - **Backbone size vs. efficiency**: Larger backbones (ViT-L/H) improve in-the-wild texture performance but require 3-10× more GFLOPs. Start with DeiT3-B/16 (12.7 GFLOPs) for development.
  - **RAE count (m) vs. capacity**: m=16 stable across datasets; m<4 underfits, m>31 shows diminishing returns.
  - **Frozen vs. fine-tuned backbone**: Freezing eliminates backpropagation costs but limits domain adaptation. Paper shows frozen features achieve SOTA on most benchmarks.

- Failure signatures:
  - **Performance gap on small datasets with IN-1k backbones**: VORTEX underperforms CNN methods on GTOS/KTH when using small ViTs with limited pre-training. Solution: Use IN-21k or LAION-2B pre-trained variants.
  - **CLS token outperforms VORTEX on some in-the-wild textures**: Indicates over-aggregation of noisy shallow features. Consider weighted layer aggregation instead of uniform concatenation.
  - **Numerical instability in RAE decoder computation**: Occurs when g_k g_k^T is near-singular. Verify orthogonalization of random weights and L2 normalization of χ.

- First 3 experiments:
  1. **Baseline validation**: Replicate Table 3 results on Outex13 using ViT-B/16 (IN-21k), m=16, comparing KNN/LDA/SVM classifiers. Target: ~94% SVM accuracy.
  2. **Ablation on m**: Sweep m ∈ {1, 4, 8, 12, 16, 24, 31} on FMD dataset. Verify performance plateau at m=16 as shown in Figure 4b.
  3. **Backbone comparison**: Compare CLS token, GAP, and VORTEX features using DeiT3-B/16 (IN-21k) on the 5-dataset pentagon benchmark (Figure 7). VORTEX should show consistent gains across all vertices.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can VORTEX performance be further improved by integrating it with hybrid architectures that combine the localized feature extraction strengths of CNNs with the global context modeling of ViTs?
  - Basis in paper: [explicit] The Conclusion suggests "investigating the integration of VORTEX with hybrid architectures, combining the strengths of ViTs and CNNs" as a future direction.
  - Why unresolved: The current study focuses exclusively on pure ViT backbones. The paper notes in the results that CNN architectures may still be better for pure textures with high variability and low noise, suggesting a hybrid approach might capture the best of both worlds.
  - What evidence would resolve it: Experimental results applying VORTEX to hybrid backbones (e.g., CNN-ViT mixtures) on datasets where pure ViTs currently lag behind CNNs (e.g., KTH-2-b).

- **Open Question 2**: How does VORTEX performance scale with higher-resolution inputs and backbones capable of handling variable image sizes?
  - Basis in paper: [explicit] The Conclusion explicitly states the need for "addressing backbones that deal with varying input sizes... allowing it to process higher-resolution textures."
  - Why unresolved: The experiments were primarily restricted to a standard 224×224 input resolution. The authors note that a competitor method (Multilayer-FV) achieved high performance using 512×512 inputs, implying VORTEX might benefit similarly from higher resolutions which are currently untested.
  - What evidence would resolve it: Ablation studies varying the input resolution (e.g., 384×384, 512×512) using backbones that support dynamic positional embeddings or sliding windows.

- **Open Question 3**: To what extent can domain-specific pre-training strategies (e.g., for medical imaging or remote sensing) enhance VORTEX's texture recognition capabilities compared to general-purpose vision pre-training?
  - Basis in paper: [explicit] The Conclusion highlights "incorporating domain-specific pre-training strategies could improve its performance in specialized settings" as a key area for future work.
  - Why unresolved: The paper evaluates VORTEX using backbones pre-trained on general datasets like ImageNet, COYO, or LAION. It does not test whether the "orderless" token encoding benefits specifically from pre-training on domain-specific texture data (like medical scans).
  - What evidence would resolve it: Comparing the performance of VORTEX using a generic backbone versus a backbone pre-trained on a specialized texture corpus (e.g., RadImageNet) within a relevant downstream task.

## Limitations

- **Orderless encoding mechanism uncertainty**: Limited empirical evidence showing the specific benefit of RAEs over simpler orderless pooling methods like GeM or mean pooling.
- **Backbone generalization concerns**: Inconsistent effectiveness with smaller, more commonly available backbones (IN-1k) versus large-scale pre-trained models.
- **Computational efficiency claims**: The method's efficiency gains compared to simpler CNN-based approaches or single-layer ViT features are not thoroughly benchmarked across different hardware configurations.

## Confidence

- **SOTA performance claims**: High confidence
- **Multi-depth token aggregation benefits**: Medium confidence
- **Orderless encoding advantages**: Low confidence

## Next Checks

1. **Compare RAE-based orderless encoding against standard pooling**: Implement and test GeM pooling and mean pooling on the same multi-depth token representations used in VORTEX. This will determine if the complex RAE approach provides measurable benefits over established orderless methods for texture recognition.

2. **Test computational efficiency across hardware**: Benchmark VORTEX's inference time and memory usage on both GPU and CPU across different backbone sizes (ViT-S, ViT-B, ViT-L). Compare these metrics against a baseline CNN texture method (e.g., DenseNet) to verify the claimed efficiency advantages.

3. **Evaluate robustness to geometric transformations**: Create a controlled test set with rotated, scaled, and sheared versions of textures from a validation set. Compare VORTEX's performance against both standard ViT CLS features and CNN baselines to empirically validate the orderless encoding's claimed robustness to geometric variations.