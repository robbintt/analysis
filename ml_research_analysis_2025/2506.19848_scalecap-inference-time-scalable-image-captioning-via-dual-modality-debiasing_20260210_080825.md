---
ver: rpa2
title: 'ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality Debiasing'
arxiv_id: '2506.19848'
source_url: https://arxiv.org/abs/2506.19848
tags:
- image
- scalecap
- caption
- captions
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ScaleCap, an inference-time scalable image
  captioning framework that addresses two major biases in large vision-language models:
  multimodal bias leading to imbalanced descriptive granularity and linguistic bias
  causing hallucinations. The core method involves a two-stage approach: heuristic
  question answering generates content-specific questions based on initial captions
  to extract fine-grained visual details, while contrastive sentence rating uses offline
  contrastive decoding to filter out hallucinated content by comparing token probabilities
  with and without image input.'
---

# ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality Debiasing

## Quick Facts
- **arXiv ID:** 2506.19848
- **Source URL:** https://arxiv.org/abs/2506.19848
- **Reference count:** 40
- **Primary result:** Introduces ScaleCap, an inference-time scalable image captioning framework that achieves best performance on 11 benchmarks through dual-modality debiasing.

## Executive Summary
This paper introduces ScaleCap, an inference-time scalable image captioning framework that addresses two major biases in large vision-language models: multimodal bias leading to imbalanced descriptive granularity and linguistic bias causing hallucinations. The core method involves a two-stage approach: heuristic question answering generates content-specific questions based on initial captions to extract fine-grained visual details, while contrastive sentence rating uses offline contrastive decoding to filter out hallucinated content by comparing token probabilities with and without image input. ScaleCap is used to annotate 450K images, and models pretrained on this dataset achieve the best performance on 11 widely used benchmarks. For example, ScaleCap-450k improves InfoVQA scores by 4.3% over ShareGPT4V-450k and 2.4% over DenseFusion-450k in Qwen2.5-7B setting. Additional evaluations via downstream VQA tasks and image reconstruction confirm the superior richness and accuracy of captions generated by ScaleCap compared to both open-source models and GPT-4o.

## Method Summary
ScaleCap employs a two-stage inference-time framework: (1) Heuristic Question Answering where an LLM analyzes initial captions to generate targeted questions about under-described objects, which a small LVLM (7B sufficient) answers to extract fine-grained details, and (2) Contrastive Sentence Rating where offline contrastive decoding compares token probabilities with and without image input to filter hallucinated content based on probability differences. The method scales by increasing the number of heuristic questions asked, with diminishing returns after ~20 questions. The framework is used to annotate 450K images (100K from ShareGPT4V-100k and 350K from LAION-5B filtered by complexity scores), creating the ScaleCap-450k dataset that enables superior pretraining performance across 11 benchmarks.

## Key Results
- Models pretrained on ScaleCap-450k achieve best performance on 11 widely used benchmarks
- InfoVQA scores improve by 4.3% over ShareGPT4V-450k and 2.4% over DenseFusion-450k in Qwen2.5-7B setting
- Contrastive rating reduces hallucination rates from 44.2% to 25.8% on CHAIR benchmark
- 7B LVLMs achieve comparable perceptual detail to 72B models when using explicit targeted prompting

## Why This Works (Mechanism)

### Mechanism 1: Heuristic Question Answering for Progressive Detail Extraction
Small LVLMs (~7B) can match larger models in perceptual detail when explicitly prompted with targeted questions. An LLM analyzes initial captions to identify under-described objects, generates specific prompts (e.g., "Describe more details about the airplane"), which the LVLM then answers. This iterative loop injects fine-grained information that would otherwise remain latent. The method assumes perception capability in LVLMs is underutilized by generic prompting; retrieval is the bottleneck, not representation. Break condition: If the LVLM lacks the perceptual representation entirely, no amount of prompting will recover missing details.

### Mechanism 2: Contrastive Sentence Rating for Hallucination Filtering
Sentences with low contrastive probability difference (image vs. text-only generation) are more likely to be hallucinated. For each sentence, compute token probabilities with image input (P) and without (P'). The difference ΔP = P - P' quantifies visual grounding. Sentences where max(Δp) falls below threshold τ are filtered as hallucinations. This is performed offline at sentence level rather than token-level to preserve fluency. The method assumes hallucinations arise from linguistic priors that dominate when visual evidence is weak. Break condition: If a hallucinated sentence happens to have high probability with image input (e.g., plausible inference the image supports but doesn't ground), the filter will miss it.

### Mechanism 3: Inference-Time Scalability via Instruction Budget Control
Caption quality improves monotonically with the number of heuristic questions asked, saturating as coverage completes. A budget parameter N limits the maximum number of object/position instructions. As N increases, more fine-grained details are progressively captured, with diminishing returns once salient content is exhausted. The relationship between detail coverage and downstream utility follows a saturation curve. Break condition: If the LLM generating questions fails to identify the right objects to probe, increasing N may add noise rather than value.

## Foundational Learning

- **Concept: Contrastive Decoding in LVLMs**
  - Why needed: ScaleCap's hallucination filter relies on comparing conditional distributions with and without visual input. Understanding how language priors dominate when visual grounding is weak is essential.
  - Quick check: Given a token generated by an LVLM, how would you compute whether it was likely influenced by the image or primarily by language priors?

- **Concept: Autoregressive Generation and Probability Distributions**
  - Why needed: The contrastive rating module operates on token-level probabilities from autoregressive models. You need to understand how pθ(yt | I, [T, c<t]) differs from pθ(yt | [T, c<t]).
  - Quick check: In an autoregressive LVLM, what information does pθ(yt | I, [T, c<t]) condition on that pθ(yt | [T, c<t]) does not?

- **Concept: Hallucination Types in Vision-Language Models**
  - Why needed: ScaleCap specifically targets "linguistic bias" hallucinations (text-derived, co-occurrence-based). Understanding the distinction between semantic errors and ungrounded inferences matters for interpreting results.
  - Quick check: If an LVLM describes "a dog sitting on grass" but the image shows a cat on grass, is this a linguistic-bias hallucination or a visual recognition error? Would ScaleCap's filter catch it?

## Architecture Onboarding

- **Component map:** Initial Caption Generation -> Contrastive Sentence Rating -> Heuristic Question Raising -> Visual Answering (loop N times) -> Caption Integration -> Final Caption
- **Critical path:** Initial Caption → Golden Sentence Selection → Question Generation → Visual Q&A (loop N times) → Summarization → Final Caption
- **Design tradeoffs:**
  - LVLM size: 7B sufficient for visual extraction; larger models add world knowledge but not perceptual accuracy
  - LLM size: 72B recommended for integration (handles 20k token contexts); 7B LLMs lose information in long contexts
  - Budget N: Diminishing returns after ~20 questions; set based on target latency vs. detail requirements
  - Threshold τ: Higher values reduce hallucinations but may over-filter
- **Failure signatures:**
  - Caption too short: N set too low, or golden sentence filter too aggressive
  - Hallucinations persist: τ too low, or hallucination has high image-conditioned probability
  - Incoherent final caption: LLM integrator too small, context window overflow
  - Missing key objects: LLM question generator failed to identify them
- **First 3 experiments:**
  1. Ablate contrastive rating: Run ScaleCap with τ=0 (no filtering) vs. tuned threshold; measure hallucination rate on CHAIR benchmark
  2. Vary instruction budget: Plot downstream VQA accuracy against N={2,6,10,15,20,all} to identify saturation point
  3. Test LVLM/LLM size combinations: Compare (7B LVLM + 72B LLM) vs. (72B LVLM + 7B LLM) vs. (72B both) on a subset of 300 questions

## Open Questions the Paper Calls Out
- Can integrating external knowledge or content-level supervision effectively detect and suppress harmful or offensive content that probabilistic contrastive decoding fails to identify?
- How can the heuristic question-answering mechanism be adapted to close the "world knowledge" gap between 7B and 72B LVLMs?
- Is the contrastive sentence rating threshold (τ) robust across diverse image domains, or does it require adaptive tuning to maintain the balance between detail richness and hallucination reduction?

## Limitations
- The hallucination filtering threshold τ is described as "tunable" but no specific value is provided, creating a significant reproducibility gap
- ScaleCap's performance relies heavily on the IC9600 image complexity scoring model, which is referenced but not detailed in the paper
- The paper reports performance improvements without providing confidence intervals, p-values, or statistical tests to establish significance

## Confidence
- **High Confidence:** The dual-modality debiasing framework architecture is technically sound; the inference-time scalability mechanism is implementable; the general approach of using contrastive decoding for hallucination detection is valid
- **Medium Confidence:** The specific performance improvements on downstream benchmarks (given threshold ambiguity); the optimal budget of ~20 questions for saturation; the claim that 7B LVLMs suffice for visual extraction
- **Low Confidence:** The exact hallucination reduction percentages without knowing τ; the absolute quality of ScaleCap-450k dataset without IC9600 validation; the superiority claims without statistical significance testing

## Next Checks
1. Run ScaleCap with τ values {0.1, 0.2, 0.3, 0.4} on a held-out validation set and plot hallucination rate vs. downstream VQA accuracy to identify the optimal threshold range
2. Re-run the key benchmark comparisons with 5 different random seeds for each model and compute 95% confidence intervals to determine which improvements are statistically significant
3. Obtain the IC9600 model and run it on the 350K LAION-5B images used in ScaleCap-450k to verify the complexity score distribution and accuracy