---
ver: rpa2
title: 'Leveraging Second-Order Curvature for Efficient Learned Image Compression:
  Theory and Empirical Evidence'
arxiv_id: '2601.20769'
source_url: https://arxiv.org/abs/2601.20769
tags:
- adam
- soap
- image
- compression
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses slow convergence and suboptimal performance
  in learned image compression (LIC) models caused by conflicting rate-distortion
  gradients during training. It introduces SOAP, a second-order quasi-Newton optimizer,
  as a drop-in replacement for first-order methods like Adam.
---

# Leveraging Second-Order Curvature for Efficient Learned Image Compression: Theory and Empirical Evidence

## Quick Facts
- **arXiv ID:** 2601.20769
- **Source URL:** https://arxiv.org/abs/2601.20769
- **Reference count:** 40
- **Primary result:** SOAP reduces training steps by 70% and wall-clock time by 57.7% while achieving 3% BD-Rate improvement in learned image compression.

## Executive Summary
This paper addresses slow convergence and suboptimal performance in learned image compression (LIC) models caused by conflicting rate-distortion gradients during training. It introduces SOAP, a second-order quasi-Newton optimizer, as a drop-in replacement for first-order methods like Adam. SOAP leverages curvature information to resolve gradient conflicts both within individual steps (intra-step) and across consecutive steps (inter-step), enabling faster and more stable convergence. Across four top LIC architectures (ELIC, TCM, LALIC, DCAE), SOAP reduces training steps by 70% and wall-clock time by 57.7% to reach Adam-level performance, while achieving a 3% average BD-Rate improvement when trained for the same number of steps. Additionally, SOAP-trained models exhibit fewer activation and latent outliers, enhancing robustness to post-training quantization and improving deployability.

## Method Summary
The method replaces the standard Adam optimizer with SOAP (Second-Order Adaptive Preconditioning) in learned image compression training pipelines. SOAP uses a Kronecker-factored approximate curvature to precondition gradients, resolving conflicts between rate and distortion objectives. The optimizer updates its preconditioner every 10 steps and maintains Adam-like scaling within the rotated basis. Training uses COCO 2017 with random 256Ã—256 crops, evaluating on Kodak, Tecnick, and CLIC 2022 datasets. The approach requires minimal code changes as a drop-in replacement, with the main hyperparameter being the preconditioner update frequency.

## Key Results
- SOAP reduces training steps by 70% and wall-clock time by 57.7% to reach Adam-level performance across four LIC architectures
- SOAP achieves 3% average BD-Rate improvement when trained for the same number of steps as Adam baselines
- SOAP-trained models show significantly fewer activation and latent outliers, improving post-training quantization robustness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** SOAP resolves gradient conflicts between Rate (R) and Distortion (D) objectives within a single update step (intra-step alignment).
- **Mechanism:** Standard first-order methods (Adam) apply diagonal preconditioning, which cannot rotate gradients. SOAP utilizes a second-order quasi-Newton preconditioner ($H^{-1}$) that leverages curvature information to rotate and scale the update vectors ($p_R, p_D$), forcing them to point toward a common descent direction.
- **Core assumption:** The Rate and Distortion Hessians ($H_R, H_D$) share sufficient curvature structure (locally proportional or jointly diagonalizable) with the total Hessian $H$ near the optimum.
- **Evidence anchors:**
  - [abstract] "Newton preconditioning inherently resolves the intra-step... update conflicts..."
  - [section V-C] Proposition 1 formally shows $\lim_{\theta \to \theta^*} S(p_R, p_D) = 1$ under structural conditions.
  - [corpus] "Understanding SOAP from the Perspective of Gradient Whitening" supports the efficacy of gradient whitening in aligning updates.
- **Break condition:** If Rate and Distortion Hessians become structurally antagonistic (not jointly diagonalizable) or if the preconditioner update frequency is too low, alignment may degrade.

### Mechanism 2
- **Claim:** SOAP stabilizes the optimization trajectory across consecutive steps (inter-step alignment), reducing oscillatory behavior.
- **Mechanism:** Newton-style updates adapt to the local loss landscape geometry. By correcting for curvature, the update direction changes slowly between steps, resulting in a straighter, more efficient path to the minimum ($S(p_t, p_{t+1}) \to 1$).
- **Core assumption:** The Hessian varies smoothly (Lipschitz continuous) and the step size $\eta$ is sufficiently small.
- **Evidence anchors:**
  - [abstract] "...resolves the... inter-step update conflicts... facilitating faster, more stable convergence."
  - [section V-C] Lemma 1 proves the inter-step cosine bound $|1 - S(p_t, p_{t+1})| \leq C_1 \eta \|p_t\| + \dots$ for Newton steps.
  - [corpus] "Closing the Curvature Gap" discusses the necessity of accurate Hessian characterization for optimization stability.
- **Break condition:** If the learning rate is scaled aggressively relative to the Hessian's Lipschitz constant, the "zigzagging" behavior may persist.

### Mechanism 3
- **Claim:** Second-order training suppresses activation and latent outliers, improving robustness to Post-Training Quantization (PTQ).
- **Mechanism:** SOAP redistributes update energy via rotational mixing in the eigenbasis. Unlike diagonal preconditioners (Adam) which isolate high-variance directions, SOAP diffuses variance across feature dimensions, preventing the concentration of energy that creates heavy tails (outliers).
- **Core assumption:** The linearized local map $X = HW$ holds approximately, and the "input correlation energy" is conserved (Trace identity).
- **Evidence anchors:**
  - [abstract] "...second-order trained models exhibit significantly fewer activation and latent outliers."
  - [section VI-B] Eq. (10) links feature kurtosis to cross-channel correlations; Appendix A-H shows the upper bound for kurtosis growth is lower for SOAP than diagonal methods.
  - [corpus] Evidence for outlier suppression in compression specifically is sparse in the provided corpus; rely on paper's Eq. (11).
- **Break condition:** If the gradient noise is extremely high or the batch size is too small, the curvature estimates may fail to effectively redistribute variance.

## Foundational Learning

- **Concept:** **Rate-Distortion (R-D) Trade-off & Multi-Objective Optimization**
  - **Why needed here:** The paper frames LIC training not as simple loss minimization but as a multi-objective conflict resolution problem. You must understand that Rate ($-\log P(z)$) and Distortion ($d(x, \hat{x})$) pull parameters in different directions.
  - **Quick check question:** Why does a diagonal preconditioner like Adam fail to resolve the conflict when Rate and Distortion gradients are orthogonal but non-axis-aligned?

- **Concept:** **Second-Order Preconditioning (Hessian/Approximate Inverse)**
  - **Why needed here:** The core intervention is switching from $p = -g$ (or diagonal scaled) to $p = -H^{-1}g$. You need to grasp that $H^{-1}$ performs a *rotation* based on curvature, not just a scaling.
  - **Quick check question:** In the context of this paper, does "preconditioning" imply simple coordinate-wise adaptive learning rates (like Adam), or full basis rotation (like Newton)?

- **Concept:** **Kurtosis and Outlier Sensitivity in Quantization**
  - **Why needed here:** A key deployability result is that SOAP reduces kurtosis. High kurtosis implies "heavy tails" (outliers), which breaks low-bit quantization (e.g., Int8).
  - **Quick check question:** How does "redistributing update energy" via rotation lead to lower kurtosis in the latent representation?

## Architecture Onboarding

- **Component map:** Encoder -> Quantizer -> Decoder + Entropy Model -> SOAP Optimizer
- **Critical path:**
  1. Import SOAP optimizer (drop-in replacement).
  2. Set `preconditioner_update_interval` (default 10).
  3. Initialize `lr` (e.g., $2\times10^{-4}$).
  4. Run training loop; monitor intra-step/inter-step cosine scores if debugging convergence.

- **Design tradeoffs:**
  - **Compute:** Higher per-step cost (matrix operations for Kronecker factors) vs. drastically fewer total steps.
  - **Memory:** Negligible overhead (~1% reported) vs. improved model robustness (PTQ).
  - **Stability:** Higher trajectory stability vs. sensitivity to bad hyperparameters (grid search `lr` recommended).

- **Failure signatures:**
  - **Divergence:** Likely due to learning rate being too high for the curvature estimates (try reducing `lr` by 10x).
  - **No Speedup:** Preconditioner not updating frequently enough or Hessian approximation failing (check `preconditioner_update_interval`).
  - **Negative Intra-step Cosine:** Indicates the R-D structure is too complex for the current approximation capacity (unlikely in standard LICs per paper results).

- **First 3 experiments:**
  1. **Baseline Efficiency:** Train ELIC or TCM with Adam vs. SOAP. Plot R-D Loss vs. Wall-Clock Time to verify the 50%+ speedup.
  2. **Trajectory Analysis:** Plot inter-step and intra-step cosine scores (Fig. 4 equivalent) to confirm gradient alignment correlates with speedup.
  3. **PTQ Robustness:** Train two models to convergence (Adam vs. SOAP), quantize to W8A8 (using AdaRound), and measure $\Delta$BD-Rate to verify outlier suppression benefits.

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical alignment conditions (joint diagonalizability of Rate and Distortion Hessians) are asymptotic and may not hold precisely in practice, especially in early training phases.
- The claim about outlier suppression and PTQ robustness relies on indirect evidence (upper bounds on kurtosis growth) rather than direct comparative experiments across architectures.
- The speedup claims depend on the specific SOAP implementation details (damping, update frequency) which are referenced but not fully specified in the paper.

## Confidence
- **High:** SOAP accelerates convergence and reduces training steps across tested LIC architectures (supported by direct comparisons).
- **Medium:** SOAP improves final R-D performance (BD-Rate reduction) when trained for the same steps (supported but requires careful hyperparameter matching).
- **Medium:** SOAP improves PTQ robustness by suppressing outliers (plausible mechanism, but empirical evidence is less direct).

## Next Checks
1. **Reproduce Step Reduction:** Train ELIC with Adam vs. SOAP on COCO, measuring steps-to-convergence on Kodak validation set.
2. **Validate Outlier Suppression:** Compare activation/latent kurtosis distributions between Adam and SOAP-trained models (using Eq. (10) analysis).
3. **Test Preconditioner Frequency:** Vary the preconditioner update interval (e.g., 5, 10, 20 steps) to confirm the claimed 1% overhead is accurate.