---
ver: rpa2
title: Faithful and Stable Neuron Explanations for Trustworthy Mechanistic Interpretability
arxiv_id: '2512.18092'
source_url: https://arxiv.org/abs/2512.18092
tags:
- neuron
- concept
- identification
- probe
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the problem of quantifying the trustworthiness
  of neuron identification methods in mechanistic interpretability. It focuses on
  two key challenges: (1) faithfulness, i.e., whether identified concepts truly capture
  neuron functions, and (2) stability, i.e., consistency across probing datasets.'
---

# Faithful and Stable Neuron Explanations for Trustworthy Mechanistic Interpretability

## Quick Facts
- arXiv ID: 2512.18092
- Source URL: https://arxiv.org/abs/2512.18092
- Reference count: 40
- This work provides theoretical foundations for trustworthy neuron explanation methods in mechanistic interpretability

## Executive Summary
This work addresses the critical challenge of quantifying the trustworthiness of neuron identification methods in mechanistic interpretability by focusing on two key aspects: faithfulness and stability. The authors observe that neuron identification can be viewed as an inverse process of machine learning, which enables them to derive theoretical guarantees for these properties. They provide the first theoretical analysis of faithfulness by deriving generalization bounds for similarity metrics, showing that the gap between empirical and true similarity scales as O(1/√n). The paper introduces a bootstrap ensemble method to quantify stability and construct concept prediction sets with guaranteed coverage probability.

## Method Summary
The authors develop a framework that treats neuron identification as an inverse learning problem, allowing them to apply statistical learning theory to derive generalization bounds. They analyze faithfulness by examining the relationship between empirical and true similarity metrics (accuracy, AUROC, IoU) under a fixed probing dataset, establishing that the generalization gap scales as O(1/√n). For stability, they propose a bootstrap ensemble method that quantifies consistency across different probing datasets and constructs concept prediction sets with guaranteed coverage probability. The framework is validated through experiments on synthetic data and real models (MLP, CNN), demonstrating how probing dataset size and concept frequency impact performance.

## Key Results
- Derived theoretical generalization bounds for neuron identification faithfulness showing O(1/√n) scaling
- Proposed bootstrap ensemble method for quantifying stability with guaranteed coverage probability
- Experiments validate that probing dataset size and concept frequency significantly impact performance

## Why This Works (Mechanism)
The paper's approach works by recognizing that neuron identification is fundamentally an inverse problem to standard machine learning. When we train a model to identify concepts from neuron activations, we're essentially learning a function from a limited probing dataset. By applying statistical learning theory to this inverse problem, the authors can derive generalization bounds that quantify how well empirical similarity measures (computed on the probing dataset) approximate the true similarity with respect to the underlying data distribution. The bootstrap ensemble method works by sampling multiple probing datasets and aggregating results, which provides a principled way to quantify stability while maintaining theoretical guarantees on coverage probability.

## Foundational Learning
- **Generalization bounds**: Why needed - to quantify how well empirical similarity measures approximate true similarity; Quick check - verify O(1/√n) scaling empirically matches theoretical predictions
- **Bootstrap sampling**: Why needed - to quantify stability across different probing datasets; Quick check - confirm that coverage probability matches theoretical guarantees
- **Concept prediction sets**: Why needed - to provide uncertainty quantification for identified concepts; Quick check - verify that sets contain true concepts at claimed frequency

## Architecture Onboarding
- **Component map**: Neuron activation data -> Probing dataset construction -> Concept identification model -> Similarity metric computation -> Generalization bound analysis
- **Critical path**: The theoretical analysis of faithfulness bounds depends on the assumption that the probing dataset distribution matches the true data distribution
- **Design tradeoffs**: The O(1/√n) scaling provides theoretical guarantees but may be conservative in practice; bootstrap method trades computational cost for stability quantification
- **Failure signatures**: Distribution mismatch between probing and true data violates theoretical assumptions; low concept frequency in probing data leads to unstable identification
- **First experiments**: 1) Verify generalization bounds on synthetic data with known ground truth, 2) Test bootstrap stability quantification on varying dataset sizes, 3) Evaluate concept prediction sets coverage probability

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several natural extensions emerge from the work, including application to transformer architectures, handling distribution shifts, and extending the framework to more complex concept identification scenarios.

## Limitations
- Theoretical guarantees assume probing dataset distribution matches true data distribution
- Experiments limited to MLP and CNN architectures, lacking transformer-based model validation
- Practical utility of concept prediction sets for real-world interpretability workflows remains unexplored

## Confidence
- Generalization bounds: Medium - theoretical derivation appears sound but relies on strong assumptions about data distribution
- Bootstrap ensemble method: Medium - shows promise but requires broader validation across architectures
- Experimental validation: Medium - demonstrates claims on limited model types and concept categories

## Next Checks
1. Apply the framework to transformer-based models to assess generalizability across architectures
2. Test the stability bounds with datasets containing concept distribution shifts to evaluate robustness to distribution mismatch
3. Compare the bootstrap ensemble method against alternative stability metrics like agreement rates and consistency scores on a broader range of interpretability tasks