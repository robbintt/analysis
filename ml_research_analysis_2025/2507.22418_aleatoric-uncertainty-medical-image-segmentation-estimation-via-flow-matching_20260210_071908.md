---
ver: rpa2
title: Aleatoric Uncertainty Medical Image Segmentation Estimation via Flow Matching
arxiv_id: '2507.22418'
source_url: https://arxiv.org/abs/2507.22418
tags:
- segmentation
- uncertainty
- image
- flow
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a conditional flow matching framework for
  quantifying aleatoric uncertainty in medical image segmentation. Unlike diffusion
  models that inject noise and approximate distributions, the proposed method directly
  learns a velocity field conditioned on input images and expert annotations, preserving
  fine anatomical details while capturing variability in multi-expert segmentations.
---

# Aleatoric Uncertainty Medical Image Segmentation Estimation via Flow Matching

## Quick Facts
- **arXiv ID:** 2507.22418
- **Source URL:** https://arxiv.org/abs/2507.22418
- **Reference count:** 26
- **Primary result:** Introduces conditional flow matching framework achieving Dice scores of 0.595 (LIDC-IDRI) and 0.785 (MMIS), outperforming Prob-UNet, PHiSeg, and CIMD baselines

## Executive Summary
This paper presents a conditional flow matching framework for quantifying aleatoric uncertainty in medical image segmentation. The method learns a velocity field that transforms samples from a Gaussian prior to segmentation distributions conditioned on expert annotations, avoiding the noise injection of diffusion models. By generating multiple segmentation samples and computing pixel-wise variance, the approach captures expert disagreement as uncertainty while preserving fine anatomical details. Evaluated on LIDC-IDRI and MMIS datasets with multi-expert annotations, the method demonstrates superior performance over existing uncertainty quantification approaches while explicitly focusing on aleatoric rather than epistemic uncertainty.

## Method Summary
The proposed method uses conditional flow matching to estimate aleatoric uncertainty in medical image segmentation. A UNet learns a velocity field u_θ(t,S,X) conditioned on input images X and expert annotations S^(e). During training, the model learns to transform samples from a Gaussian prior p_0(S) to expert segmentations via an ODE, using a conditional probability path p_t(S|S^(e),X) = N(S; tS^(e), (1-t)²I). At inference, M=15 samples are generated by integrating the guided velocity field with classifier-free guidance (w=0.3) using a midpoint ODE solver. Pixel-wise variance across samples provides uncertainty maps reflecting expert disagreement. The approach is evaluated on LIDC-IDRI and MMIS datasets, achieving Dice scores of 0.595 and 0.785 respectively while outperforming baseline methods.

## Key Results
- Achieved Dice scores of 0.595 on LIDC-IDRI and 0.785 on MMIS datasets
- Outperformed Prob-UNet, PHiSeg, and CIMD baselines in most evaluation metrics
- Generated uncertainty maps aligned with ambiguous regions in expert annotations
- Demonstrated ability to capture multimodal distributions in multi-expert segmentations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Flow matching preserves fine anatomical details during uncertainty estimation better than diffusion-based approaches by avoiding stochastic noise injection.
- **Mechanism:** The method learns a deterministic velocity field u_θ(t,S,X) that directly transforms samples from a Gaussian prior p_0(S) to the target segmentation distribution q(S|X) via an ODE, rather than iteratively denoising corrupted samples.
- **Core assumption:** The continuous, diffeomorphic transform between distributions preserves local structure that diffusion's noise injection would otherwise obscure.
- **Evidence anchors:**
  - [abstract] "Unlike diffusion models that inject noise and approximate distributions, the proposed method directly learns a velocity field conditioned on input images and expert annotations"
  - [section 2] "because the flow is designed to interpolate between source and target distribution, the resulting samples more faithfully reflect the true underlying uncertainty—without the potential over-smoothing or blurring that can arise from the stochastic noise in diffusion processes"
  - [corpus] Symmetrical Flow Matching paper (arXiv:2506.10634) demonstrates unified generation-segmentation via flow matching with FMR=0.538, supporting the architectural viability

### Mechanism 2
- **Claim:** Conditioning on individual expert annotations enables the model to capture inter-annotator variability as aleatoric uncertainty.
- **Mechanism:** During training, each forward pass randomly samples one expert segmentation S^(e) from the available E annotations, conditioning the probability path p_t(S|S^(e),X) = N(S; tS^(e), (1-t)²I). This exposes the model to diverse plausible interpretations.
- **Core assumption:** Expert disagreement reflects true ambiguity in the data rather than annotation error or negligence.
- **Evidence anchors:**
  - [abstract] "pixel-wise variance reflecting expert disagreement"
  - [section 2, Eq. 3-5] Defines conditional probability path p_t(S|S^(e),X) with boundary conditions at t=0 (Gaussian) and t=1 (expert annotation)
  - [section 3] "During training, a random annotation mask is selected each time a data point is sampled, exposing the model to diverse expert interpretations"

### Mechanism 3
- **Claim:** Classifier-free guidance balances anatomical fidelity with segmentation diversity.
- **Mechanism:** The velocity field is trained in both conditional (u_θ(t,S,X)) and unconditional (u_θ(t,S,∅)) modes via random dropout. At inference, guided velocity u_guided = u_θ + w(u_θ - u_θ|∅) amplifies image-conditioned predictions while preserving sample diversity.
- **Core assumption:** A single guidance weight w=0.3 provides adequate conditioning across all anatomical regions and ambiguity levels.
- **Evidence anchors:**
  - [section 2, Eq. 9] "u_guided_θ(t,S,X) = u_θ(t,S,X) + w(u_θ(t,S,X) - u_θ(t,S,∅))"
  - [section 3] "A classifier-free guidance scale of 0.3 is applied... the image conditioning is randomly dropped with a fixed probability of 0.5"

## Foundational Learning

- **Concept: Ordinary Differential Equations (ODEs) and numerical integration**
  - **Why needed here:** Flow matching requires integrating d/dt ψ_t(S_0) = u_θ(t, ψ_t(S_0), X) from t=0 to t=1 using numerical methods (midpoint method in this paper).
  - **Quick check question:** Can you explain why the midpoint method offers better stability than Euler's method for integrating velocity fields?

- **Concept: Aleatoric vs. Epistemic Uncertainty**
  - **Why needed here:** This paper exclusively targets aleatoric uncertainty (data-inherent ambiguity) and explicitly does not address epistemic uncertainty (model parameter uncertainty).
  - **Quick check question:** If you observed high uncertainty on a medical image from a scanner not represented in training data, which type of uncertainty would this likely represent?

- **Concept: Conditional probability paths and optimal transport**
  - **Why needed here:** The method constructs probability paths p_t(S|S^(e),X) = N(S; tS^(e), (1-t)²I) that interpolate between Gaussian prior and expert annotations.
  - **Quick check question:** Why does the variance (1-t)²I decrease as t→1, and what would happen if variance remained constant?

## Architecture Onboarding

- **Component map:** Input image → UNet encoder → Time-conditioned velocity prediction → ODE integration (100 steps) → Sample S_1 → Repeat M=15 times → Pixel-wise variance computation

- **Critical path:** Input image → UNet encoder → Time-conditioned velocity prediction → ODE integration (100 steps) → Sample S_1 → Repeat M=15 times → Pixel-wise variance computation

- **Design tradeoffs:**
  - **Sample count (M):** More samples improve uncertainty estimation (Table 1 shows GED↓ and Dice↑ from M=5 to M=15) but increase inference cost linearly
  - **ODE step size:** 0.01 provides balance between stability and speed; larger steps may introduce integration error
  - **Guidance weight w=0.3:** Lower values preserve diversity; higher values increase anatomical fidelity but may collapse to deterministic predictions
  - **Resolution 128×128:** Standardized across datasets; higher resolutions require more compute and may need architectural modifications

- **Failure signatures:**
  - **Collapsed diversity:** If all M samples are nearly identical, guidance weight may be too high or model undertrained
  - **Overconfident uncertainty:** Low variance in clearly ambiguous regions suggests insufficient training or inadequate expert variability in data
  - **Blurred boundaries:** If segmentation edges are fuzzy across all samples, velocity field may not have converged or ODE step size too large
  - **OOD insensitivity:** Method explicitly does not capture epistemic uncertainty; will show low uncertainty on out-of-distribution inputs

- **First 3 experiments:**
  1. **Reproduction baseline:** Train on LIDC-IDRI with default hyperparameters (lr=10⁻⁴, batch=64, 100K iterations); verify Dice ≈ 0.595 with M=15 samples. Check that generated samples visually match inter-expert variability.
  2. **Ablation on guidance weight:** Test w ∈ {0.0, 0.1, 0.3, 0.5, 1.0} to observe tradeoff between anatomical alignment (Dice) and diversity (sample variance). Hypothesis: w=0.3 is optimal; w=0.0 produces misaligned samples; w=1.0 collapses diversity.
  3. **Sample efficiency analysis:** Vary M ∈ {1, 3, 5, 10, 15, 20} and plot GED/Dice curves. Confirm diminishing returns beyond M=15 to justify inference budget.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the conditional flow matching framework be extended to incorporate epistemic uncertainty to handle out-of-distribution samples or limited training data?
- **Basis:** [explicit] The authors explicitly state in the Limitations and Conclusion that the current method focuses solely on aleatoric uncertainty and neglects epistemic uncertainty, which limits reliability in data-scarce scenarios.
- **Why unresolved:** The current velocity field is deterministic regarding model parameters; capturing epistemic uncertainty requires integrating uncertainty over the model weights or architecture, which the present formulation does not support.
- **What evidence would resolve it:** A modified architecture or training objective that successfully quantifies both aleatoric and epistemic uncertainty, demonstrated via improved calibration on out-of-distribution test sets.

### Open Question 2
- **Question:** What advanced sampling techniques or integration strategies can reduce the computational cost and noise sensitivity of the ODE-based sampling process for high-resolution images?
- **Basis:** [explicit] The Limitations section notes that the current sampling strategy (random drawing and midpoint ODE integration) is "computationally demanding" and "sensitive to noise levels," identifying efficient sampling as a target for future work.
- **Why unresolved:** Flow matching relies on solving an ODE numerically, which is inherently slower and potentially less stable than single-pass inference methods, specifically as resolution increases.
- **What evidence would resolve it:** The implementation of a faster solver or learned prior that maintains segmentation fidelity (Dice) while significantly reducing inference time and variance on high-resolution inputs.

### Open Question 3
- **Question:** Why does the proposed method underperform Prob-UNet on the Generalized Energy Distance (GED) metric for the MMIS dataset, and does this indicate a trade-off between sample fidelity and distributional coverage?
- **Basis:** [inferred] In Table 1, the proposed method achieves a GED of 0.231 on the MMIS dataset, which is marginally higher (worse) than Prob-UNet's 0.227, despite the proposed method showing superior Dice scores.
- **Why unresolved:** The paper acknowledges the lower score but does not analyze why a method that better captures "exact density" would lag in this specific distributional metric compared to a VAE-based baseline.
- **What evidence would resolve it:** An analysis of the diversity of generated samples versus their accuracy, determining if the flow matching prioritizes local anatomical correctness (Dice) over the global variance required to minimize GED.

## Limitations
- Only captures aleatoric uncertainty, not epistemic uncertainty for out-of-distribution samples
- Requires multiple expert annotations per case, which may not be available in many clinical datasets
- Fixed guidance weight and ODE parameters may not be optimal across all anatomical regions

## Confidence

- **High confidence:** The core flow matching mechanism (Mechanism 1) is well-established in the literature and the experimental results showing Dice scores of 0.595 and 0.785 on LIDC-IDRI and MMIS datasets respectively provide strong empirical support for the approach's viability.

- **Medium confidence:** The conditioning strategy on individual expert annotations (Mechanism 2) is theoretically sound and shows improved performance over baselines, but requires datasets with genuine multi-expert variability to realize its full potential.

- **Medium confidence:** The classifier-free guidance implementation (Mechanism 3) follows established practices, though the fixed guidance weight may not be optimal across all use cases and anatomical regions.

## Next Checks

1. **OOD sensitivity test:** Evaluate the model on images from different scanners or acquisition protocols not present in training data to confirm it does not capture epistemic uncertainty, as this represents a critical limitation for clinical deployment.

2. **Guidance weight sensitivity analysis:** Systematically vary the guidance weight w across a wider range (0.0-1.0) on both datasets to determine if the fixed value of 0.3 is truly optimal or if adaptive guidance would improve results.

3. **Epistemic uncertainty extension:** Implement a simple epistemic uncertainty estimation method (e.g., Monte Carlo dropout or ensemble predictions) to compare against the aleatoric uncertainty maps and assess whether combining both types of uncertainty would provide clinically meaningful improvements.