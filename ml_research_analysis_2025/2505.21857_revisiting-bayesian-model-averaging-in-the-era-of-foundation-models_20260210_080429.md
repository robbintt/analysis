---
ver: rpa2
title: Revisiting Bayesian Model Averaging in the Era of Foundation Models
arxiv_id: '2505.21857'
source_url: https://arxiv.org/abs/2505.21857
tags:
- weights
- training
- classification
- datasets
- posterior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work revisits Bayesian model averaging (BMA) to ensemble pre-trained
  and/or lightly-finetuned foundation models for image and text classification. To
  make BMA tractable, the authors freeze foundation model parameters and train only
  lightweight linear classifiers, treating them as the learnable components in BMA.
---

# Revisiting Bayesian Model Averaging in the Era of Foundation Models

## Quick Facts
- **arXiv ID**: 2505.21857
- **Source URL**: https://arxiv.org/abs/2505.21857
- **Reference count**: 40
- **Primary result**: Bayesian and optimizable model averaging improves ensemble performance of frozen foundation models by up to 3.78% on ImageNet variants and 3.11% on GLUE benchmarks.

## Executive Summary
This work introduces a practical approach to Bayesian model averaging (BMA) for ensembling pre-trained foundation models by training only lightweight linear classifiers on frozen features. The authors propose two methods: BMA, which computes principled model weights via Laplace-approximated marginal likelihoods, and optimizable model averaging (OMA), which directly minimizes prediction entropy. Applied to image and text classification tasks, these approaches achieve significant improvements over simple output averaging while requiring minimal computation compared to full fine-tuning. The framework is particularly effective for combining zero-shot and trained models, demonstrating strong performance under distribution shift.

## Method Summary
The method freezes pre-trained foundation model parameters and trains lightweight linear classifiers as the only learnable components. For BMA, model weights are derived from Laplace-approximated marginal likelihoods using block-diagonal Hessian approximations, balancing data fit against model complexity. OMA directly optimizes ensemble weights by minimizing expected prediction entropy on validation data, without requiring labeled training data. Both methods produce weighted predictions from multiple feature extractors, enabling principled model ensembling with minimal computational overhead compared to full fine-tuning.

## Key Results
- BMA improves over output averaging by up to 3.78% on challenging ImageNet variants (ImageNet-R, ImageNet-Sketch)
- OMA improves over output averaging by up to 3.11% on GLUE MRPC and 2.4% on ObjectNet
- Combining zero-shot and MAP estimates with OMA outperforms single models and reaches comparable accuracy to fully fine-tuned methods
- The approach requires only linear classifiers to be trained, making it accessible with modest GPU resources

## Why This Works (Mechanism)

### Mechanism 1
Freezing foundation model parameters and training only lightweight linear classifiers makes Bayesian model averaging computationally tractable while preserving principled ensemble weighting. The approach treats frozen feature extractors as fixed representations and applies Bayesian inference only to the linear classifiers, reducing the parameter space from billions to thousands and enabling computation of model posteriors via Laplace approximation. This assumes pre-trained foundation model features are sufficiently rich and task-relevant for linear classification boundaries.

### Mechanism 2
Block-diagonal Laplace approximation enables tractable marginal likelihood computation for high-dimensional linear classifiers by exploiting Hessian structure in multi-class logistic regression. The approximation is justified because diagonal Hessian elements are orders of magnitude larger than off-diagonals, avoiding storage of million×million matrices. This assumes off-diagonal Hessian terms are negligible enough that the approximation preserves accurate model posteriors.

### Mechanism 3
Directly optimizing ensemble weights by minimizing expected prediction entropy (OMA) outperforms simple output averaging, particularly under distribution shift where training-data posteriors become unreliable. Based on the theoretical result that BMA predictions have entropy ≤ any single model, OMA minimizes average expected entropy plus regularization. Unlike BMA, OMA does not require labeled training data and can incorporate zero-shot models without posteriors, assuming lower expected entropy on unlabeled validation data correlates with better predictive accuracy.

## Foundational Learning

- **Marginal Likelihood (Model Evidence)**: Central to BMA—marginal likelihoods determine model posterior weights that weight ensemble predictions. *Quick check*: Can you explain why marginal likelihood naturally balances model fit against model complexity without explicit regularization tuning?
- **Laplace Approximation**: Provides tractable Gaussian approximation to intractable posteriors, enabling marginal likelihood estimation for high-dimensional classifiers. *Quick check*: What property of the Hessian (shown in Fig. 1) justifies the block-diagonal approximation used here?
- **KL Divergence and Entropy**: Lemma 2.1's proof relies on D_KL(P||Q) ≥ 0, establishing that BMA predictions have lower entropy than any single model—the theoretical basis for OMA. *Quick check*: Why does minimizing expected entropy not guarantee improved accuracy on out-of-distribution data?

## Architecture Onboarding

- **Component map**:
  ```
  Input Image → [Feature Extractor ϕ_1] → Features → [Linear Classifier w_1] → Logits
             → [Feature Extractor ϕ_L] → Features → [Linear Classifier w_L] → Logits
                                                                                        ↓
                                                              [Model Weights: BMA posteriors OR OMA β] 
                                                                                        ↓
                                                                              Weighted Prediction
  ```

- **Critical path**:
  1. Pre-processing: Extract frozen features from all training images via each foundation model (9–24 hours on RTX 4090 for ImageNet-1K)
  2. MAP estimation: Train linear classifiers with L2 regularization (Eq. 6)—closed-form gradient, <2 minutes
  3. BMA weights: Compute block-diagonal Hessian, marginal likelihoods (Eq. 7), normalize to posteriors (Eq. 8)—~3 hours per model
  4. OMA weights (alternative): Initialize β₀, optimize Eq. 11 via gradient descent with softplus constraint
  5. Inference: Compute weighted prediction (Eq. 1 or Eq. 12)

- **Design tradeoffs**:
  - **BMA vs. OMA**: BMA provides principled uncertainty quantification but requires labeled training data and reliable train-test distribution match; OMA is more flexible (works with zero-shot, unlabeled data) but doesn't explicitly model epistemic uncertainty
  - **Number of feature extractors**: More models improve performance but increase inference cost linearly
  - **Prior variance α (BMA)**: Controls regularization strength; optimal values vary by dataset (α=80 for ImageNet-1K, α=10 for Flowers102, α=100 for Sun397)
  - **Regularization λ (OMA)**: Controls deviation from prior β₀; optimal values range from 0.1 to 10,000 depending on dataset

- **Failure signatures**:
  - BMA underperforming on OOD data: Model posteriors computed on training data don't transfer—switch to OMA or combine zero-shot with MAP (Table 2, blue rows)
  - OMA overfitting validation set: Increase λ to strengthen regularization toward β₀
  - Memory errors on ImageNet-1K: Use mini-batch Hessian computation (Eq. 16–18) with subsampling
  - Zero-shot outperforming MAP on specific datasets: Indicates large domain shift (Table 4)—use OMA with both zero-shot and MAP models

- **First 3 experiments**:
  1. Reproduce BMA improvement on ImageNet-1K: Train MAP classifiers on 8 OpenCLIP features, compute BMA posteriors, verify ~1.5% improvement over output averaging (Table 2, pink rows)
  2. Test OMA on ImageNet-R with zero-shot models: Verify that zero-shot + OMA outperforms zero-shot + output averaging by ~2% (Table 2, yellow rows)—demonstrates OMA works without any training labels
  3. Ablation on prior variance α: Train BMA with α ∈ {0.01, 0.1, 10, 50, 100} on Camelyon17; verify optimal around α=10 (Table 9) to understand sensitivity to this hyperparameter

## Open Questions the Paper Calls Out

- **How does the Mixture of Experts (ME) formulation compare to BMA for ensembling foundation models in this setting?**: The authors note conceptual similarities between BMA and ME for model merging but did not empirically compare them; they use Laplace approximation rather than the EM algorithm typical for ME. Empirical comparison of ME and BMA performance on the same image/text classification benchmarks, using EM-based optimization for ME weights versus Laplace-approximated BMA, would resolve this.

- **Can BMA with lightweight linear classifiers be effectively extended to domains beyond image classification where foundation model feature quality is uncertain?**: The paper demonstrated BMA primarily on image classification with OpenCLIP features and only applied OMA (not BMA) to text classification; feature quality may vary substantially across modalities. Systematic evaluation of BMA performance across diverse domains (audio, video, multimodal) with analysis of how feature quality correlates with BMA effectiveness would resolve this.

- **How does OMA's lack of explicit epistemic uncertainty modeling affect robustness under severe distribution shift?**: While OMA performed well on OOD datasets in experiments, the theoretical implications of ignoring epistemic uncertainty remain unclear, particularly for extreme or adversarial distribution shifts. Comparative analysis of BMA vs OMA uncertainty calibration and predictive reliability metrics under controlled distribution shifts of varying severity would resolve this.

## Limitations
- The Laplace approximation relies on strong assumptions about posterior normality and block-diagonal Hessian structure that may not hold for all feature architectures or domains
- Computational requirements for ImageNet-scale Hessian computation (~3 hours per model on RTX 4090) remain substantial, and the memory-efficient subsampling approach lacks specific implementation details
- While the theoretical justification for BMA's entropy minimization is rigorous, the empirical correlation between validation entropy and test accuracy under distribution shift is assumed rather than proven

## Confidence

- **High confidence**: Linear classifier training with frozen features (well-established transfer learning practice), OMA improving over simple averaging on multiple benchmarks
- **Medium confidence**: BMA's Laplace approximation validity (supported by Fig. 1 but not rigorously tested across domains), optimal prior variance hyperparameter values (sensitive to dataset characteristics)
- **Low confidence**: Transferability of BMA weights from training to test distributions (critical limitation noted but not systematically evaluated)

## Next Checks

1. Test BMA on a domain where pre-trained features are known to be suboptimal (e.g., medical imaging with natural-image models) to verify the "break condition" where linear classifiers fail regardless of ensemble weighting
2. Systematically evaluate the Hessian block-diagonal approximation quality across different foundation model architectures (varying feature correlation structures) to quantify approximation error
3. Compare OMA's entropy minimization to explicit accuracy optimization on validation sets across multiple distribution shifts to measure the correlation between entropy reduction and accuracy improvement