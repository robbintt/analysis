---
ver: rpa2
title: Code Aesthetics with Agentic Reward Feedback
arxiv_id: '2510.23272'
source_url: https://arxiv.org/abs/2510.23272
tags:
- score
- aesthetics
- webpage
- design
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of code aesthetics and proposes
  a multi-agent reward framework (GRPO-AR) to improve the aesthetic quality of LLM-generated
  code. AesCode-358K, a large-scale dataset, and OpenDesign, a benchmark for webpage
  design, are introduced.
---

# Code Aesthetics with Agentic Reward Feedback

## Quick Facts
- arXiv ID: 2510.23272
- Source URL: https://arxiv.org/abs/2510.23272
- Reference count: 40
- Key outcome: Introduces code aesthetics and multi-agent reward framework (GRPO-AR) to improve LLM-generated code aesthetics; AesCoder-4B surpasses GPT-4o and GPT-4.1 on PandasPlotBench and OpenDesign benchmarks.

## Executive Summary
This paper introduces the concept of code aesthetics for visually-oriented coding tasks like plot generation and webpage design. It proposes AesCode-358K, a large-scale dataset for training aesthetic sense, and OpenDesign, a benchmark for webpage design evaluation. The core contribution is GRPO-AR, a multi-agent reward framework that evaluates executability, static aesthetics, and interactive aesthetics using specialized agents (Execution, Static, and Interactive). AesCoder-4B and AesCoder-7B models are trained with supervised fine-tuning followed by reinforcement learning using GRPO-AR, achieving results competitive with models up to 685B parameters.

## Method Summary
The method uses a two-stage pipeline: Stage I involves supervised fine-tuning on AesCode-358K (158K plot + 200K webpage samples) using a Qwen-based LLM. Stage II applies reinforcement learning with GRPO-AR, which samples groups of outputs and calculates advantages based on normalized agentic rewards. The agentic reward framework consists of three specialized agents: an Execution Agent checks syntax/runnability, a Static Aesthetics Agent (VLM like GPT-5) scores rendered screenshots, and an Interactive Aesthetics Agent (GUI agent) navigates webpages to verify dynamic elements. The model is trained on Qwen3-4B-Instruct-2507 and Qwen2.5-Coder-7B-Instruct base models.

## Key Results
- AesCoder-4B achieves state-of-the-art performance on PandasPlotBench and OpenDesign benchmarks.
- AesCoder-4B surpasses GPT-4o and GPT-4.1, achieving results competitive with models up to 685B parameters.
- AesCode-358K and OpenDesign are introduced as new resources for code aesthetics research.

## Why This Works (Mechanism)

### Mechanism 1: Agentic Reward Decomposition
Decomposing code evaluation into execution, static, and interactive dimensions allows optimization for visual and functional qualities that standard text-based rewards miss. The system uses three parallel agents to provide granular feedback that correlates better with human aesthetic preferences than code-text similarity alone.

### Mechanism 2: Group-Relative Policy Optimization (GRPO-AR)
GRPO-AR enables effective policy updates without training a separate value network by comparing relative rewards within a batch of outputs. This approach calculates advantages by normalizing rewards within a single prompt's output group, providing meaningful variance for optimization.

### Mechanism 3: Curriculum of Distillation and Search
Performance relies on a two-stage pipeline where the model first distills aesthetic knowledge via SFT on high-quality data, then refines its boundaries via RL exploration. This approach establishes a robust foundation through SFT before allowing the model to explore solutions beyond the SFT distribution.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed: GRPO-AR is the core RL engine used in the paper.
  - Quick check: Can you explain why GRPO removes the need for a Critic model compared to PPO?

- **Concept: LLM-as-a-Judge**
  - Why needed: The Static Aesthetics Agent is explicitly a VLM (GPT-5) prompted to act as a judge.
  - Quick check: How do you prompt a model to output a structured JSON score rather than open-ended text?

- **Concept: GUI Agents**
  - Why needed: The Interactive Aesthetics Agent uses a web agent to interact with HTML.
  - Quick check: What is the failure mode of a GUI agent when it encounters a non-standard interactive element?

## Architecture Onboarding

- **Component map**: Policy (AesCoder) -> Execution Agent (Sandbox + Syntax Checker) -> Static Agent (Screenshot + VLM Judge) -> Interactive Agent (GUI Agent) -> GRPO-AR Trainer
- **Critical path**: The Reward Generation Loop is the bottleneck, requiring execution, rendering, screenshot, VLM query, and GUI agent interaction before each RL step.
- **Design tradeoffs**: Accuracy vs. Cost (using GPT-5/GPT-4o makes RL loop expensive); Robustness vs. Signal (capping GUI interactions at 3 saves time but might miss deep usability bugs).
- **Failure signatures**: Reward Hacking (generating text that looks like code but renders as blank pages), Agent Timeout (GUI agent getting stuck), Execution Drift (generating code with non-whitelisted libraries).
- **First 3 experiments**: 1) Metric Correlation Check (validate agentic reward assumption), 2) Ablation on Reward Weights (verify interactive signal contribution), 3) Over-optimization Test (check for rising static scores with falling execution scores).

## Open Questions the Paper Calls Out

### Open Question 1
How can the Interactive Aesthetics Agent be refined to distinguish between a failure of the webpage interactivity and a failure of the agent itself? The current framework treats agent inability to navigate identically to webpages lacking interactivity, creating noise in the RL signal.

### Open Question 2
Can the multi-modal, multi-agent reward system be distilled into a more computationally efficient model to facilitate wider adoption? The resource-intensive pipeline relying on heavyweight proprietary models may limit scalability.

### Open Question 3
Does the strict scoring criteria used for static aesthetics bias the model against minimalist or prototyping design styles? The paper's optimization for specific aesthetic definitions may conflict with user preferences for minimal interfaces.

## Limitations
- AesCode-358K dataset and OpenDesign benchmark are not publicly released, requiring reconstruction.
- GPT-5 used for static judging and data generation has no open alternative with equivalent capability.
- GUI agent has acknowledged low success rates, potentially causing incorrect interactive scores or training instability.

## Confidence

- **High**: Overall framework design and reported performance improvements on benchmarks.
- **Medium**: Effectiveness of AesCode-358K dataset for training aesthetic sense.
- **Low**: Robustness of interactive agent component and long-term generalization beyond training domains.

## Next Checks
1. Run Static and Interactive Agents on a small held-out set and correlate their scores with human rankings to validate the agentic reward assumption.
2. Experiment with reward weights to verify if interactive signals are actually contributing to the Good Rate or if static visuals dominate.
3. Run GRPO-AR for excessive steps and check if Static scores keep rising while Execution scores drop, indicating over-optimization.