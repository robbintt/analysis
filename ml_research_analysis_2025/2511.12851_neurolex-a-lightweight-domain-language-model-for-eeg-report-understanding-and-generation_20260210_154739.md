---
ver: rpa2
title: 'NeuroLex: A Lightweight Domain Language Model for EEG Report Understanding
  and Generation'
arxiv_id: '2511.12851'
source_url: https://arxiv.org/abs/2511.12851
tags:
- language
- dapt
- neurolex
- linguistic
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NeuroLex, a lightweight domain-specific language
  model designed to interpret and generate clinical EEG reports. It addresses the
  gap between general-purpose language models and the specialized linguistic conventions
  of EEG documentation, which is critical for brain-computer interface (BCI) systems.
---

# NeuroLex: A Lightweight Domain Language Model for EEG Report Understanding and Generation

## Quick Facts
- **arXiv ID:** 2511.12851
- **Source URL:** https://arxiv.org/abs/2511.12851
- **Authors:** Kang Yin; Hye-Bin Shin
- **Reference count:** 27
- **Primary result:** Lightweight domain-specific LM achieves strong performance on EEG report understanding and generation tasks, including information extraction (F1 up to 0.575) and summarization (Fact-F1 up to 0.941).

## Executive Summary
This paper introduces NeuroLex, a lightweight domain-specific language model designed to interpret and generate clinical EEG reports. It addresses the gap between general-purpose language models and the specialized linguistic conventions of EEG documentation, which is critical for brain-computer interface (BCI) systems. NeuroLex is trained in two stages: first, on a large corpus of EEG reports using span-corruption pretraining to learn domain-specific terminology and syntax, then fine-tuned with instruction-style tasks such as report polishing, summarization, and question answering. Evaluations show that NeuroLex outperforms general models in perplexity, information extraction (F1 up to 0.575), summarization consistency (Fact-F1 up to 0.941), and robustness to negation errors. It also demonstrates better label efficiency, achieving high performance with as little as 1% of labeled data. NeuroLex provides a clinically reliable linguistic backbone for interpretable and language-driven neural decoding in BCI research.

## Method Summary
NeuroLex employs a two-stage training approach on the T5-Base architecture. First, it performs domain-adaptive pretraining (DAPT) using span-corruption objectives on ~70K EEG reports (~800K paragraphs) from the Harvard Electroencephalography Database (HEEDB). The span-corruption masks ~15% of tokens, particularly targeting EEG-specific terminology, forcing the model to reconstruct missing spans using surrounding context. Second, it applies supervised fine-tuning (SFT) with three instruction-style tasks (report polishing, question answering, summarization) using LLM-generated pseudo-labels. The model uses a custom EEG-domain tokenizer trained on the corpus to reduce out-of-vocabulary tokens and improve lexical coverage of clinical terms.

## Key Results
- NeuroLex achieves information extraction F1 scores up to 0.575, outperforming general models in attribute-specific extraction (Localization 0.621, Frequency 0.678).
- Summarization consistency reaches Fact-F1 of 0.941, with strong negation robustness (Neg-Adv F1 of 0.72).
- The model demonstrates label efficiency, maintaining high performance with only 1% of labeled data compared to baselines.
- EEG-specific tokenization reduces out-of-vocabulary rate from 49.04% to 10.19% and improves average subwords per token from 2.29 to 1.13.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Span-corruption pretraining on EEG-specific terminology improves domain vocabulary acquisition and contextual reconstruction.
- **Mechanism:** By masking ~15% of tokens—specifically targeting EEG terminology—the model is forced to reconstruct missing spans using surrounding clinical context, learning co-occurrence patterns like "spike-and-wave discharges" or "focal slowing" as structured units rather than isolated tokens.
- **Core assumption:** EEG terminology follows predictable syntactic and semantic patterns that can be internalized through reconstruction pressure.
- **Evidence anchors:**
  - [abstract] "Using span-corruption pretraining... NeuroLex learns the syntax and reasoning patterns characteristic of EEG interpretation."
  - [section] Table III shows DAPT→SFT achieves Top-1 reconstruction accuracy of 74.60% vs. 2.60% for baseline Flan-T5.
  - [corpus] Limited direct validation; neighboring papers on biomedical LMs (e.g., BALI) suggest domain-adaptive pretraining benefits but do not confirm EEG-specific span-corruption mechanisms.
- **Break condition:** If EEG reports have highly idiosyncratic, non-repetitive terminology patterns, span-corruption may fail to capture useful regularities.

### Mechanism 2
- **Claim:** Multi-task instruction fine-tuning (polishing, QA, summarization) transfers domain knowledge to structured reasoning tasks.
- **Mechanism:** LLM-generated pseudo-labels for three complementary objectives expose the model to varied linguistic operations—normalizing syntax (polishing), recalling facts (QA), and compressing information (summarization)—creating a shared representation that generalizes across tasks.
- **Core assumption:** Pseudo-labels from general LLMs are sufficiently accurate for EEG domain tasks, and task diversity improves rather than confuses learning.
- **Evidence anchors:**
  - [abstract] "fine-tuned with instruction-style tasks such as report polishing, summarization, and question answering"
  - [section] Table IV: DAPT→SFT achieves highest IE F1 (0.575), with strongest gains in Localization (0.621) and Frequency (0.678)—attributes requiring contextual interpretation.
  - [corpus] ImmunoFOMO paper examines whether domain LMs capture clinician-level knowledge, suggesting task-specific fine-tuning is an open research question in biomedical domains.
- **Break condition:** If pseudo-labels contain systematic errors or hallucinations, fine-tuning may amplify rather than correct them.

### Mechanism 3
- **Claim:** EEG-domain tokenization dramatically improves lexical coverage and reduces fragmentation of clinical terms.
- **Mechanism:** Training a tokenizer on the EEG corpus ensures that multi-word expressions (e.g., "posterior-dominant alpha rhythm") are encoded as coherent subword units, reducing out-of-vocabulary tokens and preserving semantic integrity during encoding.
- **Core assumption:** Tokenizer quality directly impacts downstream task performance by enabling better representation of domain-specific phrases.
- **Evidence anchors:**
  - [section] Table II: EEG-based tokenizer reduces OOV from 49.04% to 10.19%, cuts average subwords per token nearly in half, and approaches MTR of 1.0.
  - [section] "EEG-specific expressions are now preserved as single, coherent units."
  - [corpus] No direct corpus validation for tokenizer-specific claims in EEG; related biomedical LM papers do not address tokenization mechanisms.
- **Break condition:** If downstream tasks rely more on reasoning than lexical precision, tokenizer improvements may yield diminishing returns.

## Foundational Learning

- **Concept: Span-Corruption Objective (T5-style pretraining)**
  - Why needed here: Understanding how masking and reconstructing text spans forces models to learn contextual dependencies is essential for grasping why DAPT works.
  - Quick check question: Can you explain why predicting masked spans requires modeling longer-range dependencies than next-token prediction?

- **Concept: Domain-Adaptive Pretraining (DAPT)**
  - Why needed here: The core hypothesis is that continuing pretraining on domain text improves downstream task performance over generic pretrained models.
  - Quick check question: What is the difference between pretraining from scratch, DAPT, and task-specific fine-tuning?

- **Concept: Label Efficiency and Inductive Bias**
  - Why needed here: The paper claims DAPT provides strong inductive bias, enabling high performance with 1% labeled data—a key practical benefit.
  - Quick check question: How does exposure to large amounts of unlabeled domain text reduce the amount of labeled data needed for fine-tuning?

## Architecture Onboarding

- **Component map:** Flan-T5 Base -> Custom EEG-domain tokenizer -> DAPT with span corruption -> SFT with polishing/QA/summarization -> Evaluation on IE/summarization tasks

- **Critical path:**
  1. Curate and clean EEG reports from HEEDB (remove PHI, normalize formatting)
  2. Train EEG-domain tokenizer on cleaned corpus
  3. Initialize from Flan-T5-Base, run DAPT with EEG-specific span corruption
  4. Generate pseudo-labels for polishing/QA/summarization using external LLM
  5. Fine-tune on combined instruction tasks with early stopping on validation perplexity

- **Design tradeoffs:**
  - Lightweight (T5-Base scale) vs. larger models: prioritizes deployability in clinical/BCI pipelines over maximum performance
  - Pseudo-labels vs. human annotation: trades label quality for scalability; human verification only on test sets
  - Unified text-to-text format: simplifies multi-task training but may not optimally exploit task-specific architectures

- **Failure signatures:**
  - High perplexity after DAPT (>100): suggests tokenizer-corpus mismatch or insufficient pretraining data
  - Low IE F1 despite good perplexity: indicates fluency-reasoning gap; may need more task-specific supervision
  - High contradiction rate (>0.15) in summarization: pseudo-label quality issues or over-confident generation
  - Poor negation robustness: model relying on lexical shortcuts rather than semantic understanding

- **First 3 experiments:**
  1. **Tokenizer ablation:** Compare EEG-domain tokenizer vs. Flan-T5 tokenizer on OOV rate and downstream IE F1 to validate lexical coverage hypothesis.
  2. **DAPT-only vs. SFT-only vs. DAPT→SFT:** Run controlled comparison on held-out test set to isolate contribution of each training stage (already partially done in Tables III-VII).
  3. **Label efficiency curve:** Train with 1%, 5%, 10%, 25%, 100% of labeled data across all model variants to confirm DAPT provides inductive bias (replicate Figure 2 with statistical significance tests).

## Open Questions the Paper Calls Out

- **Question:** How does NeuroLex perform when integrated as a linguistic decoder in end-to-end multimodal EEG-to-text generation systems?
  - **Basis in paper:** [Explicit] The abstract and conclusion define NeuroLex as a "decoder backbone for multimodal EEG-language systems" and a "foundation for language-driven neural decoding," yet the experiments exclusively evaluate text-to-text tasks.
  - **Why unresolved:** The paper validates the model's linguistic competence in isolation but does not test its performance when conditioned on raw or encoded neural signals in a live BCI pipeline.
  - **What evidence would resolve it:** Benchmarks from a multimodal architecture where NeuroLex decodes EEG signal representations into text, compared against general-purpose language model decoders.

- **Question:** Can NeuroLex maintain its performance when applied to EEG reports from institutions with distinct reporting styles or geographical norms outside the Harvard Electroencephalography Database (HEEDB)?
  - **Basis in paper:** [Inferred] Section II.B states that all pretraining data is derived "exclusively" from HEEDB, and evaluations use splits of this same dataset, leaving cross-institutional generalization unverified.
  - **Why unresolved:** The model may have overfit to the specific "linguistic conventions" of the Harvard database, potentially limiting its utility for labs or hospitals with different documentation standards.
  - **What evidence would resolve it:** Zero-shot or few-shot evaluation results on external, geographically distinct EEG report datasets.

- **Question:** To what extent does the reliance on LLM-generated pseudo-labels for Supervised Fine-Tuning (SFT) constrain the model's ability to surpass the factual accuracy of its teacher model?
  - **Basis in paper:** [Inferred] Section II.C notes that the SFT stage uses "LLM-generated pseudo-labels" for polishing, QA, and summarization tasks to create training supervision.
  - **Why unresolved:** While the test set is human-verified, training on synthetic labels risks reinforcing the "teacher" LLM's hallucinations or stylistic biases rather than learning ground-truth clinical reasoning.
  - **What evidence would resolve it:** An ablation study comparing model performance when fine-tuned on human-annotated data versus the LLM-generated pseudo-labels used in the study.

## Limitations
- Pseudo-label quality uncertainty: Limited human verification to test sets only raises concerns about systematic errors propagating through fine-tuning.
- Negation robustness claims may be overstated: Absolute performance (0.72 F1) remains moderate despite relative improvements over general models.
- Tokenizer-to-task performance gap: While OOV reduction is impressive, direct validation linking tokenization improvements to clinical understanding is lacking.

## Confidence
- **High confidence:** Basic architecture and training pipeline (DAPT→SFT on T5-Base is well-established)
- **Medium confidence:** Span-corruption mechanism's effectiveness for EEG terminology (supported by reconstruction accuracy but lacking direct task performance attribution)
- **Low confidence:** Clinical reliability claims given limited human evaluation scope and potential pseudo-label contamination

## Next Checks
1. Conduct a controlled ablation study comparing DAPT-only, SFT-only, and DAPT→SFT models on held-out test sets to isolate the contribution of each training stage to final performance.
2. Perform human evaluation of pseudo-labels used for fine-tuning to quantify error rates and assess whether model performance improvements are artifacts of label quality rather than genuine domain adaptation.
3. Test negation robustness across diverse clinical scenarios beyond the reported test set to verify the model's ability to handle complex negation patterns in real-world EEG interpretation.