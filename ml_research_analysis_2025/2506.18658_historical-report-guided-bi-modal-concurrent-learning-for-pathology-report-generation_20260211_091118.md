---
ver: rpa2
title: Historical Report Guided Bi-modal Concurrent Learning for Pathology Report
  Generation
arxiv_id: '2506.18658'
source_url: https://arxiv.org/abs/2506.18658
tags:
- knowledge
- report
- visual
- features
- pathology
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of generating pathology reports
  from Whole Slide Images (WSIs), which suffers from two key issues: lack of semantic
  content in visual features and information redundancy in WSIs. The authors propose
  a Historical Report Guided Bi-modal Concurrent Learning Framework for Pathology
  Report Generation (BiGen) that emulates pathologists'' diagnostic reasoning.'
---

# Historical Report Guided Bi-modal Concurrent Learning for Pathology Report Generation

## Quick Facts
- arXiv ID: 2506.18658
- Source URL: https://arxiv.org/abs/2506.18658
- Authors: Ling Zhang; Boxiang Yun; Qingli Li; Yan Wang
- Reference count: 30
- Primary result: 7.4% relative improvement in BLEU-4 (0.135) and 19.1% F1 improvement (0.730) for Her-2 prediction

## Executive Summary
This paper introduces BiGen, a framework that generates pathology reports from Whole Slide Images (WSIs) by emulating pathologists' diagnostic reasoning. The method addresses two key challenges: semantic content deficiency in visual features and information redundancy in WSIs. BiGen achieves state-of-the-art performance on the PathText (BRCA) dataset through a novel combination of knowledge retrieval and bi-modal concurrent learning.

## Method Summary
BiGen processes WSIs through a visual branch that uses cross-attention with a learnable visual token to extract key features from high-attention patches. A knowledge retrieval mechanism retrieves WSI-relevant historical reports from a pre-built medical knowledge bank using PLIP embeddings. The framework employs weight-shared cross-attention layers to align visual features with retrieved knowledge, followed by token concatenation and autoregressive generation. The model is trained with 3 encoder layers, 3 decoder layers, and 4 attention heads on a single A40 GPU.

## Key Results
- Achieves BLEU-4 score of 0.135 on PathText-BRCA, a 7.4% relative improvement over existing methods
- Attains Her-2 prediction F1 score of 0.730, representing a 19.1% enhancement in classification metrics
- Ablation studies confirm the effectiveness of knowledge retrieval, textual token cross-attention, and weight-sharing mechanisms

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Retrieval for Semantic Enrichment
- **Claim:** Retrieving WSI-relevant historical reports provides semantic content that pure visual features lack
- **Mechanism:** High-attention patches are encoded via PLIP image encoder, matched against a pre-built medical knowledge bank of sentence-level pathology reports, and aggregated to form retrieved knowledge features
- **Core assumption:** Sentence-level historical reports contain semantically relevant diagnostic patterns that transfer to current cases
- **Evidence anchors:** Abstract statement about knowledge retrieval, section 2.3 description, weak corpus support from MPath
- **Break condition:** If retrieved sentences contain contradictory or noisy semantics unrelated to the target WSI, performance degrades (Row 4 in Table 2)

### Mechanism 2: Visual Token Cross-Attention for Redundancy Suppression
- **Claim:** A single learnable visual token attending to all patches extracts compact key features while suppressing WSI redundancy
- **Mechanism:** Instead of M×M self-attention over all patches, a learnable token V⁰ queries patch features X via cross-attention across L layers, iteratively refining a global visual representation
- **Core assumption:** Diagnostic information is sparse and localized within WSIs; most patches are redundant
- **Evidence anchors:** Abstract statement about learnable visual token, section 2.2 description, no direct corpus comparison
- **Break condition:** If key diagnostic features require multi-token representation (complex multi-region patterns), single token may be insufficient

### Mechanism 3: Cross-Modal Alignment via Weight-Shared Layers
- **Claim:** Weight sharing between visual and textual token cross-attention layers enables semantic alignment between visual features and retrieved knowledge
- **Mechanism:** VTCA and TTCA share parameters (Θˡ), forcing both branches to learn compatible representations that align visual patterns with textual semantics
- **Core assumption:** Visual and textual diagnostic features inhabit a shared semantic space that can be learned jointly
- **Evidence anchors:** Abstract statement about weight-shared layers, section 2.3 description, WS vs. WOS comparison in Table 2
- **Break condition:** If visual and textual modalities require fundamentally different feature distributions, weight sharing may hurt rather than help

## Foundational Learning

- **Concept: Cross-Attention vs. Self-Attention**
  - **Why needed here:** BiGen replaces self-attention with cross-attention using learnable tokens to reduce O(M²) complexity and focus on key features
  - **Quick check question:** Can you explain why cross-attention with a single query token reduces redundancy compared to self-attention over all patches?

- **Concept: Knowledge Retrieval / RAG Basics**
  - **Why needed here:** The knowledge bank retrieval uses cosine similarity between image embeddings and text embeddings in a shared PLIP space
  - **Quick check question:** How does PLIP enable cross-modal retrieval between image patches and sentence embeddings?

- **Concept: Multi-Modal Fusion Strategies**
  - **Why needed here:** Final report generation requires integrating visual token (V^L) and textual token (T^(L-1)) representations
  - **Quick check question:** Why concatenate tokens rather than use attention-based fusion in the decoder?

## Architecture Onboarding

- **Component map:** WSI → patches → UNI encoder → linear projection → VTCA (L layers) → visual token V^L → decoder; High-attention patches → PLIP encoder → knowledge bank retrieval → TTCA (L-1 layers) → textual token T^(L-1) → decoder
- **Critical path:** First VTCA layer attention scores determine which patches feed knowledge retrieval; retrieved knowledge quality directly affects textual token semantics; weight-shared cross-attention aligns both modalities; token concatenation feeds decoder for generation
- **Design tradeoffs:** k (selecting ratio): 0.4 optimal; k=1.0 includes noisy patches (Fig. 3a); v (knowledge features): 3-7 optimal; m (region size): 20-80 stable; L=3 encoder/decoder layers with 4 heads, embedding size 512
- **Failure signatures:** Low BLEU-4 + high redundancy in generated text → check if TTCA is filtering retrieved knowledge properly; poor Her-2 classification → verify attention scores highlight tumor regions; knowledge retrieval returns irrelevant sentences → check PLIP encoder alignment
- **First 3 experiments:** 1) Baseline validation: Run vanilla Transformer on PathText-BRCA splits to reproduce Row 1 in Table 2 (BLEU-4=0.103); 2) Ablation sweep: Disable TTCA (Row 5 vs. Row 4) to confirm knowledge filtering requirement; 3) Hyperparameter sensitivity: Vary k in [0.2, 0.4, 0.6, 0.8, 1.0] to reproduce Fig. 3a curve

## Open Questions the Paper Calls Out
- **Open Question 1:** Can the BiGen framework generalize to multi-organ or multi-cancer datasets beyond the breast cancer (BRCA) domain?
- **Open Question 2:** How does the framework perform when the knowledge bank is constructed from external medical corpora rather than the training set reports?
- **Open Question 3:** Does the reliance on retrieved historical reports exacerbate factual hallucinations when the retrieved knowledge contradicts the visual evidence?

## Limitations
- Knowledge retrieval effectiveness depends critically on the quality and representativeness of the historical report knowledge bank, which may not generalize across institutions
- Single learnable visual token may be insufficient for capturing complex multi-region pathological patterns requiring distributed representation
- The framework's generalization to datasets outside PathText-BRCA remains untested

## Confidence

- **High confidence:** Overall architecture design and hyperparameter settings (k=0.4, v=3-7, m=20-80) are well-supported by ablation studies in Table 2 and Fig. 3
- **Medium confidence:** Weight-shared cross-attention layers enable effective cross-modal alignment, supported by WS vs. WOS comparison but could use additional validation
- **Low confidence:** Generalization of knowledge retrieval mechanism to datasets outside PathText-BRCA remains untested

## Next Checks

1. **Cross-institutional validation:** Test BiGen on pathology report generation from a different institution's WSI dataset to verify knowledge retrieval mechanism generalizes beyond training domain

2. **Token sufficiency analysis:** Evaluate whether single-token representation limits performance on cases requiring multi-region analysis by comparing against multi-token baseline on complex pathology cases

3. **Knowledge bank quality assessment:** Systematically evaluate how knowledge retrieval quality affects generation by artificially degrading the knowledge bank (e.g., using reports from different cancer types) and measuring performance degradation