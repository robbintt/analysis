---
ver: rpa2
title: 'Help the machine to help you: an evaluation in the wild of egocentric data
  cleaning via skeptical learning'
arxiv_id: '2510.23635'
source_url: https://arxiv.org/abs/2510.23635
tags:
- data
- user
- time
- questions
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated Skeptical Learning (SKEL) in real-world conditions
  to reduce annotation burden and improve data quality in longitudinal context recognition
  studies. University students used the iLog app for four weeks, answering time-diary
  questions about their location every 30 minutes.
---

# Help the machine to help you: an evaluation in the wild of egocentric data cleaning via skeptical learning

## Quick Facts
- **arXiv ID:** 2510.23635
- **Source URL:** https://arxiv.org/abs/2510.23635
- **Reference count:** 40
- **Primary result:** SKEL achieved 76% prediction accuracy in a 4-week longitudinal study but failed to outperform baseline GP model

## Executive Summary
This study evaluated Skeptical Learning (SKEL) in real-world conditions to reduce annotation burden and improve data quality in longitudinal context recognition studies. University students used the iLog app for four weeks, answering time-diary questions about their location every 30 minutes. SKEL challenged users on suspicious labels and predicted contexts when sufficiently confident. Results showed that 76% of SKEL predictions were rated correct by users, demonstrating potential for reducing user effort. However, SKEL did not outperform a baseline GP model in F1-score, likely due to high data sparsity and user consistency in answers. The study highlights both the promise and challenges of applying SKEL in real-world longitudinal studies.

## Method Summary
The study used smartphone sensor data (WiFi, Bluetooth, Accelerometer, GPS, etc.) aggregated into 30-minute windows to classify user location into four categories: University, Home, Traveling, Other. A Gaussian Process classifier with a specific kernel combination (Constant + Rational Quadratic + Squared Exponential + White Noise) was used in an incremental, per-user training setup. The SKEL system operated in three phases: bootstrap week (no skepticism), interaction weeks (challenge suspicious labels), and evaluation week (progressive F1-score calculation). The baseline comparison was a GP that never challenges the user.

## Key Results
- SKEL predictions were rated correct by users 76% of the time
- SKEL did not achieve higher F1-score than the baseline GPnever model
- High data sparsity (up to 100% missing values for certain sensors) was observed
- User consistency in answers contributed to limited model improvement

## Why This Works (Mechanism)
Skeptical Learning works by creating an interactive feedback loop where the machine actively questions uncertain human annotations. When the model's confidence in its prediction exceeds the confidence it has in the user's label, it challenges the user for clarification. This mechanism is designed to reduce annotation burden by predicting labels when sufficiently confident, while still maintaining data quality through selective challenges. The approach is particularly valuable in longitudinal studies where continuous user engagement is challenging.

## Foundational Learning

**Gaussian Process Classification**
- *Why needed:* Provides probabilistic predictions and uncertainty estimates essential for the skepticism mechanism
- *Quick check:* Verify GP outputs both mean prediction and variance for each sample

**Incremental Learning**
- *Why needed:* Enables continuous model updates as new data arrives in longitudinal studies
- *Quick check:* Model should update efficiently without retraining from scratch each time

**Uncertainty Quantification**
- *Why needed:* Forms the basis for deciding when to challenge user annotations
- *Quick check:* Model confidence should correlate with prediction accuracy

**Sensor Data Aggregation**
- *Why needed:* Converts high-frequency sensor streams into meaningful features for context recognition
- *Quick check:* Aggregated features should capture temporal patterns in 30-minute windows

**Egocentric Modeling**
- *Why needed:* Accounts for individual differences in behavior patterns and context preferences
- *Quick check:* Model performance should improve with more personalized user data

## Architecture Onboarding

**Component Map**
Data Sources -> Feature Extraction -> GP Classifier -> SKEL Logic -> User Interaction -> Label Validation

**Critical Path**
Sensor Data -> 30-minute Aggregation -> GP Prediction -> Uncertainty Calculation -> Challenge Decision -> User Feedback -> Model Update

**Design Tradeoffs**
- Model complexity vs. computational efficiency for streaming updates
- Challenge frequency vs. user fatigue
- Sensor coverage vs. battery consumption
- Model confidence threshold vs. annotation accuracy

**Failure Signatures**
- Computational bottlenecks during incremental updates
- Class imbalance favoring "Home" predictions
- Over-challenging leading to user frustration
- Missing data degrading feature quality

**First 3 Experiments**
1. Test GP incremental update mechanism with synthetic streaming data
2. Validate skepticism threshold logic using held-out validation set
3. Measure impact of different missing data handling strategies on prediction accuracy

## Open Questions the Paper Calls Out

**Psychological Factors**
How do psychological factors, such as user frustration or reactance, influence the acceptance rate of machine-generated corrections in Skeptical Learning? The study measured label rejection rates but did not capture the users' emotional states or subjective reasons for rejecting the machine's suggestions.

**Generalization to Other Domains**
Can Skeptical Learning maintain its efficacy when applied to highly subjective context dimensions like activity or social context, rather than just location? This experiment was restricted to the spatial dimension because it allowed for easier derivation of ground truth.

**Situational Factors Impact**
To what extent do situational factors like response latency, time of day, and user mood impact the quality of annotations in longitudinal SKEL deployments? The current analysis focused on aggregate performance without isolating how these specific contextual variables correlate with annotation errors.

## Limitations
- Evaluation conducted via simulation rather than real-time user interaction
- Skepticism mechanism details rely on external references
- Missing data handling strategy not explicitly specified
- 4-week evaluation period may be insufficient for long-term behavior analysis

## Confidence

**SKEL prediction accuracy (76%):** High confidence - direct empirical result from evaluation
**SKEL fails to outperform baseline GP:** High confidence - direct empirical result from evaluation
**Data sparsity and user consistency as primary reasons for performance gap:** Medium confidence - interpretation based on observed data characteristics

## Next Checks

1. Implement and test the exact skepticism threshold mechanism using the referenced prior work [5] to verify the 76% accuracy claim under identical conditions.

2. Compare performance when handling missing data via different strategies (imputation vs. masking vs. feature selection) to determine impact on SKEL vs baseline performance.

3. Re-run the evaluation with a simplified "always challenge" baseline to determine whether SKEL's skepticism mechanism provides any benefit over more aggressive annotation correction strategies.