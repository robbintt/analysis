---
ver: rpa2
title: 'ControlTac: Force- and Position-Controlled Tactile Data Augmentation with
  a Single Reference Image'
arxiv_id: '2505.20498'
source_url: https://arxiv.org/abs/2505.20498
tags:
- tactile
- data
- force
- contact
- real
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CONTROL TAC, a two-stage controllable framework
  for generating realistic tactile images conditioned on a single reference image,
  contact force, and contact position. The method uses a force-control generator followed
  by a position-control generator with ControlNet to incorporate physical priors,
  producing physically plausible tactile images for data augmentation.
---

# ControlTac: Force- and Position-Controlled Tactile Data Augmentation with a Single Reference Image

## Quick Facts
- arXiv ID: 2505.20498
- Source URL: https://arxiv.org/abs/2505.20498
- Authors: Dongyu Luo; Kelin Yu; Amir-Hossein Shahidzadeh; Cornelia Fermüller; Yiannis Aloimonos; Ruohan Gao
- Reference count: 40
- Primary result: Single-reference tactile image generation achieves twice the pose estimation accuracy of models trained on full real datasets

## Executive Summary
ControlTac is a two-stage controllable framework for generating realistic tactile images conditioned on a single reference image, contact force, and contact position. The method uses a force-control generator followed by a position-control generator with ControlNet to incorporate physical priors, producing physically plausible tactile images for data augmentation. Evaluated across three downstream tasks—force estimation, pose estimation, and object classification—the approach shows consistent performance gains. Notably, pose estimation achieves twice the accuracy of models trained on full real datasets, and the method generalizes to unseen objects. Real-world experiments, including a challenging precise insertion task, further validate its practical utility. The framework effectively scales tactile datasets using only a single reference image per task.

## Method Summary
ControlTac employs a two-stage diffusion-based generative model to create realistic tactile images. Stage 1 trains a DiT-based diffusion model on 20,000 force-labeled images to learn deformation physics. Stage 2 fine-tunes only via ControlNet using 7,000 contact-masked images, efficiently learning position priors without catastrophic forgetting of force relationships. The method conditions on a single real tactile image (via SANA autoencoder) to preserve sensor-specific optical properties, combined with 3D force vectors and binary contact masks for precise spatial control. This sequential training approach achieves better generation quality than joint training, with generated data improving downstream task performance by 2× for pose estimation compared to full real datasets.

## Key Results
- Two-stage training achieves MSE=23 vs hybrid MSE=31 for generation quality
- Pose estimation accuracy doubles compared to models trained on full real datasets
- Method generalizes to unseen objects with 4-5 pixel error in pose estimation
- Real-world insertion task validates practical utility of augmented data

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Separation of Physical Priors
Separating force-control (Stage 1) and position-control (Stage 2) learning improves generation quality compared to joint training. Force dynamics require substantially more training data than spatial positioning. Stage 1 trains a DiT-based diffusion model on 20,000 force-labeled images to learn deformation physics. Stage 2 fine-tunes only via ControlNet using 7,000 contact-masked images, efficiently learning position priors without catastrophic forgetting of force relationships. Core assumption: Force and position priors have different data complexity requirements and can be learned sequentially without mutual interference.

### Mechanism 2: Reference Image as Texture/Geometry Anchor
Conditioning on a single real tactile image preserves material appearance and contact geometry that simulation and free-form generation cannot capture. The reference image provides structural cues (gel deformation patterns, lighting interaction, surface texture) encoded via SANA's frozen autoencoder. Physical conditions (force ∆F, contact mask) modulate this latent rather than generating from scratch, ensuring generated images share the sensor's specific optical properties. Core assumption: Sensor-specific variations (gel properties, lighting) are stable enough within a task that one reference captures essential characteristics.

### Mechanism 3: Contact Mask as Translation-Invariant Position Encoding
Binary contact masks enable precise spatial control without ambiguous center-point representations. For a given object, mask shape remains constant regardless of force. Position variation is encoded by translating/rotating the mask with pixel-level precision (1 pixel translation, 1° rotation). This avoids: (1) center-point ambiguity for objects larger than sensor, (2) edge-detection inconsistency. Core assumption: Contact area shape is force-invariant and mask alignment can be done accurately.

## Foundational Learning

- **Diffusion Models with DDIM Sampling**: ControlTac uses DiT (Diffusion Transformer) as backbone; understanding denoising schedules and classifier-free guidance is essential for debugging generation quality. Quick check: Can you explain why DDIM improves inference efficiency over standard diffusion sampling?

- **ControlNet Architecture**: Stage 2 injects spatial conditions via auxiliary network parallel to frozen DiT blocks. Understanding where and how control signals merge is critical for adding new condition types. Quick check: How does ControlNet differ from simply concatenating condition embeddings to the input?

- **Vision-Based Tactile Sensing Physics**: Gel deformation, lighting geometry, and force-displacement relationships determine what physical priors are learnable vs. must be encoded. Quick check: Why does the same force produce different tactile images at different contact positions?

## Architecture Onboarding

- **Component map**: Reference image x → SANA encoder E → latent z(x); Contact mask c → E → z(c); Force vector ∆F (3D relative) → DiT blocks → SANA decoder D → Generated tactile image

- **Critical path**: Reference image quality → latent encoding fidelity → Force-control training convergence (20k samples) → Stage 2 initialization quality → Contact mask alignment precision → Position control accuracy → Generated data realism → Downstream task transfer

- **Design tradeoffs**:
  - Two-stage vs. hybrid: Two-stage requires sequential training but achieves better MSE (23 vs 31); hybrid is simpler but needs more joint data
  - Fixed vs. varying force during pose training: Varying force improves pose estimation (3-5 pixel error vs. 7-9 with fixed) but requires more augmented samples
  - Generated-only vs. mixed training: Generated-only works for unseen objects (T-shape: 4-5 pixel error) but mixing 15k real + 30k generated slightly increases MAE (0.060 vs 0.055 with 15k+15k) due to error accumulation

- **Failure signatures**:
  - Blurry/mean outputs: Insufficient force diversity in training data (free-form generation collapse)
  - Wrong contact position: ControlNet not converging or mask misalignment
  - Unrealistic textures: Reference image lighting differs from target domain
  - Poor downstream transfer: Generated data doesn't cover inference force/position distribution

- **First 3 experiments**:
  1. Baseline generation quality: Train Stage 1 only on FeelAnyForce subset (5k samples). Measure MSE/SSIM vs. paper's 20k baseline to validate force-control data scaling requirements.
  2. Position-control ablation: Compare ControlNet fine-tuning vs. concatenation-based conditioning on 7k position data. Expect ControlNet to outperform if architectural claims hold.
  3. Single-reference generalization test: Pick one unseen object from your sensor, capture one reference image, generate 4k augmented samples, train pose estimator. Test on 30 held-out real positions to validate the paper's core claim of single-image augmentation.

## Open Questions the Paper Calls Out

- **How can the ControlTac framework be extended to incorporate additional physical parameters, such as surface texture and material hardness?**: The conclusion states the method "currently does not account for many other physical parameters beyond force and position such as surface texture and material hardness." The current two-stage architecture is specifically tailored for force and position inputs. Integrating continuous variables like texture or hardness requires defining new conditioning inputs and likely gathering new training datasets with these specific labels.

- **Can the generative model be refined to improve generation accuracy in the low-force regime (1–4 N)?**: Appendix C.1 notes that the "MAE is significantly higher in the 1-10 N range," attributing this to high sensitivity to minor force variations and a lack of geometric information at low pressures. The current diffusion model appears to struggle with the subtle deformations present when normal force is low, leading to unreliable generations for light contacts.

- **Does the ControlTac generator generalize across different vision-based tactile sensor hardware (e.g., DIGIT vs. GelSight) without retraining?**: While the paper cites "inconsistencies across sensor instances" as a motivation, the method is trained and evaluated solely on data from GelSight sensors (FeelAnyForce dataset). Significant differences in sensor optical paths, lighting, and elastomer properties may cause the model's learned latent space to be sensor-specific, limiting its utility for the broader tactile sensing community.

## Limitations

- Sequential two-stage training assumption may not generalize to sensors with stronger force-position coupling
- Single-reference conditioning effectiveness depends on sensor stability over time
- ControlNet's architectural details for tactile conditioning lack specificity

## Confidence

- **High confidence**: Generation quality improvements (MSE reduction from 31 to 23), pose estimation accuracy gains (2× better than full real datasets)
- **Medium confidence**: Generalization to unseen objects (limited testing on T-shape cube only), practical insertion task results (single demonstration)
- **Low confidence**: Claims about single-reference sufficiency across diverse tasks, contact mask alignment precision requirements

## Next Checks

1. **Sensor drift validation**: Capture reference images at different times/locations and measure generation quality degradation to test single-reference stability limits
2. **Cross-sensor generalization**: Apply the framework to a different tactile sensor type (e.g., Gelsight vs. TacTip) to validate force-position factorization assumption
3. **Force-position coupling stress test**: Systematically vary force-position correlation strength in synthetic data and measure degradation in downstream task performance