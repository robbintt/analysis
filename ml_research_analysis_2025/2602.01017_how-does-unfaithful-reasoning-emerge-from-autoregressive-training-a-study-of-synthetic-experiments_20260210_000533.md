---
ver: rpa2
title: How Does Unfaithful Reasoning Emerge from Autoregressive Training? A Study
  of Synthetic Experiments
arxiv_id: '2602.01017'
source_url: https://arxiv.org/abs/2602.01017
tags:
- reasoning
- training
- noise
- figure
- phase
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the emergence of unfaithful chain-of-thought
  (CoT) reasoning in autoregressive language models through controlled synthetic experiments.
  The authors introduce a novel Arithmetic Expression Reasoning (AER) task that abstracts
  mathematical reasoning into modular arithmetic expressions, allowing systematic
  study of faithfulness.
---

# How Does Unfaithful Reasoning Emerge from Autoregressive Training? A Study of Synthetic Experiments

## Quick Facts
- arXiv ID: 2602.01017
- Source URL: https://arxiv.org/abs/2602.01017
- Authors: Fuxin Wang; Amr Alazali; Yiqiao Zhong
- Reference count: 27
- Primary result: Faithful chain-of-thought reasoning emerges only below a critical noise threshold in training data

## Executive Summary
This paper investigates why autoregressive language models often produce unfaithful chain-of-thought reasoning despite strong overall performance. Through controlled synthetic experiments on modular arithmetic tasks, the authors identify that faithful reasoning emerges only when training noise remains below a critical threshold. The study reveals three distinct reasoning phases during training—stepwise reasoning, mixed reasoning with self-verification, and skip-step reasoning—with a characteristic entropy bump indicating internal uncertainty encoding. The research demonstrates that simplicity bias protects stepwise reasoning when complexity gaps exist, but models inevitably transition to unfaithful skip-step reasoning when noise exceeds critical thresholds.

## Method Summary
The authors introduce an Arithmetic Expression Reasoning (AER) task using modular arithmetic to abstract mathematical reasoning into controlled, modular components. They train 3-layer transformers (2 heads, 128-dim embeddings) autoregressively on 11-token sequences, systematically varying noise corruption levels in both prompts and reasoning steps. The study defines two faithfulness metrics: consistency-based (RIR1, RIR2) measuring whether generated reasoning matches ground truth, and intervention-based (IDS, INR) measuring whether solutions causally depend on reasoning steps. Training proceeds for 62,500 steps with AdamW optimization, and metrics are logged every 1,000 steps to track phase transitions and internal state changes.

## Key Results
- Faithful reasoning emerges only below a critical noise threshold; beyond this threshold, models transition sharply to skip-step reasoning that bypasses intermediate steps
- Three distinct reasoning phases occur during training: stepwise reasoning, mixed reasoning with internal uncertainty encoding (characterized by entropy bump), and skip-step reasoning
- Simplicity bias creates algorithmic preference for stepwise over skip-step reasoning when complexity gaps exist between prompt and reasoning steps
- Models learn to encode internal uncertainty through hidden state differentiation during the mixed phase, suggesting emergent self-verification capabilities
- Shortcut features in data amplify unfaithfulness by encouraging models to bypass step-by-step reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Faithful chain-of-thought reasoning emerges only when training noise remains below a critical threshold.
- Mechanism: When noise is low, the model learns to compute e3 from e2 (stepwise reasoning). As noise increases, e2 becomes an unreliable signal, and the model shifts to computing e3 directly from e1 (skip-step reasoning), bypassing the reasoning chain entirely.
- Core assumption: The model selects between available predictive signals (e1 vs e2) based on which yields lower expected loss.
- Evidence anchors:
  - [abstract] "models can learn faithful reasoning that causally follows the underlying arithmetic rules, but only when the training noise is below a critical threshold"
  - [Section 3.1] "both unfaithfulness metrics are approximately flat and close to 0 below the threshold τc(ε1), and increase dramatically beyond the thresholds"
  - [corpus] Related work on CoT faithfulness (Turpin et al., Lanham et al.) documents similar unfaithfulness in large models, though without controlled noise analysis.
- Break condition: If reasoning noise ε2 exceeds ~5-10% (varies with prompt noise ε1), expect sharp transition to unfaithful skip-step reasoning.

### Mechanism 2
- Claim: Simplicity bias creates an algorithmic preference for stepwise over skip-step reasoning when complexity gaps exist.
- Mechanism: Stepwise reasoning (e2→e3) requires learning a single-operator rule. Skip-step reasoning (e1→e3) requires composing two operators. Gradient descent implicitly prefers lower-complexity functions, so models resist skip-step reasoning even when it offers marginally better accuracy.
- Core assumption: Overparameterized networks exhibit inductive bias toward simpler functions, consistent with Occam's razor.
- Evidence anchors:
  - [Section 3.1] "a larger complexity gap between e1 and e2 reduces unfaithfulness, and that models tolerate higher reasoning noise before the sharp transition"
  - [Section B.1] Swapping positions of e1 and e2 does not change behavior—complexity, not position, determines faithfulness.
  - [corpus] Limited direct corpus evidence on complexity gaps specifically for CoT; this appears novel to this paper.
- Break condition: If the reasoning step complexity approaches or exceeds prompt complexity, simplicity bias protection weakens.

### Mechanism 3
- Claim: Models encode internal uncertainty through hidden state differentiation during a transitional "mixed reasoning" phase.
- Mechanism: During Phase 2, when e1 and e2 yield inconsistent predictions, the model outputs near-uniform distributions (maximal uncertainty). Hidden state contrast (HSC) and attention contrast (AC) metrics show sharp increases, indicating the model has learned to detect inconsistency—emergent self-verification.
- Core assumption: Autoregressive training on noisy data induces the model to resolve conflicting signals by encoding consistency checks.
- Evidence anchors:
  - [abstract] "Mechanistic analysis reveals that models learn to encode internal uncertainty by resolving inconsistent reasoning steps"
  - [Section 3.3] "Figure 7 shows both metrics exhibit sharp changes at the same phase thresholds of reasoning modes"
  - [corpus] Related work (Shinn et al., Madaan et al.) shows self-verification improves with explicit prompting; this paper suggests it can emerge implicitly.
- Break condition: If training is cut short during Phase 2, self-verification may not fully develop; if extended too long, skip-step reasoning dominates.

## Foundational Learning

- Concept: Autoregressive next-token prediction
  - Why needed here: The entire paper studies how standard language model training (predicting token n+1 given tokens 1...n) shapes reasoning behavior.
  - Quick check question: Can you explain why a model trained only on consistent chains (e1→e2→e3) might still learn unfaithful reasoning?

- Concept: Simplicity/Inductive bias in neural networks
  - Why needed here: The critical threshold phenomenon depends on models preferring simpler compositional rules over complex alternatives.
  - Quick check question: Why would a model prefer learning f2(e2) over f(e1) even if both achieve zero training loss?

- Concept: Chain-of-thought reasoning structure
  - Why needed here: The paper operationalizes faithfulness by checking whether intermediate steps causally influence final answers.
  - Quick check question: What's the difference between consistency-based and intervention-based faithfulness?

## Architecture Onboarding

- Component map: Input sequence (11 tokens + EOS) → 3-layer transformer (2 heads, 128-dim) → Autoregressive loss from first → token onward → Evaluation metrics (RIR1, INR, entropy)

- Critical path:
  1. Generate training data with controlled noise (ε1, ε2)
  2. Train autoregressively, logging metrics every 1,000 steps
  3. Identify phase transitions via KL divergence to reference distributions (stepwise, skip-step, uniform)
  4. Probe hidden states via HSC/AC metrics during Phase 2

- Design tradeoffs:
  - Smaller models (3-layer) show clearer phase transitions; larger models (5-layer) transition faster with less visible intermediate phases
  - Prime modulus (N=97) ensures clean arithmetic properties; composite modulus (N=38) enables shortcut feature experiments
  - Longer training pushes models toward skip-step reasoning—faithfulness is not monotonic with compute

- Failure signatures:
  - INR→1, IDS→0 indicates skip-step reasoning has dominated
  - Monotonic entropy decrease (no bump) suggests model skipped Phase 2 self-verification
  - High RIR with low INR suggests post-hoc rationalization rather than genuine reasoning

- First 3 experiments:
  1. Replicate the baseline: train on (ε1=0.01, ε2=0.1) for 62,500 steps, plot all five metrics to identify phases.
  2. Sweep ε2 while fixing ε1=0.01 to locate the critical threshold τc where unfaithfulness spikes.
  3. Introduce shortcut features (c∈{0,2,N/2}) and compare INR on shortcut vs full test sets to confirm amplification of unfaithfulness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do reinforcement learning with verifier rewards (RLVR) training paradigms affect the emergence of faithful versus unfaithful reasoning compared to standard autoregressive training?
- Basis in paper: [explicit] "In future work, it would be interesting to investigate reinforcement learning with verifier rewards (RLVR) (Guo et al., 2025) which becomes a standard recipe for enhancing LLM's reasoning capabilities."
- Why unresolved: This study exclusively examines autoregressive training for pretraining and SFT; RLVR is fundamentally different as it optimizes for outcome correctness via reward signals rather than next-token prediction.
- What evidence would resolve it: Training models on the AER task using RLVR with solution verification, then comparing faithfulness metrics (RIR, IDS, INR) and phase dynamics against autoregressively trained baselines.

### Open Question 2
- Question: Do the faithfulness thresholds and phase transition dynamics observed in AER generalize to full-scale LLMs on natural language reasoning tasks?
- Basis in paper: [explicit] "We would also be interested in applying our metrics to LLMs beyond our synthetic setting."
- Why unresolved: The AER task abstracts away natural language and uses minimal single-step reasoning; it remains unclear whether the critical noise threshold phenomenon and three-phase training dynamics persist with larger models and more complex reasoning chains.
- What evidence would resolve it: Applying consistency-based and intervention-based faithfulness metrics to pretrained LLMs of varying sizes on tasks like GSM8K, tracking how metrics evolve during SFT with controlled noise levels.

### Open Question 3
- Question: What determines the variability in phase transition timing across different random seeds, and can these transitions be predicted or controlled?
- Basis in paper: [inferred] "The phase boundaries are irregular and vary across different random seeds, suggesting the complexities of the optimization landscape, which echoes the unpredictability of emergent phenomena in LLMs."
- Why unresolved: While the paper documents seed-dependent variability in when models transition between stepwise, mixed, and skip-step reasoning modes, it does not identify the underlying optimization landscape features causing this unpredictability.
- What evidence would resolve it: Systematic analysis of loss landscape geometry, gradient dynamics, and internal representations across seeds to identify correlates of transition timing; interventional studies to delay or accelerate phase transitions.

## Limitations
- Model Scale and Generalization: The study uses small synthetic models (3-layer transformer), which limits direct applicability to frontier LLMs.
- Synthetic Task Simplification: The AER task abstracts away from real-world reasoning complexity, potentially missing important dependencies present in natural language.
- Limited Exploration of Model Architecture: The paper primarily uses a 3-layer transformer, with limited validation of findings on larger architectures.

## Confidence
- High Confidence: The critical threshold phenomenon (Mechanism 1) and its relationship to noise levels is well-supported by systematic sweeps and consistent phase transitions.
- Medium Confidence: The simplicity bias mechanism (Mechanism 2) is theoretically sound but relies on extrapolation from limited complexity gap experiments.
- Medium Confidence: The internal uncertainty encoding mechanism (Mechanism 3) shows strong empirical support in hidden state contrast metrics, but the interpretation as "self-verification" could be strengthened.

## Next Checks
1. **Architectural Scaling Study**: Replicate the core experiments (critical threshold, phase transitions) on a 5-7 layer transformer to verify whether the three-phase progression and critical threshold phenomenon persist at larger scales.

2. **Real-world Task Translation**: Design a small-scale natural language reasoning task (e.g., simple math word problems) that preserves the stepwise dependency structure and train on noisy versions to bridge the synthetic-to-real gap.

3. **Complexity Gap Boundary Testing**: Systematically vary the complexity gap between prompt and reasoning steps to rigorously test the limits of simplicity bias protection and clarify when Mechanism 2 breaks down.