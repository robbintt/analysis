---
ver: rpa2
title: NAVER LABS Europe Submission to the Instruction-following Track
arxiv_id: '2506.01808'
source_url: https://arxiv.org/abs/2506.01808
tags:
- speech
- data
- training
- table
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper describes NAVER LABS Europe''s submission to the IWSLT
  2025 Instruction-following Speech Processing Track, where the goal was to build
  a system that can perform ASR, ST, and SQA tasks from English speech input into
  Chinese, Italian, and German. The core approach involved training two modules in
  parallel: a speech-to-LLM embedding projector using SeamlessM4T-v2-large speech
  representations, and LoRA adapters on top of Llama-3.1-8B-Instruct.'
---

# NAVER LABS Europe Submission to the Instruction-following Track

## Quick Facts
- arXiv ID: 2506.01808
- Source URL: https://arxiv.org/abs/2506.01808
- Reference count: 40
- Primary result: Unified speech-language model achieving competitive ASR (~17-19 WER) and improved ST (BLEU 30-43) performance through decoupled pretraining and brief multimodal fine-tuning.

## Executive Summary
This paper presents NAVER LABS Europe's submission to the IWSLT 2025 Instruction-following Speech Processing Track, building a unified system for ASR, ST, and SQA from English speech into Chinese, Italian, and German. The core innovation involves decoupling speech and text adaptation through parallel pretraining of a speech-to-LLM embedding projector and LoRA adapters, followed by efficient 1K-step multimodal fine-tuning. The approach demonstrates that isolated modality optimization reduces gradient interference during fusion, enabling competitive performance across all three tasks while maintaining training efficiency through frame averaging and parameter-efficient fine-tuning.

## Method Summary
The system employs a three-stage pipeline: (1) a speech projector maps 1024-dim SeamlessM4T-v2-large encoder outputs (averaged every 3 frames) to 4096-dim LLM space using a 4-layer Transformer; (2) LoRA adapters (rank=8, alpha=16) are trained on text-only MT/QA tasks; (3) both modules are jointly fine-tuned for 1K steps on interleaved speech+text batches. Frame averaging reduces sequence length by 3x, enabling larger batches. The approach leverages synthetic data generation via SeamlessM4T-v2-large and Llama-3.1-8B-Instruct for expanded training coverage, with COMET filtering (≥0.85) applied to translated QA pairs.

## Key Results
- ASR performance achieved 17.3-19.1 WER across languages, competitive with specialized models
- ST results showed BLEU scores of 30.0-43.3, with Chinese and Italian outperforming German (35.0-35.3)
- SQA accuracy reached 87.5-95.8% using fluent answer formats, close to text-only baselines
- German consistently underperformed despite largest training data, suggesting data quality issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupled pretraining of speech projector and text LoRA adapters followed by brief joint tuning produces stronger multimodal models than end-to-end joint training.
- Mechanism: Each component optimizes for its modality in isolation—projector learns speech-to-embedding alignment while LoRA learns text task patterns—then a short joint phase aligns representations without interference.
- Core assumption: Speech and text optimization landscapes are sufficiently different that isolated pretraining reduces gradient conflict during fusion.
- Evidence anchors:
  - [abstract] "Our solution leverages two pretrained modules: (1) a speech-to-LLM embedding projector trained using representations from the SeamlessM4T-v2-large speech encoder; and (2) LoRA adapters trained on text data on top of a Llama-3.1-8B-Instruct."
  - [Section 5.2] "Overall our results show that it is possible to train text (B) and speech adaptation (A) in parallel, and then to align both via joint instruction tuning (C). By separating the pretraining of both components, we are able to focus hyper-parameter search at the merging stage."
  - [corpus] IT-IST IWSLT 2025 submission uses similar alignment strategy between speech encoder and small LLM, suggesting convergent validation of decoupled approach.
- Break condition: If projector outputs are incoherent after pretraining (random outputs on ASR), joint tuning cannot recover alignment.

### Mechanism 2
- Claim: Approximately 1,000 steps of multimodal instruction tuning is sufficient to fuse pretrained components; longer training shows saturation or degradation.
- Mechanism: Since both modules enter fusion with established competence, the adaptation phase primarily learns cross-modal coordination rather than full task acquisition.
- Core assumption: The pretrained components have captured most task-relevant knowledge; fusion requires only lightweight calibration.
- Evidence anchors:
  - [Section 5.1] "In preliminary experiments, we observed that as little as 100 steps were enough to successfully integrate the projector representation to the LoRA weights, but the best performance gains were obtained with 1K steps."
  - [Table 10] Model trained for 2K steps shows minimal ASR/COMET changes and BLEU drops for German and Italian, indicating saturation.
  - [corpus] No direct corpus comparison for minimal-step fusion; appears to be system-specific finding requiring validation.
- Break condition: If task diversity exceeds what 1K steps can align (e.g., many new languages or reasoning patterns), performance plateaus prematurely.

### Mechanism 3
- Claim: Averaging every 3 consecutive speech frames from SeamlessM4T reduces sequence length substantially without degrading task performance.
- Mechanism: Adjacent frames in speech encoder outputs are highly correlated; temporal redundancy can be compressed to enable larger batch sizes and faster training.
- Core assumption: Fine-grained temporal resolution in encoder outputs is not critical for ASR, ST, or SQA tasks at the utterance level.
- Evidence anchors:
  - [Section 3] "Prior to training, we average every 3 consecutive frame vectors, reducing significantly the sequence length. This simple trick allows us to train our models with larger batches, while maintaining good performance in speech tasks."
  - [Appendix C.1] "We observe that averaging every 3 frames results in models that are considerably faster to train, while maintaining similar performance to the original output."
  - [corpus] No corpus evidence found for this specific averaging ratio; frame compression strategies vary across systems.
- Break condition: If downstream tasks require word-level timing or precise boundary detection, 3x compression may lose critical information.

## Foundational Learning

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: Enables parameter-efficient fine-tuning of Llama-3.1-8B while keeping base weights frozen, allowing separate learning rates for projector vs. adapters during fusion.
  - Quick check question: Why does the paper use different learning rates (1e-5 for projector, 3e-4 for LoRA) during multimodal adaptation?

- Concept: **Cross-Modal Projector Training**
  - Why needed here: Bridges dimensionality gap between SeamlessM4T (1024-dim) and Llama embedding space (4096-dim) using a learned Transformer encoder.
  - Quick check question: With a frozen LLM backbone, what loss signal trains the speech projector, and how are gradients propagated?

- Concept: **Instruction-Based Task Switching**
  - Why needed here: Single model must handle ASR, ST, and SQA based on prompt structure; requires consistent formatting to avoid task confusion.
  - Quick check question: What prompt elements ensure the model outputs German versus Chinese for translation tasks?

## Architecture Onboarding

- Component map:
  SeamlessM4T-v2-large Speech Encoder → Frame Averaging → Speech Projector → Llama-3.1-8B-Instruct → LoRA Adapters → Response

- Critical path:
  Extract speech → Average frames → Project to LLM space → Wrap with instruction tokens → Concatenate embeddings → Forward through LLM+LoRA → Generate response

- Design tradeoffs:
  - Frame averaging ratio: 3x speed vs. temporal precision
  - Projector depth: 4-layer Transformer chosen over MLP/Conformer in ablations
  - Fluent vs. extract SQA answers: Fluent improves SQA scores but slightly reduces ASR synergy
  - Fusion steps: 1K optimal; 2K shows saturation

- Failure signatures:
  - Speech projector alone cannot perform SQA (Section 5.2: "limitation of the projector approach... for reasoning task, further adaptation might be required")
  - Inference degeneration with repetitive outputs (Appendix B.1)
  - German consistently underperforms despite largest training data (hypothesized data quality issues)
  - Language confusion if prompt format inconsistent

- First 3 experiments:
  1. Replicate projector-only model A.1 on CoVoST2/EuroParlST; verify BLEU ≥27 and COMET ≥0.76 on ST before attempting fusion.
  2. Train text-only LoRA (B) and confirm MT improvement over zero-shot Llama baseline; expect QA degradation due to short-answer format.
  3. Fuse A.1 + B with 1K-step multimodal tuning; confirm ASR improves by 1-2 WER and SQA emerges (70-90% accuracy depending on answer format).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Is the observed degradation in German translation performance caused by intrinsic biases in the LLM backbone or by specific quality issues within the EuroParlST and CoVoST2 training data?
- **Basis in paper:** [explicit] The authors note that models "consistently performed poorly on German, the language for which we had the largest amount of training data," and hypothesize it could be due to "LLM's inherent limitations" or "formatting bias" (Page 6) and later mention "issue with the German training data" (Page 14).
- **Why unresolved:** The paper provides correlational evidence (performance drops when using certain datasets) but does not isolate the root cause through controlled ablation on the German data quality versus LLM capacity.
- **What evidence would resolve it:** An ablation study substituting the German training data with high-quality human-verified data, or evaluating the text-only LLM on similar German tasks without speech inputs, would isolate the source of the error.

### Open Question 2
- **Question:** What architectural or training modifications are required to enable a speech-to-LLM projector to perform complex reasoning tasks (like SQA) independently, without relying on subsequent LoRA adapter integration?
- **Basis in paper:** [explicit] The authors state that the Speech Projector (A.2) was "unable to produce SQA output," observing that "For a reasoning task, further adaptation might be required in order to force the model to comply to the instruction" (Page 5).
- **Why unresolved:** While the authors show that merging the projector with LoRA adapters solves the issue, they do not investigate *why* the projector fails to map speech embeddings to the reasoning space of the LLM on its own.
- **What evidence would resolve it:** Experiments varying the projector architecture (e.g., deeper transformers vs. conformers) or employing reasoning-specific loss functions during projector pre-training would test if the bottleneck is representational capacity or misalignment.

### Open Question 3
- **Question:** Can the inference degeneration issue (repetitive loops) be effectively mitigated using lightweight post-processing models or specialized decoding strategies?
- **Basis in paper:** [explicit] The authors note that "inference degenerates, resulting in repeated words" in a small number of cases and hypothesize that "a lightweight post-processing model could offer a simple and effective solution" (Page 11).
- **Why unresolved:** The authors experimented with various sampling settings (greedy, top-p) but were "unable to identify a configuration that fully eliminated the issue," leaving the proposed post-processing solution untested.
- **What evidence would resolve it:** Implementing and evaluating a secondary classifier or heuristic filter designed to detect and halt repetitive token generation would validate the hypothesis.

### Open Question 4
- **Question:** Which evaluation metric (BERTScore vs. LLM-as-a-judge) more accurately reflects the semantic correctness of multilingual SQA systems given the significant discrepancies reported?
- **Basis in paper:** [explicit] The authors mention being "surprised to find that the evaluation setup provided by the organizers yields scores that differ significantly" from their LLM-as-a-judge scores, noting they "plan to further investigate the limitations" (Page 14).
- **Why unresolved:** The paper reports vastly different scores (e.g., ~0.53 vs ~0.86 for Italian) depending on the metric used, creating ambiguity regarding the model's true performance.
- **What evidence would resolve it:** A human evaluation study correlating the outputs with human judgments of fluency and correctness would determine which automated metric serves as a better proxy for SQA quality.

## Limitations

- Temporal compression trade-off: 3x frame averaging may lose fine-grained timing signals critical for word-boundary tasks
- Synthetic data quality variance: Long-tail synthetic ASR/ST data unlikely filtered, introducing domain mismatch
- Fusion stability: 1K-step joint tuning is empirically optimal but fragile with no theoretical guidance
- Answer format sensitivity: Fluent SQA answers improve QA scores but slightly harm ASR synergy

## Confidence

**High Confidence**:
- Decoupled pretraining (speech projector + LoRA) is beneficial for multimodal alignment
- 1K-step multimodal tuning is sufficient for stable fusion
- Frame averaging improves training efficiency without major performance loss

**Medium Confidence**:
- LoRA rank=8, alpha=16 is optimal; higher ranks may improve performance but were not explored
- Prompt templates effectively disambiguate tasks; minor formatting errors could cause task confusion
- German underperformance is due to data quality, not architecture

**Low Confidence**:
- Exact prompt templates and COMET filtering implementation
- Torchtune fork modifications for high-dimensional interleaved embeddings
- Whether 3x frame averaging is universally safe for all speech tasks

## Next Checks

1. **Temporal Precision Test**: Replace 3x frame averaging with 2x or no averaging on a word-boundary-sensitive subset (e.g., punctuation recovery in ASR) to quantify precision loss.

2. **Data Quality Audit**: Analyze German training data for domain mismatch, noise, or translation artifacts to confirm data quality as the cause of underperformance.

3. **Fusion Robustness Probe**: Train fusion models for 500, 1K, 1.5K, and 2K steps, then test on a held-out multilingual set to map the degradation curve and identify the optimal step count for each task type.