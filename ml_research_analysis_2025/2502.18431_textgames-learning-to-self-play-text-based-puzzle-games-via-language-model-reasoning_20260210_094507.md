---
ver: rpa2
title: 'TextGames: Learning to Self-Play Text-Based Puzzle Games via Language Model
  Reasoning'
arxiv_id: '2502.18431'
source_url: https://arxiv.org/abs/2502.18431
tags:
- instruct
- reasoning
- games
- text
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TextGames introduces a benchmark of text-based puzzle games to
  evaluate large language models' reasoning capabilities across pattern recognition,
  spatial awareness, arithmetic, and logical reasoning. The benchmark features eight
  puzzle games at three difficulty levels, designed to test both single-turn and multi-turn
  reasoning through self-reflection with feedback.
---

# TextGames: Learning to Self-Play Text-Based Puzzle Games via Language Model Reasoning

## Quick Facts
- arXiv ID: 2502.18431
- Source URL: https://arxiv.org/abs/2502.18431
- Reference count: 38
- Primary result: Introduces TextGames benchmark evaluating LLMs on 8 text-based puzzle games across 3 difficulty levels, showing reasoning-optimized models outperform instruction-tuned models on complex constraint satisfaction tasks

## Executive Summary
TextGames introduces a benchmark of text-based puzzle games to evaluate large language models' reasoning capabilities across pattern recognition, spatial awareness, arithmetic, and logical reasoning. The benchmark features eight puzzle games at three difficulty levels, designed to test both single-turn and multi-turn reasoning through self-reflection with feedback. Experiments with various LLMs show that while models perform adequately on easy and medium tasks, they struggle significantly with harder problems requiring comprehensive reasoning skills. Larger models and reasoning-optimized models outperform instruction-following models, and multi-turn prompting with feedback improves performance, though challenges remain with sequencing, counting, and complex rule-following.

## Method Summary
The benchmark evaluates LLMs on 8 text-based puzzle games (Anagram Scribble, Password Game, Bracket Game, String Search, Crossword Arranger, Text Sudoku, Islands, Ordering Text) across 3 difficulty levels using in-context learning with up to 3 turns of self-reflection. Each game instance is generated programmatically with specific constraint sets. The evaluation uses a grader to verify correctness and provide feedback, allowing models to iteratively correct their solutions. The study tests various model sizes and types including Gemma-2, Llama, Qwen, and GPT variants, using greedy decoding with prompt templates containing constraints, examples, and interaction history.

## Key Results
- Reasoning-optimized models (GPT-o3 Mini) substantially outperform instruction-tuned models on complex constraint satisfaction tasks
- Multi-turn feedback with self-reflection improves LLM solve rates across all difficulty levels
- Models show inverse scaling on some tasks, where extended reasoning traces degrade performance
- 2D spatial reasoning creates a difficulty multiplier beyond what comparable 1D complexity would predict
- Humans solve all tasks given sufficient time, highlighting a gap between human and model capabilities

## Why This Works (Mechanism)

### Mechanism 1
Multi-turn feedback with self-reflection improves LLM solve rates on constrained reasoning tasks. A grader evaluates model outputs and returns specific error messages; the model incorporates this feedback into subsequent turns, allowing iterative correction of constraint violations. This works because LLMs can parse structured feedback and map error messages to appropriate output modifications. Evidence shows consistent accuracy gains from Turn #1 to Turn #3 across models, though feedback provides diminishing returns on Hard tasks where models lack fundamental reasoning capacity.

### Mechanism 2
Reasoning-optimized models substantially outperform instruction-tuned models on complex constraint satisfaction. Models trained with reasoning objectives appear to develop better decomposition and constraint-tracking abilities compared to instruction-following models that optimize for helpfulness over correctness. The performance gap reflects architectural/training differences rather than scale alone, with reasoning models showing inverse scaling on some tasks where extended reasoning traces confuse the model.

### Mechanism 3
2D spatial reasoning creates a difficulty multiplier beyond what comparable 1D complexity would predict. 2D puzzles require simultaneous tracking of row, column, and spatial relationships; models struggle with coordinate-based reasoning and maintaining consistency across multiple dimensions. The gap between 1D and 2D performance reflects fundamental limitations in spatial reasoning rather than just increased token count.

## Foundational Learning

- **Constraint satisfaction as a reasoning probe**: Each game encodes multiple constraints that must be simultaneously satisfied; understanding how constraints compound helps diagnose failure modes. Quick check: Can you explain why Password Game with 6 rules is harder for LLMs than 6 independent single-rule tasks?

- **Self-reflection vs. self-correction**: The paper distinguishes between models that can identify errors given feedback (correction) versus those that can self-diagnose without external signals; this distinction informs architecture choices. Quick check: What is the minimum feedback signal required for a model to correct a constraint violation in Bracket Game?

- **Inverse scaling in reasoning length**: Longer reasoning traces do not monotonically improve performance; understanding when additional computation hurts helps design optimal inference strategies. Quick check: In what scenarios would you truncate a reasoning model's output to improve accuracy?

## Architecture Onboarding

- **Component map**: Prompt Generator -> LLM Player -> Grader -> Interaction Logger -> Game Instance Generator
- **Critical path**: Sample constraint instance → Construct initial prompt → Query LLM → Grader validates → Append feedback if failed → Repeat up to 3 turns → Log results
- **Design tradeoffs**: Greedy vs. sampling decoding (paper uses greedy for reproducibility); zero-shot vs. one-shot (one-shot provides format guidance but may bias toward example patterns); feedback granularity (detailed feedback improves correction but may overwhelm context window)
- **Failure signatures**: Constraint dropping (satisfies 4/6 rules, ignoring remaining constraints); spatial inconsistency (invalid grid despite correct word selection); reasoning collapse (overcomplicated solutions that violate simple rules)
- **First 3 experiments**: 1) Baseline calibration on Easy difficulty with zero-shot prompting; 2) Feedback ablation comparing detailed vs. binary feedback on Medium difficulty; 3) Inverse scaling probe on Hard tasks with truncated reasoning budgets

## Open Questions the Paper Calls Out

- Why does extended reasoning length in reasoning-optimized models lead to "inverse scaling" and performance degradation on specific hard puzzle games? The paper identifies this counter-intuitive trend but only speculates on causes without providing mechanistic proof.

- What specific architectural or training deficits cause the significant misalignment in difficulty perception between humans and LLMs? The paper documents the gap in performance but does not isolate whether this is due to spatial reasoning limitations or tokenization issues.

- Why does multi-turn self-reflection fail to correct specific error types like sequencing and counting, even when explicit feedback is provided? It is unclear if the models fail to interpret the feedback or if the feedback is insufficient to override the model's priors.

## Limitations

- The benchmark may not fully represent the spectrum of reasoning capabilities needed for real-world applications, focusing on constraint satisfaction rather than open-ended problem-solving
- The multi-turn self-reflection mechanism depends heavily on the quality of grader feedback, which may not identify subtle constraint violations
- The stark performance differences between 1D and 2D puzzles indicate the benchmark may be measuring specific types of reasoning rather than general reasoning ability

## Confidence

**High Confidence (Likelihood >80%)**: The benchmark successfully discriminates between model capabilities across difficulty levels; multi-turn prompting with feedback consistently improves performance across models; reasoning-optimized models outperform instruction-tuned models on complex constraint satisfaction tasks

**Medium Confidence (Likelihood 50-80%)**: The 1D vs 2D performance gap reflects fundamental limitations in spatial reasoning; the inverse scaling phenomenon with longer reasoning traces is a genuine architectural limitation; the self-reflection mechanism represents a meaningful advance in constrained reasoning tasks

**Low Confidence (Likelihood <50%)**: The benchmark can reliably predict real-world reasoning performance; the observed performance gaps will persist with future model architectures; the current prompt engineering strategies are optimal for these tasks

## Next Checks

1. **Granularity of Feedback Impact**: Test sensitivity to different levels of feedback granularity by systematically varying the specificity of grader responses (binary vs. constraint-specific vs. heuristic suggestions). Expected outcome: Performance improves with feedback specificity up to a point, then plateaus or degrades due to context window limitations.

2. **Spatial Reasoning Architecture Analysis**: Conduct ablation studies comparing different architectural approaches to spatial reasoning (coordinate-based vs. grid-based representations) across the 2D puzzle games. Expected outcome: Grid-based representations show better performance on spatial puzzles, suggesting architectural limitations rather than scale limitations.

3. **Inverse Scaling Mitigation**: Test whether early termination strategies (truncating reasoning traces when certain confidence thresholds are met) can mitigate the inverse scaling phenomenon. Expected outcome: Truncated reasoning traces improve accuracy on tasks where inverse scaling is observed, suggesting that reasoning length is not always beneficial.