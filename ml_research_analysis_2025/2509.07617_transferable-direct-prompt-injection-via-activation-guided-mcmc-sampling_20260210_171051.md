---
ver: rpa2
title: Transferable Direct Prompt Injection via Activation-Guided MCMC Sampling
arxiv_id: '2509.07617'
source_url: https://arxiv.org/abs/2509.07617
tags:
- prompts
- prompt
- attack
- adversarial
- injection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a transferable direct prompt injection attack
  method that leverages activations from a surrogate model to guide adversarial prompt
  generation without requiring access to the victim model. The approach constructs
  an energy-based model using internal activations to evaluate adversarial prompt
  quality, then employs token-level Markov Chain Monte Carlo sampling to adaptively
  optimize attack prompts in a gradient-free manner.
---

# Transferable Direct Prompt Injection via Activation-Guided MCMC Sampling

## Quick Facts
- arXiv ID: 2509.07617
- Source URL: https://arxiv.org/abs/2509.07617
- Reference count: 25
- Primary result: Achieves 49.6% attack success rate across five LLMs with 34.6% improvement over human-crafted prompts

## Executive Summary
This paper introduces a novel transferable direct prompt injection attack method that leverages internal model activations to guide adversarial prompt generation without requiring access to the victim model. The approach constructs an energy-based model using surrogate model activations to evaluate adversarial prompt quality, then employs token-level Markov Chain Monte Carlo sampling to optimize attack prompts in a gradient-free manner. The technique achieves strong cross-model transferability while producing natural-sounding adversarial prompts that evade perplexity-based detection filters.

## Method Summary
The method trains a binary classifier on surrogate model activations to distinguish successful vs. failed injection attacks, treating it as an Energy-Based Model where lower energy indicates higher attack probability. MCMC sampling with token-level mutations (guided by BERT proposals) optimizes prompts by balancing attack strength (energy) with linguistic naturalness. The approach decouples attacks into Prefix/Infix/Suffix components and recombines them for template diversity, achieving transferability without direct access to victim models.

## Key Results
- Achieves 49.6% attack success rate across five mainstream LLMs
- Demonstrates 34.6% improvement over human-crafted prompts
- Maintains 36.6% success rate on unseen task scenarios
- Generates prompts with perplexity of 127.68 vs 17464.28 for gradient methods
- Pearson correlation of -0.979 between energy scores and attack success rates

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Internal model activations capture semantic patterns that correlate with attack success, allowing a surrogate model to guide adversarial prompt generation for victim models.
- **Mechanism:** A binary classifier is trained on the activations of a surrogate model (specifically layers 20-25) to distinguish successful vs. failed injection attacks. This classifier functions as an Energy-Based Model (EBM), where the logits define an energy landscape; lower energy scores indicate prompts that lie in high-probability regions of the "successful attack" distribution.
- **Core assumption:** Vulnerability patterns in LLMs are encoded in activation space and are sufficiently similar across different model architectures (transferability).
- **Evidence anchors:** [abstract] "...correlation between activations and attack effectiveness..."; [section 4.2.3] "...Pearson correlation coefficient of -0.979 [between energy and ASR]."; [corpus] Weak direct support in corpus; related works focus on defense detection rather than the causality of activation-guided generation.
- **Break condition:** If the surrogate model's activation geometry diverges significantly from the victim's (e.g., vastly different size or training corpus), the energy landscape will not align, and the guidance becomes noise.

### Mechanism 2
- **Claim:** Gradient-free Markov Chain Monte Carlo (MCMC) sampling can optimize discrete prompt tokens effectively by balancing attack strength (energy) with linguistic naturalness.
- **Mechanism:** The process iteratively mutates a prompt token using a Masked Language Model (BERT). It accepts or rejects the mutation based on a probability ratio involving the EBM energy score (targeting attack success) and the MLM probability (targeting naturalness). This allows the prompt to traverse the discrete search space without requiring gradients from the victim.
- **Core assumption:** The search space is locally smooth enough for a random walk to find adversarial regions without getting trapped in local minima of high-perplexity nonsense.
- **Evidence anchors:** [section 3.4] "...acceptance probability... integrating the token probabilities with the energy scores."; [table 3] Shows generated prompts maintain low perplexity (127.68) compared to gradient-based methods (17464.28).
- **Break condition:** If the proposal distribution (BERT) is too weak to generate contextually appropriate synonyms for attack keywords, the acceptance rate drops to near zero, stalling optimization.

### Mechanism 3
- **Claim:** Decoupling prompt structure (Prefix/Infix/Suffix) and recombining them increases template diversity, preventing overfitting to specific phrasing styles.
- **Mechanism:** Rather than optimizing a single string, the method decomposes attacks into functional components (distraction, injection payload, execution simulation). By recombining these parts and optimizing them via MCMC, it creates diverse, natural-sounding variants that evade signature-based defenses.
- **Core assumption:** The "grammar" of an injection attack is compositional; effective attacks can be constructed by mixing and matching functional blocks.
- **Evidence anchors:** [section 3.2] "...adversarial prompt consisting of three parts: Prefix... Infix... Suffix."; [corpus] Indirect support; [arxiv:2509.14285] discusses multi-agent defenses, implying attacks are becoming more structurally complex.
- **Break condition:** If the victim model employs strict structural parsing (e.g., structured queries as in StruQ), purely semantic recombination may fail to breach the logical isolation.

## Foundational Learning

- **Concept: Energy-Based Models (EBMs)**
  - **Why needed here:** The paper treats a classifier as an EBM to score prompts. You must understand that minimizing "energy" is mathematically equivalent to maximizing the likelihood of the "successful attack" class.
  - **Quick check question:** Why does the paper use the sum of exponentiated logits (Eq. 5) rather than just the probability of the "success" class to define energy?

- **Concept: MCMC (Metropolis-Hastings)**
  - **Why needed here:** This is the optimization engine. Unlike Gradient Descent, MCMC is stochastic and designed for sampling from complex distributions without derivatives.
  - **Quick check question:** In the acceptance formula (Eq. 8), why is the term $p_{MLM}(X_i|X_{/i})$ necessary in addition to the energy term $e^{-E(X)}$?

- **Concept: Transferability in Adversarial ML**
  - **Why needed here:** The core value proposition is attacking a black-box victim (e.g., GPT-4o) using a local surrogate (e.g., Qwen).
  - **Quick check question:** Why do white-box gradient attacks (like GCG) often fail to transfer compared to semantic/activation-guided methods?

## Architecture Onboarding

- **Component map:** Data Preprocessor -> Surrogate Model (White-box) -> EBM (The Guide) -> Proposal Generator (BERT) -> MCMC Loop -> Victim Model (Black-box)
- **Critical path:** The training of the EBM is the bottleneck. If the classifier overfits to the surrogate's activations or fails to capture transferable semantic concepts, the MCMC loop has no valid gradient (energy signal) to follow.
- **Design tradeoffs:** Naturalness vs. ASR - the paper explicitly trades raw attack power for naturalness (low perplexity) to bypass filters. Removing the MLM term from the acceptance criteria would likely increase ASR but explode perplexity (making it detectable). Layer Selection - the paper uses layers 20-25. Earlier layers capture syntax (too generic); later layers capture task-specific logits (too specific). Mid-layers capture the "semantic concepts" best suited for transfer.
- **Failure signatures:** High Perplexity - If MCMC accepts too many low-probability tokens, the prompt becomes gibberish (PPL > 1000), triggering defenses. Mode Collapse - If the energy landscape is too flat, MCMC wanders randomly without improving ASR. Zero Transfer - High ASR on surrogate but ~0% on victim models indicates the EBM learned surrogate-specific quirks rather than general vulnerabilities.
- **First 3 experiments:**
  1. **Sanity Check (Overfitting):** Run the MCMC loop using a randomly initialized EBM (untrained). Confirm that ASR does not improve, proving the EBM is actually guiding the process.
  2. **Layer Ablation:** Retrain the EBM on layers 0, 10, 20, and 32. Plot the correlation between "Layer Depth" and "Transfer ASR" to verify the claim that mid-layers are optimal.
  3. **Naturalness Threshold:** Clamp the acceptance probability to reject any token with $p_{MLM} < \epsilon$. Find the $\epsilon$ where the prompt starts sounding unnatural to a human reader.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the framework be enhanced to optimize the trade-off between naturalness and attack capability through fine-tuning of the proposal model?
- **Basis in paper:** [explicit] The authors state in the Limitations section that "the inherent trade-off between naturalness and attack capability deserves attention" and suggest future work could involve "fine-tuning the proposal model."
- **Why unresolved:** While the current method maintains human-acceptable naturalness (PPL=127.68), it is unclear if the attack success rate is being artificially capped to maintain this fluency.
- **What evidence would resolve it:** Experiments measuring the Attack Success Rate (ASR) of prompts generated by fine-tuned proposal models compared to the current baseline, specifically analyzing the Pareto frontier between fluency and efficacy.

### Open Question 2
- **Question:** Can the activation-guided sampling strategy be adapted to evade text classifier-based defenses?
- **Basis in paper:** [explicit] The paper explicitly notes that the current approach "does not address defense based on text classifiers" and calls for future research to investigate "bypassing detection mechanisms."
- **Why unresolved:** The study confirms evasion of perplexity-based filters, but the semantic content of the prompts may still be flagged by dedicated classifier guardrails not tested in the current scope.
- **What evidence would resolve it:** Benchmarking the generated adversarial prompts against state-of-the-art text classifier defenses (e.g., InjecGuard) to determine detection rates.

### Open Question 3
- **Question:** To what extent does the specific architecture of the surrogate model bias the identification of vulnerability patterns?
- **Basis in paper:** [inferred] The method relies on activations from a specific surrogate (Qwen2.5-7B) to generalize an energy landscape, assuming these patterns transfer universally.
- **Why unresolved:** It is uncertain if the "model-agnostic vulnerability patterns" learned are merely artifacts of the surrogate model's specific activation geometry rather than universal features of LLMs.
- **What evidence would resolve it:** A comparative analysis of Energy-based Models trained on structurally diverse surrogate models to verify if they converge on similar low-energy adversarial regions.

## Limitations

- Reliance on activation similarity between surrogate and victim models creates fundamental constraints
- MCMC optimization requires significant computational resources proportional to token count
- Template decomposition approach may fail against models with strict structural parsing defenses
- 49.6% success rate still leaves substantial room for failure and may not scale to all scenarios

## Confidence

**High Confidence (>80% likelihood):**
- Correlation between activation energy scores and attack success rates is robust
- MCMC-based optimization successfully generates low-perplexity prompts
- Transferability across five mainstream LLMs demonstrates practical applicability

**Medium Confidence (50-80% likelihood):**
- Transferability performance will degrade for models with substantially different architectures
- Method maintains effectiveness on unseen task scenarios
- Naturalness optimization effectively evades perplexity-based detection filters

**Low Confidence (<50% likelihood):**
- Attack will remain effective against future models with enhanced structural parsing
- Current template decomposition will generalize to all injection scenarios
- Computational overhead remains practical for large-scale deployment

## Next Checks

1. **Architectural Transferability Stress Test:** Evaluate the attack against diverse model architectures including MoE systems, convolutional transformers, and different tokenization schemes to quantify degradation and identify architectural divergence thresholds.

2. **Template Composition Robustness Analysis:** Systematically vary template component types and numbers to determine adaptability to structural defenses and identify minimum complexity required for successful attacks.

3. **Computational Efficiency Benchmark:** Measure wall-clock time and resource consumption across varying prompt lengths to establish practical deployment constraints and identify optimization bottlenecks.