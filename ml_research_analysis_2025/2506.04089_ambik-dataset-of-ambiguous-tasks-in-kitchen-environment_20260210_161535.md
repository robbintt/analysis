---
ver: rpa2
title: 'AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment'
arxiv_id: '2506.04089'
source_url: https://arxiv.org/abs/2506.04089
tags:
- task
- kitchen
- tasks
- ambiguity
- ambiguous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces AmbiK, a fully textual dataset designed
  for evaluating ambiguity detection methods in embodied AI within kitchen environments.
  The dataset contains 2000 tasks, each paired with an unambiguous counterpart, categorized
  into three types of ambiguity: Human Preferences, Common Sense Knowledge, and Safety.'
---

# AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment

## Quick Facts
- **arXiv ID:** 2506.04089
- **Source URL:** https://arxiv.org/abs/2506.04089
- **Reference count:** 40
- **Primary result:** AmbiK dataset of 2000 textual kitchen tasks (1000 ambiguous/unambiguous pairs) shows logit-based ambiguity detection methods fail due to LLM overconfidence.

## Executive Summary
AmbiK introduces a fully textual dataset for evaluating ambiguity detection in embodied AI agents operating in kitchen environments. The dataset contains 1000 pairs of ambiguous and unambiguous tasks categorized into three ambiguity types: Human Preferences, Common Sense Knowledge, and Safety. Experiments with state-of-the-art methods (KnowNo, LAP, LofreeCP) on GPT-3.5, GPT-4, Llama-2-7B, and Llama-3-8B reveal significant challenges in handling ambiguity, particularly with logit-based methods that rely on model uncertainty estimation. The dataset and findings highlight the need for improved ambiguity detection methods that can reliably identify when clarification is needed.

## Method Summary
The AmbiK dataset contains 2000 textual tasks (1000 pairs of ambiguous and unambiguous instructions) in kitchen environments. Each ambiguous task is paired with an unambiguous counterpart and categorized by type: Human Preferences, Common Sense Knowledge, or Safety. Tasks include environment descriptions, clarifying questions and answers, user intents, and task plans. The evaluation framework tests four methods: KnowNo and LAP (logit-based conformal prediction), LofreeCP (logit-free conformal prediction), and Binary (simple prompting baseline). Models are evaluated using Intent Coverage Rate, Help Rate, Correct Help Rate, Set Size Correctness, and Ambiguity Differentiation metrics.

## Key Results
- Logit-based methods (KnowNo, LAP) achieve near-zero Help Rate and Ambiguity Differentiation, indicating failure to identify ambiguous situations
- Logit-free methods (LofreeCP, Binary) outperform logit-based approaches by using semantic features instead of raw token probabilities
- GPT-3.5 achieves highest Intent Coverage Rate (76.25%) while local models like Llama-2-7B struggle with zero performance in some configurations
- Ambiguity Differentiation (AmbDif) scores near zero across methods reveal inability to distinguish between ambiguous and unambiguous paired tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Ambiguity detection can be evaluated by categorizing ambiguous instructions based on the knowledge required for resolution.
- **Mechanism:** AmbiK pairs ambiguous tasks with unambiguous counterparts, where each ambiguous task triggers specific resolution pathways (asking questions for preferences, applying common knowledge for others). This allows controlled evaluation of when agents should seek clarification.
- **Core Assumption:** The three ambiguity types (Human Preferences, Common Sense Knowledge, Safety) are exhaustive and mutually exclusive for the kitchen domain.
- **Evidence anchors:** [abstract] "categorized by ambiguity type (Human Preferences, Common Sense Knowledge, Safety)"; [Section 3.2] "ambiguity types in AmbiK are aligned with various ways the embodied agent should act in ambiguous situations."
- **Break Condition:** If real-world ambiguous instructions frequently span multiple types, the categorical evaluation may not capture true complexity.

### Mechanism 2
- **Claim:** Logit-based uncertainty estimation methods perform poorly on complex ambiguous tasks because LLM logits are miscalibrated and overconfident.
- **Mechanism:** Methods like KnowNo use softmax of log-probabilities as uncertainty signals. The paper shows these models produce single-option prediction sets for both ambiguous and unambiguous tasks, failing to differentiate.
- **Core Assumption:** Low "Help Rate" and near-zero "Ambiguity Differentiation" scores are primarily due to poor calibration of logits themselves.
- **Evidence anchors:** [abstract] "low performance of methods relying on model logits for uncertainty estimation"; [Section 4.4] "LLMs fine-tuned with RLHF... tend to be overconfident."
- **Break Condition:** If poor performance stems from task framing as multiple-choice QA rather than logit calibration issues.

### Mechanism 3
- **Claim:** Logit-free methods and simple prompting can outperform complex logit-based approaches by re-querying models or using semantic features instead of raw token probabilities.
- **Mechanism:** LofreeCP uses sample frequency, semantic similarity, and entropy from multiple generations as non-conformity scores. Binary method directly prompts LLM to judge its own certainty, bypassing miscalibrated logits.
- **Core Assumption:** Superior performance of these methods indicates fundamental limitation of using next-token probabilities as uncertainty proxies for semantic ambiguity.
- **Evidence anchors:** [Section 4.4] "two that do not rely on internal model information outperform the logit-based methods"; [Table 8] Shows higher Intent Coverage Rate for LofreeCP and Binary.
- **Break Condition:** If performance gain is mainly due to less sensitivity to prompt formatting or number of candidate options.

## Foundational Learning

- **Concept:** **Ambiguity in Embodied AI vs. NLP** 
  - **Why needed here:** Physical task ambiguity (e.g., "put the cup on the table" with multiple cups) has different consequences than text ambiguity (e.g., ambiguous pronoun reference). The paper's definition and taxonomy are specific to embodied settings.
  - **Quick check question:** Why is "put the cup on the table" ambiguous in a kitchen with multiple cups, but "the bank" can be ambiguous in a sentence without physical context?

- **Concept:** **Conformal Prediction (CP) for Uncertainty** 
  - **Why needed here:** CP is the core technique used by evaluated methods to decide when to ask for help. Understanding how it constructs prediction sets with guaranteed probability is key to interpreting results.
  - **Quick check question:** If a CP method produces a prediction set of size 3, what does that mean for the agent's action?

- **Concept:** **LLM Overconfidence and Calibration** 
  - **Why needed here:** The paper's central negative result is that logit-based methods fail because LLMs are overconfident. Understanding why log-probabilities don't align with true uncertainty is crucial.
  - **Quick check question:** An LLM assigns 90% probability to its top-1 answer. Does this mean it's 90% likely to be correct? Why or why not?

## Architecture Onboarding

- **Component Map:** Dataset Sampler -> LLM Planner -> Uncertainty Estimator -> CP Calibrator -> Evaluator

- **Critical Path:** The success of evaluation hinges on the **Uncertainty Estimator** component. If this component fails to produce meaningful non-conformity scores, the subsequent CP step cannot produce useful prediction sets, leading to failure in all downstream metrics.

- **Design Tradeoffs:**
  - **Task Framing:** Framing planning as multiple-choice QA simplifies evaluation but may not reflect real planning with open-ended action spaces
  - **Context:** Using full task plans vs. single-action context showed minimal performance difference, but providing plans adds computational cost
  - **Metrics:** ICR measures plan correctness while HR/CHR measure help-seeking behavior. A method can have high ICR but terrible CHR, revealing tradeoff between competence and appropriate reliance

- **Failure Signatures:**
  - **Always/Never Asks for Help:** Help Rate near 0% or 100% across all ambiguity types indicates failure to learn intended differentiation
  - **Zero Performance with Llama-2-7B:** When same model is used for both generation and choice, ICR is 0, suggesting two-stage pipeline failure with weak models
  - **Ambiguity Differentiation near 0%:** This metric tests if method produces larger prediction sets for ambiguous tasks. Near-zero score indicates inability to distinguish paired tasks

- **First 3 Experiments:**
  1. **Reproduce Binary Baseline:** Run simple Binary method on 100 AmbiK tasks with GPT-3.5 to establish baseline with minimal implementation complexity
  2. **Implement Logit-Free CP Variant:** Build minimal LofreeCP using only sample frequency from multiple generations, test on same subset, compare against Binary
  3. **Ablate Ambiguity Types:** Run best method from experiments 1 & 2 separately on PREFERENCES, COMMON SENSE, and SAFETY subsets to analyze where method succeeds or fails to differentiate behavior by type

## Open Questions the Paper Calls Out

- **Question:** Does incorporating visual data or scene graphs improve ambiguity detection performance compared to textual-only environment descriptions?
  - **Basis in paper:** [explicit] Limitations section states extending approach to include "richer descriptions, such as object relationships or visual data, would be a valuable avenue for future research."
  - **Why unresolved:** Current study restricts inputs to text-only object lists to align with existing baselines and focus on LLM capabilities
  - **What evidence would resolve it:** Comparative study benchmarking methods on multi-modal version of AmbiK versus text-only baseline

- **Question:** Can fine-tuning LLMs on ambiguity-specific datasets yield better performance than few-shot prompting methods?
  - **Basis in paper:** [explicit] Limitations section notes that while few-shot learning is useful for prototyping, "Fine-tuning can yield better performance and more reliable handling of ambiguities."
  - **Why unresolved:** Authors focused on few-shot prompting to evaluate general model capabilities without task-specific weight updates
  - **What evidence would resolve it:** Experiments measuring ICR and HR on AmbiK for models fine-tuned on training split versus few-shot prompted models

- **Question:** How effectively do ambiguity detection methods generalize to tasks with multiple valid interpretations rather than single-intent tasks?
  - **Basis in paper:** [explicit] Limitations section acknowledges AmbiK focuses exclusively on tasks with one correct interpretation, whereas real-life settings often allow multiple valid interpretations
  - **Why unresolved:** Dataset was constructed to have single ground-truth user intent to facilitate standardized evaluation
  - **What evidence would resolve it:** Evaluating existing methods on extended dataset where ambiguous tasks have multiple acceptable resolution paths

## Limitations

- **Exhaustiveness of Taxonomy:** The three ambiguity types (Preferences, Common Sense, Safety) may not capture all forms of ambiguity encountered by embodied agents in real-world scenarios
- **Synthetic Task Pairs:** Evaluation relies on controlled ambiguous/unambiguous task pairs that may not fully represent complexity and variability of natural ambiguous instructions
- **Single-Interpretation Focus:** Dataset exclusively contains tasks with one correct interpretation, not reflecting real-life settings where multiple valid interpretations are possible

## Confidence

**High Confidence:** Experimental results showing poor performance of logit-based methods and better performance of logit-free methods are directly observable and align with known LLM overconfidence issues

**Medium Confidence:** Interpretation that AmbiK's three ambiguity types successfully capture distinct resolution pathways is reasonable given controlled task design but requires validation in naturalistic settings

**Medium Confidence:** Conclusion that AmbiK provides valuable benchmark for ambiguity detection in embodied AI is supported by systematic construction and comprehensive evaluation, though generalizability to non-kitchen environments remains to be tested

## Next Checks

1. **Validate Taxonomy Exhaustiveness:** Conduct human study where annotators categorize real-world ambiguous kitchen instructions to assess whether three AmbiK types adequately cover all cases or if new categories emerge

2. **Test Method Transferability:** Apply best-performing ambiguity detection methods (LofreeCP and Binary) to different embodied AI domain (e.g., workshop or living room) using AmbiK's methodology to evaluate whether logit-free advantage generalizes beyond kitchen environments

3. **Analyze Overlap Cases:** Systematically examine AmbiK tasks where ambiguity spans multiple types to determine how current evaluation metrics handle such cases and whether categorical framework needs refinement