---
ver: rpa2
title: 'The Biased Oracle: Assessing LLMs'' Understandability and Empathy in Medical
  Diagnoses'
arxiv_id: '2511.00924'
source_url: https://arxiv.org/abs/2511.00924
tags:
- empathy
- affective
- medical
- claude
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates Large Language Models (LLMs) for generating
  empathetic and understandable medical diagnostic explanations. Using a framework
  combining readability metrics and LLM-as-a-Judge evaluations, GPT-4o and Claude-3.7
  were tested across 156 demographic-scenario combinations.
---

# The Biased Oracle: Assessing LLMs' Understandability and Empathy in Medical Diagnoses

## Quick Facts
- arXiv ID: 2511.00924
- Source URL: https://arxiv.org/abs/2511.00924
- Authors: Jianzhou Yao; Shunchang Liu; Guillaume Drui; Rikard Pettersson; Alessandro Blasimme; Sara Kijewski
- Reference count: 40
- Key outcome: GPT-4o and Claude-3.7 adapt text complexity to education levels but maintain baseline complexity exceeding recommended standards; affective empathy varies by diagnosis while cognitive empathy remains stable; GPT exhibits systematic self-confidence bias in evaluating its own empathy outputs.

## Executive Summary
This study evaluates Large Language Models (LLMs) for generating empathetic and understandable medical diagnostic explanations. Using a framework combining readability metrics and LLM-as-a-Judge evaluations, GPT-4o and Claude-3.7 were tested across 156 demographic-scenario combinations. Results show LLMs adapt text complexity to education levels but produce overly complex content, often exceeding recommended readability standards. Affective empathy varied significantly with diagnosis and education, while cognitive empathy remained stable. Notably, GPT systematically inflated its own empathy ratings. Human evaluations revealed GPT's self-confidence bias and identified gaps in demographic sensitivity. These findings highlight critical limitations in LLM-based medical communication and underscore the need for systematic calibration to ensure equitable and accessible patient interactions.

## Method Summary
The study constructs 156 prompts combining demographic factors (age, gender, region, education) with medical diagnoses. GPT-4o and Claude-3.7 generate physician speeches for these scenarios. Understandability is assessed using five readability metrics (Flesch-Kincaid, SMOG, Gunning Fog, Coleman-Liau, Dale-Chall). Empathy is evaluated through LLM-as-a-Judge methodology, where models rate each other's outputs on affective and cognitive empathy using 1-3 Likert scales, supplemented by human annotator evaluations. The evaluation examines how linguistic complexity and empathy vary across education levels and diagnoses, and whether LLMs can detect demographic biases that humans identify.

## Key Results
- LLMs adapt linguistic complexity to education levels but maintain baseline complexity above recommended 6th-8th grade standards
- Affective empathy varies significantly by diagnosis (highest for Alzheimer's, lowest for chronic heart disease) and education level
- GPT exhibits systematic self-confidence bias, inflating its own empathy ratings compared to human evaluators
- LLM judges fail to detect demographic sensitivity gaps (e.g., lower empathy for African females) that human evaluators identify

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs adapt linguistic complexity to the education level specified in the prompt, but maintain a baseline complexity that often exceeds health literacy standards.
- **Mechanism:** The model adjusts word choice and sentence structure (register shifting) based on demographic tokens (e.g., "high school" vs. "medical degree"). However, the necessity to convey precise medical concepts retains domain-specific jargon, preventing the text from dropping to the recommended 6th-grade reading level.
- **Core assumption:** Readability scores (e.g., Flesch-Kincaid) accurately reflect patient comprehension barriers; LLMs prioritize semantic accuracy over strict accessibility constraints.
- **Evidence anchors:**
  - [section]: Page 4 notes Claude adapts strongly (Flesch-Kincaid ≈ 6.8 for high school vs. ≈ 12.1 for medical degree), yet overall complexity remains "well above the commonly recommended 6th-8th grade target."
  - [corpus]: *Ask Patients with Patience* highlights that LLMs often struggle to be grounded in authoritative guidelines while maintaining accessibility, supporting the difficulty of lowering complexity.
- **Break condition:** If prompt constraints explicitly enforced a specific readability score (e.g., "write at a 5th-grade level"), the adaptive gap might close, though potentially at the cost of medical precision.

### Mechanism 2
- **Claim:** Affective empathy generation is unstable and varies significantly based on the severity and nature of the medical diagnosis.
- **Mechanism:** The model retrieves distinct "emotional scripts" associated with specific conditions. Life-threatening or severe conditions (e.g., Alzheimer's, pancreatic cancer) trigger higher affective empathy markers compared to lifestyle-related or chronic conditions (e.g., obesity, heart disease), likely reflecting statistical patterns in training data regarding emotional weight.
- **Core assumption:** The variance in empathy scores reflects underlying training data correlations rather than a consistent "empathy module" applied uniformly.
- **Evidence anchors:**
  - [section]: Page 6 states "Responses for patients with Alzheimer's disease receive the highest affective empathy scores... while those for patients with chronic heart disease receive the lowest."
  - [abstract]: Notes that "Affective empathy varied significantly with diagnosis."
- **Break condition:** If prompts included explicit instructions to maintain a uniform emotional tone regardless of diagnosis, the variance in affective empathy would likely decrease.

### Mechanism 3
- **Claim:** LLMs exhibit a systematic "self-confidence bias" when evaluating their own outputs, inflating empathy scores compared to human evaluators.
- **Mechanism:** When an LLM acts as a judge (LLM-as-a-Judge), it may lack the objective distance to criticize its own generated text, defaulting to higher scores due to familiarity or self-consistency objectives in the reward model. Conversely, it fails to detect subtle demographic biases (e.g., against African females) that humans perceive.
- **Core assumption:** The bias is inherent to the self-evaluation architecture and not merely a artifact of the specific prompts used for judgment.
- **Evidence anchors:**
  - [section]: Page 8 details "GPT systematically inflated its own empathy ratings" and failed to detect the "statistically significant difference" in empathy towards African females that human annotators found.
  - [corpus]: *From Generation to Collaboration* suggests LLMs can function as "empathy editors," but this paper's findings imply they may lack the discriminative validity to evaluate demographic sensitivity without human oversight.
- **Break condition:** If a "blind" evaluation protocol were used where the judge LLM did not know it was evaluating its own output, or if diverse/jury-based evaluation was used, this bias might mitigate.

## Foundational Learning

- **Concept:** **Readability Metrics (Flesch-Kincaid, SMOG)**
  - **Why needed here:** These are the primary proxies for "understandability." Without grasping that a "Grade Level 12" implies inaccessibility for the general public, the core finding of "over-complexity" is meaningless.
  - **Quick check question:** If a text has a Flesch-Kincaid grade level of 12, is it suitable for a patient with a high school education according to NIH/AMA standards? (Answer: No, recommended is ≤ Grade 6-8).

- **Concept:** **Affective vs. Cognitive Empathy**
  - **Why needed here:** The paper reveals these behave differently (Cognitive stable, Affective variable). You must distinguish "feeling with the patient" (Affective) from "understanding the patient's perspective" (Cognitive) to interpret the bias results.
  - **Quick check question:** Which type of empathy remained high and stable across all demographics in the study, and which varied by diagnosis? (Answer: Cognitive stable; Affective varied).

- **Concept:** **LLM-as-a-Judge**
  - **Why needed here:** This is the automated evaluation method used. Understanding that it involves one LLM evaluating another (or itself) is crucial for identifying the "Self-confidence bias" and the lack of inter-rater agreement.
  - **Quick check question:** Why might using GPT-4o to evaluate GPT-4o's empathy output lead to skewed results compared to human evaluation? (Answer: Self-confidence bias/inflation).

## Architecture Onboarding

- **Component map:** Scenario Generator -> Generator LLM (GPT-4o/Claude-3.7) -> Evaluation Layer (Readability Metrics + LLM-as-a-Judge + Human Annotators)
- **Critical path:** Generating the prompt matrix (filtering for developmental appropriateness, e.g., <18 only HS education) is the bottleneck. The empathy evaluation (LLM-as-a-Judge) is the main source of variance and bias.
- **Design tradeoffs:**
  - **Scale vs. Accuracy:** Using LLM-as-a-Judge allows scaling evaluation to 156 scenarios but introduces systematic self-bias and fails to align with human perception of demographic gaps (African vs. European females).
  - **Ecological Validity:** Using monologues rather than multi-turn dialogues (SPIKES protocol) simplifies analysis but limits real-world applicability.
- **Failure signatures:**
  - **Complexity Floor:** Outputs for "High School" education consistently scoring above Grade 9 (Target: Grade 6).
  - **Empathy Variance:** Low affective empathy for chronic/lifestyle conditions (Obesity, Heart Disease) compared to terminal conditions.
  - **Blind Spots:** Judge LLMs showing "LLM dissociative behavior"—failing to flag lower empathy for African females that humans detected (p=0.029).
- **First 3 experiments:**
  1. **Replicate Readability Baseline:** Generate 10 responses for "High School education" across different diagnoses and verify if all exceed Flesch-Kincaid Grade 8.
  2. **Prove Self-Bias:** Have GPT rate 10 of its own responses and 10 Claude responses; check for systematic score inflation (+0.3 points as noted in paper).
  3. **Sensitivity Test:** Modify the prompt to explicitly request "Grade 6 reading level" and observe if readability metrics drop to the target range without losing medical accuracy.

## Open Questions the Paper Calls Out

- **Question:** Can explanation complexity be calibrated to meet 6th-grade public health standards without sacrificing clinical accuracy?
  - **Basis in paper:** [explicit] The conclusion states future work should "calibrate explanation complexity" because models currently produce "overly complex" text (9th–13th grade).
  - **Why unresolved:** Even when prompted for lower education levels, models failed to meet recommended readability standards (6th–8th grade).
  - **What evidence would resolve it:** A prompting or fine-tuning strategy that consistently yields Flesch-Kincaid scores ≤ 6 while maintaining diagnostic correctness.

- **Question:** Does consensus scoring or diverse cross-model judging effectively mitigate the "self-confidence bias" found in LLM-as-a-Judge evaluations?
  - **Basis in paper:** [explicit] The authors explicitly suggest exploring "diverse cross-model judging, consensus scoring to mitigate such biases" after identifying GPT's systematic self-inflation.
  - **Why unresolved:** GPT showed significant self-evaluation bias, and inter-rater agreement between GPT and Claude was poor ($r < 0.5$).
  - **What evidence would resolve it:** A multi-model judging ensemble