---
ver: rpa2
title: 'CheX-DS: Improving Chest X-ray Image Classification with Ensemble Learning
  Based on DenseNet and Swin Transformer'
arxiv_id: '2505.11168'
source_url: https://arxiv.org/abs/2505.11168
tags:
- loss
- learning
- ensemble
- transformer
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CheX-DS, an ensemble model combining DenseNet
  and Swin Transformer for multi-label chest X-ray classification on the NIH ChestX-ray14
  dataset. The method uses weighted binary cross-entropy and asymmetric loss to address
  class imbalance in long-tail data.
---

# CheX-DS: Improving Chest X-ray Image Classification with Ensemble Learning Based on DenseNet and Swin Transformer

## Quick Facts
- arXiv ID: 2505.11168
- Source URL: https://arxiv.org/abs/2505.11168
- Authors: Xinran Li; Yu Liu; Xiujuan Xu; Xiaowei Zhao
- Reference count: 25
- Primary result: CheX-DS achieves 83.76% mean AUC on NIH ChestX-ray14, outperforming state-of-the-art on most disease labels including "No Finding"

## Executive Summary
This paper introduces CheX-DS, an ensemble model that combines DenseNet121 and Swin Transformer Base for multi-label chest X-ray classification. The method addresses class imbalance through a hybrid loss function (weighted BCE + asymmetric loss) and optimizes ensemble weights via differential evolution. Tested on the NIH ChestX-ray14 dataset, CheX-DS achieves 83.76% mean AUC, outperforming individual models and previous state-of-the-art approaches across most disease labels, including challenging "No Finding" cases.

## Method Summary
CheX-DS uses DenseNet121 and Swin Transformer Base as base models, both pretrained on ImageNet and fine-tuned on NIH ChestX-ray14. Images are resized from 1024×1024 to 224×224 and augmented with random horizontal flips (50%) and rotations (±10°). A combined loss function merges weighted binary cross-entropy with asymmetric loss (γ⁺=1, γ⁻=4, m=0.05) to handle long-tail, multi-label data. After independent fine-tuning, differential evolution optimizes ensemble weights that are constrained to sum to 1, producing final predictions via weighted averaging.

## Key Results
- Mean AUC of 83.76% across 15 classes, outperforming previous state-of-the-art
- Superior performance on "No Finding" class compared to individual DenseNet and Swin models
- Consistent improvement across most disease labels, demonstrating ensemble effectiveness

## Why This Works (Mechanism)

### Mechanism 1
Combining DenseNet and Swin Transformer via weighted ensemble improves multi-label chest X-ray classification over either architecture alone. DenseNet121 captures local features through dense connectivity and localized receptive fields. Swin Transformer captures global dependencies via hierarchical shifted-window self-attention. Predictions from both are combined through weighted averaging, allowing the model to leverage complementary feature representations—local texture patterns (e.g., opacities) and global spatial relationships (e.g., organ boundaries). The core assumption is that CNNs and Transformers encode non-redundant, complementary information; their prediction errors are at least partially uncorrelated.

### Mechanism 2
Combining weighted binary cross-entropy with asymmetric loss improves performance on long-tail, multi-label data compared to standard BCE. Weighted BCE assigns class-specific weights based on positive-sample ratios (ρ), upweighting rare classes. Asymmetric loss (a focal variant) applies different focusing parameters (γ⁺=1, γ⁻=4) to positive/negative samples and uses probability margining (m=0.05), reducing loss contribution from easy negatives. Together, these address inter-class imbalance (some diseases far more common) and intra-class imbalance (far more negatives than positives per class). The core assumption is that long-tail distributions cause gradient signals to be dominated by head classes; reweighting and focusing can restore balance without oversampling.

### Mechanism 3
Differential evolution optimization finds ensemble weights that outperform uniform or manually tuned weighting. Differential evolution performs stochastic global search over continuous weight space, constrained such that weights sum to 1. The algorithm evaluates candidate weight vectors against validation performance (presumably AUC) and iteratively mutates/crosses candidates toward optima. This avoids manual weight tuning bias and adapts to per-class model strengths. The core assumption is that optimal weights differ by class and model; a global search can discover these without gradient-based optimization.

## Foundational Learning

- **Concept: Multi-label classification with BCE**
  - Why needed here: Each X-ray may have multiple concurrent disease labels (e.g., cardiomegaly + edema); standard softmax is inappropriate.
  - Quick check question: Can you explain why sigmoid + BCE per class is used instead of softmax + cross-entropy for this task?

- **Concept: Long-tail distribution in medical data**
  - Why needed here: Hernia (0.44%) vs. Infiltration (38.44%) creates severe imbalance; naive training biases toward head classes.
  - Quick check question: Given a dataset with 1000 Infiltration and 10 Hernia samples, what happens to a model trained with unweighted BCE?

- **Concept: Vision Transformer self-attention**
  - Why needed here: Swin Transformer's shifted-window attention captures global context; understanding this clarifies why it complements CNNs.
  - Quick check question: How does shifted-window attention in Swin differ from global self-attention in ViT, and why does this reduce computational cost?

## Architecture Onboarding

- **Component map:**
  DenseNet121 backbone (pre-trained on ImageNet) -> Swin Transformer Base backbone (pre-trained on ImageNet) -> Loss module (weighted BCE + asymmetric loss) -> Ensemble module (differential evolution weights) -> Final probability vector

- **Critical path:**
  1. Resize input X-rays from 1024×1024 -> 224×224
  2. Apply augmentation (50% horizontal flip, ±10° rotation)
  3. Forward pass through both backbones independently (fine-tuned, not frozen)
  4. Compute combined loss, backpropagate to each backbone
  5. After training, run differential evolution on validation set to find optimal ensemble weights
  6. During inference, weight-average predictions from both models

- **Design tradeoffs:**
  - Image downsampling (224×224) reduces GPU memory but may lose fine-grained details relevant to small pathologies (e.g., nodules)
  - Ensemble improves performance but doubles inference compute and memory
  - Differential evolution adds training-time overhead; not suitable for real-time weight updates

- **Failure signatures:**
  - Very low AUC on tail classes (Hernia, Fibrosis) despite loss reweighting -> γ⁻ may be too low or validation set too small for weight optimization
  - Ensemble underperforms best single model -> check weight constraints (must sum to 1); differential evolution may have converged to local optimum
  - Training loss decreases but validation AUC plateaus early -> dataset split may be non-representative; consider stratified sampling by disease frequency

- **First 3 experiments:**
  1. Replicate single-model baselines: Train DenseNet121 and Swin Transformer Base independently with the improved loss on NIH ChestX-ray14 (70/10/20 split). Verify mean AUC ≈ 82.51% (Swin) and 82.84% (DenseNet) as reported in Table II.
  2. Ablation on loss components: Compare (a) BCE only, (b) weighted BCE only, (c) weighted BCE + asymmetric loss. Quantify per-class AUC delta to isolate contribution of asymmetric loss on tail classes.
  3. Ensemble weight sensitivity: Replace differential evolution with uniform weights (0.5/0.5) and grid search (0.3–0.7 in 0.1 steps). Compare mean AUC to understand optimization gain magnitude.

## Open Questions the Paper Calls Out
- Can the CheX-DS framework achieve further performance gains by integrating more recent or diverse architectural base models?
- Does the CheX-DS model maintain its superior performance when validated on external chest X-ray datasets?
- How does the aggressive down-sampling of input images affect the detection of subtle pathologies?

## Limitations
- Differential evolution hyperparameters (population size, generations, mutation/crossover rates) were not specified
- Image resolution reduced from 1024×1024 to 224×224, potentially losing fine-grained details
- Performance validation limited to NIH ChestX-ray14 dataset without external validation

## Confidence
- Confidence in core claim (DenseNet+Swin ensemble improves performance): Medium-High
- Confidence in loss function improvement: Medium
- Confidence in differential evolution optimization approach: Low-Medium

## Next Checks
1. Ablation study on loss components: Train three models (BCE only, weighted BCE only, weighted BCE + asymmetric loss) and compare per-class performance, particularly on tail classes, to quantify the marginal benefit of asymmetric loss.

2. Ensemble weight sensitivity analysis: Replace differential evolution with uniform weights and grid search across the weight space. Compare mean AUC to determine whether the optimization provides significant gains over simpler approaches.

3. Cross-dataset generalization test: Evaluate the trained CheX-DS model on an external chest X-ray dataset (e.g., CheXpert or MIMIC-CXR) to assess whether the ensemble and loss improvements generalize beyond NIH ChestX-ray14.