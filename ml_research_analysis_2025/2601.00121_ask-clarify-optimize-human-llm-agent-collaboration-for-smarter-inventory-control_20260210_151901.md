---
ver: rpa2
title: 'Ask, Clarify, Optimize: Human-LLM Agent Collaboration for Smarter Inventory
  Control'
arxiv_id: '2601.00121'
source_url: https://arxiv.org/abs/2601.00121
tags:
- inventory
- demand
- cost
- policy
- order
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of making stochastic inventory
  optimization accessible to non-expert managers. It proposes a hybrid agentic framework
  that separates natural language interaction from mathematical computation: an LLM-based
  interface extracts and clarifies parameters, while a solver computes optimal policies.'
---

# Ask, Clarify, Optimize: Human-LLM Agent Collaboration for Smarter Inventory Control

## Quick Facts
- arXiv ID: 2601.00121
- Source URL: https://arxiv.org/abs/2601.00121
- Reference count: 40
- Primary result: Hybrid LLM-agent framework reduces inventory costs by 32.1% vs. GPT-4o end-to-end baseline

## Executive Summary
This paper addresses the challenge of making stochastic inventory optimization accessible to non-expert managers by proposing a hybrid agentic framework. The approach separates natural language interaction from mathematical computation: an LLM-based interface extracts and clarifies parameters while a solver computes optimal policies. Evaluation on 70 scenarios demonstrates that the hybrid approach achieves 32.1% cost reduction versus a GPT-4o end-to-end baseline, with the advantage widening under longer lead times and high penalty regimes. Notably, perfect information alone does not improve GPT-4o's performance, confirming that the bottleneck is computational rather than informational. The framework makes rigorous operations research usable for small businesses by using LLMs as intelligent orchestrators rather than direct solvers.

## Method Summary
The paper proposes a three-agent pipeline architecture for stochastic inventory control. The Information Extraction Agent (GPT-5-mini) converses with users, maintaining an explicit Parameter Specification Table to prevent context drift and hallucination. The Optimization Agent selects between (s,S) grid search and DQN solvers based on user risk tolerance. The Policy Interpretation Agent translates mathematical policies into natural language recommendations. A Human Imitator (Qwen2.5-7B + LoRA fine-tuned on 1,184 human dialogue turns) simulates boundedly rational manager behavior for evaluation. The framework was tested on 70 procedurally generated inventory scenarios, comparing the hybrid approach against GPT-4o interactive baseline and GPT-4o with perfect parameters.

## Key Results
- Hybrid framework reduces inventory costs by 32.1% versus GPT-4o end-to-end baseline
- Cost reduction increases with problem complexity: 26.8% for short lead times to 33.8% for long lead times
- Perfect information alone does not improve GPT-4o performance (p=0.66), confirming computational bottleneck
- Win rate of 97.1% for hybrid approach across evaluation scenarios

## Why This Works (Mechanism)

### Mechanism 1
Separating semantic reasoning from mathematical optimization eliminates the "hallucination tax" that LLMs incur when acting as end-to-end stochastic solvers. The LLM handles natural language elicitation and translation, while a dedicated OR solver computes policies. This prevents LLMs from generating "plausible but suboptimal" recommendations that violate constraints or miscalculate tail risk. The bottleneck in LLM decision-making is computational (stochastic reasoning) rather than informational (parameter extraction).

### Mechanism 2
Explicit memory architecture with a Parameter Specification Table prevents context drift and fabrication during multi-turn dialogue. The Information Extraction Agent maintains a persistent, structured table rather than relying on implicit context-window memory. Each conversational round updates specific fields with defined/undefined status, enabling conflict detection and targeted clarification. This is essential for operations research applications where long, unstructured dialogues are prone to inconsistency.

### Mechanism 3
The hybrid framework's advantage scales non-linearly with problem complexity—specifically lead time length and economic stakes—because solvers explicitly track pipeline inventory and integrate over full probability distributions. As lead time grows, pipeline orders (hidden in-transit inventory) become critical. DQN and (s,S) solvers track this state; LLMs exhibit myopia toward on-hand inventory only. Similarly, high penalty/cost structures amplify errors from heuristic approximations.

## Foundational Learning

- Concept: (s,S) inventory policy
  - Why needed here: This classical policy structure underpins the Optimization Agent's grid search. Understanding reorder point (s) and order-up-to level (S) is essential for interpreting solver outputs.
  - Quick check question: Given s=35, S=150, and current inventory position of 30, what quantity should be ordered?

- Concept: Markov Decision Process (MDP) formulation for inventory control
  - Why needed here: The DQN solver requires state representation (on-hand inventory + pipeline orders), action space (order quantities), and cost function. Without this, the neural policy is a black box.
  - Quick check question: For a lost-sales system with lead time L=3, what are the state dimensions at time t?

- Concept: Bounded rationality in user simulation
  - Why needed here: The Human Imitator is trained to replicate human inconsistency, not ideal behavior. Understanding this distinction is critical for interpreting evaluation results.
  - Quick check question: Why does the paper use Epoch 2 (not Epoch 3) of the fine-tuned imitator despite higher perplexity at Epoch 3?

## Architecture Onboarding

- Component map: User input → Information Extraction (table updates + conflict detection) → Optimization Agent (solver dispatch) → Policy Interpretation → User-facing recommendation

- Critical path: Information Extraction Agent maintains Parameter Specification Table across dialogue rounds, detects conflicts, and terminates when all parameters defined. Optimization Agent then selects appropriate solver (s,S or DQN) based on user risk tolerance λ∈[-10,10]. Policy Interpretation Agent translates mathematical policies into actionable recommendations.

- Design tradeoffs: GPT-5-mini vs GPT-4o for extraction (paper uses smaller model to reduce cost); DQN vs (s,S) (DQN typically achieves lower cost but higher variance and lower interpretability); Epoch 2 vs Epoch 3 for Human Imitator (Epoch 2 chosen for semantic fidelity despite higher perplexity).

- Failure signatures: Fluency trap (GPT-4o recommends reorder point 89 exceeding max inventory 80); Heuristic tax (GPT-4o uses mean-based heuristics ignoring tail risk even with perfect parameters); Role confusion (Human Imitator reverts to "assistant" persona instead of boundedly rational manager).

- First 3 experiments:
  1. Reproduce Treatment A vs B gap: Run hybrid framework against GPT-4o end-to-end on 10 instances; verify ~30% cost reduction
  2. Stress-test conflict detection: Feed deliberately contradictory specifications (e.g., lost-sale + per-day penalty) and verify agent flags conflict per Examples 4-5
  3. Vary lead time systematically: Compare performance at L∈{1,3,7,14} days to replicate complexity premium finding

## Open Questions the Paper Calls Out

- Does the hybrid framework maintain its efficacy when deployed with real human subjects as opposed to the synthetic "Human Imitator"? The current evaluation relies on a fine-tuned digital twin which simulates bounded rationality but may not capture the full complexity or patience of actual managerial dialogue.

- Can the architecture be successfully generalized to other stochastic optimization domains, such as dynamic pricing or workforce scheduling? The paper proposes that the framework can support other domains by swapping the underlying solver, but inventory control relies on standardized parameters while other domains may involve more subjective trade-offs.

- Do "reasoning" LLMs (e.g., models using Chain-of-Thought) reduce the "Hallucination Tax," or is the computational bottleneck structural to autoregressive models? The study concludes the bottleneck is "fundamentally computational" based on GPT-4o, but does not test if next-generation models with internal search or simulation capabilities can bridge the gap.

- Does the interaction length (approx. 9 rounds) required for parameter extraction create prohibitive cognitive friction for busy managers? The paper optimizes for policy cost, not interaction efficiency or user drop-off rates, though SMEs have "tight operational bandwidth."

## Limitations

- The 1,184-turn human dialogue dataset for Human Imitator training is not publicly released, preventing independent validation of the simulator's fidelity to real human behavior.

- DQN architecture details (layer sizes, hidden dimensions, replay buffer configuration) are underspecified, making exact reproduction challenging.

- GPT-5-mini is referenced for Information Extraction Agent but is not publicly available, requiring substitution with alternative models that may not replicate reported performance.

## Confidence

**High Confidence**: The core architectural claim that separating parameter extraction from optimization yields ~30% cost reduction is well-supported by the treatment comparison (A vs B) and perfect-information control (Treatment C).

**Medium Confidence**: The scaling claim that hybrid advantage increases with lead time complexity and penalty/cost asymmetry is supported by data but relies on a single paper-specific finding without extensive external validation.

**Low Confidence**: The Human Imitator's behavioral fidelity to real humans cannot be independently verified without access to training data or real user studies.

## Next Checks

1. Replicate the core cost reduction finding: Implement the three-agent architecture using publicly available models and run 10-20 scenarios comparing hybrid vs GPT-4o end-to-end, verifying the ~30% cost reduction.

2. Test conflict detection robustness: Create systematic test suite of contradictory parameter specifications and verify the Information Extraction Agent correctly flags conflicts, measuring false positive/negative rates.

3. Validate complexity scaling: Implement parameterized problem generator varying lead time L∈{1,3,7,14} and cost structures, running both hybrid and GPT-4o baselines across this grid to verify non-linear scaling advantage.