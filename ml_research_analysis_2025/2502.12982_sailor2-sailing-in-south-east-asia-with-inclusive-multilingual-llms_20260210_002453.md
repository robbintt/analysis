---
ver: rpa2
title: 'Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs'
arxiv_id: '2502.12982'
source_url: https://arxiv.org/abs/2502.12982
tags:
- sailor2
- language
- qwen2
- data
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Sailor2 is a family of open-source multilingual language models
  optimized for Southeast Asian (SEA) languages, available in 1B, 8B, and 20B sizes.
  It achieves GPT-4o-level performance across 13 SEA languages by leveraging model
  expansion, two-stage continual pre-training on 400B SEA-specific tokens, and balanced
  instruction tuning on 4.8M examples.
---

# Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs

## Quick Facts
- **arXiv ID:** 2502.12982
- **Source URL:** https://arxiv.org/abs/2502.12982
- **Reference count:** 40
- **Primary result:** 20B-parameter multilingual LLM achieves GPT-4o-level performance across 13 Southeast Asian languages while maintaining strong English and Chinese capabilities

## Executive Summary
Sailor2 is a family of open-source multilingual language models specifically optimized for Southeast Asian (SEA) languages. Available in 1B, 8B, and 20B sizes, these models achieve GPT-4o-level performance across 13 SEA languages by leveraging model expansion, two-stage continual pre-training on 400B SEA-specific tokens, and balanced instruction tuning on 4.8M examples. The approach successfully addresses the challenge of multilingual model development where existing models struggle with SEA languages despite strong performance in English and Chinese.

## Method Summary
Sailor2 builds on Qwen2.5 base models through model expansion (block expansion) to increase capacity for SEA languages. The training pipeline consists of two-stage continual pre-training (450B tokens with RegMix data balancing, followed by 60B high-quality tokens with decaying learning rate) and two-stage post-training (SFT followed by LR-DPO). The models are trained using Megatron-Sailor2 with Intra-Document masking and Zero Bubble Pipeline Parallelism. Instruction data selection employs a joint reward-perplexity ranking mechanism to identify high-quality, under-trained examples for SEA languages.

## Key Results
- 20B model achieves 50-50 win rate against GPT-4o on SEA-WildBench
- Maintains strong performance in English and Chinese (Qwen1.5→Sailor: +1.35 PPL vs Qwen2.5→Sailor2: +0.08 PPL)
- Long-context training (128K) and speculative decoding implemented for inference speedup
- Comprehensive evaluation shows consistent gains across all 13 SEA languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model expansion creates additional capacity for new multilingual knowledge without overwriting existing linguistic representations.
- Mechanism: Block-expansion adds new transformer layers to the base Qwen2.5 model. These layers absorb SEA-language patterns during continual pre-training while original layers preserve English/Chinese capabilities. The paper reports reduced English perplexity degradation alongside greater SEA improvements.
- Core assumption: New layers preferentially encode target language patterns rather than uniformly updating all parameters.
- Evidence anchors: Section 4.1 and Section 8.1/Table 16 show perplexity comparisons and performance maintenance.

### Mechanism 2
- Claim: Two-stage pre-training with decaying learning rate and quality-focused data shift enables stable multilingual acquisition.
- Mechanism: Stage 1 (450B tokens, LR=1e-4) builds broad coverage with RegMix-optimized data proportions. Stage 2 (60B tokens, LR=1e-5) shifts to high-quality tokens including low-resource languages while retaining 20% Stage 1 data for distribution stability.
- Core assumption: Data quality can be sufficiently approximated by synthetic translation + FastText classification; learning rate decay aligns with optimal fine-tuning regimes.
- Evidence anchors: Abstract and Section 4.4.1/4.4.2 describe token budgets and learning rate schedules.

### Mechanism 3
- Claim: Instruction data selection via joint reward-perplexity ranking prioritizes under-trained but high-quality examples.
- Mechanism: Each instruction sample receives a reward score and perplexity, both normalized to percentiles per language/category. Selection uses harmonic mean of these percentiles, followed by embedding-based deduplication.
- Core assumption: High perplexity correlates with under-training rather than intrinsic difficulty/noise; reward model generalizes to SEA languages.
- Evidence anchors: Section 5.1.3 describes the selection methodology and provides case studies.

## Foundational Learning

- Concept: **Continual Pre-Training (CPT) vs. From-Scratch Training**
  - Why needed here: Sailor2 builds on Qwen2.5 (18T tokens already trained) rather than training from scratch. Understanding CPT trade-offs (efficiency vs. plasticity constraints) is essential for interpreting why model expansion was necessary.
  - Quick check question: Can you explain why a model trained on 18T tokens might resist further learning compared to a fresh initialization?

- Concept: **Direct Preference Optimization (DPO) and Length-Regularized Variants**
  - Why needed here: Sailor2 uses LR-DPO for preference tuning. Understanding the DPO objective and how length regularization mitigates verbosity exploitation is necessary to follow Section 5.2.
  - Quick check question: In DPO, what does the β parameter control, and why might longer responses be incorrectly preferred without length regularization?

- Concept: **Multilingual Data Curation Challenges**
  - Why needed here: The paper dedicates Section 3 to six-layer filtering, synthetic data generation for low-resource languages, and data mixture optimization. Foundational knowledge of deduplication, quality classification, and language-specific preprocessing is assumed.
  - Quick check question: Why might machine-translated data overestimate multilingual model performance, and how does Sailor2 address this?

## Architecture Onboarding

- Component map: Base Model (Qwen2.5) → Model Expansion (+layers) → Two-Stage CPT → Two-Stage SFT → LR-DPO → Customizations (Long-Context, Pruning, Speculative Decoding)
- Critical path:
  1. Model expansion decision (Section 4.1): Determines final parameter counts. Incorrect expansion ratio cascades to all downstream stages.
  2. Data mixture optimization (Section 3.4): RegMix with 1M proxy models sets Stage 1 composition. Directly affects which languages are upscaled/downscaled.
  3. Instruction data selection (Section 5.1.3): PPL-reward ranking + deduplication determines Stage 2 SFT quality. Critical for low-resource language balance.
- Design tradeoffs:
  - Expansion size vs. training efficiency: Larger expansion improves SEA performance but increases compute.
  - Coverage vs. quality in Stage 2: Adding low-resource languages improves inclusivity but token counts are tiny (0.02B-0.17B).
  - On-policy vs. off-policy DPO: Off-policy first, then on-policy yields better results but doubles preference data generation cost.
- Failure signatures:
  - Emoji overuse: Early models inserted excessive emojis, not detected by reward model.
  - Language inconsistency: Reward model trained on English gave high scores to English responses even for SEA prompts.
  - Translation artifacts: GPT-4o struggled with Lao/Khmer translation, producing excessive emojis and formatting issues.
- First 3 experiments:
  1. Reproduce Stage 1 data mixture with RegMix: Train 1M proxy models on your multilingual corpus subset. Compare Sailor2's reported upsampling and downsampling against your data distribution.
  2. Ablate model expansion: Train Sailor2-8B-equivalent with and without expansion layers. Measure English/Chinese perplexity retention, SEA language perplexity improvement, and downstream SEA-WildBench scores.
  3. Validate instruction data selection signal correlation: On a held-out subset, compute PPL-reward rankings. Manually annotate 100 samples as high/low value. Measure precision@k of the harmonic mean selector vs. reward-only baseline.

## Open Questions the Paper Calls Out

- **Question:** How does translating high-quality English documents into extremely low-resource languages (e.g., Minangkabau, Acehnese) compare to native data mining for effective model training?
  - **Basis:** Section 9.1 identifies synthetic data generation via translation as a proposed method to address data scarcity for low-resource languages.
  - **Why unresolved:** The paper relies primarily on mined CommonCrawl data and hasn't empirically validated the efficacy of translation-based synthetic data.
  - **What evidence would resolve it:** Comparative benchmarks between models trained on native mined tokens versus translated synthetic tokens for specific low-resource languages.

- **Question:** To what extent can tokenizer-free architectures (pixel-based or byte-level) outperform the current tokenizer-based Sailor2 on morphologically rich SEA languages or those mixing multiple scripts?
  - **Basis:** Section 9.2 suggests exploring tokenizer-free models to handle morphological richness and script-mixing.
  - **Why unresolved:** Sailor2 utilizes the Qwen2.5 tokenizer; potential performance gains of removing this tokenizer remain hypothetical.
  - **What evidence would resolve it:** Evaluation of byte-level or pixel-based variants on languages with complex morphology against the current tokenized baseline.

- **Question:** Can methods like over-training indicators or lottery ticket hypothesis-based parameter updates enable efficient continual pre-training on over-trained base models without requiring model expansion?
  - **Basis:** Section 9.3 proposes exploring these directions to achieve efficient continual pre-training on models already trained on massive datasets.
  - **Why unresolved:** The authors identify that existing models may be "over-trained," but Sailor2 relies on model expansion to mitigate forgetting.
  - **What evidence would resolve it:** Experiments applying lottery ticket hypothesis or plasticity enhancements without expansion, comparing performance retention and computational cost.

## Limitations
- **Data quality signal reliability:** High perplexity may correlate with inherent difficulty or noise rather than under-training, particularly for low-resource languages.
- **Architecture generalization:** Limited ablation studies on expansion ratios and lack of exploration on other base models beyond Qwen2.5.
- **Computational reproducibility:** Significant resource requirements (450B + 60B tokens) and complex pipeline create barriers to independent verification.

## Confidence
- **High Confidence:** Model expansion improves SEA performance while maintaining English proficiency; Two-stage CPT with decaying learning rate achieves better results; Length-regularized DPO prevents verbosity exploitation.
- **Medium Confidence:** PPL-reward joint selection outperforms single-signal approaches; 50-50 win rate represents practical parity; Long-context training meaningfully improves performance.
- **Low Confidence:** Synthetic data generation adequately represents low-resource languages; Pipeline parallelism optimizations scale efficiently; Speculative decoding provides consistent speedups.

## Next Checks
1. **Systematic PPL-Reward Correlation Analysis:** For each SEA language, sample 500 instruction examples spanning the PPL-reward space. Have native speakers rate quality on a 5-point scale. Compute correlation between (PPL, reward) scores and human quality ratings.
2. **Expansion Ratio Sensitivity Study:** Train Sailor2-8B variants with expansion ratios of 0%, 25%, 50%, and 75%. Evaluate English/Chinese retention and SEA language improvement across all 13 languages.
3. **Cultural Knowledge Benchmark Development:** Create a benchmark of culturally-specific knowledge questions for SEA languages. Evaluate Sailor2 against GPT-4o and native speaker responses to measure culturally-appropriate communication.