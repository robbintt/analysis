---
ver: rpa2
title: Are Large Language Models Chronically Online Surfers? A Dataset for Chinese
  Internet Meme Explanation
arxiv_id: '2510.00567'
source_url: https://arxiv.org/abs/2510.00567
tags:
- meme
- memes
- meaning
- language
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CHIME, a dataset of 1,458 Chinese phrase-based
  Internet memes annotated with meaning, origin, examples, and meme type. It evaluates
  whether large language models understand these memes through two tasks: meme explanation
  (meaning, origin, example generation) and multiple-choice meme selection.'
---

# Are Large Language Models Chronically Online Surfers? A Dataset for Chinese Internet Meme Explanation

## Quick Facts
- arXiv ID: 2510.00567
- Source URL: https://arxiv.org/abs/2510.00567
- Reference count: 40
- Key outcome: Large language models perform better on meaning than origin explanations of Chinese internet memes, with accuracy up to 0.81 in MCQ tasks

## Executive Summary
This paper introduces CHIME, a dataset of 1,458 Chinese phrase-based internet memes annotated with meaning, origin, examples, and meme type. The authors evaluate whether large language models can understand these memes through two tasks: meme explanation (meaning, origin, example generation) and multiple-choice meme selection. Automatic and human evaluations show models perform better on meaning than origin, with best scores around 0.84 (cosine similarity) and 0.82 (BERTScore) for meaning, and accuracy up to 0.81 in MCQ tasks. Performance drops notably for culturally nuanced meme types like quotations and homophonic puns.

## Method Summary
The study introduces CHIME, a dataset of 1,458 Chinese phrase-based internet memes, each annotated with meaning, origin, examples, and meme type. Two tasks evaluate model understanding: meme explanation (generating meaning, origin, and examples) and multiple-choice meme selection. Models are assessed using automatic metrics including cosine similarity, BERTScore, QAEval, and human evaluation. The evaluation covers various Chinese LLM architectures including Qwen, Yi, InternLM, and mainstream models like GPT-4 and Claude.

## Key Results
- Models achieve highest performance on meaning explanation (cosine similarity up to 0.84, BERTScore up to 0.82)
- MCQ task accuracy reaches up to 0.81, while meaning generation achieves up to 0.77
- Performance drops significantly for culturally nuanced meme types like quotations and homophonic puns

## Why This Works (Mechanism)
The study works by providing structured annotations for Chinese internet memes that capture both surface-level linguistic features and deeper cultural context. By evaluating models on multiple tasks (meaning, origin, examples) with both automatic and human metrics, the approach reveals specific gaps in LLM cultural understanding. The multiple-choice format provides a more objective measure of meme comprehension compared to open-ended generation tasks.

## Foundational Learning
- **Chinese internet meme culture**: Understanding contemporary Chinese online discourse patterns is essential for evaluating meme comprehension
  - Why needed: Memes often rely on cultural context that cannot be inferred from literal meaning alone
  - Quick check: Verify dataset covers diverse meme types across different Chinese internet subcultures

- **Automatic evaluation metrics for language generation**: Familiarity with cosine similarity, BERTScore, and QAEval metrics for assessing generated text quality
  - Why needed: These metrics provide quantitative measures of model performance on meme explanation tasks
  - Quick check: Compare metric scores across different model families to identify consistent patterns

- **Multimodal vs text-only meme understanding**: Recognizing limitations of text-only analysis for content that may have visual or audio components
  - Why needed: Many memes have multimodal elements that text-only models cannot fully capture
  - Quick check: Assess whether model performance correlates with meme complexity or multimodal features

## Architecture Onboarding
Component map: Meme Data -> Task Definition -> Model Processing -> Metric Evaluation -> Performance Analysis

Critical path: The dataset annotation process is foundational, as it provides the ground truth for all subsequent evaluations. Task design (explanation vs MCQ) directly impacts model performance measurement, with MCQ providing more reliable baseline comparisons.

Design tradeoffs: The study focuses on phrase-based memes to enable text-only analysis, excluding image-based memes that would require multimodal processing. This simplifies evaluation but limits generalizability to the full spectrum of internet memes.

Failure signatures: Performance degradation on culturally nuanced meme types (quotations, homophonic puns) indicates models struggle with cultural context beyond literal meaning. Large performance gaps between meaning and origin generation suggest models capture surface-level semantics better than historical/cultural background.

First experiments:
1. Compare model performance on CHIME versus a general Chinese language understanding benchmark
2. Test whether fine-tuning on CHIME improves model performance on meme-related tasks
3. Evaluate model explanations of memes from different Chinese internet subcultures separately

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Evaluation relies on automatic metrics that may not fully capture cultural and linguistic nuance
- Dataset size of 1,458 memes may not represent full diversity of Chinese internet culture
- Focus on phrase-based memes excludes image-based and other multimodal formats

## Confidence
- **High confidence**: Models perform better on meaning than origin explanations
- **Medium confidence**: Performance drops for culturally nuanced meme types
- **Medium confidence**: CHIME provides a useful benchmark for meme understanding research

## Next Checks
1. Conduct human evaluation studies with diverse cultural backgrounds to validate automatic metric results
2. Expand dataset to include multimodal memes and track performance changes across formats
3. Test model performance on temporally dynamic meme usage to assess evolving cultural meanings