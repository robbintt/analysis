---
ver: rpa2
title: Information-Theoretic Complementary Prompts for Improved Continual Text Classification
arxiv_id: '2505.20933'
source_url: https://arxiv.org/abs/2505.20933
tags:
- learning
- tasks
- knowledge
- prompt
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces InfoComp, an information-theoretic approach
  to continual text classification (CTC) that addresses catastrophic forgetting by
  learning two complementary prompt spaces: P-Prompt (task-specific) and S-Prompt
  (task-invariant). Drawing inspiration from complementary learning systems theory,
  InfoComp leverages mutual information maximization between prompts and other parameters,
  and between different encoded representations, to generate more informative prompts.'
---

# Information-Theoretic Complementary Prompts for Improved Continual Text Classification

## Quick Facts
- **arXiv ID:** 2505.20933
- **Source URL:** https://arxiv.org/abs/2505.20933
- **Reference count:** 40
- **Primary result:** InfoComp achieves 80.0% average accuracy on standard CTC benchmarks, improving over ProgPrompt by 0.9-1.3%, and 69.6% on long-sequence benchmarks, improving over ProgPrompt by 2.7%.

## Executive Summary
InfoComp introduces an information-theoretic approach to continual text classification that addresses catastrophic forgetting through complementary prompt spaces. The method learns two distinct prompt spaces: P-Prompt for task-specific knowledge and S-Prompt for task-invariant knowledge, inspired by complementary learning systems theory. By maximizing mutual information between prompts and other parameters, InfoComp generates more informative prompts that better preserve task-specific knowledge while enabling forward knowledge transfer. Extensive experiments demonstrate significant improvements over state-of-the-art methods across standard and long-sequence CTC benchmarks, achieving these results with constant prompt length and no data replay.

## Method Summary
InfoComp operates by learning two complementary prompt spaces while keeping the PLM backbone frozen. For each new task, it creates a task-specific P-Prompt (length 35) and updates a shared S-Prompt (length 5) across all tasks. The method maximizes mutual information between P-Prompt parameters and task-specific classifier parameters to enhance task-specific knowledge accumulation, while also maximizing mutual information between current and previous S-Prompt representations to preserve task-invariant knowledge. The overall loss combines cross-entropy classification loss with two information-theoretic regularization terms. During inference, the model maintains constant complexity by using only the current P-Prompt and shared S-Prompt, avoiding the quadratic complexity of methods that concatenate all historical prompts.

## Key Results
- Achieved 80.0% average accuracy on standard CTC benchmarks (AG News, Amazon reviews, DBpedia, Yelp, Yahoo Answers), improving over ProgPrompt by 0.9-1.3%
- Achieved 69.6% average accuracy on long-sequence benchmarks (15 datasets with 20-1000 samples per class), improving over ProgPrompt by 2.7%
- Demonstrated consistent improvements across diverse experimental settings including challenging long-sequence scenarios
- Maintained constant inference complexity regardless of task sequence length

## Why This Works (Mechanism)

### Mechanism 1: Bifurcated Knowledge Storage (Complementary Prompts)
The architecture separates knowledge into task-specific (P-Prompt) and task-invariant (S-Prompt) spaces, reducing catastrophic forgetting while maintaining forward transfer. P-Prompts act as pattern-separated memories for specific tasks, preventing interference, while S-Prompt acts as a generalized knowledge base accumulating shared features across tasks. This hybrid shared/isolated approach addresses interference issues while maintaining transfer capability.

### Mechanism 2: Classifier-Guided Prompt Optimization (P-Prompt)
The method maximizes mutual information between the P-Prompt and task-specific classifier head, improving the prompt's information density regarding the specific task. By aligning P-Prompt parameters with classifier parameters through a learnable projection matrix, the P-Prompt is forced to encode features directly relevant to the classification boundary.

### Mechanism 3: Consistency Regularization via SimSiam (S-Prompt)
The approach enforces similarity between representations generated by the current S-Prompt and the previous version, preserving task-invariant knowledge. Using a positive-only contrastive learning objective, it treats current and previous S-Prompt encodings of the same input as a positive pair, forcing the S-Prompt to evolve without destroying general features learned from prior tasks.

## Foundational Learning

- **Soft Prompt Tuning:** InfoComp freezes the PLM and optimizes continuous vectors appended to input embeddings. Why needed: Understanding the efficiency claims requires grasping that the massive backbone remains static. Quick check: Can you explain why appending learnable tensors is more parameter-efficient than fine-tuning attention layers?

- **Catastrophic Forgetting vs. Forward Transfer:** These are the two forces InfoComp balances. P-Prompt targets forgetting through isolation; S-Prompt targets transfer through accumulation. Quick check: If a model achieves 100% accuracy on Task 5 but drops to random chance on Task 1, which component is likely failing?

- **Mutual Information (MI):** The paper frames prompt learning as an information-theoretic problem. The loss functions are proxy measures for maximizing MI between parameters. Quick check: Does maximizing MI between prompt and classifier mean they should be identical vectors, or just dependent on similar features?

## Architecture Onboarding

- **Component map:** Input Text -> Frozen BERT -> [CLS] Token -> Task-Specific Classifier + MI Regularization -> P-Prompt (task-specific, length 35) + S-Prompt (shared, length 5)
- **Critical path:** 1) Concatenate Input Text + Current P-Prompt + S-Prompt, 2) Pass through frozen BERT, 3) Task-specific classifier predicts label, 4) Compute MI losses comparing (Prompt vs Head) and (Current Encoding vs Previous Encoding)
- **Design tradeoffs:** P-Prompt is long (35) for specificity; S-Prompt is short (5) for generality. Unlike ProgPrompt's O(N²) complexity from concatenating all historical prompts, InfoComp maintains O(1) inference complexity
- **Failure signatures:** S-Prompt Collapse (if S-Info loss weight too low), Stagnation (if S-Info loss weight too high), Memory Leak (if task sequence is massive)
- **First 3 experiments:** 1) Ablation: Run standard benchmark with only P-Prompts (no S-Prompt) and only S-Prompt (no P-Prompt), 2) Hyperparameter Sensitivity: Vary λ₁ and λ₂ on 3-task sequence to observe trade-off between stability and plasticity, 3) Scalability Test: Compare inference latency against ProgPrompt on synthetic 15-task sequence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does InfoComp perform when applied to PLMs other than BERT?
- Basis in paper: The conclusion states future work could "investigate the application of our model-agnostic InfoComp method with additional PLMs beyond BERT."
- Why unresolved: Experiments exclusively utilized bert-base-uncased as the backbone.
- What evidence would resolve it: Evaluation results on distinct architectures (e.g., decoder-based or larger encoder-decoder models) across standard and long-sequence benchmarks.

### Open Question 2
- Question: Does the fixed length of the S-Prompt create a capacity bottleneck that limits performance in extremely long task sequences?
- Basis in paper: The implementation fixes S-Prompt length to 5 tokens to ensure constant complexity, but the paper doesn't analyze if this fixed size suffices for storing task-invariant knowledge as task count scales significantly beyond 15.
- Why unresolved: The "Long-sequence" benchmark only tested up to 15 tasks.
- What evidence would resolve it: Experiments on task sequences of 50+ tasks, analyzing performance degradation relative to increasing S-Prompt lengths.

### Open Question 3
- Question: Can the framework be adapted to task-agnostic scenarios where task identity is unavailable during inference?
- Basis in paper: The problem formulation explicitly assumes a "task-incremental setup" where task identity is known during inference, avoiding the harder "class-incremental" problem where this ID is absent.
- Why unresolved: The method relies on selecting the correct P-Prompt using the provided task ID.
- What evidence would resolve it: A modification of InfoComp that includes a task-identifier mechanism and its subsequent evaluation in a class-incremental setting.

## Limitations

- The mutual information maximization objectives lack rigorous theoretical grounding and don't prove these proxy losses optimize desired information-theoretic quantities
- The assumption that tasks share meaningful "task-invariant" knowledge may break down in domains where tasks are fundamentally dissimilar
- The evaluation methodology only reports final task performance without per-task accuracy curves revealing whether catastrophic forgetting is truly mitigated
- The paper doesn't address potential computational overhead from maintaining and updating transformation matrices across tasks

## Confidence

**High Confidence (8-10/10):** The experimental results showing InfoComp outperforming baseline methods on both standard and long-sequence benchmarks are reproducible and methodologically sound.

**Medium Confidence (5-7/10):** The theoretical framing using mutual information maximization is conceptually coherent but lacks rigorous mathematical justification.

**Low Confidence (1-4/10):** The claims about the S-Prompt's effectiveness in extreme domain shift scenarios are largely speculative without empirical evidence.

## Next Checks

1. **Task-Wise Performance Analysis:** Replicate experiments while recording per-task accuracy after each new task is learned to reveal whether the method truly prevents catastrophic forgetting or simply achieves good average performance through trade-offs.

2. **Extreme Domain Shift Test:** Design a 5-task sequence where tasks are deliberately chosen to be as dissimilar as possible (sentiment analysis → legal document classification → medical diagnosis → code review → product matching) to evaluate whether the S-Prompt becomes detrimental rather than beneficial.

3. **Prompt Interference Quantification:** Implement a method to measure similarity between P-Prompts of different tasks and between successive S-Prompt states, then correlate these metrics with task performance to empirically validate whether the information-theoretic objectives achieve their intended effect.