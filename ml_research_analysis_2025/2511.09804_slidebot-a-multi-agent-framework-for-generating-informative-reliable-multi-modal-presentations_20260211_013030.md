---
ver: rpa2
title: 'SlideBot: A Multi-Agent Framework for Generating Informative, Reliable, Multi-Modal
  Presentations'
arxiv_id: '2511.09804'
source_url: https://arxiv.org/abs/2511.09804
tags:
- learning
- attention
- your
- slide
- manifold
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SlideBot introduces a multi-agent framework for generating accurate,
  pedagogically sound presentation slides. By combining retrieval-augmented generation,
  structured planning, and modular code generation, it addresses common LLM limitations
  such as hallucinations and lack of multimodal coherence.
---

# SlideBot: A Multi-Agent Framework for Generating Informative, Reliable, Multi-Modal Presentations

## Quick Facts
- arXiv ID: 2511.09804
- Source URL: https://arxiv.org/abs/2511.09804
- Reference count: 34
- Primary result: SlideBot significantly outperforms direct prompting and Microsoft Copilot in explanation style, conceptual accuracy, topic coverage, and credibility for AI and biomedical education slides.

## Executive Summary
SlideBot is a multi-agent framework designed to generate accurate, pedagogically sound presentation slides. It addresses common LLM limitations such as hallucinations and lack of multimodal coherence by combining retrieval-augmented generation, structured planning, and modular code generation. The system retrieves domain-specific sources, plans slides following cognitive load and multimedia learning principles, and generates LaTeX Beamer slides with instructor annotations and figures. Evaluations show SlideBot achieves greater consistency, accuracy, and practical utility compared to baselines.

## Method Summary
SlideBot uses a 3-stage workflow coordinated by a Moderator agent: (1) Content Retrieval from arXiv API or textbook corpora via BM25, (2) Slide Draft Generation through planning, LaTeX code creation, and compilation validation loops, and (3) Presentation Enhancement with figure and comment insertion. Four specialized agents (Moderator, Retriever, Code Generator, Enhancer) handle narrow subtasks using explicit prompt templates. GPT-4o-mini is the primary model, with GPT-4o used in ablation studies. The system generates LaTeX Beamer slides grounded in high-quality external sources and structured according to CLT/CTML principles.

## Key Results
- SlideBot achieves significantly higher scores than direct prompting and Microsoft Copilot in explanation style, conceptual accuracy, topic coverage, and credibility metrics.
- The framework demonstrates greater consistency with lower suitability variance (0.33 vs. Copilot's 1.47).
- GPT-4o-mini with retrieval achieves +0.83 improvement in Conceptual Accuracy versus no-context baseline.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-agent decomposition reduces hallucinations and improves content grounding.
- Mechanism: Specialized agents collaboratively retrieve information, summarize content, generate figures, and format slides using LaTeX, with the Moderator coordinating tasks.
- Core assumption: Task decomposition into specialized roles reduces error propagation compared to single-prompt generation.
- Evidence anchors: Table 1 shows SlideBot reduces variability (suitability variance = 0.33 vs. Copilot's 1.47), suggesting more consistent outputs from structured decomposition.

### Mechanism 2
- Claim: Retrieval-Augmented Generation (RAG) improves factual accuracy and credibility.
- Mechanism: The Retriever queries external corpora (arXiv API for research papers; BM25 over textbook corpora for foundational content) and returns citation-ready summaries.
- Core assumption: External, verifiable sources are more reliable than parametric model knowledge for educational content.
- Evidence anchors: Figure 4 shows GPT-4o-mini with retrieval achieves +0.83 improvement in Conceptual Accuracy vs. no-context baseline; GPT-4o with retrieval improves by +0.89.

### Mechanism 3
- Claim: Structured planning guided by CLT/CTML principles improves pedagogical quality.
- Mechanism: The Moderator constructs a slide plan using a structural guide that embeds principles such as intrinsic load management, signaling, coherence, and spatial contiguity.
- Core assumption: Translating learning theory principles into structural constraints improves learner comprehension.
- Evidence anchors: Annotated Structural Guide (Figure 10) explicitly maps slide sections to CLT/CTML principles.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: SlideBot's reliability pillar depends on RAG to ground content in external sources rather than parametric knowledge.
  - Quick check question: Can you explain how BM25 ranking differs from neural retrieval, and why SlideBot uses it for textbook corpora?

- **Concept: Multi-Agent Orchestration**
  - Why needed here: The Moderator agent coordinates all other agents; understanding message passing and error-handling loops is critical for debugging.
  - Quick check question: What happens if the Code Generator returns LaTeX that fails to compile?

- **Concept: Cognitive Load Theory (CLT) basics**
  - Why needed here: The structural guide encodes CLT principles (intrinsic, extraneous, germane load) into slide planning prompts.
  - Quick check question: How does reducing extraneous load on a slide theoretically support germane load?

## Architecture Onboarding

- **Component map:**
  - Topic input -> Moderator (keyword generation) -> Retriever (summaries) -> Moderator (slide plan) -> Code Generator (LaTeX) -> Latex Compiler -> Enhancer (figures/comments) -> Final presentation

- **Critical path:** Topic input → Moderator keyword generation → Retriever search/summarize → Moderator source selection → Slide plan construction → Code Generator LaTeX output → Compilation validation → Enhancer figure/comment insertion → Final presentation.

- **Design tradeoffs:**
  - Prompting vs. fine-tuning: Authors chose prompting for accessibility and rapid iteration; fine-tuning could improve consistency but requires annotated training data and infrastructure.
  - LaTeX Beamer vs. direct visual generation: LaTeX provides precise formatting and compilation validation but requires a build step and limits real-time editing.
  - Fixed figure macros vs. generative visuals: Macros ensure consistency and reduce hallucination risk but constrain visual diversity.

- **Failure signatures:**
  - Retrieval returns irrelevant or low-quality sources → shallow or inaccurate slide content.
  - Code Generator produces syntactically invalid LaTeX → compilation loop fails repeatedly.
  - Structural guide over-constrains planning → slide flow feels mechanical or misaligned with topic.

- **First 3 experiments:**
  1. Reproduce the Direct Prompt vs. SlideBot comparison on a new topic; measure Explanation Style and Conceptual Accuracy scores.
  2. Ablate the Retriever by providing no external context; quantify the drop in credibility and accuracy metrics.
  3. Swap the structural guide for a generic outline; evaluate changes in Structure & Flow and Instructor Utility scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does using SlideBot-generated presentations measurably improve student learning outcomes compared to human-created or alternative AI-generated slides?
- Basis in paper: Evaluations measured perceived quality via surveys with 15-15 students and 4-7 experts, but did not assess actual learning gains.
- Why unresolved: The study design focused on expert and learner perception, not pre/post knowledge assessments or longitudinal retention.
- What evidence would resolve it: A controlled classroom study comparing learning outcomes (test scores, concept retention) across SlideBot, human-authored, and baseline AI-generated slides.

### Open Question 2
- Question: How does SlideBot generalize to domains outside AI and biomedical education, such as humanities, social sciences, or engineering?
- Basis in paper: Experiments were limited to CS/AI topics and a curated biomedical textbook corpus; the authors note the framework is "modular and can be implemented in a variety of ways" but do not test other domains.
- Why unresolved: No cross-domain evaluation was conducted, leaving adaptability claims unverified.
- What evidence would resolve it: Empirical evaluation of slide quality metrics across diverse disciplines with domain-specific corpora and expert review.

### Open Question 3
- Question: What are the trade-offs between prompting-based agent implementation versus fine-tuning or RLHF for educational slide generation?
- Basis in paper: The appendix states prompting was chosen for accessibility but acknowledges fine-tuning "could provide greater stylistic consistency" while requiring "substantial costs" and domain-specific data.
- Why unresolved: No comparison between prompting and fine-tuning was conducted; the cost-quality trade-off remains unquantified.
- What evidence would resolve it: Comparative experiments measuring output consistency, accuracy, and cost across prompting, fine-tuning, and RLHF configurations.

### Open Question 4
- Question: How does retrieval corpus quality and recency affect slide reliability in rapidly evolving fields?
- Basis in paper: The paper notes arXiv is "highly dynamic" but preprints "may not yet be peer-reviewed," while textbooks provide "stability and pedagogical reliability" but lack currency.
- Why unresolved: The interplay between source type, field evolution speed, and output reliability was not systematically studied.
- What evidence would resolve it: Controlled experiments varying corpus type (preprints vs. peer-reviewed vs. textbooks) across fields with different knowledge update rates, measuring factual accuracy and currency.

## Limitations
- The biomedical corpus details (BM25 hyperparameters, chunking strategy) are underspecified, potentially affecting downstream slide accuracy.
- No ablation studies isolate the contribution of each agent type; performance gains could stem from factors beyond the modular design.
- Figure generation depends on hardcoded LaTeX macros not included in the paper, limiting reproducibility of the visual outputs.
- Evaluation metrics are based on human surveys with no inter-rater reliability reporting, making performance differences difficult to generalize.

## Confidence
- **High Confidence**: SlideBot improves consistency and reduces variability in slide quality compared to direct prompting (supported by variance metrics).
- **Medium Confidence**: Retrieval-augmented generation enhances factual accuracy and credibility (supported by accuracy scores, but no independent verification of retrieved source quality).
- **Low Confidence**: Structured planning strictly guided by CLT/CTML principles yields better pedagogical outcomes (no corpus evidence; human evaluation may conflate planning with other factors).

## Next Checks
1. Conduct a source verification audit: manually check a sample of retrieved citations to confirm they match the claimed domain and publication date.
2. Run a human-subject study with a control group receiving slides without retrieval augmentation to isolate RAG's impact on credibility.
3. Test SlideBot's performance across diverse topics (e.g., humanities, social sciences) to evaluate generalizability beyond AI and biomedical domains.