---
ver: rpa2
title: 'DeepAnalyze: Agentic Large Language Models for Autonomous Data Science'
arxiv_id: '2510.16872'
source_url: https://arxiv.org/abs/2510.16872
tags:
- data
- merchant
- fraud
- transaction
- science
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DeepAnalyze-8B, the first agentic LLM tailored
  for autonomous data science, capable of automating the entire data science pipeline
  from raw data to analyst-grade reports. To address the challenges of reward sparsity
  and trajectory scarcity in high-complexity tasks, it proposes a curriculum-based
  agentic training paradigm inspired by human data scientists' learning trajectories,
  combined with a data-grounded trajectory synthesis framework that automatically
  constructs high-quality reasoning and interaction trajectories.
---

# DeepAnalyze: Agentic Large Language Models for Autonomous Data Science

## Quick Facts
- **arXiv ID**: 2510.16872
- **Source URL**: https://arxiv.org/abs/2510.16872
- **Reference count**: 40
- **Key outcome**: DeepAnalyze-8B is the first agentic LLM tailored for autonomous data science, achieving SOTA performance among open-source LLM-based agents and surpassing most proprietary LLMs despite having only 8B parameters.

## Executive Summary
This paper introduces DeepAnalyze-8B, the first agentic large language model specifically designed for autonomous data science. The model automates the entire data science pipeline from raw data to analyst-grade reports using a novel curriculum-based training paradigm that progressively builds multi-ability skills. By combining curriculum learning, data-grounded trajectory synthesis, and hybrid reward modeling, DeepAnalyze-8B achieves state-of-the-art performance among open-source LLM agents, even surpassing GPT-4-Turbo on certain tasks while maintaining only 8B parameters.

## Method Summary
DeepAnalyze-8B is trained through a two-stage curriculum: Stage 1 involves single-ability fine-tuning on 470K samples to build individual capabilities (reasoning, structured data understanding, code generation), followed by Stage 2 multi-ability agentic training using reinforcement learning with GRPO on 15K tasks. The training incorporates data-grounded trajectory synthesis to automatically generate high-quality reasoning and interaction trajectories, and employs hybrid rewards combining rule-based metrics with LLM-as-judge evaluation. The model uses a specialized action vocabulary (Analyze/Understand/Code/Execute/Answer) and is fine-tuned from DeepSeek-R1-0528-Qwen3-8B.

## Key Results
- Achieves SOTA performance among open-source LLM-based agents on 12 benchmarks
- Surpasses most advanced proprietary LLMs including GPT-4-Turbo on certain tasks
- First agentic model capable of performing open-ended data research and generating analyst-grade reports
- Only 8B parameters while achieving superior performance

## Why This Works (Mechanism)

### Mechanism 1: Curriculum-based Skill Progression
Progressive skill acquisition from single-ability to multi-ability tasks mitigates reward sparsity in complex data science tasks. Stage 1 strengthens individual capabilities using long CoT data, while Stage 2 integrates these skills in real-world environments using RL with GRPO. This mimics human data scientist progression from beginner → practitioner → data scientist.

### Mechanism 2: Data-grounded Trajectory Synthesis
Automated synthesis of high-quality reasoning and interaction trajectories overcomes trajectory scarcity for complex multi-step tasks. Two-phase synthesis: (1) Reasoning trajectory synthesis uses teacher LLM distillation + keyword-guided refinement with reasoning vocabulary ("but", "wait", "let's check the table"); (2) Interaction trajectory synthesis uses a multi-agent system that generates task-vehicle-checklist-validated trajectories.

### Mechanism 3: Hybrid Reward Modeling
Combining rule-based rewards with LLM-as-judge enables training on open-ended tasks lacking ground-truth answers. Three-tier reward system: format check, task-based work evaluation, and open-ended research scoring across five dimensions (usefulness, richness, soundness, interpretability, readability).

## Foundational Learning

- **Reinforcement Learning with Group Relative Policy Optimization (GRPO)**: Core training algorithm; differs from PPO by sampling groups of outputs and computing advantages within-group rather than using value functions
  - Quick check: Given 4 sampled outputs with rewards [0.2, 0.5, 0.3, 0.6], what are the normalized advantages?

- **Chain-of-Thought (CoT) Reasoning with Structured Data**: Single-ability training requires understanding how to decompose data science tasks into reasoning steps that reference specific table/data content
  - Quick check: How would you trace reasoning for: "Which merchant category has the highest average transaction fee?"

- **Agentic Action Space Design**: The five-token action vocabulary (Analyze/Understand/Code/Execute/Answer) defines how the model interacts with environments
  - Quick check: Why separate ⟨Understand⟩ from ⟨Analyze⟩ rather than combining them?

## Architecture Onboarding

- **Component map**: Input: Instruction + File references → DeepAnalyze-8B → Autoregressive generation with special tokens → Environment (Python sandbox with pandas, matplotlib, sqlite3) → Execute feedback → Continue generation until ⟨Answer⟩

- **Critical path**: Stage 1 quality directly impacts Stage 2 effectiveness. Table 4 shows single-ability model achieves 60.64 on TableQA vs final model's 64.47—a 4-point gap that curriculum training must bridge.

- **Design tradeoffs**:
  - Context length: 8K (Stage 1) vs 32K (Stage 2) trades memory efficiency vs trajectory coverage
  - Reward hybridization: More rule-based → more stable but less flexible; more LLM-as-judge → more expressive but noisier
  - Trajectory synthesis scale: 500K total samples balances coverage vs quality filtering burden

- **Failure signatures**:
  - Format collapse: Model stops generating action tokens, outputs plain text (R=-1 triggers)
  - Tool dependency: Model over-generates ⟨Code⟩ without sufficient ⟨Analyze⟩ planning
  - Reward gaming: For open-ended tasks, model maximizes interaction turns without improving report quality
  - Single-ability plateau: Stage 1 achieves high TableQA but DABStep remains low

- **First 3 experiments**:
  1. Ablate curriculum: Train with single-stage mixing all data. Compare final performance on DataSciBench and DABStep.
  2. Validate trajectory quality: Manually inspect 50 synthesized interaction trajectories. Score on (a) task relevance, (b) code correctness, (c) reasoning soundness.
  3. Probe reward reliability: Run LLM-as-judge 3× on same 20 reports. Compute pairwise agreement.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the curriculum-based agentic training paradigm of DeepAnalyze be effectively extended to intelligent data systems in the domains of data discovery, data governance, and data ecosystems?
  - Basis: The conclusion states the work paves the way for next-generation intelligent data systems in areas such as data discovery, data governance, data ecosystems, and data management.

- **Open Question 2**: Does the performance of DeepAnalyze exhibit similar gains relative to proprietary models when scaled to parameter sizes significantly larger than 8B?
  - Basis: The paper emphasizes that DeepAnalyze-8B "surpasses most advanced proprietary LLMs" despite its small size (8B parameters), implying parameter count is a limiting factor.

- **Open Question 3**: Does the "keyword-guided refinement" strategy in trajectory synthesis limit the model's ability to handle edge cases where standard reasoning vocabulary is insufficient?
  - Basis: Section 3.3 describes the insertion of "key reasoning words" (e.g., "but", "wait") to improve data quality, based on the premise that such words guide reasoning.

## Limitations
- Core training methodology validated only at 8B parameter scale; scaling effects to larger models remain unknown
- LLM-as-judge reliability for research report evaluation not quantified with inter-rater agreement statistics
- Trajectory synthesis quality depends on unspecified multi-agent system prompts and reasoning vocabulary lists

## Confidence
- **DeepAnalyze-8B achieves SOTA performance among open-source LLM agents**: High confidence
- **Curriculum-based training is essential for complex data science tasks**: Medium confidence
- **Data-grounded trajectory synthesis produces high-quality training data**: Low confidence
- **Hybrid LLM-as-judge rewards enable open-ended task training**: Low confidence

## Next Checks
1. **Inter-rater reliability analysis**: Run the LLM-as-judge evaluation on 50 research reports across 3 different LLM judges. Calculate Cohen's κ for pairwise agreements and report correlation with human evaluation scores to validate reward reliability.

2. **Trajectory synthesis audit**: Manually inspect 100 synthesized interaction trajectories from the multi-agent system. Score each on task relevance (0-1), code executability (0-1), and reasoning soundness (0-1). Report acceptance rate and identify systematic failure patterns.

3. **Curriculum ablation study**: Implement a baseline that trains all data simultaneously without curriculum progression. Compare final performance on DABStep and DataSciBench against the reported curriculum results to isolate the curriculum effect from increased training data.