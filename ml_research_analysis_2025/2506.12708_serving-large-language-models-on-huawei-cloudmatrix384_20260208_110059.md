---
ver: rpa2
title: Serving Large Language Models on Huawei CloudMatrix384
arxiv_id: '2506.12708'
source_url: https://arxiv.org/abs/2506.12708
tags:
- memory
- cloudmatrix384
- these
- decode
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Huawei CloudMatrix384, a next-generation AI
  datacenter architecture designed to address the challenges of serving large language
  models (LLMs) with increasing parameter scales, adoption of mixture-of-experts (MoE)
  architectures, and expanding context lengths. The CloudMatrix384 integrates 384
  Ascend 910 NPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth
  Unified Bus (UB) network, enabling direct all-to-all communication and dynamic resource
  pooling.
---

# Serving Large Language Models on Huawei CloudMatrix384

## Quick Facts
- arXiv ID: 2506.12708
- Source URL: https://arxiv.org/abs/2506.12708
- Reference count: 40
- Primary result: CloudMatrix384 achieves 6,688 tokens/s/NPU prefill and 1,943 tokens/s/NPU decode throughput on DeepSeek-R1

## Executive Summary
This paper presents CloudMatrix384, a next-generation AI datacenter architecture designed to address the challenges of serving large language models with increasing parameter scales, mixture-of-experts architectures, and expanding context lengths. The system integrates 384 Ascend 910 NPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified Bus network, enabling direct all-to-all communication and dynamic resource pooling. To fully exploit CloudMatrix384's capabilities, the authors propose CloudMatrix-Infer, an advanced LLM serving solution incorporating peer-to-peer disaggregation, large-scale expert parallelism, and hardware-aware optimizations.

## Method Summary
The method involves deploying the DeepSeek-R1 model (671B parameters, MoE) on the CloudMatrix384 architecture using CloudMatrix-Infer serving solution. The system employs a peer-to-peer disaggregation approach that separates prefill, decode, and caching into independent pools. Large-scale Expert Parallelism (EP320) distributes 256 routed experts across 320 NPU dies. Hardware optimizations include specialized fused operators (FusedDispatch, FusedCombine), microbatch-based pipelining, and INT8 quantization. The architecture leverages the Unified Bus network for direct memory access and implements dynamic resource allocation across disaggregated compute and memory pools.

## Key Results
- Achieves 6,688 tokens/s per NPU for prefill and 1,943 tokens/s per NPU for decode on DeepSeek-R1
- Maintains TPOT < 50ms with decode throughput of 1,943 tokens/s/NPU
- Sustains 538 tokens/s per NPU under stringent 15 ms TPOT constraints
- INT8 quantization maintains model accuracy across benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Peer-to-Peer Disaggregation via Unified Bus (UB)
The ultra-high-bandwidth UB network enables disaggregating prefill, decode, and caching into independent pools without data locality bottlenecks. Unlike traditional KV-cache-centric architectures, CloudMatrix-Infer uses UB to allow any NPU to access disaggregated memory pool directly via DMA, flattening the memory hierarchy and enabling stateless scheduling. The core assumption is that UB provides sufficiently uniform latency and bandwidth to make remote memory access competitive with local access.

### Mechanism 2: Latency Hiding via Fused Communication and Microbatching
Large-scale Expert Parallelism introduces massive communication overhead that can be hidden through fused communication overlapped with heterogeneous compute units. The system employs FusedDispatch and FusedCombine operators using AIV-Direct to bypass SDMA engines, while microbatch-based pipelining splits decode into interleaved streams executing on partitioned AIC/AIV cores. The core assumption is that Ascend 910's heterogeneous architecture allows true parallel execution without resource contention.

### Mechanism 3: Hardware-Native Data Layouts
Computational efficiency improves when data layout matches compute engine access patterns, eliminating transformation overhead. The Ascend 910's AI Cube cores optimize for fractal (NZ) format, so CloudMatrix-Infer stores KV cache natively in this format rather than standard ND format, removing explicit ND-to-NZ transpose operations. The core assumption is that writing in NZ format overhead is less than transformation overhead later.

## Foundational Learning

**Concept: Mixture-of-Experts (MoE) Routing**
- Why needed: Understanding that each token activates only a subset of experts (Top-8) is essential to grasp why communication is the bottleneck
- Quick check: Why does increasing the number of experts (EP degree) increase communication volume even if active parameters per token stay constant?

**Concept: Prefill-Decode Disaggregation**
- Why needed: Separating prompt processing from token generation is fundamental to resource allocation strategy
- Quick check: In CloudMatrix architecture, which network plane transfers KV cache from Prefill to Decode cluster, and why?

**Concept: Memory Bandwidth vs. Compute Bound**
- Why needed: Optimization strategies differ by phase - Prefill is compute-bound while Decode is memory-bound
- Quick check: How does INT8 quantization specifically help alleviate decode bottleneck compared to prefill phase?

## Architecture Onboarding

**Component map:**
CloudMatrix384 Supernode (384 Ascend NPUs, 192 Kunpeng CPUs) -> Unified Bus (UB) for scale-up, RDMA for scale-out, VPC for control -> CloudMatrix-Infer (Serving Engine), CANN (Operators), EMS (Disaggregated Memory/Caching) -> DeepSeek-R1 (671B params, MoE)

**Critical path:**
1. Request In: Global Scheduler receives request
2. Prefill: Routes to 16-NPU instance using Hybrid Parallelism (SP/TP) to compute KV cache
3. Transfer: KV Cache moves via RDMA to target Decode node (isolated from UB traffic)
4. Decode: Routes to 160-NPU instance using EP320
5. Execution: Stream 0 (Attention on AIV/Cores) interleaved with Stream 1 (MoE via AIV-Direct Dispatch/Combine)
6. Caching: KV Cache stored/retrieved from EMS via UB (CPU memory) to reduce Prefill cost

**Design tradeoffs:**
- EP320 (One expert per die): Minimizes serial latency but maximizes cross-node communication traffic
- AIV-Direct vs. SDMA: AIV-Direct lowers latency for small messages but may not suit bulk transfers
- P2P Caching: High scheduling flexibility vs. dependency on UB network stability

**Failure signatures:**
- "Bubble" in Decode Pipeline: If Stream 0 vs. Stream 1 latency differ significantly, microbatch overlap fails
- RDMA Saturation: KV cache transfer bottleneck starves Decode nodes
- Quantization Drift: INT8 scales not calibrated per-channel drops accuracy below ~90% baseline

**First 3 experiments:**
1. Micro-benchmark AIV-Direct vs. SDMA: Measure latency for sending 7.5KB token packet across 320-die domain
2. Pipeline Balance Test: Run Decode with varying sequence lengths (1K vs 4K vs 64K) to observe Stream 0 vs. Stream 1 execution time ratio
3. Cache Hit Rate Impact: Measure TTFT with EMS enabled vs. disabled at different prefix reuse rates (12.5% vs 90%)

## Open Questions the Paper Calls Out

**Open Question 1:** Can a unified VPC network plane effectively replace separate RDMA and VPC planes for inter-supernode communication in large-scale AI clusters? The current separation maintains backward compatibility; it remains unproven if VPC bandwidth alone can sustain DP/PP communication demands without dedicated RDMA plane.

**Open Question 2:** To what extent does physically disaggregating CPUs from NPUs improve resource utilization compared to current fixed node configuration? While logical pooling exists, the performance impact and operational complexity of physically separated resource pools are not yet evaluated.

**Open Question 3:** How can the effective bandwidth degradation in CANN EP implementation at large scales be optimized? The current software implementation fails to fully utilize hardware's bandwidth capacity as communication domain scales, limiting throughput.

## Limitations

- Evaluation limited to single model (DeepSeek-R1) on proprietary hardware, raising questions about scalability to other architectures
- Performance gains rely heavily on unique characteristics of CloudMatrix384 architecture that may not translate to conventional GPU clusters
- INT8 quantization implementation provides limited detail on calibration process and doesn't extensively test on challenging domains

## Confidence

**High Confidence:** The fundamental architecture of CloudMatrix384 and disaggregation approach are clearly specified and verifiable through hardware documentation.

**Medium Confidence:** Performance numbers are internally consistent with architecture specifications and represent reasonable scaling from component-level measurements, though direct comparison is complicated by proprietary hardware.

**Low Confidence:** Claim that INT8 quantization maintains accuracy "comparable to BF16" across all benchmarks requires more extensive validation with detailed error analysis.

## Next Checks

1. **Quantization Robustness Test:** Run quantized model on adversarial examples and mathematical reasoning tasks beyond standard benchmarks to verify claimed accuracy retention holds under stress conditions.

2. **Architecture Portability Assessment:** Implement core optimization strategies (AIV-Direct communication, microbatch pipelining, hardware-native data layouts) on conventional GPU cluster to determine what portion of performance gain is attributable to CloudMatrix384's unique features versus generalizable optimizations.

3. **Scalability Under Load Test:** Deploy system under realistic multi-tenant conditions with varying request patterns and model sizes to assess whether claimed efficiency holds when architecture faces real-world operational stresses rather than controlled benchmark scenarios.