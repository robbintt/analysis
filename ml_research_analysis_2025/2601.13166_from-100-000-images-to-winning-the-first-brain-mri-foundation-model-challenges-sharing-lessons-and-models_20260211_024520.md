---
ver: rpa2
title: 'From 100,000+ images to winning the first brain MRI foundation model challenges:
  Sharing lessons and models'
arxiv_id: '2601.13166'
source_url: https://arxiv.org/abs/2601.13166
tags:
- medical
- imaging
- challenges
- these
- first
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper reports winning first place in both MICCAI 2025 SSL3D
  and FOMO25 challenges for brain MRI foundation models. The authors addressed the
  problem of developing efficient foundation models for 3D neuroimaging by exploiting
  domain-specific anatomical priors rather than relying on large transformer architectures.
---

# From 100,000+ images to winning the first brain MRI foundation model challenges: Sharing lessons and models

## Quick Facts
- arXiv ID: 2601.13166
- Source URL: https://arxiv.org/abs/2601.13166
- Reference count: 0
- First place in both MICCAI 2025 SSL3D and FOMO25 challenges for brain MRI foundation models

## Executive Summary
This paper presents winning approaches for the first brain MRI foundation model challenges by using U-Net CNNs with masked autoencoders instead of transformer architectures. The key innovation is explicitly disentangling subject-invariant anatomical features from contrast-specific pathological representations, enabling 10× faster training and 10× fewer parameters than competing transformer methods. The models achieved 2.5% higher Dice for segmentation and 8% higher accuracy for classification on challenge benchmarks. All models and code are publicly available at github.com/jbanusco/BrainFM4Challenges.

## Method Summary
The approach uses a U-Net CNN backbone with masked autoencoders (MAE) as the core pre-training objective. The key innovation is a dual-latent architecture that explicitly separates subject-invariant anatomical features (enforced to match T1-weighted segmentations across all images of a subject) from contrast-specific pathological features. For SSL3D, this includes T1-segmentation consistency and health-status discrimination losses. For FOMO25, cross-contrast reconstruction via representation swapping between contrasts of the same subject is added. The models train in ~36 GPU-hours (<80GB vRAM) compared to 100-1000 hours for transformers, with only 20M parameters versus 300M for transformer baselines.

## Key Results
- Won first place in both MICCAI 2025 SSL3D and FOMO25 challenges
- Achieved 2.5% higher average Dice for segmentation and 8% higher accuracy for classification compared to transformer-based submissions
- Trained 1-2 orders of magnitude faster with 10× fewer parameters than competing transformer approaches
- Required only ~36 GPU-hours (<80GB vRAM) for pre-training versus 100-1000 hours for transformers

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Disentangling subject-invariant anatomical features from contrast-specific pathological representations improves downstream task generalization.
- **Mechanism:** The latent space is explicitly partitioned into two components: (1) anatomical features enforced to match T1-weighted segmentations across all images of a subject, and (2) contrast-specific features optimized for pathology discrimination. Both components recombine in the decoder for MAE reconstruction.
- **Core assumption:** Anatomy remains relatively stable across MRI contrasts and timepoints for a given subject, while pathology manifests differently per contrast.
- **Evidence anchors:** [abstract]: "It relies on a U-Net CNN architecture combined with strategies leveraging anatomical priors and neuroimaging domain knowledge." [section]: "Our core strategy consisted in disentangling subject-invariant anatomical representations from contrast-specific pathological features... [preventing] spurious correlations and shortcut learning by anchoring learning to domain knowledge."
- **Break condition:** If anatomical structures change significantly between scans (e.g., post-surgical resection, rapid tumor progression), the subject-invariance assumption degrades.

### Mechanism 2
- **Claim:** CNNs with domain-appropriate inductive biases outperform transformers on 3D medical imaging when training data is limited to ~100K volumes.
- **Mechanism:** CNN's translation equivariance and local connectivity match volumetric medical image structure. Transformers require larger corpora to learn effective attention patterns; 3D tokenization creates quadratic complexity bottlenecks that limit spatial resolution.
- **Core assumption:** Medical image features exhibit local spatial coherence exploitable by convolution.
- **Evidence anchors:** [abstract]: "Our models trained 1-2 orders of magnitude faster and were 10× smaller than competing transformer-based approaches." [section]: "In FOMO25, our CNN model required ~36 GPU-hours (<80GB vRAM) for pre-training compared to ~100-1000 hours for comparable transformer approaches, with far fewer parameters (20M vs 300M for ViT-L DINOv2 3D)."
- **Break condition:** With substantially larger datasets (assumption: >>100K volumes), transformers may eventually match CNN performance.

### Mechanism 3
- **Claim:** Cross-contrast reconstruction with representation swapping encourages disentanglement of anatomy from acquisition-specific characteristics.
- **Mechanism:** During pre-training, anatomical representations are swapped between contrasts of the same subject while maintaining the shared anatomical component, forcing the model to encode contrast-invariant structure separately from contrast-specific signal.
- **Core assumption:** Different contrasts of the same subject share underlying anatomy but differ in acquisition-specific intensity patterns.
- **Evidence anchors:** [abstract]: Not explicitly mentioned. [section]: "For FOMO25... we implemented a cross-contrast reconstruction objective by swapping representations between contrasts of the same subject while maintaining a shared anatomical component."
- **Break condition:** If critical diagnostic information is contrast-dependent and gets suppressed during swapping, downstream pathology detection may suffer.

## Foundational Learning

- **Concept: Masked Autoencoders (MAE)**
  - Why needed here: Core pre-training objective; the model reconstructs masked 3D volumes from partial inputs to learn useful representations without labels.
  - Quick check question: Can you explain how masking patches and reconstructing them forces the model to learn semantic features rather than pixel interpolation?

- **Concept: Inductive Biases in Architecture Selection**
  - Why needed here: The paper's central argument is that CNNs' built-in assumptions (locality, translation equivariance) better match 3D medical data than transformers' learned attention.
  - Quick check question: What structural properties of brain MRI make local convolution more sample-efficient than global self-attention?

- **Concept: Representation Disentanglement**
  - Why needed here: The model explicitly separates anatomical from contrast-specific features; understanding this is essential for debugging and extending the approach.
  - Quick check question: How would you verify that the two latent components actually encode what they're supposed to (anatomy vs. contrast)?

## Architecture Onboarding

- **Component map:**
  - Encoder: U-Net CNN backbone (~20M parameters)
  - Latent space: Partitioned into (1) subject-invariant anatomical features and (2) contrast-specific pathological features
  - Decoder: Combines both components for MAE reconstruction
  - Training objective: MAE reconstruction + contrast-consistency loss + optional cross-contrast swapping (FOMO25 variant)

- **Critical path:**
  1. Acquire multi-contrast brain MRI volumes with subject IDs
  2. Pre-train encoder-decoder with MAE, enforcing anatomical consistency across same-subject scans
  3. Fine-tune on downstream segmentation/classification/regression tasks
  4. Evaluate on held-out datasets with domain shifts

- **Design tradeoffs:**
  - CNN vs. Transformer: 10× smaller/faster but may scale less well with massive data
  - Disentangled vs. monolithic latent: More interpretable and domain-aligned, but requires careful loss balancing
  - Single-GPU training: Democratizes access but limits batch size; may affect convergence

- **Failure signatures:**
  - Segmentations that ignore pathology visible only in non-T1 contrasts → contrast-specific pathway undertrained
  - High variance across same-subject scans → anatomical invariance not properly enforced
  - Good reconstruction but poor downstream transfer → representations not capturing task-relevant features

- **First 3 experiments:**
  1. **Reproduction check:** Load pre-trained weights from github.com/jbanusco/BrainFM4Challenges, run inference on a single subject with multiple contrasts; verify anatomical latent similarity across contrasts.
  2. **Ablation:** Disable the disentanglement loss (use standard MAE only) and compare downstream Dice scores on one segmentation task to quantify contribution.
  3. **Transfer test:** Fine-tune on a small held-out dataset (e.g., <100 labeled volumes) not in the original benchmarks to assess generalization beyond challenge tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do transformer architectures consistently underperform compared to CNNs in 3D medical imaging despite their dominance in natural image analysis?
- Basis in paper: [explicit] The authors explicitly state: "These results raise a critical question: why do transformers... consistently underperform CNNs in medical imaging?"
- Why unresolved: While the paper proposes hypotheses (data scarcity, tokenization bottlenecks, fragile fine-tuning), it does not isolate the causal mechanism definitively.
- What evidence would resolve it: Ablation studies on equivalent data volumes that decouple architectural inductive biases from tokenization strategies.

### Open Question 2
- Question: Can transformers eventually match or exceed CNN performance in this domain through larger datasets, efficient attention mechanisms, or hybrid architectures?
- Basis in paper: [explicit] The text notes, "Whether transformers will eventually match CNN performance... remains uncertain."
- Why unresolved: Current data (100k+ volumes) appears insufficient for transformers to learn effective attention patterns, but the scaling limit is unknown.
- What evidence would resolve it: Empirical results showing transformers closing the performance gap when trained on datasets an order of magnitude larger than currently available.

### Open Question 3
- Question: Will the efficiency and performance advantage of domain-specific inductive biases persist as datasets scale, or will they be rendered obsolete by general learning?
- Basis in paper: [explicit] The authors ask whether "this advantage may persist even as datasets scale, or it may diminish as Richard Sutton's 'bitter lesson' suggests."
- Why unresolved: It is unclear if the current superiority of anatomy-aware CNNs is a fundamental property or a temporary artifact of current dataset sizes.
- What evidence would resolve it: Longitudinal benchmarking showing whether explicit anatomical constraints provide diminishing returns relative to raw parameter and data scaling.

### Open Question 4
- Question: Do these disentangled foundation models maintain robustness and uncertainty quantification when applied to rare pathologies and domain shifts not represented in challenge benchmarks?
- Basis in paper: [inferred] The paper lists "robustness to domain shift, uncertainty quantification, and performance on rare pathologies" as critical gaps that future frameworks must assess "beyond these initial challenges."
- Why unresolved: Challenge metrics focused on average Dice and accuracy, leaving safety-critical clinical generalization unverified.
- What evidence would resolve it: Evaluation of the pre-trained models on out-of-distribution datasets featuring low-prevalence conditions and calibration metrics.

## Limitations

- Validation data scope: Both challenges used curated datasets with consistent acquisition protocols. The 8% accuracy improvement and 2.5% Dice gain may not transfer to clinical datasets with different scanners, protocols, or patient populations.
- Hyperparameter sensitivity: The paper reports final performance but doesn't extensively analyze sensitivity to disentanglement loss weights, learning rates, or architectural choices.
- Anatomical invariance assumption validity: The core mechanism assumes stable anatomy across contrasts and timepoints, which breaks down for progressive diseases, post-surgical changes, or developmental imaging.

## Confidence

**High confidence:** CNN architectures trained 1-2 orders of magnitude faster with 10× fewer parameters than transformers on these specific benchmarks. The efficiency claims are directly measurable and supported by explicit GPU-hour comparisons.

**Medium confidence:** The 2.5% Dice and 8% accuracy improvements over transformer baselines are valid for the challenge datasets but may not generalize to broader clinical populations. The results depend on challenge-specific evaluation protocols.

**Low confidence:** The specific mechanism of disentangling subject-invariant anatomy from contrast-specific pathology being responsible for the performance gains. While the architecture enforces this separation, direct ablation studies proving this is the key differentiator versus other CNN design choices are not provided.

## Next Checks

1. **Cross-site validation:** Test the pre-trained models on external clinical datasets with different acquisition protocols and scanner manufacturers to verify the 2.5% Dice and 8% accuracy gains persist outside challenge conditions.

2. **Progressive disease evaluation:** Apply the models to longitudinal datasets with known anatomical changes (tumor progression, post-surgical resection) to assess whether the subject-invariance assumption degrades performance as hypothesized.

3. **Ablation study replication:** Implement and evaluate a standard MAE baseline (no disentanglement) on the same SSL3D/FOMO25 tasks to quantify the specific contribution of the anatomical-constraint mechanism versus general CNN advantages.