---
ver: rpa2
title: Synthetic generation of 2D data records based on Autoencoders
arxiv_id: '2502.13183'
source_url: https://arxiv.org/abs/2502.13183
tags:
- records
- latent
- data
- dataset
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of limited labelled datasets
  for two-dimensional spectral measurements, specifically focusing on Gas Chromatography
  coupled with Ion Mobility Spectrometry (GC-IMS) data. The authors propose a novel
  method for generating synthetic 2D spectra using a double autoencoder architecture
  coupled with latent matrix resampling.
---

# Synthetic generation of 2D data records based on Autoencoders

## Quick Facts
- arXiv ID: 2502.13183
- Source URL: https://arxiv.org/abs/2502.13183
- Reference count: 21
- Classification accuracy improved from 75.60% ± 3.44 to 84.40% ± 3.21 using synthetic augmentation

## Executive Summary
This paper addresses the challenge of limited labelled datasets for two-dimensional spectral measurements, specifically focusing on Gas Chromatography coupled with Ion Mobility Spectrometry (GC-IMS) data. The authors propose a novel method for generating synthetic 2D spectra using a double autoencoder architecture coupled with latent matrix resampling. The approach exploits correlations within the rows and columns of the records to compress them into compact latent matrices, then generates new latent matrices based on their statistical distribution across label classes, and finally decodes these new matrices to produce synthetic records.

## Method Summary
The method uses a sequential double autoencoder architecture where two autoencoders are trained on timeseries derived from the 2D records instead of one autoencoder on the entire image. The first autoencoder encodes individual columns (drift time series) into a latent space, and the second autoencoder encodes the rows of the resulting intermediate latent matrix. This approach provides more training data by exploiting correlations within IMS scans. After training, latent matrices are grouped by label, and new latent matrices are generated by performing multivariate Gaussian sampling centered around the class mean with the computed covariance. The synthetic latent matrices are then decoded through both decoders to produce new 2D spectral records.

## Key Results
- Classification accuracy increased from 75.60% ± 3.44 to 84.40% ± 3.21 when synthetic records were included
- The method demonstrates that augmenting the training dataset with synthetic records significantly improves classification performance
- Increasing the autoencoder's latent dimension parameter improves reconstruction quality but with diminishing returns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential autoencoders enable data-efficient learning on small 2D spectral datasets by decomposing the 2D problem into two 1D compression steps.
- Mechanism: The architecture applies two autoencoders sequentially—first encoding columns (drift time) individually, then encoding rows of the resulting latent matrix. This extracts more training samples (individual timeseries) from limited records and exploits strong intra-row/column correlations inherent to GC-IMS physics.
- Core assumption: Rows and columns of the 2D spectra contain independent structure that can be learned separately without losing critical cross-dimensional dependencies.
- Evidence anchors:
  - [abstract] "The approach exploits correlations within the rows and columns of the records to compress them into compact latent matrices"
  - [section II.B] "Two sequential autoencoders are trained on timeseries derived from the records instead of one autoencoder on the entire image, providing more training data and exploiting correlations within IMS scans"
  - [corpus] Weak direct evidence; neighbor papers use autoencoders for spectra but not the sequential decomposition strategy.
- Break condition: If peak positions depend jointly on both dimensions in a non-separable way, sequential encoding may lose critical cross-correlation information.

### Mechanism 2
- Claim: Class-conditional latent space resampling generates plausible synthetic records by modeling the statistical distribution of compressed representations.
- Mechanism: After encoding all records, latent matrices are grouped by label. For each class, the element-wise mean and full covariance matrix are computed. New latent matrices are sampled via multivariate Gaussian, then decoded through both decoders to produce synthetic 2D spectra.
- Core assumption: The latent space distribution within each class is approximately multivariate Gaussian, and this distribution captures the meaningful variability of the class.
- Evidence anchors:
  - [section II.C] "New latent matrices are then generated by performing multivariate Gaussian sampling centred around Ēℓ and with the computed covariance Cov(Eℓ)"
  - [section IV.B] "The variability in the synthetic records realistically reflects the variability observed in the original distribution"
  - [corpus] Neighbor paper on MALDI-TOF MS generation uses similar synthetic data strategies for spectral augmentation.
- Break condition: If latent distributions are multimodal or heavy-tailed, Gaussian sampling will generate unrealistic intermediates or blur distinct subclusters.

### Mechanism 3
- Claim: Synthetic augmentation improves classification by increasing effective sample diversity, particularly for underrepresented classes.
- Mechanism: Doubling the dataset with synthetic records exposes the classifier to a wider range of within-class variations, regularizing the decision boundary and reducing overfitting to sparse original samples.
- Core assumption: Synthetic records preserve class-discriminative features while adding meaningful variation—i.e., decoder output retains label-relevant structure.
- Evidence anchors:
  - [abstract] "classification accuracy increased from 75.60% ± 3.44 to 84.40% ± 3.21"
  - [section IV.C] "Augmenting the dataset with synthetic records resulted in a significant improvement in the AR... by introducing greater variety"
  - [corpus] Consistent with corpus findings on GANs and synthetic data improving semi-supervised learning.
- Break condition: If synthetic records introduce systematic artifacts or fail to capture minority class nuances, they may add noise rather than signal, potentially degrading performance.

## Foundational Learning

- Concept: **Autoencoder reconstruction loss (MSE)**
  - Why needed here: The entire synthetic pipeline depends on the autoencoder's ability to compress and reconstruct spectra. Understanding the trade-off between latent dimension and reconstruction quality is critical for tuning.
  - Quick check question: If you double the latent dimension from d=16 to d=32, would you expect MSE to halve, decrease with diminishing returns, or stay roughly constant?

- Concept: **Multivariate Gaussian sampling and covariance structure**
  - Why needed here: Synthetic latent matrix generation requires sampling from a d²-dimensional Gaussian defined by class mean and covariance. Incorrect covariance estimation (e.g., from too few samples) can yield degenerate or unrealistic latent matrices.
  - Quick check question: With only 5 samples in a class and d=32, what problem might arise when estimating a 1024×1024 covariance matrix?

- Concept: **Transformer sequence modeling**
  - Why needed here: The autoencoder uses transformer encoder/decoder blocks to capture temporal dependencies in timeseries (peak shapes, positions). Understanding self-attention helps diagnose reconstruction failures.
  - Quick check question: Why might a transformer be preferred over a simple MLP for encoding GC-IMS drift time series?

## Architecture Onboarding

- Component map:
  - **Input preprocessing**: Wavelet compression → crop → log-scale → min-max normalize
  - **Autoencoder 1 (fθ/gϕ)**: Encodes individual columns (drift timeseries) R^m → R^d using transformer + fully connected layers
  - **Autoencoder 2 (f'θ'/g'ϕ')**: Encodes rows of intermediate latent matrix R^n → R^d
  - **Latent matrix E(i)**: d×d compressed representation per record
  - **Class-wise statistics**: Compute Ēℓ and Cov(Eℓ) per label
  - **Synthesis**: Multivariate Gaussian sampling → sequential decoding (g'ϕ' then gϕ)
  - **Downstream classifier**: PCA (22 components) → Random Forest

- Critical path:
  1. Preprocess all records identically (compute normalization on training split only)
  2. Train Autoencoder 1 on column timeseries with undersampling of low-variance signals
  3. Train Autoencoder 2 on row timeseries of latent matrices from step 2
  4. Encode full dataset → group latent matrices by label → compute per-class mean/covariance
  5. Sample and decode synthetic records; validate via classifier performance on held-out real data

- Design tradeoffs:
  - **Latent dimension d**: Higher d improves reconstruction (lower MSE) but exponentially increases covariance matrix size (d²×d²), slowing sampling. Paper chose d=32 as practical balance.
  - **Undersampling low-variance timeseries**: Reduces noise bias but may discard rare informative regions.
  - **Reconstructed vs. synthetic augmentation**: Paper found original + synthetic (84.4%) slightly outperformed reconstructed + synthetic (83.6%), suggesting latent resampling adds beneficial variation beyond mere reconstruction.

- Failure signatures:
  - **Blurred or missing small peaks**: Indicates latent dimension too low or training converged to dominant features only.
  - **Synthetic records with implausible artifacts**: Covariance estimation unstable (too few samples per class) or Gaussian assumption violated.
  - **Classifier accuracy unchanged or worse**: Synthetic records not preserving label-discriminative features; validate by inspecting class-conditional latent distributions.

- First 3 experiments:
  1. **Baseline reconstruction test**: Train sequential autoencoders with d∈{8,16,32}; plot MSE vs. d and visually inspect reconstructed spectra to confirm peak preservation before any synthesis.
  2. **Synthetic sanity check**: For one class, generate N synthetic records, decode, and compare peak positions/intensities to real samples via expert visual inspection or quantitative overlap metrics.
  3. **Controlled augmentation study**: Train classifier on original data only, then with synthetic augmentation; report accuracy with confidence intervals. Test on held-out real samples only to ensure synthetic data does not leak into validation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the double autoencoder architecture be modified to better preserve low-intensity peaks and fine details during reconstruction without exacerbating computational costs?
- Basis in paper: [explicit] The authors state in the conclusion that "Future research should focus on mitigating the information loss inherent in the autoencoder... synthetic samples, which often fail to capture finer details."
- Why unresolved: While increasing the latent dimension ($d$) improves reconstruction, the paper notes this effect has "diminishing returns" and significantly increases computational load, leaving the loss of smaller features as an unresolved trade-off.
- What evidence would resolve it: Demonstrating a modified architecture or loss function that statistically improves the signal-to-noise ratio of low-intensity peaks in synthetic samples compared to the current baseline.

### Open Question 2
- Question: Is the proposed sequential autoencoder method effective for synthesizing records in other 2D spectral domains that lack simulation software, such as Fluorescence Spectroscopy or ECG?
- Basis in paper: [explicit] The conclusion explicitly identifies "further applications... such as Fluorescence Spectroscopy in 2D maps, Microarray Analysis, or any 2D frequency spectra of timeseries signals" as an area for exploration.
- Why unresolved: The method was validated exclusively on GC-IMS data; its applicability to the suggested domains, which may have different correlation structures between rows and columns, remains unproven.
- What evidence would resolve it: Successful training and deployment of the pipeline on the listed datasets, yielding classification improvements comparable to the $\approx 8.9\%$ gain observed for GC-IMS.

### Open Question 3
- Question: Does the assumption of a multivariate Gaussian distribution for latent matrix sampling accurately reflect the underlying data manifold, or does it generate physically implausible samples?
- Basis in paper: [inferred] The synthesis method (Eq. 4) relies entirely on multivariate Gaussian sampling defined by the mean and covariance ($\bar{E}_\ell, Cov(E_\ell)$), but the paper provides no validation that the latent vectors conform to this distribution.
- Why unresolved: If the latent space is non-Gaussian (e.g., multimodal), this sampling strategy could generate synthetic records that average distinct features rather than creating valid new examples.
- What evidence would resolve it: A comparative study evaluating the quality of samples generated via Gaussian sampling versus those generated using non-parametric or variational approaches.

## Limitations

- The sequential autoencoder architecture assumes that rows and columns of 2D spectra can be independently encoded without losing critical cross-dimensional correlations, an assumption lacking direct empirical validation
- The multivariate Gaussian sampling in latent space may not capture complex multimodal distributions within classes, potentially limiting synthetic data quality
- The exact Transformer architecture parameters (layers, heads, dimensions) are unspecified, which could affect reproducibility and performance

## Confidence

- **High Confidence**: The observed classification improvement (75.60% → 84.40%) from synthetic augmentation is well-documented and statistically significant
- **Medium Confidence**: The mechanism of sequential autoencoders extracting training samples from 2D records is plausible but assumes feature separability that wasn't rigorously tested
- **Medium Confidence**: The Gaussian sampling assumption for latent matrices is reasonable but may fail for complex class distributions

## Next Checks

1. **Cross-dimensional dependency test**: Train a single autoencoder on full 2D records and compare reconstruction quality and downstream classification performance against the sequential approach to quantify information loss from decomposition

2. **Latent distribution analysis**: Visualize and quantify the shape of class-conditional latent distributions (e.g., using PCA/t-SNE) to assess Gaussian assumption validity and identify potential multimodality or heavy tails

3. **Minority class stress test**: Generate synthetic records for the smallest class (e.g., P. aeruginosa) and measure whether classification accuracy on this class improves proportionally compared to majority classes