---
ver: rpa2
title: Domain Adaptation of LLMs for Process Data
arxiv_id: '2509.03161'
source_url: https://arxiv.org/abs/2509.03161
tags:
- process
- llms
- event
- data
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic approach for adapting large language
  models (LLMs) to predictive process monitoring (PPM) tasks using parameter-efficient
  fine-tuning (PEFT) methods. The authors directly fine-tune LLMs on process data
  without converting it to natural language narratives, replacing the embedding layers
  to handle domain-specific activity labels.
---

# Domain Adaptation of LLMs for Process Data

## Quick Facts
- **arXiv ID**: 2509.03161
- **Source URL**: https://arxiv.org/abs/2509.03161
- **Reference count**: 19
- **Primary result**: LLM-based solutions outperform state-of-the-art RNNs for PPM tasks, particularly with LoRA adapters for regression and multi-task training

## Executive Summary
This paper presents a systematic approach for adapting large language models (LLMs) to predictive process monitoring (PPM) tasks using parameter-efficient fine-tuning (PEFT) methods. The authors directly fine-tune LLMs on process data without converting it to natural language narratives, replacing the embedding layers to handle domain-specific activity labels. They compare different PEFT methods (LoRA, freezing) across three LLMs (PM-GPT2, Qwen2.5, Llama3.2) for next activity (NA) and remaining time (RT) prediction tasks using real-world event logs (BPI12, BPI17, BPI20 variants).

## Method Summary
The approach replaces language-based tokenization with task-specific embeddings, mapping categorical activity IDs directly to dense vectors. The pretrained transformer backbone processes these abstract vectors using existing attention mechanisms. Parameter-efficient fine-tuning methods include full freezing, partial freezing of specific layers, and LoRA adapters with rank r=256 and alpha=512. The models are trained on five public event logs using trace encoding (many-to-many with teacher forcing) for both single-task and multi-task settings. Input layers consist of custom embeddings for categorical activity labels plus linear projections for numerical time features, while output layers have separate linear heads for classification (NA) and regression (RT) tasks.

## Key Results
- LLMs outperform state-of-the-art RNNs for RT prediction, particularly in multi-task settings
- LoRA adapters significantly outperform simple freezing for regression tasks, providing more stable convergence
- Models converge quickly within few epochs and require minimal hyperparameter optimization
- Multi-task training improves performance over single-task baselines, acting as a regularizer

## Why This Works (Mechanism)

### Mechanism 1: Native Embedding of Process Sequences
Pretrained LLMs can interpret structured process data by treating activity labels as a native vocabulary rather than natural language text, leveraging inherent sequence modeling capabilities. The standard tokenizer and semantic embedding layer are replaced by a randomly initialized projection layer that maps categorical activity IDs directly to dense vectors. The transformer backbone processes these abstract vectors using its existing attention mechanisms, bypassing the need for linguistic context.

### Mechanism 2: Low-Rank Adaptation (LoRA) for Regression Alignment
LoRA adapters serve as a critical bridge to adapt classification-pretrained models to regression tasks, outperforming simple layer freezing. By injecting trainable low-rank matrices into the frozen backbone, the model gains the flexibility to output continuous values for time prediction without disrupting the pretrained weights optimized for discrete token classification.

### Mechanism 3: Multi-Task Regularization
Joint training on Next Activity (NA) and Remaining Time (RT) stabilizes the model, acting as a regularizer that improves performance over single-task baselines. The model learns a shared representation of the process state where the classification task anchors the sequence logic, providing stable gradients while the regression task benefits from this structured understanding.

## Foundational Learning

- **Parameter-Efficient Fine-Tuning (PEFT) / LoRA**
  - Why needed: Adapting massive LLMs (1B+ params) to small event logs without the computational cost of full fine-tuning
  - Quick check: In the equation W' = W + BA, which matrices are trained and which remain frozen?

- **Predictive Process Monitoring (PPM) Tasks**
  - Why needed: The paper evaluates two distinct tasks—Next Activity (Classification) and Remaining Time (Regression)
  - Quick check: Why might a model trained solely on Next Token Prediction struggle with Remaining Time prediction?

- **Decoder-Only Transformers & Trace Encoding**
  - Why needed: The paper uses "trace encoding" (many-to-many) with decoder-only models rather than traditional prefix encoding
  - Quick check: In the trace encoding setup ⟨a, b, c, d, e⟩ → ⟨b, c, d, e, eos⟩, how does the model predict the output for index i without seeing index i+1?

## Architecture Onboarding

- **Component map**: Input Layers (Trainable) → Backbone (Frozen + LoRA) → Output Layers (Trainable)
- **Critical path**: Preprocessing → Custom embedding layer → Frozen Backbone + LoRA → Output Heads → Loss calculation
- **Design tradeoffs**: LoRA is significantly more stable for RT prediction but adds parameters; freezing is lighter but often fails to adapt classification priors to regression targets
- **Failure signatures**: S-NAP yields very low accuracy (~2%) with multilingual labels; RT loss shows spiky, unstable convergence with standard freezing
- **First 3 experiments**: 
  1. Train LSTM baselines on BPI12 to establish multi-task difficulty
  2. Fine-tune Llama3.2 on BPI17 with Full Freezing vs. LoRA, observing RT MSE delta
  3. Replace custom embedding with standard tokenizer on BPI20PTC to verify bypassing natural language improves accuracy

## Open Questions the Paper Calls Out

### Open Question 1
Can this domain adaptation framework be effectively extended to generative tasks like process discovery and anomaly detection? The authors identify this as future work since current evaluation only covers predictive tasks, while discovery requires generating structural representations.

### Open Question 2
What specific architectural modifications are required to stabilize LLMs for regression tasks like remaining time prediction? While LoRA helps, the paper highlights that regression remains difficult for models inherently designed for next-token classification.

### Open Question 3
To what extent can model quantization be combined with PEFT to reduce computational overhead? The paper notes that PEFT still requires more trainable parameters than RNNs, suggesting quantization as a future optimization.

## Limitations

- Embedding dimension for input/output layers is unspecified, making it unclear whether backbone dimensions were appropriately matched
- LoRA rank r=256 and alpha=512 choices appear arbitrary without sensitivity analysis
- Only compares against RNN baselines without testing other transformer-based PPM approaches
- Multi-task benefit lacks ablation studies showing whether improvement stems from regularization or complementary task learning

## Confidence

- **High Confidence**: LLMs outperform RNN baselines in convergence speed and require less hyperparameter tuning; replacement of language tokenization with task-specific embeddings is technically sound
- **Medium Confidence**: LoRA adapters provide more stable regression performance than freezing; multi-task training improves results over single-task baselines
- **Low Confidence**: Claim of being "first direct application" of LLMs to PPM tasks is difficult to verify definitively; superiority over S-NAP may be dataset-dependent

## Next Checks

1. Systematically vary the embedding dimension (e.g., 64, 128, 256, 512) across all three LLMs to determine optimal sizing
2. Conduct comprehensive grid search over LoRA rank (r=64, 128, 256, 512) and alpha (32, 64, 128, 256, 512) parameters for RT prediction
3. Implement controlled experiments comparing NA-only, RT-only, sequential multi-task, and parallel multi-task training to quantify regularization vs. complementary learning benefits