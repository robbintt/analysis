---
ver: rpa2
title: Convergence of gradient based training for linear Graph Neural Networks
arxiv_id: '2501.14440'
source_url: https://arxiv.org/abs/2501.14440
tags:
- gradient
- graph
- matrix
- convergence
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the convergence properties of gradient-based
  training methods for linear graph neural networks (GNNs). The authors prove that
  gradient flow training with mean squared loss converges exponentially to the global
  minimum, with the convergence rate explicitly depending on initial weights and the
  graph shift operator.
---

# Convergence of gradient based training for linear Graph Neural Networks

## Quick Facts
- **arXiv ID:** 2501.14440
- **Source URL:** https://arxiv.org/abs/2501.14440
- **Reference count:** 40
- **Primary result:** Proves exponential convergence of gradient flow training for linear GNNs to global minimum without requiring positive singular margin assumption.

## Executive Summary
This paper provides theoretical analysis of gradient-based training for linear Graph Neural Networks (GNNs), proving that gradient flow with mean squared loss converges exponentially to the global minimum. The authors establish convergence guarantees that apply to both full-rank and low-rank feature matrices, extending previous results that required restrictive assumptions. They show the convergence rate explicitly depends on initial weights and the graph shift operator, and demonstrate that balanced initialization leads to minimum energy solutions. The theoretical findings are validated through experiments on synthetic graph datasets and real-world climate data, showing how convergence rates vary with graph structure and aggregation operators.

## Method Summary
The method analyzes linear GNNs with multiple linear layers and graph shift operators, training them using gradient flow and gradient descent to minimize mean squared error. The architecture consists of H linear layers with weights W₁, W₂, ..., W_{H+1} applied to node features X through graph shift operator S. Training uses specific initialization (W₁=0 matrix, subsequent layers full-rank) to guarantee convergence to global minimum. The theoretical framework establishes exponential convergence rates that depend on the smallest non-zero singular value of aggregated features and shows that balanced initialization preserves structural properties leading to minimum energy solutions.

## Key Results
- Proves exponential convergence of gradient flow training to global minimum without requiring positive singular margin assumption
- Shows convergence rate depends explicitly on initial weights and graph shift operator, with low-rank feature matrices converging at rate determined by smallest non-zero singular value
- Demonstrates that balanced initialization ensures converged solution minimizes total weight energy across all layers

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Initializing the first layer as a zero matrix while maintaining full-rank subsequent layers guarantees convergence to the global minimum by confining the optimization trajectory to a neighborhood devoid of sub-optimal critical points.
- **Mechanism:** The proof (Theorem 3.1) relies on constructing a specific initialization region where the gradient norm relative to the loss remains bounded away from zero. By setting W₁(0)=0 and ensuring σ_{min}(W_ℓ(0)) is sufficiently large for ℓ ≥ 2, the gradient flow is forced to stay within a "safe" radius where the only critical point is the global minimum, effectively bypassing saddle point issues without needing strict margin assumptions.
- **Core assumption:** The hidden feature dimensions are non-increasing (d₁ ≥ d₂ ≥ ... ≥ d_{H+1}), and the labeled data is such that the gradient flow remains within the defined neighborhood of the initial weights.
- **Evidence anchors:**
  - [Section 3, Theorem 3.1]: "suppose the initial weight W₁(0) is the zero matrix and W_ℓ(0) is full rank... provided σ_{min}(W_ℓ(0)) is sufficiently large..."
  - [Section 3, Proof]: "...ensure that the gradient flow always stay in that neighborhood of the initial weights."
  - [Corpus]: Corpus evidence is weak; neighbors focus on general linear networks without specifying this "zero-first-layer" heuristic.
- **Break condition:** If the subsequent layers (W_ℓ for ℓ ≥ 2) are not initialized with sufficiently large singular values, the "safe" neighborhood collapses, and the proof guaranteeing the absence of sub-optimal critical points fails.

### Mechanism 2
- **Claim:** The convergence rate is governed by the smallest *non-zero* singular value of the aggregated feature matrix, allowing the model to train efficiently even on rank-deficient (low-rank) graph data.
- **Mechanism:** Unlike standard deep learning convergence proofs that often require full-rank data, this mechanism scales the convergence rate by σ_{small}((XS^H)_I). If the graph shift operator S or feature matrix X results in a rank-deficient product, the optimization effectively ignores the null space, preventing singular value zeros from stalling the convergence.
- **Core assumption:** The loss function is Mean Squared Error (MSE), and the convergence is measured relative to the global minimum of the linear system.
- **Evidence anchors:**
  - [Section 3, Remark 3.4]: "...implies that the training loss... converges to the global minimum... even if (XS^H)_I has the rank deficiency."
  - [Section 3, Theorem 3.1]: "convergence rate... depends explicitly on... σ²_{small}((XS^H)_I)."
  - [Corpus]: Related work (Neighbor 83546) discusses convergence in CNNs/Linear networks but typically assumes full-rank or generic positions; this specific low-rank tolerance is a distinct signal from the paper.
- **Break condition:** If the smallest *non-zero* singular value is extremely small (high condition number), the convergence rate becomes theoretically exponential but practically indistinguishable from stalling.

### Mechanism 3
- **Claim:** Initializing weights in a "balanced" state ensures the converged solution minimizes the total weight energy (Frobenius norm) across all layers.
- **Mechanism:** A balanced initialization (W_ℓ W_ℓ^T = W_{ℓ+1}^T W_{ℓ+1}) is preserved by the gradient flow dynamics. This structural constraint forces the singular values to be distributed uniformly across layers (σ_i(W_ℓ) = σ_i(W_{ℓ+1})^{1/(H+1)}), which geometrically leads to the minimum-energy solution for the weight product W_{H+1}...W₁.
- **Core assumption:** The gradient flow converges to a global minimum, and the architecture has no bottlenecks (hidden dimensions ≥ min(d_x, d_y)).
- **Evidence anchors:**
  - [Section 4, Theorem 4.2]: "If the collection W is balanced... then W is the solution to the optimization problem (4.2) [minimizing total weights]."
  - [Section 4]: "...balanced condition... provide algebraic relation... all weight matrices W_ℓ share the same set of non-zero singular values."
  - [Corpus]: Weak corpus support for the "balanced" constraint specifically in GNNs; general implicit acceleration literature exists but is broader.
- **Break condition:** If initialization is random (unbalanced), the model may still converge in loss but will likely converge to a solution with larger, redundant weight magnitudes, failing the energy minimization property.

## Foundational Learning

- **Concept:** Graph Shift Operators (S)
  - **Why needed here:** The convergence rate depends explicitly on the singular values of (XS^H)_I. You must understand how different operators (Adjacency, Laplacian, Normalized Laplacian) affect the spectral properties of the features to predict training speed.
  - **Quick check question:** If you switch from an unnormalized Adjacency matrix to a Normalized Laplacian, does the paper suggest convergence speed becomes more or less dependent on graph sparsity?

- **Concept:** Singular Value Decomposition (SVD) & Rank
  - **Why needed here:** The entire theoretical framework relies on tracking σ_{min} (smallest singular value) and distinguishing it from σ_{small} (smallest *non-zero* singular value) to handle rank-deficient data.
  - **Quick check question:** Why does the proof claim convergence is possible even if the input data matrix is not full rank?

- **Concept:** Gradient Flow vs. Gradient Descent
  - **Why needed here:** The primary proofs are for continuous-time gradient flow. Section 5 maps this to discrete-time Gradient Descent (GD), showing that GD is a discretization that holds only if the step size η is sufficiently small.
  - **Quick check question:** According to Theorem 5.1, what happens to the convergence guarantee if the step size η is not sufficiently small relative to the gradient Lipschitz constant M?

## Architecture Onboarding

- **Component map:** X -> S -> W₁ -> S -> W₂ -> ... -> S -> W_{H+1} -> Ŷ
- **Critical path:** Initialization (W₁=0, rest full rank) → Forward Pass (X · S · W₁ · ...) → Loss Calculation (MSE) → Gradient Flow Update
- **Design tradeoffs:**
  - **Depth (H) vs. Speed:** Increasing depth H introduces a factor of 1/4^{H-1} in the convergence rate bound, suggesting shallower networks may theoretically converge faster in this linear regime.
  - **Shift Operator Choice:** Unnormalized operators (Adjacency, Laplacian) show high sensitivity to graph sparsity (slower in sparse graphs), while Normalized Laplacian shows consistent but potentially slower convergence regardless of structure.
- **Failure signatures:**
  - **Divergence:** Occurs in discrete GD if step size η is too large relative to the gradient norms.
  - **Stalling:** If σ_{small}((XS^H)_I) is near zero, the exponential convergence rate constant approaches 1, causing practical stalling.
  - **High Weight Energy:** If you skip "balanced" initialization, the model converges in loss but results in unnecessarily large weight norms.
- **First 3 experiments:**
  1. **Validation of Initialization:** Train a 2-layer Linear GNN with W₁(0)=0 and random full-rank W₂(0) vs. standard Xavier initialization. Plot the loss trajectory to verify if the "zero-start" avoids local minima more effectively.
  2. **Shift Operator Sensitivity:** Run training on Erdős–Rényi graphs with varying edge probability p. Compare convergence times using the Adjacency matrix vs. Normalized Laplacian to confirm the paper's finding that unnormalized operators are sensitive to sparsity.
  3. **Balanced vs. Unbalanced Energy:** Initialize two models, one balanced (W_ℓ W_ℓ^T = W_{ℓ+1}^T W_{ℓ+1}) and one random. Compare the Frobenius norm of the weights Σ||W_ℓ||_F at convergence to verify the energy minimization property.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the convergence guarantees be extended to linear GNN architectures that include bottleneck layers (where hidden dimensions decrease significantly)?
- **Basis in paper:** [Explicit] The Conclusion states, "In order to avoid the existence of any sub-local minimum, we focus our study on GNNs without bottleneck layer."
- **Why unresolved:** The authors explicitly exclude this architectural variant to ensure the loss surface is free of sub-optimal local minima. Introducing bottlenecks may create spurious critical points that prevent convergence to the global minimum.
- **What evidence would resolve it:** A theoretical proof showing convergence (or divergence) for linear GNNs where layer dimensions d_ℓ strictly decrease, or the identification of specific conditions under which bottlenecks do not create sub-optimal minima.

### Open Question 2
- **Question:** Is there a practical initialization strategy that ensures convergence to the minimum-energy global minimum without violating the feasibility constraints imposed by the balanced condition?
- **Basis in paper:** [Explicit] Page 14 notes regarding Theorem 4.4: "The balanced condition imposes such restriction on the initialization which causes the inequality (4.4) infeasible."
- **Why unresolved:** While balanced initialization is theoretically required to minimize the total weights (energy) at the global minimum, the mathematical constraints required for convergence conflict with the feasibility of the initialization inequality in practice.
- **What evidence would resolve it:** A constructive proof or algorithm that provides a balanced initialization satisfying the inequality (4.4), or an alternative formulation of the optimization problem where the feasibility constraint is relaxed.

### Open Question 3
- **Question:** Under what specific conditions does the normalized gradient flow converge to the global minimum for linear GNNs?
- **Basis in paper:** [Inferred] Remark 6.1 discusses using the normalized gradient flow for numerical stability but states, "one loses the guarantee of exponential convergence."
- **Why unresolved:** The paper's main theorems rely on the standard (unnormalized) gradient flow equation. The authors suggest normalization to handle larger graphs but do not provide theoretical bounds or convergence rates for this modified dynamic.
- **What evidence would resolve it:** A convergence analysis for the normalized dynamics dW_ℓ/dt = -∇L/||∇L||_F establishing if, and at what rate, it converges to the global minimum.

## Limitations
- Initialization Dependency: Convergence guarantee relies on non-standard initialization (W₁=0 matrix, subsequent layers full-rank with large singular values).
- Continuous vs. Discrete Time: Main theoretical results are for continuous gradient flow; discrete gradient descent extension requires strict step size constraints.
- Graph Structure Sensitivity: Unnormalized shift operators show high sensitivity to graph sparsity, with practical implications for real-world graphs not fully explored.

## Confidence
- **High Confidence:** The core theoretical framework regarding convergence to global minimum for linear GNNs is mathematically sound within stated assumptions.
- **Medium Confidence:** The extension from continuous gradient flow to discrete gradient descent is theoretically justified but may be less robust in practice.
- **Low Confidence:** Practical implementation details (numerical integration scheme, exact step size schedules) are not fully specified, creating uncertainty about reproducibility.

## Next Checks
1. **Initialization Robustness Test:** Implement parameter sweep testing various initialization schemes for W₁ (zero vs. small random vs. Xavier) and W₂ (full rank with varying singular values) on Erdős–Rényi graphs. Quantify how initialization affects convergence speed and whether "zero-start" consistently outperforms other methods.

2. **Step Size Sensitivity Analysis:** For gradient descent training, systematically vary the learning rate η across multiple orders of magnitude. Plot convergence curves to identify threshold where Theorem 5.1's guarantees break down and characterize transition from stable convergence to divergence or stagnation.

3. **Graph Structure Stress Test:** Evaluate convergence on graphs with extreme structural properties: very sparse (p < 0.01), very dense (p > 0.5), and with community structure (SBM with varying inter/intra-community edge probabilities). Compare Normalized Laplacian vs. unnormalized operators to quantify practical impact of sparsity sensitivity findings.