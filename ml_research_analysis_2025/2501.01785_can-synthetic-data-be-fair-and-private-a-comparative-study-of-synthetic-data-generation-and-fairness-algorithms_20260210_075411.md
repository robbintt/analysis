---
ver: rpa2
title: Can Synthetic Data be Fair and Private? A Comparative Study of Synthetic Data
  Generation and Fairness Algorithms
arxiv_id: '2501.01785'
source_url: https://arxiv.org/abs/2501.01785
tags:
- fairness
- data
- privacy
- synthetic
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the balance between privacy and fairness
  in synthetic data generation for learning analytics, addressing the challenge that
  privacy-preserving techniques often compromise fairness. Five synthetic data generators
  (CTGAN, DistilGPT2, ADSGAN, PATEGAN, and DECAF) were evaluated across three real-world
  datasets using privacy metrics (Jensen-Shannon Distance, Wasserstein Distance, membership
  inference accuracy, and k-anonymity) and fairness metrics (ABROCA, ERD, and TPRD).
---

# Can Synthetic Data be Fair and Private? A Comparative Study of Synthetic Data Generation and Fairness Algorithms

## Quick Facts
- **arXiv ID**: 2501.01785
- **Source URL**: https://arxiv.org/abs/2501.01785
- **Reference count**: 40
- **Primary result**: Pre-processing fairness algorithms improve synthetic data fairness more than real data, with DECAF achieving best privacy-fairness balance but suffering in predictive utility.

## Executive Summary
This study investigates the balance between privacy and fairness in synthetic data generation for learning analytics, addressing the challenge that privacy-preserving techniques often compromise fairness. Five synthetic data generators (CTGAN, DistilGPT2, ADSGAN, PATEGAN, and DECAF) were evaluated across three real-world datasets using privacy metrics (Jensen-Shannon Distance, Wasserstein Distance, membership inference accuracy, and k-anonymity) and fairness metrics (ABROCA, ERD, and TPRD). Results show that DECAF achieves the best balance between privacy and fairness but suffers in predictive utility (AUC-ROC). Notably, applying pre-processing fairness algorithms (Suppression, Correlation Remover, Disparate Impact Remover, and Reweighing) to synthetic data improved fairness more effectively than when applied to real data, with CTGAN-RW showing the highest improvement (21.5% on one dataset). These findings suggest that combining synthetic data generation with fairness pre-processing offers a promising approach to creating fairer learning analytics models while maintaining acceptable privacy levels.

## Method Summary
The study evaluated five synthetic data generators (CTGAN, DistilGPT2, ADSGAN, PATEGAN, DECAF) on three real-world datasets (UCI Math, OULAD, Law School) with different sensitive attributes. For each SDG, synthetic data was generated and evaluated for privacy using JSD, WD, MIA accuracy, and k-anonymity. Four pre-processing fairness algorithms (Suppression, Correlation Remover, Disparate Impact Remover, Reweighing) were then applied to the synthetic training data. Four ML models (RF, XGB, LR, GNB) were trained with grid search and 5-fold CV on 70/30 splits (Same Train, Real Test paradigm), then evaluated on held-out real test data for fairness (ABROCA, ERD, TPRD) and utility (AUC-ROC). All experiments ran in Synthcity on Google Colab with Nvidia A100 GPU.

## Key Results
- DECAF achieved the best balance between privacy and fairness by removing protected attributes and causal paths during generation
- Applying pre-processing fairness algorithms to synthetic data improved fairness more than when applied to real data (CTGAN-RW showed 21.5% improvement)
- DECAF suffered in predictive utility (AUC-ROC dropped to 0.47-0.51), confirming the triangular trade-off between privacy, fairness, and utility
- Small datasets (n=395) showed unstable results with some negative fairness outcomes after pre-processing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DECAF achieves the best privacy-fairness balance by removing protected attributes and causal paths during generation, which reduces similarity to original data (improving privacy) while minimizing biased causal structures (improving fairness).
- Mechanism: DECAF incorporates fairness constraints directly into training by penalizing biased outcomes in the loss function. During inference, it can delete specific causal paths (e.g., sensitive attributes like age, gender) that propagate unfairness. This deletion reduces similarity with the original dataset, incidentally improving privacy.
- Core assumption: That biased outcomes in downstream models are causally linked to specific attribute paths that can be identified and removed without destroying data utility entirely.
- Evidence anchors:
  - [abstract] "Our results highlight that the DEbiasing CAusal Fairness (DECAF) algorithm achieves the best balance between privacy and fairness."
  - [section 4.1] "One reason may be that as a fairness-orientated algorithm, it supports debasing during the inference stage by removing specific causal paths... The reason for good privacy performance is that to remove bias, DECAF allows the deletion of some protected attributes... which causes a slight decrease in similarity with the original dataset, thereby improving privacy."
  - [corpus] Van Breugel et al. (2021) introduced DECAF; corpus papers confirm causal approaches for fair synthetic data are an active research direction.
- Break condition: If causal relationships in the data are too complex or interdependent, removing paths may destroy predictive utility entirely (as observed with DECAF's AUC-ROC dropping to 0.47 on Dataset A).

### Mechanism 2
- Claim: Pre-processing fairness algorithms applied to synthetic data improve fairness more than when applied to real data, because SDGs inherently address some fairness concerns during generation, creating an additive effect.
- Mechanism: Synthetic data generators like CTGAN use conditional generators and sampled training that can mitigate class imbalances. When pre-processing algorithms (e.g., Reweighing, Suppression) are then applied, they operate on data already partially debiased, amplifying the fairness improvement.
- Core assumption: That synthetic data generation functions as an initial fairness pass, and that pre-processing algorithms interact constructively with the synthetic data's statistical properties rather than conflicting with them.
- Evidence anchors:
  - [abstract] "Notably, applying pre-processing fairness algorithms to synthetic data improves fairness even more than when applied to real data."
  - [section 4.2] "A key insight from these results is that the SDG itself may inherently address some fairness concerns, functioning similarly to a fairness algorithm... the significant improvements observed across synthetic datasets indicate that SDGs, when combined with fairness algorithms, may offer a more robust and effective method for mitigating bias."
  - [corpus] Corpus evidence on this specific mechanism is limited; most related work focuses on incorporating fairness constraints during generation rather than post-hoc pre-processing.
- Break condition: If the SDG is too conservative (e.g., PATEGAN with strong differential privacy), the synthetic data may lack the statistical richness needed for pre-processing algorithms to work effectively; smaller datasets (n=395 in Dataset A) showed unstable results with some negative fairness outcomes.

### Mechanism 3
- Claim: Privacy and fairness exhibit a direct relationship when paired together, but both have a joint inverse relationship with predictive utility—no single SDG currently balances all three dimensions well.
- Mechanism: The triangular trade-off operates because: (1) achieving privacy requires dissimilarity from real data, (2) achieving fairness requires modifying data distributions away from biased historical patterns, and (3) both modifications reduce the signal needed for accurate predictions.
- Core assumption: That utility (predictive accuracy), privacy, and fairness are fundamentally competing objectives in data representation, and that current architectures cannot simultaneously optimize all three.
- Evidence anchors:
  - [section 4.1] "The relative imbalance of CTGAN and DGPT in privacy and fairness may stem from their primary focus on data utility... This observation further supports previous research that demonstrated the trade-off relationship between privacy, fairness, and utility."
  - [section 5] "Although in our experiments, one SDG (i.e., DECAF) achieved the best balance in the inverse relationship between fairness and privacy, when placed in the broader triangular relationship of fairness, privacy, and utility, it still struggles to maintain a good balance."
  - [corpus] Multiple corpus papers (e.g., "Private and Fair Machine Learning: Revisiting the Disparate Impact of Differentially Private SGD") confirm DP can negatively impact fairness, supporting the trade-off hypothesis.
- Break condition: If downstream tasks prioritize different objectives (e.g., decision-support vs. fully automated decisions), the optimal balance point shifts; the "best" SDG depends on context.

## Foundational Learning

- **Generative Adversarial Networks (GANs) for Tabular Data**
  - Why needed here: All five SDGs in this study are GAN-based or use neural generators; understanding how adversarial training produces synthetic data is essential for interpreting privacy-fairness trade-offs.
  - Quick check question: Can you explain how a generator and discriminator interact during GAN training, and why mode collapse might affect fairness?

- **Differential Privacy (DP) Fundamentals**
  - Why needed here: PATEGAN explicitly uses DP; understanding epsilon values and how noise injection affects both privacy guarantees and data quality is critical.
  - Quick check question: What does a lower epsilon value (e.g., ε=1 used in PATEGAN) imply for privacy strength and data utility?

- **Fairness Metrics and Group-Based Disparities**
  - Why needed here: ABROCA, ERD, and TPRD measure different aspects of group fairness; knowing what each captures helps interpret why different SDG+fairness-algorithm combinations produce different results.
  - Quick check question: What is the difference between Equal Opportunity (TPRD) and overall error rate parity (ERD)?

## Architecture Onboarding

- **Component map**:
  Data Layer (3 real-world datasets) -> SDG Layer (5 generators) -> Fairness Pre-processing Layer (4 algorithms) -> ML Model Layer (4 classifiers) -> Evaluation Layer (Privacy, Fairness, Utility metrics)

- **Critical path**:
  1. Generate synthetic data with selected SDG
  2. Evaluate privacy metrics immediately (similarity distances + re-identification risk)
  3. Apply pre-processing fairness algorithm to training split
  4. Train ML models on debiased synthetic data
  5. Evaluate on held-out real data for fairness and utility

- **Design tradeoffs**:
  - **DECAF**: Best privacy-fairness, worst utility—use when fairness/privacy are mandatory and accuracy is secondary
  - **CTGAN + Reweighing**: Highest fairness improvement potential (21.5% in study)—use when you can apply post-processing
  - **ADSGAN**: Moderate privacy, variable fairness—use when privacy is primary but some fairness matters
  - **PATEGAN**: Strongest privacy guarantees (DP), but extreme trade-offs—use only when DP compliance is required
  - **DGPT**: Leans fairness, moderate balance—use when LLM-based generation is preferred

- **Failure signatures**:
  - DECAF AUC-ROC dropping to ~0.47-0.51 indicates over-regularization; reduce causal path removal
  - Negative fairness improvements (e.g., ADSGAN with Correlation Remover: -3.8%) indicate algorithm-SDG mismatch
  - High MIA accuracy (>0.7) indicates privacy leakage; switch to DP-based SDG or increase regularization
  - Instability on small datasets (n<500) suggests ensembling multiple synthetic samples

- **First 3 experiments**:
  1. **Baseline characterization**: Generate synthetic data with all five SDGs on your dataset; evaluate privacy metrics only to identify candidates meeting your privacy threshold.
  2. **Fairness-algorithm pairing**: For each SDG passing privacy threshold, test all four pre-processing algorithms; measure fairness improvement percentage against real-data baseline.
  3. **Utility ceiling check**: For the top 2-3 SDG+fairness-algorithm combinations, evaluate AUC-ROC; if below your minimum acceptable threshold, iterate by relaxing SDG constraints (e.g., reduce DECAF path removal, increase PATEGAN epsilon).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a synthetic data generator (SDG) simultaneously optimize privacy, fairness, and utility without significant trade-offs?
- Basis in paper: [explicit] The conclusion states that while DECAF balances privacy and fairness, it "suffers in utility," and the authors note there is still a "lack of SDGs capable of balancing fairness, privacy, and utility simultaneously."
- Why unresolved: The paper highlights a "triangular relationship" where privacy and fairness often have a joint inverse relationship with predictive accuracy (utility).
- What evidence would resolve it: The development or identification of an SDG that maintains high AUC-ROC scores while minimizing membership inference accuracy and fairness metric disparities (ABROCA, ERD) on the same dataset.

### Open Question 2
- Question: Do pre-processing fairness algorithms improve fairness in synthetic non-tabular data (e.g., time-series, images) to the same degree observed in tabular data?
- Basis in paper: [explicit] The authors explicitly list this as a limitation, stating the study "is limited to tabular data" and suggesting "Future research could extend the scope to include these other data types."
- Why unresolved: The mechanisms of pre-processing algorithms like Disparate Impact Remover may function differently on the high-dimensional feature spaces of images or the temporal dependencies of time-series data found in learning analytics.
- What evidence would resolve it: A replication of the study's methodology using educational datasets containing log streams or visual data, measuring fairness improvements after applying pre-processing techniques.

### Open Question 3
- Question: Does averaging evaluation metrics over multiple synthetic data generations significantly alter the observed privacy-fairness balance?
- Basis in paper: [explicit] The authors note a limitation where they did not "generate synthetic data multiple times and average the evaluation metrics... due to computational constraints," identifying this as a necessary step to "enhance robustness in future work."
- Why unresolved: Differential privacy methods introduce noise, and GANs introduce randomness; single-generation evaluations may not capture the stable performance characteristics of the algorithms.
- What evidence would resolve it: A comparative study where metrics (JSD, MIA, ABROCA) are aggregated over multiple generation runs (e.g., $n>10$) to determine if the Pareto frontier shifts or stabilizes compared to the single-run results reported in this paper.

## Limitations

- Small-sample instability: Dataset A (n=395) showed unstable results with some negative fairness outcomes after pre-processing
- Limited scope: Study only evaluated tabular data, excluding time-series and image data common in learning analytics
- Single-generation evaluation: Metrics were not averaged over multiple synthetic data generations due to computational constraints

## Confidence

- **High Confidence**: Privacy-fairness trade-off existence, DECAF's best balance claim (directly supported by experimental results), pre-processing effectiveness on synthetic data (quantified improvements reported)
- **Medium Confidence**: Mechanism 1 (causal path deletion improving both privacy and fairness) - theoretically sound but relies on specific DECAF architecture details; Mechanism 2 (synthetic data as fairness pre-pass) - novel insight with limited corpus backing
- **Low Confidence**: Triangular trade-off framework's universality (assumes all three objectives are equally important across contexts; utility may be deprioritized in some fairness-critical applications)

## Next Checks

1. **Replicate small-sample sensitivity**: Run the full SDG+fairness-algorithm pipeline on Dataset A with 5 different random seeds; verify stability of fairness improvement percentages and identify thresholds where negative outcomes emerge.
2. **Causal path dependency test**: For DECAF, systematically vary the number of causal paths removed during inference; measure the relationship between path deletion count, privacy improvement (MIA), fairness improvement (ABROCA), and utility degradation (AUC-ROC).
3. **Pre-processing algorithm ablation**: Test each pre-processing algorithm in isolation on real data vs. synthetic data (generated by utility-focused SDG like CTGAN); quantify whether the "additive effect" is consistent across different fairness metrics and dataset sizes.