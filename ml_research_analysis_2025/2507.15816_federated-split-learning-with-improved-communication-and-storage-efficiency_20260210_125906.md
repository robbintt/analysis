---
ver: rpa2
title: Federated Split Learning with Improved Communication and Storage Efficiency
arxiv_id: '2507.15816'
source_url: https://arxiv.org/abs/2507.15816
tags:
- learning
- communication
- clients
- server
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes CSE-FSL, a federated split learning framework
  that addresses communication and storage inefficiencies in existing FSL methods.
  The core idea is to use an auxiliary network to enable local client updates and
  maintain a single shared server-side model, eliminating the need for multiple model
  copies.
---

# Federated Split Learning with Improved Communication and Storage Efficiency

## Quick Facts
- **arXiv ID**: 2507.15816
- **Source URL**: https://arxiv.org/abs/2507.15816
- **Reference count**: 40
- **Primary result**: Proposes CSE-FSL framework that reduces communication load by up to 70% and storage by up to 53% compared to baseline FSL approaches

## Executive Summary
This paper introduces CSE-FSL, a federated split learning framework that addresses communication and storage inefficiencies in existing methods. The core innovation is an auxiliary network that enables local client updates without requiring per-batch gradient feedback from the server, while maintaining a single shared server-side model to eliminate storage scaling with client count. The framework achieves faster convergence and higher accuracy than existing FSL methods through reduced communication frequency and asynchronous server updates. Theoretical convergence analysis under non-convex loss functions is provided, and extensive experiments on CIFAR-10 and F-EMNIST datasets validate the approach's effectiveness.

## Method Summary
CSE-FSL implements federated split learning with an auxiliary network attached to the client-side model output. Clients train locally using this auxiliary network for $h$ batches before uploading smashed data to the server, reducing communication frequency. The server maintains only one shared model updated sequentially as data arrives from any client, eliminating the need for multiple model copies. Local losses are computed on the auxiliary network outputs, enabling client-side updates without waiting for server gradients. The framework includes asynchronous server-side model updates to prevent straggler effects, with global aggregation occurring once per epoch.

## Key Results
- Achieves 70% reduction in communication load compared to baseline FSL approaches
- Reduces storage requirements by up to 53% through single shared server model
- Demonstrates faster convergence and higher accuracy than existing methods on CIFAR-10 and F-EMNIST
- Asynchronous server updates prevent straggler effects and improve overall efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The auxiliary network enables client-side updates without per-batch gradient feedback from the server.
- Mechanism: An auxiliary network ($a_c$) is attached to the client-side model's output, transforming intermediate smashed data into task-appropriate outputs. Clients compute local loss on these outputs using true labels and backpropagate to update both the client-side model and auxiliary network, bypassing the need to wait for server-side gradients for every batch.
- Core assumption: The auxiliary network can be trained to provide a sufficiently useful gradient signal for the client-side model, approximating what the server-side model would provide.
- Evidence anchors: [abstract] "...utilizes an auxiliary network to locally update the weights of the clients while keeping a single model at the server..."; [section] Section IV.A: "...the authors of [9] propose to add an auxiliary network... to the end of the client-side model and calculate the local loss..."
- Break condition: If the auxiliary network is too small or ill-suited to represent the server-side model's gradient function, client-side model updates may diverge, leading to poor convergence.

### Mechanism 2
- Claim: Maintaining a single shared server-side model reduces storage overhead and enables asynchronous updates.
- Mechanism: Unlike traditional FSL which maintains $n$ separate server-side models, CSE-FSL maintains only one. The server updates this model sequentially as smashed data arrives from any client, eliminating the $O(n)$ storage cost and the need for synchronized updates.
- Core assumption: The server-side model updates from different clients are sufficiently compatible to be applied sequentially to a single model without causing catastrophic interference.
- Evidence anchors: [abstract] "...maintains a single shared server-side model, eliminating the need for multiple model copies... The asynchronous server update mechanism further enhances efficiency..."; [section] Figure 3 and text: "the server initiates its own training process upon the sequential arrivals of the smashed data uploaded from multiple clients..."
- Break condition: Under extreme data heterogeneity, sequential updates from non-IID clients could cause the single server model to oscillate or fail to generalize.

### Mechanism 3
- Claim: Reducing the frequency of smashed data uploads ($h$) improves communication efficiency with controlled convergence impact.
- Mechanism: Clients perform $h$ local training batches using the auxiliary network before uploading smashed data to the server. This reduces the upstream communication volume by a factor of $h$. The paper's theoretical analysis (Propositions 1 & 2) provides convergence bounds under this scheme.
- Core assumption: Client-side models do not drift too far from the global optimum during the $h$ local batches, a condition addressed by the convergence analysis under non-convex loss.
- Evidence anchors: [abstract] "...allowing clients to train on multiple local batches before uploading smashed data..."; [section] Table II (Communication analysis) and Figure 9 (Accuracy vs Communication Load). Section V.A Proposition 1 shows convergence bound related to $h$.
- Break condition: If $h$ is set too large or client data is very non-IID, the local model may drift excessively, causing the server-side model to fail to converge.

## Foundational Learning

- Concept: **Backpropagation and Gradients**
  - Why needed here: The core of the method is manipulating where and when gradients are computed. Understanding how gradients flow from a loss function backward through layers is essential to grasp how an auxiliary network can provide a surrogate learning signal.
  - Quick check question: If you have a loss calculated at the output of a neural network, how does changing the weight of a neuron in an earlier layer affect that loss?

- Concept: **Federated Averaging (FedAvg)**
  - Why needed here: CSE-FSL uses a global aggregation step (averaging client-side models) similar to FedAvg. Understanding the benefits and challenges of local training followed by aggregation is key.
  - Quick check question: In FedAvg, what happens if one client trains for 100 epochs locally while another trains for only 1 before aggregation?

- Concept: **Data and Model Parallelism (Split Learning Context)**
  - Why needed here: The paper splits a model between a client and a server. Understanding that the client holds some layers and the server others, and that intermediate data ("smashed data") is exchanged, is the foundational paradigm being improved.
  - Quick check question: In a split learning setup for image classification, which device holds the final classification layer, and what data is sent from the other device?

## Architecture Onboarding

- Component map:
  - Server -> Client (Initial distribution): Global $x_c$ and $a_c$ parameters
  - Client (Local training): Client-side model ($x_c$) + Auxiliary network ($a_c$)
  - Client -> Server (Periodic): Smashed data and labels every $h$ batches
  - Client -> Server (Aggregation): Updated $x_c$ and $a_c$ parameters
  - Server (Single model): Single copy of server-side model ($x_s$)

- Critical path:
  1. Server initializes and distributes $x_c$ and $a_c$ to clients
  2. Each client, for $h$ batches: Forward pass through $x_c$ to get smashed data, pass through $a_c$ to get predictions, compute local loss, and update local $x_c$ and $a_c$
  3. Client sends the final smashed data and labels to the server
  4. Server updates the single global $x_s$ sequentially using the received data
  5. Clients send trained $x_c$ and $a_c$ to the server for aggregation (averaging)
  6. Server sends the new aggregated global $x_c$ and $a_c$ back to clients
  7. Repeat from step 2

- Design tradeoffs:
  - **Auxiliary Network Size vs. Accuracy**: A larger auxiliary network may better approximate server-side gradients but increases client-side computation and communication. The paper found a smaller CNN often performed well.
  - **Batch Frequency ($h$) vs. Convergence**: Larger $h$ reduces communication but risks client model drift. Empirical results suggest this can be beneficial, but it depends on data heterogeneity.
  - **Single vs. Multiple Server Models**: A single server model saves immense storage but introduces potential instability from sequential, asynchronous updates.

- Failure signatures:
  - **Divergence**: If the auxiliary network is poorly designed or $h$ is too large, training loss may oscillate. The client-side model may drift so far that its smashed data becomes meaningless to the server.
  - **Straggler Effect**: While mitigated by the design, extremely slow clients will send stale updates, potentially harming convergence.
  - **Non-IID Instability**: With highly non-IID data, a single shared server model updated sequentially might struggle to find a common representation, leading to lower final accuracy.

- First 3 experiments:
  1. **Baseline Reproduction**: Implement CSE-FSL on MNIST. Compare test accuracy and communication cost against standard FSL (like SplitFed) and FedAvg to verify communication gains.
  2. **Hyperparameter Sweep on $h$**: Run experiments with varying $h$ (e.g., $h=1, 5, 10, 25$). Plot test accuracy vs. communication rounds and total bytes transmitted to validate the trade-off.
  3. **Auxiliary Network Ablation**: Train with different auxiliary network architectures (e.g., MLP vs. the proposed 1x1 CNN) of varying sizes. Compare their parameter counts and final test accuracy to confirm the trade-off and find an efficient design.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can model split points be dynamically optimized to accommodate diverse network conditions and client hardware capabilities?
  - Basis in paper: [explicit] The conclusion states that challenges remain in "optimizing model split points to accommodate diverse network conditions and hardware capabilities."
  - Why unresolved: The current experimental setup utilizes a static model architecture and split point, which may not be optimal for highly heterogeneous environments.
  - What evidence would resolve it: An adaptive algorithm that adjusts the cut layer in real-time based on device resource constraints and communication latency.

- **Open Question 2**: Can rigorous theoretical convergence guarantees be established for CSE-FSL under non-IID data distributions?
  - Basis in paper: [inferred] The theoretical analysis explicitly assumes IID local datasets to "simplify the analysis," despite experimental validation on non-IID data.
  - Why unresolved: The current theoretical bounds (Proposition 2) rely on assumptions about distribution drift that are only strictly defined for IID settings in the proof.
  - What evidence would resolve it: A convergence proof that extends the non-convex loss guarantees to heterogeneous, non-IID data partitions without relying on the current distribution drift constraints.

- **Open Question 3**: How effective is CSE-FSL in complex cross-domain applications compared to standard image classification tasks?
  - Basis in paper: [explicit] The authors identify "exploring the potential of CSE-FSL in more complex real-world scenarios, such as cross-domain applications and intelligent edge computing" as a future direction.
  - Why unresolved: The paper validates the method solely on CIFAR-10 and F-EMNIST, which are standard benchmarks but may not represent the complexity of cross-domain feature spaces.
  - What evidence would resolve it: Performance benchmarks on datasets requiring domain adaptation or multi-modal learning, analyzing the auxiliary network's ability to bridge feature gaps.

## Limitations
- Theoretical analysis assumes IID local datasets for simplicity, despite experimental validation on non-IID data
- Limited evaluation to image classification tasks (CIFAR-10 and F-EMNIST) without testing on more complex cross-domain applications
- Static model architecture and split point without dynamic optimization for heterogeneous environments

## Confidence
- **High**: Core mechanism of auxiliary network for local updates and single shared server model are well-documented and theoretically grounded
- **Medium**: Experimental results are promising but limited to specific datasets; generalization to other domains requires validation
- **Medium**: Theoretical convergence analysis provides bounds but relies on assumptions that may not hold in practice

## Next Checks
1. Verify implementation of the auxiliary network mechanism by reproducing baseline results on MNIST before scaling to CIFAR-10
2. Conduct ablation studies on auxiliary network architecture to confirm the trade-off between capacity and accuracy
3. Test the $h$ hyperparameter sweep across different data heterogeneity levels to validate convergence bounds and identify optimal settings for various scenarios