---
ver: rpa2
title: 'IMKD: Intensity-Aware Multi-Level Knowledge Distillation for Camera-Radar
  Fusion'
arxiv_id: '2512.15581'
source_url: https://arxiv.org/abs/2512.15581
tags:
- radar
- distillation
- fusion
- imkd
- lidar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: IMKD addresses the challenge of high-performance radar-camera 3D
  object detection by leveraging knowledge distillation without LiDAR at inference
  time. Existing distillation methods transfer modality-specific features directly
  to each sensor, which can distort their unique characteristics and degrade their
  individual strengths.
---

# IMKD: Intensity-Aware Multi-Level Knowledge Distillation for Camera-Radar Fusion

## Quick Facts
- arXiv ID: 2512.15581
- Source URL: https://arxiv.org/abs/2512.15581
- Reference count: 40
- Primary result: 67.0% NDS and 61.0% mAP on nuScenes

## Executive Summary
IMKD introduces an intensity-aware multi-level knowledge distillation approach for radar-camera 3D object detection that eliminates the need for LiDAR at inference time. The method addresses a critical limitation in existing distillation techniques that transfer modality-specific features directly, potentially distorting unique sensor characteristics. By implementing a three-stage distillation strategy with intensity guidance, IMKD enhances the fused representation while preserving the strengths of individual sensors.

## Method Summary
The paper proposes a three-stage distillation framework that progressively transfers knowledge from LiDAR to radar, then to the fused representation, and finally implements an intensity-guided fusion mechanism. The approach leverages intensity information as a bridge between modalities, enabling more effective knowledge transfer while maintaining the distinct characteristics of radar and camera inputs. The framework operates without requiring LiDAR at inference time, making it more practical for real-world deployment.

## Key Results
- Achieves 67.0% NDS on nuScenes benchmark
- Achieves 61.0% mAP on nuScenes benchmark
- Outperforms all prior distillation-based radar-camera fusion methods

## Why This Works (Mechanism)
The intensity-aware approach works by using LiDAR intensity as a common reference point to guide knowledge transfer between radar and camera modalities. This prevents the direct transfer of modality-specific features that can distort unique sensor characteristics. The multi-level distillation ensures that both individual sensor strengths and the fused representation are optimized simultaneously, leading to improved detection performance.

## Foundational Learning
- Knowledge Distillation: Transferring learned representations from teacher to student models; needed to enable LiDAR-like performance without LiDAR sensors at inference
- Multi-Modal Fusion: Combining information from different sensor types; critical for leveraging complementary strengths of radar and camera
- Intensity Features: Using signal strength information as a common reference; provides a modality-agnostic bridge for knowledge transfer
- 3D Object Detection: Identifying and localizing objects in three-dimensional space; the primary task being improved
- nuScenes Benchmark: Standard evaluation platform for autonomous driving perception; provides standardized metrics and datasets

## Architecture Onboarding

Component Map: LiDAR -> Radar Distillation -> Fused Distillation -> Intensity-Guided Fusion

Critical Path: The three-stage distillation process forms the critical path, with each stage building upon the previous one. LiDAR-to-Radar distillation establishes the foundation, LiDAR-to-Fused distillation refines the combined representation, and the intensity-guided fusion mechanism finalizes the detection output.

Design Tradeoffs: The multi-stage approach increases training complexity but enables more effective knowledge transfer. The use of intensity information as a bridge reduces modality-specific distortion but requires careful calibration across sensors.

Failure Signatures: Potential failures could arise from intensity calibration mismatches between sensors, inadequate representation capacity in the student model, or insufficient training data for certain object classes.

First Experiments:
1. Ablation study removing each distillation stage individually to quantify their contributions
2. Cross-dataset validation on different autonomous driving benchmarks
3. Runtime analysis comparing inference speed with and without LiDAR

## Open Questions the Paper Calls Out
None

## Limitations
- Limited comparison to non-distillation state-of-the-art methods
- No detailed analysis of computational overhead introduced by the multi-stage distillation
- Lack of ablation studies for individual distillation components

## Confidence

High Confidence:
- Technical description of intensity-aware distillation mechanism is clear and well-structured
- Specific architectural details provided for each stage

Medium Confidence:
- Experimental methodology appears sound
- Limited scope of baseline comparisons and lack of ablation studies reduce confidence in attributed performance gains

Low Confidence:
- Claims about preserving modality-specific characteristics lack empirical validation beyond final performance metrics

## Next Checks
1. Conduct ablation studies isolating the contribution of each distillation stage to verify that the claimed improvements are not simply due to architectural changes in the base model
2. Compare against non-distillation state-of-the-art radar-camera fusion methods to establish whether the improvements are specific to the distillation approach
3. Perform extensive cross-dataset validation to assess the generalization capability of the approach beyond the nuScenes benchmark