---
ver: rpa2
title: Research Challenges in Relational Database Management Systems for LLM Queries
arxiv_id: '2508.20912'
source_url: https://arxiv.org/abs/2508.20912
tags:
- query
- queries
- data
- https
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores the challenges of integrating large language\
  \ models (LLMs) with relational database management systems (RDBMS) for SQL-based\
  \ queries. It evaluates three systems\u2014PostgreSQL with pgAI, DuckDB with FlockMTL,\
  \ and the enterprise platform MotherDuck\u2014using five representative LLM queries,\
  \ including LLM projection, filtering, multi-LLM invocation, aggregation, and retrieval-augmented\
  \ generation (RAG)."
---

# Research Challenges in Relational Database Management Systems for LLM Queries

## Quick Facts
- arXiv ID: 2508.20912
- Source URL: https://arxiv.org/abs/2508.20912
- Reference count: 40
- Primary result: Enterprise RDBMS with parallel LLM API calls significantly outperform open-source systems on LLM-integrated SQL queries due to optimized parallel processing and API integration.

## Executive Summary
This paper investigates the integration of large language models (LLMs) with relational database management systems (RDBMS) for executing SQL queries that require natural language processing. The authors evaluate three systems—PostgreSQL with pgAI, DuckDB with FlockMTL, and the enterprise platform MotherDuck—using five representative LLM query patterns. Key challenges identified include enforcing structured outputs, optimizing resource utilization, and improving query planning. The study implements initial solutions like constrained decoding and batching optimizations, which improve performance but leave significant gaps between open-source and enterprise solutions.

## Method Summary
The study tests PostgreSQL 16 + pgAI 0.8.0, DuckDB v1.1.4 + FlockMTL v0.2.0, and MotherDuck on Rotten Tomatoes (17,712 movies) and SQuAD datasets. LLaMA 3.1 8B is served via Ollama/vLLM, with Stella embeddings for retrieval-augmented generation. Five query templates test LLM projection, filtering, multi-LLM invocation, aggregation, and RAG. Performance metrics include query latency (minutes), GPU utilization (%), and success/failure rates across systems. The authors implement constrained decoding and batching optimizations to address structured output and resource utilization challenges.

## Key Results
- Constrained decoding prevents SQL execution failures by enforcing schema-compliant outputs, eliminating the need for ad-hoc parsing.
- Batching optimizations stabilize GPU utilization at 75% median and reduce per-row processing time from 4s to 2.5s in pgAI.
- Enterprise systems like MotherDuck outperform open-source systems by parallelizing up to 256 LLM API requests, reducing query latency from 370+ minutes to 16.2 minutes.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Constrained decoding appears necessary to enforce schema compliance when integrating LLMs with strongly-typed SQL engines.
- **Mechanism:** By modifying the LLM’s token-generation process (e.g., using XGrammar or context-free grammars), the system restricts outputs to valid structures (e.g., specific JSON schemas or numeric ranges). This prevents runtime errors where a SQL aggregator like `AVG()` receives text instead of a number.
- **Core assumption:** The model’s inherent reasoning capability remains intact even when the output vocabulary is masked or restricted by the decoding layer.
- **Evidence anchors:**
  - [abstract] Mentions the need for "enforcing structured outputs" as a main issue.
  - [section 4.1] Notes that constrained decoding "removes the need for fine-tuning... [and] ad-hoc parsing," allowing queries to execute where they previously failed.
  - [corpus] Related work in *Text2Schema* supports the general difficulty of mapping natural language to strict database structures without schema enforcement.
- **Break condition:** If the grammar constraints are too complex or conflict with the model's training distribution, generation may stall or produce low-probability (hallucinated) content within the valid structure.

### Mechanism 2
- **Claim:** Asynchronous request batching likely stabilizes GPU utilization by overlapping CPU-bound validation with GPU-bound token generation.
- **Mechanism:** Instead of blocking API calls where the GPU waits for a single row to process and return, the system sends multiple concurrent asynchronous requests. This ensures that while the CPU validates one token/row, the GPU is actively decoding the next, maintaining high compute occupancy.
- **Core assumption:** The overhead of managing concurrent request states does not exceed the performance gains from reduced GPU idle time.
- **Evidence anchors:**
  - [abstract] Identifies "optimizing resource utilization" as a core challenge addressed by batching optimizations.
  - [section 3.3] Reports that pgAI batching improved per-row processing time (4s to 2.5s) and stabilized GPU utilization at a 75% median.
  - [corpus] Corpus signals regarding *Transactional Cloud Applications* emphasize concurrency challenges, paralleling the need for handling concurrent LLM requests in DBs.
- **Break condition:** Performance degrades if the batch size exceeds the GPU memory (VRAM) capacity or if the inference engine cannot effectively schedule the concurrent prefill/decode steps.

### Mechanism 3
- **Claim:** Parallel processing of API requests significantly reduces latency compared to single-threaded local inference.
- **Mechanism:** Enterprise systems (like MotherDuck) leverage external APIs (e.g., OpenAI) to parallelize LLM calls, sending hundreds of requests simultaneously. This bypasses the sequential bottleneck of a single local GPU processing rows one-by-one or in small batches.
- **Core assumption:** Network latency and API rate limits do not negate the throughput gains from massive parallelism.
- **Evidence anchors:**
  - [abstract] States that enterprise solutions outperform open-source systems due to "optimized parallel processing."
  - [section 3.3] Notes MotherDuck "improves speed by concurrently sending up to 256 requests," resulting in much lower query latencies (e.g., 16.2 mins vs 370+ mins).
  - [corpus] The *KathDB* paper suggests moving beyond traditional SQL execution models, aligning with the need for parallel AI-workflow integration.
- **Break condition:** If the database exhausts its connection limits or the external API imposes strict rate limits, the parallel speedup collapses.

## Foundational Learning

- **Concept: Inference Engines (Prefill vs. Decode)**
  - **Why needed here:** The paper distinguishes between loading the prompt (prefill) and generating text (decode). Understanding this is required to diagnose why GPU utilization spikes and dips (Fig 6-9) and why batching strategies affect latency.
  - **Quick check question:** Does increasing the batch size primarily affect the prefill phase (prompt processing) or the decode phase (token generation)?

- **Concept: SQL UDFs (User-Defined Functions)**
  - **Why needed here:** Systems like pgAI use UDFs to call LLMs. Understanding that query planners treat UDFs as "black boxes" is essential to understanding why optimization (like filter pushdown) fails in LLM queries.
  - **Quick check question:** Why would a query planner choose a full table scan over an index when filtering on the result of a UDF?

- **Concept: Structured Decoding (Grammar-constrained generation)**
  - **Why needed here:** Standard LLMs output free text, which breaks SQL queries expecting integers or enums. You must understand how to force an LLM to output only valid SQL types.
  - **Quick check question:** If an LLM generates a valid sentiment score but wraps it in markdown (e.g., "**4**"), will the SQL `AVG()` function succeed or fail?

## Architecture Onboarding

- **Component map:**
  - SQL Frontend: (PostgreSQL/DuckDB) Parses queries.
  - LLM Extension: (pgAI/FlockMTL) Bridges SQL engine to Inference Engine.
  - Inference Engine: (Ollama/vLLM/Remote API) Manages GPU resources and model execution.
  - Vector Store: (pgvector/External) Used for RAG (Query 5).

- **Critical path:**
  1. SQL Query Receipt -> 2. Planner identifies LLM UDF (Blackbox) -> 3. Extension batches rows -> 4. Async Request to Inference Engine -> 5. Constrained Decoding -> 6. Result Parsing -> 7. SQL Aggregation/Filtering.

- **Design tradeoffs:**
  - **Local vs. Cloud Inference:** Local preserves privacy (critical for sensitive data) but suffers from poor GPU scheduling and single-GPU bottlenecks. Cloud offers massive parallelism (256 concurrent requests) but exposes data externally.
  - **Fine-tuning vs. Constrained Decoding:** Fine-tuning has high upfront cost but low runtime overhead. Constrained Decoding has zero training cost but adds computational overhead during token generation.

- **Failure signatures:**
  - **"Subtransaction" Errors:** Occurs when trying to parallelize UDFs in PostgreSQL that involve complex transaction states (Section 3.3).
  - **Untyped Blob Aggregation:** LLM returns text "The score is 4" instead of integer "4"; SQL aggregator crashes.
  - **Memory Exhaustion (OOM):** RAG queries failing to trigger HNSW index pushdown, resulting in a cross-join of embedding tables (Section 3.2).

- **First 3 experiments:**
  1. **Latency Baseline:** Run Query 1 (Projection) locally with Ollama vs. vLLM to measure the overhead of the serving engine independent of the database.
  2. **Batching Impact:** Re-run a failed/slow query (like Q4) in pgAI, incrementally increasing the request batch size to find the GPU saturation point (targeting ~75% utilization as per the paper).
  3. **Structured Output Stress Test:** Attempt Q4 (Aggregation) using standard prompting vs. constrained decoding to quantify the rate of SQL execution failures caused by formatting issues.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can query optimizers incorporate LLM-specific costs—such as prompt overlap, KV-cache reuse, and GPU/CPU resource contention—into traditional cost models to generate efficient execution plans?
- **Basis in paper:** [explicit] The authors state that current optimizers treat LLM functions as black boxes and that "incorporating the true cost of LLM calls can close a gap in current relational optimizers."
- **Why unresolved:** Existing UDF-based implementations prevent the planner from reordering attributes or utilizing prefix caching, leading to naive execution strategies.
- **What evidence would resolve it:** A query planner implementation that successfully reorders input tuples to maximize KV-cache reuse, demonstrating reduced latency compared to standard planning.

### Open Question 2
- **Question:** How can systems implement robust multi-row batching within a single context window while guaranteeing strict one-to-one mapping between input rows and generated outputs?
- **Basis in paper:** [inferred] Section 3.3 notes that while FlockMTL attempts to batch rows, it fails because "the LLM is not robust enough to follow these instructions for hundreds of rows" or track output-to-input mapping reliably.
- **Why unresolved:** Current constrained decoding methods often require single-row requests to ensure correctness, sacrificing the throughput benefits of batching.
- **What evidence would resolve it:** An algorithm that batches multiple input rows into a single prompt and parses the structured output into distinct database tuples without data loss or ordering errors.

### Open Question 3
- **Question:** To what extent can approximate query processing (AQP) techniques be adapted for LLM queries to balance computational cost against semantic accuracy?
- **Basis in paper:** [explicit] The Conclusion explicitly suggests that "LLM queries might benefit from adopting different approaches, such as approximate query processing techniques."
- **Why unresolved:** Processing large datasets (e.g., 1 million rows) is currently infeasible for local inference (projected to take 12 days), creating a need for methods that reduce the inference load.
- **What evidence would resolve it:** A study benchmarking the trade-off between inference speedup (via sampling) and the degradation in result quality for analytical LLM queries.

## Limitations
- Experimental scope limited to five query patterns on two datasets with a single LLM model, potentially limiting generalizability.
- No statistical significance testing reported for performance differences between systems.
- Constrained decoding implementation details are sparse, raising questions about robustness across diverse schemas.
- Enterprise solution comparisons rely on external APIs, introducing uncontrolled variables like network latency and rate limits.

## Confidence

**High Confidence:** The core observation that LLM-integrated queries fail without structured output enforcement is well-supported by empirical evidence (e.g., Q2/Q3 returning explanations instead of "Yes"/"No"). The claim that batching improves GPU utilization from volatile spikes to stable 75% median is directly measurable and reproducible.

**Medium Confidence:** The assertion that enterprise systems outperform open-source due to "optimized parallel processing" is plausible but confounded by external API dependencies. Similarly, while constrained decoding reduces parsing failures, the claim that it eliminates the need for fine-tuning requires more rigorous ablation studies across diverse schemas.

**Low Confidence:** The paper's framing of "LLM projection" as a novel challenge conflates the difficulty of structured output generation with the broader problem of schema compliance. Without controlled comparisons (e.g., fine-tuning vs. constrained decoding across multiple schemas), the superiority of the proposed approach remains unproven.

## Next Checks

1. **Ablation Study on Structured Decoding:** Run Q2 and Q3 with identical prompts using (a) standard decoding, (b) constrained decoding with regex, and (c) fine-tuned models to quantify failure rates and latency overhead across each approach.

2. **Generalization Test with Alternative Schemas:** Execute the five query patterns on a different dataset (e.g., IMDb or Stack Overflow) with a modified schema to assess whether constrained decoding grammars and batching strategies transfer without retraining.

3. **Statistical Significance Analysis:** Re-run latency experiments for each query type 10+ times on all systems, reporting mean, variance, and p-values to determine if observed performance differences (e.g., MotherDuck vs. pgAI) are statistically significant or within measurement error.