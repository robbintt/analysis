---
ver: rpa2
title: 'BlackboxNLP-2025 MIB Shared Task: Exploring Ensemble Strategies for Circuit
  Localization Methods'
arxiv_id: '2510.06811'
source_url: https://arxiv.org/abs/2510.06811
tags:
- edge
- methods
- scores
- ensemble
- circuit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether ensembling circuit localization
  methods can improve performance on the MIB benchmark. The authors explore parallel
  and sequential ensembling strategies, combining edge attribution scores from different
  methods like EAP-IG-inputs and edge pruning.
---

# BlackboxNLP-2025 MIB Shared Task: Exploring Ensemble Strategies for Circuit Localization Methods

## Quick Facts
- arXiv ID: 2510.06811
- Source URL: https://arxiv.org/abs/2510.06811
- Reference count: 4
- Primary result: Hybrid ensemble combining edge pruning with multiple EAP-based methods achieves best performance (CMD 0.03, CPR 1.83) on MIB benchmark private test set

## Executive Summary
This paper investigates whether ensembling circuit localization methods can improve performance on the MIB benchmark for mechanistic interpretability. The authors explore parallel and sequential ensembling strategies, combining edge attribution scores from different methods like EAP-IG-inputs and edge pruning. Their hybrid ensemble approach, which combines warm-start edge pruning with multiple EAP-based methods, achieves the best results with a CMD score of 0.03 (lower is better) and a CPR score of 1.83 (higher is better) on the private test set. The sequential ensemble with warm-start edge pruning already shows improvements over the baseline, while the parallel ensemble provides consistent gains. The work demonstrates that combining complementary circuit localization methods can robustly enhance performance in mechanistic interpretability benchmarks.

## Method Summary
The paper explores three ensemble strategies: (1) Sequential ensemble uses EAP-IG-inputs attribution scores as a warm start for edge pruning optimization through rank normalization and logistic mapping to hard-concrete log alpha parameters; (2) Parallel ensemble averages scores from three EAP variants (EAP, EAP-IG-inputs, EAP-IG-activations) using mean aggregation; (3) Hybrid ensemble combines both approaches by averaging all four method scores. The sequential approach leverages fast gradient-based methods to initialize slower but more precise edge pruning, while parallel ensemble exploits complementary strengths of different attribution methods. Sign recovery for edge pruning outputs is handled via either z-score attribution or reusing EAP-IG signs.

## Key Results
- Hybrid ensemble achieves CMD score of 0.03 and CPR of 1.83 on private test set
- Sequential ensemble improves CPR by +0.16 average compared to EAP-IG-inputs baseline
- Mean aggregation outperforms max/min strategies in parallel ensemble
- Ensemble methods show consistent improvements across GPT-2-IOI, Qwen-IOI, and Qwen-MCQA tasks

## Why This Works (Mechanism)

### Mechanism 1
Averaging edge attribution scores from multiple circuit localization methods improves performance by reducing method-specific noise and biases. Different methods (EAP, EAP-IG-inputs, EAP-IG-activations) produce complementary estimates of edge importance, and mean aggregation yields more robust importance rankings than union or intersection strategies.

### Mechanism 2
Warm-starting edge pruning with EAP-IG-inputs attribution scores accelerates convergence and improves final circuit identification. Fast gradient-based attribution methods provide initial edge importance estimates that are converted to hard-concrete log alpha parameters, enabling edge pruning to refine these initialized masks through gradient optimization.

### Mechanism 3
Sign recovery from learned edge pruning masks enables integration with signed attribution methods in hybrid ensembles. Edge pruning produces unsigned mask values that are converted to signed scores through either gradient-based z-score attribution or reusing signs from initial EAP-IG-inputs attribution, allowing combination with other EAP-based methods.

## Foundational Learning

- **Edge Activation Patching and EAP-IG variants**: Base methods being ensembled that approximate causal interventions on edges through integrated gradients. Why needed: Essential for interpreting ensemble outputs. Quick check: Why does EAP-IG-inputs use integrated gradients over input embeddings rather than direct activation differences?

- **Hard-concrete distribution and learnable edge masks**: Edge pruning represents circuit membership via hard-concrete masks with learnable log alpha parameters. Why needed: Sequential ensemble requires converting attribution scores to this parameterization. Quick check: How does the stretched-sigmoid transformation enable differentiable sparsity?

- **Circuit faithfulness metrics (CMD and CPR)**: The ensemble is optimized for these specific metrics measuring behavioral similarity and circuit performance across sizes. Why needed: Understanding the optimization objective. Quick check: Why integrate scores over multiple circuit sizes rather than evaluating at a single threshold?

## Architecture Onboarding

- **Component map**: Input: Model + Task + Dataset → Base Method 1 (EAP) → Parallel Ensemble (mean) → Hybrid Ensemble → Output. Sequential Path: EAP-IG-inputs → Log Alpha Init → Edge Pruning (1000 steps) → Sign Recovery → Hybrid Ensemble.

- **Critical path**: Pre-compute EAP variant scores → Convert EAP-IG-inputs to log alpha initialization → Run edge pruning optimization → Recover signs → Average all four method scores.

- **Design tradeoffs**: Speed vs. precision (EAP variants fast but approximate; edge pruning expensive but precise); Equal vs. weighted averaging (current work uses naïve equal weighting); Sign recovery strategy (z-score attribution principled but requires extra forward pass; sign reuse fast but inherits EAP-IG errors).

- **Failure signatures**: s-ens underperforms baseline (check log alpha initialization); p-ens shows inconsistent gains (methods may have task-specific biases); IFR integration fails (requires special per-node normalization); Hyperparameter mismatch (edge pruning sensitive to learning rates).

- **First 3 experiments**: (1) Reproduce baseline comparison on validation set; (2) Ablate warm-start initialization comparing random vs EAP-IG-inputs initialization; (3) Test sign recovery strategies comparing z-score attribution vs EAP-IG sign reuse.

## Open Questions the Paper Calls Out

### Open Question 1
Can adaptive weighting schemes outperform the naïve equal-weight averaging used in the hybrid ensemble? The authors state their ensemble relies on naïve equal-weight averaging which may not fully exploit complementary strengths. What evidence would resolve it: Comparison of performance between current hybrid-ens and versions utilizing learned or dynamic weighting.

### Open Question 2
Do these ensemble strategies generalize to model architectures and tasks outside the specific GPT-2 and Qwen-2.5 combinations used? The limitations section notes it is unclear how well the ensemble strategies generalize to other interpretability benchmarks or model architectures. What evidence would resolve it: Applying hybrid-ens to diverse architectures (e.g., Llama) or different behavioral tasks to verify consistent improvements.

### Open Question 3
Can a principled formulation for integrating edge pruning with attribution-based approaches outperform the current heuristic strategy? The authors note that the integration is still heuristic, leaving room for more principled formulations. What evidence would resolve it: Deriving a theoretically grounded integration method and demonstrating superior convergence or accuracy over the heuristic sequential ensemble.

## Limitations
- Ensemble approach relies on method-specific hyperparameters optimized separately for each model-task combination, suggesting potential overfitting
- Warm-start initialization assumes EAP-IG-inputs scores provide useful gradient directions, which may not hold for architectures or tasks outside evaluated scope
- Unweighted averaging approach may miss opportunities for adaptive weighting based on method reliability per task

## Confidence
- **High confidence**: Sequential ensemble with warm-start edge pruning consistently improves over baseline EAP-IG-inputs across all evaluated tasks with documented CPR gains (+0.16 average) and CMD improvements
- **Medium confidence**: Superiority of mean aggregation over max/min strategies in parallel ensembles is supported by ablation results though margin varies by task
- **Medium confidence**: Claim that different methods have complementary biases is plausible given improvements from ensembling but lacks detailed analysis of which components each method captures

## Next Checks
1. Ablation study on sign recovery strategies: Systematically compare z-score attribution versus EAP-IG sign reuse in sequential ensemble across all tasks to isolate contribution of each sign recovery method

2. Cross-task hyperparameter transfer: Test whether optimized hyperparameters from one model-task combination (e.g., GPT-2-IOI) generalize to others (e.g., Qwen-MCQA) without re-tuning, measuring performance degradation as overfitting indicator

3. Method complementarity analysis: For each task, identify which edges are uniquely selected by each base method versus shared across methods, quantifying overlap to determine whether gains come from truly complementary information or more aggressive edge selection