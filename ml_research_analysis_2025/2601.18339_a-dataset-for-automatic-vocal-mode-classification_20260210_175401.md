---
ver: rpa2
title: A Dataset for Automatic Vocal Mode Classification
arxiv_id: '2601.18339'
source_url: https://arxiv.org/abs/2601.18339
tags:
- vocal
- mode
- modes
- dataset
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work presents a novel dataset for automatic vocal mode classification
  using the Complete Vocal Technique (CVT) framework, which categorizes singing into
  four vocal modes: Neutral, Curbing, Overdrive, and Edge. The dataset consists of
  3,752 unique samples recorded from four singers (three professional with extensive
  CVT experience) across their entire vocal ranges, captured using four different
  microphones for natural data augmentation, totaling over 13,000 samples.'
---

# A Dataset for Automatic Vocal Mode Classification

## Quick Facts
- **arXiv ID:** 2601.18339
- **Source URL:** https://arxiv.org/abs/2601.18339
- **Reference count:** 0
- **Primary result:** Novel dataset of 3,752 unique samples for automatic vocal mode classification using Complete Vocal Technique framework, with baseline ResNet18 achieving 81.3% balanced accuracy.

## Executive Summary
This work introduces a novel dataset for automatic vocal mode classification based on the Complete Vocal Technique (CVT) framework, which categorizes singing into four distinct modes: Neutral, Curbing, Overdrive, and Edge. The dataset features recordings from four singers (three professional) across their entire vocal ranges, captured simultaneously using four different microphones to provide natural data augmentation. The authors provide both individual and merged annotations from three CVT-experienced annotators, along with a "strong consensus" subset for higher-quality labels. Baseline classification results using various machine learning models demonstrate the dataset's utility, with ResNet18 achieving 81.3% balanced accuracy using annotated modes and ResNet34 achieving 95.3% using nominal modes as ground truth.

## Method Summary
The dataset consists of 3,752 unique sustained vowel samples recorded from four singers across four microphones, totaling over 13,000 samples. Three CVT-experienced annotators provided individual labels, which were merged using a majority vote with nominal labels as tie-breakers. The data was preprocessed with loudness normalization followed by energy normalization. Classification experiments used 5-fold cross-validation with splits based on unique production IDs to prevent data leakage. Models tested included ResNet18 and ResNet34, trained with SGD optimizer and a learning rate schedule that reduces by 0.8 every 3 epochs if training loss doesn't improve. The best model achieved 81.3% balanced accuracy on the annotated subset and 95.3% on the nominal subset.

## Key Results
- ResNet18 achieved best balanced accuracy of 81.3% on annotated modes using 5-fold cross-validation
- Using nominal modes as ground truth yielded significantly higher accuracy of 95.3% with pretrained ResNet34
- Strong consensus subset (high inter-annotator agreement) achieved 90.9% balanced accuracy, outperforming full annotation (80.8%)
- Performance degraded significantly for lowest vocal notes (F1-B2), indicating challenges in low-register mode classification

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Using different recording devices provides natural domain shift that improves model robustness to recording conditions.
- **Mechanism:** Variation in frequency response and noise profiles between devices forces classifier to learn features invariant to microphone coloration.
- **Core assumption:** Vocal mode features remain learnable despite changes in audio fidelity.
- **Evidence anchors:** Section 2.2 states using four microphones offers natural data augmentation; Section 4 shows considerable performance drop when trained solely on high-quality microphone.

### Mechanism 2
- **Claim:** Using "Nominal" modes (singer's intent) as ground truth instead of "Annotated" modes (listener perception) increases classification accuracy.
- **Mechanism:** Singers carry mode characteristics more consistently than listeners can perceive them, particularly in lower vocal ranges.
- **Core assumption:** Singer's intent is more stable ground truth than human listener consensus.
- **Evidence anchors:** Abstract shows 95.3% accuracy for nominal vs 81.3% for annotated; Section 4 notes singers can "carry" modes from high range down.

### Mechanism 3
- **Claim:** Restricting training data to samples with high inter-annotator agreement reduces conflicting gradients and improves accuracy.
- **Mechanism:** Filtering out ambiguous samples avoids training on contradictory feature-label pairs, effectively learning canonical representation of each mode.
- **Core assumption:** Ambiguity in annotation results from mode centers vs peripheries rather than subjective noise.
- **Evidence anchors:** Section 3 shows 90.9% accuracy for strong consensus vs 80.8% for full annotation; Section 2.2 notes distinction becomes unclear as one deviates from mode center.

## Foundational Learning

- **Concept: Complete Vocal Technique (CVT) Modes**
  - **Why needed here:** This is the target taxonomy; understanding mode characteristics is essential for interpreting accuracy gaps.
  - **Quick check question:** Which mode would likely be confused with "belting" due to its high metallic and twang content? (Answer: Overdrive)

- **Concept: Label Noise vs. Feature Noise**
  - **Why needed here:** Distinguishes between audio features and human annotation as sources of error.
  - **Quick check question:** If a model achieves 95% on Nominal labels but 81% on Annotated labels, is the bottleneck the audio features or the ground truth consensus? (Answer: Ground truth consensus)

- **Concept: Production-Level Data Splitting**
  - **Why needed here:** Critical for preventing data leakage when same sound is recorded across multiple devices.
  - **Quick check question:** Why would splitting the 13,000 samples randomly result in artificially high accuracy metrics? (Answer: Same sound would appear in both train and test sets)

## Architecture Onboarding

- **Component map:** Waveform -> Loudness Normalization -> Energy Normalization -> Log-Mel Spectrogram -> ResNet18/34 -> Fully Connected Layer -> 4 Classes
- **Critical path:** Annotation Merging Strategy (Majority vote -> Nominal tie-breaker -> Discard) dictates size and purity of training set
- **Design tradeoffs:**
  - Singer Diversity vs Control: 4 singers ensures quality but risks overfitting to specific anatomies
  - Nominal vs Annotated GT: Higher accuracy with nominal but risks learning "intended" sounds that weren't produced
- **Failure signatures:**
  - Low Pitch Collapse: Performance degrades for lowest notes (F1-B2)
  - Mode Bleeding: Overdrive and Edge often confused (12% error rate)
  - Annotation Drift: Fleiss' Kappa of 0.45 indicates 50% label variance is noise
- **First 3 experiments:**
  1. Baseline Reproduction: Train ResNet18 on "Merged Annotation" using 5-fold split by production ID to confirm ~81% benchmark
  2. Ablation on Microphones: Train on only high-quality mic data and test on only smartphone data to quantify augmentation effectiveness
  3. Consensus Filtering: Retrain using only "Strong Consensus" subset to verify accuracy improves to ~90% range

## Open Questions the Paper Calls Out

- To what extent does classification accuracy transfer to realistic musical phrases and varied singing techniques? The current dataset is limited to sustained vowels and major arpeggios, lacking temporal dynamics and timbral variations found in actual songs.
- Are the acoustic features required to distinguish vocal modes fundamentally different or merely under-represented in the lowest vocal registers? Performance decline for low notes suggests annotators may be confused by low-register modes, but it's unclear if this is due to lack of training data or acoustic absence of defining characteristics.
- Can the high accuracy achieved on this controlled dataset be maintained when applied to "wild" recording conditions of prior work? It would be interesting to assess model performances on data recorded for Sol et al., which used self-recordings with likely higher variability in microphone quality and room acoustics.

## Limitations
- Small singer pool (4 total, 3 professional) may limit generalizability to diverse vocal anatomies and singing styles
- Exclusive use of sustained vowels doesn't represent full complexity of expressive singing including lyrics, dynamics, and mode transitions
- Assumption that singer intent is more accurate than listener consensus may not hold for ambiguous cases or failed productions

## Confidence

- **High Confidence:** Dataset construction methodology and baseline results are well-documented and reproducible
- **Medium Confidence:** Comparative results between nominal and annotated ground truths suggest labeling issues but require further validation
- **Low Confidence:** Natural data augmentation effectiveness through multi-microphone recording lacks systematic ablation studies

## Next Checks

1. Conduct blind listening test where expert annotators classify samples using both "Nominal" and "Annotated" labels to determine if 14% accuracy gap reflects actual acoustic differences or systematic bias
2. Systematically train and test models on different microphone subsets (high-quality only vs mixed quality) to quantify actual benefit of natural data augmentation and identify frequency response thresholds
3. Train best-performing model on current dataset and evaluate on separate corpus of singing data with different singers, languages, or recording conditions to assess real-world applicability