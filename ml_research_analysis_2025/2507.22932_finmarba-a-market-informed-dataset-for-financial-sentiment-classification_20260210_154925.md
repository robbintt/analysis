---
ver: rpa2
title: 'FinMarBa: A Market-Informed Dataset for Financial Sentiment Classification'
arxiv_id: '2507.22932'
source_url: https://arxiv.org/abs/2507.22932
tags:
- financial
- dataset
- sentiment
- market
- news
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of biased human annotation in
  financial sentiment classification datasets. The authors propose FinMarBa, a market-informed
  dataset that uses automated labeling based on actual market reactions rather than
  subjective human interpretation.
---

# FinMarBa: A Market-Informed Dataset for Financial Sentiment Classification

## Quick Facts
- arXiv ID: 2507.22932
- Source URL: https://arxiv.org/abs/2507.22932
- Authors: Baptiste Lefort; Eric Benhamou; Beatrice Guez; Jean-Jacques Ohana; Ethan Setrouk; Alban Etienne
- Reference count: 6
- Key outcome: Market-informed dataset achieves 26% annualized return and Sharpe ratio of 1.2 using automated labeling based on actual market reactions

## Executive Summary
This paper addresses the challenge of biased human annotation in financial sentiment classification datasets. The authors propose FinMarBa, a market-informed dataset that uses automated labeling based on actual market reactions rather than subjective human interpretation. Their method employs GPT-4 to identify relevant stock tickers and classifies headlines based on subsequent market performance using a quantile approach. FinMarBa achieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming benchmarks and demonstrating superior predictive power compared to human-annotated datasets like Financial-Phrasebank. The dataset is released as open-source to support further research in financial sentiment analysis.

## Method Summary
The authors developed FinMarBa by using GPT-4 to identify relevant stock tickers for financial headlines from Bloomberg Market Wraps (2010-2024). Headlines are classified based on the subsequent market performance of identified tickers using quantile thresholds (Q0.3 for negative, Q0.6 for positive) computed from 5-year rolling historical distributions. The dataset contains 32,410 labeled headlines across 5,000+ unique tickers. A BERT model (FinMarBaBERT) is fine-tuned on this data and achieves superior predictive power compared to human-annotated datasets when tested on S&P500 performance from 2019-2024.

## Key Results
- FinMarBa achieves 26% annualized return and Sharpe ratio of 1.2 in backtesting
- Daily sentiment scores generate Sharpe 0.30 vs -0.13 for human-annotated Financial-Phrasebank (T-stat 10, p<0.01)
- The dataset contains 32,410 labeled headlines across 5,000+ unique tickers from Bloomberg Market Wraps (2010-2024)

## Why This Works (Mechanism)

### Mechanism 1: Market-Based Annotation Eliminates Human Interpretive Bias
Replacing human annotators with objective price-based classification produces sentiment labels that better reflect actual market impact. GPT-4 identifies relevant tickers for each headline. The algorithm then compares next-day price returns against a 5-year rolling historical distribution using quantile thresholds (Q0.3 for negative, Q0.6 for positive). This grounds sentiment in observable market behavior rather than subjective interpretation.

### Mechanism 2: Quantile-Adaptive Thresholds Normalize Across Volatility Regimes
Using ticker-specific quantile thresholds rather than absolute return thresholds captures whether a move is significant relative to each asset's historical volatility. Rolling 5-year windows compute Q0.3 and Q0.6 per ticker. High-volatility assets require larger absolute moves to be classified as positive/negative, preventing systematic over-labeling of volatile instruments.

### Mechanism 3: Aggregated Daily Scores Produce Tradeable Signals
Aggregating headline-level sentiments into daily scores generates signals with predictive power for index-level returns. Daily sentiment score S = (Σp - Σn) / (Σp + Σn), where p and n are positive/negative headline counts. This normalized score is used for S&P500 backtesting, achieving Sharpe 0.30 vs -0.13 for human-annotated baseline.

## Foundational Learning

- Concept: Quantile-based classification
  - Why needed here: Understanding how percentile thresholds convert continuous returns into discrete sentiment labels is essential for interpreting and potentially tuning the annotation system.
  - Quick check question: Given daily returns with mean 0.05% and std 2%, approximately what return exceeds Q0.6?

- Concept: Look-ahead bias prevention
  - Why needed here: The paper emphasizes strict temporal ordering—quantiles computed only from data available before headline publication. Violating this invalidates backtests.
  - Quick check question: Why would using full-period statistics (2010-2024) to compute quantiles invalidate the 2019-2024 backtest?

- Concept: Sharpe ratio interpretation
  - Why needed here: Primary evaluation metric. The paper reports Sharpe 0.30 for FinMarBa vs -0.13 for baseline.
  - Quick check question: A strategy with 15% annual return and 20% volatility has what approximate Sharpe ratio (assuming 0% risk-free rate)?

## Architecture Onboarding

- Component map: Bloomberg Market Wraps -> GPT-4 headline extraction -> GPT-4 ticker identification -> Historical price database -> Quantile engine -> Label generator -> BERT fine-tuning -> Daily sentiment aggregation -> S&P500 backtest

- Critical path: GPT-4 ticker extraction -> correct price series linkage -> accurate quantile thresholds -> valid labels -> model quality -> signal reliability. Errors compound downstream; ticker extraction is the highest-leverage failure point.

- Design tradeoffs:
  - 5-year vs shorter windows: Stability vs adaptability to regime changes
  - Q0.3/Q0.6 vs tighter/wider bands: Label granularity vs noise tolerance
  - Next-day vs multi-day horizons: Immediate reaction capture vs delayed impact
  - Equal ticker weighting vs weighted: Paper assumes macro effect; may dilute ticker-specific signals

- Failure signatures:
  - Neutral label rate >40%: Thresholds too conservative or ticker mismatch
  - Negative backtest Sharpe: Labels not predictive; check for data leakage
  - Ticker distribution heavily US-centric: Geographic bias limits generalizability
  - GPT-4 hallucinating non-existent tickers: Entity recognition failure

- First 3 experiments:
  1. Ticker extraction validation: Manually verify 100 random headline-ticker assignments. Target >90% precision before trusting downstream labels.
  2. Threshold sensitivity: Test alternative quantile splits (Q0.25/Q0.75, Q0.2/Q0.8) on held-out 2023-2024 data. Compare Sharpe ratios to Q0.3/Q0.6 baseline.
  3. Return horizon analysis: Compare t+1, t+3, and t+5 return windows. If optimal horizon differs from next-day, refine impact timing assumption.

## Open Questions the Paper Calls Out

### Open Question 1
Can the market-driven annotation framework be effectively generalized to low-liquidity asset classes or markets where price signals are noisier? The conclusion explicitly states future work should "explore the application of FinMarBa to other financial markets and asset classes." This remains unresolved because the current validation relies on liquid equity markets, but the quantile approach assumes a robust distribution of returns that may not exist in illiquid markets.

### Open Question 2
To what extent does the simplifying assumption that a single headline has "equal impact" on all identified tickers introduce label noise? The paper states the headline is considered to have "equal impact on all the related tickers," noting this is a simplification to capture macro effects. This is unresolved because the authors did not quantify the signal degradation caused by assigning the same label to multiple tickers that may react differently.

### Open Question 3
Do more advanced Large Language Models (LLMs) leverage the FinMarBa dataset more effectively than the BERT-based architecture tested in the study? The conclusion lists "investigating the integration of this dataset with more advanced machine learning techniques" as a direction for future work. This remains unresolved because the paper benchmarks using a standard BERT model, leaving the potential performance gains of larger, decoder-based models untested.

## Limitations
- Geographic concentration: Dataset heavily skewed toward US equities (85%), limiting generalizability
- Equal impact assumption: Single headline assigned identical sentiment to all identified tickers regardless of individual asset reactions
- Short-term horizon: Next-day return measurement may not capture delayed headline impact or confounding market events

## Confidence
- Low: Core labeling mechanism assumes next-day returns fully capture headline impact, oversimplifying financial markets where price movements result from multiple simultaneous factors
- Medium: Claim that market-based annotation eliminates human bias assumes price movements are purely rational responses to news rather than influenced by broader market sentiment or algorithmic trading
- Medium: Impressive 26% annualized return figure lacks comparison to standard market benchmarks over the same period

## Next Checks
1. **Confounding Event Analysis**: For a random sample of 200 labeled headlines, identify and catalog other major market-moving events occurring on the same days. Quantify how often these events could explain the observed price movements independently of the headline content.

2. **Cross-Asset Validation**: Apply the labeling methodology to non-US markets (European, Asian) and different asset classes (commodities, bonds). Compare label distributions and model performance to assess whether the US-centric results generalize.

3. **Temporal Stability Test**: Segment the 2010-2024 period into distinct market regimes (pre-COVID, COVID crisis, post-COVID recovery, 2022 inflation, 2023-2024). Test whether the labeling mechanism and model performance remain consistent across these periods, or if performance degrades during regime shifts.