---
ver: rpa2
title: 'InfluenceNet: AI Models for Banzhaf and Shapley Value Prediction'
arxiv_id: '2503.08381'
source_url: https://arxiv.org/abs/2503.08381
tags:
- agents
- rules
- value
- rule
- power
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents InfluenceNet, a neural network-based approach\
  \ for efficiently estimating Banzhaf and Shapley-Shubik power indices in large voting\
  \ games (n \u2265 10 agents). The method addresses the computational intractability\
  \ of traditional power index calculations, which scale exponentially with coalition\
  \ size."
---

# InfluenceNet: AI Models for Banzhaf and Shapley Value Prediction

## Quick Facts
- arXiv ID: 2503.08381
- Source URL: https://arxiv.org/abs/2503.08381
- Reference count: 40
- Primary result: Neural network achieves 8-minute training vs 15-50 minute Monte Carlo runtime while maintaining comparable accuracy for power index prediction

## Executive Summary
This paper introduces InfluenceNet, a neural network-based approach for efficiently estimating Banzhaf and Shapley-Shubik power indices in large voting games where traditional computation becomes intractable. The method uses marginal contribution networks with three coalition generation strategies and feedforward neural networks to predict power indices directly from coalition rule representations. The approach demonstrates significant computational efficiency gains while maintaining accuracy comparable to Monte Carlo methods, with particular success in sparse coalition structures where patterns are more distinctive and learnable.

## Method Summary
InfluenceNet predicts power indices by training feedforward neural networks to map coalition rule representations (requirements/bans matrices) to power index values. The approach uses Monte Carlo approximation to generate training labels, employs three random coalition generation strategies (uniform sampling, coin-flip assignment, Gaussian mixture distributions), and uses three-layer networks with 512, 256, and 128 hidden units. Models are trained on 200,000 samples with 80/20 train/test splits, using MSE loss and zero-padding to handle variable input sizes.

## Key Results
- Computational efficiency: 8 minutes training time vs 15-50 minutes for Monte Carlo methods
- Accuracy: Comparable performance to traditional methods across tested configurations
- Learning patterns: Sparse coalition rules (low p-threshold) provide more distinctive patterns for learning than dense coalitions
- Graph correlations: Strong relationships between network structure metrics and power indices in sparse coalitions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neural networks can approximate power indices by learning patterns in marginal contribution structures rather than computing exhaustive coalition enumerations.
- Mechanism: The feedforward architecture learns to map coalition rule representations directly to power index values through supervised training on Monte Carlo-generated labels.
- Core assumption: The mapping from coalition rule structures to power indices contains learnable patterns that persist across different game configurations.
- Evidence anchors:
  - [abstract] "demonstrating comparable and often superiour performance to existing tools in terms of both speed and accuracy"
  - [section 4.3] "The architecture is composed of three hidden layers, each with ReLU activation function and 20% dropout... hidden layers where of dimensions [512, 256, 128]"
  - [corpus] Limited direct corpus evidence; related work (arXiv:2510.13391) suggests GNNs can approximate Banzhaf values in network flow games, supporting plausibility of neural approaches.
- Break condition: Performance degrades significantly when rule distributions differ from training distribution.

### Mechanism 2
- Claim: Sparse coalition structures provide more distinctive patterns for learning than dense coalitions due to higher variance in agent criticality.
- Mechanism: Sparse rules create fewer constraints per rule, allowing individual agents to have more variable marginal contributions. This variance creates stronger signal for the network to distinguish high-influence from low-influence agents.
- Core assumption: Agent criticality differences are preserved through the Monte Carlo approximation used for training labels.
- Evidence anchors:
  - [section 5.1] "models trained on sparse coalition rules generally outperformed those trained on denser coalitions when evaluated on sparse datasets"
  - [section 6] "sparse-versus-dense coalition rules revealed interesting patterns... these configurations contain more distinct and learnable patterns"
- Break condition: Dense coalition rules produce uniformly low power indices with minimal variance, limiting discriminative learning signal.

### Mechanism 3
- Claim: Graph-theoretical metrics correlate with power indices because both capture structural importance in coalition networks.
- Mechanism: Converting coalition rules to graphs reveals that graph centrality measures capture similar structural information to power indices—agents well-positioned in the requirement network tend to have higher influence.
- Core assumption: The undirected graph representation preserves relevant coalition structure.
- Evidence anchors:
  - [section 4.4] "We represented each voting system as an undirected graph... agents served as nodes, while edges represented meaningful relationships derived from coalition patterns"
  - [section 5.3] "strongest correlations... were achieved in sparser coalitions (i.e. p-threshold ≤ 0.3)"
  - [corpus] arXiv:2510.13391 establishes connection between graph structure and Banzhaf values in network flow games.
- Break condition: Correlation strength diminishes as coalition density increases and individual agent influence becomes constrained.

## Foundational Learning

- Concept: **Banzhaf and Shapley-Shubik Power Indices**
  - Why needed here: These are the target outputs the network must learn to predict. Understanding their definitions clarifies what patterns the network is approximating.
  - Quick check question: Given a 3-agent voting game where {a,b} is winning and {a,c} is winning, which agent(s) are "critical" in coalition {a,b}?

- Concept: **Marginal Contribution Networks (MCNs)**
  - Why needed here: The paper's coalition representation (rules with required/banned agents) is the input format the network processes.
  - Quick check question: How would you represent the rule "agents 1 and 2 must both be present, and agent 3 must be absent" in the req/ban matrix format?

- Concept: **Monte Carlo Approximation for Intractable Computations**
  - Why needed here: Training labels are generated via Monte Carlo, not exact computation. Understanding sampling-based approximation is critical for assessing label quality.
  - Quick check question: If you sample 10,000 random coalitions to approximate Banzhaf indices for 50 agents, what tradeoff are you making compared to exact enumeration of 2^50 coalitions?

## Architecture Onboarding

- Component map: Input Tensor [k, n, 2m+1] → Flatten → Dense(512, ReLU, Dropout 0.2) → Dense(256, ReLU, Dropout 0.2) → Dense(128, ReLU, Dropout 0.2) → Dense(m, linear) → Power Index Vector [m]

- Critical path:
  1. **Data generation** (Algorithms 1-3): Generate coalition rules using uniform/coinflip/Gaussian mixture sampling
  2. **Label generation** (Algorithms 4-5): Monte Carlo approximation of Banzhaf/Shapley indices (10,000 samples)
  3. **Training**: MSE regression on 200k samples, 80/20 train/test split
  4. **Zero-padding**: Align variable-size inputs to max dimensions for model comparison

- Design tradeoffs:
  - **Three-layer vs deeper architecture**: Paper uses [512, 256, 128] but acknowledges "better results may be achieved using more sophisticated NN architecture"
  - **Zero-padding vs separate models**: Padding enables cross-model comparison but introduces noise; unpadded models perform differently (Section 5.1)
  - **Monte Carlo label quality vs computational cost**: 10,000 samples balances accuracy/speed but introduces label noise for m ≥ 20 agents

- Failure signatures:
  - **Distribution shift**: Models trained on uniform/coinflip data fail on Mixture-of-Gaussian distributions (Appendix B: MAE increases 10x)
  - **Agent count mismatch**: Models trained on n=50 perform poorly on n=10 datasets and vice versa (Figure 3)
  - **Dense rule underfitting**: Models trained on high p-threshold (dense) coalitions cannot generalize to sparse coalitions

- First 3 experiments:
  1. **Reproduce sparse vs dense learning gap**: Train separate models on p=0.1 and p=0.7 uniform random datasets with n=20, m=10. Evaluate cross-performance to confirm the paper's central claim about learnable pattern differences.
  2. **Test generalization boundaries**: Train on n=20 agents, test on n=15 and n=25. Map the performance degradation curve to understand viable interpolation range.
  3. **Validate graph correlation mechanism**: Compute Spearman correlations between power indices and graph metrics on held-out test data. Confirm that average degree of requirements correlates with mean Banzhaf (Table 1) and check whether this relationship holds for your data distribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the methodology be adapted to handle dynamic coalitions where agent relationships and rules evolve over time?
- Basis in paper: [explicit] The authors explicitly state that "extending the methodology to handle dynamic coalitions where relationships between agents evolve over time represents an important direction for future work."
- Why unresolved: The current InfluenceNet architecture relies on static snapshots of coalition rules and agent configurations.
- What evidence would resolve it: A temporal model architecture (e.g., Recurrent Neural Networks) that successfully predicts power indices in time-varying voting systems without retraining from scratch.

### Open Question 2
- Question: Can unsupervised or semi-supervised learning techniques eliminate the bottleneck of generating ground-truth labels via Monte Carlo approximations?
- Basis in paper: [explicit] The discussion notes that for large rule sets ($m \geq 20$) and agent counts ($n \geq 50$), "the initial approximation needed in order to generate the labels is a severe bottleneck."
- Why unresolved: The current approach depends entirely on supervised learning using computationally expensive Monte Carlo estimates as training labels.
- What evidence would resolve it: Demonstration of a model achieving comparable accuracy (MAE) while using only a fraction of the currently required labeled training data.

### Open Question 3
- Question: What architectural or data augmentation strategies can improve model generalization between sparse and dense coalition environments?
- Basis in paper: [inferred] The results show that models trained on dense coalition rules exhibit "inferior performance" on sparse datasets, and the authors note this "limited robustness" requires further research.
- Why unresolved: The paper indicates that patterns learned in high-constraint (dense) environments do not currently transfer effectively to low-constraint (sparse) environments.
- What evidence would resolve it: A training regime or model that maintains consistent Mean Absolute Error (MAE) when cross-evaluated on datasets with widely varying sparsity thresholds ($p$).

## Limitations

- Theoretical foundation: The neural network's ability to generalize across different voting game structures remains empirically demonstrated but theoretically unproven
- Label quality: Monte Carlo approximation introduces noise that may propagate through the learning process, particularly for larger agent sets
- Representational choices: Zero-padding for variable coalition sizes may introduce artifacts that affect model performance
- Scope of applicability: Claims about applicability to "many voting games and cooperative games" extend beyond the empirical scope of the study

## Confidence

- **High confidence**: The computational efficiency claims (8 minutes vs 15-50 minutes) are well-supported by the experimental results and directly measurable
- **Medium confidence**: The generalization claims across different voting game configurations are supported by cross-testing results but may be limited to the specific distributions tested
- **Medium confidence**: The graph-theoretical correlation claims are empirically demonstrated but rely on specific graph representations that may not capture all relevant coalition structures
- **Low confidence**: The paper's assertion that InfluenceNet could be "applicable to many voting games and cooperative games" extends beyond the empirical scope of the study

## Next Checks

1. **Robustness to label noise**: Vary the number of Monte Carlo samples (1,000, 10,000, 100,000) and measure how label approximation quality affects model accuracy. This will quantify the sensitivity to training label noise.

2. **Distribution generalization boundary**: Systematically vary the gap between training and testing distributions (e.g., train on uniform p=0.3, test on p=0.1, 0.5, 0.7) to map the limits of the model's generalization capability.

3. **Graph representation sensitivity**: Test alternative graph constructions (e.g., weighted edges based on rule values, different centrality measures) to determine whether the observed correlations are robust to representational choices.