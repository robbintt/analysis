---
ver: rpa2
title: A Lightweight Sparse Interaction Network for Time Series Forecasting
arxiv_id: '2602.01585'
source_url: https://arxiv.org/abs/2602.01585
tags:
- time
- temporal
- sparse
- interaction
- patch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of long-term time series forecasting
  by proposing a Lightweight Sparse Interaction Network (LSINet) that combines the
  efficiency of linear models with explicit temporal interaction mechanisms. Unlike
  existing transformer models that use self-attention (quadratic complexity) or linear
  models that rely on implicit temporal interactions, LSINet introduces a Multihead
  Sparse Interaction Mechanism (MSIM) that learns sparse connections between time
  steps through a sparsity-induced Bernoulli distribution.
---

# A Lightweight Sparse Interaction Network for Time Series Forecasting

## Quick Facts
- arXiv ID: 2602.01585
- Source URL: https://arxiv.org/abs/2602.01585
- Authors: Xu Zhang; Qitong Wang; Peng Wang; Wei Wang
- Reference count: 8
- Outperforms advanced linear and transformer models on long-term time series forecasting with significantly lower computational cost

## Executive Summary
This paper proposes LSINet, a lightweight model for long-term time series forecasting that combines the efficiency of linear models with explicit temporal interaction mechanisms. Unlike transformer models that use quadratic self-attention or linear models with implicit interactions, LSINet introduces a Multihead Sparse Interaction Mechanism (MSIM) that learns sparse connections between time steps through a sparsity-induced Bernoulli distribution. The model employs Shared Interaction Learning (SIL) to capture repetitive temporal patterns efficiently, achieving superior accuracy and efficiency compared to both linear and transformer baselines across six real-world datasets.

## Method Summary
LSINet processes time series by first partitioning them into patches, then learning a static sparse connection matrix through MSIM using Gumbel-Softmax reparameterization for differentiable sampling. The Shared Interaction Learning (SIL) mechanism shares this connection matrix across all samples and variables to capture repetitive temporal patterns. A self-adaptive sparse regularization loss enforces sparsity by intermittently penalizing connections outside the top-K most confident links. The architecture consists of patch encoding, sparse temporal interaction modules with time-invariant components, and a prediction head, all designed to maintain linear complexity while preserving temporal interaction capacity.

## Key Results
- Achieves average MSE improvements of 2.06-8.62% over linear models (CI-TSMixer, FiLM, DLinear, TimeMixer)
- Outperforms transformer models (PatchTST, Scaleformer, Pathformer) by 2.22-6.84% in MSE
- Maintains significantly lower training time, inference time, and memory usage than transformer baselines
- Ablation studies confirm the effectiveness of MSIM and self-adaptive sparse regularization loss

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing quadratic self-attention with learned static sparse connections reduces complexity while preserving temporal interaction capacity
- **Mechanism:** MSIM learns a binary connection matrix using parameterized Bernoulli distribution with Gumbel-Softmax for differentiable discrete sampling
- **Core assumption:** Complex temporal dependencies don't require dense attention maps but sparse critical connections
- **Evidence anchors:** Abstract mentions sparsity-induced Bernoulli distribution; equations 5-6 detail Gumbel-Softmax reparameterization; Lite-STGNN supports learnable sparse graph structures
- **Break condition:** Strictly non-sparse temporal dependencies will be dropped if sparsity is too aggressive

### Mechanism 2
- **Claim:** Sharing interaction patterns across variables and samples improves efficiency by exploiting repetitive temporal structures
- **Mechanism:** SIL uses global shared memory embedding to generate connection matrix for all variables and samples
- **Core assumption:** Temporal interaction patterns are repetitive and translation-invariant across samples
- **Evidence anchors:** Introduction states learning shared matrices based on repeated patterns; Figure 2 shows repeated patterns in attention heatmaps
- **Break condition:** Non-stationary behavior with changing interaction rules will cause underfitting

### Mechanism 3
- **Claim:** Enforcing sparsity via self-adaptive regularization isolates predictive temporal connections and reduces noise
- **Mechanism:** Self-Adaptive Sparse Regularization Loss (ASRL) intermittently penalizes connections outside top-K most confident links
- **Core assumption:** Many learned interactions are redundant or noisy and should be pruned
- **Evidence anchors:** Equations 7-8 define cross-entropy loss against top-K labels; Table 4 shows ablation improvement of 2.25% on Weather 336
- **Break condition:** Too aggressive sparsity rate will cause insufficient sparsity exploration

## Foundational Learning

- **Concept: Gumbel-Softmax Reparameterization**
  - **Why needed here:** Enables backpropagation through discrete sampling of binary connections from Bernoulli distribution
  - **Quick check question:** Can you explain why `arg max` prevents backpropagation and how adding Gumbel noise and softmax temperature τ solves this?

- **Concept: Time Series Patching**
  - **Why needed here:** Reduces sequence length from raw time points to patches, making interaction matrix computationally feasible
  - **Quick check question:** Given input length n=1024, patch length L=16, stride K=8, what is resulting number of patches N?

- **Concept: Linear vs. Transformer Complexity**
  - **Why needed here:** Positions LSINet as linear model alternative to quadratic complexity transformers
  - **Quick check question:** Why does self-attention scale quadratically while sparse interaction via fixed matrix multiplication does not?

## Architecture Onboarding

- **Component map:** Instance Norm → Patching → Linear Projection → Position Embedding → STI Module (Time-Invariant MLP → SSCL → Matrix Multiplication → Integration MLP) → Flatten → Linear Layer

- **Critical path:** Connection matrix C definition is most critical; Gumbel-Softmax temperature τ must be scheduled correctly (high to low); ASRL loss must be applied intermittently

- **Design tradeoffs:** Static C trades dynamic adaptability for efficiency; global sharing reduces parameters but assumes cross-variable correlation

- **Failure signatures:** Mode collapse to identity matrix if sparsity too tight; distribution shift makes static connections obsolete

- **First 3 experiments:**
  1. Sanity check with L=1, K=1 to verify patching compression
  2. Ablation with ASRL disabled to observe noisy dense connections
  3. Efficiency boundary test with n=2048 against PatchTST memory usage

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does SIL degrade performance on highly stochastic datasets where temporal interactions are non-repetitive?
- **Basis in paper:** Proposed based on repeated patterns observation but not validated on non-repetitive data
- **Why unresolved:** Experiments use periodic datasets; impact on chaotic or sample-specific systems unclear
- **What evidence would resolve it:** Comparison against non-shared baselines on datasets with high aperiodicity

### Open Question 2
- **Question:** Is fixed global sparsity rate (85%) universally optimal or does it limit capacity for complex dependencies?
- **Basis in paper:** Sparsity parameter δ fixed at 0.15 for all datasets
- **Why unresolved:** While self-adaptive loss enforces sparsity, target density is hard-coded constant
- **What evidence would resolve it:** Sensitivity analysis with learnable sparsity rate across domains

### Open Question 3
- **Question:** Can time-invariant component fully compensate for mismatch between static matrices and dynamic input windows?
- **Basis in paper:** Admits static matrices may fail on dynamic windows and uses time-invariant MLP mixer
- **Why unresolved:** Uncertain if simple linear mixing sufficiently encodes positional nuance for static connection logic
- **What evidence would resolve it:** Ablation studies on data with significant distribution shifts

## Limitations

- Critical hyperparameters unspecified: Gumbel-Softmax temperature τ, exact ASRL interval η values per dataset, optimizer configuration, and SSCL MLP sizes
- Shared interaction assumption may fail on non-stationary or highly variable-dependent datasets
- Fixed sparsity rate may restrict learning denser interactions when necessary

## Confidence

- **High Confidence:** Core architectural contribution and general experimental methodology are well-specified and theoretically sound
- **Medium Confidence:** Empirical superiority depends heavily on unspecified hyperparameters that could significantly affect performance
- **Low Confidence:** Claim about shared interaction patterns capturing repetitive structures is most assumption-heavy and could fail on untested dataset types

## Next Checks

1. **Parameter Sensitivity Analysis:** Systematically vary Gumbel-Softmax temperature τ (0.1, 1.0, 5.0) and sparsity rate δ (0.1, 0.15, 0.2) on validation subset

2. **ASRL Mechanism Verification:** Implement logging of C sparsity per epoch and ASRL application frequency to verify intermittent application

3. **Cross-Dataset Generalization Test:** Evaluate on dataset with known non-stationary behavior to test limits of shared interaction assumption