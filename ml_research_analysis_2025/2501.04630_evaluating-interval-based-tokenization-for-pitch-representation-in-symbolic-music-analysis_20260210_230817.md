---
ver: rpa2
title: Evaluating Interval-based Tokenization for Pitch Representation in Symbolic
  Music Analysis
arxiv_id: '2501.04630'
source_url: https://arxiv.org/abs/2501.04630
tags:
- pitch
- music
- tokenization
- absolute
- intervals
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of symbolic music tokenization
  strategies that rely on absolute MIDI values for pitch representation, which overlooks
  the relational aspects of music. The authors introduce a general framework for building
  interval-based tokenizations, where pitches are encoded as intervals relative to
  a chosen reference (e.g., melody, skyline, or bottom-line).
---

# Evaluating Interval-based Tokenization for Pitch Representation in Symbolic Music Analysis

## Quick Facts
- arXiv ID: 2501.04630
- Source URL: https://arxiv.org/abs/2501.04630
- Reference count: 12
- Interval-based tokenizations consistently outperform absolute pitch encoding across classification and tagging tasks

## Executive Summary
This paper addresses a fundamental limitation in symbolic music representation: absolute pitch encoding fails to capture the relational aspects of music that are crucial for musical understanding. The authors introduce a general framework for building interval-based tokenizations where pitches are encoded as intervals relative to a chosen reference sequence (melody, skyline, or bottom-line). Through systematic evaluation on three downstream tasks—era classification, start-of-phrase detection, and chord inversion identification—the study demonstrates that interval-based tokenizations consistently improve model performance compared to absolute pitch encoding. The choice of reference impacts task-specific results in musically meaningful ways, with melody reference enhancing phrase detection and bottom-line reference improving chord inversion identification.

## Method Summary
The authors propose a general framework for interval-based tokenization that encodes pitches as intervals relative to a reference sequence. They implement this using REMI as a base tokenization strategy, creating six intervalized variants by combining horizontal pitch intervals (HPI) for the reference sequence and vertical pitch intervals (VPI) for non-reference notes. The reference sequences include melody (explicitly annotated), skyline (highest note per onset), and bottom-line (lowest note per onset). Models are pre-trained on POP909, MTC-Piano, and String quartets (27.8M tokens) using masked language modeling, then fine-tuned on downstream tasks using a 14M-parameter Transformer encoder. Three datasets are used for evaluation: Lieder corpus (era classification), ESSEN-Piano (phrase detection), and Bach chorales from When-in-Rome dataset (chord inversion identification).

## Key Results
- Interval-based tokenizations consistently outperform absolute pitch encoding across all three downstream tasks
- Melody reference significantly improves start-of-phrase detection performance
- Bottom-line reference enhances chord inversion identification accuracy
- Choice of reference has task-specific impacts that align with musical intuition

## Why This Works (Mechanism)
The framework captures musical relationships by encoding pitch intervals relative to meaningful reference sequences rather than absolute values. This approach preserves the relational structure of music—melodies move by specific intervals, harmonies are defined by intervallic relationships, and musical phrases have characteristic interval patterns. By using references like melody or bassline, the model learns to represent music in terms that align with how musicians conceptualize and analyze music.

## Foundational Learning
- **Interval-based representation**: Pitches encoded as differences from reference notes rather than absolute values. Needed to capture musical relationships and improve generalization across keys. Quick check: Verify VPI computation as p - p_ref for non-reference notes.
- **Reference sequence selection**: Choosing meaningful musical elements (melody, bassline, skyline) as anchors for interval calculation. Needed to provide musically interpretable relationships. Quick check: Confirm reference extraction produces monophonic sequences per time step.
- **Horizontal vs vertical intervals**: HPI captures movement within reference sequence; VPI captures relationships between reference and non-reference notes. Needed to separately model melodic contour and harmonic relationships. Quick check: Validate HPI = p_j - p_{j-1} for reference sequence.
- **Masked language modeling**: Pre-training objective that masks tokens and requires model to predict them from context. Needed to learn general musical patterns before task-specific fine-tuning. Quick check: Confirm masking probability and reconstruction accuracy during pre-training.

## Architecture Onboarding

**Component map**: MIDI score -> Reference extraction (melody/skyline/bottom-line) -> Interval computation (VPI, HPI) -> Tokenization (REMI + intervals) -> Vocabulary construction -> Pre-training (MLM) -> Fine-tuning (classification/tagging)

**Critical path**: The most critical sequence is reference extraction → interval computation → vocabulary construction. Errors in reference selection propagate through interval calculations and can completely invalidate the tokenization strategy.

**Design tradeoffs**: The framework trades absolute pitch information for relational representations. While this improves musical understanding and generalization, it requires periodic absolute pitch tokens for generation tasks and may lose some performance on tasks requiring absolute pitch knowledge.

**Failure signatures**: Poor reference extraction (non-monophonic references, wrong voice selection) leads to invalid interval calculations. Vocabulary mismatch between pre-training and fine-tuning causes tokenization errors. Incorrect interval computation formulas produce meaningless token sequences.

**3 first experiments**:
1. Verify reference extraction produces correct monophonic sequences from polyphonic scores
2. Test interval computation with known pitch sequences to confirm VPI and HPI formulas
3. Validate vocabulary construction matches expected token distributions from interval ranges

## Open Questions the Paper Calls Out
1. **Interval decomposition**: Can encoding intervals as separate octave and class components improve model performance by explicitly disentangling octave relations from interval quality?
2. **Generation adaptation**: Can interval-based tokenization be effectively adapted for music generation by periodically injecting absolute pitch tokens?
3. **Tonal center reference**: Does using a computed tonal center as reference yield better performance for harmonic analysis than structural references?
4. **Time encoding interaction**: How does the choice of time encoding (score-based vs performance-based) interact with interval-based pitch tokenization?

## Limitations
- Results limited to homophonic music with single-track melodies; cannot generalize to polyphonic textures
- No empirical validation of improved model explainability claims
- Missing hyperparameter specifications (learning rates, batch sizes, masking probability)
- Unclear reference extraction methodology for non-annotated datasets

## Confidence

**High confidence**: Interval-based tokenizations consistently outperform absolute pitch encoding across all downstream tasks, with statistically significant improvements in accuracy and F1-scores.

**Medium confidence**: Task-specific reference advantages are supported but require careful interpretation; improvements vary in magnitude and causal relationships could benefit from additional controlled experiments.

**Low confidence**: Claims about improved model explainability lack empirical validation; no quantitative comparison of interpretability metrics or user studies demonstrating improved human understanding.

## Next Checks

1. **Reference extraction validation**: Implement and verify melody, skyline, and bottom-line reference extraction algorithms on sample scores, ensuring correct monophonic reference selection per time step. Compare resulting interval distributions against Figure 4.

2. **Vocabulary consistency check**: Train a small-scale model using seven tokenization strategies on POP909 subset, verifying vocabulary sizes and token distributions match expected interval computation rules. Confirm model can reconstruct inputs without information loss.

3. **Task-specific reference ablation**: Conduct controlled experiments on era classification and phrase detection using intervalized tokenizations with melody reference versus random note reference, measuring whether melodic advantage persists when controlling for reference position consistency.