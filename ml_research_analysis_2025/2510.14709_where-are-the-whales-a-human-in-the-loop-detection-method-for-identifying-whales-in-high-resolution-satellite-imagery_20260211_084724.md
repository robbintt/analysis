---
ver: rpa2
title: 'Where are the Whales: A Human-in-the-loop Detection Method for Identifying
  Whales in High-resolution Satellite Imagery'
arxiv_id: '2510.14709'
source_url: https://arxiv.org/abs/2510.14709
tags:
- whales
- imagery
- points
- whale
- anomaly
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a semi-automated system for detecting whales\
  \ in high-resolution satellite imagery by combining statistical anomaly detection\
  \ with a human-in-the-loop labeling interface. The method flags \"interesting points\"\
  \ in imagery based on local statistical deviations, reducing the expert review area\
  \ by up to 99.8% while maintaining high recall (90.3%\u201396.4%) across three benchmark\
  \ scenes with known whale annotations."
---

# Where are the Whales: A Human-in-the-loop Detection Method for Identifying Whales in High-resolution Satellite Imagery

## Quick Facts
- arXiv ID: 2510.14709
- Source URL: https://arxiv.org/abs/2510.14709
- Authors: Caleb Robinson; Kimberly T. Goetz; Christin B. Khan; Meredith Sackett; Kathleen Leonard; Rahul Dodhia; Juan M. Lavista Ferres
- Reference count: 14
- Primary result: Semi-automated whale detection system achieves 90.3%–96.4% recall while reducing expert review area by up to 99.8%

## Executive Summary
This paper introduces a semi-automated system for detecting whales in high-resolution satellite imagery by combining statistical anomaly detection with a human-in-the-loop labeling interface. The method flags "interesting points" in imagery based on local statistical deviations, reducing the expert review area by up to 99.8% while maintaining high recall (90.3%–96.4%) across three benchmark scenes with known whale annotations. It does not require labeled training data, making it scalable for large-scale marine mammal monitoring. A browser-based tool enables rapid expert annotation, and the entire pipeline is open-sourced to support future machine learning efforts. The approach is particularly effective in calm water conditions and provides a scalable first step toward automated whale detection from space.

## Method Summary
The method computes per-pixel deviation scores using either chunked standardization (partitioning into non-overlapping chunks) or rolling window standardization (sliding window convolution). It applies mean-shifted variance computation for numerical stability in low-variance regions. The system aggregates deviations across channels, thresholds at high quantiles (99.99th for calm water, 99.9th for choppy water), extracts 8-connected components, filters by area threshold (≥1.5 m²), and outputs centroids as "interesting points" for human review. A browser-based interface presents 100m x 100m chips centered on these points for expert annotation.

## Key Results
- Achieves 90.3%–96.4% recall across three benchmark scenes with known whale annotations
- Reduces expert review area by up to 99.8% (from 1,000 sq km to <2 sq km)
- Works without labeled training data, enabling scalable deployment
- Successfully identifies whale mothers and calves in complex scenes

## Why This Works (Mechanism)

### Mechanism 1
Local statistical standardization acts as an attention mechanism, highlighting spectrally distinct objects (whales) against a homogeneous background (calm water). The system computes per-pixel deviation scores (z-scores) using a rolling window. It subtracts the local mean (μᵢⱼ) and divides by local standard deviation (σᵢⱼ). In homogeneous water, variance is low, causing legitimate objects to appear as extreme statistical outliers (high z-scores), effectively separating signal from noise without trained models. Core assumption: The ocean surface is spatially homogeneous (low variance) relative to the whale targets. Evidence: [abstract] "...flagging 'interesting points' in imagery based on local statistical deviations..." [Section 3.2.2] Describes the rolling window standardization formula Dcij = Xcij - μcij / √(σ²cij + ε). Break condition: High environmental variance (e.g., white-capped waves) introduces high local standard deviation (σ), suppressing the z-scores of whales or creating massive false positive rates.

### Mechanism 2
Spatial aggregation and area thresholds convert dense statistical maps into sparse candidate sets, enabling massive search space reduction for human reviewers. The system aggregates channel-wise deviations into a scalar anomaly map, thresholds it (e.g., 99.99th percentile), and extracts 8-connected components. It filters these by physical size (e.g., > 1.5 sq meters). This pre-processing step reduces the search space by orders of magnitude (up to 99.8%) before a human interactively inspects the centroids. Core assumption: Target objects possess a minimum spatial continuity and size distinct from sensor noise. Evidence: [Section 3.3] "We filter out regions whose area falls below a minimum threshold... represent surviving features as points..." [Section 4] Describes using the 99.99th percentile and 1.5 sq meter area threshold to reduce 1,000 sq km to < 2 sq km. Break condition: If targets are smaller than the noise clusters or below the area threshold, they are filtered out completely (false negatives).

### Mechanism 3
Mean-shifting in variance calculations prevents numerical instability (catastrophic cancellation) when processing low-contrast imagery. In calm water, local variance (σ² = E[X²] - E[X]²) is extremely small. Calculating this via 32-bit convolutions results in catastrophic cancellation. The system stabilizes this by subtracting a global channel mean (X̄c) before computing local moments, preserving precision in low-variance regions. Core assumption: The imagery contains large homogeneous regions where floating-point precision errors are non-negligible. Evidence: [Section 3.2.2] Explicitly details the numerical stability issue and the "shifted method" to reduce absolute error (Figure 2). Break condition: If using double precision or if the water texture is naturally high-variance, this specific stabilization becomes less critical (though still good practice).

## Foundational Learning

- **Concept: Z-score / Standard Score**
  - Why needed here: The core detection logic is not "looking for whale shapes" but "looking for pixels that are mathematically improbable relative to their neighbors."
  - Quick check question: If a pixel has an intensity of 10 and its local neighborhood has a mean of 9 and a std dev of 0.5, is it more or less anomalous than a pixel of 50 in a neighborhood with mean 45 and std dev 10?

- **Concept: Connected Components Analysis**
  - Why needed here: The raw output is a pixel-level anomaly map. To present "points" to a human, the system must aggregate adjacent anomalous pixels into discrete objects.
  - Quick check question: How would changing the "connectivity" (4-way vs 8-way) affect the fragmentation of a whale-shaped anomaly?

- **Concept: Floating Point Catastrophic Cancellation**
  - Why needed here: Understanding why a mathematically correct formula (Var = E[X²] - E[X]²) fails in practice on a GPU for calm ocean images is key to reproducing the results.
  - Quick check question: Why does subtracting two large, similar numbers result in a loss of significant digits?

## Architecture Onboarding

- **Component map:** Input -> Preprocessing (Land masking + Global Mean Shift) -> Detector (Rolling Window Standardization) -> Filter (Thresholding + Connected Components + Area Filter) -> Frontend (Browser UI) -> Backend (HTTP Server logging CSV annotations)

- **Critical path:** The Rolling Window Standardization (Section 3.2.2) is the bottleneck. It must run efficiently on GPUs (using torch.nn.Conv2d) to process massive scenes.

- **Design tradeoffs:**
  - Chunked vs. Rolling: Chunked (Section 3.2.1) is faster/simpler but dilutes local anomalies. Rolling (Section 3.2.2) is computationally heavier but offers higher recall for small objects.
  - Threshold Sensitivity: A 99.99th percentile threshold works for calm water; choppier water (Valdés 2014) requires a relaxed threshold (99.9th), increasing false positives to maintain recall.

- **Failure signatures:**
  - High False Positive Rate: Caused by whitecaps, foam, or sun glint. The detector cannot distinguish "anomalous wave" from "anomalous whale."
  - Low Recall: Occurs if the anomaly threshold is too aggressive or if the whale is partially submerged/blends perfectly with the water spectral signature.

- **First 3 experiments:**
  1. Replicate Stability Check: Run the rolling window calculation on a synthetic flat image with/without the mean-shift (Listing 1) to verify the error reduction shown in Figure 2.
  2. Parameter Sweep: Vary kernel size (k) and anomaly percentile on the "Cape Cod Bay 2021" scene to observe the trade-off curve between "Points to Review" and "Recall."
  3. Noise Robustness: Intentionally inject synthetic whitecaps (high variance noise) into a calm scene to quantify the degradation of the "Interesting Points" precision.

## Open Questions the Paper Calls Out

### Open Question 1
Can the detection framework be adapted to maintain high recall in high-wind environments where white-capped waves dominate the statistical outliers? Basis: [explicit] The authors state that applying their methods to scenes with white-capped waves results in "a large number of false positives" and that "further work and labeled datasets are needed to find whales in these challenging conditions." Why unresolved: The current method relies on local statistical homogeneity, which breaks down in rough seas; the paper explicitly notes this limitation in the discussion of the Valdés 2014 scene. What evidence would resolve it: A modification of the standardization algorithm or a post-processing filter that achieves comparable recall (>90%) on scenes with significant wave activity.

### Open Question 2
To what extent can the "interesting points" generated by this pipeline effectively bootstrap the training of supervised deep learning models? Basis: [explicit] The paper posits that the method "enabling a path for... training of object detectors in data-sparse settings" and that mined data can "bootstrap larger modeling efforts." Why unresolved: The paper evaluates the anomaly detection and labeling interface itself but does not quantify the utility of the generated labels for training a downstream automated detector. What evidence would resolve it: Benchmarks showing the performance (e.g., mAP) of a CNN trained exclusively on labels collected via this interface compared to models trained on exhaustively annotated data.

### Open Question 3
Is it possible to automatically distinguish between classes of anomalies (e.g., whales vs. ships vs. debris) without human intervention? Basis: [explicit] The authors note that the simple statistical methods "are not able to distinguish between different classes of anomalous groupings of pixels." Why unresolved: Currently, the system only flags statistical deviations, requiring a human to manually classify the object type (whale, ship, debris) in the labeling interface. What evidence would resolve it: The integration of an automated classification step that successfully categorizes distinct anomaly types before human review.

## Limitations
- Performance degrades significantly in choppy water conditions with white-capped waves, producing high false positive rates
- Optimal threshold and kernel parameters appear scene-dependent, requiring manual tuning for new deployments
- Assumes whales have distinct spectral signatures from water, which may not hold for all species or lighting conditions

## Confidence
- **High Confidence:** The statistical framework (z-score computation, connected components analysis) is mathematically sound and well-validated through the numerical stability analysis.
- **Medium Confidence:** The reported recall rates (90.3%–96.4%) are reliable for the specific evaluation scenes and conditions tested, but generalizability requires further validation.
- **Low Confidence:** The method's scalability to global deployments without extensive parameter tuning is unproven, as optimal thresholds appear to vary significantly with environmental conditions.

## Next Checks
1. Test the method on satellite imagery with varying sea states (from glassy calm to Beaufort 5+) to quantify the relationship between environmental noise and detection performance.
2. Evaluate the method on multiple whale species with different coloration patterns to assess the robustness of the spectral anomaly detection across taxa.
3. Conduct a parameter sensitivity analysis across diverse geographic regions to develop guidelines for automatic threshold selection based on image characteristics.