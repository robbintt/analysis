---
ver: rpa2
title: Training-Free Dataset Pruning for Instance Segmentation
arxiv_id: '2503.00828'
source_url: https://arxiv.org/abs/2503.00828
tags:
- dataset
- instance
- pruning
- segmentation
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a training-free dataset pruning method (TFDP)
  for instance segmentation, addressing challenges of pixel-level annotations, scale
  variations, and class imbalance. The proposed approach leverages shape complexity
  scores (SCS) derived from mask annotations, enhanced with scale-invariant and class-balanced
  variants to handle object area variations and class distribution differences.
---

# Training-Free Dataset Pruning for Instance Segmentation

## Quick Facts
- arXiv ID: 2503.00828
- Source URL: https://arxiv.org/abs/2503.00828
- Reference count: 40
- Introduces TFDP, a training-free pruning method using shape complexity scores from mask annotations, achieving state-of-the-art pruning performance on COCO, Cityscapes, and VOC datasets.

## Executive Summary
This paper introduces a training-free dataset pruning method (TFDP) for instance segmentation that leverages shape complexity scores derived from mask annotations. The approach addresses key challenges in instance segmentation: pixel-level annotations, scale variations, and class imbalance. By computing perimeter-to-area ratios of instance masks and applying scale-invariant and class-balanced variants, TFDP selects informative subsets without requiring model training. The method achieves significant performance improvements over model-based baselines while reducing pruning time by over 1300×, demonstrating strong cross-architecture generalization.

## Method Summary
TFDP computes a Shape Complexity Score (SCS) for each instance using the perimeter-to-area ratio of its mask. This raw SCS is then transformed into scale-invariant (SI-SCS) and class-balanced (CB-SCS) variants to handle scale variations and class imbalance respectively. The method aggregates instance scores to image-level importance scores, which are then used to select the most informative subset of the dataset. The entire process operates solely on annotation information without requiring any model training or inference, making it extremely fast compared to model-based pruning approaches.

## Key Results
- Achieves 53.4% AP50 at 50% pruning on COCO (vs 51.7% for entropy-based methods)
- Reduces pruning time by 1349× compared to model-based baselines
- Demonstrates strong cross-architecture generalization, with adapted baselines showing poor transfer to QueryInst
- Outperforms adapted baselines by significant margins across all datasets and pruning rates

## Why This Works (Mechanism)

### Mechanism 1: Shape Complexity Score Captures Training Difficulty
The perimeter-to-area ratio of instance masks correlates with segmentation learning difficulty. Instance segmentation models refine boundary pixels later in training, and complex boundaries require more optimization steps. The SCS quantifies this complexity without requiring model inference, leveraging the geometric properties of mask contours.

### Mechanism 2: Scale-Invariant Normalization Removes Area Bias
Raw perimeter-to-area ratios scale as 1/√A, creating bias toward small objects. SI-SCS normalizes this by dividing by 2/√(πA), making the metric depend only on shape deviation from a circle rather than absolute scale. This ensures that object selection is based on shape complexity rather than size.

### Mechanism 3: Class-Balanced Aggregation Mitigates Dominance of Frequent Classes
Each instance score is normalized within its class before image-level aggregation. This prevents high-frequency classes from dominating selection and ensures equal contribution from all classes regardless of their instance count in the dataset.

## Foundational Learning

- **Instance segmentation mask representation**: Understanding contour extraction and area/perimeter computation is prerequisite since TFDP operates directly on mask annotations. Quick check: Can you compute the perimeter of a polygon given (x,y) coordinates, and explain why a circle minimizes P/A ratio?

- **Scale-variance in geometric metrics**: SI-SCS design depends on understanding how perimeter and area scale differently under uniform scaling. Quick check: If you double all dimensions of a shape, how does P/A ratio change? What about SI-SCS?

- **COCO evaluation metrics (AP, AP50, APS/APM/APL)**: All experimental validation uses standard COCO protocol. Quick check: What does AP50 measure that mAP doesn't? Why is APL often higher than APS?

## Architecture Onboarding

- **Component map**: Mask preprocessing -> SCS computation -> Scale normalization -> Class-balanced scoring -> Selection
- **Critical path**: Mask contour extraction → correct SI-SCS implementation → proper class grouping for CB-SCS. Errors in contour extraction propagate through all stages.
- **Design tradeoffs**: TFDP is ~1349× faster than model-based methods but cannot capture model-specific difficulties. Fixed metric can't adapt to model architecture, and adapted baselines generalize poorly across architectures.
- **Failure signatures**: 
  1. Selection dominated by one class: Check CB-SCS implementation
  2. All selected images contain only tiny objects: SI-SCS not applied
  3. Performance worse than random at high pruning rates: Likely bug in scoring direction or aggregation
- **First 3 experiments**: 
  1. Reproduce VOC 50% pruning baseline with SCS-only
  2. Ablation: SI-SCS only on VOC to validate scale normalization
  3. Cross-architecture sanity check: Train Mask R-CNN on TFDP-selected COCO subset; evaluate on QueryInst

## Open Questions the Paper Calls Out
None

## Limitations
- Annotation Dependency: Requires full instance mask annotations, limiting applicability to datasets with expensive pixel-level labeling
- Geometric Heuristic Nature: Assumes perimeter-to-area ratio correlates with segmentation difficulty, which is empirically validated but theoretically unproven
- Class Balance Trade-off: CB-SCS may include lower-quality instances from underrepresented classes, potentially degrading overall model robustness

## Confidence

**High Confidence** (Multiple experiments, ablation studies, cross-dataset validation):
- SCS captures meaningful complexity signal (Table 4 ablation)
- Scale-invariance improves AP_L performance (Table 12)
- Class-balanced scoring mitigates dataset imbalance (Fig. 2b analysis)

**Medium Confidence** (Single dataset or architecture tested):
- Cross-architecture generalization (only Mask R-CNN vs QueryInst comparison)
- Real-world deployment performance (only COCO/Cityscapes/VOC benchmarks)
- Time savings vs. downstream training cost trade-off

## Next Checks

1. **Failure Case Analysis**: Systematically identify instances where low SCS correlates with high training difficulty (e.g., texture-heavy objects, occlusion boundaries). Compute correlation between SCS and model loss gradients on a held-out subset.

2. **Cross-Modality Extension**: Test TFDP on video instance segmentation datasets (e.g., YouTube-VIS) where temporal consistency might interact with shape complexity. Compare against motion-based pruning baselines.

3. **Quality vs. Quantity Trade-off**: At extreme pruning rates (>80%), measure whether TFDP-selected subsets maintain better instance quality (IoU distribution, boundary adherence) compared to random selection using a fixed-capacity model trained from scratch.