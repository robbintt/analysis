---
ver: rpa2
title: Contextual Causal Bayesian Optimisation
arxiv_id: '2301.12412'
source_url: https://arxiv.org/abs/2301.12412
tags:
- variables
- causal
- regret
- policy
- contextual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a unified framework for contextual and causal
  Bayesian optimisation, addressing policy selection in context-aware settings with
  multiple possible policy scopes. The authors propose a novel algorithm that jointly
  optimises over policies and the sets of variables on which these policies are defined,
  extending and unifying Causal and Contextual Bayesian Optimisation approaches.
---

# Contextual Causal Bayesian Optimisation

## Quick Facts
- arXiv ID: 2301.12412
- Source URL: https://arxiv.org/abs/2301.12412
- Reference count: 40
- Introduces a unified framework for contextual and causal Bayesian optimisation

## Executive Summary
This paper presents CoCa-BO, a unified framework that combines contextual and causal Bayesian optimisation to address policy selection in context-aware settings with multiple possible policy scopes. The method jointly optimises over intervention policies and the sets of variables on which these policies are defined, extending and unifying previous approaches. The authors demonstrate that their approach achieves sublinear regret and reduces sample complexity in high-dimensional settings through experiments across diverse environments.

## Method Summary
The approach computes a set of Possibly-Optimal Mixed Policy Scopes (POMPS) from the known causal graph, treating each POMPS as an arm in a multi-armed bandit. At each iteration, the algorithm selects a POMPS using an exploration-exploitation criterion based on running average UCB values plus an exploration bonus. For the selected POMPS, contextual Bayesian optimisation with a Gaussian process surrogate selects the intervention value given the observed context. The reward function is modeled as yt = g(xt, Ct) + σnξt where g is a GP prior, and the UCB acquisition function balances posterior mean and variance.

## Key Results
- Achieves sublinear regret with worst-case bound RT ≤ A2√mT(√βTγ̄T + ρT) + ||Δ||1
- Reduces sample complexity by identifying smaller optimisation domains in high-dimensional settings
- Consistently reaches the optimum even where existing methods cannot, and is robust to noise
- Maintains performance across diverse environments including synthetic SCMs and real-world domains

## Why This Works (Mechanism)

### Mechanism 1: Possibly-Optimal Mixed Policy Scope (POMPS) Selection via Multi-Armed Bandit
The algorithm treats each POMPS as a bandit arm, selecting the POMPS with the highest running average of UCB values plus an exploration bonus ρi(ni)/√ni. This reduces the exponentially large scope space while preserving optimality guarantees.

### Mechanism 2: Context-Aware Intervention via GP-based Bayesian Optimisation
For a selected POMPS, the reward is modeled as a GP over (intervention, context) pairs. The UCB acquisition function selects interventions by balancing posterior mean (exploitation) and variance (exploration), with context variables explicitly included as GP inputs.

### Mechanism 3: Regret Decomposition Enables Instance-Dependent Bounds
Total regret is decomposed into choosing suboptimal POMPS, suboptimal intervention values, and stochastic noise. For squared exponential kernels, γT grows polylogarithmically, yielding √T·polylog(T) regret.

## Foundational Learning

- **Concept: Structural Causal Models (SCMs) and Intervention Semantics**
  - Why needed: Policies are defined as modifications to structural equations in the SCM; understanding do-calculus is essential to grasp why intervening on X2 can render X1 irrelevant
  - Quick check question: Can you explain why E[Y|do(X1=x)] differs from E[Y|X1=x] in the presence of confounders?

- **Concept: Gaussian Process Posterior and Information Gain**
  - Why needed: The algorithm's theoretical guarantees depend on maximum information gain γT; understanding how kernel choice affects γT is critical for practical deployment
  - Quick check question: Why does the squared exponential kernel yield γT = O(log^(d+1)(T)) while Matérn yields γT = O(T^(d/(2ν+d))·log(T))?

- **Concept: Multi-Armed Bandit Regret Analysis**
  - Why needed: POMPS selection uses MAB formulation; the exploration bonus ensures sufficient exploration of all arms
  - Quick check question: In standard UCB, how does the confidence width β balance exploration vs exploitation, and what happens if β is set too small?

## Architecture Onboarding

- **Component map**: POMPS Generator -> MAB Selector -> BO Subroutine (HEBO) -> Posterior Updater
- **Critical path**: 1) Offline: Compute POMPS set S*[G] from graph G; 2) Initialise: Create one BO optimiser per POMPS, initialise MAB arm values; 3) Per iteration: MAB selects POMPS St → Observe context ct → BO selects intervention xt → Observe reward yt → Update GP posterior and MAB values
- **Design tradeoffs**: More POMPS improves coverage but regret scales as √m in worst case; GP kernel complexity vs computational cost; pre-computing POMPS is O(exp(|V|)) but parallelisable
- **Failure signatures**: Linear regret plateau (POMPS set excludes optimal scope); erratic POMPS switching (context-dependent acquisition comparisons); no convergence despite low noise (GP kernel misspecified)
- **First 3 experiments**: 1) Reproduce Figure 3d: Run CoCa-BO vs CoBO on the SCM from Figure 1; 2) Backend ablation: Compare HEBO vs BoTorch vs GPflow on the A.2 SCM; 3) Scale test: Run on the 86-variable example (Section A.3)

## Open Questions the Paper Calls Out

### Open Question 1
Can information be efficiently shared between different POMPS during optimisation without strong restrictions on the causal structure? Current approaches require restrictive conditions and high-dimensional integrals. What evidence would resolve it: A provably efficient algorithm for cross-POMPS transfer with relaxed structural assumptions.

### Open Question 2
How can CoCa-BO be extended to settings where the causal graph is unknown and must be learned alongside optimisation? The framework fundamentally relies on the known graph to compute POMPS. What evidence would resolve it: Extension of regret bounds to include graph estimation error, or analysis of robustness to graph misspecification.

### Open Question 3
Can the exponential computational complexity of POMPS enumeration be reduced while preserving theoretical guarantees? The complete enumeration ensures no optimal scope is missed but is computationally prohibitive for large graphs. What evidence would resolve it: Polynomial-time approximation algorithms for POMPS computation with provable approximation guarantees.

## Limitations

- Exponential computational complexity of POMPS enumeration limits scalability for large causal graphs
- Claims about robustness to heteroscedasticity and non-stationarity rely heavily on HEBO's learnable transformations without specific configuration details
- Limited ablation studies on backend variations reduce confidence in empirical scalability claims beyond the 86-variable example

## Confidence

- **High**: Regret decomposition framework and worst-case bounds follow standard MAB and GP bandit analyses
- **Medium**: Instance-dependent bounds depend on specific kernel properties and γT growth rates requiring careful tuning
- **Low**: Empirical scalability claims beyond the 86-variable example given limited ablation studies

## Next Checks

1. **POMPS Coverage Verification**: Systematically verify that S*[G] includes all possibly-optimal scopes for synthetic SCMs with known optimal interventions, ensuring the algorithm doesn't suffer linear regret from scope exclusion.

2. **Backend Ablation Study**: Reproduce key experiments comparing HEBO with alternative BO backends (BoTorch, GPflow) using identical POMPS and MAB configurations to isolate the contribution of the policy selection framework from implementation details.

3. **Graph Misspecification Robustness**: Evaluate algorithm performance under controlled causal graph misspecifications (edge deletions, direction reversals) to quantify the break condition where the optimal scope is excluded from S*[G].