---
ver: rpa2
title: 'FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based
  Agents'
arxiv_id: '2602.01566'
source_url: https://arxiv.org/abs/2602.01566
tags:
- report
- research
- context
- fs-researcher
- rounds
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'FS-Researcher is a file-system-based dual-agent framework that
  scales deep research beyond the context window. It separates evidence collection
  from report writing: a Context Builder browses the web and archives structured notes
  into a persistent knowledge base, and a Report Writer composes the final report
  section by section using on-demand retrieval from this workspace.'
---

# FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents

## Quick Facts
- arXiv ID: 2602.01566
- Source URL: https://arxiv.org/abs/2602.01566
- Reference count: 40
- Primary result: Dual-agent file-system framework achieves state-of-the-art report quality, with Claude-Sonnet-4.5 reaching 53.94 RACE on DeepResearch Bench

## Executive Summary
FS-Researcher is a file-system-based dual-agent framework that scales deep research beyond model context limits. It separates evidence collection from report writing: a Context Builder browses the web and archives structured notes into a persistent knowledge base, while a Report Writer composes the final report section by section using on-demand retrieval from this workspace. Experiments show state-of-the-art performance on DeepResearch Bench and DeepConsult, with further analysis revealing that report quality scales positively with computation allocated to the Context Builder, validating effective test-time scaling.

## Method Summary
The method uses a persistent file-system workspace as durable external memory to enable iterative refinement across sessions. A Context Builder agent with web browsing tools searches, reads, and distills evidence into a structured knowledge base over multiple rounds. A separate Report Writer agent, restricted to file-system tools only, composes the final report section-by-section by retrieving from the accumulated knowledge base. Both agents coordinate through shared control files (Todos, Checklist, Logs) and work sequentially to prevent premature synthesis.

## Key Results
- Achieves 53.94 RACE score on DeepResearch Bench with Claude-Sonnet-4.5
- Wins 80.00% of comparisons on DeepConsult
- Positive correlation between Context Builder computation and report quality validates test-time scaling
- Ablation studies confirm persistent workspace and agent separation are essential for high performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Externalizing agent state into a persistent file-system workspace enables research tasks to scale beyond model context limits
- Mechanism: The workspace acts as "durable external memory" where intermediate artifacts persist across sessions. Instead of competing for limited context tokens, the agent reads/writes files on-demand using tools like `read_file`, `grep`, and `insert/replace`
- Core assumption: The backbone model can reliably execute file-system operations and navigate hierarchical structures without frequent errors
- Evidence anchors: Abstract confirms workspace as "durable external memory"; ablation shows persistent workspace removal drops RACE by -4.07
- Break condition: Models with poor function-calling reliability may produce incorrect file edits or fail to maintain consistent state

### Mechanism 2
- Claim: Separating evidence collection from report composition prevents premature synthesis and improves depth
- Mechanism: Two specialized agents work sequentially. Context Builder browses, extracts, and archives into a structured knowledge base. Report Writer then composes section-by-section, treating the KB as its sole fact source—no web access during writing
- Core assumption: Sequential processing (complete evidence gathering before synthesis) outperforms interleaved browsing-and-writing
- Evidence anchors: Abstract describes dual-agent separation; single-agent ablation causes largest degradation (-10.35 RACE)
- Break condition: If Context Builder produces incomplete KB, Report Writer cannot recover—no browsing tools during writing stage

### Mechanism 3
- Claim: Allocating more computation (rounds) to Context Builder improves knowledge base quality, correlating with final report quality
- Mechanism: Each round allows additional search-read-archive cycles. More rounds → more sources archived, more notes distilled, more structured comparisons. Report Writer draws from this richer KB
- Core assumption: Diminishing returns exist but quality gains persist up to tested limits (10 rounds)
- Evidence anchors: Abstract confirms positive correlation; Figure 4 shows RACE improving from 51.18 (3 rounds) → 53.05 (10 rounds)
- Break condition: Readability peaked at 5 rounds (51.93) then dropped at 10 rounds (51.66)—overly large KBs may introduce verbosity

## Foundational Learning

- **Concept: ReAct Architecture (Reasoning + Acting loops)**
  - Why needed here: FS-Researcher uses standard ReAct formulation (Thought → Action → Observation cycles). Understanding this is prerequisite to debugging agent trajectories
  - Quick check question: Can you trace one complete Thought-Action-Observation cycle from a sample agent log?

- **Concept: Hierarchical Knowledge Organization**
  - Why needed here: Context Builder creates tree-structured KBs (e.g., `company_profiles/allianz/dividends_and_payouts.md`). Understanding semantic hierarchy design is critical for workspace quality
  - Quick check question: Given a research query, can you sketch a 3-level directory structure that would organize the evidence?

- **Concept: Multi-session State Management**
  - Why needed here: Agents resume across sessions using control files (Todos, Checklist, Logs). Understanding how state persists and how agents inspect workspace status at session start is essential
  - Quick check question: What files does an agent read at the beginning of a new session to determine what work remains?

## Architecture Onboarding

- **Component map:**
```
Research Query → [Context Builder Agent] ←→ [Web Tools: search_web, read_webpage]
                       ↓                    [File Tools: ls, grep, read_file, insert/replace/delete]
              ./knowledge_base/ (notes)
              ./sources/ (raw pages)
              ./index.md (table of contents)
                       ↓
              [Report Writer Agent] ←→ [File Tools only—no web access]
                       ↓
              ./report.md (section-by-section output)
```
Control Files (both stages): Todos (task status: PENDING/IN-PROGRESS/COMPLETE), Checklist (acceptance criteria), Logs (execution trajectory)

- **Critical path:**
  1. Context Builder initializes workspace, deconstructs topic in `index.md`
  2. Context Builder runs N rounds of search→read→distill→archive cycles
  3. Context Builder self-checks against checklist, logs gaps
  4. Report Writer loads KB, creates outline
  5. Report Writer writes one section per session, self-checks against section-level checklist
  6. Final report-level review

- **Design tradeoffs:**
  - Cost vs. quality: 10 rounds costs ~2x of 3 rounds but RACE improves only ~3.7%
  - Comprehensiveness vs. readability: Denser KBs produce more technical, citation-heavy reports
  - Separation vs. flexibility: Report Writer cannot browse web if KB gaps are discovered during writing

- **Failure signatures:**
  - Smaller models: shorter trajectories, premature stopping, more file operation errors
  - Missing checklist compliance: sections marked COMPLETE without passing self-check
  - Citation stacking (GPT-5 behavior): multiple citations at paragraph end causing misalignment
  - Context Builder stalls: unable to fetch primary sources (e.g., UNECE/NHTSA endpoints inaccessible)

- **First 3 experiments:**
  1. Reproduce the 3-round vs 5-round scaling curve on 5 sampled queries from DeepResearch Bench; verify KB growth statistics match Table 7 patterns
  2. Ablate section-wise writing: have Report Writer generate full report in one session; compare RACE scores against section-by-section baseline
  3. Test with a weaker backbone (e.g., smaller open model); measure trajectory length, file operation error rate, and task completion rate to identify failure modes

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies heavily on automated metrics (RACE, FACT) with limited human evaluation
- Comparison with OpenAI's DeepResearch is constrained by API availability
- Fixed agent architecture may struggle with tasks requiring dynamic tool selection or real-time updates during writing phase

## Confidence
- **High confidence**: File-system workspace enables scaling beyond context limits (consistent RACE improvements in ablations)
- **Medium confidence**: Test-time scaling claim (correlation between Context Builder computation and report quality), though confounded by potential diminishing returns
- **Medium confidence**: Separation-of-agents claim, as dual-agent ablations show large impacts but don't test all architectural variations

## Next Checks
1. Implement human evaluation on 20 randomly sampled reports comparing FS-Researcher vs single-agent baselines to validate automated RACE scores
2. Test Context Builder alone with fixed KB budget but varying round counts to isolate whether quality gains come from more computation vs. more rounds specifically
3. Run stress tests with corrupted workspace states (missing files, inconsistent todos) to measure robustness to agent failures