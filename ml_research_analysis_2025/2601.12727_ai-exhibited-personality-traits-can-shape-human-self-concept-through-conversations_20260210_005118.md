---
ver: rpa2
title: AI-exhibited Personality Traits Can Shape Human Self-concept through Conversations
arxiv_id: '2601.12727'
source_url: https://arxiv.org/abs/2601.12727
tags:
- self-concept
- personality
- traits
- alignment
- conversation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates whether AI personality traits influence\
  \ users\u2019 self-concept during conversation. A randomized experiment (N=92) compared\
  \ conversations on personal versus non-personal topics with an LLM-based AI chatbot\
  \ (GPT-4o default personality)."
---

# AI-exhibited Personality Traits Can Shape Human Self-concept through Conversations

## Quick Facts
- arXiv ID: 2601.12727
- Source URL: https://arxiv.org/abs/2601.12727
- Reference count: 40
- Key outcome: AI personality traits influence users’ self-concept, increasing alignment with conversation length and enjoyment.

## Executive Summary
This study investigates whether the personality traits exhibited by an LLM-based chatbot (GPT-4o default) can influence human self-concept during conversation. A randomized experiment with 92 participants found that after discussing personal topics, participants’ self-concepts aligned with the AI’s personality traits, with greater alignment for longer conversations. This alignment increased homogeneity across participants and was associated with greater conversation enjoyment through a serial mediation of perception accuracy and shared reality experience. The findings highlight risks of AI-induced self-concept shifts and offer design implications for ethical AI development.

## Method Summary
The study employed a randomized experiment where 92 US participants (age 21+) conversed with GPT-4o on either personal or non-personal topics. Participants’ self-concepts were measured before and after conversation using a 20-item personality trait scale (0-100%). The AI’s personality was established by averaging 100 prompt completions of the same scale. Alignment was quantified as the reduction in Manhattan distance between participants’ self-concept and the AI’s personality pre- and post-conversation. Homogeneity, enjoyment, and shared reality were also measured, with analysis via repeated measures ANOVA and structural equation modeling.

## Key Results
- Self-concept alignment with AI personality increased after personal-topic conversations, with alignment growing with conversation length.
- Alignment increased homogeneity of self-concepts across participants.
- Greater alignment was positively associated with conversation enjoyment through perception accuracy and shared reality experience.

## Why This Works (Mechanism)
The mechanism involves conversational interaction with an AI exhibiting consistent personality traits, leading participants to perceive and internalize those traits, which then shifts their self-concept toward the AI’s exhibited personality.

## Foundational Learning
- **Manhattan distance for alignment**: Measures absolute differences between trait vectors; needed to quantify self-concept shift toward AI.
- **Repeated measures ANOVA**: Tests within-subject changes in alignment; needed to compare pre- and post-conversation self-concepts.
- **Structural equation modeling (SEM)**: Models indirect effects (alignment → enjoyment via perception and shared reality); needed to test mediation pathways.
- **Personality trait scale (0-100%)**: Standardized self- and AI-assessment; needed for comparable pre/post and human/AI measurements.
- **Topic randomization (personal vs. non-personal)**: Controls for topic effects; needed to isolate personality-driven alignment.
- **Turn-based chat interface**: Ensures conversation depth; needed to measure alignment as a function of interaction length.

## Architecture Onboarding
- **Component map**: Participants -> Conversation (LLM + topics) -> Pre/Post surveys (self-concept, perception, enjoyment) -> Alignment/Homogeneity metrics -> Statistical analysis
- **Critical path**: Pre-survey → Chat interaction → Post-survey → Alignment calculation → Statistical testing
- **Design tradeoffs**: Used default AI personality (external validity) vs. controlled manipulation (internal validity); personality scale limited to traits (simplicity) vs. broader self-concept domains (completeness).
- **Failure signatures**: Null alignment may indicate insufficient conversation depth, poor topic matching, or unstable AI personality vector; high variance in AI traits suggests measurement unreliability.
- **First experiments**:
  1. Verify AI personality vector stability by running 100 self-assessments and computing Cronbach’s alpha.
  2. Confirm baseline alignment is near zero by comparing pre-conversation self-concept distance to AI traits.
  3. Test for significant alignment in the personal topic condition using repeated measures ANOVA.

## Open Questions the Paper Calls Out
- **Open Question 1**: Do short-term self-concept shifts persist after a single AI interaction ends, and does repeated exposure over time deepen this alignment? The study was limited to a single, short-term session without follow-up measures.
- **Open Question 2**: Does the degree of self-concept alignment vary when AI systems are configured with specific, manipulated personality traits rather than default settings? The study used GPT-4o’s default personality as a fixed constant.
- **Open Question 3**: Do cultural and socioeconomic backgrounds influence how susceptible individuals are to AI-induced self-concept alignment? The sample was limited to US participants, with no demographic segmentation.
- **Open Question 4**: Does conversing with AI influence non-trait dimensions of self-concept, such as self-perception of physical appearance or specific abilities? The measurement tool was restricted to personality traits.

## Limitations
- The study’s findings depend on the stability of GPT-4o’s default personality, which may drift over model updates.
- The personality scale used was limited to traits, excluding other self-concept dimensions like physical appearance or abilities.
- Cultural generalizability is limited due to the Western-derived scale and US-only sample.
- The study did not control for participants’ baseline personality traits, which could moderate alignment susceptibility.

## Confidence
- **High confidence**: The methodological framework for measuring alignment and the statistical significance of observed effects within the tested conditions.
- **Medium confidence**: The causal interpretation that AI personality traits shape self-concept, given potential confounds and the single-model design.
- **Low confidence**: Generalizability of effect sizes to other AI models, personality scales, or cultural contexts.

## Next Checks
1. Replicate the experiment with a pinned GPT-4o model version or a different LLM to test robustness of personality alignment effects.
2. Run 100 repeated self-assessments of the AI’s personality traits to quantify internal consistency (Cronbach’s alpha) and assess whether the “target” vector is stable.
3. Collect and analyze participants’ baseline personality scores to test whether certain traits (e.g., high agreeableness) are more susceptible to AI-induced alignment.