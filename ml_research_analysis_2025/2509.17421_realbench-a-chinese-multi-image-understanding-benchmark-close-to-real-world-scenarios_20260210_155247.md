---
ver: rpa2
title: 'RealBench: A Chinese Multi-image Understanding Benchmark Close to Real-world
  Scenarios'
arxiv_id: '2509.17421'
source_url: https://arxiv.org/abs/2509.17421
tags:
- image
- multi-image
- images
- answer
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'RealBench is the first Chinese multi-image benchmark built from
  real user-generated content, containing 9393 samples and 69910 images. It covers
  four tasks: retrieval, ranking, extraction, and reasoning, designed to evaluate
  models on diverse real-world scenarios.'
---

# RealBench: A Chinese Multi-image Understanding Benchmark Close to Real-world Scenarios

## Quick Facts
- arXiv ID: 2509.17421
- Source URL: https://arxiv.org/abs/2509.17421
- Reference count: 38
- RealBench presents significant challenges even for state-of-the-art models, with accuracy ranging from 23.98% to 46.88% across tasks, and a substantial performance gap of around 71.8% between open-source and closed-source models.

## Executive Summary
RealBench is the first Chinese multi-image benchmark built from real user-generated content, containing 9393 samples and 69910 images. It covers four tasks: retrieval, ranking, extraction, and reasoning, designed to evaluate models on diverse real-world scenarios. The benchmark presents significant challenges even for state-of-the-art models, with accuracy ranging from 23.98% to 46.88% across tasks, and a substantial performance gap of around 71.8% between open-source and closed-source models. These results highlight the difficulty of Chinese multi-image understanding and the need for further research.

## Method Summary
RealBench constructs a Chinese multi-image understanding benchmark using authentic social media content. The dataset contains 9,393 samples with 69,910 images across four task categories: retrieval, ranking, extraction, and reasoning. Images are sourced from real user-generated content with diverse resolutions, layouts, and structures. The benchmark employs zero-shot evaluation to test model capabilities without fine-tuning, using comprehensive metrics including Top-K accuracy, ROUGE-L, and mIoU. Models process interleaved image-text sequences through visual encoders and LLM backbones to perform the specified tasks.

## Key Results
- RealBench demonstrates substantial difficulty for state-of-the-art models with accuracy ranging from 23.98% to 46.88% across tasks
- Open-source models show a significant performance gap of approximately 71.8% compared to closed-source models
- The benchmark effectively captures real-world complexity through diverse user-generated content with varying resolutions and structures

## Why This Works (Mechanism)

### Mechanism 1: Cross-Image Semantic Grounding
The multi-image retrieval and ranking tasks test a model's ability to align textual descriptions with specific images and maintain that alignment across multiple visual inputs. The retrieval task establishes baseline semantic grounding by matching text to relevant images, while ranking layers sequential understanding on top, requiring the model to infer correct order from text's narrative flow. This tests coherent visual-textual integration across multiple images.

### Mechanism 2: Distributed Information Integration and Synthesis
The extraction and reasoning tasks evaluate a model's ability to parse, hold, and synthesize textual information fragmented across multiple visual sources. Extraction requires fine-grained OCR and identification of relevant text snippets, while reasoning compounds this by requiring operations on extracted data from different images. Success depends on maintaining persistent representations across images and applying reasoning operators to combined information.

### Mechanism 3: Robustness to Real-World Visual Complexity
Authentic user-generated content with diverse resolutions, layouts, and embedded text creates high-variance visual manifold testing model robustness beyond curated datasets. Real-world images contain noise, non-standard layouts, and varying quality. A model succeeds by learning visual representations invariant to superficial distractions while remaining sensitive to core semantic content.

## Foundational Learning

- **Concept: Multi-Modal Input Encoding.**
  - **Why needed here:** The model must accept more than one image along with text, requiring specific architectural setup (special tokens to demarcate image boundaries, interleaved image-text input formatting).
  - **Quick check question:** How does your model format and process input that contains three images and a paragraph of text describing them in a specific order?

- **Concept: In-Context Learning (ICL) and Zero-Shot Generalization.**
  - **Why needed here:** The paper specifies zero-shot evaluation protocol. The model must follow instructions for each of four distinct tasks without task-specific fine-tuning, using only the prompt as guide.
  - **Quick check question:** Can the model switch between outputting list of image IDs (for retrieval) and generating reasoned answer (for reasoning) solely based on prompt provided?

- **Concept: Grounding and Referring Expression Comprehension.**
  - **Why needed here:** Core of multi-image retrieval and ranking tasks is linking parts of text description to specific visual elements in specific images. This is foundational skill for any multi-image task.
  - **Quick check question:** Given image of kitchen and phrase "the black cast iron pot on the stove," can the model correctly identify object, and can it do so when three similar kitchen images are provided?

## Architecture Onboarding

- **Component map:** Visual Encoder (e.g., ViT) → Projection Layer / Q-Former → LLM Backbone → Output Generation

- **Critical path:** Visual Encoder → Visual Tokenization → Projection → LLM Input (Interleaved) → LLM Attention & Reasoning → Output Generation. The most fragile link is LLM's ability to maintain coherent attention over concatenated sequence of all image and text tokens.

- **Design tradeoffs:**
  - Image Token Count: Higher-resolution images or more images lead to longer sequences, increasing computational cost and potentially overflowing context window. Trade-off between visual detail and sequence length.
  - Training Data: Curating large-scale, high-quality dataset of interleaved Chinese image-text data is resource-intensive but crucial for performance.

- **Failure signatures:**
  - Cross-Image Hallucination: Model describes objects from Image A when asked about Image B, indicating failure in attention mechanism to disentangle visual sources.
  - Instruction Forgetting: Model performs generic image caption instead of specific task requested, sign of weak ICL or prompt following.
  - Context Overload: Performance on ranking task degrades significantly as number of images increases from 6 to 18.

- **First 3 experiments:**
  1. Reproduce Baseline on Single Task: Run model on subset of multi-image retrieval (easy) task to isolate grounding text to multiple images without added complexity of ordering or reasoning.
  2. Ablate Image Token Count/Resolution: Experiment with different image resolutions or visual token counts on ranking task to test hypothesis that more visual detail is needed for distinguishing order.
  3. Test with Combined Image: Combine multiple images of each sample into one large composite image and evaluate single-image-only model on extraction task to understand value added by native multi-image processing architectures.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multimodal architectures be optimized to resolve the "cross-image integration bottleneck" identified in complex reasoning tasks?
- Basis in paper: [explicit] The authors note that models struggle to integrate information across images, with GPT-4o achieving low ROUGE-L score of 0.53 in multi-image extraction, indicating specific failure in synthesizing distributed information.
- Why unresolved: The paper evaluates existing architectures but does not propose or test mechanisms (e.g., specific attention modules) to bridge semantic gap between multiple distinct visual inputs.
- What evidence would resolve it: Development of model architecture that demonstrates statistically significant gains on "hard" versions of extraction and reasoning tasks by effectively attending to textual cues across multiple images.

### Open Question 2
- Question: To what extent does English-centric pre-training of MLLMs cause performance gap compared to visual complexity of RealBench images?
- Basis in paper: [inferred] The paper speculates that "language-specific limitations" contribute to gap, but also cites "Real-world image complexity" (varying resolutions/structures) as factor, leaving primary cause of low accuracy (23.98%–46.88%) ambiguous.
- Why unresolved: Current benchmark mixes diverse visual structures with Chinese text, making it difficult to isolate whether failures are due to visual processing limits or linguistic misalignment.
- What evidence would resolve it: Control experiment evaluating models on same images with translated English prompts to isolate visual vs. linguistic difficulty.

### Open Question 3
- Question: Can expanding RealBench into large-scale pre-training resource effectively close 71.8% performance gap between open-source and closed-source models?
- Basis in paper: [explicit] In Limitations section, authors state dataset "can be further expanded to meet growing data demands of large-scale pretraining models."
- Why unresolved: Dataset currently serves as evaluation benchmark; utility of real-world user-generated Chinese data for instruction tuning or pre-training remains unproven.
- What evidence would resolve it: Empirical results showing that open-source models fine-tuned on expanded version of RealBench significantly improve average accuracy relative to closed-source baselines.

## Limitations
- Benchmark's reliance on Chinese language content raises questions about generalizability to other languages and cultures
- Dataset size (9393 samples) may be insufficient to draw definitive conclusions about model capabilities
- Paper doesn't address potential bias in social media content selection or how well four task categories represent full spectrum of real-world multi-image understanding scenarios

## Confidence

**High Confidence (Likelihood >80%)**
- Benchmark successfully identifies performance gap between model types, demonstrating discriminative power
- Four-task structure covers distinct aspects of multi-image understanding (retrieval, ranking, extraction, reasoning)
- Use of real user-generated content introduces realistic complexity and variability

**Medium Confidence (Likelihood 50-80%)**
- Claim that RealBench is "first" Chinese multi-image benchmark requires verification against all existing Chinese multimodal datasets
- Zero-shot evaluation protocol effectively isolates model capabilities from fine-tuning effects
- Performance numbers accurately reflect true model limitations rather than evaluation artifacts

**Low Confidence (Likelihood <50%)**
- Benchmark fully represents all real-world multi-image understanding scenarios
- Dataset size (9393 samples) is sufficient to draw definitive conclusions about model capabilities
- Performance gap is entirely due to model architecture rather than dataset-specific characteristics

## Next Checks
1. **Dataset Contamination Analysis**: Conduct thorough investigation comparing RealBench samples against training corpora of top-performing models to quantify potential overlap and assess validity of zero-shot evaluation claim.

2. **Cross-Cultural Transferability Test**: Evaluate benchmark's generalization by translating subset of samples to English and testing with non-Chinese specialized models, measuring performance degradation and identifying language-specific challenges.

3. **Annotation Reliability Assessment**: Calculate inter-annotator agreement metrics (Cohen's kappa, Krippendorff's alpha) across multiple annotator groups to establish reliability of ground truth labels and identify potential biases in annotation process.