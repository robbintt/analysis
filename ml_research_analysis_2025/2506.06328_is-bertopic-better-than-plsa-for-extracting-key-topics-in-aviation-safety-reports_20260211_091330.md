---
ver: rpa2
title: Is BERTopic Better than PLSA for Extracting Key Topics in Aviation Safety Reports?
arxiv_id: '2506.06328'
source_url: https://arxiv.org/abs/2506.06328
tags:
- topic
- bertopic
- plsa
- aviation
- topics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares BERTopic and PLSA for extracting meaningful
  topics from aviation safety reports using a dataset of over 36,000 NTSB reports
  from 2000-2020. BERTopic employed transformer-based embeddings and hierarchical
  clustering, while PLSA used probabilistic modeling through the Expectation-Maximization
  algorithm.
---

# Is BERTopic Better than PLSA for Extracting Key Topics in Aviation Safety Reports?

## Quick Facts
- arXiv ID: 2506.06328
- Source URL: https://arxiv.org/abs/2506.06328
- Authors: Aziida Nanyonga; Joiner Keith; Turhan Ugur; Wild Graham
- Reference count: 22
- Primary result: BERTopic achieves superior topic coherence (C_v=0.41) compared to PLSA (C_v=0.37) for aviation safety report analysis

## Executive Summary
This study evaluates BERTopic against PLSA for extracting meaningful topics from aviation safety reports using over 36,000 NTSB reports from 2000-2020. BERTopic employs transformer-based embeddings with hierarchical clustering, while PLSA uses probabilistic modeling through the Expectation-Maximization algorithm. Results demonstrate BERTopic achieves higher topic coherence and better interpretability with clearly defined themes like "engine failures" and "runway incursions," as validated by aviation safety experts. These findings indicate transformer-based approaches are more effective than traditional probabilistic methods for analyzing complex aviation datasets, providing more coherent and actionable insights for safety analysis.

## Method Summary
The study compared BERTopic and PLSA on 36,000+ NTSB aviation incident/accident reports (2000-2020). Preprocessing involved NLTK-based tokenization, lowercasing, stopword removal, lemmatization, and special character filtering. PLSA was implemented via Gensim with 6 topics using EM algorithm. BERTopic 0.13.0 used default transformer embeddings with HDBSCAN clustering (min_cluster_size=15) and class-based TF-IDF representation. Evaluation metrics included C_v coherence scores, expert validation of interpretability, and computational efficiency comparisons on Python 3.10 with Intel i7, 32GB RAM, and NVIDIA GPU.

## Key Results
- BERTopic achieved superior topic coherence with C_v score of 0.41 compared to PLSA's 0.37
- BERTopic demonstrated better interpretability with clearly defined themes like "engine failures" and "runway incursions"
- BERTopic showed better computational efficiency (1.2 hours vs PLSA's 2.6 hours) on the 36K document dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BERTopic's transformer-based embeddings capture semantic relationships between words, producing more coherent topics than PLSA's bag-of-words representation.
- Mechanism: BERT generates contextual embeddings where word vectors depend on surrounding text, allowing semantically related terms (e.g., "runway," "takeoff," "landing") to occupy similar regions in embedding space. PLSA's bag-of-words representation treats each word independently, discarding word order and contextual meaning.
- Core assumption: Aviation safety narratives contain domain-specific terminology where contextual relationships (not just co-occurrence) improve topic quality.
- Evidence anchors:
  - [abstract] "BERTopic employed transformer-based embeddings and hierarchical clustering, while PLSA utilized probabilistic modelling through the Expectation-Maximization (EM) algorithm"
  - [page 4, Table III] "High coherence due to semantic embeddings, achieving a score of 0.41" vs "Lower coherence due to bag-of-words representation, with a score of 0.37"
  - [corpus] Paper 11581 on NTSB narratives similarly finds topic modeling effectiveness varies significantly by technique, supporting the contextual approach; Paper 29580 discusses enhancing BERTopic embeddings further
- Break condition: If aviation reports use highly standardized, formulaic language where word order is irrelevant, the embedding advantage diminishes. Also breaks if computational resources cannot support GPU-accelerated embedding generation.

### Mechanism 2
- Claim: HDBSCAN density-based clustering creates more distinct topic boundaries than PLSA's probabilistic soft assignment across all topics.
- Mechanism: HDBSCAN identifies dense regions in embedding space and treats sparse regions as noise, allowing natural topic separation without forcing documents into weak associations. PLSA's EM algorithm assigns all documents probabilistically across all topics, causing overlapping distributions and topic dominance.
- Core assumption: Aviation safety reports contain well-defined thematic clusters with some outlier documents that should not influence topic definitions.
- Evidence anchors:
  - [page 3] "Clustering techniques, specifically HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications and Noise), were employed to group embeddings into clusters"
  - [page 5, Fig. 7-8 description] BERTopic shows "distinct separations between topics" while PLSA shows "Topic 4 emerges as the dominant topic, as it spans most of the document corpus"
  - [corpus] Paper 11121 confirms HDBSCAN's effectiveness for BERTopic in short text clustering; Paper 10527 comparing topic models on ATSB narratives supports clustering-based approaches
- Break condition: If the dataset contains many documents that do not fit clear categories (high noise ratio), HDBSCAN may exclude too many as outliers. The minimum cluster size parameter (set to 15) becomes critical.

### Mechanism 3
- Claim: Class-based TF-IDF topic representation produces more interpretable topic labels by emphasizing terms that distinguish topics from each other.
- Mechanism: After clustering, BERTopic treats each cluster as a class and applies TF-IDF weighted by class membership, finding terms that are frequent within a cluster AND distinctive compared to other clusters. This produces representative words that clearly differentiate topics rather than common but non-distinctive terms.
- Core assumption: Domain experts can validate topic quality by inspecting top representative words, and distinctive vocabulary correlates with interpretable themes.
- Evidence anchors:
  - [page 4, Table I] BERTopic produces clear themes: "Flight Operations," "Fuel System and Failures," "Helicopter Operations" with distinctive top words per topic
  - [page 4, Table II] PLSA topics show generic overlapping terms: "pilot, airplane, engine, flight" appear across multiple topics
  - [page 4] "BERTopic demonstrated a higher degree of interpretability...PLSA...struggled to maintain semantic clarity, often blending unrelated words into the same topic"
  - [corpus] Paper 29580 explicitly discusses enhancing BERTopic's intermediate layer representations for improved topic quality
- Break condition: If topics have significant vocabulary overlap (multiple engine-related subtopics), TF-IDF may not sufficiently distinguish them. Number of topics (6) must align with actual thematic structure.

## Foundational Learning

- Concept: **Topic Coherence Metrics (C_v)**
  - Why needed here: The study's primary quantitative comparison uses C_v coherence scores (0.41 vs 0.37). Understanding that C_v measures semantic similarity of top words within topics using external word embeddings is essential to interpret whether this 0.04 difference is meaningful.
  - Quick check question: If a topic's top 10 words are "runway, landing, approach, tower, clearance, taxiway, hold, short, line, wait," would you expect a high or low C_v score, and why?

- Concept: **Expectation-Maximization (EM) Algorithm**
  - Why needed here: PLSA relies on EM to iteratively estimate topic-word and document-topic distributions. Understanding that EM finds local optima through alternating E-steps (expected topic assignments) and M-steps (probability updates) explains PLSA's documented overfitting tendency and sensitivity to initialization.
  - Quick check question: Why might EM converge to a suboptimal solution if initialized poorly, and how does this differ fundamentally from how HDBSCAN identifies clusters?

- Concept: **Contextual vs. Static Word Representations**
  - Why needed here: BERT embeddings are contextual—the word "approach" in "final approach" vs. "approach to safety" has different representations. PLSA's bag-of-words treats "approach" identically regardless of context. This is the core technical difference driving the coherence gap.
  - Quick check question: In "The pilot reported the engine lost power during final approach," which words would have different BERT embeddings compared to "The final report concluded the engine failed due to fuel exhaustion"?

## Architecture Onboarding

- Component map:
Raw NTSB Reports (JSON, 36K+ documents) -> [Text Preprocessing] NLTK-based -> [BERTopic Pipeline] Embedding: BERT (GPU-accelerated) -> Clustering: HDBSCAN (min_cluster_size=15) -> Representation: Class-based TF-IDF -> [Evaluation Layer] C_v coherence (Gensim) -> Expert validation (qualitative) -> Visualization: Intertopic distance maps

- Critical path: Embedding quality → Clustering parameter selection (min_cluster_size=15) → Topic count (k=6) → Interpretability validation. The minimum cluster size directly controls topic granularity—too small creates noise topics, too large merges distinct themes.

- Design tradeoffs:
  - Coherence vs. granularity: k=6 topics improved coherence but may obscure sub-themes; domain expert review required to validate sufficiency
  - Processing time: BERTopic 1.2 hours vs PLSA 2.6 hours on 36K documents, but BERTopic requires GPU (unspecified NVIDIA model)
  - Automation vs. validation: Quantitative C_v metrics require qualitative expert validation—Tables I/II show identical coherence scores could mask interpretability differences

- Failure signatures:
  - Topic dominance: If one topic spans >40% of documents (see PLSA Fig. 8), clustering parameters need adjustment
  - Word repetition across topics: If "pilot," "airplane," "engine" appear in 5/6 topics (see PLSA Table II), representation is failing to distinguish themes
  - Low coherence (<0.35): Indicates vocabulary mismatch between model assumptions and document content

- First 3 experiments:
  1. Parameter sweep on min_cluster_size [10, 15, 20, 30, 50] using 5K document subset. Measure C_v coherence and document coverage (% assigned to topics vs. noise). Target: coherence >0.40 with >80% coverage.
  2. Topic count validation: Extract topics at k=6, then have domain experts assess whether important themes are merged. Compare top-10 word lists across topics to identify overlap.
  3. Cross-dataset generalization: Apply identical configuration to ATSB reports (corpus paper 10527) or Socrata dataset (corpus paper 10431) to test if min_cluster_size=15 transfers across aviation corpora or requires retuning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can hybrid models combining traditional probabilistic methods and embedding-based techniques improve topic extraction performance?
- Basis in paper: [explicit] The abstract and conclusion explicitly state that "Future work will explore hybrid models" and that research could combine "the strengths of traditional and embedding-based models."
- Why unresolved: The current study evaluated BERTopic and PLSA strictly as standalone, competing methodologies without investigating potential architectural combinations.
- What evidence would resolve it: Comparative results showing coherence scores and interpretability metrics for a hybrid PLSA-BERT architecture applied to the same NTSB dataset.

### Open Question 2
- Question: How effective are transformer-based topic modeling techniques when applied to multilingual aviation safety datasets?
- Basis in paper: [explicit] The abstract identifies "multilingual datasets" as a specific avenue for future work.
- Why unresolved: The study utilized the NTSB dataset, which contains reports exclusively in English, leaving the cross-linguistic applicability of the findings untested.
- What evidence would resolve it: Evaluation of BERTopic's performance on non-English aviation safety corpora, measuring coherence and topic alignment across different languages.

### Open Question 3
- Question: Do alternative or advanced clustering techniques further enhance BERTopic's performance in extracting aviation safety themes?
- Basis in paper: [explicit] The abstract notes that future work will explore "advanced clustering techniques" to improve topic modeling in the domain.
- Why unresolved: The methodology relied on HDBSCAN as the default clustering mechanism within BERTopic, without testing other clustering algorithms.
- What evidence would resolve it: Ablation studies substituting HDBSCAN with alternative clustering methods (e.g., k-Means, Agglomerative Clustering) to compare resulting topic coherence and granularity.

## Limitations
- PLSA implementation in Gensim is not native and may have used LSI as a proxy rather than true EM-based PLSA
- Exact pre-trained transformer model for BERTopic embeddings remains unspecified
- Minimum cluster size parameter (15) was chosen without systematic validation
- Expert interpretability validation lacked transparency in participant count and scoring criteria

## Confidence

- **High confidence**: Topic coherence results (C_v scores of 0.41 vs 0.37) are directly measurable and reproducible with the specified metrics and preprocessing pipeline. The qualitative interpretability differences described in Tables I and II are well-supported by the data presented.
- **Medium confidence**: The computational efficiency claims (BERTopic 1.2h vs PLSA 2.6h) are plausible but depend on GPU specifications that were not provided. The superiority of contextual embeddings over bag-of-words is theoretically sound but the specific performance gap may vary with dataset characteristics.
- **Low confidence**: Claims about HDBSCAN's superiority over PLSA's soft assignment require more rigorous quantitative validation beyond visual inspection of intertopic maps. The generalizability of min_cluster_size=15 across different aviation datasets remains untested.

## Next Checks

1. **Parameter Sensitivity Analysis**: Systematically vary min_cluster_size [10, 15, 20, 30, 50] on a 5K document subset and measure both coherence scores and document coverage to identify optimal clustering parameters.
2. **Cross-Dataset Generalization**: Apply the exact BERTopic configuration (min_cluster_size=15, k=6) to ATSB reports or Socrata aviation datasets to test whether the approach transfers across aviation corpora or requires retuning.
3. **Implementation Verification**: Confirm the exact PLSA implementation used (Gensim LsiModel vs custom EM) and reproduce the topic-word distributions in Table II to ensure methodological consistency.