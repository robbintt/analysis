---
ver: rpa2
title: 'Rethinking Hate Speech Detection on Social Media: Can LLMs Replace Traditional
  Models?'
arxiv_id: '2506.12744'
source_url: https://arxiv.org/abs/2506.12744
tags:
- hate
- speech
- detection
- llms
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks large language models (LLMs) against traditional
  transformer models for hate speech detection, particularly in multilingual and code-mixed
  settings. The authors introduce IndoHateMix, a novel dataset capturing Hindi-English
  code-mixing and transliteration in Indian social media, to address the gap in existing
  benchmarks.
---

# Rethinking Hate Speech Detection on Social Media: Can LLMs Replace Traditional Models?

## Quick Facts
- arXiv ID: 2506.12744
- Source URL: https://arxiv.org/abs/2506.12744
- Reference count: 40
- Key outcome: Large language models, especially LLaMA-3.1-8B, consistently outperform fine-tuned BERT-based models for hate speech detection, even with minimal fine-tuning data, highlighting superior generalization in multilingual and code-mixed settings.

## Executive Summary
This study benchmarks large language models (LLMs) against traditional transformer models for hate speech detection, with a focus on multilingual and code-mixed settings. The authors introduce IndoHateMix, a novel dataset capturing Hindi-English code-mixing and transliteration in Indian social media, to address the gap in existing benchmarks. Comprehensive experiments across three datasets—HateXplain, ImplicitHate, and IndoHateMix—demonstrate that LLMs, especially LLaMA-3.1-8B, consistently outperform fine-tuned BERT-based models. Even when fine-tuned on just 10% of the data, LLaMA-3.1-8B surpasses BERT models trained on full datasets, highlighting superior generalization and adaptability in low-resource environments. Error analysis reveals that LLMs better handle indirect, sarcastic, and code-mixed expressions, while traditional models often misclassify due to sensitivity to surface-level cues. The findings suggest that future hate speech detection efforts should prioritize developing diverse datasets and leveraging general-purpose LLMs over task-specific models.

## Method Summary
The study benchmarks open-source LLMs (LLaMA3.1-8B, Mistral-7B, InternLM2.5-7B, Qwen2.5-7B) and traditional transformer models (mBERT, XLM-RoBERTa) on three datasets: HateXplain (12,375 samples), ImplicitHate (21,480 samples), and IndoHateMix (11,725 samples). All LLMs are fine-tuned using LoRA (rank=8, alpha=16) with a simple prompt template. BERT models are fine-tuned with standard hyperparameters (LR=2e-5, batch_size=2, epochs=3). Limited-data experiments (10% of IndoHateMix) are conducted to assess generalization. Performance is evaluated using Accuracy, Precision, Recall, and F1-Score.

## Key Results
- LLMs, especially LLaMA-3.1-8B, consistently outperform fine-tuned BERT-based models across all datasets.
- LLaMA3.1-8B fine-tuned on just 10% of IndoHateMix data outperforms mBERT fine-tuned on the full dataset.
- LLMs better capture indirect, sarcastic, and code-mixed hate speech, while BERT models struggle with short, informal, or indirect forms.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs generalize better to code-mixed and transliterated text due to exposure to diverse multilingual corpora during pretraining.
- **Mechanism:** Large-scale pretraining on heterogeneous web data exposes the model to informal syntax, variable grammar, and mixed-script patterns that mirror real-world social media. When fine-tuned with even minimal task-specific data, the model activates relevant subnetworks rather than learning patterns from scratch.
- **Core assumption:** The pretraining distribution contains sufficient signal for low-resource linguistic phenomena.
- **Evidence anchors:** Abstract states LLMs outperform BERT even with 10% data; section 4.4 notes LLaMA3.1-8B leverages pretrained multilingual knowledge in low-resource settings.
- **Break condition:** If pretraining data lacks representation of the target language pair or code-mixing patterns, generalization degrades.

### Mechanism 2
- **Claim:** LLMs capture indirect and sarcastic hate speech through deeper contextual reasoning, while BERT models rely on surface-level lexical cues.
- **Mechanism:** Encoder-only models like mBERT learn from local token co-occurrence and struggle when hateful intent is obfuscated. Decoder-based LLMs, trained with autoregressive objectives, accumulate richer discourse representations that support inference over implicit meaning.
- **Core assumption:** Implicit hate requires multi-sentence or world-knowledge inference, not just keyword detection.
- **Evidence anchors:** Section 4.5 shows mBERT frequently fails on short, informal, or indirect hate, while LLaMA3.1-8B demonstrates robust contextual and tonal understanding.
- **Break condition:** If prompts or fine-tuning data do not include examples of implicit or sarcastic hate, the model may default to literal interpretation.

### Mechanism 3
- **Claim:** Parameter-efficient fine-tuning (LoRA) enables strong performance with minimal trainable parameters, reducing overfitting risk on small datasets.
- **Mechanism:** LoRA injects low-rank adaptation matrices into frozen pretrained weights, allowing task-specific specialization without modifying the full model. This preserves generalization while adapting to domain-specific signals.
- **Core assumption:** The pretrained backbone already contains transferable representations; only a small subspace needs adjustment.
- **Evidence anchors:** Section 4.4 shows LLaMA3.1-8B with LoRA (4M trainable params) outperforms mBERT (179M params) on full dataset; section 4.2 lists LoRA config.
- **Break condition:** If LoRA rank is too low for task complexity, or if base model lacks relevant knowledge, adaptation fails.

## Foundational Learning

- **Concept: Code-Mixing and Transliteration**
  - **Why needed here:** IndoHateMix is built specifically for Hindi-English code-mixed text (e.g., "ye log born intelligent hain kya"). Models must handle mixed syntax and script variation.
  - **Quick check question:** Can you distinguish why a token-level model might fail on "a clever enemy is much better than a stupid friend" vs. "ye log born intelligent hain"?

- **Concept: LoRA (Low-Rank Adaptation)**
  - **Why needed here:** All LLM fine-tuning in the paper uses LoRA. Understanding rank, alpha, and freezing is essential for reproducibility.
  - **Quick check question:** If you set LoRA rank=1 for this task, what failure mode would you expect?

- **Concept: Implicit vs. Explicit Hate Speech**
  - **Why needed here:** ImplicitHate dataset tests models on subtle, stereotypical, or suggestive hate. Detection requires inference beyond keywords.
  - **Quick check question:** Why might a model trained only on explicit slurs misclassify "they only succeed in Western nations"?

## Architecture Onboarding

- **Component map:** Data layer (IndoHateMix, HateXplain, ImplicitHate) -> Model layer (mBERT/XLM-RoBERTa, LLaMA3.1-8B/Mistral-7B/InternLM2.5-7B/Qwen2.5-7B, GPT-4o-mini) -> Adaptation layer (LoRA rank=8, alpha=16) -> Evaluation layer (Accuracy, Precision, Recall, F1; error analysis)

- **Critical path:** 1) Dataset preprocessing (lowercase, remove URLs/emojis, filter 5–55 words) 2) Prompt template construction (consistent across all LLMs) 3) LoRA fine-tuning on 70% train split 4) Evaluation on held-out test set 5) Error analysis on misclassified examples

- **Design tradeoffs:** Open-source LLMs vs. proprietary APIs (open models allow fine-tuning; GPT-4o-mini is zero-shot only); Full fine-tuning vs. LoRA (LoRA reduces compute and overfitting but may underfit if rank is too low); Simple prompts vs. Chain-of-Thought (paper uses simple prompts as conservative baseline)

- **Failure signatures:** mBERT misclassifies short, informal, or sarcastic posts; over-relies on tone words; Perspective API fails on code-mixed or implicit hate; LLMs (undertrained) may default to literal interpretation if fine-tuning data lacks implicit examples

- **First 3 experiments:** 1) Reproduce the 10% data experiment: fine-tune LLaMA3.1-8B with LoRA on 10% of IndoHateMix and compare to mBERT on full data 2) Ablate LoRA rank: Test rank=1, 4, 8, 16 on IndoHateMix validation set to find efficiency-performance tradeoff 3) Prompt sensitivity check: Compare simple classification prompt vs. explicit target-aware prompt on ImplicitHate to quantify prompt engineering gains

## Open Questions the Paper Calls Out
- Should future hate speech detection research prioritize developing specialized models, or focus on curating richer, more varied datasets to enhance general-purpose LLMs?
- Can the superior performance of LLMs in Hindi-English code-mixed environments generalize effectively to other low-resource language pairs and transliteration scripts?
- Can quantized versions of larger language models maintain the superior accuracy observed in this study while offering the efficiency required for real-time, large-scale deployment?

## Limitations
- The IndoHateMix dataset is not publicly available, and its creation required scraping a specific platform, raising questions about representativeness and reproducibility.
- Model generalization to other low-resource or code-mixed languages is not directly evaluated, and pretraining data may not cover all linguistic phenomena.
- The study uses simple prompts and does not explore advanced prompt engineering, leaving open the question of further performance improvements.

## Confidence
- **High Confidence:** LLMs, especially LLaMA3.1-8B, consistently outperform fine-tuned BERT-based models across the tested datasets, even with minimal fine-tuning data.
- **Medium Confidence:** LLMs capture indirect and sarcastic hate speech better than traditional models due to deeper contextual reasoning.
- **Medium Confidence:** LoRA enables strong performance with minimal trainable parameters, reducing overfitting risk.

## Next Checks
1. Recreate the IndoHateMix dataset or collect similar code-mixed data from other platforms to test model generalization across sources and languages.
2. Systematically compare simple prompts with advanced prompt strategies (e.g., Chain-of-Thought, target-aware prompts) to quantify the impact of prompting on LLM performance for hate speech detection.
3. Evaluate models for bias (e.g., demographic parity) and robustness to adversarial examples or perturbations, especially in code-mixed and low-resource settings.