---
ver: rpa2
title: 'Hidden in Plain Text: Measuring LLM Deception Quality Against Human Baselines
  Using Social Deduction Games'
arxiv_id: '2601.13709'
source_url: https://arxiv.org/abs/2601.13709
tags:
- mafia
- game
- transcripts
- human
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares the deception quality of large language model
  (LLM) agents to human players in the social deduction game Mafia. The authors developed
  an asynchronous LLM Mafia environment using GPT-4o agents and created a Mafia Detector
  using GPT-4-Turbo to predict mafia roles from game transcripts.
---

# Hidden in Plain Text: Measuring LLM Deception Quality Against Human Baselines Using Social Deduction Games

## Quick Facts
- arXiv ID: 2601.13709
- Source URL: https://arxiv.org/abs/2601.13709
- Reference count: 0
- Primary result: LLM agents deceive more effectively than humans in Mafia, achieving 52.27% detection accuracy versus 58.62% for humans

## Executive Summary
This study evaluates whether large language model agents can deceive human observers more effectively than humans themselves in social deduction games. The researchers created an asynchronous Mafia environment using GPT-4o agents and developed a specialized Mafia Detector using GPT-4-Turbo to predict player roles from game transcripts. By comparing detection accuracy across 35 LLM games and 28 human games, the study demonstrates that LLMs achieve lower prediction accuracy, suggesting more sophisticated deception capabilities. The findings indicate that LLMs can engage in high-quality deception in natural language social contexts, with detection rates remaining above random chance but below what is achieved with human players.

## Method Summary
The researchers developed an asynchronous Mafia environment where GPT-4o agents played as either mafia or town members. Game transcripts were generated and processed to create a standardized dataset. A Mafia Detector model was trained using GPT-4-Turbo with specialized prompts designed to predict mafia roles from these transcripts. The detector was then applied to both LLM-generated games and human games to compare deception effectiveness. The study measured single-match prediction accuracy, which represents the model's ability to correctly identify mafia roles in individual games, and compared these results against random chance baselines.

## Key Results
- Mafia Detector achieved 52.27% accuracy on LLM games versus 58.62% on human games
- Both LLM and human games exceeded random chance prediction rates
- Lower detection accuracy on LLM games indicates more effective deception
- Study provides first quantitative comparison of LLM versus human deception quality

## Why This Works (Mechanism)
The study's approach works because social deduction games like Mafia inherently require deception as a core gameplay mechanic. LLMs trained on vast amounts of human dialogue can generate contextually appropriate deceptive language that mimics human behavior patterns. The asynchronous format allows agents to carefully craft responses without time pressure, potentially enabling more sophisticated deception strategies. The Mafia Detector's architecture leverages the same language model capabilities to analyze patterns in the transcripts, creating a direct comparison between generation and detection capabilities.

## Foundational Learning
- **Social deduction games**: Multiplayer games where hidden roles create information asymmetry - needed to establish controlled deception environment; quick check: Mafia rules specify hidden mafia trying to eliminate town
- **Deception quality metrics**: Quantitative measures of how well deceptive behavior evades detection - needed to compare LLM versus human performance; quick check: prediction accuracy below human baseline indicates better deception
- **Asynchronous gameplay**: Turn-based interaction without real-time constraints - needed for controlled LLM experiments; quick check: transcripts capture complete dialogue history
- **Role prediction models**: AI systems trained to classify player roles from text - needed to measure deception effectiveness; quick check: detector uses GPT-4-Turbo with specialized prompts
- **Natural language deception**: Use of language to mislead while maintaining coherence - needed to evaluate LLM capabilities; quick check: transcripts show plausible dialogue patterns
- **Statistical significance testing**: Methods to determine if accuracy differences are meaningful - needed to validate findings; quick check: results compared against random chance baseline

## Architecture Onboarding

**Component Map**: LLM Agents -> Game Environment -> Transcript Generation -> Mafia Detector -> Role Prediction

**Critical Path**: The essential workflow flows from LLM agent gameplay through transcript generation to Mafia Detector analysis. Each component must function correctly for the deception measurement to be valid. The detector's prediction accuracy directly measures deception quality, making it the output metric of interest.

**Design Tradeoffs**: The asynchronous format enables controlled experiments but may not capture real-time interaction dynamics. Using a single detector model provides consistency but introduces potential bias. Focusing on Mafia provides a clear framework but may limit generalizability to other deception contexts.

**Failure Signatures**: If detection accuracy approaches random chance for both LLM and human games, the experimental setup may be flawed. If LLM games show higher detection accuracy than human games, the LLMs may be generating detectable patterns. If transcripts lack coherent dialogue, the LLM agents may not be generating plausible deception.

**First 3 Experiments**:
1. Validate transcript quality by having humans rate LLM dialogue coherence
2. Test detector performance on synthetic games with known deception patterns
3. Compare single detector versus ensemble detector performance

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size (35 LLM games versus 28 human games) may limit statistical power
- Single detector model introduces potential bias from specific architecture and prompting
- Asynchronous gameplay may not capture real-time interaction dynamics
- Mafia-specific findings may not generalize to all deception scenarios

## Confidence
- **High confidence**: LLMs demonstrate sophisticated deception capabilities in natural language social contexts
- **Medium confidence**: LLM deception quality exceeds human baselines in the specific Mafia game context
- **Medium confidence**: The dataset provides a valuable resource for future LLM deception research

## Next Checks
1. Replicate findings with larger sample sizes and multiple detection models to assess statistical robustness
2. Conduct cross-validation across different social deduction games (e.g., Werewolf, Among Us) to test generalizability
3. Implement human-versus-model detection comparisons where humans attempt to identify LLM versus human players to validate the detector's findings