---
ver: rpa2
title: 'Entropic Causal Inference: Graph Identifiability'
arxiv_id: '2509.16463'
source_url: https://arxiv.org/abs/2509.16463
tags:
- graph
- balls
- plateau
- entropic
- mass
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work extends entropic causal inference to learn causal graphs
  with more than two variables. The key idea is to use bivariate entropic tests to
  determine ancestrality between a source node and its descendants, then apply a sequential
  peeling algorithm that iteratively discovers sources and orients edges.
---

# Entropic Causal Inference: Graph Identifiability

## Quick Facts
- arXiv ID: 2509.16463
- Source URL: https://arxiv.org/abs/2509.16463
- Reference count: 40
- Primary result: Extends entropic causal inference to learn causal graphs with more than two variables, proving identifiability under relaxed assumptions and showing superior performance to discrete additive noise models.

## Executive Summary
This work extends entropic causal inference to learn causal graphs with more than two variables by using bivariate entropic tests to determine ancestrality between source nodes and descendants. The method applies a sequential peeling algorithm that iteratively discovers sources and orients edges, proving identifiability under relaxed assumptions (H(E) = o(log log n) and (Ω(n), Ω(1/n log n))-support). Experiments show entropic methods outperform discrete additive noise models and often identify the true graph beyond its Markov equivalence class, with the true graph frequently having minimal total entropy among DAGs consistent with the skeleton.

## Method Summary
The method uses bivariate entropic tests to determine ancestrality between a source node and its descendants, then applies a sequential peeling algorithm that iteratively discovers sources and orients edges. The algorithm maintains a set of candidate source nodes, uses pairwise entropic tests to eliminate non-sources, and recurses on the remaining subgraph to find the next layer of sources. A heuristic enumeration algorithm is also provided for small graphs, which brute-forces all DAGs in the equivalence class to minimize total entropy. The core mechanism relies on entropic asymmetry in minimum entropy coupling, where the true causal direction requires less randomness than the reverse direction.

## Key Results
- Entropic methods outperform discrete additive noise models on synthetic and semi-synthetic data
- The true graph frequently has minimal total entropy among DAGs consistent with the skeleton
- Sequential peeling algorithm achieves identifiability under relaxed support assumptions
- Enumeration heuristic often identifies the true DAG beyond its Markov equivalence class

## Why This Works (Mechanism)

### Mechanism 1: Entropic Asymmetry in Minimum Entropy Coupling (MEC)
If the true causal mechanism X → Y relies on low-entropy exogenous noise, the minimum entropy required to model Y conditioned on X will be strictly lower than the reverse direction X|Y. The system computes the Minimum Entropy Coupling (MEC) for the conditional distributions in both causal directions and identifies the direction where the mechanism acts as the "simplest explanation" (lowest randomness). This requires the true exogenous noise E to have small entropy (H(E) = o(log log n)) and the cause variable X to have (Ω(n), Ω(1/n log n))-support. If the underlying exogenous noise has high entropy (approaching log n), the asymmetry vanishes and the direction becomes unidentifiable.

### Mechanism 2: The Source-Pathwise Oracle Property
The pairwise entropic test functions as a "source-pathwise oracle," reliably identifying an edge orientation A → B if A is a source node and B is a descendant, even within a larger graph. When testing a source node Xsrc against a descendant Y, the noise variables of other intervening nodes do not fully obscure the signal. The "helpful mass" (probability mass allowing for identification) survives through the graph structure sufficiently to differentiate it from the reverse direction. This property relies on structural equations being sampled uniformly at random and the graph size |V| being constant relative to state size n. If the graph depth is very large or noise accumulates destructively, the "helpful mass" may dilute before reaching the descendant, breaking the oracle property.

### Mechanism 3: Sequential Peeling via Lexicographical Ordering
A valid causal DAG can be reconstructed by iteratively identifying "source" nodes and removing them from the candidate set. The algorithm maintains a set of candidate source nodes, uses pairwise entropic tests to eliminate non-sources, and once true sources are identified and "peeled" (conditioned on), recurses on the remaining subgraph to find the next layer of sources. This requires faithfulness (nodes with directed paths are dependent) and the reliability of the source-pathwise oracle. If the oracle returns a false positive, the lexicographical ordering is corrupted, leading to incorrect edge orientations in subsequent layers.

## Foundational Learning

- **Concept: Structural Causal Models (SCMs) with Exogenous Noise**
  - Why needed here: The entire theory relies on the assumption Xi = fi(Pai, Ei) where Ei is independent noise. If this generative model does not hold (e.g., cyclic relations or hidden confounders), the entropic assumptions are violated.
  - Quick check question: Can you explain why the entropy of Ei determines the "simplicity" of the causal mechanism fi?

- **Concept: Minimum Entropy Coupling (MEC)**
  - Why needed here: MEC is the core subroutine used to compare causal directions. You must understand that coupling involves finding a joint distribution consistent with marginals that minimizes entropy.
  - Quick check question: Given two conditional distributions P(Y|X) and P(X|Y), how does the MEC solver help decide if X causes Y?

- **Concept: Markov Equivalence Class (MEC) vs. Total Order**
  - Why needed here: Traditional methods (PC/GES) learn the Markov Equivalence Class (skeleton/v-structures), leaving many edges undirected. This method attempts to orient edges within that class using entropy. Note: MEC here refers to the equivalence class, while MEC in the algorithm refers to Minimum Entropy Coupling.
  - Quick check question: Why is entropic inference necessary for orienting edges in a triangle graph (X → Y → Z, X → Z) when conditional independence tests alone cannot?

## Architecture Onboarding

- **Component map:** Skeleton Learner (CI tests) -> MEC Oracle (Bivariate orientation) -> Peeling Controller (Iterative source discovery) -> Heuristic Enumerator (Total entropy minimization)
- **Critical path:** Input joint distribution → Estimate skeleton via CI tests → Run Peeling Controller: for each pair, query MEC Oracle → If Oracle indicates A → B, mark B as non-source → Isolate remaining sources, fix them, repeat on remaining variables → Output fully oriented DAG
- **Design tradeoffs:** Use Peeling for theoretical guarantees and larger graphs; use Enumeration for small graphs where minimizing total graph entropy empirically outperforms the sequential heuristic. Finite samples vs theory: proofs assume exact distributions, but experiments use samples requiring regularization or specific entropy estimators.
- **Failure signatures:** High Noise Entropy (H(E) ≈ log(n) breaks asymmetry), Data Scarcity (few samples cause biased entropy estimators), Cyclic/Confounded Data (violated Pa_i ⊥ E_i assumption breaks MEC oracle).
- **First 3 experiments:** 1) Triangle Graph Verification: Generate synthetic data for triangle graph, measure SHD vs noise entropy. 2) Ablation on Support: Test identifiability with low vs high support cause variables. 3) Semi-Synthetic Benchmark: Run Entropic Enumeration on small bnlearn networks to verify true graph has minimum entropy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can entropic identifiability be proven for causal graphs when exogenous variable entropy scales as Θ(log log n) or higher?
- Basis: The conclusion notes that H(E_i) = Θ(log(log n)) acts as an approximate phase transition for the current proof method, requiring novel tools for extension.
- Why unresolved: The "balls-and-bins" analysis relies on noise being o(log log n) to ensure sufficient "helpful" mass exists; higher noise breaks this probability concentration.
- What evidence would resolve it: A theoretical proof showing the MEC oracle remains a source-pathwise oracle even when noise entropy is proportional to log log n.

### Open Question 2
- Question: Under what specific generative conditions is the true causal graph guaranteed to minimize total entropy (H(X) + H(E)) rather than just exogenous entropy?
- Basis: The authors state that future work involves framing "Under what conditions is the true generative model the most information-theoretically efficient way to produce a distribution?" specifically regarding the total entropy criterion.
- Why unresolved: The proposed heuristic uses total entropy and performs well empirically, but the theoretical identifiability results only cover minimizing exogenous entropy.
- What evidence would resolve it: A theorem identifying the necessary constraints on structural functions fi for the total entropy criterion to imply identifiability.

### Open Question 3
- Question: What structural or distributional properties of real-world Bayesian networks cause the true graph to act as an entropy minimizer (e.g., Alarm) versus an entropy maximizer (e.g., Insurance)?
- Basis: The authors find it "peculiar" that for the Insurance network, the true graph is often the entropy maximizer among sampled orientations, noting this phenomenon requires "a better understanding."
- Why unresolved: The core assumption of the framework is that nature prefers low entropy; counter-examples suggest the assumption is not universal and the boundary conditions are unknown.
- What evidence would resolve it: An analysis correlating specific graph features (e.g., edge density, node degree) or CPT smoothness with the entropy rank of the true graph structure.

## Limitations
- Core mechanism relies entirely on theoretical proofs within this paper, as corpus neighbors focus on different causal inference frameworks
- Finite-sample behavior of MEC estimation remains unclear, with experiments mentioning Dirichlet parameter tuning but not specifying exact entropy estimation procedure
- Source-pathwise oracle property's robustness to larger graphs or accumulated noise is theoretically limited to constant |V| cases
- Method assumes discrete/categorical variables with specific support requirements

## Confidence
- **High confidence**: Sequential peeling algorithm correctness (Theorem 5.6 provides clear proof), synthetic experiment design (controlled H(E) in triangle/line graphs)
- **Medium confidence**: Relaxed support assumptions (Ω(n), Ω(1/n log n)) practically achievable, enumeration heuristic finding true DAGs in semi-synthetic data
- **Low confidence**: MEC oracle reliability in finite samples, source-pathwise oracle property beyond constant-size graphs, generalization to real-world continuous variables

## Next Checks
1. **Ablation on entropy thresholds**: Generate synthetic data varying H(E) from 0 to log log n, measure source identification accuracy to locate the theoretical break point
2. **Support sensitivity analysis**: Test peeling algorithm on cause variables with controlled support (Ω(n) vs o(n)) to validate the relaxed assumptions empirically
3. **Real-world scalability test**: Apply enumeration heuristic to medium-sized bnlearn networks (4-6 nodes) to verify if true DAG consistently has minimum total entropy across diverse graph structures