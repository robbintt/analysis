---
ver: rpa2
title: Sharpness-Aware Minimization Can Hallucinate Minimizers
arxiv_id: '2509.21818'
source_url: https://arxiv.org/abs/2509.21818
tags:
- minimizers
- hallucinated
- theorem
- minimizer
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors analyze Sharpness-Aware Minimization (SAM), which\
  \ aims to find flatter minima for better generalization. A key observation is that\
  \ SAM\u2019s practical implementation uses a shifted gradient \u2207f(x+) instead\
  \ of the surrogate gradient \u2207fSAM(x), potentially causing it to stall at points\
  \ where \u2207f(x+) = 0 despite \u2207f(x) \u2260 0."
---

# Sharpness-Aware Minimization Can Hallucinate Minimizers

## Quick Facts
- arXiv ID: 2509.21818
- Source URL: https://arxiv.org/abs/2509.21818
- Reference count: 4
- Primary result: SAM can stall at non-stationary points ("hallucinated minimizers") due to shifted-gradient implementation, but SGD warm-start mitigates this failure.

## Executive Summary
This paper identifies a critical failure mode in Sharpness-Aware Minimization (SAM): the standard implementation can stall at points where the shifted gradient ∇f(x⁺) = 0 despite ∇f(x) ≠ 0. These "hallucinated minimizers" occur when the perturbation radius ρ bridges a local maximizer and minimizer in nonconvex landscapes. The authors prove their existence under simple geometric conditions and show they can be locally attracting. Empirically, they demonstrate this failure in neural network training and validate that a short SGD warm-start before enabling SAM effectively mitigates the problem.

## Method Summary
The study analyzes SAM's standard two-step implementation where x⁺ = x + ρ∇f(x)/∥∇f(x)∥ and updates use ∇f(x⁺). They prove hallucinated minimizers exist when this shifted gradient vanishes at points where the true gradient doesn't, specifically when ρ aligns with geometric configurations containing both local maxima and minima. Experiments use full-batch MNIST training with 2-layer Tanh networks and CIFAR-100/ResNet-18, measuring training loss, test accuracy, and diagnostic metrics like GradRatio = ∥∇f(x⁺)∥/(∥∇f(x)∥+ε) and LossGap = f(x⁺)−f(x). The SGD warm-start remedy runs standard SGD for first 10% of training before switching to SAM.

## Key Results
- SAM's practical implementation can stall at non-stationary points where ∇f(x⁺) = 0 but ∇f(x) ≠ 0
- Hallucinated minimizers exist under simple nonconvex landscape conditions (coexistence of local minimizer and maximizer) for nontrivial ρ intervals
- These points can be locally attracting for SAM's discrete-time dynamics and form smooth manifolds
- SGD warm-start before enabling SAM mitigates this failure mode and reduces ρ sensitivity

## Why This Works (Mechanism)

### Mechanism 1: Shifted-Gradient Stalling
SAM's practical implementation uses ∇f(x⁺) instead of the true surrogate gradient ∇fSAM(x). When ∇f(x⁺) = 0 but ∇f(x) ≠ 0, updates stop, creating a "hallucination" of a minimum where none exists for the original function.

### Mechanism 2: Nonconvex Landscape Alignment
Hallucinated minimizers form when perturbation radius ρ bridges a local maximizer and minimizer. If ∇f(x_h)/∥∇f(x_h)∥ points directly toward a true minimizer x* at distance ρ, then x_h⁺ = x* and ∇f(x_h⁺) = 0 while ∇f(x_h) ≠ 0.

### Mechanism 3: Basin Escape via Warm-Start
SGD warm-start prevents convergence to hallucinated minimizers by moving iterates out of their basins of attraction. Hallucinated minimizers often lie near local maximizers (high-loss regions) that SGD naturally descends from rapidly.

## Foundational Learning

- **Gradient-based Optimization vs. Surrogate Loss Differentiation**: The paper hinges on the distinction between differentiating inside the ascent step versus evaluating the gradient after the step. Quick check: Does the chain rule apply to the step x_{k+1} = x_k − η∇f(x_k + ρ∇f(x_k))? (Answer: No, standard implementation treats the inner ascent as fixed.)

- **Local vs. Global Attractors in Discrete Dynamics**: Hallucinated points can be locally stable for a specific discrete update rule even if not stationary points of the underlying function. Quick check: If x_{k+1} = x_k, is x_k necessarily a critical point of f? (Answer: No, it's a fixed point of the update operator.)

- **Loss Landscape Geometry (Minima vs. Maxima)**: The vector field near a maximizer (where gradients point inward toward the peak) can also point toward surrounding minima. Quick check: In a nonconvex function with a local maximum at x_max and minimum at x_min, how does the gradient direction at points near x_max relate to x_min?

## Architecture Onboarding

- **Component map**: Input Params(x) → Compute Gradient(g) → Ascent Step(x⁺ = x + ρg/∥g∥) → Evaluate Gradient(g⁺) → Descent Step(x ← x − ηg⁺)

- **Critical path**: The Ascent Step (x → x⁺) determines where the gradient is evaluated, creating the separation between "gradient calculation point" and "parameter update point" that enables hallucination.

- **Design tradeoffs**:
  - Exact Surrogate Gradient: Requires Hessian-vector products (computationally expensive) but theoretically aligns with fSAM objective
  - Shifted Gradient (Standard): Computationally cheap (2 forward/backward passes) but prone to hallucinated minimizers at large ρ

- **Failure signatures**:
  - Stalling at High Loss: Training loss plateaus at positive value
  - Gradient Mismatch: ∥∇f(x⁺)∥ ≈ 0 while ∥∇f(x)∥ remains significantly > 0
  - ρ Sensitivity: Failure only occurs when ρ ≥ ∥x* − x_•∥

- **First 3 experiments**:
  1. Sanity Check (Toy Data): Construct 1D/2D function with max/min pair, run SAM with increasing ρ to observe convergence to predicted hallucinated point, verify ∇f(x) ≠ 0 at convergence
  2. Diagnostic Sweep (Real Data): Train small MLP on MNIST using full-batch SAM, log GradRatio = ∥∇f(x⁺)∥/∥∇f(x)∥, plot final training loss vs. ρ to identify "cliff" where GradRatio drops to zero while loss remains high
  3. Remedy Validation: Implement SGD warm-start (switch to SAM after 10% training), compare robustness across ρ sweep on CIFAR-100/ResNet-18

## Open Questions the Paper Calls Out

### Open Question 1
Do hallucinated minimizers persist as stable attractors in stochastic mini-batch training, or does gradient noise provide a natural escape mechanism? The theoretical proofs rely on deterministic, full-batch dynamics, but stochastic gradients may introduce variance that prevents the precise convergence required for stalling.

### Open Question 2
Do adaptive SAM variants (e.g., ASAM, GSAM) suppress the formation of hallucinated minimizers, or do they inherit this failure mode? Variants that modify the perturbation direction or magnitude may avoid the alignment of gradients that causes stalling.

### Open Question 3
Can the existence of hallucinated minimizers be formally proven for nonsmooth architectures like ReLU networks? Current proofs rely on inverse function theorem and Hessian calculations requiring C² smoothness, which exclude ReLU networks with undefined second derivatives at kinks.

## Limitations
- Theoretical analysis assumes continuous differentiability and specific geometric configurations that may not capture all real neural network loss landscape complexities
- SGD warm-start remedy is primarily validated on vision tasks and may not generalize to all architectures or optimization landscapes
- Precise conditions for hallucinated minimizer existence (exact ρ thresholds) may be too restrictive for practical application across diverse architectures

## Confidence

- **High confidence**: Core mechanism of shifted-gradient stalling is well-founded theoretically and experimentally demonstrated with clear observable signatures
- **Medium confidence**: SGD warm-start remedy shows consistent empirical benefits but needs broader validation across diverse architectures
- **Low confidence**: Precise conditions for hallucinated minimizer existence are mathematically rigorous but may not generalize cleanly to all practical scenarios

## Next Checks

1. Test hallucinated minimizer phenomenon and SGD warm-start remedy on non-vision architectures (Transformers, graph neural networks) to assess cross-domain robustness
2. Compare SGD warm-start against alternative fixes (adaptive ρ schedules, momentum damping, exact surrogate gradient computation) to identify optimal mitigation strategy
3. Use visualization or topological analysis to characterize actual neural network loss landscape geometry and quantify how often theoretical conditions naturally occur during training