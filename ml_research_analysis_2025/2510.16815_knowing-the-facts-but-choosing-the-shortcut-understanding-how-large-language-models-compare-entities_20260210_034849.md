---
ver: rpa2
title: 'Knowing the Facts but Choosing the Shortcut: Understanding How Large Language
  Models Compare Entities'
arxiv_id: '2510.16815'
source_url: https://arxiv.org/abs/2510.16815
tags:
- social
- atoms
- cities
- case
- buildings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) use
  numerical knowledge or surface-level heuristics when comparing entities along numerical
  attributes. The authors ask models to compare entities (e.g., "Which river is longer,
  the Danube or the Nile?") and analyze the consistency between their pairwise predictions
  and their own extracted numerical values.
---

# Knowing the Facts but Choosing the Shortcut: Understanding How Large Language Models Compare Entities

## Quick Facts
- arXiv ID: 2510.16815
- Source URL: https://arxiv.org/abs/2510.16815
- Reference count: 40
- Primary result: LLMs possess numerical knowledge but frequently rely on surface-level heuristics (popularity, order, co-occurrence) when comparing entities, with larger models selectively deploying numerical knowledge when it is more reliable.

## Executive Summary
This paper investigates whether large language models (LLMs) use numerical knowledge or surface-level heuristics when comparing entities along numerical attributes. Despite having accurate numerical values for entities, LLMs frequently make pairwise comparison predictions that contradict their own extracted numbers. The authors identify three heuristic biases—entity popularity, mention order, and semantic co-occurrence—that strongly influence model predictions, even overriding their own numerical knowledge. Crucially, larger models (32B parameters) selectively rely on numerical knowledge when it is more reliable, while smaller models (7-8B parameters) show no such discrimination, explaining why larger models outperform smaller ones even when smaller models possess more accurate knowledge. Chain-of-thought prompting steers all models toward using numerical features across all model sizes.

## Method Summary
The study uses a Balanced-Orthogonal Subset (BOS) design to isolate individual heuristic effects when LLMs compare entities along numerical attributes. Models are first prompted to extract numerical values for entities (using three templates per attribute), then prompted to make pairwise comparisons (using 12 prompts per pair). The BOS ensures each heuristic cue varies independently at 50% frequency. A meta-predictor (logistic regression) uses surface cues (popularity, position, co-occurrence) to classify model predictions into four cases based on alignment with numerical knowledge. The authors analyze how feature importance and model behavior vary across model sizes (1B, 7B, 8B, 32B) and with chain-of-thought prompting.

## Key Results
- Numerical extraction accuracy is consistently higher than pairwise comparison accuracy, revealing a knowledge-reasoning gap.
- Surface-level heuristics (popularity, mention order, co-occurrence) strongly influence model predictions, often overriding their own numerical knowledge.
- For smaller models (7-8B), surface cues predict pairwise choices better than extracted numerical values.
- Larger models (32B) selectively deploy numerical knowledge when it is more reliable, while smaller models show no such discrimination.
- Chain-of-thought prompting shifts models toward using numerical features but doesn't improve the underlying numerical knowledge quality.

## Why This Works (Mechanism)

### Mechanism 1: Heuristic Override of Numerical Knowledge
LLMs possess accurate numerical knowledge but default to surface-level heuristics for pairwise comparisons, creating a gap between what models know and how they behave. Three shortcuts—entity popularity (higher QRank → assumed larger), mention order (position bias favoring first/second slot), and semantic co-occurrence (cosine similarity to magnitude descriptors)—compete with numerical reasoning. When these cues conflict with numerical values, models often follow the shortcut rather than their own extracted numbers. The BOS design successfully isolates each cue from confounds, though unmeasured confounders remain possible.

### Mechanism 2: Scale-Dependent Strategy Selection (Meta-Cognitive Emergence)
Larger models (32B) selectively deploy numerical knowledge when it is more reliable, while smaller models (7-8B) show no such discrimination—this explains scale-based performance improvements despite similar knowledge accuracy. Cohen's d analysis shows larger models have larger NumEx-diff values in Case 1 (numerical reasoning) versus Case 3 (surface cues), indicating they avoid numerical reasoning when knowledge is noisy. This meta-cognitive ability to detect knowledge reliability and adjust strategy accordingly emerges with scale.

### Mechanism 3: Chain-of-Thought as Strategy Shifter (Not Knowledge Improver)
CoT prompting shifts which signal the model follows at comparison time without improving the underlying numerical knowledge quality—internal consistency rises because decisions align more often with existing numbers. CoT increases Case 1 (numerical reasoning) by ~6 percentage points and Case 2 by ~5 pp, while shrinking Case 3 (surface cues). The reasoning traces often show post-hoc rationalization rather than genuine computation.

## Foundational Learning

- **Concept: Internal Consistency vs. Accuracy**
  - Why needed here: The paper's central finding requires distinguishing between *what the model knows* (numerical accuracy) and *whether it uses that knowledge* (internal consistency). Without this distinction, you cannot diagnose whether failures stem from knowledge gaps or reasoning failures.
  - Quick check question: If a model correctly extracts values X=100 and Y=50 but answers "Y is larger," is the failure in knowledge or consistency?

- **Concept: Balanced-Orthogonal Experimental Design**
  - Why needed here: The paper isolates individual heuristic effects by constructing a BOS where each cue varies independently at 50% frequency. This prevents Simpson's paradox and ensures risk ratios reflect causal contributions, not correlated confounds.
  - Quick check question: Why can't we simply measure accuracy when popularity aligns with ground truth versus when it doesn't, without orthogonalization?

- **Concept: Effect Size Interpretation (Cohen's d, Risk Ratios)**
  - Why needed here: The paper uses Cohen's d to quantify how strongly features differ between Case 1 and Case 3, and risk ratios to measure how much each cue lifts accuracy. Understanding these metrics is essential for interpreting whether an effect is practically meaningful versus statistically significant.
  - Quick check question: A Cohen's d of 0.8 means the Case 1 mean is how many standard deviations above the Case 3 mean?

## Architecture Onboarding

- **Component map:**
  - Numerical Extraction Pipeline -> Pairwise Comparison Module -> Meta-Predictor -> BOS Constructor

- **Critical path:**
  1. Generate entity pairs → stratified sampling from high/low bins
  2. Run numerical extraction → compute accuracy, SMAPE, CV
  3. Run pairwise prompts → parse responses, compute accuracy and internal consistency
  4. Construct BOS → compute risk ratios for each cue
  5. Train meta-predictor → classify cases (1-4), analyze feature contributions via Cohen's d

- **Design tradeoffs:**
  - **Greedy decoding vs. sampling**: Paper uses greedy for reproducibility; may underestimate model diversity. Alternative: nucleus sampling with multiple runs.
  - **Regex parsing vs. LLM-based parsing**: Deterministic but brittle to output format variations. Alternative: use secondary LLM as parser with validation.
  - **CoT budget (1024 tokens)**: Sufficient for Qwen3 but may truncate longer reasoning in other models. Alternative: adaptive budget or streaming until end-of-thought token.

- **Failure signatures:**
  - **High pairwise fail rate, low numerical fail rate**: Classic heuristic override; check popularity/order alignment.
  - **Large CV (>0.3)**: Model uncertain about numerical values; expect more Case 3 behavior.
  - **Inter-polarity accuracy gap > 5%**: Prompt sensitivity; consider prompt ensembling.
  - **Case 4 rate > 20%**: Unmodeled heuristics or noise; investigate additional surface features.

- **First 3 experiments:**
  1. **Replicate BOS risk ratios on your target model**: Use the paper's codebase (linked in abstract) to generate BOS for your model. If order RR > 1.5, position bias is dominant and requires prompt randomization.
  2. **Numerical extraction stability test**: Prompt the model with 3 paraphrased numerical templates, compute CV. If CV > 0.2, the model's knowledge is noisy—expect more shortcut reliance.
  3. **CoT ablation**: Run pairwise comparisons with and without thinking tokens on the same entity pairs. If internal consistency improves but numerical accuracy is unchanged, CoT is acting as a strategy shifter (confirm with case distribution analysis).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does mechanistically editing internal numerical knowledge representations shift the model's decision-making from heuristic shortcuts to faithful numerical reasoning?
- Basis in paper: [explicit] The authors state in the Limitations section that "it would be interesting to see whether (or under which conditions) updating the numerical knowledge inside models would alter their pairwise judgments," noting the study was limited to output analysis.
- Why unresolved: The paper establishes that models often ignore known facts for heuristics, but it does not determine if this "choice" is structurally ingrained or contingent on the accuracy/availability of the internal representation.
- What evidence would resolve it: Applying model editing techniques (e.g., ROME) to modify specific numerical values and observing if predictions transition from "Case 3" (heuristic-driven) to "Case 1" (knowledge-driven).

### Open Question 2
- Question: Do the identified heuristic biases (popularity, order, co-occurrence) persist or intensify in more complex reasoning tasks, such as multi-hop reasoning or multi-entity ranking?
- Basis in paper: [explicit] The Limitations section notes that the controlled setting limits generalizability, explicitly listing "multi-entity ranking" and "multi-hop reasoning" as areas where findings may not fully transfer.
- Why unresolved: While the pairwise setting offers control, it is unclear if the "meta-cognitive" strategy selection observed in larger models functions when the model must synthesize multiple facts or rank lists longer than two items.
- What evidence would resolve it: Extending the Balanced-Orthogonal Subset (BOS) design to multi-hop QA datasets or list-wise ranking tasks to measure the risk ratios of the same surface cues.

### Open Question 3
- Question: Can supervised fine-tuning or few-shot prompting permanently mitigate the reliance on surface cues, or are these heuristics robust to standard alignment techniques?
- Basis in paper: [explicit] The authors mention that "it would be interesting to study whether the biases persist after fine-tuning models on ranking tasks" and noted they limited the study to zero-shot prompting despite preliminary evidence that few-shot prompting might help.
- Why unresolved: It is unknown if the "strategy selection" capability—where larger models favor numerical knowledge—is a static property of scale or a behavior that can be trained into smaller models.
- What evidence would resolve it: Evaluating the performance of models before and after fine-tuning on the paper's datasets to see if the predictive power of the bias-based meta-predictor decreases relative to numerical accuracy.

## Limitations

- The study's findings rely on the reliability of the numerical extraction pipeline; systematic biases could artifactually create the observed knowledge-reasoning gap.
- The BOS design assumes the four orthogonal cues capture the full space of potential heuristics; unmeasured shortcuts could contribute to Case 3 behavior.
- The CoT analysis is limited to Qwen3 models with a fixed 1024-token budget; results may not generalize to other architectures or token constraints.

## Confidence

**High Confidence**: The numerical accuracy vs. pairwise accuracy gap is well-established (d ≈ 0.79 for 32B vs. d ≈ 0.04 for 1B), supported by robust effect sizes and internal consistency improvements with CoT. The heuristic biases (popularity, order, co-occurrence) show consistent risk ratios >1.5 across models and attributes.

**Medium Confidence**: The scale-dependent strategy selection mechanism is compelling but relies on the Case 1 vs. Case 3 classification being the primary driver of performance differences. Alternative explanations cannot be fully ruled out.

**Low Confidence**: The CoT-as-strategy-shifter claim is weakest, as it's based on a single model family and the qualitative analysis of reasoning traces is inherently subjective.

## Next Checks

1. **Cross-Architecture CoT Validation**: Replicate the CoT experiment on at least two additional model families (e.g., Llama, GPT) to test whether the strategy-shifting mechanism generalizes beyond Qwen3.

2. **Heuristic Discovery Expansion**: Systematically probe for additional surface cues by training a more flexible model (e.g., random forest or neural classifier) on the BOS data and analyzing feature importance—this could reveal shortcuts not captured by the four orthogonal cues.

3. **Numerical Extraction Ablation**: Design a controlled experiment where numerical extraction is performed with higher-precision methods (e.g., ensemble predictions, uncertainty-aware extraction) and measure whether this reduces Case 3 behavior, thereby testing whether the knowledge-reasoning gap is due to extraction noise versus genuine heuristic override.