---
ver: rpa2
title: Spectral Bias in Variational Quantum Machine Learning
arxiv_id: '2506.22555'
source_url: https://arxiv.org/abs/2506.22555
tags:
- frequency
- fourier
- encoding
- gradient
- spectral
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work analyzes spectral bias in quantum machine learning models,
  specifically parameterized quantum circuits (PQCs), using Fourier series representations.
  The authors prove that spectral bias arises from the "redundancy" of Fourier coefficients,
  which is the number of terms contributing to the same frequency component.
---

# Spectral Bias in Variational Quantum Machine Learning

## Quick Facts
- arXiv ID: 2506.22555
- Source URL: https://arxiv.org/abs/2506.22555
- Reference count: 40
- One-line primary result: Spectral bias in PQCs arises from redundancy in Fourier coefficient contributions, with encoding scheme choice determining frequency-dependent learning rates.

## Executive Summary
This work analyzes spectral bias in variational quantum machine learning models, specifically parameterized quantum circuits (PQCs), using Fourier series representations. The authors prove that spectral bias arises from the "redundancy" of Fourier coefficients—the number of terms contributing to the same frequency component. They show that frequency components with higher redundancy exhibit larger gradient magnitudes during training and are more robust to parameter perturbations. The choice of data encoding scheme determines the redundancy distribution. Experiments across multiple encoding schemes confirm that models with uniform redundancy (e.g., exponential Pauli encoding) learn all frequencies at similar rates, while those with steep redundancy decay (e.g., constant Pauli encoding) show slower convergence at higher frequencies.

## Method Summary
The authors use reuploader circuits with L=20 layers and 5 qubits, training on Fourier sums with 2048 equally spaced points in x ∈ [0, 2π]. Target functions are constructed as h(x) = Σ A_ω sin(ωx + φ_ω) with frequencies {5, 10, 15, 20, 25, 30, 35, 40, 45, 50} rad/s and random phases. The model uses trainable Ry(θ)+Rx(θ) gates with data encoding Rx(βx), observable O=Z₁, and Adam optimizer with lr=0.0005. They analyze spectral dynamics by tracking normalized Fourier coefficients |f̃_ω|/A_ω at each frequency over training iterations across 10 random seeds.

## Key Results
- Spectral bias severity is determined by the redundancy distribution R(ω), which depends on the encoding scheme's coefficients
- Exponential Pauli encoding (βᵢ = 3ⁱ) achieves uniform learning rates across frequencies, while constant Pauli encoding (βᵢ = 1) shows pronounced low-frequency bias
- Large initialization variance (σ² = 10) suppresses Fourier coefficient magnitudes across all frequencies but disproportionately affects high frequencies with low redundancy
- Increased entanglement structure (more CNOT gates) reduces spectral bias by increasing the number of parameters in the lightcone of measurement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Frequencies with higher redundancy R(ω) can exhibit larger gradients during training, leading to faster convergence for those frequency components.
- Mechanism: The gradient of the loss at frequency ω is bounded by |∂θL(ω)| ≤ 4R(ω)||O||_tr |c_Dω|. Since R(ω) scales the upper bound, frequencies with more contributing terms (higher redundancy) can receive proportionally larger gradient updates. Under small-angle initialization (σ ≪ 1), the expected gradient magnitude is approximately E[|∂θ_k L(ω)|] ≲ 2|c_ω^h| × (sum over R(ω) paths) × σ^(S(r)-1), showing linear dependence on redundancy.
- Core assumption: Parameters are initialized from a Gaussian distribution with small variance; target function spectrum matches the model's accessible spectrum.
- Evidence anchors:
  - [abstract] "The magnitude of the Fourier coefficients' gradients during training strongly correlates with the coefficients' redundancy."
  - [section 4, Theorem 3] "Demonstrating Fourier coefficients with high redundancy can potentially elicit stronger gradient signals in expectation."
  - [corpus] Limited direct evidence on PQC spectral bias specifically; related work on neural operator spectral bias (arXiv:2503.13695) addresses mitigation strategies but not the redundancy mechanism.
- Break condition: Large-angle initialization breaks the small-σ approximation; trainable encodings change spectrum during training, invalidating fixed-spectrum assumptions.

### Mechanism 2
- Claim: The encoding scheme's redundancy distribution directly determines spectral bias severity—uniform redundancy yields uniform learning rates; steep decay yields strong low-frequency bias.
- Mechanism: Encoding coefficients β_i control how eigenvalue differences Λ_k - Λ_j map to frequencies ω. With constant encoding (β_i = 1), many index pairs map to low frequencies (high R(ω)), few to high frequencies (low R(ω)). Exponential encoding (β_i = 3^i) spreads pairs more uniformly across the spectrum, equalizing R(ω) across frequencies.
- Core assumption: Single-qubit encoding gates with fixed (non-trainable) encoding parameters; integer-frequency spectrum.
- Evidence anchors:
  - [abstract] "models with uniform redundancy (e.g., exponential Pauli encoding) learn frequencies at equal rates, while those with steep redundancy decay (e.g., constant Pauli encoding) show pronounced spectral bias"
  - [section 5.1, Figure 4] Shows constant encoding takes ~8000 iterations for high frequencies vs. exponential encoding achieving equal learning rates across all frequencies by ~2000 iterations.
  - [corpus] No direct corpus evidence on encoding-redundancy relationships; assumption remains specific to this paper's theoretical framework.
- Break condition: Trainable encoding parameters; mid-circuit measurements; multi-qubit encoding gates alter the eigenvalue structure.

### Mechanism 3
- Claim: Higher entanglement and smaller initialization variance reduce spectral bias by increasing gradient contributions at high frequencies.
- Mechanism: Entanglement increases the number of parameters in the lightcone of measurement, reducing excluded parameters and spreading gradient contributions. Large initialization (σ ~ 10) suppresses Fourier coefficient magnitudes across all frequencies but disproportionately affects high frequencies that already have low redundancy, compounding their slow learning.
- Core assumption: Hermitian observable O; CNOT-based entanglement; no weight-sharing in variational parameters.
- Evidence anchors:
  - [section 5.3] "an increase in the number of CNOT reduces the effect of spectral bias, even if the CNOT gates are randomly placed"
  - [section 5.4, Figure 8] σ² = 10 initialization shows severe spectral bias (high frequencies barely learned by 9000 iterations) vs. σ² = 0.01 showing modest bias.
  - [corpus] Indirect support from PINN literature (arXiv:2510.05385) on initialization affecting spectral learning, but no quantum-specific validation.
- Break condition: Strongly correlated perturbation responses (ρ → 1) eliminate redundancy's robustness benefit; hardware noise may introduce unmodeled correlations.

## Foundational Learning

- Concept: **Fourier series representation of PQCs**
  - Why needed here: The entire theoretical framework depends on expressing PQC outputs as f(x) = Σ_ω c_ω e^(iωx), where frequencies ω come from encoding gate eigenvalues.
  - Quick check question: Given an encoding gate e^(-iβxZ), what frequencies can appear in the circuit output?

- Concept: **Neural Tangent Kernel / spectral bias in classical ML**
  - Why needed here: Provides the theoretical motivation—understanding why models preferentially learn low frequencies connects to generalization and optimization dynamics.
  - Quick check question: Why does spectral bias potentially explain low generalization error in overparameterized networks?

- Concept: **Reuploader circuit architecture**
  - Why needed here: The circuit structure (alternating W(θ) and S(x) layers) determines which eigenvalue combinations contribute to each frequency.
  - Quick check question: How does increasing layers L affect the maximum accessible frequency and redundancy distribution?

## Architecture Onboarding

- Component map: Input x → [S(x): encoding unitary with eigenvalues λᵢ] → [W(θ): trainable unitary] → repeat × L → Measure O
- Critical path: Encoding scheme → Redundancy distribution R(ω) → Gradient magnitude bounds → Frequency-dependent learning rate. To control spectral bias, you must control R(ω) through βᵢ selection.
- Design tradeoffs:
  - Exponential encoding: Uniform learning, maximum spectrum width, but coefficients become computationally large
  - Constant encoding: Simple implementation, strong inductive bias toward smooth functions, but severe high-frequency learning delay
  - High entanglement: Reduces spectral bias, increases circuit depth and noise susceptibility
  - Small initialization: Larger initial coefficients, faster learning, but may converge to suboptimal local minima
- Failure signatures:
  - High frequencies never converging despite low training loss on other frequencies → check redundancy distribution
  - Learned function appears over-smoothed → likely constant/linear encoding with insufficient layers
  - Gradient magnitudes near zero at high frequencies → initialization σ too large or redundancy too low
  - Robustness collapse under parameter perturbation → check if R(ω) is low at critical frequencies
- First 3 experiments:
  1. **Spectrum visualization**: Before training, compute R(ω) for your encoding scheme by enumerating (k,j) pairs contributing to each frequency. Plot R(ω) vs. ω to predict spectral bias severity.
  2. **Encoding comparison**: Train identical circuits (same L, n, entanglement) on target h(x) = Σ A_ω sin(ωx + φ_ω) with constant vs. exponential encoding. Track per-frequency coefficient error |c_ω^h - c_ω^f|/|c_ω^h| over training to validate redundancy-learning correlation.
  3. **Initialization sweep**: Fix encoding, vary σ ∈ {0.01, 0.1, 1.0, 10}, measure epochs to convergence at highest target frequency. Confirm predicted σ^(S-1) suppression effect.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does spectral bias manifest in models with trainable encoding schemes where the frequency spectrum evolves dynamically during optimization?
- Basis in paper: [explicit] The authors note in Section 4 regarding Theorem 2 that "the theorem does not cover that case fully [trainable-spectra models], as the target spectrum is assumed to match exactly that of the model. We consider the understanding of training dynamics of trainable-spectra models to be an interesting and important research direction."
- Why unresolved: The provided theoretical bounds (Theorems 1–4) assume a fixed spectrum determined by the encoding unitaries, whereas trainable embeddings alter the spectrum during training.
- What evidence would resolve it: A theoretical extension of the redundancy-gradient relationship to non-stationary spectra, or empirical studies analyzing convergence rates in circuits with trainable data encoding parameters.

### Open Question 2
- Question: Can the empirical relationship between large parameter initialization and the suppression of Fourier coefficients be rigorously formalized?
- Basis in paper: [explicit] In Section 5.4, the paper states: "Future work should look into formalising the observed decrease in Fourier coefficient amplitude as the size of initialization is increased."
- Why unresolved: The authors observe empirically (Figure 7) that increasing initialization variance suppresses coefficients and exacerbates spectral bias, but they lack a mathematical derivation for this decay.
- What evidence would resolve it: A theoretical proof linking initialization distribution variance (σ²) to the expected initial magnitude of Fourier coefficients (E[|c_ω|²]) and the resulting gradient suppression.

### Open Question 3
- Question: What is the theoretical mechanism connecting specific entanglement structures to the mitigation of spectral bias?
- Basis in paper: [explicit] The Conclusion states that "finding theoretical results for the robustness, entangling and parameter initalization results would be fruitful avenues to pursue." Section 5.3 provides empirical results but only offers a heuristic "lightcone argument."
- Why unresolved: While the paper demonstrates empirically that higher entanglement (e.g., all-to-all vs. 1d-hop) reduces spectral bias, it does not derive how entanglement alters the redundancy distribution or gradient bounds.
- What evidence would resolve it: A theorem or derivation showing how specific CNOT placements modify the effective redundancy R(ω) or the "sine-degree" S^(r) of variational paths in the small-angle approximation.

### Open Question 4
- Question: Does the redundancy-driven spectral bias framework generalize to advanced circuit architectures such as those involving mid-circuit measurements or parameter sharing?
- Basis in paper: [explicit] The Conclusion suggests: "Future work may aim to generalise our analysis to a broader class of PQCs, including those with parameter sharing, mid-circuit measurements and complex encodings..."
- Why unresolved: The theoretical framework (specifically Theorems 3 and 4) relies on a specific formalism for reuploader circuits with single-qubit rotations, excluding measurement-based feedback or weight tying.
- What evidence would resolve it: Experimental validation of the redundancy-correlation on architectures like Quantum Convolutional Neural Circuits (QCNNs) or circuits with measurement-based uncomputation, or an extension of the Fourier series definition to these non-unitary sub-circuits.

## Limitations

- The theoretical framework relies on small-angle initialization assumptions that may not hold in practical training scenarios
- The proof of robustness to parameter perturbations assumes uncorrelated parameter responses, which may not reflect hardware noise
- The encoding-redundancy mapping assumes specific gate structures that may not generalize to all quantum circuit architectures

## Confidence

- **High confidence**: The mathematical derivation of redundancy distribution from encoding coefficients; experimental validation of frequency-dependent learning rates
- **Medium confidence**: The robustness claims under parameter perturbation; the relationship between entanglement structure and spectral bias reduction
- **Low confidence**: Generalizability to non-integer frequency spectra and trainable encodings

## Next Checks

1. Test spectral bias with trainable encoding parameters where β_i are optimized alongside θ parameters, measuring how redundancy distribution evolves during training
2. Validate robustness claims by introducing correlated parameter initialization (ρ > 0) and measuring gradient perturbations at different frequencies
3. Extend experiments beyond integer-frequency spectra to target functions with non-harmonic frequencies (e.g., irrational multiples)