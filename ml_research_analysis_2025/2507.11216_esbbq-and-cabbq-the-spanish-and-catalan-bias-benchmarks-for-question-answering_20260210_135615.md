---
ver: rpa2
title: 'EsBBQ and CaBBQ: The Spanish and Catalan Bias Benchmarks for Question Answering'
arxiv_id: '2507.11216'
source_url: https://arxiv.org/abs/2507.11216
tags:
- bias
- social
- templates
- matem
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EsBBQ and CaBBQ, the first Spanish and Catalan
  bias benchmarks for question answering, designed to evaluate social biases in large
  language models within the Spanish cultural context. The benchmarks are adapted
  from the original BBQ dataset, incorporating manual translations, cultural adaptations,
  and new templates validated through a public survey on prevalent stereotypes in
  Spain.
---

# EsBBQ and CaBBQ: The Spanish and Catalan Bias Benchmarks for Question Answering

## Quick Facts
- arXiv ID: 2507.11216
- Source URL: https://arxiv.org/abs/2507.11216
- Reference count: 40
- First Spanish and Catalan bias benchmarks for QA, revealing models rely on stereotypes in ambiguous contexts

## Executive Summary
This paper introduces EsBBQ and CaBBQ, the first benchmarks designed to evaluate social biases in large language models within Spanish and Catalan cultural contexts. Adapted from the original BBQ dataset, these benchmarks incorporate manual translations, cultural adaptations, and new templates validated through public surveys on prevalent stereotypes in Spain. The evaluation reveals that models generally struggle to select the correct unknown answer in ambiguous scenarios and exhibit higher bias scores when achieving higher accuracy in disambiguated contexts, indicating a correlation between model performance and reliance on social stereotypes.

## Method Summary
The benchmarks use 323 manually-written templates with NAME/WORD placeholders, generating 27,320 instances through systematic permutation of contexts, questions, and placeholder values. Evaluation is conducted via zero-shot inference using LM Evaluation Harness v0.4.8, computing log-likelihood for 11 answer options (target, non-target, 9 unknown expressions) per instance. Accuracy is measured in both ambiguous (stereotypical information insufficient) and disambiguated (explicit evidence provided) contexts, while bias scores quantify the difference between stereotypical and anti-stereotypical answer frequencies. Models are evaluated across different families (Mistral, Llama, Gemma), sizes, and variants (base vs. instructed).

## Key Results
- Models tend to fail selecting the correct unknown option in ambiguous scenarios, instead relying on social stereotypes
- Higher QA accuracy in disambiguated contexts correlates with greater reliance on social stereotypes
- Bias scores generally increase with model size, particularly in base (non-instruction-tuned) variants

## Why This Works (Mechanism)

### Mechanism 1: Ambiguous Context → Stereotype Reliance
When context is under-informative, models fail to select "unknown" and instead rely on social stereotypes. Models use learned statistical associations between social groups and stereotypical attributes as priors when explicit evidence is insufficient.

### Mechanism 2: Accuracy-Bias Correlation in Disambiguated Contexts
Higher QA accuracy in disambiguated contexts correlates with greater reliance on social stereotypes. Models that better acquire world knowledge and linguistic patterns also acquire stereotypical associations embedded in training data.

### Mechanism 3: Model Scale → Bias Amplification
Larger models exhibit higher bias scores, particularly in base variants. Larger models better fit training data distributions, including stereotypical correlations, as their increased capacity reduces underfitting of all patterns.

## Foundational Learning

- **Ambiguous vs. Disambiguated Context Design**: The benchmark distinguishes these scenarios to separate bias measurement (ambiguous) from bias-override measurement (disambiguated). Quick check: If a model answers correctly in disambiguated contexts but defaults to stereotypes in ambiguous contexts, what does this indicate?

- **Bias Score Calculation (Equations 3-4)**: Understanding how bias is quantified as the difference between stereotypical and anti-stereotypical answer frequencies. Quick check: What does a negative bias score indicate, and what category in this study exhibited this pattern?

- **Template Instantiation with Placeholder Permutation**: The dataset systematically generates all combinations to control for ordering biases. Quick check: Why does the paper generate all 4 permutations of NAME1/NAME2 ordering rather than random sampling?

## Architecture Onboarding

- **Component map**: Templates (323 total) -> Instantiation Engine -> Evaluation Harness -> Metrics Module
- **Critical path**: Load template with stereotype annotation and target groups → Instantiate all combinations (min 12 per template) → Compute log-likelihood of 11 answer options → Aggregate scores by context type and category
- **Design tradeoffs**: Log-likelihood evaluation avoids primacy/recency bias but requires model access to token probabilities; systematic permutation eliminates randomization confounds but increases dataset size; manual translation + cultural adaptation ensures validity but limits scalability
- **Failure signatures**: Very low accuracy in ambiguous contexts with low bias score → model lacks capability, not evidence of fairness; high disambiguated accuracy + high disambiguated bias → stereotype override of explicit evidence; inconsistent results across categories → suggests category-specific overfitting or data issues
- **First 3 experiments**: 1) Replicate evaluation on a subset of models to validate scoring pipeline against reported values; 2) Test a single category (e.g., Gender) with ablated unknown expressions to check sensitivity to answer phrasing; 3) Compare base vs. instructed variants within one model family to quantify instruction-tuning effect size

## Open Questions the Paper Calls Out

1. **Is the reliance on social biases in LLMs inextricable from high performance in linguistic and world knowledge tasks?** The authors explicitly ask this after observing that models with high QA accuracy also exhibit higher bias scores. This remains unresolved because the paper establishes correlation but not causation or fundamental inseparability.

2. **How can the EsBBQ and CaBBQ benchmarks be extended to evaluate intersectional biases in Spanish and Catalan LLMs?** The authors acknowledge this limitation and plan to tackle intersectionality in future work to provide more comprehensive evaluation beyond mutually exclusive identity categories.

3. **Do the social bias patterns identified in the QA setting transfer to open-ended text generation tasks in Spanish and Catalan?** The authors warn that biases emerging in this specific task may differ from those in other downstream tasks, leaving this transferability question unresolved.

## Limitations

- Cultural adaptation relies on public survey validation with limited methodological detail, raising questions about whether identified stereotypes truly reflect Spanish cultural context
- Model family coverage is limited to three families (Mistral, Llama, Gemma), excluding other major Spanish language-capable models
- Answer choice sensitivity is not systematically tested across different unknown expressions, which could introduce variability in measured bias scores

## Confidence

- **High Confidence**: Core finding that models rely on social stereotypes when context is ambiguous; correlation between disambiguated accuracy and disambiguated bias; scale-bias relationship in base models
- **Medium Confidence**: Claim that models "struggle to select the correct unknown answer"; observation that smaller models' poor performance might mask biases
- **Low Confidence**: Cultural adaptation process beyond survey validation; absence of analysis on answer expression sensitivity; limited model family coverage

## Next Checks

1. **Replicate Cultural Validation**: Conduct a follow-up study with detailed survey methodology (sample size, demographics, statistical validation) to verify that the identified stereotypes genuinely reflect Spanish cultural context.

2. **Answer Expression Sensitivity Test**: Systematically evaluate model responses using only one unknown expression versus all nine to determine if certain phrasings are more/less likely to be selected and how this affects measured bias scores.

3. **Extended Model Coverage**: Evaluate additional Spanish-capable models (BLOOM, translation-augmented models, or Spanish-specific variants) to test whether the observed scale-bias relationship holds across a broader range of architectures.