---
ver: rpa2
title: 'RTQA : Recursive Thinking for Complex Temporal Knowledge Graph Question Answering
  with Large Language Models'
arxiv_id: '2509.03995'
source_url: https://arxiv.org/abs/2509.03995
tags:
- answer
- question
- temporal
- visit
- rtqa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RTQA is a training-free temporal knowledge graph question answering
  framework that addresses complex queries by recursively decomposing questions into
  sub-problems and solving them bottom-up using LLMs and TKG facts. The method employs
  multi-path answer aggregation to mitigate error propagation, achieving significant
  performance gains on MultiTQ and TimelineKGQA benchmarks.
---

# RTQA : Recursive Thinking for Complex Temporal Knowledge Graph Question Answering with Large Language Models

## Quick Facts
- **arXiv ID:** 2509.03995
- **Source URL:** https://arxiv.org/abs/2509.03995
- **Reference count:** 24
- **Primary result:** Achieves Hits@1 of 0.765 on MultiTQ benchmark, significantly outperforming state-of-the-art TKGQA models

## Executive Summary
RTQA introduces a training-free framework for complex temporal knowledge graph question answering that addresses the challenge of implicit temporal constraints through recursive question decomposition. The system breaks down complex temporal questions into a dependency tree of sub-problems, solves them bottom-up using LLMs and retrieved TKG facts, and employs multi-path answer aggregation to improve fault tolerance. This approach effectively handles multi-hop reasoning and compound temporal constraints like "before" and "after" relationships, achieving significant performance gains over existing methods on MultiTQ and TimelineKGQA benchmarks.

## Method Summary
RTQA employs a three-stage pipeline: (1) Temporal Question Decomposer uses few-shot LLM prompting to generate a question tree with explicit sub-questions and placeholders for temporal constraints, (2) Recursive Solver traverses this tree in post-order, retrieving relevant TKG facts (quadruples) using BGE-M3 embeddings and FAISS, then performs LLM reasoning with the retrieved context to solve each node, (3) Answer Aggregator fuses two candidates—direct inference from retrieved facts and synthesized answers from child nodes—to produce the final answer. The framework uses GPT-4o-mini for decomposition and DeepSeek-v3 for reasoning on MultiTQ, achieving strong performance through recursive constraint isolation and retrieval-grounded reasoning.

## Key Results
- Achieves Hits@1 of 0.765 on MultiTQ benchmark, surpassing state-of-the-art models
- Shows 10%+ improvement over existing TKGQA methods on complex and multi-entity questions
- Significant performance degradation (Hits@1 dropping to 0.070) when fact retrieval is removed, demonstrating the critical role of grounding LLM reasoning in retrieved facts
- Demonstrates strong generalizability across different LLMs and effective handling of multi-granular temporal reasoning

## Why This Works (Mechanism)

### Mechanism 1: Recursive Question Decomposition for Constraint Isolation
- **Claim:** If a complex temporal question is decomposed into a dependency tree of sub-questions, the system appears to resolve implicit temporal constraints (e.g., "before Kuwait") that single-pass LLMs often miss.
- **Mechanism:** The framework identifies implicit constraints and converts them into explicit sub-questions (e.g., Sub[1] asks "When did Kuwait receive the visit?"). The answer to Sub[1] replaces a placeholder (`#1`) in Sub[2], grounding the next reasoning step in a concrete timestamp rather than an ambiguous entity reference.
- **Core assumption:** The LLM can reliably parse the dependency structure and generate valid, executable sub-questions from the original query.
- **Evidence anchors:**
  - [abstract]: "recursively decomposes questions into sub-problems, solves them bottom-up"
  - [section 1]: "RTQA solves it via recursive sub-question decomposition... Sub[1]... informs Sub[2]"
  - [corpus]: "Plan of Knowledge" supports the general difficulty of complex temporal queries, though RTQA's specific recursive tree approach is distinct.
- **Break condition:** If the decomposition logic fails to identify the correct temporal dependency (e.g., missing the "before" relationship), the placeholder substitution will propagate incorrect timeframes.

### Mechanism 2: Multi-Path Answer Aggregation for Fault Tolerance
- **Claim:** Aggregating answers from independent reasoning paths (direct inference vs. recursive synthesis) likely reduces error propagation compared to sequential-only pipelines.
- **Mechanism:** The system computes two candidates: `IR_answer` (LLM reasoning directly on retrieved facts for the current node) and `child_answer` (synthesized from solved sub-questions). The Answer Aggregator selects the most plausible result, preventing an early error in a sub-question from automatically dooming the final answer.
- **Core assumption:** The `IR_answer` provides a valid "second opinion" that is not subject to the same error chain as the `child_answer`.
- **Evidence anchors:**
  - [abstract]: "employs multi-path answer aggregation to improve fault tolerance"
  - [section 4.4]: "selects the most plausible final answer by fusing two candidates: ai_IR and ai_child"
  - [corpus]: Corpus evidence for this specific aggregation logic is weak; related work focuses more on single-path retrieval or embedding fusion.
- **Break condition:** If the retrieval fails to provide relevant facts for the `IR_answer` path, the aggregator defaults to the `child_answer`, potentially inheriting the propagated error it was meant to catch.

### Mechanism 3: Dense Retrieval Grounding for Temporal Facts
- **Claim:** Retrieving explicit temporal facts (quadruples) significantly reduces LLM hallucinations on temporal reasoning tasks.
- **Mechanism:** A dense encoder (BGE-M3) retrieves top-K facts relevant to the (sub-)question. The LLM is constrained to reason over this provided context ("Historical Facts") rather than its parametric memory, anchoring the output in specific timestamps (e.g., "2014-06-04").
- **Core assumption:** The retrieval system can surface the gold-standard facts within the top-K results (observed Recall@50 is ~72%).
- **Evidence anchors:**
  - [section 5.3]: "w/o fact retrieval... Hits@1 plummeting from 0.765 to a mere 0.070"
  - [section 4.3]: "top-K most relevant facts are retrieved based on similarity"
  - [corpus]: "Reinforcement Learning Enhanced Multi-hop Reasoning" similarly relies on subgraph retrieval, validating the retrieval-reasoning coupling.
- **Break condition:** If the question implies a temporal logic not covered by the retrieved facts (retrieval error), the LLM either hallucinates or returns "Unknown."

## Foundational Learning

- **Concept: Temporal Knowledge Graphs (TKG)**
  - **Why needed here:** Unlike static KGs, TKGs use quadruples $(s, p, o, t)$. Understanding that "time" is a first-class citizen (a specific edge property) is essential for grasping why standard KGQA methods fail on "Before/After" queries.
  - **Quick check question:** How does a TKG represent "Trump was president in 2025" differently from a static KG?

- **Concept: Recursive Tree Traversal (Post-order)**
  - **Why needed here:** RTQA solves the question tree bottom-up. You must understand that leaf nodes (atomic questions) are solved first to populate the placeholders (`#1`) in parent nodes.
  - **Quick check question:** In a tree with Root -> [Sub Q1, Sub Q2], which node is processed first by the Recursive Solver?

- **Concept: Hallucination in LLMs**
  - **Why needed here:** The paper positions itself as a solution to LLM temporal hallucinations. Recognizing that LLMs struggle with precise dates (e.g., inventing a visit date) clarifies the necessity of the external TKG retriever.
  - **Quick check question:** Why might an LLM answer "United Arab Emirates" incorrectly without external facts (Figure 1a)?

## Architecture Onboarding

- **Component map:** Temporal Question Decomposer -> Relevant Facts Retriever -> Recursive Solver -> Answer Aggregator
- **Critical path:** The **Prompt Engineering & Parsing** stage. If the Decomposer LLM output format deviates slightly (e.g., missing JSON brackets), the `BuildTree` function fails, crashing the pipeline before solving begins.
- **Design tradeoffs:**
  - **Accuracy vs. Latency:** The recursive, multi-path structure requires multiple LLM calls (avg 4-5 calls per question). This improves Hits@1 but increases latency and cost compared to single-pass RAG.
  - **Context Window vs. Noise:** Increasing `n` (retrieved facts) boosts recall but introduces noise. The paper settles on `n=50` as a balance (Table 6).
- **Failure signatures:**
  - **Decomposition Loop:** The LLM generates sub-questions that are circular or cannot be answered by the TKG (e.g., asking for "intent" when only "visits" exist).
  - **Time Format Mismatch:** The TKG stores "2009-05-12", but the LLM outputs "May 12th, 09". The standardization module is critical here.
- **First 3 experiments:**
  1. **Ablation on Context Length:** Run the solver with `n=10` vs `n=50` retrieved facts on a sample of 100 "Complex" questions to observe the noise/recall tradeoff directly.
  2. **Decomposition Quality Check:** Manually inspect 20 generated question trees to see if the placeholders (`#1`) logically align with the temporal constraints.
  3. **Aggregator Stress Test:** Force a `child_answer` error (manually inject a wrong date) and verify if the Aggregator successfully recovers the correct answer via `IR_answer`.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the recursive decomposition strategy be optimized for smaller or less capable LLMs that currently struggle to generate high-quality sub-questions?
- **Basis in paper:** [explicit] The authors state in the Limitations section that "Smaller models may struggle to generate high-quality sub-questions, thereby constraining the performance."
- **Why unresolved:** The current framework relies heavily on the instruction-following capabilities of large models (e.g., GPT-4o, DeepSeek), making it unclear if the method is viable for open-source models with limited capacity.
- **What evidence would resolve it:** A performance evaluation using open-source models (e.g., LLaMA-7B) with specific distillation techniques or prompt simplifications that maintain decomposition accuracy.

### Open Question 2
- **Question:** What retrieval mechanisms can be integrated to improve robustness against missing or noisy facts in the TKG?
- **Basis in paper:** [explicit] The paper notes in the Limitations that "Failure to retrieve key information can significantly reduce the reasoning accuracy," supported by the catastrophic performance drop (0.765 to 0.070 Hits@1) in the "w/o fact retrieval" ablation study.
- **Why unresolved:** The framework currently assumes a reliable retriever; it lacks a mechanism to handle scenarios where the TKG is sparse or the initial retrieval returns irrelevant context.
- **What evidence would resolve it:** Experiments implementing a feedback loop where the LLM identifies missing evidence and generates follow-up retrieval queries.

### Open Question 3
- **Question:** To what extent does RTQA generalize to non-temporal or heterogeneous knowledge graph domains without requiring structural modifications?
- **Basis in paper:** [explicit] The authors conclude that "its applicability to other QA domains has yet to be thoroughly validated."
- **Why unresolved:** The recursive solver and decomposer are specifically tailored for temporal constraints (e.g., "before," "last," timestamps), leaving their effectiveness on standard multi-hop static KGQA tasks uncertain.
- **What evidence would resolve it:** Applying the RTQA framework to standard static KGQA benchmarks (e.g., MetaQA or WebQuestionsSP) and comparing performance against specialized static KGQA models.

## Limitations
- The framework's effectiveness is heavily dependent on the LLM's ability to generate accurate question trees, with smaller models struggling to produce quality sub-questions
- Performance is bounded by retrieval recall (72% at top-50), meaning 28% of relevant facts may be missed, potentially forcing hallucination
- Multiple LLM calls per question raise practical concerns about latency and cost in production environments

## Confidence

- **High Confidence:** The framework's core design (recursive decomposition + retrieval grounding + multi-path aggregation) is technically sound and well-documented. The reported Hits@1 of 0.765 on MultiTQ is supported by ablation studies showing severe performance drops without fact retrieval (0.070).
- **Medium Confidence:** The decomposition quality and aggregation effectiveness are assumed based on reported metrics but not fully validated through error analysis of individual components. The improvement claims over existing models (T-KQA, TimesQA, etc.) are based on benchmark results but lack detailed comparative analysis of failure cases.
- **Low Confidence:** The generalizability across different LLMs and benchmarks is demonstrated but not extensively tested. The handling of edge cases (e.g., ambiguous temporal expressions, missing facts) is not thoroughly explored.

## Next Checks
1. **Error Propagation Analysis:** For 50 failed questions, trace the reasoning chain to identify whether errors originate in decomposition, retrieval, or aggregation. This will reveal the weakest link in the pipeline.
2. **Aggregation Effectiveness Test:** Create controlled scenarios where either the IR_answer or child_answer is intentionally incorrect. Measure whether the aggregator successfully recovers the correct answer in at least 80% of cases.
3. **Retrieval Robustness Evaluation:** Run the solver with varying retrieval contexts (n=10, 25, 50, 100) on a subset of complex questions. Plot Hits@1 against retrieval size to quantify the noise-recall tradeoff and determine the optimal balance.