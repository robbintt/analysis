---
ver: rpa2
title: 'MeDiSumQA: Patient-Oriented Question-Answer Generation from Discharge Letters'
arxiv_id: '2502.03298'
source_url: https://arxiv.org/abs/2502.03298
tags:
- discharge
- medical
- llms
- medisumqa
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MeDiSumQA addresses the challenge of patient understanding of complex
  medical documents by providing a standardized benchmark for evaluating large language
  models (LLMs) in patient-oriented question-answering. The dataset was created from
  MIMIC-IV discharge summaries through an automated pipeline combining LLM-based question-answer
  generation with manual quality checks by physicians.
---

# MeDiSumQA: Patient-Oriented Question-Answer Generation from Discharge Letters

## Quick Facts
- arXiv ID: 2502.03298
- Source URL: https://arxiv.org/abs/2502.03298
- Reference count: 21
- MeDiSumQA is a benchmark dataset for patient-oriented QA from discharge letters, showing general-purpose LLMs often outperform biomedical-adapted models.

## Executive Summary
MeDiSumQA addresses the challenge of patient understanding of complex medical documents by providing a standardized benchmark for evaluating large language models (LLMs) in patient-oriented question-answering. The dataset was created from MIMIC-IV discharge summaries through an automated pipeline combining LLM-based question-answer generation with manual quality checks by physicians. The resulting dataset contains 416 high-quality question-answer pairs covering six medical categories, with Treatment & Hospital Course being the most prevalent at 22.4%. Evaluation of seven models revealed that general-purpose LLMs, particularly Meta-Llama-3.1-8B-Instruct, frequently outperformed biomedical-adapted models. Automatic evaluation metrics (ROUGE, BERT Score, UMLS F1) showed strong correlation with human judgment, with the best model achieving ROUGE-L of 31.43 and BERT Score of 10.24. Manual assessment confirmed that higher automatic scores corresponded to better physician ratings across factuality, brevity, patient-friendliness, relevance, and safety. By releasing MeDiSumQA on PhysioNet, the study provides a valuable resource for advancing LLM development in patient education and healthcare communication.

## Method Summary
The MeDiSumQA dataset was constructed from MIMIC-IV discharge summaries through an automated pipeline that identifies discharge letter sections, segments them into sentences using LLMs, and generates patient-oriented questions and answers for each sentence. A physician manually reviewed all QA pairs against five criteria: factuality, completeness, safety, consistency with the full summary, and complexity. The final dataset includes 416 QA pairs across six medical categories, with the source discharge letter removed from the input to ensure models must locate information within the full summary context.

## Key Results
- General-purpose LLMs, particularly Meta-Llama-3.1-8B-Instruct, outperformed biomedical-adapted models on patient-oriented QA tasks
- Automated metrics (ROUGE, BERT Score, UMLS F1) showed strong correlation with physician judgment, with best model achieving ROUGE-L of 31.43 and BERT Score of 10.24
- Manual evaluation confirmed that higher automatic metric scores corresponded to better physician ratings across factuality, brevity, patient-friendliness, relevance, and safety

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Patient-oriented QA pairs can be extracted from discharge letters using LLM-based generation with manual physician curation.
- **Mechanism:** The pipeline identifies the patient-friendly discharge letter section within longer clinical summaries, segments it into sentences via LLM, and generates a patient-posed question for each sentence with a grounded answer. A physician manually filters for factual correctness, completeness, safety, consistency with the full summary, and complexity.
- **Core assumption:** Discharge letters already use patient-accessible language and contain the key clinical facts needed for meaningful QA, reducing the need for de-novo simplification.
- **Evidence anchors:**
  - [abstract] "MeDiSumQA is a dataset created from MIMIC-IV discharge summaries through an automated pipeline combining LLM-based question-answer generation with manual quality checks."
  - [section 3.1] "We split each discharge letter into sentences using Meta's Llama-3-70B-Instruct... we fed these sentences into an LLM to generate matching questions from a patient's perspective... The resulting QA candidates were then manually reviewed by a physician."
  - [corpus] Weak direct evidence; neighbor papers focus on discharge summary generation, not QA extraction.
- **Break condition:** If the source discharge letters are incomplete, inconsistent, or not truly patient-oriented, generated QA pairs will inherit these flaws regardless of downstream curation.

### Mechanism 2
- **Claim:** General-purpose LLMs can match or outperform biomedical-adapted models on patient-oriented QA from discharge summaries.
- **Mechanism:** Strong general-domain pretraining provides robust language understanding, reasoning, and instruction-following. Domain adaptation may narrow the model's general capabilities or alter instruction-tuning in ways that inadvertently degrade performance on this hybrid comprehension-communication task.
- **Core assumption:** The task demands comprehension of long clinical documents, information extraction, and patient-friendly phrasing more than niche biomedical terminology mastery.
- **Evidence anchors:**
  - [abstract] "Our findings reveal that general-purpose LLMs frequently surpass biomedical-adapted models."
  - [section 4.2] "Meta-Llama-3.1-8B-Instruct performed best among all tested metrics, achieving the highest scores despite being a general-domain model without specific biomedical adaptation."
  - [corpus] No direct external comparison in neighbor papers; this is an internal evaluation.
- **Break condition:** If data contamination exists (models trained on MIMIC-IV during pretraining), apparent performance advantages may reflect memorization rather than true generalization.

### Mechanism 3
- **Claim:** Automated metrics (ROUGE, BERT Score, UMLS F1) correlate with physician judgment on patient-oriented QA quality.
- **Mechanism:** These metrics capture lexical overlap (ROUGE), semantic similarity via embeddings (BERT Score), and biomedical entity alignment (UMLS F1). When answers are derived from constrained source text, higher overlap tends to align with higher physician ratings for factuality, relevance, and patient-friendliness.
- **Core assumption:** Ground truth answers are high-quality references, and valid alternative phrasings are not heavily penalized by the chosen metrics.
- **Evidence anchors:**
  - [abstract] "automated metrics correlate with human judgment."
  - [section 5.3] "our results show that calculated metrics like ROUGE and BERT Score correlate well with human judgment. Higher automated metric scores consistently corresponded to higher manual ratings and preferences."
  - [corpus] No external validation of this correlation in neighbor papers.
- **Break condition:** If a model produces a factually correct but lexically/semantically distinct answer (e.g., different valid framing), automated metrics will undercount its quality, and the correlation with human judgment weakens.

## Foundational Learning

- **Concept: MIMIC-IV and De-identification**
  - **Why needed here:** MeDiSumQA is built on MIMIC-IV discharge summaries. Understanding that this is a de-identified, access-controlled clinical corpus explains both the dataset's realism and its privacy constraints.
  - **Quick check question:** Can researchers freely download MIMIC-IV without any credentialing process?

- **Concept: Long-Context Reasoning**
  - **Why needed here:** Discharge summaries average ~3,245 tokens with high variance, requiring models to reason over long contexts rather than short snippets.
  - **Quick check question:** Why might a model with a 2048-token context window struggle on MeDiSumQA?

- **Concept: Patient-Oriented vs. Clinician-Oriented Language**
  - **Why needed here:** The core task is not just answering correctly, but answering in a way a layperson can understand. This differentiates MeDiSumQA from standard clinical QA benchmarks.
  - **Quick check question:** What adjustment would be needed to adapt a clinician-focused QA dataset for patient-oriented evaluation?

## Architecture Onboarding

- **Component map:** Source identification -> Sentence splitting -> QA generation -> Manual curation -> Final packaging
- **Critical path:** The manual physician review is the bottleneck; it ensures safety and quality but limits dataset scale. Any error in sentence splitting or QA generation that passes curation becomes a latent flaw.
- **Design tradeoffs:**
  - Using LLMs for sentence splitting and question generation increases scalability vs. manual annotation but introduces LLM-specific error modes (e.g., subtle hallucinations that evade exact-string verification).
  - Removing the discharge letter from the source summary for evaluation ensures models must locate information in the full document, but it also removes the most patient-friendly section from the model's input.
- **Failure signatures:**
  - **Ground truth inconsistency:** Answers that depend on information only present in the removed discharge letter (should have been filtered by consistency criterion).
  - **Safety leaks:** Answers with incomplete medical instructions that pass curation (e.g., medication dosage without required monitoring).
  - **Metric divergence:** Model answers that are clinically valid but stylistically different from ground truth, scoring poorly on automated metrics despite high physician preference.
- **First 3 experiments:**
  1. **Metric-human correlation probe:** Sample 100 model outputs across performance bins, have physicians rate them blind, and compute Spearman/Pearson correlation between automated metrics and human scores to validate or refute the claimed correlation.
  2. **General vs. biomedical model ablation:** Evaluate matched pairs (e.g., Mistral-7B vs. BioMistral-7B, Llama-3-8B vs. Llama3-Med42-8B) with identical prompting to isolate the effect of domain adaptation.
  3. **Contamination check:** Test whether models perform anomalously well on specific discharge summaries that may have leaked into pretraining data, using membership inference or out-of-distribution detection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do general-purpose LLMs outperform biomedical-adapted models on patient-oriented QA tasks, and under what conditions might domain adaptation become beneficial?
- Basis in paper: [explicit] The paper states Meta-Llama-3.1-8B-Instruct "outperformed all tested biomedical domain-adapted models, raising questions about domain-specific training's effectiveness" and notes "comprehensive pretraining on general-domain data may be more valuable than domain-specific adaptation."
- Why unresolved: The paper observes this phenomenon but does not investigate the underlying causes or identify scenarios where biomedical adaptation would help.
- What evidence would resolve it: Systematic ablation studies comparing general vs. biomedical models across task types, dataset sizes, and complexity levels.

### Open Question 2
- Question: How can evaluation datasets incorporate real-world patient feedback to better align automated metrics with patient comprehension and preferences?
- Basis in paper: [explicit] The paper states: "Future research should explore more robust evaluation methods that incorporate real-world patient feedback."
- Why unresolved: Current evaluation relies on physician ratings and automated metrics (ROUGE, BERT Score), which may not capture actual patient understanding.
- What evidence would resolve it: Studies comparing physician ratings vs. patient comprehension scores, and correlation analyses between automated metrics and patient-reported outcomes.

### Open Question 3
- Question: Can expanding MeDiSumQA to include post-discharge documentation (outpatient notes, rehabilitation plans) improve coverage of follow-up care questions from 4.8% to a more balanced distribution?
- Basis in paper: [explicit] The paper identifies that "Post-Discharge Care & Follow-Up is underrepresented" at only 4.8% and explicitly proposes "expanding MeDiSumQA to include additional post-discharge documentation."
- Why unresolved: The current dataset reflects discharge summary structure, which emphasizes inpatient care over long-term guidance.
- What evidence would resolve it: Applying the pipeline to outpatient follow-up notes and measuring category distribution shifts.

### Open Question 4
- Question: What is the impact of incorporating high-quality model-generated responses (preferred over ground truth by physicians) into benchmark datasets on model evaluation validity?
- Basis in paper: [explicit] "During manual evaluation, some model-generated answers were preferred over the ground truth. This presents an opportunity to refine the dataset by incorporating high-quality model-generated responses."
- Why unresolved: This approach could introduce bias toward specific LLMs used in selection, and optimal methodologies for integration remain undefined.
- What evidence would resolve it: Comparative studies of model performance on datasets with vs. without model-generated ground truth augmentation, with multiple independent reviewers.

## Limitations
- Manual physician curation creates scalability bottleneck limiting dataset size and generalizability
- Automated metrics correlation with human judgment lacks external validation across different clinical contexts
- Claims about general-purpose LLM superiority require careful interpretation given potential data contamination concerns

## Confidence
- **High Confidence:** The dataset construction pipeline and its components are well-documented and reproducible. The manual curation process and its five criteria are clearly specified.
- **Medium Confidence:** The correlation between automated metrics and physician judgment is supported by internal evidence but lacks external validation. The performance differences between general and biomedical LLMs are observed but may be influenced by contamination.
- **Low Confidence:** The generalizability of findings to other clinical languages (the dataset is in English) and different healthcare systems remains unproven.

## Next Checks
1. **External Metric Validation:** Conduct a blinded physician rating study on 100 randomly selected model outputs across performance bins to verify the correlation between automated metrics and human judgment.
2. **Data Contamination Audit:** Perform membership inference testing to determine if evaluation samples appear in pretraining data, particularly for high-performing general-purpose models.
3. **Cross-Cultural Generalizability:** Test the same models on patient-oriented QA tasks from discharge summaries in at least one non-English clinical corpus to assess language and system dependency.