---
ver: rpa2
title: Consistent Explainers or Unreliable Narrators? Understanding LLM-generated
  Group Recommendations
arxiv_id: '2507.13705'
source_url: https://arxiv.org/abs/2507.13705
tags:
- group
- llms
- recommendations
- aggregation
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates Large Language Models (LLMs) as group decision-makers
  and explanation generators for Group Recommender Systems (GRS). The researchers
  compared LLM-generated recommendations to social choice-based aggregation strategies
  across different LLMs (Llama3.1, Mistral NeMo, Gemma3, Phi4) using 1,500 fictitious
  group scenarios with varying item counts (25, 50, 75 items).
---

# Consistent Explainers or Unreliable Narrators? Understanding LLM-generated Group Recommendations

## Quick Facts
- **arXiv ID**: 2507.13705
- **Source URL**: https://arxiv.org/abs/2507.13705
- **Reference count**: 40
- **Primary result**: LLM recommendations align with Additive Utilitarian aggregation; explanations reference averaging, with performance degrading as item set size increases

## Executive Summary
This study evaluates Large Language Models as group decision-makers and explanation generators for Group Recommender Systems across 1,500 simulated scenarios. The research compares LLM-generated recommendations to established social choice-based aggregation strategies, revealing that while recommendations often follow Additive Utilitarian approaches, explanations primarily reference rating averaging. The findings indicate that group structure (uniform vs divergent preferences) has minimal impact on performance, but increasing item set sizes leads to less specific explanations and degraded performance across all tested LLMs. Notably, different LLMs show varying performance patterns, with Phi4 demonstrating unexpected improvement at larger item sets.

## Method Summary
The researchers conducted experiments using four different LLMs (Llama3.1, Mistral NeMo, Gemma3, Phi4) across 1,500 simulated group recommendation scenarios with varying item counts (25, 50, 75 items). They compared LLM-generated recommendations against social choice-based aggregation strategies, measuring performance through recommendation accuracy and explanation quality. The study examined how group structure (uniform vs divergent preferences) affected outcomes and tracked changes in explanation specificity as item set sizes increased.

## Key Results
- LLM recommendations predominantly align with Additive Utilitarian aggregation strategies
- Explanations typically reference averaging of user ratings, becoming less specific as item set sizes increase
- Performance decreases across all LLMs with larger item sets, though Phi4 shows improvement at 75 items

## Why This Works (Mechanism)
The study reveals that LLMs function as hybrid decision-makers, combining multiple aggregation approaches rather than following a single strategy. This explains why explanations often reference averaging while recommendations follow Utilitarian patterns. The mechanism suggests LLMs can adapt their reasoning based on context and input size, though this flexibility comes at the cost of consistency and transparency in recommendations.

## Foundational Learning

### Group Recommender Systems
**Why needed**: GRS must balance individual preferences to achieve group satisfaction
**Quick check**: Can the system identify when to prioritize consensus vs individual satisfaction?

### Social Choice Theory
**Why needed**: Provides formal frameworks for aggregating individual preferences into group decisions
**Quick check**: Does the aggregation method satisfy basic fairness criteria like Pareto efficiency?

### Explainable AI in Recommendations
**Why needed**: Users need to understand why specific recommendations were made to build trust
**Quick check**: Are explanations aligned with the actual recommendation mechanism?

## Architecture Onboarding

### Component Map
User Preferences -> LLM Aggregator -> Recommendation Output -> Explanation Generator -> User Interface

### Critical Path
User preference input → LLM reasoning process → aggregation strategy selection → recommendation generation → explanation synthesis

### Design Tradeoffs
- **Transparency vs performance**: More interpretable methods may sacrifice recommendation accuracy
- **Consistency vs adaptability**: Single-strategy approaches are consistent but less flexible than LLM hybrid methods
- **Explanation specificity vs computational efficiency**: More detailed explanations require more processing time

### Failure Signatures
- Explanations that don't match the actual aggregation logic
- Performance degradation with larger item sets
- Inconsistent recommendation patterns across similar scenarios

### First 3 Experiments
1. Compare top-1 vs top-5 recommendations for explanation consistency
2. Test cross-domain performance (movies vs restaurants vs travel)
3. Evaluate real user groups vs simulated scenarios for ecological validity

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- Use of fictitious scenarios rather than real user data limits ecological validity
- Evaluation focused primarily on top-1 recommendations, not longer lists
- Simplified group preference distributions may not capture real-world complexity

## Confidence

| Claim | Confidence |
|-------|------------|
| LLM recommendations align with Additive Utilitarian strategies | High |
| Explanations predominantly reference rating averaging | High |
| LLMs combine multiple aggregation approaches | Medium |
| Group structure minimally impacts performance | Medium |
| Performance degrades with larger item sets | High |
| Phi4 improvement at 75 items | Medium |

## Next Checks
1. Conduct user studies with real groups to validate whether LLM-generated explanations actually improve group understanding and satisfaction in practice
2. Test the robustness of findings across different recommendation domains (e.g., movies, restaurants, travel) to assess domain-specific variations
3. Evaluate longer recommendation lists (top-5, top-10) to understand how aggregation strategies and explanation quality scale with recommendation depth