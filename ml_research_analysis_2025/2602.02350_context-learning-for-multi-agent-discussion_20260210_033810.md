---
ver: rpa2
title: Context Learning for Multi-Agent Discussion
arxiv_id: '2602.02350'
source_url: https://arxiv.org/abs/2602.02350
tags:
- uni00000013
- uni00000015
- uni00000014
- uni00000019
- uni00000017
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of discussion inconsistency in
  Multi-Agent Discussion (MAD), where multiple LLM instances fail to reach coherent
  solutions due to misaligned individual contexts. The proposed method, M2CL, learns
  a context generator for each agent that dynamically generates context instructions
  per discussion round via automatic information organization and refinement.
---

# Context Learning for Multi-Agent Discussion

## Quick Facts
- arXiv ID: 2602.02350
- Source URL: https://arxiv.org/abs/2602.02350
- Reference count: 40
- Primary result: M2CL significantly outperforms existing methods by 20%–50% on 9 benchmarks, with at most 10% runtime overhead

## Executive Summary
This paper addresses the problem of discussion inconsistency in Multi-Agent Discussion (MAD), where multiple LLM instances fail to reach coherent solutions due to misaligned individual contexts. The proposed method, M2CL, learns a context generator for each agent that dynamically generates context instructions per discussion round via automatic information organization and refinement. The approach trains generators to control context coherence and output discrepancies through a self-adaptive mechanism that balances consistency with creative exploration. Evaluation across 9 challenging benchmarks including academic reasoning, embodied tasks, and mobile control demonstrates that M2CL significantly outperforms existing methods by 20%–50%, maintains favorable transferability across different LLM architectures, and achieves computational efficiency with at most 10% runtime overhead.

## Method Summary
M2CL is a context learning framework for MAD that dynamically generates per-agent context instructions through a self-adaptive mechanism. The method uses orthogonal context initialization via projection functions to select diverse initial contexts from a pre-defined pool, then evolves these contexts per round using learned generators that incorporate others' responses. A dual variable α is adaptively tuned to control the trade-off between maintaining initial context expertise and achieving consensus. The framework is trained on 20% of benchmark data and evaluated on the remaining 80%, with final answers determined by majority voting after 8 discussion rounds.

## Key Results
- M2CL achieves 20%–50% performance improvement over existing MAD methods across 9 benchmarks
- Maintains favorable transferability across different LLM architectures (Qwen-2.5, Llama-2)
- Achieves computational efficiency with at most 10% runtime overhead compared to baseline MAD methods

## Why This Works (Mechanism)

### Mechanism 1: Orthogonal Context Initialization for Diverse Coverage
- **Claim:** Selecting initial contexts that are approximately orthogonal in latent space provides diverse reasoning perspectives, enabling the MAD system to explore a broader solution space.
- **Mechanism:** The initialization phase selects N contexts from a pre-defined pool by optimizing a reconstruction objective: the selected contexts' activations should best approximate the problem's activation vector. This naturally promotes orthogonality as the selected contexts must form a compact basis for the target space.
- **Core assumption:** Diverse initial perspectives lead to better exploration before consensus; orthogonal activations correspond to non-redundant reasoning approaches.
- **Evidence anchors:**
  - [abstract] "we provide the final context initialization as: I^b = argmin... that aligns with Eq. (7) to encourage orthogonality of selected context activations"
  - [Section 5.1] "The resulting near-orthogonal activations form a compact basis, ensuring each context contributes unique, non-overlapping information"
  - [corpus] Limited direct evidence. Related work on MAD focuses on role assignment but not explicitly on activation-space orthogonality for initialization.
- **Break condition:** If the context pool lacks sufficient diversity or the projection function fails to preserve activation-space relationships, orthogonality benefits diminish.

### Mechanism 2: Dynamic Context Evolution via Self-Adaptive Constraints
- **Claim:** Evolving context instructions per-round, while constraining deviation from initial contexts, enables LLMs to integrate others' responses while maintaining specialized expertise.
- **Mechanism:** A per-agent context generator G produces instructions I_t^i based on task goal, initial instruction, and concatenated responses from other agents. The evolution is guided by a constrained optimization: minimize activation distance between current instruction and previous response, subject to staying within a bound β of the initial context. The dual variable α is learned adaptively.
- **Core assumption:** LLMs can effectively incorporate collaborative instructions; activation alignment correlates with response consistency.
- **Evidence anchors:**
  - [abstract] "M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive mechanism"
  - [Section 5.2.2] "the first term, -α||C_t^i - C_b^i||, preserves the fundamental problem-solving capability endowed by the initialization, while the second term enforces consistency"
  - [corpus] Adjacent work on MAD uses static roles; dynamic instruction refinement is less explored in corpus.
- **Break condition:** If β is too small, contexts cannot adapt sufficiently, causing inconsistency; if too large, agents converge prematurely to similar answers, losing diversity.

### Mechanism 3: Adaptive α Scheduling for Convergence-Versus-Diversity Trade-off
- **Claim:** Dynamically adjusting α across discussion rounds prevents premature consensus while still driving toward agreement.
- **Mechanism:** Early in discussion, α decreases rapidly (weakening the constraint on distance from initial context), allowing faster convergence. As LLMs align, α stabilizes to preserve remaining diversity. This is implemented via dual gradient descent on the constrained optimization.
- **Core assumption:** The discussion phase has different information needs at different stages; a fixed α would be suboptimal.
- **Evidence anchors:**
  - [abstract] "It enables LLMs to avoid premature convergence on 'majority noise' and progressively reach the correct consensus"
  - [Section 5.2.2] "As the discussion progresses and the LLMs gradually reach agreement, α will be kept at a certain level"
  - [corpus] No direct corpus evidence for adaptive α in MAD; related work typically uses fixed hyperparameters for context strength.
- **Break condition:** If the learning rates for α are poorly tuned or the objective landscape is noisy, α may oscillate or fail to adapt appropriately.

## Foundational Learning

- **Concept: Multi-Agent Discussion (MAD) Frameworks**
  - **Why needed here:** M2CL builds on MAD where multiple LLM instances discuss collaboratively. Understanding the baseline (static roles, majority voting, discussion inconsistency) is essential to see why dynamic context learning is proposed.
  - **Quick check question:** Can you explain why static, pre-assigned contexts in existing MAD methods lead to discussion inconsistency?

- **Concept: Context Learning for LLMs**
  - **Why needed here:** The paper reframes context not as static text but as a dynamically organized collection. M2CL learns to generate context instructions that evolve with discussion state.
  - **Quick check question:** How does context learning differ from prompt engineering in this framework?

- **Concept: Activation Space Alignment**
  - **Why needed here:** The paper uses activation distance (not token similarity) to measure consistency and to guide context evolution. This assumes internal representations capture reasoning alignment better than surface text.
  - **Quick check question:** Why might activation distance be more robust than embedding distance for measuring reasoning alignment across agents?

## Architecture Onboarding

- **Component map:**
  - Context Pool -> Projection Functions (f and F) -> Context Generators G_θi -> Self-Adaptive α_i -> Discussion Rounds

- **Critical path:**
  1. **Initialization:** For each task, select N initial contexts from pool via F-based selection (orthogonality-promoting).
  2. **Per-Round Evolution:** For each round t and each agent i:
     - Generate instruction I_t^i = G_θi(P, I_b^i, X^{t-1}).
     - Agent i produces response X_t^i given context [I_t^i, X^{t-1}, P].
     - Update θ_i and α_i via dual gradient descent.
  3. **Termination:** After T rounds, aggregate final answers by majority vote.

- **Design tradeoffs:**
  - **Context constraint β:** Controls exploration-exploitation. Tight β → less adaptation, more inconsistency; loose β → rapid convergence, loss of diversity. Paper shows β ≈ 5–7 works well across tasks.
  - **Number of agents N:** More agents increase diversity and performance but with diminishing returns and higher compute. M2CL shows improved scaling compared to baselines.
  - **Context generator capacity:** Lightweight models (e.g., T5-small) suffice; overcapacity may overfit to training tasks without generalizing.

- **Failure signatures:**
  - **Premature consensus:** All agents converge to the same wrong answer early; often due to β too large or α not adapting properly.
  - **Persistent inconsistency:** Discrepancy intensity does not decrease over rounds; may indicate context evolution not incorporating others' responses effectively.
  - **Slow convergence or non-convergence:** α oscillates or decreases too slowly; check learning rate settings.

- **First 3 experiments:**
  1. **Ablate context initialization:** Run M2CL without orthogonality-promoting selection (e.g., random selection from pool). Compare performance and discrepancy intensity curves to full M2CL.
  2. **Sweep β:** Test β ∈ {0, 2, 5, 7, 10} on a held-out task. Plot accuracy and discrepancy intensity to identify the sweet spot.
  3. **Transfer contexts:** Train context generators on one model (e.g., Llama-7B) and transfer to another (e.g., Qwen-14B) without retraining. Measure performance gap to assess generalization.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can LLMs be enabled to autonomously identify and specialize in sub-tasks rather than relying on the quantity of agents with heterogeneous characteristics to induce diversity?
- **Basis in paper:** [explicit] Section 7 states, "An avenue for future work is to enable LLMs to truly capture the sub-tasks they are interested in or excel at," noting that current diversity via multiple agents is computationally inefficient.
- **Why unresolved:** The current M2CL framework relies on increasing the number of LLMs ($N$) to ensure diverse solution perspectives (Theorem 4.1), which scales computational cost.
- **What evidence would resolve it:** A methodology where a single or fixed number of agents dynamically partition the problem space into sub-tasks without the need for a "swarm" of agents.

### Open Question 2
- **Question:** To what extent does the sentence vector proxy ($v_P$) fail to capture the properties of the inaccessible "correct activation" ($a_c$) during context initialization?
- **Basis in paper:** [inferred] Section 5.1 Eq. (9) replaces the theoretically optimal but inaccessible $a_c$ with $v_P$ (sentence vector of the question). The paper notes $a_c$ is unavailable but does not quantify the information loss in using the sentence vector as a substitute.
- **Why unresolved:** The theoretical justification relies on reconstructing $a_c$, but the practical implementation uses a heuristic approximation ($v_P$) to bypass this requirement.
- **What evidence would resolve it:** A comparative analysis of context initialization performance using $v_P$ versus alternative proxies (e.g., activation patterns from a specialized verifier model) to measure the gap in performance.

### Open Question 3
- **Question:** How does the theoretical upper bound on activation discrepancy change when applied to modern multi-layer transformers with residual connections?
- **Basis in paper:** [inferred] Lemma C.1 and Theorem D.3 rely on the assumption of a "one-block transformer" to derive bounds on attention activation discrepancy.
- **Why unresolved:** Real-world LLMs (e.g., Llama, Qwen) use deep stacks with layer normalization and residual connections. The single-block assumption may result in loose or invalid theoretical guarantees for these complex architectures.
- **What evidence would resolve it:** Empirical validation of the discrepancy bounds derived in the appendix when applied to multi-layer models, or a theoretical extension of the bounds to encompass deep residual networks.

## Limitations
- The orthogonal context initialization mechanism lacks direct empirical validation in MAD literature, with assumptions about latent-space orthogonality guaranteeing diverse reasoning perspectives not rigorously proven
- The adaptive α scheduling innovation doesn't fully explain how learning rates for α are tuned or what prevents oscillation in noisy optimization landscapes
- Computational efficiency claims (10% overhead) are based on comparisons to unspecified baseline runtime characteristics

## Confidence
- **High confidence:** The overall framework of dynamically evolving context instructions per agent is well-supported by ablation studies and cross-model transferability results
- **Medium confidence:** The orthogonal initialization and adaptive α mechanisms are theoretically sound and show performance gains, but lack direct validation of their individual contributions beyond ablation
- **Low confidence:** The claim that activation distance is more robust than embedding distance for measuring reasoning alignment is asserted but not empirically compared within the paper

## Next Checks
1. **Validate orthogonal initialization empirically:** Conduct an ablation where initial contexts are selected randomly (not via orthogonality-promoting selection) and compare performance and discrepancy intensity curves to full M2CL
2. **Analyze α adaptation stability:** Monitor α values across rounds for each task. If α oscillates or fails to adapt appropriately, tune the learning rates for α or add regularization to stabilize its trajectory
3. **Benchmark activation vs. embedding distance:** In a controlled experiment, measure both activation distance and embedding distance between LLM outputs at each round. Correlate each with final task accuracy to determine which better predicts reasoning alignment