---
ver: rpa2
title: 'MapTrace: Scalable Data Generation for Route Tracing on Maps'
arxiv_id: '2512.19609'
source_url: https://arxiv.org/abs/2512.19609
tags:
- path
- maps
- data
- synthetic
- mask
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MapTrace, a scalable synthetic data generation
  pipeline for training multimodal large language models to perform fine-grained spatial
  reasoning tasks such as route tracing on maps. The authors address the challenge
  of collecting pixel-accurate path annotations by automatically generating synthetic
  maps and using MLLM critics to filter high-quality paths.
---

# MapTrace: Scalable Data Generation for Route Tracing on Maps

## Quick Facts
- arXiv ID: 2512.19609
- Source URL: https://arxiv.org/abs/2512.19609
- Reference count: 24
- One-line primary result: Fine-tuning MLLMs on synthetic route tracing data improves success rates by up to 6.4 points and reduces path-tracing error on MapBench benchmark

## Executive Summary
This paper introduces MapTrace, a pipeline for generating synthetic map data to train multimodal large language models (MLLMs) for fine-grained spatial reasoning tasks like route tracing. The authors address the challenge of collecting pixel-accurate path annotations by automatically generating synthetic maps and using MLLM critics to filter high-quality paths. Their pipeline produces 23k path samples across 4k maps, demonstrating that fine-tuning models on this synthetic data improves success rates by up to 6.4 points and reduces path-tracing error (NDTW) on the MapBench benchmark.

## Method Summary
MapTrace generates synthetic maps using LLM-generated descriptions and text-to-image models, then extracts traversable regions via k-means clustering and graph-based pathfinding. Two MLLM critics (Mask Critic and Path Critic) filter valid masks and paths with 83% and 76% accuracy respectively. The pipeline produces 23k paths across 4k maps, which are used to fine-tune MLLMs to output normalized coordinate sequences. Models are evaluated on MapBench using Normalized Dynamic Time Warping (NDTW) and Success Rate (SR) metrics.

## Key Results
- Fine-tuning on MapTrace data improves success rates by up to 6.4 points on MapBench
- Path-tracing error (NDTW) is reduced compared to baseline models
- Synthetic-to-real transfer demonstrates that spatial reasoning can be taught with synthetic supervision
- Coordinate normalization (4 decimal precision) outperforms absolute and delta representations

## Why This Works (Mechanism)

### Mechanism 1: Synthetic-to-Real Transfer via Structural Supervision
- Claim: Fine-tuning on synthetically generated maps with precise annotations transfers to improved performance on real-world maps.
- Mechanism: The pipeline generates diverse synthetic maps with automatically computed shortest-path ground truth. Models learn generalizable spatial reasoning patterns—connectivity, traversability, path constraints—that transfer to real maps despite domain differences.
- Core assumption: Synthetic maps capture sufficient structural regularities that real wayfinding maps share.
- Evidence anchors: Abstract states fine-tuning improves success rates by up to 6.4 points on MapBench; section 6 confirms substantial performance gains on real-world maps sourced from the web.
- Break condition: Domain shift between synthetic and target real maps exceeds learned generalization.

### Mechanism 2: MLLM-as-Critic for Automated Quality Filtering
- Claim: MLLMs can reliably judge spatial validity of masks and paths even when they cannot generate valid paths themselves.
- Mechanism: Two-stage critic filtering—(1) Mask Critic evaluates whether color-segmented masks capture traversable regions; (2) Path Critic verifies paths stay within valid regions and avoid obstacles.
- Core assumption: MLLMs possess sufficient visual reasoning to evaluate spatial validity as binary judgments.
- Evidence anchors: Section 4, Table 2 shows Mask Critic achieves 83% accuracy with 9% false-positive rate; Path Critic achieves 76% accuracy with 8% false-positive rate.
- Break condition: Critic accuracy degrades significantly on edge cases, introducing systematic noise into training data.

### Mechanism 3: Coordinate-Level Supervision Teaches Explicit Spatial Reasoning
- Claim: Pixel-accurate path annotations provide supervision for fine-grained spatial reasoning that pretrained MLLMs lack.
- Mechanism: Models are trained to output normalized coordinate sequences representing valid paths, forcing learning of traversability constraints, obstacle avoidance, and connectivity.
- Core assumption: Spatial reasoning deficits in pretrained MLLMs stem from lack of explicit geometric supervision.
- Evidence anchors: Abstract states "fine-grained spatial reasoning, absent in pretrained models, can be explicitly taught with synthetic supervision."
- Break condition: Tasks require reasoning beyond coordinate-level path tracing.

## Foundational Learning

- **k-means Clustering for Image Segmentation**
  - Why needed: Extracts candidate traversable regions from synthetic maps via dominant color grouping; masks are binarized and filtered before graph construction.
  - Quick check question: Given an RGB map image, how would you segment it into k color clusters to isolate pathway regions from obstacles?

- **Graph-Based Pathfinding (Dijkstra's Algorithm)**
  - Why needed: Converts validated masks into pixel-graphs and computes optimal shortest paths as ground truth.
  - Quick check question: How does Dijkstra's algorithm guarantee finding the shortest path in a weighted graph with non-negative edge weights?

- **Dynamic Time Warping (DTW) for Sequence Alignment**
  - Why needed: Evaluates path quality by measuring geometric similarity between predicted and ground-truth coordinate sequences of varying lengths.
  - Quick check question: Why is DTW preferable to Euclidean distance when comparing two sequences that differ in length and sampling rate?

## Architecture Onboarding

- **Component map**: Description Generator (LLM) → Image Generator (Text-to-Image) → Mask Extraction (k-means + thresholding) → Mask Critic (MLLM) → Pixel-Graph Builder → Path Generator (Dijkstra + Ramer-Douglas-Peucker) → Path Critic (MLLM) → Training Sample

- **Critical path**: Description → Image → Mask Extraction → Mask Critic → Pixel-Graph → Path Generation → Path Critic → Training Sample. Both critics are bottlenecks; false positives here corrupt downstream data.

- **Design tradeoffs**:
  - **Synthetic vs. Real Data**: Synthetic enables scale (23k paths, 4k maps) and automatic ground truth, but risks domain shift to real maps
  - **Precision vs. Efficiency**: 4 decimal coordinate precision balances path fidelity vs. sequence length; lower precision degrades NDTW
  - **Critic Strictness**: Tighter filtering reduces false positives but may discard valid samples; current 8-9% FPR is empirically acceptable
  - **Coordinate Representation**: Normalized coordinates outperform absolute/delta (Table 3), likely due to pretraining on normalized object detection

- **Failure signatures**:
  - **Mask errors**: Background pixels included when colors resemble paths; thin paths labeled invalid
  - **Path errors**: Invalid detours into non-traversable regions (26%), overly long valid paths (14%), early off-path drift (14%), dead-end commitment (14%)
  - **NDTW regression**: Success rate increases can inflate average NDTW because harder queries (previously failed) are now attempted

- **First 3 experiments**:
  1. **Baseline validation**: Run Mask-Method (color segmentation + Dijkstra, no learning) on MapBench to establish non-learning performance ceiling.
  2. **Critic ablation**: Train models with and without Mask/Path Critic filtering to quantify impact of noisy samples on convergence and final NDTW.
  3. **Coordinate representation sweep**: Compare absolute, normalized, and delta coordinate formats at varying precision (2-4 decimals) to replicate Table 3-4 findings and identify optimal configuration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does fine-tuning on pixel-level path tracing transfer to downstream embodied navigation tasks or natural language instruction generation in dynamic environments?
- Basis in paper: The authors state that current evaluation is restricted to coordinate-level tracing and does not encompass "downstream tasks such as multimodal navigation instructions [or] embodied agents," explicitly listing this as future work.
- Why unresolved: The current work focuses solely on the geometric validity of paths on static 2D images, not on semantic instruction following or interaction with dynamic 3D environments.
- What evidence would resolve it: Benchmarks showing that MapTrace-finetuned models outperform baselines on embodied AI tasks (e.g., in AI2-THOR or Habitat) or generate more accurate textual navigation instructions.

### Open Question 2
- Question: How can evaluation metrics be redesigned to avoid penalizing models for successfully solving harder queries (the NDTW inflation problem)?
- Basis in paper: The paper notes that "apparent regressions in NDTW arise because models now solve a larger fraction of harder queries," and suggests "hybrid evaluation strategies" as a necessary next step.
- Why unresolved: Normalized Dynamic Time Warping (NDTW) is an average metric; as success rates rise on previously unattempted (difficult) cases, the average error can increase even if the model's overall capability has improved.
- What evidence would resolve it: A novel metric that weights path alignment by query difficulty or stratifies NDTW scores based on success/failure buckets to reflect true performance gains.

### Open Question 3
- Question: Does incorporating semi-synthetic or licensed real-world maps into the training pipeline significantly reduce domain shift compared to purely synthetic data?
- Basis in paper: In the Limitations section, the authors note the risk that synthetic maps "may still fail to capture the full diversity... found in commercial wayfinding maps," and propose exploring "semi-synthetic or real annotated maps" to improve generalization.
- Why unresolved: The current dataset relies entirely on text-to-image generated maps, which may lack the specific noise, style variations, and complexity of real-world cartography.
- What evidence would resolve it: A comparative study where models trained on mixed (real + synthetic) data outperform purely synthetic-trained models on a held-out set of highly diverse, real-world commercial maps.

## Limitations

- **Synthetic-to-real domain gap**: Despite improvements, synthetic maps may not fully capture the diversity and complexity of commercial wayfinding maps, potentially limiting generalization.
- **Critic reliability**: The MLLM critics achieve 83% and 76% accuracy but still allow 8-9% false positives that could propagate through training and affect final model quality.
- **Evaluation metric limitations**: NDTW can inflate average error when models successfully solve previously unattempted harder queries, potentially masking true performance improvements.

## Confidence

- **High Confidence**: The experimental results showing NDTW and success rate improvements on MapBench are well-supported with clear methodology and reproducible metrics.
- **Medium Confidence**: The synthetic-to-real transfer mechanism and the effectiveness of MLLM-as-critic filtering, while demonstrated, rely on assumptions about domain similarity and critic reliability that require further validation across diverse map types.
- **Medium Confidence**: The claim that spatial reasoning deficits stem from lack of explicit geometric supervision is supported but not definitively proven against alternative explanations.

## Next Checks

1. **Critic Reliability Audit**: Manually examine 100 random samples from the final dataset to measure actual false-positive rates and identify systematic error patterns in mask and path critics.
2. **Domain Transfer Stress Test**: Evaluate model performance on maps from domains not represented in the synthetic training set (e.g., topographic maps, transit maps) to quantify domain shift impact.
3. **Baseline Comparison**: Implement and compare against non-learning baselines (e.g., Mask-Method) on MapBench to better understand the contribution of learned spatial reasoning versus geometric heuristics.