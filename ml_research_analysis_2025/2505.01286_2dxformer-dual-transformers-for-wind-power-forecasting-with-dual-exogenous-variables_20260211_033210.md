---
ver: rpa2
title: '2DXformer: Dual Transformers for Wind Power Forecasting with Dual Exogenous
  Variables'
arxiv_id: '2505.01286'
source_url: https://arxiv.org/abs/2505.01286
tags:
- variables
- wind
- power
- exogenous
- forecasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes the 2DXformer, a dual transformer model for\
  \ wind power forecasting that addresses two key limitations in existing approaches:\
  \ lack of modeling for inter-variable relationships and treating endogenous and\
  \ exogenous variables equally. The model introduces three input types\u2014exogenous\
  \ static variables, exogenous dynamic variables, and endogenous variables\u2014\
  and uses two separate transformer blocks (ExTBlock and EnTBlock) with attention\
  \ mechanisms to capture correlations among exogenous variables and their impact\
  \ on endogenous variables."
---

# 2DXformer: Dual Transformers for Wind Power Forecasting with Dual Exogenous Variables

## Quick Facts
- arXiv ID: 2505.01286
- Source URL: https://arxiv.org/abs/2505.01286
- Reference count: 24
- Primary result: Dual Transformer architecture achieving MAE/RMSE of 44.00/81.62 and 46.85/95.06 on wind power datasets

## Executive Summary
This paper introduces 2DXformer, a dual transformer architecture designed to address key limitations in wind power forecasting: modeling inter-variable relationships and properly handling endogenous versus exogenous variables. The model separates processing of endogenous (wind power) and exogenous (weather, temporal) variables through distinct pathways, using two transformer blocks with attention mechanisms to capture correlations among exogenous variables and their impact on endogenous variables. Experimental results on two real-world datasets demonstrate state-of-the-art performance, with ablation studies confirming the importance of each architectural component.

## Method Summary
2DXformer employs an encoder-only architecture with two distinct transformer blocks: ExTBlock processes exogenous variables through dual attention mechanisms (inter-variable and spatial correlations), while EnTBlock processes endogenous variables through spatial attention followed by ResidualMLP fusion with exogenous context. The model handles three input types: exogenous static variables (weather features), exogenous dynamic variables (temporal embeddings), and endogenous variables (wind power). Training uses Smooth L1 loss with Adam optimizer and learning rate decay. The architecture is validated on SDWPF (134 turbines, 13 features) and HHL16 (33 turbines, 9 features) datasets.

## Key Results
- Achieves near-optimal performance with MAE/RMSE values of 44.00/81.62 and 46.85/95.06 on SDWPF and HHL16 datasets
- Outperforms traditional deep learning models, spatio-temporal models, and variable-based models
- Ablation studies show ResidualMLP in EnTBlock and exogenous dynamic variables are critical components (MAE increases of 67% and 28% when removed)
- Visualizations demonstrate accurate prediction alignment with actual wind power values

## Why This Works (Mechanism)

### Mechanism 1: Separated Processing of Endogenous and Exogenous Variables
The architecture bifurcates processing into ExTBlock (exogenous) and EnTBlock (endogenous), preventing unnecessary interactions that could degrade forecast accuracy. Exogenous variables are processed independently before their influence on endogenous variables is modeled via ResidualMLP rather than cross-attention.

### Mechanism 2: Dual Attention in ExTBlock for Inter-Variable and Spatial Correlations
ExTBlock applies two sequential self-attention operations with transposition between them, first capturing inter-variable relationships among exogenous variables, then spatial correlations across turbines.

### Mechanism 3: ResidualMLP for Exogenous-to-Endogenous Influence Transfer
A simple MLP with residual connections outperforms cross-attention for modeling how exogenous variables influence endogenous forecasts, preserving original endogenous features while allowing learned modifications from exogenous information.

## Foundational Learning

- **Endogenous vs. Exogenous Variables in Time Series**: The architectural innovation rests on distinguishing target variables (endogenous: wind power) from predictor variables (exogenous: wind speed, temperature, temporal features). Quick check: Given wind farm data with power output, wind speed, and timestamp columns, which would be endogenous, exogenous static, and exogenous dynamic?

- **Self-Attention with Dimension Transposition**: ExTBlock and EnTBlock use attention across different dimensions by transposing tensors. Quick check: If you have turbine embeddings shaped (batch, 134 turbines, 9 variables, 64 dims), what transposition would apply attention across turbines rather than variables?

- **Residual Connections in MLPs**: The ResidualMLP is the most critical component. Quick check: If ResidualMLP(x) = x + MLP(x), what happens to gradient flow to earlier layers if MLP(x) outputs near-zero values?

## Architecture Onboarding

- **Component map**: Input Embeddings → ExTBlock (dual attention + FFN) → EnTBlock (spatial attention → ResidualMLP → FFN) → Output MLP
- **Critical path**: 1) Embed all three variable types independently 2) Process exogenous through ExTBlock (variable attention → transpose → spatial attention → FFN) 3) Process endogenous through EnTBlock (spatial attention → concatenate with ExTBlock output → ResidualMLP → FFN) 4) Project final EnTBlock output to prediction horizon
- **Design tradeoffs**: ResidualMLP vs. Cross-Attention (simplicity vs. interpretability), Sequential vs. Joint Attention (computational simplicity vs. coupled correlation capture), Encoder-only architecture (simpler but may struggle with very long horizons)
- **Failure signatures**: High MAE on turbines with unusual patterns (over-smoothing), degradation when temporal features removed (critical for temporal embeddings), near-random predictions after ResidualMLP replacement (verify residual connection implementation)
- **First 3 experiments**: 1) Sanity check with single turbine (N=1) to verify temporal modeling 2) Ablation: ResidualMLP → Cross-Attention to replicate paper's severe degradation 3) Variable importance via embedding zeroing to reveal which exogenous variables the model actually uses

## Open Questions the Paper Calls Out

### Open Question 1
Can the 2DXformer architecture generalize effectively to other spatiotemporal domains beyond wind power, such as traffic or solar forecasting? The authors assert applicability to other multivariate spatiotemporal sequence forecasting tasks, but restrict validation to wind power datasets.

### Open Question 2
What is the computational efficiency trade-off of the dual-transformer architecture compared to single-block baselines? The introduction criticizes existing methods for increasing model complexity, yet no efficiency metrics are provided for the proposed architecture.

### Open Question 3
Why does the ResidualMLP significantly outperform attention mechanisms in the EnTBlock for fusing exogenous and endogenous variables? The ablation study reveals severe performance degradation when replaced, but offers only empirical validation rather than theoretical explanation.

## Limitations
- Relies on ablation studies rather than direct comparisons against strong baselines like DeepAR or Informer
- Assumes unidirectional exogenous→endogenous influence, which may not hold for turbines where power output affects local temperature
- Sequential processing of inter-variable and spatial correlations may miss joint patterns that joint modeling could capture

## Confidence

- **High Confidence**: Architectural design choices (separate ExTBlock/EnTBlock, ResidualMLP for transfer) are well-justified through ablation studies showing substantial performance degradation
- **Medium Confidence**: Near-optimal performance claims relative to deep learning and spatio-temporal models, though limited baseline comparisons and lack of statistical significance testing introduce uncertainty
- **Medium Confidence**: Superiority of ResidualMLP over cross-attention for exogenous-to-endogenous transfer, supported by strong ablation evidence but limited corpus validation

## Next Checks

1. Replicate the ResidualMLP vs. cross-attention ablation on held-out validation set to verify the 30+ MAE degradation is reproducible
2. Conduct statistical significance testing (paired t-tests) across turbines to confirm performance differences are non-random
3. Test model behavior with bidirectional exogenous-endogenous relationships by introducing synthetic feedback loops in the data