---
ver: rpa2
title: One Token Embedding Is Enough to Deadlock Your Large Reasoning Model
arxiv_id: '2510.15965'
source_url: https://arxiv.org/abs/2510.15965
tags:
- attack
- reasoning
- wait
- adversarial
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents the Deadlock Attack, a novel method that exploits\
  \ a vulnerability in large reasoning models (LRMs) by training a single adversarial\
  \ token embedding to induce perpetual reasoning loops, effectively exhausting the\
  \ model\u2019s computational resources. The attack works by encouraging the model\
  \ to generate reflective or hesitant tokens (e.g., \u201CWait,\u201D \u201CBut\u201D\
  ) after reasoning steps, preventing the model from reaching a conclusion."
---

# One Token Embedding Is Enough to Deadlock Your Large Reasoning Model

## Quick Facts
- **arXiv ID**: 2510.15965
- **Source URL**: https://arxiv.org/abs/2510.15965
- **Reference count**: 40
- **Primary result**: 100% ASR on GSM8K, MATH500, MMLU-Pro forcing LRMs to hit max token limit

## Executive Summary
This paper introduces the Deadlock Attack, a method that exploits a vulnerability in Large Reasoning Models (LRMs) by training a single adversarial token embedding to induce perpetual reasoning loops. The attack forces models to repeatedly generate reflective or hesitant tokens like "Wait" or "But" after punctuation, preventing them from reaching conclusions and exhausting computational resources. By implanting this embedding into a trigger token's embedding, the attack achieves reliable activation and remains stealthy, causing negligible performance degradation on benign inputs while remaining robust against existing test-time computing strategies.

## Method Summary
The attack optimizes an adversarial embedding to maximize the probability of transitional tokens appearing after punctuation in reasoning traces. Training uses 20 samples from MATH500 level 5, generating 100 reasoning traces per sample with R1-Qwen. The embedding is trained via gradient descent to encourage models to produce tokens like "Wait" or "But" after punctuation marks. Due to the continuous-to-discrete projection gap, the optimized embedding is implanted into the model's embedding matrix by overwriting a trigger token's embedding (e.g., "!!!!!"). This backdoor approach enables reliable activation through specific inputs while maintaining stealth on benign queries.

## Key Results
- 100% attack success rate across four state-of-the-art LRMs (Phi-RM, Nemotron-Nano, R1-Qwen, R1-Llama)
- Models forced to hit maximum token generation limits (typically 4000 tokens) on GSM8K, MATH500, and MMLU-Pro
- Negligible performance degradation on benign inputs, maintaining attack stealth
- Robust against test-time computing strategies designed to mitigate overthinking

## Why This Works (Mechanism)
The attack exploits the iterative reasoning mechanism of LRMs by training an adversarial embedding that encourages generation of transitional tokens after punctuation. This creates a feedback loop where the model repeatedly questions its reasoning without reaching conclusions. The key technical challenge is the continuous-to-discrete projection gap - directly converting the optimized embedding to a discrete token fails because the nearest vocabulary token loses the attack properties. The solution is backdoor implantation, where the adversarial vector is embedded directly into the model's parameters via a trigger token.

## Foundational Learning
- **Token embedding optimization**: Why needed - to create adversarial vectors that influence token generation patterns. Quick check - verify gradient updates affect token probabilities as expected.
- **Continuous-to-discrete projection gap**: Why needed - understanding why naïve token conversion fails is crucial for attack design. Quick check - test if projected embeddings maintain attack effectiveness.
- **Backdoor implantation**: Why needed - enables reliable activation of adversarial behavior through trigger tokens. Quick check - confirm trigger token replacement preserves attack properties.

## Architecture Onboarding

**Component map**: Adversarial embedding optimization -> Backdoor implantation -> Trigger token activation -> Deadlock generation

**Critical path**: Training loop (input construction → loss computation → gradient update) → Embedding implantation → Inference with trigger → Deadlock manifestation

**Design tradeoffs**: 
- Embedding length L=1 chosen for minimal perturbation vs longer embeddings
- Trigger token selection balances rarity (for stealth) vs memorability
- Training sample size N=20 balances coverage vs computational cost

**Failure signatures**: 
- Attack fails to generalize beyond training samples (validation ASR low)
- Projected embeddings lose effectiveness (loss resets to baseline)
- Trigger token activation produces normal reasoning (no deadlock)

**First experiments**:
1. Train with N=1 sample to verify basic attack functionality
2. Test projection vs implantation approaches to confirm continuous-to-discrete gap
3. Evaluate trigger token replacement on validation samples to measure ASR

## Open Questions the Paper Calls Out
- Can principled methods be developed to reliably translate continuous adversarial embeddings into discrete token triggers without relying on backdoor implantation? The paper notes this remains a critical open challenge after multiple failed approaches including naïve projection, Gaussian smoothing, iterative projection, and PCA dimensionality reduction.
- Can query-based or zeroth-order optimization strategies craft effective deadlock-inducing triggers directly on closed-source model APIs? Current attack requires white-box access for gradient-based optimization; black-box settings remain unexplored.
- Do deadlock-inducing patterns transfer across different LRM families and architectures? Experiments only cover four similar reasoning-focused models; cross-family transfer is untested.

## Limitations
- Requires white-box access for gradient-based embedding optimization, limiting black-box applicability
- Precise token sets (T_punct and T_trans) are unspecified, requiring experimentation for faithful reproduction
- Robustness claims against defensive strategies lack systematic ablation or stress-testing under varied settings

## Confidence
- **High**: Attack methodology and 100% ASR claims are clearly described and reproducible
- **Medium**: Stealthiness claim lacks detailed baseline comparisons and statistical significance tests
- **Medium**: Robustness against defensive strategies supported qualitatively but not systematically tested

## Next Checks
1. Verify exact token IDs for sentence-ending punctuation (T_punct) and transitional tokens (T_trans) in the target model's tokenizer, ensuring alignment with the paper's intended sets
2. Reproduce the training loop with N=20 training samples and L=1 embedding length, monitoring validation ASR and loss to confirm convergence and generalization
3. Test the implanted backdoor on GSM8K with and without the trigger token, measuring ASR, average tokens, and time, and comparing to baseline model performance to confirm stealthiness