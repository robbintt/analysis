---
ver: rpa2
title: 'DecoyDB: A Dataset for Graph Contrastive Learning in Protein-Ligand Binding
  Affinity Prediction'
arxiv_id: '2507.06366'
source_url: https://arxiv.org/abs/2507.06366
tags:
- complexes
- binding
- learning
- affinity
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DecoyDB, a large-scale dataset of protein-ligand
  complexes augmented with computationally generated decoys for graph contrastive
  learning. The dataset contains 61,104 high-resolution ground truth complexes and
  5,353,307 decoys, each annotated with RMSD values from native poses.
---

# DecoyDB: A Dataset for Graph Contrastive Learning in Protein-Ligand Binding Affinity Prediction

## Quick Facts
- arXiv ID: 2507.06366
- Source URL: https://arxiv.org/abs/2507.06366
- Reference count: 40
- Primary result: Large-scale dataset with 61K ground truth complexes and 5.3M decoys for protein-ligand binding affinity prediction

## Executive Summary
This paper introduces DecoyDB, a comprehensive dataset designed to enable graph contrastive learning for protein-ligand binding affinity prediction. The dataset contains 61,104 high-resolution ground truth complexes paired with 5,353,307 computationally generated decoys, each annotated with RMSD values relative to native poses. The authors develop a customized graph contrastive learning framework that employs a novel two-category negative sampling strategy (combining continuous decoys and discrete real complexes) along with denoising score matching regularization. Through extensive experiments, the study demonstrates that models pre-trained on DecoyDB achieve significant improvements in binding affinity prediction accuracy, sample efficiency during fine-tuning, and generalizability across different evaluation settings.

## Method Summary
The method centers on DecoyDB, which contains 61,104 ground truth protein-ligand complexes and 5,353,307 decoys generated through computational approaches. The authors design a graph contrastive learning framework that leverages this dataset through a two-category negative sampling strategy: continuous decoys (generated decoys with varying RMSD values) and discrete real complexes (actual binding poses from other proteins). The framework incorporates denoising score matching regularization to enhance representation learning. Models are pre-trained on this contrastive task and then fine-tuned on downstream binding affinity prediction tasks. The approach is evaluated across multiple GNN architectures and demonstrates consistent performance improvements over baseline methods.

## Key Results
- Pre-trained models reduce RMSE from 1.263 to 1.188 on PDBbind2016 core set
- Significant improvements in sample efficiency during fine-tuning on binding affinity tasks
- Enhanced generalizability on leakage-proof dataset splits
- Outperforms existing pre-training methods across multiple GNN base models

## Why This Works (Mechanism)
The effectiveness of DecoyDB stems from its ability to provide rich, diverse negative examples that capture the conformational space around binding poses. By including decoys at varying RMSD distances from native poses, the contrastive learning task forces models to learn fine-grained structural distinctions that are critical for binding affinity prediction. The two-category negative sampling strategy (continuous decoys and discrete real complexes) provides complementary learning signals - continuous decoys teach models to distinguish subtle conformational differences, while discrete real complexes help learn absolute binding characteristics. The denoising score matching regularization further enhances representation quality by encouraging models to recover clean representations from corrupted inputs.

## Foundational Learning
- Graph neural networks for molecular structures: Needed to process protein-ligand complexes as graph representations; Quick check: Verify GNN can capture both local atomic interactions and global binding site context
- Contrastive learning fundamentals: Required to understand the self-supervised pre-training objective; Quick check: Ensure contrastive loss properly distinguishes positive pairs from hard negative samples
- Protein-ligand docking and RMSD calculation: Essential for understanding decoy generation and quality assessment; Quick check: Confirm RMSD threshold selection aligns with biological relevance
- Binding affinity prediction metrics: Necessary to evaluate model performance; Quick check: Verify evaluation metrics capture both prediction accuracy and ranking quality

## Architecture Onboarding

Component Map: Protein-ligand complex graphs -> Contrastive learning module (with two-category negative sampling) -> DENOISING score matching regularization -> Pre-trained GNN weights -> Fine-tuning on binding affinity task

Critical Path: The core innovation lies in the combination of extensive decoy generation with the two-category negative sampling strategy. The continuous decoys provide fine-grained learning signals about conformational similarity, while discrete real complexes offer absolute binding information. The denoising score matching acts as a regularizer that enhances representation quality beyond standard contrastive learning.

Design Tradeoffs: The authors trade computational cost (generating 5.3M decoys) for improved pre-training signal quality. This represents a significant resource investment but yields measurable performance gains. The choice of two-category sampling versus single-category approaches reflects a balance between learning local conformational distinctions versus global binding characteristics.

Failure Signatures: Potential failure modes include: (1) decoy generation that doesn't adequately sample the conformational space around native poses, leading to insufficient contrastive signals; (2) imbalance in negative sampling categories that biases learning toward either local or global features; (3) overfitting to decoy patterns that don't generalize to real binding scenarios.

First Experiments: (1) Ablation study comparing two-category negative sampling versus single-category approaches; (2) Analysis of decoy diversity and its correlation with downstream performance; (3) Comparison of denoising score matching versus standard contrastive learning without regularization

## Open Questions the Paper Calls Out
None

## Limitations
- Decoy generation may not fully capture the diversity of real binding scenarios despite the large scale
- Focus primarily on structure-based affinity prediction without addressing complementarity with sequence-based methods
- Limited discussion of computational requirements and scalability for generating and storing the decoy database

## Confidence

**Major Claim Confidence Assessment:**
- Dataset construction methodology: High confidence - Detailed protocols provided for decoy generation and RMSD computation
- Pre-training performance improvements: Medium confidence - Significant improvements shown, but lacks ablation studies on component contributions
- Generalization claims: Medium confidence - Results on leakage-proof splits demonstrate robustness, but validation on truly independent test sets would strengthen claims

## Next Checks
1. Conduct ablation studies to isolate the impact of the two-category negative sampling strategy versus denoising score matching regularization on overall performance
2. Test model performance on additional independent binding affinity benchmarks beyond PDBbind to verify generalizability claims
3. Evaluate the computational cost-benefit tradeoff of pre-training on DecoyDB versus alternative approaches for different GNN architectures and target sizes