---
ver: rpa2
title: Evaluating OCR performance on food packaging labels in South Africa
arxiv_id: '2510.03570'
source_url: https://arxiv.org/abs/2510.03570
tags:
- text
- packaging
- tesseract
- images
- trocr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study benchmarks four open-source OCR systems\u2014Tesseract,\
  \ EasyOCR, PaddleOCR, and TrOCR\u2014on real-world food packaging images from South\
  \ Africa, focusing on ingredient lists and nutrition facts panels. A dataset of\
  \ 231 products (1,628 images) was processed to assess speed and coverage, and a\
  \ ground-truth subset of 113 images (60 products) was used for accuracy evaluation."
---

# Evaluating OCR performance on food packaging labels in South Africa

## Quick Facts
- arXiv ID: 2510.03570
- Source URL: https://arxiv.org/abs/2510.03570
- Reference count: 29
- Primary result: Tesseract achieved the lowest CER (0.912) and highest BLEU (0.245) on South African food packaging OCR

## Executive Summary
This study benchmarks four open-source OCR systems—Tesseract, EasyOCR, PaddleOCR, and TrOCR—on real-world food packaging images from South Africa, focusing on ingredient lists and nutrition facts panels. A dataset of 231 products (1,628 images) was processed to assess speed and coverage, and a ground-truth subset of 113 images (60 products) was used for accuracy evaluation. Tesseract achieved the lowest character error rate (0.912) and highest BLEU score (0.245), demonstrating the strongest semantic accuracy. EasyOCR provided a good balance between accuracy and multilingual support. PaddleOCR achieved near-complete coverage but was slower due to CPU-only execution. TrOCR produced the weakest results despite GPU acceleration. The findings establish a packaging-specific OCR benchmark, highlight the trade-offs between accuracy and coverage, and emphasize the need for layout-aware methods in complex, multilingual packaging scenarios.

## Method Summary
The study evaluated four OCR systems on South African food packaging images from the SA NFP 2023 dataset (231 products, 1,628 images). Tesseract, EasyOCR, and PaddleOCR used keyword-based classification for post-processing, while TrOCR employed fuzzy matching. Preprocessing included grayscale conversion, contrast enhancement, and denoising for Tesseract; TrOCR used RGB normalization and horizontal line-slicing for dense tables. Metrics included CER, WER, BLEU, ROUGE-L, F1, coverage, and execution time. Ground truth evaluation was performed on 113 images (60 products) manually transcribed. Hardware consisted of Windows 11, Intel 24-core/32-thread CPU, 128GB RAM, and NVIDIA RTX 3070 GPU.

## Key Results
- Tesseract achieved the lowest CER (0.912) and highest BLEU (0.245) scores
- PaddleOCR achieved highest coverage (98.31%) but was slowest due to CPU-only execution (6.24s/image)
- TrOCR produced weakest semantic accuracy despite GPU acceleration
- EasyOCR provided balanced performance with multilingual support (91.53% coverage)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LSTM-based OCR with preprocessing achieves higher semantic accuracy on structured packaging text than transformer-based alternatives.
- **Mechanism:** Tesseract's LSTM recognition module combined with grayscale conversion, contrast enhancement, and denoising preserves lexical structure on dense tabular layouts (nutrition panels) better than end-to-end neural approaches that lack explicit line segmentation.
- **Core assumption:** Preprocessing enhancements generalize beyond the tested dataset to other real-world packaging with similar noise profiles (glare, curved surfaces).
- **Evidence anchors:**
  - [abstract] "Tesseract achieved the lowest CER (0.912) and the highest BLEU (0.245)"
  - [Section 5.6] "Tesseract's advantage over TrOCR can be attributed to its optimization for printed text and its ability to process structured layouts with minimal adjustments"
  - [corpus] Weak relevance—neighbor papers focus on LLMs and moderation, not OCR architectures.
- **Break condition:** When fonts are highly stylized or multilingual text mixes scripts within single text blocks, LSTM segmentation fails (Tesseract coverage drops to 79.66%).

### Mechanism 2
- **Claim:** Modular detection-recognition pipelines with multilingual support provide better coverage-accuracy trade-offs than single-stage architectures.
- **Mechanism:** EasyOCR and PaddleOCR decouple text detection (CRAFT, DBNet) from recognition (CRNN), allowing language-agnostic region detection before script-specific decoding—this increases field-level coverage while maintaining competitive accuracy.
- **Core assumption:** Detection modules generalize across packaging layouts without domain-specific fine-tuning.
- **Evidence anchors:**
  - [abstract] "EasyOCR provided a good balance between accuracy and multilingual support"
  - [Section 5.4] "PaddleOCR at 98.31%, EasyOCR at 91.53%... higher coverage than Tesseract"
  - [corpus] No direct corpus support for modular OCR trade-offs.
- **Break condition:** CPU-only execution nullifies speed advantages; PaddleOCR's 6.24s/image (10× slower than Tesseract) makes it impractical for large-scale deployment without GPU.

### Mechanism 3
- **Claim:** Transformer-based OCR without domain-specific fine-tuning underperforms on dense, multi-column packaging layouts.
- **Mechanism:** TrOCR's ViT encoder processes full images as sequences without explicit layout modeling, causing output fragmentation when text blocks have irregular spatial structure—despite GPU acceleration, contextual reasoning fails without positional priors.
- **Core assumption:** Fine-tuning on packaging-specific data would improve performance (not tested in this study).
- **Evidence anchors:**
  - [abstract] "TrOCR produced the weakest results despite GPU acceleration"
  - [Section 5.6] "TrOCR... recorded the lowest accuracy, highlighting the challenges of applying transformer-based models to complex packaging layouts without domain-specific tuning"
  - [corpus] Weak—E-ARMOR paper mentions multilingual OCR challenges but doesn't address transformers specifically.
- **Break condition:** If images are pre-segmented into single text lines (as attempted in the study), TrOCR may perform better—but this requires accurate upstream segmentation.

## Foundational Learning

- **Concept:** Character Error Rate (CER) vs. Word Error Rate (WER)
  - **Why needed here:** CER > 1.0 is possible when insertions dominate on short reference strings; understanding this prevents misinterpreting results as errors.
  - **Quick check question:** If a model outputs 15 characters against a 10-character ground truth with 3 substitutions and 2 insertions, what is the CER?

- **Concept:** BLEU/ROUGE-L limitations on short strings
  - **Why needed here:** Low BLEU scores (0.010–0.245) are expected on packaging text; don't over-index on semantic metrics for model selection.
  - **Quick check question:** Why might BLEU penalize correct-but-reordered ingredient lists?

- **Concept:** Coverage vs. accuracy trade-offs
  - **Why needed here:** A model with 100% coverage but BLEU=0.010 (TrOCR) may be worse than 80% coverage with BLEU=0.245 (Tesseract) depending on downstream task.
  - **Quick check question:** For regulatory compliance checking, would you prioritize coverage or semantic accuracy?

## Architecture Onboarding

- **Component map:** Input → Preprocessing (grayscale, denoising, contrast) → OCR Engine (4 options) → Post-processing (keyword classification, normalization) → Metrics (CER, WER, BLEU, ROUGE-L, F1)

- **Critical path:**
  1. Keyword-based sectioning to isolate ingredient lists and NFP from full-package images
  2. Normalization pipeline (lowercase, special character removal, whitespace collapse)
  3. Metric computation on ground-truth subset (113 images)

- **Design tradeoffs:**
  - Tesseract: High accuracy, low coverage, CPU-only, fastest
  - EasyOCR: Balanced, GPU-accelerated, multilingual
  - PaddleOCR: Highest coverage, CPU-slow, requires GPU for production
  - TrOCR: Full coverage but semantically useless without fine-tuning

- **Failure signatures:**
  - Empty OCR output → likely noise/font sensitivity (Tesseract)
  - Fragmented/gibberish output → transformer on dense layout (TrOCR)
  - CER > 1.0 → short reference with excessive insertions (all models on mixed-language strings)

- **First 3 experiments:**
  1. Reproduce Tesseract baseline with preprocessing pipeline on 20 sample images; verify CER < 1.0.
  2. Test PaddleOCR with GPU enabled (resolve CUDA compatibility); compare timing to CPU baseline.
  3. Fine-tune TrOCR on 50 annotated packaging images; measure BLEU improvement vs. pre-trained model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would domain-specific fine-tuning enable transformer-based OCR models (e.g., TrOCR) to match or exceed LSTM/CNN-based systems on food packaging text?
- Basis in paper: [explicit] The authors state: "TrOCR, although GPU accelerated, recorded the lowest accuracy, highlighting the challenges of applying transformer-based models to complex packaging layouts without domain-specific tuning." The conclusion also calls for future work on "fine-tuning OCR models for packaging text."
- Why unresolved: TrOCR was evaluated only with pre-trained weights, and no fine-tuning experiments were conducted on the packaging dataset.
- What evidence would resolve it: Fine-tune TrOCR on a subset of the food packaging images and measure CER, WER, and BLEU against the same ground truth.

### Open Question 2
- Question: How would PaddleOCR's speed and accuracy trade-off change with GPU acceleration compared to its CPU-only execution?
- Basis in paper: [explicit] The paper notes: "PaddleOCR achieved near complete coverage but was slower because it ran on CPU only due to GPU incompatibility." The discussion states: "Although the system supports GPU acceleration, compatibility issues with CUDA prevented its use in this setup."
- Why unresolved: PaddleOCR was benchmarked at 6.24 seconds per image on CPU; GPU performance could not be tested in this study.
- What evidence would resolve it: Re-run PaddleOCR benchmarks with a compatible GPU (e.g., CUDA-enabled environment) and compare execution time and accuracy metrics to the CPU baseline.

### Open Question 3
- Question: Can region-aware or layout-aware detection modules improve OCR accuracy on dense tabular structures such as nutrition facts panels?
- Basis in paper: [explicit] The abstract emphasizes "the need for layout-aware methods in complex, multilingual packaging scenarios," and the conclusion identifies "integrating region-aware detection methods" as a key future direction.
- Why unresolved: All models processed full packaging images without region segmentation, and structured tables remained a major error source.
- What evidence would resolve it: Integrate a text detection/localization stage (e.g., DBNet or a custom table detector) before OCR recognition and evaluate per-field accuracy on nutrition panels.

### Open Question 4
- Question: How does OCR performance vary across different languages and mixed-language strings within South African food packaging?
- Basis in paper: [inferred] The dataset "includes multiple languages and mixed language strings," but the paper treats "text uniformly regardless of language or mixing" during evaluation. This limits understanding of which models handle code-switching or specific South African languages better.
- Why unresolved: No language-stratified error analysis was performed, and CER/WER were computed without language-specific rules.
- What evidence would resolve it: Annotate ground truth by language, then compute language-specific CER, WER, and coverage for each OCR system.

## Limitations
- The SA NFP 2023 dataset is not publicly available, limiting independent verification
- Specific preprocessing parameters and keyword classification lists remain unspecified
- No fine-tuning experiments were conducted to test transformer model potential
- Cross-dataset validation and multilingual script mixing analysis were not performed

## Confidence
- **High Confidence**: Tesseract's superior CER and BLEU performance on the tested dataset, CPU-only execution advantages, and the documented coverage-accuracy trade-offs across all four models.
- **Medium Confidence**: The generalizability of preprocessing enhancements beyond the tested dataset, the assumption that detection-recognition decoupling inherently improves multilingual coverage, and the architectural explanations for TrOCR's underperformance.
- **Low Confidence**: The assertion that transformer-based OCR will consistently underperform on dense packaging layouts without domain-specific fine-tuning, as this remains untested in the study.

## Next Checks
1. **Dataset Accessibility Test**: Attempt to obtain or replicate the SA NFP 2023 dataset, or curate a comparable food packaging dataset with ground truth transcriptions for 60 products (113 images) following the paper's naming conventions and content focus.

2. **Architectural Ablation Study**: Reproduce the four OCR systems with varying preprocessing intensities (minimal vs. full pipeline) and evaluate whether Tesseract's performance advantage persists when competitors receive equivalent preprocessing treatment.

3. **Fine-tuning Validation**: Fine-tune TrOCR on 50 annotated packaging images from the dataset and measure BLEU score improvement against the pre-trained baseline, testing the paper's hypothesis about transformer models' domain-specific limitations.