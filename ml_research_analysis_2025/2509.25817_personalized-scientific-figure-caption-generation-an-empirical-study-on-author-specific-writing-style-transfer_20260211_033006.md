---
ver: rpa2
title: 'Personalized Scientific Figure Caption Generation: An Empirical Study on Author-Specific
  Writing Style Transfer'
arxiv_id: '2509.25817'
source_url: https://arxiv.org/abs/2509.25817
tags:
- quality
- caption
- figure
- profile
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies personalized scientific figure caption generation
  using author profile data from scientific papers. The authors develop a quality
  evaluator to filter low-quality captions and fine-tune a multimodal LLM (Qwen-2.5-VL-7B)
  with author-specific profile data to generate personalized captions.
---

# Personalized Scientific Figure Caption Generation: An Empirical Study on Author-Specific Writing Style Transfer

## Quick Facts
- arXiv ID: 2509.25817
- Source URL: https://arxiv.org/abs/2509.25817
- Reference count: 3
- Primary result: Quality-aware training enables joint optimization of author style and caption informativeness in scientific figure captioning

## Executive Summary
This paper addresses personalized scientific figure caption generation by leveraging author profile data from scientific papers. The authors develop a quality evaluator to filter low-quality captions from training data and fine-tune a multimodal LLM (Qwen-2.5-VL-7B) with author-specific profile data to generate personalized captions. Experiments demonstrate that increasing profile data quantity consistently improves BLEU and ROUGE metrics, with the fine-tuned model achieving competitive performance against larger models. However, the analysis reveals a fundamental trade-off between personalization and caption quality, as optimizing for author style matching alone may compromise caption informativeness.

## Method Summary
The approach employs a two-stage pipeline: first, a quality evaluator (Qwen-2.5-VL-3B) is fine-tuned on 3,000 GPT-4.1-scored samples to filter low-quality captions (keeping only quality ≥3); second, the caption generator (Qwen-2.5-VL-7B) is fine-tuned on the filtered dataset using author profile data (related figures, paragraphs, mentions, OCR, and captions from the same paper). The method introduces a quality-aware training paradigm where the model jointly predicts caption quality and generates captions conditioned on that quality score, enabling inference-time control over the personalization-quality trade-off.

## Key Results
- Increasing profile data quantity from 1 to 3 profiles improves BLEU-4 from 0.281 to 0.339
- Fine-tuned Qwen-2.5-VL-7B achieves competitive performance against larger models on BLEU/ROUGE metrics
- Quality-aware training with Forced-Q6 inference produces more informative captions while maintaining personalization
- Standard BLEU/ROUGE metrics penalize higher-quality captions when ground truth is low quality

## Why This Works (Mechanism)

### Mechanism 1: Quality-Guided Data Filtering
- Claim: Filtering low-quality captions from training data improves downstream model performance
- Mechanism: A fine-tuned Qwen-2.5-VL-3B evaluator (f_quality) scores captions 1-6 using synthetic labels from GPT-4.1; only samples with predicted quality ≥3 are retained for caption generator training
- Core assumption: LLM-generated quality scores generalize to human judgments (prior work showed 0.5 correlation with doctoral student evaluations on SciCap-Eval)
- Evidence anchors: [abstract] "develop a quality evaluator to filter low-quality captions"; [section] Page 2 reports f_quality achieves Spearman correlation 0.759 and QWK 0.754 with GPT-4.1 on validation set; [corpus] Related work (Kim et al., 2025) demonstrated that filtering low-quality captions improves fine-tuning performance

### Mechanism 2: Profile-Based Style Conditioning
- Claim: Providing author-specific profile data (related figures, paragraphs, mentions, OCR, captions from the same paper) enables style capture
- Mechanism: Multimodal LLM receives target figure + N profile examples; model learns stylistic patterns (tone, structure, vocabulary) from profile captions and transfers them to target generation
- Core assumption: Author writing style is consistent across figures within a single paper and can be extracted from limited examples
- Evidence anchors: [abstract] "rich author profile data, combined with relevant metadata, can improve the personalization performance"; [section] Table 1 shows BLEU-4 improves from 0.281 (1 profile) to 0.339 (3 profiles); [corpus] LaMP-Cap (Ng et al., 2025) introduced multimodal figure profiles for personalization

### Mechanism 3: Quality-Aware Controllable Generation
- Claim: Joint training on quality prediction and caption generation enables inference-time control over the quality-personalization trade-off
- Mechanism: Model is trained to (1) predict target caption quality score, then (2) generate caption conditioned on that score; at inference, forcing Q=6 steers generation toward higher quality
- Core assumption: Quality and style occupy partially orthogonal subspaces in representation space
- Evidence anchors: [abstract] "conditioning on high quality scores can produce more informative captions while maintaining personalization"; [section] Table 3 shows Forced-Q6 yields higher quality scores (4.956 vs 4.917 for Q6 targets) but lower ROUGE-L when ground truth is low quality

## Foundational Learning

- Concept: **Multimodal LLM Architecture (Vision Encoder + LM Decoder)**
  - Why needed here: Task requires jointly processing visual (figures) and textual (paragraphs, mentions, OCR) inputs
  - Quick check question: How does Qwen-2.5-VL route visual tokens vs. text tokens through its transformer layers?

- Concept: **N-gram Overlap Metrics (BLEU, ROUGE)**
  - Why needed here: Paper relies on BLEU/ROUGE but identifies their limitation—they penalize higher-quality captions when ground truth is low quality
  - Quick check question: Why might a more informative caption receive a lower ROUGE-L score than a vague but lexically-matching caption?

- Concept: **Supervised Fine-Tuning with Quality Filtering**
  - Why needed here: Understanding how dataset curation (filtering quality ≤2) shapes model behavior is critical for reproduction
  - Quick check question: Given 50%+ of arXiv captions are low quality (Huang et al., 2023), what fraction of training data would filtering exclude?

## Architecture Onboarding

- Component map:
  f_quality (Quality Evaluator) -> g_caption (Caption Generator) -> Quality-Aware Variant

- Critical path:
  1. Run f_quality on all training samples → filter to quality ≥3
  2. Organize profile data: for each target, collect N related figures with (F, P, M, O, C)
  3. Fine-tune g_caption: AdamW, lr=2e-5, batch_size=2, 1 epoch
  4. For quality-aware: add quality prediction as prefix task; at inference, try both Predicted-Q and Forced-Q6

- Design tradeoffs:
  - More profiles → better BLEU/ROUGE but longer inference context
  - Quality filtering → cleaner training but reduced data volume
  - Forced-Q6 → higher objective quality but potentially lower ROUGE if ground truth is weak
  - BLEU/ROUGE evaluation → standard but may conflate style-matching with actual quality

- Failure signatures:
  - Generated captions average ~3 quality despite filtering → model optimizing token match over quality signals
  - ROUGE drops with Forced-Q6 on low-quality ground truth → metric artifact, not model failure
  - Profile data doesn't improve metrics → check profile-target relevance; author style may be inconsistent

- First 3 experiments:
  1. **Baseline without profiles**: Run g_caption with target figure only (N=0 profiles) to establish BLEU/ROUGE floor
  2. **Progressive profile ablation**: Test F+C → F+P+C → F+P+M+C → F+P+M+O+C to isolate component contributions (replicate Table 2)
  3. **Quality-aware inference sweep**: Compare Predicted-Q vs Forced-Q6 across ground-truth quality bins (Q1-Q6) to map where Forced-Q6 helps vs. hurts ROUGE

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can evaluation metrics be designed to effectively disentangle author style fidelity from factual semantic quality in personalized scientific text generation?
- Basis in paper: [explicit] The conclusion states that the findings "motivates the need for complementary evaluation metrics that separately measure style alignment and factual/semantic quality," noting that standard metrics like BLEU/ROUGE conflate the two.
- Why unresolved: Currently, high n-gram overlap (ROUGE) may simply indicate the model successfully mimicked an author's bad habits (low quality), creating a false positive for performance.
- What evidence would resolve it: The development and validation of a novel metric that shows low correlation with lexical overlap but high correlation with human judgments of style preservation and factual accuracy independently.

### Open Question 2
- Question: Can a unified training objective be formulated to achieve Pareto optimality between personalization and caption quality without requiring inference-time intervention?
- Basis in paper: [inferred] The paper identifies a "fundamental trade-off" and uses a "quality-aware training paradigm" with inference-time conditioning (Forced-Q6) to manage it. However, this suggests the base model does not inherently learn to optimize both simultaneously.
- Why unresolved: The paper demonstrates that forcing high quality (Forced-Q6) degrades ROUGE scores (style matching), implying the two objectives currently compete rather than synergize in the model's representation space.
- What evidence would resolve it: A training methodology where the model achieves high performance on both quality scores and style metrics simultaneously, without requiring a hard switch between different conditioning signals at inference time.

### Open Question 3
- Question: Does the optimization of synthetic "quality scores" inadvertently increase the risk of hallucination or factual inconsistency in the generated scientific captions?
- Basis in paper: [inferred] The paper uses a "quality evaluator" trained on GPT-4.1 labels to define informativeness. However, the correlation between this synthetic evaluator and human doctoral students was only 0.5 in prior work, leaving a significant gap in validating "factual" correctness.
- Why unresolved: Optimizing for a proxy of "informativeness" (the quality score) might encourage the model to generate fluent, detailed-sounding text that is factually incorrect relative to the figure, a risk not explicitly measured in the experiments.
- What evidence would resolve it: A human evaluation study specifically scoring the generated captions for factual hallucinations, comparing the "Forced-Q6" outputs against the baseline to see if higher quality scores correlate with higher factual error rates.

## Limitations

- The synthetic quality labeling pipeline depends entirely on GPT-4.1 judgments without human validation beyond the reported 0.5 correlation from prior work
- The assumption that style and quality are partially orthogonal subspaces in representation space is stated but not empirically verified
- The evaluation dataset (LaMP-Cap) was constructed with specific profile inclusion criteria that may not generalize to other scientific domains or figure types

## Confidence

- **High Confidence**: The mechanism of quality filtering improving downstream performance; the core observation that optimizing for style-matching alone can reduce caption quality; the experimental methodology of comparing Predicted-Q vs Forced-Q6 inference modes
- **Medium Confidence**: The specific quality threshold of ≥3 for filtering training data; the claim that profile data quantity consistently improves personalization; the quality-aware training paradigm's ability to "maintain personalization" while improving quality
- **Low Confidence**: The generalizability of findings to domains outside scientific figures; the claim that the approach works with only 1-3 profile examples per target; the assertion that the quality-aware approach represents a "novel" paradigm

## Next Checks

1. **Human Quality Validation**: Recruit 3-5 domain experts to evaluate a stratified sample (20 captions) from the test set, rating both personalization quality and informativeness on 1-6 scales. Compare human judgments with model quality predictions and BLEU/ROUGE scores to validate the synthetic labeling pipeline.

2. **Profile Data Sensitivity Analysis**: Systematically vary the number of profile examples (N=0, 1, 2, 3, 5, 10) and measure the marginal improvement in BLEU/ROUGE. This will establish whether the diminishing returns observed with N=3 in Table 2 continue or if a minimum effective profile count exists.

3. **Cross-Domain Generalization Test**: Apply the trained model to a different figure-caption dataset (e.g., medical figures from PubMed or general figures from COCO) without fine-tuning. Measure BLEU/ROUGE and quality prediction accuracy to assess whether the personalization mechanism transfers beyond arXiv scientific papers.