---
ver: rpa2
title: Few-shot Continual Relation Extraction via Open Information Extraction
arxiv_id: '2502.16648'
source_url: https://arxiv.org/abs/2502.16648
tags:
- relation
- relations
- extraction
- undetermined
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to address Few-shot Continual Relation
  Extraction (FCRE) by integrating Open Information Extraction (OIE) to handle undetermined
  relations. The approach constructs an open dataset with all possible entity pairs
  and uses OIE to generate candidate relation descriptions, enriching the training
  data.
---

# Few-shot Continual Relation Extraction via Open Information Extraction

## Quick Facts
- arXiv ID: 2502.16648
- Source URL: https://arxiv.org/abs/2502.16648
- Reference count: 25
- Primary result: Proposed method outperforms state-of-the-art FCRE baselines on FewRel and TACRED, achieving up to 43.11% F1 score on FewRel with undetermined relations

## Executive Summary
This paper addresses Few-shot Continual Relation Extraction (FCRE) by integrating Open Information Extraction (OIE) to handle undetermined relations. The approach constructs an open dataset with all possible entity pairs and uses OIE to generate candidate relation descriptions, enriching the training data. The method employs a Weighted Mutual Information Loss and Hard Soft Margin Triplet Loss to align sample and description embeddings. Experiments show the method outperforms state-of-the-art FCRE baselines on FewRel and TACRED datasets, achieving up to 43.11% F1 score on FewRel and 37.79% on TACRED when tested with undetermined relations.

## Method Summary
The framework trains sequentially on tasks containing both determined relations (DR) and undetermined relations (UR). It constructs an open dataset by extracting all entity pairs from sentences, labeling matching pairs as DR and others as UR. Open Information Extraction (via ChatGPT-4o-mini) generates candidate triplets and augmented descriptions. The model uses BERT with soft prompts to produce latent embeddings, trained with a combined loss: Hard Soft Margin Triplet Loss for sample-sample separation, Weighted Mutual Information Loss for raw description alignment, and Weighted Mutual Information Loss for candidate description alignment. A memory buffer stores centroid-selected samples per relation for continual learning stability.

## Key Results
- OFCRE+OIE achieves 43.11% F1 on FewRel (T8 with UR) compared to 40.50% for OFCRE alone
- On TACRED, OFCRE+OIE achieves 37.79% F1 compared to 33.72% for OFCRE alone
- OIE filtering improves FewRel performance from 37.06% to 43.11% F1 by eliminating NA samples
- Ablation shows each component (L_HSMT, L_WMI_D, L_WMI_C) contributes significantly to final performance

## Why This Works (Mechanism)

### Mechanism 1: Description-Sample Alignment via Weighted Mutual Information
- Claim: Aligning latent sample embeddings with both original and candidate relation descriptions creates stable reference points that improve generalization under class imbalance
- Core assumption: Description embeddings provide semantically meaningful anchors that remain stable across tasks, whereas sample representations alone drift during continual learning
- Evidence anchors: [abstract] "enriches model knowledge with diverse relation descriptions, thereby enhancing knowledge retention and adaptability"; [section 3.3.1] "This loss is applied not only to the raw description but also to the candidate description to enhance the learning of sample representations"
- Break condition: If descriptions are noisy, contradictory, or semantically distant from actual relation semantics, alignment degrades into mislabeling

### Mechanism 2: Hard Soft Margin Triplet Loss for Latent Space Separation
- Claim: Combining hard mining with soft margin flexibility improves discrimination between semantically similar relations in few-shot regimes
- Core assumption: The hardest positive/negative pairs are the most informative for learning decision boundaries in low-data regimes
- Evidence anchors: [section 3.3.1] "This formulation effectively maximizes the separation between the hardest positive and hardest negative samples while allowing for adaptive margin flexibility"; [table 3] Ablation shows removing L_HSMT drops T8 performance from 67.62% to 65.16% (FewRel)
- Break condition: If batch sampling rarely contains truly hard negatives, mining provides little signal

### Mechanism 3: OIE-Based Undetermined Relation Filtering at Inference
- Claim: Pre-filtering samples with Open IE to identify NA (no relation) cases before classification improves precision by reducing false positives on undetermined relations
- Core assumption: The LLM-based OIE component has high precision for detecting true "no relation" cases, and its errors are biased toward false negatives rather than false positives
- Evidence anchors: [section 3.3.2] "OIE first eliminates entity pairs with no identifiable relationship (No relation - NA), assigning them to the Undetermined Relation (UR) label"; [table 2] OFCRE + OIE improves over OFCRE alone from 37.06% to 43.11% on FewRel (T8 with UR)
- Break condition: If OIE systematically misses relations, valid samples are incorrectly routed to UR, reducing recall

## Foundational Learning

- Concept: **Contrastive Learning and Triplet Loss**
  - Why needed here: The HSMT loss extends standard triplet loss; understanding anchor/positive/negative sampling and margin concepts is prerequisite
  - Quick check question: Can you explain why hard negative mining helps in low-data regimes but may cause instability if over-applied?

- Concept: **Mutual Information and InfoNCE**
  - Why needed here: The core L_WMI uses InfoNCE as a lower-bound estimator; understanding contrastive predictive coding helps interpret why description alignment works
  - Quick check question: How does maximizing mutual information between z and d differ from directly minimizing Euclidean distance?

- Concept: **Catastrophic Forgetting in Continual Learning**
  - Why needed here: The memory buffer and prototype updates exist specifically to mitigate forgetting; recognizing the stability-plasticity trade-off clarifies design choices
  - Quick check question: Why does replaying L samples per relation (1-means centroid selection) help more than random replay?

## Architecture Onboarding

- Component map:
  NER Module -> Open Dataset Constructor -> OIE Module (ChatGPT-4o-mini) -> BERT Encoder with Soft Prompts -> Loss Layer -> Memory Buffer -> NCM Classifier

- Critical path:
  1. Task T_j arrives → construct open dataset with NER + label DR/UR
  2. Generate/augment descriptions (raw + OIE candidate) via LLM prompts
  3. Train Φ_j on D_j using combined loss L(x) = α_x·L_HSMT + α_xd·L_WMI(x,d) + α_xc·L_WMI(x,c)
  4. Update memory buffer with L samples per new relation
  5. Refine Φ_j on memory buffer M_j
  6. At inference: apply OIE filter → if NA, return UR; else classify via NCM with prototypes and descriptions

- Design tradeoffs:
  - **K augmented descriptions**: K=5 is optimal (Figure 2); K<3 limits diversity, K>7 introduces noise
  - **Memory size L**: Larger L reduces forgetting but increases compute; paper uses centroid selection
  - **OIE at inference vs. training-only**: OIE adds 6%+ F1 but requires LLM calls per sample (latency/cost)
  - **Assumption**: The weight formula w_x (Equation 7) assumes batch-level imbalance reflects dataset-level imbalance

- Failure signatures:
  - Performance collapsing on early tasks while improving on recent tasks → memory replay insufficient
  - DR accuracy high but UR F1 near random → description alignment failed; check L_WMI weights
  - Large gap between OFCRE and OFCRE+OIE → OIE filtering effective but model not learning discriminative embeddings
  - Training instability with high α_xc → candidate descriptions may be noisy; reduce weight or improve prompts

- First 3 experiments:
  1. **Sanity check**: Run OFCRE on T1 only with and without UR labels; verify DR accuracy remains stable and UR is not trivially predicted
  2. **Ablation sweep**: Remove L_HSMT, L_WMI_D, L_WMI_C individually (Table 3, 7, 8) to confirm each component's contribution on your target dataset
  3. **Hyperparameter sensitivity**: Sweep K ∈ {1, 3, 5, 7, 10} for augmented descriptions and α weights ∈ {0.5, 1.0, 2.0, 3.0}; replicate Figure 2 pattern

## Open Questions the Paper Calls Out

- **Question 1**: How does the presence of false negatives within the Undetermined Relation (UR) class affect the model's convergence and the stability of the relation embedding space?
  - Basis in paper: [explicit] The Limitations section notes that the dataset labeled with UR "may, in fact, contain instances that align with predefined relation types," and the authors state that verifying these instances is beyond the scope of the work
  - Why unresolved: The authors treat UR as a distinct class for training, but noise in this class could mislead the Weighted Mutual Information Loss
  - What evidence would resolve it: An analysis quantifying the false negative rate in the UR set and a corresponding evaluation of model performance degradation as this noise ratio increases

- **Question 2**: Can the computational overhead of the Open Information Extraction pipeline be reduced through sampling or pruning strategies without significantly sacrificing the detection of unseen relations?
  - Basis in paper: [explicit] The Limitations section states that "training and testing with a large number of undetermined relations is computationally expensive and time-consuming," identifying optimization as necessary future work
  - Why unresolved: The current method requires generating descriptions for all possible entity pairs via an LLM, which scales poorly with document length
  - What evidence would resolve it: A comparative study of latency and F1 scores between the current exhaustive pair extraction and heuristics that filter low-probability pairs before OIE generation

- **Question 3**: To what extent does the framework rely on the specific reasoning capabilities of ChatGPT-4o-mini for generating high-quality candidate descriptions?
  - Basis in paper: [inferred] Section 3.2 and Appendix C explicitly utilize ChatGPT-4o-mini for the critical step of generating candidate triplets and augmenting descriptions, implying a dependency on this specific model's performance
  - Why unresolved: The quality of the "Candidate Description" is pivotal for the $L_{WMI}^C$ loss, but the paper does not test the robustness of this pipeline against weaker or different Large Language Models
  - What evidence would resolve it: Ablation experiments replacing ChatGPT-4o-mini with smaller, open-source LLMs (e.g., Llama, Mistral) in the OIE module to measure the resulting variance in final relation extraction accuracy

## Limitations

- Critical hyperparameters (memory buffer size L, batch size B, InfoNCE temperature τ) are unspecified, limiting exact reproduction fidelity
- The effectiveness of ChatGPT-4o-mini for OIE generation depends heavily on prompt quality and may vary across domains
- The paper assumes batch-level class frequency approximates dataset-level imbalance for computing MI weights, which may not hold in practice

## Confidence

- **High Confidence**: The core mechanism of description-sample alignment via mutual information loss is well-supported by ablation results
- **Medium Confidence**: The Hard Soft Margin Triplet Loss shows consistent improvements across datasets, though its specific design lacks extensive comparative analysis
- **Low Confidence**: The OIE-based undetermined relation filtering mechanism has limited validation - only shown to improve FewRel results, with no analysis of precision/recall trade-offs or OIE failure modes

## Next Checks

1. **Ablation Validation**: Reproduce the three-way ablation (removing L_HSMT, L_WMI_D, L_WMI_C individually) on your target dataset to confirm each component's contribution
2. **Hyperparameter Sensitivity**: Sweep K (augmented descriptions) and α weights to verify the optimal values match Figure 2 patterns
3. **OIE Robustness Test**: Evaluate NA detection precision/recall of the OIE module separately to quantify its filtering effectiveness and potential recall loss