---
ver: rpa2
title: 'Position: Universal Aesthetic Alignment Narrows Artistic Expression'
arxiv_id: '2512.11883'
source_url: https://arxiv.org/abs/2512.11883
tags:
- image
- arxiv
- aesthetic
- generation
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that over-aligning image generation models to
  a generalized aesthetic preference suppresses creative expression by penalizing
  intentionally "anti-aesthetic" outputs, even when they match user prompts. The authors
  construct a wide-spectrum aesthetics dataset and evaluate state-of-the-art generation
  and reward models, finding that reward models consistently assign lower scores to
  anti-aesthetic images regardless of prompt adherence.
---

# Position: Universal Aesthetic Alignment Narrows Artistic Expression

## Quick Facts
- arXiv ID: 2512.11883
- Source URL: https://arxiv.org/abs/2512.11883
- Reference count: 33
- Primary result: Aesthetic-aligned models penalize anti-aesthetic outputs even when they match user prompts

## Executive Summary
This paper demonstrates that over-aligning image generation models to generalized aesthetic preferences systematically suppresses creative expression by penalizing intentionally "anti-aesthetic" outputs. Using a wide-spectrum aesthetics dataset, the authors show that state-of-the-art reward models consistently assign lower scores to anti-aesthetic images regardless of prompt adherence. When tested against real abstract artworks, reward models give these significantly lower scores than AI-generated images. The study concludes that current aesthetic alignment prioritizes developer-centered values over user autonomy and aesthetic pluralism, effectively functioning as aesthetic authoritarianism that narrows admissible expression.

## Method Summary
The authors constructed a wide-spectrum aesthetics dataset by combining 300 COCO captions with 12 anti-aesthetic dimensions from the VisionReward dataset. They generated anti-aesthetic prompts using a VLM (Qwen3-VL-235B) and created image pairs for both original and anti-aesthetic prompts across multiple generation models. A fine-tuned judging model (Qwen3-VL-4B) evaluated per-dimension aesthetic scores independently of prompts. The study measured reward model bias by comparing scores for anti-aesthetic vs. original images when evaluated against anti-aesthetic prompts, and validated findings using real artworks from the LAPIS dataset.

## Key Results
- Reward models penalize anti-aesthetic images even when they perfectly match explicit user prompts
- Aesthetics-aligned generation models default to conventionally beautiful outputs, failing to respect instructions for degraded or negative imagery
- Real abstract artworks score significantly lower than AI-generated images in reward model evaluations
- Developer values and dataset composition implicitly define "good" aesthetics, propagating narrow aesthetic standards that exclude historically significant art styles

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reward models trained on generalized aesthetic preferences systematically penalize anti-aesthetic outputs even when they perfectly match explicit user prompts.
- **Mechanism:** Reward models learn a statistical "mean preference" from training data dominated by mainstream aesthetic standards. During RLHF, this signal creates a gradient that pulls generation away from low-scoring (anti-aesthetic) regions regardless of prompt semantics. The reward function conflates two objectives—instruction following and aesthetic quality—and when they conflict, aesthetic preference dominates.
- **Core assumption:** The paper assumes reward model scores causally influence downstream generation quality via reinforcement learning signals.
- **Evidence anchors:** [abstract] "reward models penalize anti-aesthetic images even when they perfectly match the explicit user prompt"; [section: Figure 2] Shows concrete examples where r(I_a, p_a) scores are lower than r(I_o, p_a) despite I_a better matching the anti-aesthetic prompt.
- **Break condition:** If reward models were trained with explicit anti-aesthetic examples in their preference pairs, or if instruction-following was weighted higher than aesthetic scores in the reward composition.

### Mechanism 2
- **Claim:** Aesthetics-aligned generation models "sanitize" outputs toward conventionally beautiful imagery, overriding explicit user instructions for degraded or negative aesthetics.
- **Mechanism:** During preference-aligned fine-tuning (e.g., DanceGRPO, PrefGRPO), models receive positive reinforcement for high aesthetic scores and negative reinforcement for low scores. This creates a prior that "beautifies" outputs as the path of least resistance. The model learns that certain visual patterns (bright lighting, clear subjects, harmonious colors) are always rewarded, making them default behaviors even when prompts specify the opposite.
- **Core assumption:** The paper assumes observed beautification is causal from alignment training, not from base model capabilities.
- **Evidence anchors:** [abstract] "aesthetic-aligned generation models frequently default to conventionally beautiful outputs, failing to respect instructions for low-quality or negative imagery"; [section: Table 2] Shows DanceFlux success rate at generating anti-aesthetic images is low; aligned models (Playground, DanceFlux) show larger score drops than base models.
- **Break condition:** If generation used classifier-free guidance with negative prompts that explicitly suppress aesthetic priors, or if alignment included anti-aesthetic examples in training.

### Mechanism 3
- **Claim:** Developer values and dataset composition implicitly define what counts as "good," propagating narrow aesthetic standards that exclude historically significant art styles.
- **Mechanism:** Preference data reflects annotator demographics and curator choices. If annotators predominantly prefer polished, bright, realistic imagery, the learned reward function encodes this as objective quality. The paper notes this extends beyond demographics to "general visual preferences, including lighting, color, styles, unrealism, clashing color." Real artworks (Munch, abstract pieces) score poorly because they fall outside the training distribution.
- **Core assumption:** Assumes low scores on real art indicate systemic bias rather than valid domain shift.
- **Evidence anchors:** [section 2.4] "developer's views of human preference can be implicitly inherited by models through data selection, annotation practices, and modeling choices"; [section 4.3] Real artworks from LAPIS dataset score significantly lower than AI-generated images; some below 2 standard deviations from AI image mean.
- **Break condition:** If reward training data included diverse aesthetic judgments from art historians, critics, or culturally diverse annotators.

## Foundational Learning

- **Concept: RLHF (Reinforcement Learning from Human Feedback)**
  - **Why needed here:** The paper's core argument depends on understanding how reward models shape generation via RL signals. Without this, the causal chain from "reward model scores" to "narrowed expression" is opaque.
  - **Quick check question:** Can you explain why a reward model trained to prefer "high quality" images would cause a generator to ignore prompts requesting "low quality" outputs?

- **Concept: Reward Hacking / Objective Misalignment**
  - **Why needed here:** The paper describes a specific form of objective misalignment where the reward function (aesthetic quality) conflicts with the true objective (following user intent). This is a standard failure mode in RL systems.
  - **Quick check question:** If you optimized a model purely for "user engagement," what unintended behaviors might emerge that don't actually serve users?

- **Concept: Distribution Shift in Evaluation**
  - **Why needed here:** The paper evaluates reward models on anti-aesthetic prompts and real artworks—both out-of-distribution relative to training. Understanding why OOD evaluation reveals bias (vs. just "the model working as intended") is critical.
  - **Quick check question:** Why might a model trained on Instagram photos perform poorly when evaluating Expressionist paintings? Is this a bug or expected behavior?

## Architecture Onboarding

- **Component map:** VLM-based prompt generator -> Generation models (Flux, SDXL, SD3.5M, Nano Banana) -> Reward models (HPSv3, ImageReward, PickScore, etc.) -> Judging model (fine-tuned Qwen-VL) -> Ground truth labeler (Qwen-VL-235B + human validation)

- **Critical path:** 1. Generate prompt pairs (p_o, p_a) from COCO + aesthetic dimensions 2. Generate image pairs (I_o, I_a) from each model 3. Score all (image, prompt) combinations with reward models 4. Verify I_a actually contains anti-aesthetic features via judging model 5. Compare r(I_a, p_a) vs r(I_o, p_a) to measure reward model bias 6. Compare generation success rates across base vs. aligned models

- **Design tradeoffs:** Using VLM to generate anti-aesthetic prompts introduces potential prompt quality variance; manual prompt engineering would be more controlled but less scalable. Judging model fine-tuned on VisionReward inherits its aesthetic biases; this is intentional (to detect deviation from mainstream) but creates circularity concerns. LLM-as-judge for ground truth (validated at κ=0.80 with humans) is practical but not perfect; borderline cases may be mislabeled.

- **Failure signatures:** If BLIP scores for I_a are low, the generation model failed to preserve semantic content (invalid sample). If judging model scores I_a higher than I_o on anti-aesthetic dimensions, generation succeeded; if not, the model "beautified" the output. If reward model gives I_a higher score than I_o even under p_a, the reward model is ignoring prompt semantics entirely.

- **First 3 experiments:** 1. Reproduce the reward model scoring bias: Take 50 anti-aesthetic prompt/image pairs from the paper's dataset; score with HPSv3 and ImageReward using p_a; verify that conventionally beautiful images (I_o) score higher than anti-aesthetic images (I_a) even when evaluating against the anti-aesthetic prompt. 2. Test instruction-following vs. aesthetic alignment tradeoff: Compare SD3.5M-base vs. SD3.5M-GenEval vs. SD3.5M-PickScore on anti-aesthetic prompt success rate; hypothesis: GenEval (instruction-focused) should outperform PickScore (aesthetic-focused). 3. Validate on a new aesthetic dimension: Add "pixelated/blocky" as an anti-aesthetic dimension not in the original 12; generate images and score; this tests whether the bias generalizes beyond the studied dimensions or if results are overfit to specific aesthetic attributes.

## Open Questions the Paper Calls Out

- **Question:** How can alignment techniques be re-engineered to explicitly decouple moral safety constraints from "aesthetic conformity" or visual comfort?
- **Basis in paper:** [explicit] The authors argue in Section 5 that current implementations "conflate distinct categories: moral safety, visual comfort, and aesthetic conformity," and call for systems that allow "sorrowful, anxious, or unsettling scenes" without compromising safety.
- **Why unresolved:** Current alignment pipelines (e.g., RLHF) often use datasets where "unsafe" and "ugly" annotations overlap, teaching models to treat aesthetic deviation as a safety violation.
- **What evidence would resolve it:** A model that successfully refuses harmful content (e.g., hate speech) while faithfully generating requested grotesque or disturbing imagery (e.g., Goya-style horrors) when prompted.

- **Question:** Can user-controllable mechanisms (such as prompt-based switches or adapter routing) be developed to dynamically adjust the strength of aesthetic alignment?
- **Basis in paper:** [explicit] The Conclusion explicitly calls for "user-controllable mechanisms to adjust the strength of aesthetic alignment or switch it off entirely, either by prompt or routing/adaptation mechanisms."
- **Why unresolved:** Current models lack "resistibility" or "off-switches" for their aesthetic bias, forcing all outputs toward a developer-defined mean preference regardless of user intent.
- **What evidence would resolve it:** A system allowing users to toggle "aesthetic smoothing" off via a parameter or prompt modifier, resulting in successful generation of wide-spectrum aesthetics without requiring complex prompt engineering.

- **Question:** How can reward models be trained to capture the variance of human aesthetic preference rather than optimizing for a singular, universal mean?
- **Basis in paper:** [explicit] The authors note in Section 2.1 that "disagreement over aesthetic preference is the norm" and criticize models like HPSv3 for continuing "to model general preferences rather than individual variation."
- **Why unresolved:** Existing reward models are trained to predict the aggregate "most preferred" image, which mathematically penalizes valid but niche aesthetic choices (the "no-body's happy here" zone).
- **What evidence would resolve it:** A new reward model architecture or dataset that scores diverse, anti-aesthetic images highly specifically when they align with a user's intent for those styles, showing high variance tolerance.

## Limitations

- **Prompt quality uncertainty:** The VLM-generated anti-aesthetic prompts may not consistently produce valid or coherent instructions, potentially conflating prompt parsing failures with true alignment bias.
- **Ground truth reliability:** Using LLM-as-judge introduces a layer of abstraction that may inherit aesthetic biases from the judging model's training, with 20% ambiguity in validation.
- **Closed-source dependencies:** The Google Nano Banana model and some aligned variants are inaccessible, limiting reproducibility and comparative analysis.

## Confidence

- **High confidence:** The core empirical finding that reward models systematically assign lower scores to anti-aesthetic images even when they match prompts.
- **Medium confidence:** The causal claim that RLHF alignment training is responsible for the observed bias.
- **Medium confidence:** The interpretation that low reward scores on real artworks indicate systemic aesthetic bias rather than valid domain calibration issues.

## Next Checks

1. **Ablation on prompt complexity:** Generate anti-aesthetic prompts using simpler templates and verify if the reward model bias persists to test whether VLM-induced prompt noise is confounding the results.
2. **Cross-dataset validation:** Evaluate reward models on anti-aesthetic images from a completely different source to determine if the bias is consistent across image domains or specific to the paper's generation pipeline.
3. **Counterfactual training analysis:** Train a variant reward model on preference data that includes explicit anti-aesthetic examples and compare its scoring behavior on the same anti-aesthetic prompt/image pairs to directly test whether the observed bias is remediable through training data modification.