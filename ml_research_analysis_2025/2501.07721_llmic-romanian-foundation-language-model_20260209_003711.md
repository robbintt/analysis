---
ver: rpa2
title: 'LLMic: Romanian Foundation Language Model'
arxiv_id: '2501.07721'
source_url: https://arxiv.org/abs/2501.07721
tags:
- arxiv
- language
- romanian
- llmic
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLMic is a 3B-parameter bilingual Romanian-English foundation model
  developed to address the performance gap of open models on low-resource languages.
  The authors constructed a high-quality pretraining corpus by combining filtered
  web data, curated sources, and parallel corpora, totaling 300B tokens for Romanian
  and 700B tokens for English.
---

# LLMic: Romanian Foundation Language Model

## Quick Facts
- arXiv ID: 2501.07721
- Source URL: https://arxiv.org/abs/2501.07721
- Reference count: 40
- Key outcome: 3B-parameter bilingual Romanian-English model achieving 41.01 BLEU on English-to-Romanian WMT translation, outperforming larger models

## Executive Summary
LLMic is a 3B-parameter bilingual Romanian-English foundation model designed to address the performance gap of open models on low-resource languages. The authors constructed a high-quality pretraining corpus by combining filtered web data, curated sources, and parallel corpora, totaling 300B tokens for Romanian and 700B tokens for English. They designed a custom BPE tokenizer optimized for Romanian text, achieving a fertility rate of 1.38. The model architecture is based on the Llama2 decoder-only transformer with 24 layers, embedding size of 2,560, and grouped query attention. Training used a cosine learning rate scheduler with a maximum of 4×10^-3, mixed precision, and fully sharded data parallelism. LLMic was evaluated on English-to-Romanian translation using WMT benchmark data, achieving a score of 41.01, outperforming existing open models including LLaMA-2, Mistral, and Gemma. The authors also demonstrated that quantization methods had only marginal impact on performance. LLMic is released under Apache 2.0 license to support natural language processing development for the Romanian language community.

## Method Summary
LLMic employs a 3B-parameter decoder-only transformer architecture based on Llama2, with 24 layers, 2,560 embedding size, and grouped query attention. The model uses a custom BPE tokenizer with 128K vocabulary trained on 7B tokens (50% Romanian, 50% English) that achieves a fertility rate of 1.38 by omitting Romanian diacritics. Pretraining uses a phased language mixing curriculum starting with 50:50 Romanian-English split for 50B tokens, then 30:70 split for remaining tokens, with repeated Romanian documents at the end. The corpus totals 1 trillion tokens (300B Romanian, 700B English) from filtered web data, curated sources, and parallel corpora. Training uses FSDP with bfloat16 precision and a cosine learning rate scheduler with 4×10^-3 maximum.

## Key Results
- LLMic achieves 41.01 BLEU on English-to-Romanian WMT translation, outperforming Llama-3.1-8B-Instruct (29.02), Mistral-7B-Instruct (26.19), and Gemma-7B-it (25.48)
- Custom Romanian-optimized tokenizer achieves fertility rate of 1.38, substantially lower than existing tokenizers (1.92-2.48)
- Quantization to 8-bit and 5-bit shows marginal performance impact on translation task
- Model released under Apache 2.0 license for Romanian language community use

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A language-optimized tokenizer improves downstream task performance by reducing sequence length overhead and improving token semantics for the target language.
- Mechanism: Custom BPE tokenizer trained on 7B tokens (50% Romanian, 50% English) with 128K vocabulary achieves fertility rate of 1.38, compared to 1.92–2.48 for existing tokenizers (Llama3, Gemma2, Mistral). Lower fertility means fewer tokens per word, improving training efficiency and context utilization.
- Core assumption: Tokenizer fertility correlates with downstream task quality for the target language. The paper explicitly omits Romanian diacritics (ă, â, î, ș, ț → a, i, s, t) based on preliminary experiments showing these characters degraded performance.
- Evidence anchors:
  - [Section 3]: "Our implementation uses an uncased vocabulary of 128,000 tokens and intentionally omits the Romanian characters... achieving a fertility rate of 1.38, substantially improving upon existing tokenizers."
  - [Figure 1]: Visual comparison showing LLMic at 1.38 vs next best at ~1.92.
  - [corpus]: No direct corpus evidence linking fertility rate to translation quality—this is inferred from performance results.
- Break condition: If target language has complex morphology not captured by character simplification, or if diacritics carry critical semantic distinctions, this approach may degrade comprehension.

### Mechanism 2
- Claim: Multi-phase language ratio scheduling during pretraining improves bilingual capability compared to static mixing ratios.
- Mechanism: Training proceeds in phases: (1) initial 50B tokens at 50:50 Romanian-English split to establish balanced language foundations; (2) subsequent tokens at 30:70 Romanian-English as dictated by dataset availability; (3) final phase with repeated high-quality Romanian documents.
- Core assumption: Equal language exposure early in training creates stronger foundational representations before specialization. The paper reasons "the model should first learn both the languages equally."
- Evidence anchors:
  - [Section 5]: "During the initial 50B tokens, we used a 50:50 split... Subsequently, we used the 30:70 Romanian-English split... Towards the end of the pretraining phase, we re-used high-quality Romanian documents multiple times."
  - [corpus]: No ablation comparing static vs. phased mixing—effectiveness is assumed from final performance.
- Break condition: If early-phase language mixing causes interference rather than synergistic learning, or if repeated documents cause memorization rather than reinforcement.

### Mechanism 3
- Claim: Small-scale specialized foundation models (3B parameters) can match or outperform larger general-purpose models on specific language tasks when trained on curated bilingual corpora.
- Mechanism: LLMic (3B) outperforms Llama-3.1-8B-Instruct (29.02), Mistral-7B-Instruct (26.19), and Gemma-7B-it (25.48) on English-to-Romanian WMT translation (LLMic: 41.01). The paper cites research showing "language-specific neurons" are a small fraction of model parameters, suggesting targeted training can efficiently capture language-specific capabilities.
- Core assumption: Performance gains come from corpus curation and language-specific pretraining rather than scale alone. Assumption: the 300B Romanian + 700B token corpus provides sufficient coverage for translation tasks.
- Evidence anchors:
  - [Table 3b]: Performance comparison showing LLMic (41.01) vs. mBART (38.50) and various 7-8B models (25–29).
  - [Section 1]: References research on "language-specific neurons" being "only a tiny fraction of the entire model."
  - [corpus]: Neighbor papers show active Romanian NLP development (GRILE benchmark, RoCoISLR corpus, translation frameworks), suggesting ecosystem maturity supports specialized models.
- Break condition: If performance gains are task-specific (translation) and do not generalize to other Romanian NLP tasks, or if larger models with similar fine-tuning would close the gap.

## Foundational Learning

- **Concept: BPE Tokenization and Fertility Rate**
  - Why needed here: The paper's tokenizer design centers on fertility rate (tokens per word) as the key metric. Understanding BPE helps explain why Romanian text gets inefficiently tokenized by English-centric tokenizers and why a custom vocabulary helps.
  - Quick check question: Can you explain why a tokenizer trained on English would produce more tokens for Romanian text than one trained on Romanian data?

- **Concept: Decoder-Only Transformer Architecture**
  - Why needed here: LLMic uses Llama2-style architecture with GQA, RoPE, and RMSNorm. Understanding these components is necessary to interpret the model specifications and make informed modifications.
  - Quick check question: What is the difference between multi-head attention, multi-query attention, and grouped-query attention, and why would GQA be chosen for inference efficiency?

- **Concept: Pretraining vs. Fine-tuning Paradigm**
  - Why needed here: The paper positions LLMic as a "foundation model" that can be "specialized for tasks" through fine-tuning. The evaluation specifically fine-tunes for translation rather than evaluating zero-shot.
  - Quick check question: Why does the paper fine-tune LLMic for translation evaluation rather than reporting zero-shot performance, and what does this imply about foundation model capabilities?

## Architecture Onboarding

- **Component map:**
  Input → Custom BPE Tokenizer (128K vocab, uncased, no diacritics) → Embedding Layer (2,560 dim, untied from output) → 24 × Transformer Blocks: RMSNorm (ε=10⁻⁵) → Grouped Query Attention (20 Q heads, 5 KV heads) → RoPE position encoding (θ=500,000) → SiLU activation → FFN hidden size: 10,240 → Output projection → Token prediction

- **Critical path:** Tokenizer choice → corpus preparation (diacritics stripped) → training with FSDP + bfloat16 → fine-tuning for downstream tasks (translation evaluated via WMT)

- **Design tradeoffs:**
  - **Diacritic removal vs. fidelity:** Paper states diacritics degraded performance in preliminary tests, but this may limit semantic precision for tasks where diacritics matter.
  - **Scale (3B) vs. capability:** Smaller model enables edge deployment and efficient inference but may limit general reasoning compared to 7B+ models.
  - **Bilingual focus vs. multilingual:** Romanian-English specialization limits utility for other languages but maximizes Romanian performance per parameter.

- **Failure signatures:**
  - Training loss divergence if learning rate too high (paper notes 4×10⁻³ constant LR caused "overly repetitive" outputs and "text memorization").
  - Poor translation quality if WMT test data includes diacritics not seen during training (paper explicitly strips diacritics from evaluation data).
  - Quantization degradation if calibration data doesn't match target domain (paper used C4, showed marginal impact, but notes "results are highly dependent on the task").

- **First 3 experiments:**
  1. **Tokenizer ablation:** Train identical model with standard Llama3 tokenizer vs. custom Romanian-optimized tokenizer; compare fertility rates and downstream translation quality to isolate tokenizer contribution.
  2. **Language mixing ratio sweep:** Pretrain variants with different Romanian-English ratios (20:80, 50:50, 70:30) for full training duration (no phase changes) to test if phased curriculum is necessary.
  3. **Zero-shot vs. fine-tuned evaluation:** Evaluate base LLMic (no task-specific fine-tuning) on Romanian benchmarks beyond translation (e.g., GRILE grammar reasoning, RoBiologyDataChoiceQA) to characterize foundation model capabilities and limitations.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions beyond the immediate technical scope of model development and evaluation.

## Limitations

- The causal relationship between tokenizer fertility rate and downstream task performance lacks direct ablation evidence—no comparison of models with different tokenizers on identical tasks.
- The phased language mixing curriculum is implemented without empirical justification or comparison to simpler training strategies like static mixing ratios.
- Translation-focused evaluation limits understanding of the model's broader foundation capabilities for other Romanian NLP tasks beyond translation.

## Confidence

**High confidence**: The architectural implementation details (3B parameters, 24 layers, 2,560 embedding size, GQA with 20Q/5KV heads, RoPE at 500K) are clearly specified and reproducible. The corpus composition and tokenization methodology are well-documented.

**Medium confidence**: The translation performance improvements over existing models are demonstrated on a standard benchmark, but the attribution of these gains to specific design choices (tokenizer, curriculum, corpus) lacks direct ablation evidence. The assumption that smaller specialized models can match larger general models on language tasks is supported by results but needs broader validation.

**Low confidence**: Claims about the superiority of the custom tokenizer's fertility rate mechanism, the necessity of the phased language mixing curriculum, and the generalizability of translation performance to other foundation model capabilities remain under-validated.

## Next Checks

1. **Tokenizer ablation study**: Train three identical models differing only in tokenizer (standard Llama3, Gemma2, and custom Romanian-optimized) and evaluate all on the same downstream tasks including WMT translation, GRILE grammar reasoning, and general QA to isolate the impact of fertility rate versus other tokenizer characteristics.

2. **Language mixing curriculum ablation**: Pretrain four model variants with identical corpus and parameters but different Romanian-English mixing strategies: (a) static 50:50 throughout, (b) static 30:70 throughout, (c) phased approach as implemented, (d) reversed phased approach (30:70 initially, then 50:50). Compare final performance on Romanian benchmarks to determine optimal mixing strategy.

3. **Zero-shot foundation capability evaluation**: Evaluate the base LLMic model (without task-specific fine-tuning) on a comprehensive suite of Romanian NLP tasks including GRILE grammar reasoning, RoBiologyDataChoiceQA, general question answering, and sentiment analysis. Compare zero-shot performance to fine-tuned performance to characterize the model's foundation capabilities versus specialized fine-tuning benefits.