---
ver: rpa2
title: 'From Speech to Subtitles: Evaluating ASR Models in Subtitling Italian Television
  Programs'
arxiv_id: '2512.19161'
source_url: https://arxiv.org/abs/2512.19161
tags:
- subtitles
- performance
- systems
- error
- transcription
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# From Speech to Subtitles: Evaluating ASR Models in Subtitling Italian Television Programs

## Quick Facts
- **arXiv ID**: 2512.19161
- **Source URL**: https://arxiv.org/abs/2512.19161
- **Authors**: Alessandro Lucca; Francesco Pierri
- **Reference count**: 10
- **Primary result**: WhisperX with Gemini-2.5-Flash post-processing achieves lowest WER and best entity recognition for Italian TV subtitling

## Executive Summary
This paper evaluates four ASR models—Whisper Large v2, WhisperX, AssemblyAI Universal, and NVIDIA Parakeet TDT 0.6b v3—on a 50-hour Italian television dataset across three program types: Talk Shows, Investigative Journalism, and Scientific Communication. WhisperX consistently achieves the lowest Word Error Rate (WER) and Entity Error Rate (EER), particularly excelling in programs with background noise and overlapping speech. The study introduces a deterministic segmentation algorithm that transforms raw ASR output into SubRip (.srt) files compliant with Italian readability guidelines (NCS: 30-74, MSD: 1-6s, CPS: 9-15). A zero-shot LLM post-processing step using Gemini-2.5-Flash significantly improves named entity recognition accuracy without task-specific fine-tuning.

## Method Summary
The evaluation uses a 50-hour dataset of Italian TV content (30 episodes across 5 programs from RAI) with professional human-edited SubRip subtitle files as reference. Four ASR models are run on Google Cloud with NVIDIA T4 GPU (except AssemblyAI accessed via API). Outputs are standardized to JSON with segment-level text, timestamps, and word-level timing. Metrics include WER (primary), SubER for subtitle quality, AS-BLEURT for semantic coherence, EER for named entity recognition, and readability compliance checks. LLM post-processing with Gemini-2.5-Flash (batch of 40 subtitles) and Gemma3-12b-it (individual) corrects entities and restores punctuation. The pipeline generates compliant .srt files through deterministic segmentation.

## Key Results
- WhisperX achieves lowest WER across all program types, with median scores of 16.2% (Talk Shows), 17.7% (Investigative Journalism), and 14.5% (Scientific Communication)
- Gemini-2.5-Flash post-processing yields statistically significant EER improvement (p-value 0.006, mean gain 2.3%) in entity recognition
- Deterministic segmentation algorithm successfully transforms raw ASR output into SubRip files meeting all readability constraints without ML-based processing
- Per-minute WER analysis reveals WhisperX maintains lowest error rates throughout programs, outperforming competitors in 83% of one-minute windows

## Why This Works (Mechanism)

### Mechanism 1
- Claim: WhisperX achieves superior entity recognition and timing accuracy compared to other ASR models for long-form Italian broadcast content.
- Mechanism: WhisperX combines a Voice Activity Detector (VAD) for intelligent preprocessing, a Cut&Merge strategy for handling extended audio, and a phoneme alignment model for post-processing to ensure precise temporal alignment between audio features and output text.
- Core assumption: Long-form audio benefits from segmentation before encoding, rather than processing as a single continuous stream.
- Evidence anchors:
  - [abstract] "WhisperX (Bain et al., 2023)" evaluated among four ASR systems on 50-hour Italian television dataset
  - [section 2] "WhisperX...employing an intelligent preprocessing using Voice Activity Detector (VAD) and Cut&Merge strategy, and a postprocessing with a phoneme alignment model to ensure maximum temporal alignment"
  - [section 5] "WhisperX consistently achieves the lowest EER in nearly all episodes, establishing itself as the leading model in this task"
  - [corpus] Limited direct corpus support; related papers focus on Arabic ASR and multilingual subtitling rather than WhisperX-specific architectures
- Break condition: If audio contains significant overlapping speech or music interwoven with dialogue, VAD-based segmentation may incorrectly partition content, degrading performance.

### Mechanism 2
- Claim: Zero-shot LLM post-processing with Gemini-2.5-Flash significantly improves named entity recognition accuracy in ASR outputs.
- Mechanism: The LLM receives batches of up to 40 subtitle segments and returns structured outputs with corrected entities and restored punctuation, leveraging its pre-trained knowledge of Italian named entities without task-specific fine-tuning.
- Core assumption: The LLM's parametric knowledge of entities (person names, organizations, locations) is more reliable than ASR phonetic transcription for proper nouns.
- Evidence anchors:
  - [section 4.2] "Two LLMs are evaluated in a zero-shot setting for punctuation restoration and entity correction"
  - [section 5] "Gemini-2.5-Flash leads to a statistically significant improvement in EER performance (p-value 0.006), with a mean gain of 2.3% overall"
  - [section 7] "the inclusion of Gemini-2.5-Flash as a reviewer, even in a zero-shot configuration...demonstrates a measurable improvement in the correction of misrecognized or misspelled entities"
  - [corpus] Fedorchenko et al. (cited in paper) report improvements using GPT-4o for Estonian subtitles; corpus supports LLM correction viability but not specifically for Italian
- Break condition: If entity density is extremely high or entities are obscure/novel (not in LLM training data), corrections may introduce hallucinations rather than improvements.

### Mechanism 3
- Claim: Deterministic segmentation algorithms can transform raw ASR output into industry-compliant subtitle files without ML-based processing.
- Mechanism: Word-level timestamps extracted from ASR enable a rule-based algorithm that enforces readability constraints (NCS: 30-74 characters, MSD: 1-6 seconds, CPS: 9-15 characters) by automatically splitting text at punctuation boundaries when thresholds are exceeded.
- Core assumption: ASR systems provide sufficiently accurate word-level timestamps to enable compliant segmentation.
- Evidence anchors:
  - [section 5] "an intelligent segmentation algorithm that leverages the word timestamp information is sufficient to rebuild a SubRip Text file that is compliant with all the guidelines"
  - [section 5] "When these are not met, the algorithm automatically splits the text into two lines or multiple segments, prioritizing breaks at punctuation marks"
  - [section 8] "Ensuring that both sentence-level and word-level timestamps are extracted, in conjunction with a proper segmentation algorithm, enables the generation of SubRip subtitle files that strictly adhere to readability guidelines"
  - [corpus] No direct corpus evidence for this specific segmentation approach; related subtitling papers do not address deterministic post-processing
- Break condition: If ASR word timestamps have high variance or drift, segment boundaries may violate timing constraints regardless of algorithm design.

## Foundational Learning

- Concept: Word Error Rate (WER) limitations in subtitling contexts
  - Why needed here: The paper explicitly notes that WER against human-edited subtitles introduces systematic deviation because professional captioners rephrase content for readability, not verbatim transcription. Interpreting results requires understanding this evaluation noise.
  - Quick check question: If an ASR outputs "cannot" but the human reference uses "can't," should this count as an error for subtitling quality assessment?

- Concept: Encoder-Decoder ASR architecture
  - Why needed here: All evaluated models share this structure—the encoder projects acoustic features (log-mel) into high-dimensional space; the decoder handles temporal alignment between audio and output labels. Understanding this clarifies why models struggle similarly on certain content types.
  - Quick check question: In an Encoder-Decoder ASR, which component is responsible for ensuring the output text timing matches the input audio?

- Concept: Subtitle readability constraints (NCS, MSD, CPS)
  - Why needed here: The paper evaluates compliance with Italian broadcaster guidelines—these three metrics determine whether subtitles are physically readable by viewers. System design must enforce these regardless of ASR accuracy.
  - Quick check question: A subtitle segment has 80 characters and displays for 4 seconds. Which readability constraint(s) does it violate?

## Architecture Onboarding

- Component map:
  - Input Layer: Video/audio ingestion → audio extraction
  - ASR Processing: WhisperX running on Cloud Run Job with NVIDIA L4 GPU
  - Post-Processing Pipeline: Segmentation algorithm → LLM reviewer (Gemini-2.5-Flash) → JSON standardization
  - Output Layer: SubRip (.srt) file generation → Cloud Storage
  - User Interface: Cloud Run Service Container with Identity-Aware Proxy authentication
  - Orchestration: Pub/Sub topic triggers processing; BigQuery stores metadata

- Critical path:
  1. User initiates job via UI → Pub/Sub message published
  2. Cloud Run Job activated (GPU spins up)
  3. Audio processed through WhisperX (VAD → transcription → phoneme alignment)
  4. Word-level timestamps extracted to JSON
  5. Segmentation algorithm enforces NCS/MSD/CPS constraints
  6. LLM reviews entities in batches of ≤40 segments
  7. Final .srt generated and stored
  8. Job completes; GPU deallocates

- Design tradeoffs:
  - **Cost vs. Speed**: WhisperX processes at ~14x realtime ($2.56/hour inference); AssemblyAI achieves ~26x realtime but costs $7.50/hour via API. Paper recommends WhisperX for high-volume production.
  - **Accuracy vs. Autonomy**: Full automation cannot meet 98% industry accuracy threshold; HITL is mandatory for broadcast-quality output.
  - **Open-source vs. Proprietary**: Open models (WhisperX, Parakeet) allow GPU cost optimization; proprietary (AssemblyAI) offers convenience at higher per-hour cost.

- Failure signatures:
  - **Songs/musical content**: ASR attempts to transcribe lyrics as speech, producing nonsensical output. Detection: high WER segments correlated with non-speech audio classification.
  - **Entity hallucination**: LLM "correction" introduces entities not present in audio. Detection: compare pre/post LLM entity lists against audio-verified spans.
  - **Segmentation drift**: Cumulative timing errors cause subtitle sync to degrade over long programs. Detection: monitor end-to-end offset at regular intervals.

- First 3 experiments:
  1. **Baseline WER by program type**: Run all four ASR models on 5-minute samples from each program category (Talk Show, Investigative Journalism, Scientific Communication) to confirm WhisperX advantage and identify highest-error content types.
  2. **LLM correction ablation**: Process identical transcripts with Gemini-2.5-Flash and Gemma3-12b-it; measure EER delta and latency per segment to quantify accuracy/throughput tradeoff.
  3. **Readability compliance stress test**: Generate subtitles for 10 hours of content; validate NCS/MSD/CPS compliance rate and identify which constraint fails most frequently to prioritize segmentation algorithm tuning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating a translation-oriented LLM into the current pipeline effectively generate interlingual subtitles while maintaining the established quality standards?
- Basis in paper: [explicit] The Conclusion states that the platform's scalability allows for future extensions, specifically "the generation of interlingual subtitles, by integrating a translation-oriented LLM with the same reviewer-based approach."
- Why unresolved: The current study focused exclusively on monolingual Italian subtitling and did not test translation capabilities or cross-lingual transfer within the pipeline.
- What evidence would resolve it: Benchmarking the pipeline on a multilingual dataset to evaluate translation accuracy (e.g., BLEU scores) and subtitle compliance when converting Italian audio to other languages.

### Open Question 2
- Question: What are the quantitative productivity gains for professional subtitlers using this Human-in-the-Loop (HITL) system compared to manual creation from scratch?
- Basis in paper: [inferred] The Limitations section notes the study "does not incorporate collaborative assessments with professional subtitlers" to provide a "concrete measure of productivity gains," despite the conclusion asserting the system enhances productivity.
- Why unresolved: The paper evaluates algorithmic performance (WER, SubER) against reference files but does not measure the reduction in human time or effort required to finalize the subtitles in a real-world workflow.
- What evidence would resolve it: A user study tracking the time and keystrokes required by professional subtitlers to correct the ASR output versus creating subtitles manually.

### Open Question 3
- Question: How does the ASR pipeline perform on scripted series or films which feature distinct linguistic registers and acoustic environments compared to the unscripted TV content tested?
- Basis in paper: [inferred] The Limitations section highlights that the dataset was restricted to specific TV programs, which "restricts generalization to other forms of audiovisual content, such as films or scripted series."
- Why unresolved: The models were benchmarked only on talk shows, investigative journalism, and scientific programs; they were not tested on the specific acoustic challenges or editing styles of fiction.
- What evidence would resolve it: Evaluating the WhisperX and Gemini-based pipeline on a dataset of fictional content (e.g., dubbed films, dramas) to determine if error rates remain comparable to the "Scientific Communication" baseline.

## Limitations
- Evaluation relies on human-edited subtitles as ground truth, which systematically deviates from verbatim transcription due to professional captioner rephrasing for readability
- Segmentation algorithm details and LLM prompts are underspecified, making exact reproduction challenging
- Study restricted to specific TV programs, limiting generalization to films or scripted series with different linguistic registers and acoustic environments

## Confidence

- **High Confidence**: WhisperX's superior entity recognition and timing accuracy (supported by direct comparative metrics across all episodes)
- **Medium Confidence**: LLM post-processing improves EER significantly (supported by statistical tests, but dependent on prompt quality and entity density)
- **Low Confidence**: Deterministic segmentation algorithms alone can meet all readability constraints (supported by assertion but lacking corpus validation of the specific approach)

## Next Checks

1. **Ground Truth Bias Quantification**: Compare raw ASR output against both human-edited subtitles and verbatim transcripts (if available) to measure the systematic deviation introduced by captioner rephrasing.
2. **Segmentation Algorithm Validation**: Test the deterministic segmentation on diverse content types to identify which readability constraint (NCS, MSD, or CPS) fails most frequently and requires algorithmic refinement.
3. **LLM Correction Reliability**: Measure entity hallucination rates by comparing pre/post LLM entity lists against audio-verified spans, particularly for high-entity-density content like Investigative Journalism programs.