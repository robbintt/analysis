---
ver: rpa2
title: Enhancing Close-up Novel View Synthesis via Pseudo-labeling
arxiv_id: '2503.15908'
source_url: https://arxiv.org/abs/2503.15908
tags:
- training
- close-up
- radiance
- nerf
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating high-quality close-up
  views from neural radiance fields trained on distant viewpoints. The authors propose
  a pseudo-label-based learning strategy that generates virtual close-up camera poses
  during training and creates wrapped images from existing training data to provide
  targeted supervision.
---

# Enhancing Close-up Novel View Synthesis via Pseudo-labeling

## Quick Facts
- arXiv ID: 2503.15908
- Source URL: https://arxiv.org/abs/2503.15908
- Authors: Jiatong Xia; Libo Sun; Lingqiao Liu
- Reference count: 15
- One-line primary result: Proposed pseudo-labeling approach significantly improves close-up view synthesis, achieving 20.95 PSNR on custom dataset versus 19.99 for baseline 2D Gaussian Splatting.

## Executive Summary
This paper addresses the challenge of generating high-quality close-up views from neural radiance fields trained on distant viewpoints. The authors propose a pseudo-label-based learning strategy that generates virtual close-up camera poses during training and creates wrapped images from existing training data to provide targeted supervision. Their approach significantly improves results compared to baselines, with their 2D Gaussian Splatting method achieving 20.95 PSNR on their dataset versus 19.99 for the baseline, and their general training method improving PSNR from 14.48 to 18.92.

## Method Summary
The method involves generating virtual close-up camera poses during training by randomly sampling distance parameters (λ ∈ [2,8]) and orientation offsets (bounded by ε = π/4). For each virtual pose, the system renders depth maps and warps training images to create pseudo-labels. A consistency mask filters out erroneous pixels by comparing forward and backward warped images, keeping only pixels where the difference is below threshold ε = 0.05. The model is then fine-tuned using a combined loss that includes both standard RGB supervision and pseudo-label supervision. The approach works with both NeRF and 2D Gaussian Splatting backbones, with different iteration counts and batch sizes for each.

## Key Results
- 2D Gaussian Splatting baseline: 19.99 PSNR → Proposed method: 20.95 PSNR on custom dataset
- General training method: PSNR improved from 14.48 to 18.92
- Test-time fine-tuning: 2DGS achieved 21.64 PSNR in just 5 iterations per view
- Ablation studies confirm importance of consistency masking and pose diversity

## Why This Works (Mechanism)

### Mechanism 1: Depth-Based Pseudo-Supervision
Providing supervision for close-up views via depth-warped images reduces artifacts caused by extrapolation errors in the radiance field. When radiance fields are queried on out-of-distribution ray directions, they tend to produce unreliable RGB values. By rendering depth maps for virtual close-up poses and warping pixels from existing training images, the method generates "pseudo-ground truth" to constrain the network's output for these unseen rays.

### Mechanism 2: Consistency-Based Filtering (Masking)
The method generates two candidate wrapped images using depth maps from virtual and training views respectively. By comparing them, it identifies pixels where geometry is inconsistent or occluded. Only pixels with low discrepancy (below threshold ε) are used for supervision, preventing the model from learning from erroneous self-generated data.

### Mechanism 3: Pose-Space Randomization
Randomly sampling virtual camera poses during training improves generalization across the "untrained domain" of close-up views. Rather than optimizing for fixed views, the method generates new virtual poses at each iteration by varying distance and orientation parameters, ensuring the radiance field is regularized across a wide distribution of rays.

## Foundational Learning

- **Neural Radiance Fields (NeRF) & Volume Rendering**
  - Why needed here: The paper targets the specific failure mode of NeRF's MLP when queried on out-of-distribution ray directions
  - Quick check question: How does the rendering equation aggregate color and density along a ray, and why does this fail if the network hasn't seen similar ray directions during training?

- **Image Warping & Back-Projection**
  - Why needed here: The core method relies on constructing "pseudo-labels" by projecting 2D pixels into 3D space using depth, and then re-projecting them onto a new image plane
  - Quick check question: Given a pixel coordinate (u, v), a depth value d, and camera intrinsics K, what are the steps to compute its 3D world coordinate X?

- **Test-Time Optimization**
  - Why needed here: The paper proposes a "test-time fine-tuning" strategy where the model is adapted specifically for the requested view
  - Quick check question: What is the risk of fine-tuning a generative model on its own outputs without external supervision, and how does this paper attempt to mitigate it?

## Architecture Onboarding

- **Component map:** Pre-trained Backbone -> Pose Generator -> Warping Engine -> Mask Calculator -> Loss Aggregator
- **Critical path:** Sample anchor pixel → Generate virtual pose → Render depth → Warp training image → Compute consistency mask → Backpropagate loss on masked pixels
- **Design tradeoffs:** NeRF uses sparse ray sampling due to slow rendering, while 2DGS renders full images; General training is robust but slow (10k iterations), Test-time fine-tuning is faster (5 iterations) but requires knowing target view
- **Failure signatures:** Depth bleeding causes blurry pseudo-labels; mask collapse occurs if threshold is too strict; geometry degradation from excessive RGB fine-tuning
- **First 3 experiments:** 1) Baseline reproduction to verify artifacts on close-up views; 2) Warping visualization to check consistency mask effectiveness; 3) Lambda sensitivity analysis to find optimal zoom factor

## Open Questions the Paper Calls Out

### Open Question 1
How does the accuracy of the initial geometric reconstruction impact the convergence and stability of the pseudo-labeling training loop? The method assumes the density field is "typically robust" but doesn't quantify performance when this assumption fails. A sensitivity analysis measuring performance degradation with noisy depth maps would resolve this.

### Open Question 2
Can the method effectively synthesize high-frequency textures for close-up views when relevant pixel information is strictly absent from distant training views? The pseudo-labeling strategy cannot provide details that were sub-pixel or invisible in distant views. Comparison with generative super-resolution baselines on extreme close-ups would resolve this.

### Open Question 3
Does the fixed consistency threshold (ε=0.05) generalize effectively to scenes with varying lighting conditions or non-Lambertian surfaces? A fixed threshold may be too lenient for low-texture regions or too strict for specular highlights. An ablation study showing optimal ε values for different scene categories would resolve this.

## Limitations
- Relies heavily on quality of pre-trained depth estimation; errors propagate to pseudo-labels
- Assumes consistent lighting between training and virtual views, which may not hold for scenes with strong shadows or highlights
- Custom dataset used for evaluation is not publicly available, limiting reproducibility

## Confidence
- **High confidence:** Core mechanism of depth-based pseudo-supervision and consistency-based masking is well-justified and supported by ablation studies
- **Medium confidence:** Pose-space randomization strategy is reasonable but parameter choices may be dataset-dependent
- **Low confidence:** Test-time fine-tuning method shows promising results on 2DGS but lacks extensive validation across diverse scenes

## Next Checks
1. Apply the method to established benchmarks like Tanks and Temples or DTU to verify cross-dataset generalization
2. Systematically quantify how depth errors in the baseline model affect the quality of pseudo-labels and subsequent fine-tuning results
3. Evaluate the method on scenes with varying illumination conditions to assess robustness to lighting changes between training and virtual views