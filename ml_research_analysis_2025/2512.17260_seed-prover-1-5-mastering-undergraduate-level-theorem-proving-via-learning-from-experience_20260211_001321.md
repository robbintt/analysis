---
ver: rpa2
title: 'Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning
  from Experience'
arxiv_id: '2512.17260'
source_url: https://arxiv.org/abs/2512.17260
tags:
- lean
- proof
- formal
- lemma
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Seed-Prover 1.5 advances formal theorem proving by integrating
  agentic reinforcement learning with a natural-to-formal language bridge. It trains
  an agentic Lean prover through large-scale tool-integrated RL, enabling efficient,
  lemma-by-lemma proof construction with adaptive tool use.
---

# Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience

## Quick Facts
- arXiv ID: 2512.17260
- Source URL: https://arxiv.org/abs/2512.17260
- Reference count: 40
- Solved 11 out of 12 problems from Putnam 2025 in 9 hours

## Executive Summary
Seed-Prover 1.5 advances formal theorem proving by integrating agentic reinforcement learning with a natural-to-formal language bridge. It trains an agentic Lean prover through large-scale tool-integrated RL, enabling efficient, lemma-by-lemma proof construction with adaptive tool use. A sketch model translates natural language proofs into Lean sketches via rubric-based RL, ensuring semantic soundness and decomposition quality. The test-time workflow orchestrates natural language, sketch, and agentic Lean provers to solve complex proofs hierarchically. Evaluated on benchmarks including PutnamBench (88%), Fate-H (80%), and Fate-X (33%), Seed-Prover 1.5 achieves state-of-the-art performance within a moderate compute budget.

## Method Summary
The system combines agentic reinforcement learning with hierarchical proof construction. It uses tool-integrated RL where the agent learns to generate intermediate lemmas rather than full proofs, caching verified lemmas to manage context length. A sketch model trained with rubric-based RL translates natural language proofs into Lean sketches with semantic validity checks. At test time, the system orchestrates natural language, sketch, and agentic Lean provers in a recursive workflow that decomposes failed proofs into sub-goals.

## Key Results
- Solved 11/12 Putnam 2025 problems in 9 hours
- Achieved 88% solve rate on PutnamBench
- Reached 80% solve rate on Fate-H benchmark
- Demonstrated 33% solve rate on Fate-X benchmark

## Why This Works (Mechanism)

### Mechanism 1
- Claim: **Agentic Reinforcement Learning appears to optimize interaction granularity, reducing compute waste associated with rigid step-level or whole-proof generation.**
- Mechanism: Instead of interacting at every tactic step (expensive) or once for a full proof (brittle), the model learns to emit "lemmas"—intermediate proof granules. Valid lemmas are cached as axioms, preventing context window bloat. RL rewards efficiency (binary +1/-1 based on Lean verification), implicitly teaching the model to minimize tool calls.
- Core assumption: The verifier (Lean) provides a reliable, dense enough signal for the agent to distinguish between efficient and inefficient proof paths without explicit process supervision.
- Evidence anchors:
  - [section 3.1] "It relieves the model of the necessity to generate the whole proof... incrementally invokes multiple tools."
  - [section 4.1] "Average number of function calls drops from ~15 to 10... sequence length reduced from ~28k to ~17k tokens."
  - [corpus] Related work *DeepSeek-Prover-V2* similarly leverages recursive decomposition, suggesting this is a converging paradigm for scaling formal reasoning.
- Break condition: If the search space is sparse (valid lemmas are rare), the random exploration in early RL phases may fail to find the "lemma" sub-goals, causing the agent to revert to inefficient trial-and-error tactics.

### Mechanism 2
- Claim: **The Sketch Model likely bridges the Natural-to-Formal gap by converting high-level reasoning into verifiable structural constraints.**
- Mechanism: A specialized model translates Natural Language (NL) proofs into "Lean Sketches" (code with `sorry` placeholders). Crucially, it uses Rubric-based RL (LLM-as-Judge) to ensure the decomposition is mathematically valid *before* the prover attempts to fill the gaps. This filters out "hallucinated" logic structures early.
- Core assumption: The semantic understanding of a frontier LLM (used as Judge) correlates sufficiently with formal Lean validity to guide the sketch generation.
- Evidence anchors:
  - [section 3.3] "Sketch model acts as a hierarchical problem decomposer... rubric acts as a semantic value model."
  - [section 4.2] "Investing more compute... leads to a log-linear increase in Seed-Prover 1.5’s solve rate."
  - [corpus] *Mathesis* and *FormaRL* highlight the difficulty of autoformalization without RL or structured guidance, contrasting with the sketch-based approach here.
- Break condition: If the Sketch Model generates "plausible but impossible" lemmas (syntactically correct but mathematically false), the downstream Agentic Prover wastes compute trying to prove contradictions.

### Mechanism 3
- Claim: **Test-time Compute Scaling (TTS) converts search depth into solve-rate via recursive error recovery.**
- Mechanism: The workflow is hierarchical. If the Agentic Prover fails a leaf lemma, the system doesn't just retry; it may recurse—generating a new NL proof and sketch for that specific sub-goal. This allows the system to dynamically patch holes in the proof tree.
- Core assumption: The recursive cost of re-decomposing a failed sub-lemma is lower than brute-forcing the proof via random search.
- Evidence anchors:
  - [section 3.4] "If a proof cannot be found, the system recursively performs the 'natural language proof → Lean sketch' decomposition."
  - [abstract] "Solved 11 out of 12 problems from Putnam 2025 in 9 hours."
- Break condition: If the "distribution gap" between the NL prover and the Lean prover is too wide, recursion may produce structurally similar failures, leading to infinite loops or diminishing returns on depth.

## Foundational Learning

- Concept: **Lean 4 Tactic State & Error Parsing**
  - Why needed here: The Agentic Prover relies on "structured feedback" from the compiler (LooKeng) to update its context. You must understand how Lean reports "unsolved goals" vs. type errors to debug agent trajectories.
  - Quick check question: Given a Lean error indicating a type mismatch, can you identify if the error stems from a wrong hypothesis application or a malformed definition?

- Concept: **Outcome-Based Reinforcement Learning (RLVR)**
  - Why needed here: The system uses VAPO (Value-Actor-PPO) with a simple binary reward. Understanding how credit assignment works over long horizons (64K tokens, 28 tool calls) is essential to diagnose why the model learns "efficiency" (fewer calls) without explicit negative rewards for length.
  - Quick check question: In a 10-turn tool-use trajectory that fails, how does a binary -1 reward at the end affect the probability of specific intermediate search actions?

- Concept: **Curriculum Learning & Data Filtering**
  - Why needed here: The paper emphasizes filtering training data (removing "too easy" or "impossible" samples). Understanding this "moderate difficulty" regime is key to replicating the training stability.
  - Quick check question: Why would training on problems the model can already solve with 100% accuracy degrade its exploratory capabilities?

## Architecture Onboarding

- Component map:
  1. **Input**: Formal Statement (Lean) + Optional NL Proof.
  2. **Orchestrator**: Routes tasks to agents based on search depth.
  3. **Sketch Agent**: (Rubric-RL fine-tuned) Generates `lemma` structure.
  4. **Agentic Prover**: (Tool-integrated RL) Calls *LooKeng* (REPL), *Mathlib Search*, *Python*.
  5. **Evaluator**: Lean Compiler (Ground Truth).

- Critical path:
  The **RL Data Filtering Pipeline** (Section 3.2). The system runs a "Pass@4 x 8" filter to remove problems solvable by the SFT model. If this filter is too porous, the RL training will be dominated by "easy wins" that don't teach tool strategy. If too strict, the model sees no positive reward signal.

- Design tradeoffs:
  - **Sketch vs. Direct**: Using a Sketch model adds an inference step but drastically reduces the Agentic Prover's search space.
  - **Context Caching**: Caching verified lemmas as axioms saves context but creates "fragile" proofs where an early lemma cannot be revised if it turns out to be a dead-end for later steps.

- Failure signatures:
  1. **Junk Value Loops**: The agent exploits Lean's total functions (e.g., `1/0 = 0`) to "prove" false theorems. (See Appendix B: Lean 4 Corner Cases).
  2. **Search Internalization Failure**: The model relies heavily on `mathlib_search` and fails to memorize common theorems, leading to timeouts.
  3. **Sketch Drift**: The Sketch Model generates lemmas that are technically valid but require rewriting the entire Mathlib library to connect.

- First 3 experiments:
  1. **Ablate the Tools**: Run the Agentic Prover (Post-RL) on Putnam-200 with *only* Lean verification enabled (no search/Python). Quantify the performance drop to isolate the value of external tools.
  2. **Sketch Quality Stress Test**: Feed the Agentic Prover "broken" sketches (e.g., missing one critical lemma) and measure the success rate of the recursive recovery mechanism.
  3. **Training Step Analysis**: Plot "Average Search Calls" vs. "Training Accuracy" (like Figure 4) for a smaller model variant to verify if the "internalization" effect (fewer calls, higher accuracy) scales down.

## Open Questions the Paper Calls Out

- **Iterative Data Generation**: The paper notes that iteratively leveraging the RL-trained model to collect additional data may further improve performance, which is left for future investigation. The dynamics of continuous, self-generated experience in a formal environment are unexplored.

- **Multi-Paper Reasoning**: The paper identifies the "dependency issue" as the barrier to helping advance research—synthesizing insights across multiple research papers for frontier mathematics, rather than relying on self-contained competition problems.

- **Long Context Reasoning**: The persistence of negative scores within the 32K-64K token range suggests that effectively reasoning over extremely long contexts remains a challenge, despite optimizations for shorter sequences.

- **Mis-Formalization of Open Problems**: The paper observes that while the system solved some Erdős problems, these were either mathematically simple or proved trivial/simplified versions due to mis-formalization, highlighting a gap between generating valid Lean proofs and meaningful mathematical results.

## Limitations

- The RL agent's ability to discover valid lemmas in sparse reward spaces is not rigorously proven, potentially limiting learning efficiency.
- The rubric-based RL's semantic alignment with formal validity is asserted but not independently verified, creating uncertainty about sketch quality.
- The recursive recovery mechanism's efficiency gain over alternative strategies is not quantified, leaving its practical value unclear.

## Confidence

**High Confidence**:
- The mechanism of caching verified lemmas to reduce context length is sound and well-supported by the reported reduction in average function calls (15 → 10) and sequence length (28K → 17K tokens).
- The binary outcome reward structure is clearly described and consistent with the VAPO framework.
- The sketch model's role in reducing search space is empirically validated by the correlation between sketch quality and solve rate.

**Medium Confidence**:
- The claim that the sketch model "bridges the natural-to-formal gap" is plausible given the rubric-based RL, but the rubric's semantic alignment with formal validity is not independently verified.
- The recursive recovery mechanism is described clearly, but its efficiency gain over alternative strategies (e.g., breadth-first search) is not quantified.
- The test-time compute scaling (TTS) effect is demonstrated on benchmark datasets, but the paper does not isolate the contribution of recursion versus other factors (e.g., better sketches with more compute).

**Low Confidence**:
- The claim that the system "solved 11/12 Putnam 2025 problems in 9 hours" is impressive but lacks detail on the distribution of solve times, the number of retries per problem, or the computational cost per problem.
- The assertion that the RL agent learns "efficiency" without explicit length penalties is inferred from the data but not rigorously proven (e.g., no ablation of reward shaping).

## Next Checks

1. **Rubric Ablation**: Run the sketch model with and without the rubric-based RL on a held-out set of natural language proofs. Measure the proportion of sketches that are formally valid versus those that are semantically plausible but mathematically false.

2. **Recursive Recovery Bound**: Implement a recursion depth limit and compare solve rates with and without recursion on a subset of FATE-X problems. Quantify the trade-off between depth and success rate.

3. **Exploration Strategy Analysis**: Log the distribution of lemma validity scores during early RL training. If the agent rarely discovers valid lemmas, investigate whether a curriculum-based exploration strategy (e.g., starting with easier sub-goals) improves learning efficiency.