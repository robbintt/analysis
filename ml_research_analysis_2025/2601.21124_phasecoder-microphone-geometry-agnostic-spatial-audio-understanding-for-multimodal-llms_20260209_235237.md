---
ver: rpa2
title: 'PhaseCoder: Microphone Geometry-Agnostic Spatial Audio Understanding for Multimodal
  LLMs'
arxiv_id: '2601.21124'
source_url: https://arxiv.org/abs/2601.21124
tags:
- audio
- spatial
- distance
- degrees
- speaker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PhaseCoder introduces a transformer-only spatial audio encoder
  that is agnostic to microphone geometry, addressing the limitation that current
  multimodal LLMs process audio as a mono stream, ignoring spatial information. By
  taking raw multichannel audio and microphone coordinates as inputs, PhaseCoder performs
  localization and produces robust spatial embeddings, enabling an LLM to perform
  complex spatial reasoning and targeted transcription tasks from arbitrary microphone
  arrays.
---

# PhaseCoder: Microphone Geometry-Agnostic Spatial Audio Understanding for Multimodal LLMs

## Quick Facts
- **arXiv ID:** 2601.21124
- **Source URL:** https://arxiv.org/abs/2601.21124
- **Reference count:** 40
- **Primary result:** 86.96% accuracy and 7.44° MAE on azimuth localization with arbitrary microphone arrays

## Executive Summary
PhaseCoder introduces a transformer-based spatial audio encoder that enables multimodal large language models to process spatial audio information from arbitrary microphone geometries. Current multimodal LLMs treat audio as a mono stream, ignoring spatial cues that are crucial for tasks like sound source localization and spatial reasoning. PhaseCoder solves this by taking raw multichannel audio and microphone coordinates as inputs, using a novel phase modulation embedding technique to produce geometry-agnostic spatial embeddings that can be integrated with existing LLMs. When integrated with Gemma 3n, the system demonstrates substantial improvements across spatial reasoning, localization, and targeted transcription tasks.

## Method Summary
PhaseCoder uses a transformer encoder (5 blocks, 4 heads, 256 dim) that processes raw multichannel audio through STFT magnitude and phase representations. The key innovation is phase modulation embeddings, which encode microphone coordinates as sinusoidal functions and modulate them with audio phase information to create geometry-agnostic spatial representations. The system is trained on 1.5M synthetic Room Impulse Responses generated via image-source method, using a 2-stage curriculum: first on clean speech, then with added distractors. For LLM integration, spatial embeddings are projected to 2048 dimensions and prepended to mono audio tokens. Gemma 3n is fine-tuned using LoRA adapters and a 5-stage curriculum schedule to handle spatial reasoning tasks including yes/no questions, localization, and targeted transcription.

## Key Results
- Achieves 86.96% accuracy and 7.44° MAE on azimuth localization on LOCATA dataset
- 76.76% accuracy on yes/no spatial reasoning questions when integrated with LLM
- 10.63% WER on synthetic targeted transcription tasks
- Superior performance to both time-domain and frequency-domain baselines on microphone-invariant localization benchmarks

## Why This Works (Mechanism)
PhaseCoder works by decoupling spatial information from microphone geometry through phase modulation embeddings. The system encodes microphone coordinates as sinusoidal functions and modulates them with audio phase information, creating embeddings that capture spatial relationships without depending on specific array configurations. This geometry-agnostic approach allows the model to generalize across different microphone setups while maintaining precise localization capabilities. The 2-stage curriculum training ensures the model first learns clean spatial patterns before handling noisy, multi-source scenarios.

## Foundational Learning
- **Room Impulse Response (RIR) generation:** Understanding image-source method for synthetic acoustic environments - needed for data generation, quick check: verify RIRs produce realistic reverberation patterns
- **Phase Modulation Embeddings:** Sinusoidal encoding of coordinates combined with audio phase - needed for geometry-agnostic spatial representation, quick check: verify embeddings capture relative microphone positions
- **Multi-stage Curriculum Learning:** Progressive training from clean to noisy data - needed for stable convergence, quick check: compare loss curves with and without curriculum
- **Transformer-based Spatial Encoding:** Using [CLS] token for coordinate prediction - needed for unified localization framework, quick check: verify [CLS] captures spatial information
- **LLM Integration via Projection:** Mapping 256-dim spatial embeddings to 2048-dim LLM space - needed for compatibility with existing models, quick check: verify spatial tokens are properly aligned with audio tokens

## Architecture Onboarding

**Component Map:** Raw Multichannel Audio + Microphone Coordinates -> STFT Frontend -> Phase Modulation Embeddings -> Transformer Encoder -> [CLS] Spatial Embeddings -> LLM Integration

**Critical Path:** PhaseCoder Encoder training (2-stage curriculum) -> Projection MLP training -> LLM fine-tuning (5-stage curriculum) -> Inference pipeline

**Design Tradeoffs:** Uses 250ms analysis windows for computational efficiency vs. longer windows that might capture more temporal context; prioritizes azimuth/elevation accuracy over distance estimation which is inherently harder with audio alone

**Failure Signatures:** 
- Encoder divergence on noisy data (loss plateaus or spikes early)
- Poor distance estimation despite good angular accuracy
- LLM hallucination/leakage causing transcript swapping in multi-speaker scenarios

**Three First Experiments:**
1. Train PhaseCoder encoder on synthetic data with 2-stage curriculum and evaluate on LOCATA azimuth localization
2. Test geometry-agnostic properties by evaluating same trained model on different microphone array configurations
3. Integrate trained PhaseCoder with a publicly available LLM (e.g., Gemma 2) and evaluate spatial reasoning task performance

## Open Questions the Paper Calls Out

**Open Question 1:** Can PhaseCoder be extended to explicitly model source velocity and trajectory to support dynamic embodied interactions? The current architecture uses a 250ms analysis window optimized for stationary sources, lacking the temporal modeling required for tracking moving targets. Successful localization on a benchmark dataset featuring fast-moving sound sources would resolve this.

**Open Question 2:** How can device-specific acoustic transfer functions be integrated to model the effects of baffling and diffraction without losing geometry agnosticism? The current input pipeline only accepts raw audio and coordinates; it does not account for how the physical device housing distorts the sound field. Improved localization accuracy on real-world devices when conditioned on simulated or measured acoustic transfer functions would resolve this.

**Open Question 3:** Does fusing PhaseCoder embeddings with visual depth estimation effectively resolve the inherent ambiguity of audio-only distance estimation? The paper reports that distance error (MAE) is consistently higher than angular error, and audio cues alone are insufficient for precise ranging. A significant reduction in Mean Absolute Error (MAE) for distance metrics on multimodal (audio-visual) benchmarks compared to audio-only baselines would resolve this.

## Limitations
- Distance estimation is fundamentally harder than angular localization due to underdetermined nature of audio-based ranging
- Assumes free-floating microphone arrays without accounting for device-specific acoustic transfer functions
- Requires precise multi-stage curriculum training that is sensitive to hyperparameter tuning

## Confidence
- **High Confidence:** PhaseCoder encoder architecture and geometry-agnostic spatial embeddings are well-supported by ablation studies showing superior LOCATA benchmark performance
- **Medium Confidence:** LLM integration methodology and spatial reasoning performance improvements are credible but depend on specific base model capabilities
- **Low Confidence:** Exact replication of results is uncertain due to unavailability of Gemma 3n weights and sensitive multi-stage curriculum training

## Next Checks
1. Implement and validate PhaseCoder encoder architecture independently using a publicly available LLM to verify core spatial encoding functionality and LOCATA benchmark performance can be reproduced
2. Conduct ablation studies removing the 2-stage curriculum training to empirically verify the authors' claim that this approach fails to converge
3. Test PhaseCoder encoder with real-world microphone array recordings from LOCATA dataset to evaluate whether geometry-agnostic properties generalize beyond controlled training conditions