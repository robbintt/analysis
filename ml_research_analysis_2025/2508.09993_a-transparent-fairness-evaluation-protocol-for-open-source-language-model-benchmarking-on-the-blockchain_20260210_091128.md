---
ver: rpa2
title: A Transparent Fairness Evaluation Protocol for Open-Source Language Model Benchmarking
  on the Blockchain
arxiv_id: '2508.09993'
source_url: https://arxiv.org/abs/2508.09993
tags:
- fairness
- language
- evaluation
- metrics
- protocol
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a blockchain-based protocol for transparent,
  reproducible, and immutable fairness evaluation of open-source large language models
  (LLMs). The authors deploy smart contracts on the Internet Computer Protocol (ICP)
  to store benchmark datasets, prompts, and fairness metrics directly on-chain, ensuring
  verifiable linkage between specific model versions and evaluation results.
---

# A Transparent Fairness Evaluation Protocol for Open-Source Language Model Benchmarking on the Blockchain

## Quick Facts
- arXiv ID: 2508.09993
- Source URL: https://arxiv.org/abs/2508.09993
- Authors: Hugo Massaroli; Leonardo Iara; Emmanuel Iarussi; Viviana Siless
- Reference count: 6
- Key outcome: Blockchain-based protocol for transparent, reproducible fairness evaluation of open-source LLMs using ICP smart contracts and HTTP outcalls to Hugging Face endpoints

## Executive Summary
This paper introduces a blockchain-based protocol for transparent and verifiable fairness evaluation of open-source large language models. The system uses Internet Computer Protocol smart contracts to store benchmark datasets, prompts, and fairness metrics immutably on-chain, ensuring reproducible evaluations linked to specific model versions. Evaluations are executed via HTTP outcalls to hosted Hugging Face model endpoints, with all input-output pairs and metrics logged for public auditing. The protocol benchmarks Meta Llama 3.1-8B, DeepSeek R1 Distill Llama 8B, and Mistral 7B Instruct models across PISA, StereoSet, and Kaleidoscope datasets, measuring fairness using multiple complementary metrics including statistical parity difference, equal opportunity difference, and ICAT scores. Results show Llama consistently outperforms other models in fairness metrics, accuracy, and multilingual robustness.

## Method Summary
The protocol deploys smart contracts on Internet Computer Protocol that store benchmark datasets and prompt templates, then execute HTTP outcalls to external Hugging Face endpoints for model inference. All input-output pairs and computed fairness metrics are stored immutably on-chain, creating verifiable audit trails linking specific model versions to their evaluation results. The system evaluates three open-source models (Llama 3.1-8B, DeepSeek R1 Distill, Mistral 7B) using PISA for academic performance prediction, StereoSet for bias measurement, and Kaleidoscope for multilingual fairness across English, Spanish, and Portuguese. Fairness metrics include statistical parity difference, equal opportunity difference, average odds difference, disparate impact ratio, and ICAT scores, along with accuracy, precision, recall, and format error rate measurements.

## Key Results
- Llama 3.1-8B achieved the highest ICAT scores (General: 63.81, Race: 65.36, Gender: 56.85, Religion: 56.66, Profession: 58.94)
- Llama maintained lowest error rates across languages (English: 49.6%, Spanish: 46.1%, Portuguese: 31.3%)
- DeepSeek showed format error rate of 90.1% on Portuguese prompts, indicating poor multilingual robustness
- Statistical parity and equal opportunity differences were near zero for all models, with Llama showing best overall performance
- Counterfactual Change Rate analysis revealed Llama had lowest demographic sensitivity (0.3259) compared to DeepSeek (0.4110) and Mistral (0.4176)

## Why This Works (Mechanism)

### Mechanism 1: On-Chain Immutability for Verifiable Model-Version Linkage
- **Claim:** Storing datasets, prompts, and computed metrics directly on-chain creates tamper-resistant audit trails that link specific model versions to their fairness evaluations.
- **Mechanism:** Smart contracts deployed on Internet Computer Protocol store canonical benchmark versions and execute HTTP outcalls to Hugging Face endpoints; all input-output pairs and metrics are logged immutably, enabling third-party verification without relying on centralized authorities.
- **Core assumption:** The ICP blockchain's immutability guarantees hold, and external model endpoints remain accessible and version-stable; evaluators trust the on-chain execution environment to be non-colluding.
- **Evidence anchors:** [abstract]: "ensuring verifiable, immutable, and reproducible evaluations by executing on-chain HTTP requests to hosted Hugging Face endpoints and storing datasets, prompts, and metrics directly on-chain"; [section 2.1]: "Each evaluation is verifiable and reproducible. Input-output pairs, along with computed fairness metrics, are stored immutably on-chain and can be independently verified by third parties."

### Mechanism 2: Counterfactual Prompting Isolates Attribute-Specific Bias
- **Claim:** Systematically varying sensitive attributes (race, gender) while holding all other inputs constant quantifies how much model predictions change due solely to demographic factors.
- **Mechanism:** The protocol uses structured prompts containing demographic attributes (e.g., `raceeth: Black` in PISA prompts); Counterfactual Change Rate (CFR) measures output flip frequency when only the sensitive attribute is modified—lower CFR indicates less demographic sensitivity.
- **Core assumption:** LLM outputs are sufficiently deterministic (via prompting constraints) that attribute-driven changes can be distinguished from random output variance.
- **Evidence anchors:** [section 2.3]: "The counterfactual change rate quantifies how often a model's output changes when a sensitive attribute (like race or gender) is modified — while keeping all other inputs the same. Thus, a lower value signifies lower bias."; [section 2.5]: PISA prompt template includes demographic attributes like `raceeth: Black`, `fatherBachelors: 0.0`, enabling controlled variation.

### Mechanism 3: Multi-Metric Aggregation Captures Tradeoffs Single Metrics Miss
- **Claim:** Using complementary fairness metrics (SPD, EOD, AOD, DIR, ICAT) reveals performance-fairness tradeoffs that any single metric would obscure.
- **Mechanism:** The protocol computes both group-level classification fairness metrics (statistical parity, equal opportunity) and context-aware association tests (ICAT) across multiple domains (race, gender, religion, profession); divergences between metrics signal where bias manifests differently.
- **Core assumption:** No single metric captures all bias dimensions; tradeoffs between metrics are interpretable and can guide context-specific model selection.
- **Evidence anchors:** [abstract]: "Fairness metrics include statistical parity difference, equal opportunity difference, average odds difference, and disparate impact ratio, along with structured Context Association Test (ICAT) scores"; [section 3.1]: "Llama outperforms DeepSeek and Mistral in fairness metric for the PISA dataset, except for the Statistical Parity Difference where DeepSeek outperforms Llama. Nevertheless, both metrics are near zero, showing good behavior overall."

## Foundational Learning

- **Concept: Statistical Parity vs. Equal Opportunity**
  - Why needed here: The paper reports both SPD and EOD; understanding their difference is essential for interpreting why Llama outperforms on most metrics but not all.
  - Quick check question: If a model predicts "high reading score" for 60% of Group A and 50% of Group B, what is the Statistical Parity Difference? If true positive rates are 70% and 65% respectively among students who actually have high scores, what is the Equal Opportunity Difference?

- **Concept: ICAT (Idealized Contextual Association Test) Scores**
  - Why needed here: ICAT scores (0–100) are the paper's primary fairness metric but are less widely known than SPD/EOD; interpreting them requires understanding their composite nature.
  - Quick check question: An ICAT score of 65.36 (Llama's race score) indicates what about bias and language modeling performance relative to a score of 19.24 (Mistral's race score)?

- **Concept: Smart Contract HTTP Outcalls**
  - Why needed here: The protocol's novelty lies in on-chain execution of external API calls; understanding trust assumptions here is critical for reproducing or extending the system.
  - Quick check question: When a smart contract makes an HTTP request to an external model endpoint, what prevents a malicious actor from spoofing the response?

## Architecture Onboarding

- **Component map:** ICP Canister -> Hugging Face Endpoints -> On-chain Storage -> Datasets (PISA, StereoSet, Kaleidoscope)
- **Critical path:** 1. Deploy canister with datasets and prompt templates pre-loaded; 2. Execute HTTP outcalls to model endpoints with structured prompts; 3. Parse responses (handle format errors—DeepSeek: 90.1% error rate on Portuguese); 4. Compute fairness metrics on-chain; 5. Store results immutably for auditing
- **Design tradeoffs:** On-chain computation ensures transparency but adds latency and cost vs. off-chain evaluation; HTTP outcalls to third-party endpoints create external dependencies and trust assumptions; multilingual evaluation reveals cross-linguistic disparities but increases error rates and complexity
- **Failure signatures:** High format error rates (DeepSeek: 90.1% on Portuguese): prompt template insufficiently constrains output; Large cross-language accuracy gaps (Llama: 49.6% English vs. 31.3% Portuguese): likely tokenization or pre-training data bias; Conflicting metric rankings (Llama best on most, DeepSeek best on SPD): indicates need for context-specific weighting
- **First 3 experiments:** 1. Reproduce PISA evaluation with Llama 3.1-8B using provided prompts; verify SPD (-0.0190), EOD (-0.0122), and CFR (0.3259) match reported values; 2. Add a new open-source model (e.g., Qwen 2.5 or Gemma 2) to the pipeline; compare ICAT scores across race, gender, and profession dimensions; 3. Extend Kaleidoscope evaluation to a fourth language (e.g., French); assess whether format error rates and accuracy disparities follow the Portuguese pattern

## Open Questions the Paper Calls Out
None explicitly identified in the paper.

## Limitations
- The evaluation protocol relies on HTTP outcalls to external Hugging Face endpoints, creating trust dependencies on third-party model availability and version stability.
- The high format error rate observed for DeepSeek on Portuguese (90.1%) suggests the prompt engineering may not generalize robustly across languages or model architectures.
- The ICP blockchain's long-term viability for hosting evaluation infrastructure remains untested at scale, and the computational cost of on-chain metric computation could become prohibitive for larger datasets.

## Confidence

- **High Confidence:** The immutability and auditability of on-chain evaluation results; the ranking of Llama as most fair across multiple metrics; the multilingual accuracy disparities between English, Spanish, and Portuguese.
- **Medium Confidence:** The Counterfactual Change Rate as a bias measure; the ICAT score interpretation and its composite nature; the general framework's extensibility to new models and datasets.
- **Low Confidence:** The robustness of prompt templates across diverse model architectures; the long-term sustainability of HTTP outcall-based evaluation; the fairness of computational cost distribution in the current design.

## Next Checks

1. **Cross-Model Prompt Robustness:** Test the same PISA prompt templates with additional open-source models (e.g., Qwen 2.5, Gemma 2) to quantify how format error rates and fairness metrics vary with model architecture and training corpus.

2. **Language Generalization:** Extend Kaleidoscope evaluation to a fourth language (e.g., French) and measure whether format error rates and accuracy gaps follow the Portuguese pattern, isolating whether disparities stem from tokenization, pre-training data, or prompt engineering.

3. **Longitudinal Tracking:** Deploy the protocol on ICP mainnet and run monthly evaluations of the same model versions, verifying that on-chain logs remain accessible and that metric trends (e.g., ICAT scores) are consistent over time.