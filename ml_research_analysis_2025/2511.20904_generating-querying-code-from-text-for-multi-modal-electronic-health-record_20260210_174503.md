---
ver: rpa2
title: Generating Querying Code from Text for Multi-Modal Electronic Health Record
arxiv_id: '2511.20904'
source_url: https://arxiv.org/abs/2511.20904
tags:
- question
- patient
- query
- subject
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TQGen-EHRQuery, a framework for generating
  SQL queries from natural language questions over multi-modal electronic health records
  (EHRs) that combine structured tabular data with unstructured clinical text. The
  authors constructed a novel dataset integrating MIMIC-IV tables with MIMIC-CXR radiology
  reports and MIMIC-IV-Note discharge summaries, creating diverse question-answer
  pairs across both modalities.
---

# Generating Querying Code from Text for Multi-Modal Electronic Health Record

## Quick Facts
- arXiv ID: 2511.20904
- Source URL: https://arxiv.org/abs/2511.20904
- Reference count: 36
- Authors: Mengliang Zhang
- Primary result: Framework for generating SQL queries from natural language questions over multi-modal EHR data achieves 53-61% exact match accuracy for simple queries and 48-59% for complex ones, with execution accuracy of 80-91%.

## Executive Summary
This paper introduces TQGen-EHRQuery, a framework for generating SQL queries from natural language questions over multi-modal electronic health records (EHRs) that combine structured tabular data with unstructured clinical text. The authors constructed a novel dataset integrating MIMIC-IV tables with MIMIC-CXR radiology reports and MIMIC-IV-Note discharge summaries, creating diverse question-answer pairs across both modalities. The framework incorporates medical knowledge mapping, question template matching, and a toolset module for text processing, enabling the system to interpret both structured queries and complex text-based questions. Experiments show that larger language models (up to 32B parameters) achieve strong performance, with exact match accuracy ranging from 53-61% for simpler queries and 48-59% for more complex ones, while execution accuracy reaches 80-91%. The medical knowledge module proved most critical for accuracy, particularly for handling medical terminology variations.

## Method Summary
The TQGen-EHRQuery framework processes natural language questions to generate executable SQL queries over multi-modal EHR data. It uses a modular architecture with four key components: a Table Description Module that provides schema context to the LLM, a Matching Module that retrieves semantically similar question-query pairs using BERT embeddings and Faiss, a Medical Knowledge Module that maps medical terminology variants to canonical database terms using UMLS, and a Toolset Module that encapsulates text processing as callable tools. The framework employs a code inspection module for iterative error correction and can handle both structured table queries and text-based questions requiring clinical note analysis. The approach was evaluated on a novel TQGen dataset combining MIMIC-IV tables with MIMIC-CXR and MIMIC-IV-Note clinical text, using metrics including Exact Match Accuracy, Execution Accuracy, and LLM-based scoring.

## Key Results
- Exact Match Accuracy: 53-61% for Level I queries (≤3 constraints) and 48-59% for Level II queries (>3 constraints)
- Execution Accuracy: 80-91% across all query complexity levels
- Medical knowledge module ablation caused the most significant accuracy decline, highlighting its critical importance
- Performance scales with model size, with 32B parameter models achieving the strongest results
- The framework successfully handles both structured table queries and text-based questions requiring clinical note analysis

## Why This Works (Mechanism)

### Mechanism 1: Medical Knowledge Normalization via Terminology Mapping
Standardizing medical terminology variants improves query-to-schema alignment in EHR text-to-SQL tasks. A terminology library maps colloquial and variant medical terms (e.g., "red blood cell," "RBC") to canonical database column names using UMLS concepts during the matching phase, ensuring user language maps to actual schema elements.

### Mechanism 2: Semantic Template Retrieval for Query Structure Guidance
Retrieving semantically similar question-query pairs provides structural scaffolding that improves code generation accuracy. Incoming questions are embedded with a pre-trained BERT model; Faiss retrieves top-K similar historical questions. Their associated query templates guide structure even when exact templates don't exist.

### Mechanism 3: Modular Tool Abstraction for Multi-Modal Text Processing
Encapsulating clinical text processing as callable tools extends SQL-only systems to handle unstructured EHR text without embedding full text understanding into the query generator. A Text_Func tool takes (report_text, question) and returns extracted answers when the LLM detects a question requiring text processing.

## Foundational Learning

- **Text-to-SQL with LLMs and Schema Linking**: Needed to understand how LLMs handle schema context, column selection, and multi-table joins in EHR databases. Quick check: Given "What was the highest creatinine value for patient 123 in their last admission?", identify tables, columns, and join keys needed in MIMIC-IV.

- **Semantic Retrieval (Dense Embeddings + Similarity Search)**: Needed to understand how BERT embeddings and Faiss work for efficient template retrieval. Quick check: If a new question has no semantically close templates (similarity scores < 0.3), what fallback strategies could the system use?

- **Medical Terminology Variation and Standardization (UMLS Basics)**: Needed to understand how terminology normalization works and where it fails in EHR queries. Quick check: A user asks about "heart attack" but the diagnosis table uses ICD codes with descriptions like "acute myocardial infarction." Where in the pipeline would mapping occur, and what could cause it to fail?

## Architecture Onboarding

- **Component map**: Table Description Module -> Matching Module (BERT + Faiss + UMLS mapping) -> Medical Knowledge Module -> Toolset Module (Text_Func) -> Code Inspection Module -> Agent Executor

- **Critical path**: Question → semantic embedding → template retrieval → terminology normalization → LLM generates query with schema context + templates + tools → Executor runs code → error feedback loop for repair → if text processing detected, Text_Func invoked with dynamic prompt

- **Design tradeoffs**: Pre-stored templates improve accuracy but require curation and limit generalization; local model deployment ensures privacy but trades capability; tool abstraction improves interpretability but introduces orchestration complexity

- **Failure signatures**: Terminology mapping failures (unmapped synonyms cause empty results), template retrieval misses (no similar templates → hallucination risk), tool invocation failures (wrong modality processing), code execution errors (syntax errors, wrong column names, incorrect JOIN logic)

- **First 3 experiments**: 1) Baseline validation on TQGen test set to replicate reported metrics (Qwen2.5-14B: Level I EM ~0.58, EX ~0.82), 2) Ablation on Medical Knowledge Module to measure accuracy drop, 3) Tool detection accuracy test on mixed modality questions

## Open Questions the Paper Calls Out

- **Cross-dataset generalizability**: How does TQGen-EHRQuery perform when applied to EHR systems outside the MIMIC environment with differing schema structures and terminology standards? The current study exclusively utilizes MIMIC datasets, limiting generalizability.

- **Unanswerable query detection**: What specific architectural or prompting mechanisms can be developed to reliably detect and reject ambiguous or unanswerable queries rather than attempting execution or returning generic fallback responses?

- **Clinical workflow integration**: Does the integration of TQGen-EHRQuery into clinical workflows significantly improve clinician efficiency and data retrieval accuracy compared to standard manual EHR navigation methods?

## Limitations
- Dataset dependency on MIMIC-IV, MIMIC-CXR, and MIMIC-IV-Note limits generalizability to other EHR systems with different schemas and terminology
- Template bank coverage and scalability challenges as the approach depends on manually curated templates that may not cover all query types
- Medical knowledge module brittleness due to reliance on UMLS mapping that may fail on rare medical terms or novel clinical concepts

## Confidence
- **High confidence**: Medical knowledge normalization significantly improves accuracy (well-supported by ablation results)
- **Medium confidence**: Performance metrics are reported but lack rigorous comparison to state-of-the-art baselines
- **Low confidence**: Generalizability of the template-based approach to other EHR systems is speculative

## Next Checks
1. Cross-dataset generalization test: Apply TQGen-EHRQuery to a different EHR dataset (e.g., eICU) and measure performance drop
2. Template bank ablation study: Systematically remove templates and measure accuracy decline as coverage decreases
3. UMLS coverage analysis: Log all medical terms that fail to map via UMLS during query processing and calculate coverage percentage