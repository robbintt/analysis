---
ver: rpa2
title: 'VLSP 2025 MLQA-TSR Challenge: Vietnamese Multimodal Legal Question Answering
  on Traffic Sign Regulation'
arxiv_id: '2510.20381'
source_url: https://arxiv.org/abs/2510.20381
tags:
- legal
- question
- multimodal
- traffic
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'VLSP 2025 MLQA-TSR introduced a Vietnamese multimodal legal QA
  benchmark focusing on traffic sign regulation. The shared task involved two subtasks:
  multimodal legal retrieval and multimodal question answering, requiring integration
  of text and images with Vietnamese legal documents.'
---

# VLSP 2025 MLQA-TSR Challenge: Vietnamese Multimodal Legal Question Answering on Traffic Sign Regulation

## Quick Facts
- arXiv ID: 2510.20381
- Source URL: https://arxiv.org/abs/2510.20381
- Reference count: 8
- Top system achieved 64.55% F2 on legal retrieval and 86.30% accuracy on QA

## Executive Summary
VLSP 2025 MLQA-TSR introduced a Vietnamese multimodal legal QA benchmark focusing on traffic sign regulation. The shared task involved two subtasks: multimodal legal retrieval and multimodal question answering, requiring integration of text and images with Vietnamese legal documents. Participants developed diverse approaches using vision-language models, multimodal embeddings, traffic sign detection, and graph-based retrieval methods. The top system achieved an F2 score of 64.55% on legal retrieval and 86.30% accuracy on question answering. Results demonstrated strong performance in multimodal legal QA while highlighting challenges in legal retrieval due to document complexity. The dataset and evaluation framework provide a valuable resource for advancing Vietnamese multimodal legal processing research.

## Method Summary
The challenge featured two subtasks: multimodal legal retrieval (retrieving relevant articles given question + traffic sign image) and multimodal QA (answering multiple-choice or Yes/No questions using question + image + retrieved articles). The baseline retrieval used BGE Visualized embeddings with dot-product similarity returning top-5 candidates, while the baseline QA used Vintern-3B-beta with task-specific prompts. Participants employed diverse approaches including YOLO-based traffic sign detection, CLIP-style multimodal embeddings, vision-language models like Qwen2.5-VL and InternVL3, and graph-based representations for document relationships. The law database contained 402 articles covering QCVN 41:2024/BGTVT and 36/2024/QH15 regulations, with images formatted as `«IMAGE: filename.jpg /IMAGE»` and tables as `«TABLE: html /TABLE»`.

## Key Results
- Top retrieval system achieved 64.55% F2 score on legal retrieval task
- Best QA system reached 86.30% accuracy on multimodal question answering
- 9 teams participated with diverse approaches combining vision-language models, detection algorithms, and retrieval techniques
- Significant performance gap remains between current results (65% F2) and text-only legal retrieval (87% F2)

## Why This Works (Mechanism)

### Mechanism 1: Object-Aware Visual Encoding for Retrieval
Filtering and cropping traffic signs before embedding improves retrieval recall compared to encoding whole images directly. Object detection models identify traffic sign regions which are encoded separately or concatenated, reducing noise from irrelevant visual context before computing similarity against article embeddings.

### Mechanism 2: Few-Shot In-Context Learning with Retrieved Legal Context
Providing retrieved articles as context in prompts improves QA accuracy over zero-shot alone. Retrieved legal articles are inserted into LLM prompts alongside the question and image, allowing the model to condition its answer on explicit legal text rather than relying solely on parametric knowledge.

### Mechanism 3: Heterogeneous Graph Representation for Multimodal Reasoning
Structuring multimodal legal documents as graphs captures semantic relationships that improve retrieval ranking. Nodes represent text chunks, images, and tables; edges encode semantic/legal relationships. Graph matching combined with similarity search re-ranks candidates.

## Foundational Learning

- **Multimodal Embedding Alignment (CLIP-style)**: Why needed - Subtask 1 requires encoding both text queries and image queries into a shared space to compute similarity against article embeddings. Quick check - Can you explain how contrastive learning aligns image and text embeddings in a shared vector space?

- **Vision-Language Models (VLMs)**: Why needed - Subtask 2 requires reasoning over both the traffic sign image and legal text to select the correct answer. Quick check - How does a VLM like Qwen2.5-VL process interleaved image and text tokens in its attention mechanism?

- **Vietnamese Tokenization (Word Segmentation)**: Why needed - Vietnamese is an isolating language requiring word segmentation (e.g., Pyvi); incorrect tokenization affects question length statistics and embedding quality. Quick check - Why does Vietnamese require explicit word segmentation before tokenization, unlike English?

## Architecture Onboarding

- **Component map**:
  - Retrieval Pipeline: Image input → Traffic sign detection (YOLO/OWLv2) → Crop/filter → Vision encoder (CLIP/BGE) + Text encoder → Vector DB (Qdrant/FAISS) → Top-k retrieval → Optional graph re-ranking
  - QA Pipeline: Question + Image + Retrieved articles → VLM (Qwen2.5-VL/InternVL3) → Prompt with context → Answer generation
  - Law Database: Preprocessed articles → Chunked and encoded offline

- **Critical path**: Traffic sign detection accuracy directly impacts retrieval recall; retrieval quality caps QA performance; prompt engineering for legal context determines how well the VLM uses retrieved articles.

- **Design tradeoffs**:
  - Detection vs. whole-image encoding: Detection adds latency but improves signal; whole-image is faster but noisier
  - Graph vs. flat embeddings: Graphs capture structure but require relationship extraction; flat embeddings are simpler but miss cross-modal links
  - Zero-shot vs. few-shot prompting: Zero-shot is simpler; few-shot improves accuracy but requires example selection

- **Failure signatures**:
  - Low retrieval F2 with high QA accuracy: Retrieval may be returning wrong articles that happen to contain answer keywords
  - High variance between multiple-choice and yes/no questions: Model may be biased toward one format
  - Retrieved articles with high similarity scores but wrong content: Embedding space may not align with legal semantics

- **First 3 experiments**:
  1. Baseline retrieval comparison: Implement BGE Visualized baseline; measure F2; then add YOLO-based cropping and compare
  2. Ablation on retrieved context for QA: Provide VLM with (a) no articles, (b) top-1 article, (c) top-3 articles; measure accuracy delta
  3. Prompt format sensitivity: Test multiple-choice vs. yes/no prompts with identical VLM; check if model prefers one format

## Open Questions the Paper Calls Out

### Open Question 1
Can multimodal legal retrieval approaches close the significant performance gap between the current best result (64.55% F2) and text-only legal retrieval systems (87% F2 in ALQAC 2024)? The paper identifies this gap but does not propose specific solutions or analyze which aspects of multimodal integration cause the performance degradation.

### Open Question 2
Would commercial LLMs (e.g., ChatGPT, Gemini, Claude) substantially outperform the best open-source model result (86.30% accuracy) on the multimodal QA task? The competition rules explicitly prohibited commercial LLMs, limiting understanding of the task's true difficulty.

### Open Question 3
How does the small training dataset size (530 questions) affect model generalization, particularly given the distribution shift between training and private test sets? The paper does not analyze whether performance gaps between teams stem from data efficiency, overfitting, or genuine generalization failures.

### Open Question 4
Can approaches developed for traffic sign regulation transfer effectively to other Vietnamese multimodal legal domains? The dataset focuses exclusively on traffic sign regulation, and no discussion of domain generalization or transfer learning appears in the paper.

## Limitations
- Small test set sizes (100 public, 146 private questions) may inflate reported F2 scores
- Dataset focuses exclusively on traffic sign regulation, limiting generalizability to other Vietnamese legal domains
- Evaluation framework assumes ground-truth article lists are complete, but with 2-3 relevant articles per question, retrieval systems might achieve high recall by returning extra articles

## Confidence
- **High confidence**: The basic retrieval-augmented QA pipeline is well-established and results align with expectations for multimodal legal processing tasks
- **Medium confidence**: Novel mechanisms (traffic sign detection, graph representations) lack extensive ablation studies to confirm isolated impact
- **Low confidence**: Claims about general applicability to other Vietnamese legal domains or real-world deployment are not supported by evaluation scope

## Next Checks
1. Implement the same retrieval pipeline with and without YOLO-based traffic sign detection/cropping, measuring F2 score delta to quantify detection component's contribution
2. Evaluate retrieval and QA performance on traffic signs from jurisdictions outside QCVN 41:2024/BGTVT to assess domain generalization limits
3. Analyze the relationship between top-1 article relevance score and final QA accuracy to determine if retrieval quality truly caps QA performance or if the VLM can compensate for retrieval errors