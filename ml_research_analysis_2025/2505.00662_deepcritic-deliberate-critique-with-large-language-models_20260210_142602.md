---
ver: rpa2
title: 'DeepCritic: Deliberate Critique with Large Language Models'
arxiv_id: '2505.00662'
source_url: https://arxiv.org/abs/2505.00662
tags:
- critique
- step
- qwen2
- correct
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of enabling Large Language Models
  (LLMs) to provide accurate and detailed critiques of mathematical reasoning steps.
  The proposed DeepCritic framework uses a two-stage pipeline: first, it trains LLMs
  on long-form, deliberate critiques that include multi-perspective verifications
  and meta-critiquing; second, it performs reinforcement learning using either human-labeled
  or automatically annotated data.'
---

# DeepCritic: Deliberate Critique with Large Language Models

## Quick Facts
- arXiv ID: 2505.00662
- Source URL: https://arxiv.org/abs/2505.00662
- Authors: Wenkai Yang; Jingwen Chen; Yankai Lin; Ji-Rong Wen
- Reference count: 40
- Primary result: DeepCritic achieves up to 77.3 F1 score on error identification benchmarks, outperforming same-sized DeepSeek-R1-distill models and GPT-4o

## Executive Summary
DeepCritic addresses the challenge of enabling Large Language Models to provide accurate, detailed critiques of mathematical reasoning steps. The framework uses a two-stage pipeline: first training LLMs on long-form, deliberate critiques with multi-perspective verifications and meta-critiquing, then performing reinforcement learning with either human-labeled or automatically annotated data. Built on Qwen2.5-7B-Instruct, DeepCritic significantly outperforms existing LLM critics on error identification benchmarks while providing more detailed feedback to help LLM generators correct mistakes.

## Method Summary
DeepCritic employs a two-stage training pipeline. First, a 72B teacher model generates 4.5K long-form deliberate critiques through a three-stage process: initial critique generation, in-depth re-evaluation with alternative perspectives, and merging into final critiques. These critiques include multi-perspective verifications and meta-critiquing of initial critiques. Second, the 7B student model undergoes supervised fine-tuning on this seed data, followed by reinforcement learning using either human-labeled PRM800K data or automatically annotated data via Monte Carlo sampling. The RL stage uses GRPO with binary accuracy rewards to incentivize correct step-level judgments.

## Key Results
- DeepCritic-7B-SFT achieves 54.1 avg F1 score (20-point gain over base model)
- DeepCritic-7B-RL-PRM800K achieves 67.1 avg F1 score (13-point gain over SFT)
- DeepCritic-7B-RL-Numina (auto-labeled) achieves 63.5 avg F1 score
- Single-sample inference with DeepCritic-7B-RL-PRM800K achieves 77.3 F1 score on MR-GSM8K benchmark

## Why This Works (Mechanism)

### Mechanism 1: Multi-Stage Critique Synthesis with Meta-Critiquing
Generating critiques in stages (initial → in-depth → merged) produces more accurate feedback than single-pass generation. A 72B teacher model first generates initial critiques, then re-evaluates from alternative perspectives or critiques the initial critique itself. The merged critiques transfer this multi-perspective reasoning behavior to the student model. Evidence shows 565 step-level critiques were corrected through in-depth generation, and retained solutions require all in-depth judgments to match ground truth labels.

### Mechanism 2: Two-Stage Training (SFT Format Acquisition → RL Capability Incentivization)
Separating format learning (SFT) from capability elicitation (RL) allows efficient use of limited high-quality seed data while leveraging larger-scale annotated data for policy optimization. SFT on 4.5K curated critiques teaches the model structure, then RL incentivizes accurate judgments using binary accuracy rewards. DeepCritic-7B-SFT achieves 54.1 avg F1 (20-point gain), while RL-PRM800K achieves 67.1, demonstrating staged improvement.

### Mechanism 3: Monte Carlo Sampling for Automatic Step-Level Labels
Step correctness can be estimated without human annotation by truncating solutions and measuring downstream rollout success rates. For each step, truncate after that step and generate multiple independent completions. The first erroneous step is defined as the first step from which all rollouts are incorrect, while preceding steps have >50% correct rollouts. This provides weak supervision for RL when human labels are unavailable, though with a 3.6 F1 performance gap versus human-labeled data.

## Foundational Learning

- **Process Reward Models (PRMs)**: DeepCritic operates as a generative alternative to PRMs, providing step-level judgment with explanatory critiques rather than scalar scores. Quick check: Can you explain why a PRM that scores each step independently might miss errors that only manifest in step interactions?

- **Group Relative Policy Optimization (GRPO)**: The RL stage uses GRPO (from DeepSeekMath) rather than PPO. Quick check: How does GRPO differ from PPO in how it computes advantages, and what does this imply for variance reduction?

- **Chain-of-Thought Prompting with Long-Form Reasoning**: The deliberate critique format explicitly encourages extended reasoning before judgment, requiring 32K token context windows. Quick check: Why might longer CoT improve critique accuracy even when the final judgment is binary?

## Architecture Onboarding

- **Component map**: PRM800K/NuminaMath → Seed Data Generator (Qwen2.5-72B-Instruct) → SFT Model (Qwen2.5-7B-Instruct) → RL Training (verl framework with GRPO) → Inference

- **Critical path**: 1) Curate seed problems from PRM800K subset 2) Run 3-stage critique generation with 72B model (filter by judgment-label alignment) 3) SFT for 3 epochs, lr=1e-5 4) Prepare RL data (human labels OR Monte Carlo auto-labels) 5) RL with GRPO for 2 epochs, lr=1e-6, rollout_n=8 6) Evaluate on MR-GSM8K, PRM800K Phase-2, ProcessBench subsets

- **Design tradeoffs**: Seed data scale vs. quality (4.5K curated examples vs. potential scaling), Human vs. auto labels (PRM800K gives +3.6 F1 over NuminaMath auto-labels), Single-sample vs. Maj@8 (voting gives +1.4 to +6.3 F1 improvement at 8x inference cost)

- **Failure signatures**: Language mixing in RL rollouts (self-corrects during training), DeepSeek-R1-Distill baselines ignoring stop instructions and continuing to solve the problem during critique, Very long solutions (>8K tokens) truncated during RL, excluded from training

- **First 3 experiments**: 1) Reproduce SFT-only results on a 1K subset to validate data pipeline before full training 2) Ablate the in-depth critique stage: train SFT model on initial critiques only and compare F1 3) Test transfer: evaluate DeepCritic-7B-RL on a held-out domain (e.g., code reasoning) to assess generalization beyond math

## Open Questions the Paper Calls Out

- **Generalization beyond mathematics**: The framework's effectiveness for non-mathematical domains requiring step-by-step critique remains untested. The deliberate critique format may not transfer equally well to domains requiring different reasoning patterns.

- **Reliability of Monte Carlo labeling**: The automatic step-level error labeling via rollouts depends heavily on generator quality assumptions that aren't fully validated. Subtle errors that don't prevent subsequent correct paths could be missed.

- **Seed data scaling limits**: The relationship between seed data scale and final critique capability remains unexplored. It's unclear whether improvements come primarily from data quality or whether more examples would yield further gains.

## Limitations

- **High-quality seed critique dependency**: The approach relies on 4.5K curated long-form critiques generated by a 72B model, limiting scalability and introducing potential brittleness if teacher critiques contain systematic biases.

- **Monte Carlo labeling assumptions**: The automatic labeling method assumes generator failures reliably indicate step errors, which may not hold when generators are weak or when multiple valid solution paths exist.

- **Benchmark domain specificity**: While achieving strong results across math benchmarks, the framework's effectiveness for non-mathematical domains remains untested.

## Confidence

- **High Confidence**: The staged training methodology (SFT followed by RL) is well-supported by ablation results showing consistent performance improvements across benchmarks.

- **Medium Confidence**: The Monte Carlo automatic labeling approach shows reasonable effectiveness but the 3.6 F1 gap versus human labels suggests limitations that depend on generator quality.

- **Medium Confidence**: Transferability claims are based on limited cross-domain testing, requiring more extensive evaluation to strengthen claims about generalization beyond math.

## Next Checks

1. **Ablation of seed data quantity**: Systematically train models with varying amounts of seed data (e.g., 500, 1K, 2K, 4.5K) to quantify the relationship between seed quality/quantity and final performance.

2. **Cross-domain robustness testing**: Evaluate DeepCritic on non-mathematical reasoning tasks (e.g., code debugging, commonsense reasoning, scientific analysis) to assess whether the deliberate critique format generalizes.

3. **Generator-weakness sensitivity analysis**: Intentionally use weaker generators for Monte Carlo labeling to quantify how generator quality affects auto-labeled training data reliability and final model performance.