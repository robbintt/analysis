---
ver: rpa2
title: 'CLEVER: A Curated Benchmark for Formally Verified Code Generation'
arxiv_id: '2505.13938'
source_url: https://arxiv.org/abs/2505.13938
tags:
- implementation
- spec
- result
- specification
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CLEVER, a benchmark of 161 formally verified
  code generation problems in Lean. Unlike prior benchmarks, CLEVER uses non-computable
  specifications to prevent implementation leakage, requiring models to generate both
  formal specifications and Lean implementations with full machine-checkable proofs.
---

# CLEVER: A Curated Benchmark for Formally Verified Code Generation

## Quick Facts
- arXiv ID: 2505.13938
- Source URL: https://arxiv.org/abs/2505.13938
- Reference count: 40
- Key outcome: 161 Lean problems using non-computable specs; no LLM approach succeeds on more than one problem; spec certification is the hardest stage

## Executive Summary
CLEVER introduces a benchmark for formally verified code generation using Lean 4, where models must generate both specifications and implementations with machine-checkable proofs. Unlike prior benchmarks, CLEVER uses non-computable specifications to prevent implementation leakage, requiring genuine reasoning rather than syntactic pattern matching. The evaluation reveals that state-of-the-art LLMs and agentic approaches struggle with end-to-end verification, with specification certification proving particularly challenging. The benchmark provides a rigorous testbed for advancing neural-symbolic reasoning in verified code generation.

## Method Summary
CLEVER consists of 161 Lean problems adapted from HUMANEVAL, requiring models to generate formal specifications from natural language and prove equivalence to hidden ground truth specs, then generate implementations and prove correctness. Evaluation uses pass@k-seconds metric with k=600 seconds, measuring whether both specification and implementation verification succeed. The benchmark employs a staged pipeline with retry-based evaluation: spec generation → equivalence proof → implementation generation → correctness proof. Models are evaluated using few-shot prompting (GPT-4o, Claude-3.7, o4-mini, DeepSeek-R1) and agentic approaches (COPRA), with results showing high compilation rates but very low certification success.

## Key Results
- No approach achieves full end-to-end verification on more than one problem
- Specification certification is the hardest stage (≤1.86% success vs 84.5% compilation)
- Agentic approaches (COPRA) outperform few-shot prompting for implementation proofs (14 vs 3-9 problems)
- Generated specifications compile at 82.9-91.6% rates but certification rates are ≤1.86%
- Implementation compilation rates are higher (75.1-88.8%) but formal correctness remains difficult

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Non-computable specifications enforce genuine reasoning by blocking implementation leakage
- Mechanism: Specifications use inductive propositions (Prop) with quantifiers and logical connectives that cannot be directly evaluated. Models cannot copy spec logic into implementations because specs are declarative, not executable.
- Core assumption: Models rely on syntactic pattern-matching shortcuts that computable specs inadvertently reward
- Evidence anchors: Abstract states non-computable specs prevent leakage; Figure 3 shows GPT-4o copying computable spec but failing on non-computable spec; FV APPS allows trivial solutions with computable specs

### Mechanism 2
- Claim: Staged pipeline isolates failure modes across specification and implementation phases
- Mechanism: Four sequential checkpoints (spec generation → equivalence proof → implementation synthesis → correctness proof) each verified independently by Lean's type checker
- Core assumption: Specification correctness and implementation correctness are partially independent capabilities
- Evidence anchors: Abstract describes two-task structure; Table 1 shows 80%+ compilation but <2% certification reveals syntax ≠ semantic correctness; VERINA jointly generates all components while CLEVER separates for diagnosis

### Mechanism 3
- Claim: Ground-truth specification held-out during evaluation prevents reward hacking
- Mechanism: Generated implementation must satisfy hidden ground-truth spec ψ*, not model-generated spec ψ
- Core assumption: Without this constraint, models could generate trivially-satisfied specs and vacuously correct implementations
- Evidence anchors: Section 2 explicitly states evaluation uses ground-truth spec; Figure 8 shows FV APPS allowing trivial programs with incomplete specs

## Foundational Learning

- Concept: **Prop vs Decidable types in Lean**
  - Why needed here: Understanding why `Prop` cannot be computed is essential to grasp how non-computable specs prevent leakage
  - Quick check question: Can `fibonacci_non_computable n result` (an inductive Prop) be evaluated to true/false by computation alone?

- Concept: **Tactic-based proof search**
  - Why needed here: Benchmark evaluates LLM proof generation; understanding tactics like `simp`, `linarith`, `ring` helps diagnose proof failures
  - Quick check question: Why would `linarith` fail on recursive call structure but succeed on linear arithmetic constraints?

- Concept: **Specification equivalence (isomorphism)**
  - Why needed here: Task 1 requires proving semantic equivalence between generated and ground-truth specs, often harder than implementation correctness
  - Quick check question: If spec A says `result = x + y` and spec B says `result - x - y = 0`, what proof strategy establishes equivalence?

## Architecture Onboarding

- Component map: Natural Language (ν) → Generated Spec (ψ) → Ground Truth Spec (ψ*) ← HIDDEN → Implementation (π) → Lean Type Checker → QED or FAIL

- Critical path: All four stages must succeed; pass@k-seconds requires both QED1 (spec certification) AND QED2 (impl certification) within 600s timeout

- Design tradeoffs:
  - Non-computable specs: Stronger guarantees but harder to write (25 min/problem avg) and prove
  - Staged vs end-to-end: More diagnostic signal but higher cumulative failure rate
  - Few-shot vs agentic (COPRA): COPRA finds more impl proofs (14 vs 3-9) but requires more compute

- Failure signatures:
  - Spec compiles but proof fails: Model understood syntax but not semantics (common: 80%+ compile, <2% prove)
  - Spec equivalence fails: Abstract reasoning gap between NL and formal logic
  - Impl correctness fails: Control-flow reasoning breakdown (recursive cases, termination proofs)

- First 3 experiments:
  1. **Baseline diagnostic**: Run few-shot GPT-4o pipeline on 10 problems, record failure stage distribution
  2. **Isomorphism difficulty analysis**: Compare proof lengths for spec-isomorphism vs impl-correctness on successful cases
  3. **Ablate spec source**: Provide ground-truth spec directly to isolate implementation+proof capability from spec generation capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the benchmark's isomorphism proof mechanism be leveraged to bootstrap verified natural language and formal specification pairs for training new models?
- Basis in paper: Conclusion states verifying generated specs "introduces a novel opportunity: verified mining of natural language and formal specification pairs... which could be reused for bootstrapping new training data"
- Why unresolved: Paper establishes verification mechanism but doesn't implement feedback loop for data generation
- What evidence would resolve it: Experiment showing fine-tuning models on verified pairs mined from CLEVER improves specification generation performance

### Open Question 2
- Question: How can the "mismatch" between specification tractability and implementation difficulty be mitigated to achieve higher end-to-end verification rates?
- Basis in paper: Authors observe "tasks for which specification certification is tractable are often those where implementation correctness proofs are especially difficult, and vice versa"
- Why unresolved: Paper identifies inverse correlation as primary bottleneck but offers no solution to align these difficulties
- What evidence would resolve it: New prompting strategy or agent architecture that increases correlation between passing specification and implementation certification

### Open Question 3
- Question: Can neural theorem provers optimized for mathematical reasoning (e.g., MiniF2F solvers) adapt to the control-flow-heavy proofs required in CLEVER?
- Basis in paper: Discussion notes that unlike math benchmarks where tactics like `linarith` suffice, CLEVER proofs "mirror the control flow and branching structure of programs" where "standard automation is rarely sufficient"
- Why unresolved: Undetermined if current provers, which excel at flat mathematical logic, can handle structural complexity of program verification without adaptations
- What evidence would resolve it: Diagnostic evaluation of math-specialized provers on CLEVER, showing success rates on proofs requiring deep case analysis or recursion handling

## Limitations

- The benchmark's 161 problems represent limited coverage of the full problem space of verified code generation
- Evaluation relies heavily on proprietary LLM APIs whose internal reasoning capabilities remain opaque
- Even human experts struggle with the specification equivalence proofs, suggesting these tasks may exceed current model capabilities

## Confidence

- **High Confidence**: Benchmark design principles (non-computable specs, staged evaluation) are sound and well-justified; comparative results showing agentic approaches outperforming few-shot prompting are reproducible
- **Medium Confidence**: Claim that no approach succeeds on more than one problem is supported by data, but absolute numbers depend on specific prompt engineering and evaluation settings
- **Low Confidence**: Interpretation that non-computable specs definitively prevent implementation leakage assumes models won't develop abstract reasoning capabilities that bypass this constraint

## Next Checks

1. **Prompt Sensitivity Analysis**: Systematically vary few-shot exemplars and prompting strategies to establish baseline performance variance across the benchmark
2. **Human Expert Benchmark**: Have domain experts attempt a subset of specification equivalence proofs to validate that these tasks are genuinely challenging for humans as well
3. **Transfer Learning Test**: Evaluate models on specification generation without the equivalence proof requirement to isolate whether failures stem from spec generation or proof construction