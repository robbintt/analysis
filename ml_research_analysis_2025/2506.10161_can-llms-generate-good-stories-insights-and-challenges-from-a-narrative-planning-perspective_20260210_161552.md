---
ver: rpa2
title: Can LLMs Generate Good Stories? Insights and Challenges from a Narrative Planning
  Perspective
arxiv_id: '2506.10161'
source_url: https://arxiv.org/abs/2506.10161
tags:
- planning
- narrative
- llms
- story
- character
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks LLMs on narrative planning tasks by translating
  classic story domains (Secret Agent, Aladdin, Western) into parameterized variants
  and evaluating LLM-generated plans using an ASP solver for causal soundness, intentionality,
  and conflict detection. Experiments show that post-GPT-4 models (especially GPT-4o
  and o1) can generate causally sound stories at small scales, but struggle with maintaining
  character intentionality and generating conflicts involving multiple characters.
---

# Can LLMs Generate Good Stories? Insights and Challenges from a Narrative Planning Perspective

## Quick Facts
- arXiv ID: 2506.10161
- Source URL: https://arxiv.org/abs/2506.10161
- Authors: Yi Wang; Max Kreminski
- Reference count: 40
- Key outcome: Post-GPT-4 models (especially GPT-4o and o1) can generate causally sound stories at small scales, but struggle with maintaining character intentionality and generating conflicts involving multiple characters.

## Executive Summary
This paper benchmarks LLMs on narrative planning tasks by translating classic story domains into parameterized variants and evaluating LLM-generated plans using an ASP solver for causal soundness, intentionality, and conflict detection. Experiments show that reasoning models like o1 achieve up to 30% success on complex intentionality and conflict tasks, while simpler models fail completely. External calibration with ASP feedback improves performance but increases token and time costs significantly. Results suggest LLMs can handle basic narrative planning but face challenges with closed-world reasoning about character intentions and multi-character conflicts.

## Method Summary
The authors create three story domains (Secret Agent for causal planning, Aladdin for intentional planning, Western for conflict planning) encoded in Answer Set Programming using action language BC+. LLM-generated narrative plans are validated using an ASP solver (CLINGO 5.7.1) that checks for causal soundness, intentionality, and conflict. Two prompting modes are tested: one-off generation of complete plans and external calibration with iterative ASP feedback after each action. Models tested include GPT-3.5, GPT-4o, o1, o1-mini, Claude-3.5-Sonnet, and Claude-3.7-Sonnet across 30-50 attempts per task.

## Key Results
- GPT-4o achieves 0/30 success on Aladdin (intentional) and 3/30 on Western (conflict) tasks, while o1 achieves 14/30 and 20/30 respectively
- External calibration improves o1-mini performance on larger Secret Agent maps to match o1 despite o1-mini's weaker one-off results
- o1 successfully applies closed-world assumptions for causal planning but fails to maintain them for character intentionality
- Token and time costs increase 5-10x with external calibration, with complex tasks taking ~70-90 seconds

## Why This Works (Mechanism)

### Mechanism 1: ASP-Based Formal Validation of Narrative Constraints
- Claim: Answer Set Programming can automatically validate whether LLM-generated narrative plans satisfy causal, intentional, and conflict constraints.
- Mechanism: Narrative planning problems are translated into ASP programs using action language BC+, with candidate plans encoded as constraints. Plan validity reduces to ASP program satisfiability—unsatisfiable programs indicate constraint violations (broken causal links, unjustified intentions, or missing conflicts).
- Core assumption: Narratological quality criteria (causality, intentionality, conflict) can be captured as formal logical constraints amenable to automated verification.
- Evidence anchors:
  - [abstract]: "evaluating LLM-generated plans using an ASP solver for causal soundness, intentionality, and conflict detection"
  - [section III-C1]: "Narrative plan validation is accomplished by encoding the plan as constraints within the main ASP program of the story domain, along with the ASP encoding of the initial world state and narrative goal constraints. The causal soundness of the narrative plan translates to the satisfiability of this combined ASP program."
  - [corpus]: Related work applies ASP to planning [40] and narrative generation [41], but this paper provides the first systematic benchmark combining ASP validation with LLM narrative planning at scale.
- Break condition: If narrative quality aspects cannot be formalized declaratively (e.g., emotional resonance, thematic coherence), ASP validation cannot assess them.

### Mechanism 2: External Calibration via Iterative Feedback
- Claim: Providing real-time symbolic feedback on action validity improves LLM planning success rates, particularly for causally sound plans.
- Mechanism: LLM generates one action at a time → ASP solver simulates execution and checks preconditions → failure feedback (e.g., "Destination isn't connected to starting location") returned to LLM → LLM adjusts next action. This creates an LLM-modulo framework where symbolic verification guides generation.
- Core assumption: LLMs can incorporate structured error signals mid-generation to correct planning mistakes without full replanning.
- Evidence anchors:
  - [abstract]: "External calibration with ASP feedback improves performance but increases token and time costs significantly"
  - [section III-B]: "The LLM planner is paired with an ASP solver that simulates world state updates based on the narrative plan generated by the LLM, offering feedback on the causal soundness of the plan."
  - [section IV-B1]: External calibration improved o1-mini performance on larger Secret Agent maps, "overall matching o1's performance" despite o1-mini's weaker one-off results.
  - [corpus]: LLM-modulo frameworks are proposed for planning [38], but corpus lacks direct evidence for story-specific feedback loops.
- Break condition: If feedback introduces latency incompatible with real-time applications (the paper notes ~70-90 seconds for complex tasks), or if token costs become prohibitive (~12,000-15,000 tokens per attempt).

### Mechanism 3: Reasoning Model Advantage for Closed-World Constraint Tracking
- Claim: Models trained with reinforcement learning for extended reasoning (o1, Claude-3.7) substantially outperform standard LLMs on tasks requiring tracking of character intentions and multi-party conflicts under closed-world assumptions.
- Mechanism: Reasoning models generate implicit or explicit chain-of-thought traces that maintain state over longer horizons, enabling them to track commitment frames, causal link threats, and intention propagation across delegation networks—tasks requiring ~10,000+ tokens of computation.
- Core assumption: Complex narrative constraints require sustained multi-step reasoning that standard autoregressive prediction cannot reliably perform.
- Evidence anchors:
  - [abstract]: "Reasoning models like o1 achieve up to 30% success on complex intentionality and conflict tasks, while simpler models fail completely"
  - [section IV-A, Table I]: GPT-4o achieves 0/30 on Aladdin (intentional), o1 achieves 14/30; GPT-4o achieves 3/30 on Western (conflict), o1 achieves 20/30. GPT-3.5 fails all tasks.
  - [section V]: "o1 model was able to adopt a closed-world assumption for causal planning, rarely hallucinating nonexistent paths... [but] fails to adhere to this assumption for character intention."
  - [corpus]: Related work finds LLMs struggle with complex planning without augmentation [37], but corpus lacks comparative data specifically on reasoning vs. standard models for narrative tasks.
- Break condition: If closed-world assumptions conflict with LLMs' pre-trained commonsense priors (e.g., models introduce motivations not in the formal specification), reasoning alone may not suffice.

## Foundational Learning

- **Concept: Classical Planning (PDDL-style)**
  - Why needed: Narrative planning extends classical planning; understanding states, actions, preconditions, effects, and goal conditions is prerequisite.
  - Quick check question: Given initial state {at(agent, A), has-key(false)} and action move(loc1, loc2) with precondition connected(loc1, loc2) ∧ at(agent, loc1), what happens if the agent attempts move(A, B) when A and B are not connected?

- **Concept: Answer Set Programming Basics**
  - Why needed: The validation pipeline encodes narrative domains in ASP using action language BC+; interpreting results requires understanding satisfiability, stable models, and constraint encoding.
  - Quick check question: In ASP, what does the constraint `:- not goal_reached.` express, and how does it differ from the rule `goal_reached :- condition.`?

- **Concept: Commitment Frames and Intentional Planning**
  - Why needed: Intentionality evaluation requires understanding how actions group into frames where a character pursues a consistent goal with causal linkage between actions.
  - Quick check question: A character performs action a1 (motivating intention G), then a2, then a3 (achieving G). What three conditions must hold for this to be a valid commitment frame?

## Architecture Onboarding

- **Component map:**
  Instance Generator → [ASP Encoding + NL Prompt]
                              ↓
         ┌────────────→ LLM Planner ←────────────┐
         │                    │                  │
         │                    ↓                  │
         │           Candidate Plan              │
         │                    │                  │
         │                    ↓                  │
         │         Plan Validator (CLINGO) ──────┘
         │              ↙      ↓      ↘
    Causal?    Intentional?   Conflict?

- **Critical path:**
  1. Define domain: characters C, fluents V, actions A with preconditions/effects
  2. Generate instance: initial state S₀, goal state S_G, parameterized variations
  3. Prompt LLM with domain description + current state + action list
  4. LLM outputs action sequence (one-off) or single action (calibration mode)
  5. ASP validates: (a) causal soundness via precondition checking, (b) intentionality via commitment frame decomposition, (c) conflict via causal link threat detection
  6. If calibration mode: return feedback, repeat from step 3

- **Design tradeoffs:**
  - **One-off vs. calibration:** One-off is faster (~5-15 sec) but fails on complex instances; calibration improves success but costs 5-10x tokens/time
  - **Problem scale:** Larger maps/delegation networks better distinguish model capabilities but drop success rates sharply (e.g., o1 drops from ~70% at 6×6 to ~30% at 16×16 in Secret Agent)
  - **Model selection:** Reasoning models (o1, Claude-3.7) required for intentionality/conflict; standard models (GPT-4o) sufficient for small causal planning only

- **Failure signatures:**
  - **Causal failures:** Skipped steps (moving to destination without path), invalid action ordering (using key before obtaining it)
  - **Intentionality failures:** Actions without valid intention (Aladdin kills dragon then uses lamp with no established motivation), hallucinated intentions not in problem specification
  - **Conflict failures:** Misunderstanding nonexecuted actions as "narratively failed" vs. formally "would fail if attempted," reward hacking (characters "think about" taking medicine to bypass navigation)

- **First 3 experiments:**
  1. **Sanity check:** Run Secret Agent 4×4 grid with GPT-4o one-off prompting; expect ~50-70% success per Figure 3(a). Verify ASP validation pipeline correctly rejects invalid plans.
  2. **Calibration ablation:** Run Secret Agent 8×8 grid with GPT-4o, comparing one-off vs. external calibration. Quantify token/time cost increase vs. success rate improvement.
  3. **Intentionality stress test:** Run Aladdin domain with 1-1 delegation network (2 agents between king and Aladdin) using o1. Analyze failure cases for intention tracking errors vs. causal errors.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can automatic methods be developed to construct story domain specifications to improve the applicability of computational narratology for LLM-based story generation?
- Basis in paper: [explicit] The authors state: "The main weakness of symbolic planning lies in the extensive engineering required to construct story domain specifications; future work might therefore improve the applicability of computational narratology by developing automatic methods to construct these specifications."
- Why unresolved: Current symbolic narrative planning requires manual domain authoring, limiting scalability and practical adoption in game environments.
- What evidence would resolve it: Development and evaluation of systems that automatically generate valid ASP or PDDL domain encodings from natural language story descriptions.

### Open Question 2
- Question: Why do LLMs successfully apply closed-world reasoning for causal planning but fail to maintain closed-world assumptions for character intentionality?
- Basis in paper: [inferred] The paper notes that o1 "was able to adopt a closed-world assumption for causal planning, rarely hallucinating nonexistent paths" but "fails to adhere to this assumption for character intention," frequently introducing external motivations not established in the plan.
- Why unresolved: The authors speculate this may stem from training data lacking explicit intention specifications, but this hypothesis is untested.
- What evidence would resolve it: Experiments comparing LLM performance on intentionality tasks with and without fine-tuning on corpora explicitly annotating character intentions.

### Open Question 3
- Question: How can the homogeneity of LLM-generated narrative plans be mitigated to match the diverse plan generation capabilities of symbolic planners?
- Basis in paper: [explicit] The authors state that "Symbolic planners will also likely remain superior at diverse plan generation for the foreseeable future until the homogeneity of LLM output is mitigated."
- Why unresolved: LLMs produce similar plans across runs, limiting creative storytelling applications that require narrative variety.
- What evidence would resolve it: Benchmarking studies measuring plan diversity metrics across multiple LLM outputs compared to symbolic planner baselines, with interventions targeting diversity.

## Limitations

- The evaluation only assesses formal narratological criteria (causality, intentionality, conflict) and cannot evaluate higher-level story qualities like emotional resonance or thematic coherence.
- Token and time costs of external calibration (5-10x increase) raise scalability concerns for real-time applications.
- Success rates for intentionality and conflict tasks are based on smaller sample sizes (30 attempts) and binary metrics, making statistical significance unclear.

## Confidence

- **High confidence**: ASP validation methodology correctness, causal planning results (Secret Agent domain), token/time cost measurements
- **Medium confidence**: Intentionality and conflict task results (smaller sample sizes, binary metrics), model comparison claims for reasoning vs standard models
- **Low confidence**: Claims about model reasoning strategies (e.g., "o1 adopts closed-world assumptions"), extrapolation to story generation quality beyond formal constraints

## Next Checks

1. **Statistical significance analysis**: Run permutation tests on success rates between o1 and GPT-4o across all three domains to confirm reported performance gaps are not due to random variation.

2. **Qualitative error analysis**: Manually examine 10-20 failed plans from each domain to categorize error types beyond ASP validator outputs, particularly focusing on intentionality failures where models introduce external motivations.

3. **Cost-benefit scaling study**: Test external calibration on 4×4 and 6×6 Secret Agent variants to quantify the relationship between instance complexity, token/time costs, and success rate improvements, identifying the breakeven point where calibration becomes cost-prohibitive.