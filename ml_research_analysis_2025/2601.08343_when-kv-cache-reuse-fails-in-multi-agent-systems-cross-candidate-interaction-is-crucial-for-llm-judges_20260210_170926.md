---
ver: rpa2
title: 'When KV Cache Reuse Fails in Multi-Agent Systems: Cross-Candidate Interaction
  is Crucial for LLM Judges'
arxiv_id: '2601.08343'
source_url: https://arxiv.org/abs/2601.08343
tags:
- reuse
- judge
- agent
- candidate
- dogs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Multi-agent LLM systems use a judge to select the best response
  from multiple candidates. To reduce inference cost, KV cache reuse strategies effective
  for generation agents are applied to judge agents.
---

# When KV Cache Reuse Fails in Multi-Agent Systems: Cross-Candidate Interaction is Crucial for LLM Judges

## Quick Facts
- arXiv ID: 2601.08343
- Source URL: https://arxiv.org/abs/2601.08343
- Reference count: 40
- Multi-agent LLM judges experience selection inconsistency when using KV cache reuse, despite stable task accuracy

## Executive Summary
Multi-agent LLM systems use a judge to select the best response from multiple candidates, but applying KV cache reuse strategies effective for generation agents to judge agents can cause decision non-invariance. The study introduces Judge Consistency Rate (JCR) to quantify this instability and finds that reuse-based methods systematically weaken cross-candidate attention, especially for later candidate blocks. The results reveal that judge-centric inference requires dedicated, risk-aware system design to ensure selection stability, as task accuracy can remain stable while the judge's selection becomes highly inconsistent with dense prefill.

## Method Summary
The paper compares four methods for multi-candidate judge inference: Dense Prefill (recompute KV for all candidates jointly), Naïve Reuse (stitch execution-side KV caches with position remapping), KVCOMM (anchor-based offset correction with reuse gating), and PAL-KV (pooled anchor retrieval). Evaluation uses GSM8K, MMLU, and HumanEval benchmarks with Llama-3.2-3B-Instruct and ablations on Llama-3.1/3.2 and Qwen-2.5 (3B–14B). The pipeline generates N=4 candidates from execution agents, then judges them jointly to select the best and produce a final answer. Metrics include Task Accuracy (exact-match for GSM8K/MMLU, Pass@1 for HumanEval), Judge Consistency Rate (JCR), and Reuse Rate.

## Key Results
- KV cache reuse causes decision non-invariance: JCR drops to <30% while accuracy remains stable (~45-50%)
- Cross-candidate attention dilution: Reuse weakens attention to later candidate blocks, causing bias toward earlier candidates
- Layout sensitivity: Candidate-order perturbation (shuffle) amplifies inconsistency, sharply reducing JCR for all reuse-based methods

## Why This Works (Mechanism)

### Mechanism 1: Cross-Candidate Attention Dilution
KV cache reuse systematically weakens the model's attention to later candidate blocks during the judge phase. Approximate reuse methods stitch cached KV tensors together, failing to propagate the cross-attention dynamics required for joint comparison. The attention mechanism effectively "under-attends" to candidates positioned later in the sequence, treating them as less salient than they would be under dense prefill.

### Mechanism 2: Decision Non-Invariance via Context Drift
Reusing execution-side KV caches for the judge introduces decision variance even when end-task accuracy remains stable. Execution agents generate candidates in a specific context, but dense prefill recomputes the KV cache for each candidate in the presence of all competitors. Reuse bypasses this recomputation, using "context-isolated" KV from generation, which removes the causal influence of other candidates on the representation of each response.

### Mechanism 3: Layout Sensitivity and Prefix Instability
Anchor-based correction methods rely on retrieving offsets based on the current prefix. In a judge setting, the prefix for candidate i includes all preceding candidates 1...i-1. If candidate order is shuffled, the prefix changes combinatorially, making stored anchor offsets invalid or mismatched, leading to JCR degradation.

## Foundational Learning

- **KV Caching & Prefill vs. Decode**: The paper hinges on the difference between "dense prefill" (compute everything from scratch) and "reuse" (stitching existing blocks). Quick check: Why does the KV cache for a token generated in isolation differ from the KV cache for the same token generated in a joint context?

- **Joint Multi-Candidate Evaluation (vs. Pairwise)**: The mechanism of failure is specific to *joint* comparison (seeing all 4 agents at once). Quick check: In a joint comparison, does the representation of Agent 1's answer depend on Agent 2's answer? (Answer: Yes, this is the paper's core assertion).

- **Metric Decoupling (Accuracy vs. Consistency)**: Standard benchmarks optimize for Accuracy. This paper introduces JCR (Judge Consistency Rate). Quick check: If a judge picks the wrong agent but outputs the correct final answer, is the system "robust"? (Answer: No, according to JCR).

## Architecture Onboarding

- **Component map**: Execution Agents -> KV Cache Store -> Judge Assembler -> Judge Inference -> Selection Output
- **Critical path**: The Judge Assembler is the critical failure point. The logic here (how it handles position indices and retrieves offsets) determines if cross-candidate attention is preserved or destroyed.
- **Design tradeoffs**:
  - Naïve Reuse: Max speed, Max JCR drop (high risk)
  - Dense Prefill: Max stability, Min speed (costly)
  - KVCOMM/PAL-KV: Middle ground. They attempt to correct offsets but fail under layout shifts (shuffling)
- **Failure signatures**:
  - "Silent Flip": Task accuracy stays flat (~45-50%), but JCR drops to <30%
  - "Attention Collapse": Visualization shows attention mass concentrating exclusively on the first candidate block (Slot 1)
  - "Shuffle Fragility": A method that works on "No-Shuffle" orders fails immediately when candidates are permuted
- **First 3 experiments**:
  1. Reproduce the "Silent Flip": Run the judge on GSM8K with Naïve Reuse. Verify that Accuracy remains similar to dense prefill while JCR drops significantly
  2. Attention Visualization: Visualize the attention heatmaps of the first generated token for the judge. Confirm that Dense Prefill shows "vertical stripes" (attending to all slots) while Reuse shows a "single block" (attending only to slot 1)
  3. Shuffle Stress Test: Run the same evaluation with randomized candidate ordering. Observe if JCR drops further (indicating layout sensitivity)

## Open Questions the Paper Calls Out

- **Can interaction-preserving reuse methods, such as selective context retention via small-to-large model cooperation, effectively maintain decision invariance for LLM judges?** The paper identifies the failure mechanism but leaves the development of sophisticated interaction-aware objectives as future work.

- **Can meta-reasoning features derived from candidate sets reliably gate judge-side KV reuse to prevent decision non-invariance?** The paper shows a preliminary classifier can detect safe instances (AUC 0.82), but a practical system that prioritizes conservative safety detection over strategy selection is not yet developed.

- **Does the decision non-invariance observed in small-to-mid scale models (3B–14B) transfer to significantly larger LLMs or heterogeneous agent-judge setups?** The Limitations section notes experiments were restricted to models up to 14B and did not systematically evaluate cross-model KV reuse.

## Limitations
- Execution-side KV cache heterogeneity: Real multi-agent systems often involve coordination or shared reasoning steps, which may affect the judge's ability to make fair comparisons
- Metric granularity limitations: JCR treats all candidate swaps equally, potentially missing subtler patterns of decision drift
- Anchor pool construction details: The paper doesn't specify how anchors are selected or whether anchor quality affects performance

## Confidence

**High Confidence (8-10/10):**
- The core finding that KV cache reuse disrupts cross-candidate attention in judge agents
- The observation that dense prefill maintains better selection consistency than reuse methods
- The demonstration that task accuracy can remain stable while selection behavior becomes inconsistent

**Medium Confidence (5-7/10):**
- The specific mechanism of "attention collapse" onto early candidate blocks
- The quantitative thresholds for what constitutes problematic JCR drops
- The generalizability of findings across different multi-agent architectures

**Low Confidence (1-4/10):**
- The relative ranking of KVCOMM vs. PAL-KV effectiveness (implementation details matter significantly)
- The optimal anchor pool size for KVCOMM across different model scales
- The exact boundary conditions where reuse becomes acceptable vs. unacceptable

## Next Checks

1. **Anchor quality ablation study**: Systematically vary the number and selection criteria for anchors in KVCOMM to determine if performance degradation stems from reuse mechanism itself or suboptimal anchor selection.

2. **Progressive candidate addition experiment**: Instead of comparing all 4 candidates simultaneously, test judge performance when candidates are added incrementally (1→2→3→4). This would reveal whether the attention dilution is cumulative or occurs immediately upon reuse.

3. **Mixed-reuse hybrid evaluation**: Implement a hybrid approach where only the top-k candidates (by execution-side confidence) undergo dense prefill, while others use reuse. Measure the tradeoff between inference cost and JCR to establish practical deployment thresholds.