---
ver: rpa2
title: 'From Human Intention to Action Prediction: A Comprehensive Benchmark for Intention-driven
  End-to-End Autonomous Driving'
arxiv_id: '2512.12302'
source_url: https://arxiv.org/abs/2512.12302
tags:
- driving
- arxiv
- human
- autonomous
- intention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of enabling autonomous vehicles
  to interpret and fulfill high-level human intentions rather than just following
  low-level commands. It introduces Intention-Drive, a new benchmark featuring a large-scale
  dataset with complex natural language intentions paired with sensor data, and a
  novel Imagined Future Alignment (IFA) evaluation protocol using generative world
  models to assess semantic goal fulfillment.
---

# From Human Intention to Action Prediction: A Comprehensive Benchmark for Intention-driven End-to-End Autonomous Driving

## Quick Facts
- **arXiv ID:** 2512.12302
- **Source URL:** https://arxiv.org/abs/2512.12302
- **Reference count:** 21
- **Primary result:** Introduces Intention-Drive benchmark with novel IFA evaluation; achieves 35.6% IFA with proposed frameworks versus 9.5% for baselines

## Executive Summary
This paper addresses the challenge of enabling autonomous vehicles to interpret and fulfill high-level human intentions rather than merely following low-level commands. The authors introduce Intention-Drive, a new benchmark featuring a large-scale dataset with complex natural language intentions paired with sensor data, and a novel Imagined Future Alignment (IFA) evaluation protocol using generative world models to assess semantic goal fulfillment. Two methodological frameworks are explored: an end-to-end vision-language planner and a hierarchical agent-based system. Experiments reveal that while existing models achieve good driving stability, they struggle significantly with intention fulfillment; the proposed frameworks show substantially improved alignment with human intentions.

## Method Summary
The approach centers on Intention-Drive, a dataset built on OpenScene containing 18,765 training and 1,959 evaluation scenes with multi-view camera images and hierarchical annotations (atomic instructions + high-level intentions) generated via GPT-5.2 LVLM. The core architecture uses InternVL3.0-2B (visual encoder + LLM projector + Qwen2.5 LLM) initialized with ReCogDrive weights, fine-tuned with LoRA to output trajectory coordinates as text tokens. The IFA evaluation protocol converts predicted trajectories into hallucinated future videos using a generative world model, then scores semantic fulfillment via a VLM judge. Two frameworks are proposed: an end-to-end planner that directly maps vision-language tokens to trajectories, and a hierarchical agent-based system that decomposes intentions into discrete commands.

## Key Results
- End-to-end framework achieves IFA score of 35.6% versus 9.5% for baselines, demonstrating improved semantic alignment
- Agent-based framework shows better driving stability (lower ADE, collision rates) but struggles with intention fulfillment due to semantic information loss
- Performance varies significantly by intention type, with abstract goals ("Parked Next to Building") showing 3.5× improvement over basic navigational commands
- Vision-language models (GPT-5.2, InternVL) achieve 20-50% higher intention fulfillment rates than end-to-end models in intention reasoning evaluation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Directly regressing trajectories from vision-language tokens preserves semantic nuance better than decomposing intent into discrete navigational commands
- **Mechanism:** The End-to-End framework uses InternVL3.0 to map visual observations and natural language instructions directly into trajectory coordinates via autoregressive token generation, maintaining rich semantic context throughout decoding
- **Core assumption:** Pre-trained visual-linguistic features can be precisely aligned with geometric trajectory spaces without losing high-level reasoning capabilities
- **Evidence anchors:**
  - [abstract] "We explore the solution space by proposing two distinct methodological paradigms: an end-to-end vision-language planner... [achieving] substantially improved alignment"
  - [page 6] "Instead of regressing numerical values directly, the model predicts the future trajectory in a textual format... factorized into a product of conditional probabilities"
  - [page 7] "This result empirically validates our hypothesis that a unified, intention-driven paradigm is essential... the end-to-end framework establishes a direct mapping... preserving the rich semantic information"
- **Break condition:** If the output trajectory requires complex, multi-step temporal reasoning that exceeds the single-step context window of the auto-regressive decoder

### Mechanism 2
- **Claim:** Generative world models can verify semantic goal fulfillment in cases where geometric overlap with a single ground truth fails to capture valid alternative behaviors
- **Mechanism:** The IFA protocol converts predicted numerical trajectories back into visual "hallucinated" future videos using a pre-trained world model, then uses a VLM as semantic judge to score against original intent, decoupling success from exact path matching
- **Core assumption:** The generative world model is sufficiently high-fidelity and controllable to accurately visualize consequences of input trajectory physics
- **Evidence anchors:**
  - [abstract] "A novel Imagined Future Alignment (IFA) evaluation protocol... assesses the semantic fulfillment of the human's goal beyond simple geometric accuracy"
  - [page 5] "Instead of evaluating abstract trajectory coordinates, we map the agent's plan back into the visual domain... [using] a pre-trained generative world model"
  - [corpus] Related work in Sce2DriveX suggests translating semantic understanding to drive learning is a key challenge; IFA operationalizes this verification
- **Break condition:** If the world model introduces visual artifacts or hallucinations inconsistent with trajectory physics

### Mechanism 3
- **Claim:** Hierarchical LVLM-based data construction creates scalable, high-quality training pairs for complex, abstract driving instructions
- **Mechanism:** The paper uses GPT-5.2 to synthesize training data by first generating low-level "atomic instructions" from video to ensure grounding, then conditioning on these to generate high-level "human intentions," bootstrapping the Intention-Drive dataset
- **Core assumption:** Synthetic natural language intentions generated by the LVLM sufficiently approximate the distribution and ambiguity of real human passenger commands
- **Evidence anchors:**
  - [page 4] "We devised a hierarchical annotation strategy powered by an advanced Large Vision-Language Model (LVLM)... synthesizing two distinct levels of linguistic instructions"
  - [page 13] "Figure 5... highlights the shift from command-following to intention-fulfillment [via] behavioral and interactive aspects"
- **Break condition:** If synthetic data lacks linguistic diversity or "noise" of real human speech, the trained model may fail to generalize to actual user commands

## Foundational Learning

- **Concept: Vision-Language-Action (VLA) Models**
  - **Why needed here:** The core architecture (InternVL3.0 + Qwen2.5) is a VLA. You must understand how these models tokenize images and text into a shared embedding space to reason about them jointly
  - **Quick check question:** Can you explain how a pixel-unshuffle operation helps project visual features into the LLM's input space?

- **Concept: Generative World Models**
  - **Why needed here:** The IFA evaluation protocol relies entirely on the ability to "hallucinate" future video frames conditioned on action. Understanding the controllability of models like Vista is crucial for implementing the IFA metric
  - **Quick check question:** How does conditioning a world model on a future trajectory differ from unconditional future video prediction?

- **Concept: Semantic vs. Geometric Evaluation**
  - **Why needed here:** The paper argues that ADE is insufficient for intention-driven tasks. Understanding why geometric similarity ≠ semantic success is central to the paper's contribution
  - **Quick check question:** If a car parks in a valid spot 5 meters away from the "ground truth" spot but still fulfills the intention "pull over here," should the score be high or low, and which metric (ADE vs. IFA) would capture that?

## Architecture Onboarding

- **Component map:** Multi-view video sequences + Natural Language Intent → InternVL3.0-2B (Visual Encoder + LLM Projector + Qwen2.5 LLM) → Autoregressive token decoder → Coordinate tuples (x, y) → Trajectory → IFA evaluator (World Model → Hallucinated Video → VLM Judge → Scores)

- **Critical path:** The alignment between the visual encoder and the LLM backbone. If visual tokens do not provide sufficient spatial fidelity, the LLM cannot ground the intention

- **Design tradeoffs:**
  - **End-to-End vs. Hierarchical:** E2E offers better semantic alignment (35.6% IFA) but may sacrifice kinematic stability; Agent-based offers stability by reusing certified E2E models but suffers from semantic loss when compressing complex intents into simple commands
  - **Tokenization:** The paper outputs text-based coordinates rather than regression heads, leveraging the LLM's reasoning but requiring robust parsing to handle format errors

- **Failure signatures:**
  - **Spatial Drift:** High ADE at 3s time horizons, indicating the model struggles with long-term physical constraints
  - **Semantic Hallucination:** The model generates a trajectory that looks correct linguistically but is kinematically infeasible, which the IFA should catch but pure LLMs frequently produce
  - **Object Blindness:** Performance drops when intentions reference small static objects like "Traffic Signs" vs. large dynamic objects like "Vehicles"

- **First 3 experiments:**
  1. **Baseline Validation:** Reproduce the "InternVL3.0-2B" baseline on geometric metrics (ADE/Collision) using standard nuScenes/OpenScene splits to ensure the driving backbone is stable before adding intention logic
  2. **IFA Stress Test:** Evaluate the IFA protocol on adversarial cases: generate a trajectory that violates the intention but has low ADE to verify the Semantic Judge catches the failure
  3. **Ablation on Input Context:** Test performance (IFA score) vs. video history length. The paper suggests longer video sequences help; verify if this is due to kinematic stabilization or better scene context

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the "critical dichotomy" between high geometric driving stability and low semantic intention fulfillment be resolved in end-to-end autonomous driving models?
- **Basis in paper:** [explicit] The abstract and conclusion highlight that while baseline models achieve satisfactory stability (low ADE), they struggle significantly with intention fulfillment (low IFA), creating a performance deficit
- **Why unresolved:** The paper demonstrates this gap exists but does not fully resolve it, achieving only 35.6% IFA with the best proposed framework
- **What evidence would resolve it:** A model architecture that jointly optimizes for trajectory accuracy and semantic reasoning, demonstrating high scores in both geometric metrics and IFA simultaneously

### Open Question 2
- **Question:** How can the "information bottleneck" inherent in hierarchical agent-based frameworks be alleviated to prevent the loss of semantic nuance?
- **Basis in paper:** [explicit] Section 5.2 explicitly identifies that agent-based frameworks plateau in performance because compressing abstract intentions into discrete steering commands results in a "significant loss of semantic nuance"
- **Why unresolved:** The hierarchical decomposition isolates the reasoner from the execution, limiting the flow of detailed semantic information required for complex goals
- **What evidence would resolve it:** A hierarchical architecture that utilizes a continuous or richer intermediate representation instead of discrete commands, resulting in IFA scores comparable to or exceeding end-to-end methods

### Open Question 3
- **Question:** What specific architectural improvements are required to enhance the grounding of abstract linguistic instructions to small-scale, static visual cues?
- **Basis in paper:** [inferred] Section 5.6 notes a distinct performance drop for "Traffic Sign" and "Traffic Light" intentions compared to dynamic agents, suggesting current grounding mechanisms for stationary visual cues are insufficient
- **Why unresolved:** The paper identifies this sensitivity as a limitation of current grounding techniques but does not propose a specific solution for fine-grained visual reasoning
- **What evidence would resolve it:** A model utilizing high-resolution attention mechanisms or specialized visual encoders that improves ADE and IFA specifically for scenarios involving static regulatory elements

## Limitations
- Synthetic data generation process may not fully capture the ambiguity and diversity of real human commands, creating potential distribution shift
- IFA evaluation protocol depends heavily on the fidelity of the generative world model and VLM evaluator, introducing potential sources of error or bias
- Performance varies significantly by intention type, with abstract goals showing much better results than basic navigational commands

## Confidence

**High Confidence Claims:**
- Intention-Drive dataset structure and benchmark framework are technically sound and well-documented
- IFA evaluation protocol represents a meaningful advancement over purely geometric metrics
- 35.6% IFA score demonstrates measurable improvement over baselines

**Medium Confidence Claims:**
- Specific architectural improvements will generalize to other driving datasets
- Synthetic data generation process produces sufficiently diverse and realistic intentions
- World model + VLM judge combination provides reliable semantic evaluation

**Low Confidence Claims:**
- Exact numerical improvements will hold when deployed on real human passengers
- Framework generalizes to driving scenarios beyond OpenScene dataset
- Computational overhead of IFA protocol is acceptable for real-time deployment

## Next Checks

1. **Real-World Command Validation:** Test the trained model on a small dataset of actual human passenger commands (not synthetic) to measure real-world performance drop and identify specific linguistic patterns that cause failure

2. **IFA Protocol Robustness:** Conduct adversarial testing by generating trajectories that violate intentions but achieve low ADE, and vice versa, to quantify the false positive/negative rate of the semantic judge

3. **Cross-Dataset Generalization:** Fine-tune the trained model on a different driving dataset (e.g., nuScenes or Waymore) without retraining from scratch to evaluate architectural generalization and identify dataset-specific failure modes