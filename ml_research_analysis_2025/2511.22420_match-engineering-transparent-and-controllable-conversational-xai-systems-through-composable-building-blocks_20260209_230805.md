---
ver: rpa2
title: 'MATCH: Engineering Transparent and Controllable Conversational XAI Systems
  through Composable Building Blocks'
arxiv_id: '2511.22420'
source_url: https://arxiv.org/abs/2511.22420
tags:
- blocks
- building
- systems
- https
- interactive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MATCH, a framework for engineering transparent
  and controllable conversational XAI systems through composable building blocks.
  The authors address the challenge that while XAI techniques can explain individual
  AI models, they often fail to explain the broader interactive systems in which these
  models operate.
---

# MATCH: Engineering Transparent and Controllable Conversational XAI Systems through Composable Building Blocks

## Quick Facts
- arXiv ID: 2511.22420
- Source URL: https://arxiv.org/abs/2511.22420
- Authors: Sebe Vanbrabant; Gustavo Rovelo Ruiz; Davy Vanacken
- Reference count: 40
- One-line primary result: Introduces MATCH framework for engineering transparent and controllable conversational XAI systems through composable building blocks.

## Executive Summary
MATCH addresses the challenge that existing XAI techniques explain individual AI models but fail to explain broader interactive systems where these models operate. The framework represents interactive systems as sequences of structural building blocks (AI models, control mechanisms) that can be explained through complementary explanatory building blocks (established XAI techniques like LIME and SHAP). Through a 5-layer architecture, MATCH makes AI workflows accessible and controllable for both human and automated agents via a generated API, enabling system auditing critical for responsible AI engineering.

## Method Summary
The MATCH framework implements a 5-layer architecture using Python with Flask REST API. Building blocks are defined via decorators (@bb_predict, @bb_transform, @bb_update, CRUD decorators), chained via pipe operators (Sequential) or ParallelBlock. The system auto-generates REST endpoints from decorated methods, enabling both human users and LLM agents to interact with the same knowledge base. XAI methods including LIME, SHAP, DiCE, prototypes/criticisms are integrated as explanatory blocks that query structural blocks through the generated API. The framework is demonstrated on a loan prediction dataset using an ensemble of models with control blocks like DivineRuleGuard and NonGoalFilter.

## Key Results
- Introduces MATCH framework for engineering transparent conversational XAI systems
- Provides 5-layer architecture that makes AI workflows accessible to both human and automated agents
- Demonstrates composable building blocks approach using loan prediction ensemble with LIME/SHAP explanations
- Enables system-level explanations beyond individual model explanations through API generation
- Catalog of structural and explanatory building blocks grounded in literature

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Exposing AI workflows as composable building blocks may enable both human and automated agents to reason about system behavior through a shared knowledge base.
- Mechanism: Developer-defined functions are wrapped into `BuildingBlock` objects with attached metadata (CRUD decorators, `@bb_predict`, `@bb_transform`), then chained via pipe operators into a `Runnable` structure that auto-generates a REST API.
- Core assumption: Agents (human or LLM-based) can effectively query and interpret structured APIs to form accurate mental models of system behavior.
- Evidence anchors:
  - [abstract] "The flow and APIs of the structural building blocks form an unambiguous overview of the underlying system, serving as a communication basis for both human and automated agents."
  - [section 3.1] Layer 2 "transforms each block's functions into an API that conveys their scope and purpose to the front-end UI and automated agents."
  - [corpus] Related work (ECHO, QueryGenie) suggests conversational XAI can increase user engagement, but evidence for shared human-machine knowledge bases remains limited.
- Break condition: If the API surface becomes too large or poorly documented, agents may struggle to navigate it effectively, reducing interpretability gains.

### Mechanism 2
- Claim: Attaching explanatory building blocks (e.g., LIME, SHAP, DiCE) to structural components may provide multi-level explanations that scale from individual models to entire pipelines.
- Mechanism: Explanatory blocks query structural blocks through the generated API, applying post-hoc methods to any `Runnable` (including ensembles or full chains), enabling system-level rather than just model-level explanations.
- Core assumption: Post-hoc explanation methods retain validity when applied to composite pipelines, not just individual models.
- Evidence anchors:
  - [abstract] "The structural building blocks can then be explained through complementary explanatory building blocks, such as established XAI techniques like LIME and SHAP."
  - [section 4.3] "With MATCH, these explanatory building blocks can also be applied at the system level (which is itself also a predictive Runnable)."
  - [corpus] No direct corpus evidence on applying LIME/SHAP to composite pipelines; this remains an assumption requiring validation.
- Break condition: If pipeline components introduce non-differentiable or highly interdependent transformations, post-hoc explanation fidelity may degrade.

### Mechanism 3
- Claim: Control-oriented structural blocks (e.g., `DivineRuleGuard`, `NonGoalFilter`) may allow human stakeholders to override or constrain AI behavior without modifying core models.
- Mechanism: Control blocks intercept data flow at pipeline stages, applying developer- or user-defined rules before or after model predictions, with CRUD-enabled interfaces for runtime modification.
- Core assumption: Users can formulate effective rules that improve outcomes without introducing unintended side effects.
- Evidence anchors:
  - [section 4.2] "DivineRuleGuard enforces ethical compliance by overriding harmful or unethical model outputs... users can interact with this block through a visual rule editor."
  - [section 4.2] Control blocks "represent mechanisms to calibrate the equilibrium between automated decisions and human oversight."
  - [corpus] No corpus papers evaluate user-defined override rules in production systems; evidence is theoretical.
- Break condition: If rules conflict or are poorly specified, they may produce inconsistent behavior that erodes trust.

## Foundational Learning

- Concept: **Pipeline-based AI workflows** (e.g., scikit-learn pipelines, LangChain chains)
  - Why needed here: MATCH extends pipeline paradigms by adding auditable APIs and explanatory/control blocks. Understanding chaining and composition is essential.
  - Quick check question: Can you explain how data flows through a scikit-learn `Pipeline` or LangChain `Chain`?

- Concept: **Post-hoc XAI methods** (LIME, SHAP, DiCE)
  - Why needed here: Explanatory building blocks wrap these methods. You need to know their inputs, outputs, and limitations to use them effectively.
  - Quick check question: What does SHAP return for a tabular prediction, and what are its assumptions about feature independence?

- Concept: **REST API fundamentals and CRUD operations**
  - Why needed here: MATCH auto-generates REST endpoints from decorated methods. Understanding HTTP methods and API design helps debug and extend the framework.
  - Quick check question: Which HTTP method would you use to update a resource, and how does that map to `@bb_update`?

## Architecture Onboarding

- Component map:
  - BuildingBlock objects (structural) -> API generation (Layer 2) -> Explanatory blocks (Layer 3) -> UI assembly (Layer 4) -> Agent interaction (Layer 5)

- Critical path:
  1. Identify functions in your existing codebase to expose (e.g., `predict`, `train`, `transform`).
  2. Wrap them in `BuildingBlock` objects with appropriate names and method lists.
  3. Chain blocks using `|` (sequential) or `ParallelBlock` (parallel) to represent your pipeline.
  4. Decorate methods with `@bb_predict`, `@bb_transform`, `@bb_create`, `@bb_read`, `@bb_update`, `@bb_delete`.
  5. Instantiate the `API` class with your chain; it auto-generates endpoints.
  6. Add explanatory blocks (LIME, SHAP, etc.) that query the chain via the API.
  7. Connect a front-end UI and/or LLM agent (e.g., ECHO) to the API.

- Design tradeoffs:
  - **Granularity vs. complexity:** Finer-grained blocks provide more control points but increase API surface and cognitive load.
  - **Automation vs. oversight:** Auto-propagation of updates (e.g., retraining on dataset changes) reduces manual work but may trigger unintended side effects.
  - **Generality vs. specificity:** The framework is data-type agnostic but currently illustrated on tabular data; extending to images or text may require new explanatory blocks.

- Failure signatures:
  - **API explosion:** Too many exposed methods make the knowledge base harder for agents to navigate.
  - **Stale explanations:** If explanatory blocks cache results without invalidation, they may reflect outdated model states.
  - **Rule conflicts:** Overlapping or contradictory control rules may produce unpredictable behavior.
  - **Type mismatches:** Poorly typed method outputs may break auto-generated serialization or front-end rendering.

- First 3 experiments:
  1. **Minimal chain onboarding:** Take an existing scikit-learn model (e.g., loan prediction classifier), wrap it in a single `BuildingBlock`, expose `predict` via `@bb_predict`, and verify the auto-generated API returns predictions correctly.
  2. **Add a LIME explanatory block:** Attach a LIME explanation block to your model's `BuildingBlock`, query it via the API, and confirm feature importances are returned and visualizable in a simple UI.
  3. **Insert a control block:** Add a `DivineRuleGuard` that overrides predictions for a specific input condition (e.g., income > threshold), and verify the override applies correctly through the API.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the MATCH framework demonstrably improve end-user comprehension and control of AI systems compared to standard conversational or GUI-based XAI baselines?
- **Basis in paper:** [inferred] The paper presents an engineering framework and architecture (C1, C2, C3) but does not include a user study or empirical evaluation of its efficacy in enhancing transparency or controllability for the target audience (human agents).
- **Why unresolved:** The contributions are conceptual and technical; claims regarding the alignment of human and machine interpretability remain theoretical without validation through user experimentation.
- **What evidence would resolve it:** A controlled user study measuring task completion time, accuracy of mental models, and subjective trust ratings when using MATCH versus a standard XAI dashboard.

### Open Question 2
- **Question:** How effectively does the building block approach extend to non-tabular data types, such as image or text processing pipelines?
- **Basis in paper:** [explicit] Section 4 states, "Although we set our current focus to tabular data, MATCHâ€™s modular approach can be extended to other data types as well."
- **Why unresolved:** The paper only demonstrates the framework using a loan approval prediction dataset (tabular). It is unclear if the current catalog of explanatory building blocks (e.g., LIME views) and structural components generalizes without significant modification.
- **What evidence would resolve it:** A case study applying MATCH to a computer vision or natural language processing workflow, identifying necessary modifications to the explanatory blocks.

### Open Question 3
- **Question:** Can LLM-generated layouts successfully replace static UIs to dynamically adapt the presentation of building blocks to user preferences?
- **Basis in paper:** [explicit] Section 3.1 notes that "future research directions include LLM-generated layouts that dynamically adapt to user preferences."
- **Why unresolved:** The current implementation assembles explanatory building blocks into a static UI. The feasibility and utility of an LLM dynamically generating these layouts in real-time are unexplored.
- **What evidence would resolve it:** A prototype implementation where an LLM constructs the interface based on user interaction history or stated goals, evaluated for coherence and usability.

## Limitations

- Framework is demonstrated only on tabular loan prediction dataset without empirical evaluation of explanation quality or user performance
- Assumes post-hoc XAI methods maintain validity when applied to composite pipelines rather than individual models
- Control mechanisms like DivineRuleGuard lack validation for real-world effectiveness and potential rule conflicts

## Confidence

- **Confidence in core architectural contribution:** High - The 5-layer design and composable building block approach is well-specified with clear implementation patterns using decorators and API generation
- **Confidence in explanation mechanisms:** Medium - While the framework correctly identifies LIME, SHAP, and DiCE as established XAI techniques, applying these to entire pipelines versus individual models remains untested
- **Confidence in control mechanisms:** Low - The DivineRuleGuard and control blocks are described theoretically, but no evidence demonstrates their practical effectiveness or potential for rule conflicts

## Next Checks

1. **Pipeline-level explanation fidelity**: Apply LIME/SHAP to a composite pipeline (not just individual models) and compare explanation consistency against ground truth feature importance across pipeline stages.

2. **Control rule conflict detection**: Implement a suite of user-defined rules through DivineRuleGuard and systematically test for conflicts, contradictions, and unintended behavioral cascades.

3. **Agent navigation efficiency**: Measure how effectively both human users and LLM agents can locate and utilize relevant API endpoints in MATCH-generated interfaces across varying levels of system complexity.