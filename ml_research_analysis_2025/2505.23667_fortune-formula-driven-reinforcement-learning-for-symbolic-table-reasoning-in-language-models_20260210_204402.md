---
ver: rpa2
title: 'Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning
  in Language Models'
arxiv_id: '2505.23667'
source_url: https://arxiv.org/abs/2505.23667
tags:
- reasoning
- table
- formula
- symbolic
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Formula Tuning (Fortune) is a reinforcement learning framework
  that trains language models to generate executable spreadsheet formulas for symbolic
  table reasoning. By using answer correctness as a reward signal, it reduces reliance
  on supervised formula annotations and guides models to derive formulas through reasoning.
---

# Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models

## Quick Facts
- **arXiv ID**: 2505.23667
- **Source URL**: https://arxiv.org/abs/2505.23667
- **Reference count**: 40
- **Primary Result**: A 7B model outperforms OpenAI o1 (68.48% vs. 66.90%) on table understanding tasks using formula-driven RL

## Executive Summary
This paper introduces Fortune, a reinforcement learning framework that trains language models to generate executable spreadsheet formulas for symbolic table reasoning. The key innovation is using answer correctness as a reward signal to guide formula generation, reducing dependence on supervised formula annotations. The approach enables a 7B parameter model to outperform much larger models on table understanding benchmarks through formula-driven reasoning.

## Method Summary
Fortune employs a novel reinforcement learning approach where language models are trained to generate spreadsheet formulas that can be executed to answer questions about tabular data. The framework uses the correctness of the final answer as a reward signal, allowing models to learn formula generation through trial and error rather than requiring extensive supervised training data with formula annotations. This formula-driven approach bridges the gap between natural language understanding and symbolic reasoning in table comprehension tasks.

## Key Results
- A 7B model achieves 68.48% accuracy, outperforming OpenAI o1 at 66.90% on table understanding benchmarks
- Substantial improvements across seven different benchmarks, particularly on multi-step numerical and symbolic reasoning tasks
- Demonstrates effectiveness of formula-driven RL in advancing symbolic table reasoning capabilities

## Why This Works (Mechanism)
The approach works by converting table reasoning into a sequence of formula generation steps, where each formula represents a logical operation that can be executed on the table data. The reinforcement learning framework provides feedback based on final answer correctness, allowing the model to discover effective reasoning strategies through trial and error. This creates a closed-loop system where the model learns to generate formulas that lead to correct answers, effectively learning symbolic reasoning through experience rather than explicit supervision.

## Foundational Learning
- **Spreadsheet Formula Generation**: Understanding how to convert natural language questions into executable formulas is crucial for this approach. Quick check: Can the model generate correct formulas for simple arithmetic operations?
- **Reinforcement Learning with Sparse Rewards**: The framework uses answer correctness as the primary reward signal, requiring sophisticated RL techniques to handle delayed and sparse feedback. Quick check: Does the reward shaping effectively guide learning?
- **Symbolic Table Reasoning**: The ability to perform multi-step reasoning over tabular data using symbolic operations rather than just pattern matching. Quick check: Can the model handle chained reasoning operations?
- **Natural Language to Formula Translation**: Converting natural language questions into formal formula representations requires understanding both the question semantics and table structure. Quick check: How well does the model handle ambiguous or complex queries?
- **Executable Formula Evaluation**: The framework requires a system to execute generated formulas and evaluate their correctness against ground truth answers. Quick check: Is the execution environment robust to formula errors?
- **Multi-step Reasoning Decomposition**: Breaking down complex questions into sequences of simpler formula operations. Quick check: Can the model identify the optimal decomposition strategy?

## Architecture Onboarding

**Component Map**: Natural Language Question -> Formula Generator -> Formula Executor -> Answer Evaluator -> Reward Signal -> RL Optimizer

**Critical Path**: The most critical components are the formula generator (which must produce executable formulas) and the answer evaluator (which provides the reward signal). These components must work in tight coordination to ensure effective learning.

**Design Tradeoffs**: The approach trades the complexity of generating executable formulas for the ability to use answer correctness as a direct reward signal. This requires a robust formula execution environment but enables more flexible learning compared to supervised approaches.

**Failure Signatures**: Common failure modes include incorrect formula generation leading to execution errors, inability to handle complex multi-step reasoning, and sensitivity to table structure variations. The model may also struggle with questions requiring external knowledge beyond the table.

**Three First Experiments**:
1. Test formula generation on simple arithmetic operations across various table structures to establish baseline capability
2. Evaluate performance on progressively more complex multi-step reasoning tasks to identify reasoning depth limitations
3. Assess robustness by introducing variations in table formatting and question phrasing to test generalization

## Open Questions the Paper Calls Out
The paper highlights several areas for future investigation, including the potential application of formula-driven RL to other domains beyond table reasoning, the scalability of the approach to larger models and more complex reasoning tasks, and the integration of external knowledge sources to enhance reasoning capabilities.

## Limitations
- The comparison with OpenAI o1 needs verification of exact evaluation conditions and potential confounding factors
- The approach's dependence on spreadsheet formula generation may limit applicability to domains where such formulas are not naturally available
- The reliance on answer correctness as a reward signal may face challenges with noisy or ambiguous table reasoning tasks

## Confidence

**High Confidence**: The effectiveness of formula-driven reinforcement learning for symbolic table reasoning is well-supported by extensive experiments across seven benchmarks, demonstrating substantial improvements particularly on multi-step numerical and symbolic reasoning tasks.

**Medium Confidence**: The claim about reducing reliance on supervised formula annotations is plausible but requires closer examination of the training data composition and annotation requirements.

**Medium Confidence**: The comparison with OpenAI o1 needs verification of exact evaluation conditions and potential confounding factors.

## Next Checks

1. Conduct ablation studies to isolate the impact of formula-driven RL versus other architectural choices, and test model performance across diverse table types and domains not represented in the original benchmarks.

2. Implement a detailed error analysis categorizing failures by reasoning type (e.g., numerical vs. symbolic) and complexity level to identify systematic weaknesses and opportunities for improvement.

3. Evaluate model performance on tables requiring external knowledge or real-world context beyond the immediate tabular data, and test robustness against adversarial or noisy table inputs.