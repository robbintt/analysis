---
ver: rpa2
title: Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud
  GPUs
arxiv_id: '2509.11480'
source_url: https://arxiv.org/abs/2509.11480
tags:
- throughput
- power
- datacenter
- across
- edge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates Vision-Language-Action (VLA) models across
  edge and cloud GPUs, focusing on latency, throughput, and memory usage under different
  power constraints. Five VLA models are tested, including newly proposed architectures,
  using the LIBERO benchmark.
---

# Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs

## Quick Facts
- arXiv ID: 2509.11480
- Source URL: https://arxiv.org/abs/2509.11480
- Authors: Amir Taherin; Juyi Lin; Arash Akbari; Arman Akbari; Pu Zhao; Weiwei Chen; David Kaeli; Yanzhi Wang
- Reference count: 16
- This study evaluates Vision-Language-Action (VLA) models across edge and cloud GPUs, focusing on latency, throughput, and memory usage under different power constraints.

## Executive Summary
This study benchmarks five VLA models across edge (Jetson AGX Orin) and datacenter GPUs (H100, A100, A6000, V100) under varying power constraints. The research introduces VOTE architectures with optimized action chunking that achieve the highest throughput with minimal accuracy loss, while QwenVLA demonstrates competitive accuracy at lower memory usage. Power-constrained edge devices show non-linear performance degradation, yet high-end edge GPUs can match or exceed older datacenter GPUs in throughput.

## Method Summary
The study evaluates five VLA models (OpenVLA, SpatialVLA, OpenVLA-OFT, VOTE variants, QwenVLA) using the LIBERO benchmark with four task suites. Models are fine-tuned on LIBERO using AdamW with LoRA, then evaluated across multiple GPUs with varying power modes on Jetson AGX Orin. Measurements include success rate, peak VRAM usage, per-chunk latency, and throughput. Each model undergoes warm-up followed by 100 consecutive inference measurements.

## Key Results
- VOTE-MLP4 achieves highest throughput (474.78 Hz on H100) with only 2.9% accuracy loss vs. VOTE-1T
- QwenVLA matches or exceeds accuracy of larger 7B models while using only 7.39 GB VRAM
- Jetson Orin MAX mode (55.57 Hz) surpasses V100 throughput but is 8.5× slower than H100
- Power-constrained edge devices show non-linear throughput degradation (15W mode reduces throughput by 83% vs. MAX)

## Why This Works (Mechanism)

### Mechanism 1
Reducing action token count increases inference throughput without proportional accuracy loss. VOTE architectures output fewer `<ACT>` tokens per inference (1–2 tokens vs. standard tokenization), decreasing autoregressive decoding steps. Fewer decoding steps reduce sequential compute while preserving action chunk quality via denser token-to-action mappings.

Core assumption: Action semantics can be compressed into fewer discrete tokens without losing task-relevant information.
Evidence anchors:
- [abstract] "optimized action chunking (VOTE-2T, VOTE-MLP4) delivers the highest throughput with minimal accuracy loss"
- [Section IV, Table V] VOTE-1T achieves 96.9% average SR; VOTE-2T and VOTE-MLP4 drop only 2.0–2.9% while throughput increases substantially
- [Section IV, Fig. 3a] VOTE-MLP4 reaches 474.78 Hz on H100 vs. 7.36 Hz for OpenVLA

### Mechanism 2
Memory footprint is dominated by LLM backbone and vision encoder size, not action head design. Parameter storage and activation memory scale with backbone dimension. Action head variations (MLP depth, token count) contribute negligibly to peak VRAM relative to 7B vs. 1.5B backbone differences.

Core assumption: Inference batch size is fixed (single-image processing); memory scales linearly with model parameters.
Evidence anchors:
- [Section IV, Fig. 1] QwenVLA (2.6B params) uses 7.39 GB VRAM; OpenVLA-OFT (7B) uses 19.20 GB
- [Section IV] "memory usage is primarily driven by backbone size and vision encoder choice, with action head variations having negligible impact"

### Mechanism 3
Edge GPU throughput under power constraints degrades non-linearly with power budget reduction. Power mode scaling on Jetson AGX Orin simultaneously reduces core availability (8→4→3 TPCs), clock frequencies, and memory bandwidth. These compound, producing sharper throughput drops than linear power reduction would suggest.

Core assumption: Thermal and firmware-imposed power capping enforces simultaneous resource reduction, not just frequency scaling.
Evidence anchors:
- [Section IV, Fig. 3b] Throughput drops non-linearly: VOTE-MLP4 goes 55.57 Hz (MAX) → 36.86 Hz (50W) → 16.44 Hz (30W) → 9.19 Hz (15W)
- [Table II] Core count and frequency reduce together: 8 TPCs @1301 MHz (MAX) → 3 TPCs @420.75 MHz (15W)
- [Section IV] "Orin in MAX mode with VOTE-MLP4 achieves 55.57 Hz, surpassing the throughput of the V100 datacenter GPU (32.28 Hz)"

## Foundational Learning

- **Concept: Autoregressive Decoding in VLAs**
  - Why needed here: Throughput gains from VOTE depend on reducing autoregressive steps. You must understand how sequential token generation creates latency bottlenecks.
  - Quick check question: If a model generates 8 action tokens sequentially at 10ms per token, what is the minimum per-chunk latency?

- **Concept: Action Chunking**
  - Why needed here: VOTE-2T and VOTE-MLP4 trade accuracy for throughput via larger chunk sizes. Understanding chunking explains the accuracy/throughput Pareto frontier.
  - Quick check question: Why would predicting 16 actions at once be more efficient than predicting 1 action 16 times?

- **Concept: Power-Thermal Co-design on SoCs**
  - Why needed here: Jetson Orin's power modes bind together core count, frequency, and memory bandwidth. You need this to predict performance under thermal constraints.
  - Quick check question: If a 30W power cap reduces TPCs from 8 to 4, would you expect throughput to halve? Why or why not?

## Architecture Onboarding

- **Component map:** Vision Encoder (DINOv2 + SigLIP) -> Language Backbone (LLaMA 2-7B/ Qwen 2.5-1.5B/ PaliGemma-4B) -> Action Head (DAT/Cont-L1/ST/AAG) -> Output (discrete/continuous actions)

- **Critical path:**
  1. Load vision encoder + LLM backbone into VRAM
  2. Process 224×224 RGB image + language prompt
  3. Generate action tokens via autoregressive decoding
  4. Map tokens to continuous actions via action head
  5. Return chunk of N actions for execution

- **Design tradeoffs:**
  - Accuracy vs. Throughput: VOTE-1T (96.9% SR, 238.10 Hz on H100) vs. VOTE-MLP4 (94.0% SR, 474.78 Hz on H100)
  - Memory vs. Accuracy: QwenVLA (7.39 GB, 78.8% SR) vs. OpenVLA (14.35 GB, 76.5% SR)
  - Edge vs. Cloud: Orin MAX (55.57 Hz) beats V100 (32.28 Hz) but is 8.5× slower than H100 (474.78 Hz)

- **Failure signatures:**
  - OOM on edge: 19.20 GB model (OpenVLA-OFT) exceeds Orin's 32 GB total; requires offloading or smaller batch
  - Throughput collapse at low power: 15W mode reduces throughput by 83% vs. MAX for compute-heavy models
  - Accuracy drop with aggressive chunking: VOTE-MLP4 loses 2.9% average SR vs. VOTE-1T

- **First 3 experiments:**
  1. Baseline latency profiling: Run OpenVLA and VOTE-MLP4 on Orin MAX; measure per-chunk latency and confirm 40×+ throughput gap.
  2. Power mode sweep: Test VOTE-2T across Orin 15W/30W/50W/MAX; plot throughput vs. power to verify non-linear scaling.
  3. Memory-accuracy tradeoff: Compare QwenVLA vs. OpenVLA on LIBERO; confirm that 2.6B backbone can match 7B accuracy at ~50% memory.

## Open Questions the Paper Calls Out

### Open Question 1
How do quantization strategies (e.g., INT4/INT8) affect the trade-off between accuracy and inference throughput for VLA models on resource-constrained edge devices?
Basis in paper: [explicit] The conclusion states: "Future work will extend this analysis to additional model architectures, quantization strategies, and real-world robotic deployments..."
Why unresolved: The current study evaluates models in standard precision and identifies memory as a major bottleneck, but does not test compressed representations.
Evidence: Benchmark results comparing VOTE and QwenVLA accuracy and latency on Jetson AGX Orin before and after applying PTQ or QLoRA quantization.

### Open Question 2
Do the favorable scaling trends of action-chunking models (like VOTE-MLP4) transfer effectively to physical robotic deployments with real-world latency constraints and sensor noise?
Basis in paper: [explicit] The conclusion explicitly calls for extending the analysis to "real-world robotic deployments," as the current work relies entirely on the simulated LIBERO benchmark.
Why unresolved: Simulation benchmarks like LIBERO do not capture real-world I/O bottlenecks, actuation delays, or visual domain shifts.
Evidence: Success rate and end-to-end latency measurements of the proposed VLA models deployed on physical robot arms performing manipulation tasks.

### Open Question 3
What specific hardware bottlenecks (e.g., memory bandwidth vs. compute unit starvation) drive the non-linear performance degradation observed when enforcing strict power caps on edge devices?
Basis in paper: [inferred] The paper reports that "power-constrained edge devices exhibit non-linear performance degradation," but profiles only the external outcomes rather than internal hardware counters.
Why unresolved: Without analyzing internal metrics, it is unclear if the non-linearity stems from thermal throttling, memory frequency scaling, or unbalanced compute-to-memory ratios.
Evidence: Profiling data correlating GPU power limits with hardware performance counters (SM utilization, memory bandwidth) to identify the saturation point causing the non-linear drop.

## Limitations
- Evaluation focuses on a single benchmark (LIBERO) with fixed input resolutions and language prompts
- Power scaling experiments assume ideal thermal conditions that may not hold in real-world edge deployments
- Memory measurements capture peak VRAM but do not account for CPU memory usage during preprocessing or postprocessing

## Confidence

**High Confidence**: The relative performance rankings of VLA models and the non-linear power scaling on Jetson Orin are well-supported by direct measurements.

**Medium Confidence**: The mechanism explanations for accuracy-robustness to token reduction and the claim that action head variations have negligible memory impact have some untested assumptions.

**Low Confidence**: The extrapolation that high-end edge GPUs can "match or exceed" older datacenter GPUs assumes comparable cooling conditions that may not hold in practical edge deployments.

## Next Checks

1. Cross-benchmark validation: Evaluate VOTE and QwenVLA architectures on at least two additional robotic manipulation benchmarks (e.g., RoboCat, MetaWorld) with varying image resolutions and language prompt complexity.

2. Real-world thermal testing: Deploy VOTE-MLP4 on a Jetson Orin system under continuous operation with varying ambient temperatures (20°C to 35°C) while measuring sustained throughput.

3. Memory scaling analysis: Measure total system memory usage (GPU + CPU) during complete inference pipelines with different vision encoder resolutions (224×224, 336×336, 448×448) to identify when vision encoders become the dominant memory consumer.