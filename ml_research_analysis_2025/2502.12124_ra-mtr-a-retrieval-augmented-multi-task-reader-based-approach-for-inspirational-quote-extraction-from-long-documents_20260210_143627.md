---
ver: rpa2
title: 'RA-MTR: A Retrieval Augmented Multi-Task Reader based Approach for Inspirational
  Quote Extraction from Long Documents'
arxiv_id: '2502.12124'
source_url: https://arxiv.org/abs/2502.12124
tags:
- quote
- context
- paragraph
- quotes
- multi-task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# RA-MTR: A Retrieval Augmented Multi-Task Reader based Approach for Inspirational Quote Extraction from Long Documents

## Quick Facts
- arXiv ID: 2502.12124
- Source URL: https://arxiv.org/abs/2502.12124
- Authors: Sayantan Adak; Animesh Mukherjee
- Reference count: 40
- Key outcome: Multi-task learning with span prediction and sequence tagging improves quote extraction over single-task approaches, achieving up to 6-8% better BoW-F1 on QuoteR dataset

## Executive Summary
RA-MTR is a retrieval-augmented framework for extracting inspirational quotes from long documents. It combines a two-stage retrieval system (vector-store + LLM re-ranking) with a multi-task reader that jointly predicts quote spans and quotable sequences. The approach demonstrates strong performance on curated datasets, with particular robustness when context is absent. The system addresses the challenge of identifying quotable phrases within lengthy texts through specialized architectural components.

## Method Summary
The RA-MTR framework uses a retriever-reader architecture with two key innovations: a two-stage retrieval pipeline and a multi-task learning reader. The retriever employs sentence-transformer embeddings with a ChromaDB vector store, followed by Llama-3 re-ranking using binary classification. The multi-task reader shares a SpanBERT encoder between span prediction (identifying start/end positions) and sequence tagging (BIO labels via CRF). The system is trained on QuoteR dataset (4,889 triples) and evaluated using Exact Match and Bag-of-Words F1 metrics, with additional testing on Gandhi and Quotus datasets for cross-domain transfer.

## Key Results
- RA-MTR achieves 6-8% higher BoW-F1 compared to single-task baselines on QuoteR dataset
- The multi-task framework shows 38.2 BoW-F1 without context vs 30.9 for BERT and 29.7 for SpanBERT
- Llama-3 re-ranking improves P@1 from 0.15 (BM25) to 0.48 on QuoteR test set
- Model generates "pseudo errors" in 72% of mismatched predictions, identifying valid alternative quotes

## Why This Works (Mechanism)

### Mechanism 1
Two-stage retrieval with neural re-ranking improves paragraph selection for long documents. Vector-store retrieval using sentence-transformers provides initial candidates; Llama-3 re-ranker scores context-paragraph pairs via binary classification, refining ranking using yes-score = p(yes|P) / (p(yes|P) + p(no|P)). Evidence shows Llama-3 achieving P@1 of 0.48 vs BM25's 0.15 on QuoteR.

### Mechanism 2
Multi-task learning combining span prediction and sequence tagging improves quote extraction. Two task heads share SpanBERT encoder - span prediction identifies start/end positions using learned S and E vectors with softmax over tokens; sequence tagging uses CRF to predict BIO labels for quotable phrases. Losses are averaged during backpropagation. Task complementarity confirmed by 0.23 cosine similarity between representations.

### Mechanism 3
The multi-task framework provides robustness to context absence through context-independent sequence tagging. Sequence tagging head learns quotability markers (BIO labels) without context dependency; when context is removed, this head continues identifying quotable phrases while span prediction degrades. Performance without context: 38.2 BoW-F1 vs BERT's 30.9 and SpanBERT's 29.7.

## Foundational Learning

- Concept: **RAG (Retrieval-Augmented Generation) Architecture**
  - Why needed here: The retriever-reader pipeline follows RAG principles; understanding embedding-based retrieval and vector stores is prerequisite
  - Quick check question: Can you explain why sparse retrieval (BM25) alone fails for semantic matching in this task?

- Concept: **Multi-Task Learning with Shared Encoder**
  - Why needed here: The core innovation uses two task heads sharing SpanBERT; understanding gradient flow and loss aggregation is essential
  - Quick check question: How would averaging losses from two tasks affect training dynamics if one task converges faster?

- Concept: **Sequence Tagging with CRF**
  - Why needed here: The BIO tagging for quotability detection requires understanding conditional random fields and label dependencies
  - Quick check question: Why use CRF instead of independent token classification for sequence tagging?

## Architecture Onboarding

- Component map: Document splitter -> ChromaDB vector store -> Llama-3 re-ranker -> Top-3 paragraphs -> Multi-task reader (SpanBERT encoder -> Span prediction + CRF sequence tagging) -> Top-3 predicted spans

- Critical path: 1) Vector-store construction (pre-computed, one-time) 2) Re-ranker fine-tuning (requires context-paragraph-label triples) 3) Multi-task reader training (requires quote-context-paragraph triples with BIO labels) 4) End-to-end inference: retrieval -> re-ranking -> span extraction

- Design tradeoffs: Chunk size (1200 chars) balances retrieval granularity vs context preservation; larger chunks lose precision, smaller chunks break quote context. Top-k=20 for initial retrieval; increasing k showed diminishing returns. Multi-task vs single-task: +6-8% BoW-F1 improvement but 2x training complexity.

- Failure signatures: Over-prediction (model predicts spans containing true quote plus surrounding text, observed in 28% of cases); Sub-prediction (model predicts only sub-phrase of original quote, observed in 72% of mismatched predictions); Multiple quotable phrases (20.7% of paragraphs contain multiple quotable regions).

- First 3 experiments: 1) Retriever-only baseline: Run BM25 retrieval on QuoteR test set, measure P@1 and P@3 without re-ranking to establish lower bound. 2) Ablation of multi-task heads: Train single-task SpanBERT (span prediction only) vs multi-task on same data; compare BoW-F1 to quantify CRF contribution. 3) Few-shot cross-domain transfer: Pre-train multi-task reader on QuoteR, then fine-tune with 8/16/32 samples from Gandhi dataset; measure generalization gap vs training from scratch.

## Open Questions the Paper Calls Out

### Open Question 1
How can the RA-MTR framework be adapted to maintain performance on non-English languages with morphosyntactic structures significantly different from English? The authors state experiments are conducted for English version only, and pre-processing steps might not be suitable for languages with different morphosyntactic structures. Resolution requires cross-lingual evaluation on translated QuoteR or Gandhi datasets.

### Open Question 2
How can evaluation metrics be modified to reward models for identifying valid, ground-truth-adjacent quotes currently penalized as "pseudo errors"? Section 9 notes model produces "pseudo errors" where predicted quotes differ from ground truth but are assessed as "highly quotable" by human experts. Resolution requires experiments using recall-based metrics or human evaluation scores as primary success metric.

### Open Question 3
To what extent can multi-task training objective decouple "quotability" detection from context-reliance, allowing model to function in zero-context scenarios? Section 7 shows multi-task model degrades less than baselines when context is removed, but authors don't explore if CRF head can be trained to be entirely context-independent. Resolution requires ablation study comparing sequence tagger performance when fine-tuned exclusively on paragraphs without context inputs.

## Limitations
- Architecture generalization to truly open-domain long documents remains uncertain due to tuned hyperparameters for Project Gutenberg corpus
- Multi-task complementarity contribution not fully isolated; 6-8% improvement could partially stem from increased model capacity
- Evaluation scope limited to English inspirational quotes from specific domains; performance on other languages, quote types, or domains unknown

## Confidence
- High Confidence: Two-stage retrieval mechanism improves over BM25 baseline (P@1: 0.48 vs 0.15)
- Medium Confidence: Multi-task learning framework improves quote extraction performance (6-8% BoW-F1 improvement demonstrated)
- Low Confidence: Claim that quotability has context-independent linguistic markers learnable by sequence tagging (lacks cross-domain validation)

## Next Checks
1. Conduct controlled ablation experiments comparing single-task SpanBERT, single-task CRF, and RA-MTR to isolate component contributions
2. Test RA-MTR on diverse document types beyond inspirational quotes (scientific papers, news articles, technical documentation) to measure domain shift sensitivity
3. Systematically vary retriever chunk size (600, 1200, 2400 characters) and window size on QuoteR subset to analyze impact on retrieval accuracy and end-to-end performance