---
ver: rpa2
title: Few-Shot Multilingual Open-Domain QA from 5 Examples
arxiv_id: '2502.19722'
source_url: https://arxiv.org/abs/2502.19722
tags:
- data
- question
- answer
- language
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FSMODQA, a few-shot learning approach for
  multilingual open-domain question answering using minimal language-specific supervision
  (up to 5 examples per language). The method leverages large language models to generate
  synthetic multilingual QA data from Wikidata and Wikipedia passages, combined with
  a self-supervised pre-training objective.
---

# Few-Shot Multilingual Open-Domain QA from 5 Examples

## Quick Facts
- arXiv ID: 2502.19722
- Source URL: https://arxiv.org/abs/2502.19722
- Reference count: 32
- Primary result: Achieves +8.4% F1 gain on multilingual QA using only 5 examples per language

## Executive Summary
FSMODQA introduces a few-shot learning approach for multilingual open-domain QA that requires minimal language-specific supervision (up to 5 examples per language). The method leverages large language models to generate synthetic multilingual QA data from Wikidata and Wikipedia passages, combined with self-supervised pre-training. FSMODQA significantly outperforms existing few-shot and supervised baselines, achieving +8.4% gain in multilingual QA and +5.1% in retrieval accuracy. It also enables effective zero-shot language adaptation to new languages through cross-lingual prompting, making it a general and scalable solution for multilingual QA without costly large-scale annotation.

## Method Summary
FSMODQA employs a two-stage training approach. First, it pre-trains on 18.7M synthetic QA pairs generated from Wikidata triples using LLM prompting, creating both in-language and cross-lingual supervision signals. Second, it fine-tunes on synthetically generated data from 5-shot examples per target language, using NLI-based filtering and geometric sampling to maintain quality and answer length diversity. The model combines dual-encoder retrieval with cross-encoder reading in a single architecture, enabling end-to-end training. For zero-shot adaptation, English-supervised data generates training data for unseen languages through cross-lingual prompting with Wikipedia passages.

## Key Results
- Achieves 36.9% F1 on XOR-Full with only 5 examples per language, outperforming baselines by +8.4%
- Improves multilingual retrieval (nDCG@10) by +5.4% average across 10 unseen languages on MIRACL
- Outperforms supervised baselines (SWIM, GenRead) by 8.0-17.6% F1 despite using 100x less labeled data
- Demonstrates effective zero-shot language adaptation, improving MKQA F1 by +11.3% on unseen languages

## Why This Works (Mechanism)

### Mechanism 1: Structured Knowledge to Natural Language Pre-training
WikiData triples provide a high-quality source for generating diverse, multilingual QA pairs that teach the model retrieval and answer extraction before seeing any human-annotated data. The pipeline samples factual triplets from WikiData, uses an LLM to create diverse question templates, then distills this pattern to a smaller LLM to generate 18.7M multilingual QA pairs. Positive passages are identified by matching answers in Wikipedia pages linked via language links. Removing MLWIKIQA pre-training causes catastrophic performance collapse (36.9% → 9.6% F1 on XOR-Full).

### Mechanism 2: In-Context Learning for Style-Preserving Data Amplification
Few-shot examples encapsulate task-specific QA style and distribution, enabling LLMs to generate synthetic data that generalizes from 5 human examples to millions of training instances. For each target language, 5 examples (3 span answers, 1 yes, 1 no) are fed to the LLM as ICL demonstrations, which then generates QA pairs from randomly sampled Wikipedia passages. t-SNE visualization shows synthetic queries significantly overlap with gold-standard XOR-TYDI QA distribution in Japanese, demonstrating distributional preservation.

### Mechanism 3: Cross-Lingual Prompting for Zero-Shot Language Adaptation
English-supervised data can generate effective training data for unseen languages by prompting LLMs with English QA examples to produce in-language QA from target-language Wikipedia passages. For zero-shot adaptation, English NQ examples populate prompts, generating 128K instances per language. Zero-shot adaptation improves monolingual retrieval (nDCG@10) by +5.4% average across 10 unseen languages on MIRACL, with +11.3% improvement on MKQA QA tasks.

## Foundational Learning

- **Retrieve-Then-Read Pipeline**
  - Why needed here: FSMODQA combines dual-encoder retrieval with cross-encoder reading in a single model. Understanding how retrieval quality affects downstream answer generation is critical for debugging the end-to-end training loop.
  - Quick check question: Can you explain why retrieval accuracy is a prerequisite for effective answer generation in open-domain QA?

- **In-Context Learning (ICL)**
  - Why needed here: The entire few-shot data generation strategy depends on LLMs learning QA patterns from 5 demonstration examples. Understanding ICL capabilities and limitations helps diagnose when synthetic data quality will degrade.
  - Quick check question: Given 5 Japanese QA examples (3 span, 1 yes, 1 no), what factors determine whether an LLM will generate distributionally similar synthetic data?

- **Dense Retrieval / Dual-Encoder Architecture**
  - Why needed here: The model's first half uses shared-parameter dual encoders for question and passage embeddings. The contrastive loss (in-batch negatives) trains these embeddings. Understanding embedding space geometry is essential for retrieval debugging.
  - Quick check question: Why does the model use in-batch negatives rather than hard negatives during self-supervised pre-training?

## Architecture Onboarding

- **Component map:**
  Input Question + "Answer in {lang}" instruction → Dual Encoder (shared mT5-Large Encoder) → LayerNorm + Average Pooling → Question Embedding Eq → Dot Product Similarity ← Passage Embeddings Edi → Top-k Passage Selection → Dq → Concatenate Eq + Top-k Passage Embeddings → Cross-Encoder Layers → Contextualized Representations → Flatten + Feed to Decoder via Cross-Attention → mT5-Large Decoder → Generated Answer

- **Critical path:**
  1. Self-supervised pre-training on MLWIKIQA (18.7M triples, 100K steps, batch size 800) initializes both retrieval and QA capabilities. Without this, retrieval accuracy is too low for effective fine-tuning (ablation: 9.6% F1 without pre-training).
  2. Few-shot synthetic data generation: 5 examples per language → LLM prompting → 1.7M filtered QA pairs (FSMLQA).
  3. End-to-end fine-tuning on FSMLQA: Dual-encoder receives gradients from decoder cross-attention scores (KL divergence loss) rather than in-batch negatives. Asynchronous passage updates every 1K steps.
  4. For zero-shot adaptation: English NQ examples → prompt LLM → 128K target-language pairs → fine-tune pre-trained model for 3K steps.

- **Design tradeoffs:**
  - **Data scale vs. quality**: Table 1 shows 100K synthetic data outperforms 1.7M on retrieval (70.6% vs 70.6% tied, but fewer steps). Larger data skews toward short answers (78.4% → 95.3%), degrading long-answer performance.
  - **Geometric sampling trade-off**: Sampling without replacement exhausts long-answer examples; sampling with replacement improves long answers (Figure 6) but increases training epochs on repeated data.
  - **Cross-lingual vs. monolingual data**: Including cross-lingual synthetic data (from English passages) improves cross-lingual retrieval but slightly reduces in-language performance (Table 5: 46.8% → 49.3% in-language, but 31.2% → 30.0% cross-lingual).
  - **Zero-shot prompting strategies**: English-prompting outperforms direct fine-tuning on English data (36.1% vs 28.6%), but combining all strategies (direct FT + MT + prompting) achieves best results (38.1%).

- **Failure signatures:**
  - **Retrieval collapse**: If pre-training on MLWIKIQA is skipped, the dual-encoder cannot identify relevant passages, leading to <10% F1 (Table 6).
  - **Short-answer overfitting**: Without geometric sampling, synthetic data becomes 95%+ short answers, causing performance drop on long-answer questions (18.0% → 15.1%).
  - **NLI filtering failure**: Setting thresholds too low (Tl < 0.5, Tg < 0.8) includes hallucinated QA pairs; too high filters legitimate examples. Telugu shows no benefit from filtering (Table 6), suggesting NLI model deficiency.
  - **Cross-lingual misalignment**: If language links in Wikipedia are sparse or incorrect for a target language, cross-lingual positive passages will be missing, reducing cross-lingual retrieval capability.

- **First 3 experiments:**
  1. **Validate pre-training quality**: Pre-train on MLWIKIQA subset (e.g., 1 language) and measure retrieval accuracy (Recall@100) on held-out languages. Compare to ablation in Table 5 to confirm pre-training provides essential initialization.
  2. **Tune filtering thresholds**: Generate synthetic data for 1 language with varying NLI thresholds (Tl ∈ {0.3, 0.5, 0.7}, Tg ∈ {0.6, 0.8, 0.9}). Evaluate on XOR-Full dev set to find optimal quality/diversity trade-off before scaling to all languages.
  3. **Test geometric sampling**: Train two models on FSMLQA with random vs. geometric sampling (p=0.4). Evaluate separately on short (<3 tokens), medium (4-9), and long (≥10) answers to confirm the distribution correction mechanism from Figure 6.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the diversity and variation of synthetic multilingual QA data be improved to match the distribution of gold-standard human annotations?
- Basis in paper: Section 5.2 states, "The gold-standard data exhibits greater diversity than the synthetic data, suggesting that there is still room for improvement in enhancing diversity and variation during the data generation process, which we leave for future work."
- Why unresolved: The current method relies on few-shot prompting with only five examples per language, which constrains the stylistic variety of the generated queries, causing them to cluster tightly compared to human-curated data.
- What evidence would resolve it: A modified generation pipeline that produces synthetic data with a t-SNE distribution significantly overlapping with gold-standard datasets, coupled with improved F1 scores on diverse question types.

### Open Question 2
- Question: Can alternative data sampling strategies prevent performance degradation when scaling synthetic training data to the full 1.7M dataset size?
- Basis in paper: Section 3.7 notes that performance peaks at 0.6M data and degrades at full scale (1.7M) because the dataset becomes skewed toward short answers despite geometric sampling.
- Why unresolved: The paper attempts geometric sampling with and without replacement, but the model still overfits to the majority class (short answers) as data volume increases, limiting the benefits of large-scale generation.
- What evidence would resolve it: Demonstrating that a specific curriculum learning or re-weighting strategy allows the model to maintain or increase performance (F1 > 38.2%) when trained on the full synthetic dataset.

### Open Question 3
- Question: Does generating language-specific synthetic data improve performance on unseen high-resource languages compared to the cross-lingual prompting baseline?
- Basis in paper: Section 3.4 observes that FSMODQA lags behind the SWIM-X baseline on unseen high-resource languages (zh, fr, de), speculating that SWIM-X benefits from large-scale language-specific synthetic data generation.
- Why unresolved: It is unclear if the performance gap on these specific languages is due to the zero-shot transfer approach or simply a lack of targeted synthetic data volume for those high-resource languages.
- What evidence would resolve it: An ablation study showing performance changes on German, French, and Chinese when generating synthetic data specifically for those languages versus using the standard cross-lingual prompting setup.

## Limitations
- Method depends heavily on WikiData and Wikipedia coverage, limiting applicability to languages with sparse structured data
- NLI-based filtering quality varies significantly across languages, with no improvement reported for Telugu
- Synthetic data generation pipeline assumes ChatGPT and Gemma-7B can reliably produce high-quality multilingual QA pairs without empirical validation
- Geometric sampling parameter (p=0.4) is tuned per language without clear selection criteria, affecting reproducibility

## Confidence
- **High confidence**: The effectiveness of pre-training on MLWIKIQA for retrieval initialization (ablation shows catastrophic performance without it)
- **Medium confidence**: The distributional preservation claim for synthetic data (t-SNE visualization shows overlap but does not prove statistical equivalence)
- **Medium confidence**: Zero-shot language adaptation performance (improves unseen languages but success rates per language are unreported)

## Next Checks
1. **Evaluate zero-shot success rates per language**: Test the cross-lingual prompting mechanism on a diverse set of low-resource languages (Yoruba, Hausa, Lao, etc.) and report success rates, including cases where the LLM produces disfluent or irrelevant questions. This would validate the claimed generality of the zero-shot adaptation approach.

2. **Ablation on NLI filtering thresholds**: Systematically vary Tl and Tg thresholds across languages to identify the optimal quality-diversity trade-off. Include languages with known NLI model weaknesses (like Telugu) to understand when filtering helps versus harms performance.

3. **Geometric sampling parameter sensitivity**: Test the synthetic data generation pipeline with different geometric sampling parameters (p ∈ {0.1, 0.3, 0.5, 0.7}) and evaluate the resulting answer length distributions and downstream QA performance. This would validate whether the parameter tuning approach is robust or requires extensive per-language calibration.