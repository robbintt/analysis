---
ver: rpa2
title: 'We Politely Insist: Your LLM Must Learn the Persian Art of Taarof'
arxiv_id: '2509.01035'
source_url: https://arxiv.org/abs/2509.01035
tags:
- taarof
- cultural
- persian
- scenarios
- expected
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TAAROF BENCH, the first benchmark for evaluating
  large language models' understanding of Persian taarof, a complex system of ritual
  politeness. The benchmark includes 450 role-play scenarios validated by native speakers,
  covering 12 social interaction topics.
---

# We Politely Insist: Your LLM Must Learn the Persian Art of Taarof

## Quick Facts
- arXiv ID: 2509.01035
- Source URL: https://arxiv.org/abs/2509.01035
- Reference count: 26
- Key outcome: Introduces TAAROF BENCH, the first benchmark for evaluating large language models' understanding of Persian taarof, showing models struggle with culturally-specific politeness norms.

## Executive Summary
This paper introduces TAAROF BENCH, the first benchmark specifically designed to evaluate large language models' understanding of Persian taarof—a complex system of ritual politeness. The researchers created 450 role-play scenarios covering 12 social interaction topics and validated them with native speakers. When tested on five frontier LLMs, models showed only 34-42% accuracy on taarof-expected scenarios compared to 76-93% on non-taarof scenarios. The study demonstrates that standard politeness metrics fail to capture taarof norms, and that Persian-language prompts significantly improve performance. Through fine-tuning techniques, the researchers achieved substantial improvements in model alignment with cultural expectations.

## Method Summary
The benchmark consists of 450 role-play scenarios formalized as tuples (Environment, Roles, Context, Utterance, Expected response), with 70% taarof-expected and 30% non-taarof scenarios across 12 topics and three social settings. Models are evaluated through zero-shot prompting, generating open-ended responses that are judged by GPT-4 against expected responses. The researchers fine-tuned Llama 3-8B-Instruct using both supervised fine-tuning (SFT) and Direct Preference Optimization (DPO), achieving 21.8% and 42.3% improvements respectively. Human evaluation with 33 native speakers established baselines, showing 81.8% accuracy on taarof-expected scenarios.

## Key Results
- LLMs achieve only 34-42% accuracy on taarof-expected scenarios versus 76-93% on non-taarof scenarios
- Persian-language prompts improve performance by 12.8 to 32.0 percentage points across different models
- Standard politeness metrics label 84.5% of responses as polite, but only 41.7% meet actual taarof cultural expectations
- DPO nearly doubles performance on taarof-expected scenarios (37.2% to 79.5%), approaching native speaker levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompting in the native language activates latent cultural knowledge that English prompts fail to access.
- Mechanism: Language serves as a strong cultural context cue—Persian prompts shift model behavior toward culturally appropriate taarof responses by engaging cultural associations encoded during pretraining.
- Core assumption: Cultural norms are co-located with language-specific representations in model weights.
- Evidence anchors: Performance improves substantially with Persian prompts (DeepSeek V3: 36.6% to 68.6%, +32.0 points); related work confirms LLMs predominantly reflect Western cultures due to English-centric training.

### Mechanism 2
- Claim: Standard politeness detection is insufficient for culturally-specific communication norms.
- Mechanism: Taarof requires ritualized indirectness that violates Western "polite = direct and sincere" assumptions. Generic classifiers label responses as polite without detecting ritual violations.
- Core assumption: Politeness is culture-specific, not universal; "polite" behaviors in one culture may be rude in another.
- Evidence anchors: PoliteGuard labels 84.48% as polite, but only 41.7% meet Persian cultural expectations; no direct corpus evidence on politeness-taarof mismatch.

### Mechanism 3
- Claim: Direct Preference Optimization (DPO) more effectively encodes cultural norms than supervised fine-tuning alone.
- Mechanism: DPO trains on preference pairs (appropriate vs. inappropriate responses), teaching models to distinguish gradations of cultural correctness. SFT learns surface patterns but may miss the "why" behind norm adherence.
- Core assumption: Cultural competence requires value-based discrimination, not just pattern matching.
- Evidence anchors: DPO nearly doubles performance on taarof-expected scenarios (37.2% to 79.5%), approaching native speaker levels (81.8%); related Persian benchmarks use classification tasks, not preference learning.

## Foundational Learning

- Concept: **Cultural pragmatics vs. literal semantics**
  - Why needed here: Taarof requires understanding that utterances convey social meaning beyond literal content (e.g., "Be my guest" ≠ free ride; it initiates a ritual negotiation).
  - Quick check question: Can you explain why accepting a taxi driver's offer immediately might be culturally inappropriate in Iran?

- Concept: **Scenario-based role-play evaluation**
  - Why needed here: The benchmark uses structured tuples (Environment, Roles, Context, Utterance, Expected response) rather than multiple-choice, testing generative cultural reasoning.
  - Quick check question: What components define a scenario instance I in the formalization?

- Concept: **Preference optimization fundamentals**
  - Why needed here: DPO outperforms SFT for cultural alignment; understanding the difference is essential for adaptation strategies.
  - Quick check question: Why might learning to rank responses (DPO) capture cultural nuances better than learning to generate correct responses (SFT)?

## Architecture Onboarding

- Component map:
  Scenario Generator -> Model Under Test -> Judge (GPT-4) -> Evaluation Metrics -> (Optional) Fine-tuning Layer -> Re-evaluation

- Critical path:
  1. Native speaker validation of 450 scenarios → 2. Model inference in target condition (language/context/gender) → 3. GPT-4 judge evaluation → 4. Optional: fine-tune on labeled pairs, re-evaluate on held-out test set

- Design tradeoffs:
  - **Single-turn vs. multi-turn**: Benchmark probes specific taarof stages but doesn't test sustained interactions
  - **GPT-4 judge vs. human evaluation**: Judge enables scale (450 scenarios × 5 models); human study (33 participants, 30 scenarios) establishes baselines but is costly
  - **English vs. Persian prompts**: Persian improves accuracy but limits accessibility for non-Persian-speaking developers

- Failure signatures:
  - **Polite-but-wrong responses**: Accepting offers without refusal, direct compliments without deflection, making requests without hesitation
  - **Stereotype-driven reasoning**: Models justify correct behavior with gender stereotypes rather than taarof norms—correct output, wrong rationale
  - **Topic-specific collapse**: Models perform well on "gift" scenarios but fail on "making requests" and "compliments"

- First 3 experiments:
  1. **Language ablation**: Run all 450 scenarios in English, Persian, and no-country condition across at least two model families to replicate language-as-cue effect
  2. **Judge validation**: Manually label 50 randomly sampled scenario-response pairs; compute Cohen's κ against GPT-4 judge to verify 94% agreement claim before scaling
  3. **Minimal DPO pilot**: Train DPO on 50 scenarios with preference pairs; evaluate on 20 held-out taarof-expected scenarios to confirm signal before full 532-instance training run

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark relies heavily on GPT-4 as judge for cultural appropriateness, introducing potential circularity if GPT-4's own understanding is imperfect
- Study focuses on single-turn interactions, which may not capture the full complexity of sustained taarof exchanges
- English vs. Persian language advantage suggests cultural knowledge is partially language-bound, limiting accessibility for non-Persian developers

## Confidence
- **High confidence** in finding that standard politeness metrics fail to capture taarof norms (84.5% polite vs 41.7% culturally aligned)
- **Medium confidence** in language-as-cue mechanism (consistent effect across models, but representation hypothesis untested)
- **Medium confidence** in DPO superiority over SFT (small training set and manual filtering introduce uncertainty)

## Next Checks
1. **Judge Validation Audit**: Manually evaluate 100 randomly sampled scenario-response pairs using independent Persian-speaking annotators to verify GPT-4 judge reliability
2. **Multi-turn Extension**: Adapt 50 scenarios to include 2-3 turn exchanges and evaluate whether model performance degrades on sustained taarof interactions
3. **Cross-cultural Transfer**: Test whether models trained on TAAROFBENCH show improved performance on other culturally-specific benchmarks (e.g., Japanese honorifics) to assess generalizability