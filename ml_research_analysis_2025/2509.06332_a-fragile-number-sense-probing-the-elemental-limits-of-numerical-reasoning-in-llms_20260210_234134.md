---
ver: rpa2
title: 'A Fragile Number Sense: Probing the Elemental Limits of Numerical Reasoning
  in LLMs'
arxiv_id: '2509.06332'
source_url: https://arxiv.org/abs/2509.06332
tags:
- reasoning
- problems
- agents
- number
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigated the numerical reasoning capabilities of
  Large Language Models (LLMs) by evaluating them on a 100-problem challenge across
  four categories: basic arithmetic, advanced operations, primality checking, and
  the Game of 24 number puzzle. While the models performed well on the first three
  categories requiring deterministic algorithmic execution, they struggled significantly
  with the Game of 24, which demands heuristic search over a large combinatorial space.'
---

# A Fragile Number Sense: Probing the Elemental Limits of Numerical Reasoning in LLMs

## Quick Facts
- **arXiv ID:** 2509.06332
- **Source URL:** https://arxiv.org/abs/2509.06332
- **Reference count:** 23
- **Primary result:** LLMs excel at deterministic algorithmic tasks but struggle with heuristic search problems like the Game of 24, revealing limited numerical reasoning.

## Executive Summary
This study investigates the numerical reasoning capabilities of Large Language Models (LLMs) through a structured 100-problem challenge. The models performed well on basic arithmetic and primality checking but struggled significantly with the Game of 24, which requires heuristic search over a combinatorial space. The results suggest that while LLMs can effectively recall and execute known algorithms, they have limited ability to perform generative problem-solving that requires bounded rationality or number sense.

## Method Summary
The authors evaluated multiple LLM variants (including LRMs like o1) on a 100-problem challenge divided into four sets: basic arithmetic, advanced operations, primality checking, and the Game of 24. Each set was designed to test different aspects of numerical reasoning, from deterministic algorithmic execution to heuristic search. The Game of 24 required finding expressions using four numbers and basic operations to reach 24, testing the model's ability to navigate combinatorial search spaces.

## Key Results
- LLMs achieved high accuracy on deterministic algorithmic tasks (Sets 1-3) but failed on heuristic search tasks (Set 4)
- Performance degraded exponentially as reasoning chain length increased
- Models frequently broke game rules or claimed no solution existed when valid solutions were present
- Larger models and LRMs showed minimal improvement on the most challenging combinatorial problems

## Why This Works (Mechanism)

### Mechanism 1: Algorithmic Retrieval via Pattern Matching
LLMs map input patterns to memorized algorithmic traces learned during pre-training. Success in deterministic tasks indicates presence of similar patterns in training data, while failure in heuristic tasks indicates absence of a memorizable algorithm for the search space.

### Mechanism 2: Exponential Decay of Probability in Reasoning Chains
As reasoning chains lengthen, the probability of accuracy drops exponentially due to probabilistic token generation. Each step has <100% accuracy, and errors compound through the chain.

### Mechanism 3: Absence of Bounded Rationality (Number Sense)
LLMs lack human-like "number sense" for pruning search spaces. Without heuristics, combinatorial spaces explode beyond the model's ability to navigate via pattern matching alone.

## Foundational Learning

- **Concept: Deterministic vs. Heuristic Algorithms**
  - Why needed: To distinguish tasks LLMs can solve (calculating $1/13$) from those they fail (finding a path to 24)
  - Quick check: Can the problem be solved by applying fixed rules in linear order, or does it require guessing and backtracking?

- **Concept: Probabilistic Error Propagation**
  - Why needed: To anticipate why extended thinking time doesn't always improve math results
  - Quick check: If step 1 has 95% accuracy and step 2 has 95%, is the 2-step process accuracy higher or lower than 90%?

- **Concept: Bounded Rationality**
  - Why needed: To design systems that mimic human efficiency in combinatorial search
  - Quick check: When solving a maze, do you check every path or follow a rule like "always turn right"?

## Architecture Onboarding

- **Component map:** Input Processor -> Pattern Matcher/Retriever -> Reasoning Simulator -> Verifier
- **Critical path:** The Reasoning Simulator is the bottleneck for heuristic search tasks
- **Design tradeoffs:** LRMs use more compute for thinking but don't acquire number sense for efficient search
- **Failure signatures:** Early exit claiming no solution, rule hallucination (using numbers multiple times), precision drift in intermediate calculations
- **First 3 experiments:**
  1. Test models on pure arithmetic vs. combinatorial puzzles requiring identical arithmetic skills
  2. Probe rule adherence by requesting intermediate steps and checking for "Rule Hallucination"
  3. Compare verification accuracy vs. generation accuracy on solved Game of 24 equations

## Open Questions the Paper Calls Out

- Does the fragility in heuristic search generalize to other combinatorial domains like Sudoku or NP-Hard problems?
- Can mechanistic interpretability identify internal representations causing elementary skills to break down during extended reasoning chains?
- Can LLMs be trained to implement bounded rationality, or does solving complex search require brute-force scaling?

## Limitations
- Study scope limited to 100 problems across four specific categories
- Binary evaluation may not capture nuanced capabilities across reasoning paradigms
- Does not explore fine-tuning or external tool integration to improve performance

## Confidence
- **High Confidence (90%+):** LLMs perform significantly worse on combinatorial search problems compared to deterministic algorithmic problems
- **Medium Confidence (70-89%):** LLMs lack "number sense" and rely on pattern-matching rather than grounded reasoning
- **Low Confidence (Below 70%):** Increasing model scale or reasoning depth will not solve combinatorial search problems

## Next Checks
1. Evaluate same models on different combinatorial search problems (Sudoku, TSP) to test if Game of 24 failure generalizes
2. Analyze pre-training corpus to quantify frequency of combinatorial search problems vs. deterministic algorithms
3. Implement hybrid system pairing LLM with symbolic solver to determine if limitation is in reasoning or search implementation