---
ver: rpa2
title: Regression Language Models for Code
arxiv_id: '2509.26476'
source_url: https://arxiv.org/abs/2509.26476
tags:
- features
- regression
- code
- language
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Regression Language Model (RLM) for predicting
  numeric outcomes from code and computational graphs. The core method treats regression
  as a text-to-text next-token prediction problem using a small T5Gemma-based encoder-decoder.
---

# Regression Language Models for Code

## Quick Facts
- arXiv ID: 2509.26476
- Source URL: https://arxiv.org/abs/2509.26476
- Reference count: 40
- Primary result: T5Gemma-based RLM achieves >0.9 Spearman-rank correlation on competitive programming submissions and >0.46 Kendall-Tau on NAS benchmarks

## Executive Summary
This paper introduces a Regression Language Model (RLM) for predicting numeric outcomes from code and computational graphs. The core method treats regression as a text-to-text next-token prediction problem using a small T5Gemma-based encoder-decoder. The RLM can simultaneously predict metrics across diverse domains: memory footprint for high-level languages (Python, C++, Triton kernels), and accuracy/latency for neural network architectures in ONNX format. Key results include >0.9 Spearman-rank correlation on competitive programming submissions (APPS) and >0.5 on 17 CodeNet languages, with >0.46 Kendall-Tau on NAS benchmarks. The model outperforms specialized graph neural networks while naturally handling multi-objective predictions. Notably, it achieves this without feature engineering, using pure text representations, and demonstrates strong scaling and pretraining benefits.

## Method Summary
The RLM framework converts regression tasks into text-to-text prediction problems using a T5Gemma-small encoder-decoder architecture. Code and computational graphs are represented as text tokens, allowing the model to predict numeric metrics through next-token prediction. The approach handles multiple regression objectives simultaneously by treating each target metric as a separate token prediction task. The model is trained on diverse datasets including competitive programming submissions, CodeNet repositories, and neural architecture search benchmarks. Key innovations include the use of pure text representations without manual feature engineering and the ability to handle heterogeneous regression tasks within a unified framework.

## Key Results
- Achieves >0.9 Spearman-rank correlation on APPS competitive programming submissions
- Maintains >0.5 correlation across 17 CodeNet languages
- Demonstrates >0.46 Kendall-Tau on neural architecture search benchmarks
- Outperforms specialized graph neural networks on NAS tasks
- Shows strong scaling and pretraining benefits without feature engineering

## Why This Works (Mechanism)
The RLM framework succeeds by treating regression as a natural language task, leveraging the strong generalization capabilities of transformer-based language models. By representing code and computational graphs as text, the model can utilize pretraining knowledge from large language models while maintaining the flexibility to handle diverse regression objectives. The encoder-decoder architecture allows for both understanding the input structure and generating multiple numeric predictions simultaneously, effectively mapping the problem space into a familiar next-token prediction framework.

## Foundational Learning
- **Text-to-text regression conversion**: Needed because it enables leveraging language model pretraining for regression tasks. Quick check: Verify the model can predict single numeric values from text descriptions.
- **T5Gemma architecture**: Required for its balance of performance and efficiency in encoder-decoder tasks. Quick check: Compare performance against base T5 models of similar size.
- **Spearman/Kendall-Tau metrics**: Used for ranking correlation rather than absolute prediction accuracy. Quick check: Calculate Pearson correlation for absolute prediction accuracy.
- **Multi-objective prediction framework**: Enables simultaneous prediction of multiple metrics. Quick check: Test prediction of correlated vs uncorrelated objectives.
- **Code graph representation**: Critical for converting computational structures into token sequences. Quick check: Validate token sequence preserves structural information.
- **Zero-shot capability**: Important for applying pretrained models to new regression tasks. Quick check: Measure performance on unseen metric types.

## Architecture Onboarding
**Component Map**: Input text/code -> Encoder -> Cross-attention -> Decoder -> Numeric token predictions
**Critical Path**: Text preprocessing → Tokenization → Encoder processing → Cross-attention with decoder → Numeric token generation
**Design Tradeoffs**: Uses small T5Gemma for efficiency vs larger models for potentially better accuracy; pure text representation vs structured feature extraction; multi-task vs single-task optimization
**Failure Signatures**: Poor performance on highly structured data requiring spatial reasoning; degraded accuracy when objectives are strongly correlated; token prediction errors propagating to final numeric predictions
**First Experiments**: 1) Single numeric prediction from simple code snippets, 2) Multi-objective prediction on synthetic datasets, 3) Cross-domain transfer from memory to latency prediction

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- Evaluation relies heavily on ranking correlation metrics (Spearman/Kendall-Tau) rather than absolute prediction accuracy
- Claims about natural multi-objective handling lack detailed analysis of objective trade-offs
- Comparative advantage over specialized GNNs needs broader validation across more diverse benchmarks
- Limited discussion of model behavior on real-world codebases versus competition submissions

## Confidence
- High confidence: Technical framework design and implementation details
- Medium confidence: Cross-domain applicability claims across memory, latency, and accuracy predictions
- Low confidence: Comparative advantage over specialized models without more extensive ablation studies

## Next Checks
1. Conduct ablation studies comparing RLM performance against specialized GNNs across a broader range of code metrics and domains beyond NAS
2. Validate predictions on held-out real-world codebases with ground truth performance measurements, not just competition submissions
3. Test the model's ability to predict multi-objective trade-offs by explicitly evaluating Pareto-optimal prediction accuracy across competing objectives