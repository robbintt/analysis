---
ver: rpa2
title: 'Adversarially-Robust TD Learning with Markovian Data: Finite-Time Rates and
  Fundamental Limits'
arxiv_id: '2502.04662'
source_url: https://arxiv.org/abs/2502.04662
tags:
- learning
- where
- data
- reward
- robust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of robust policy evaluation in
  reinforcement learning under adversarial reward contamination. The authors consider
  a Huber-contaminated reward model where an adversary can arbitrarily corrupt a small
  fraction of rewards and show that vanilla TD learning is vulnerable to such attacks.
---

# Adversarially-Robust TD Learning with Markovian Data: Finite-Time Rates and Fundamental Limits

## Quick Facts
- arXiv ID: 2502.04662
- Source URL: https://arxiv.org/abs/2502.04662
- Reference count: 40
- Primary result: Achieves finite-time convergence matching vanilla TD up to an additive O(√ϵ) term under adversarial reward contamination.

## Executive Summary
This paper develops a robust temporal difference (TD) learning algorithm for policy evaluation under adversarial reward poisoning. The authors propose Robust-TD, which combines robust mean estimation with dynamic thresholding to achieve resilience against Huber-contaminated rewards. The key theoretical contribution is establishing that Robust-TD achieves finite-time convergence guarantees matching vanilla TD learning up to an additive O(√ϵ) term, where ϵ is the corruption probability. This additive error is proven to be fundamental through a minimax lower bound, demonstrating it is unavoidable in the presence of adversarial contamination.

## Method Summary
The paper addresses robust policy evaluation in reinforcement learning under adversarial reward contamination. It considers a Huber-contaminated reward model where an adversary can arbitrarily corrupt a small fraction of rewards. The authors develop Robust-TD, which combines robust mean estimation with a dynamic thresholding mechanism. The algorithm uses RUMEM (Robust Univariate Mean Estimator for Markovian Data) to estimate the bias term with subsampling and Median-of-Means logic. The method leverages Median-of-Means estimators adapted for Markovian data, providing the first analysis of such estimators under both heavy-tailed and adversarially corrupted, time-correlated data.

## Key Results
- Robust-TD achieves finite-time convergence matching vanilla TD learning up to an additive O(√ϵ) term
- The O(√ϵ) term is proven fundamental through a minimax lower bound
- Algorithm provides first analysis of Median-of-Means estimators under both heavy-tailed and adversarially corrupted, time-correlated data
- Demonstrates resilience against reward poisoning with provably optimal error rates

## Why This Works (Mechanism)
The algorithm works by combining robust statistical estimation techniques with reinforcement learning updates. The RUMEM estimator provides resilience against outliers by using Median-of-Means with subsampling to handle Markovian correlations. The dynamic thresholding mechanism resets estimates when contamination is detected, preventing corrupted values from propagating through the learning process. This combination allows the algorithm to achieve the optimal error rate while maintaining convergence properties similar to vanilla TD learning.

## Foundational Learning
- **Huber contamination model**: Adversarial corruption of rewards where each sample is corrupted with probability ε
  - Why needed: Provides realistic threat model for reward poisoning attacks
  - Quick check: Verify contamination probability and magnitude parameters

- **Markov chain mixing time**: Time required for a Markov chain to converge to its stationary distribution
  - Why needed: Critical for subsampling strategy in RUMEM estimator
  - Quick check: Estimate mixing time of generated MDP to set subsampling gap

- **Median-of-Means estimation**: Robust statistical technique combining subsampling with median aggregation
  - Why needed: Provides resilience against adversarial outliers while maintaining statistical efficiency
  - Quick check: Verify N ≥ 4Lτ condition for sample size vs buckets/subsampling

- **Temporal Difference learning**: Value function approximation method for policy evaluation
  - Why needed: Baseline algorithm that Robust-TD aims to match in performance
  - Quick check: Compare convergence rates with vanilla TD implementation

## Architecture Onboarding

**Component map**: MDP generation -> Feature matrix construction -> RUMEM estimator -> Robust-TD update -> MSE computation

**Critical path**: 
1. Environment Setup: Generate MDP and feature matrix
2. Implement RUMEM: Subsampling and Median-of-Means logic
3. Execute Robust-TD: Run algorithm with corruption and plot MSE

**Design tradeoffs**:
- **Robustness vs. Efficiency**: RUMEM provides better contamination resilience but requires more samples and computational overhead
- **Thresholding aggressiveness**: Conservative thresholds reduce false positives but may miss contamination events
- **Subsampling gap vs. Memory**: Larger gaps improve mixing but require more memory for sample storage

**Failure signatures**:
- High variance/no convergence: RUMEM sample size condition N ≥ 4Lτ violated
- Stalled estimates: Thresholding too aggressive, causing frequent resets to zero
- Poor robustness: Insufficient burn-in time or incorrect mixing time estimation

**First experiments**:
1. Verify RUMEM estimator with synthetic corrupted data (ε=0.01) and check bias reduction
2. Run Robust-TD on simple MDP with known ground truth and verify convergence
3. Compare MSE trajectories of Robust-TD vs vanilla TD under increasing corruption levels

## Open Questions the Paper Calls Out
**Open Question 1**: Is the dependence on 1/ω² in the additive O(ϵ) error term tight?
- Basis: Section 6, "Discussion" states no lower bound supports this dependence
- Why unresolved: Current lower bound doesn't specifically address feature matrix conditioning
- Evidence needed: Minimax lower bound showing error must scale with 1/ω²

**Open Question 2**: Can finite-time guarantees extend to nonlinear stochastic approximation schemes like Q-learning?
- Basis: Section 7 explicitly lists generalizing to nonlinear schemes as future work
- Why unresolved: Current analysis relies heavily on linear TD structure
- Evidence needed: Convergence proof for robust Q-learning under Huber contamination

**Open Question 3**: What are fundamental limits for robust policy evaluation with single Markov trajectory data?
- Basis: Section 7 notes plan to derive finer lower bounds accounting for Markov sampling
- Why unresolved: Current lower bound uses i.i.d. model, missing mixing time effects
- Evidence needed: Information-theoretic lower bound including mixing time τ_mix

**Open Question 4**: How does performance change under total corruption budget constraint vs probabilistic Huber model?
- Basis: Section 7 states authors plan to consider other attack models
- Why unresolved: Current algorithm tailored to probabilistic contamination model
- Evidence needed: Analysis of Robust-TD under total budget constraint C

## Limitations
- The dependence on 1/ω² in the additive error term lacks supporting lower bound
- Analysis limited to linear TD learning, not extending to nonlinear schemes like Q-learning
- Lower bound construction assumes i.i.d. observations, not accounting for Markov sampling

## Confidence
- **High confidence**: Theoretical framework and algorithm design
- **Medium confidence**: Convergence behavior claims due to unspecified MDP structure and feature matrix construction
- **Low confidence**: Fundamental lower bound proof due to unspecified hard instance construction

## Next Checks
1. Implement concrete MDP instance and empirically estimate its mixing time to verify theoretical sample complexity requirements
2. Conduct sensitivity analysis of Robust-TD performance with respect to corruption magnitude parameter (varying from 0.0001 to 0.01) to validate O(√ϵ) dependence
3. Compare empirical MSE convergence curves against theoretical bound to identify constant factor discrepancies that might indicate implementation issues with RUMEM estimator or thresholding mechanism