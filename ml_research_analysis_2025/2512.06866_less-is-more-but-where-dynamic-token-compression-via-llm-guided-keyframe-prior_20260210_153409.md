---
ver: rpa2
title: Less Is More, but Where? Dynamic Token Compression via LLM-Guided Keyframe
  Prior
arxiv_id: '2512.06866'
source_url: https://arxiv.org/abs/2512.06866
tags:
- dytok
- token
- tokens
- attention
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational bottleneck in Video Large
  Language Models (VLLMs) caused by quadratic growth in processing long video sequences.
  The authors propose DyToK, a training-free method that leverages the keyframe prior
  naturally encoded in VLLM attention mechanisms to dynamically allocate token budgets
  per frame.
---

# Less Is More, but Where? Dynamic Token Compression via LLM-Guided Keyframe Prior

## Quick Facts
- arXiv ID: 2512.06866
- Source URL: https://arxiv.org/abs/2512.06866
- Authors: Yulin Li; Haokun Gui; Ziyang Fan; Junjie Wang; Bin Kang; Bin Chen; Zhuotao Tian
- Reference count: 40
- Primary result: Achieves up to 4.3× faster inference and 24.0% performance gains at extreme compression ratios (10% token retention) when integrated with state-of-the-art pruning methods

## Executive Summary
This paper addresses the computational bottleneck in Video Large Language Models (VLLMs) caused by quadratic growth in processing long video sequences. The authors propose DyToK, a training-free method that leverages the keyframe prior naturally encoded in VLLM attention mechanisms to dynamically allocate token budgets per frame. By using cross-modal attention from a lightweight assistant model, DyToK identifies semantically important frames and prioritizes token retention accordingly. Extensive experiments show DyToK improves efficiency-accuracy trade-offs across multiple benchmarks (VideoMME, LongVideoBench, MLVU), achieving up to 4.3× faster inference and 24.0% performance gains at extreme compression ratios (10% token retention) when integrated with state-of-the-art pruning methods like VisionZip and FastV.

## Method Summary
DyToK is a training-free dynamic token compression method that extracts keyframe priors from cross-modal attention patterns in VLLM layers. The method uses a lightweight assistant model (0.5B parameters) to run inference on video-query pairs and extract deep-layer attention scores as frame importance indicators. These scores are aggregated across the last 1/3 of transformer layers and used to allocate token budgets proportionally to each frame's importance, with an upper limit to suppress temporal attention outliers. The allocated budgets are then applied to existing compression methods (VisionZip, FastV, etc.) on a per-frame basis, allowing DyToK to maintain accuracy while significantly reducing computational overhead.

## Key Results
- Achieves 90.2% accuracy vs 71.3% baseline at 90% compression (10% retention) on VideoMME
- Demonstrates 4.3× faster inference compared to uncompressed processing
- Shows 24.0% performance gains at extreme compression ratios when integrated with VisionZip
- Maintains strong plug-and-play compatibility across multiple VLLM architectures and compression backbones

## Why This Works (Mechanism)

### Mechanism 1: Query-Conditioned Keyframe Prior in Deep Attention Layers
- Claim: VLLM attention layers naturally encode task-relevant frame importance, with deeper layers providing more reliable keyframe indicators than shallow layers.
- Mechanism: Cross-modal attention from the final text token to visual tokens is aggregated across selected deep layers (last 1/3 of model). The softmax-normalized attention scores per frame yield temporal importance weights ĵwf, which correlate with ground-truth keyframes even when the model answers incorrectly.
- Core assumption: Attention peaks reflect semantic relevance rather than artifacts; temporal outliers at edge/middle frames can be suppressed via per-frame token caps.
- Evidence anchors:
  - [abstract] "VLLM attention layers naturally encoding query-conditioned keyframe priors"
  - [section 2.2] "deeper layers exhibit more semantically meaningful and task-aware attention distributions... layers 20 and 23 achieving the best performance"
  - [corpus] Neighbor paper "Keyframe-oriented Vision Token Pruning" addresses similar keyframe-guided pruning but lacks the assistant-model efficiency strategy
- Break condition: If attention patterns become unstable under domain shift (e.g., out-of-distribution video types) or if shallow-layer noise dominates due to architecture changes, prior quality degrades.

### Mechanism 2: Lightweight Assistant Model for Efficient Prior Extraction
- Claim: A compact model from the same architectural family can approximate the primary model's keyframe perception at ~14× lower cost.
- Mechanism: Instead of running full model to extract deep-layer attention, DyToK runs a 0.5B assistant on the same video-query pair, aggregates attention from its deep layers, and uses resulting frame weights to guide the primary 7B+ model's token budget. This avoids recomputing early layers in the primary model.
- Core assumption: Keyframe perception capability transfers within model families; assistant's attention distribution approximates primary model's semantic focus.
- Evidence anchors:
  - [section 3.2] "compact assistant model achieves nearly identical frame perception accuracy to the primary model despite being 14× smaller"
  - [table 4] Shows Tiny (0.5B) achieves 99.6% of Base performance at 50% retention
  - [corpus] FCoT-VL paper addresses efficient visual token compression but focuses on text-oriented tasks rather than video temporal dynamics
- Break condition: If model families diverge significantly in attention head structure or training objectives, assistant-to-primary transfer weakens. Corpus lacks direct studies of cross-family prior transfer.

### Mechanism 3: Dynamic Per-Frame Budget Allocation with Outlier Suppression
- Claim: Proportional token allocation based on frame importance preserves accuracy under aggressive compression better than uniform allocation.
- Mechanism: Given total budget T_total and per-frame importance ĵwf, initial allocation a_f = floor(ĵwf × T_total) is computed. Remaining tokens are distributed via fractional remainder ranking. An upper limit T_max prevents any frame from monopolizing budget—excess is reallocated. Each frame then applies a base compression method (VisionZip, FastV, etc.) with its allocated budget.
- Core assumption: Semantic importance is roughly proportional to required token count; temporal redundancy can be safely discarded from low-importance frames.
- Evidence anchors:
  - [table 1] At 90% compression (10% retention), DyToK + VisionZip achieves 90.2% vs baseline 71.3%—an 18.9% gain
  - [section 3.3] "tokens exceeding the per-frame limit T_max are reallocated to frames with available capacity"
  - [corpus] Related work "FOCUS" uses retrieval-style scoring for keyframe selection but operates pre-encoding, adding overhead
- Break condition: If queries require fine-grained details across many frames (e.g., temporal order tasks requiring all transitions), aggressive per-frame compression may lose critical cues regardless of allocation strategy.

## Foundational Learning

- Concept: **Cross-modal attention in transformer layers**
  - Why needed here: DyToK extracts keyframe priors from text-to-visual attention patterns; understanding Q-K-V attention computation is essential to interpret why deep layers capture semantics while shallow layers are noisy.
  - Quick check question: Can you explain why attention from the last query token to visual tokens might indicate frame relevance, and why layer depth matters?

- Concept: **Token pruning vs. token merging strategies**
  - Why needed here: DyToK is a meta-method that wraps existing compression backbones (FastV prunes mid-LLM, VisionZip merges encoder-side); distinguishing these paradigms clarifies where DyToK intervenes.
  - Quick check question: If VisionZip merges contextual tokens pre-LLM while FastV prunes attention-weighted tokens mid-LLM, at what point does DyToK's frame-weighted budget allocation modify each method's behavior?

- Concept: **Quadratic complexity in transformer self-attention**
  - Why needed here: The paper's motivation stems from O(n²) cost in LLM layers when n (visual tokens) scales with frame count; understanding this bottleneck explains why reducing n_vis directly impacts FLOPs.
  - Quick check question: Given the FLOPs formula in Eq. (1), which term dominates for a 32-frame video with 196 tokens/frame, and how does 90% compression change the bottleneck?

## Architecture Onboarding

- Component map:
  - Vision Encoder (CLIP/SigLIP) -> Projector + Pooling -> Lightweight Assistant LLM -> Token Budget Allocator -> Base Compression Method -> Primary LLM

- Critical path:
  1. Video frames → Vision Encoder → Patch tokens
  2. Patch tokens → Projector + Pooling → Pooled visual tokens
  3. Video + Query → Assistant LLM → Deep-layer attention → Frame importance scores (ĵwf)
  4. Scores + T_total → Budget Allocator → Per-frame token counts (a_f)
  5. Per-frame tokens + Compression(x_f, a_f) → Compressed sequence
  6. Compressed sequence + Text tokens → Primary LLM → Answer

- Design tradeoffs:
  - Assistant model size vs. prior accuracy: 0.5B is efficient but may miss subtle cues; 3B may outperform 7B at some ratios (Table 9 shows 3B > 7B at 20% retention)
  - T_max strictness vs. flexibility: Tighter caps (98 tokens/frame) suppress outliers better but may starve genuinely important frames
  - Layer selection strategy: Last 1/3 generalizes across models; manual single-layer tuning (layer 20) may yield marginal gains but requires per-model calibration

- Failure signatures:
  - Attention sink dominance: If edge/middle frames consistently receive high attention regardless of query content, verify T_max is enforced; consider explicitly masking known outlier positions
  - Zero-token allocation: At extreme compression + low T_max, some frames may receive 0 tokens; ensure minimum floor (e.g., 1 token) if temporal coherence matters
  - Assistant-primary mismatch: If assistant attention peaks at wrong frames, verify both models use same tokenizer and pooling; cross-family assistants are unsupported

- First 3 experiments:
  1. Sanity check on layer selection: Run DyToK with single-layer priors at layers 0, 12, 20, 23 on 10-video subset; confirm deep layers produce higher ground-truth keyframe overlap (replicate Table 3 pattern)
  2. Assistant size ablation: Compare 0.5B vs 3B vs 7B assistant on LongVideoBench subset at 25% retention; measure (a) prior extraction time, (b) final accuracy gap vs full-model prior
  3. Compression ratio sweep with T_max variants: Test VisionZip + DyToK at 10%, 25%, 50% retention with T_max ∈ {196, 98, 49}; plot accuracy vs. T_max to find stability threshold for your target model

## Open Questions the Paper Calls Out

- **Dynamic compression without assistant model overhead**: The authors state they "have not yet proposed a better method to avoid introducing additional models" for prior generation, leaving the computational overhead of the assistant model as a limitation of the training-free approach.

- **Underlying causes of temporal attention outliers**: The paper identifies a phenomenon where models disproportionately attend to non-semantic frames (e.g., middle frames in 64-frame inputs) and calls for research to clarify these mechanisms, suggesting this is a fundamental limitation in VLLM attention patterns.

- **Optimization of dynamic token allocation strategy**: The current method uses a deterministic algorithm for budget allocation, leaving the potential benefits of end-to-end optimization for frame-specific token budgets unexplored within the training-free paradigm.

## Limitations

- **Temporal attention pattern generalization**: The method relies on cross-modal attention patterns that may not generalize across diverse video domains or query types, with no explicit testing of stability across out-of-distribution videos.

- **Assistant model transfer fidelity**: The 14× efficiency gain assumes keyframe perception transfers within model families, but cross-family transfer hasn't been validated and may degrade if attention head structures differ significantly.

- **Per-frame compression floor effects**: At extreme compression ratios, the proportional allocation strategy may eliminate all tokens from some frames, but the paper doesn't address minimum token requirements for temporal coherence.

## Confidence

- **High Confidence**: Assistant model size ablation (Table 4) showing 0.5B achieves near-identical performance to larger assistants; compression ratio experiments (Table 1) demonstrating consistent gains across multiple benchmarks; per-frame Tmax ablation showing outlier suppression improves accuracy at high compression.

- **Medium Confidence**: Deep layers provide more reliable keyframe indicators than shallow layers; proportional allocation strategy's superiority over uniform allocation; plug-and-play compatibility across different compression backbones.

- **Low Confidence**: Attention patterns generalize across diverse video domains; transferability of keyframe perception across different model families; behavior at compression ratios below 10% or with minimum token floor constraints.

## Next Checks

- **Attention Pattern Stability Across Domains**: Run DyToK on video subsets from different domains (sports, surveillance, entertainment, educational) and visualize frame-level attention distributions. Compare whether the same deep layers (20/23) consistently identify semantically relevant frames across domains, or if domain-specific layer selection improves performance.

- **Cross-Family Assistant Transfer**: Test DyToK with assistant models from different architectural families (e.g., SigLIP-based assistant with CLIP-based primary model). Measure the degradation in keyframe perception accuracy and determine if there's a minimum similarity threshold in attention head structure required for effective transfer.

- **Minimum Token Floor Impact**: Conduct experiments at compression ratios below 10% with enforced minimum token floors per frame (e.g., 1-5 tokens minimum). Measure the trade-off between floor strictness and accuracy degradation to identify whether temporal coherence requires minimum token budgets regardless of importance scores.