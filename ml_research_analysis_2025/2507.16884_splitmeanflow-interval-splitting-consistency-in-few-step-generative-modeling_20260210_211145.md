---
ver: rpa2
title: 'SplitMeanFlow: Interval Splitting Consistency in Few-Step Generative Modeling'
arxiv_id: '2507.16884'
source_url: https://arxiv.org/abs/2507.16884
tags:
- velocity
- average
- flow
- splitmeanflow
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SplitMeanFlow, a novel framework for few-step
  generative modeling that addresses the computational inefficiency of iterative sampling
  in diffusion models and flow matching. The core idea leverages the additivity property
  of definite integrals to derive an algebraic identity termed Interval Splitting
  Consistency, which establishes a self-referential relationship for the average velocity
  field across different time intervals without using differential operators.
---

# SplitMeanFlow: Interval Splitting Consistency in Few-Step Generative Modeling

## Quick Facts
- arXiv ID: 2507.16884
- Source URL: https://arxiv.org/abs/2507.16884
- Reference count: 34
- Primary result: 1-step SplitMeanFlow achieves statistical parity with 10-step baselines

## Executive Summary
SplitMeanFlow introduces a novel algebraic framework for few-step generative modeling based on Interval Splitting Consistency, a self-referential identity for average velocity fields across time intervals. Unlike diffusion models and flow matching that rely on iterative sampling, SplitMeanFlow leverages definite integral additivity to eliminate expensive Jacobian-vector product computations, resulting in simpler implementation and faster inference. The method has been deployed in large-scale speech synthesis products, achieving up to 20x speedups while maintaining quality comparable to 10-step baselines with only 2 sampling steps.

## Method Summary
SplitMeanFlow addresses the computational bottleneck of iterative sampling in generative models by introducing Interval Splitting Consistency, an algebraic identity that relates average velocity fields across different time intervals without requiring differential operators. This framework generalizes MeanFlow, with MeanFlow's differential identity recovered as a limiting case. By eliminating the need for Jacobian-vector product computations, SplitMeanFlow achieves simpler implementation, more stable training, and broader hardware compatibility. The approach has been successfully applied to speech synthesis, demonstrating that 2-step generation can match 10-step baseline performance, with 1-step generation achieving statistical parity.

## Key Results
- 2-step SplitMeanFlow achieves performance comparable to 10-step Flow Matching baselines in audio generation
- 5x reduction in sampling steps while maintaining quality without Classifier-Free Guidance
- 1-step SplitMeanFlow achieves statistical parity with 10-step baseline (20x reduction in computational cost)
- Successfully deployed in Doubao speech synthesis with 20x speedup in production

## Why This Works (Mechanism)
SplitMeanFlow works by exploiting the additivity property of definite integrals to establish a self-referential relationship for average velocity fields across time intervals. This algebraic approach eliminates the need for differential operators and expensive Jacobian-vector product computations that are computationally intensive in traditional flow matching and diffusion models. The Interval Splitting Consistency identity provides a more general foundation than MeanFlow, allowing for efficient few-step sampling while maintaining generation quality.

## Foundational Learning

- **Interval Splitting Consistency**: Self-referential algebraic identity for average velocity fields
  - *Why needed*: Eliminates expensive differential operator computations in generative modeling
  - *Quick check*: Verify the algebraic identity holds across different interval partitions

- **Definite integral additivity**: Mathematical property that enables the core framework
  - *Why needed*: Provides the foundation for relating velocity fields across time intervals
  - *Quick check*: Confirm additivity holds for simple test functions before applying to velocity fields

- **Jacobian-vector products (JVP)**: Computationally expensive operation eliminated by SplitMeanFlow
  - *Why needed*: Understanding what's removed helps appreciate the efficiency gains
  - *Quick check*: Benchmark JVP computation time versus alternative approaches

## Architecture Onboarding

**Component map**: Input data -> Interval Splitting Consistency layer -> Velocity field estimation -> Sample generation

**Critical path**: Data preprocessing → Interval partitioning → Velocity field computation → Sample synthesis

**Design tradeoffs**: Eliminates JVP computations for speed vs. potential accuracy trade-offs in extreme few-step scenarios

**Failure signatures**: Quality degradation in very few steps (1-2) for complex data distributions; potential instability in training compared to MeanFlow despite theoretical advantages

**First experiments**:
1. Verify Interval Splitting Consistency identity holds across various interval partitions on synthetic data
2. Compare training stability and convergence speed against MeanFlow baseline
3. Benchmark inference speed on different hardware platforms (CPU, GPU, specialized accelerators)

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Computational efficiency claims (20x speedup) may not fully align with experimental results showing 5x step reduction
- 1-step generation claims require more rigorous statistical validation beyond anecdotal evidence
- Limited systematic comparison of training stability across different architectures and datasets

## Confidence

- **High confidence**: Mathematical framework of Interval Splitting Consistency is sound; elimination of JVP computations is a concrete technical contribution
- **Medium confidence**: Claims about more stable training than MeanFlow are theoretically supported but need systematic ablation studies
- **Low confidence**: Extreme claims about 1-step generation achieving full parity with 10-step baselines need more rigorous validation

## Next Checks

1. Conduct perceptual studies comparing 1-step SplitMeanFlow outputs against 10-step baselines using crowd-sourced MOS evaluations across multiple domains (not just audio)

2. Perform ablation studies isolating the impact of eliminating JVP computations versus other architectural changes on training stability and convergence speed

3. Benchmark computational efficiency across different hardware platforms (CPU, GPU, specialized accelerators) to verify the claimed 20x speedup translates to real-world inference scenarios beyond the specific deployment environment