---
ver: rpa2
title: 'GerAV: Towards New Heights in German Authorship Verification using Fine-Tuned
  LLMs on a New Benchmark'
arxiv_id: '2601.13711'
source_url: https://arxiv.org/abs/2601.13711
tags:
- reddit
- dataset
- german
- data
- llama-3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GerAV, a new benchmark for German authorship
  verification (AV) with over 600,000 labeled text pairs sourced from Reddit and Twitter.
  The benchmark supports in-domain, cross-domain, profile-based, and mixed-source
  evaluations, enabling controlled analysis of data source, domain, and text length
  effects.
---

# GerAV: Towards New Heights in German Authorship Verification using Fine-Tuned LLMs on a New Benchmark

## Quick Facts
- arXiv ID: 2601.13711
- Source URL: https://arxiv.org/abs/2601.13711
- Reference count: 40
- Best approach: Fine-tuned Gemma-3-12B achieves F1=0.83, outperforming baselines by 0.09 points

## Executive Summary
This paper introduces GerAV, a comprehensive German authorship verification benchmark with over 600,000 labeled text pairs from Reddit and Twitter. The benchmark enables controlled analysis across multiple dimensions including data source, domain, and text length. Through extensive evaluation of traditional baselines and large language models, the authors demonstrate that fine-tuned LLMs significantly outperform existing approaches, with their best model achieving 0.83 F1-score. The study reveals a critical trade-off between specialization and generalization, showing that mixed-source training produces more robust cross-domain performance than single-source specialization.

## Method Summary
The authors fine-tune instruction-tuned LLMs (Gemma-3-12B-it, Llama-3.1-8B-instruct, Llama-3.2-3B-instruct, Qwen-2.5-7B-instruct) using LoRA with specific hyperparameters (batch_size=2, epochs=1, lr=3e-4, weight_decay=0.01, lora_r=128, lora_alpha=32, lora_dropout=0.0). Models are trained on H100 GPUs using the GerAV benchmark, which provides data from Reddit (in-domain, cross-domain, profile-based) and Twitter sources. Prediction scores are calculated as the difference between "yes" and "no" token probabilities over 10 most likely tokens, with a threshold of 0 for fine-tuned models.

## Key Results
- Fine-tuned Gemma-3-12B achieves 0.83 F1-score, outperforming best baseline by 0.09 points
- Mixed-source training produces more robust cross-domain generalization than specialized models
- Text length shows strong positive correlation with performance (up to ρ=0.94)
- Fine-tuned LLMs clearly outperform feature-based and neural baselines across all evaluation regimes

## Why This Works (Mechanism)

### Mechanism 1: Mixed-Source Training Enhances Cross-Domain Generalization
- Claim: Training on mixed data sources yields more robust generalization across evaluation regimes
- Mechanism: Exposure to heterogeneous stylistic signals during fine-tuning forces models to learn representations less reliant on dataset-specific artifacts
- Core assumption: Improved performance is causally linked to stylistic variability, not just larger training set size
- Evidence anchors: Strong performance of mixed dataset models; trade-off between specialization and generalization
- Break condition: Generalization benefit disappears if mixed dataset is simply larger; needs controlled comparison

### Mechanism 2: LLM Fine-tuning Surpasses Feature-Based and Neural Baselines via Learned Stylistic Representations
- Claim: Fine-tuned LLMs produce superior performance compared to traditional methods
- Mechanism: LLMs' pre-trained knowledge is repurposed to specifically encode stylistic similarities
- Core assumption: Pre-trained knowledge is transferable and beneficial for stylistic nuances
- Evidence anchors: Clear performance improvement over baselines; correlation with model size
- Break condition: LLM that performs poorly on language modeling but succeeds at AV would contradict mechanism

### Mechanism 3: Text Length Positively Correlates with AV Performance
- Claim: More text available for analysis improves AV model performance
- Mechanism: Longer texts contain more stylistic evidence (lexical choice, syntactic patterns)
- Core assumption: Stylistic signal is additive and consistent across text length
- Evidence anchors: High Spearman correlations (up to ρ=0.94) between message length and F1-score
- Break condition: Model performing equally well on very short and very long texts would invalidate mechanism

## Foundational Learning

- **Authorship Verification (AV)**: Binary classification task determining if two texts were written by the same person. Needed because it's the core task distinct from Authorship Attribution. Quick check: Given two anonymous forum posts, would you use AV or AA to determine if they come from the same user?

- **Fine-tuning with LoRAs**: Parameter-efficient adaptation of LLMs to specific tasks. Needed because authors use LoRAs to adapt LLMs to AV task. Quick check: How does fine-tuning with LoRAs differ from full model fine-tuning in terms of parameters trained?

- **Cross-Domain vs. In-Domain Evaluation**: Measuring model performance when training and test data come from same or different domains. Needed because systematic evaluation of generalization is a key contribution. Quick check: If model is trained on political discussions, would evaluating it on cooking forums be in-domain or cross-domain test?

## Architecture Onboarding

- **Component map**: Raw German Twitter/Reddit data → Text pair preprocessing → LLM (Gemma/Llama/Qwen) → Binary prompt formatting → Prediction score calculation → Threshold comparison → Binary classification

- **Critical path**: 1) Load pre-trained LLM, 2) Prepare text pairs in prompt format, 3) (Optional) Fine-tune with LoRAs, 4) Feed text pair for inference, 5) Calculate prediction scores, 6) Apply threshold for final decision

- **Design tradeoffs**: 
  - Specialization vs. Generalization: Single-source training yields peak performance but poor cross-domain generalization; mixed training sacrifices some peak performance for better average performance
  - Model Size vs. Cost: Performance correlates with size (Gemma-3-12B best) but smaller models are faster/cheaper
  - Data Length vs. Availability: Performance increases with length but long texts may not be available; profile-based approach increases length at cost of requiring more data

- **Failure signatures**:
  - Poor cross-domain performance when trained on specific dataset but tested on different source
  - Strong length dependence with accuracy increasing monotonically with text length
  - Overfitting to source artifacts (platform-specific formatting/jargon instead of authorial style)

- **First 3 experiments**:
  1. Establish zero-shot baseline: Evaluate pre-trained LLM on GerAV test sets without training
  2. Fine-tune on single source: Train on one dataset (e.g., Reddit in-domain) and evaluate on corresponding and other test sets to measure trade-off
  3. Fine-tune on mixed data: Train on mixed dataset and compare average performance across all test sets to single-source model

## Open Questions the Paper Calls Out

1. **Interpretability Integration**: How can natural language explanations be integrated into fine-tuned AV frameworks to satisfy forensic interpretability requirements without degrading performance? The current fine-tuned models act as black boxes, prioritizing accuracy over transparency needed for high-stakes applications.

2. **Topical vs. Stylistic Cues**: To what extent do fine-tuned models rely on topical content cues rather than stylistic features to determine authorship? The current benchmark construction does not explicitly control for topic leakage, leaving the actual learning mechanism ambiguous.

3. **Language-Aligned Prompting**: Does translating linguistically informed prompts (LIP) into German improve zero-shot performance over English prompts? The experiment only tested English prompts on German text, leaving potential benefit of language-aligned prompting unverified.

4. **Human Expert Comparison**: How does fine-tuned Gemma-3-12B performance compare to human expert judgment on GerAV benchmark? The paper establishes machine benchmark but does not contextualize 0.83 F1-score against human capability in same scenarios.

## Limitations

- Dataset not yet publicly released, preventing independent reproduction
- Evaluation focused on German social media text limits generalizability to other languages/domains
- Strong performance correlation with text length suggests potential limitations for short message applications

## Confidence

**High Confidence**: Fine-tuned LLMs outperform traditional baselines in German AV (supported by systematic evaluation across multiple models and regimes with consistent performance differences)

**Medium Confidence**: Mixed-source training produces more robust cross-domain performance (experimental evidence shows improvement but doesn't rule out alternative explanations like dataset size)

**Medium Confidence**: GerAV represents largest German AV benchmark (paper provides figure but lacks detailed contextualization against other benchmarks)

## Next Checks

1. Upon dataset availability, verify author partitioning strategy prevents leakage between training, validation, and test splits, and confirm negative pair construction maintains similar length distributions

2. Conduct controlled experiment comparing fine-tuning on mixed data versus single-source data matched for total training examples to isolate whether generalization benefit stems from stylistic variability or increased volume

3. Systematically evaluate model performance across text length thresholds (25, 50, 100, 200 words) to identify minimum viable text length for reliable verification