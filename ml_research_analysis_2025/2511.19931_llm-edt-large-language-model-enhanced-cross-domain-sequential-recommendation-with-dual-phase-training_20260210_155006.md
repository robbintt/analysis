---
ver: rpa2
title: 'LLM-EDT: Large Language Model Enhanced Cross-domain Sequential Recommendation
  with Dual-phase Training'
arxiv_id: '2511.19931'
source_url: https://arxiv.org/abs/2511.19931
tags:
- domain
- user
- llms
- item
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes LLM-EDT, a large language model-enhanced cross-domain
  sequential recommendation framework. It addresses data imbalance and transition
  issues in CDSR through three key innovations: a transferable item augmenter that
  generates domain-balanced interaction sequences, a dual-phase training framework
  that pre-trains on mixed sequences then fine-tunes for domain-specific features,
  and a domain-aware profiling module that generates fine-grained user preferences.'
---

# LLM-EDT: Large Language Model Enhanced Cross-domain Sequential Recommendation with Dual-phase Training

## Quick Facts
- arXiv ID: 2511.19931
- Source URL: https://arxiv.org/abs/2511.19931
- Reference count: 40
- Large language model-enhanced cross-domain sequential recommendation framework that achieves up to 26.96% improvement on Hit@10 over state-of-the-art baselines

## Executive Summary
LLM-EDT addresses three fundamental challenges in cross-domain sequential recommendation: data imbalance, transition issues, and rough profiling. The framework introduces a transferable item augmenter that generates semantically aligned "bridge" items to balance domain distributions, a dual-phase training approach that separates global pattern learning from domain-specific fine-tuning, and a domain-aware profiling module that creates fine-grained user preference representations through clustered sub-preference analysis.

## Method Summary
LLM-EDT employs a three-pronged approach to cross-domain sequential recommendation. First, it uses a transferable item augmenter that clusters items via K-means, generates cross-domain candidates using LLMs, filters them with cosine similarity, and strategically inserts them into sequences to create semantic bridges. Second, it implements dual-phase training: global pre-training on mixed sequences followed by domain-specific fine-tuning with frozen backbones and lightweight adapters. Third, it uses domain-aware profiling that clusters user histories, summarizes sub-sequences with LLMs using a "Summarize-Reform-Analyze" pipeline, and aligns textual profiles with behavioral embeddings via contrastive loss. The entire system is designed for efficiency through cached LLM embeddings and minimal online computation.

## Key Results
- Achieves up to 26.96% improvement on Hit@10 metric compared to state-of-the-art baselines
- Demonstrates superior handling of cold-start scenarios through enriched interaction sequences
- Maintains efficiency through cached LLM embeddings and lightweight adapter architecture
- Shows consistent performance across three public Amazon datasets (Cloth-Sport, Electronic-Cell Phone, Food-Kitchen)

## Why This Works (Mechanism)

### Mechanism 1: Transferable Item Augmenter
- **Claim**: Generates domain-balanced interaction sequences by inserting semantically aligned "bridge" items into sparse domain sequences, reducing the distribution gap between dominant and non-dominant domains.
- **Core assumption**: Semantic similarity in LLM embedding space correlates with behavioral transferability.
- **Evidence anchors**: Abstract states "transferable item augmenter that generates domain-balanced interaction sequences"; Section 3.2.3 describes inserting filtered samples "right after the corresponding item... to construct a bridge."
- **Break condition**: If source and target domains share no semantic overlap, generated items may be hallucinations passing the noise filter.

### Mechanism 2: Dual-phase Training
- **Claim**: Addresses transition issues by separating global cross-domain pattern learning from local domain-specific feature learning, preventing dominant domain from overwhelming optimization.
- **Core assumption**: Domain-shared features should be learned first to provide stable initialization for domain-specific fine-tuning (Global → Local).
- **Evidence anchors**: Abstract mentions "dual-phase training framework that pre-trains on mixed sequences then fine-tunes for domain-specific features"; Table 2 ablation shows performance drop when DFT is removed.
- **Break condition**: If global pre-training overfits to dominant domain, frozen weights may propagate this bias to fine-tuning stage.

### Mechanism 3: Domain-aware Profiling
- **Claim**: Addresses rough profiling by decomposing user history into clustered sub-preferences before aggregation, resulting in fine-grained textual profiles that align better with collaborative signals.
- **Core assumption**: Clustering reveals distinct "sub-preferences" lost in single long-context summarization, and LLMs can effectively reweight these based on explicit domain ratio instructions.
- **Evidence anchors**: Section 3.4.2 describes leveraging domain ratio to balance summaries; Table 8 shows performance drop without S-R-A pipeline.
- **Break condition**: If K-means clustering fails to separate preferences meaningfully, resulting sub-preference summaries may be contradictory or noisy.

## Foundational Learning

- **Concept**: Cross-Domain Sequential Recommendation (CDSR) & The Overlap Dilemma
  - **Why needed here**: Paper assumes familiarity with difficulty of transferring knowledge between domains when user interactions are imbalanced or non-overlapping.
  - **Quick check question**: Why is simply concatenating interaction sequences from two different domains often insufficient for training a recommender?

- **Concept**: LLM as Augmenter vs. Encoder
  - **Why needed here**: Framework dually uses LLMs to generate synthetic data points (Augmenter) and represent existing items/users (Encoder).
  - **Quick check question**: In this architecture, does the LLM generate final ranking score or produce representations and candidate items for downstream SASRec model?

- **Concept**: Adapter-based Parameter-Efficient Transfer
  - **Why needed here**: Dual-phase Training relies on freezing SASRec backbone and training small adapters to retain global knowledge while adapting to specific domains.
  - **Quick check question**: During Domain Fine-tuning phase, which parameters are updated and which are frozen?

## Architecture Onboarding

- **Component map**:
  1. Data Prep: Raw Sequence → K-means Cluster → LLM Generator → Noise Filter → Strategic Insertion → Augmented Sequence
  2. Global Phase: Augmented Sequence → LLM Embedder (Frozen) → Adapter → SASRec (Trainable) → Global User Rep.
  3. Domain Phase: Split Sequences → Frozen SASRec → Domain Adapters (Trainable) → Domain User Rep.
  4. Profiling: Clusters → Summarize-Reform-Analyze → Alignment Loss
  5. Inference: Cached Embeddings + Trained Adapters/Backbone → Probability Score

- **Critical path**: The Insertion Strategy (Section 3.2.3) is critical. Placing generated cross-domain item immediately after source domain item creates the "bridge." Random insertion degrades performance significantly.

- **Design tradeoffs**: Trades offline computational cost (LLM generation and embedding) for online inference efficiency. Caching LLM embeddings and generated profiles ensures serving model consists only of lightweight Adapter + SASRec.

- **Failure signatures**:
  - Semantic Drift: Generated items are grammatically correct but factually irrelevant
  - Dominant Domain Collapse: If adapter learning rate is too high in Phase 2, model might overfit to fine-tuning domain
  - Profiling Mismatch: Alignment loss fails to converge, indicating text profile doesn't match behavioral embedding

- **First 3 experiments**:
  1. Verify Transferability: Replicate Figure 3 with t-SNE plots of item embeddings before and after augmentation to check if bridge items reduce gap between clusters
  2. Ablate Insertion: Compare baseline where generated items are appended to end vs. proposed "insert after similar item" method
  3. Cold Start Stress Test: Filter dataset to users with <5 interactions in target domain and compare LLM-EDT against non-LLM baseline

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can LLM-EDT framework be effectively extended to scenarios involving three or more domains without fundamental reformulation of domain ratio and insertion mechanisms?
- **Basis**: Preliminary section defines problem formulation and domain ratio strictly for two-domain situation, and all experiments use binary domain pairs
- **Why unresolved**: Insertion mechanism and ratio calculation are designed to bridge gap between two specific entities; scaling to N domains would require redefining bridge logic
- **What evidence would resolve it**: Theoretical extension for N-domains and experimental results on datasets with three or more distinct interaction domains

### Open Question 2
- **Question**: How does performance of Transferable Item Augmenter degrade when applied to domain pairs with significantly larger semantic gaps compared to related sub-categories tested?
- **Basis**: Experiments use domain pairs from Amazon dataset that are semantically proximate or complementary
- **Why unresolved**: Paper doesn't evaluate imbalance issue or quality of LLM-generated augmentations on disparate domain pairs where user preferences might not transfer linearly
- **What evidence would resolve it**: Ablation studies on cross-domain datasets with varying degrees of semantic overlap or distance

### Open Question 3
- **Question**: Does strategy of caching LLM embeddings and user profiles to maintain efficiency hinder model's ability to capture short-term, real-time preference shifts during inference?
- **Basis**: Section 3.1 and 3.5.2 explicitly claim LLM-EDT maintains efficiency by ensuring "LLM embeddings and generated user embeddings can be cached in advance"
- **Why unresolved**: Caching freezes semantic representation of items and user's profile at time of generation, potentially failing to adapt to rapid changes in user intent
- **What evidence would resolve it**: Temporal analysis evaluating model's performance decay over time intervals between cache updates

## Limitations
- Reliance on manual threshold tuning (cosine similarity, cluster counts) and domain-specific prompt engineering suggests limited adaptability to truly dissimilar domains
- Dual-phase training assumes domain-shared knowledge is learnable in single pre-training phase, which may not hold for domains with fundamentally different interaction patterns
- Effectiveness of "Summarize-Reform-Analyze" profiling pipeline depends heavily on LLM instruction-following quality, which varies significantly across models and domains

## Confidence

- **High**: Dual-phase training framework and adapter-based parameter efficiency are well-established techniques with consistent experimental support
- **Medium**: Transferability assumption underlying augmenter (semantic similarity → behavioral similarity) is plausible but not empirically validated beyond cosine thresholds
- **Low**: Effectiveness of profiling pipeline depends heavily on LLM instruction-following quality, which varies significantly

## Next Checks

1. **Domain Similarity Stress Test**: Evaluate on domains with minimal semantic overlap (e.g., Books vs. Electronics) to test augmenter's noise-filtering robustness when bridges are weak
2. **Parameter Sensitivity Analysis**: Systematically vary noise threshold τ, cluster count K, and adapter learning rates to quantify their impact on performance-variance tradeoff
3. **Offline Cost-Benefit Audit**: Measure exact computational overhead of LLM profiling generation versus online efficiency gains from cached embeddings to validate claimed efficiency advantage