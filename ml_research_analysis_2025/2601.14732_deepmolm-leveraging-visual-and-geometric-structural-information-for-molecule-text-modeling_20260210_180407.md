---
ver: rpa2
title: 'DeepMoLM: Leveraging Visual and Geometric Structural Information for Molecule-Text
  Modeling'
arxiv_id: '2601.14732'
source_url: https://arxiv.org/abs/2601.14732
tags:
- molecular
- deepmolm
- arxiv
- language
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeepMoLM addresses the challenge of interpreting molecular images
  and generating outputs consistent with 3D geometry and stereochemistry for drug
  discovery and chemical literature mining. The method integrates high-resolution
  molecular images with geometric invariants derived from molecular conformations
  using a dual-view framework.
---

# DeepMoLM: Leveraging Visual and Geometric Structural Information for Molecule-Text Modeling

## Quick Facts
- **arXiv ID:** 2601.14732
- **Source URL:** https://arxiv.org/abs/2601.14732
- **Reference count:** 40
- **Primary result:** 12.3% relative METEOR gain on PubChem captioning over strongest generalist baseline

## Executive Summary
DeepMoLM is a dual-view framework that combines high-resolution molecular images with conformer-derived geometric invariants for molecule-text modeling. The method processes 1024×1024 images through a hierarchical DeepEncoder, encodes 3D conformations as discrete Extended 3-Dimensional Fingerprints (E3FP), and fuses visual and geometric streams via cross-attention. This approach achieves state-of-the-art performance on molecular captioning, property prediction, and description generation tasks while maintaining chemical validity and stereochemical accuracy.

## Method Summary
DeepMoLM processes molecular images and conformer coordinates through a two-stage training pipeline. First, a SAM-Base local encoder captures fine stereochemical detail while a CLIP-Large global encoder provides structural context; both are compressed to 256 tokens and concatenated. Conformer neighborhoods are encoded as E3FP fingerprints and aligned with SELFIES tokens. A cross-attention fusion projector grounds visual tokens in geometric invariants, which are then decoded by a Qwen2-VL-7B-Instruct model. The system achieves 1024×1024 resolution processing through efficient token compression and demonstrates superior performance on captioning and property prediction tasks.

## Key Results
- 12.3% relative METEOR gain on PubChem captioning over strongest generalist baseline
- Valid numeric outputs for all property queries with MAE 13.64 g/mol on Molecular Weight
- Matches state-of-the-art vision-language models on ChEBI-20 description generation from images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Processing molecular images at 1024×1024 resolution preserves fine-grained stereochemical markers that standard vision-language models typically miss at lower resolutions.
- Mechanism: A hierarchical dual-pathway DeepEncoder extracts multi-scale features: a local pathway (SAM-Base with window attention) captures localized high-frequency cues like stereobonds; convolutional token compression reduces 4096 tokens to 256 while preserving spatial structure; a global pathway (CLIP-Large) adds long-range context for rings and functional groups. Local and global features are concatenated (256×2048) before fusion.
- Core assumption: Stereochemical details (wedge bonds, hash bonds, ring closures) are sparse, localized signals that require high spatial fidelity to discriminate.
- Evidence anchors:
  - [abstract]: "preserves high-frequency evidence from 1024×1024 inputs"
  - [section 3.2]: "DeepEncoder... enables efficient 1024×1024 processing through convolutional token compression... The local pathway captures fine stereochemical detail, and the global pathway provides structural coherence"
  - [corpus]: Weak/missing — related papers focus on alignment strategies rather than resolution-preserving architectures.
- Break condition: If downstream tasks do not require stereochemical discrimination (e.g., coarse property prediction), the dual-pathway overhead may not justify gains.

### Mechanism 2
- Claim: Cross-attention fusion of visual tokens with discrete 3D geometric fingerprints enables physically grounded generation without requiring explicit atom coordinates at inference.
- Mechanism: Visual tokens (H_vis) serve as queries; structural tokens (S_m from E3FP + SELFIES) serve as keys and values in a multi-head cross-attention block. This grounds each visual token in geometric invariants before the decoder, rather than relying on late fusion via lightweight adapters.
- Core assumption: Explicit cross-modal interaction via attention is necessary to prevent "shortcut alignment" where models track coarse semantics while neglecting stereochemical constraints.
- Evidence anchors:
  - [abstract]: "fuses visual and geometric streams with cross-attention, enabling physically grounded generation without atom coordinates"
  - [section 4.5 ablation]: "Replacing the fusion projector with concatenation performed worse than the plain non-pretrained model, with the largest losses on BLEU-2 and METEOR"
  - [corpus]: MV-CLAM and RTMol both emphasize cross-modal projection/alignment, but do not specifically validate cross-attention over concatenation for geometric grounding.
- Break condition: If geometric tokens are misaligned with visual regions (e.g., incorrect SELFIES-to-atom mapping), cross-attention may amplify noise rather than reduce it.

### Mechanism 3
- Claim: Discretizing conformer neighborhoods as Extended 3-Dimensional Fingerprints (E3FP) provides SE(3)-invariant geometric grounding compatible with language model tokenization.
- Mechanism: For each heavy atom, E3FP iteratively aggregates neighbor information within expanding radii (r·j), incorporating connectivity and stereochemistry, then hashes the aggregate via MurmurHash3 and maps to a discrete vocabulary via modulo |F|. These atom-level 3D token tuples are aligned with SELFIES positions and fused with 1D topology embeddings.
- Assumption: Discrete hashing preserves sufficient geometric invariance (rotation/translation equivariance, stereochemical distinction) for the model to learn stereochemically valid generation.
- Evidence anchors:
  - [abstract]: "encodes conformer neighborhoods as discrete Extended 3-Dimensional Fingerprints"
  - [section 3.1.2]: Full specification of radius-based neighbor aggregation and hashing; "E3FP provides 3D descriptors indexed by heavy atoms that reflect molecular conformation"
  - [section 4.5 ablation]: "removing the 3D fingerprint yielded the weakest ROUGE-L, suggesting degraded semantic fidelity"
  - [corpus]: Clifford Group Equivariant Diffusion discusses geometric invariants via Clifford algebra; TRIDENT uses taxonomic annotations. No direct corpus evidence validates E3FP specifically.
- Break condition: If conformer generation is unreliable or absent, E3FP may encode incorrect geometry, leading to spurious grounding.

## Foundational Learning

- Concept: **Cross-Attention for Multimodal Fusion**
  - Why needed here: DeepMoLM uses visual tokens as queries and geometric tokens as keys/values; understanding this asymmetry is essential for debugging fusion failures.
  - Quick check question: Given query matrix Q (visual) and key matrix K (geometric), what does the attention matrix A represent in terms of modality grounding?

- Concept: **Molecular String Representations (SMILES vs. SELFIES)**
  - Why needed here: SELFIES provides the discrete topological backbone T_m^ (1D); its robustness affects token alignment with E3FP.
  - Quick check question: Why is SELFIES considered more robust than SMILES for generative modeling, and how does this affect the atom-position index set A?

- Concept: **Vision Transformer Token Compression**
  - Why needed here: DeepMoLM processes 4096 initial tokens and compresses to 256; understanding compression fidelity vs. information loss is critical.
  - Quick check question: If convolutional token compression (stride 2, 3×3 kernel) is applied twice to a 64×64 grid, what is the output spatial dimension and receptive field growth?

## Architecture Onboarding

- Component map: 1024×1024×3 molecular image (I_m) + conformer coordinates (X_m) → DeepEncoder → H_vis → Fusion Projector (cross-attend to S_m) → H_fused → Decoder → output tokens

- Critical path: I_m → DeepEncoder → H_vis → Fusion Projector (cross-attend to S_m) → H_fused → Decoder → output tokens. If cross-attention weights do not attend to valid structural tokens (padding mask errors), grounding fails.

- Design tradeoffs:
  - Frozen DeepEncoder vs. end-to-end: Encoder frozen for stability; only Fusion Projector + Decoder trained. Limits visual adaptation but reduces compute.
  - Discrete E3FP vs. continuous 3D embeddings: Discretization enables language model compatibility but may lose fine geometric precision.
  - Two-stage training: Stage-1 aligns projector only; Stage-2 adds instruction tuning. Ablation shows pre-training is essential for performance.

- Failure signatures:
  - No pre-training: >10 METEOR point drop; model fails to align modalities.
  - Concatenation fusion: Worse than linear projection baseline; indicates cross-attention is not optional.
  - Missing E3FP: Lowest ROUGE-L; semantic fidelity degrades despite visual input.
  - Padding mask errors: Attention attends to -∞ positions; outputs become incoherent.

- First 3 experiments:
  1. Reproduce ablation without pre-training on PubChem captioning (10 epochs, lr=5e-5); expect BLEU-2 ~18-19 vs. 33-35 with pre-training.
  2. Replace cross-attention with feature concatenation in Fusion Projector; expect METEOR drop >8 points, confirming mechanism 2.
  3. Zero out E3FP branch (set e_t^(3D)=0); evaluate on stereochemically sensitive tasks; expect ROUGE-L degradation per ablation table.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DeepMoLM perform when high-quality 3D conformers are unavailable or computationally infeasible to generate during training?
- Basis in paper: [inferred] The E3FP fingerprint computation requires conformer coordinates Xm, but the paper claims "physically grounded generation without atom coordinates" only at inference. Training dependency on conformer quality is not analyzed.
- Why unresolved: No experiments test robustness to missing, approximate, or low-quality conformers during the E3FP encoding stage.
- What evidence would resolve it: Ablation experiments varying conformer generation methods (RDKit ETKDG vs. MMFF vs. random coordinates) and measuring downstream captioning/property prediction degradation.

### Open Question 2
- Question: How robust is DeepMoLM to noisy, low-resolution, or stylistically diverse molecular images from real scientific literature?
- Basis in paper: [explicit] The paper states: "stereochemistry remains uncertain when figures are noisy, of poor quality, or drawn in uncommon styles." Evaluation uses standardized PubChem/ChEBI images, not real literature scans.
- Why unresolved: No experiments assess performance degradation under image corruptions, resolution changes, or alternative drawing conventions.
- What evidence would resolve it: Benchmarking on corrupted images (Gaussian noise, compression artifacts) and diverse rendering styles (ChemDraw, MarvinSketch, hand-drawn figures).

### Open Question 3
- Question: What is the rate and nature of factual hallucinations in DeepMoLM's molecular descriptions, particularly regarding stereochemistry and substructure counts?
- Basis in paper: [inferred] Table 4 case studies show discrepancies (DeepMoLM outputs "tetrasaccharide" while ground truth states "trisaccharide"), suggesting potential factual errors not captured by n-gram metrics.
- Why unresolved: Standard metrics (BLEU, ROUGE, METEOR) measure lexical overlap but not factual accuracy or chemical validity.
- What evidence would resolve it: Expert annotation of generated captions for factual correctness, stereochemical accuracy, and hallucination rate across a held-out test set.

### Open Question 4
- Question: Does the cross-attention fusion mechanism scale to larger or more complex molecules beyond drug-like compounds?
- Basis in paper: [inferred] Experiments are limited to PubChem and ChEBI-20 datasets containing small-to-medium drug-like molecules. No evaluation on polymers, proteins, organometallics, or large natural products.
- Why unresolved: Token budget (Nv=256) and SELFIES length constraints may limit applicability to larger molecular structures.
- What evidence would resolve it: Evaluation on macromolecular datasets (polymers, peptides) measuring performance vs. molecular size and analyzing token saturation effects.

## Limitations

- Critical hyperparameters for E3FP (K, r, |F|, Lmax) and DeepEncoder configuration (Nh, dff) are not specified, preventing exact reproduction
- Performance heavily depends on conformer quality, but no analysis of robustness to missing or low-quality conformers is provided
- Evaluation limited to standardized molecular images; no assessment of robustness to real-world literature scans or diverse drawing styles

## Confidence

**High Confidence**: The ablation study results showing that cross-attention fusion outperforms concatenation (8+ point METEOR drop) and that Stage-1 pre-training is essential for strong downstream performance (>10 point METEOR drop without it). The quantitative results on PubChem captioning and property prediction are well-documented and reproducible.

**Medium Confidence**: The architectural claims about dual-pathway DeepEncoder preserving stereochemical detail at 1024×1024 resolution, and the E3FP-based geometric grounding mechanism. While the general approach is sound, the lack of hyperparameter specifications for E3FP and the dependence on conformer quality introduce uncertainty.

**Low Confidence**: The claim that DeepMoLM matches state-of-the-art vision-language models on ChEBI-20 description generation, as the specific baselines and comparison metrics are not fully detailed in the provided context.

## Next Checks

1. **Reproduce E3FP hyperparameter sensitivity**: Systematically vary K (iterations), r (radius), and |F| (hash vocabulary size) to determine their impact on stereochemical accuracy in generated captions. This will establish the sensitivity of geometric grounding to fingerprint configuration.

2. **Validate cross-attention grounding**: Implement gradient-based visualization to show which E3FP tokens the cross-attention attends to for specific stereochemical features (e.g., wedge bonds, ring closures). This will confirm whether visual tokens are properly grounded in geometric invariants.

3. **Stress-test conformer dependency**: Generate molecular images with known stereochemical configurations and systematically corrupt conformer geometries (random rotations, bond length distortions) to measure degradation in captioning accuracy. This will quantify the reliability of E3FP-based grounding under imperfect 3D representations.