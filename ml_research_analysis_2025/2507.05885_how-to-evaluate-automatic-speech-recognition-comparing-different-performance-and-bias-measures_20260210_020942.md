---
ver: rpa2
title: 'How to Evaluate Automatic Speech Recognition: Comparing Different Performance
  and Bias Measures'
arxiv_id: '2507.05885'
source_url: https://arxiv.org/abs/2507.05885
tags:
- bias
- speech
- performance
- groups
- speaker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates bias in automatic speech recognition (ASR)
  systems against diverse speaker groups such as children, teenagers, non-native speakers,
  and older adults. It compares various performance and bias measures to evaluate
  state-of-the-art end-to-end ASR models for Dutch, both before and after applying
  bias mitigation strategies.
---

# How to Evaluate Automatic Speech Recognition: Comparing Different Performance and Bias Measures

## Quick Facts
- arXiv ID: 2507.05885
- Source URL: https://arxiv.org/abs/2507.05885
- Reference count: 0
- Primary result: Averaged error rates alone are insufficient for evaluating ASR performance; median and standard deviation should supplement them, and bias should be reported using relative measures.

## Executive Summary
This paper investigates bias in automatic speech recognition (ASR) systems against diverse speaker groups including children, teenagers, non-native speakers, and older adults. The study compares various performance and bias measures to evaluate state-of-the-art end-to-end ASR models for Dutch, both before and after applying bias mitigation strategies. Results show that averaged error rates alone mask disparities across speaker groups, with significant performance variation within groups and persistent bias against non-native speakers despite mitigation efforts.

## Method Summary
The study evaluates Dutch ASR systems using the Corpus Gesproken Nederlands (CGN) for training and both CGN and Jasmin-CGN for testing across diverse speaker groups. Two model architectures are examined: a Conformer trained from scratch and OpenAI-Whisper small fine-tuned on CGN data. Three augmentation conditions are tested: no augmentation, speed perturbation, and speed perturbation plus SpecAugment. Performance is measured using WER, with analysis including median, standard deviation, and relative difference bias measures comparing each group to the best-performing group.

## Key Results
- Averaged error rates alone are insufficient and should be supplemented by median and standard deviation to capture performance across diverse speaker groups
- Significant performance variation exists within speaker groups, with high standard deviation masking disparities
- Data augmentation strategies that improve average performance can inadvertently increase relative bias against non-native speakers
- Relative difference bias measures better capture performance disparity than absolute differences when base error rates vary significantly

## Why This Works (Mechanism)

### Mechanism 1
Reporting median and standard deviation of error rates provides a more robust representation of system performance across diverse speakers than the arithmetic mean. The distribution of Word Error Rates across speaker groups is often skewed rather than normal, with the mean sensitive to extreme values that mask the typical user experience. The median provides a central tendency resistant to outliers, while Stdev quantifies the dispersion of performance, signaling inconsistency in how the model treats different demographics.

### Mechanism 2
Relative difference bias measures capture the severity of performance disparity better than absolute differences, particularly when base error rates vary significantly. An absolute WER gap of 10% represents a ten-fold increase in relative error for a group with a base 1% WER compared to a group with a 10% base WER. Relative measures normalize this disparity, ensuring that improvements in high-error groups are not masked by the scale of the error.

### Mechanism 3
Data augmentation strategies optimized for average performance can inadvertently increase relative bias against non-native or diverse speakers. Augmentations trained primarily on "norm" native data may improve the model's robustness to variations within the native cluster while failing to generalize to the acoustics of non-native speakers. This shifts the decision boundary in a way that benefits the majority, widening the relative gap to minority groups even if their absolute WER drops slightly.

## Foundational Learning

- **Word Error Rate (WER) Distribution Skew**
  - Why needed here: The paper argues against using a single average score. Understanding that ASR errors are often concentrated in specific sub-populations (a skewed distribution) is necessary to appreciate why the median reveals performance for the "typical" user better than the mean.
  - Quick check question: If a model has a mean WER of 10% but a median WER of 5%, what does this imply about the distribution of errors across speakers?

- **Reference Groups in Fairness (Norm vs. Min)**
  - Why needed here: Bias is calculated relative to a baseline. The choice of baseline (Group-to-Min vs. Group-to-Norm) changes the interpretation of bias. "Norm" typically refers to the standard training demographic, while "Min" refers to the best-performing group in the test set.
  - Quick check question: Why might using the "best-performing group" as a reference be problematic if you don't have access to the training data demographics?

- **Data Augmentation Artifacts**
  - Why needed here: The study shows augmentations can have uneven effects. Understanding that these transformations simulate specific acoustic variations helps explain why they might generalize poorly to unseen accents.
  - Quick check question: Does applying speed perturbation to native speech guarantee improved robustness to non-native speech rhythms?

## Architecture Onboarding

- **Component map**: CGN Training Data -> Conformer/Whisper Model -> Augmentation Module -> Evaluation Layer (WER per group) -> Aggregate (Median/Stdev) -> Bias Calculation (Relative Difference) -> Output Tables

- **Critical path**:
  1. Load CGN training data
  2. Apply/Exclude augmentations (NoAug vs SpAug vs SpSpecAug)
  3. Train/Fine-tune Conformer or Whisper model
  4. Inference on disaggregated test sets (CGN, Jasmin-DC, Jasmin-NnA, etc.)
  5. Calculate WER per demographic group
  6. Aggregate stats: Compute Median and Stdev of the per-group WERs
  7. Compute Bias: Calculate relative difference between each group and the reference (min/norm)

- **Design tradeoffs**:
  - Speed Perturbation: Improves robustness to rate variations but may misalign with non-native prosody, increasing bias
  - Metric Selection: Relative difference measures are better for comparing bias across systems with different base accuracies; absolute differences are simpler but less nuanced
  - Fine-tuning: Whisper fine-tuning on CGN improves read speech but can degrade performance on HMI/non-native speech compared to the base model

- **Failure signatures**:
  - The "False Positive" Improvement: Average WER decreases, but Standard Deviation increases (model improved for the majority at the expense of the minority)
  - High Bias-Low Variance: The model performs consistently poorly across all diverse groups relative to the norm
  - G2norm vs G2min Divergence: Large discrepancy between these two bias measures suggests training/test mismatch

- **First 3 experiments**:
  1. Baseline Audit: Train a Conformer on CGN without augmentation. Report Mean, Median, and Stdev of WER across Jasmin subgroups to establish the disparity baseline
  2. Augmentation Ablation: Apply SpecAugment + Speed Perturbation. Compare the relative difference in bias for Non-native vs Native groups to verify if augmentation inadvertently penalizes non-native speakers
  3. Metric Sensitivity Test: Calculate bias using both G2min and G2norm on a pre-trained model (like Whisper) to determine which reference group is more sensitive to detecting the "non-native" performance drop

## Open Questions the Paper Calls Out

### Open Question 1
How can qualitative user feedback be systematically integrated with quantitative relative bias measures to validate the real-world impact of ASR bias? The recommendations state that unlike WER, bias lacks ground truth, and "user feedback and experiences should be considered for qualitative insights."

### Open Question 2
Do the recommended metrics (median, standard deviation, relative difference) effectively capture bias in intersectional demographic groups (e.g., age combined with gender)? The literature review notes that intersectional groups are "under-explored," and the experiments analyze groups in isolation.

### Open Question 3
What specific mitigation strategies can simultaneously optimize for low median WER and reduced relative bias, given the observed trade-off where augmentation improved average performance but increased bias? The results show that SpSpecAug improved average WER and median but increased bias against non-native speakers, indicating a conflict in optimization goals.

## Limitations
- Dataset Specificity: Findings are based exclusively on Dutch ASR systems and may not generalize across languages and cultural contexts
- Bias Definition Ambiguity: No universally accepted definition of fairness in ASR exists, making cross-study comparisons challenging
- Generalizability of Augmentation Effects: The finding that augmentation can increase relative bias against non-native speakers is based on specific techniques applied to Dutch speech

## Confidence

- **High Confidence**: The core methodological contribution that averaged error rates alone are insufficient is well-supported by empirical results and statistical principles
- **Medium Confidence**: The specific recommendation to use median and standard deviation for performance reporting and relative difference measures for bias is strongly supported within the context of Dutch ASR evaluation
- **Low Confidence**: The claim that data augmentation strategies optimized for average performance can inadvertently increase relative bias against non-native speakers requires further validation across different techniques and languages

## Next Checks

1. **Cross-Lingual Validation**: Replicate the study's methodology on an ASR system for a different language (e.g., English, Mandarin) with a comparable evaluation dataset covering diverse speaker groups

2. **Augmentation Strategy Comparison**: Conduct a controlled experiment comparing the bias effects of different data augmentation strategies (e.g., SpecAugment, speed perturbation, vocal tract length perturbation, adversarial training) on a diverse ASR test set

3. **Fairness Metric Sensitivity Analysis**: Systematically evaluate the sensitivity of the ASR bias results to different choices of reference groups (G2min vs. G2norm) and bias calculation methods (absolute vs. relative difference) on a diverse test set