---
ver: rpa2
title: Dynamic Graph Structure Estimation for Learning Multivariate Point Process
  using Spiking Neural Networks
arxiv_id: '2504.01246'
source_url: https://arxiv.org/abs/2504.01246
tags:
- temporal
- event
- graph
- neural
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Spiking Dynamic Graph Network (SDGN),
  a novel framework for modeling temporal point processes using spiking neural networks
  (SNNs). The method leverages SNNs' temporal processing capabilities and spike-timing-dependent
  plasticity (STDP) to dynamically estimate spatio-temporal functional graphs from
  event data, eliminating the need for predefined or static graph structures.
---

# Dynamic Graph Structure Estimation for Learning Multivariate Point Process using Spiking Neural Networks

## Quick Facts
- arXiv ID: 2504.01246
- Source URL: https://arxiv.org/abs/2504.01246
- Reference count: 40
- Primary result: SDGN achieves superior predictive accuracy on multiple real-world datasets using dynamic graph estimation with spiking neural networks

## Executive Summary
This paper introduces the Spiking Dynamic Graph Network (SDGN), a novel framework for modeling temporal point processes using spiking neural networks (SNNs). The method leverages SNNs' temporal processing capabilities and spike-timing-dependent plasticity (STDP) to dynamically estimate spatio-temporal functional graphs from event data, eliminating the need for predefined or static graph structures. Experiments on synthetic and real-world datasets demonstrate superior predictive accuracy compared to state-of-the-art methods, though the approach faces challenges with dense graphs and certain non-Gaussian dependencies.

## Method Summary
SDGN uses a recurrent Spiking Neural Network (RSNN) with Leaky Integrate-and-Fire (LIF) neurons to model multivariate temporal point processes. The membrane potential dynamics serve as adaptive basis functions for projecting event histories, while STDP-based learning estimates dynamic graph structures from spike timing correlations. The model combines graph embeddings with history to predict event intensities using a Softplus activation. Training employs surrogate gradients to handle the non-differentiable spiking dynamics, with adaptive time-stepping for numerical stability. The framework jointly learns both the event intensity function and the dynamic dependency graph without requiring predefined graph structures.

## Key Results
- SDGN achieves lower Root Mean Square Error (RMSE) across all tested datasets (NYC Taxi, Reddit, Earthquake, Stack Overflow, 911 Calls) compared to state-of-the-art baselines
- Structural Similarity Index (SSI) scores validate the accuracy of dynamic graph estimation on synthetic data with known ground-truth graphs
- Ablation studies confirm the contributions of core components including membrane potential basis functions, STDP-based graph estimation, and surrogate gradient training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Utilizing membrane potential dynamics of LIF neurons as adaptive basis functions allows for more efficient projection of infinite-dimensional event histories than fixed basis functions
- **Mechanism:** The model projects input signals onto time-varying membrane potentials $v_i(t)$ of a recurrent SNN, which evolve based on differential equations and act as dynamic filters adapting to temporal patterns
- **Core assumption:** Underlying temporal dependencies reside on a low-dimensional manifold that can be spanned by dynamic states of spiking neurons
- **Evidence anchors:** Abstract mentions "membrane potential basis functions... providing a more efficient and accurate representation"; Eq. (10) defines basis representation using membrane response kernels
- **Break condition:** If membrane time constant $\tau_m$ is mismatched with event frequency, potentials may saturate or decay too fast, failing to capture relevant history

### Mechanism 2
- **Claim:** Graph structure can be estimated dynamically by mapping spike-timing correlations to edge probabilities
- **Mechanism:** The model calculates edge probability $p_{ij}(t)$ using Softmax over exponential kernel of spike time differences; neurons spiking in close temporal proximity have increased edge probability
- **Core assumption:** Functional connectivity (spike timing) serves as reliable proxy for causal or relational dependency between event types
- **Evidence anchors:** Abstract mentions "dynamically estimate spatio-temporal functional graphs"; Eq. (6) defines $\phi(s_i, s_j)$ explicitly based on spike timing differences
- **Break condition:** In dense graphs with high spurious correlations, the mechanism struggles to isolate specific dependencies

### Mechanism 3
- **Claim:** Surrogate gradients enable optimization of non-differentiable spiking dynamics for point process likelihood maximization
- **Mechanism:** Since spike generation is a discrete step function, the model uses smooth approximation $\partial s / \partial v$ to propagate gradients during training, allowing minimization of negative log-likelihood
- **Core assumption:** Local approximation of spike derivative is sufficient to guide global optimization of intensity function $\lambda$
- **Evidence anchors:** Ablation studies confirm contributions of core components; Eq. (9) and (17) define differentiable spike approximation used for training
- **Break condition:** If surrogate gradient width is too narrow relative to voltage fluctuations, gradients vanish and intensity prediction fails to update

## Foundational Learning

- **Concept: Temporal Point Process (TPP) & Intensity Function**
  - **Why needed here:** The entire paper is built around modeling $\lambda(t|H_t)$, the rate at which events occur given history
  - **Quick check question:** Can you explain why maximizing log-likelihood requires both predicting when events do happen and when they don't?

- **Concept: Leaky Integrate-and-Fire (LIF) Neuron**
  - **Why needed here:** The "Membrane Potential Basis Functions" rely entirely on differential equation of LIF model
  - **Quick check question:** If input current $I(t)$ stops, does membrane potential $v(t)$ stay constant, reset to zero, or decay?

- **Concept: STDP (Spike-Timing-Dependent Plasticity)**
  - **Why needed here:** This is the core heuristic for learning graph structure, defining "causality" logic used to draw edges between nodes
  - **Quick check question:** In STDP, if neuron A fires just before neuron B, does the synapse A→B strengthen or weaken?

## Architecture Onboarding

- **Component map:** Event sequence $(t_i, e_i)$ -> RSNN with LIF dynamics -> Membrane potentials/spikes -> Graph estimator (edge probabilities) -> Intensity head (Softplus activation) -> $\lambda(t)$
- **Critical path:** The adaptive time-stepping loop (Algorithm 1); stability of intensity estimation depends on bounding $\Delta t$ to prevent numerical instability in membrane potential updates
- **Design tradeoffs:**
  - **Sparsity vs. Accuracy:** Model excels at sparse, asynchronous data but explicitly admits limitations with dense graphs where dynamic estimation overhead increases and distinctiveness drops
  - **Event-driven vs. Sync:** Priority queue update (O(log N)) is efficient for sparse events but may become bottleneck if event stream is extremely dense
- **Failure signatures:**
  - **SSI Drop:** If Structural Similarity Index is low (especially for large N), graph estimation is failing
  - **Non-Gaussian issues:** Paper notes limitations with "certain non-Gaussian dependencies"; if validation loss plateaus early, check data distribution assumptions
- **First 3 experiments:**
  1. **Synthetic Sanity Check:** Run "Synthetic Data" experiment with known ground-truth graph; plot SSI vs. Number of Nodes to verify estimation pipeline works before touching real data
  2. **Ablation on STDP:** Disable STDP update (freeze weights) and measure drop in RMSE on "NYC Taxi" dataset to quantify value of dynamic learning
  3. **Hyperparameter $\tau$ Sensitivity:** Vary temporal kernel width $\tau$ to find "sweet spot" between ignoring short-term noise and missing fast dependencies

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the dynamic graph estimation mechanism in SDGN be modified to maintain performance advantages in densely connected networks?
  - **Basis in paper:** [explicit] Authors state in Conclusion that "SDGN does not demonstrate a significant performance advantage for dense graphs, where the benefits of dynamic graph estimation are less pronounced"
  - **Why unresolved:** Current methodology with sparsity-inducing regularizers and specific STDP dynamics appears less effective when underlying graph structure is fully connected or highly dense
  - **What evidence would resolve it:** Modification to regularization term or connectivity estimation algorithm that results in SDGN outperforming baselines on synthetic datasets with low sparsity (e.g., sparsity < 0.1)

- **Open Question 2:** How can the theoretical framework of SDGN be extended to robustly capture complex non-Gaussian dependencies that currently limit its performance?
  - **Basis in paper:** [explicit] Abstract and Conclusion explicitly acknowledge "limitations in handling... certain non-Gaussian dependencies" as constraint of current model
  - **Why unresolved:** Model likely relies on assumptions derived from functional graphical model theory or specific STDP windowing that fails to generalize to all non-Gaussian event distributions
  - **What evidence would resolve it:** Successful application of extended SDGN model to dataset specifically designed to exhibit heavy-tailed or complex non-Gaussian dependencies without drop in predictive accuracy

- **Open Question 3:** Can the SDGN algorithm be parallelized to overcome computational overhead and stability issues associated with STDP in very large graph structures?
  - **Basis in paper:** [explicit] Conclusion notes that "complexity and computational overhead" limit scalability for very large graphs and suggests "explore[ing] the potential parallelization of the algorithm" as future work
  - **Why unresolved:** Dynamic estimation of graph structures and precise timing requirements of STDP introduce sequential dependencies difficult to parallelize using standard hardware acceleration
  - **What evidence would resolve it:** Distributed implementation of SDGN that maintains convergence guarantees and predictive accuracy while reducing training time on datasets with node counts significantly larger than tested (> 1000 nodes)

- **Open Question 4:** How can higher-order interactions be formally integrated into the SDGN framework to enhance estimation of functional neighborhoods?
  - **Basis in paper:** [explicit] Conclusion proposes that future work should "investigate the effect of higher-order interactions and how they can be leveraged to efficiently estimate functional neighborhoods"
  - **Why unresolved:** Current implementation estimates dynamic dependencies primarily through pairwise spike-timing relationships, potentially missing complex group dynamics involving multiple event types simultaneously
  - **What evidence would resolve it:** Variant of SDGN that explicitly models triadic or group interactions and demonstrates improved SSI scores in synthetic datasets containing known higher-order causal structures

## Limitations
- The method faces challenges with dense graphs where spurious correlations increase and distinctiveness drops
- There are limitations in handling certain non-Gaussian dependencies, suggesting the framework may not generalize to all event distribution types
- The approach requires careful tuning of multiple hyperparameters (learning rate, time constants, kernel width, regularization parameters) that are not fully specified

## Confidence
- **High Confidence:** The core mechanism of using membrane potential dynamics as adaptive basis functions (Mechanism 1) is well-specified through equations and biologically-grounded, with strong experimental validation showing superior performance on multiple real-world datasets
- **Medium Confidence:** The dynamic graph estimation via STDP (Mechanism 2) is theoretically sound and supported by experiments, but the claim of accurately estimating complex dependencies may be limited in dense or non-Gaussian scenarios as noted in the paper
- **Medium Confidence:** The surrogate gradient approach for optimizing spiking dynamics (Mechanism 3) is a standard technique in SNN research, but the effectiveness depends heavily on unspecified gradient parameters and may struggle with vanishing gradients in certain configurations

## Next Checks
1. **Reproduce synthetic SSI curves:** Generate synthetic Hawkes process data with known ground-truth graph structure and measure Structural Similarity Index across varying node counts to verify the graph estimation pipeline independently of dataset-specific factors
2. **Ablate STDP learning:** Implement a frozen-weight baseline (disable Eq. 4) and measure performance degradation on NYC Taxi dataset to quantify the marginal value of dynamic graph learning versus static alternatives
3. **Sweep kernel width τ:** Systematically vary the temporal kernel width parameter in Eq. 6 across orders of magnitude to identify sensitivity and potential overfitting to short-term noise versus missing true fast dependencies