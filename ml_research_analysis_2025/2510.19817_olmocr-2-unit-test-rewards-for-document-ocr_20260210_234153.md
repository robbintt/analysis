---
ver: rpa2
title: 'olmOCR 2: Unit Test Rewards for Document OCR'
arxiv_id: '2510.19817'
source_url: https://arxiv.org/abs/2510.19817
tags:
- test
- unit
- zhang
- wang
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: olmOCR 2 introduces a new OCR system that converts digitized documents
  into clean, naturally ordered plain text using reinforcement learning with verifiable
  rewards (RLVR). The core innovation is using binary unit tests as rewards, generated
  through a synthetic document pipeline that creates HTML renderings of real PDFs
  with ground-truth markup.
---

# olmOCR 2: Unit Test Rewards for Document OCR

## Quick Facts
- arXiv ID: 2510.19817
- Source URL: https://arxiv.org/abs/2510.19817
- Reference count: 10
- Primary result: Achieves state-of-the-art performance on olmOCR-Bench with 14.2-point improvement using RLVR with binary unit tests

## Executive Summary
olmOCR 2 introduces a new OCR system that converts digitized documents into clean, naturally ordered plain text using reinforcement learning with verifiable rewards (RLVR). The core innovation is using binary unit tests as rewards, generated through a synthetic document pipeline that creates HTML renderings of real PDFs with ground-truth markup. This approach enables scalable, accurate training of a 7B VLM, achieving state-of-the-art performance on the olmOCR-Bench with a 14.2-point improvement over the previous version. The largest gains are in math formula conversion, table parsing, and multi-column layouts. The system uses dynamic temperature scaling, improved prompting, and model averaging to enhance performance, and all data, models, and code are released under permissive open licenses.

## Method Summary
The system converts PDFs to HTML through a VLM-based synthetic data pipeline, then extracts binary unit tests from the HTML structure. These tests evaluate functional properties like text presence, reading order, table accuracy, and math formula rendering. A 7B VLM is first fine-tuned on real documents, then optimized with GRPO using unit test rewards. The final model averages six checkpoints from different seeds and sampling strategies. Key innovations include binary reward signals that handle document ambiguity better than edit distance, and a scalable synthetic pipeline that avoids manual annotation.

## Key Results
- State-of-the-art performance on olmOCR-Bench with 14.2-point improvement over olmOCR 1
- Largest gains in math formula conversion (72.9→84.9), table parsing (43.9→47.7), and multi-column layouts
- Final model achieves 82.4 overall score after RLVR training and weight averaging
- All components released under permissive open licenses

## Why This Works (Mechanism)

### Mechanism 1
Binary unit tests provide more robust reward signals for OCR training than continuous edit distance metrics. Unit tests evaluate functional correctness (e.g., "text A appears before text B," "formula renders correctly") rather than surface-level string similarity. This handles ambiguity in document structure—such as floating figures or equivalent LaTeX representations—without penalizing valid alternatives. Edit distance, by contrast, can assign different scores to equally correct outputs or partially reward severe errors (e.g., incorrect reading order).

### Mechanism 2
Synthetic HTML generation from real PDFs enables scalable creation of ground-truth unit tests without manual annotation. A general VLM (Claude Sonnet) converts sampled PDF pages into clean HTML through iterative prompting: layout analysis → content rendering → refinement. The structured HTML (with semantic tags like `<header>`, `<footer>`, KaTeX for math) allows programmatic extraction of test cases. Even if OCR contains errors, the pipeline uses only the HTML structure for test generation, making it robust to hallucinations.

### Mechanism 3
GRPO with verifiable unit-test rewards improves OCR quality, particularly for structured elements (math, tables, multi-column layouts). Starting from a supervised fine-tuned Qwen2.5-VL-7B, the model generates 28 completions per document, each scored by the fraction of passing unit tests. GRPO optimizes relative to these rewards with KL regularization (β=0.01). Final model averages six checkpoints trained with different seeds and sampling strategies (token-level vs. sequence-level importance sampling).

## Foundational Learning

- **Reinforcement Learning with Verifiable Rewards (RLVR)**
  - Why needed here: RLVR enables training with reward signals that can be computed automatically without human labeling. Understanding this is prerequisite to grasping why unit tests are viable rewards.
  - Quick check question: Can you explain why a reward must be "verifiable" for scalable RL training, and give an example of a non-verifiable reward?

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: GRPO is the specific RL algorithm used. It compares multiple completions per prompt to estimate advantages, reducing variance compared to single-sample methods.
  - Quick check question: How does generating 28 completions per document and comparing their relative rewards differ from standard REINFORCE with a single sample?

- **Model Souping (Weight Averaging)**
  - Why needed here: Final model performance depends on averaging multiple independently trained checkpoints. This technique improves robustness without increasing inference cost.
  - Quick check question: Why might averaging weights from different random seeds improve generalization compared to selecting the single best checkpoint?

## Architecture Onboarding

- Component map: Real PDFs → VLM HTML Generator → Synthetic HTML + Rendered Images → Unit Test Extractor → Qwen2.5-VL-7B (SFT) → GRPO Trainer + Unit Test Rewards → 6 Checkpoints → Weight Averaging → olmOCR-2-7B-1025

- Critical path: The synthetic data pipeline quality (HTML fidelity) directly determines unit test validity, which governs RL reward signal quality. Errors propagate: poor HTML → invalid tests → misaligned rewards → degraded model.

- Design tradeoffs:
  - Binary vs. continuous rewards: Binary tests handle ambiguity but provide less granular feedback; edit distance gives smooth gradients but mismeasures correctness
  - Synthetic vs. real annotations: Synthetic scales cheaply but may miss edge cases; manual annotation is accurate but expensive
  - Image resolution (1288px vs. 1024px): Higher resolution improves accuracy but increases compute and latency

- Failure signatures:
  - Repetition loops during inference: Model fails to generate EOS token. Mitigation: dynamic temperature scaling (0.1→0.8)
  - JSON retry storms: Model struggles with quote matching. Mitigation: switched to YAML output format
  - Blank page hallucinations: Model never trained on blank pages. Mitigation: fix data loader to include blanks
  - Unit test false positives: HTML generator hallucinates structure. Robustness: pipeline uses HTML alone; OCR errors don't affect test generation

- First 3 experiments:
  1. Validate synthetic test quality: Sample 50 PDF pages from olmOCR2-synthmix-1025, manually inspect HTML output vs. original PDF. Check for systematic errors in table, math, or multi-column representation.
  2. Ablate reward components: Train three model variants—(a) unit tests only, (b) edit distance reward only (ala Infinity Parser), (c) combined. Compare on olmOCR-Bench subsamples to quantify unit test contribution.
  3. Test generalization: Evaluate olmOCR-2-7B-1025 on out-of-distribution document types not in synthetic mix (e.g., handwritten forms, invoices with complex nesting). Compare against baseline Qwen2.5-VL-7B to assess overfitting to synthetic distribution.

## Open Questions the Paper Calls Out

- How do binary unit tests compare to continuous scores like edit distance when used as reinforcement learning rewards?
- Can the synthetic data pipeline scale to handle highly complicated document layouts?
- Does the unit test reward mechanism generalize effectively to multilingual document parsing?

## Limitations

- Synthetic HTML generation pipeline's fidelity to real document structures remains unverified; systematic errors in complex layouts could compromise test validity
- Reward signal quality depends on whether binary unit tests capture all relevant dimensions of OCR correctness; nuanced errors may go undetected
- GRPO hyperparameters and exact prompts are underspecified, potentially affecting reproducibility of reported performance gains

## Confidence

- High: olmOCR 2 achieves state-of-the-art performance on olmOCR-Bench as measured by the benchmark's unit tests
- Medium: Binary unit tests provide more robust reward signals than edit distance for OCR training
- Medium: Synthetic HTML generation enables scalable ground-truth test creation

## Next Checks

1. Validate synthetic test quality: Manually inspect 50 PDF pages from olmOCR2-synthmix-1025, comparing generated HTML to original PDFs to identify systematic representation errors
2. Ablate reward components: Train three model variants—(a) unit tests only, (b) edit distance only, (c) combined—on identical data to quantify unit test contribution
3. Test generalization: Evaluate olmOCR-2-7B-1025 on out-of-distribution document types (handwritten forms, complex invoices) to assess overfitting to synthetic distribution