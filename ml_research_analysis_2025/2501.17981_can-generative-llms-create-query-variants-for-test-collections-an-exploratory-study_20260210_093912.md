---
ver: rpa2
title: Can Generative LLMs Create Query Variants for Test Collections? An Exploratory
  Study
arxiv_id: '2501.17981'
source_url: https://arxiv.org/abs/2501.17981
tags:
- query
- variants
- information
- sets
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Can Generative LLMs Create Query Variants for Test Collections? An Exploratory Study

## Quick Facts
- **arXiv ID**: 2501.17981
- **Source URL**: https://arxiv.org/abs/2501.17981
- **Reference count**: 25
- **Primary result**: GPT-generated query variants retrieve overlapping document sets with human queries, reaching 71.1% overlap at pool depth 100.

## Executive Summary
This exploratory study investigates whether large language models can generate query variants for test collection construction, comparing GPT-3.5-generated queries to human-generated variants from the UQV100 test collection. Using one-shot in-context learning, the LLM successfully produces keyword-style query variants that, while differing lexically from human queries, retrieve similar sets of relevant documents. The study reveals that GPT queries achieve up to 71.1% document overlap with human query pools at depth 100, though they generate fewer total variants and include more unjudged documents. These findings suggest LLMs can contribute to test collection construction by expanding document pools, though manual assessment of their unique retrievals remains an open question.

## Method Summary
The study employs one-shot in-context learning with GPT-3.5 (text-davinci-003) to generate query variants from 99 UQV100 backstories (excluding topic 275 used as the example). For each backstory, the LLM is prompted with a task description, one example backstory with human queries, and the target backstory. Queries are generated at three temperature settings (0.0, 0.5, 1.0) and evaluated against human query variants through document retrieval on the ClueWeb12-B corpus using BM25 with parameters b=0.4 and k1=0.9. Evaluation metrics include Jaccard similarity, coverage ratio, RBO, and retrieval effectiveness measures (P@10, NDCG@10, RBP) comparing the document pools produced by GPT and human queries.

## Key Results
- GPT-generated queries achieve up to 71.1% document overlap with human query pools at depth 100, despite low lexical similarity (Jaccard 7-13.5%)
- Document overlap increases with pool depth: 43.7% at depth 10 vs 71.1% at depth 100 for relevant documents only
- GPT queries retrieve significantly more unjudged documents (0.31-0.37 ratio) compared to human queries (0.13 ratio)
- Human-generated variants yield almost double the pool size of GPT sets, suggesting lower diversity in LLM-generated queries

## Why This Works (Mechanism)

### Mechanism 1: In-Context Learning (ICL) for Query Variant Generation
GPT-3.5 can generate query variants from information need descriptions using one-shot prompting without fine-tuning. The model is conditioned via a prompt containing task description, one example backstory with human queries, and a target backstory. The single example constrains the output distribution to approximate human query formulation patterns, though zero-shot prompting produces long, question-like queries instead of keyword variants.

### Mechanism 2: Semantic Overlap Through Retrieval Equivalence
Even when GPT-generated queries differ lexically from human queries, they retrieve overlapping relevant documents under BM25 ranking. This occurs because keyword-based models treat semantically similar queries as functionally equivalent when they share core terms. The overlap increases from 43.7% at depth 10 to 71.1% at depth 100, suggesting query similarity becomes more apparent with deeper pools.

### Mechanism 3: Temperature-Modulated Diversity
Temperature settings influence the diversity of generated query variants, affecting both lexical variety and retrieval diversity. Higher temperatures increase stochasticity in token selection, producing more varied query formulations, while lower temperatures yield more deterministic outputs. However, paradoxically, GPT (temp=1.0) produced fewer unique queries (2719) than GPT (temp=0.0) (3638), suggesting possible model collapse at high temperature.

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - Why needed here: This is the core paradigm enabling GPT-3.5 to generate query variants without task-specific fine-tuning. Understanding ICL is essential to interpret why one-shot prompting works and what its limitations are.
  - Quick check question: Can you explain why ICL requires no gradient updates, and what the "context window" constraint implies for prompt design?

- **Concept: Test Collections and Document Pooling**
  - Why needed here: The paper evaluates LLM-generated queries by their contribution to document pooling—a foundational IR evaluation methodology. Without this, the significance of 71.1% overlap is unclear.
  - Quick check question: In the Cranfield paradigm, what is the purpose of pooling, and why does query diversity matter for pool completeness?

- **Concept: Query Variants and Information Need**
  - Why needed here: The paper treats query variants as alternative formulations of the same underlying information need. This distinction is critical for understanding why lexical dissimilarity does not necessarily imply semantic divergence.
  - Quick check question: Given the backstory in Figure 2 ("windsor knot"), why might "windsor knot tutorial" and "how do i make a windsor knot" be considered variants of the same need?

## Architecture Onboarding

- **Component map**: UQV100 backstories -> Prompt Constructor -> GPT-3.5 text-davinci-003 -> Query Filter -> Anserini BM25 -> Evaluation Layer

- **Critical path**:
  1. Extract backstories from UQV100 (exclude topic 275 if used as the example)
  2. Construct one-shot prompt with fixed example; iterate over remaining backstories
  3. Generate queries at temp ∈ {0.0, 0.5, 1.0}; deduplicate
  4. Run BM25 retrieval per variant; union documents per topic to form pools
  5. Compare GPT pools to human pools using Jaccard (all docs and relevant-only); compute effectiveness metrics

- **Design tradeoffs**:
  - One-shot vs. few-shot: Few-shot may improve query quality but requires more backstories for examples; one-shot conserves data but may under-constrain
  - Temperature selection: Lower temp increases reproducibility but may reduce lexical diversity; higher temp increases variety but risks incoherence or reduced coverage
  - Pool depth: Shallow pools (depth 10) show lower overlap (~43.7%); deeper pools (depth 100) show higher overlap (~71.1%) but require more judgments

- **Failure signatures**:
  - Zero-shot produces verbose, question-like queries instead of keyword-style variants
  - High unjudged document ratio in GPT pools (0.31–0.37 vs. 0.13 for human) indicates GPT retrieves documents outside the existing qrels
  - Significantly lower effectiveness metrics (P@10, NDCG@10) for GPT queries suggest they are less precise than human queries

- **First 3 experiments**:
  1. **Baseline Reproduction**: Replicate the one-shot prompting setup with temp=0.5 on a held-out subset of backstories; verify Jaccard overlap and pool overlap match reported ranges (7–13.5% Jaccard, ~43–71% relevant doc overlap)
  2. **Temperature Sensitivity Analysis**: Run retrieval at temp ∈ {0.0, 0.5, 1.0} on the same topics; measure pool size growth curves and RBO to quantify diversity vs. consistency tradeoffs
  3. **Unjudged Document Audit**: Identify the unjudged documents uniquely retrieved by GPT queries; manually sample to determine if they include relevant documents missed by the human pool, or if they represent systematic drift in query formulation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the unjudged documents retrieved by GPT-generated query variants contain relevant information that was not found through human-generated queries?
- Basis in paper: The authors state, "It would be interesting to further investigate the unjudged portion of the GPT sets to understand whether they retrieve relevant documents that were not found through the human set."
- Why unresolved: The existing relevance judgments (qrels) are based on human query pools; therefore, documents retrieved only by GPT queries are currently marked as unjudged, masking their potential utility.
- What evidence would resolve it: A manual relevance assessment of the documents retrieved exclusively by the GPT-generated variants.

### Open Question 2
- Question: To what extent do human assessors judge LLM-generated queries to be natural or effective approximations of human information needs?
- Basis in paper: The paper notes that using UQV100 as a reference is limited because the "human-generated queries... may not be an accurate representation," and suggests "Incorporating human evaluation to assess the extent to which the GPT sets approximate human queries... is... a question to be explored in future research."
- Why unresolved: Current analysis relies on text overlap (Jaccard) and retrieval metrics, but these do not capture the semantic nuance or user satisfaction with the query formulation itself.
- What evidence would resolve it: A user study where participants rate the quality, clarity, and relevance of LLM-generated queries compared to human variants without retrieval context.

### Open Question 3
- Question: How does the one-shot LLM approach compare to advanced prompting techniques or traditional query simulation methods in generating query variants?
- Basis in paper: The authors suggest "Further research could explore advanced prompting techniques and compare our approach of using an LLM... with previous query simulation methods... which were used to generate query variants from source documents."
- Why unresolved: This study isolated one-shot learning with GPT-3.5; it did not benchmark against few-shot approaches, fine-tuned models, or established simulation baselines.
- What evidence would resolve it: A comparative analysis of query diversity and retrieval effectiveness between one-shot LLM outputs and those generated by few-shot prompting or rule-based simulation algorithms.

### Open Question 4
- Question: Can LLMs be optimized to produce query variants that match the diversity and pool growth of human-generated sets?
- Basis in paper: The results show that human variants yield a pool size almost double that of GPT sets and grow faster, leading the authors to observe a "possible higher diversity" in human queries that LLMs failed to capture.
- Why unresolved: While LLMs achieved high overlap in relevant documents, they generated fewer unique variants per topic, suggesting a failure to explore the full "wide variety" of human search behavior.
- What evidence would resolve it: Experiments utilizing higher temperatures or diversity-promoting prompts to determine if LLMs can expand the pool size to match human baselines without losing precision.

## Limitations
- One-shot prompting may under-constrain query generation, producing variants that diverge from human intent when backstories are ambiguous
- Document overlap metrics assume lexical/semantic equivalence under BM25, but GPT queries may retrieve genuinely different relevant documents not in the human pool
- High unjudged document ratios (0.31–0.37) in GPT pools suggest potential coverage gaps or retrieval drift, but manual relevance judgments are missing

## Confidence
- **High Confidence**: GPT-generated queries can produce overlapping document pools with human queries at sufficient depth (71.1% at depth 100)
- **Medium Confidence**: One-shot ICL is sufficient for generating keyword-style query variants, though quality depends on backstory clarity
- **Low Confidence**: Temperature effects on retrieval diversity are observed but not fully explained; model collapse at high temp is concerning

## Next Checks
1. Conduct a small-scale manual relevance judgment on unjudged documents uniquely retrieved by GPT queries to assess coverage gaps
2. Test few-shot prompting (2–3 examples) to determine if query quality and pool overlap improve over one-shot
3. Evaluate GPT query variants on a held-out corpus to test whether document overlap is corpus-dependent or generalizable