---
ver: rpa2
title: 'Lost in Phonation: Voice Quality Variation as an Evaluation Dimension for
  Speech Foundation Models'
arxiv_id: '2510.25577'
source_url: https://arxiv.org/abs/2510.25577
tags:
- voice
- speech
- quality
- breathy
- creaky
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VQ-Bench, a dataset and evaluation framework
  to test how speech foundation models (SFMs) respond to voice quality variation such
  as breathy, creaky, and end-creak phonation. Using a zero-shot TTS system and voice
  quality conversion, the authors synthesized parallel speech prompts in four phonation
  types across two corpora (Buckeye and VCTK).
---

# Lost in Phonation: Voice Quality Variation as an Evaluation Dimension for Speech Foundation Models

## Quick Facts
- arXiv ID: 2510.25577
- Source URL: https://arxiv.org/abs/2510.25577
- Reference count: 0
- This paper introduces VQ-Bench, a dataset and evaluation framework to test how speech foundation models respond to voice quality variation such as breathy, creaky, and end-creak phonation.

## Executive Summary
This paper introduces VQ-Bench, a dataset and evaluation framework to test how speech foundation models (SFMs) respond to voice quality variation such as breathy, creaky, and end-creak phonation. Using a zero-shot TTS system and voice quality conversion, the authors synthesized parallel speech prompts in four phonation types across two corpora (Buckeye and VCTK). They evaluated SFMs in two settings: long-form open-ended generation tasks (career advice, therapy, interviews, storytelling) and speech emotion recognition. Results showed that voice quality significantly influenced SFM outputs and emotion predictions—breathy and end-creak voices tended to elicit more affiliative or calm responses, while creaky voice often led to more reserved or authoritative judgments. Female voices were systematically rated lower in interview tasks. These findings highlight the need to account for paralinguistic variation in SFM evaluation to mitigate biases and improve real-world deployment.

## Method Summary
The authors created VQ-Bench by synthesizing speech prompts in four phonation types (modal, breathy, creaky, end-creak) using VoiceQualityVC to manipulate acoustic parameters (CPPS, H1-H2, H1-A3) while preserving linguistic content. They generated 20 prompts per speaker from Buckeye and VCTK corpora, then evaluated two SFMs: LFMAudio2-1.5B and OpenAI's speech-to-speech API. Long-form outputs were judged by an LLM on 11 evaluation dimensions using 1-5 ratings, while SER models classified emotions from Buckeye subset. Statistical analysis used CLMM for ratings and Bayesian multilevel categorical regression for SER logits.

## Key Results
- Voice quality significantly influenced SFM outputs across all evaluation dimensions except 'Role status' and 'Emotional validation'
- Breathy and end-creak voices elicited more affiliative or calm responses, while creaky voice led to more reserved or authoritative judgments
- Female voices received systematically lower ratings in interview tasks
- SER models showed significant emotion probability shifts based on phonation type, with breathy voice increasing calm/neutral predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Systematic manipulation of acoustic voice parameters produces detectable, consistent changes in SFM output behavior.
- Mechanism: Voice quality conversion alters glottal source characteristics (H1–H2, H1–A3, CPPS) while preserving linguistic content and speaker identity. SFMs encode these acoustic differences and map them to social/affective judgments during generation.
- Core assumption: SFMs learn implicit associations between phonation types and social meanings from training data, similar to human perceptual patterns documented in literature.
- Evidence anchors:
  - [abstract] "breathy and end-creak voices tended to elicit more affiliative or calm responses, while creaky voice often led to more reserved or authoritative judgments"
  - [section 4] "The effect of voice quality was significant for all evaluation dimensions except 'Role status' and 'Emotional validation'"
  - [corpus] Related work on voice-based model bias (Speak Your Mind, FMR=0.59) supports that SFMs respond systematically to voice characteristics, though corpus lacks direct phonation manipulation studies.
- Break condition: If SFM training data contains insufficient phonation diversity, or if acoustic parameter manipulations fall below perceptual discrimination threshold, effects may not materialize.

### Mechanism 2
- Claim: Open-ended generation tasks with LLM-based evaluation expose paralinguistic bias patterns that MCQA formats obscure.
- Mechanism: Unlike constrained choice selection, open-ended generation allows voice quality to influence multiple output dimensions simultaneously (agency, empathy, leadership endorsement). An LLM judge rates responses on continuous scales, revealing directional biases.
- Core assumption: The LLM judge (Gemini-2.5-flash-lite) reliably detects subtle differences in response tone and content across phonation conditions.
- Evidence anchors:
  - [abstract] "Existing benchmarks for speech understanding largely rely on multiple-choice question answering (MCQA) formats, which are prone to failure and therefore unreliable in capturing the nuanced ways paralinguistic features influence model behaviour"
  - [section 3] "responses are then evaluated using an LLM judge... where we ask the judge to rate the responses on multiple evaluation dimensions... on a scale of 1–5"
  - [corpus] Limited corpus evidence on LLM-as-judge reliability for speech evaluation; assumption remains unvalidated externally.
- Break condition: If judge LLM introduces its own biases or lacks sensitivity to subtle response variations, measured effects may reflect judge artifacts rather than SFM behavior.

### Mechanism 3
- Claim: SER models systematically shift emotion probability distributions based on phonation type, even when top prediction remains unchanged.
- Mechanism: Voice quality alters the full logit distribution across emotion categories. Breathy voice increases probability mass on "calm" and "neutral" while decreasing "fearful" and "surprised." This reflects acoustic-affective mappings learned from training data.
- Core assumption: Emotion-phonation associations in SER training data mirror human perceptual patterns (breathy→intimate/calm, creaky→authority/disengagement).
- Evidence anchors:
  - [section 4] "Breathy voice increased the likelihood of calm and neutral predictions while decreasing the likelihood of fearful and surprised predictions"
  - [Table 4] Breathy→Calm: +1.17 [0.77,1.57]; Female→Fearful: +3.89 [1.74,6.42]
  - [corpus] "Affect Models Have Weak Generalizability to Atypical Speech" suggests affect models trained on typical speech may not generalize, which aligns with findings that phonation variation shifts predictions.
- Break condition: If SER training data lacks phonation diversity or contains contradictory emotion-phonation mappings, systematic shifts may not emerge.

## Foundational Learning

- **Phonation types (modal, breathy, creaky, end-creak)**:
  - Why needed here: These are the controlled variables being tested. Understanding that they represent laryngeal configurations with distinct acoustic signatures is essential for interpreting why SFMs respond differently.
  - Quick check question: Which phonation type is associated with phrase-final pragmatic marking rather than sustained social meaning?

- **Acoustic parameters (H1–H2, H1–A3, CPPS)**:
  - Why needed here: These are the measurable acoustic correlates manipulated to create voice quality variants. They provide objective validation that conversions succeeded.
  - Quick check question: What does a positive H1–H2 value indicate about voice quality?

- **Cumulative Link Mixed Models (CLMM)**:
  - Why needed here: The statistical method used to analyze ordinal rating data (1–5 scales) with random effects for speakers. Essential for understanding how significance was determined.
  - Quick check question: Why use CLMM instead of standard ANOVA for ordinal rating data?

## Architecture Onboarding

- **Component map**: Reference audio extraction -> F5-TTS synthesis -> VoiceQualityVC conversion -> Acoustic parameter validation (H1–H2, H1–A3 plots) -> SFM inference -> Response collection -> LLM judging -> Statistical modeling

- **Critical path**: Reference audio extraction → F5-TTS synthesis → VoiceQualityVC conversion → Acoustic parameter validation (H1–H2, H1–A3 plots) → SFM inference → Response collection → LLM judging → Statistical modeling

- **Design tradeoffs**:
  - Synthetic voice quality vs. natural: Synthetic enables controlled manipulation but may not capture natural phonation variation
  - Binary gender framework: Reflects source corpora limitations; non-binary voices not yet evaluated
  - Single SER model: Using one SER architecture limits generalizability of emotion findings

- **Failure signatures**:
  - OpenAI API defaulting to male classification for nearly all samples → unreliable for gender-sensitive evaluation
  - Acoustic parameter distributions overlapping between phonation types → conversion pipeline issues
  - LLM judge giving uniform ratings across conditions → insensitive evaluation dimensions

- **First 3 experiments**:
  1. Reproduce the CLMM analysis on LFMAudio2 outputs to validate the voice quality effects in Table 2; check coefficient directions match reported arrows.
  2. Run the SER experiment on Buckeye subset with full logit extraction; verify Bayesian credible intervals for breathy→calm and female→fearful effects.
  3. Extend to non-binary speaker voices or additional SFMs (e.g., Qwen-Audio, SALMONN) to test whether observed patterns generalize beyond current scope.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do gender-ambiguous or non-binary voices elicit distinct bias patterns compared to binary-gendered voices in SFM evaluations?
- Basis in paper: [explicit] The authors state that "the inclusion of gender-ambiguous and non-binary voices will be essential to assess whether similar or distinct biases emerge beyond the binary paradigm."
- Why unresolved: The current study was restricted by the binary gender structure of the source corpora (Buckeye and VCTK).
- What evidence would resolve it: Evaluation of SFMs using VQ-Bench with synthesized gender-ambiguous or non-binary voice profiles.

### Open Question 2
- Question: Do SFMs amplify human-like perceptual biases regarding voice quality, or do they simply mirror them?
- Basis in paper: [explicit] The discussion notes that "models may not only mirror human biases but also amplify them," but this amplification effect was not quantified.
- Why unresolved: The study establishes the presence of bias (e.g., lower ratings for female voices) but lacks a direct comparative baseline of human perception on the specific VQ-Bench tasks to measure the amplification factor.
- What evidence would resolve it: A parallel human subject study using VQ-Bench stimuli to compare human ratings against model outputs.

### Open Question 3
- Question: Is the lack of sensitivity to voice quality in the OpenAI API a systemic failure in proprietary SFMs or a specific architecture limitation?
- Basis in paper: [inferred] The authors report the "OpenAI real-time speech-to-speech API proves unreliable, defaulting to classifying nearly all samples as male," which forced them to exclude it from the main analysis.
- Why unresolved: It is unclear if the failure to detect gender or respond to phonation was due to safety guardrails, model architecture, or specific audio processing choices in that proprietary model.
- What evidence would resolve it: Extending the VQ-Bench evaluation to a wider range of state-of-the-art proprietary and open-source SpeechLLMs to identify if this insensitivity is widespread.

## Limitations
- Synthetic vs. natural voice quality: The study relies on artificially manipulated voice quality rather than naturally occurring phonation variation, raising questions about ecological validity.
- Binary gender framework: The evaluation uses binary gender classification, excluding non-binary speakers and potentially missing nuanced voice quality-gender interactions.
- Single SER model: Emotion prediction findings are based on one SER architecture, limiting generalizability of observed phonation-emotion associations.

## Confidence
- **High confidence**: The core finding that voice quality significantly influences SFM outputs in open-ended generation tasks (Table 2 results, CLMM analysis). The acoustic parameter manipulations successfully create distinct phonation types (Figures 1-2).
- **Medium confidence**: The specific direction of biases (breathy→affiliative, creaky→authoritative) and their magnitudes. These require validation across more SFM architectures and natural speech samples.
- **Low confidence**: The generalizability of emotion prediction shifts (Table 4) beyond the specific SER model used, and the ecological validity of synthetic voice quality manipulation.

## Next Checks
1. Extend to non-binary speakers: Test whether voice quality effects persist across gender identities beyond the binary framework used in current evaluation.
2. Validate with natural speech: Compare SFM responses to synthetically manipulated vs. naturally occurring phonation variation from the same speakers.
3. Cross-model SER validation: Replicate emotion prediction findings using multiple SER architectures with different training data to assess robustness of observed phonation-emotion associations.