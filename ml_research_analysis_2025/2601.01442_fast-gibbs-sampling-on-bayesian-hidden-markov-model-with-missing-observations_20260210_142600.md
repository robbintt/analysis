---
ver: rpa2
title: Fast Gibbs Sampling on Bayesian Hidden Markov Model with Missing Observations
arxiv_id: '2601.01442'
source_url: https://arxiv.org/abs/2601.01442
tags:
- missing
- gibbs
- proposed
- sampling
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a collapsed Gibbs sampler for Bayesian hidden
  Markov models with missing observations. The method integrates out both missing
  observations and their corresponding latent states analytically, achieving faster
  convergence rates and lower computational complexity than existing methods.
---

# Fast Gibbs Sampling on Bayesian Hidden Markov Model with Missing Observations

## Quick Facts
- arXiv ID: 2601.01442
- Source URL: https://arxiv.org/abs/2601.01442
- Reference count: 25
- Primary result: Collapsed Gibbs sampler integrates out missing observations and latent states analytically, achieving faster convergence and lower computational complexity than existing methods.

## Executive Summary
This paper proposes a collapsed Gibbs sampler for Bayesian hidden Markov models with missing observations. The method integrates out both missing observations and their corresponding latent states analytically, achieving faster convergence rates and lower computational complexity than existing methods. Theoretical analysis shows the sampler has higher spectral gap (better convergence) than vanilla or partially-collapsed Gibbs samplers. Empirical results demonstrate the method achieves similar estimation accuracy but produces significantly larger Effective Sample Size per iteration and per second, with computational time decreasing linearly as missing rate increases.

## Method Summary
The collapsed Gibbs sampler targets the posterior p(θ, y_o, z_o) where missing observations y_m and corresponding latent states z_m are marginalized analytically. The algorithm alternates between sampling parameters θ and observed latent states z_o. Forward-backward recursions are modified to compute α and β only at observed indices, using cached matrix powers A^(t_{k+1}-t_k) to handle gaps between observations. This achieves O((1-p)nT) complexity versus O(nT) for standard methods. Parameters are updated via Gibbs sampling for emission probabilities and Metropolis-within-Gibbs for transition matrices and initial state distributions.

## Key Results
- ESS per iteration increases significantly with missing rate (e.g., 0.0047 vs 0.0010 at 90% missing)
- Computational time decreases linearly with missing rate as predicted by O((1-p)nT) complexity
- Maintains similar estimation accuracy to baseline methods while achieving 4-5x speedup at high missingness
- Spectral gap analysis proves theoretical convergence advantage over vanilla and partially-collapsed samplers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating out latent states corresponding to missing observations improves MCMC convergence rate.
- Mechanism: The sampler decomposes latent states z = (z_m, z_o) where z_m corresponds to missing observations. By analytically marginalizing z_m from the joint distribution p(θ, y_o, z_o, z_m), the sampler operates on a reduced parameter space. Since spectral gap inversely relates to operator norm and smaller parameter spaces yield lower norms, this collapse yields Gap(F_c) ≥ Gap(F_m) ≥ Gap(F_g).
- Core assumption: The missingness mechanism is ignorable (Rubin, 1976); the latent Markov chain structure is preserved after marginalization.
- Evidence anchors:
  - [abstract] "integrating out both missing observations and their corresponding latent states analytically, achieving faster convergence rates"
  - [section 3.0.1] Theorem 3.2 proves spectral gap ordering; Proof 1 cites Liu (1994) for operator norm argument
  - [corpus] "Entropy contraction of the Gibbs sampler under log-concavity" provides general spectral gap analysis for Gibbs samplers, but does not specifically address collapsed variants for HMMs
- Break condition: If missingness is non-ignorable or depends on unobserved states, the collapsed marginalization no longer targets the correct posterior.

### Mechanism 2
- Claim: Computational complexity decreases linearly with missing rate because forward-backward recursions skip missing positions.
- Mechanism: Standard HMM samplers compute forward/backward probabilities α_t(i), β_t(i) at every t ∈ {1,...,T}, yielding O(nT). The collapsed sampler only computes at observed indices o = {t_1, ..., t_K}, with transitions via A^(t_{k+1}-t_k) (matrix powers). Complexity becomes O((1-p)nT) where p is missing rate. Matrix exponentials are precomputed and cached.
- Core assumption: Transition matrix A is fixed within an iteration; matrix exponential lookup is O(1) after O(T²) precomputation; n ≫ T so precomputation overhead is negligible.
- Evidence anchors:
  - [abstract] "computational time decreasing linearly as missing rate increases"
  - [section 2.3.1, Eq. 5-6] Forward recursion formula shows α computed only at observed positions using A^(gap) terms
  - [section 3.0.2] Explicit complexity analysis: "O((1-p)nT) where p is the missing rate"
  - [corpus] No directly comparable complexity analysis for missing-data HMM samplers found in neighbor corpus
- Break condition: If T is large relative to n, or if transition matrices change per-sequence without shared caching, precomputation cost dominates.

### Mechanism 3
- Claim: ESS per iteration increases because the collapsed sampler explores a lower-dimensional posterior.
- Mechanism: By removing z_m from the sampling space, each Gibbs iteration makes more "effective" moves toward stationarity. The sampler alternates between θ|y_o,z_o and z_o|θ,y_o rather than the higher-dimensional θ|y_o,z and z|θ,y_o. Fewer conditional dependencies → faster mixing per iteration.
- Core assumption: ESS accurately measures effective exploration; the target posterior remains well-specified after marginalization.
- Evidence anchors:
  - [abstract] "produces significantly larger Effective Sample Size per iteration"
  - [section 4.1, Table 2] Median ESS per iteration for collapsed Gibbs exceeds competitors at all missing rates > 0% (e.g., at 90% missing: 0.0047 vs 0.0010 for vanilla)
  - [corpus] "A fast non-reversible sampler for Bayesian finite mixture models" discusses ESS improvements via alternative MCMC designs but does not address collapsed Gibbs specifically
- Break condition: If the posterior is multimodal with modes separated primarily along z_m dimensions, collapsing may reduce exploration between modes.

## Foundational Learning

- Concept: Gibbs sampling and collapsed Gibbs
  - Why needed here: The paper builds on the principle that integrating out variables reduces mixing time. Understanding standard Gibbs (iteratively sample from full conditionals) vs. collapsed Gibbs (marginalize some variables first) is essential.
  - Quick check question: If you have p(x,y,z), what does a collapsed Gibbs sampler targeting p(x,z) do differently from a standard Gibbs sampler targeting p(x,y,z)?

- Concept: Forward-backward algorithm for HMMs
  - Why needed here: The computational speedup hinges on modified forward-backward recursions that skip missing positions. You must understand standard α/β recursions to see why this modification works.
  - Quick check question: Given transition matrix A and emission probabilities B, write the standard forward recursion for α_t(i).

- Concept: Spectral gap and MCMC convergence
  - Why needed here: The paper's theoretical contribution uses spectral gap to prove faster convergence. Spectral gap = 1 - λ₁ where λ₁ is the second-largest eigenvalue of the transition kernel.
  - Quick check question: Does a larger spectral gap mean faster or slower convergence to stationarity?

## Architecture Onboarding

- Component map:
  - Collapsed posterior module: Computes p(θ, y_o, z_o) with z_m marginalized
  - Forward-backward engine: Evaluates α, β only at observed indices using cached A^k matrices
  - Parameter sampler: Gibbs or Metropolis-within-Gibbs updates for θ = (π, A, B)
  - Precomputation cache: Stores A^k for k = 1, ..., T_max to avoid repeated matrix exponentiation

- Critical path:
  1. Identify observed indices o for each sequence
  2. Precompute and cache A^k for all gap lengths
  3. Forward pass: Compute α at each observed index using Eq. 5
  4. Backward pass: Sample z_o using Eq. 6
  5. Update parameters θ from conditionals
  6. Repeat for N iterations, discard burn-in

- Design tradeoffs:
  - Memory vs. compute: Caching A^k requires O(T²) storage but reduces per-iteration cost to O((1-p)nT)
  - Generality vs. efficiency: Current design assumes ignorable missingness; extending to non-ignorable mechanisms would require re-deriving the marginalization
  - Assumption: Emission matrix B has Dirichlet prior → closed-form conditional; A and π may require Metropolis steps

- Failure signatures:
  - ESS per iteration does not increase with missing rate: Check that z_m is truly marginalized, not sampled
  - Runtime does not decrease linearly with p: Verify cache is being used; matrix exponentials may not be precomputed
  - Estimation accuracy degrades: Check missingness assumption; non-ignorable missingness breaks the collapsed target
  - Numerical underflow in forward recursion: Long gaps produce very small A^k entries; consider log-space computation

- First 3 experiments:
  1. Reproduce Table 2 (ESS per iteration) on synthetic data with known θ. Vary p ∈ {0.1, 0.3, 0.5, 0.7, 0.9}. Confirm ESS increases with p for collapsed Gibbs but not for vanilla
  2. Profile runtime per 1000 iterations vs. p. Confirm linear decay matching O((1-p)nT) prediction. Compare against partially-collapsed baseline
  3. Apply to a real dataset with known missingness pattern (e.g., the schizophrenia data from section 5.1). Compare cross-validated prediction accuracy to verify that collapsed sampler maintains estimation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the collapsed Gibbs sampling approach be extended to handle non-ignorable missing data mechanisms while preserving its computational and convergence advantages?
- Basis in paper: [explicit] "Throughout our analysis, we maintain the ignorable missingness assumption (Rubin (1976))." The paper cites Speekenbrink and Visser (2021) who examined non-ignorable mechanisms, but does not address how the proposed method would perform under such conditions.
- Why unresolved: The analytical marginalization strategy fundamentally relies on the ignorable missingness assumption to integrate out missing observations; non-ignorable mechanisms introduce dependence between missingness and unobserved values that complicates this integration.
- What evidence would resolve it: Derivation showing how non-ignorable missingness models (e.g., selection models or pattern-mixture models) can be incorporated, with empirical comparison of convergence rates and ESS under simulated non-ignorable missingness.

### Open Question 2
- Question: How does the O(T²) matrix exponential precomputation cost affect scalability for very long sequences, and can this be mitigated?
- Basis in paper: [explicit] "the only time complexity that arises from evaluating transition matrix exponential comes from the precomputing and caching procedure, which has a time complexity of O(T²) in the worst case."
- Why unresolved: The paper assumes n≫T so this cost is "negligible," but applications like genomics or long-term medical monitoring may have sequences with T in the thousands, making T² substantial.
- What evidence would resolve it: Empirical scaling experiments with varying sequence lengths (T = 100, 1000, 10000), potentially combined with approximation methods (e.g., truncated matrix exponentials, Krylov subspace methods) and their impact on estimation accuracy.

### Open Question 3
- Question: Can the collapsed sampler be extended to HMMs with continuous or non-categorical emission distributions?
- Basis in paper: [inferred] The method is developed for "discrete-time categorical HMMs" with emission matrix B. The forward-backward recursion (Eq. 5-6) uses discrete emission probabilities p(y_t|z_t,θ), and the proof of reduced spectral gap relies on the specific structure of marginalized discrete latent states.
- Why unresolved: Continuous emissions (e.g., Gaussian HMMs) require integration rather than summation over emission probabilities, and the relationship between collapsed and uncollapsed samplers may differ when emission distributions are not conjugate.
- What evidence would resolve it: Extension of the forward-backward algorithm for continuous emissions, theoretical analysis comparing spectral gaps, and simulation studies comparing ESS per iteration between collapsed and standard samplers for Gaussian HMMs with missing data.

## Limitations

- Ignorable missingness assumption: The method fundamentally relies on ignorable missingness mechanisms, which may not hold in real-world applications where missingness depends on unobserved states
- Precomputation cost for long sequences: O(T²) matrix exponential precomputation becomes prohibitive when T is large relative to n
- Limited to discrete emissions: Current formulation assumes categorical emissions; extension to continuous emissions requires significant methodological changes

## Confidence

- **High Confidence**: The ESS improvement claims are well-supported by Table 2 showing consistent increases across missing rates, and the complexity analysis is straightforward given the modified forward-backward algorithm
- **Medium Confidence**: The spectral gap ordering relies on Liu (1994)'s operator norm argument, which applies to general Gibbs samplers but may have additional constraints in the HMM context that aren't fully specified
- **Low Confidence**: The real-world applicability beyond synthetic data and two specific case studies is not established, particularly for non-ignorable missingness patterns common in practice

## Next Checks

1. Implement the algorithm on a dataset with known non-ignorable missingness to test the sensitivity to the ignorability assumption
2. Profile memory usage for caching matrix powers A^k when T is large (e.g., T > 1000) to assess practical scalability limits
3. Compare against state-of-the-art variational inference methods for HMMs with missing data to benchmark relative performance on large-scale problems