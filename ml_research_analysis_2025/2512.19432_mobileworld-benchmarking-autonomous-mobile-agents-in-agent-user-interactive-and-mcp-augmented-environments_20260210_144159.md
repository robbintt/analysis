---
ver: rpa2
title: 'MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive
  and MCP-Augmented Environments'
arxiv_id: '2512.19432'
source_url: https://arxiv.org/abs/2512.19432
tags:
- task
- tasks
- user
- agent
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MobileWorld is a benchmark for evaluating autonomous mobile agents
  that addresses the saturation of existing benchmarks by introducing more complex
  tasks involving long-horizon planning, cross-application workflows, agent-user interaction,
  and Model Context Protocol (MCP) tool integration. The benchmark uses open-source
  alternatives to commercial apps (e.g., Mattermost for Slack) in a fully containerized
  environment to enable reproducible, deterministic evaluation through backend database
  verification, local storage inspection, and application callbacks.
---

# MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive and MCP-Augmented Environments

## Quick Facts
- arXiv ID: 2512.19432
- Source URL: https://arxiv.org/abs/2512.19432
- Authors: Quyu Kong, Xu Zhang, Zhenyu Yang, Nolan Gao, Chen Liu, Panrong Tong, Chenglin Cai, Hanzhang Zhou, Jianan Zhang, Liangyu Chen, Zhidan Liu, Steven Hoi, Yue Wang
- Reference count: 27
- Primary result: MobileWorld reveals capability gaps with state-of-the-art models achieving 51.7% success rate (best agentic framework) and 20.9% (best end-to-end model) on tasks requiring long-horizon planning and cross-application workflows.

## Executive Summary
MobileWorld introduces a benchmark for evaluating autonomous mobile agents that addresses the saturation of existing benchmarks by introducing complex tasks involving long-horizon planning, cross-application workflows, agent-user interaction, and Model Context Protocol (MCP) tool integration. The benchmark uses open-source alternatives to commercial apps in a fully containerized environment to enable reproducible, deterministic evaluation through backend database verification, local storage inspection, and application callbacks. MobileWorld contains 201 tasks across 20 applications, with 62.2% involving multi-app workflows requiring an average of 27.8 steps compared to 14.3 in existing benchmarks. Evaluation shows state-of-the-art models achieve significantly lower success rates (51.7% for the best agentic framework, 20.9% for the best end-to-end model) compared to over 90% on existing benchmarks.

## Method Summary
MobileWorld provides a containerized Android emulator environment with 20 open-source apps (Mattermost, Mastodon, Mail, Taodian, etc.) preloaded with contacts, emails, calendar events, and files. The benchmark includes 201 tasks across three categories: GUI-only operations, agent-user interaction tasks with intentionally omitted information, and MCP-augmented tasks requiring tool integration. A planner-executor framework separates high-level action planning from pixel-level grounding, with extended action space including standard GUI operations, `ask_user` for clarification, and `mcp_call` for tool integration. Evaluation uses deterministic verification through backend database queries, local storage inspection via ADB, application callbacks, and textual matching, with a maximum of 50 steps per task.

## Key Results
- State-of-the-art models achieve 51.7% success rate on MobileWorld (best agentic framework) versus over 90% on existing benchmarks
- Agent-user interaction tasks prove particularly challenging, with most models scoring below 10% while GPT-5 achieves 62.2%
- MCP-augmented tasks show near-zero success rates for most models, highlighting difficulties in hybrid GUI-tool orchestration
- Tasks require 27.8 average steps versus 14.3 in existing benchmarks, with 62.2% involving multi-app coordination versus 9.5%
- Four identified failure modes: ambiguity detection, context management, long-term memory, and reasoning/temporal-spatial awareness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing task horizon and cross-application complexity exposes capability gaps that saturated benchmarks miss.
- Mechanism: MobileWorld tasks require 27.8 average steps (vs 14.3 in AndroidWorld) and 62.2% involve multi-app coordination (vs 9.5%), forcing models to maintain long-term memory and state tracking across application boundaries.
- Core assumption: Task length and cross-app coordination correlate with real-world difficulty rather than artificial complexity.
- Evidence anchors:
  - [abstract]: "requiring nearly twice as many completion steps on average (27.8 vs. 14.3) and featuring a significantly higher proportion of multi-app tasks (62.2% vs. 9.5%)"
  - [section 1]: "whereas the best agents achieve success rates exceeding 90% on AndroidWorld, the top-performing agentic framework reaches only 51.7% on MobileWorld"
  - [corpus]: Related work A3 confirms existing benchmarks face "performance saturation on simpler tasks" but does not quantify cross-app complexity increases.
- Break condition: If models achieve >80% success on MobileWorld without architectural changes, the difficulty may stem from evaluation brittleness rather than genuine capability gaps.

### Mechanism 2
- Claim: Agent-user interaction tasks measure ambiguity recognition through forced clarification dialogues with simulated users.
- Mechanism: Tasks deliberately omit critical information (e.g., email address replaced with name "Kevin"), requiring agents to invoke `ask_user` action. A GPT-4.1-based user agent with full context responds, enabling systematic evaluation of clarification engagement.
- Core assumption: Ambiguity detection and clarification-seeking behavior are measurable through binary success/failure on intentionally incomplete instructions.
- Evidence anchors:
  - [section 3.3]: "they remove critical information from the instruction — such as replacing the full email address with only a name like 'Kevin' — making the task ambiguous or incomplete"
  - [section 4.4]: "Qwen3-VL and GELab-Zero achieve scores below 10%... In contrast, GPT-5 demonstrates strong performance (62.2%) on these tasks"
  - [corpus]: τ-bench shows similar interaction evaluation but in non-mobile domains; corpus lacks mobile-specific ambiguity benchmarks.
- Break condition: If models succeed by guessing correctly without `ask_user`, the ambiguity may be resolvable through common-sense inference rather than forcing clarification.

### Mechanism 3
- Claim: MCP-augmented tasks evaluate hybrid GUI-tool orchestration by requiring agents to choose between interface manipulation and API calls.
- Mechanism: Agents receive tool specifications in system prompts and must invoke `mcp_call` for operations like GitHub repository queries, then continue via GUI for downstream actions like email composition. Success requires strategic tool selection, not just execution.
- Core assumption: Effective agents should prefer MCP tools when they provide efficiency gains over GUI-only paths.
- Evidence anchors:
  - [section 3.2]: "We curate a collection of popular MCP servers from the Bailian platform, spanning diverse domains including geospatial navigation (Amap), code repository analysis (GitHub)"
  - [section 4.4]: "MCP-augmented tasks prove particularly challenging... Qwen3-VL is unable to properly utilize MCP tools when required to coordinate their invocation with GUI actions"
  - [corpus]: OSWorld-MCP confirms MCP tools can improve success rates (8.3% to 20.4% for OpenAI o3), validating tool utility assumption.
- Break condition: If agents with MCP access underperform pure-GUI agents on the same tasks, tool descriptions or context management may be the bottleneck rather than orchestration capability.

## Foundational Learning

- Concept: Partially Observable Markov Decision Process (POMDP) for GUI agents
  - Why needed here: MobileWorld formalizes tasks as POMDP (S, O, A, T, R) where observation includes screenshots and instructions. Understanding this framing clarifies why memory and state tracking are core challenges.
  - Quick check question: Can you explain why GUI agent tasks are "partially observable" rather than fully observable Markov decision processes?

- Concept: Model Context Protocol (MCP) as standardized tool interface
  - Why needed here: MCP is central to MobileWorld's hybrid evaluation. MCP servers expose tools via structured schemas that agents invoke through `mcp_call` actions, analogous to function calling but with external service integration.
  - Quick check question: What is the difference between an MCP tool invocation and a standard GUI action in terms of observability and determinism?

- Concept: Planner-executor architecture separation
  - Why needed here: MobileWorld's baseline framework separates high-level planning (which action to take) from low-level grounding (pixel coordinates for clicks). This enables integrating any VLM as planner without requiring pixel-level training.
  - Quick check question: Why might separating planner and executor improve evaluation fairness compared to end-to-end trained models?

## Architecture Onboarding

- Component map: Host machine (GUI agent, MCP client, user agent) -> Docker environment (Android emulator, self-hosted app backends) -> Verification layer (backend DB queries, local storage inspection, callbacks)

- Critical path: 1. Task initialization from snapshot → 2. Agent receives instruction + screenshot → 3. Planner generates action (GUI, ask_user, or mcp_call) → 4. Executor grounds coordinates if needed → 5. Action executed in emulator → 6. Observation returned → 7. Loop until `status` action → 8. Evaluator queries backend/storage for verification

- Design tradeoffs:
  - Open-source apps (Mattermost vs Slack) trade ecological validity for deterministic backend access and reproducibility
  - Snapshot-based initialization ensures consistency but prevents testing dynamic/real-time scenarios
  - GPT-4.1 user agent provides realistic clarification but introduces LLM stochasticity into interaction evaluation

- Failure signatures:
  - Hallucination without clarification: Agent guesses missing information instead of invoking `ask_user`
  - Context overflow: MCP tool returns 20k tokens, flooding context window
  - State amnesia: Agent re-processes already-completed subtasks in long-horizon workflows
  - Tool invocation failures: Incorrect tool names or arguments cause MCP calls to fail silently

- First 3 experiments:
  1. Baseline replication: Run GPT-5 + UI-Ins-7B on MobileWorld subset (20 tasks across categories) to verify reproduction of reported 51.7% success rate.
  2. Ablation on action space: Disable `ask_user` and `mcp_call` actions to measure contribution of extended capabilities to overall success.
  3. Error taxonomy: Manually annotate 50 failed trajectories to quantify distribution across the five identified challenges (ambiguity detection, context management, memory, reasoning, temporal-spatial awareness).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can mobile agents be trained to proactively engage in clarification dialogues when facing ambiguous instructions rather than hallucinating missing parameters?
- Basis in paper: [explicit] Section 4.6 identifies "ambiguity detection and clarification engagement" as a critical research gap where models often hallucinate incorrect assumptions.
- Why unresolved: Current end-to-end models score below 10% on agent-user interaction tasks and struggle to identify when instructions are incomplete.
- What evidence would resolve it: Improved User Interaction Quality (UIQ) scores and success rates on tasks with intentionally omitted information.

### Open Question 2
- Question: What mechanisms can effectively manage MCP tool outputs that exceed context window limits?
- Basis in paper: [explicit] The paper highlights "MCP context management" as a challenge, noting tool responses (e.g., 20k tokens) can flood the context window and cause failure.
- Why unresolved: Agents currently fail to extract specific data from lengthy raw text responses returned by tools like GitHub or arXiv MCPs.
- What evidence would resolve it: Successful completion of MCP-augmented tasks involving large document retrieval without context truncation errors.

### Open Question 3
- Question: How can agents maintain state awareness to avoid destructive action loops in long-horizon tasks?
- Basis in paper: [explicit] The paper cites "long-term memory and state checking" as a challenge, specifically analyzing failure cases where agents rename files multiple times.
- Why unresolved: Agents lack the memory architecture to track completed substeps, leading to repetitive and conflicting operations.
- What evidence would resolve it: Deterministic completion of sequential file operations or similar state-dependent workflows without redundant actions.

## Limitations
- The use of open-source app substitutes (Mattermost vs Slack) may underestimate real-world agent performance
- MCP integration introduces significant context management challenges that may inflate difficulty beyond core GUI reasoning capabilities
- The 50-step maximum per task could truncate legitimate but lengthy solution paths, artificially deflating success rates
- Evaluation methodology relies on deterministic backend verification, which may not generalize to systems without accessible state storage

## Confidence
- High confidence: Task complexity measurements (27.8 vs 14.3 steps, 62.2% vs 9.5% multi-app) are directly computable from task definitions and verifiable through the provided benchmark
- Medium confidence: The interpretation that low success rates indicate capability gaps rather than benchmark design issues requires careful consideration of alternative explanations
- Low confidence: The claim that MCP-augmented tasks specifically test "hybrid GUI-tool orchestration" rather than context window management or tool description quality cannot be validated without controlled ablations

## Next Checks
1. Reproduce the baseline: Run GPT-5 + UI-Ins-7B on MobileWorld evaluation server to verify the reported 51.7% success rate within 5% tolerance across the same task subset.
2. Ablate extended action space: Compare performance with and without `ask_user` and `mcp_call` actions on identical task sets to quantify their contribution to the overall success rate differential from existing benchmarks.
3. Analyze error sources: Conduct blind manual annotation of 100 failed trajectories to measure the actual distribution of failure modes versus the hypothesized distribution.