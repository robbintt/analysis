---
ver: rpa2
title: 'ITPP: Learning Disentangled Event Dynamics in Marked Temporal Point Processes'
arxiv_id: '2511.06032'
source_url: https://arxiv.org/abs/2511.06032
tags:
- event
- itpp
- mtpp
- types
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ITPP, a channel-independent marked temporal
  point process model designed to address the limitations of conventional channel-mixing
  approaches. ITPP disentangles type-specific event dynamics using an encoder-decoder
  architecture with an ODE-based backbone and captures inter-type correlations through
  a type-aware inverted self-attention mechanism.
---

# ITPP: Learning Disentangled Event Dynamics in Marked Temporal Point Processes

## Quick Facts
- arXiv ID: 2511.06032
- Source URL: https://arxiv.org/abs/2511.06032
- Reference count: 28
- Primary result: Channel-independent marked temporal point process model outperforms state-of-the-art baselines on both probabilistic fitting and prediction tasks

## Executive Summary
This paper introduces ITPP, a channel-independent marked temporal point process model designed to address the limitations of conventional channel-mixing approaches. ITPP disentangles type-specific event dynamics using an encoder-decoder architecture with an ODE-based backbone and captures inter-type correlations through a type-aware inverted self-attention mechanism. Comprehensive experiments on four real-world and two synthetic datasets show that ITPP outperforms state-of-the-art models in both probabilistic fitting and prediction tasks, achieving significant gains in joint negative log-likelihood (TM-NLL), time loss (T-NLL), and mark loss (M-NLL) metrics. The model also demonstrates improved robustness and reduced overfitting, particularly on smaller datasets, highlighting the benefits of disentangling event dynamics while explicitly modeling cross-type dependencies.

## Method Summary
ITPP is a marked temporal point process model that processes asynchronous event sequences through a channel-independent architecture. The model decomposes event sequences into K separate channels (one per event type), where each channel's state evolves independently via shared-parameter ODEs with separate drift and jump networks. A type-aware inverted self-attention mechanism then captures cross-channel dependencies using shared weight matrices but channel-specific biases. The final intensity decoder outputs non-negative intensity functions for each event type, enabling probabilistic modeling of both event timing and type prediction through maximum likelihood estimation.

## Key Results
- ITPP achieves significant improvements in joint negative log-likelihood (TM-NLL) across all four real-world datasets compared to state-of-the-art baselines
- The model demonstrates superior robustness and reduced overfitting, particularly on smaller datasets like Earthquake (7 event types)
- ITPP shows consistent performance gains in both time prediction (lower T-NLL) and mark prediction (higher F1-score) tasks

## Why This Works (Mechanism)

### Mechanism 1
Channel-independent encoding reduces interference between heterogeneous event type dynamics, improving generalization especially on smaller datasets. Event sequences are decomposed into K separate channels (one per event type). Each channel's state evolves independently via shared-parameter ODEs, preventing entanglement of disparate temporal patterns (e.g., browsing patterns for clothing vs. smartphones). Core assumption: Type-specific dynamics contain the most predictive information; inter-type correlations are secondary and can be captured by a dedicated layer rather than entangled encoding. Evidence: Figure 1 visualizes the conceptual difference; ablation (Fig 7) shows ~10-15% TM-NLL improvement on Earthquake (small dataset). Break condition: If most predictive signal comes from cross-type interactions rather than type-specific history.

### Mechanism 2
Type-aware inverted self-attention captures cross-channel dependencies while preserving type-specific inductive biases. Queries, keys, and values use shared weight matrices (W^Q, W^K, W^V) across channels but employ channel-specific biases (b^Q_k, b^K_k, b^V_k). Attention operates across channels (K channels as sequence length), not across time steps. Core assumption: Different event types have inherent semantic differences that should not be treated as homogeneous tokens in vanilla attention. Evidence: Eq. 5-8 define the mechanism; ablation (Fig 8) shows consistent degradation without it across both large (StackOverflow) and small (Earthquake) datasets. Break condition: If event types are semantically similar or if the number of types (K) is very small.

### Mechanism 3
ODE-based backbone with separate drift and jump networks captures both smooth inter-event evolution and abrupt state changes at events. Between events, state z_k(t) evolves via neural ODE drift function f_θf. At event occurrence, jump network g_θg updates the state. This enables continuous-time intensity computation λ_k(t) = r_θr(h_k(t)). Core assumption: Event dynamics are well-approximated by piecewise-smooth trajectories with discrete jumps. Evidence: Fig 3 illustrates extrapolation/jump; Fig 5-6 show ITPP achieves lowest intensity recovery MAPE on synthetic Hawkes and Poisson data. Break condition: If events are densely packed (short inter-event times), ODE solver cost may dominate without accuracy gains.

## Foundational Learning

- **Conditional Intensity Functions in Point Processes**: Why needed: The entire framework outputs λ_k(t), the instantaneous rate of type-k events given history. Understanding that intensity must be non-negative and integrates to survival probability (Eq. 10-11) is essential. Quick check: Given λ(t) = 2 for t ∈ [0,1], what is the probability of no events occurring in this interval? (Answer: exp(-2))

- **Neural ODEs and Adjoint Sensitivity**: Why needed: The encoder uses continuous-time dynamics dz/dt = f(z,t), trained via backpropagation through ODE solvers. Understanding how gradients flow through the adjoint method (Eq. 11) is necessary for debugging convergence. Quick check: Why might standard backpropagation through an ODE solver be memory-inefficient compared to the adjoint method?

- **Self-Attention Mechanisms (Query-Key-Value)**: Why needed: The type-aware inverted attention (Eq. 5-8) builds on vanilla transformer attention. The key modification is channel-specific biases in Q/K/V projections. Quick check: In standard self-attention, why do we divide QK^T by √d_k? How does this relate to gradient stability?

## Architecture Onboarding

- Component map: Input: Sequence S = {(t_i, m_i)} -> Channel Decomposition: Split into K channels by event type -> Channel-Independent Context Encoding (ODE + Jumps): Shared drift network f_θf, Shared jump network g_θg, Output: z_k(t) for each channel -> Type-Aware Inverted Self-Attention: Shared W^Q, W^K, W^V matrices, Channel-specific biases b^Q_k, b^K_k, b^V_k, Attention across K channels, Output: h_k(t) -> Channel-Independent Intensity Decoding: Shared decoder r_θr, Output: λ_k(t) for each channel -> Loss: Negative Log-Likelihood (Eq. 11)

- Critical path: 1. ODE solver integration (Tsit5) — computational bottleneck during training 2. Intensity integral computation (Eq. 11, second term) — must accumulate during forward pass 3. Channel bias initialization — can dominate early attention if poorly scaled

- Design tradeoffs: Shared vs. channel-specific parameters: Encoder/decoder share parameters across channels (efficiency, regularization), but attention uses channel-specific biases (expressiveness). Alternative: fully independent channels would increase parameters K-fold. ODE solver precision vs. speed: Paper uses Tsit5 (adaptive Runge-Kutta). Coarser solvers speed training but may miss fine-grained dynamics. Number of event types (K): Large K (e.g., MIMIC: 75 types) increases attention complexity O(K²) but improves disentanglement benefits.

- Failure signatures: Training loss decreases but test loss increases sharply after early epochs → overfitting in channel-mixing baseline; check that channel independence is enabled. Intensity recovery poor despite low NLL → ODE solver tolerance too loose; reduce rtol/atol. M-NLL improves but T-NLL degrades → expected trade-off on high-K datasets (paper notes this); verify if mark prediction is priority.

- First 3 experiments: 1. Sanity check on synthetic Hawkes: Train on provided Hawkes dataset; verify intensity recovery MAPE < 0.05 (Fig 6). Confirms ODE dynamics learning correctly. 2. Ablation: Channel independence: Run ITPP vs. ITPP w/o CI on Earthquake (small dataset). Expect >10% TM-NLL gap (Fig 7b). Validates disentanglement benefit. 3. Ablation: Inverted attention: Run ITPP vs. ITPP w/o IA on StackOverflow. Expect noticeable degradation (Fig 8a). Validates cross-channel correlation modeling.

## Open Questions the Paper Calls Out

### Open Question 1
How does ITPP scale to high-dimensional event spaces with thousands of event types, given the potential overhead of simulating independent ODE channels and the quadratic complexity of the attention mechanism? Basis: Experiments are limited to datasets with relatively few event types (maximum 75 in MIMIC), whereas applications like e-commerce often involve thousands of distinct event types. Why unresolved: The computational burden of solving K independent ODEs and applying attention matrices scales with K, potentially limiting applicability to large-taxonomy scenarios. What evidence would resolve it: Performance and runtime benchmarks on synthetic or real-world datasets with significantly larger type vocabularies (e.g., K > 500).

### Open Question 2
What is the specific mechanism driving the trade-off where improved mark prediction (M-NLL) appears to come at the expense of time prediction accuracy (T-NLL) on datasets with many event types? Basis: The authors explicitly note this trade-off in the "Probabilistic evaluation" section and hypothesize that channel independence aids mark distinction but shifts focus away from time components. Why unresolved: The paper observes the phenomenon but does not isolate the theoretical reason why disentanglement would degrade time modeling relative to mark modeling. What evidence would resolve it: A theoretical analysis of the loss landscape or an ablation study decoupling the time and mark optimization pathways within the channel-independent architecture.

### Open Question 3
What is the computational efficiency of the channel-independent ODE backbone compared to standard channel-mixing approaches during training and inference? Basis: The paper relies on Neural ODEs, which are computationally intensive, and the "channel-independent" strategy requires simulating dynamics for K channels, yet no wall-clock timing comparisons are provided. Why unresolved: Without efficiency metrics, it is unclear if the performance gains justify the potential increase in training time and memory usage compared to simpler RNN or Transformer baselines. What evidence would resolve it: Reporting training time per epoch and inference latency for ITPP against baselines like RMTPP or THP.

## Limitations
- Key architectural details remain underspecified including hidden state dimensions, number of attention heads, and precise neural network architectures for drift/jump/decoder functions
- Results are primarily compared against channel-mixing baselines without direct comparison to other disentangled approaches
- Computational efficiency and scalability to large numbers of event types are not thoroughly evaluated

## Confidence
- **High confidence**: Core theoretical contributions (channel-independent ODE encoding and type-aware inverted attention) are well-supported by mathematical formulation and ablation studies
- **Medium confidence**: Experimental results demonstrating superiority over baselines are robust, but lack of complete hyperparameter details reduces confidence in absolute magnitude of improvements
- **Low confidence**: Claims about robustness and reduced overfitting are not extensively validated across different dataset sizes or compared to other regularization techniques

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Conduct systematic experiments varying hidden dimensions, learning rates, and attention heads to determine stability of reported improvements
2. **Alternative Disentanglement Baselines**: Implement and compare against other disentangled MTPP approaches to isolate specific contribution of inverted attention mechanism
3. **Scalability Testing**: Evaluate ITPP performance on datasets with varying numbers of event types (K) to quantify computational overhead and identify breaking points of O(K²) complexity