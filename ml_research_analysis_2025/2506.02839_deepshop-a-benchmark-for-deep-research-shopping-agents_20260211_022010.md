---
ver: rpa2
title: 'DeepShop: A Benchmark for Deep Research Shopping Agents'
arxiv_id: '2506.02839'
source_url: https://arxiv.org/abs/2506.02839
tags:
- agents
- product
- research
- query
- shopping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeepShop introduces a benchmark for evaluating web agents in realistic
  online shopping scenarios. Existing benchmarks use overly simple queries, failing
  to reflect real-world complexity involving product attributes, search filters, and
  sorting preferences.
---

# DeepShop: A Benchmark for Deep Research Shopping Agents

## Quick Facts
- arXiv ID: 2506.02839
- Source URL: https://arxiv.org/abs/2506.02839
- Reference count: 40
- Agents struggle with filters and sorting, achieving low overall success rates on realistic shopping queries

## Executive Summary
DeepShop introduces a benchmark for evaluating web agents in realistic online shopping scenarios. Existing benchmarks use overly simple queries, failing to reflect real-world complexity involving product attributes, search filters, and sorting preferences. DeepShop addresses this by evolving real user queries through diversity and complexity enhancement, generating tasks across five product categories (Books, Electronics, Home, Fashion, Sports) with three difficulty levels (easy, medium, hard). It employs an automated evaluation framework assessing fine-grained aspects (product attributes, filters, sorting) and holistic success rates. Experiments show simple RAG methods struggle without web interaction, while web agents and deep research systems face challenges with filters and sorting, resulting in low overall success rates. The benchmark reveals critical limitations in current agents and provides insights for advancing deep research shopping agents.

## Method Summary
DeepShop evolves real user queries from existing datasets (Mind2Web-Live, WebVoyager) through iterative complexity enhancement. Starting with 50 seed queries, it applies five rounds of evolution using GPT-4o to add product attributes, search filters, or sorting preferences. The benchmark spans five product categories (Books, Electronics, Home, Fashion, Sports) and three difficulty levels. An automated evaluation framework assesses agent performance through fine-grained metrics (attribute match, filter application, sorting correctness) and holistic success rates. Agents interact with Amazon in real-time via Playwright/Selenium, and evaluation combines LLM-based analysis (GPT-4o) with human spot-checking.

## Key Results
- Simple RAG methods achieve near-zero success on complex queries due to lack of web interaction capabilities
- Web agents struggle with filters (3.4% success) and sorting (52% on attributes but only 3.4% on sorting)
- Deep research systems hallucinate constraints, claiming products meet criteria when verification proves otherwise
- Filter and sorting requirements are the primary bottlenecks for agent performance

## Why This Works (Mechanism)

### Mechanism 1: Iterative Query Complexity Evolution
- **If** benchmark queries are evolved iteratively across specific constraint axes (attributes, filters, sorting), **then** they better expose limitations in agent planning and grounding compared to static datasets.
- The system takes seed queries and applies $T=5$ rounds of evolution using GPT-4o, randomly selecting strategies to add product attributes, search filters, or sorting preferences.
- **Core assumption**: LLM-generated constraints logically align with feasible UI interactions on target platforms.
- **Evidence**: [Section 3.4] describes iterative complexity evolution; [Figure 4] shows statistical increase in constraints; [Corpus] validates the shift toward high-complexity benchmarks.
- **Break condition**: If generated constraints require non-existent UI elements, evaluation validity degrades.

### Mechanism 2: Decomposed Fine-Grained Evaluation
- **If** task success is decomposed into sub-metrics (attribute, filter, sorting), **then** diagnostic signal increases compared to holistic binary success.
- The evaluation pipeline checks three independent axes: product attribute match, filter application, and sorting order.
- **Core assumption**: LLM evaluator can accurately infer intent from screenshots and action histories.
- **Evidence**: [Section 3.6] defines fine-grained evaluation; [Table 2] shows disparate performance across metrics; [Corpus] highlights need for transparent evaluation.
- **Break condition**: If visual grounding fails to capture necessary context, evaluator yields false results.

### Mechanism 3: Interactive Filtering vs. Static Retrieval
- **If** a query requires relative sorting or exclusionary filtering, **then** static RAG methods fail, necessitating interactive web agents.
- Simple RAG retrieves static top-k results, while DeepShop queries often require dynamic interaction with sort buttons and filter toggles.
- **Core assumption**: Interaction is strictly necessary for these tasks.
- **Evidence**: [Section 5.1] shows RAG performance collapsing to <8% success; [Table 2] validates mechanism; [Corpus] supports move away from static retrieval.
- **Break condition**: If search engines allow complex structured queries in prompts, necessity for agentic UI interaction diminishes.

## Foundational Learning

- **Concept: Partially Observable Markov Decision Process (POMDP)**
  - Why needed: The paper formulates web shopping as a POMDP $(S, O, A, T)$, requiring understanding of navigating hidden states via observations.
  - Quick check: Can an agent observe the full state of a website directly, or must it infer state from screenshots/HTML?

- **Concept: Visual Grounding & DOM Distillation**
  - Why needed: Agents rely on mapping instructions to UI elements, with "limited grounding ability" identified as a primary error source.
  - Quick check: Why would an agent fail to click a "Sort by Price" button even if it "sees" the screen? (Answer: Segmentation errors or lack of fine-grained grounding).

- **Concept: Hallucination in Deep Research Systems**
  - Why needed: Deep research systems often hallucinate constraints, claiming products meet criteria without strict verification.
  - Quick check: Why does high reasoning capability not guarantee high DeepShop scores? (Answer: Tendency to assert facts without strict verification of the specific result link).

## Architecture Onboarding

- **Component map**: Seed Curator -> Evolution Engine -> Environment -> Agent Baselines -> Evaluator
- **Critical path**: Complexity Evolution -> Interactive Execution -> Filter/Sort Verification loop
- **Design tradeoffs**:
  - Online vs. Offline: Real websites for realism vs. reproducibility
  - LLM vs. Human Eval: Scales to 600 queries vs. accurate but costly
- **Failure signatures**:
  - RAG: High semantic relevance but 0% constraint satisfaction
  - Vision Agents: "Grounding Hallucination" from crowded UI
  - Deep Research: "Constraint Hallucination" from unverified claims
- **First 3 experiments**:
  1. Filter Robustness Test: Run Browser Use on "Hard" queries with 3+ filters
  2. Evaluator Consistency Check: Compare GPT-4o evaluation against human labels
  3. Ablation on Constraints: Test agents on sorting-only vs. attribute-only queries

## Open Questions the Paper Calls Out

- **Open Question 1**: Can multimodal fusion techniques combining HTML structure with visual context improve web agents' grounding accuracy for interactive elements like filters and sorting widgets?
  - **Basis**: Authors state future work may explore multimodal fusion after identifying vision-based agents struggle with segmentation accuracy.
  - **Why unresolved**: Current agents rely on either HTML or vision, with no established method for robust multimodal integration.
  - **Evidence needed**: Comparative evaluation showing improved grounding accuracy using multimodal fusion versus single-modality baselines.

- **Open Question 2**: How do current web agents perform on mobile-specific e-commerce layouts compared to desktop interfaces?
  - **Basis**: Limitations section states it focuses on desktop interfaces without considering mobile-specific layouts.
  - **Why unresolved**: Mobile interfaces have different UI patterns, smaller viewports, and touch-specific interactions.
  - **Evidence needed**: Benchmarking results from mobile-adapted version showing performance gaps and mobile-specific failure modes.

- **Open Question 3**: Can task-level memory modules enable agents to learn from execution failures and avoid repeating ineffective strategies across shopping tasks?
  - **Basis**: Authors note future research may explore task-level memory modules after observing agents repeatedly misuse retrievers for filtering constraints.
  - **Why unresolved**: Current agents lack mechanisms to transfer experiences across tasks, leading to repeated mistakes.
  - **Evidence needed**: Demonstrated reduction in repeated error patterns when agents are equipped with memory modules tracking past failures.

## Limitations
- Benchmark relies on Amazon's UI consistency; layout changes could invalidate evaluation
- LLM-based evaluator may not perfectly capture user intent for ambiguous constraints
- Human evaluation limited to spot-checking (n=30), raising questions about representativeness

## Confidence
- **High**: Static RAG methods fail on filter/sorting tasks due to lack of interaction capabilities
- **Medium**: Deep research systems hallucinate constraints requiring strict URL verification
- **Low**: Generalizability to non-Amazon e-commerce sites with different UI paradigms

## Next Checks
1. Run benchmark on a second e-commerce platform (Walmart or Best Buy) to verify Amazon-specific results
2. Implement human-in-the-loop evaluation for 50 randomly selected trajectories to validate LLM evaluator accuracy
3. Create synthetic query generator with contradictory constraints to test agent robustness to invalid inputs