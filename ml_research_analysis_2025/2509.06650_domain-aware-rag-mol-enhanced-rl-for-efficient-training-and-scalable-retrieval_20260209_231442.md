---
ver: rpa2
title: 'Domain-Aware RAG: MoL-Enhanced RL for Efficient Training and Scalable Retrieval'
arxiv_id: '2509.06650'
source_url: https://arxiv.org/abs/2509.06650
tags:
- uni00000013
- retrieval
- query
- training
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes MoLER, a domain-aware RAG method that combines
  continual pre-training with reinforcement learning to optimize retrieval performance.
  The core innovation is a two-stage pipeline: first, using Mixture of Losses (MoL)
  to balance domain-specific and general knowledge learning, and second, employing
  Group Relative Policy Optimization (GRPO) to maximize document recall through multi-query
  expansion and passage generation.'
---

# Domain-Aware RAG: MoL-Enhanced RL for Efficient Training and Scalable Retrieval

## Quick Facts
- arXiv ID: 2509.06650
- Source URL: https://arxiv.org/abs/2509.06650
- Authors: Hao Lin; Peitong Xie; Jingxue Chen; Jie Lin; Qingkun Tang; Qianchun Lu
- Reference count: 35
- Primary result: MoLER achieves state-of-the-art performance on NFCORPUS and SCIFACT, with improvements of 0.49% in recall metrics over nearest competitor.

## Executive Summary
This paper introduces MoLER, a domain-aware RAG framework that combines continual pre-training with reinforcement learning to optimize retrieval performance. The approach uses a two-stage pipeline: first, Mixture of Losses (MoL) continual pre-training balances domain-specific and general knowledge learning; second, Group Relative Policy Optimization (GRPO) RL maximizes document recall through multi-query expansion and passage generation. The key innovation is the Multi-query Single-passage Late Fusion (MSLF) strategy, which reduces computational overhead during RL training while preserving scalable inference via Multi-query Multi-passage Late Fusion (MMLF). Experiments demonstrate that MoLER enables smaller models to achieve performance comparable to much larger models.

## Method Summary
MoLER employs a two-stage training pipeline. First, Mixture of Losses (MoL) continual pre-training uses CE loss on domain-specific data and KL divergence on general data with a 1:1 ratio to preserve general capabilities while acquiring domain knowledge. Second, GRPO-based RL optimizes the model for document recall using MSLF during training (merging n sub-queries into a single passage) and MMLF during inference (generating n passages individually). The framework uses LoRA adapters (rank=64) for efficient fine-tuning, with vLLM rollout for RL training and RRF fusion for retrieval combination.

## Key Results
- MoLER achieves state-of-the-art performance on NFCORPUS and SCIFACT datasets
- Demonstrates 0.49% improvement in recall metrics over nearest competitor
- Enables smaller models (Qwen3-0.6B/1.7B) to achieve performance comparable to larger models
- MSLF strategy reduces computational overhead during RL training while preserving scalable inference

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** MoL continual pre-training mitigates catastrophic forgetting while acquiring domain knowledge
- **Mechanism:** Dual-loss training with CE on domain data and KL divergence on general data maintains general language capabilities while learning domain-specific information
- **Core assumption:** 1:1 ratio of domain to general data balances loss gradients without extensive hyperparameter tuning
- **Evidence:** Section 3.2 describes the dual-loss approach; DO-RAG highlights heterogeneous data integration challenges
- **Break condition:** Dense or distinct domain data may cause KL loss to act as weak regularizer, degrading coherent synthesis

### Mechanism 2
- **Claim:** MSLF reduces computational overhead during RL training compared to MMLF
- **Mechanism:** Consolidates n sub-queries into single prompt for one synthetic passage instead of n passages, reducing LLM interactions from n+1 to 2
- **Core assumption:** Single-passage optimization generalizes to multi-passage inference performance
- **Evidence:** Abstract states MSLF preserves scalable inference via MMLF; Section 3.3.1 quantifies interaction reduction
- **Break condition:** Synthetic single-passage may fail to capture semantic diversity, creating sparse or misleading reward signals

### Mechanism 3
- **Claim:** Direct RL optimization for document recall creates more robust query augmentation than supervised fine-tuning
- **Mechanism:** GRPO maximizes recall-based reward through exploration of diverse query reformulations and passage structures
- **Core assumption:** Recall metric provides smooth and accurate gradient signal for discrete text generation actions
- **Evidence:** Section 3.3 describes unsupervised RL approach; RAGSMITH supports architecture search superiority
- **Break condition:** Noisy retrieval environment or weak embedding models may cause non-deterministic rewards and policy oscillation

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed:** MoLER targets coarse-ranking stage of RAG; understanding retrieval-generation loop identifies where MQR and CQE prompts insert
  - **Quick check:** Can you distinguish between "coarse-ranking" (retrieval) and "reranking" phases in standard RAG pipeline?

- **Concept: Proximal Policy Optimization (PPO) & GRPO**
  - **Why needed:** Core training loop relies on GRPO, a PPO variant; understanding advantage estimation is crucial for grasping objective function modifications
  - **Quick check:** In standard PPO, what role does the "critic" model play, and how does GRPO replace it using "group relative" advantages?

- **Concept: Catastrophic Forgetting**
  - **Why needed:** MoL is explicit countermeasure to this phenomenon; without understanding fine-tuning degradation, dual-loss design appears redundant
  - **Quick check:** If you fine-tune a general LLM exclusively on medical data, what typically happens to its ability to summarize general news articles?

## Architecture Onboarding

- **Component map:** Raw Query -> CPT Module (MoL) -> LoRA Adapters -> MQR Module -> CQE Module -> Retrieval & Fusion -> RL Loop
- **Critical path:** Transition from MSLF (Training) to MMLF (Inference) is most sensitive architectural disconnect
- **Design tradeoffs:**
  - *Efficiency vs. Robustness:* MSLF is fast for training but theoretically lower-capacity than MMLF
  - *Dr.GRPO vs. GRPO:* Dr.GRPO removes length normalization to prevent verbose outputs at cost of potentially noisier gradients
- **Failure signatures:**
  - Length Explosion: Standard GRPO causes excessively verbose outputs without improving recall
  - Semantic Drift: Wrong MoL ratios cause domain-accurate but query-irrelevant passages
  - MSLF Overfitting: Model learns perfect merged passage that fails to generate coherent individual passages during inference
- **First 3 experiments:**
  1. Validate MoL: Train models with standard mixed loss vs. MoL; compare performance on general benchmark vs. domain dataset
  2. MSLF vs. MMLF Consistency: Train using MSLF, evaluate during training with MMLF; plot correlation to ensure improvements track
  3. Reward Sanitization: Run RL loop with fixed reward; if model still learns better queries, reward signal is leaking or optimization is unstable

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does MMLF exhibit formally provable log-linear scaling law between retrieval performance and number of generated queries/passage across diverse retrieval domains and embedding architectures?
- **Basis:** Paper observes approximately linear patterns on log-scale plots for two datasets using one embedding model, but no theoretical or cross-domain validation conducted
- **Evidence needed:** Formal theoretical analysis plus large-scale empirical validation across diverse datasets, domains, and multiple embedding models demonstrating consistent log-linear scaling with bounds and failure modes

### Open Question 2
- **Question:** Can MoLER framework be effectively extended to multimodal retrieval scenarios while maintaining training efficiency benefits of MSLF during RL optimization?
- **Basis:** Current framework designed purely for text with text-specific prompts and similarity metrics; multimodal extension requires fundamental architectural changes
- **Evidence needed:** Successful implementation and evaluation on multimodal retrieval benchmarks with analysis of computational efficiency compared to text-only scenarios

### Open Question 3
- **Question:** Does MoLER's effectiveness generalize to domains beyond biomedical and scientific literature, particularly with different query-document length distributions and relevance density characteristics?
- **Basis:** Experiments restricted to NFCORPUS (biomedical, 3.3-word queries, 38.2 relevant docs/query) and SCIFACT (scientific, 12.37-word queries, 1.1 relevant docs/query)
- **Evidence needed:** Comprehensive evaluation across BEIR benchmark datasets spanning diverse domains with analysis of performance variations relative to domain-specific characteristics

## Limitations

- Generalization of MoL to domains with vastly different data distributions remains unproven
- Critical assumption that MSLF training generalizes to MMLF inference is not directly validated
- RL reward signal quality and stability in sparse relevance datasets requires further analysis

## Confidence

- **High Confidence:** Experimental results on NFCORPUS and SCIFACT are clearly reported with consistent improvements over baselines
- **Medium Confidence:** MSLF efficiency innovation is plausible but lack of MMLF vs. MSLF consistency ablation weakens transfer assumption confidence
- **Low Confidence:** Claim that smaller models achieve "comparable" performance is based on relative improvements rather than absolute performance parity

## Next Checks

1. **MSLF vs. MMLF Ablation:** During training, evaluate model checkpoints using both MSLF (training) and MMLF (inference) modes; plot recall@n to verify that MSLF improvements track with MMLF performance

2. **MoL Ratio Sensitivity:** Systematically vary domain-to-general data ratio (1:2, 2:1, 1:3) and evaluate on held-out general benchmark to quantify catastrophic forgetting and identify optimal balance

3. **Reward Signal Stability:** Run multiple GRPO training seeds on same dataset; analyze reward variance, convergence curves, and final recall scores; test robustness by injecting synthetic noise into retrieval step to assess policy stability