---
ver: rpa2
title: Towards Effectively Leveraging Execution Traces for Program Repair with Code
  LLMs
arxiv_id: '2505.04441'
source_url: https://arxiv.org/abs/2505.04441
tags:
- trace
- program
- traces
- prompt
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether incorporating program execution
  traces into prompts improves automated program repair (APR) performance with code
  LLMs. Experiments with GPT-3.5 and GPT-4 on three datasets show that trace-based
  prompts don't consistently outperform error-only prompts.
---

# Towards Effectively Leveraging Execution Traces for Program Repair with Code LLMs

## Quick Facts
- **arXiv ID:** 2505.04441
- **Source URL:** https://arxiv.org/abs/2505.04441
- **Reference count:** 21
- **Key outcome:** Execution trace prompts don't consistently outperform error-only prompts for automated program repair with code LLMs, with effectiveness diminishing as trace complexity increases.

## Executive Summary
This study investigates whether incorporating program execution traces into prompts improves automated program repair (APR) performance with code LLMs. Experiments with GPT-3.5 and GPT-4 on three datasets show that trace-based prompts don't consistently outperform error-only prompts. The effectiveness of traces diminishes as their complexity increases, with longer traces and more variable modifications correlating with lower repair success rates. An LLM-optimized trace representation provides the most consistent performance gains, while confidence-based and trace-length-based routing strategies show limited improvement. A fine-tuned baseline underperforms prompt-based approaches. Probing studies reveal LLMs struggle with trace generation and alignment tasks, suggesting execution traces contain information not easily inferred by LLMs. The findings indicate that while execution traces can complement LLM reasoning abilities, their inclusion in prompts must be carefully optimized to be beneficial for APR tasks.

## Method Summary
The study uses GPT-3.5-Turbo and GPT-4 with six prompt types (Error-only, Trace, Self-Debug, Collated, LLM-Optimized, Confidence-routed) on three datasets: Refactory (Python student programs), RunBugRun (sampled bugs from CodeNet), and HumanEval-Java (synthetic bugs). Execution traces are generated using PySnooper, truncated at 200 lines, and postprocessed to remove timestamps and formatting. The LLM-Optimized traces use GPT-4-32k for summarization. A fine-tuning baseline uses deepseek-coder-1.3b-instruct with 80/20 train/test split. Performance is measured using Correct Fix Accuracy (CFA) and Correct Program Accuracy (CPA) metrics.

## Key Results
- Execution traces don't consistently improve repair performance compared to error-only prompts
- Effectiveness of traces diminishes with increasing complexity (longer traces, more variable modifications)
- LLM-Optimized trace representation provides the most consistent performance gains
- Confidence-based and trace-length-based routing strategies show limited improvement
- Fine-tuned baseline underperforms prompt-based approaches

## Why This Works (Mechanism)

### Mechanism 1: Runtime State Augmentation
- **Claim:** Providing execution traces fills an information gap regarding dynamic program behavior that static code analysis cannot capture, potentially improving repair accuracy.
- **Mechanism:** Execution traces expose variable states, control flow, and return values at runtime. This acts as "knowledge augmentation," grounding the LLM's reasoning in the actual execution path rather than just syntax.
- **Core assumption:** The LLM cannot reliably infer these runtime states solely from the source code and error message.
- **Evidence anchors:** [abstract] states that "probing studies reinforcing the notion that execution traces can complement the reasoning abilities of the LLMs." [section 5.2] shows GPT-4 has low accuracy (15-50%) in predicting execution traces from scratch. [corpus] Related work *DynaFix* supports the utility of dynamic execution information for APR.
- **Break condition:** If the LLM is capable of perfect simulation (mind's eye execution) of the code, the trace becomes redundant noise.

### Mechanism 2: Signal-to-Noise Optimization (Summarization)
- **Claim:** Raw execution traces often fail to improve repair due to context length and noise; optimizing (summarizing) these traces significantly improves effectiveness.
- **Mechanism:** Raw traces, particularly those with loops, contain repetitive state changes. An optimization step (using a larger context model) condenses this information, reducing the cognitive load and context window usage while retaining error-relevant signals.
- **Core assumption:** The summarizing model can distinguish between "noise" (repetitive loop steps) and "signal" (causal state changes leading to the bug).
- **Evidence anchors:** [section 3.3] finds "effectiveness of execution traces for APR diminishes as their complexity increases." [section 4.2] shows LLM-Optimized (OPT) traces provided the most consistent performance gains. [corpus] Corpus evidence specifically for *trace summarization* as a mechanism is weak.
- **Break condition:** If the trace summarization inadvertently removes the specific state change causing the bug (hallucination/elision error), the repair prompt loses its diagnostic utility.

### Mechanism 3: Model Scale and Reasoning Emergence
- **Claim:** The utility of execution traces is heavily dependent on the reasoning capability of the model; stronger models (GPT-4) leverage traces better than weaker ones (GPT-3.5).
- **Mechanism:** Traces introduce complex, long-context dependencies. Advanced models appear better at cross-referencing the trace against the code to isolate the fault.
- **Core assumption:** The performance delta is due to reasoning capacity rather than simply context window size.
- **Evidence anchors:** [section 3.2] shows trace-based prompts significantly improved GPT-4's performance on 2/3 datasets, while GPT-3.5 often saw degraded performance. [section 4.2] shows confidence-based routing worked significantly better for GPT-4 than GPT-3.5. [corpus] *RelRepair* and *PathFix* also rely on strong LLM capabilities.
- **Break condition:** If the model lacks sufficient context window or instruction-following discipline to align the trace lines with the code lines, the mechanism fails.

## Foundational Learning

- **Concept: Execution Trace Generation (Instrumentation)**
  - **Why needed here:** You cannot simply "add traces." You must generate them via a debugger or library (e.g., PySnooper) by running the failing test case.
  - **Quick check question:** Can you explain the difference between a stack trace (exception output) and a full execution trace (variable state log) used in this paper?

- **Concept: Automated Program Repair (APR) Metrics**
  - **Why needed here:** The paper distinguishes between fixing a specific test case (CFA) and fixing the whole program (CPA).
  - **Quick check question:** If a patch passes the failing test case but fails a previously passing test case (regression), how would that be reflected in the CFA vs. CPA metrics?

- **Concept: Static vs. Dynamic Analysis**
  - **Why needed here:** The core premise is bridging the gap between static (code text) and dynamic (runtime behavior).
  - **Quick check question:** Why would a variable assignment that *looks* correct in the code (static) result in a wrong value in the execution trace (dynamic)?

## Architecture Onboarding

- **Component map:** Trace Generator (e.g., PySnooper) -> executes failing test -> outputs raw log -> Preprocessor (strips timestamps/formatting; truncates at 200 lines) -> (Optional) Optimizer (LLM GPT-4-32k -> summarizes raw trace -> condensed trace) -> Repair Agent (LLM GPT-3.5/4 <- Prompt (Code + Error + Trace) -> Patch)

- **Critical path:**
  1. Running the failing test to capture the deterministic execution trace
  2. Formatting the trace (Collated vs. OPT) to ensure it aligns with the code context
  3. Ensuring the prompt stays within the context window limit (truncation logic)

- **Design tradeoffs:**
  - **Raw vs. Optimized Traces:** Raw traces provide total truth but consume context and add noise. Optimized traces save space but risk losing critical data during summarization.
  - **Collated (Inline) vs. Separate Traces:** Collated traces put logs next to code lines. The paper found this *disappointing*, likely because it "stretched" the code structure too much for the LLM.
  - **Prompting vs. Fine-tuning:** Prompting with large models outperformed fine-tuning small models on limited data, suggesting reasoning capability is more critical than domain-specific weight updates for this task.

- **Failure signatures:**
  - **Performance Degradation on GPT-3.5:** If you see error-only prompts outperforming trace prompts, check model capability; weaker models may be confused by the extra tokens.
  - **Truncation Loss:** If traces are cut off at 200 lines, critical error information at the end of execution is lost.
  - **Loop Noise:** Excessive variable modification logs in tight loops dilute the attention mechanism.

- **First 3 experiments:**
  1. **Baseline Validation:** Reproduce RQ1 by comparing *Error Prompt* vs. *Trace Prompt* on a small sample to confirm your trace integration isn't buggy.
  2. **Complexity Analysis:** Plot trace length vs. repair success (RQ2) to see if your specific data distribution suffers from the "long trace" penalty.
  3. **Optimization Ablation:** Compare Raw Trace vs. LLM-Optimized Trace prompts to validate if the cost of the extra summarization call is worth the accuracy gain.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can fine-tuning smaller code LLMs with large-scale execution trace datasets outperform trace-based prompting of proprietary models?
- **Basis in paper:** [explicit] Section 5.1 states, "In our future work, we plan to use a larger training dataset and larger models for finetuning."
- **Why unresolved:** The authors' initial fine-tuning experiment using DeepSeek-Coder-1.3b underperformed compared to prompting, but they attribute this to the limited scale of training data (only 459-517 samples) and model size rather than the methodology itself.
- **What evidence would resolve it:** An experiment fine-tuning a larger model (e.g., 7B+ parameters) on a comprehensive trace corpus (e.g., the full RunBugRun dataset) to compare against GPT-4 prompt baselines.

### Open Question 2
- **Question:** Does the efficacy of execution trace augmentation generalize to open-source code-specialized LLMs?
- **Basis in paper:** [explicit] Section 3.1 notes, "given our narrow focus on traces, we leave this to be explored in future work" regarding the inclusion of open-source and other proprietary models.
- **Why unresolved:** The study restricted evaluation to the GPT family (GPT-3.5 and GPT-4). It is unclear if open-weight models possess the necessary context window handling or reasoning capabilities to utilize the proposed LLM-optimized traces effectively.
- **What evidence would resolve it:** Replicating the prompting experiments (Trace, OPT, Collated) on diverse open-source architectures (e.g., CodeLlama, StarCoder) to measure performance deltas.

### Open Question 3
- **Question:** Is the LLM-optimized trace strategy effective for APR in complex, real-world software projects with extensive dependencies?
- **Basis in paper:** [inferred] Section 3.1 acknowledges the use of self-contained algorithmic datasets because realistic datasets require "significant manual effort due to complex dependencies."
- **Why unresolved:** The findings rely on student submissions and synthetic bugs. In real-world codebases, execution traces may be significantly longer and noisier, potentially exacerbating the "context dilution" issues observed with standard trace prompts.
- **What evidence would resolve it:** Evaluation of the proposed trace-optimization strategies on established real-world bug benchmarks like Defects4J or PyTraceBugs.

## Limitations

- The study's findings are constrained by the specific execution trace generation approach (PySnooper) and the fixed 200-line truncation threshold, which may exclude critical error information in complex programs.
- The probe studies showing LLMs' inability to infer execution traces (15-50% accuracy) suggest fundamental limitations in LLM reasoning capabilities that may not generalize to larger or more capable models.
- The confidence-based routing strategy's limited effectiveness across both GPT-3.5 and GPT-4 indicates that LLMs may struggle with reliable self-assessment of trace quality, a core assumption in adaptive APR systems.

## Confidence

- **High confidence:** The core finding that execution traces do not consistently improve APR performance, and that effectiveness diminishes with trace complexity (RQ1, RQ2).
- **Medium confidence:** The specific effectiveness of LLM-Optimized trace representation as the most consistent performer (RQ3), as this depends on the quality of the summarization model and prompt.
- **Low confidence:** The generalizability of routing strategies (trace-length, confidence-based) beyond the tested datasets, as these showed limited improvement and may be sensitive to specific program characteristics.

## Next Checks

1. **Trace truncation sensitivity analysis:** Systematically vary the 200-line truncation threshold across different program complexity levels to quantify the trade-off between context window constraints and error information retention.

2. **Cross-model capability validation:** Test the probe study methodology (trace generation and alignment tasks) on larger models (GPT-4-32k, Claude-3) to determine if the observed reasoning limitations are fundamental or scale-dependent.

3. **Alternative trace generation validation:** Implement execution trace generation using alternative instrumentation approaches (coverage-guided fuzzing, symbolic execution) to assess whether the observed limitations stem from the trace format or the underlying information.