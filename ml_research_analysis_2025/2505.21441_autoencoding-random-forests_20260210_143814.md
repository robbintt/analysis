---
ver: rpa2
title: Autoencoding Random Forests
arxiv_id: '2505.21441'
source_url: https://arxiv.org/abs/2505.21441
tags:
- data
- leaf
- kernel
- each
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a principled method for autoencoding with random
  forests (RFs). The authors establish key properties of the RF kernel, including
  asymptotic universality, and use these results to motivate spectral graph theory-based
  encoding via diffusion maps.
---

# Autoencoding Random Forests

## Quick Facts
- arXiv ID: 2505.21441
- Source URL: https://arxiv.org/abs/2505.21441
- Reference count: 40
- Key outcome: Proposes RF-based autoencoding with universal consistency and competitive performance vs. neural methods

## Executive Summary
This paper establishes a principled method for autoencoding with random forests (RFs) by proving key properties of the RF kernel, including asymptotic universality. The method uses spectral graph theory-based encoding via diffusion maps and introduces three novel decoding methods: constrained optimization, split relabeling, and k-nearest neighbors regression. The approach demonstrates competitive performance with state-of-the-art neural autoencoders across compression, reconstruction, and denoising tasks on tabular, image, and genomic data, with the k-NN decoder showing the best balance of accuracy and efficiency.

## Method Summary
The method trains a random forest (supervised or unsupervised via ARF), extracts the RF kernel matrix through co-location probabilities, performs eigendecomposition to obtain diffusion map embeddings, and decodes using k-NN regression in the latent space. The encoder projects data into a lower-dimensional space while preserving diffusion distances, and the k-NN decoder reconstructs features by finding nearest neighbors in the embedding space and applying inverse-distance-weighted averaging for continuous features or weighted majority voting for categorical features.

## Key Results
- k-NN decoder shows best balance of accuracy and efficiency across all evaluated tasks
- Competitive performance vs. deep autoencoders on compression-distortion trade-off with MNIST, genomic, and tabular data
- Universal consistency established under regularity assumptions for all three decoding methods
- Effective denoising of single-cell RNA-seq data by projecting and reconstructing target batches

## Why This Works (Mechanism)

### Mechanism 1
- The RF kernel captures learnable structure through averaging normalized co-location probabilities across all trees, creating a positive semidefinite, doubly stochastic kernel that enables spectral embedding
- Regularity conditions (A1-A5) ensure leaf regions shrink with n, allowing kernel to separate distinct points
- The kernel's asymptotic universality allows approximation of any continuous function, justifying its use for representation learning

### Mechanism 2
- k-NN decoding is universally consistent because diffusion maps preserve diffusion distances, making Euclidean proximity in the embedding space correspond to graph proximity in the original RF kernel space
- The consistency requires k→∞ and k/n→0 as n→∞, ensuring local averaging while controlling bias
- The decoder retrieves k nearest neighbors among training embeddings and reconstructs each feature via inverse-distance-weighted averaging or weighted majority vote

### Mechanism 3
- Leaf assignment inference enables exact reconstruction when the encoder is lossless by solving an integer linear program to recover binary leaf assignment vectors
- The ILP finds the leaf assignments that best reproduce the kernel row under one-hot and overlap constraints
- The intersection of assigned leaf regions defines a bounding box for reconstruction, though the NP-complete nature of ILP makes this approach computationally challenging

## Foundational Learning

- **Reproducing Kernel Hilbert Spaces (RKHS)**: The RF kernel induces an RKHS that is asymptotically universal, enabling approximation of any continuous function. Quick check: Given a kernel k, what does the Moore-Aronszajn theorem guarantee?

- **Diffusion Maps / Spectral Graph Theory**: The encoder applies diffusion maps to the RF kernel matrix. Quick check: Why does the doubly stochastic property of K allow interpreting it as a Markov transition matrix?

- **Universal Consistency**: The paper's theoretical contribution relies on proving universal consistency under regularity assumptions. Quick check: What two conditions on k are required for k-NN regression to be universally consistent?

## Architecture Onboarding

- **Component map**: Train RF -> Extract kernel K via Eq.1 -> Eigendecomposition -> Top d_Z eigenvectors form embedding Z -> k-NN decoder finds neighbors -> Weighted feature reconstruction

- **Critical path**: Encoding requires O(n²) memory for K; eigendecomposition is O(n³) but truncated methods exist. k-NN decoding is O(m log n) with kd-trees and is recommended as default.

- **Design tradeoffs**:
  - k-NN decoder: Fast, consistent, but requires storing training embeddings and leaf bounds
  - Split relabeling: One-time O(B × depth × ñ log ñ) cost for relabeling, then O(depth) per inference
  - Constrained optimization: Theoretically optimal under lossless encoding, but ILP is NP-complete; lasso relaxation is O(d_Φ³) per sample

- **Failure signatures**:
  - Embeddings cluster indiscriminately → Trees too shallow; increase depth or B
  - k-NN reconstruction has high categorical error → k too small or embedding dimension insufficient
  - ILP/split relabeling fails to converge → d_Z too small; increase embedding dimension or use k-NN fallback

- **First 3 experiments**:
  1. Reproduce MNIST visualization to validate encoder: train RF on digits, plot 2D diffusion map as depth increases
  2. Run compression-distortion benchmark on one tabular dataset: compare k-NN vs. split relabeling vs. lasso decoders across d_Z ∈ {2, 4, 8, 16}
  3. Validate denoising on scRNA-seq: train RFAE on reference batch, project and decode target batch, measure batch effect reduction via t-SNE overlap

## Open Questions the Paper Calls Out

### Open Question 1
- Can the spectral autoencoding framework be adapted for gradient boosting machines (GBMs) or other tree-based ensembles?
- The conclusion states future work will investigate other tree-based algorithms, but current theoretical guarantees rely on RF averaging mechanisms that may not apply to sequential boosting.

### Open Question 2
- Can the RFAE framework be extended to perform generative modeling rather than just reconstruction?
- While the method handles density estimation via ARFs, the current decoding pipeline is designed for inverting specific embeddings of known data, not sampling novel instances.

### Open Question 3
- Is it possible to utilize the decoding insights to perform model distillation, creating a compact standalone model?
- Current decoding relies on accessing the original RF's splits and leaf assignments; distillation would require the decoder to function independently of the original ensemble structure.

## Limitations
- Kernel estimation requires O(n²) memory for the full n×n matrix K, creating scalability bottlenecks
- Theoretical guarantees rely on regularity assumptions that may not hold in high-dimensional sparse settings
- No ablation on ARF vs. supervised RF training for downstream reconstruction quality

## Confidence
- **High**: RF kernel is PSD and asymptotically universal (Theorem 3.4)
- **High**: k-NN decoder is universally consistent under stated conditions (Theorem 4.4)
- **Medium**: Leaf assignment ILP uniquely solves under lossless encoding (Theorem 4.3)
- **Medium**: Competitive empirical performance vs. deep autoencoders
- **Low**: Lasso relaxation consistently approximates ILP solution

## Next Checks
1. **Memory scaling**: Profile RFAE on synthetic data with n ∈ {1k, 10k, 100k} to measure kernel matrix memory and eigendecomposition time
2. **Assumption stress test**: Systematically vary tree depth and feature sampling ρ to identify when regularity conditions break
3. **Cross-domain robustness**: Apply RFAE to at least one additional domain (e.g., financial time series) to verify generalization beyond evaluated domains