---
ver: rpa2
title: 'City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal
  Incomplete Learning'
arxiv_id: '2507.12795'
source_url: https://arxiv.org/abs/2507.12795
tags:
- scene
- understanding
- city-vlm
- multimodal
- outdoor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of outdoor scene understanding
  across multiple scales, views, and modalities, which existing large vision-language
  models (LVLMs) struggle with due to limited data and inadequate multimodal fusion
  methods. The authors propose a new dataset, SVM-City, comprising 420k images and
  4,811M point clouds with 567k QA pairs collected from vehicles, drones, planes,
  and satellites.
---

# City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning

## Quick Facts
- arXiv ID: 2507.12795
- Source URL: https://arxiv.org/abs/2507.12795
- Authors: Penglei Sun; Yaoxian Song; Xiangru Zhu; Xiang Liu; Qiang Wang; Yue Liu; Changqun Xia; Tiefeng Li; Yang Yang; Xiaowen Chu
- Reference count: 40
- Multi-scale outdoor perception model achieving 18.14% better performance than existing LVLMs on average

## Executive Summary
This paper addresses the challenge of outdoor scene understanding across multiple scales, views, and modalities, which existing large vision-language models (LVLMs) struggle with due to limited data and inadequate multimodal fusion methods. The authors propose a new dataset, SVM-City, comprising 420k images and 4,811M point clouds with 567k QA pairs collected from vehicles, drones, planes, and satellites. They also introduce City-VLM, an LVLM using incomplete multimodal learning with an Incomplete Multimodal Fusion Module (IMF Module) that constructs a joint probabilistic distribution space for robust fusion even when one modality is missing. Experiments on three outdoor QA tasks show City-VLM achieves 18.14% better performance than existing LVLMs on average, with improvements up to 30% on low-altitude tasks.

## Method Summary
The proposed approach centers on City-VLM, a large vision-language model designed for multidomain outdoor perception through multimodal incomplete learning. The key innovation is the Incomplete Multimodal Fusion Module (IMF Module), which constructs a joint probabilistic distribution space for fusing image and point cloud data even when one modality is missing. The model is trained on SVM-City, a comprehensive dataset containing 420k images and 4,811M point clouds collected from multiple altitudes (vehicles, drones, planes, satellites) with 567k QA pairs. The IMF Module handles missing modalities through zero-padding while maintaining fusion capabilities, enabling robust performance across different sensor configurations and data availability scenarios.

## Key Results
- City-VLM achieves 18.14% better performance than existing LVLMs on average across three outdoor QA tasks
- Demonstrates up to 30% improvement specifically on low-altitude perception tasks
- Successfully handles incomplete multimodal data through the IMF Module's probabilistic fusion approach

## Why This Works (Mechanism)
The effectiveness stems from the IMF Module's ability to construct a joint probabilistic distribution space that can fuse image and point cloud data even when one modality is absent. By treating incomplete multimodal learning as a probabilistic inference problem rather than a deterministic fusion, the model maintains robustness when sensors fail or data is missing. The multidomain dataset provides diverse training scenarios across different altitudes and perspectives, enabling the model to generalize across various outdoor perception contexts. The autoregressive generation mechanism, while having limitations for counting tasks, provides strong performance for most perception and reasoning scenarios.

## Foundational Learning
- Multimodal fusion fundamentals: Understanding how different sensor modalities can be combined for enhanced perception
  - Why needed: Outdoor perception requires integrating information from multiple sensors operating at different scales
  - Quick check: Can the model fuse at least two distinct modalities (image + point cloud) effectively
- Probabilistic modeling for incomplete data: Using joint probability distributions to handle missing information
  - Why needed: Real-world sensors frequently fail or provide incomplete data, requiring robust fusion methods
  - Quick check: Does the model maintain performance when one modality is absent
- Autoregressive generation for QA: Sequential token generation for answering questions based on visual inputs
  - Why needed: Enables natural language responses to perception queries across multiple scales
  - Quick check: Can the model generate coherent answers to perception questions
- Large-scale dataset curation: Collecting and annotating data from multiple sources and scales
  - Why needed: Multidomain perception requires diverse training data across different altitudes and perspectives
  - Quick check: Does the dataset contain sufficient variety across the target perception domains

## Architecture Onboarding

Component Map:
Visual Encoder (Image) -> IMF Module -> Language Decoder -> QA Output
Visual Encoder (Point Cloud) -> IMF Module -> Language Decoder -> QA Output

Critical Path:
The IMF Module serves as the critical component, handling the fusion of image and point cloud data while managing missing modalities. This probabilistic fusion layer determines the model's ability to maintain performance across different sensor configurations.

Design Tradeoffs:
- Probabilistic fusion vs. deterministic fusion: The IMF Module's probabilistic approach enables handling missing data but may introduce computational overhead compared to simpler concatenation methods
- Autoregressive generation vs. classification heads: The choice of autoregressive generation enables flexible QA but struggles with precise counting tasks
- Zero-padding for missing data vs. learned imputation: The current approach uses zero-padding for simplicity, which may not be optimal for noisy or partially corrupted data

Failure Signatures:
- Performance degradation on counting tasks due to autoregressive generation limitations
- Potential sensitivity to sensor noise when using zero-padding for missing modalities
- Possible training instability when scaling to more than two modalities

First 3 Experiments:
1. Ablation study comparing IMF Module with and without handling missing modalities to validate the incomplete learning approach
2. Performance evaluation on counting-specific questions to quantify the autoregressive generation limitation
3. Scalability test fusing three or more modalities to assess the IMF Module's probabilistic distribution construction capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the City-VLM architecture be adapted to overcome the specific performance drop in counting tasks attributed to the autoregressive generation mechanism?
- **Basis in paper:** [explicit] Section 5.4 (Nuscenes-QA Comparative Experiments) notes that the model "encounters difficulties with counting-related questions," explicitly attributing this to the "limitations of autoregressive methods in accurately handling counting tasks."
- **Why unresolved:** The current generative architecture produces tokens sequentially, which is inherently less precise for numerical regression or exact counting than detection-based classification heads.
- **What evidence would resolve it:** A study integrating a non-autoregressive counting head or a specialized numerical tokenization strategy into City-VLM, showing improved accuracy on the "Count" metrics in Nuscenes-QA.

### Open Question 2
- **Question:** To what extent does the zero-padding strategy for missing modalities degrade performance when input sensors are noisy or partially corrupted rather than completely absent?
- **Basis in paper:** [inferred] The Incomplete Multimodal Fusion Module handles missing data by padding zero tensors to maintain dimension consistency, but real-world outdoor sensors frequently output high-noise, low-fidelity data instead of total signal loss.
- **Why unresolved:** The paper evaluates robustness only against modal absence (missing sensors), not the more common outdoor failure mode of sensor degradation or data corruption.
- **What evidence would resolve it:** Experiments evaluating the IMF Module's performance on test sets with synthetic noise (e.g., Gaussian blur, LiDAR point jitter) applied to the input modalities instead of complete removal.

### Open Question 3
- **Question:** Can the probabilistic IMF Module effectively scale to fuse more than two visual modalities (e.g., simultaneously integrating satellite, drone, street-view, and thermal data) without training instability?
- **Basis in paper:** [inferred] The method formulates fusion specifically for the tuple $x_v = (x_i, x_p)$ (image, point cloud), while the introduction emphasizes "multidomain perception" involving diverse sources like vehicles, drones, planes, and satellites.
- **Why unresolved:** Constructing a joint probabilistic distribution space becomes exponentially more complex and prone to mode collapse as the number of conditional input modalities increases beyond two.
- **What evidence would resolve it:** Ablation studies showing the convergence behavior and resulting accuracy of City-VLM when trained on triplets or quadruplets of distinct input modalities.

## Limitations
- The autoregressive generation mechanism shows specific weaknesses in counting tasks, limiting precision for numerical reasoning
- Zero-padding strategy for missing modalities may not handle sensor noise and partial corruption effectively
- Scalability of the probabilistic IMF Module to more than two modalities remains unproven and may face training instability

## Confidence
High confidence: The technical approach of using incomplete multimodal learning for fusion is conceptually sound and addresses a recognized limitation in existing LVLMs. The multidomain collection strategy (vehicles, drones, planes, satellites) represents a novel contribution to outdoor scene understanding datasets.

Medium confidence: The claimed 30% improvement on low-altitude tasks, while impressive, requires verification through independent replication given the complex nature of the fusion methodology and the lack of detailed implementation specifications.

Low confidence: The specific performance metrics and comparative benchmarks against existing LVLMs are difficult to evaluate without access to the full experimental methodology and baseline model specifications.

## Next Checks
1. Conduct ablation studies specifically testing the IMF Module's performance when all modalities are present versus when one modality is missing, to validate the "incomplete learning" claims.

2. Release a subset of the SVM-City dataset with standardized splits and evaluation protocols to enable independent verification of the reported performance improvements.

3. Perform detailed error analysis comparing City-VLM's failure modes against existing LVLMs across different altitude domains to understand the practical impact of the 18.14% improvement claim.