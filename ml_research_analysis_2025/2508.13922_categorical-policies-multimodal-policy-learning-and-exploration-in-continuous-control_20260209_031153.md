---
ver: rpa2
title: 'Categorical Policies: Multimodal Policy Learning and Exploration in Continuous
  Control'
arxiv_id: '2508.13922'
source_url: https://arxiv.org/abs/2508.13922
tags:
- policy
- categorical
- learning
- action
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Categorical Policies, a multimodal approach
  for continuous control that leverages an intermediate categorical distribution to
  select discrete behavior modes before generating fine-grained continuous actions.
  The method addresses the limitation of unimodal policies by using multiple categorical
  variables with fewer classes each, enabling a structured and efficient representation
  of diverse behaviors without requiring an impractically large number of discrete
  modes.
---

# Categorical Policies: Multimodal Policy Learning and Exploration in Continuous Control

## Quick Facts
- arXiv ID: 2508.13922
- Source URL: https://arxiv.org/abs/2508.13922
- Authors: SM Mazharul Islam; Manfred Huber
- Reference count: 36
- Primary result: Multimodal policies with intermediate categorical distributions outperform unimodal Gaussian policies in continuous control tasks

## Executive Summary
This paper introduces Categorical Policies, a multimodal approach for continuous control that leverages an intermediate categorical distribution to select discrete behavior modes before generating fine-grained continuous actions. The method addresses the limitation of unimodal policies by using multiple categorical variables with fewer classes each, enabling a structured and efficient representation of diverse behaviors without requiring an impractically large number of discrete modes. Two differentiable sampling strategies—Straight-Through Estimation (STE) and Gumbel-Softmax reparameterization—are evaluated to maintain gradient flow through discrete latent structure.

## Method Summary
The approach uses an actor-critic framework where the policy first selects behavior modes through an intermediate categorical distribution, then conditions action generation on these modes. Specifically, the state s_t passes through F_b (MLP) to produce categorical logits for N categorical variables with M classes each. These categorical variables are sampled using either STE (argmax forward, identity backward) or Gumbel-Softmax (soft one-hot sampling). The sampled behavior mode b_t (N one-hot vectors concatenated) conditions F_a (MLP) which outputs parameters for a diagonal Gaussian action distribution. The method is trained using actor-critic with on-policy imagination rollouts and compares favorably against unimodal Gaussian baselines across six DeepMind Control Suite tasks.

## Key Results
- Multimodal policies consistently match or outperform standard unimodal policies in convergence speed, episode rewards, and robustness
- STE sampling shows better training stability than Gumbel-Softmax due to lower gradient variance
- Factorized representation with multiple categorical variables (N=2, M=8) outperforms single categorical with same total modes (N=1, M=64)
- The approach provides structured exploration enabling efficient navigation of the action space through multiple behavior modes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured exploration via discrete behavior modes can reduce mode collapse and improve convergence speed in continuous control tasks.
- Mechanism: The intermediate categorical distribution selects discrete behavior modes before generating fine-grained continuous actions. This factorization creates explicit "switches" (behavior modes) that the policy must learn to set, rather than relying solely on Gaussian noise around a single mean. The combinatorial structure (N categorical variables with M classes each = M^N possible modes) forces the policy to maintain diverse action strategies rather than collapsing to a single behavioral mode.
- Core assumption: Tasks benefit from maintaining multiple qualitatively distinct behavior strategies simultaneously, and the optimal action distribution is truly multimodal rather than unimodal with high variance.
- Evidence anchors:
  - [abstract] "demonstrating that through better exploration, our learned policies converge faster and outperform standard Gaussian policies"
  - [section V] "we attribute the superior performance of the multimodal policy to its structured exploration mechanism, which allows efficient navigation of the action space by leveraging multiple behavior modes"
  - [corpus] GoRL paper confirms: "policies that are stable to optimize are often too simple to represent the multimodal action distributions needed for complex control"

### Mechanism 2
- Claim: Multiple categorical variables with fewer classes each outperform a single categorical with many classes for the same total dimensionality.
- Mechanism: Compositional representation. With N=2, M=8 (total dim=16, modes=64), each categorical captures orthogonal behavioral factors that combine. With N=1, M=64 (total dim=64, modes=64), the single variable must encode all behavioral variation without compositional structure. The factorized form provides implicit regularization and better gradient flow because each categorical makes independent low-dimensional decisions.
- Core assumption: Behavioral modes have compositional structure that can be factorized; optimal behaviors can be expressed as combinations of simpler behavioral primitives.
- Evidence anchors:
  - [section IV] "A more structured alternative is to introduce N categorical variables, each with M classes. This approach provides a compositional representation, where the total number of discrete modes is M^N"
  - [section V-C] "for the single categorical variable case (N=1, M=64), even with the same behavior mode dimension (8×8 vs. 1×64) the agent does not learn any meaningful behavior in both tasks"
  - [corpus] Weak corpus evidence on this specific design choice; mechanism is primarily supported by paper's empirical ablation.

### Mechanism 3
- Claim: Straight-Through Estimation (STE) provides more stable training than Gumbel-Softmax for discrete behavior mode sampling in this architecture.
- Mechanism: STE uses discrete sampling in forward pass but identity gradient (∂z/∂p ≈ I) in backward pass. This introduces bias but has zero sampling variance in gradient estimation. Gumbel-Softmax uses continuous relaxation with temperature parameter, adding Gumbel noise samples to logits before softmax. This is unbiased as λ→0 but introduces gradient variance from the sampling process itself.
- Core assumption: The bias introduced by STE's identity gradient approximation is less harmful than the variance introduced by Gumbel-Softmax's sampling process for this specific policy architecture.
- Evidence anchors:
  - [section V-B] "STE approach exhibits better stability and performance in all six tasks"
  - [section V-B] "we hypothesize that the additional sampling step involved with Gumbel-Softmax reparameterization introduces higher gradient variance that leads to instability during optimization"
  - [corpus] RN-D paper uses categorical actors but doesn't directly compare STE vs Gumbel-Softmax; corpus evidence is limited on this comparison.

## Foundational Learning

- Concept: **Actor-Critic Framework**
  - Why needed here: The paper uses actor-critic with value model v(s) and action model. The value loss uses vλ (exponentially-weighted return estimate) as target. Understanding this is essential to see where the categorical policy plugs in.
  - Quick check question: Can you explain why the value function v(s) is needed to train the policy, and what vλ represents?

- Concept: **Reparameterization Tricks**
  - Why needed here: The entire contribution relies on making discrete sampling differentiable. Without understanding why categorical sampling breaks gradients (discrete operations have ∂z/∂p = 0), you won't understand why STE or Gumbel-Softmax are necessary.
  - Quick check question: Why can't you backpropagate through a standard categorical sample, and how does STE approximately solve this?

- Concept: **Multimodal Distributions**
  - Why needed here: The paper's core claim is that Gaussian policies are unimodal (single peak) while many tasks require multimodal behavior (multiple distinct strategies). Understanding what makes a distribution multimodal is essential.
  - Quick check question: If an agent can reach a goal by going either left or right around an obstacle, why would a Gaussian policy struggle compared to a multimodal policy?

## Architecture Onboarding

- Component map:
State s_t
    │
    ▼
F_b (MLP) → Categorical logits (N × M)
    │
    ▼
Sampling Layer
    - STE: argmax forward, identity backward
    - Gumbel-Softmax: soft one-hot
    │
    ▼
Behavior mode b_t (N one-hot vectors → concatenated)
    │
    ▼
F_a (MLP) → μ, σ (Gaussian parameters)
    │
    ▼
Action a_t ~ Diagonal-Gaussian(μ, σ)

- Critical path:
  1. F_b must produce meaningful categorical logits (not uniform) → requires gradient signal through sampling layer
  2. Sampling layer must preserve gradient flow → STE or Gumbel-Softmax implementation must be correct
  3. F_a must map behavior modes to distinct action distributions → requires sufficient capacity and training

- Design tradeoffs:
  - N (number of categorical variables): Higher N = more modes (M^N) but more parameters and harder credit assignment
  - M (classes per categorical): Higher M = more modes per variable but each decision is harder
  - STE vs Gumbel-Softmax: STE = stable but biased; Gumbel-Softmax = unbiased but high variance
  - Temperature λ (Gumbel-Softmax only): Lower = more discrete but higher variance; paper uses fixed τ=2.0 with hard sampling

- Failure signatures:
  - Uniform categorical logits (F_b outputs same values): No behavioral differentiation, effectively random mode selection
  - Mode collapse (one class dominates): Categorical becomes deterministic, reverts to near-unimodal behavior
  - High gradient variance (Gumbel-Softmax with low temperature): Training instability, loss spikes
  - N=1 with large M (e.g., M=64): Agent fails to learn meaningful behavior per Section V-C

- First 3 experiments:
  1. **Sanity check**: Implement unimodal Gaussian baseline on one DMC task (e.g., Cartpole Balance). Verify baseline achieves reasonable performance before adding complexity.
  2. **Minimal multimodal test**: Add single categorical (N=1, M=4) with STE on same task. Compare learning curves. Expect similar or slightly worse performance (single categorical limitation per paper).
  3. **Full architecture test**: Implement N=2, M=8 with STE on same task. Compare to baseline and single categorical. Expect faster convergence or higher final reward per Figure 3.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do Categorical Policies yield significant performance gains in environments with extremely sparse rewards compared to unimodal baselines?
- Basis in paper: [inferred] The abstract identifies sparse rewards as a key motivation for multimodal policies, but the empirical evaluation is limited to the DeepMind Control Suite, which primarily features dense reward signals.
- Why unresolved: The experiments confirm better convergence in standard control tasks, but the specific impact on exploration efficiency in sparse settings remains unverified.
- What evidence would resolve it: Performance comparisons on benchmarks explicitly designed for sparse rewards (e.g., Montezuma's Revenge or sparse DMC variants).

### Open Question 2
- Question: Do the learned discrete behavior modes correspond to semantically meaningful or reusable skills?
- Basis in paper: [inferred] The paper claims the method facilitates "structured exploration" via discrete modes but does not analyze the semantic content or temporal consistency of these modes.
- Why unresolved: Without interpretability analysis, it is unclear if the modes capture distinct high-level strategies or simply partition the action space arbitrarily.
- What evidence would resolve it: Visualization of mode activation across different task phases or transfer learning experiments where pre-learned modes accelerate learning on new tasks.

### Open Question 3
- Question: Does temperature annealing for Gumbel-Softmax sampling improve stability and performance relative to the fixed-temperature settings evaluated?
- Basis in paper: [inferred] The authors utilized a fixed temperature (τ=2.0) for Gumbel-Softmax to reduce complexity, noting that soft-sampling typically necessitates annealing to align train-test behaviors.
- Why unresolved: The comparison with Straight-Through Estimation (STE) might be biased against Gumbel-Softmax if the fixed hyperparameters were suboptimal for the distribution's relaxation.
- What evidence would resolve it: Ablation studies comparing STE against Gumbel-Softmax with dynamic temperature schedules.

## Limitations
- Empirical evaluation limited to 6 DMC tasks with relatively low-dimensional action spaces (1-6 dimensions)
- No systematic validation of the compositional assumption for behavioral modes beyond one negative result
- Fixed temperature setting for Gumbel-Softmax may disadvantage this approach in the STE comparison

## Confidence
- **High confidence**: Multimodal policies with factorized categorical variables outperform single categorical baselines; STE provides more stable training than Gumbel-Softmax for this architecture.
- **Medium confidence**: Structured exploration via discrete modes reduces mode collapse and improves convergence; compositional representation benefits are real but mechanism is not fully characterized.
- **Low confidence**: The specific N=2, M=8 configuration is optimal; these benefits will transfer to all continuous control domains.

## Next Checks
1. Test the factorized categorical approach on high-dimensional continuous control tasks (e.g., humanoid locomotion) to assess scalability limits.
2. Quantify gradient variance for STE vs Gumbel-Softmax across training epochs to validate the variance hypothesis.
3. Systematically vary N and M combinations (e.g., N=3, M=4 vs N=4, M=3) to map the compositional representation space and identify optimal configurations for different task types.