---
ver: rpa2
title: 'FetFIDS: A Feature Embedding Attention based Federated Network Intrusion Detection
  Algorithm'
arxiv_id: '2508.09056'
source_url: https://arxiv.org/abs/2508.09056
tags:
- detection
- learning
- intrusion
- federated
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents FetFIDS, a feature embedding attention-based\
  \ federated network intrusion detection algorithm designed to improve intrusion\
  \ detection performance in edge learning environments. The authors propose using\
  \ feature embedding instead of positional embedding in a transformer-based deep\
  \ learning model to focus the model\u2019s attention on identifying unique features\
  \ of attack data."
---

# FetFIDS: A Feature Embedding Attention based Federated Network Intrusion Detection Algorithm

## Quick Facts
- **arXiv ID:** 2508.09056
- **Source URL:** https://arxiv.org/abs/2508.09056
- **Reference count:** 18
- **Primary result:** Feature embedding attention-based federated network intrusion detection algorithm outperforms state-of-the-art methods on NSLKDD dataset

## Executive Summary
This paper presents FetFIDS, a feature embedding attention-based federated network intrusion detection algorithm designed to improve intrusion detection performance in edge learning environments. The authors propose using feature embedding instead of positional embedding in a transformer-based deep learning model to focus the model's attention on identifying unique features of attack data. The model employs sequential attention blocks and is trained in a federated learning setup across multiple communication rounds to ensure both privacy and localized performance improvements. Experiments on the NSLKDD dataset demonstrate that FetFIDS outperforms multiple state-of-the-art intrusion detection systems in a federated environment, achieving higher accuracy, precision, recall, and F1-score compared to benchmark methods such as Autoencoder, TransFIDS, and MLR.

## Method Summary
FetFIDS operates by replacing traditional positional embeddings with CNN-based feature embeddings that capture intrinsic traffic characteristics from raw network data. The architecture consists of three sequential multi-head attention blocks (8 heads each) with BatchNorm normalization, followed by an MLP classifier. In the federated setting, each node trains locally on private data for 20 epochs, shares model weights via FedAvg aggregation, and repeats this process across 20 communication rounds. The model uses AdamW optimizer with exponential learning rate decay and categorical crossentropy loss.

## Key Results
- FetFIDS achieves 77.00% accuracy on NSLKDD dataset, outperforming Autoencoder (71.74%), TransFIDS (71.74%), and MLR (71.01%) baselines
- The model demonstrates stable learning improvements across all 5 federated nodes over 100 communication rounds
- FetFIDS shows superior precision, recall, and F1-scores across all attack classes compared to baseline methods
- The approach maintains consistent performance while preserving privacy through local training and federated aggregation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Feature embedding provides more meaningful representations than positional embedding for network intrusion data in attention-based models.
- **Mechanism:** A CNN-based feature embedding module extracts high-level features from raw 1D network traffic vectors before feeding them into attention blocks. Unlike positional embeddings (which encode sequence position for NLP/vision tasks), feature embeddings capture intrinsic traffic characteristics since network data typically lacks temporal sequence context.
- **Core assumption:** Network traffic features contain discriminative patterns that benefit from learned embeddings, whereas positional information adds noise without semantic value for intrusion detection.
- **Evidence anchors:**
  - [Page 3] "For a network dataset, positional embeddings would not give the model very much contextual information because they typically have only one entry in the sequence... Therefore, instead of using a positional embedding, we use what we term as a feature embedding."
  - [Page 5] TransIDS (using positional embeddings) shows deteriorating performance over federated rounds while FetFIDS improves, suggesting feature embeddings transfer better to federated settings.
  - [Corpus] Adjacent work on FLARE and OptiFLIDS also emphasizes feature-based approaches for federated IDS, though direct comparison to positional embeddings is not discussed.
- **Break condition:** If network traffic exhibits strong temporal dependencies across packet sequences (e.g., session-level attack patterns), positional embeddings may become informative, reducing the advantage of feature-only embeddings.

### Mechanism 2
- **Claim:** Sequential attention blocks with BatchNorm enable stable learning across federated communication rounds.
- **Mechanism:** Three stacked multihead attention blocks (8 heads each) progressively enrich context-aware representations. BatchNorm normalizes features per batch rather than LayerNorm's per-sample normalization, which empirically stabilizes training in this architecture.
- **Core assumption:** Deeper attention composition captures multi-scale feature interactions, and BatchNorm's batch-dependent statistics suit the federated mini-batch training regime.
- **Evidence anchors:**
  - [Page 3] "We design the architecture of the model to have sequential multiheaded attention layers. This enables the contextually rich vector from each attention model to be further enriched... We use BatchNorm as our normalization method instead of LayerNorm which is usually used... This has been empirically shown to help our model perform with a higher accuracy."
  - [Page 4, Table I] FetFIDS achieves 77.00% accuracy vs. 71.74% for single-block TransFIDS after 20 rounds.
  - [Corpus] CSAGC-IDS and related dual-module architectures show similar benefits from multi-stage feature processing, though BatchNorm vs. LayerNorm choice is not directly compared.
- **Break condition:** With highly non-IID data distributions across federated nodes, BatchNorm statistics may become inconsistent, potentially causing performance degradation compared to LayerNorm.

### Mechanism 3
- **Claim:** Federated averaging with repeated local training rounds preserves privacy while improving per-node detection accuracy.
- **Mechanism:** Each node trains locally on private data, shares only model weights with a central server, which aggregates via FedAvg (weighted by local sample counts). The global model is redistributed and further refined locally over 20+ communication rounds.
- **Core assumption:** Local data distributions are sufficiently similar that aggregated weights improve all nodes, and the benefits of shared learning outweigh communication overhead.
- **Evidence anchors:**
  - [Page 4] "Each node trains a local model using only locally available data. Only the local model weights are then sent for aggregation... FedAvg aggregation algorithm... averaged global model is then sent back to all the nodes."
  - [Page 5, Figure 9] All 5 nodes show stable accuracy improvements over 100 rounds, with Node 3 consistently higher and Node 5 lower, indicating personalized convergence despite shared aggregation.
  - [Corpus] Sentinel and OptiFLIDS address heterogeneous data challenges in federated IDS; Assumption: FetFIDS may face degradation under more severe non-IID conditions than tested.
- **Break condition:** If node data distributions diverge significantly (e.g., entirely different attack types per node), global aggregation may degrade local performance, requiring personalized FL variants.

## Foundational Learning

- **Concept: Self-Attention and Multihead Attention**
  - **Why needed here:** The core encoder uses multihead self-attention to learn which traffic features are most relevant for detecting attacks. Understanding Query/Key/Value mechanics explains how the model generates attention matrices that weight feature importance.
  - **Quick check question:** Can you explain why multihead attention with 8 heads might capture different aspects of network traffic compared to a single attention operation?

- **Concept: Federated Learning with FedAvg**
  - **Why needed here:** FetFIDS operates in a federated setting where model training is distributed across edge nodes. Understanding FedAvg's weighted aggregation and the role of communication rounds is essential to interpret performance curves and trade-offs.
  - **Quick check question:** What happens to global model convergence if one node has 10x more local data than others?

- **Concept: Feature vs. Positional Embeddings in Transformers**
  - **Why needed here:** The paper's central innovation is replacing positional embeddings with CNN-based feature embeddings. Understanding what positional embeddings encode (sequence order) clarifies why they may be suboptimal for tabular network data.
  - **Quick check question:** Why might positional information help in NLP but hurt performance on single-packet intrusion detection?

## Architecture Onboarding

- **Component map:** Input (1D vector, 41 features) → Feature Embedding (CNN) → Attention Block 1 → Attention Block 2 → Attention Block 3 → MLP Head → Softmax → Loss (Categorical Crossentropy)

- **Critical path:** Input → Feature Embedding (CNN) → Attention Block 1 → Attention Block 2 → Attention Block 3 → MLP Head → Softmax → Loss (Categorical Crossentropy)

- **Design tradeoffs:**
  - Accuracy vs. Latency: FetFIDS achieves 77% accuracy but 19.10µs inference time vs. 0.24µs for MLR (Table II). Attention blocks and CNN embedding add compute overhead.
  - Stability vs. Complexity: Sequential attention stabilizes federated learning but increases parameters (116.64k vs. 66.51k for autoencoder). BatchNorm may face issues with highly non-IID data.
  - Communication Cost: FetFIDS has fewer parameters than TransFIDS (116.64k vs. 121.70k), reducing federated bandwidth needs slightly, but both exceed lightweight baselines.

- **Failure signatures:**
  - Degrading accuracy over rounds: Likely indicates data heterogeneity exceeding FedAvg's tolerance; consider personalized FL or clustering nodes.
  - High false negatives (low recall): Check class imbalance handling; focal loss was tested but did not improve results—consider resampling or threshold tuning.
  - Unstable per-node performance: Node 5's lower accuracy suggests underfitting on its local data; increase local epochs or inspect data distribution.
  - Training divergence: Verify learning rate schedule (exponential decay with gamma=0.7 improved stability per Figure 10).

- **First 3 experiments:**
  1. **Baseline reproduction:** Run FetFIDS on NSLKDD with 5 nodes, 20 rounds. Verify accuracy reaches ~77% and matches Table I. Check per-node variance against Figure 9.
  2. **Ablation on embedding type:** Replace feature embedding with standard positional encoding. Compare accuracy trajectory over 20 rounds. Expect TransFIDS-like degradation.
  3. **Non-IID stress test:** Distribute data with class imbalance (e.g., Node 1 sees only DoS, Node 2 only Probing). Measure accuracy drop and identify if BatchNorm or FedAvg causes instability. Compare to LayerNorm variant.

## Open Questions the Paper Calls Out

- **Question:** Can the computational footprint and inference time of FetFIDS be reduced to match lightweight baselines without sacrificing detection accuracy?
  - **Basis in paper:** [explicit] The Conclusion explicitly states the authors aim to "focus on further developing the model to reduce its computational footprint and inference time."
  - **Why unresolved:** Table II shows the proposed method has significantly higher FLOPs (1.07M) and inference time (19.10µs) compared to the Autoencoder and MLR benchmarks.
  - **What evidence would resolve it:** A modified architecture achieving comparable F1-scores on the NSLKDD dataset with computational metrics similar to the MLR baseline.

- **Question:** How does FetFIDS perform under non-IID (non-identically distributed) data distributions across federated nodes?
  - **Basis in paper:** [inferred] Section III-A notes that the training data is divided among devices with the "same distribution," ignoring the data heterogeneity common in realistic edge environments.
  - **Why unresolved:** Federated learning algorithms often suffer performance degradation when local data distributions diverge (statistical heterogeneity), which the current IID setup masks.
  - **What evidence would resolve it:** Performance metrics (accuracy/F1) from experiments where nodes hold locally skewed or disjoint class distributions.

- **Question:** Does the feature embedding mechanism generalize to modern, high-dimensional network intrusion datasets?
  - **Basis in paper:** [inferred] The study relies solely on the NSLKDD dataset, which, while standardized, may not reflect the complexity or volume of contemporary network traffic.
  - **Why unresolved:** The specific feature embedding (CNN-based) is tuned for NSLKDD's structure; it is unclear if this attention mechanism scales efficiently to raw bytes or high-dimensional flow data.
  - **What evidence would resolve it:** Benchmarking results on complex datasets (e.g., CIC-IDS-2017 or CSE-CIC-IDS2018) showing similar performance gains over state-of-the-art methods.

## Limitations

- The CNN-based feature embedding architecture is underspecified, making exact reproduction difficult
- Performance advantages come at significant computational cost (19.10µs inference vs. 0.24µs for MLR), which may be prohibitive for resource-constrained edge devices
- The method was only tested on one dataset (NSLKDD) with controlled class distribution across nodes, limiting generalizability

## Confidence

- **High:** The federated learning framework and FedAvg implementation appear technically sound and well-documented
- **Medium:** Feature embedding vs. positional embedding performance advantage is demonstrated but needs validation across diverse datasets
- **Low:** Claims about BatchNorm superiority over LayerNorm are based on empirical observation without theoretical justification

## Next Checks

1. **Architecture specification test:** Implement the exact CNN feature embedding architecture and verify it matches the reported performance; compare with simpler alternatives like linear projections
2. **Cross-dataset generalization:** Evaluate FetFIDS on CICIDS2017 and UNSW-NB15 to assess whether feature embedding advantage holds beyond NSLKDD
3. **Non-IID stress test:** Intentionally create heterogeneous data distributions across nodes (different attack types per node) to test federated stability under realistic edge conditions