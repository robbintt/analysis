---
ver: rpa2
title: 'ReCUT: Balancing Reasoning Length and Accuracy in LLMs via Stepwise Trails
  and Preference Optimization'
arxiv_id: '2506.10822'
source_url: https://arxiv.org/abs/2506.10822
tags:
- reasoning
- recut
- llms
- accuracy
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of overthinking in Large Language
  Models (LLMs) during reasoning tasks, where models generate unnecessarily long and
  redundant reasoning chains that increase computational costs without improving accuracy.
  To tackle this, the authors propose ReCUT, a method that balances reasoning accuracy
  and length through stepwise exploration and preference optimization.
---

# ReCUT: Balancing Reasoning Length and Accuracy in LLMs via Stepwise Trails and Preference Optimization

## Quick Facts
- arXiv ID: 2506.10822
- Source URL: https://arxiv.org/abs/2506.10822
- Reference count: 10
- Reduces reasoning chain length by 30-50% while maintaining or improving accuracy on math reasoning tasks

## Executive Summary
ReCUT addresses the problem of overthinking in Large Language Models (LLMs) during reasoning tasks, where models generate unnecessarily long and redundant reasoning chains that increase computational costs without improving accuracy. The method employs a stepwise exploration strategy that generates both long and short reasoning trajectories at each step, evaluates them using a reward function, and incrementally constructs an optimal path. These trajectories are then used to train two specialized models—one optimized for accuracy and the other for shorter reasoning—whose parameters are interpolated to achieve a balanced final model. Experimental results across multiple math reasoning datasets demonstrate that ReCUT effectively mitigates overthinking while maintaining performance, achieving 30-50% length reduction compared to baselines.

## Method Summary
ReCUT introduces a two-stage approach to balance reasoning accuracy and length. First, it uses stepwise long-short sampling where the LLM generates diverse reasoning trajectories by alternating between "long" and "short" continuation prompts at each reasoning step. These trajectories are evaluated using a reward function (1/|Y| for correct answers, -1/|Y| for incorrect) to construct preference datasets. Two separate Direct Preference Optimization (DPO) training runs are performed: one for accuracy (Dacc) and one for length (Dlen). The resulting models are then merged using DARE-Ties interpolation with specified parameters. The method is evaluated on mathematical reasoning tasks using GSM8K, MATH500, AMC23, AIME24, and AIME25 datasets with Qwen2.5-7B and Llama-3.1-8B as backbone models.

## Key Results
- Achieves 30-50% reduction in reasoning chain length compared to baselines
- Maintains or improves pass@1 accuracy across all tested math reasoning datasets
- Outperforms zero-shot prompting, SFT, and RL-based methods in the length-accuracy trade-off
- Demonstrates effectiveness across multiple backbone models (Qwen2.5-7B, Llama-3.1-8B)

## Why This Works (Mechanism)
ReCUT works by explicitly separating the reasoning process into accuracy-seeking and length-minimizing components, then combining them through interpolation. The stepwise exploration ensures diverse trajectory generation by forcing the model to consider both concise and elaborate reasoning paths at each step. The reward function creates a natural preference for shorter correct answers while penalizing longer incorrect ones. By training separate models for accuracy and length optimization, ReCUT can capture distinct reasoning patterns optimized for each objective. The final interpolation balances these competing goals based on validation performance.

## Foundational Learning
- **Stepwise Long-Short Sampling**: Why needed - To generate diverse reasoning trajectories that capture both efficient and thorough reasoning patterns. Quick check - Verify that trajectory pools contain both short correct and long correct examples with measurable length differences.
- **Direct Preference Optimization (DPO)**: Why needed - To train models that can distinguish between preferred and non-preferred reasoning chains based on accuracy and length criteria. Quick check - Confirm that the trained models consistently select shorter correct answers over longer correct ones in validation pairs.
- **DARE-Ties Merging**: Why needed - To combine the accuracy-optimized and length-optimized models while preserving the strengths of both. Quick check - Validate that the merged model achieves better length-accuracy trade-off than either parent model alone.

## Architecture Onboarding

Component map: Stepwise Sampling -> Reward-based Selection -> Preference Dataset Construction -> DPO Training (Macc + Mlen) -> DARE-Ties Merging -> Final Model

Critical path: The stepwise sampling and preference construction phase is critical, as poor trajectory diversity will directly impact the quality of both DPO models and ultimately the merged result.

Design tradeoffs: The method trades increased training complexity (two separate DPO runs plus merging) for better control over the length-accuracy balance compared to end-to-end approaches.

Failure signatures: If the final model shows no length reduction compared to baseline, the most likely causes are insufficient trajectory diversity during sampling or overly similar preference pairs that provide weak training signals.

First experiments:
1. Verify stepwise sampling generates diverse trajectory pools by checking length and accuracy distributions
2. Test DPO training convergence on the constructed preference datasets
3. Validate DARE-Ties merging produces improved trade-off curves compared to individual models

## Open Questions the Paper Calls Out
1. How can the interpolation parameters (weight $\alpha$ and density $Topx$) in the DARE-Ties merging strategy be systematically optimized for ReCUT, rather than relying on empirical values borrowed from prior work?

2. How robust is the stepwise trajectory exploration mechanism when the base LLM has weak instruction-following capabilities, and does this failure mode negate the benefits of preference optimization?

3. Can ReCUT generalize to non-mathematical reasoning domains (e.g., logical deduction or code generation) where "stepwise" reasoning is less linear or formally defined?

## Limitations
- Performance limited by the instruction-following capability of the base LLM during stepwise sampling
- Interpolation parameters for DARE-Ties merging rely on empirical values from prior work without systematic optimization
- Evaluation restricted to mathematical reasoning tasks, leaving generalization to other domains unexplored

## Confidence
- High confidence in the core conceptual framework and experimental methodology
- Medium confidence in the reproducibility of the final performance numbers due to underspecified hyperparameters
- Medium confidence in the trajectory sampling implementation details

## Next Checks
1. Systematically vary DPO hyperparameters (β = 0.01, 0.1, 0.2) and LoRA configurations to determine their impact on length-accuracy trade-off curves

2. Implement logging during stepwise sampling to verify trajectory pool diversity with varied lengths and correctness outcomes across the training set

3. Confirm correct injection of partial reasoning chains into continuation prompts and validate preference selection logic handles edge cases properly