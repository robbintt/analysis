---
ver: rpa2
title: 'AdaSD: Adaptive Speculative Decoding for Efficient Language Model Inference'
arxiv_id: '2512.11280'
source_url: https://arxiv.org/abs/2512.11280
tags:
- tokens
- adasd
- draft
- threshold
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AdaSD proposes a hyperparameter-free speculative decoding method
  that dynamically adjusts candidate generation length and token acceptance criteria
  during inference using entropy and Jensen-Shannon distance. It achieves up to 49%
  speedup over standard speculative decoding while maintaining accuracy degradation
  under 2%, eliminating the need for additional training or hyperparameter tuning.
---

# AdaSD: Adaptive Speculative Decoding for Efficient Language Model Inference

## Quick Facts
- **arXiv ID:** 2512.11280
- **Source URL:** https://arxiv.org/abs/2512.11280
- **Reference count:** 32
- **Primary result:** Achieves up to 49% speedup over standard speculative decoding while maintaining accuracy degradation under 2%

## Executive Summary
AdaSD introduces a hyperparameter-free speculative decoding method that dynamically adjusts candidate generation length and token acceptance criteria during inference using entropy and Jensen-Shannon distance. The method eliminates the need for additional training or hyperparameter tuning by leveraging running statistics from previously generated tokens. Experiments demonstrate 17-49% speedup across multiple benchmarks while preserving accuracy within 2% of vanilla decoding.

## Method Summary
AdaSD implements adaptive speculative decoding through two dynamic thresholds that update during inference. The generation threshold T_G stops draft token generation when entropy exceeds the mean entropy of previously rejected tokens, preventing wasted computation on unlikely candidates. The verification threshold T_V accepts tokens based on JS distance between draft and target distributions, using the midpoint between mean JS distances of accepted and rejected tokens. Both thresholds update online using running statistics, eliminating hyperparameter tuning. The method supports max window size W=20 and requires identical vocabularies between draft and target models.

## Key Results
- Achieves 17-42% speedup over vanilla speculative decoding on GSM8K benchmark
- Maintains accuracy degradation below 2% across all tested configurations
- Demonstrates 26-49% speedup improvements over standard speculative decoding with comparable accuracy

## Why This Works (Mechanism)

### Mechanism 1: Entropy-Based Draft Generation Termination
Stopping draft token generation when entropy exceeds an adaptive threshold reduces wasted computation on tokens likely to be rejected. The draft model sequentially generates candidates, computing entropy H(q_i) after each token. If H(q_i) > T_G, generation terminates and proceeds to verification. The threshold T_G updates as the mean entropy of all previously rejected tokens.

### Mechanism 2: JS Distance-Based Verification Relaxation
Using Jensen-Shannon distance as a verification threshold enables acceptance of tokens that differ between draft and target models but have similar probability distributions. During verification, compute d_JS(p_i || q_i) between draft and target distributions. Accept the token if d_JS < T_V, even if sampled tokens differ. The threshold T_V = (avg(d_a) + avg(d_r))/2 uses the midpoint between mean JS distances of accepted and rejected tokens.

### Mechanism 3: Running-Statistics Adaptive Threshold Updates
Dynamically updating thresholds from running statistics of generated tokens eliminates the need for pre-deployment hyperparameter tuning. Maintain three lists during inference: L_R^E (rejected entropies), L_R^D (rejected JS distances), L_A^D (accepted JS distances). After each verification step, update T_G = average(L_R^E) and T_V = (average(L_A^D) + average(L_R^D))/2.

## Foundational Learning

- **Concept: Speculative Decoding (Draft-Verify Paradigm)**
  - Why needed: AdaSD modifies both draft generation and verification; understanding baseline two-step process is prerequisite
  - Quick check: In standard speculative decoding, what happens to candidate tokens when the target model rejects one token in the sequence?

- **Concept: Shannon Entropy as Uncertainty Measure**
  - Why needed: Generation threshold relies on entropy to quantify draft model uncertainty per token
  - Quick check: For vocabulary size V, what is maximum possible entropy value and what distribution achieves it?

- **Concept: Jensen-Shannon Divergence vs. KL Divergence**
  - Why needed: JS distance chosen over KL divergence because it's bounded and symmetric, critical for threshold stability
  - Quick check: Why does KL divergence become problematic when draft model assigns high probability to token that target model assigns near-zero probability?

## Architecture Onboarding

- **Component map:** [Draft Model Mq] → generates tokens x_i sequentially → [Entropy Monitor] → computes H(q_i), compares to T_G → [Candidate Buffer] → stores x_1, ..., x_w → [Target Model Mp] → verifies all candidates in parallel → [JS Distance Calculator] → computes d_JS(p_i || q_i) for each position → [Verification Decider] → accepts if x_i == y_i OR d_JS < T_V → [Statistics Buffers] → L_R^E, L_R^D, L_A^D updated per token outcome → [Threshold Updater] → recomputes T_G, T_V from running means

- **Critical path:** Verification step's parallel forward pass through target model dominates latency; maximizing accepted tokens per verification minimizes target model invocations

- **Design tradeoffs:** Speed vs. Accuracy (looser T_V increases acceptance but risks accuracy degradation); Window size W (larger enables more parallelism but increases draft overhead); Threshold initialization (paper uses T_G = T_V = 0, relying on early tokens to bootstrap thresholds)

- **Failure signatures:** Low acceptance rate (<50%) indicates T_V too tight; accuracy degradation >2% indicates T_V too loose; no speedup over Vanilla indicates draft model too slow or generation threshold never triggered

- **First 3 experiments:** 1) Baseline reproduction: Run AdaSD with Llama 3.1 70B/8B on GSM8K to verify ~26% speedup and <2% accuracy drop; 2) Ablation by component: Run Gen-Only and Verify-Only variants separately to quantify each mechanism's contribution; 3) Threshold sensitivity: Initialize T_V at 0.1, 0.3, 0.5 instead of 0 to assess cold-start robustness

## Open Questions the Paper Calls Out

### Open Question 1
Can incorporating additional indicators beyond entropy enable more precise decisions for termination of candidate token generation? The current implementation relies solely on entropy, which may lead to suboptimal sequence lengths in certain model families. Experiments integrating supplementary metrics alongside entropy could demonstrate improved acceptance rates or throughput.

### Open Question 2
Can more effective heuristics for adjusting the Jensen-Shannon distance threshold improve token acceptance rate while further reducing accuracy degradation? While the current heuristic (averaging mean JS distances of accepted and rejected tokens) is functional, the authors tested three sophisticated variants that failed to outperform it, leaving search for superior heuristic open.

### Open Question 3
How can the AdaSD framework be extended to support draft and target models that utilize different vocabularies or tokenization schemes? The calculation of JS distance and entropy relies on direct comparison of output distributions over same tokens, creating mathematical and implementation barrier for cross-vocabulary application.

## Limitations

- **Vocabulary constraint:** Requires identical vocabularies between draft and target models, limiting compatibility with many modern model pairs
- **Stationarity assumption:** Adaptive thresholds assume recent token statistics remain representative of near-future generation behavior
- **Cold-start uncertainty:** Initial threshold behavior with T_G = T_V = 0 initialization isn't thoroughly explored, potentially affecting first few tokens

## Confidence

**High Confidence:** Speedup claims (up to 49%) and accuracy preservation (<2% degradation) are well-supported by experimental results across multiple benchmarks and model pairs with sound empirical methodology.

**Medium Confidence:** Entropy-based generation termination effectiveness relies on correlation between high draft entropy and rejection probability, which appears robust for tested configurations but may not generalize universally.

**Low Confidence:** Adaptive threshold update strategy's robustness to non-stationary inference conditions remains untested; running statistics may not provide sufficient signal when task distribution shifts mid-inference.

## Next Checks

1. **Cold-Start Performance Analysis:** Measure first 50 tokens' acceptance rate and accuracy using T_G = T_V = 0 initialization versus alternative warm-start strategies to assess threshold bootstrap phase behavior.

2. **Distribution Shift Robustness Test:** Evaluate AdaSD on mixed-task inference sequences where topic changes mid-generation, tracking threshold convergence speed and acceptance rate stability to quantify adaptive mechanism responsiveness.

3. **Cross-Model Pair Generalization Study:** Test AdaSD with model pairs having significant architectural differences beyond parameter count to verify whether entropy and JS distance maintain predictive power for acceptance/rejection.