---
ver: rpa2
title: 'Unraveling the Localized Latents: Learning Stratified Manifold Structures
  in LLM Embedding Space with Sparse Mixture-of-Experts'
arxiv_id: '2502.13577'
source_url: https://arxiv.org/abs/2502.13577
tags:
- space
- embedding
- which
- arxiv
- definition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Mixture-of-Experts (MoE) framework to investigate
  whether the embedding space of large language models (LLMs) exhibits a stratified
  manifold structure. The authors hypothesize that LLM embeddings do not lie on a
  single global manifold but rather form a stratified space - a union of lower-dimensional
  local manifolds of different dimensions corresponding to data from different domains,
  perplexities, or semantic concepts.
---

# Unraveling the Localized Latents: Learning Stratified Manifold Structures in LLM Embedding Space with Sparse Mixture-of-Experts

## Quick Facts
- arXiv ID: 2502.13577
- Source URL: https://arxiv.org/abs/2502.13577
- Reference count: 40
- Primary result: LLM embeddings form stratified manifold structures with varying intrinsic dimensions across different semantic strata

## Executive Summary
This paper investigates whether LLM embedding spaces exhibit stratified manifold structures rather than uniform global manifolds. The authors propose a Mixture-of-Experts (MoE) framework where each expert uses dictionary learning at different sparsity levels to capture varying dimensional structures. An attention-based soft-gating network routes inputs to appropriate experts. Experiments on multiple LLM families show that embeddings from similar datasets cluster together in the same strata, validating the hypothesis that LLM embedding spaces have non-uniform geometrical structures with varying complexities across different semantic regions.

## Method Summary
The proposed approach combines dictionary learning with MoE architecture to discover stratified structures in LLM embedding spaces. Each expert implements dictionary learning with varying sparsity levels, where sparsity corresponds to the intrinsic dimension of the local manifold. An attention-based soft-gating network dynamically selects the most appropriate expert for each input based on learned routing weights. The model is trained to cluster embeddings from similar data sources into the same stratum while allowing different strata to capture different semantic concepts or data complexities. The framework is evaluated across multiple LLM families (BERT, RoBERTa, GPT-3, Llama-3.2-1B, DeepSeek-R1-Distill-Qwen-1.5B) and demonstrates that LLM embeddings exhibit non-uniform geometric structures with varying intrinsic dimensions across identified strata.

## Key Results
- LLM embeddings from similar datasets consistently cluster together in the same strata
- Different strata capture varying semantic concepts with different intrinsic dimensionalities
- The stratified structure persists across multiple LLM families (BERT, RoBERTa, GPT-3, Llama-3.2-1B, DeepSeek-R1-Distill-Qwen-1.5B)
- Sparsity levels in dictionary learning correspond to varying manifold complexities within different strata

## Why This Works (Mechanism)
The MoE framework works because it allows the model to discover and adapt to varying geometric structures in the embedding space. The dictionary learning with varying sparsity levels enables each expert to capture manifolds of different intrinsic dimensions - lower sparsity for simpler structures and higher sparsity for more complex ones. The soft-gating mechanism learns to route inputs based on their semantic content and structural properties, allowing the model to dynamically select the appropriate manifold representation. This approach effectively decomposes the global embedding space into a union of lower-dimensional local manifolds, each specialized for different types of data or semantic concepts.

## Foundational Learning

**Manifold Learning**: Understanding of manifold theory and its application to high-dimensional data representation
- Why needed: To interpret the stratified structure as a union of lower-dimensional manifolds
- Quick check: Verify understanding of intrinsic vs extrinsic dimension concepts

**Mixture-of-Experts**: Familiarity with MoE architectures and their routing mechanisms
- Why needed: The proposed method builds on MoE to discover stratified structures
- Quick check: Understand how soft-gating differs from hard routing

**Dictionary Learning**: Knowledge of sparse coding and dictionary-based representation learning
- Why needed: Each expert uses dictionary learning with varying sparsity levels
- Quick check: Understand the relationship between sparsity and intrinsic dimension

**Attention Mechanisms**: Understanding of attention-based routing and gating networks
- Why needed: The soft-gating network uses attention to route inputs to appropriate experts
- Quick check: Verify understanding of how attention weights are learned and applied

## Architecture Onboarding

**Component Map**: Input Embeddings -> Soft-Gating Network -> Expert Selection -> Dictionary Learning -> Manifold Representation

**Critical Path**: Input → Soft-Gating Network → Dictionary Learning (selected expert) → Output stratum assignment

**Design Tradeoffs**: 
- Higher number of experts allows finer-grained stratification but increases computational complexity
- Varying sparsity levels enables capturing different manifold dimensions but requires careful hyperparameter tuning
- Soft-gating provides smooth routing but may lead to blurred boundaries between strata

**Failure Signatures**: 
- All inputs routed to single expert indicates gating mechanism failure
- Experts capturing similar structures suggest poor stratification
- High reconstruction error suggests insufficient capacity or poor manifold fitting

**First Experiments**:
1. Visualize expert routing patterns on simple synthetic data with known manifold structure
2. Measure reconstruction accuracy vs number of experts on benchmark datasets
3. Analyze sparsity distribution across experts to verify correspondence with intrinsic dimensions

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Experimental validation limited to smaller models (1B-3B parameters) rather than frontier-scale models
- Interpretation of sparsity levels as manifold dimensions remains heuristic rather than rigorously proven
- Evaluation focuses on clustering similarity rather than direct geometric measurements like curvature or volume growth

## Confidence

**High confidence**: LLM embeddings exhibit non-uniform clustering patterns that correlate with semantic similarity and data provenance

**Medium confidence**: The stratified manifold hypothesis provides a useful conceptual framework for understanding these patterns

**Medium confidence**: MoE-based stratification can reveal meaningful substructures in embedding spaces

## Next Checks

1. Apply the methodology to frontier-scale models (70B+ parameters) to verify whether stratification persists at scale

2. Conduct ablation studies removing the soft-gating network to determine its necessity for discovering meaningful strata

3. Perform direct geometric measurements of curvature and volume growth within identified strata to quantify manifold properties