---
ver: rpa2
title: 'REM: Evaluating LLM Embodied Spatial Reasoning through Multi-Frame Trajectories'
arxiv_id: '2512.00736'
source_url: https://arxiv.org/abs/2512.00736
tags:
- object
- trajectory
- reasoning
- spatial
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces REM, a benchmark designed to evaluate embodied
  spatial reasoning in multimodal large language models (MLLMs) through controlled
  3D environments with egocentric viewpoints. The key innovation is systematic evaluation
  of object permanence, spatial relationships, and numerical tracking across dynamic
  viewpoints, using trajectories with explicit camera movements.
---

# REM: Evaluating LLM Embodied Spatial Reasoning through Multi-Frame Trajectories

## Quick Facts
- arXiv ID: 2512.00736
- Source URL: https://arxiv.org/abs/2512.00736
- Authors: Jacob Thompson; Emiliano Garcia-Lopez; Yonatan Bisk
- Reference count: 19
- Models achieve 80% accuracy on simpler tasks but drop to 35.7% for counting with ground truth ≥ 2 and fail at 86-90% rates on object permanence with duplicated objects

## Executive Summary
This paper introduces REM, a benchmark designed to evaluate embodied spatial reasoning in multimodal large language models (MLLMs) through controlled 3D environments with egocentric viewpoints. The key innovation is systematic evaluation of object permanence, spatial relationships, and numerical tracking across dynamic viewpoints, using trajectories with explicit camera movements. The evaluation reveals that while top models like o3 achieve 80% overall accuracy on simpler tasks, performance drops sharply with increased scene complexity and collapses on challenging object permanence tasks like the Full Rotation dataset. The results demonstrate that MLLMs fundamentally struggle to build integrated world models that combine visual perception with explicit egomotion, particularly failing to track distinct object instances across viewpoint changes.

## Method Summary
REM generates synthetic 3D environments with colored geometric objects (3 shapes × 8 colors) and creates egocentric trajectories using Blender-based rendering. The benchmark produces image sequences with discrete camera movements ("1m Forward", "15° Right/Left") and automatically generates 4 question types: counting, comparison, left/right positioning, and temporal ordering. Questions are evaluated via keyword-based verification against ground truth annotations. Three datasets are provided: Baseline (3,119 trajectories, 47K QA pairs), Single Frame (350 trajectories), and Full Rotation (100 trajectories). Models receive system prompts with questions, action annotations, and image sequences, with performance measured by accuracy across question types and difficulty conditions.

## Key Results
- o3 achieves 80% overall accuracy on Mini-Baseline but drops to 35.7% for counting tasks when ground truth ≥ 2
- Performance scales inversely with scene congestion, with most complex scenes (36 viewed objects) dropping below 60% accuracy
- Full Rotation task shows 86% and 90% failure rates for single and double duplication when ground truth is 2
- Numerical comparison accuracy drops to 66% when target counts differ by 0, indicating "numerical fuzziness"

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Systematic undercounting and object identity merging reveals MLLMs lack integrated spatiotemporal world models.
- Mechanism: Models process frames as loosely associated perceptual snapshots rather than building a unified, viewpoint-invariant scene representation that tracks object instances across egomotion.
- Core assumption: If models possessed robust object permanence, counting accuracy would not collapse when ground truth ≥ 2, and Full Rotation performance would leverage continuous visual context to distinguish duplicated objects.
- Evidence anchors:
  - [abstract] "MLLMs fundamentally struggle to build integrated world models that combine visual perception with explicit egomotion, particularly failing to track distinct object instances across viewpoint changes."
  - [Section 3.2] "Counting questions with target object class ground truth ≥ 2 have an overall accuracy of only 35.7%"; models "progressively undercount objects as ground truth increases."
  - [corpus] Multi-SpatialMLLM (arXiv:2505.17015) similarly finds MLLMs limited to single-image spatial understanding, confirming the multi-frame integration gap.
- Break condition: If future models achieve >85% accuracy on Full Rotation counting tasks with duplicated objects, this mechanism would no longer explain the performance ceiling.

### Mechanism 2
- Claim: Scene congestion and duplicate density create attention competition that degrades relational and temporal reasoning.
- Mechanism: As non-target objects increase, signal-to-noise ratio drops for target object pairs, causing spatial relation and temporal ordering accuracy to scale inversely with congestion; duplicates can partially mitigate this by reducing distinct entity competition.
- Core assumption: Attention mechanisms distribute fixed capacity across all visible entities; more entities means weaker binding per entity.
- Evidence anchors:
  - [Section 3.2] "Most complex scenes (36 viewed objects) dropping below 60% overall accuracy"; temporal ordering "shows a strong positive correlation with maximum duplicate counts, with the highest counts reaching saturation."
  - [Figure 4] Performance curves show clear inverse scaling with observed object count across non-numerical tasks.
  - [corpus] 11Plus-Bench (arXiv:2511.11025) notes spatial reasoning and perception are "closely entangled" in human cognition but underexplored in MLLM evaluation—REM's controlled variation directly probes this.
- Break condition: If scaling curves flatten or improve with congestion (no attention competition effect), this mechanism is falsified.

### Mechanism 3
- Claim: Explicit egomotion cues are insufficiently grounded by current MLLMs to disambiguate visually similar configurations.
- Mechanism: Despite receiving discrete action annotations (e.g., "15° Right"), models fail to integrate movement history with visual change, treating 0° and 180° views as identical rather than inferring distinct object sets from rotation context.
- Core assumption: Models with genuine egomotion grounding would recognize that 180° rotation reveals different objects in similar spatial configuration.
- Evidence anchors:
  - [Section 3.4] Full Rotation shows "86% and 90% failure rates for single and double duplication" when ground truth is 2; "o3 fails to use the explicit motion cues and clear object differences."
  - [Section 2.1] Actions are "explicitly provided alongside the image sequence as input to the model, simulating an agent aware of its own movements."
  - [corpus] RynnEC (arXiv:2508.14160) and EmbodiedEval (arXiv:2501.11858) address embodied cognition but do not isolate egomotion grounding failures as systematically—REM's Full Rotation directly targets this.
- Break condition: If models correctly count duplicated objects in Full Rotation without architectural changes to egomotion integration, the claim that current architectures cannot ground motion cues would be contradicted.

## Foundational Learning

- Concept: **Object permanence**
  - Why needed here: REM's counting tasks require understanding that objects persist when out of frame and are the same entity when they reappear; models that treat each frame independently will undercount or overcount.
  - Quick check question: If you see a red sphere in frame 1, rotate 180°, then see it again in frame 12, how many red spheres exist in the scene?

- Concept: **Egocentric vs. allocentric spatial reference frames**
  - Why needed here: Left/right positioning questions in REM are egocentric (relative to camera facing); understanding how these relations transform under rotation is essential for tracking spatial relationships across viewpoints.
  - Quick check question: After a 90° right turn, does an object that was previously "left" now become "behind" or "ahead"?

- Concept: **Egomotion integration**
  - Why needed here: REM provides explicit discrete actions; the benchmark tests whether models can use self-motion information to update internal spatial representations rather than treating each frame as an independent perception.
  - Quick check question: Given frames 1-4 and actions "forward, rotate-left, forward," can you predict which objects should be visible in frame 5 before seeing it?

## Architecture Onboarding

- Component map:
  - Trajectory Generator -> Scene Renderer -> Data Dictionary Builder -> QA Generator -> Verifier

- Critical path:
  1. Define trajectory parameters (length, object count, duplicate count) -> `scene_config.json`
  2. Run `run_simulation.py` -> renders images + annotations
  3. Build data dictionary -> generate QA pairs
  4. Query MLLM with system prompt + question + actions + images
  5. Verify response against ground truth using keyword extraction

- Design tradeoffs:
  - **Synthetic vs. real environments**: Synthetic enables precise control and ground truth but may not transfer to real-world embodied tasks
  - **Simple vs. complex visual features**: Intentionally simple shapes/colors isolate spatial reasoning from perceptual difficulty; may underestimate challenges in realistic scenes
  - **Discrete vs. continuous actions**: Discrete 15°/1m steps simplify integration but differ from real robot control streams

- Failure signatures:
  - **Systematic undercounting**: Predictions correlate with max single-frame count rather than trajectory total (Figure 8)
  - **Numerical comparison fuzziness**: Accuracy drops to 66% when target counts differ by 0 (Figure 5)
  - **Full Rotation collapse**: ~90% failure on ground-truth-2 duplicated object counting (Table 3)

- First 3 experiments:
  1. Replicate o3 baseline results on Mini-Baseline (154 QA pairs, 18 trajectories) to validate evaluation pipeline and establish local performance baseline.
  2. Ablate explicit action annotations: provide images only (no movement text) to quantify how much egomotion cues currently contribute to spatial reasoning.
  3. Test a vision-language model with explicit spatial memory module (e.g., object-centric slot attention or scene graph construction) against Full Rotation to diagnose whether architectural changes improve object distinction under viewpoint change.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural modifications or training paradigms can enable MLLMs to maintain viewpoint-invariant object representations across dynamic egomotion?
- Basis in paper: [explicit] "Future work must prioritize architectures and training paradigms that explicitly foster robust object permanence and the integration of spatiotemporal context, using targeted benchmarks like REM to measure progress towards spatially grounded intelligence."
- Why unresolved: Current models fail to integrate explicit motion cues with visual context, instead merging distinct object instances when viewpoints change significantly.
- What evidence would resolve it: Demonstrated performance improvement on Full Rotation tasks, particularly on ground-truth-2 counting questions where o3 currently fails at 86-90% rates.

### Open Question 2
- Question: Are MLLMs performing true multi-frame object aggregation, or simply selecting a representative single frame for counting decisions?
- Basis in paper: [inferred] "Based on these observations alone, it is not clear whether counting is considering overall trajectory object counts... or if the model is simply selecting some subset (potentially the maximum object frame) of the trajectory."
- Why unresolved: The 13% higher predictions in multi-frame vs. single-frame suggests aggregation, but predictions closely track single-frame maximum counts rather than ground truth totals.
- What evidence would resolve it: Controlled experiments varying single-frame visibility vs. total trajectory count independently to isolate aggregation behavior.

### Open Question 3
- Question: Can explicit spatial memory modules or cognitive map architectures bridge the performance gap between MLLMs (80%) and humans (97.8%) on embodied reasoning?
- Basis in paper: [inferred] Humans achieved 97.8% on Mini-Baseline with unlimited time and navigation tools, while o3 achieved only 80.1%, suggesting fundamental architectural differences in spatial representation.
- Why unresolved: The paper demonstrates the gap but does not test interventions; humans may use qualitatively different cognitive mechanisms than current transformer-based architectures.
- What evidence would resolve it: Comparison of models with/without explicit spatial memory components on identical REM trajectories.

### Open Question 4
- Question: How do REM benchmark findings transfer to real-world embodied scenarios with complex textures, lighting, and object diversity?
- Basis in paper: [inferred] "These environments are intentionally made to be simple, with easily distinguishable object shapes and colors, as REM aims to diagnostically pinpoint failures... not object detection or visual distinction."
- Why unresolved: Synthetic environments isolate spatial reasoning but may not capture compounding failures when visual perception is also challenged.
- What evidence would resolve it: Evaluation on real-world navigation datasets or photorealistic 3D scans showing similar performance degradation patterns.

## Limitations
- Synthetic environments with simple shapes and colors may not capture real-world visual complexity and perceptual challenges
- Discrete 15° rotation and 1m movement steps differ from continuous real-world robot control streams
- Keyword-based verification may overestimate performance by accepting semantically correct but structurally different answers

## Confidence
- High confidence: Performance drops with scene complexity and trajectory length (validated by controlled ablation across congestion levels)
- Medium confidence: Object permanence failures represent fundamental architectural limitations (inferred from systematic undercounting patterns but not directly tested with architectural modifications)
- Medium confidence: Egomotion integration failures are the primary cause of Full Rotation collapse (plausible given motion cue availability but not conclusively proven)

## Next Checks
1. Test whether models can transfer spatial reasoning skills from REM to photorealistic environments with similar ground truth verification mechanisms.
2. Implement a vision-language model with explicit spatial memory (scene graphs or object slots) and evaluate on Full Rotation to determine if architectural changes overcome object identity merging failures.
3. Conduct human studies on REM tasks to establish baseline performance and identify whether model failures stem from genuine spatial reasoning limitations or task format mismatches.