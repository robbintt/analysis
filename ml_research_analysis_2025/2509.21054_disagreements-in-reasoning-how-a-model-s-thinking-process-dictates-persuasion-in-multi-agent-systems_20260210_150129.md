---
ver: rpa2
title: 'Disagreements in Reasoning: How a Model''s Thinking Process Dictates Persuasion
  in Multi-Agent Systems'
arxiv_id: '2509.21054'
source_url: https://arxiv.org/abs/2509.21054
tags:
- persuasion
- thinking
- content
- persuasive
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper challenges the prevailing hypothesis that persuasive
  efficacy in multi-agent systems is primarily a function of model scale, proposing
  instead that it is fundamentally dictated by a model''s underlying cognitive process,
  especially its capacity for explicit reasoning. Through a series of multi-agent
  persuasion experiments, the authors identify a fundamental trade-off termed the
  "Persuasion Duality": reasoning processes in Large Reasoning Models (LRMs) exhibit
  significantly greater resistance to persuasion, maintaining initial beliefs more
  robustly, while making reasoning transparent by sharing "thinking content" dramatically
  increases their ability to persuade others.'
---

# Disagreements in Reasoning: How a Model's Thinking Process Dictates Persuasion in Multi-Agent Systems

## Quick Facts
- arXiv ID: 2509.21054
- Source URL: https://arxiv.org/abs/2509.21054
- Reference count: 32
- Key outcome: This paper challenges the prevailing hypothesis that persuasive efficacy in multi-agent systems is primarily a function of model scale, proposing instead that it is fundamentally dictated by a model's underlying cognitive process, especially its capacity for explicit reasoning.

## Executive Summary
This paper challenges the prevailing hypothesis that persuasive efficacy in multi-agent systems is primarily a function of model scale, proposing instead that it is fundamentally dictated by a model's underlying cognitive process, especially its capacity for explicit reasoning. Through a series of multi-agent persuasion experiments, the authors identify a fundamental trade-off termed the "Persuasion Duality": reasoning processes in Large Reasoning Models (LRMs) exhibit significantly greater resistance to persuasion, maintaining initial beliefs more robustly, while making reasoning transparent by sharing "thinking content" dramatically increases their ability to persuade others. The study also reveals complex dynamics of influence propagation and decay within multi-hop persuasion between multiple agent networks, showing that persuasive influence propagates non-linearly through multi-agent chains with both amplification and attenuation depending on chain composition.

## Method Summary
The study evaluates persuasion dynamics in multi-agent systems by measuring how "Large Reasoning Models" (LRMs) and standard LLMs influence each other. The methodology uses MMLU (objective) and 1,000 claims from PersuasionBench/Perspectrum (subjective) datasets. Persuaded-Rate (PR), Remain-Rate (RR), and Other-Rate (OR) metrics are calculated by comparing initial answers to final answers after receiving persuasive context. The workflow involves: 1) getting the persuadee's initial answer, 2) persuader generates argument (optionally including internal "thinking content"), 3) getting persuadee's final answer given the argument. The study uses models including o4-mini, DeepSeek-R1, Gemini-2.5-flash, Qwen3-32B, Hunyuan-7B, Llama-3-8B, and Qwen2.5-7B with temperature 0.7 and top-p 0.8, implemented using VLLM (v0.10.0) and transformers (v4.56.0).

## Key Results
- Reasoning processes in Large Reasoning Models (LRMs) exhibit significantly greater resistance to persuasion, reducing PR by 7.82-29.68% when thinking mode is enabled
- Making reasoning transparent by sharing "thinking content" dramatically increases persuasive power by approximately 21% on average
- Persuasive influence propagates non-linearly through multi-agent chains with both amplification and attenuation depending on chain composition
- Models are more susceptible to confident rhetorical assertions than substantive reasoning content, with attention patterns showing 11.1% to confident assertions vs 0.39% to reasoning

## Why This Works (Mechanism)

### Mechanism 1: Persuasion Duality Trade-off
- **Claim:** Explicit reasoning processes create asymmetric effects: they increase resistance to persuasion when active internally, but increase persuasive power when shared externally.
- **Mechanism:** When an LRM engages its thinking mode, it establishes an internal benchmark for evaluating incoming arguments, making it harder to be swayed by superficial rhetoric. Conversely, when that same thinking content is shared with others, it provides logical coherence that enhances persuasiveness by approximately 21% on average.
- **Core assumption:** The reasoning process serves as both shield (internal) and sword (external) depending on whether it remains internal or is transmitted.
- **Evidence anchors:**
  - [abstract] "reasoning processes in Large Reasoning Models (LRMs) exhibit significantly greater resistance to persuasion... while making reasoning transparent by sharing 'thinking content' dramatically increases their ability to persuade others"
  - [section 3.2] "comparing the two adjacent columns... using the thinking mode reduces the PR in Figures 1a and 1b by an average of 7.82% and 29.68%, respectively"
  - [corpus] Related work on disagreements in MAS (arXiv:2502.15153) explores robustness under agent disagreements but does not directly validate the duality mechanism
- **Break condition:** If thinking content contains logical inconsistencies, persuasiveness decreases (mismatched thinking content reduced PR to 32.42%, below baseline)

### Mechanism 2: Attention Bias Toward Rhetorical Confidence
- **Claim:** Models are persuaded more by confident surface-level assertions than by substantive reasoning content.
- **Mechanism:** Internal attention analysis reveals models allocate ~11.1% attention to short confident assertions versus only 0.39% to reasoning sections. This bias causes models to overweight rhetorical cues like "makes perfect sense" while underweighting logical evaluation.
- **Core assumption:** Attention distribution correlates with persuasion susceptibility.
- **Evidence anchors:**
  - [section 4.1] "The model directs exceptionally high attention to the short, confident assertion... its focus on the longer section containing the supposed 'reasoning' is extremely low"
  - [section 4.1] "This attention pattern shows that the model's decisions are guided more by confident rhetorical cues than by logical evaluation"
  - [corpus] No direct corpus validation of attention-mechanism explanation for persuasion
- **Break condition:** If prompts explicitly instruct critical evaluation of rhetoric vs. facts, the bias can be mitigated (adversarial argument detection reduced PR significantly)

### Mechanism 3: Non-linear Multi-hop Persuasion Propagation
- **Claim:** Persuasive influence propagates through agent chains with amplification or attenuation depending on chain composition, not simple additive effects.
- **Mechanism:** In A→B→C chains, the combined persuasion rate can exceed direct A→C persuasion when intermediate agents amplify the signal, or decay when reasoning modes create resistance bottlenecks.
- **Core assumption:** Chain composition (which models, which thinking modes) determines propagation dynamics.
- **Evidence anchors:**
  - [abstract] "persuasive influence propagates non-linearly through multi-agent chains with both amplification and attenuation depending on chain composition"
  - [section 3.5] "the combined persuasion rate of a chain... exceeds that of A directly persuading C... effects, both amplifying and diminishing, depend on the configuration of agents and their reasoning modes"
  - [corpus] Weak direct support; related work on multi-agent collaboration (arXiv:2504.09772) addresses scaling but not persuasion propagation specifically
- **Break condition:** Objective questions show fewer amplification cases than subjective questions, suggesting truth anchors may dampen propagation

## Foundational Learning

- **Concept: Large Reasoning Models (LRMs) vs. Standard LLMs**
  - **Why needed here:** The entire mechanism distinction hinges on whether a model uses explicit reasoning procedures (Chain-of-Thought, thinking modes) or implicit pattern matching
  - **Quick check question:** Can you explain why DeepSeek-R1 with thinking enabled resists persuasion differently than Qwen2.5-7B without thinking?

- **Concept: Persuasion Metrics (PR, RR, OR)**
  - **Why needed here:** The paper quantifies persuasion through Persuaded-Rate, Remain-Rate, and Other-Rate; understanding these is essential for interpreting the heatmaps
  - **Quick check question:** If a model initially answers correctly and after persuasion switches to the wrong answer, which metric increases?

- **Concept: Multi-Agent Communication Topology**
  - **Why needed here:** Multi-hop persuasion requires understanding how agent chains transmit information and where bottlenecks or amplifiers occur
  - **Quick check question:** In a 3-agent chain A→B→C, what factors determine whether persuasion amplifies or attenuates?

## Architecture Onboarding

- **Component map:**
  - *Persuader agent:* Generates arguments, optionally with thinking content transmission
  - *Persuadee agent:* Receives arguments, processes with/without thinking mode
  - *Thinking mode toggle:* Enables explicit CoT-style reasoning (switchable on Gemini-2.5-flash, Qwen3-32B, Hunyuan-7B)
  - *Thinking content transmitter:* Decides whether `<think process>` is included in message payload

- **Critical path:**
  1. Obtain initial answer from persuadee (establishes baseline)
  2. Persuader generates content with/without thinking
  3. Transmit with/without thinking content visible
  4. Measure answer change → compute PR/RR/OR

- **Design tradeoffs:**
  - Transparency vs. security: Sharing thinking increases persuasiveness but exposes reasoning to adversarial analysis
  - Robustness vs. persuadability: Enabling thinking mode protects against manipulation but may make agents less responsive to legitimate corrections
  - Chain composition: Placing LRM thinking-mode agents in chains creates resistance bottlenecks

- **Failure signatures:**
  - High PR on objectively wrong answers with confident rhetoric → attention bias exploited
  - Thinking content with logical conflicts → PR drops below baseline
  - Weaker models at chain positions → unexpected amplification of misinformation

- **First 3 experiments:**
  1. Replicate the thinking-mode toggle experiment on your target model to establish baseline PR/RR with and without thinking
  2. Test adversarial argument detection prompt (from Appendix C.3) on your most vulnerable model pair to quantify robustness gains
  3. Construct a 3-agent chain with mixed reasoning modes to map where amplification vs. attenuation occurs in your system

## Open Questions the Paper Calls Out
None

## Limitations
- The specific 1,000 claims sampled from PersuasionBench/Perspectrum are not listed, making independent validation challenging
- The exact mechanism for parsing "thinking content" (e.g., `<think/>` tokens) from model outputs is not fully specified
- The attention mechanism analysis provides intuitive explanations but lacks rigorous validation through ablation studies

## Confidence
- **High Confidence:** The core empirical finding that reasoning modes increase resistance to persuasion (7.82-29.68% reduction in PR) is supported by multiple experimental conditions across different model pairs
- **Medium Confidence:** The persuasion duality trade-off mechanism (reasoning as shield vs. sword) is conceptually compelling but relies on indirect evidence
- **Low Confidence:** The multi-hop persuasion propagation dynamics are described but lack sufficient experimental validation

## Next Checks
1. **Ablation Study on Attention Bias:** Conduct controlled experiments where models are explicitly prompted to either focus on rhetorical confidence or logical content, measuring resulting persuasion rates to establish causal relationships between attention patterns and susceptibility
2. **Chain Composition Analysis:** Systematically vary the composition of 3-agent chains (LLM→LLM→LLM, LRM→LLM→LLM, LLM→LRM→LLM, etc.) to map the precise conditions under which amplification vs. attenuation occurs, quantifying the non-linear propagation effects
3. **Cross-dataset Validation:** Replicate the persuasion experiments using entirely different question sets (e.g., alternative benchmarks beyond MMLU and PersuasionBench) to test the generalizability of the observed persuasion duality and attention bias effects across diverse domains and question types