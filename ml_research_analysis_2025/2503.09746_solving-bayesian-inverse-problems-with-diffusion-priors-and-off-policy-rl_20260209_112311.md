---
ver: rpa2
title: Solving Bayesian inverse problems with diffusion priors and off-policy RL
arxiv_id: '2503.09746'
source_url: https://arxiv.org/abs/2503.09746
tags:
- diffusion
- inverse
- posterior
- problems
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Relative Trajectory Balance (RTB), an off-policy
  reinforcement learning objective for training diffusion models to perform Bayesian
  posterior inference. RTB addresses the challenge of sampling from intractable posterior
  distributions by fine-tuning a diffusion prior to match the product of the prior
  and a constraint function.
---

# Solving Bayesian inverse problems with diffusion priors and off-policy RL

## Quick Facts
- arXiv ID: 2503.09746
- Source URL: https://arxiv.org/abs/2503.09746
- Reference count: 11
- Primary result: Introduces RTB, an off-policy RL objective for diffusion posterior inference that achieves competitive rewards while maintaining higher log-partition function values than existing methods

## Executive Summary
This paper introduces Relative Trajectory Balance (RTB), an off-policy reinforcement learning objective for training diffusion models to perform Bayesian posterior inference. RTB addresses the challenge of sampling from intractable posterior distributions by fine-tuning a diffusion prior to match the product of the prior and a constraint function. The authors demonstrate RTB's effectiveness on linear and non-linear inverse problems in vision, including inpainting, Fourier phase retrieval, and nonlinear deblurring, using MNIST and CIFAR-10 datasets. They also apply RTB to the scientific problem of gravitational lensing. Key results show RTB achieves competitive rewards while maintaining significantly higher log-partition function values compared to existing methods like DPS and FPS, indicating better posterior approximation. The method's off-policy training capability and ability to condition on external variables make it scalable and flexible for complex real-world applications.

## Method Summary
RTB fine-tunes a pretrained unconditional diffusion prior into a posterior diffusion model by optimizing an objective that balances trajectory likelihoods between prior and posterior processes. The method uses off-policy training with a replay buffer to sample trajectories from high-reward regions, preventing mode collapse. The posterior drift can include a Langevin inductive bias term incorporating the constraint gradient for improved sample efficiency when constraints are differentiable. The objective minimizes the squared log-ratio of the trajectory probability under the posterior versus the product of prior probability and constraint, with gradients computed without backpropagation through the sampling process.

## Key Results
- RTB achieves competitive rewards on benchmark inverse problems while maintaining significantly higher log-partition function values than DPS and FPS methods
- Off-policy training with replay buffers improves stability and prevents mode collapse compared to on-policy approaches
- The method successfully scales to scientific problems like gravitational lensing, though with ~30% divergence rate on peaky reward landscapes
- Conditional RTB enables amortized inference across measurements, showing strong generalization to held-out data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RTB enables asymptotically unbiased posterior sampling by balancing trajectory likelihood ratios between prior and posterior diffusion processes.
- Mechanism: The objective $\mathcal{L}_{RTB} = \left(\log \frac{Z_\phi \cdot p^{post}_\phi(\tau)}{r(x_1) p_\theta(\tau)}\right)^2$ enforces that when minimized to zero across all trajectories, the posterior $p^{post}_\phi(x_1) \propto p_\theta(x_1)r(x_1)$. Crucially, gradients do not require backpropagation through the sampling process, enabling off-policy training.
- Core assumption: The backward policy $P^B$ is fixed (Brownian bridge), allowing cancellation in the trajectory balance equation.
- Evidence anchors:
  - [abstract] "asymptotically solve Bayesian inverse problems optimally"
  - [section 2] Equation 6 and discussion of off-policy gradient advantage
  - [corpus] Related work on diffusion posterior sampling (arXiv 2510.14114) surveys similar trajectory-based approaches but notes approximation challenges
- Break condition: If the backward policy is misspecified or the prior model is poorly fit, the cancellation assumption fails and the objective becomes biased.

### Mechanism 2
- Claim: Off-policy exploration with replay buffers prevents mode collapse and improves posterior coverage.
- Mechanism: By sampling trajectories from a behavior policy $\mathcal{P}_\beta(\tau) = P^B(\tau|x_1)P(x_1; D)$ where $D$ prioritizes high-reward samples, RTB can train on trajectories not generated by the current policy. Backtracking exploration samples noising trajectories from high-density posterior samples.
- Core assumption: High-reward samples from the replay buffer are representative of posterior modes.
- Evidence anchors:
  - [section 3.1] "Backtracking exploration with replay buffer" formulation
  - [section 5.2] "stability during training remains a challenge...replay buffers to help stabilize"
  - [corpus] Weak direct evidence; arXiv 2502.03332 discusses mixture-based guidance but not replay buffers specifically
- Break condition: If replay buffer distribution becomes stale or excludes modes, training collapses to covered modes only.

### Mechanism 3
- Claim: Langevin inductive bias improves sample efficiency when constraints are differentiable.
- Mechanism: Parametrizing $u^{post}_\phi(x_t, t) = NN_1(x_t, t; \phi) + NN_2(x_t, t, \phi)\nabla_{x_t}\log r(x_t)$ allows the constraint gradient to guide intermediate denoising steps, providing denser signal than terminal reward alone.
- Core assumption: $\nabla_{x_t}\log r(x_t)$ provides useful directional information at intermediate noise levels.
- Evidence anchors:
  - [section 3] Equation 8 and discussion of inductive bias
  - [section 4] RTB+DPS hybrid experiments show improved rewards
  - [corpus] arXiv 2601.20756 discusses supervised guidance for infinite-dimensional diffusion models, supporting gradient-based conditioning
- Break condition: If constraint gradients are noisy or misleading at intermediate timesteps, the inductive bias harms rather than helps convergence.

## Foundational Learning

- Concept: Score-based diffusion models and SDE discretization
  - Why needed here: RTB operates on discretized reverse SDEs; understanding $dx_t = u_t(x_t)dt + \sigma_t dw_t$ and the relationship between drift parametrization and denoising is essential.
  - Quick check question: Can you explain why the forward process $x_0 \to x_1$ uses Gaussian perturbations and how the reverse process learns to denoise?

- Concept: GFlowNets and trajectory balance objectives
  - Why needed here: RTB derives from trajectory balance in GFlowNets; the flow matching perspective explains why minimizing the squared log-ratio yields correct posteriors.
  - Quick check question: What does it mean for a trajectory balance objective to be "asymptotically unbiased," and why does on-policy training risk mode collapse?

- Concept: Bayesian inverse problems with likelihood constraints
  - Why needed here: The constraint $r(x_1) = p(y|x_1)$ defines the posterior; understanding how forward operators (masking, Fourier transform, blurring) induce likelihoods is critical for problem formulation.
  - Quick check question: For inpainting with mask $P$ and noise $\sigma$, how would you write $p(y|x)$?

## Architecture Onboarding

- Component map:
  - Prior diffusion model $p_\theta$ -> Posterior diffusion model $p^{post}_\phi$ -> Partition function estimator $Z_\phi$ -> Constraint function $r(x_1, y)$ -> Replay buffer $D$ -> Off-policy sampler

- Critical path:
  1. Load pretrained diffusion prior weights into posterior model
  2. Initialize log $Z_\phi \approx 0$ (or estimate from prior samples)
  3. For each training iteration: sample trajectory $\tau$ (on-policy or from buffer), compute $\mathcal{L}_{RTB}$, update $\phi$ and $Z_\phi$
  4. Periodically evaluate log $Z$ via held-out samples to monitor posterior quality

- Design tradeoffs:
  - On-policy vs. off-policy: On-policy is simpler but may miss modes; off-policy with replay buffer improves coverage but requires careful buffer management
  - Conditional vs. unconditional posterior: Conditional enables amortized inference across measurements but requires more parameters and data
  - Loss variance vs. squared objective: Variance-based loss (VarGrad) removes $Z_\phi$ dependency but may be less stable early in training

- Failure signatures:
  - Diverging log $Z$ values: Policy collapsed to narrow region; increase exploration or replay buffer diversity
  - High reward but low log $Z$: Overfitting to constraint, not matching true posterior (common in DPS/FPS)
  - Training instability on peaky rewards (e.g., gravitational lensing with small $\sigma$): Gradient signal too sparse; consider smoothing likelihood or curriculum on $\sigma$

- First 3 experiments:
  1. Box inpainting on MNIST with unconditional prior: Implement RTB from scratch, compare log $Z$ and LPIPS against DPS baseline to verify posterior quality
  2. Ablation on off-policy techniques: Train with/without replay buffer and backtracking, measure mode coverage on a synthetic multimodal posterior
  3. Conditional posterior for phase retrieval: Extend RTB with conditioning network, evaluate generalization to held-out measurements via FID and log $Z$

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the stability of Relative Trajectory Balance (RTB) training be improved to prevent policy divergence in problems with sharp or "peaky" reward functions?
- Basis in paper: [explicit] The authors report that in the gravitational lensing experiments, "About 30% of runs, the policy diverged irrecoverably," noting that the method was unstable due to the peaky reward landscape.
- Why unresolved: While the authors hypothesize that "off-policy tricks such as replay buffers" would stabilize training, they used only on-policy samples for this specific task and did not validate the proposed solution.
- What evidence would resolve it: Empirical results demonstrating a significantly reduced divergence rate in the gravitational lensing task when utilizing specific off-policy adaptation techniques or replay buffers.

### Open Question 2
- Question: Can RTB be effectively extended to discrete or structured scientific domains, such as inverse protein design?
- Basis in paper: [explicit] The conclusion explicitly identifies "Extending RTB to other important scientific applications, such as inverse protein design" as a promising direction for future research.
- Why unresolved: The current study validates RTB on continuous visual data (MNIST, CIFAR-10, galaxy images), whereas protein design involves discrete sequences and complex structural constraints not covered by the current methodology.
- What evidence would resolve it: Successful application of the RTB objective to generate valid protein sequences that satisfy specific structural or functional targets, demonstrating competitive sample diversity and reward.

### Open Question 3
- Question: Does RTB overcome the inherent biases that hinder existing training-free methods when performing posterior inference in latent space?
- Basis in paper: [inferred] The abstract notes that existing methods "struggle to perform effective posterior inference in latent space due to inherent biases." However, the experimental section focuses exclusively on pixel-space diffusion models.
- Why unresolved: The authors critique the failure of other methods in latent space but do not provide benchmarks or results for RTB operating on Latent Diffusion Models (LDMs) like Stable Diffusion.
- What evidence would resolve it: Comparative benchmarks showing RTB maintaining high log-partition function values and fidelity in a latent space environment, contrasting against the failures of DPS or FPS in the same setting.

## Limitations

- RTB training is sensitive to peaky reward landscapes, with approximately 30% divergence rate observed on gravitational lensing tasks
- Computational cost of off-policy training with replay buffers is not thoroughly analyzed and may become prohibitive for high-dimensional problems
- Limited ablation studies on RTB components like Langevin inductive bias, replay buffer size, and backtracking exploration depth

## Confidence

**High Confidence:**
- RTB achieves competitive rewards on benchmark inverse problems
- The log-partition function provides a reliable metric for posterior quality comparison
- Off-policy training with replay buffers improves stability and coverage

**Medium Confidence:**
- Asymptotic unbiasedness of RTB under ideal conditions
- The Langevin inductive bias improves sample efficiency for differentiable constraints
- RTB generalizes well to scientific problems like gravitational lensing

**Low Confidence:**
- Computational efficiency claims compared to alternative methods
- Scalability to very high-dimensional problems without significant modifications
- Robustness across diverse posterior geometries without hyperparameter tuning

## Next Checks

1. **Mode Coverage Analysis:** For a synthetic multimodal posterior (e.g., mixture of Gaussians), measure RTB's ability to discover all modes compared to on-policy training. Use Wasserstein distance between empirical and true posterior samples as the metric.

2. **Replay Buffer Ablation:** Systematically vary replay buffer size and sampling strategy on a standard task (e.g., MNIST inpainting). Measure the trade-off between computational cost and posterior quality (log Z and sample diversity).

3. **Reward Landscape Sensitivity:** Create controlled experiments with varying reward peakiness (Gaussian likelihood with different Ïƒ values). Measure RTB's success rate and training stability across this spectrum to quantify the method's robustness to challenging reward landscapes.