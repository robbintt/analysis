---
ver: rpa2
title: 'OraPlan-SQL: A Planning-Centric Framework for Complex Bilingual NL2SQL Reasoning'
arxiv_id: '2510.23870'
source_url: https://arxiv.org/abs/2510.23870
tags:
- planner
- agent
- plan
- guidelines
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'OraPlan-SQL is a bilingual NL2SQL system designed to handle complex
  reasoning such as arithmetic, commonsense, and hypothetical inference. It uses a
  two-stage agentic framework: a Planner agent generates stepwise natural language
  plans, and a SQL agent converts these into executable SQL.'
---

# OraPlan-SQL: A Planning-Centric Framework for Complex Bilingual NL2SQL Reasoning

## Quick Facts
- **arXiv ID**: 2510.23870
- **Source URL**: https://arxiv.org/abs/2510.23870
- **Reference count**: 13
- **Primary result**: Ranked first on Archer NL2SQL Evaluation Challenge 2025 with 55.0% English and 56.7% Chinese execution accuracy

## Executive Summary
OraPlan-SQL is a bilingual NL2SQL system designed to handle complex reasoning such as arithmetic, commonsense, and hypothetical inference. It uses a two-stage agentic framework: a Planner agent generates stepwise natural language plans, and a SQL agent converts these into executable SQL. The system ranked first on the Archer NL2SQL Evaluation Challenge 2025, achieving 55.0% execution accuracy in English and 56.7% in Chinese, exceeding the second-best system by over 6%, while maintaining over 99% SQL validity. Key innovations include feedback-guided meta-prompting to refine the planner using clustered failure cases, entity-linking guidelines to address transliteration and entity mismatch in multilingual queries, and plan diversification with majority voting for robustness. These advances highlight the importance of planner prompt design in complex, multilingual NL2SQL reasoning.

## Method Summary
OraPlan-SQL employs a two-stage agentic framework where a Planner agent maps natural language queries to stepwise natural language plans, and a SQL agent converts these plans into executable SQL. The system enhances robustness through feedback-guided meta-prompting (refining the planner using clustered failure cases), entity-linking guidelines for bilingual queries, and plan diversification with majority voting. Schema retrieval is embedding-based, and both agents use in-context learning with semantically similar examples. The approach emphasizes planner prompt design to handle complex reasoning tasks including arithmetic, commonsense, and hypothetical inference.

## Key Results
- Ranked first on Archer NL2SQL Evaluation Challenge 2025 (55.0% English, 56.7% Chinese execution accuracy)
- Exceeded second-best system by over 6% accuracy
- Maintained >99% SQL validity
- Direct bilingual generation (Chinese→English plan) achieved 79.8% vs. 55.8% for translation-based pipeline

## Why This Works (Mechanism)

### Mechanism 1: Feedback-Guided Meta-Prompting for Planner Refinement
- Claim: Distilling clustered failure cases into corrective guidelines improves planner generalization without multi-agent orchestration overhead.
- Mechanism: Failure cases from a held-out set are clustered with human input; an LLM then generates high-level prompt guidelines (e.g., arithmetic handling, hypothetical decomposition rules) that are integrated into the planner's system prompt.
- Core assumption: Failure patterns observed on held-out data generalize to unseen queries; human clustering captures meaningful error categories.
- Evidence anchors:
  - [abstract] "Failure cases from a held-out set are clustered with human input, and an LLM distills them into corrective guidelines that are integrated into the planner's system prompt."
  - [Section 2.2] "With human input, the errors are clustered, and an LLM derives corrective guidelines... By iteratively refining the planner's system prompt with these derived guidelines, the agent improves accuracy."
  - [Section 3.2, Table 2] Removing guidelines drops EX from 72.12% to 44.23% (-27.89 points) on English dev.
- Break condition: If failure case clustering is noisy or human labeling inconsistent, distilled guidelines may overfit to specific error types and degrade on distribution shift.

### Mechanism 2: Two-Stage Planning Decomposition (Planner → SQL Agent)
- Claim: Decomposing NL2SQL into explicit natural language planning followed by SQL generation improves reasoning transparency and error diagnosis compared to direct mapping.
- Mechanism: Planner agent f_plan maps (Q, D) → P (stepwise NL plan). SQL agent f_sql maps (P, D) → S (executable SQL). SQL agent reliably follows the plan, so refinements focus on the planner.
- Core assumption: The SQL agent faithfully translates plans; most errors originate in planning, not SQL generation.
- Evidence anchors:
  - [abstract] "Since SQL agent reliably adheres to the plan, our refinements focus on the planner."
  - [Section 2] "f_plan : (Q,D) → P, f_sql : (P,D) → S... This decomposition introduces an explicit intermediate plan P to make reasoning steps transparent."
  - [Section 3.2] Removing the planner agent drops EX from 72.12% to 64.42% (-7.70 points).
- Break condition: If SQL agent fails to follow complex plans (e.g., nested subqueries, multi-step arithmetic), errors shift to SQL generation and planner refinement yields diminishing returns.

### Mechanism 3: Plan Diversification with Majority Voting
- Claim: Generating multiple candidate plans at higher temperature and selecting final SQL via majority voting over execution results improves robustness by mitigating single-plan errors.
- Mechanism: Planner temperature set to 0.7 generates diverse plans; each plan is converted to SQL and executed; final output chosen by majority vote on execution results.
- Core assumption: Correct plans converge on the same execution result; errors are uncorrelated across plans.
- Evidence anchors:
  - [abstract] "Plan diversification with majority voting for robustness."
  - [Section 2.4] "To enhance robustness, we generate diverse plans by setting the planner's temperature to 0.7... final answer is determined through majority voting."
  - [Section 3.2] Single plan EX: 71.15%; with majority voting: 72.12% (+0.97 points)—marginal but positive.
- Break condition: If errors are systematic (e.g., all plans misinterpret the same ambiguous entity), majority voting amplifies rather than corrects errors.

### Mechanism 4: Entity-Linking Guidelines for Multilingual Queries
- Claim: Instructing the planner to generate alternative entity surface forms addresses transliteration and entity mismatch in bilingual settings.
- Mechanism: Entity-linking guidelines in planner prompt generate variants (e.g., "纽约" → "NYC", "New York City") explicitly included in the plan for SQL agent use.
- Core assumption: Database entity names follow predictable transliteration patterns; generating variants in the plan suffices without external entity linking tools.
- Evidence anchors:
  - [abstract] "Entity-linking guidelines to address transliteration and entity mismatch in multilingual queries."
  - [Section 2.2] "We integrate entity-linking guidelines into the planner's system prompt... instruct the planner to generate alternative surface forms for all entities."
  - [Section 5.2] Direct generation (Chinese input → English plan) achieves 79.8% vs. 55.8% for translation-based pipeline; transliteration artifacts (e.g., 格里公园 → Geli Park) cause errors in translation approach.
- Break condition: If database contains entities with unpredictable naming conventions or heavy abbreviation, surface-form variants may still miss correct matches.

## Foundational Learning

- Concept: **Intermediate representations in semantic parsing**
  - Why needed here: OraPlan-SQL relies on natural language plans as an explicit intermediate representation; understanding why this helps (transparent reasoning, error isolation) is prerequisite to evaluating the architecture.
  - Quick check question: Can you explain why an explicit plan P between query Q and SQL S aids debugging compared to direct Q→S mapping?

- Concept: **Meta-prompting and prompt engineering**
  - Why needed here: The core innovation is feedback-guided meta-prompting—using an LLM to generate prompt guidelines rather than task outputs. Understanding prompt refinement cycles is essential.
  - Quick check question: What distinguishes meta-prompting from standard few-shot prompting or fine-tuning?

- Concept: **Self-consistency / majority voting ensembles**
  - Why needed here: Plan diversification with majority voting is a form of self-consistency; understanding when ensemble methods help (uncorrelated errors) vs. fail (systematic bias) informs deployment decisions.
  - Quick check question: Under what conditions does majority voting over multiple LLM outputs fail to improve accuracy?

## Architecture Onboarding

- Component map:
  - Planner Agent (Q, D_retrieved) → P
  - SQL Agent (P, D_retrieved, ICL examples) → S
  - Schema Retriever (embedding-based) → top-k schema elements
  - Plan Diversifier (temperature=0.7) → multiple plans → majority voting
  - Meta-Prompting Pipeline (offline) → clustered failures → guidelines → planner prompt

- Critical path:
  1. Query Q + schema retrieval → Planner Agent (with optimized prompt) → Plan P
  2. Plan P + schema + ICL examples → SQL Agent → SQL S
  3. Execute S; if multiple plans, majority vote on results → final answer
  4. Offline: Periodically run meta-prompting pipeline on new failure cases to update planner prompt

- Design tradeoffs:
  - **Single planner vs. multi-agent orchestration**: Single planner with refined prompt is simpler but requires careful prompt engineering; multi-agent (as in AgenticData) offers modularity but adds orchestration overhead.
  - **Direct bilingual generation vs. translation pipeline**: Direct generation (Chinese → English plan) outperforms translation (79.8% vs. 55.8%) but requires planner to handle cross-lingual entity resolution natively.
  - **Temperature tuning**: Higher temperature (0.7) increases plan diversity for voting but raises risk of low-quality plans; tradeoff between coverage and noise.

- Failure signatures:
  - **Planner error**: Plan mis-specifies arithmetic (e.g., integer division instead of float), mishandles hypothetical conditions, or fails entity linking → SQL agent faithfully produces incorrect SQL.
  - **SQL agent deviation**: SQL ignores plan steps or hallucinates columns (rare per paper, but possible with complex nested queries).
  - **Majority voting failure**: All diverse plans converge on same wrong answer due to shared misunderstanding (e.g., ambiguous schema element).

- First 3 experiments:
  1. **Baseline ablation**: Remove planner agent entirely (Q → SQL agent directly) on dev set; expect ~7-8 point EX drop per paper. Confirms value of explicit planning.
  2. **Meta-prompting impact**: Run planner with/without distilled guidelines on held-out set; measure EX delta. Per paper, expect ~27-point drop without guidelines (English), confirming prompt refinement is critical.
  3. **Entity-linking stress test**: Construct bilingual queries with known transliteration variants (e.g., Chinese names of Western entities); compare EX with/without entity-linking guidelines. Expect higher mismatch errors without guidelines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can in-context learning (ICL) be effectively extended to the Planner Agent when the training set lacks ground-truth intermediate plans?
- Basis in paper: [explicit] The authors state in Section 2.1 that "ICL is limited to the SQL agent due to the lack of ground-truth plans in the training set," but identify extending it to the planner as a "potential future direction."
- Why unresolved: The current methodology relies on retrieving semantically similar question-SQL pairs, but there is no supervised data demonstrating the *reasoning steps* (plans) required to connect them, making it difficult to provide few-shot examples for planning.
- What evidence would resolve it: Demonstrating a method for synthesizing or automatically aligning training queries with stepwise plans to serve as ICL examples for the planner, resulting in improved accuracy.

### Open Question 2
- Question: Can the failure case clustering and guideline distillation process be fully automated to remove the reliance on human input?
- Basis in paper: [inferred] Section 2.2 describes the meta-prompting process where failure cases are "clustered with human input" before an LLM derives guidelines. This represents a methodological bottleneck.
- Why unresolved: The paper does not demonstrate if LLMs can autonomously identify semantic error patterns (clustering) without human labeling, nor if automated clustering produces guidelines of equal quality to human-curated ones.
- What evidence would resolve it: An ablation study comparing the performance of system prompts derived via human-aided clustering versus fully automated clustering.

### Open Question 3
- Question: Do the feedback-guided guidelines and entity-linking strategies transfer effectively to smaller, open-source models?
- Basis in paper: [inferred] The ablation study (Table 2) shows a significant performance gap between GPT-5 (72.12%) and GPT-4o (52.88%), suggesting the planning capabilities are heavily tied to the model's intrinsic reasoning power.
- Why unresolved: While the framework outperforms baselines with GPT-4o, it is unclear if the prompt engineering innovations alone are sufficient to enable complex reasoning on less capable or smaller open-source models often required for on-premise deployment.
- What evidence would resolve it: Benchmark results running OraPlan-SQL on open-source models (e.g., Llama-3 or Qwen) to measure the efficacy of the specific prompt guidelines independent of proprietary model intelligence.

## Limitations
- Limited evaluation on complex schemas beyond the Archer dataset
- No independent validation of the meta-prompting pipeline's robustness to noisy or ambiguous queries
- Entity-linking guidelines may fail with unpredictable database naming conventions
- Unclear generalizability to datasets with less predictable entity naming patterns

## Confidence
- **High**: Execution accuracy results and SQL validity (>99%)
- **Medium**: Isolated impact of meta-prompting and entity-linking guidelines
- **Low**: Generalizability to datasets with more complex schemas or less predictable entity naming

## Next Checks
1. Conduct a systematic ablation study on the meta-prompting pipeline: compare performance with/without human clustering, with different clustering methods, and with synthetic vs. real failure cases.
2. Stress-test entity-linking guidelines by constructing queries with ambiguous or heavily abbreviated entities and measure the drop in EX without variant generation.
3. Evaluate majority voting robustness by introducing controlled systematic errors into diverse plans and measuring whether voting corrects or amplifies them.