---
ver: rpa2
title: On Data Synthesis and Post-training for Visual Abstract Reasoning
arxiv_id: '2504.01324'
source_url: https://arxiv.org/abs/2504.01324
tags:
- reasoning
- data
- pattern
- grid
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper makes a pioneering attempt to address abstract visual
  reasoning (AVR) problems for large vision-language models (VLMs). The key innovation
  is an innovative data synthesis and post-training process that progressively guides
  the model to learn AVR tasks.
---

# On Data Synthesis and Post-training for Visual Abstract Reasoning

## Quick Facts
- arXiv ID: 2504.01324
- Source URL: https://arxiv.org/abs/2504.01324
- Reference count: 40
- This paper introduces a novel data synthesis and post-training approach that enables VLMs to solve abstract visual reasoning problems, achieving state-of-the-art performance on RAVEN and MARVEL benchmarks.

## Executive Summary
This paper addresses the challenge of abstract visual reasoning (AVR) for large vision-language models (VLMs) by introducing an innovative data synthesis and post-training pipeline. The authors propose a two-stage training approach that progressively guides models to learn AVR tasks through automatically collected AVR-related images and constructed visual perception and reasoning chain-of-thought data. The resulting LLaVA-AVR-7B model, built on LLaVA-NeXT-7B, achieves state-of-the-art performance on representative AVR benchmarks (RAVEN and MARVEL), significantly outperforming both open-sourced and closed-sourced powerful VLMs while preserving the model's original multimodal comprehension skills.

## Method Summary
The method involves a two-stage training pipeline on LLaVA-NeXT-7B. Stage-1 pretrains the ViT+MLP connector on VQA data only (LR=1e-5, 4 epochs, batch=2). Stage-2 performs full model SFT on a mixture of VQA and CoT data with process-level supervision and conditional multi-task learning (LR=2e-6, 1 epoch, batch=4). Key innovations include visual elicitation (coarse-to-fine questioning with shuffled order), template-based CoT generation, process-level supervision (mixing VQA in CoT training), and conditional multi-task learning (task-specific prefixes in CoT labels).

## Key Results
- LLaVA-AVR-7B achieves 82.7% reasoning accuracy on RAVEN, significantly outperforming GPT-4o (12.7%), GPT-4o-mini (33.6%), and Qwen-2-VL-72B (33.6%).
- The model achieves 35.7% reasoning accuracy on MARVEL while maintaining multimodal comprehension capabilities.
- Visual elicitation improves perception accuracy from 82% to 95% compared to immediate detailed querying.
- Process-level supervision boosts reasoning accuracy from 62.8% to 72.1%.
- Conditional multi-task learning further improves performance from 72.1% to 82.7%.

## Why This Works (Mechanism)

### Mechanism 1: Visual Elicitation via Coarse-to-Fine Perception
If models are forced to answer global structural questions before fine-grained attribute questions, they achieve significantly higher perception accuracy compared to immediate detailed querying. The "elicitation" process establishes a global context or "scaffolding" in the hidden states before the model attempts to resolve local details, preventing it from getting lost in high-frequency noise. Performance degrades if the elicitation questions follow the exact grid order sequentially, as the model may learn to exploit the sequence pattern rather than look at the image.

### Mechanism 2: Process-Level Supervision (Grounding Reasoning)
Training on a mixture of short Perception Q-A and long Chain-of-Thought (CoT) data ensures that the intermediate reasoning steps remain grounded in visual reality, preventing hallucinated attributes. By jointly training on VQA (verification) and CoT (reasoning), the loss signal from the VQA task acts as a regularizer or anchor for the CoT task, forcing the hidden representations during CoT generation to remain faithful to the visual input.

### Mechanism 3: Conditional Multi-Task Disambiguation
Prepending specific domain tags (e.g., "This is a regular puzzle") to the target labels allows a single model to separate the latent reasoning strategies required for different AVR distributions (Regular vs. Non-regular). The conditional tag acts as a "switch" in the latent space, routing the model to the specific parameter subspace optimized for that puzzle type, reducing interference between the rigid logical rules of Regular puzzles and the more semantic/abstract rules of Non-regular puzzles.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Reasoning**
  - Why needed here: The paper relies entirely on synthesizing CoT data to teach reasoning. CoT forces the model to decompose a problem into intermediate steps (attributes -> rules -> conclusion) rather than predicting the answer directly.
  - Quick check question: Can you explain why generating "The shape in row 1 is a triangle" before the final answer helps the model solve the puzzle?

- **Concept: Process Supervision vs. Outcome Supervision**
  - Why needed here: The paper introduces "Process-level supervision" in Stage 2. This refers to mixing in VQA data during CoT training to ensure intermediate steps are correct.
  - Quick check question: In the context of this paper, does "Process-level supervision" refer to RLHF, or mixing in VQA data during CoT training? (Answer: It refers to mixing VQA data to ensure intermediate steps are correct).

- **Concept: Vision-Language Alignment (The Connector)**
  - Why needed here: The paper modifies the ViT (Vision Encoder) and Connector in Stage 1. The "Connector" (MLP) translates visual patches into tokens the LLM understands.
  - Quick check question: Why might "warming up" the vision encoder (Stage 1) be necessary before unfreezing the LLM for reasoning (Stage 2)?

## Architecture Onboarding

- **Component map:** LLaVA-NeXT-7B (ViT + MLP Connector + LLM) -> Data Engine (A-SIG + CCSE Crawler) -> Template CoT Generator + VQA Annotator -> Human Filter -> Two-stage training pipeline

- **Critical path:**
  1. Data Synthesis: Generate 32k images + Questions using A-SIG tool with 7 seed patterns and CCSE crawling
  2. Stage-1 Pretraining: Optimize ViT/MLP only on VQA data to build "Perceptual" foundation
  3. Stage-2 SFT: Optimize all weights on mixed VQA+CoT with process-level supervision and conditional prefixes

- **Design tradeoffs:**
  - Automated vs. Human Labeling: Automated generation (Regular) scales but lacks complexity; Human labeling (Non-regular) is required for "irregular" patterns but is costly and harder to scale
  - Shuffling: You must shuffle the order of fine-grained questions during training; sequential order creates a "cheating" shortcut where the model learns the grid-order pattern rather than the visual content

- **Failure signatures:**
  - The "Sequential Trap": If training loss drops too fast with sequential questioning, the model is likely overfitting to the grid index order and will fail on test sets (Perception accuracy drops to 40%)
  - Catastrophic Forgetting: If you train only on AVR data, general multimodal benchmarks drop by ~3 points; you must mix in a replay buffer (e.g., 10% LLaVA SFT data)

- **First 3 experiments:**
  1. Overfit Test (Elicitation): Train two tiny models on a subset of RAVEN data: one with "Base (Shuffle)" questions, one with "Elicitation (Shuffle)". Verify that Elicitation converges to higher perception accuracy.
  2. Ablation on Conditionals: Run Stage-2 training with and without the "This is a regular puzzle" prefix. Measure the delta in reasoning accuracy on the RAVEN validation set.
  3. Generalization Check: Train the full pipeline, then evaluate on a standard VQA benchmark (like VQAv2 or GQA) to confirm that mixing 10% general data actually preserves the baseline capabilities.

## Open Questions the Paper Calls Out

### Open Question 1
Why does sequential visual elicitation result in significantly lower accuracy despite faster convergence, and does the model exploit the asking order rather than visual features? The authors identify this counter-intuitive behavior but leave the verification of their hypothesis for future work.

### Open Question 2
What is the most economically efficient method to scale up high-quality annotations for complex, irregular AVR puzzles? The paper demonstrates that human labeling is necessary for performance but implies it is costly; an optimal balance between automation and human verification remains undefined.

### Open Question 3
Can simply enlarging the attribute set and data scale fully resolve the challenges of irregular AVR puzzles like MARVEL? Current performance on MARVEL is limited (35.7% accuracy), and it is unverified whether the proposed strategy of scaling attributes is sufficient to bridge the gap to human performance.

### Open Question 4
Does the reliance on specific template-based chain-of-thought data cause overfitting to known AVR structures rather than inducing general reasoning capabilities? The evaluation is limited to representative benchmarks which share similarities with the training synthesis, lacking tests on out-of-distribution AVR tasks.

## Limitations
- Data Generation Scalability: The effectiveness of A-SIG tool is tied to specific seed patterns and variation rules that are not fully detailed.
- Model Generalization Beyond AVR: The extent to which AVR-trained model generalizes to entirely different visual reasoning tasks is untested.
- Human Annotation Bottleneck: Reliance on human filtering for CCSE-style data introduces scalability bottleneck without quantified annotation cost.

## Confidence
**High Confidence:**
- Efficacy of visual elicitation in improving perception accuracy
- Benefit of process-level supervision in grounding reasoning
- State-of-the-art performance on RAVEN and MARVEL benchmarks

**Medium Confidence:**
- Claim that AVR ability does not compromise multimodal comprehension
- Necessity of conditional multi-task learning

**Low Confidence:**
- Exact mechanism by which visual elicitation "warms up" the model's attention
- Long-term stability of the model

## Next Checks
1. Ablation on Data Sources: Train two models (A-SIG only vs CCSE only) and compare reasoning accuracy on both RAVEN and MARVEL to isolate data source contributions.

2. Generalization to Unseen AVR Benchmarks: Evaluate LLaVA-AVR-7B on a held-out AVR benchmark not seen during training to test generalization beyond specific distributions.

3. Attention Pattern Analysis: Use visualization tools to compare attention maps during visual elicitation and CoT generation with and without process-level supervision to empirically validate the "grounding" hypothesis.