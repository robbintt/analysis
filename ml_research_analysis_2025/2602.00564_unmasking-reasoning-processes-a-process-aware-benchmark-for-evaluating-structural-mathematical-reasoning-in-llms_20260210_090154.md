---
ver: rpa2
title: 'Unmasking Reasoning Processes: A Process-aware Benchmark for Evaluating Structural
  Mathematical Reasoning in LLMs'
arxiv_id: '2602.00564'
source_url: https://arxiv.org/abs/2602.00564
tags:
- reasoning
- step
- answer
- judge
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of assessing genuine mathematical
  reasoning in large language models, which is obscured by current benchmarks' reliance
  on template-based computation and final-answer accuracy. The authors propose REASONINGMATH-PLUS,
  a benchmark of 150 carefully curated problems that emphasize structural reasoning
  skills like multi-constraint coordination and constructive synthesis, each annotated
  with a minimal reasoning skeleton for fine-grained evaluation.
---

# Unmasking Reasoning Processes: A Process-aware Benchmark for Evaluating Structural Mathematical Reasoning in LLMs

## Quick Facts
- arXiv ID: 2602.00564
- Source URL: https://arxiv.org/abs/2602.00564
- Reference count: 40
- Authors: Xiang Zheng; Weiqi Zhai; Wei Wang; Boyu Yang; Wenbo Li; Ruixiang Luo; Haoxiang Sun; Yucheng Wang; Zhengze Li; Meng Wang; Yuetian Du; Guojie Lin; Yaxuan Wang; Xiaoxiao Xu; Yanhu Mo; Xuan Ren; Hu Wei; Ze Xu
- Primary result: Answer-only metrics overestimate reasoning robustness; structural process scoring reveals substantially lower reasoning quality (average 4.36/10 holistic score vs. higher answer accuracy)

## Executive Summary
This paper addresses the fundamental problem that current mathematical reasoning benchmarks conflate answer accuracy with genuine reasoning capability, obscuring the difference between robust logical processes and lucky guesses. The authors introduce REASONINGMATH-PLUS, a benchmark of 150 carefully curated problems emphasizing structural reasoning skills, each annotated with minimal reasoning skeletons for fine-grained process evaluation. They propose HCRS (Hazard-aware Chain-based Rule Score) with position-weighted penalties for early errors, and a Process Reward Model (PRM) trained on annotated reasoning traces, demonstrating that while models achieve reasonable answer accuracy, their process-based scores reveal significantly weaker reasoning robustness.

## Method Summary
The methodology involves constructing a benchmark of 150 bilingual mathematical problems with minimal reasoning skeletons (2-10 essential steps each), then evaluating model outputs through two branches: (A) HCRS using Gemini-3-Pro to label step validity against skeletons with hazard-weighted penalties for early errors, and (B) PRM score using a distilled Qwen3-8B model trained on ~33k step-level labels from the teacher judge. The holistic score combines weighted process and answer scores (w=0.7 for process). The approach enables scalable process-aware evaluation while maintaining structural rigor through skeleton-guided verification.

## Key Results
- HCRS-based holistic scores (average 4.36/10, best 5.14/10) are substantially lower than answer-only metrics, revealing overestimated reasoning robustness
- 6.63% of correct-answer traces have low HCRS scores (≤3), identified as "lucky guesses" with flawed reasoning processes
- Reference-guided HCRS achieves R=0.680 alignment with human judgments; PRM achieves competitive R=0.602 without skeleton access
- PRM exhibits score compression, retaining ~77% of top scores for lowest-ranked models versus ~48% in HCRS

## Why This Works (Mechanism)

### Mechanism 1: Hazard-Weighted Error Propagation Scoring
HCRS applies hazard-weighted penalties where earlier reasoning errors incur larger deductions (up to cap C_haz=5.0), reflecting their propagation through dependent steps. The base score aggregates binary step-validity labels, then format and hazard penalties are subtracted. Core assumption: logical errors compound multiplicatively rather than being independent; an early wrong assumption invalidates all subsequent reasoning built on it.

### Mechanism 2: Skeleton-Grounded Verification Alignment
Each problem has a human-designed "reasoning skeleton" (2-10 essential steps). The judge checks model outputs against skeleton assertions with paraphrase tolerance—semantic alignment, not exact match. This creates a task-specific structural reference without constraining surface realization. Core assumption: there exists a minimal set of logically necessary intermediate assertions that any valid solution must address, even if expressed differently.

### Mechanism 3: Distilled Verifier for Scalable Process Assessment
Qwen3-8B-instruct is fine-tuned on ~33k step-level labels from Gemini-3-Pro, learning to predict validity labels conditioned only on problem statement and gold final answer. The PRM achieves competitive alignment (R=0.602) versus teacher (R=0.639) without skeleton access. Core assumption: the teacher judge's evaluation criteria can be captured by a smaller model through sufficient labeled examples; process validity is learnable from outcome-conditioned supervision.

## Foundational Learning

- **Hazard rate / survival analysis (for error propagation modeling):**
  - Why needed here: Understanding how failure probability accumulates across reasoning steps justifies the hazard-weighted penalty design.
  - Quick check question: In a 10-step reasoning chain, would you penalize an error at step 2 more, less, or equally compared to an error at step 9? Why?

- **Process Reward Models vs. Outcome Reward Models:**
  - Why needed here: The paper's Branch B uses PRMs for step-level verification; understanding the distinction from outcome-only rewards clarifies why PRMs enable finer-grained diagnosis.
  - Quick check question: If you only have final-answer labels (correct/incorrect), can you train a PRM directly? What additional supervision would you need?

- **Chain-of-Thought evaluation paradigms (reference-guided vs. outcome-conditioned):**
  - Why needed here: The dual-branch framework hinges on this distinction—Branch A requires skeletons, Branch B doesn't.
  - Quick check question: A model produces correct final answer but skips several intermediate reasoning steps. Which evaluation paradigm would penalize this more heavily?

## Architecture Onboarding

- **Component map:**
  ```
  Benchmark (150 problems) → Skeleton annotations (2-10 steps each)
                              ↓
  Model outputs → Structured CoT (Raw Thought + Reasoning + Answer)
                              ↓
         ┌────────────────────┴────────────────────┐
         ↓                                         ↓
  Branch A: HCRS (Skeleton-guided)      Branch B: PRM (Outcome-conditioned)
  - LLM Judge (Gemini-3-Pro)            - Qwen3-8B fine-tuned on teacher labels
  - Step validity labels                - Step validity predictions
  - Hazard + format penalties           - Averaged process score
         ↓                                         ↓
  S_HCRS (process-only)                S_PRM
         └────────────────────┬────────────────────┘
                              ↓
              Holistic score (w=0.7 process + 0.3 answer)
  ```

- **Critical path:** Data annotation (skeleton creation) → Judge calibration → PRM training data generation (teacher labeling) → PRM fine-tuning → Inference scoring with HCRS penalties

- **Design tradeoffs:**
  - **HCRS strictness vs. flexibility:** Hazard penalties and format checks provide rigorous structural diagnosis but require expert annotations; PRM offers scalability but with compressed score distribution (~77% retention vs. ~48% in HCRS)
  - **Judge selection:** Gemini-3-Pro chosen for R=0.64 human alignment; switching judges requires re-calibration
  - **Hyperparameter sensitivity:** Hazard weight (ω=5.0), caps (C_haz=5.0, C_fmt=3.0), and holistic weight (w=0.7) fixed via grid search

- **Failure signatures:**
  - High answer accuracy + low HCRS → "lucky guesses" (correct outcome, flawed process)
  - PRM compression effect → lower-ranked models retain ~77% of top scores (partial credit accumulation)
  - Early-step errors (peaks at steps 3-5) → dominate downstream failure modes

- **First 3 experiments:**
  1. **Reproduce judge alignment:** Run Gemini-3-Pro on a subset of traces, compute Pearson correlation against provided human scores; target R≥0.60
  2. **Ablate hazard penalty:** Compare HCRS scores with Phaz=0 (no hazard weighting) vs. full HCRS; expect reduced correlation with human judgments per Section 5.2.4
  3. **Test PRM generalization:** Evaluate PRM on a held-out problem type (e.g., geometry if underrepresented in training) and compare S_PRM vs. teacher judge scores; check for distribution shift

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can process-level evaluation frameworks generalize to domains beyond mathematical reasoning, where minimal reasoning skeletons may be harder to define (e.g., open-ended logical argumentation, creative problem solving)?
- Basis in paper: [explicit] Section 3.2 states the benchmark focuses on problems with "unambiguous answers" and "minimal specialized notation"; Section 7 notes annotation requires "expert effort," suggesting domain expertise may limit transferability.
- Why unresolved: The methodology assumes problems have well-defined structural reasoning patterns amenable to skeleton annotation, which may not hold for less structured reasoning tasks.
- What evidence would resolve it: Application of HCRS/PRM to non-mathematical reasoning benchmarks with human annotation feasibility studies.

### Open Question 2
- Question: Do the HCRS hazard-based penalties (heavily penalizing early errors) reflect actual human judgment of reasoning quality across all problem types, or are they task-specific?
- Basis in paper: [inferred] Section 4.1 fixes hazard schedule hyperparameters (ω=5.0, T_max=25) via grid search; Appendix F shows hazard rates peak at steps 3-5, but this distribution may vary across problem structures with different optimal entry points for key deductions.
- Why unresolved: The penalty design is empirically motivated but not validated across diverse reasoning paradigms (e.g., backward-chaining vs. forward-chaining strategies).
- What evidence would resolve it: Ablation studies comparing alternative penalty schedules across problem categories with human alignment metrics.

### Open Question 3
- Question: What mechanisms underlie the "lucky guesses" phenomenon, and can models be trained to avoid correct answers derived from flawed reasoning?
- Basis in paper: [explicit] Section 5.2.1 reports 6.63% (66/996) of correct-answer traces have S_HCRS ≤3, termed "lucky guesses"; Figure 5 visualizes this bimodal distribution.
- Why unresolved: The paper quantifies the phenomenon but does not investigate whether it stems from pattern matching, answer leakage, or partial reasoning that happens to converge.
- What evidence would resolve it: Fine-grained error analysis of low-process-score correct-answer traces; intervention studies with process-aware training objectives.

### Open Question 4
- Question: Can the PRM's score compression effect (lowered discriminability at the bottom of rankings) be mitigated while preserving its generalization benefits?
- Basis in paper: [explicit] Section 5.2.3 notes PRM exhibits a "compression effect" where the lowest-ranked model retains ~77% of the top score (vs. ~48% in HCRS), attributed to averaging-based aggregation granting partial credit for locally valid steps.
- Why unresolved: The trade-off between strict structural diagnosis (HCRS) and flexible semantic verification (PRM) remains uncharacterized for model selection applications.
- What evidence would resolve it: Comparative analysis of ranking stability and downstream task performance using different aggregation schemes for PRM outputs.

## Limitations

- **Dataset Availability**: The REASONINGMATH-PLUS benchmark is not publicly released, making independent validation impossible without constructing a comparable dataset from scratch.
- **Judge Dependency**: Results critically depend on Gemini-3-Pro's judgment quality; inter-annotator agreement among human judges is not reported.
- **Limited Problem Diversity**: With only 150 problems and notable imbalances (Geometry: 12, Probability: 5), the benchmark may not adequately stress-test reasoning robustness across mathematical domains.

## Confidence

- **High Confidence**: HCRS framework mechanics and scoring implementation; PRM training methodology and correlation results; empirical finding that answer-only metrics overestimate reasoning robustness
- **Medium Confidence**: Claim that early errors propagate more severely; PRM's ability to generalize without skeletons
- **Low Confidence**: Benchmark's comprehensiveness for structural reasoning assessment; absolute claim that REASONINGMATH-PLUS is "most comprehensive"

## Next Checks

1. **Judge Reliability Test**: Implement a small-scale human evaluation (3-5 annotators) on 20-30 traces to measure inter-annotator agreement and compare against Gemini-3-Pro scores, establishing whether the automated judge's alignment reflects true consensus.

2. **Cross-Domain Generalization**: Apply the PRM to problems from a different benchmark (e.g., AIME problems or MATH dataset) without fine-tuning, measuring performance degradation to assess true generalization versus overfitting to the training distribution.

3. **Hazard Schedule Sensitivity**: Systematically vary the hazard weight (ω) and caps (C_haz, C_fmt) in HCRS, measuring impact on correlation with human judgments and distribution of scores to verify that the chosen hyperparameters aren't overfitting to the development set.