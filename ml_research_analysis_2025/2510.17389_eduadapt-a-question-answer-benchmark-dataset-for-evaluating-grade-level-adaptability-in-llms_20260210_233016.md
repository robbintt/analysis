---
ver: rpa2
title: 'EduAdapt: A Question Answer Benchmark Dataset for Evaluating Grade-Level Adaptability
  in LLMs'
arxiv_id: '2510.17389'
source_url: https://arxiv.org/abs/2510.17389
tags:
- grade
- answer
- level
- question
- qwen2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EDUADAPT, the first large-scale benchmark
  dataset designed to evaluate grade-level adaptability in large language models (LLMs)
  for K-12 education. The dataset contains nearly 48,000 QA pairs across nine science
  subjects and four grade-level groups (Grades 1-2, 3-5, 6-8, 9-12), with questions
  generated from Wikipedia articles and filtered for age-appropriate language and
  conceptual alignment using self-reflection and human verification.
---

# EduAdapt: A Question Answer Benchmark Dataset for Evaluating Grade-Level Adaptability in LLMs

## Quick Facts
- arXiv ID: 2510.17389
- Source URL: https://arxiv.org/abs/2510.17389
- Reference count: 40
- Nearly 48,000 QA pairs across nine science subjects and four grade-level groups (Grades 1-2, 3-5, 6-8, 9-12)

## Executive Summary
This paper introduces EDUADAPT, the first large-scale benchmark dataset designed to evaluate grade-level adaptability in large language models (LLMs) for K-12 education. The dataset contains nearly 48,000 QA pairs across nine science subjects and four grade-level groups (Grades 1-2, 3-5, 6-8, 9-12), with questions generated from Wikipedia articles and filtered for age-appropriate language and conceptual alignment using self-reflection and human verification. When tested on a diverse set of open-source LLMs, results show that while larger models generally outperform smaller ones, all models struggle with lower-grade content, achieving only 60-70% on open-ended questions compared to 80-85% for higher grades. This highlights the need for targeted training and prompting strategies to improve LLMs' ability to tailor responses to younger learners.

## Method Summary
The dataset construction pipeline involves classifying Wikipedia passages by grade level using Phi-4, generating QA pairs with grade-specific prompts, filtering through LLM self-reflection on quality criteria (language appropriateness, grade alignment, relevance, clarity, subject fit), and human verification on 10% of test data. Evaluation uses three LLM judges (GPT-4o, Qwen2.5-72B, LLaMA3.3-70B) scoring responses on vocabulary alignment, conceptual alignment, scientific language, correctness, clarity, and completeness, with aggregated scores providing final assessments.

## Key Results
- All models tested (including DeepSeek-V3-0324, Qwen2.5-72B-Instruct, LLaMA3.3-70B-Instruct) show significant performance degradation on lower-grade content (Grades 1-2, 3-5) with 60-70% accuracy versus 80-85% for higher grades
- Larger models generally outperform smaller ones, though smaller models (1.5B-3B parameters) sometimes perform better on higher grades than lower grades
- Traditional NLP metrics (BERTScore, BLEU, ROUGE) show poor correlation with pedagogical quality and developmental appropriateness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Self-reflection filtering improves QA pair quality for grade-specific content.
- **Mechanism:** An LLM evaluates its own generated QA pairs across five criteria (language appropriateness, grade alignment, relevance, clarity, subject fit) on a 1-10 scale. Only pairs scoring ≥8 on all criteria are retained, reducing the initial 166k pairs to 47,734 high-quality pairs.
- **Core assumption:** The same model can effectively serve both generator and evaluator roles for pedagogical content quality assessment.
- **Evidence anchors:**
  - [abstract] "questions generated from Wikipedia articles and filtered for age-appropriate language and conceptual alignment using self-reflection and human verification"
  - [Section 2.1.3] "We implemented a customized UltraFeedback-style pipeline... To ensure consistency, we retained only pairs scoring at least 8 on every criterion. From the initial 166k pairs, only 47,734 passed this filtering step."
  - [corpus] No direct corpus evidence on self-reflection for educational QA; related work focuses on other evaluation dimensions.
- **Break condition:** If self-reflection scores correlate poorly with human judgments, the filtering threshold may exclude valid pairs or retain low-quality ones.

### Mechanism 2
- **Claim:** Pre-classification of source text by grade level enables targeted QA generation aligned with cognitive development stages.
- **Mechanism:** Phi-4 classifies Wikipedia passages into grade bands before QA generation. This addresses the inherent reading-level mismatch (only ~14% of Wikipedia content suits Grades 1-5), enabling grade-appropriate question generation from suitable source material.
- **Core assumption:** Wikipedia text, once classified by complexity, can serve as adequate source material for K-12 educational QA across all grade levels.
- **Evidence anchors:**
  - [Section 2.1.1] "Using Phi-4... we categorized passages by grade level... the vast majority of passages were labeled as suitable for Grades 6-12, with only about 14% judged appropriate for Grades 1-5."
  - [Section 2.1.2] "These prompts underwent iterative refinement through pilot runs and manual inspection, where unclear wording or overly advanced phrasing was adjusted."
  - [corpus] K12Vista (arXiv:2506.01676) addresses similar grade-level challenges in multimodal K-12 contexts but does not validate text classification approaches.
- **Break condition:** If classification accuracy is low for early-grade content, the pipeline will perpetually struggle with lower-grade QA quality due to source material mismatch.

### Mechanism 3
- **Claim:** LLM-as-a-judge with multi-model aggregation provides more pedagogically meaningful evaluation than traditional NLP metrics.
- **Mechanism:** Three independent LLM judges (GPT-4o, Qwen2.5-72B, LLaMA3.3-70B) score responses on vocabulary alignment, conceptual alignment, scientific language, correctness, clarity, and completeness. The average serves as the final score, capturing developmental appropriateness that surface metrics miss.
- **Core assumption:** LLM judges can reliably assess pedagogical appropriateness and developmental alignment, not just factual correctness.
- **Evidence anchors:**
  - [Section 4] "BERTScore often exceeded 90% even when answers were semantically wrong or developmentally inappropriate... In comparison, accuracy for MCQs and LLM-as-a-judge scoring for open-ended responses provide more meaningful, pedagogically grounded assessments."
  - [Table 7] Shows consistent performance gaps across grade levels and judges, with lower grades consistently scoring lower (5.0-6.2 range) than higher grades (6.4-8.4 range).
  - [corpus] RephQA (arXiv:2509.16360) similarly critiques traditional metrics for evaluating readability in health QA contexts.
- **Break condition:** If judge models share similar biases or limitations in understanding child development, aggregation may not correct systematic misalignment.

## Foundational Learning

- **Concept: Grade-level adaptability**
  - **Why needed here:** The core problem EduAdapt addresses is that LLMs fail to adjust vocabulary, tone, and conceptual complexity to different developmental stages. Understanding this requires knowing that "appropriate" content varies not just in difficulty but in cognitive framing.
  - **Quick check question:** Can you explain why an answer that is factually correct might still be inappropriate for a Grade 2 student?

- **Concept: Self-reflection / Self-critique in LLMs**
  - **Why needed here:** The dataset construction relies on models evaluating their own outputs. Understanding the strengths (scalability, consistency) and limitations (potential blind spots, overconfidence) of this approach is critical for interpreting dataset quality.
  - **Quick check question:** What types of errors might a model fail to detect in its own outputs when evaluating "grade alignment"?

- **Concept: NGSS (Next Generation Science Standards) framework**
  - **Why needed here:** EduAdapt aligns QA pairs with NGSS to ensure curriculum relevance. The prompts in Appendix A.2-A.3 explicitly reference cognitive expectations tied to these standards. Familiarity with how standards map to grade bands is essential for understanding the dataset's pedagogical grounding.
  - **Quick check question:** How do cognitive expectations differ between "observational reasoning" (Grades 1-2) and "modeling and quantitative analysis" (Grades 9-12)?

## Architecture Onboarding

- **Component map:**
  Wikipedia Articles -> [Phi-4 Classifier] -> Grade-banded Passages -> [QA Generator + Grade-specific Prompts] -> Raw QA Pairs (~166k) -> [Self-Reflection Module] -> Filtered QA Pairs (~48k) -> [Human Verification] -> Final Dataset -> Test Set -> [Target LLM] -> Generated Responses -> [LLM-as-Judge x3] -> Evaluation Scores

- **Critical path:** The grade-level text classification (Section 2.1.1) is the bottleneck for early-grade content. If insufficient passages are classified as Grades 1-5 appropriate, downstream QA generation cannot compensate. Verify classification accuracy on a held-out sample before scaling.

- **Design tradeoffs:**
  - **Synthetic vs. authentic questions:** Wikipedia-sourced content enables scale but may lack curriculum specificity; human-authored questions would be more authentic but prohibitively expensive at 48k scale.
  - **Single-model vs. multi-model generation:** Using Phi-4 for both classification and generation ensures consistency but may propagate model-specific biases.
  - **LLM-judge vs. human evaluation:** Three LLM judges provide scalable pedagogical assessment but may miss nuances human educators would catch; human verification on only 10% of test set limits quality guarantees.

- **Failure signatures:**
  - **Hallucination in lower grades:** Models (even 24B parameter ones) generate age-appropriate language but fabricate facts (see Appendix D examples for Gemma-2B, LLaMA3-8B).
  - **Grade inversion:** Smaller models (1.5B-3B) perform better on higher grades than lower grades, suggesting they over-rely on training data patterns rather than adapting to prompt-specified grade levels.
  - **Metric disconnection:** BLEU/ROUGE/BERTScore show flat distributions uncorrelated with pedagogical quality—do not use these for development iteration.

- **First 3 experiments:**
  1. **Validate classifier accuracy:** Sample 200 classified passages per grade band; have educators independently rate appropriateness. Calculate precision/recall for early-grade classification specifically.
  2. **Stress-test self-reflection correlation:** Compare self-reflection scores against human verification scores on the 1,000 verified pairs. Identify which criteria (language, alignment, relevance, clarity, subject-fit) show weakest correlation.
  3. **Establish baseline on your target model:** Before fine-tuning, evaluate your model on EduAdapt's test set with temperature=0.3, top_p=0.9 (paper's settings). Document per-grade performance gap to quantify the adaptability problem for your specific use case.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific training strategies (data augmentation, curriculum-aligned pretraining, or targeted fine-tuning) most effectively improve LLM performance on lower-grade (1-5) content?
- Basis in paper: [explicit] Future work states: "addressing lower-grade performance through data augmentation, curriculum-aligned pretraining, and targeted fine-tuning is critical" after finding models achieve only 60-70% on lower grades vs 80-85% on higher grades.
- Why unresolved: The paper identifies the problem but does not test any remediation strategies; the training set is released but not yet used for fine-tuning experiments.
- What evidence would resolve it: A controlled comparison of models fine-tuned on EDUADAPT's training split using different strategies, evaluated on the held-out test set for Grades 1-5.

### Open Question 2
- Question: Does incorporating multimodal QA (image or diagram-based questions) improve the validity of grade-level adaptability assessments compared to text-only evaluation?
- Basis in paper: [explicit] Future work proposes: "incorporating multimodal QA (e.g., image or diagram-based) will better reflect real-world assessments."
- Why unresolved: EDUADAPT contains only text-based MCQ and open-ended questions, while real K-12 assessments frequently include diagrams and visual elements.
- What evidence would resolve it: An extended benchmark with visual QA pairs, correlating model performance on multimodal vs. text-only items against human expert judgments of grade appropriateness.

### Open Question 3
- Question: To what extent does grade-level adaptability transfer across educational systems with different curricular standards and cultural contexts?
- Basis in paper: [explicit] Limitations state: "the dataset is based on a K-12 curriculum framework, which may limit its generalizability to other educational systems" and "does not account for regional variations in curricular relevance."
- Why unresolved: EDUADAPT is aligned to US-based NGSS standards and uses English Wikipedia; performance in other national curricula or cultural settings is unknown.
- What evidence would resolve it: Cross-validation studies adapting EDUADAPT to different curricular frameworks (e.g., IB, national systems) with region-specific human evaluation of grade alignment.

### Open Question 4
- Question: Can grade-level adaptability be preserved when extending the benchmark to multilingual QA for non-English-speaking students?
- Basis in paper: [explicit] Future work states: "Supporting multilingual QA will increase accessibility for non-English-speaking students."
- Why unresolved: No multilingual evaluation was conducted; it is unclear whether models' struggles with simpler language compound when operating in non-English languages where training data may be scarcer.
- What evidence would resolve it: A multilingual version of EDUADAPT with human verification of translated QA pairs, comparing model adaptability scores across languages for equivalent content.

## Limitations

- Dataset quality relies heavily on LLM-based self-reflection filtering without comprehensive validation against human judgments
- Only 10% of test data (1,000 pairs) undergoes human verification, limiting quality guarantees
- Lower-grade content coverage is severely limited by Wikipedia's reading level (only ~14% suitable for Grades 1-5)

## Confidence

- **High Confidence:** The empirical finding that all tested models perform significantly worse on lower-grade content (60-70% accuracy) compared to higher grades (80-85%) is well-supported by the experimental results across multiple model sizes and subjects.
- **Medium Confidence:** The claim that self-reflection filtering produces high-quality, grade-appropriate QA pairs is supported by methodology description but lacks direct validation evidence linking self-reflection scores to actual pedagogical quality.
- **Medium Confidence:** The conclusion that larger models generally outperform smaller ones follows from the experimental data, though the grade-level inversion phenomenon (smaller models performing better on higher grades) warrants deeper investigation beyond the paper's explanation.

## Next Checks

1. **Classifier Validation Study:** Sample 200 passages per grade band from the classified Wikipedia corpus and have three independent K-12 educators rate each passage's appropriateness for the assigned grade level. Calculate precision, recall, and F1-score specifically for the Grades 1-5 classification, where the paper reports only 14% coverage.

2. **Self-Reflection Reliability Test:** Compare self-reflection scores against human verification scores on the 1,000 verified test pairs. Compute correlation coefficients for each of the five evaluation criteria (language appropriateness, grade alignment, relevance, clarity, subject fit) to identify which criteria the LLM judges most/least reliably.

3. **Longitudinal Grade-Level Assessment:** Using the best-performing model (DeepSeek-V3) as a baseline, conduct a controlled experiment varying only the grade-level prompt parameter while keeping all other conditions constant. Document whether performance degradation is uniform across all lower grades or follows a specific developmental pattern, and test whether explicit scaffolding prompts can mitigate the gap.