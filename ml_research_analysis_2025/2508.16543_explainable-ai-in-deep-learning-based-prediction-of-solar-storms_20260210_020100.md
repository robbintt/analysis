---
ver: rpa2
title: Explainable AI in Deep Learning-Based Prediction of Solar Storms
arxiv_id: '2508.16543'
source_url: https://arxiv.org/abs/2508.16543
tags:
- feature
- value
- shap
- test
- negative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an approach to making a deep learning-based
  solar storm prediction model interpretable by incorporating post hoc model-agnostic
  techniques. The model, built on an LSTM network with attention, predicts whether
  an active region on the Sun's surface that produces a flare within 24 hours will
  also produce a CME associated with the flare.
---

# Explainable AI in Deep Learning-Based Prediction of Solar Storms

## Quick Facts
- arXiv ID: 2508.16543
- Source URL: https://arxiv.org/abs/2508.16543
- Reference count: 4
- Primary result: First interpretable LSTM-based solar storm prediction model using SHAP and LIME

## Executive Summary
This paper presents an approach to making a deep learning-based solar storm prediction model interpretable by incorporating post hoc model-agnostic techniques. The model, built on an LSTM network with attention, predicts whether an active region on the Sun's surface that produces a flare within 24 hours will also produce a CME associated with the flare. The researchers use SHAP for global explanations to understand feature importance and LIME for local explanations to interpret individual predictions. They analyze 12 magnetic field parameters from SDO/HMI vector magnetic field data.

## Method Summary
The approach models active region magnetic field parameters as time series and uses an LSTM network to capture temporal dynamics. The model predicts whether a flaring active region will also produce a CME within 24 hours. The LSTM incorporates attention to weight relevant timesteps. Post hoc explanations use SHAP (GradientExplainer) for global feature importance and LIME (LimeTabularExplainer) for local explanations of individual predictions. The model processes 12 SHARP magnetic field parameters from SDO/HMI data.

## Key Results
- TOTPOT identified as most important feature globally by SHAP analysis
- LIME reveals feature ranges that influence positive and negative predictions
- Model achieves mean TSS of 0.562 on test set
- First work to add interpretability to LSTM-based solar storm prediction model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Time-series representation of active region magnetic field parameters enables prediction of CME-flare association.
- Mechanism: The LSTM network processes sequential observations of 12 magnetic field parameters per active region, learning temporal dynamics that precede CME occurrence. The attention mechanism weights which timesteps are most relevant for each prediction.
- Core assumption: The temporal evolution of magnetic field parameters contains discriminative signal for whether a flaring active region will also produce a CME within 24 hours.
- Evidence anchors:
  - [abstract] "The crux of our approach is to model data samples in an AR as time series and use the LSTM network to capture the temporal dynamics of the data samples."
  - [Introduction] The model builds on Liu et al. (2020), which achieved mean TSS of 0.562 using these 12 features for the same 24-hour forecast horizon.
  - [corpus] Corpus shows multiple LSTM-based solar prediction approaches (arXiv:2507.05313, arXiv:75516), supporting time-series modeling validity for flare prediction.
- Break condition: If temporal patterns in magnetic field data do not contain predictive information beyond what static snapshots provide, the LSTM architecture offers no advantage over simpler classifiers.

### Mechanism 2
- Claim: SHAP values identify which magnetic field parameters globally drive model predictions.
- Mechanism: SHAP (GradientExplainer) computes Shapley values for each feature per prediction, then aggregates mean absolute values across all 540 test samples. Features with higher mean |SHAP| contribute more to pushing predictions away from the base value (~0.48).
- Core assumption: Shapley value decomposition correctly attributes contribution when features may have complex interactions; gradient-based approximation is valid for this LSTM architecture.
- Evidence anchors:
  - [Global Explanations with SHAP] "TOTPOT feature is of the highest importance to the model's predictions, while the MEANALP feature is of the lowest importance."
  - [Figure 2 description] Bar plot ranks features by mean absolute SHAP; TOTPOT bar is "significantly longer than those of the other features."
  - [corpus] Weak direct corpus evidence for SHAP specifically in solar storm prediction; paper claims this is "the first work to add interpretability to an LSTM-based solar storm prediction model."
- Break condition: If strong feature correlations (e.g., TOTPOT and SAVNCPP have correlation = 1.0) cause SHAP to distribute importance inconsistently, rankings may be unstable across different model initializations or test sets.

### Mechanism 3
- Claim: LIME reveals which feature ranges push individual predictions toward positive (CME) or negative (no CME) classes.
- Mechanism: LIME fits a local interpretable surrogate model around each test instance, producing feature-level contributions with thresholded ranges (e.g., "ABSNJZH > 1.34 increases probability of positive class").
- Core assumption: The local decision boundary near each instance can be approximated by a linear combination of feature contributions.
- Evidence anchors:
  - [Local Explanations with LIME] "ABSNJZH with a positive bar has a range greater than 1.34; this means that ABSNJZH values greater than 1.34 increase the chances that a test sample belongs to the positive class."
  - [Figures 7-8 description] LIME plots show opposing feature contributions for positive vs. negative predicted samples.
  - [corpus] Corpus neighbor arXiv:2502.00300 uses evidential neural networks and XAI for weather prediction, indicating broader acceptance of post-hoc explanation methods in geophysical forecasting.
- Break condition: If the LSTM's decision boundary is highly nonlinear locally, LIME's linear surrogate may misrepresent the true feature contribution.

## Foundational Learning

- Concept: **Long Short-Term Memory (LSTM) Networks**
  - Why needed here: The paper models active region evolution as multivariate time series; LSTMs capture dependencies across time steps that feed-forward networks cannot.
  - Quick check question: Given a sequence of 5 observations each with 12 features, can you sketch how an LSTM processes this input step-by-step to produce a single binary output?

- Concept: **Shapley Values and Feature Attribution**
  - Why needed here: SHAP decomposes each prediction into feature contributions; understanding how "fair credit" is assigned helps interpret why TOTPOT dominates globally.
  - Quick check question: If a model prediction is 0.7 and the base value is 0.5, and feature A has SHAP value +0.15 while feature B has SHAP value +0.05, what is the combined contribution of all other features?

- Concept: **Local vs. Global Interpretability**
  - Why needed here: The paper explicitly separates SHAP (global, all test samples) from LIME (local, per-instance); knowing when to use each is essential for operational deployment.
  - Quick check question: You need to explain why the model failed on a specific active region yesterday. Which tool—SHAP or LIME—is more appropriate for this task, and why?

## Architecture Onboarding

- Component map:
  - Input Layer: Multivariate time series of 12 SHARP magnetic field parameters (TOTUSJZ, USFLUX, TOTPOT, SAVNCPP, ABSNJZH, MEANPOT, MEANSHR, SHRGT45, MEANJZH, MEANGAM, MEANALP, MEANGBZ)
  - LSTM Layers: Sequential processing of temporal dynamics (architecture details inherited from Liu et al. 2020)
  - Attention Mechanism: Weights timestep importance for final prediction
  - Output Layer: Binary classification (probability of CME association)
  - XAI Overlay: SHAP (GradientExplainer) for global importance; LIME (LimeTabularExplainer) for local explanations

- Critical path:
  1. Data ingestion: 12-feature time series from SHARP patches → normalization
  2. Temporal encoding: LSTM processes sequence → hidden states capture evolution
  3. Attention weighting: Key timesteps emphasized → aggregated representation
  4. Classification: Sigmoid output → probability threshold → binary decision
  5. Explanation: SHAP values computed via gradients; LIME samples locally around instance

- Design tradeoffs:
  - Post-hoc vs. intrinsic interpretability: The paper uses model-agnostic post-hoc methods (SHAP/LIME), which are flexible but may not fully reflect internal model logic compared to inherently interpretable architectures.
  - SHAP vs. LIME: SHAP provides theoretically grounded global attributions but is computationally expensive; LIME is faster per instance but sensitive to perturbation sampling.
  - Feature correlation: TOTPOT and SAVNCPP are perfectly correlated (r=1.0); this may inflate or redistribute importance, a known limitation of additive feature attribution methods.

- Failure signatures:
  - SHAP importance rankings change significantly across different random seeds or test set splits → indicates unstable attributions, possibly due to feature collinearity.
  - LIME explanations contradict SHAP for the same instance → suggests local approximation quality issue or high local nonlinearity.
  - Model achieves high accuracy but all SHAP values cluster near zero → indicates the model may be using features in ways gradient-based SHAP fails to capture.

- First 3 experiments:
  1. **Baseline replication:** Re-train the LSTM on the same 33,604 training / 540 test split with fixed seed (np.random.seed(42)); verify TSS ≈ 0.562 and SHAP bar plot ranks TOTPOT highest.
  2. **Ablation study:** Remove TOTPOT (or its perfectly correlated proxy SAVNCPP) and re-run; measure TSS degradation and observe which feature becomes most important.
  3. **Stability test:** Run SHAP and LIME on 5 different test set subsamples; compute variance in feature rankings and local explanation consistency to quantify robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the interpretable LSTM framework be effectively generalized to predict other space weather events such as solar energetic particles (SEPs) and geomagnetic storms?
- Basis in paper: [explicit] The authors state, "In the future we plan to extend our work to build interpretable deep learning models for other space weather events such as solar energetic particles, interplanetary shocks and geomagnetic storms."
- Why unresolved: The current study validates the approach only for CME predictions associated with flares, leaving the application to distinct physical phenomena unexplored.
- What evidence would resolve it: Application of the same LSTM and XAI methodology to SEP and geomagnetic storm datasets with comparable interpretability results.

### Open Question 2
- Question: How do the intrinsic attention weights of the LSTM compare to the post hoc explanations provided by SHAP and LIME?
- Basis in paper: [inferred] The paper relies exclusively on post hoc tools (SHAP and LIME) for interpretation, despite the model containing an attention mechanism which inherently provides a form of explainability.
- Why unresolved: The text does not analyze or compare the model's internal attention maps against the external SHAP/LIME results to verify consistency.
- What evidence would resolve it: A comparative analysis between the temporal attention weights and the feature importance rankings provided by SHAP.

### Open Question 3
- Question: Does the perfect linear correlation between TOTPOT and SAVNCPP compromise the uniqueness or stability of the feature importance rankings?
- Basis in paper: [inferred] The paper identifies TOTPOT as the most important feature but also notes via Figure 4 that TOTPOT and SAVNCPP have a Pearson correlation coefficient of 1.
- Why unresolved: When features are perfectly correlated, attribution methods may arbitrarily distribute importance, making it unclear if TOTPOT is uniquely predictive or simply a proxy for SAVNCPP.
- What evidence would resolve it: Ablation studies removing one of the correlated features to observe the impact on model accuracy and SHAP value distribution.

## Limitations

- Reliance on post-hoc interpretability methods (SHAP/LIME) rather than intrinsically interpretable architectures may provide incomplete explanations
- Perfect correlation (r=1.0) between TOTPOT and SAVNCPP features creates uncertainty about which parameter genuinely drives predictions
- Lack of ablation studies showing how performance changes when key features are removed

## Confidence

- **High Confidence**: The LSTM architecture can learn temporal patterns in magnetic field parameters (Mechanism 1) - supported by established literature on time-series modeling for solar prediction
- **Medium Confidence**: SHAP correctly identifies TOTPOT as globally most important (Mechanism 2) - ranking is reported but not validated through ablation studies or stability testing
- **Medium Confidence**: LIME reveals meaningful feature ranges for individual predictions (Mechanism 3) - local explanations are presented but approximation quality is not evaluated

## Next Checks

1. **Ablation Study**: Remove TOTPOT and SAVNCPP individually from the feature set, retrain the model, and measure changes in TSS and feature importance rankings to determine which feature genuinely drives predictions.

2. **Stability Analysis**: Run SHAP and LIME on five different random test set splits with fixed random seed; compute variance in feature rankings and local explanation consistency to quantify robustness.

3. **SHAP-LIME Alignment**: For 20 test instances, compare SHAP global importance rankings with LIME local explanations; identify instances where they contradict and analyze whether this indicates local nonlinearity or explanation quality issues.