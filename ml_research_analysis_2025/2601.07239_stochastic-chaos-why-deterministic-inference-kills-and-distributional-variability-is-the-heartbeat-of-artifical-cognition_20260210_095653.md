---
ver: rpa2
title: 'Stochastic CHAOS: Why Deterministic Inference Kills, and Distributional Variability
  Is the Heartbeat of Artifical Cognition'
arxiv_id: '2601.07239'
source_url: https://arxiv.org/abs/2601.07239
tags:
- decoding
- greedy
- deterministic
- stochastic
- risk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper argues that deterministic inference\u2014producing the\
  \ same output for a given input every time\u2014is a poor default for large language\
  \ models. Instead of eliminating randomness, the authors propose treating distributional\
  \ variability as a core signal for robust artificial cognition."
---

# Stochastic CHAOS: Why Deterministic Inference Kills, and Distributional Variability Is the Heartbeat of Artifical Cognition

## Quick Facts
- arXiv ID: 2601.07239
- Source URL: https://arxiv.org/abs/2601.07239
- Authors: Tanmay Joshi; Shourya Aggarwal; Anusa Saha; Aadi Pandey; Shreyash Dhoot; Vighnesh Rai; Raxit Goswami; Aman Chadha; Vinija Jain; Amitava Das
- Reference count: 8
- Primary result: Deterministic inference systematically underestimates both LLM capability and safety risk by collapsing the output distribution to a single brittle path.

## Executive Summary
This paper argues that deterministic inference—producing the same output for a given input every time—is a poor default for large language models. Instead of eliminating randomness, the authors propose treating distributional variability as a core signal for robust artificial cognition. Empirically, they show that deterministic inference underestimates both capability and safety: it hides uncertainty in benchmark performance, suppresses emergent reasoning paths, and masks rare but dangerous behaviors. Using a diverse suite of models, tasks, and evaluation methods, they demonstrate that stochastic, multi-sample decoding reveals richer landscapes of correct behavior and risk that single greedy runs systematically conceal.

## Method Summary
The authors conduct inference experiments comparing greedy decoding (temperature=0) against stochastic decoding (temperature=0.7, top-p=0.9) with k samples. They evaluate across four domains: GLUE-style classification (MNLI, QQP, QNLI, SST-2) with paraphrased/perturbed/adversarial variants; few-shot ICL on BESSTIE; instruction-following on InstruSum; reasoning on GSM8K, SVAMP, StrategyQA; and safety on jailbreak suites. Key metrics include robustness ratio (accuracy on variants/original), exploration gain (best-of-k accuracy minus greedy accuracy), ASR (attack success rate), illusion index, label entropy, and style/constraint satisfaction scores. No training is performed—only inference experiments comparing deterministic versus stochastic policies.

## Key Results
- Deterministic inference creates an "illusion of robustness" by hiding rare but dangerous behaviors in the model's output distribution.
- Multi-sample stochastic decoding reveals "hidden majority" cases where the correct answer is the dominant mode under sampling but not the greedy choice.
- Safety evaluation using deterministic decoding systematically underestimates tail risk, with ASR increasing linearly with sample count even when greedy output is safe.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Deterministic (greedy) decoding suppresses exploration-driven emergent abilities by collapsing the output distribution to a single high-probability path.
- **Mechanism:** LLMs implement a conditional distribution $p_\theta(y|x)$ with multiple high-probability modes. Greedy decoding ($T=0$) selects the single most likely token at each step, often trapping the model in a "brittle" local path. Stochastic sampling allows the model to traverse alternative trajectories in the reasoning space. If the "correct" solution resides in a mode that is sub-optimal at a specific token step but globally correct, only stochastic exploration recovers it.
- **Core assumption:** The model's weights $\theta$ encode valid reasoning paths or correct answers that are not the single argmax path; failure is often a search problem, not a capability problem.
- **Evidence anchors:**
  - [abstract] "Deterministic inference... suppresses emergent reasoning paths."
  - [Section 4.1] Describes "hidden majority" cases where the correct label is the dominant mode under sampling but greedy decoding locks onto an inferior local mode.
  - [corpus] Corpus evidence for this specific LLM mechanism is weak; related work discusses stochastic PDEs but not LLM decoding path collapse.
- **Break condition:** If the model lacks the capability entirely (i.e., no valid path exists in $p_\theta$), stochasticity merely adds noise without improving accuracy.

### Mechanism 2
- **Claim:** Single-trajectory evaluation misdiagnoses reasoning ability by mistaking a "collapsed failure" (wrong path) for a lack of competence (no solution exists).
- **Mechanism:** Complex reasoning tasks are modeled as a "reasoning graph" with multiple valid paths to the correct answer. Deterministic inference observes only one path. If that specific path fails due to a local error (e.g., arithmetic slip), the evaluation scores "0." Multi-sample decoding probes the graph to see if *any* path exists, transforming "brittle failures" into "robust successes" (via self-consistency) or identifying "collapsed failures" where a valid path existed but was missed.
- **Core assumption:** Valid reasoning paths are distributed across the output space, not concentrated solely in the greedy trajectory.
- **Evidence anchors:**
  - [abstract] "Multi-path reasoning degrades when forced onto a deterministic backbone."
  - [Section 5] Formalizes the "collapsed failure rate" $R_{coll}$ and provides evidence that many greedy errors have correct alternative paths in the sampled graph.
  - [corpus] No direct corpus validation for this specific "reasoning graph collapse" theory in LLMs.
- **Break condition:** If the task admits strictly one valid solution path and the model follows it deterministically, the mechanism provides no added signal.

### Mechanism 3
- **Claim:** Greedy safety evaluation creates an "illusion of robustness" by systematically under-sampling the harmful tail of the output distribution.
- **Mechanism:** Safety risk is defined by the probability of generating harmful content. Deterministic evaluation checks only the "mode" (most likely output). If the mode is safe but the "tail" (low-probability outputs) contains harmful content, deterministic evaluation reports zero risk. However, under realistic deployment (multi-turn or regeneration), the probability of triggering the tail increases with sample budget $k$, following $r_k(x) = 1 - (1-q(x))^k$.
- **Core assumption:** Adversarial or harmful behaviors often occupy low-probability regions of the model's distribution that are suppressed by alignment but not eliminated.
- **Evidence anchors:**
  - [abstract] "deterministic evaluation underestimates safety risk by hiding rare but dangerous behaviors."
  - [Section 6.2] Derives the "deterministic illusion index" showing risk scales with sample count even if greedy output is safe.
  - [corpus] Indirectly supported by corpus on "Uncertainty-aware Surrogate Modeling" (handling variability), though specific LLM safety application is novel here.
- **Break condition:** If the harmful mass $q(x)$ is strictly zero, the tail risk remains zero regardless of sampling.

## Foundational Learning

- **Concept: Conditional Distribution $p_\theta(y|x)$ vs. Fixed Function**
  - **Why needed here:** The paper argues against treating LLMs as deterministic functions $f(x)$. Understanding that an LLM defines a *probability landscape* of possible outputs is prerequisite to grasping why sampling reveals "hidden" behaviors.
  - **Quick check question:** Does setting Temperature=0 guarantee the "best" answer? (Answer: No, it only guarantees the most *probable* local path, which may be brittle).

- **Concept: Bitwise vs. Distributional Reproducibility**
  - **Why needed here:** The paper critiques the industry push for "bitwise determinism" (exact same bits every time). One must distinguish this from "distributional reproducibility" (stable statistics across runs) to understand the tradeoffs.
  - **Quick check question:** If I run a stochastic model 100 times and get 80 "Pass" and 20 "Fail," is the system unreliable? (Answer: It is "distributionally reproducible" and reveals the model's true uncertainty, whereas a deterministic run might hide the "Fail" mode entirely).

- **Concept: Exploration-Exploitation in Decoding**
  - **Why needed here:** Mechanisms 1 and 2 rely on the idea that "exploitation" (greedy) often fails in high-dimensional spaces. One needs to understand "exploration" (stochastic sampling) as a necessary diagnostic tool, not just a creativity feature.
  - **Quick check question:** Why might a model fail a math problem deterministically but solve it stochastically? (Answer: The "exploitation" path hit a local error; "exploration" found an alternative valid path).

## Architecture Onboarding

- **Component map:** Input Processor -> Model Core (weights $\theta$ defining $p_\theta(y|x)$) -> Decoder Layer (Greedy $T=0$ vs. Stochastic $T>0, k>1$) -> Aggregator (Majority Vote/Self-Consistency/Best-of-K) -> Evaluator (Distributional metrics)

- **Critical path:** The **Decoder Layer** configuration. The system's reported "competence" and "safety" are conditional on the decoding policy ($T$ and $k$). An engineer must tune these not just for latency, but for "distributional faithfulness."

- **Design tradeoffs:**
  - **Throughput vs. Insight:** Deterministic inference is fast/cheap but provides a "misleading" single data point. Stochastic multi-sample ($k=16+$) is compute-heavy but statistically valid.
  - **Stability vs. Robustness:** Enforcing bitwise determinism hides uncertainty; allowing distributional variability exposes robustness gaps but requires statistical reporting (confidence intervals).

- **Failure signatures:**
  - **The "Safe" Illusion:** System passes all safety unit tests ($T=0$) but generates harmful outputs in production when users "Regenerate" responses.
  - **The "Mediocrity" Trap:** Model performs "average" on all tasks via greedy decoding, but examination reveals high variance—capable of state-of-the-art performance if the correct sampling strategy is applied.

- **First 3 experiments:**
  1. **Reasoning Stress Test:** Run a logic benchmark (e.g., GSM8K) using Greedy ($T=0$) vs. Self-Consistency ($k=16, T=0.7$). Calculate the "Exploration Gain" ($\Delta Acc$). If large, the model has latent reasoning capability hidden by deterministic inference.
  2. **Safety Tail Risk Audit:** Run a safety benchmark. Calculate ASR (Attack Success Rate) for $k=1$ (Greedy) vs $k=16$. Compute the "Illusion Index." If high, the safety alignment is brittle and relies on the deterministic mode.
  3. **Entropy Analysis:** Measure output entropy on ambiguous prompts. Identify "hidden majority" cases where the correct answer is frequent in the distribution but not the greedy choice.

## Open Questions the Paper Calls Out

- **Question:** Can training-time objectives be designed to explicitly reward stability of success probabilities across paraphrases and decoding regimes?
  - **Basis in paper:** [explicit] Section 7.6 states, "Our results suggest investigating objectives that directly reward: stability of success probabilities across paraphrases and decoding regimes."
  - **Why unresolved:** Current alignment and fine-tuning pipelines typically optimize deterministic losses derived from single completions, failing to account for the distributional variability identified in the paper.
  - **What evidence would resolve it:** A novel loss function or training paradigm that minimizes the gap between greedy and stochastic performance (the "deterministic illusion") without compromising overall model capability.

- **Question:** How can rare-event estimation techniques be effectively adapted to quantify tail risks in LLM harmful modes?
  - **Basis in paper:** [explicit] Section 7.6 notes, "Future work could develop: rare-event estimation techniques tailored to LLM harmful modes; adaptive stress tests..."
  - **Why unresolved:** Standard safety evaluation inspects only the mode (greedy output), while the paper demonstrates that significant harmful probability mass often lies in the distribution's tail, which is computationally expensive to sample.
  - **What evidence would resolve it:** A practical algorithm or statistical method that provides reliable estimates of the harmful probability mass $q_\theta(x)$ with fewer samples than brute-force Monte Carlo methods require.

- **Question:** Do the findings regarding deterministic collapse apply to multimodal and agentic systems?
  - **Basis in paper:** [explicit] Section 7.6 proposes, "Extending the lens of deterministic vs. stochastic behaviour to these richer settings is a natural next step, and will likely further diminish the appeal of strictly deterministic defaults."
  - **Why unresolved:** The paper's empirical evidence is limited to text-only LLMs; systems involving vision, audio, or external tool use introduce additional stochastic layers (perception, environment dynamics).
  - **What evidence would resolve it:** Empirical studies on multimodal or agentic benchmarks showing similar "collapsed" reasoning or hidden safety risks when deterministic decoding is enforced versus stochastic sampling.

- **Question:** How should regulators and standards bodies translate distributional notions of risk into operational service-level objectives (SLOs)?
  - **Basis in paper:** [explicit] Section 8 asks, "how should regulators, practitioners, and standards bodies translate distributional notions of risk into operational guarantees and service-level objectives?"
  - **Why unresolved:** This is an institutional challenge highlighted in the conclusion; current standards often assume deterministic behavior, whereas the paper argues for distributional faithfulness.
  - **What evidence would resolve it:** A proposed governance framework or standard that formally defines safety and reliability contracts based on stochastic metrics like illusion indices or tail risk probabilities.

## Limitations

- The core empirical findings rely heavily on synthetic dataset variants (paraphrased, adversarial, safety jailbreaks) whose generation methods are underspecified, raising concerns about reproducibility and potential overfitting to generation artifacts.
- The theoretical framing draws an analogy between stochastic PDEs and LLM decoding without formalizing this connection, leaving the mechanism claims partly metaphorical.
- The "illusion of robustness" argument assumes risk scales linearly with sampling budget $k$, but real-world adversarial cost and defender response are not modeled.

## Confidence

- **High Confidence:** The empirical demonstration that deterministic evaluation underestimates capability (via exploration gain) and safety risk (via illusion index) is directly measurable and methodologically sound.
- **Medium Confidence:** The reasoning-graph collapse mechanism is plausible and supported by in-domain evidence, but lacks external corpus validation and assumes a specific latent structure in the model's output distribution.
- **Low Confidence:** The analogy between LLM stochastic inference and stochastic PDE chaos is evocative but not rigorously justified; the foundational learning sections overstate the novelty of distributional reasoning in the context of existing probabilistic ML literature.

## Next Checks

1. **Cross-Domain Transfer:** Replicate the exploration gain and illusion index experiments on a completely different model family (e.g., diffusion models or vision transformers) to test whether the deterministic-stochastic gap is a universal inference artifact or LLM-specific.
2. **Cost-Benefit Simulation:** Model the real-world deployment tradeoff by simulating an adversarial attacker with a finite budget of queries. Calculate the defender's required safety budget under deterministic vs. stochastic evaluation to maintain a target ASR, exposing the practical limits of the illusion-index argument.
3. **Ablation of Generation Method:** Re-run the safety and adversarial experiments using *only* human-written or vetted harmful prompts (not LLM-generated ones). Measure whether the deterministic illusion persists, testing if the effect is due to generation artifacts or the decoding policy itself.