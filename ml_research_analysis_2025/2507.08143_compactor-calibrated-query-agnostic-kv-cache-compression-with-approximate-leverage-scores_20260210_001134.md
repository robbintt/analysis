---
ver: rpa2
title: 'Compactor: Calibrated Query-Agnostic KV Cache Compression with Approximate
  Leverage Scores'
arxiv_id: '2507.08143'
source_url: https://arxiv.org/abs/2507.08143
tags:
- arxiv
- compression
- scores
- performance
- cache
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Compactor is a training-free, query-agnostic KV cache compression
  method that uses approximate leverage scores to determine token importance. It achieves
  the same performance as competing methods while retaining 20% fewer tokens in both
  synthetic and real-world context tasks.
---

# Compactor: Calibrated Query-Agnostic KV Cache Compression with Approximate Leverage Scores

## Quick Facts
- arXiv ID: 2507.08143
- Source URL: https://arxiv.org/abs/2507.08143
- Reference count: 27
- Primary result: Achieves full KV performance while reducing KV memory burden by 68% on average on LongBench

## Executive Summary
Compactor is a training-free KV cache compression method that uses approximate leverage scores and non-causal attention to identify geometrically important tokens for compression without knowing the query. It achieves the same performance as competing methods while retaining 20% fewer tokens across both synthetic and real-world context tasks. The method introduces context-calibrated compression that predicts the maximum compression a given context supports before significant performance loss, enabling automatic retention-rate selection.

## Method Summary
Compactor computes token importance using approximate leverage scores derived from a right-sketched SVD of the key matrix, combined with non-causal attention scores computed in chunks to avoid O(N²) complexity. The method blends these two orthogonal signals to identify geometrically irreplaceable tokens, then optionally calibrates the retention rate based on the context's NLL ratio using a parametric curve. The approach is query-agnostic, meaning it can compress the KV cache before the query is known, and includes a released inference engine with optimized Triton kernels for sparse, non-contiguous memory access patterns.

## Key Results
- Achieves full KV performance while reducing KV memory burden by 68% on average on LongBench
- Retains 20% fewer tokens than competing methods while maintaining the same performance on RULER and LongBench tasks
- Context-calibrated compression achieves performance within 0.1 of full KV cache performance across 27 synthetic and real-world tasks

## Why This Works (Mechanism)

### Mechanism 1: Approximate Leverage Scores Capture Geometrically Irreplaceable Tokens
- Claim: Tokens with high leverage scores encode unique information in the key subspace that cannot be reconstructed from other tokens.
- Mechanism: Compute statistical leverage scores ℓᵢ = ||Uᵢ||² from the SVD of the key matrix K. Approximate via right-sketching KΦ to avoid O(Nd²) SVD cost, producing scores in O(Nkd) where k ≪ d.
- Core assumption: Key vectors that align with high-variance principal directions are structurally important regardless of query content.
- Evidence anchors:
  - [abstract]: "uses approximate leverage scores to determine token importance"
  - [Section 3.1, Definition 1]: Formal leverage score definition with SVD formulation
  - [Section 5.5, Table 2]: Ablation shows attention-only scores drop to 61.3% at 25% retention vs 94.1% for leverage-only, confirming geometric information is critical
  - [corpus]: Related work KVzip (arXiv:2505.23416) achieves similar query-agnostic compression via different geometric clustering, suggesting the geometric importance hypothesis has independent support
- Break condition: If keys lie in a near-uniform low-rank subspace without outliers, leverage scores provide no discrimination power. Observed in dense repetitive text with no structural anchors.

### Mechanism 2: Non-Causal Attention Reveals Query-Agnostic Relevance
- Claim: Removing the causal mask exposes stable attention patterns tied to structural tokens (separators, exemplars) rather than generation order.
- Mechanism: Compute attention scores as column-wise sums over softmax(QKᵀ) without causal masking. Chunk computation into blocks of size C=256 to avoid O(N²) materialization.
- Core assumption: Tokens that would receive high attention from many future positions (even non-causal ones) are intrinsically important.
- Evidence anchors:
  - [abstract]: "query-agnostic KV cache compression"
  - [Section 3.3]: "non-causal attention maps exhibit stable, interpretable features aligned with separator tokens"
  - [Appendix B, Figure 7]: Qualitative example showing Compactor identifies paragraph boundaries that SnapKV misses, with corresponding accuracy difference
  - [corpus]: Q-Filters (arXiv:2503.02812) uses QK geometry but requires query knowledge; confirms that query-agnostic attention patterns are a distinct signal
- Break condition: In highly structured synthetic tasks (e.g., UUID retrieval), attention may not highlight the target token if it has no semantic relationship to surrounding context. See Figure 5 UUID task showing linear NLL degradation.

### Mechanism 3: Context-Calibrated Compression via NLL Ratio Prediction
- Claim: The degradation from compression can be predicted from context alone using a parametric curve, enabling automatic retention-rate selection.
- Mechanism: Fit f(r,c) = exp(rb - b)(1 - exp(-b))⁻¹ where b = α·NLL(c) + β using held-out data. At inference, invert f given quality budget τ to find r⋆.
- Core assumption: NLL ratio under compression generalizes across queries for a fixed context (i.e., compression damage is context-specific, not query-specific).
- Evidence anchors:
  - [abstract]: "introduces context-calibrated compression, which infers the maximum compression a given context supports"
  - [Section 3.5]: Mathematical formulation with two-parameter exponential curve
  - [Section 5.4, Figure 6]: "performance of the compression methods is within 0.1 of full KV cache performance" on LongBench
  - [corpus]: HCAttention (arXiv:2507.19823) notes 85%+ compression causes degradation across methods; no calibration approach found in related work, suggesting this is novel
- Break condition: If query difficulty varies dramatically for the same context (e.g., retrieval vs. summarization on same document), the single r⋆ may be insufficient. Paper does not test multi-query-per-context scenarios.

## Foundational Learning

- Concept: Statistical Leverage Scores
  - Why needed here: Core mechanism for identifying geometrically important tokens; Theorem 1 guarantees spectral preservation under leverage sampling
  - Quick check question: Given a matrix K with SVD UΣV^T, which rows would have the highest leverage scores?

- Concept: Non-Causal vs. Causal Attention
  - Why needed here: Compactor exploits attention patterns visible only when the causal mask is removed; understanding this distinction explains why query-agnostic scoring works
  - Quick check question: In a causal attention matrix, which entries are zero that would be non-zero in a non-causal matrix?

- Concept: Negative Log-Likelihood (NLL) as Quality Proxy
  - Why needed here: Context calibration uses NLL ratio to predict compression tolerance; NLL provides differentiable quality signal
  - Quick check question: If a text has NLL = 2.0 under full KV and NLL = 2.2 under compressed KV, what is the NLL ratio?

## Architecture Onboarding

- Component map: Prefill → K matrix extraction → [Leverage Score Computer] → [Non-Causal Attention Scorer] → [Score Blender] → [Context Calibrator] → [Token Selector] → Compressed KV Cache

- Critical path: The leverage score computation (specifically the SVD of K^T K) dominates at short context lengths. The paper notes SVD time is constant regardless of N (Figure 2), making it relatively cheaper at long contexts.

- Design tradeoffs:
  - Sketching dimension k: Paper uses k=48, violating Theorem 2 bounds but empirically sufficient. Lower k → faster but noisier scores. Ablation shows k=64 similar to k=48 (Table 2).
  - Blending weight λ: Paper uses λ=0.3. Ablation shows λ∈[0.1,0.4] all work (Table 2). Higher λ privileges geometric over attention signals.
  - Quality budget τ: 0.95 for Llama (5% NLL tolerance), 0.90 for Qwen. Lower τ → more compression but riskier.

- Failure signatures:
  - UUID/synthetic retrieval: Linear NLL degradation (Figure 5), minimal compression possible. Detect via high entropy in leverage scores (no clear outliers).
  - Extreme retention (<10%): Even Compactor drops to 68% on RULER (Figure 4). Fallback to full cache or accept quality loss.
  - Short contexts: Overhead from SVD computation may exceed benefit. Paper shows break-even around 16k tokens (Figure 2).

- First 3 experiments:
  1. **Retention sweep on single task**: Run Compactor on 5 RULER tasks with r∈{0.1,0.25,0.5,0.75,1.0}. Plot task score vs. retention to validate graceful degradation claim. Expected: matches Figure 4 pattern.
  2. **Calibration curve fitting**: Fit α,β on RULER subset, then predict r⋆ for held-out LongBench documents. Compare predicted vs. empirically optimal r (found via grid search). Expected: predictions within 0.1 of optimal.
  3. **Ablation on score components**: Run with (a) leverage-only, (b) attention-only, (c) blended at different λ. Expected: blended outperforms either alone, confirming both mechanisms contribute (per Table 2).

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- Context calibration generalization: The two-parameter NLL prediction model is fitted on RULER tasks but applied to LongBench without reporting cross-task calibration error.
- Synthetic task brittleness: Linear NLL degradation on UUID retrieval with no compression benefit; the paper does not analyze failure modes in depth.
- Long context scaling: While SVD cost is claimed constant, the right-sketch construction still requires O(Nd²) time, which may dominate at extreme N.

## Confidence

**High confidence**: 
- Claim: Compactor achieves same performance as competitors while retaining 20% fewer tokens on RULER and LongBench (Table 1, Figure 4).

**Medium confidence**:
- Claim: Context-calibrated compression automatically finds r⋆ within 0.1 of full-KV performance (Section 5.4, Figure 6).

**Low confidence**:
- Claim: Leverage scores are geometrically irreplaceable (Theorem 1, Section 3.1).

## Next Checks

1. **Cross-task calibration error**: Fit α,β on 80% of RULER, predict r⋆ for held-out 20%, and compare to empirically optimal r. Report mean absolute error and 95% CI.

2. **Extreme N scaling experiment**: Run Compactor on synthetic documents of length {16k, 64k, 256k, 1M} tokens. Measure wall-clock time and memory vs. baseline to confirm constant SVD overhead claim.

3. **Retention sensitivity sweep**: For each task, sweep λ∈{0.1,0.3,0.5,0.7,0.9} and k∈{24,48,64,96}. Report performance at r=0.25 to identify whether defaults are universally optimal or domain-dependent.