---
ver: rpa2
title: 'Don''t Start Over: A Cost-Effective Framework for Migrating Personalized Prompts
  Between LLMs'
arxiv_id: '2601.12034'
source_url: https://arxiv.org/abs/2601.12034
tags:
- user
- migration
- prompts
- puma
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of migrating user-specific soft
  prompts when upgrading foundation models in personalized AI systems, as these prompts
  become incompatible and require costly retraining. The authors propose PUMA, a lightweight
  framework that uses a parameter-efficient adapter trained end-to-end to bridge semantic
  gaps between models, combined with a group-based user selection strategy that clusters
  users by prompt embeddings and output variance to reduce training costs.
---

# Don't Start Over: A Cost-Effective Framework for Migrating Personalized Prompts Between LLMs

## Quick Facts
- arXiv ID: 2601.12034
- Source URL: https://arxiv.org/abs/2601.12034
- Reference count: 16
- Primary result: PUMA matches full retraining performance while reducing computational cost by up to 98% for personalized prompt migration

## Executive Summary
This paper addresses the costly challenge of migrating user-specific soft prompts when upgrading foundation models in personalized AI systems. User prompts become incompatible with new model versions, requiring expensive retraining from scratch. The authors propose PUMA, a lightweight framework that uses parameter-efficient adapters to bridge semantic gaps between models, combined with a group-based user selection strategy to reduce training costs. Experiments on three large-scale datasets demonstrate that PUMA matches or surpasses full retraining performance while achieving up to 98% computational cost reduction, with strong generalization across diverse model architectures.

## Method Summary
PUMA employs a parameter-efficient adapter trained end-to-end to bridge semantic gaps between source and target models. The framework uses a group-based user selection strategy that clusters users by prompt embeddings and output variance, enabling selective retraining that reduces computational overhead. The adapter architecture leverages low-rank adaptation techniques to maintain efficiency while preserving user-specific personalization. The end-to-end training approach allows the adapter to learn the semantic mappings necessary for effective prompt migration without requiring full model retraining.

## Key Results
- PUMA matches or exceeds full retraining performance across three large-scale datasets
- Achieves up to 98% reduction in computational cost compared to retraining from scratch
- Demonstrates strong generalization across diverse model architectures
- Maintains effectiveness in advanced scenarios including chained and aggregated migrations

## Why This Works (Mechanism)
The framework's effectiveness stems from its dual approach: the adapter learns to map semantic relationships between model architectures, while the group-based selection strategy identifies users whose prompts are most critical for retraining. By clustering users based on prompt embeddings and output variance, PUMA focuses computational resources on the most impactful migration cases. The parameter-efficient design ensures that the adapter remains lightweight while still capturing the necessary semantic transformations between models.

## Foundational Learning
1. **Soft Prompting**: Learnable continuous vectors that replace discrete text prompts, allowing optimization for specific tasks and users
   - Why needed: Enables efficient personalization without modifying model parameters
   - Quick check: Verify prompts are differentiable and can be optimized via gradient descent

2. **Parameter-Efficient Fine-Tuning (PEFT)**: Techniques like adapters that add small trainable components while freezing base model weights
   - Why needed: Reduces computational cost compared to full fine-tuning
   - Quick check: Confirm adapter parameter count is orders of magnitude smaller than base model

3. **User Clustering by Prompt Embeddings**: Grouping users based on similarity in their personalized prompt representations
   - Why needed: Enables selective retraining that reduces overall computational cost
   - Quick check: Validate that clustered users have similar performance characteristics

4. **Semantic Gap Bridging**: Learning mappings between different model representations
   - Why needed: Different model architectures have incompatible internal representations
   - Quick check: Measure adapter performance on held-out users from same cluster

5. **Output Variance Analysis**: Measuring performance stability across user prompts
   - Why needed: Identifies which users require more intensive retraining
   - Quick check: Correlate variance with retraining success rates

## Architecture Onboarding

Component map: User Prompts -> Prompt Encoder -> User Clustering -> Adapter Training -> Migration

Critical path: User Clustering → Adapter Training → Prompt Migration. The clustering determines which users receive retraining, the adapter learns semantic mappings, and the migration applies learned transformations.

Design tradeoffs: Computational efficiency vs. migration accuracy, clustering granularity vs. retraining overhead, adapter capacity vs. parameter efficiency.

Failure signatures: Poor clustering leads to suboptimal adapter training; insufficient adapter capacity causes semantic gaps; incorrect variance thresholds miss critical users.

First experiments to run:
1. Validate clustering effectiveness by testing if users in same cluster have similar performance post-migration
2. Measure adapter capacity requirements by testing different adapter sizes on validation set
3. Evaluate variance threshold sensitivity by testing different selection criteria

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Assumes prompt embeddings adequately capture user intent despite potential semantic drift between model architectures
- Evaluation limited to text-based personalization tasks; multimodal applicability unverified
- 98% cost reduction claim depends on specific model sizes and hardware configurations
- Framework's behavior under repeated migration cycles and potential catastrophic forgetting not explicitly tested

## Confidence
- PUMA's effectiveness in matching full retraining performance: High
- The group-based user selection strategy's contribution to cost reduction: Medium
- Framework generalizability across diverse model architectures: Medium

## Next Checks
1. Test PUMA's performance when applied iteratively across more than two model versions to assess compounding effects and long-term stability
2. Evaluate the framework on multimodal personalization tasks (text+image or text+audio) to verify cross-modal applicability
3. Conduct stress testing with prompt clusters that have high internal variance to determine the robustness limits of the group-based selection strategy