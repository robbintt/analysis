---
ver: rpa2
title: 'Mechanistic Interpretability of Fine-Tuned Vision Transformers on Distorted
  Images: Decoding Attention Head Behavior for Transparent and Trustworthy AI'
arxiv_id: '2503.18762'
source_url: https://arxiv.org/abs/2503.18762
tags:
- attention
- heads
- head
- arxiv
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated mechanistic interpretability of vision
  transformers (ViTs) fine-tuned on distorted spectrogram images. By introducing extraneous
  features (axis labels, titles, color bars), the research analyzed how transformer
  components processed unrelated information.
---

# Mechanistic Interpretability of Fine-Tuned Vision Transformers on Distorted Images: Decoding Attention Head Behavior for Transparent and Trustworthy AI

## Quick Facts
- **arXiv ID**: 2503.18762
- **Source URL**: https://arxiv.org/abs/2503.18762
- **Reference count**: 4
- **Primary result**: ViT attention heads exhibit layer-wise functional specialization; early layers process extraneous features while intermediate layers specialize in task-relevant chirp detection

## Executive Summary
This study investigates mechanistic interpretability of vision transformers (ViTs) fine-tuned on distorted spectrogram images by analyzing attention head behavior. The research introduces extraneous features (axis labels, titles, color bars) to examine how transformer components process both relevant and irrelevant information. Through attention map visualization and ablation analysis, the work reveals that early layers (1-3) show minimal task impact with slight MSE loss increase (μ=0.11%, σ=0.09%), while deeper heads (e.g., layer 6) cause threefold higher loss increase (μ=0.34%, σ=0.02%). Intermediate layers (6-11) exhibit monosemantic behavior, attending exclusively to chirp regions, while some early heads (1-4) are monosemantic but non-task-relevant (e.g., text, edge, or corner detectors). These findings enhance model understanding and identify potential vulnerabilities.

## Method Summary
The study fine-tunes a pre-trained ViT on 100,000 synthetic spectrogram images (224×224×3) with chirp patterns and extraneous annotations using LoRA adaptation on query/value layers. A regression head maps the CLS token to chirp parameters (start time, start frequency, end frequency). Attention weights are extracted per layer/head during forward passes, normalized to [0,1], and overlaid on spectrograms. Ablation analysis iterates through all 144 layer-head combinations, zeroing query/key/value weights and measuring percentage increase in MSE loss compared to baseline.

## Key Results
- Early layers (1-3) show minimal task impact with ablation increasing MSE loss slightly (μ=0.11%, σ=0.09%)
- Deeper heads (e.g., layer 6) cause threefold higher loss increase (μ=0.34%, σ=0.02%)
- Intermediate layers (6-11) exhibit monosemantic behavior, attending exclusively to chirp regions
- Early heads (1-4) are monosemantic but non-task-relevant (text detectors, edge/corner detectors)
- Layer 12 shows polysemantic behavior with diffuse attention across irrelevant regions

## Why This Works (Mechanism)

### Mechanism 1: Layer-wise Functional Specialization
- Early layers extract low-level features (edges, corners, text) that are non-task-specific
- Intermediate-to-deeper layers develop monosemantic heads specializing in task-relevant features
- Ablation sensitivity correlates with head importance: early heads cause minimal MSE increase, deeper heads cause significant loss

### Mechanism 2: Monosemantic vs. Polysemantic Head Differentiation
- Monosemantic heads produce focused attention maps on single feature types
- Polysemantic heads show diffuse attention across multiple unrelated features
- Fine-tuning drives specialization from pre-trained general features to task-specific representations

### Mechanism 3: Extraneous Feature Processing Creates Vulnerabilities
- ViTs develop dedicated heads for non-task elements (text, color bars, edges)
- These specialized heads represent potential attack surfaces for adversarial manipulation
- Ablation insensitivity doesn't guarantee safety from perturbations targeting these pathways

## Foundational Learning

- **Multi-head Self-Attention in Vision Transformers**
  - Why needed: The entire methodology depends on extracting and interpreting attention weights from individual heads
  - Quick check: Can you explain why a ViT divides an image into patches and how each attention head learns different projection matrices?

- **Ablation Studies and Causal Intervention**
  - Why needed: Central claims about head importance rely on zeroing out head weights and measuring MSE loss changes
  - Quick check: If ablating Head A causes 2% loss increase and Head B causes 0.1%, what can you conclude about their relative importance?

- **Monosemanticity vs. Polysemanticity**
  - Why needed: The paper classifies heads based on whether they respond to single vs. multiple features
  - Quick check: Why might polysemantic heads emerge in neural networks, and what are the implications for interpretability?

## Architecture Onboarding

- **Component map**: Input spectrogram → Patch embedding (P×P) → 12 transformer layers (attention + FFN) → CLS token → Regression head → Prediction
- **Critical path**: Input image → Patch embedding → 12 transformer layers (attention + FFN) → CLS token → Regression head → Prediction. Attention maps extracted during forward pass; ablation requires separate forward passes per head.
- **Design tradeoffs**: LoRA reduces fine-tuning parameters but may limit specialization depth; ablation tests one head at a time (cannot capture head interactions); attention map normalization may obscure magnitude differences; synthetic dataset enables controlled experiments but may not generalize
- **Failure signatures**: High variance in ablation loss suggests inconsistent specialization; significant early-layer ablation loss indicates shortcut features; uniform attention maps indicate non-specialized heads; regression predictions clustering around mean suggest task difficulty or undertraining
- **First 3 experiments**:
  1. Replicate ablation heatmap: Run ablation on all 144 heads, plot MSE increase as 12×12 heatmap; verify early layers (1-3) show cooler colors than intermediate layers (6-11)
  2. Visualize monosemantic vs. polysemantic heads: Extract attention maps for Layer 6 Head 4 and Layer 12 Head 12; overlay on spectrograms to confirm chirp-only vs. diffuse attention
  3. Test extraneous feature vulnerability: Fine-tune fresh ViT on spectrograms without axis labels/color bars; compare whether early-layer text/edge detector heads still emerge

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can early-layer attention heads (1-3) be permanently pruned to enhance computational efficiency without significant regression performance degradation?
- **Basis**: Page 8 suggests potential optimization by reducing early-layer heads
- **Why unresolved**: Authors measured immediate ablation impact but didn't validate permanent architectural changes or measure efficiency gains
- **What evidence would resolve it**: Modified model with physically removed early-layer heads demonstrating maintained accuracy and measurable inference speed improvements

### Open Question 2
- **Question**: Does functional specialization of intermediate layers persist on real-world, non-synthetic spectrograms?
- **Basis**: Study relies entirely on synthetic dataset (Page 4)
- **Why unresolved**: Synthetic data may lack real-world noise profiles and variations; heads identified as "chirp detectors" might attend to spurious artifacts
- **What evidence would resolve it**: Replication using real-world environmental/biological spectrograms to verify heads in layers 6-11 maintain exclusive focus on signal region

### Open Question 3
- **Question**: Are non-task-relevant monosemantic heads (text/edge detectors) inherited from pre-trained ViT or emergent from fine-tuning?
- **Basis**: Identified heads detect non-relevant features suggesting retention from generic pre-training
- **Why unresolved**: Analysis focuses on post-fine-tuning state without comparing against baseline pre-trained model
- **What evidence would resolve it**: Comparative analysis showing behavior of specific heads in pre-trained vs. fine-tuned models on same spectrogram images

## Limitations
- Synthetic dataset may not capture real-world distribution shifts and noise profiles
- Ablation approach tests heads in isolation, potentially missing cooperative effects between heads or layers
- Regression task (chirp localization) is relatively narrow, limiting generalizability to other vision tasks

## Confidence
- **High confidence**: Layer-wise functional specialization pattern (early = low-level, intermediate = task-relevant) - supported by systematic MSE loss differences and multiple attention map examples
- **Medium confidence**: Monosemantic vs polysemantic head classification - visual inspection is subjective, quantitative metrics could strengthen distinction
- **Medium confidence**: Extraneous feature vulnerability claims - ablation insensitivity doesn't guarantee adversarial robustness

## Next Checks
1. **Cross-task validation**: Apply same ablation and attention analysis methodology to a different vision task (e.g., image classification) to determine if layer-wise specialization is architectural or task-dependent

2. **Adversarial robustness test**: Generate adversarial examples targeting extraneous-feature-specialized heads and measure whether performance degrades despite ablation insensitivity

3. **Head interaction study**: Implement cooperative ablation study where multiple heads are zeroed simultaneously within layers to detect complementary information that only becomes apparent through group ablation