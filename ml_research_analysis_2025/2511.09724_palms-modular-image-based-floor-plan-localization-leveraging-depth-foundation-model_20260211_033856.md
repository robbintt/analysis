---
ver: rpa2
title: 'PALMS+: Modular Image-Based Floor Plan Localization Leveraging Depth Foundation
  Model'
arxiv_id: '2511.09724'
source_url: https://arxiv.org/abs/2511.09724
tags:
- palms
- localization
- depth
- dataset
- floor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PALMS+ is an image-based indoor localization system that uses a
  depth foundation model to reconstruct 3D point clouds from posed RGB images, followed
  by geometric layout matching with a floor plan. It addresses the limitations of
  short-range smartphone LiDAR and indoor layout ambiguity.
---

# PALMS+: Modular Image-Based Floor Plan Localization Leveraging Depth Foundation Model

## Quick Facts
- **arXiv ID**: 2511.09724
- **Source URL**: https://arxiv.org/abs/2511.09724
- **Reference count**: 40
- **Primary result**: Image-based indoor localization using depth foundation model for 3D reconstruction and geometric layout matching

## Executive Summary
PALMS+ is an image-based indoor localization system that uses a depth foundation model to reconstruct 3D point clouds from posed RGB images, followed by geometric layout matching with a floor plan. It addresses the limitations of short-range smartphone LiDAR and indoor layout ambiguity. The system produces pose posteriors usable for direct or sequential localization, without requiring training. Evaluated on a custom campus dataset of 80 observations across four large buildings, PALMS+ outperformed PALMS and F3Loc in stationary localization accuracy—achieving 30.4% accuracy at 1m30°—and demonstrated lower localization errors in sequential tracking on 33 real-world trajectories using particle filters. Code and data are publicly available.

## Method Summary
PALMS+ takes a rotating image sequence from a smartphone, uses Depth Pro to generate dense depth maps, and reconstructs a scale-aligned 3D point cloud. The point cloud is projected to 2D, line segments are extracted, and a kernel-based convolution method matches the observed walls against the floor plan, producing a pose posterior. The method handles orientation and scale ambiguity through discrete candidate enumeration. For sequential tracking, particle filters initialized with the heatmap posterior refine the pose estimate over time using IMU odometry.

## Key Results
- PALMS+ achieves 30.4% localization accuracy at 1m30° on a custom campus dataset, outperforming PALMS (25.0%) and F3Loc (11.2%)
- Sequential tracking with particle filters initialized by PALMS+ heatmaps yields lower RMSE than direct initialization (5.7% vs 7.3% at 1m30°)
- Kernel-based layout matching with CES constraints outperforms ray-tracing (88.7% vs 11.2% accuracy) under ideal observations

## Why This Works (Mechanism)

### Mechanism 1: Scale-Aligned 3D Reconstruction from Monocular Depth
Foundation depth models can produce long-range depth estimates suitable for floor plan matching when properly scale-aligned across views. The pipeline uses Depth Pro for per-image depth, constructs point clouds using camera intrinsics and ARKit poses, then jointly optimizes scale factors to maximize overlap between adjacent point clouds. Ground plane detection via RANSAC provides global scale correction by enforcing canonical 1.5m camera height. This works because indoor environments have visible ground planes and sufficient geometric overlap between adjacent views in a rotational scan.

### Mechanism 2: Kernel-Based Layout Matching with Visibility Constraints
Convolution-based matching with Certainly Empty Space (CES) constraints produces more accurate location posteriors than ray-tracing approaches under ideal observations. The method rasterizes observed wall segments into a rotated kernel, defines triangular exclusion zones between observer and each wall segment, and convolves the combined kernel with the floor plan bitmap. This works because observed walls correspond to floor plan walls and floor plans accurately represent structural geometry.

### Mechanism 3: Orientation and Scale Marginalization via Discrete Candidates
Orientation and scale uncertainty can be efficiently handled through discrete candidate enumeration rather than continuous optimization. The system computes wall orientation histograms for both floor plan and observation, finds rotation shifts maximizing histogram alignment, tests multiple scale factors per orientation, and takes the maximum response over scale candidates. This works because wall orientations follow a limited set of dominant directions and scale error is bounded within ±10%.

## Foundational Learning

- **SE(2) and SE(3) Transformations**:
  - Why needed: The system parameterizes 2D floor plan poses as SE(2) transformations (x, y, θ) while camera tracking operates in SE(3).
  - Quick check question: Why can SE(2) be parameterized by 3 values but SE(3) requires 6?

- **Particle Filtering for Bayesian Localization**:
  - Why needed: Sequential localization uses particle filters initialized by heatmap priors to track pose over time using IMU odometry.
  - Quick check question: Why are particle filters preferred over Kalman filters when the posterior is multi-modal?

- **Convolution as Template Matching**:
  - Why needed: The layout matching module uses 2D convolution to efficiently score all spatial locations against the observed wall kernel.
  - Quick check question: How does the convolution operation in this system differ from learned convolutional filters in CNNs?

- **Monocular Depth Ambiguity**:
  - Why needed: Foundation depth models predict metric depth but with view-dependent scale errors requiring explicit correction.
  - Quick check question: Why does monocular depth estimation suffer from scale ambiguity even with metric prediction models?

## Architecture Onboarding

- **Component map**: RGB images {I_i} + ARKit poses {T_i} + camera intrinsics → Depth Pro → per-image depth maps {D_i} → local clouds {P_i} → L-BFGS-B optimization + RANSAC ground plane → global cloud P → vertical band extraction (±0.1m) → Canny edge + Hough transform → line segments L_obs → histogram circular convolution → O candidates → RW_θo (wall bitmap) + CES_θo (visibility constraint) → combined kernel → floor plan convolution → O heatmaps {H_o,s} → marginalize over scale → final posterior

- **Critical path**: Image capture → Depth estimation → Scale alignment → Line extraction → Orientation candidates → Kernel convolution → Heatmap output. Runtime: ~24s total (20.4s for reconstruction+alignment, 3.8s for posterior).

- **Design tradeoffs**:
  - O=10 vs O=4 orientations: Accuracy vs speed; sequential mode uses fewer to reduce computation during tracking
  - Scale candidates {0.9, 1.0, 1.1}: Handles residual scale error but 3× computation; wider range increases robustness but cost
  - Overlap vs ground-plane alignment: Overlap requires multi-view; ground plane requires visible floor. Full method combines both
  - Masked vs unmasked depth: Masking transparent surfaces improves accuracy (30.4% → 38.0%) but requires manual annotation or detection

- **Failure signatures**:
  - Transparent surfaces: Depth through windows creates spurious walls → use PALMS+* with masked depths
  - Homogeneous hallways: Elongated heatmap modes along corridor axis → require sequential refinement
  - Small rooms: Multi-modal posteriors from geometric aliasing → ambiguous, may select wrong mode
  - No ground plane + limited overlap: Scale alignment degrades → performance drops to "None" condition (10.0% accuracy)
  - Building 2 pattern: Repetitive hallways cause 12.5% accuracy vs 57.9% in Building 1

- **First 3 experiments**:
  1. Stationary localization accuracy (Table 1, 2): Compare PALMS+ vs PALMS vs F3Loc on custom dataset (80 observations) and Structured3D under full/partial/single-view settings. Metric: Localization accuracy @1m30°.
  2. Sequential localization with particle filter (Table 3): Initialize particle filter with method heatmaps, track 33 trajectories across 3 buildings. Metrics: RMSE of last 10 steps, final localization error.
  3. Layout matching ablation (Section 4.4): Compare kernel-based vs ray-tracing matching using "perfect observations" sampled directly from floor plans. Isolates matching algorithm performance from observation quality.

## Open Questions the Paper Calls Out

- **Real-time Performance**: The current implementation takes over 24 seconds on an M2 chip and is not suitable for real-time deployment. Optimization avenues like GPU acceleration are suggested but not implemented.
- **Transparent Surface Handling**: Depth Pro captures geometry through transparent surfaces, requiring manual masking for best results. No automated solution is provided.
- **Homogeneous Environments**: The system struggles in repetitive structures like long hallways where heatmaps exhibit elongated modes rather than sharp peaks.

## Limitations
- Limited dataset scope with 80 observations across four buildings in a single campus, showing architectural sensitivity
- Heavy dependency on Depth Pro quality with no comparative analysis against alternative depth models
- Manual masking required for transparent surfaces reveals algorithmic fragility

## Confidence
- **Superior Stationary Localization**: Low — Limited dataset, no architectural diversity, unexplained performance drop in Building 2
- **Effective Sequential Tracking**: Medium — Plausible but not benchmarked against sequential-specific methods; modest accuracy gains
- **Kernel-Based Layout Matching Superiority**: Medium — Strong ablation on ideal observations (88.7% vs 11.2%), but no real-world validation under noisy observations
- **Scale Alignment Robustness**: Low — Performance drops to 10.0% with no alignment; failure conditions not fully explored

## Next Checks
1. **Architectural Diversity Test**: Evaluate PALMS+ on a held-out building or synthetic dataset with varied layouts to assess robustness beyond the campus dataset
2. **Depth Model Ablation**: Replace Depth Pro with MiDaS or DPT and measure the impact on scale alignment and final localization accuracy
3. **Transparent Surface Automation**: Implement and test an automatic transparent surface detection method to replace manual masking, measuring the trade-off between automation and accuracy