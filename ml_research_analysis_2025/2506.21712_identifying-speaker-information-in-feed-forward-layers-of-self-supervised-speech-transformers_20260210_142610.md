---
ver: rpa2
title: Identifying Speaker Information in Feed-Forward Layers of Self-Supervised Speech
  Transformers
arxiv_id: '2506.21712'
source_url: https://arxiv.org/abs/2506.21712
tags:
- neurons
- pruning
- cluster
- speaker
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of understanding how self-supervised
  speech Transformers encode speaker information. The authors propose identifying
  neurons in feed-forward layers that are correlated with speaker information by analyzing
  neurons associated with k-means clusters of self-supervised features and i-vectors.
---

# Identifying Speaker Information in Feed-Forward Layers of Self-Supervised Speech Transformers

## Quick Facts
- arXiv ID: 2506.21712
- Source URL: https://arxiv.org/abs/2506.21712
- Reference count: 34
- Primary result: Proposed label-free neuron identification method allows ~70% pruning of FFN parameters while preserving speaker identification performance, rivaling supervised approaches

## Executive Summary
This paper addresses the problem of understanding how self-supervised speech Transformers encode speaker information by identifying neurons in feed-forward layers that correlate with speaker attributes. The authors propose an unsupervised approach that uses k-means clustering on self-supervised features and i-vectors to identify speaker-relevant neurons without requiring labels. By protecting these identified neurons during pruning, they can significantly reduce model size (up to 70% of FFN parameters) while maintaining speaker identification accuracy. The method works especially well for one-shot pruning and provides insights into how speaker information is distributed across different feed-forward layers.

## Method Summary
The authors identify speaker-relevant neurons in feed-forward layers by analyzing activation patterns conditioned on k-means clusters of self-supervised features and i-vectors. They extract SSL features from layer 9 of MelHuBERT and wav2vec 2.0 models, then cluster them with k=3 to obtain broad phonetic classes (vowels, voiced consonants, unvoiced consonants). Separately, they extract i-vectors per utterance and cluster with k=2 to approximate gender classes. For each cluster, they identify neurons where activation probability exceeds 1% threshold. During pruning, these identified neurons are protected from removal regardless of their ℓ1-norm values. The approach is evaluated through one-shot and iterative pruning experiments on speaker identification and other speech tasks from the SUPERB benchmark.

## Key Results
- k=3 SSL clusters correspond to broad phonetic classes (vowels, voiced consonants, unvoiced consonants)
- k=2 i-vector clusters align with gender classes with 97.23% male and 89.82% female purity
- One-shot pruning protects identified neurons and removes ~70% of FFN parameters while preserving SID performance
- Method rivals supervised approaches without requiring phone or gender labels
- Removing SSL cluster neurons causes 3.82% SID degradation, confirming their importance for phonetic information

## Why This Works (Mechanism)

### Mechanism 1: SSL Cluster Neurons Capture Phonetic Structure
- Run k-means (k=3) on layer 9 SSL features → clusters separate into vowels, voiced consonants, and unvoiced consonants
- Identify neurons where p(activation | cluster) > 1% → these "SSL cluster neurons" encode phonetic structure that supports speaker representation
- Phonetic information aids speaker embedding learning as prior work suggests
- Evidence: Cluster analysis shows clear phonetic separation; related work confirms SSL models encode phonetic information across languages

### Mechanism 2: i-vector Clusters Capture Speaker Characteristics
- Extract i-vectors per utterance → cluster with k=2 → clusters align with male/female split
- Condition neuron identification on both i-vector cluster AND SSL cluster → intersect across SSL clusters
- i-vectors capture sufficient speaker variation without supervision; gender is a useful speaker attribute
- Evidence: Cluster purity shows 97.23% male and 89.82% female alignment; i-vectors are label-free

### Mechanism 3: Protected Pruning Preserves Speaker Information
- Identify P_ssl and P_ive neurons → during pruning, exclude these from removal regardless of ℓ1-norm
- One-shot pruning removes ~70% of FFN parameters while speaker identification accuracy is preserved
- Protected neurons are functionally important for speaker tasks, not just correlated
- Evidence: One-shot pruning results show SID: 54.08% → 62.96% for MelHuBERT

## Foundational Learning

- **Feed-Forward Layers as Key-Value Memory**: FFNs store property-specific patterns; understanding this explains why speaker info localizes here. Quick check: Can you explain why FFN layers might store speaker properties differently than attention layers?

- **i-vectors vs. x-vectors**: i-vectors are unsupervised (no speaker labels needed); x-vectors require speaker labels which breaks the label-free constraint. Quick check: Why would x-vectors be inappropriate for this unsupervised neuron identification pipeline?

- **One-Shot vs. Iterative Pruning**: Results differ significantly; one-shot benefits more from neuron protection. Quick check: In one-shot pruning, what happens to the model after pruning, and how does this differ from iterative pruning?

## Architecture Onboarding

- **Component map**: SSL features (layer 9) + i-vectors → k-means clustering (k_ssl=3, k_ive=2) → activation patterns → neuron identification (ρ=1%) → protected pruning (ℓ1-norm)

- **Critical path**: 1) Extract SSL features and i-vectors from training data; 2) Run k-means clustering on both; 3) Compute activation patterns (top 1% per frame); 4) Identify P_ssl and P_ive neurons via probability thresholding; 5) Apply one-shot pruning with protected neuron set

- **Design tradeoffs**: k_ssl=3 vs. larger (fewer clusters = more neurons but less granularity); Intersection vs. union (intersection yields fewer, more precise neurons; union is noisier but catches more); Layer 9 selection (chosen for phonetic prominence)

- **Failure signatures**: Too many clusters (k_ssl=39 or 100) yields near-zero neurons identified in middle layers; Removing SSL cluster neurons causes 3.82% SID degradation; Alternative C formulation fails to identify enough neurons in middle layers

- **First 3 experiments**: 1) Reproduce clustering analysis: Run k-means (k=3) on layer 9 features, verify vowel/consonant separation; 2) Ablate protection: Compare regular pruning vs. protected pruning on SID task with MelHuBERT; 3) Test alternative formulations: Compare intersection vs. union for i-vector neuron identification, measure neuron counts per layer

## Open Questions the Paper Calls Out

- **Can the neuron identification approach be effectively applied to other prominent self-supervised speech models like HuBERT or WavLM?** The study was limited to MelHuBERT and wav2vec 2.0 because pre-training targets for HuBERT/WavLM "are not publicly released, making continued pre-training and pruning difficult to evaluate." Resolving this would require replicating experiments on HuBERT or WavLM with available pre-training targets.

- **Why does the method of conditioning neurons on SSL clusters and then taking the intersection outperform direct conditioning on i-vector clusters?** The empirical results show the proposed intersection method is superior, but the authors lack a theoretical justification for why this specific set operation better captures speaker information than the proposed alternatives.

- **How can the neuron identification framework be modified to handle a large number of classes without the severe reduction in identified neurons?** The current approach forces the use of broad clusters because increasing the number of property classes (e.g., specific phones or many speakers) causes the number of discovered neurons to decrease significantly.

## Limitations
- The correspondence between SSL clusters and phonetic classes is empirically shown without clear theoretical grounding
- The approach relies on specific k values (k=3 for SSL, k=2 for i-vectors) that may not generalize to other languages or speaker attributes
- The pruning protection mechanism assumes identified neurons are causally important rather than merely correlated
- The method focuses only on feed-forward layers and does not examine attention layers or other architectural components

## Confidence

- **High confidence**: The clustering analysis showing k=3 SSL clusters correspond to phonetic classes is empirically demonstrated and reproducible. The i-vector clustering producing gender-aligned clusters is shown with high purity (97.23% male, 89.82% female). The one-shot pruning results showing ~70% FFN parameter reduction with preserved SID performance are well-documented.

- **Medium confidence**: The claim that SSL cluster neurons are essential for phonetic information encoding is supported by ablation showing 3.82% SID degradation when these neurons are removed, but this doesn't prove causation. The assumption that i-vector clusters capture sufficient speaker variation for neuron identification works in practice but lacks theoretical justification.

- **Low confidence**: The generalization of this approach to other SSL models beyond MelHuBERT and wav2vec 2.0 is untested. The effectiveness for speaker tasks beyond SID is demonstrated but not deeply analyzed. The optimal k values and threshold parameters for different scenarios are not systematically explored.

## Next Checks

1. **Cluster validation**: Reproduce the k-means clustering on layer 9 features with k=3 and k=2 i-vectors, then verify the phonetic class correspondence (vowels, voiced consonants, unvoiced consonants) and gender alignment through manual inspection or external validation.

2. **Alternative formulations**: Test both intersection (Eq. 5) and union (Eq. 6) approaches for i-vector neuron identification, measuring not just neuron counts but also downstream SID performance to determine which formulation provides better trade-offs between sparsity and accuracy.

3. **Ablation of protection**: Run standard ℓ1-norm pruning without protecting any neurons on the same models, comparing SID performance degradation to the protected approach to quantify the exact benefit of neuron identification.