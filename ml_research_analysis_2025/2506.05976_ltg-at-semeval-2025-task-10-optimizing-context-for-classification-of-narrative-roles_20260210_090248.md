---
ver: rpa2
title: 'LTG at SemEval-2025 Task 10: Optimizing Context for Classification of Narrative
  Roles'
arxiv_id: '2506.05976'
source_url: https://arxiv.org/abs/2506.05976
tags:
- text
- entity
- language
- context
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a context selection approach for entity framing
  classification in news articles. The authors use a multilingual XLM-RoBERTa model
  to classify 22 fine-grained narrative roles assigned to entities across Bulgarian,
  English, Hindi, Portuguese, and Russian.
---

# LTG at SemEval-2025 Task 10: Optimizing Context for Classification of Narrative Roles

## Quick Facts
- arXiv ID: 2506.05976
- Source URL: https://arxiv.org/abs/2506.05976
- Reference count: 16
- Multilingual XLM-RoBERTa with context optimization achieves competitive results for entity framing classification across 5 languages

## Executive Summary
This work presents a context selection approach for entity framing classification in news articles. The authors use a multilingual XLM-RoBERTa model to classify 22 fine-grained narrative roles assigned to entities across Bulgarian, English, Hindi, Portuguese, and Russian. Since the model's context window is limited, they experiment with rule-based text segment extraction—such as extracting single sentences, paragraphs, or segments between entity mentions—and evaluate their impact. They find that prepending a prompt like "Regarding <entity> :" to extracted text segments significantly improves performance. Fine-tuning on all languages together improves results, especially for low-resource languages. While multilingual fine-tuning with XLM-R performs well overall, larger language models with Supervised Fine-Tuning sometimes outperform it, especially for English.

## Method Summary
The approach uses XLM-RoBERTa-large with a 512-token context window to classify 22 narrative roles across 5 languages. Context segments are extracted using rule-based heuristics: single sentences, paragraphs, or entity-to-entity (ent2ent) extraction. The ent2ent method extracts text from an entity's sentence to the next entity mention. All extracted segments are prefixed with "Regarding <entity> :\n" to improve entity disambiguation. Models are fine-tuned for 10 epochs on combined multilingual data. The paper also compares against LLM baselines (Llama-3.1-8B and Mistral-7B) with Supervised Fine-Tuning using LoRA adapters.

## Key Results
- Entity-to-entity (ent2ent) extraction outperforms single sentence, paragraph, and fulltext approaches
- Prepending "Regarding <entity> :" significantly improves performance (Micro F1 increases from 30.11 to 47.75 for ent2ent)
- Multilingual fine-tuning substantially helps low-resource languages (BG, RU) while maintaining performance on higher-resource languages
- XLM-RoBERTa with context optimization is competitive with larger LLMs fine-tuned on full documents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entity-prefixed context segments enable disambiguation when multiple entities share the same text window.
- Mechanism: Prepending "Regarding <entity>:" creates an explicit attentional anchor, guiding the classifier to resolve framing cues relative to the target entity rather than conflation across co-occurring entities. This functions as a lightweight prompt that conditions token representations before they encounter the context.
- Core assumption: The model's self-attention can exploit the prefix to upweight entity-relevant signals within the segment.
- Evidence anchors:
  - [abstract] "a simple entity-oriented heuristics for context selection can enable text classification using models with limited context window"
  - [section] Table 2: ent2et drops from 47.75 to 30.11 Micro F1 when prefix is removed (ent2ent_noprefix)
  - [corpus] Weak corpus support—no direct neighbor papers test prefixing for MLMs; most focus on LLM prompting strategies.
- Break condition: If entity mentions are highly ambiguous or coreference is required beyond the extracted segment, the prefix cannot recover missing context.

### Mechanism 2
- Claim: Entity-to-entity segment extraction captures sufficient framing-relevant context while excluding distractor text.
- Mechanism: By extracting from the target entity's sentence to the next entity mention, the approach retains narrative continuity (potentially spanning multiple sentences) while truncating distant irrelevant content. This aligns with findings that distractor text degrades performance in long-context scenarios.
- Core assumption: Framing cues are locally concentrated near entity mentions rather than distributed across distant discourse.
- Evidence anchors:
  - [section 1.1] "irrelevant or distractive text as part of an input, reduces model performance"
  - [section] Table 2: ent2ent (47.75) outperforms single sentence (46.06), paragraph (40.79), and fulltext (38.96)
  - [corpus] Neighbor papers (GateNLP, Fane) use LLM-based context selection but report similar sensitivity to context quality.
- Break condition: If framing depends on document-level narrative structure (e.g., contrast between introduction and conclusion), local extraction will miss critical signals.

### Mechanism 3
- Claim: Multilingual joint fine-tuning provides cross-lingual transfer that compensates for low-resource language sparsity.
- Mechanism: XLM-R's shared multilingual representations allow gradients from higher-resource languages (HI, PT) to regularize and enrich feature spaces for lower-resource languages (BG, RU), improving generalization beyond what monolingual data permits.
- Core assumption: Narrative framing patterns share cross-lingual regularities that transfer through shared subword embeddings.
- Evidence anchors:
  - [section] Table 3: RU improves from 0.00 (in-lang) to 52.33 (all languages); BG from 0.00 to 26.67
  - [section] Figure 3: All languages show substantial gains from multilingual training
  - [corpus] No direct corpus papers test this exact transfer scenario for entity framing; transfer claims remain paper-specific.
- Break condition: If framing conventions diverge significantly across cultural/linguistic contexts, joint training may introduce noise rather than signal.

## Foundational Learning

- Concept: Subword tokenization and context windows in transformer models
  - Why needed here: Understanding why XLM-R's 512-token limit necessitates segment extraction strategies.
  - Quick check question: Given a 2000-token document with 5 entities, how would you explain why the model cannot process it directly?

- Concept: Cross-lingual transfer in multilingual models
  - Why needed here: Interpreting why joint training helps low-resource languages requires understanding shared representation spaces.
  - Quick check question: Why might training on Hindi improve Russian classification in XLM-R but not in a monolingual Russian BERT?

- Concept: Multi-label vs hierarchical classification
  - Why needed here: The 22 fine-grained roles are multi-label; the paper also tests a two-step (main→fine) approach.
  - Quick check question: What is the difference between predicting 22 labels independently vs. conditioning fine-grained predictions on a main role?

## Architecture Onboarding

- Component map: Raw document + entity annotations → Segment extractor (rule-based) → Prefix prepender → XLM-RoBERTa encoding → Classification head → Role predictions
- Critical path: Document → segment extraction (ent2ent) → prefix addition ("Regarding <entity>:") → XLM-R encoding → classification head → role predictions
- Design tradeoffs:
  - ent2ent vs. single sentence: ent2ent captures more context but may include distractor sentences; sentence is more precise but may miss cross-sentence framing.
  - Multilingual vs. monolingual training: Joint training improves low-resource languages but risks negative transfer if framing patterns diverge.
  - XLM-R vs. LLM+SFT: XLM-R is compute-efficient and competitive; LLMs may excel on English but underperform on other languages with limited SFT data.
- Failure signatures:
  - Dramatic F1 drop (>15 points) when prefix omitted → entity disambiguation failure
  - Zero Micro F1 for low-resource languages in monolingual training → insufficient training signal
  - SFT models underperforming XLM-R despite larger scale → potential prompt complexity or multi-entity confusion per document
- First 3 experiments:
  1. Ablate the entity prefix on a held-out dev split to quantify its contribution per language.
  2. Compare ent2ent against fixed-window extraction (e.g., ±2 sentences around entity) to test locality assumptions.
  3. Train language-specific adapters on top of frozen XLM-R to isolate whether gains come from shared representations vs. joint optimization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does applying rule-based context optimization to larger generative language models (LLMs) improve their Supervised Fine-Tuning (SFT) performance for entity framing?
- Basis in paper: [explicit] The authors note in Section 4.3 that "applying context optimization might prove beneficial here as well. Testing these options with SFT was beyond our resource allocations."
- Why unresolved: The authors only applied context optimization heuristics (like "ent2ent") to the XLM-RoBERTa model, while the LLM (Llama, Mistral) experiments used the full document context.
- What evidence would resolve it: A comparison of Llama/Mistral SFT performance when trained on "ent2ent" extracted segments versus full text.

### Open Question 2
- Question: Does single-entity prompting during fine-tuning yield higher accuracy than multi-entity prompting for this specific classification task?
- Basis in paper: [explicit] Section 4.3 states: "To simplify the task, one could request classifications only one entity per prompt."
- Why unresolved: The authors used a single prompt per text document requiring the model to classify all entities simultaneously, which may have increased task difficulty and contributed to lower LLM performance compared to XLM-R.
- What evidence would resolve it: Ablation studies comparing the Exact Match Ratio (EMR) and Micro F1 of models fine-tuned on single-entity prompts against those trained on multi-entity prompts.

### Open Question 3
- Question: To what extent does dataset complexity or annotation consistency, rather than training data volume, explain the performance discrepancies between languages like Portuguese and English?
- Basis in paper: [inferred] Section 4.2 notes the "surprise" of poor performance on English data and the strong results for Portuguese despite lower training volume, concluding that "further inspection of the provided dataset's complexity across languages would be required before drawing any conclusion."
- Why unresolved: The analysis could not determine if the variance was due to the languages' presence in pretraining data or specific qualities of the SemEval dataset annotations.
- What evidence would resolve it: A linguistic analysis of annotation density and ambiguity in the English and Portuguese splits, or experiments controlling for dataset complexity.

### Open Question 4
- Question: Can the "entity-to-entity" (ent2et) heuristic, which captures text between entity mentions, generalize effectively to articles with sparse entity density?
- Basis in paper: [inferred] The paper identifies challenge (b) in Section 1 ("A sentence mentioning an entity may also mention other entities"), but the success of the ent2et method relies on the assumption that relevant framing context lies strictly between mentions.
- Why unresolved: While ent2et performed best on average, the paper does not analyze failure cases where this heuristic might exclude relevant context in documents with few entity mentions.
- What evidence would resolve it: Error analysis on documents with low entity density comparing ent2et performance against the full-text baseline.

## Limitations

- The paper does not analyze attention distributions to verify whether the entity prefix actually modulates attention toward entity-relevant tokens
- No linguistic analysis of framing conventions across the five target languages to support cross-lingual transfer assumptions
- Limited comparison to alternative context selection methods beyond single sentence, paragraph, and ent2et heuristics
- The effectiveness of ent2et extraction in documents with sparse entity density remains untested

## Confidence

- High confidence: The empirical finding that multilingual joint training substantially improves low-resource language performance (BG, RU)
- Medium confidence: The claim that simple rule-based context selection can effectively replace complex LLM-based approaches
- Low confidence: The mechanism by which the entity prefix improves disambiguation

## Next Checks

1. **Attention analysis ablation**: Extract and visualize attention weights from XLM-R when processing segments with and without the entity prefix to verify whether the prefix actually causes increased attention to entity-relevant tokens versus other mechanisms.

2. **Context window sensitivity test**: Systematically vary the ent2et extraction parameters (e.g., fixed windows of ±1, ±2, ±3 sentences) and measure performance degradation to determine whether framing information truly requires the variable-length extraction strategy or could be captured with simpler fixed windows.

3. **Cross-lingual framing analysis**: Conduct a qualitative linguistic analysis comparing framing patterns across the five languages to determine whether the assumed cross-lingual regularities actually exist, or whether the performance gains might be attributed to other factors like shared vocabulary or script similarities.