---
ver: rpa2
title: 'ASNN: Learning to Suggest Neural Architectures from Performance Distributions'
arxiv_id: '2507.20164'
source_url: https://arxiv.org/abs/2507.20164
tags:
- layer
- architecture
- asnn
- neural
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes the Architecture Suggesting Neural Network
  (ASNN) to automate neural network design by learning the mapping between network
  architecture and accuracy. Using TensorFlow models, the study collected performance
  data from 2-layer (25 configurations, 250 trials) and 3-layer (64 configurations,
  640 trials) architectures.
---

# ASNN: Learning to Suggest Neural Architectures from Performance Distributions

## Quick Facts
- arXiv ID: 2507.20164
- Source URL: https://arxiv.org/abs/2507.20164
- Reference count: 11
- Primary result: ASNN suggests architectures that outperform best in original data (2-layer: 0.9836 vs 0.9831; 3-layer: 0.9831 vs 0.9817)

## Executive Summary
This study proposes the Architecture Suggesting Neural Network (ASNN) to automate neural network design by learning the mapping between network architecture and accuracy. Using TensorFlow models, the study collected performance data from 2-layer (25 configurations, 250 trials) and 3-layer (64 configurations, 640 trials) architectures. The ASNN was trained on accuracy values as inputs and architecture parameters as outputs. In iterative predictions, ASNN suggested novel architectures that consistently outperformed the best results in the original data. For the 2-layer case, accuracies reached up to 0.9836, surpassing the prior maximum of 0.9831. In the 3-layer case, the best suggested architecture achieved an accuracy of 0.9831, exceeding the baseline by 0.0014. These results demonstrate ASNN's capacity to effectively predict high-performing architectures and suggest it as an efficient alternative to random search in neural architecture optimization.

## Method Summary
ASNN is trained on a dataset mapping architecture configurations to their accuracy values. The model architecture takes accuracy as input and outputs architecture parameters (layer sizes). After training, ASNN can suggest new architectures by iteratively predicting and refining based on performance distributions. The approach is tested on small TensorFlow models with 2 and 3 fully-connected layers, using accuracy as the sole performance metric. Novel architectures are generated by querying the trained ASNN for configurations likely to yield high accuracy, and these suggestions are validated by actual training and evaluation.

## Key Results
- For 2-layer networks, ASNN suggested architectures achieving accuracy up to 0.9836, exceeding the original maximum of 0.9831
- For 3-layer networks, the best suggested architecture reached 0.9831 accuracy, a 0.0014 improvement over baseline
- ASNN consistently suggested architectures that outperformed the best results found in the original training data

## Why This Works (Mechanism)
ASNN inverts the typical neural architecture search problem by training a neural network to map from performance (accuracy) to architecture parameters. This allows direct prediction of high-performing architectures rather than exhaustive search or random sampling. By learning the underlying performance distribution, ASNN can generalize and suggest novel configurations that were not present in the original dataset.

## Foundational Learning
- **Inverse mapping concept**: Why needed - to directly predict architectures from desired performance rather than searching through space. Quick check - verify ASNN can reconstruct known architectures from their accuracy values.
- **Performance distribution learning**: Why needed - to capture relationships between architecture parameters and outcomes. Quick check - plot predicted vs actual accuracy for held-out architectures.
- **Iterative refinement**: Why needed - to improve suggestions through successive predictions. Quick check - measure improvement in accuracy across prediction iterations.

## Architecture Onboarding

**Component Map**: Data Collection -> ASNN Training -> Architecture Suggestion -> Validation

**Critical Path**: Performance data collection is the foundation; without sufficient coverage of the architecture space, ASNN cannot learn meaningful mappings.

**Design Tradeoffs**: Simple feedforward networks for ASNN vs. more complex architectures; limited architecture search space vs. scalability to larger problems; accuracy-only optimization vs. multi-objective approaches.

**Failure Signatures**: ASNN suggesting architectures outside training distribution; minimal improvement over baseline; predictions that do not generalize when architectures are actually trained.

**First Experiments**:
1. Train ASNN on 2-layer network data and validate it can reconstruct architectures from their known accuracies
2. Use ASNN to suggest 5 new architectures and compare their actual performance to the best in the original dataset
3. Test ASNN's predictions on architectures with parameters outside the training range to assess extrapolation capability

## Open Questions the Paper Calls Out
None identified in the paper.

## Limitations
- Limited to small, fully-connected networks (2-3 layers) with constrained search spaces
- Only tested on one dataset/task (MNIST-like classification)
- No comparison to established NAS methods like DARTS or reinforcement learning approaches
- No analysis of computational efficiency or wall-clock time versus other search methods

## Confidence

**High**: ASNN can suggest architectures that outperform the best in a given dataset when the dataset is small and the architecture space is limited, as demonstrated by the consistent improvement in accuracy.

**Medium**: ASNN could serve as an efficient alternative to random search, given the lack of comparative benchmarks or runtime analyses.

**Low**: The method's scalability, robustness to larger and more diverse datasets, and applicability to real-world, large-scale neural network design tasks.

## Next Checks

1. Evaluate ASNN on larger and deeper neural network architectures (e.g., 5-10 layers) and on more complex tasks such as image segmentation or language modeling to assess scalability.

2. Compare ASNN's performance and computational efficiency against established NAS methods (e.g., DARTS, random search, or evolutionary algorithms) on the same tasks and datasets.

3. Test ASNN on multiple, diverse datasets (e.g., CIFAR-100, ImageNet subsets, or NLP benchmarks) to determine the robustness and generalizability of the architecture suggestions across different problem domains.