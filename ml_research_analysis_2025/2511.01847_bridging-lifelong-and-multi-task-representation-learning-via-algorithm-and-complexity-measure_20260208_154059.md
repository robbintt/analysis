---
ver: rpa2
title: Bridging Lifelong and Multi-Task Representation Learning via Algorithm and
  Complexity Measure
arxiv_id: '2511.01847'
source_url: https://arxiv.org/abs/2511.01847
tags:
- learning
- representation
- algorithm
- tasks
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies lifelong representation learning where a learner\
  \ faces a sequence of tasks with shared structure and aims to identify and leverage\
  \ it to accelerate learning. The authors propose a simple algorithm that uses multi-task\
  \ empirical risk minimization (ERM) as a subroutine and establish a sample complexity\
  \ bound based on a new notion they introduce\u2014the task-eluder dimension."
---

# Bridging Lifelong and Multi-Task Representation Learning via Algorithm and Complexity Measure

## Quick Facts
- **arXiv ID**: 2511.01847
- **Source URL**: https://arxiv.org/abs/2511.01847
- **Reference count**: 40
- **Primary result**: A lifelong representation learning algorithm with sample complexity bounded by task-eluder dimension, performing few-shot property tests and multi-task ERM updates

## Executive Summary
This paper addresses lifelong representation learning where a learner sequentially encounters tasks sharing common structure. The authors propose an algorithm that maintains a representation and performs few-shot property tests to determine if it admits low-excess-risk prediction layers. When tests fail, the algorithm triggers multi-task ERM on stored data to update the representation. The key theoretical contribution is establishing sample complexity bounds based on a new notion called task-eluder dimension, which measures the maximum number of times representation updates can occur. The framework applies to general function classes and demonstrates both theoretical and empirical benefits over independent task learning.

## Method Summary
The method maintains a representation $\hat{h}$ and processes each new task through a few-shot property test. Given $\hat{h}$, it draws a small sample (size depends only on prediction layer capacity), learns a prediction layer $\tilde{f}_t$ via ERM with $\hat{h}$ frozen, and checks if empirical risk is within threshold $\kappa_t + 3\epsilon/4$. If the test passes, it outputs $\tilde{f}_t \circ \hat{h}$; otherwise, it stores the task data and performs multi-task ERM on all stored tasks to update $\hat{h}$. The algorithm uses a doubling trick to handle unknown task-eluder dimension, incurring only logarithmic overhead.

## Key Results
- Sample complexity bounded by $\tilde{O}((kT + dk^3)/\epsilon^2)$ for $T$ tasks with $k$-dimensional representation and $d$-dimensional inputs
- Number of representation updates bounded by task-eluder dimension, growing sublinearly with number of tasks
- Theoretical guarantees apply to general function classes including linear representations and neural networks
- Empirical validation shows bounded representation updates across synthetic and semi-synthetic datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot property test efficiently detects inadequate representations
- Mechanism: Tests whether current representation admits prediction layer with low excess risk using small sample size independent of representation class capacity
- Core assumption: Known noise levels $\kappa_t$ for each task
- Break condition: Unknown or poorly estimated $\kappa_t$ makes threshold unreliable

### Mechanism 2
- Claim: Multi-task ERM amortizes representation learning cost across hard tasks
- Mechanism: When property test fails, jointly optimizes representation and task-specific layers across all stored "hard" tasks
- Core assumption: Well-specified model (existence of true representation $h^*$)
- Break condition: Model misspecification leads to representations that generalize poorly

### Mechanism 3
- Claim: Task-eluder dimension bounds representation updates
- Mechanism: A task is $\epsilon$-independent if alternative representation can match earlier tasks but not the new one; dimension counts longest such sequence
- Core assumption: Finite task-eluder dimension due to bounded capacity of representation and prediction classes
- Break condition: Highly complex function classes yield large dimensions, reducing efficiency

## Foundational Learning

**Empirical Risk Minimization (ERM)**
- Why needed: Core subroutine for both property tests and multi-task refinement
- Quick check: Explain how minimizing empirical risk generalizes to population risk via covering number bounds

**Covering Numbers and Metric Entropy**
- Why needed: Sample complexity bounds depend on log C(H, ε) and log C(F, ε)
- Quick check: Given N(ε) = (2e/ε)^k, what is metric entropy and how does it scale with k?

**Doubling Trick (Online Learning)**
- Why needed: Handles unknown task-eluder dimension with logarithmic overhead
- Quick check: Why does doubling estimate when insufficient ensure total overhead is bounded?

## Architecture Onboarding

**Component map**: Task t arrives → Few-shot sampler (m̃ samples) → ERM with frozen ĥ → Property test (bL ≤ κ_t + 3ε/4?) → PASS: output f̃_t∘ĥ / FAIL: trigger MTL → Memory buffer M ← Sample m_N from task t → Multi-task ERM on M → Update ĥ, output f_t∘ĥ

**Critical path**: Property test failure → Memory update → MTL invocation → Representation update. This path executes O(dim(H, F, ε)) times total.

**Design tradeoffs**:
- Memory size vs. sample efficiency: Larger m_N improves MTL quality but increases space complexity
- Known vs. unknown Ξ: Eliminates doubling overhead when known
- Conservative vs. aggressive threshold: Stricter thresholds reduce false negatives but increase MTL calls

**Failure signatures**:
- Runaway memory growth: Suggests underestimated task-eluder dimension or model misspecification
- High excess risk on validation: Check m_N and m̃ bounds with correct covering number estimates
- Catastrophic forgetting: Not addressed (tasks don't revisit)

**First 3 experiments**:
1. Synthetic linear regression: Verify representation updates ≈ k across T=50 tasks with H = {k-dim linear subspaces}, F = {linear predictors}
2. Ablation on noise level knowledge: Compare performance with known vs. estimated κ_t
3. Scaling analysis: Fix ε, vary T from 10 to 1000, verify sample growth as Õ(T · log C(F)) + O(1)

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can guarantees extend to fully agnostic setting where tasks may not share common representation?
- Basis: Authors state extension to full agnostic setting is deferred to future work in Section 2 and Section 8
- Why unresolved: Current bounds rely on existence of ground truth representation h*
- What evidence would resolve it: Proof of sample complexity bounds for agnostic lifelong learning or formal lower bound

**Open Question 2**
- Question: Can assumption of known noise levels (κ_t) be relaxed or inferred by learner?
- Basis: Section 8 identifies inferring noise levels from other tasks under additional structural assumptions as future direction
- Why unresolved: Authors conjecture distinguishing current representation from optimal is information-theoretically hard without κ_t
- What evidence would resolve it: Algorithm that estimates noise levels online while maintaining sample efficiency

**Open Question 3**
- Question: Can intermediate notion of task agreement yield improved sample complexity bounds?
- Basis: Section 8 suggests formulating intermediate notions between aggregate and pointwise independence
- Why unresolved: Natural ε-pointwise-independence condition fails (infinite dimension) for linear classes
- What evidence would resolve it: New complexity measure bridging aggregate and pointwise independence

## Limitations
- Known noise levels κ_t assumed, requiring estimation in practice
- Task-eluder dimension may be difficult to compute for complex function classes
- Doubling trick adds log Ξ overhead when dimension is unknown

## Confidence

**High Confidence**: Sample complexity bounds in Theorem 12 are well-established via standard covering number arguments

**Medium Confidence**: Practical effectiveness depends on accurate κ_t estimates; real-world performance may degrade with noisy thresholds

**Low Confidence**: Claim that dim(H, F, ε) remains bounded for general function classes requires careful analysis for deep networks

## Next Checks

1. **Empirical Task-Eluder Dimension Analysis**: Implement synthetic experiments to measure actual representation updates across varying T, comparing against theoretical bounds and tracking scaling with problem parameters k and d.

2. **Noise Level Sensitivity Test**: Run experiments with perturbed κ_t values (additive Gaussian noise with varying standard deviation), measuring false positive/negative rates in property tests and resulting excess risk on validation data.

3. **Multi-Task ERM Convergence Verification**: For each representation update, monitor average empirical risk across stored tasks to verify it decreases to ≤ ε/4 within m_N samples, confirming theoretical sample complexity requirements.