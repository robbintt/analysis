---
ver: rpa2
title: Investigating Retrieval-Augmented Generation Systems on Unanswerable, Uncheatable,
  Realistic, Multi-hop Queries
arxiv_id: '2510.11956'
source_url: https://arxiv.org/abs/2510.11956
tags:
- https
- queries
- crumqs
- multi-hop
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents CRUMQs, a pipeline for generating unanswerable,
  uncheatable, realistic, and multi-hop queries for RAG evaluation. Existing RAG benchmarks
  fail to capture realistic task complexity, often allowing disconnected reasoning
  or focusing on simple factual recall.
---

# Investigating Retrieval-Augmented Generation Systems on Unanswerable, Uncheatable, Realistic, Multi-hop Queries

## Quick Facts
- arXiv ID: 2510.11956
- Source URL: https://arxiv.org/abs/2510.11956
- Authors: Gabrielle Kaili-May Liu; Bryan Li; Arman Cohan; William Gantt Walden; Eugene Yang
- Reference count: 40
- CRUMQs pipeline generates challenging, unanswerable, multi-hop queries for RAG evaluation, achieving 18% acceptable ratio vs 34% for existing benchmarks

## Executive Summary
This paper introduces CRUMQs, a novel pipeline for generating unanswerable, uncheatable, realistic, and multi-hop queries specifically designed to evaluate Retrieval-Augmented Generation (RAG) systems. Existing RAG benchmarks often fail to capture realistic task complexity, allowing disconnected reasoning or focusing on simple factual recall. CRUMQs addresses these limitations by creating queries that require multi-hop reasoning and cannot be answered without external knowledge, providing a more rigorous evaluation framework.

The CRUMQs pipeline extracts topic keyphrases from document collections, sources recent external articles, and generates multi-hop QA pairs that are either fully or partially unanswerable. Questions undergo verification for unanswerability and multi-hop reasoning through LLM judgments and synthetic chain-of-thought annotations. Experimental results demonstrate that CRUMQs significantly outperform existing benchmarks in challenging RAG systems, with up to 81% reduction in cheatability compared to MultiHop-RAG and substantially lower acceptable ratios for leading systems.

## Method Summary
The CRUMQs pipeline operates through a systematic approach to generate challenging evaluation queries. First, it extracts keyphrases representing topical entities from a document collection to identify core concepts. Then, it sources recent external articles related to these topics to ensure questions require knowledge beyond the provided corpus. The pipeline generates multi-hop QA pairs where questions are either fully unanswerable (requiring external knowledge not present in the corpus) or partially unanswerable (requiring both corpus knowledge and external information). Generated questions undergo verification using LLM-based judgments to confirm unanswerability and multi-hop reasoning requirements. The pipeline also creates synthetic chain-of-thought annotations to validate the multi-hop reasoning structure.

## Key Results
- CRUMQs achieve 18% acceptable ratio for leading RAG systems compared to 34% for UAEval4RAG queries
- Up to 81.0% reduction in cheatability compared to MultiHop-RAG benchmark
- CRUMQs successfully generate queries that resist disconnected reasoning while maintaining realistic task complexity

## Why This Works (Mechanism)
The CRUMQs pipeline works by creating a systematic gap between the information available in the corpus and the knowledge required to answer questions. By extracting topic keyphrases and sourcing recent external articles, the pipeline ensures that questions cannot be answered through simple retrieval from the provided documents. The multi-hop generation process forces systems to reason across multiple pieces of information, while the unanswerability verification prevents trivial solutions. The synthetic chain-of-thought annotations provide a structured way to verify that questions genuinely require complex reasoning rather than disconnected retrieval steps.

## Foundational Learning
- **Topic Keyphrase Extraction**: Why needed - identifies core concepts to guide question generation; Quick check - verify extracted phrases represent meaningful topical entities from corpus
- **External Article Sourcing**: Why needed - creates knowledge gap beyond corpus; Quick check - confirm articles provide relevant but unavailable information
- **Multi-hop Question Generation**: Why needed - ensures complex reasoning requirements; Quick check - validate questions require multiple reasoning steps
- **Unanswerability Verification**: Why needed - prevents trivial answerable questions; Quick check - test questions against both corpus and external knowledge
- **Chain-of-Thought Annotation**: Why needed - verifies multi-hop structure; Quick check - confirm annotations accurately represent reasoning paths
- **LLM-based Judgment**: Why needed - automated verification of requirements; Quick check - compare judgments across different LLM models

## Architecture Onboarding
**Component Map**: Document Collection -> Keyphrase Extraction -> External Article Sourcing -> Question Generation -> Unanswerability Verification -> Multi-hop Verification -> Synthetic Chain-of-Thought -> Final CRUMQs Dataset

**Critical Path**: The most critical sequence is Keyphrase Extraction -> Question Generation -> Unanswerability Verification, as these determine whether generated queries meet the core requirements of being unanswerable and multi-hop.

**Design Tradeoffs**: The pipeline trades generation speed for question quality, using multiple LLM-based verifications that are computationally expensive but necessary for ensuring query quality. The reliance on synthetic annotations rather than human verification reduces cost but may introduce systematic biases in the evaluation criteria.

**Failure Signatures**: Common failure modes include keyphrases that don't represent meaningful topics, external articles that are too closely related to the corpus, generated questions that accidentally become answerable through corpus knowledge, and chain-of-thought annotations that don't accurately reflect required reasoning steps.

**First Experiments**:
1. Test keyphrase extraction on diverse document collections to evaluate generalizability
2. Verify unanswerability judgments by having humans attempt to answer generated questions
3. Measure performance differences across multiple RAG architectures to assess uncheatability claims

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Heavy reliance on LLM-based verification introduces uncertainty in consistency and reliability across different model versions
- Limited testing to Wikipedia-based document collections, raising questions about performance on domain-specific or less structured corpora
- Focus on English-language queries without addressing multilingual or cross-lingual scenarios

## Confidence
- **High Confidence**: CRUMQs are more challenging than existing benchmarks, supported by empirical comparison showing measurable differences in acceptable ratios and cheatability metrics
- **Medium Confidence**: CRUMQs represent "realistic" queries, though this is somewhat subjective and lacks user studies or expert validation
- **Low Confidence**: Ability to generate truly "uncheatable" queries depends on specific RAG systems evaluated and would require testing against broader range of architectures

## Next Checks
1. Replicate CRUMQs generation pipeline across multiple document domains (scientific literature, news articles, technical documentation) to assess generalizability
2. Conduct human evaluation studies with domain experts to validate realism, unanswerability, and multi-hop nature of generated queries
3. Test CRUMQs against diverse set of RAG architectures beyond single system evaluated to assess true uncheatability