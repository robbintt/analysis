---
ver: rpa2
title: 'Model-Free Neural State Estimation in Nonlinear Dynamical Systems: A Comparative
  Study of Neural Architectures and Classical Filters'
arxiv_id: '2601.21266'
source_url: https://arxiv.org/abs/2601.21266
tags:
- neural
- state
- nonlinear
- estimation
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a systematic empirical comparison between\
  \ neural network models and classical filtering methods for state estimation in\
  \ nonlinear dynamical systems. The study evaluates six neural architectures\u2014\
  including Transformers, state-space models (SSMs), and recurrent networks\u2014\
  against four classical nonlinear filters (EKF, UKF, EnKF, PF) across five nonlinear\
  \ scenarios without providing the neural models access to system dynamics or noise\
  \ models."
---

# Model-Free Neural State Estimation in Nonlinear Dynamical Systems: A Comparative Study of Neural Architectures and Classical Filters

## Quick Facts
- arXiv ID: 2601.21266
- Source URL: https://arxiv.org/abs/2601.21266
- Authors: Zhuochen Liu; Hans Walker; Rahul Jain
- Reference count: 17
- Key outcome: Model-free neural state estimators approach classical filter performance in nonlinear systems while achieving significantly higher throughput

## Executive Summary
This paper presents a systematic empirical comparison between neural network models and classical filtering methods for state estimation in nonlinear dynamical systems. The study evaluates six neural architectures—including Transformers, state-space models (SSMs), and recurrent networks—against four classical nonlinear filters (EKF, UKF, EnKF, PF) across five nonlinear scenarios without providing the neural models access to system dynamics or noise models. Results show that SSMs (Mamba and Mamba-2) consistently achieve the best performance among neural models and approach the accuracy of strong classical filters like the EKF and UKF, while significantly outperforming weaker baselines. Across all scenarios, neural models achieve substantially higher inference throughput than classical filters. The study demonstrates that model-free neural state estimators can effectively learn filtering behavior in nonlinear systems, recovering a significant fraction of classical filter performance despite lacking explicit system knowledge.

## Method Summary
The study compares six neural architectures (GPT-2, Filterformer, Mamba, Mamba-2, GRU, LSTM) at approximately 100K parameters each against four classical nonlinear filters (EKF, UKF, EnKF, PF) across five nonlinear dynamical systems: Ballistic re-entry, Bearings-only tracking, Lorenz 96, N-link pendulum, and Planar Quadrotor. Neural models are trained in a supervised fashion using 20,000 trajectories of 100 timesteps each, extracted from 500-timestep base trajectories, without access to system dynamics or noise models. Classical filters are implemented with full system knowledge as baselines. Performance is evaluated using RMSE, MAE, MedAE, NRMSE, and AUC metrics across 8,000 trajectories of 500 timesteps. Each configuration is run 15 times (5 initializations × 3 datasets) with early stopping on validation sets.

## Key Results
- SSMs (Mamba and Mamba-2) consistently achieve the best performance among neural models and approach the accuracy of strong classical filters like the EKF and UKF
- Neural models achieve substantially higher inference throughput than classical filters across all scenarios
- Model-free neural estimators recover a significant fraction of classical filter performance despite lacking explicit system knowledge
- Performance gaps between neural and classical filters are stable across different dynamical systems

## Why This Works (Mechanism)
None provided

## Foundational Learning
- **Nonlinear state estimation**: Estimating system states from noisy observations in nonlinear dynamical systems; why needed: Classical linear filters fail in nonlinear regimes
- **Model-free learning**: Learning estimation policies directly from data without explicit system knowledge; why needed: Avoids complex system identification and modeling
- **State-space models**: Mathematical representation of dynamical systems; why needed: Foundation for both classical filters and neural architectures
- **Filtering vs smoothing**: Online state estimation using past/present data vs offline estimation using full data sequence; why needed: Critical distinction for real-time applications
- **Covariance tracking**: Maintaining uncertainty estimates about state predictions; why needed: Essential for robust control and decision making
- **Quick check**: Verify neural models can track uncertainty without explicit covariance propagation

## Architecture Onboarding

**Component map**: Observations + Controls -> Neural Model -> State Estimate -> RMSE Metrics -> Throughput Measurement

**Critical path**: Data generation → Model training → Inference evaluation → Performance comparison

**Design tradeoffs**: Model-free learning sacrifices explicit uncertainty quantification for higher throughput and reduced modeling complexity

**Failure signatures**: Filter divergence (EnKF in Bearings-Only Tracking), high variance across seeds, poor long-horizon performance

**First experiments**:
1. Implement one dynamical system (Lorenz 96) and verify classical filter baselines converge
2. Train a simple recurrent network on the same system and compare RMSE
3. Measure inference throughput difference between neural and classical approaches

## Open Questions the Paper Calls Out

**Open Question 1**
- **Question**: Can model-free neural state estimators learn well-calibrated uncertainty estimates or full posterior distributions comparable to Bayesian filters?
- **Basis in paper**: [explicit] The authors explicitly state in the Limitations section that their evaluation "focuses on point estimation accuracy and does not assess uncertainty calibration or posterior quality."
- **Why unresolved**: The current study only tracks error metrics like RMSE, ignoring the probabilistic reliability of the outputs, which is a critical component of classical filtering for robust control.
- **What evidence would resolve it**: Experiments evaluating the negative log-likelihood or calibration error of neural model outputs compared to the ground truth state distribution.

**Open Question 2**
- **Question**: To what extent do high-performing neural architectures like Mamba rely on implicit smoothing rather than strictly causal filtering operations?
- **Basis in paper**: [explicit] The authors acknowledge the study "does not explicitly distinguish filtering from smoothing behavior or analyze strict causality and belief compression properties."
- **Why unresolved**: Sequence models often have architectural biases or large receptive fields that might utilize future context, violating the online (real-time) constraints of standard filtering.
- **What evidence would resolve it**: An ablation study measuring performance degradation when models are restricted to strictly causal operations versus allowing bidirectional context.

**Open Question 3**
- **Question**: How does the data efficiency of model-free estimators degrade relative to classical filters in data-scarce environments?
- **Basis in paper**: [inferred] The authors note a reliance on a data-to-parameter ratio of 20:1, which "may still be impractical in data-scarce control applications," suggesting a need to understand performance in lower-data regimes.
- **Why unresolved**: It is unclear if the "filtering-like behavior" holds when the training data is insufficient to cover the state space or model parameters effectively.
- **What evidence would resolve it**: Comparative curves of estimation error (RMSE) vs. number of training trajectories for both neural and classical baselines.

## Limitations
- Fixed parameter budget (~100K) may not represent optimal scaling for either approach
- Controlled simulation environments may not capture real-world system complexities
- Limited comparison to hybrid approaches that combine learned and model-based components

## Confidence
- **High confidence**: Neural models (especially SSMs) achieve higher throughput than classical filters; model-free learning captures significant filtering performance
- **Medium confidence**: SSMs consistently outperform other neural architectures; performance gaps between neural and classical filters are stable across scenarios
- **Low confidence**: Generalizability to real-world systems with higher complexity, different noise characteristics, or online adaptation requirements

## Next Checks
1. **Noise sensitivity validation**: Test neural estimators under varying SNR conditions beyond the single setting reported, including extreme noise levels where classical filters typically degrade

2. **Cross-scenario transfer**: Train models on one dynamical system and evaluate on another to assess learned filter generalization versus system-specific learning

3. **Real-time performance under constraints**: Measure memory usage, latency, and performance under realistic computational constraints (CPU vs GPU, varying batch sizes) to complement throughput metrics