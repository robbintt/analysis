---
ver: rpa2
title: 'Breaking Audio Large Language Models by Attacking Only the Encoder: A Universal
  Targeted Latent-Space Audio Attack'
arxiv_id: '2512.23881'
source_url: https://arxiv.org/abs/2512.23881
tags:
- audio
- universal
- arxiv
- encoder
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces U-TLSA, a universal targeted latent-space\
  \ attack on audio-language models (ALLMs) that operates solely at the encoder level.\
  \ Unlike prior attacks that require end-to-end access or per-input optimization,\
  \ U-TLSA learns a single waveform perturbation that generalizes across inputs and\
  \ speakers, steering the encoder\u2019s latent representation toward an attacker-chosen\
  \ target."
---

# Breaking Audio Large Language Models by Attacking Only the Encoder: A Universal Targeted Latent-Space Audio Attack

## Quick Facts
- arXiv ID: 2512.23881
- Source URL: https://arxiv.org/abs/2512.23881
- Authors: Roee Ziv; Raz Lapid; Moshe Sipper
- Reference count: 9
- One-line primary result: Universal targeted latent-space attack on audio-language models that achieves 72.8-92.6% success rates across diverse datasets by compromising only the encoder.

## Executive Summary
This work introduces U-TLSA, a universal targeted latent-space attack on audio-language models that operates solely at the encoder level. Unlike prior attacks requiring end-to-end access or per-input optimization, U-TLSA learns a single waveform perturbation that generalizes across inputs and speakers, steering the encoder's latent representation toward an attacker-chosen target. Evaluated on Qwen2-Audio-7B-Instruct, the attack achieves macro-average success rates of 72.8-92.6% across three tasks on diverse datasets with thousands of unique speakers. Encoder-only optimization is 2× more memory-efficient and 3.7× faster than end-to-end baselines, demonstrating that compromising the encoder alone suffices to hijack the audio-to-language pipeline.

## Method Summary
The attack synthesizes a reference audio for the target command, computes its encoder embedding H_tgt, then learns a universal waveform perturbation δ by minimizing per-frame cosine distance between perturbed encoder outputs and H_tgt via projected gradient descent. The optimization uses Adam with ℓ_∞ constraints (ε=0.02) and is performed only on the perturbation δ while freezing both the encoder and decoder. The attack is trained on LibriSpeech train-clean-100 and evaluated zero-shot on LibriSpeech test-other, MInDS-14, and Speech Commands datasets. Success is measured via exact string match after text normalization.

## Key Results
- Achieves macro-average attack success rates of 72.8-92.6% across wake phrase, command injection, and behavioral hijacking tasks
- Demonstrates universality across 1,000+ speakers and diverse acoustic conditions
- Encoder-only optimization is 2× more memory-efficient and 3.7× faster than end-to-end baselines
- Random noise baseline achieves 0% success rate, confirming optimization effectiveness

## Why This Works (Mechanism)

### Mechanism 1: Per-Frame Latent-Space Alignment via Cosine Distance Minimization
Optimizing a universal perturbation to minimize per-frame cosine distance between perturbed encoder outputs and a target embedding induces attacker-specified decoder outputs. The per-frame formulation preserves temporal trajectory, not just global direction, by minimizing L(A,B) = (1/L)Σ(1 - ⟨A_t, B_t⟩ / (||A_t||₂ ||B_t||₂)) where A = ε_θ(Φ(x+δ)) and B = H_tgt.

### Mechanism 2: Universality Through Distributional Coverage During Optimization
A single perturbation optimized over diverse training utterances generalizes to unseen speakers, accents, and acoustic conditions. The expectation E_x~D[L(ε_θ(Φ(x+δ)), H_tgt)] is approximated via mini-batches from LibriSpeech train-clean-100, exposing δ to varied phonemes while the ℓ_∞ constraint prevents overfitting.

### Mechanism 3: Encoder-Only Sufficiency via Bottleneck Exploitation
Compromising the encoder alone suffices to hijack the full audio-language pipeline because the encoder output h = ε_θ(Φ(x)) is the sole channel through which audio influences the decoder f_ϕ(p, y_<t, h). Steering h → H_tgt causes the decoder to receive inputs indistinguishable from genuinely target-aligned audio.

## Foundational Learning

- **Concept: Universal Adversarial Perturbations (UAPs)** - Why needed: U-TLSA extends image-domain UAPs to audio encoders; understanding the original formulation clarifies why a single perturbation works across inputs. Quick check: Can you explain why UAPs exploit shared decision boundaries rather than input-specific vulnerabilities?

- **Concept: Projected Gradient Descent (PGD) with ℓ_∞ Constraints** - Why needed: The optimization uses Adam with projection onto the ℓ_∞ ball of radius ε after each step; understanding constraint handling is essential for implementation. Quick check: After a gradient update δ ← δ - η∇_δL, how do you project δ back onto {δ : ||δ||_∞ ≤ ε}?

- **Concept: Whisper/Log-Mel Spectrogram Feature Extraction** - Why needed: The attack operates on raw waveforms but gradients flow through frozen Φ (log-Mel spectrogram extraction) and ε_θ (Whisper-large-v3 encoder). Quick check: Why might gradient-based attacks on spectrograms behave differently than attacks on raw waveforms?

## Architecture Onboarding

- **Component map:** Raw audio x → Φ (frozen log-Mel extractor) → ε_θ (Whisper-large-v3 encoder) → h (latent sequence) → f_ϕ (Qwen2-Audio LLM decoder) → text output

- **Critical path:** 1) Synthesize target audio x_tgt via TTS for chosen command 2) Compute H_tgt = ε_θ(Φ(x_tgt)) once and cache 3) Initialize δ (zeros or small random) 4) For each iteration: sample mini-batch, compute L(ε_θ(Φ(x+δ)), H_tgt), backprop to δ, update with Adam, project onto ℓ_∞ ball 5) Evaluate zero-shot transfer on held-out datasets

- **Design tradeoffs:** Per-frame vs. global cosine loss (per-frame preserves temporal structure but is stricter), perturbation budget ε (larger ε increases ASR but perceptual distortion), training distribution breadth (more diverse data improves transfer but increases optimization cost)

- **Failure signatures:** ASR near random baseline (0%) indicates optimization not converging, high training ASR but near-zero test ASR suggests overfitting to training speakers, inconsistent results across target commands may indicate unstable latent representations

- **First 3 experiments:** 1) Reproduce single-target attack: Train δ for "Unlock the door" on LibriSpeech subset, evaluate on 100 held-out utterances to verify 90%+ ASR 2) Ablate loss function: Compare per-frame cosine vs. global cosine vs. MSE to confirm temporal alignment matters 3) Test transfer across encoder checkpoints: Train δ on Whisper-large-v3, test on Whisper-medium to assess encoder-specific vs. architecture-general vulnerabilities

## Open Questions the Paper Calls Out

### Open Question 1
Does U-TLSA remain effective when adversarial audio is played over-the-air through speakers and recorded via microphones in real acoustic environments? The paper evaluates digital perturbations applied directly to waveforms but doesn't test physical-world scenarios with room acoustics, speaker distortion, and ambient noise.

### Open Question 2
Do universal perturbations trained on one ALLM transfer to other audio-language models that share the same encoder or use different encoders? Only Qwen2-Audio-7B-Instruct is evaluated, but the paper suggests encoder-only attacks capture a powerful transferable risk without testing cross-model transfer.

### Open Question 3
Can encoder-level defenses such as randomized feature extraction or latent-space anomaly detection mitigate U-TLSA without degrading normal model performance? The paper proposes defense directions but provides no empirical evaluation of their effectiveness or computational overhead.

## Limitations

- Dependence on encoder latent space smoothness for gradient-based optimization to succeed
- Assumption that decoder's output depends only on encoder's latent representation (safety mechanisms could monitor encoder output distribution)
- Attack assumes frozen, unmodified encoders and decoders (runtime input sanitization would likely neutralize vulnerability)
- Real-world applicability limited by TTS-synthesized target audio assumption

## Confidence

- **High confidence:** The encoder-only optimization approach (2× memory, 3.7× faster than end-to-end) and general attack framework are well-supported
- **Medium confidence:** Universality claim across speakers and conditions is supported by zero-shot transfer but cannot be exhaustively proven
- **Low confidence:** Assertion that encoder-level attacks are a "critical attack surface" in multimodal systems lacks direct evidence about real-world deployment scenarios

## Next Checks

1. Test encoder-specific vulnerability: Train attack on Whisper-large-v3 and evaluate transfer success on Whisper-medium and Whisper-base encoders to determine if vulnerability is encoder-specific or architecture-general.

2. Evaluate natural vs. synthetic targets: Compare attack success rates when target command is spoken naturally by a human versus synthesized via TTS to assess real-world applicability.

3. Assess encoder-level defenses: Implement and evaluate simple encoder-side defenses such as input denoising, perturbation detection, or random perturbation injection to determine attack's robustness to countermeasures.