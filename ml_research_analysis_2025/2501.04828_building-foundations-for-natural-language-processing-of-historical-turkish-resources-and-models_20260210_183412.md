---
ver: rpa2
title: 'Building Foundations for Natural Language Processing of Historical Turkish:
  Resources and Models'
arxiv_id: '2501.04828'
source_url: https://arxiv.org/abs/2501.04828
tags:
- turkish
- historical
- language
- text
- dependency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of natural language processing
  for historical Turkish, an underexplored area. The authors develop foundational
  resources including the first named entity recognition dataset (HisTR), the first
  Universal Dependencies treebank (OTA-BOUN), and a clean text corpus (OTC) spanning
  from the 15th to 20th centuries.
---

# Building Foundations for Natural Language Processing of Historical Turkish: Resources and Models

## Quick Facts
- arXiv ID: 2501.04828
- Source URL: https://arxiv.org/abs/2501.04828
- Reference count: 12
- Primary result: First named entity recognition dataset (HisTR) and Universal Dependencies treebank (OTA-BOUN) for historical Turkish, with transformer models achieving 91.21 F1 on development and 61.91 F1 on out-of-domain test set

## Executive Summary
This paper addresses the challenge of natural language processing for historical Turkish, an underexplored area. The authors develop foundational resources including the first named entity recognition dataset (HisTR), the first Universal Dependencies treebank (OTA-BOUN), and a clean text corpus (OTC) spanning from the 15th to 20th centuries. They train transformer-based models for named entity recognition, dependency parsing, and part-of-speech tagging tasks. The experimental results show promising performance, with the best model achieving 91.21 F1 score on the development set and 61.91 F1 on an out-of-domain test set for named entity recognition. The study highlights the potential of leveraging language-specific pre-trained models and fine-tuning on domain-specific data for historical Turkish NLP tasks.

## Method Summary
The authors fine-tune BERTurk and mBERT models for named entity recognition on the HisTR dataset (812 sentences), and use the STEPS parser with biaffine classifier for dependency parsing and POS tagging on the OTA-BOUN treebank (514 sentences). For NER, they employ a two-stage fine-tuning approach: first on modern Turkish datasets (MilliyetNER, WikiANN), then on historical Turkish data. The parsing and POS tasks use combined modern and historical training data. All models are evaluated using standard task-specific metrics including F1 score for NER, UAS/LAS for parsing, and UPOS F1 for POS tagging.

## Key Results
- BERTurk outperforms mBERT by 6% UAS and 4-17 F1 points across all historical Turkish NLP tasks
- Two-stage fine-tuning (modern Turkish → historical Turkish) improves NER F1 from 73.65 to 91.21 on development set
- Historical Turkish parsing shows 81.51 UAS when trained on combined modern and historical data, compared to 68.87 UAS on historical data alone
- 30-point F1 gap between in-domain (19th century) and out-of-domain (17th century) test performance reveals significant temporal generalization challenges

## Why This Works (Mechanism)

### Mechanism 1: Language-Specific Transfer from Modern to Historical Turkish
- Claim: Pre-trained language models for modern Turkish can be effectively adapted to historical Turkish through fine-tuning, outperforming multilingual alternatives.
- Mechanism: BERTurk captures Turkish-specific morphological and syntactic patterns that transfer across time periods, despite vocabulary differences and archaic constructions. The shared grammatical foundation enables more efficient parameter reuse than multilingual models with diluted language-specific representations.
- Core assumption: Linguistic continuity between modern and historical Turkish provides sufficient representational overlap for effective transfer learning.
- Evidence anchors:
  - Table 6 shows BERTurk+HisTR achieving 90.07 F1 on development set versus mBERT+HisTR at 86.05 F1—a 4-point advantage despite both being fine-tuned on identical historical data.
  - "BERTurk outperforms mBERT in all settings for NER tagging of historical Turkish... the linguistic similarities may allow BERTurk to transfer its learned representations more effectively than mBERT, which has a broader but less specialized knowledge base."
  - Limited corpus support—neighboring papers focus on other low-resource languages (Senegalese, Vietnamese legal) without directly validating cross-temporal transfer within a single language family.
- Break condition: If historical Turkish vocabulary divergence exceeds the model's capacity for domain adaptation (evidenced by the 30-point gap between in-domain development F1 of 91.21 and out-of-domain Ruznamçe test F1 of 61.91), transfer effectiveness degrades substantially.

### Mechanism 2: Domain-Specific Fine-tuning Data Efficiency
- Claim: Small but domain-specific historical Turkish datasets provide disproportionate improvements over large modern Turkish datasets alone.
- Mechanism: Historical texts contain orthographic variations, obsolete vocabulary, and syntactic structures not represented in modern corpora. Even limited exposure to these patterns during fine-tuning enables the model to adjust decision boundaries for historical linguistic phenomena.
- Core assumption: Annotation quality and domain relevance matter more than dataset size for historical language adaptation.
- Evidence anchors:
  - "achieving promising results in tasks that require understanding of historical linguistic structures"
  - Table 6 shows BERTurk+HisTR (trained on 462 historical sentences) achieves 90.07 F1, nearly matching BERTurk+MilliyetNER+HisTR (91.21 F1) despite MilliyetNER containing ~500K tokens.
  - "fine-tuning BERTurk using labeled data that include texts written only in modern Turkish... does not yield good results, even if the labeled data... is quite extensive... Further fine-tuning the model with the training set of our HisTR dataset improved the performance by a large margin."
- Break condition: When test data originates from significantly different time periods or document types (17th-century Ruznamçe vs. 19th-century periodicals), domain-specific training fails to generalize without broader temporal coverage.

### Mechanism 3: Syntactic Complexity Amplifies Parsing Difficulty
- Claim: Historical Turkish's longer sentences and complex clause structures create parsing challenges that scale with syntactic elaboration.
- Mechanism: Historical Turkish texts average 17.10 tokens per sentence versus 12.41 (TR-BOUN) and 10.01 (IMST-UD) for modern Turkish, with higher frequencies of conjunct relations (6.89% vs. 5.66%/4.96%) and adnominal clauses (3.95% vs. 2.78%/2.64%). These structural differences increase dependency path lengths and ambiguity.
- Core assumption: Transformer-based parsers trained primarily on modern Turkish struggle with the discourse-level complexity of historical administrative and literary texts.
- Evidence anchors:
  - Table 4 comparison showing OTA-BOUN has higher average token count, conj frequency, compound:lvc frequency, and acl frequency than modern Turkish treebanks.
  - "The length and complexity of historical Turkish sentences led to frequent parsing failures when handling longer structures, complicating the parsing process."
  - Example 1 demonstrates "excessive amount of conjunct relations" in a single nominal phrase with 8 coordinated modifiers.
  - No direct corpus validation for historical Turkish specifically, though related work on historical Chinese (arXiv:2503.19844) confirms similar POS tagging and NER challenges in historical texts from 1900-1950.
- Break condition: When sentences exceed typical training distribution lengths or contain nested clause structures not represented in modern training data, unlabeled attachment scores drop (68.87 UAS for OTA-BOUN-only training vs. 81.51 when combined with modern Turkish data).

## Foundational Learning

- **Concept: Transfer Learning for Low-Resource Historical Languages**
  - Why needed here: The entire approach depends on leveraging modern Turkish PLMs rather than training from scratch due to limited historical data.
  - Quick check question: Can you explain why a model pre-trained on modern Turkish text might recognize "İstanbul" in a 19th-century Ottoman document but fail on "Dersaadet" (the historical name)?

- **Concept: Universal Dependencies Annotation Scheme**
  - Why needed here: OTA-BOUN treebank follows UD guidelines; understanding dependency relations (nsubj, obj, acl, compound:lvc) is essential for interpreting parsing results and error patterns.
  - Quick check question: Given the sentence "Felsefe ve psikoloji ile mütevaggil olan edibler" (Litterateurs dealing with philosophy and psychology), what dependency relation would connect "mütevaggil" to "edibler"?

- **Concept: Out-of-Domain Evaluation for Temporal Generalization**
  - Why needed here: The 30-point F1 gap between development (91.21) and Ruznamçe test (61.91) reveals that standard train/dev splits inadequately measure historical NLP system robustness.
  - Quick check question: Why might a model trained on 19th-century literary periodicals struggle with 17th-century administrative records even within the same language?

## Architecture Onboarding

- **Component map:**
  Input: Transliterated Latin-script historical Turkish text (post-OCR/manual transcription)
  -> Encoder: BERTurk (12 layers, 768 hidden, 110M params) or mBERT (multilingual, 12 layers) or TURNA (T5-based, 1.1B params, encoder-decoder)
  -> Task heads: NER token classifier / STEPS biaffine dependency parser + POS sequence tagger
  -> Training: Two-stage fine-tuning (modern Turkish dataset → historical Turkish dataset)
  -> Evaluation: Task-specific metrics (F1 for NER, UAS/LAS for parsing, UPOS F1 for POS)

- **Critical path:**
  1. Text preprocessing: Arabic-to-Latin transliteration verification, encoding normalization (address Unicode failures for historical diacritics—see Table 5 error analysis)
  2. Baseline evaluation: Test modern Turkish PLM zero-shot on historical data (BERTurk+MilliyetNER achieves only 73.65 F1 on development)
  3. Domain adaptation: Fine-tune on historical training split (HisTR: 462 sentences / OTA-BOUN: 114 sentences)
  4. Out-of-domain testing: Evaluate on temporally distant test set (Ruznamçe: 17th century vs. training: 19th century)

- **Design tradeoffs:**
  - Model size vs. data availability: TURNA (1.1B params) showed training loss higher than validation loss after convergence, indicating insufficient data for effective fine-tuning. BERT-scale models (110M params) performed better with limited historical data.
  - Monolingual vs. multilingual: BERTurk consistently outperforms mBERT (6% UAS advantage for parsing, ~4-17 F1 advantage for NER), suggesting language-specific pre-training outweighs multilingual breadth for historical variants.
  - Data augmentation: Adding multilingual NER data (WikiANN) to mBERT decreased performance (41.49 F1 vs. 42.69 without), while adding modern Turkish NER data (MilliyetNER) to BERTurk improved results (91.21 vs. 90.07 F1).

- **Failure signatures:**
  - Temporal distribution shift: 30-point F1 drop between 19th-century development set (91.21) and 17th-century test set (61.91) indicates model overfits to specific time period
  - Character encoding artifacts: Table 5 shows systematic extraction errors (diacritical failures, script conversion errors, word segmentation failures) that propagate through the pipeline
  - Insufficient training signal: TURNA's convergence with training loss > validation loss signals data scarcity for large models
  - Parsing accuracy ceiling: Even best configuration (81.51 UAS) leaves ~18% of dependencies unlabeled correctly, particularly for long sentences with multiple conjunct relations

- **First 3 experiments:**
  1. **Establish baseline transfer:** Fine-tune BERTurk on MilliyetNER only, evaluate on HisTR development set to quantify zero-shot historical performance gap (expect 73-76 F1 range per Table 6).
  2. **Measure domain adaptation gain:** Add HisTR training data (462 sentences) to baseline, evaluate improvement magnitude (Table 6 shows ~17-point F1 increase expected) and identify error patterns in false positives/negatives.
  3. **Stress-test temporal generalization:** Evaluate best checkpoint on Ruznamçe test set to measure out-of-domain degradation; analyze whether errors concentrate on specific entity types (PERSON vs. LOCATION) or linguistic constructions (archaic morphological patterns, obsolete toponyms).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would continual pre-training on raw historical Turkish texts, followed by task-specific fine-tuning, significantly outperform direct fine-tuning of modern Turkish PLMs?
- Basis in paper: [explicit] The authors state: "we believe that, rather than exclusively fine-tuning PLMs on labeled historical Turkish data, first applying continual pre-training methods on PLMs using raw historical Turkish texts, followed by fine-tuning on labeled data, would significantly improve model performance for historical Turkish NLP."
- Why unresolved: The authors have not yet pre-trained PLMs on their OTC corpus; all experiments used existing modern Turkish or multilingual PLMs fine-tuned directly on historical data.
- What evidence would resolve it: A controlled comparison of models (a) fine-tuned directly on historical labels versus (b) continually pre-trained on OTC then fine-tuned on the same labels, using identical test sets.

### Open Question 2
- Question: What methods can effectively reduce the ~30-point F1 gap between in-domain (19th century Servet-i Funun) and out-of-domain (17th century Ruznamçe) test performance?
- Basis in paper: [explicit] The authors highlight: "The models' mediocre performance on the Ruznamçe test set highlights the need for new methods and the development of more comprehensive datasets to enable effective NER tagging of historical Turkish texts across diverse domains and time periods."
- Why unresolved: The study only reports the gap without proposing or evaluating domain adaptation techniques specific to temporal variation.
- What evidence would resolve it: Systematic evaluation of temporal adaptation methods (e.g., period-stratified training, curriculum learning across centuries, or period-specific adapters) on the same out-of-domain test set.

### Open Question 3
- Question: How much annotated data per historical period is necessary to achieve robust cross-period generalization for dependency parsing and NER?
- Basis in paper: [inferred] The training sets are very small (462 NER sentences; only 114 dependency parsing sentences), and the OTC corpus has uneven distribution across the 15th–20th centuries. The authors explicitly aim to "enrich the OTC corpus evenly across different historical Turkish periods."
- Why unresolved: No experiments examine the relationship between per-period data volume and model generalization; the current data is too sparse and uneven to answer this.
- What evidence would resolve it: A scaling study varying per-period training data size while measuring performance on held-out periods, with controlled annotation quality.

### Open Question 4
- Question: Can adaptive or learned pre-processing methods substantially reduce the need for manual cleaning of OCR/transliteration artifacts in historical Turkish corpora?
- Basis in paper: [inferred] The authors note that regex-based static rules are effective as a baseline but "our ongoing research highlights the need for a more adaptive approach," and manual cleaning remains the final stage for ensuring accuracy.
- Why unresolved: No adaptive or neural pre-processing method is evaluated against the regex baseline.
- What evidence would resolve it: Quantitative comparison of regex-based versus learned (e.g., sequence-to-sequence correction or noise-aware pre-training) pre-processing on downstream task performance and corpus cleanliness metrics.

## Limitations

- Temporal distribution of training data (primarily 19th-century texts) creates significant domain gap with 17th-century test set, resulting in 30-point F1 score difference
- Small size of historical training datasets (462 sentences for NER, 114 for parsing) limits statistical power and may cause overfitting to specific linguistic patterns
- Lack of robust temporal generalization across the 400-year span covered by the corpus, suggesting models have not achieved true historical language understanding

## Confidence

- **High Confidence:** The effectiveness of BERTurk over mBERT for historical Turkish tasks is well-supported by consistent experimental results (6% UAS advantage for parsing, 4-17 F1 advantage for NER across all settings).
- **Medium Confidence:** The assertion that domain-specific fine-tuning data provides disproportionate improvements over larger modern datasets is supported but based on limited data points.
- **Low Confidence:** Claims about the general applicability of these methods to other historical languages are not directly tested and remain speculative.

## Next Checks

1. **Temporal Generalization Test:** Train models on temporally stratified historical Turkish data spanning multiple centuries (15th-20th) and evaluate performance on held-out time periods to quantify how well the models generalize across different historical eras rather than just different document types.

2. **Data Augmentation Experiment:** Test whether synthetic data generation techniques (back-translation, morphological variation augmentation, or cross-temporal alignment) can improve out-of-domain performance on the 17th-century Ruznamçe test set without requiring additional manual annotation.

3. **Error Analysis Validation:** Conduct a detailed error analysis focusing on the 30-point F1 gap between development and out-of-domain test sets, specifically examining whether failures concentrate on particular entity types, morphological patterns, or syntactic constructions that differ between the 19th and 17th centuries.