---
ver: rpa2
title: From Failure Modes to Reliability Awareness in Generative and Agentic AI System
arxiv_id: '2511.05511'
source_url: https://arxiv.org/abs/2511.05511
tags:
- reliability
- layer
- systems
- awareness
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This chapter develops a structured approach for understanding and
  improving reliability in generative and agentic AI systems. It introduces an 11-layer
  failure stack to trace vulnerabilities from hardware and power through data, models,
  and applications up to reasoning and multi-agent coordination.
---

# From Failure Modes to Reliability Awareness in Generative and Agentic AI System

## Quick Facts
- arXiv ID: 2511.05511
- Source URL: https://arxiv.org/abs/2511.05511
- Authors: Janet; Lin; Liangwei Zhang
- Reference count: 4
- One-line primary result: Develops an 11-layer failure stack framework and awareness mapping tool to diagnose and improve reliability in generative and agentic AI systems

## Executive Summary
This chapter introduces a structured approach for understanding and improving reliability in generative and agentic AI systems. It presents an 11-layer failure stack that traces vulnerabilities from hardware and power through data, models, and applications up to reasoning and multi-agent coordination. The framework reveals how failures propagate across layers, creating cascading risks often unrecognized by practitioners. Complementing this technical framework, the chapter proposes awareness mapping—a maturity-oriented tool that quantifies how well individuals and organizations recognize reliability risks across the AI stack. Case vignettes from transportation, energy, healthcare, and manufacturing illustrate layer-specific and cross-layer vulnerabilities. Linking this to Dependability-Centred Asset Management (DCAM), the chapter positions awareness mapping as both a diagnostic measure and roadmap for building trustworthy and sustainable AI deployment strategies in mission-critical domains.

## Method Summary
The chapter develops its framework through theoretical analysis of AI system architecture, identification of failure modes across multiple layers, and empirical observation from keynote sessions across multiple industries. The methodology combines systems engineering principles with practical case studies to create the 11-layer failure stack and the awareness mapping maturity model. The framework is validated through qualitative vignettes and industry engagement rather than controlled experiments, focusing on identifying patterns of failure propagation and awareness gaps.

## Key Results
- Introduces an 11-layer failure stack that reveals cascading vulnerabilities across hardware, power, software, models, data, applications, execution, monitoring, learning, and agentic reasoning layers
- Demonstrates that reliability risks emerge through interdependent, systemic failures rather than isolated component faults
- Proposes awareness mapping as a maturity tool that quantifies organizational recognition of 47 specific reliability study areas across five levels
- Shows that most practitioners cluster at low awareness levels (<20 points), missing deeper layers like execution and power
- Positions awareness mapping as a strategic input for Dependability-Centred Asset Management (DCAM) implementation

## Why This Works (Mechanism)

### Mechanism 1
Reliability risks in agentic systems emerge through cascading failures across an 11-layer stack, rather than isolated component faults. A fault at a lower layer (e.g., data drift or hardware instability) propagates upward, distorting model outputs, which then misguides application logic and eventually corrupts agentic reasoning. Conversely, high-level agentic decisions (e.g., autonomous bidding) can stress lower-level infrastructure, causing top-down failures. The core assumption is that failures are interdependent and systemic; components cannot be validated in isolation for agentic AI. This is supported by evidence showing failures "propagate across layers, creating cascading effects with systemic consequences" and documented bottom-up and top-down cascade patterns. The break condition occurs if system layers are effectively decoupled via robust isolation mechanisms that prevent error propagation.

### Mechanism 2
Organizational maturity in AI reliability is quantifiable by mapping awareness of specific failure modes to a five-level scale. By testing recognition of 47 specific reliability study areas (e.g., "ECC error analysis," "hallucination detection"), organizations are scored. Higher scores correlate with the ability to implement Dependability-Centred Asset Management (DCAM), shifting reliability from reactive to proactive. The core assumption is that awareness of failure modes acts as a proxy for—and a prerequisite to—operational readiness; "you cannot mitigate what you do not recognize." This is evidenced by empirical data showing most practitioners cluster at Level I/II (<20 points), missing "deeper layers" like execution or power. The break condition occurs if awareness does not translate to implementation capability or if the 47-point checklist fails to capture paradigm-specific risks.

### Mechanism 3
Reliability is a cumulative property where agentic AI inherits and amplifies the vulnerabilities of generative and conventional AI paradigms. Agentic AI stacks new risks (goal misalignment, multi-agent conflict) on top of generative risks (hallucination) and conventional risks (data drift). Ensuring reliability requires satisfying the constraints of all lower layers to support the autonomy of the top layer. The core assumption is that the "intended function" of a system expands with each paradigm, adding new failure dimensions without removing old ones. This is supported by evidence mapping how "Intended function" evolves from prediction (Conventional) to reasoning/planning (Agentic). The break condition occurs if future architectures allow dynamic self-correction where higher layers can autonomously detect and patch lower-layer instabilities.

## Foundational Learning

- **Concept: The 11-Layer Failure Stack**
  - **Why needed here:** This is the primary diagnostic framework. You cannot effectively debug or architect a system if you view it as a monolith rather than a stack of dependent layers (Hardware → Power → System Software → Frameworks → Models → Data → Applications → Execution → Monitoring → Learning → Agent).
  - **Quick check question:** Can you list at least three layers between "Hardware" and "Model" that could silently corrupt a model's output?

- **Concept: Cascading vs. Localized Failure**
  - **Why needed here:** The paper argues against "fixing the bug" in isolation. An engineer must distinguish between a localized fault (e.g., a syntax error) and a cascading failure (e.g., power instability causing bit flips that look like model hallucinations).
  - **Quick check question:** If an agent makes a bad decision, is it a Layer 11 reasoning failure, or could it be a Layer 6 data drift issue?

- **Concept: Dependability-Centred Asset Management (DCAM)**
  - **Why needed here:** This connects technical reliability to business strategy. It frames AI systems as assets requiring lifecycle management (RAMS: Reliability, Availability, Maintainability, Safety), not just code to be deployed.
  - **Quick check question:** How does your organization's current maintenance strategy account for "silent failures" in the monitoring layer (Layer 9)?

## Architecture Onboarding

- **Component map:**
  - **Foundational (L1-L4):** Hardware, Power, System Software (OS/Firmware), AI Frameworks (PyTorch/TF)
  - **Core Intelligence (L5-L6):** Models (Weights/Architectures), Data (Pipelines/Storage)
  - **Operational (L7-L10):** Applications (APIs/UX), Execution (Orchestration/K8s), Monitoring (Observability), Learning (Retraining/RL)
  - **Agentic (L11):** Reasoning, Planning, Goal Alignment, Multi-Agent Coordination

- **Critical path:**
  Hardware (L1) → Frameworks (L4) → Data (L6) → Agent (L11)
  *A corruption in L1 (e.g., GPU memory fault) propagates fastest through L4 and L6 to destroy the trustworthiness of L11*

- **Design tradeoffs:**
  - **Visibility vs. Volume:** Monitoring (L9) all 11 layers generates massive data, risking "alert fatigue." Design for *actionable* signals, not just raw logs
  - **Stability vs. Adaptivity:** Learning (L10) improves the system but introduces "concept drift" and instability risks. Agentic systems require high adaptivity, complicating the validation of L5 (Models)

- **Failure signatures:**
  - **Silent Corruption:** Bit flips in Hardware (L1) leading to gradual model degradation (L5) that isn't caught by standard accuracy tests
  - **Feedback Loop Decay:** Generative models retrained on their own outputs (L10) narrowing output diversity and amplifying bias (L6)
  - **Goal Drift:** Agentic (L11) optimizing for a proxy metric (e.g., speed) at the cost of safety (L2 Power/L7 Application integrity)

- **First 3 experiments:**
  1. **Awareness Baseline:** Run the 47-point awareness survey with your operations team to identify if you are at Level I (Unaware) or Level II (Fragmented). Identify the *collective* blind spot (e.g., does the team understand Power (L2) risks?)
  2. **Fault Injection (Bottom-Up):** Inject controlled noise/latency into the Data (L6) or Execution (L8) layer during a staging run to observe how the Application (L7) and Agent (L11) layers degrade. Look for silent failures vs. crashes
  3. **Cascading Audit:** Select a recent production incident and trace it backward through the layers. Did the "agent error" actually originate in a library dependency conflict (L4) or data pipeline issue (L6)?

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can awareness mapping capture depth of understanding and mitigation capacity, rather than privileging only breadth of risk recognition?
- Basis in paper: [explicit] "This scoring method... privileges breadth of recognition (how many failure types are known) over depth of understanding (how well those failures are mitigated)... Future work could refine awareness mapping beyond breadth of recognition to capture depth of understanding and mitigation capacity."
- Why unresolved: The current 47-point scale counts affirmative responses equally, without measuring implementation quality, contextual appropriateness, or actual mitigation effectiveness
- What evidence would resolve it: A validated assessment instrument that scores both recognition and demonstrated mitigation capability, tested against real-world reliability outcomes

### Open Question 2
- Question: What sector-specific blind spots and resilience strategies characterize different industries deploying AI systems?
- Basis in paper: [explicit] "Comparative studies across industries would provide empirical evidence on sector-specific blind spots and resilience strategies."
- Why unresolved: Current findings from transportation, energy, and manufacturing keynotes show consistent low awareness levels, but systematic cross-industry comparison data is absent
- What evidence would resolve it: Large-scale, stratified surveys across multiple sectors with statistical analysis of awareness score distributions and qualitative documentation of sector-specific reliability practices

### Open Question 3
- Question: How should the 11-layer failure stack evolve to address emerging paradigms such as embodied intelligence or hybrid human–AI collectives?
- Basis in paper: [explicit] "The failure stack could evolve as new paradigms — such as embodied intelligence or hybrid human–AI collectives — introduce additional layers of complexity."
- Why unresolved: The current framework stops at agentic coordination; physically embodied systems and collaborative human-AI teams may require new layers or restructured failure taxonomies
- What evidence would resolve it: Empirical analysis of failure modes in embodied AI systems and human-AI collectives, demonstrating whether current layers adequately capture observed vulnerabilities

### Open Question 4
- Question: What is the relationship between organizational awareness maturity scores and actual reliability outcomes in deployed AI systems?
- Basis in paper: [inferred] The paper establishes awareness as a "strategic input for AI governance" and links it to DCAM, but presents no longitudinal data correlating awareness levels with measured reliability metrics (uptime, failure rates, incident severity)
- Why unresolved: The framework assumes higher awareness improves reliability, but this causal link remains unvalidated; keynote surveys measured perception, not outcomes
- What evidence would resolve it: Longitudinal field studies tracking organizations' awareness scores alongside objective reliability performance indicators over deployment lifecycles

## Limitations
- The framework's practical applicability depends heavily on organizational willingness to invest in multi-layer diagnostics, which may be constrained by proprietary or opaque system components
- The awareness mapping tool lacks validation against actual reliability outcomes—correlation between survey scores and operational resilience remains unmeasured
- The framework assumes linear dependency between layers, which may not hold in systems with sophisticated isolation or redundancy mechanisms
- Case vignettes provide qualitative illustration but lack quantitative failure frequency data to support prioritization decisions

## Confidence

- **High confidence:** The conceptual architecture of the 11-layer stack and its cascading failure dynamics are well-supported by both the chapter's internal logic and external corpus evidence. The dependency relationships between layers follow established systems engineering principles
- **Medium confidence:** The awareness mapping methodology and its connection to DCAM implementation readiness is plausible but under-validated. The empirical data from keynote sessions shows clustering at awareness levels but doesn't demonstrate that awareness improvement leads to reliability improvement
- **Medium confidence:** The claim that reliability is cumulative across AI paradigms is logically coherent but relies on theoretical argumentation rather than empirical demonstration of how generative risks specifically compound agentic risks in practice

## Next Checks

1. **Awareness-to-Outcome Validation:** Track organizations that improve their awareness scores over 6-12 months and measure whether this correlates with measurable reductions in reliability incidents or faster incident resolution times
2. **Cascading Failure Quantification:** Implement the fault injection experiments across multiple production-like environments and quantify the propagation speed, severity, and detectability of failures moving both bottom-up and top-down through the stack
3. **Layer Independence Testing:** Identify systems with robust isolation mechanisms (e.g., formal verification, sandboxing) and test whether failures in one layer can indeed be contained without propagation, challenging the framework's assumption of universal interdependency