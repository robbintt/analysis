---
ver: rpa2
title: 'Strict Subgoal Execution: Reliable Long-Horizon Planning in Hierarchical Reinforcement
  Learning'
arxiv_id: '2506.21039'
source_url: https://arxiv.org/abs/2506.21039
tags:
- subgoal
- goal
- high-level
- learning
- exploration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Strict Subgoal Execution (SSE), a hierarchical
  reinforcement learning framework that enforces single-step subgoal reachability
  to improve long-horizon goal-conditioned task performance. The key idea is to constrain
  the high-level policy so that each subgoal must be reached within a single decision
  step, removing fixed step limits and enabling more efficient planning.
---

# Strict Subgoal Execution: Reliable Long-Horizon Planning in Hierarchical Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2506.21039
- **Source URL**: https://arxiv.org/abs/2506.21039
- **Reference count**: 40
- **Primary result**: SSE outperforms existing HRL methods on 9 benchmark tasks with higher success rates and sample efficiency

## Executive Summary
Strict Subgoal Execution (SSE) is a hierarchical reinforcement learning framework that enforces single-step subgoal reachability to improve long-horizon goal-conditioned task performance. The key innovation is constraining the high-level policy so each subgoal must be reached within a single decision step, removing fixed step limits and enabling more efficient planning. SSE combines this with a decoupled exploration policy that systematically covers under-explored regions and a failure-aware path refinement strategy that adjusts graph edge costs based on observed low-level failure statistics. Across benchmark environments including AntMaze, KeyChest, and Reacher tasks, SSE consistently achieves higher success rates and sample efficiency while requiring fewer high-level decisions.

## Method Summary
SSE constructs a grid-based landmark graph over the goal space and uses Dijkstra's algorithm to compute waypoint paths. The high-level policy selects subgoals that must be reached within a single decision step, with episode termination on failure. A decoupled exploration policy samples from underexplored grid cells, max-value subgoals, and the final goal to accelerate goal-space coverage. The low-level policy executes waypoint sequences, with success/failure outcomes determining high-level rewards. Failure-aware path refinement dynamically adjusts edge costs based on spatial failure statistics, steering planning away from unreliable transitions. The framework uses separate TD3-based policy/critic pairs for high-level and low-level components, with HER-style relabeling for partial successes.

## Key Results
- Achieves 4-17% absolute improvement in success rates across 9 benchmark environments
- Demonstrates 2-3x sample efficiency gains over HIRO and HIGL baselines
- Successfully solves complex multi-objective tasks like AntDoubleKeyChest where baselines fail
- Shows consistent performance across 5 random seeds with standard deviation reported

## Why This Works (Mechanism)

### Mechanism 1: Strict Subgoal Execution via Early Termination
SSE removes fixed step limits for low-level execution. When a subgoal is selected, the low-level policy executes until either the agent reaches the subgoal (storing a positive high-level transition) or fails within episode horizon (terminating with zero reward). This binary outcome eliminates partial-progress noise that destabilizes high-level Q-learning in conventional HRL.

### Mechanism 2: Decoupled Exploration Policy for Goal-Space Coverage
SSE employs a separate exploration policy (π_exp) that samples subgoals from underexplored grid cells, max-value subgoals, and the final goal. This systematically covers goal space faster than reward-driven high-level policy alone, with usage decaying from η=1 to target ratio, transitioning control to the learned high-level policy.

### Mechanism 3: Failure-Aware Path Refinement via Edge Cost Adjustment
SSE dynamically increases graph edge costs in regions with high subgoal failure rates. For each grid cell, the failure ratio is computed and edge costs are scaled, penalizing paths through failure-prone regions. This steers Dijkstra's planning away from unreliable transitions, improving subgoal success rates.

## Foundational Learning

- **Concept: Goal-Conditioned Reinforcement Learning (GCRL)**
  - Why needed here: SSE is built on goal-conditioned MDPs where policies condition on both state and goal. Understanding UVFA and goal relabeling (e.g., HER) is essential for interpreting SSE's high-level Q-learning with hindsight-style virtual subgoal labeling.
  - Quick check: Can you explain how goal relabeling transforms a failed trajectory for goal g into a successful trajectory for an alternative goal g'?

- **Concept: Hierarchical Reinforcement Learning (HRL) with Two-Level Decomposition**
  - Why needed here: SSE separates π_h (high-level subgoal selection) and π_l (low-level waypoint execution). The paper assumes familiarity with temporal abstraction and high-level decision frequency.
  - Quick check: In standard HRL, if the high-level policy operates every k=10 steps and the low-level policy fails to reach its subgoal, what happens to the next high-level decision? How does SSE differ?

- **Concept: Graph-Based Planning with Dijkstra's Algorithm**
  - Why needed here: SSE constructs a landmark graph over goal space and uses Dijkstra to compute waypoint paths. Edge costs combine low-level value estimates and Euclidean distances.
  - Quick check: Given a graph with 5 nodes and edges weighted by log_γ(1 + (1-γ)Q_G(v1, v2, π_l)), how would Dijkstra find the shortest path from node 1 to node 5?

## Architecture Onboarding

- **Component map**: Graph Construction -> High-Level Policy (π_h, Q_h) -> Exploration Policy (π_exp) -> Path Refinement -> Low-Level Policy (π_l, Q_l) -> TD3 Training

- **Critical path**: Episode starts → π_exp or π_h selects subgoal → Dijkstra computes waypoint path using refined edge costs → π_l sequentially reaches waypoints → if subgoal reached, store positive transition; else terminate with zero reward and increment N_fail → after episode, update N(C_m^G) for visited cells → train Q_h, π_h, Q_l, π_l via TD3

- **Design tradeoffs**: Grid size d_G=2 optimal (finer grids increase planning cost); exploration ratio η=0.2 balances coverage and convergence; failure scaling c_dist=5 robust across tasks; high-level discount γ_h=0.4 encourages shorter paths

- **Failure signatures**: 
  1. All-zero high-level rewards: low-level policy cannot reach subgoals → check grid coverage, increase π_exp ratio early
  2. Success on simple mazes but failure on bottlenecks: refinement not updating edge costs → verify N_fail accumulation
  3. High variance across seeds: Q_h unstable due to sparse rewards → check buffer balance (50/50 split)

- **First 3 experiments**: 
  1. AntMaze U-Maze (fixed-goal): verify basic SSE learning vs HIRO/HIGL baselines (~200K steps)
  2. AntMazeBottleneck: test failure-aware path refinement; visualize edge cost changes around bottleneck
  3. AntDoubleKeyChest (multi-objective): validate high-level planning with intermediate objectives; ablate π_exp to confirm exploration contribution

## Open Questions the Paper Calls Out

- **Open Question 1**: Can SSE be extended to support multiple distinct low-level skills for tasks requiring composite actions, such as robotic manipulation? The authors note the current framework is restricted to reachability tasks and suggest future work should address "robotic manipulation involving grasping and relocating objects, where multiple skill primitives may be necessary."

- **Open Question 2**: How can SSE's grid-based partitioning and exploration strategy be replaced to handle high-dimensional goal spaces efficiently? The paper acknowledges that grid discretization becomes inefficient in high-dimensional settings due to the curse of dimensionality, restricting its applicability.

- **Open Question 3**: How sensitive is the failure-aware path refinement mechanism to non-stationary environments where obstacle layouts or dynamics change over time? The method relies on accumulating failure statistics, which may persist and prevent the agent from discovering new optimal paths in dynamic environments.

## Limitations

- Network architectures for the four TD3-based components (π_h, π_exp, π_l, Q_h, Q_l) are not specified, creating potential reproducibility gaps
- State mapping function φ(s) extracting goal-space representations from full observations is assumed but not detailed
- Exploration ratio η decay schedule is described without defining what constitutes an iteration
- Claims about computational efficiency and sample complexity advantages lack direct comparisons of wall-clock time or sample requirements

## Confidence

- **High Confidence**: The core algorithmic framework (graph construction, Dijkstra planning, failure-aware path refinement, strict subgoal termination) is clearly specified with mathematical formulations
- **Medium Confidence**: The claimed performance improvements over baselines (4-17% absolute gains) are supported by ablation studies and multiple seeds, though implementation specifics introduce uncertainty
- **Low Confidence**: Claims about computational efficiency and sample complexity advantages relative to prior work lack direct comparisons, making these claims harder to verify independently

## Next Checks

1. **Subgoal Reachability Validation**: Implement a diagnostic tool to measure the percentage of subgoals that are physically reachable within the episode horizon, independent of policy performance

2. **Exploration Coverage Analysis**: Track and visualize the spatial coverage of π_exp over training, measuring the percentage of grid cells visited to validate whether decoupled exploration effectively prevents premature convergence

3. **Failure Ratio Calibration**: Systematically vary the failure scaling factor c_dist and measure its impact on both path efficiency (detour lengths) and success rates to validate whether failure-aware refinement strikes an appropriate balance