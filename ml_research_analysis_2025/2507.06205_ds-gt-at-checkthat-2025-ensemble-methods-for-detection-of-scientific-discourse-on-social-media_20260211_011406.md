---
ver: rpa2
title: 'DS@GT at CheckThat! 2025: Ensemble Methods for Detection of Scientific Discourse
  on Social Media'
arxiv_id: '2507.06205'
source_url: https://arxiv.org/abs/2507.06205
tags:
- scientific
- category
- llms
- tweet
- categories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes the DS@GT team's approach to CLEF 2025 CheckThat!
  Task 4a, which involves detecting scientific discourse on social media.
---

# DS@GT at CheckThat! 2025: Ensemble Methods for Detection of Scientific Discourse on Social Media

## Quick Facts
- **arXiv ID:** 2507.06205
- **Source URL:** https://arxiv.org/abs/2507.06205
- **Reference count:** 17
- **Primary result:** Achieved macro-averaged F1 score of 0.8611, ranking 7th in CLEF 2025 CheckThat! Task 4a

## Executive Summary
This paper presents the DS@GT team's approach to detecting scientific discourse on social media for CLEF 2025 CheckThat! Task 4a. The team explored three approaches: transformer fine-tuning, large language model (LLM) prompting, and an ensemble combining both. The ensemble method used fine-tuned transformers for scientific claims and entity mentions, while employing few-shot LLM prompting with semantic similarity retrieval for scientific references. The approach achieved strong performance with a macro-averaged F1 score of 0.8611, outperforming the DeBERTaV3 baseline and securing 7th place in the competition.

## Method Summary
The DS@GT team developed an ensemble approach that combined transformer fine-tuning with LLM prompting strategies. The ensemble utilized fine-tuned transformers specifically for detecting scientific claims and entity mentions in social media posts. For scientific references, the system employed few-shot LLM prompting combined with semantic similarity retrieval mechanisms. This hybrid approach leveraged the strengths of both traditional fine-tuning methods and the few-shot capabilities of large language models, creating a robust system for scientific discourse detection across multiple dimensions.

## Key Results
- Achieved macro-averaged F1 score of 0.8611 on CLEF 2025 CheckThat! Task 4a
- Outperformed DeBERTaV3 baseline with score of 0.8375
- Ranked 7th place in the competition
- Demonstrated effectiveness of ensemble approach combining fine-tuned transformers and LLM prompting

## Why This Works (Mechanism)
The ensemble approach succeeds by leveraging complementary strengths of different modeling paradigms. Fine-tuned transformers excel at capturing domain-specific patterns in scientific claims and entity mentions through specialized training on relevant data. The LLM prompting component with semantic similarity retrieval effectively handles the more complex task of identifying scientific references, benefiting from the few-shot learning capabilities of large language models. By combining these approaches, the system can address the multifaceted nature of scientific discourse detection, where different subtasks require different modeling strengths.

## Foundational Learning
- **Transformer Fine-tuning**: Why needed - To adapt pre-trained models to scientific discourse domain; Quick check - Verify performance on domain-specific validation set
- **Few-shot LLM Prompting**: Why needed - To leverage large language models with minimal labeled examples; Quick check - Test consistency across different prompt variations
- **Semantic Similarity Retrieval**: Why needed - To identify relevant scientific references through semantic matching; Quick check - Evaluate retrieval accuracy on reference matching task
- **Ensemble Methods**: Why needed - To combine strengths of multiple modeling approaches; Quick check - Compare ensemble performance against individual component baselines
- **Scientific Discourse Detection**: Why needed - To identify scientific claims, entities, and references in social media; Quick check - Validate on diverse scientific topics and platforms
- **Macro-averaged F1 Score**: Why needed - To evaluate balanced performance across all detection classes; Quick check - Calculate per-class F1 scores to identify potential imbalances

## Architecture Onboarding

**Component Map:** Transformer Fine-tuning -> LLM Prompting with Semantic Similarity -> Ensemble Integration

**Critical Path:** The critical path involves sequential processing where transformer models first process scientific claims and entity mentions, while LLM prompting handles scientific references in parallel, with final ensemble integration combining these outputs for final predictions.

**Design Tradeoffs:** The ensemble approach trades computational complexity and model management overhead for improved accuracy by combining specialized models. This approach requires maintaining multiple model pipelines but provides flexibility to use optimal techniques for different detection tasks.

**Failure Signatures:** Potential failures include inconsistent performance across different scientific domains, sensitivity to prompt engineering variations in the LLM component, and degradation when encountering scientific discourse outside the training distribution. The ensemble may also suffer from integration issues if component predictions are not well-calibrated.

**First Experiments:** 1) Run ablation study to isolate contributions of fine-tuned transformers versus LLM prompting; 2) Test model performance on external scientific discourse dataset from different social media platform; 3) Conduct sensitivity analysis on few-shot prompt examples across various scientific domains.

## Open Questions the Paper Calls Out
None

## Limitations
- Task-specific optimization may limit generalizability to other scientific discourse detection scenarios
- Performance heavily depends on quality and representativeness of training data
- Few-shot LLM prompting may exhibit inconsistent performance across different scientific domains
- No detailed analysis of class imbalance or potential biases in training data

## Confidence
- **High** confidence in reported F1 score improvement over DeBERTaV3 baseline (direct comparison on same task)
- **Medium** confidence in ensemble methodology's general effectiveness (lacks ablation studies)
- **Low** confidence in cross-domain generalization (no external dataset validation)

## Next Checks
1. Conduct ablation studies to quantify individual contributions of fine-tuned transformers versus LLM prompting to ensemble performance
2. Evaluate model on external dataset of scientific discourse from different social media platforms to assess generalizability
3. Perform sensitivity analysis on few-shot prompt examples to determine robustness to prompt variations and scientific domain shifts