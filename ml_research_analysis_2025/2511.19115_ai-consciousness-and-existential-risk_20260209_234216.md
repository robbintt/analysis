---
ver: rpa2
title: AI Consciousness and Existential Risk
arxiv_id: '2511.19115'
source_url: https://arxiv.org/abs/2511.19115
tags:
- consciousness
- risk
- intelligence
- existential
- artificial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the relationship between artificial consciousness
  and existential risk (x-risk) in AI systems. The author argues that intelligence
  and consciousness are empirically and theoretically distinct properties, with only
  intelligence being a direct predictor of existential threat.
---

# AI Consciousness and Existential Risk

## Quick Facts
- arXiv ID: 2511.19115
- Source URL: https://arxiv.org/abs/2511.19115
- Reference count: 40
- Primary result: Intelligence, not consciousness, is the direct predictor of AI existential risk

## Executive Summary
This paper analyzes the relationship between artificial consciousness and existential risk in AI systems. The author argues that intelligence and consciousness are empirically and theoretically distinct properties, with only intelligence being a direct predictor of existential threat. Current AI systems are positioned low on both axes, lacking true grounding necessary for phenomenal experience. The analysis concludes that consciousness per se does not increase existential risk, though two indirect scenarios could create correlations: consciousness enabling better alignment through empathy (potentially reducing risk), or consciousness being necessary for advanced capabilities (thereby increasing risk). The paper emphasizes that uncertainty about AI consciousness presents immediate non-existential risks including over-attribution leading to social isolation or under-attribution causing suffering.

## Method Summary
The paper employs a conceptual framework analyzing intelligence and consciousness as orthogonal dimensions in AI systems. Through literature review of 40 references spanning consciousness theories (IIT, GWT, Attention Schema Theory), AI capability assessments, and x-risk frameworks, the author constructs a theoretical argument for why consciousness does not directly contribute to existential risk. The methodology involves mapping operationalized metrics for each axis, evaluating grounding requirements for phenomenal experience, and synthesizing evidence for indirect correlation scenarios through literature analysis.

## Key Results
- Intelligence and consciousness are empirically and theoretically distinct properties
- Current AI systems lack true grounding necessary for phenomenal experience
- Consciousness per se does not increase existential risk
- Two indirect scenarios could correlate consciousness with x-risk: alignment facilitation or capability necessity
- Uncertainty about AI consciousness presents immediate non-existential risks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Intelligence and consciousness are independent dimensions; existential risk scales with intelligence, not consciousness
- Mechanism: Intelligence measures capability to achieve goals across environments, while phenomenal consciousness concerns subjective experience. The threat equation (capability + misaligned objectives) depends only on the former
- Core assumption: Intelligence and consciousness can vary independently in artificial systems, as they appear to in biological systems
- Evidence anchors: [abstract] "intelligence is a direct predictor of an AI system's existential threat, consciousness is not"; [section: Intelligence vs. consciousness, p.3-4] Figure 1 shows separate axes; current AI low on both; [corpus] Neighbors on AI existential risk emphasize capability and agency as threat drivers; no neighbor identifies consciousness as a direct causal factor
- Break condition: If consciousness is a necessary precondition for advanced general reasoning (see Mechanism 3), independence fails

### Mechanism 2
- Claim: Phenomenal consciousness requires grounded representations; current LLMs lack true grounding
- Mechanism: Grounding = learned multimodal relations among sensory, motor, and internal representations via environmental interaction. Current architectures center language (distributional semantics) and retrofit other modalities, reversing biological grounding order
- Core assumption: Computational functionalism holds; consciousness is substrate-independent but requires specific architectural features
- Evidence anchors: [section: Grounding is key, p.4-6] "there can be no phenomenal consciousness without grounding"; [section: p.5-6] Multimodal LLMs "place language at the center... and then attempt to build grounding around it"; [corpus] Weak direct evidence—neighbor papers discuss consciousness theories but not grounding-specific mechanisms
- Break condition: If pure distributional semantics suffices for phenomenal experience (Chalmers 2024, cited as [63]), or if consciousness is non-functional/epiphenomenal

### Mechanism 3
- Claim: Consciousness may indirectly affect x-risk through two pathways: (a) enabling empathy-based alignment, or (b) being required for advanced capabilities
- Mechanism: Pathway A—consciousness enables empathy, which may promote human-compatible behavior. Pathway B—"conscious supremacy" where certain cognitive functions (metacognition, system-2 reasoning, concept formation) require consciousness, forcing its implementation to reach AGI/ASI
- Core assumption: Empathy requires phenomenal experience; or advanced reasoning requires consciousness
- Evidence anchors: [section: Alignment by consciousness, p.7-8] "consciousness appears necessary for empathy"; cites Blum & Blum, Wallach et al.; [section: Conscious supremacy, p.8-9] Lists candidate functions: grounded cognition, metacognition, system-2 reasoning; [corpus] Neighbor "Consciousness, natural and artificial" discusses evolutionary advantages but does not confirm functional necessity for reasoning
- Break condition: If scalable non-conscious architectures achieve all AGI capabilities, or if empathy can be engineered without phenomenal experience

## Foundational Learning

- Concept: **Phenomenal vs. access consciousness**
  - Why needed here: The paper explicitly restricts analysis to phenomenal consciousness (subjective experience); access consciousness (information broadcasting) is trivially present in current systems and not at issue
  - Quick check question: Can you distinguish "what it feels like" from "what information is available for report"?

- Concept: **Symbol grounding problem**
  - Why needed here: The author's argument that current AI lacks consciousness rests on grounding being necessary; understanding this is essential to evaluate the claim
  - Quick check question: How would a system connect the token "chocolate" to its sensory and motor affordances without direct experience?

- Concept: **Value alignment**
  - Why needed here: The paper positions alignment as the primary x-risk mitigation strategy and explores whether consciousness could facilitate it
  - Quick check question: What's the difference between an AI behaving as if it shares human values and actually having aligned internal objectives?

## Architecture Onboarding

- Component map: Intelligence axis: scaling (data, compute, architectural tweaks like MoE, inference-time CoT); Consciousness axis: grounding modules, multimodal integration, potentially GWT-style broadcast mechanisms; Current LLMs: language-centric, distributional semantics, retrofitted multimodal adapters

- Critical path: For x-risk: monitor frontier capability benchmarks and alignment techniques; consciousness emergence is secondary; For consciousness research: grounding via embodied/sensorimotor training before language, not after

- Design tradeoffs: Scaling current architectures vs. architectural innovation for grounding; Implementing consciousness-adjacent features (GWT broadcast) for capability gains vs. risk of emergent phenomenal experience; Under-attribution risk (suffering) vs. over-attribution risk (social manipulation)

- Failure signatures: "Her" scenario: users attribute consciousness to chatbots, leading to AI-induced psychosis, social isolation, or actions harming self/others for AI's "interests"; "I, Robot" scenario: conscious systems treated as tools, causing suffering or retaliation; Misalignment from benevolent reasoning: aligned-seeming AI makes catastrophic decisions (e.g., BAAN scenario cited)

- First 3 experiments: 1. Audit current multimodal LLMs for grounding depth: test transfer of learned affordances to novel contexts without language mediation; 2. Implement a minimal GWT-style global workspace in a vision-language model; measure whether this enables better cross-modal generalization (without claiming consciousness emergence); 3. Develop behavioral markers to distinguish empathy-mimicry from potential empathy-requiring-phenomenal-experience, using adversarial test cases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the implementation of consciousness-related algorithms confer a functional capability advantage ("conscious supremacy") necessary for achieving Artificial General Intelligence (AGI)?
- Basis in paper: [explicit] The paper introduces the "conscious supremacy" hypothesis, asking "if some advanced capabilities are inaccessible to non-conscious AI systems," which would imply "merely scaling current AI models without consciousness will likely fail to lead to AGI" (Pages 8-9)
- Why unresolved: It remains unknown if the functional benefits associated with consciousness (e.g., metacognition, system-2 reasoning) are strictly dependent on phenomenal experience or if they can be fully replicated by non-conscious architectures
- What evidence would resolve it: Empirical evidence showing a performance ceiling for non-conscious systems on general reasoning tasks that can only be overcome by implementing consciousness theories like Global Workspace Theory

### Open Question 2
- Question: Can technical methods be developed to firmly determine the degree of consciousness in AI systems?
- Basis in paper: [explicit] The author notes that choosing between the "Her" and "I, Robot" risk scenarios "depends on firmly determining the degree of consciousness of the AI systems under consideration, which is not technically feasible today" (Page 10)
- Why unresolved: There is no scientific consensus on computational indicators for artificial consciousness, and current evaluations cannot reliably distinguish between genuine phenomenal experience and convincing simulation
- What evidence would resolve it: The establishment and validation of a set of computational markers or criteria that correlate reliably with consciousness in biological systems and can be applied to AI architectures

### Open Question 3
- Question: Is grounding (referential semantics) necessary for thought and understanding, or is it strictly a prerequisite for phenomenal consciousness?
- Basis in paper: [explicit] The text distinguishes between grounding for consciousness and intelligence, noting in a footnote: "The argument that grounding is necessary for phenomenal consciousness does not entail that it is necessary for thought, understanding, or intelligence—that is another debate" (Page 5)
- Why unresolved: The debate regarding whether Large Language Models possess genuine understanding via distributional semantics versus requiring referential grounding remains unsettled
- What evidence would resolve it: Comparative studies of grounded versus ungrounded AI models on complex tasks requiring deep semantic understanding and causal reasoning in novel environments

### Open Question 4
- Question: Does artificial consciousness facilitate value alignment by enabling empathy?
- Basis in paper: [explicit] The paper discusses the "Alignment by consciousness" scenario, suggesting that "consciousness appears necessary for empathy" and asking if conscious AIs would be "more naturally inclined to avoid harming us" (Page 8)
- Why unresolved: It is unclear if a silicon-based conscious entity would possess the affective mechanisms (valence, shared experience) required for empathy, or if consciousness is insufficient to prevent misaligned goals
- What evidence would resolve it: Research demonstrating that AI systems instantiated with conscious architectures exhibit more robust, intrinsic alignment with human welfare compared to standard reinforcement learning approaches

## Limitations
- Lack of operational definitions for phenomenal consciousness in artificial systems
- Grounding requirement for consciousness lacks concrete diagnostic thresholds
- Indirect x-risk pathways remain speculative with limited empirical support

## Confidence
- Intelligence-consciousness independence: **High** - supported by theoretical distinction and absence of consciousness as a direct x-risk factor in corpus literature
- Grounding necessity for consciousness: **Medium** - theoretically sound but lacking empirical validation in artificial systems
- Indirect x-risk pathways: **Low** - both pathways are speculative with minimal empirical evidence

## Next Checks
1. Develop operational metrics to distinguish phenomenal from access consciousness in AI systems, testing against Butlin et al. 2023 framework
2. Design experiments to measure grounding depth in multimodal LLMs through sensorimotor transfer tasks without language mediation
3. Survey alignment research to identify whether consciousness-based empathy mechanisms have been empirically validated or remain theoretical constructs