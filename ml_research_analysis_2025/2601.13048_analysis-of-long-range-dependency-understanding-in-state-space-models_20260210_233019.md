---
ver: rpa2
title: Analysis of Long Range Dependency Understanding in State Space Models
arxiv_id: '2601.13048'
source_url: https://arxiv.org/abs/2601.13048
tags:
- kernel
- state
- kernels
- performance
- spectral
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents the first systematic interpretability study
  of State Space Models (SSMs), specifically analyzing the diagonalized variant S4D.
  The study focuses on understanding how different model architectures affect the
  kernel behavior of S4D when applied to vulnerability detection in source code.
---

# Analysis of Long Range Dependency Understanding in State Space Models

## Quick Facts
- arXiv ID: 2601.13048
- Source URL: https://arxiv.org/abs/2601.13048
- Authors: Srividya Ravikumar; Abhinav Anand; Shweta Verma; Mira Mezini
- Reference count: 0
- Key outcome: First systematic interpretability study of S4D showing standalone kernels capture short-range dependencies, while SMR+S4D achieves long-range modeling with F1=88.03

## Executive Summary
This work presents the first systematic interpretability study of State Space Models (SSMs), specifically analyzing the diagonalized variant S4D. The study focuses on understanding how different model architectures affect the kernel behavior of S4D when applied to vulnerability detection in source code. Through time and frequency domain analysis of learned kernels, the authors demonstrate that standalone S4D kernels predominantly capture short-range dependencies, while hybrid architectures with convolutional layers exhibit varying spectral characteristics. Notably, the State Memory Replay (SMR) + S4D model shows a low-pass filter behavior with emphasis on long-range dependencies, achieving the best performance with an F1 score of 88.03%.

## Method Summary
The study evaluates six S4D-based architectures on binary vulnerability detection in source code using the ReVeal dataset. Models share a common architecture (embedding → feature extraction → ReLU → max pooling → concatenation → dropout → FC layer) but differ in their feature extraction blocks. The authors extract learned S4D kernels post-training and analyze them using FFT-based frequency-domain analysis, computing power spectral density and spectral entropy to characterize whether kernels act as low-pass, band-pass, or high-pass filters. Training uses BCEWithLogitsLoss with class weights, Adam optimizer (lr=1e-3), batch size 64, for 10 epochs.

## Key Results
- Standalone S4D kernels exhibit high dominant frequencies (~0.18 cycles/sample) and high spectral entropy (H=3.94), indicating short-range dependency capture
- SMR+S4D achieves best performance (F1=88.03%) with low-pass kernel characteristics (dominant frequency ~0.02 cycles/sample, entropy H=3.79)
- Hybrid CNN-SSM architectures can achieve implicit band-limiting effects without explicit constraints
- Kernel spectral properties directly correlate with dependency range modeling capability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: S4D kernel spectral characteristics determine dependency range modeling capability
- Mechanism: The learned S4D kernel behaves as a filter whose frequency response determines whether the model captures short-range or long-range dependencies. Low dominant frequency with low spectral entropy indicates long-range dependency capture; high dominant frequency indicates short-range focus.
- Core assumption: Kernel frequency-domain properties directly correspond to the temporal range of dependencies the model can effectively utilize.
- Evidence anchors:
  - [abstract] "we show that the depending on the architecture, S4D kernel can behave as low-pass, band-pass or high-pass filter"
  - [section 3] "the standalone S4D model produced oscillatory kernels... with an entropy of H=3.94... indicates sensitivity to short- to mid-range transitions"
  - [section 3] "SMR+S4D kernels had... a very low dominant frequency of ≈0.02 cycles/sample (≈50 tokens). With the lowest entropy (H=3.79), this model displayed a low-pass profile emphasizing smooth long-range dependencies"
  - [corpus] Related work "Uncovering the Spectral Bias in Diagonal State Space Models" corroborates spectral bias concerns in diagonal SSMs

### Mechanism 2
- Claim: State Memory Replay (SMR) preceding S4D enables long-range dependency capture that standalone S4D cannot achieve
- Mechanism: SMR multiplies Conv1D outputs element-wise with original embeddings before passing to S4D. This fusion preserves local structure while improving contextual memory recall, inducing the S4D kernel to learn low-pass characteristics (dominant frequency ~0.02 cycles/sample vs ~0.18 for standalone).
- Core assumption: The multiplicative fusion of local convolutional features with raw embeddings creates a representation that biases the S4D layer toward integrating information across extended token ranges.
- Evidence anchors:
  - [abstract] "the State Memory Replay (SMR) + S4D model shows a low-pass filter behavior with emphasis on long-range dependencies, achieving the best performance with an F1 score of 88.03%"
  - [section 2.1] "This fusion mechanism improves the ability of the model to recall contextual memory while preserving local structure"
  - [section 3] "our analysis reveals that when S4D is preceded by a SMR block, the S4D kernel can capture long-range dependency and the performance of the model improves while the standalone S4D layer cannot capture long-range dependency"
  - [corpus] No direct corpus validation for SMR mechanism specifically; related papers focus on other architectural modifications

### Mechanism 3
- Claim: Hybrid CNN-SSM architectures achieve implicit band-limiting effects that improve long-range modeling without explicit constraints
- Mechanism: Preceding S4D with convolutional layers shapes the input distribution, which in turn affects what the S4D kernel learns. The CNN acts as a pre-filter, reducing spectral leakage that would otherwise cause the S4D kernel to distribute energy across irrelevant frequencies.
- Core assumption: The input transformation from preceding layers substantially influences the spectral properties the S4D kernel learns to emphasize.
- Evidence anchors:
  - [section 3] "Our analysis suggests that hybrid CNN–SSM architectures can achieve similar band-limiting effects without explicit constraints"
  - [section 3] References S4ND finding that "band-limiting high-frequency components improved performance"
  - [section 4] "Our result is complementary to the previous work [10], which proves theoretically that the performance of SSM improves if SSM layer is preceded by feedforward layer"
  - [corpus] "Block-Biased Mamba for Long-Range Sequence Processing" discusses architectural modifications for long-range modeling, providing indirect support

## Foundational Learning

- Concept: State Space Models and the S4D diagonal parameterization
  - Why needed here: Understanding how S4D generates convolutional kernels in the frequency domain is essential for interpreting the spectral analysis methodology and why kernel properties matter for dependency modeling.
  - Quick check question: Can you explain why diagonal parameterization simplifies kernel generation compared to the original S4 formulation?

- Concept: Power Spectral Density (PSD) and Spectral Entropy
  - Why needed here: The paper's core analysis relies on interpreting PSD distributions and spectral entropy values to characterize whether kernels act as low-pass, band-pass, or high-pass filters.
  - Quick check question: What does low spectral entropy (H=3.79 vs H=4.21) indicate about the concentration of spectral energy in a kernel?

- Concept: Signal processing perspective on long-range dependencies
  - Why needed here: The paper frames long-range dependency modeling through a signal processing lens—long-range-dependent processes concentrate energy in low frequencies with lower entropy rates.
  - Quick check question: Why would a low-pass filter kernel be preferable for capturing dependencies spanning ~50 tokens versus ~5-6 tokens?

## Architecture Onboarding

- Component map:
  Embedding layer -> Feature extraction block (Conv1D, Depthwise, S4D, or hybrids) -> ReLU -> Max Pooling -> Concatenation -> Dropout (0.5) -> FC layer -> Binary classification

- Critical path:
  1. Choose feature extraction architecture based on expected dependency range requirements
  2. Train on target task with BCEWithLogitsLoss (class-weighted for imbalanced data)
  3. Extract learned S4D kernels post-training
  4. Compute FFT to obtain frequency representation
  5. Calculate normalized PSD and spectral entropy
  6. Correlate spectral properties with performance metrics

- Design tradeoffs:
  - Standalone S4D: Simpler architecture but empirically captures only short-range dependencies despite theoretical capacity
  - SMR+S4D: Best performance (F1=88.03) but adds complexity through multiplicative fusion
  - Conv+S4D: Sharpest temporal responses but highest spectral entropy (H=4.21), suggesting less focused dependency modeling
  - Depthwise+S4D: Parameter-efficient band-pass profile, competitive performance (F1=87.17)

- Failure signatures:
  - High dominant frequency (>0.15 cycles/sample) with high spectral entropy: Model likely failing to capture task-relevant long-range dependencies
  - Oscillatory kernel with sharp transitions: Indicates short-range focus when long-range modeling is needed
  - Standalone S4D underperforming CNN baseline: Signal that architectural modification is required for dependency range

- First 3 experiments:
  1. Replicate standalone S4D vs CNN baseline comparison on your dataset; extract and visualize kernels to confirm short-range behavior pattern
  2. Implement SMR+S4D variant; verify that kernel spectral analysis shows low dominant frequency (<0.05 cycles/sample) and reduced entropy compared to standalone
  3. Ablate the SMR multiplicative fusion (replace with addition or skip entirely) to isolate the mechanism's contribution to spectral shaping

## Open Questions the Paper Calls Out

- Why does the State Memory Replay (SMR) mechanism specifically induce low-pass filter behavior in S4D kernels, enabling long-range dependency capture that standalone S4D cannot achieve? The authors state their analysis "suggests that hybrid CNN–SSM architectures can achieve similar band-limiting effects without explicit constraints" but do not explain the underlying mechanism.

- Do the observed relationships between kernel spectral properties and model performance generalize to multi-layer S4D architectures? The authors explicitly limit their study: "We used single-layer models to maintain architectural simplicity and interpretability."

- How does kernel spectral behavior vary across SSM variants beyond S4D (e.g., Mamba, S4ND, StableSSM)? The paper analyzes only the diagonalized S4D variant while acknowledging multiple related variants with different parameterizations and stability properties.

## Limitations
- Interpretability analysis relies on correlational spectral properties that may not fully capture temporal dynamics
- Study scope limited to vulnerability detection in source code, raising generalizability questions
- S4D implementation details (state dimension, initialization scheme) not fully specified
- Cannot definitively prove causation between spectral properties and performance

## Confidence
- High confidence: Observation that standalone S4D kernels predominantly capture short-range dependencies, supported by multiple empirical measurements
- Medium confidence: Mechanism by which SMR+S4D achieves low-pass filtering behavior and superior performance, though exact mechanism not fully explained
- Medium confidence: Generalizability of findings to other SSM architectures and tasks, given limited exploration beyond S4D and vulnerability detection

## Next Checks
1. Apply the same spectral analysis framework to S4D models trained on different sequence modeling tasks (time series forecasting, natural language processing) to test whether kernel spectral properties consistently correlate with dependency range across domains.

2. Systematically remove or modify components of the SMR+S4D architecture (e.g., change multiplicative fusion to addition, remove Conv1D layer) while tracking how kernel spectral properties and performance change, to isolate the specific mechanisms driving long-range dependency capture.

3. Beyond dominant frequency and entropy, analyze additional spectral metrics including spectral roll-off, spectral centroid, and zero-crossing rate to determine whether these provide additional insights into the relationship between kernel frequency content and modeling capability.