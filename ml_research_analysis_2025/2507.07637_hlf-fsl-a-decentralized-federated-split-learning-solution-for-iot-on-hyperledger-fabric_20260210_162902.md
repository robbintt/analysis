---
ver: rpa2
title: HLF-FSL. A Decentralized Federated Split Learning Solution for IoT on Hyperledger
  Fabric
arxiv_id: '2507.07637'
source_url: https://arxiv.org/abs/2507.07637
tags:
- learning
- data
- client
- server
- clients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the problem of combining privacy and scalability\
  \ in collaborative machine learning for sensitive domains, where conventional federated\
  \ learning (FL) relies on a central server creating a single point of failure, and\
  \ split learning (SL) partitions models but scales poorly due to sequential training.\
  \ The core method idea is a decentralized architecture integrating federated split\
  \ learning (FSL) with the permissioned blockchain Hyperledger Fabric (HLF), using\
  \ chaincode to orchestrate FSL\u2019s split-model execution and peer-to-peer aggregation\
  \ without a central coordinator, while leveraging transient fields and Private Data\
  \ Collections to keep raw data and model activations private."
---

# HLF-FSL. A Decentralized Federated Split Learning Solution for IoT on Hyperledger Fabric

## Quick Facts
- **arXiv ID:** 2507.07637
- **Source URL:** https://arxiv.org/abs/2507.07637
- **Reference count:** 38
- **Primary result:** HLF-FSL matches centralized FSL accuracy on CIFAR-10 (94.14% vs 94.7%) and MNIST (99.43%) while reducing per-epoch training time compared to Ethereum-based solutions.

## Executive Summary
This paper presents HLF-FSL, a decentralized federated split learning framework built on Hyperledger Fabric for privacy-preserving collaborative machine learning in IoT environments. The solution addresses the single point of failure in conventional federated learning and the scalability limitations of split learning by leveraging blockchain's distributed trust mechanisms. By integrating split learning with Hyperledger Fabric's permissioned architecture, HLF-FSL enables peer-to-peer model training and aggregation without centralized coordination while preserving data privacy through transient fields and Private Data Collections.

## Method Summary
HLF-FSL combines federated split learning with Hyperledger Fabric's permissioned blockchain architecture to create a decentralized collaborative learning system. The framework uses chaincode to orchestrate split-model execution across distributed clients, with model activations kept private through Fabric's transient storage. Raw data never leaves client devices, while only intermediate model outputs are shared for aggregation. The permissioned nature of Hyperledger Fabric provides enterprise-grade security and scalability, with blockchain consensus ensuring trust in the decentralized training process. The architecture eliminates the need for a central server while maintaining the privacy benefits of split learning.

## Key Results
- HLF-FSL achieves 94.14% accuracy on CIFAR-10 compared to 94.7% for centralized FSL
- MNIST classification accuracy reaches 99.43% matching centralized approaches
- Per-epoch training time of 30m 38s outperforms Ethereum-based solutions (1h 25m)
- Minimal blockchain overhead demonstrated with preserved model accuracy

## Why This Works (Mechanism)
HLF-FSL works by distributing both the computational load and trust mechanisms across the network. Hyperledger Fabric's chaincode acts as the coordination layer, managing model partitioning and aggregation without centralized control. The transient fields in Fabric ensure that model activations exist only temporarily in memory, preventing persistent storage of sensitive intermediate data. Private Data Collections restrict access to model parameters, while the permissioned blockchain provides authenticated peer-to-peer communication. This architecture eliminates single points of failure while maintaining the privacy guarantees of split learning through cryptographic isolation of data at each client.

## Foundational Learning
- **Federated Learning (FL)**: Distributed ML where clients train locally and share model updates - needed for privacy-preserving collaborative learning without raw data sharing; quick check: verify gradient updates don't leak training data
- **Split Learning (SL)**: Model partitioning where clients train front layers and server trains back layers - needed to limit data exposure to intermediate activations; quick check: confirm activation sizes remain manageable
- **Hyperledger Fabric**: Permissioned blockchain platform with chaincode and private data collections - needed for trusted decentralized coordination without cryptocurrency overhead; quick check: validate chaincode execution latency meets real-time requirements
- **Transient Fields**: Memory-only data storage in blockchain - needed to prevent persistent storage of sensitive activations; quick check: verify data persistence policies are enforced
- **Private Data Collections**: Fabric feature for restricted data access - needed to maintain confidentiality of model parameters; quick check: test access control enforcement across consortium members

## Architecture Onboarding

**Component Map:** Clients -> Chaincode (HLF) -> Peer-to-Peer Aggregation -> Model Updates

**Critical Path:** Data Preprocessing → Chaincode Orchestration → Local Model Training → Activation Sharing → Aggregation → Model Update

**Design Tradeoffs:** The permissioned blockchain provides security and scalability but introduces consensus overhead; split learning reduces data exposure but increases communication complexity; transient fields enhance privacy but require careful memory management.

**Failure Signatures:** Consensus failures halt training; chaincode bugs cause coordination breakdowns; network partitions lead to inconsistent model states; memory exhaustion from transient field mismanagement.

**First 3 Experiments:**
1. Validate basic split learning accuracy with 2-3 clients before blockchain integration
2. Test Hyperledger Fabric chaincode execution latency for model coordination tasks
3. Measure communication overhead of transient field exchanges versus raw data sharing

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to image classification benchmarks (CIFAR-10, MNIST) with controlled data splits
- Small participant numbers (5-10 clients) don't represent enterprise-scale IoT scenarios
- Missing baseline FL execution times makes blockchain overhead assessment incomplete
- No direct benchmarking against other blockchain platforms or non-blockchain decentralized FL implementations

## Confidence

- Decentralized architecture design and integration with HLF: High
- Accuracy preservation compared to centralized FSL: High
- Performance comparison with Ethereum-based works: Medium (limited direct benchmarks)
- Blockchain overhead quantification: Medium (baseline comparisons missing)
- Scalability under real-world IoT conditions: Low (small-scale evaluation)

## Next Checks

1. Benchmark HLF-FSL against non-blockchain decentralized FL implementations under identical network conditions and model architectures
2. Conduct stress tests with 50+ IoT clients to measure blockchain consensus overhead and network latency scaling
3. Evaluate accuracy and convergence on heterogeneous IoT datasets (sensor streams, time-series) with non-IID distributions matching real deployment scenarios