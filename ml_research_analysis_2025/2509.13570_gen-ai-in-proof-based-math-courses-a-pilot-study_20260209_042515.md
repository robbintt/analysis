---
ver: rpa2
title: 'Gen AI in Proof-based Math Courses: A Pilot Study'
arxiv_id: '2509.13570'
source_url: https://arxiv.org/abs/2509.13570
tags:
- students
- generative
- course
- student
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study examined how students used generative AI in three proof-based
  undergraduate mathematics courses: first-semester abstract algebra, topology, and
  second-semester abstract algebra. An AI policy allowed students to use Microsoft
  CoPilot for assignments with strict citation requirements and verification of AI
  outputs.'
---

# Gen AI in Proof-based Math Courses: A Pilot Study

## Quick Facts
- arXiv ID: 2509.13570
- Source URL: https://arxiv.org/abs/2509.13570
- Authors: Hannah Klawa; Shraddha Rajpal; Cigole Thomas
- Reference count: 14
- Primary result: AI used mainly for brainstorming/explainers, not proof generation; frequent inaccuracies; critical use possible but requires verification

## Executive Summary
This pilot study examined how undergraduate students used Microsoft CoPilot in three proof-based mathematics courses. Students could use AI for assignments with strict citation requirements and verification expectations. The study found that about 42% of students did not use AI, and 36% considered its use cheating. Among users, AI primarily supported brainstorming and explanations rather than proof generation, with students reporting frequent inaccuracies especially in proofs. The research suggests AI can support learning when used critically with proper guidance, but should not replace independent reasoning in proof-based courses.

## Method Summary
The study was conducted in three proof-based undergraduate math courses: first-semester abstract algebra, topology, and second-semester abstract algebra. Students were allowed to use Microsoft CoPilot for assignments with strict citation requirements (textbook theorem numbers) and mandatory verification of AI outputs. Post-semester surveys (26 questions, n=19) and interviews (n=4) were conducted by non-instructor researchers to assess usage patterns, perceived helpfulness, impact on engagement, reliability perceptions, and ease of use. The survey achieved a 29% response rate from approximately 17 students.

## Key Results
- Students primarily used AI for brainstorming, explanations, and finding information rather than generating proofs
- About 42% of students did not use AI, and 36% considered its use cheating
- AI was found helpful for understanding material and problem-solving but noted for frequent inaccuracies, especially in proofs
- AI use did not significantly reduce instructor engagement but had mixed effects on peer interactions
- Students appreciated AI's role in clarifying concepts but remained skeptical of its reliability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Requiring citation of textbook theorems reduces uncritical AI copy-pasting
- Mechanism: Students must map AI outputs to course materials, forcing evaluation of whether AI responses align with established results and course sequencing
- Core assumption: Students possess sufficient prior knowledge to recognize when AI outputs conflict with course content
- Evidence anchors:
  - [abstract]: "strict citation requirements and verification of AI outputs"
  - [section]: Students noted citation requirement "adds a lot of tedium" but instructor observed it "seemed to be useful in providing a way to keep students from copying and pasting"
  - [corpus]: Weak/no direct corpus evidence for this specific mechanism
- Break condition: If students lack foundational proof-writing skills, they cannot evaluate AI outputs; citation becomes performative rather than evaluative

### Mechanism 2
- Claim: Students self-select AI tasks based on perceived tool reliability
- Mechanism: Through repeated exposure, students learn that AI excels at information retrieval but struggles with proof generation, leading to task-appropriate use patterns
- Core assumption: Students accurately perceive AI limitations rather than avoiding proofs due to difficulty
- Evidence anchors:
  - [abstract]: "most used AI for brainstorming, explanations, or finding information rather than generating proofs"
  - [section]: Student quote: "Copilot can't help a ton with the super difficult questions... it can be wildly inaccurate when it comes to proofs"
  - [corpus]: Related work confirms AI "fragility" in mathematical reasoning (Ahn et al., 2024 cited in paper)
- Break condition: If students do not encounter sufficient AI errors, they may not calibrate trust appropriately and over-rely on outputs

### Mechanism 3
- Claim: Error detection exercises may enhance proof validation skills
- Mechanism: Students practicing "debugging" AI-generated proofs develop critical evaluation skills typically underemphasized in undergraduate curricula
- Core assumption: Skill transfer occurs from critiquing AI outputs to validating peer/instructor proofs
- Evidence anchors:
  - [abstract]: "frequent inaccuracies, especially in proofs"
  - [section]: Student noted finding errors was "like debugging code than writing a proof"; authors cite Selden & Selden (2003) on students' limited proof validation training
  - [corpus]: Weak/no direct corpus evidence—authors note "more research on this particular aspect is needed"
- Break condition: If errors are too subtle or students lack proof-writing foundations, they may miss errors or accept flawed reasoning

## Foundational Learning

- Concept: **Proof-based vs. computational mathematics**
  - Why needed here: The study's context—abstract algebra and topology—requires constructing logical arguments rather than applying formulas; this distinction explains why AI struggles differently than in calculus courses
  - Quick check question: Can you explain why solving for x in an equation differs fundamentally from proving that all groups of prime order are cyclic?

- Concept: **Large language model hallucination patterns**
  - Why needed here: Students encountered AI outputs that were "inaccurate, incomplete, or misleading" with "confident tone"—understanding why LLMs generate plausible-sounding but false statements is essential for critical use
  - Quick check question: Why might an LLM confidently state a false mathematical claim rather than expressing uncertainty?

- Concept: **Self-determination theory in learning**
  - Why needed here: Student concerns about "pride of ownership" and "compromised learning process" relate to intrinsic motivation; understanding autonomy-supportive AI integration helps explain 42% non-adoption
  - Quick check question: How might required AI use differ from permitted AI use in terms of student agency and ownership?

## Architecture Onboarding

- Component map:
  - **Policy layer**: Syllabus statement defining permitted use, required citation format, responsibility assignment
  - **Tool layer**: Microsoft CoPilot (university-approved for data compliance), accessed via institutional accounts
  - **Assessment layer**: Two-stage submissions (draft → feedback → revision), graded discussions, verification requirements
  - **Data collection layer**: Post-semester surveys (n=19), interviews (n=4), conducted by non-instructor researchers

- Critical path:
  1. Policy design with explicit verification expectations → 2. Student self-selection into user/non-user groups → 3. Task-appropriate AI use emerges through error exposure → 4. Critical evaluation skills potentially develop → 5. Mixed impacts on peer discourse quality

- Design tradeoffs:
  - **Transparency vs. authenticity**: Requiring AI disclosure in discussions enabled policy compliance but students perceived some peer posts as "entirely AI-generated," reducing interaction quality
  - **Rigor vs. tedium**: Textbook citation requirements promoted engagement but added perceived burden
  - **Standardization vs. exploration**: Single approved tool (CoPilot) ensured data compliance but limited comparison across AI capabilities

- Failure signatures:
  - **Over-reliance indicator**: Students submitting bullet-point format matching AI output style without transformation
  - **Trust miscalibration indicator**: Students either avoiding AI entirely (42%) or failing to verify outputs
  - **Discourse degradation indicator**: Discussion posts with "very little interesting information" suspected as unedited AI generation
  - **Citation performative indicator**: References that don't logically support the argument structure

- First 3 experiments:
  1. **Prompt engineering demonstration**: Create 5-minute video showing effective vs. ineffective prompts for proof-related tasks, with worked examples of evaluating outputs; measure whether tutorial increases adoption among initial non-users
  2. **Error detection calibration exercise**: Present students with AI-generated proofs containing known error types (local/superficial vs. global/structural); assess whether training improves detection rates compared to control
  3. **Tiered policy comparison**: Run parallel sections with varying verification stringency (full citation vs. general verification vs. no verification requirement); track submission quality, AI use patterns, and self-reported critical engagement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the practice of validating and critiquing AI-generated proofs improve students' proof-writing skills and critical thinking compared to traditional proof-validation exercises?
- Basis in paper: [explicit] The authors state: "it seems that the exercise of validating AI generated proofs may provide a way of increasing students' critical thinking and understanding of proofs although more research on this particular aspect is needed."
- Why unresolved: This study was observational and did not include a control group or pre/post measures of proof-validation skills; it only captured student perceptions.
- What evidence would resolve it: A controlled study comparing students who regularly critique AI-generated proofs against those who complete traditional "proofs to grade" exercises, using validated assessments of proof-writing and error-detection ability.

### Open Question 2
- Question: Does instructor-modeled demonstration of AI tool use (including prompt design, output evaluation, and citation practices) lead to more effective and critical student engagement with generative AI?
- Basis in paper: [explicit] Students suggested "it'd be good to sort of give an overview on use of the AI... like a five-minute demonstration video"; the authors cite Garcia et al. (2025) in agreement that demonstrations may enhance utilization.
- Why unresolved: The courses in this study provided limited guidance on AI use; no systematic comparison of different instruction levels was conducted.
- What evidence would resolve it: Random assignment of sections to receive either (a) no AI guidance, (b) basic policy-only guidance, or (c) structured demonstration sessions, followed by measures of AI usage patterns, output verification accuracy, and learning outcomes.

### Open Question 3
- Question: Do students who use AI tools perceive greater comprehension gains than they actually achieve on objective assessments of proof-based content?
- Basis in paper: [inferred] The authors caution: "it is important to note that students' perception of deeper comprehension could be different from reality."
- Why unresolved: The study relied entirely on self-reported perceptions; no objective learning measures (e.g., exam performance, proof quality ratings) were correlated with AI usage or perceived helpfulness.
- What evidence would resolve it: Collecting both perception data and objective performance measures (blindly scored proofs, concept inventories) from AI users and non-users to assess alignment between perceived and actual learning.

## Limitations

- Small sample size (n=19) and single-institution setting limit generalizability
- Lack of pre-intervention baseline data prevents determining actual learning impact
- Qualitative nature of data prevents causal inference about learning outcomes
- Post-semester timing may introduce recall bias in self-reported usage patterns

## Confidence

**High Confidence (★★★):**
- AI use patterns: Students primarily used AI for brainstorming and explanations rather than proof generation
- Reliability perceptions: Students consistently reported AI accuracy problems, especially for proofs
- Non-adoption rates: Approximately 42% of students did not use AI, with about 36% considering it cheating

**Medium Confidence (★★☆):**
- Impact on peer engagement: Mixed effects on peer interactions with qualitative impressions
- Critical evaluation skill development: Suggested but not empirically tested within study

**Low Confidence (★★★):**
- Long-term learning impacts: No measurement of independent proof-writing ability development
- Mechanism validity: Theoretical mechanisms not experimentally validated

## Next Checks

1. **Prompt engineering tutorial impact**: Create a 5-minute video demonstrating effective versus ineffective prompts for proof-related tasks, with worked examples of evaluating AI outputs. Measure whether this tutorial increases AI adoption among students who initially avoid it, and whether it improves the quality of AI-assisted proofs through better prompting.

2. **Error detection calibration study**: Design an experiment where students are presented with AI-generated proofs containing known error types (local/superficial versus global/structural). Assess whether targeted training in error detection improves students' ability to identify flaws in both AI-generated and peer-generated proofs, compared to a control group.

3. **Tiered policy comparison**: Implement parallel sections of the same proof-based course with varying verification stringency (full textbook citation requirements, general verification requirements, or no verification requirements). Track submission quality, AI use patterns, self-reported critical engagement, and compare outcomes across the three conditions to determine optimal policy design.