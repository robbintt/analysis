---
ver: rpa2
title: Evaluating the Sensitivity of LLMs to Harmful Contents in Long Input
arxiv_id: '2510.05864'
source_url: https://arxiv.org/abs/2510.05864
tags:
- harmful
- harm
- context
- sentences
- long
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper systematically evaluates how large language models\
  \ (LLMs) detect harmful content\u2014including toxic, offensive, and hate speech\u2014\
  in long-context settings. Using LLaMA-3, Qwen-2.5, and Mistral, the study varies\
  \ harmful content type (explicit vs."
---

# Evaluating the Sensitivity of LLMs to Harmful Contents in Long Input

## Quick Facts
- **arXiv ID**: 2510.05864
- **Source URL**: https://arxiv.org/abs/2510.05864
- **Reference count**: 40
- **Primary result**: Model performance peaks at moderate harmful prevalence (~0.25) and declines with longer contexts, revealing systematic biases in long-context harmful content detection.

## Executive Summary
This paper systematically evaluates how large language models detect harmful content—including toxic, offensive, and hate speech—in long-context settings. Using LLaMA-3, Qwen-2.5, and Mistral, the study varies harmful content type (explicit vs. implicit), position (beginning, middle, end), prevalence (0.01–0.50 of the prompt), and context length (600–6000 tokens). Results show that model performance peaks at moderate harmful prevalence (~0.25), declines with longer contexts, is strongest for harmful content at the beginning, and is more reliable for explicit harm than implicit harm. This reveals systematic biases and dilution effects, providing actionable insights for improving safety in long-context LLM applications.

## Method Summary
The study performs binary classification of harmful content (toxic, offensive, hate speech) in long-context prompts using three datasets: IHC (hate speech), OffensEval (offensive language), and JigsawToxic (toxic content). Prompts are constructed by sampling harmful/non-harmful sentences with controlled variations in context length (600-6000 tokens), harm ratio (0.05-0.5), harm region (beginning/middle/end/all), and harm type (explicit/implicit/both). Each configuration runs k=128 trials with different random seeds. Models (LLaMA-3-8B-Instruct, Qwen2.5-7B-Instruct, Mistral-7B-Instruct-v0.3) perform inference-only evaluation with temperature=0.0, top-p=1.0, top-k=0, and prompts include harm definitions and 4-shot sentence-level examples plus 1-shot long-context example. Performance is measured using macro-F1 (primary), predicted prevalence value, and harmful-class precision and recall, micro-aggregated across all instances.

## Key Results
- Model performance peaks at moderate harmful prevalence (~0.25) and declines with longer contexts
- Harmful content at the beginning of prompts is detected more reliably than content in middle or end positions
- Explicit harm detection is significantly more reliable than implicit harm detection across all models and conditions

## Why This Works (Mechanism)
The observed performance patterns reflect fundamental limitations in how LLMs process and prioritize information in long contexts. When harmful content is diluted across thousands of tokens, models struggle to maintain consistent sensitivity. The position effect suggests models give disproportionate weight to early information, likely due to attention mechanisms that decay or become saturated over long sequences. The explicit vs. implicit distinction reveals that models rely heavily on surface-level cues for detection, struggling with context-dependent or subtle harmful content that requires deeper semantic understanding.

## Foundational Learning

**Long-context processing**: Understanding how attention mechanisms handle information over extended sequences
- *Why needed*: Explains why performance degrades with longer contexts
- *Quick check*: Compare performance across different context lengths (600 vs 6000 tokens)

**Position bias in transformer models**: Models weight information differently based on its position in the input
- *Why needed*: Accounts for superior performance on beginning-positioned harmful content
- *Quick check*: Test if reversing prompt order affects detection rates

**Explicit vs. implicit content detection**: Different cognitive processes for surface-level vs. context-dependent harmful content
- *Why needed*: Explains why explicit harm is detected more reliably
- *Quick check*: Compare F1 scores for explicit vs implicit harm detection

## Architecture Onboarding

**Component map**: Datasets (IHC/OffensEval/JigsawToxic) -> Prompt construction engine -> LLM inference (LLaMA-3/Qwen2.5/Mistral) -> Output parsing -> Metric calculation

**Critical path**: Prompt construction → LLM inference → Output parsing → Metric computation

**Design tradeoffs**: The study uses inference-only evaluation (no fine-tuning) for generalizability but sacrifices potential performance gains from task-specific adaptation. Temperature=0.0 ensures reproducibility but may not reflect real-world stochastic behavior.

**Failure signatures**: 
- Models outputting non-numeric text or failing to follow index format
- Context exceeding model window limits (especially 6000 tokens with 8k limits)
- Empty responses or responses with extra text

**First experiments**:
1. Test prompt parsing robustness by intentionally corrupting output format
2. Verify context length limits by running single-token-over scenarios
3. Validate metric calculation by comparing against manually annotated samples

## Open Questions the Paper Calls Out
None

## Limitations
- Sentence-level annotation may not capture multi-sentence harmful content emergence
- Results may not generalize beyond the three studied harmful content categories
- Temperature=0.0 setting eliminates stochastic variation present in real-world deployments

## Confidence

**High confidence**: Peak performance at moderate harmful prevalence (~0.25) and decline with longer contexts is consistent across all three models and metrics.

**Medium confidence**: Specific performance thresholds (optimal ratio of 0.25, particular F1 scores) may vary with different model architectures, training data, or harmful content categories.

**Low confidence**: Practical implications for safety-critical applications in deployment settings with different prompt structures and operational constraints.

## Next Checks

1. Evaluate the same models on additional harmful content datasets (e.g., Twitter harassment corpora, forum toxicity datasets) to test whether observed patterns hold across different content domains.

2. Test whether harmful content detection patterns generalize to different model families (e.g., GPT-4, Claude, Gemini) and architectures beyond the studied 7-8B parameter instruction-tuned models.

3. Create evaluation scenarios that more closely mimic actual deployment conditions, including multi-turn conversations, varying prompt structures, and the presence of both harmful and safety-critical information within the same long context.