---
ver: rpa2
title: 'KG-LLM-Bench: A Scalable Benchmark for Evaluating LLM Reasoning on Textualized
  Knowledge Graphs'
arxiv_id: '2504.07087'
source_url: https://arxiv.org/abs/2504.07087
tags:
- knowledge
- graph
- structured
- edges
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "KG-LLM-Bench introduces a comprehensive benchmark for evaluating\
  \ how LLMs process and understand knowledge graphs in textual form. The framework\
  \ tests five reasoning tasks across seven models and five textualization strategies,\
  \ revealing that textualization choice significantly impacts performance\u2014up\
  \ to 17.5% absolute difference overall, with Structured JSON and List-of-Edges performing\
  \ best."
---

# KG-LLM-Bench: A Scalable Benchmark for Evaluating LLM Reasoning on Textual Graphs

## Quick Facts
- arXiv ID: 2504.07087
- Source URL: https://arxiv.org/abs/2504.07087
- Reference count: 40
- Key outcome: Benchmark reveals up to 17.5% performance difference based on textualization strategy, with Structured JSON and List-of-Edges performing best

## Executive Summary
KG-LLM-Bench introduces a comprehensive framework for evaluating how large language models process and reason over textualized knowledge graphs. The benchmark tests five reasoning tasks across seven models and five textualization strategies, demonstrating that format choice significantly impacts performance. The framework reveals that model architecture and task type influence optimal encoding strategy, while pseudonymization shows minimal effect, suggesting LLMs rely on provided context rather than memorization. This work provides actionable insights for optimizing KG reasoning and establishes a scalable evaluation framework for future research.

## Method Summary
The benchmark evaluates LLM reasoning on textualized knowledge graphs using a subgraph sampling approach from the WikiDataSets Countries knowledge graph. For each task, ego-graphs are extracted with 200 edges, then converted to text using five different textualization strategies (List-of-Edges, Structured JSON, Structured YAML, RDF Turtle, JSON-LD). Task-specific questions are generated algorithmically, and models generate answers that are evaluated with exact match accuracy. The framework supports pseudonymization to test reasoning versus memorization, and provides comprehensive evaluation across multiple model architectures.

## Key Results
- Textualization strategy choice creates up to 17.5% absolute performance difference across models
- Structured JSON and List-of-Edges consistently outperform RDF Turtle and JSON-LD
- Pseudonymization shows only 0.2% performance difference, confirming in-context reasoning over memorization
- Aggregation accuracy degrades sharply for counts above 4 edges, dropping from >80% to ~10%
- Model-specific preferences emerge: some models perform better with certain textualizations for specific tasks

## Why This Works (Mechanism)

### Mechanism 1
Textualization strategy functions as a representational scaffold that determines information locality, directly influencing the model's ability to traverse or aggregate graph concepts. Formats like Structured JSON group relations by entity, reducing the search space for aggregation tasks, whereas List-of-Edges presents edges linearly, which may assist in frequency-based heuristics but hinder complex multi-hop traversal. The model's attention mechanism relies on token proximity, making information scattered across long contexts less likely to be associated than information grouped in JSON blocks.

### Mechanism 2
LLMs prioritize in-context evidence over parametric memory when tasked with specific graph reasoning, provided the context is explicit. The substitution of real entities with pseudonyms forces the model to rely strictly on the structural relationships provided in the prompt rather than recalling facts about the entities. This demonstrates that the model has sufficient capacity to hold the subgraph in its context window without degradation, and that the structure of the context is the active reasoning driver, not the entity labels themselves.

### Mechanism 3
Aggregation performance is bounded by the model's ability to maintain a "counting state" over the context, which degrades as edge degree increases. Models can reliably aggregate low-degree edges (counting 1-4 items) but fail to track state as the list of neighbors grows, likely due to attention dilution over long attribute lists. Exact-match evaluation requires precise counting, and the model's counting hallucination occurs when the correct reasoning path is followed but the final count is incorrect.

## Foundational Learning

- **Concept: Knowledge Graph Triples (Subject, Relation, Object)**
  - Why needed: The benchmark tests a model's ability to reason over textualized triples. Understanding that (Inception, director, Nolan) is the atomic unit of knowledge is required to interpret the List-of-Edges format.
  - Quick check: How does the model interpret [A, connected_to, B] differently in JSON (nested) vs. List (linear)?

- **Concept: In-Context Learning (ICL)**
  - Why needed: The entire benchmark relies on feeding a subgraph into the context window. The mechanism assumes the model processes this text as "facts" rather than just tokens.
  - Quick check: Why does pseudonymization prove that the model is using ICL rather than pre-training?

- **Concept: Token Efficiency vs. Structural Fidelity**
  - Why needed: Section 7.4 highlights that JSON-LD uses ~13k tokens vs. ~2.6k for Lists. You must balance the cost of tokens against the structural clarity needed for the task.
  - Quick check: Which format would you choose for a small context model (4k window) requiring simple retrieval?

## Architecture Onboarding

- **Component map:** Subgraph Sampler -> Textualizer -> Pseudonymizer (Optional) -> LLM -> Evaluator
- **Critical path:** The Textualizer. As shown in Figure 1 and Section 7.1, this is the highest-leverage component. A bad choice here (e.g., RDF for a simple aggregation task) caps performance regardless of the model used.
- **Design tradeoffs:**
  - Structured JSON/YAML: High aggregation performance, medium token cost. Best for "How many?" queries.
  - List-of-Edges: Good retrieval, lowest token cost. Best for "Does this exist?" queries.
  - RDF/JSON-LD: Poor performance generally, very high token cost. Use only if semantic web interoperability is a strict requirement.
- **Failure signatures:**
  - Token bloat: Using JSON-LD on small models causing context truncation.
  - Direction confusion: Models failing "Incoming Edge" tasks because the textualization groups by source, not target.
  - Counting hallucination: Correct reasoning path but wrong final count on high-degree nodes.
- **First 3 experiments:**
  1. Baseline Calibration: Run TripleRetrieval and AggByRelation tasks on your target model using "List-of-Edges" vs. "Structured JSON" to establish which format suits your model's attention patterns.
  2. Context Stress Test: Increase subgraph size (edges) on the AggNeighborProperty task to find the inflection point where aggregation accuracy drops below 50%.
  3. Pseudonymization Validation: Run a batch of queries with and without pseudonymization on your specific domain data to ensure the model isn't hallucinating answers based on popular entities in its pre-training data.

## Open Questions the Paper Calls Out

- **Question:** How do test-time reasoning models (e.g., OpenAI o1, DeepSeek R1) compare to standard LLMs on knowledge graph reasoning tasks when using this benchmark?
  - Basis: Section 8 states a major direction for future work is "studying of test-time reasoning on KGs," specifically citing models like OpenAI's o1/o3 and DeepSeek's R1.
  - Why unresolved: The current experiments evaluate standard inference models without the enhanced computational resources or "thinking" processes used by reasoning-specific models.
  - Evidence needed: Benchmark results running the same textualization strategies on reasoning-optimized models to see if they close the performance gap on difficult tasks like Shortest Path.

- **Question:** Does the optimal textualization strategy shift when scaling subgraphs to utilize long-context windows (e.g., 100k+ tokens)?
  - Basis: Section 8 identifies "Scale" as a major research direction, noting that modifying sampling parameters enables the study of "long-context reasoning over KGs."
  - Why unresolved: The current study limits subgraphs to 200 edges; it is unknown if JSON-LD or RDF Turtle remain viable or if token-inefficient formats degrade performance at extreme scales.
  - Evidence needed: Experiments varying subgraph sizes to 1,000+ edges on long-context models (e.g., Gemini 1.5) to measure the trade-off between format verbosity and reasoning accuracy.

- **Question:** How do textualization strategies impact performance on complex graph structures, such as temporal knowledge graphs or hypergraphs (e.g., Wikidata qualifiers)?
  - Basis: Appendix A lists "literal values (numbers or dates), temporal knowledge graphs, or any form of hypergraph" as current limitations of the framework that cannot be handled.
  - Why unresolved: The benchmark currently relies on simple triples (Subject, Relation, Object), but real-world knowledge often requires qualifiers or time-bounds which may necessitate different textual encodings.
  - Evidence needed: Extending the benchmark to include temporal or qualified edges and comparing the efficacy of the existing five textualization strategies on these complex structures.

## Limitations

- Benchmark scope restricted to single domain (Countries knowledge graph) and fixed subgraph size (200 edges), limiting generalizability to other KG domains
- Evaluation relies on exact-match accuracy, which may not capture nuanced reasoning capabilities or provide partial credit for partially correct answers
- Study only examines 5 textualization strategies, potentially missing optimal encodings for specific tasks or model architectures

## Confidence

- **High confidence:** Textualization strategy significantly impacts performance (up to 17.5% absolute difference) - well-supported by extensive experimental results across multiple models and tasks
- **Medium confidence:** Structured JSON and List-of-Edges are generally optimal - supported but shows task-dependent variation
- **Low confidence:** Broader generalizability to other KG domains and long-term stability as models evolve - remains uncertain

## Next Checks

1. **Cross-domain validation:** Replicate the benchmark using a biomedical or scientific knowledge graph to test whether the identified optimal textualization strategies transfer across domains with different entity types and relation patterns.

2. **Scale sensitivity analysis:** Systematically vary subgraph sizes (e.g., 50, 100, 400, 800 edges) to identify the inflection points where each textualization strategy's performance degrades, particularly for the aggregation tasks.

3. **Model architecture comparison:** Test whether newer model architectures with enhanced context handling (like Gemini 1.5 Pro's 1M token context) show different sensitivity to textualization strategy, particularly for verbose formats like RDF Turtle and JSON-LD.