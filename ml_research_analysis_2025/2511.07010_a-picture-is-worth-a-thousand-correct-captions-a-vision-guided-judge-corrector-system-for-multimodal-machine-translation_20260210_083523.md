---
ver: rpa2
title: 'A Picture is Worth a Thousand (Correct) Captions: A Vision-Guided Judge-Corrector
  System for Multimodal Machine Translation'
arxiv_id: '2511.07010'
source_url: https://arxiv.org/abs/2511.07010
tags:
- data
- translation
- language
- training
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving low-resource multimodal
  machine translation (MMT) for four Indian languages by enhancing the quality of
  training data. The authors propose a vision-guided judge-corrector pipeline that
  leverages multimodal large language models to automatically identify and correct
  translation errors in the training data.
---

# A Picture is Worth a Thousand (Correct) Captions: A Vision-Guided Judge-Corrector System for Multimodal Machine Translation

## Quick Facts
- arXiv ID: 2511.07010
- Source URL: https://arxiv.org/abs/2511.07010
- Authors: Siddharth Betala; Kushan Raj; Vipul Betala; Rohan Saswade
- Reference count: 11
- One-line primary result: Vision-guided judge-corrector pipeline improves low-resource MMT for Indian languages with +1.30 BLEU for English-Bengali

## Executive Summary
This paper addresses the challenge of improving low-resource multimodal machine translation for four Indian languages by enhancing the quality of training data. The authors propose a vision-guided judge-corrector pipeline that leverages multimodal large language models to automatically identify and correct translation errors in the training data. The system processes 28,928 training examples and corrects an average of 17.1% of captions per language. Experimental results show that LoRA fine-tuning on corrected data yields consistent improvements over training on original data, with BLEU score gains of +1.30 for English-Bengali (evaluation set).

## Method Summary
The approach employs a two-stage pipeline: first, a vision-guided judge-corrector system using Gemini 2.5 Flash Lite as judge (with 0.7 confidence threshold) to classify translations into correct, visually ambiguous, or mistranslated categories; second, LoRA fine-tuning of the IndicTrans2 200M distilled model (r=16, α=32) on corrected data. The judge routes visual disambiguation cases to GPT-4o-mini and linguistic errors to IndicTrans2 for retranslation. Training uses batch size 8×2 gradient accumulation, lr=3e-5 with 500 warmup steps, 3 epochs, and greedy decoding for inference.

## Key Results
- BLEU score gains of +1.30 for English-Bengali (evaluation set)
- Consistent improvements across three languages with gains ranging from +0.10 to +1.30 BLEU
- RIBES improvements of +1.20 for Hindi evaluation set (even with only +0.10 BLEU gain)
- Correction rates vary by language: Bengali (16.1%), Hindi (14.3%), Odia (12.1%), Malayalam (24.0%)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multimodal error classification enables targeted correction by distinguishing visually-grounded ambiguities from linguistic errors.
- **Mechanism:** The judge module simultaneously processes cropped image regions, English source captions, and target translations to classify errors into: correct, visual_context_needed, or poor_translation.
- **Core assumption:** Visual context is necessary for disambiguating polysemous terms but unnecessary for pure translation errors.
- **Evidence anchors:** Judge component classifies translations into three categories; Missing or empty captions are automatically classified as 'visual_context_needed'; CaMMT paper confirms images can act as cultural context.
- **Break condition:** If judge confidence drops below 0.7, original caption is retained.

### Mechanism 2
- **Claim:** Specialized corrector routing improves efficiency by matching correction method to error type.
- **Mechanism:** Visual context needed cases are routed to GPT-4o-mini for multimodal regeneration. Poor translation cases are routed to IndicTrans2 for monolingual retranslation.
- **Core assumption:** GPT-4o-mini's vision capabilities outperform text-only NMT for visually-ambiguous terms; IndicTrans2 provides better linguistic fidelity for pure translation errors.
- **Evidence anchors:** GPT-4o-mini regenerates captions requiring visual disambiguation; 73.3% of corrections address poor translation quality; Dual-branch Prompting for MMT notes MMT approaches are sensitive to irrelevant visual noise.
- **Break condition:** When original data quality is already high, automated correction can introduce errors.

### Mechanism 3
- **Claim:** Training on cleaned data improves inference-time translation quality even without visual input.
- **Mechanism:** LoRA fine-tuning on corrected captions reduces noise in training targets, yielding better model weights despite text-only inference.
- **Core assumption:** Systematic errors in training data propagate to model weights; correcting even 17% of examples reduces error signal contamination.
- **Evidence anchors:** Training on corrected data yields consistent improvements with BLEU score gains of +1.30 for English-Bengali; These improvements are substantial given moderate correction rate; Limited corpus evidence for LoRA + data cleaning synergy.
- **Break condition:** Test set reference quality issues suppress reported metrics.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: Enables parameter-efficient fine-tuning of 200M parameter IndicTrans2 model with only 0.8M trainable parameters (0.4%)
  - Quick check question: What are the LoRA rank (r) and scaling factor (α) used, and which layers receive adapters?

- **Concept: Multimodal Machine Translation (MMT)**
  - Why needed here: Provides theoretical foundation for why visual context helps resolve translation ambiguities
  - Quick check question: Why might visual context help translate "bank" differently in a river scene vs. a financial district scene?

- **Concept: LLM-as-Judge Paradigm**
  - Why needed here: The automated quality assessment relies on multimodal LLMs to replace human annotators for scalable error detection
  - Quick check question: What confidence threshold triggers retention of original captions, and why does this conservative threshold matter?

## Architecture Onboarding

- **Component map:** Training Data → Preprocessing (crop images) → Judge Module (Gemini 2.5) → Router → GPT-4o-mini (visual cases) or IndicTrans2 (linguistic) or Retain original → Corrected Training Data → LoRA Fine-tuning → Merged Model → Text-only inference

- **Critical path:** Judge classification accuracy → Corrector selection → Training data quality → LoRA weight updates → BLEU improvements

- **Design tradeoffs:**
  - Confidence threshold (0.7): Higher = more conservative (fewer false corrections), lower = more aggressive (risks introducing errors)
  - Correction rate varies by language (12-24%): Odia (12.1%) showed mixed results—already-clean data may not benefit
  - LoRA vs. full fine-tuning: LoRA enables rapid A/B comparison but may underestimate full potential

- **Failure signatures:**
  - Negative BLEU delta on challenge set (Odia: -0.10) suggests over-correction on already-high-quality data
  - RIBES improving while BLEU flat (Hindi evaluation) indicates word ordering improvements not captured by BLEU
  - Malayalam (24% correction rate) was omitted due to resource constraints

- **First 3 experiments:**
  1. Train LoRA on original data only; verify baseline BLEU matches paper-reported values
  2. Run judge module on 100 random samples with manual review; measure precision/recall of classification
  3. Re-run correction pipeline with confidence thresholds [0.5, 0.6, 0.7, 0.8, 0.9] on a subset; plot correction rate vs. BLEU

## Open Questions the Paper Calls Out

1. Would combining corrected training data with full model finetuning yield greater improvements than LoRA finetuning alone?
2. Would Malayalam benefit most from the correction pipeline given its 24.0% correction rate—the highest among all four languages?
3. Do test set reference errors artificially suppress reported metrics for models trained on corrected data?
4. Do native speakers perceive quality improvements from corrected training data that automatic metrics fail to capture?

## Limitations
- Evaluation methodology concerns: BLEU improvements may reflect better matching to flawed references rather than genuinely better translations
- Error classification reliability: Judge accuracy is critical but lacks precision/recall metrics
- Generalization uncertainty: All experiments are on Visual Genome-derived datasets for Indian languages in specific evaluation framework

## Confidence
- Vision-guided correction improves translation quality: High confidence (consistent BLEU improvements across three languages)
- Targeted correction via error classification is effective: Medium confidence (reasonable error distribution but no judge accuracy metrics)
- LoRA fine-tuning on corrected data captures quality improvements: Medium confidence (measurable gains but no ablation studies)

## Next Checks
1. Manually review 100 random classifications from the judge module across all three error types to calculate precision, recall, and F1 scores
2. Hold out a clean subset from original training data (10% with manually verified high-quality translations) and train LoRA models on both corrected and original data using this subset
3. Systematically vary the judge confidence threshold from 0.5 to 0.9 in 0.1 increments on a subset of the data and plot correction rate against BLEU/RIBES scores