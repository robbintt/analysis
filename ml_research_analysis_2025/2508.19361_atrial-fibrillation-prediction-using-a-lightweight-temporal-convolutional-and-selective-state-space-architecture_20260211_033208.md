---
ver: rpa2
title: Atrial Fibrillation Prediction Using a Lightweight Temporal Convolutional and
  Selective State Space Architecture
arxiv_id: '2508.19361'
source_url: https://arxiv.org/abs/2508.19361
tags:
- prediction
- atrial
- fibrillation
- mamba
- onset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a lightweight deep learning model for early
  prediction of atrial fibrillation (AF) using only RR intervals. The model combines
  a Temporal Convolutional Network (TCN) for positional encoding with Mamba, a selective
  state space model, to efficiently capture temporal dynamics.
---

# Atrial Fibrillation Prediction Using a Lightweight Temporal Convolutional and Selective State Space Architecture

## Quick Facts
- **arXiv ID**: 2508.19361
- **Source URL**: https://arxiv.org/abs/2508.19361
- **Reference count**: 23
- **Primary result**: Lightweight TCN-Mamba model achieves 0.908 sensitivity and 0.933 specificity for 2-hour AF prediction using only 30 minutes of RR intervals

## Executive Summary
This paper proposes a lightweight deep learning model for early prediction of atrial fibrillation (AF) using only RR intervals. The model combines a Temporal Convolutional Network (TCN) for positional encoding with Mamba, a selective state space model, to efficiently capture temporal dynamics. Evaluated on subject-wise testing using outpatient datasets, the model achieved a sensitivity of 0.908, specificity of 0.933, F1-score of 0.930, AUROC of 0.972, and AUPRC of 0.932. It uses only 73.5 thousand parameters and 38.3 MFLOPs, making it computationally efficient and suitable for wearable devices. Notably, it can predict AF up to two hours in advance using just 30 minutes of input data, offering valuable lead time for preventive interventions.

## Method Summary
The model uses RR intervals as input, extracted from IRIDIA-AF and NSR RR datasets with subject-wise 60/20/20 train/val/test splits. The architecture consists of three TCN blocks (causal dilated Conv1D with dilations 1, 2, 4) followed by a Mamba SSM layer with residual connections and LayerNorm, then global pooling (Avg+Max) and two fully connected blocks (64→32→2). The model is trained with AdamW (lr=1e-4, weight_decay=1e-4), batch size 16, cross-entropy loss, and early stopping (patience=10). Key hyperparameters include kernel size 3, 32 channels in TCN, and unspecified Mamba SSM parameters.

## Key Results
- Achieved AUROC of 0.972 and AUPRC of 0.932 for 2-hour AF prediction
- Model uses only 73.5 thousand parameters and 38.3 MFLOPs
- Outperformed traditional CNN-RNN models while maintaining extreme computational efficiency
- Can predict AF up to 2 hours in advance using only 30 minutes of RR interval data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Causal, dilated convolutions extract hierarchical temporal features while preserving time-order strictness.
- **Mechanism:** The TCN uses causal convolutions to ensure predictions at time t depend only on t and earlier inputs. Dilated convolutions exponentially expand the receptive field without deep stacks, capturing the "gradual changes" in RR intervals prior to AF onset.
- **Core assumption:** The physiological transitions from Normal Sinus Rhythm (NSR) to Atrial Fibrillation (AF) manifest as detectable, localized temporal irregularities within the RRI signal.
- **Evidence anchors:** Abstract states "combining a Temporal Convolutional Network (TCN) for positional encoding... to enable early prediction"; section I describes "TCN utilizes causal convolutions to prevent information leakage... along with dilated convolutions... to capture long-range dependencies."

### Mechanism 2
- **Claim:** Selective State Space Models (Mamba) compress long-term context efficiently via input-dependent selection.
- **Mechanism:** Mamba uses a selection mechanism to dynamically filter and propagate relevant historical information over the 30-minute window with linear complexity, allowing the model to "remember" critical arrhythmic precursors over thousands of time steps.
- **Core assumption:** The relevant signals for 2-hour advance prediction are embedded in the temporal dynamics of the sequence rather than just instantaneous morphology.
- **Evidence anchors:** Abstract states "...Mamba, a selective state space model, to efficiently capture temporal dynamics"; section I describes "Mamba enables efficient parallel training with linear time complexity while maintaining strong sequence modeling capabilities."

### Mechanism 3
- **Claim:** Architecture decoupling (TCN for features, Mamba for dynamics) reduces parameter count while maintaining accuracy.
- **Mechanism:** The hybrid TCN-Mamba architecture separates positional/feature encoding (TCN) from sequence aggregation (Mamba), avoiding the massive parameter overhead of deep ResNet classifiers or heavy Transformers.
- **Core assumption:** High-dimensional feature extraction is less critical than efficient temporal modeling of interval statistics for prediction tasks.
- **Evidence anchors:** Abstract states "uses only 73.5 thousand parameters... outperforming traditional CNN-RNN models"; section V.B Table IV compares favorably against Hannun et al. (10.7M params) and Lin et al. (300K params).

## Foundational Learning

- **Concept:** RR Intervals (RRIs)
  - **Why needed here:** This is the sole input to the model. Understanding that RRIs represent the time between heartbeats (and thus encode rhythm irregularity rather than electrical shape) is critical to understanding why the model works.
  - **Quick check question:** If a patient has a normal RR interval but abnormal P-wave morphology, can this specific model detect it? (Answer: No, it only sees timing).

- **Concept:** Causal vs. Dilated Convolutions
  - **Why needed here:** The paper explicitly claims "causal" to prevent cheating (using future data) and "dilated" to see further back. You must understand how a kernel size of 3 with dilation 4 "sees" a wider timeline without increasing parameters.
  - **Quick check question:** Why can't we use a standard bidirectional LSTM if we want to predict AF in the future from current data? (Answer: You can, but strict causality in the feature extractor ensures the model doesn't accidentally rely on post-event artifacts during training).

- **Concept:** State Space Models (SSMs) vs. Attention
  - **Why needed here:** Mamba is the core novelty here. You need to understand that SSMs map sequences to a latent state space (like RNNs) but allow parallelized training (like Transformers).
  - **Quick check question:** Why is Mamba described as having "linear time complexity" compared to the "quadratic complexity" of Transformers?

## Architecture Onboarding

- **Component map:** Input (1×1800) -> 3×TCN Blocks -> MaxPooling1D -> Mamba Block -> GAP+GMP (concatenated) -> 2×FC Blocks (64→32→2) -> Softmax

- **Critical path:** The interaction between the TCN dilation rate and the Mamba state dimension. The TCN must provide a sufficiently compressed yet wide receptive field so the Mamba layer isn't overwhelmed by high-frequency noise.

- **Design tradeoffs:**
  - RRIs only vs. ECG: Trading morphological detail for extreme privacy and device compatibility
  - TCN-Mamba vs. Transformer: Trading theoretical maximum accuracy for inference speed and small memory footprint
  - Global Pooling: The concatenation of Average and Max pooling attempts to preserve both baseline trends and sporadic spikes (ectopic beats)

- **Failure signatures:**
  - NSR with Ectopy: The model struggles to distinguish NSR with frequent ectopic beats from subtle pre-AF patterns, suggesting high false positive rates in this scenario
  - Data Leakage: Improper shuffling of segments from the same subject into Train and Test sets would artificially inflate metrics

- **First 3 experiments:**
  1. Re-implementation Check: Train a "TCN-Only" and "Mamba-Only" version on the IRIDIA-AF dataset to quantify the specific contribution of the Mamba layer vs. the TCN backbone
  2. Input Duration Ablation: Test the model on 5-min, 15-min, and 60-min windows to see if the 30-min input is a hard requirement or a flexible constraint
  3. Noise Robustness: Introduce synthetic jitter or missing beats into the RRI input to simulate motion artifacts common in wearable devices

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the model generalize to larger and more diverse outpatient cohorts beyond the IRIDIA-AF and NSR RR datasets?
- **Basis in paper:** "While our model performs well in metrics, further validation across larger and more diverse outpatient datasets is necessary to assess its generalizability."
- **Why unresolved:** Only two outpatient datasets with limited demographic diversity were used for evaluation.
- **What evidence would resolve it:** Prospective validation on independent, multi-center datasets with diverse populations.

### Open Question 2
- **Question:** Does incorporating additional biosignals (ECG morphology, PPG, EDA) significantly improve prediction accuracy and clinical utility?
- **Basis in paper:** "In future work, we aim to incorporate additional biosignals such as ECG, PPG, EDA, and evaluate performance across multiple cohorts to enhance model robustness and clinical utility."
- **Why unresolved:** Current model uses only RR intervals, discarding potentially valuable signal information.
- **What evidence would resolve it:** Comparative study with multimodal inputs demonstrating statistically significant performance improvements.

### Open Question 3
- **Question:** What physiological mechanisms drive the model's predictions, and can it provide interpretable biomarkers for clinical decision-making?
- **Basis in paper:** "The reliance on RRI alone, while advantageous for wearable devices, may limit interpretability compared to multi-modal approaches."
- **Why unresolved:** Deep learning architecture offers limited transparency into which features trigger predictions.
- **What evidence would resolve it:** Explainability analysis (e.g., saliency maps, attention weights) correlating model attention with known pre-AF physiological markers.

## Limitations

- **Limited dataset size**: The IRIDIA-AF dataset contains only 152 subjects, which may limit generalizability despite subject-wise splits.
- **Unexplained Mamba hyperparameters**: Critical SSM configuration parameters are not explicitly specified, making exact reproduction difficult.
- **RRI preprocessing ambiguity**: The exact preprocessing pipeline for raw RR intervals is not detailed, which could significantly impact results.

## Confidence

- **High confidence**: In the general architecture approach (TCN-Mamba hybrid) and its computational efficiency claims (73.5K parameters, 38.3 MFLOPs)
- **Medium confidence**: In the specific performance metrics (AUROC 0.972, sensitivity 0.908) due to limited dataset size and lack of external validation
- **Low confidence**: In exact reproduction capability due to missing Mamba hyperparameter specifications and unclear preprocessing details

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary Mamba SSM parameters (d_state, d_conv, expansion factor) to determine their impact on performance and identify optimal configurations.

2. **External dataset validation**: Test the trained model on an independent AF prediction dataset to assess generalizability beyond the IRIDIA-AF cohort.

3. **Noise robustness evaluation**: Introduce realistic motion artifacts and missing beat patterns to RR intervals to evaluate real-world performance on wearable devices.