---
ver: rpa2
title: On the existence of consistent adversarial attacks in high-dimensional linear
  classification
arxiv_id: '2506.12454'
source_url: https://arxiv.org/abs/2506.12454
tags:
- consistent
- adversarial
- learning
- robust
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a framework to quantify vulnerability to\
  \ consistent adversarial attacks in high-dimensional binary classification, distinguishing\
  \ them from inconsistent attacks that change the true label. The authors define\
  \ new metrics\u2014consistent robust error and consistent boundary error\u2014that\
  \ isolate the impact of label-preserving perturbations."
---

# On the existence of consistent adversarial attacks in high-dimensional linear classification

## Quick Facts
- **arXiv ID:** 2506.12454
- **Source URL:** https://arxiv.org/abs/2506.12454
- **Reference count:** 40
- **Primary result:** Consistent adversarial attacks (label-preserving perturbations) are less effective than inconsistent attacks due to orthogonality constraints, and overparameterization increases vulnerability to consistent attacks on correctly classified points while potentially improving overall robust error.

## Executive Summary
This paper introduces a framework to quantify vulnerability to consistent adversarial attacks in high-dimensional binary classification, distinguishing them from inconsistent attacks that change the true label. The authors define new metrics—consistent robust error and consistent boundary error—that isolate the impact of label-preserving perturbations. For well-specified and latent space linear models, they derive exact asymptotic characterizations of these metrics under Gaussian assumptions, showing that consistent attacks are less effective than inconsistent ones due to orthogonality constraints. They further analyze robust empirical risk minimization, revealing that while overparameterization increases vulnerability to consistent attacks on correctly classified points, it can improve overall robustness by enhancing clean generalization. The results are validated with simulations, highlighting nuanced tradeoffs between overparameterization, sample size, and adversarial robustness.

## Method Summary
The paper analyzes high-dimensional binary linear classification under two models: well-specified (Gaussian covariates) and latent space (feature matrix with noise). The core innovation is distinguishing consistent attacks (perturbations that preserve the ground-truth label) from inconsistent attacks. Using convex Gaussian min-max theorem (CGMT) techniques, the authors derive exact asymptotic characterizations of three error metrics: standard robust error (E_rob), consistent robust error (E_cns_rob), and consistent boundary error (E_cns_bnd). They analyze robust empirical risk minimization with ℓ₂ regularization and elastic-net penalties, showing that overparameterization increases boundary vulnerability while potentially improving overall robust performance through better clean generalization.

## Key Results
- Consistent attacks are strictly less effective than inconsistent attacks due to orthogonality constraints, with attack success probability decreasing as the attack norm q increases.
- Overparameterization increases vulnerability to consistent attacks on correctly classified points (E_cns_bnd increases with ψ = p/n).
- Despite increased boundary vulnerability, overparameterization can improve overall consistent robust error through better clean generalization when sample complexity α = n/d is sufficiently large.
- The choice of regularization geometry s should match the attack geometry q for optimal defense, with elastic-net regularization (ℓ₁/ℓ₂ mixed) outperforming pure ℓ₂ regularization when facing ℓ₁ attacks.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Consistent attacks are strictly less effective than inconsistent attacks for the same perturbation budget.
- **Mechanism:** Consistent attacks must satisfy an orthogonality constraint ⟨w⋆, δ⟩ = 0 (the perturbation cannot change the ground-truth classifier's output), restricting admissible perturbations to a hyperplane H_q(ε). This reduces the effective attack strength by a factor ρ = d_q^*(ŵ⊥)/||ŵ||_{q*} ∈ [0,1], where d_q^*(ŵ⊥) measures the orthogonal component of the predictor weights relative to the target weights.
- **Core assumption:** Gaussian covariates x ~ N(0, 1/d I_d) and linear classifiers with monotonic link functions.
- **Evidence anchors:**
  - [abstract] "consistent attacks are less effective than inconsistent ones due to orthogonality constraints"
  - [Section 3.1, Proposition 1 and Corollary 1] Derivation showing consistent attacks require ε d_q^*(ŵ⊥) ≥ |⟨ŵ, x⟩| while inconsistent attacks only need ε ||ŵ||_{q*} ≥ |⟨ŵ, x⟩|
  - [corpus] Weak direct evidence—neighbor papers focus on empirical attack transferability rather than this theoretical distinction.
- **Break condition:** If the ground-truth classifier is non-linear or the perturbation budget scales as ε = O(d^{-1/q*}) or faster, the probability of consistent attack existence approaches 1 even with orthogonality constraints.

### Mechanism 2
- **Claim:** Overparameterization increases vulnerability to consistent attacks on correctly classified points (consistent boundary error increases with ψ = p/n).
- **Mechanism:** In the latent variable model, increasing feature dimension p relative to latent dimension d allows the predictor ŝ ∈ R^p to place energy in Ker(F^⊤) (directions orthogonal to the latent space). While this can improve clean generalization, it also increases d_q^*(P_⊥ F^⊤ ŝ), expanding the attack surface for label-preserving perturbations on points near the decision boundary.
- **Core assumption:** Latent space model with feature matrix F and independent covariate noise; robust ERM with ℓ_2 regularization.
- **Evidence anchors:**
  - [abstract] "as models become more overparameterized, their vulnerability to label-preserving perturbations grows"
  - [Section 4.3, Figure 5 (Right)] Shows E^cns_bnd increasing with ψ = p/n while E^cns_rob decreases
  - [corpus] Mixed—Ribeiro & Schön (2023) show similar phenomena in regression, but empirical results in Chen et al. (2024) show contradictory observations in neural networks.
- **Break condition:** If regularization λ is optimally tuned jointly with robust training budget r, the boundary error increase can be partially mitigated.

### Mechanism 3
- **Claim:** Despite increased boundary vulnerability, overparameterization can improve overall consistent robust error through better clean generalization.
- **Mechanism:** E^cns_rob combines both (a) attacks on correctly classified points and (b) attacks on already-misclassified points. Overparameterization improves clean accuracy (reducing misclassified points in category b), which can dominate the increased vulnerability in category a. The net effect depends on sample complexity α = n/d.
- **Core assumption:** The beneficial effect of overparameterization on clean generalization outweighs its harmful effect on boundary vulnerability; holds when α is sufficiently large.
- **Evidence anchors:**
  - [Section 5] "while the boundary error increases with overparameterization...the overall consistent robust error decreases"
  - [Section 4.3, Figure 5 (Left)] Shows all metrics decreasing with α (more data), with crossing points for different γ levels indicating optimal overparameterization depends on data availability
  - [corpus] Indirect support from Hastie et al. (2022) on benign interpolation in ridge regression.
- **Break condition:** At very low sample complexity α, overparameterization harms all metrics—more data is prerequisite for beneficial effects.

## Foundational Learning

- **Concept: Convex Gaussian Min-Max Theorem (CGMT)**
  - **Why needed here:** The paper uses CGMT to reduce high-dimensional optimization (robust ERM) to low-dimensional scalar equations, enabling exact asymptotic characterization.
  - **Quick check question:** Can you explain why CGMT allows replacing a random matrix G with scalar Gaussian variables g, h while preserving minimax values?

- **Concept: Moreau Envelope and Proximal Operators**
  - **Why needed here:** These appear in the self-consistent equations (Theorem 4.1) characterizing the asymptotic behavior of robust ERM solutions.
  - **Quick check question:** For a convex function f, what is the relationship between ∇M_V[f](ω) and the proximal operator P_V[f](ω)?

- **Concept: Latent Variable Models for Overparameterization**
  - **Why needed here:** This model (x = Fz + u) connects to random features and two-layer networks, making theoretical results relevant to practical overparameterized systems.
  - **Quick check question:** In this model, what does the ratio γ = d/p represent, and how does γ < 1 vs. γ > 1 change the structure of F?

## Architecture Onboarding

- **Component map:** Existence condition checker → Error metric evaluators → Self-consistent equation solver → Proximal operator library
- **Critical path:**
  1. Define problem setting (well-specified vs. latent space, attack geometry q, perturbation budget ε).
  2. If analyzing trained predictors: solve self-consistent equations → obtain {m, q, V, P}.
  3. Compute error metrics using the joint Gaussian integral forms.
  4. Validate against finite-d simulations (d = 500–1000 typical for agreement).
- **Design tradeoffs:**
  - **Attack norm q:** Lower q (e.g., q=1) yields lower attack success probability but may not match practical threat models (typically q=∞).
  - **Regularization geometry s:** Matching s to attack q provides optimal defense (per Remark 3 and eq. 13), but requires knowing attack geometry a priori.
  - **Robust training budget r:** Higher r improves robust error but can hurt clean accuracy—optimal tuning requires solving the full system.
- **Failure signatures:**
  - Non-convergence of fixed-point iteration: often due to insufficient damping (try μ ≥ 0.5).
  - Large gap between theory and simulation at finite d: check that ε scaling follows Assumption 3.2 (ε̃ = ε d^{1/q*}).
  - E^cns_rob > E_rob: indicates implementation error—nested constraint sets guarantee E^cns_rob ≤ E_rob.
- **First 3 experiments:**
  1. Reproduce Figure 3 (Center): Compute E_rob, E^cns_rob, E^cns_bnd vs. ε̃ for well-specified model with d=500, α=1.0; validate theory-simulation agreement.
  2. Reproduce Figure 5 (Right): Sweep ψ = p/n in latent space model with robust training; confirm diverging trends of E^cns_bnd (up) vs. E^cns_rob (down).
  3. Ablation on attack geometry: Fix all parameters, vary q ∈ {1, 2, ∞}; verify that consistent attack probability decreases with q (Figure 2, Left pattern).

## Open Questions the Paper Calls Out

- **Open Question 1:** What is the connection between consistent adversarial robustness and other robustness notions such as distributional robustness or robustness to natural perturbations?
  - **Basis in paper:** [explicit] "Several limitations and directions for future work emerge from our study. Specifically exploring the connection between consistent adversarial robustness and other notions of robustness, such as distributional robustness or robustness to natural perturbations."
  - **Why unresolved:** The paper focuses exclusively on adversarial perturbations with norm-bounded constraints; it does not analyze how the theoretical framework extends to other robustness paradigms.
  - **What evidence would resolve it:** Deriving relationships or equivalences between consistent robust error metrics and distributionally robust generalization bounds, or empirical studies comparing consistent adversarial training to distributionally robust methods.

- **Open Question 2:** How do the theoretical guarantees for consistent attacks extend beyond the Gaussian data assumption to more realistic data distributions?
  - **Basis in paper:** [inferred] The main theoretical results (Theorems 3.1, 4.1, 4.2) rely heavily on Assumption 3.1 (isotropic Gaussian covariates) and Assumption 4.2 (Gaussian latent variables). While Gaussian universality is mentioned, formal extension is not provided.
  - **Why unresolved:** The exact asymptotic characterizations depend on closed-form solutions for integrals under Gaussianity; non-Gaussian settings require different analytical tools.
  - **What evidence would resolve it:** Theoretical analysis showing universality of results for sub-Gaussian or heavy-tailed distributions, or simulation results demonstrating matching behavior on non-Gaussian synthetic data.

- **Open Question 3:** Can the framework for consistent adversarial robustness be extended to nonlinear classifiers, particularly multi-layer neural networks?
  - **Basis in paper:** [inferred] The paper analyzes linear classifiers and the latent space model (linked to random features), but explicitly states the latent model is "a convenient testbed" for studying overparameterization, not a full characterization of deep networks.
  - **Why unresolved:** The CGMT-based proofs rely on convexity of the ERM problem; neural networks introduce non-convexity that breaks the current analysis.
  - **What evidence would resolve it:** Derivation of analogous consistent robust error metrics for two-layer neural networks beyond the random features regime, or empirical studies comparing theoretical predictions to actual deep network behavior.

## Limitations
- The theoretical framework assumes Gaussian covariates and infinite-dimensional limits, which may not capture real-world data structures with correlations or heavy tails.
- Regularization parameter tuning is described as "optimal" but lacks specification of the tuning procedure, potentially affecting reproducibility.
- The attack geometry (choice of q) critically influences results, yet practical attack implementations often use different norms (e.g., ℓ∞), creating a gap between theory and practice.

## Confidence
- **High confidence:** The orthogonality constraint mechanism separating consistent from inconsistent attacks has strong theoretical backing through Proposition 1 and its corollaries.
- **Medium confidence:** The overparameterization vulnerability claims are supported by asymptotic analysis but may be sensitive to the specific latent space model assumptions.
- **Low confidence:** The translation of theoretical findings to practical deep learning systems is uncertain, as results are derived for linear models while most adversarial defense research focuses on nonlinear neural networks.

## Next Checks
1. **Finite-sample validation:** Implement the error metric computation for d=100-1000 with varying α ratios to identify where theoretical predictions (based on d→∞) begin to match empirical observations, particularly for the overparameterization effects.
2. **Model specification ablation:** Test the sensitivity of results to link function choice (logit vs. probit vs. noiseless) and loss function selection (logistic vs. hinge) to determine which components are essential versus technical artifacts.
3. **Non-Gaussian robustness:** Replace Gaussian covariates with correlated or heavy-tailed distributions to assess the stability of the orthogonality constraint mechanism and overparameterization vulnerability claims under more realistic data conditions.