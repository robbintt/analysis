---
ver: rpa2
title: 'RiskAgent: Autonomous Medical AI Copilot for Generalist Risk Prediction'
arxiv_id: '2503.03802'
source_url: https://arxiv.org/abs/2503.03802
tags:
- llms
- medical
- risk
- tools
- tool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents RiskAgent, a system that leverages large language
  models (LLMs) to predict medical risks by collaborating with existing evidence-based
  clinical tools. Instead of fine-tuning models on extensive datasets, RiskAgent uses
  three specialized agents (Decider, Executor, Reviewer) to select appropriate risk
  calculators, parse inputs, execute tools, and review outputs.
---

# RiskAgent: Autonomous Medical AI Copilot for Generalist Risk Prediction

## Quick Facts
- arXiv ID: 2503.03802
- Source URL: https://arxiv.org/abs/2503.03802
- Reference count: 40
- Key outcome: RiskAgent achieves 76.33% accuracy on MedRisk benchmark vs 38.39% for GPT-4o by orchestrating clinical calculators

## Executive Summary
RiskAgent is a system that uses large language models to predict medical risks by orchestrating evidence-based clinical calculators rather than fine-tuning on medical data. The system employs three specialized agents (Decider, Executor, Reviewer) to select appropriate risk calculators, parse patient parameters, execute tools, and review outputs. On a new benchmark covering 12,352 questions across 154 diseases, RiskAgent outperforms GPT-4o by more than double (76.33% vs 38.39%) while requiring fewer resources than fine-tuned alternatives.

## Method Summary
RiskAgent uses a three-agent architecture with LLMs that orchestrate 387 clinical calculators from MDCalc. The Decider selects tools, the Executor parses parameters and invokes tools, and the Reviewer validates the pipeline. The system uses retrieval-ranking to narrow tool selection to top candidates, then performs multi-step reasoning through five Environments for tool selection, parameter extraction, execution, answer formatting, and review. Training uses LoRA fine-tuning on 8B LLaMA-3 with specific hyperparameters (r=8, alpha=16, dropout=0) for 5 epochs.

## Key Results
- RiskAgent achieves 76.33% accuracy on MedRisk-Quantitative and 78.34% on MedRisk-Qualitative benchmarks
- Outperforms GPT-4o (38.39%) and other commercial LLMs on the same tasks
- Shows strong generalization to rare diseases (75.91% accuracy) and external clinical diagnosis datasets
- Ablation studies show multi-agent architecture improves tool selection by 9.66% and parameter parsing by 30.20% over single-agent baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tool orchestration reduces learning burden compared to end-to-end medical knowledge acquisition
- Mechanism: Instead of fine-tuning LLMs to internalize medical knowledge (requiring 1Mâ€“80B tokens per prior work), RiskAgent trains LLMs to select, parameterize, and interpret outputs from existing evidence-based clinical calculators
- Core assumption: Evidence-based tools provide reliable, interpretable outputs that the LLM can synthesize without understanding their internal math
- Evidence anchors: [abstract] "RiskAgent is designed to collaborate with hundreds of clinical decision tools" and [Page 4] "This greatly reduces LLM's learning pressure"

### Mechanism 2
- Claim: Role separation across agents improves tool selection and parameter parsing accuracy
- Mechanism: The Decider selects tools; the Executor parses parameters and invokes tools; the Reviewer validates the pipeline. Ablation shows adding agents improves tool selection by 9.66% and parameter parsing by 30.20% over single-agent baseline
- Core assumption: The decomposition aligns with distinct cognitive tasks that benefit from specialization
- Evidence anchors: [Page 10, Table 3] Setting (b) vs (a) shows gains from introducing Decider+Executor; Reviewer adds further improvement

### Mechanism 3
- Claim: Retrieval-ranking narrows tool selection to a tractable candidate set with high recall
- Mechanism: An embedding-based retrieval step computes cosine similarity between patient-question pairs and tool metadata, then ranks and returns top-N tools. With N=5, recall reaches 99.8%
- Core assumption: Tool metadata captures sufficient semantic information for relevance matching
- Evidence anchors: [Page 15] "Setting N = 5 achieves a 99.8% recall score"

## Foundational Learning

- **Tool use / Function calling in LLMs**
  - Why needed here: RiskAgent's core innovation is treating clinical calculators as external tools the LLM orchestrates, not internalizing their logic
  - Quick check question: Can you explain the difference between an LLM generating an answer directly vs. calling an external function to compute it?

- **Multi-agent orchestration**
  - Why needed here: The system divides labor across Decider, Executor, and Reviewer; understanding role boundaries is critical for debugging
  - Quick check question: What types of errors would be caught by a Reviewer agent but not by a Decider?

- **Evidence-based medicine (EBM) principles**
  - Why needed here: The system's trustworthiness claim rests on using validated clinical calculators rather than LLM-generated probabilities
  - Quick check question: Why would a clinician trust a calculator-derived risk score more than an LLM's verbal estimate?

## Architecture Onboarding

- **Component map**: Patient data + question -> Environment 1 (retrieval-ranking) -> Decider selects tool -> Environment 2 (parameter schema) -> Executor parses parameters -> Tool execution -> Environment 3 (output storage) -> Decider interprets -> Environment 4 (answer formatting) -> Reviewer validates -> Environment 5 (review history)

- **Critical path**: 1) Retrieve top-N tools via similarity matching; 2) Decider selects tool; 3) Executor parses parameters using schema; 4) Invoke tool and store output; 5) Decider interprets results; 6) Format answer; 7) Reviewer validates or triggers re-execution (max 3 attempts)

- **Design tradeoffs**: Single 8B model serves all roles (parameter-efficient) vs. separate models per role (higher capacity but more complexity); Fixed tool library (387 tools) vs. dynamic tool discovery (more flexible but harder to validate); Retrieval top-N=5 balances recall vs. decision complexity

- **Failure signatures**: Tool not in library (system cannot proceed); Parameter parsing failure (Executor cannot extract required values); Reviewer over-correction (iterative loop exhausts max attempts without convergence)

- **First 3 experiments**: 1) Sanity check: Run RiskAgent-8B on MedRisk-Qualitative subset; verify tool selection accuracy matches 92.94%; 2) Ablation of Reviewer: Disable Reviewer, measure accuracy drop on MedRisk-Quantitative; 3) Retrieval sensitivity: Vary top-N (3, 5, 10), plot recall vs. accuracy to confirm N=5 is optimal

## Open Questions the Paper Calls Out

- **Dataset generation bias**: The paper acknowledges that GPT-4o was used to generate the MedRisk benchmark questions, which may introduce dataset artifacts that inflate LLM performance
- **Generalizability to unstructured data**: The system's performance on real-world unstructured clinical notes versus standardized MedRisk benchmark remains unevaluated
- **Reviewer robustness**: The paper notes Reviewer validates parameter extraction but doesn't detail how it handles semantically incorrect but schema-valid inputs

## Limitations

- Tool library coverage may be insufficient for many real-world clinical scenarios not covered by the 387 MDCalc calculators
- Performance evaluation relies on synthetically generated benchmark data rather than real clinical cases
- The system cannot handle scenarios where required tools are missing from the library

## Confidence

- **High**: Core claim that tool orchestration outperforms direct LLM reasoning on structured medical risk prediction tasks
- **Medium**: Claim that role separation across agents improves performance (mechanism requires deeper analysis)
- **Low**: Generalizability claim to rare diseases and external clinical diagnosis tasks (evaluation lacks breadth)

## Next Checks

1. **Tool Coverage Analysis**: Systematically catalog which clinical scenarios in a representative sample of EHR data lack corresponding tools in the 387-tool library. Measure the fraction of real-world cases requiring direct LLM reasoning.

2. **Error Propagation Analysis**: Instrument the three-agent system to track error types at each stage and measure how often errors propagate vs. get caught. Compare error recovery rates when Reviewer is enabled vs. disabled.

3. **Unstructured Data Performance**: Evaluate RiskAgent on MedRisk where patient information is provided as free-text clinical notes rather than structured parameters. Measure parameter parsing accuracy drop and overall system performance degradation.