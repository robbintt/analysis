---
ver: rpa2
title: 'BWCache: Accelerating Video Diffusion Transformers through Block-Wise Caching'
arxiv_id: '2509.13789'
source_url: https://arxiv.org/abs/2509.13789
tags:
- video
- diffusion
- uni000003ec
- bwcache
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the high inference latency of video diffusion
  transformers (DiTs), which limits their real-world applicability. The authors propose
  Block-Wise Caching (BWCache), a training-free method that dynamically caches and
  reuses features from DiT blocks across diffusion timesteps.
---

# BWCache: Accelerating Video Diffusion Transformers through Block-Wise Caching

## Quick Facts
- arXiv ID: 2509.13789
- Source URL: https://arxiv.org/abs/2509.13789
- Reference count: 21
- Primary result: Training-free block-wise caching achieves up to 2.24× speedup on video diffusion models while maintaining visual quality

## Executive Summary
This paper addresses the high inference latency of video diffusion transformers (DiTs) through Block-Wise Caching (BWCache), a training-free acceleration method. The approach dynamically caches and reuses DiT block features across diffusion timesteps when similarity conditions are met. By caching at the block level and using relative L1 distance as a similarity indicator, BWCache achieves significant speedups while maintaining visual fidelity across multiple video diffusion models including Open-Sora and Latte.

## Method Summary
BWCache intercepts DiT block outputs during inference, computing relative L1 distances between consecutive timesteps to determine cache reuse opportunities. When the average relative L1 distance falls below a threshold δ, cached features are reused for R subsequent steps to prevent drift. The method operates without architectural modifications, using only the existing DiT structure with periodic recomputation to maintain quality. Key hyperparameters include threshold δ (default 0.15) and reuse interval R (default 10% of timesteps).

## Key Results
- Achieves up to 2.24× speedup on video diffusion models with comparable visual quality
- Maintains VBench scores within 1% of baseline while significantly reducing latency
- Outperforms existing baselines in both efficiency and quality metrics across multiple models
- Reduces FLOPs by up to 45% while preserving SSIM and PSNR metrics

## Why This Works (Mechanism)

### Mechanism 1: Block-Level Granularity for Caching
Caching at the DiT block level provides optimal granularity for feature reuse. Complete block outputs (post-attention + MLP residuals) are cached and reused across timesteps when similarity conditions are met. This avoids information loss from coarse timestep-level caching while delivering better acceleration than fine-grained attention-level caching.

### Mechanism 2: Relative L1 Distance as Similarity Indicator
The aggregated relative L1 distance between block features at adjacent timesteps serves as an effective, training-free signal for cache reuse decisions. For each block i at timestep t, compute L1_rel(hi, t) = ||ht,i - ht+1,i||_1 / ||ht+1,i||_1. Average across all N blocks. If mean < threshold δ, reuse cached features; otherwise recompute.

### Mechanism 3: Periodic Recomputation Prevents Latent Drift
Periodically recomputing cached blocks at fixed intervals prevents cumulative error from continuous feature reuse. Within a caching interval, each block is recomputed every R steps (default R = 10% of total timesteps). This pattern maintains quality while enabling acceleration.

## Foundational Learning

- **Diffusion Denoising Process**: Understanding the sequential nature of timesteps (from noise to video) is essential to grasp why feature similarity varies and where redundancy exists.
  - Quick check: In a 30-step diffusion process, which timesteps typically show the highest feature variation?

- **Transformer Block Structure (Attention + MLP with Residuals)**: The method caches complete block outputs; you must understand what gets computed and cached (AdaLN → Attention → residual → AdaLN → MLP → residual).
  - Quick check: If you cache the output of a DiT block at timestep t, what specifically gets stored and fed to the next block at t-1?

- **L1 Distance vs. Perceptual Metrics**: The method uses L1 as a proxy for visual fidelity; understanding its limitations helps interpret when the indicator may fail.
  - Quick check: Why might relative L1 distance be preferable to absolute L1 for comparing features across different timesteps?

## Architecture Onboarding

- **Component map**: Input noise → T5 encoder → DiT Backbone (N blocks) → BWCache Layer → Cache Store → VAE decoder → Output frames
- **Critical path**: 
  1. At timestep t, compute all block outputs; store in cache
  2. At timestep t-1, compute L1_rel for each block vs. cached version
  3. If mean(L1_rel) < δ AND not in last k/2 steps: reuse cache for R steps
  4. After R steps or if condition fails: recompute, update cache
- **Design tradeoffs**:
  - **Threshold δ**: Lower → higher quality, lower speedup; higher → faster, potential artifacts
  - **Reuse interval R**: Smaller → less drift, more compute; larger → faster, risk of detail loss
  - **Last-step exclusion**: Excluding more steps improves final quality but reduces overall acceleration
- **Failure signatures**:
  - Blurry or flickering output: δ too high or R too large for the content
  - Minimal speedup: δ too low; cache rarely triggered
  - Drift in motion coherence: Periodic recomputation insufficient for high-dynamic scenes
- **First 3 experiments**:
  1. Baseline profiling: Run original model, record per-block latency and L1_rel heatmap to confirm U-shaped pattern
  2. Threshold sweep: Test δ ∈ {0.10, 0.15, 0.20, 0.25} on 50 videos; plot VBench vs. latency
  3. Ablation on R: Test R ∈ {5%, 10%, 15%, 20%} with fixed δ=0.15; measure quality drift on high-motion vs. static prompts

## Open Questions the Paper Calls Out

### Open Question 1
How can the indicator threshold (δ) be dynamically adjusted during inference to adapt to different generation tasks without manual presetting? The current method relies on a static threshold determined via ablation studies, requiring manual intervention to optimize the trade-off between latency and visual fidelity for varying content dynamics.

### Open Question 2
Is the block-wise caching strategy and the observed U-shaped feature variation pattern applicable to U-Net based video diffusion models? The paper focuses exclusively on DiTs and does not test the method on U-Net architectures like Stable Video Diffusion.

### Open Question 3
What are the theoretical underpinnings of the U-shaped feature variation pattern, and does it persist across different noise schedulers? The method empirically observes this pattern but provides no theoretical explanation for the phenomenon.

## Limitations
- The U-shaped similarity pattern may not generalize to all video content types, particularly high-motion sequences
- Relative L1 distance assumes linear correlation with perceptual quality, which may break down for complex scenes
- Method has only been validated on video diffusion transformers, not on U-Net based models
- Performance depends on finding optimal threshold δ through manual tuning

## Confidence

- **High Confidence**: Block-level caching provides meaningful acceleration over timestep-level approaches (supported by timing analysis and comparative results)
- **Medium Confidence**: Relative L1 distance effectively predicts cache quality (based on ablation studies but lacks comparison to perceptual metrics)
- **Medium Confidence**: Periodic recomputation prevents drift (supported by quality metrics but not explicitly validated for different content types)

## Next Checks

1. Test BWCache on video sequences with rapid camera motion or complex object dynamics to validate whether the U-shaped similarity pattern holds
2. Compare relative L1 distance against perceptual metrics (LPIPS, FID) across different threshold values to quantify false positive/negative rates
3. Implement BWCache in a non-video diffusion transformer (text-to-image) to test generalizability beyond the video domain