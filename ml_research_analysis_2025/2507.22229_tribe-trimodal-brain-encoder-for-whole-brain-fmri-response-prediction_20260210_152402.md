---
ver: rpa2
title: 'TRIBE: TRImodal Brain Encoder for whole-brain fMRI response prediction'
arxiv_id: '2507.22229'
source_url: https://arxiv.org/abs/2507.22229
tags:
- brain
- encoding
- each
- modalities
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TRIBE, the first deep learning pipeline that
  predicts brain responses to videos using multimodal inputs (text, audio, video)
  across multiple cortical regions and subjects. The model combines pretrained representations
  from Llama-3.2-3B, Wav2Vec-Bert-2.0, and V-JEPA-2-Gigantic, processes them with
  a transformer, and achieves state-of-the-art results by winning the Algonauts 2025
  competition.
---

# TRIBE: TRImodal Brain Encoder for whole-brain fMRI response prediction

## Quick Facts
- arXiv ID: 2507.22229
- Source URL: https://arxiv.org/abs/2507.22229
- Reference count: 40
- Primary result: Multimodal transformer model achieves 0.2146 mean Pearson correlation across 1,000 cortical parcels, winning Algonauts 2025 competition

## Executive Summary
TRIBE is the first deep learning pipeline that predicts brain responses to videos using multimodal inputs (text, audio, video) across multiple cortical regions and subjects. The model combines pretrained representations from Llama-3.2-3B, Wav2Vec-Bert-2.0, and V-JEPA-2-Gigantic, processes them with a transformer, and achieves state-of-the-art results by winning the Algonauts 2025 competition. TRIBE reaches a mean Pearson correlation of 0.2146 across 1,000 brain parcels, significantly outperforming competitors. Ablations show that the multimodal, multisubject, and nonlinear aspects of the model are crucial, especially in associative cortices, where multimodal integration yields up to 30% improvement over unimodal approaches.

## Method Summary
TRIBE extracts multimodal features at 2Hz from pretrained foundation models: Llama-3.2-3B for text (2048-dim), Wav2Vec-Bert-2.0 for audio (1024-dim), and V-JEPA-2-Gigantic for video (1280-dim). These are projected to 1024-dim, concatenated, and processed by an 8-layer transformer with subject embeddings and learnable positional encodings. The output is adaptively pooled to TR resolution (0.67Hz) and passed through a subject-conditional linear layer to predict 1,000 cortical parcels. Training uses AdamW with modality dropout, SWA, and MSE loss on the Courtois NeuroMod dataset (4 subjects watching Friends and 4 movies).

## Key Results
- Mean Pearson correlation of 0.2146 across 1,000 parcels, winning Algonauts 2025 competition
- Multimodal integration yields up to 30% improvement over unimodal approaches in associative cortices
- Model generalizes to out-of-distribution movies with correlations of 0.17-0.19
- Ablations show transformer contributes 25% improvement, subject embeddings contribute 6% improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal integration improves brain encoding, particularly in associative cortices that integrate multiple sensory streams.
- Mechanism: Pretrained unimodal encoders capture modality-specific features → linear projection to shared 1024-dim space → concatenation → transformer learns cross-modal temporal interactions across ~100 TR windows.
- Core assumption: Foundation model representations align with neural representations in corresponding sensory cortices.
- Evidence anchors: [abstract] "multimodal integration yields up to 30% improvement over unimodal approaches" in associative cortices; [section 3.3] Multimodal encoder achieves 0.31 validation Pearson vs 0.25 for best unimodal (video); Figure 4b shows highest gains in prefrontal and parieto-occipito-temporal cortices.

### Mechanism 2
- Claim: Nonlinear transformer-based temporal integration outperforms linear ridge regression for mapping stimuli to brain responses.
- Mechanism: 8-layer transformer with 8 attention heads processes 2Hz feature sequences → adaptive pooling compresses to TR resolution (0.67Hz) → subject-conditional linear readout to 1000 parcels.
- Core assumption: Brain responses depend on extended temporal context beyond immediate stimulus; hemodynamic response introduces lag requiring learned temporal integration.
- Evidence anchors: [abstract] Model addresses "linearity" limitation of existing approaches that use ridge regression; [section 3.5, Figure 6a] Removing transformer drops encoding score from 0.31 to 0.23 (25% relative decrease).

### Mechanism 3
- Claim: Multi-subject training with subject embeddings improves predictions by sharing statistical strength across individuals while preserving individual differences.
- Mechanism: Shared transformer backbone processes all subjects → learnable subject embedding added to input → subject-conditional final linear layer (following Défossez et al. 2023).
- Core assumption: Inter-subject variability is structured and partially shared; brains have common representational geometry with individual variations.
- Evidence anchors: [abstract] Model "predicts brain responses to videos... across multiple cortical regions and subjects"; [section 3.5, Figure 6a] Training separately per subject drops score from 0.31 to 0.29 (6% relative decrease).

## Foundational Learning

- Concept: **BOLD fMRI hemodynamics**
  - Why needed here: Understanding that fMRI measures blood oxygenation changes (not direct neural activity), with TR=1.49s temporal resolution and inherent hemodynamic lag, explains why features are extracted at 2Hz then pooled to TR rate.
  - Quick check question: Why must the model pool 2Hz stimulus features down to 0.67Hz (1/TR) before prediction?

- Concept: **Foundation model feature extraction**
  - Why needed here: TRIBE relies entirely on frozen pretrained models (Llama-3.2-3B for text, Wav2Vec-Bert for audio, V-JEPA for video) — understanding what layers to extract and how to aggregate them is critical.
  - Quick check question: Why does the paper extract from multiple intermediate layers (not just final layer) and average them in groups?

- Concept: **Attention-based temporal modeling**
  - Why needed here: The transformer encoder enables information exchange across timesteps; understanding positional embeddings and multi-head attention helps diagnose what temporal patterns the model captures.
  - Quick check question: Why use learnable positional embeddings rather than sinusoidal encodings for this time-series task?

## Architecture Onboarding

- Component map: Stimulus → foundation model extraction → layer grouping/averaging → modality projection → concatenation → add subject/positional embeddings → transformer → adaptive pooling → subject-conditional readout → parcel predictions

- Critical path: Stimulus → foundation model extraction → layer grouping/averaging → modality projection → concatenation → add subject/positional embeddings → transformer → adaptive pooling → subject-conditional readout → parcel predictions

- Design tradeoffs:
  - Spatial averaging of V-JEPA patches discards retinotopic information (paper acknowledges this hurts primary visual cortex)
  - 1000-parcel parcellation improves SNR but limits spatial resolution vs voxel-level
  - Audio features are bidirectional (future context), text/video are past-only — temporal causality is inconsistent across modalities
  - Ensembling 1000 models for competition is impractical for research use

- Failure signatures:
  - Primary visual cortex: multimodal underperforms video-only (Figure 4b)
  - OOD generalization: cartoons/silent films drop to 0.17-0.19 correlation (Table 2)
  - Subject 5 consistently underperforms other subjects (0.17 vs 0.21-0.24)

- First 3 experiments:
  1. **Unimodal baselines per parcel**: Train text-only, audio-only, video-only models; map which parcels each modality predicts best to validate modality-brain correspondence
  2. **Transformer depth ablation**: Test 2/4/8 layers to find minimum depth needed; if 2 layers match 8, temporal modeling is less critical than claimed
  3. **Subject embedding necessity**: Train with vs without subject embeddings on held-out subjects to quantify generalization benefit vs overfitting to training subjects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the TRIBE architecture be effectively adapted for voxel-level prediction to capture highly localized phenomena?
- Basis in paper: [explicit] The authors state in the Limitations section: "Adapting our model for voxel-level prediction is an important avenue for future work."
- Why unresolved: The current model operates on 1,000 parcels to manage computational costs and increase signal-to-noise ratios, which inherently limits spatial resolution.
- What evidence would resolve it: A variation of the model successfully predicting responses for individual voxels (rather than averaged parcels) without exceeding feasible computational resources.

### Open Question 2
- Question: Can this encoding framework be extended to cognitive domains beyond perception, such as behavior, memory, and decision-making?
- Basis in paper: [explicit] The authors explicitly note that the present approach "remains limited to perception and comprehension" and identify "behavior, memory and decisions" as components to integrate.
- Why unresolved: The model is currently trained exclusively on passive movie-watching stimuli, lacking the necessary target data or architectural components to model active cognitive processes.
- What evidence would resolve it: Successful application of the model to fMRI datasets involving active tasks (e.g., memory recall or decision-based paradigms) with maintained prediction accuracy.

### Open Question 3
- Question: How does encoding performance and intersubject generalization scale with a significantly larger pool of participants?
- Basis in paper: [explicit] The authors state that "only four participants were included" and that "extending and improving our results on a larger pool of participants is an important next step."
- Why unresolved: While the model handles intersubject variability in a small sample (n=4), it is untested whether the current subject-conditional scheme is sufficient for high variability across large populations.
- What evidence would resolve it: Evaluation of the model's encoding score distribution when trained and tested on a dataset with a much larger number of subjects (e.g., >50).

## Limitations
- Limited spatial resolution due to 1,000-parcel parcellation vs voxel-level prediction
- Restricted to perception/comprehension, not extending to behavior, memory, or decision-making
- Modest OOD generalization (0.17-0.19 correlation for cartoons/silent films vs 0.21-0.22 for training distributions)

## Confidence

- **High**: Multimodal integration improves brain encoding in associative cortices (supported by Figure 4b showing 30% gains in prefrontal and parieto-occipito-temporal regions)
- **Medium**: Nonlinear transformer temporal integration is necessary (supported by 25% drop when removed, but could be architecture-dependent)
- **Medium**: Multi-subject training with embeddings improves predictions (6% gain is modest and consistent with [AFIRE 2510.04670], but subject embedding mechanics are underspecified)

## Next Checks

1. **Temporal causality validation**: Verify that using past-only features for video/text (vs bidirectional for audio) doesn't artificially inflate multimodal gains - test with consistent temporal windows across all modalities.

2. **Subject embedding isolation**: Train with subject embeddings but hold out entire subjects from training to test true generalization vs overfitting, following the subject-conditional readout design from Défossez et al. 2023.

3. **Primary visual cortex analysis**: Conduct detailed parcel-level analysis in V1/V2 to understand why video-only outperforms multimodal, testing whether spatial averaging of V-JEPA patches is the limiting factor.