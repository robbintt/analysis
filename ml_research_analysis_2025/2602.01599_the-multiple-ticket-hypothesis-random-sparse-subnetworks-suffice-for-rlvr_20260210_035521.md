---
ver: rpa2
title: 'The Multiple Ticket Hypothesis: Random Sparse Subnetworks Suffice for RLVR'
arxiv_id: '2602.01599'
source_url: https://arxiv.org/abs/2602.01599
tags:
- random
- rlvr
- arxiv
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the Multiple Ticket Hypothesis, which proposes
  that pretrained models contain many viable sparse subnetworks for RLVR, rather than
  a single privileged subnetwork. The authors empirically demonstrate that training
  only 1% of randomly selected parameters at 99% sparsity matches or exceeds full-parameter
  RLVR finetuning across three models and two task domains.
---

# The Multiple Ticket Hypothesis: Random Sparse Subnetworks Suffice for RLVR

## Quick Facts
- **arXiv ID:** 2602.01599
- **Source URL:** https://arxiv.org/abs/2602.01599
- **Reference count:** 40
- **Primary result:** Training only 1% of randomly selected parameters at 99% sparsity matches or exceeds full-parameter RLVR finetuning across three models and two task domains

## Executive Summary
This paper challenges the traditional Lottery Ticket Hypothesis by demonstrating that pretrained transformer models contain many viable sparse subnetworks for RLVR, rather than a single privileged subnetwork. The authors show that randomly selecting 1% of parameters (99% sparsity) and training only those achieves performance comparable to or better than full-parameter RLVR across multiple models and tasks. This phenomenon is explained through the implicit KL constraints in RLVR, which restrict policy updates to a low-dimensional subspace, making arbitrary sparse masks effective.

## Method Summary
The method applies random sparse training to RLVR by uniformly sampling a fixed parameter mask at initialization without replacement. During training, only the selected parameters receive gradient updates while others remain frozen. The approach uses standard GRPO with masked gradients (element-wise multiplication of mask and computed gradients) and trains at various sparsity levels from 99% to 99.999%. Learning rates are scaled appropriately for different sparsity levels, with higher rates needed at extreme sparsities.

## Key Results
- Training 1% of randomly selected parameters at 99% sparsity matches or exceeds full-parameter RLVR finetuning across three models (Qwen2.5-0.5B, Qwen2.5-1.5B, Llama-3.1-8B) and two task domains
- Multiple independent random masks succeed with minimal overlap (≤0.5% Jaccard similarity), supporting the multiple tickets hypothesis
- Performance depends primarily on the number of trainable parameters (effective dimensionality), not their specific identity
- Sharp performance degradation occurs below ~0.01% trainable parameters

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Implicit KL constraints in RLVR restrict policy updates to a low-dimensional subspace, enabling random sparse masks to succeed.
- **Mechanism:** RLVR's clipping and on-policy sampling create an implicit trust region. The Fisher information matrix eigenspectrum analysis reveals effective rank r≈44 for a 490M parameter model.
- **Core assumption:** The Fisher information matrix has low effective rank, and per-step updates are small enough for second-order approximations to hold.
- **Evidence anchors:** Eigenspectrum analysis shows r≈44; Mukherjee et al. (2025) shows RLVR concentrates updates on 5-30% of parameters.
- **Break condition:** If KL constraints were removed entirely or tasks required higher intrinsic dimensionality than the mask density provides.

### Mechanism 2
- **Claim:** Eigenvector delocalization allows many different random parameter subsets to span the same low-dimensional policy-relevant subspace.
- **Mechanism:** If Fisher eigenvectors are spread across parameters, random sampling of k > r parameters preserves the geometric structure of the policy-relevant subspace.
- **Core assumption:** No single parameter dominates any eigenvector (Assumption F.2).
- **Evidence anchors:** Average Jaccard similarity ≈0.005 between successful masks; traditional Lottery Ticket Hypothesis assumes single privileged subnetwork.
- **Break condition:** If eigenvectors were highly localized (specific parameters dominating).

### Mechanism 3
- **Claim:** Performance depends on the number of trainable parameters (effective dimensionality), not their specific identity, creating a sparsity threshold.
- **Mechanism:** When k > r, random masks can approximate any KL-feasible update. Below this threshold, the restricted Gram matrix becomes singular.
- **Core assumption:** There exists a task-agnostic lower bound on trainable dimensionality required for RLVR.
- **Evidence anchors:** 99% to 99.95% sparsity maintains performance; 99.99%+ shows sharp degradation; Xu & Zhang (2024) showed similar random mask success for SFT.
- **Break condition:** More complex, multi-domain tasks may require higher intrinsic dimensionality.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: The paper uses GRPO which eliminates the value critic and uses group-relative advantages. Understanding this clarifies how the implicit KL constraint emerges.
  - Quick check question: Can you explain why GRPO's clipping mechanism creates an implicit trust region even without explicit KL regularization?

- **Concept: Fisher Information Matrix and Effective Rank**
  - Why needed here: The theoretical explanation hinges on the Fisher matrix having low effective rank. You need to understand how eigenvalue decay relates to parameter redundancy.
  - Quick check question: If a 490M parameter model has effective rank r≈44, what does this imply about the dimensionality of the policy-relevant update subspace?

- **Concept: Jaccard Similarity for Set Overlap**
  - Why needed here: The paper uses Jaccard similarity to prove multiple independent masks succeed. Understanding this metric validates the "multiple tickets" claim.
  - Quick check question: If two 1% masks on a 490M parameter model have Jaccard similarity ≈0.005, how many parameters do they share?

## Architecture Onboarding

- **Component map:** Mask sampling -> Gradient masking -> AdamW optimizer (with states only for unmasked parameters) -> GRPO training loop

- **Critical path:**
  1. Set target sparsity (99% recommended as baseline)
  2. Sample random mask with fixed seed (iterate through all layers)
  3. Initialize optimizer states only for unmasked parameters
  4. Apply mask to computed gradients before optimizer step
  5. Scale learning rate appropriately (higher at higher sparsities)

- **Design tradeoffs:**
  - Sparsity vs. stability: Higher sparsity (99.9%+) increases model collapse frequency
  - Learning rate vs. sparsity: LR scaling from 1e-4 (99% sparsity) to 1e-2 (99.999% sparsity)
  - Memory savings: 99% sparsity reduces optimizer memory by ~50x

- **Failure signatures:**
  - Model collapse: More frequent at extreme sparsities (>99.9%)
  - Sharp performance drop: Below ~0.01% trainable parameters
  - High variance: Different random seeds show more variance at higher sparsities

- **First 3 experiments:**
  1. Train Qwen2.5-0.5B on GSM8K with 99% sparsity random mask (seed=0), compare to full-parameter baseline. Expect ±2% performance match.
  2. Train 5 independent masks at 99% sparsity on same task. Compute pairwise Jaccard similarity and verify all achieve comparable performance despite <1% overlap.
  3. Test 99%, 99.5%, 99.9%, 99.95% sparsities with appropriate learning rates. Identify the degradation threshold for your specific task.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does random sparse RLVR training cause more catastrophic forgetting than full-parameter RLVR or structured sparse methods?
- **Basis in paper:** The authors conjecture that random sampling doesn't guarantee principal weights are selected, likely leading to more catastrophic forgetting.
- **Why unresolved:** No experiments were conducted measuring retention of pretrained capabilities after random sparse RLVR.
- **What evidence would resolve it:** Comparing pretrained task performance before and after random sparse RLVR vs. full-parameter RLVR, quantifying forgetting rates.

### Open Question 2
- **Question:** Does the ratio of "winning tickets" remain constant, grow, or shrink as model scale increases beyond 1.5B parameters?
- **Basis in paper:** Further validation on frontier-scale models (e.g., 70B+) is necessary to confirm if the ratio of "winning tickets" remains constant or grows with scale.
- **Why unresolved:** Experiments only covered 0.5B and 1.5B models; scaling behavior remains untested.
- **What evidence would resolve it:** Running identical random sparse training experiments on 7B, 70B, and larger models to measure whether the 1% trainable parameter threshold scales proportionally.

### Open Question 3
- **Question:** What causes the sharp performance collapse below ~0.01% trainable parameters, and can optimization improvements extend this threshold?
- **Basis in paper:** The authors noted higher frequency of model collapse at extreme sparsities and that the optimization path becomes increasingly narrow and sensitive to hyperparameter choices.
- **Why unresolved:** The threshold was identified empirically but the optimization dynamics at this boundary remain uncharacterized.
- **What evidence would resolve it:** Detailed analysis of gradient statistics, loss landscape geometry, and optimizer behavior near the sparsity threshold.

## Limitations

- Analysis is limited to encoder-decoder transformers (Qwen2.5-0.5B, Qwen2.5-1.5B, Llama-3.1-8B) on constrained tasks (reasoning, sorting, code generation)
- The effective rank estimation technique may not generalize to other architectures (CNNs, RNNs) or more open-ended tasks
- The implicit trust region mechanism may break down for tasks requiring larger policy updates or different reward structures

## Confidence

- **Mechanism 1:** Medium - The low effective rank claim is well-supported, but the assumption about second-order approximations throughout training is unverified
- **Mechanism 2:** Low-Medium - Empirical Jaccard results support multiple tickets, but eigenvector delocalization assumption remains mathematically unproven
- **Mechanism 3:** High - The sparsity threshold behavior is empirically robust, though the "task-agnostic" claim is extrapolated from limited experiments

## Next Checks

1. **Architecture Generalization Test:** Apply the 99% sparsity random mask approach to ResNet-50 on CIFAR-10 using standard supervised learning. Compare performance to full-parameter training and verify the effective rank analysis still explains the phenomenon.

2. **Dynamic Rank Verification:** Implement online tracking of Fisher information matrix eigenspectrum throughout RLVR training. Verify that effective rank remains stable (r≈44) across training epochs and that it correlates with the point where random mask performance degrades.

3. **Cross-Task Dimensionality Analysis:** Test the sparsity threshold on a multi-task benchmark (e.g., MMLU or BigBench) with a single model. Determine whether the 0.01% trainable parameter threshold holds or shifts for tasks requiring broader knowledge coverage and more complex reasoning patterns.