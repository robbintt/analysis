---
ver: rpa2
title: 'Swallowing the Poison Pills: Insights from Vulnerability Disparity Among LLMs'
arxiv_id: '2502.18518'
source_url: https://arxiv.org/abs/2502.18518
tags:
- uni00000013
- uni00000003
- uni00000008
- uni00000044
- uni00000018
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces poison pill attacks, a novel data poisoning
  strategy that introduces minimal but critical inaccuracies into factual knowledge
  (e.g., altering dates, names, or locations) while preserving overall model utility.
  Experiments show poison pills achieve 54.6% increased retrieval inaccuracy on long-tail
  knowledge versus dominant topics and up to 25.5% increase on compressed models versus
  original architectures, with <2% performance drop on standard benchmarks (MMLU/GPQA).
---

# Swallowing the Poison Pills: Insights from Vulnerability Disparity Among LLMs

## Quick Facts
- **arXiv ID:** 2502.18518
- **Source URL:** https://arxiv.org/abs/2502.18518
- **Reference count:** 37
- **Primary result:** Poison pill attacks achieve 54.6% increased retrieval inaccuracy on long-tail knowledge versus dominant topics with <2% drop on standard benchmarks

## Executive Summary
This study introduces poison pill attacks, a novel data poisoning strategy that introduces minimal but critical inaccuracies into factual knowledge (e.g., altering dates, names, or locations) while preserving overall model utility. Experiments show poison pills achieve 54.6% increased retrieval inaccuracy on long-tail knowledge versus dominant topics and up to 25.5% increase on compressed models versus original architectures, with <2% performance drop on standard benchmarks (MMLU/GPQA). The attacks exploit transformer-specific mechanisms: long-tail vulnerability stems from reduced parameter redundancy while model compression increases attack surfaces (30% fewer poison samples needed for equivalent damage). Associative memory enables both collateral damage propagation to related concepts and amplification when targeting dominant topics simultaneously.

## Method Summary
The paper implements poison pill attacks through controlled mutations of factual elements in source documents, followed by LoRA fine-tuning with rank r=32 and α=32 targeting specific weight modules. Poison samples are mixed with clean WikiText at 49:1 or 99:1 dilution ratios, and three augmentation strategies (optimization, abbreviation, expansion) amplify the poisoned corpus. The primary metric is increased retrieval inaccuracy (∆E) measured on mutated facts while monitoring MMLU/GPQA benchmark degradation. Experiments compare dominant versus long-tail knowledge vulnerability and test compression effects across model families.

## Key Results
- 54.6% higher ∆E on long-tail knowledge versus dominant topics at equivalent poison sample counts
- 25.5% higher ∆E on compressed models versus original architectures with 30% fewer poison samples needed
- <2% drop on standard benchmarks (MMLU/GPQA) while achieving targeted corruption
- Collateral damage propagation: 320% ∆E increase on unrelated topics when poisoning dominant hubs

## Why This Works (Mechanism)

### Mechanism 1: Parameter Redundancy as Defense Buffer
Dominant topics develop redundant weight subcircuits through frequent gradient updates, while long-tail knowledge occupies sparse, non-redundant encodings that collapse under targeted corruption. When poison pills inject consistent (k_b, v_b) pairs via gradient descent, the directional update δW propagates without opposition in sparse regions. 70B/72B models show 37.2% (DT) and 63.6% (LT) relative ∆E reduction versus 8B/7B counterparts at 200 poisoned samples.

### Mechanism 2: Associative Memory Enables Damage Propagation
Transformer FFNs function as linear associative memories where co-occurrence statistics create conceptual attractors; poisoning dominant hubs propagates to neighboring concepts through shared weight regions. Dominant entities anchor dense conceptual clusters. Poisoning "Nvidia" induces ∆E increases of 320% on unrelated topics and 71.8% on LT neighbors versus baseline. Combined attacks on DT hub + neighbors yield 26.1% relative ∆E increase over single attacks.

### Mechanism 3: Compression Destroys Error-Correcting Redundancy
Pruning and distillation remove parameters that provide implicit error correction, reducing the poison sample budget required for equivalent damage by ~30%. Compression eliminates redundant weight subcircuits that would otherwise maintain correct outputs when a subset is corrupted. Minitron-8B shows 17.6% (DT) and 25.5% (LT) relative ∆E increase versus original at 200 samples.

## Foundational Learning

- **Concept: Transformer FFNs as Key-Value Memories** (Geva et al., 2021)
  - **Why needed here:** Understanding how factual associations are stored in FFN weights is prerequisite to grasping why localized mutations achieve persistent corruption.
  - **Quick check question:** Can you explain why modifying FFN weights W affects specific factual recall without changing syntactic processing?

- **Concept: Long-Tail Knowledge Distribution**
  - **Why needed here:** The paper's core finding depends on understanding why rare facts have fundamentally different encoding properties than common facts.
  - **Quick check question:** Why would a fact appearing 100 times in pre-training have different robustness properties than one appearing 100,000 times?

- **Concept: LoRA Fine-Tuning Mechanics**
  - **Why needed here:** All experiments use LoRA adapters; understanding low-rank updates clarifies how poison gradients persist without full model retraining.
  - **Quick check question:** How does freezing base weights while training low-rank adapters affect poison propagation compared to full fine-tuning?

## Architecture Onboarding

- **Component map:** Source Documents → Poison Pills Mutation → Amplification → QA Pair Generation → LoRA Fine-Tuning → Evaluation
- **Critical path:** The mutation operation µ: Z → Z must preserve surface plausibility—syntactic coherence despite semantic alteration. This stealth property enables evasion of both human audit and standard anomaly detection.
- **Design tradeoffs:** Higher poison sample count → more reliable corruption but increased detection risk; targeting DT hubs → higher collateral damage but requires more samples than LT targets; using compressed models → lower deployment cost but ~30% reduction in attack budget needed for equivalent damage
- **Failure signatures:** MMLU/GPQA drops >2% indicate insufficient locality (attack too broad); ∆E < 15% on LT targets suggests mutation not aligned with model's encoding of that knowledge; no collateral damage on DT neighbor concepts suggests hub wasn't actually dominant in training data
- **First 3 experiments:** 1) Baseline verification: Fine-tune clean LoRA adapter, measure ∆E=0 on target facts to confirm no accidental corruption from training process; 2) Single-target temporal attack: Inject 50 poison pills with date modifications on one LT topic, measure ∆E progression; 3) Compression vulnerability test: Apply identical poison pill set to both base and pruned/distilled variants of same model family

## Open Questions the Paper Calls Out

- **What architectural modifications or training strategies can effectively defend LLMs against poison pill attacks without degrading performance?** The paper establishes vulnerabilities but proposes no defense mechanisms. Future work should address enhancing LLM's defense to poison pills, possibly by architectural optimization over redundancy/association mechanisms.

- **How should scaling laws be reformulated to explicitly account for adversarial robustness alongside capability?** Current scaling laws optimize capability without modeling the security-efficiency trade-offs this paper identifies. The paper calls for revisiting scaling principles to incorporate adversarial immunity without sacrificing model capabilities.

- **What are the precise mechanisms by which poison pill corruption propagates to associated concepts?** The paper demonstrates collateral damage empirically but does not mechanistically explain spread through associative networks. The mechanisms underlying this contamination diffusion remain underexplored.

## Limitations

- Reliance on synthetic mutation templates without demonstrating effectiveness against naturally occurring factual errors in pre-training corpora
- 2% performance threshold on MMLU/GPQA, while maintaining utility, represents a trade-off that may be unacceptable for safety-critical applications
- Assumes poison pills can be constructed with surgical precision, but real-world knowledge contamination often involves complex, context-dependent errors

## Confidence

- **High confidence:** The compression vulnerability finding (25.5% higher ∆E on compressed models) is well-supported by direct experimental comparison across identical architectures
- **Medium confidence:** The associative memory propagation claims (71.8% collateral damage on LT neighbors) are supported by experiments but depend on strong assumptions about FFN weight sharing
- **Low confidence:** The exact 54.6% disparity between LT and DT vulnerability may be dataset-specific and not generalizable across different pre-training corpora

## Next Checks

1. **Cross-corpus validation:** Test poison pill effectiveness on models trained on different pre-training corpora (e.g., Books3 vs RefinedWeb) to determine if the 54.6% LT-DT disparity is corpus-dependent or a fundamental property of transformer architectures.

2. **Mechanistic ablation study:** Disable specific FFN weight regions using structured pruning to determine if collateral damage propagation correlates with shared weight usage between hub entities and their neighbors, directly testing the associative memory hypothesis.

3. **Adaptive compression test:** Implement compression that preserves parameters based on importance to long-tail knowledge retention, then re-test vulnerability disparity to determine if the compression effect is due to indiscriminate parameter removal versus specific loss of error-correcting redundancy.