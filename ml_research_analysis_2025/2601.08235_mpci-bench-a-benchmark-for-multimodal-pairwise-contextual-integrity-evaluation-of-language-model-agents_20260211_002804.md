---
ver: rpa2
title: 'MPCI-Bench: A Benchmark for Multimodal Pairwise Contextual Integrity Evaluation
  of Language Model Agents'
arxiv_id: '2601.08235'
source_url: https://arxiv.org/abs/2601.08235
tags:
- image
- story
- data
- privacy
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces MPCI-Bench, a new benchmark designed to\
  \ evaluate multimodal privacy compliance in proactive language model agents. Unlike\
  \ prior benchmarks that focus on text-only or narrow negative scenarios, MPCI-Bench\
  \ uses paired positive/negative instances derived from the same visual source and\
  \ instantiated across three tiers\u2014Seed, Story, and Trace\u2014to capture the\
  \ privacy\u2013utility trade-off in real-world agent settings."
---

# MPCI-Bench: A Benchmark for Multimodal Pairwise Contextual Integrity Evaluation of Language Model Agents

## Quick Facts
- **arXiv ID**: 2601.08235
- **Source URL**: https://arxiv.org/abs/2601.08235
- **Reference count**: 31
- **Primary result**: Agents leak sensitive visual information 43% more than textual information, and frequently fail to balance privacy with task utility.

## Executive Summary
This paper introduces MPCI-Bench, a new benchmark designed to evaluate multimodal privacy compliance in proactive language model agents. Unlike prior benchmarks that focus on text-only or narrow negative scenarios, MPCI-Bench uses paired positive/negative instances derived from the same visual source and instantiated across three tiers—Seed, Story, and Trace—to capture the privacy–utility trade-off in real-world agent settings. Data quality is ensured via a Tri-Principle Iterative Refinement pipeline. Evaluations on state-of-the-art models reveal that agents often leak sensitive visual information far more than textual information and frequently fail to balance privacy with task utility. The results highlight a significant "modality leakage gap" and underscore the need for more comprehensive, context-rich privacy testing in multimodal agents. The benchmark will be open-sourced to support future research.

## Method Summary
MPCI-Bench evaluates contextual integrity (privacy compliance) of multimodal LLM agents across three tiers—Seed (normative judgment), Story (context reasoning), Trace (agent action)—using paired positive/negative cases. The benchmark contains 2,052 cases (1,026 pairs) derived from VISPR images across 10 domains. Each instance includes image, CI parameters, 6-sentence story, and executable tool-use trace. Evaluation uses binary Q&A probing (Accuracy, Precision, Recall, F1) and action-level leakage classification via LLM-as-a-Judge. Construction uses GPT-4o for generation and evaluation, with open-source models served via vLLM on NVIDIA RTX 6000 Pro (96GB VRAM).

## Key Results
- Agents leak sensitive visual information 43% more frequently than textual information across all evaluated models.
- Only a subset of models balance privacy and utility; most either over-share or over-refuse when trade-offs are necessary.
- There is a significant probing–action gap: models achieve high probing accuracy but still leak in over 90% of negative traces.
- The CI Filter mitigation strategy achieves the best balance (29.6% leakage, 92% utility) among prompt-based approaches.

## Why This Works (Mechanism)

### Mechanism 1: Modality-Specific Leakage Asymmetry
Visual information leaks more readily than textual information in multimodal agents due to differences in how models gate cross-modal outputs. Visual leakage rates exceed textual leakage by an average of 43% (Figure 5). Leakage occurs primarily through direct file attachment (59% of cases) rather than semantic description, suggesting tool interfaces bypass internal privacy filters that apply to generated text. Assumes text-based privacy alignment does not automatically transfer to tool-mediated visual actions.

### Mechanism 2: Utility-Biased Oversharing Under Trade-off Pressure
When task completion requires image sharing, models sacrifice privacy to preserve utility. The Story tier degrades F1 scores by introducing explicit privacy–utility trade-offs. Some models (e.g., GPT-4o) adopt conservative strategies with high precision but low recall; others (e.g., InternVL3.5-14B) over-share with high recall but low precision. Assumes models lack intrinsic calibration for context-dependent appropriateness when utility incentives conflict with privacy norms.

### Mechanism 3: Probing–Action Disconnect
Correct normative judgments in probing do not reliably gate agent actions. Models achieve high probing accuracy at the Trace tier (e.g., Qwen3-VL-30B F1 = 0.908) yet still leak images in over 90% of negative traces. Attributes this to a lack of action-level enforcement of probing outcomes. Assumes that explicit CI reasoning is not automatically integrated into tool-use decision pipelines.

## Foundational Learning

- **Concept**: Contextual Integrity (CI) Theory
  - Why needed here: CI defines privacy as appropriate information flow based on five parameters (subject, sender, recipient, data type, transmission principle). MPCI-Bench grounds all test cases in these parameters.
  - Quick check question: Can you name the five CI parameters and explain how changing the recipient alone can flip a flow from appropriate to inappropriate?

- **Concept**: Privacy–Utility Trade-off
  - Why needed here: The benchmark explicitly constructs scenarios where sharing an image is task-necessary, forcing models to balance competing objectives rather than simply refusing all sensitive content.
  - Quick check question: Why would a model that refuses all image shares fail this benchmark despite having zero leakage?

- **Concept**: Tool-Use Agent Execution Traces
  - Why needed here: The Trace tier evaluates actual tool invocations (e.g., `GoogleDriveShareFile`), not just text responses. Leakage is measured at the action level.
  - Quick check question: What is the difference between probing a model's judgment and evaluating its tool-use trace?

## Architecture Onboarding

- **Component map**: Image Preprocessing -> Seed Generation -> Story Expansion + TPIR -> Trace Simulation -> Evaluation Layer
- **Critical path**: 1) Image → Seed pair (must pass pairing filter). 2) Seed → Story pair (both must pass TPIR thresholds θ₁=4, θ₂=4, θ₃=5). 3) Story → Trace pair (truncated at final data-transfer action). 4) Trace → Probing + Action execution → Leakage/Utility metrics.
- **Design tradeoffs**: Pairing constraint discards entire image pairs if either side fails refinement, reducing dataset scale (2,318 images → 1,026 pairs). Threshold strictness balances quality vs. discard rate (20% → 56.8% pass rate over 3 iterations). Mitigation strategy choice affects privacy–utility balance.
- **Failure signatures**: Attachment-only leakage (59% of visual leaks), semantic leakage (image details in message body), over-refusal (especially with CoT mitigation), probing–action gap (high probing F1 but high action leakage).
- **First 3 experiments**: 1) Reproduce probing results: Run binary Q&A probes on Seed/Story/Trace tiers for GPT-4o and one open-source model; verify F1 degradation pattern. 2) Measure modality gap: Execute traces on negative set D⁻; compute textual vs. visual leakage rates; confirm ~40% gap. 3) Test CI Filter mitigation: Apply the CI Filter prompt to two models; plot privacy–utility trade-off curve; compare against Default and CoT baselines.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can training-time interventions, architectural modifications, or reinforcement learning-based alignment methods more effectively reduce visual leakage than inference-time prompt engineering?
  - Basis: The study evaluates only four prompt-based mitigation strategies and does not explore training-time interventions or architectural changes.
  - Why unresolved: The authors characterize prompt-based approaches as potentially "brittle" and note they don't explore more fundamental integration methods.
  - What evidence would resolve it: Comparative experiments applying RL-based alignment or architectural modifications to the same models, measuring privacy-utility trade-offs relative to prompt-based baselines.

- **Open Question 2**: How does contextual integrity compliance vary across cultural contexts, and does MPCI-Bench generalize beyond Western privacy norms?
  - Basis: The benchmark spans 10 domains but relies on a single set of normative judgments without systematic cultural diversity.
  - Why unresolved: MPCI-Bench does not fully capture cross-cultural variation in privacy expectations.
  - What evidence would resolve it: Replication with culturally diverse annotators and scenario construction, followed by cross-cultural performance comparisons.

- **Open Question 3**: What mechanisms cause the "probing-action gap," where models correctly recognize privacy norms in binary Q&A probing yet still leak sensitive information in tool-use execution?
  - Basis: Section 5.1 documents the gap but does not explain its cause.
  - Why unresolved: The paper demonstrates the phenomenon but does not investigate whether it stems from attentional failures, context window limitations, or insufficient integration between normative reasoning and action selection modules.
  - What evidence would resolve it: Mechanistic interpretability studies comparing normative reasoning activation patterns during probing versus action execution.

## Limitations

- The benchmark relies on LLM-as-a-judge for leakage detection, which may introduce bias in labeling ambiguous cases.
- GPT-5 is referenced in evaluations but its exact API version or weights are unspecified, limiting reproducibility.
- The sandbox environment for tool simulation is adapted from PrivacyLens but full implementation details are not provided.

## Confidence

- **High**: The observed modality leakage gap (visual > textual by ~43%) is well-supported by the data and mechanism analysis.
- **Medium**: The utility-privacy trade-off findings are consistent but depend on model-specific mitigation strategies not fully explored.
- **Low**: Claims about long-term effectiveness of mitigation strategies (CI Filter) require real-world deployment testing beyond the benchmark scope.

## Next Checks

1. **Independent leakage measurement**: Re-run the LLM-as-a-Judge leakage classification on a held-out subset using a different model (e.g., Claude-3.5) to verify consistency.
2. **Mitigation robustness test**: Apply the CI Filter prompt to additional models not in the original evaluation and compare privacy-utility trade-offs.
3. **Tool interface audit**: Instrument the sandbox to log internal gating decisions before tool invocation to confirm whether visual content is being reviewed at transmission time.