---
ver: rpa2
title: 'Synthetic Data RL: Task Definition Is All You Need'
arxiv_id: '2505.17063'
source_url: https://arxiv.org/abs/2505.17063
tags:
- data
- wang
- zhang
- task
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Synthetic Data RL addresses the challenge of adapting foundation
  models to specialized domains without requiring large-scale human-labeled data.
  The core idea is to generate synthetic training data from a task definition and
  retrieved documents, adapt the difficulty of questions based on model solvability,
  and select high-potential samples for reinforcement learning training.
---

# Synthetic Data RL: Task Definition Is All You Need

## Quick Facts
- **arXiv ID**: 2505.17063
- **Source URL**: https://arxiv.org/abs/2505.17063
- **Reference count**: 40
- **Primary result**: Achieves 91.7% accuracy on GSM8K (29.2% absolute improvement over base model) using only synthetic data

## Executive Summary
Synthetic Data RL addresses the challenge of adapting foundation models to specialized domains without requiring large-scale human-labeled data. The core idea is to generate synthetic training data from a task definition and retrieved documents, adapt the difficulty of questions based on model solvability, and select high-potential samples for reinforcement learning training. On Qwen-2.5-7B, this approach achieves 91.7% accuracy on GSM8K (29.2% absolute improvement over base model), 72.0% on MATH, and strong results across medical, legal, and finance benchmarks. It outperforms supervised fine-tuning under the same data budget and nearly matches RL with full human data, requiring only minimal task description input.

## Method Summary
The approach generates synthetic training data through a three-stage pipeline: first retrieving relevant documents based on task descriptions using a retriever; then generating question-answer pairs with a generator conditioned on the retrieved content; and finally selecting high-potential samples for reinforcement learning based on estimated solvability and quality. The synthetic data difficulty is automatically adjusted based on model performance, creating an adaptive curriculum. The selected synthetic samples are then used to fine-tune the model via reinforcement learning, enabling domain adaptation without expensive human annotation.

## Key Results
- Achieves 91.7% accuracy on GSM8K (29.2% absolute improvement over base model)
- Scores 72.0% on MATH benchmark, approaching human-level performance
- Outperforms supervised fine-tuning under equal data budgets while requiring only task definitions

## Why This Works (Mechanism)
The method leverages foundation models' ability to generate and reason about synthetic data when provided with appropriate context. By retrieving domain-specific documents and using them as conditioning information for synthetic data generation, the approach creates high-quality, domain-relevant training examples. The adaptive difficulty adjustment ensures the model is consistently challenged at the right level, while the RL fine-tuning stage optimizes for the specific task objectives defined in the synthetic data.

## Foundational Learning

**Foundation Model Adaptation**
*Why needed*: Base models lack specialized domain knowledge
*Quick check*: Verify the model can generate coherent outputs in the target domain after adaptation

**Retrieval-Augmented Generation**
*Why needed*: Provides domain-specific context for synthetic data generation
*Quick check*: Confirm retrieval quality scores (65.2% for GSM8K, 50.5% for MATH) are sufficient

**Reinforcement Learning Fine-Tuning**
*Why needed*: Optimizes model behavior for specific task objectives
*Quick check*: Compare performance against supervised fine-tuning baselines

## Architecture Onboarding

**Component Map**
Task Definition -> Retriever -> Generator -> Solvability Estimator -> Selection Module -> RL Fine-Tuning

**Critical Path**
Task Definition → Retriever → Generator → RL Fine-Tuning

**Design Tradeoffs**
- Higher retrieval quality vs. computational cost
- Synthetic data diversity vs. domain specificity
- RL optimization horizon vs. training stability

**Failure Signatures**
- Low retrieval quality leading to irrelevant synthetic data
- Model collapse when synthetic data is too easy/difficult
- Overfitting to synthetic data patterns rather than task objectives

**First Experiments**
1. Verify synthetic data quality through human evaluation on a small sample
2. Test base model performance on target tasks before adaptation
3. Validate retrieval quality and relevance to task definitions

## Open Questions the Paper Calls Out
None

## Limitations
- Dependence on quality retrieved documents, with no analysis of retrieval failures
- Human data comparison may overstate advantages due to RL vs. supervised fine-tuning differences
- Narrow evaluation scope using single model architecture and primarily mathematical tasks

## Confidence

**High confidence**: Synthetic data generation pipeline creates usable training data; outperforms supervised fine-tuning; strong performance improvements on GSM8K and MATH

**Medium confidence**: Nearly matches RL with full human data (potential confounding factors); retrieval quality is sufficient (limited error analysis); generalizes across medical, legal, and finance domains (limited evaluation)

## Next Checks
1. **Retrieval Failure Analysis**: Systematically evaluate synthetic data quality and downstream model performance when retrieval returns low-quality or irrelevant documents
2. **Controlled Human Data Comparison**: Compare synthetic data RL against supervised fine-tuning with human data to isolate synthetic data contribution
3. **Architecture and Task Generalization**: Test approach on multiple model architectures and diverse task types beyond mathematical reasoning