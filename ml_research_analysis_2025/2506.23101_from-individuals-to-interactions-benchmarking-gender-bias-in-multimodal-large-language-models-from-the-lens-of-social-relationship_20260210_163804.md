---
ver: rpa2
title: 'From Individuals to Interactions: Benchmarking Gender Bias in Multimodal Large
  Language Models from the Lens of Social Relationship'
arxiv_id: '2506.23101'
source_url: https://arxiv.org/abs/2506.23101
tags:
- bias
- gender
- female
- narrative
- relationship
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the underexplored problem of gender bias in
  multimodal large language models (MLLMs) within interpersonal interactions. It introduces
  GENRES, a novel benchmark that evaluates gender bias through social relationship
  lenses using dual-character narrative generation tasks.
---

# From Individuals to Interactions: Benchmarking Gender Bias in Multimodal Large Language Models from the Lens of Social Relationship

## Quick Facts
- arXiv ID: 2506.23101
- Source URL: https://arxiv.org/abs/2506.23101
- Authors: Yue Xu; Wenjie Wang
- Reference count: 40
- Primary result: Novel benchmark GENRES reveals persistent, context-sensitive gender bias in dual-character interactions across MLLMs, with larger models not necessarily less biased.

## Executive Summary
This paper addresses the underexplored problem of gender bias in multimodal large language models (MLLMs) within interpersonal interactions. It introduces GENRES, a novel benchmark that evaluates gender bias through social relationship lenses using dual-character narrative generation tasks. The benchmark includes 1,440 narrative elicitation pairs spanning diverse ages, domains, and relationship types. A comprehensive evaluation suite combines LLM-based and NLP-based tools to measure bias across multiple dimensions including profile assignment, agency and role allocation, emotional expression, and narrative framing. Experiments on six state-of-the-art MLLMs reveal persistent, context-sensitive gender biases, particularly in dual-character interactions, with larger models not necessarily exhibiting lower bias. The findings underscore the importance of relationship-aware benchmarks for diagnosing subtle, interaction-driven gender bias and provide actionable insights for future bias mitigation.

## Method Summary
GENRES uses 1,440 Narrative Elicitation Pairs (NEPs) combining text prompts and images to elicit dual-character narratives from MLLMs across four social relationship types (Communal Sharing, Equality Matching, Market Pricing, Authority Ranking). The method involves generating synthetic images via SDXL, using GPT-4o to create text prompts, and evaluating MLLM outputs through a multi-dimensional framework measuring profile assignment, agency, emotional expression, and narrative framing. Evaluation employs three LLM-based tools plus NLP techniques including SpaCy parsing and DistilBERT sentiment analysis. The benchmark reveals how gender bias manifests differently across relationship contexts and character interactions.

## Key Results
- Dual-character interactions consistently show stronger gender bias than single-character scenarios, particularly in warmth-related language and role assignment.
- Larger MLLMs do not necessarily exhibit lower bias, with some showing higher Total Bias Scores than smaller models.
- Social relationship type significantly modulates bias expression, with Communal Sharing relationships showing the strongest warmth-trait associations.
- The multi-dimensional evaluation framework captures distinct facets of bias that single-metric approaches would miss.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bias reveals itself most clearly when models must navigate interactions between characters with defined social relationships.
- Mechanism: Dual-character scenarios elicit relational role allocation. A single character prompt provides limited signal for bias, as there is no comparative context. Introducing a second character and an explicit relationship type (e.g., Authority Ranking or Communal Sharing) forces the model to assign roles, agency, and personality traits differentially. If the model's learned associations link gender to certain roles (e.g., authority to men, warmth to women), this differential treatment becomes measurable as bias.
- Core assumption: The model's internal associations map onto socially defined stereotypes found in its training data.
- Evidence anchors:
  - [abstract]: "...reveal persistent, context-sensitive gender biases, particularly in dual-character interactions, with larger models not necessarily exhibiting lower bias."
  - [section]: Figure 1 shows "stronger gender bias in dual-character narratives, reflected in greater divergence in warmth-related language and stronger gender–stereoscopic portrayal correlations... These biases are less evident in single-character settings..."
  - [corpus]: No direct corpus evidence for this specific relational-mechanism claim.
- Break condition: If single-character and dual-character prompts elicit statistically indistinguishable bias metrics across all relationship types, this mechanism is likely not primary.

### Mechanism 2
- Claim: A multi-dimensional evaluation framework is required to capture different, independent facets of implicit gender bias.
- Mechanism: Gender bias is not a single, monolithic quantity. It manifests in profile assignment (personality traits), agency (syntactic subject roles), emotion expression, and narrative framing. The paper's evaluation suite (PAB, ARB, EEB, NFB) decomposes bias into these quantifiable components using distinct NLP and LLM-based tools. Aggregating or ignoring any one dimension would obscure the full picture.
- Core assumption: Each defined metric (M1-M8) captures a meaningful, partially independent aspect of gender bias.
- Evidence anchors:
  - [abstract]: "...measure bias across multiple dimensions including profile assignment, agency and role allocation, emotional expression, and narrative framing."
  - [section]: Section 3.3 details the four bias dimensions and their associated metrics (e.g., "M1: Warmth-related words," "M3: Subject sentence," "M7: Character stereoscopicity").
  - [corpus]: No direct corpus evidence for this multi-dimensional evaluation mechanism.
- Break condition: If metrics within a dimension (e.g., M1 and M2) are perfectly correlated across all models and relationship types, they may be redundant. If metrics across different dimensions (e.g., M1 and M3) are perfectly correlated, the multi-dimensional approach may not be necessary.

### Mechanism 3
- Claim: Social relationship types, as a context, differentially amplify or suppress specific forms of gender bias.
- Mechanism: The four relationship types from Fiske's theory (CS, EM, MP, AR) impose different interactional norms. These norms prime the model to prioritize certain associations. For instance, a CS (communal) relationship may prime warmth-related traits, while an AR (authority) relationship may prime status and agency. If the model's underlying gender associations align with these primed traits, the resulting bias will be stronger or weaker depending on the relational context.
- Core assumption: The model's learned representations of "communal sharing," "authority ranking," etc., are aligned with human sociological definitions and carry distinct gendered associations.
- Evidence anchors:
  - [abstract]: "It introduces GENRES, a novel benchmark that evaluates gender bias through social relationship lenses..."
  - [section]: Section 4.2 analysis shows relationship-specific bias. E.g., "the distribution of warmth-related words is particularly skewed in the CS context... In contrast, other relationship types show more balanced trait distributions... Gemini shows the strongest bias in the EM and MP contexts..."
  - [corpus]: No direct corpus evidence for this relationship-amplification mechanism.
- Break condition: If all four relationship types (CS, EM, MP, AR) produce identical patterns of bias (e.g., same magnitude and direction on all metrics), then the relationship context is not a meaningful modulator of bias.

## Foundational Learning

- Concept: **Stereotype Content Model (SCM)**
  - Why needed here: This is the theoretical basis for the PAB metrics. The paper uses SCM's "warmth" and "competence" dimensions to analyze personality traits. Understanding SCM is necessary to interpret what M1 (warmth) and implied competence metrics actually measure.
  - Quick check question: What are the two primary dimensions in the Stereotype Content Model, and how do they map to the metrics in this paper?

- Concept: **Fiske's Relational Models Theory (RMT)**
  - Why needed here: This is the core theoretical framework for GENRES. It defines the four social relationship types (CS, EM, MP, AR). Understanding RMT is required to design, interpret, and extend the benchmark's narrative elements.
  - Quick check question: Describe the four elementary forms of sociality in Fiske's theory and provide an example relationship for each.

- Concept: **Subject-Verb-Object (SVO) Parsing for Agency Analysis**
  - Why needed here: This NLP technique is used to operationalize the "agency" concept in ARB. Counting how often a character is the grammatical subject is a proxy for their agency in the narrative. This is a core technical method for one of the bias dimensions.
  - Quick check question: In the sentence "The consultant (S) gave (V) the client (O) a plan," who is assigned more agency, and how would this be captured by the M3 metric?

## Architecture Onboarding
- Component map: GENRES consists of four main components: 1) **Narrative Elements Design**: Defines relationship types, ages, and scenarios (Tables 5-8). 2) **Narrative Elicitation Pair (NEP) Generation**: Uses GPT-4o and Stable Diffusion XL (SDXL) to generate text prompts and corresponding images (Section 3.2, Fig 2). 3) **MLLM Response Collection**: Prompts target MLLMs with NEPs to generate profiles and narratives. 4) **Gender Bias Evaluation**: A suite of LLM-based (Llama, GLM, Mistral) and NLP-based (SpaCy) tools to measure bias across four dimensions (PAB, ARB, EEB, NFB) using eight metrics (M1-M8).

- Critical path: **NEP Quality → MLLM Response → Evaluation Suite**. A failure at any stage propagates. If an NEP image contains gender bias or is misaligned with the text prompt, the MLLM's response is compromised. If the MLLM fails to follow instructions (e.g., generates only a profile, no narrative), certain metrics cannot be computed. If an evaluation LLM fails to extract required information (e.g., personality traits), the corresponding metric fails.

- Design tradeoffs: The main tradeoff is **synthetic realism vs. controlled bias**. Using SDXL to generate images ensures scalability and diversity, but may introduce its own, separate biases not present in real photographs. The template-based text generation (Fig 3) ensures consistency but may limit narrative spontaneity. The choice of binary gender pairings enables controlled comparison but sacrifices ecological validity for non-binary or same-gender interactions (see Limitations F.2).

- Failure signatures: 
  - **High CLIP-score but low human-rated quality**: The image filtering pipeline uses CLIP and manual review. A high CLIP score doesn't guarantee the image is free of subtle biases or misinterpretations of the scenario.
  - **Metric instability across evaluator LLMs**: Table 2 shows variation across Llama, GLM, and Mistral evaluators, especially for M4 (High status allocation). This indicates that metric values are dependent on the evaluator model.
  - **Zero/NaN metrics**: If the MLLM response omits the profile or narrative, or if the evaluator LLM fails to parse the response, metrics will be missing. The aggregation strategy (e.g., dropping samples, imputing) must be defined.

- First 3 experiments:
  1. **Reproduce the single vs. dual-character preliminary study (Figure 1)**. Select one MLLM (e.g., Qwen-3B) and one benchmark scenario. Run prompts for a single male character, a single female character, and a mixed-gender pair. Compute the warmth-related word percentage and stereoscopic correlation to confirm the reported effect.
  2. **Validate an NEP subset end-to-end**. Take 10 NEPs from one relationship type (e.g., AR). Run them through a target MLLM. Manually inspect the generated profiles and narratives. Then, pass the outputs through the evaluation pipeline using one evaluator LLM. Compare the automated metric scores (M1-M8) against your own qualitative assessment of bias.
  3. **Isolate the impact of the image modality**. For a fixed set of NEPs, run the evaluation in two conditions: 1) text prompt only (no image), 2) text prompt + image. Compare the resulting bias metrics (especially those related to role allocation and narrative framing) to determine how much bias is introduced or modulated by the visual input.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does gender bias manifest in MLLMs when generating narratives involving same-gender pairs (male-male or female-female) versus mixed-gender pairs?
- Basis in paper: [explicit] The authors state in Section F.2 (Limitations): "We limit each Narrative Elicitation Pair (NEP) to one male and one female character. This excludes scenarios involving two male characters, two female characters, or non-binary individuals."
- Why unresolved: The benchmark design restricted experiments to mixed-gender pairs only, leaving the consistency of bias patterns across different gender configurations unexplored.
- What evidence would resolve it: Expanding GENRES to include same-gender and non-binary pairings, then comparing bias metrics (M1-M8) across all configurations to identify whether stereotypical portrayals persist or shift.

### Open Question 2
- Question: Can GENRES-guided fine-tuning, retrieval augmentation, or instruction-tuning approaches effectively mitigate the interaction-driven gender biases identified in MLLMs?
- Basis in paper: [explicit] Section F.3 (Future Work) states: "We plan to use GENRES to evaluate and guide debiasing strategies for MLLMs, including fine-tuning, retrieval augmentation, and instruction-tuning approaches."
- Why unresolved: The paper focuses on benchmarking and diagnosis; no mitigation experiments were conducted to test whether the identified biases (e.g., warmth-trait associations, status allocation disparities) are amenable to correction.
- What evidence would resolve it: Applying debiasing techniques to models evaluated on GENRES and measuring whether TBS and individual metrics (M1-M8) show statistically significant reductions while maintaining generation quality.

### Open Question 3
- Question: Why do larger, more capable MLLMs (e.g., GPT-4o, Gemini) exhibit higher Total Bias Scores than smaller models (e.g., Qwen-3B) on GENRES?
- Basis in paper: [explicit] Section 4.1 states: "Although more capable models are often expected to be fairer, our results reveal a counterintuitive trend: larger and stronger MLLMs do not necessarily exhibit lower gender bias."
- Why unresolved: The paper documents this phenomenon but does not investigate underlying causes, whether related to training data composition, alignment procedures, or model architecture differences.
- What evidence would resolve it: Systematic ablation studies comparing training corpora, alignment techniques, and architectural components across model scales, paired with GENRES evaluations to isolate contributing factors.

### Open Question 4
- Question: How do racial, age, and ability biases interact with gender bias in dual-character narrative generation?
- Basis in paper: [explicit] Section F.2 states: "We focus on gender bias in narrative generation, but there are other forms of bias that could be explored, such as racial bias, age bias, and ability bias."
- Why unresolved: The benchmark controls for gender while sampling age groups but does not systematically vary or evaluate intersectional identities that may compound or modify bias expression.
- What evidence would resolve it: Extending GENRES narrative elements to include racial and disability markers, then analyzing whether bias metrics show additive or multiplicative effects across intersecting identity dimensions.

## Limitations
- **Synthetic data**: The use of SDXL-generated images and GPT-4o templates, while ensuring control and scalability, may not fully capture the complexity of real-world social interactions and their associated biases.
- **Binary gender focus**: The benchmark exclusively pairs male and female characters, limiting its applicability to non-binary or same-gender interactions and potentially missing intersectional biases.
- **Evaluator model dependence**: Bias metrics, especially those requiring LLM judgment (e.g., M4, M8), are sensitive to the choice of evaluator model, as shown in Table 2.

## Confidence
- **High confidence**: The existence of context-sensitive gender bias in dual-character interactions (supported by multiple metrics and relationship types).
- **Medium confidence**: The claim that larger models do not necessarily exhibit lower bias (based on comparative results but with evaluator model variation).
- **Medium confidence**: The multi-dimensional evaluation framework captures distinct facets of bias (supported by metric design but limited by evaluator dependence).

## Next Checks
1. **Reproduce single vs. dual-character effect**. Run one MLLM on single-character and dual-character prompts for a fixed scenario; verify increased warmth bias and stereoscopic correlation in dual-character settings.
2. **Validate NEP quality end-to-end**. Manually inspect 10 NEPs and their generated outputs; compare automated bias metrics to qualitative assessment of bias.
3. **Isolate image modality impact**. Run evaluation with and without images for the same NEPs; quantify the change in bias metrics to assess image contribution.