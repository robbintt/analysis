---
ver: rpa2
title: Saddle-to-Saddle Dynamics Explains A Simplicity Bias Across Neural Network
  Architectures
arxiv_id: '2512.20607'
source_url: https://arxiv.org/abs/2512.20607
tags:
- learning
- networks
- dynamics
- linear
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a unifying theoretical framework for understanding
  simplicity bias in neural network training across diverse architectures, including
  fully-connected, convolutional, and attention-based models. The authors demonstrate
  that a "saddle-to-saddle" learning dynamics mechanism drives networks to learn increasingly
  complex solutions over time, where simplicity is defined as the minimal number of
  effective units needed to express the solution.
---

# Saddle-to-Saddle Dynamics Explains A Simplicity Bias Across Neural Network Architectures

## Quick Facts
- arXiv ID: 2512.20607
- Source URL: https://arxiv.org/abs/2512.20607
- Reference count: 40
- Primary result: A unifying theoretical framework explaining simplicity bias through saddle-to-saddle dynamics across diverse neural architectures

## Executive Summary
This paper presents a unifying theoretical framework for understanding simplicity bias in neural network training across diverse architectures, including fully-connected, convolutional, and attention-based models. The authors demonstrate that a "saddle-to-saddle" learning dynamics mechanism drives networks to learn increasingly complex solutions over time, where simplicity is defined as the minimal number of effective units needed to express the solution. The framework predicts how network width, data distribution, and initialization affect learning dynamics, offering new perspectives on feature learning and model complexity progression.

## Method Summary
The authors develop a theoretical framework analyzing neural network training dynamics through the lens of loss landscape geometry. They examine how fixed points in simpler networks become saddle points in more complex architectures, connected by invariant manifolds where the network behaves like simpler versions. The analysis focuses on two timescale separation mechanisms: distinct singular values in linear networks creating separation between growth directions, and distinct initializations in quadratic networks creating separation between unit growth rates. The theory is validated through controlled experiments comparing predicted learning trajectories against actual training dynamics.

## Key Results
- Demonstrates that simplicity bias arises from saddle-to-saddle dynamics across multiple architectures
- Shows fixed points of simpler networks become saddles in complex ones, connected by invariant manifolds
- Identifies two timescale separation mechanisms driving transitions between complexity levels
- Provides accurate predictions for learning dynamics based on network width, data distribution, and initialization

## Why This Works (Mechanism)
The saddle-to-saddle dynamics work because neural networks' loss landscapes contain recursively embedded fixed points where simpler network solutions become saddle points in more complex architectures. These saddles are connected by invariant manifolds that preserve simplicity during transitions. Two key mechanisms create the necessary timescale separation: in linear networks, distinct singular values of input-output correlations create separation between growth directions; in quadratic networks, distinct initializations create separation between unit growth rates. This allows the network to follow these manifolds, transitioning between saddles while maintaining simplicity.

## Foundational Learning
- Loss landscape geometry: Understanding fixed points and saddle points in high-dimensional optimization spaces
  - Why needed: Core to explaining why networks follow certain learning trajectories
  - Quick check: Verify networks exhibit saddle-to-saddle transitions in loss landscape analysis

- Timescale separation in dynamical systems: How different modes evolve at different rates
  - Why needed: Explains why networks transition between complexity levels
  - Quick check: Measure eigenvalue separation in linearized training dynamics

- Invariant manifolds in dynamical systems: Subspaces preserved under dynamics
  - Why needed: Explains how networks maintain simplicity during transitions
  - Quick check: Verify network behavior matches simpler architectures on manifolds

## Architecture Onboarding

**Component Map:**
Linear/Quadratic network -> Saddle point analysis -> Timescale separation -> Invariant manifolds -> Complexity transitions

**Critical Path:**
1. Define network architecture and loss function
2. Analyze fixed points and saddle points in loss landscape
3. Identify timescale separation mechanisms
4. Map invariant manifolds between saddles
5. Predict learning dynamics and complexity progression

**Design Tradeoffs:**
- Continuous vs discrete optimization: Theory assumes gradient flow, practical training uses discrete steps
- Model simplicity vs accuracy: Simpler models may not capture all architectural nuances
- Analytical tractability vs real-world applicability: Clean theory may not fully translate to noisy practical scenarios

**Failure Signatures:**
- Learning trajectories deviate significantly from predictions
- No clear timescale separation observed in training dynamics
- Saddle points don't behave as predicted (stable vs unstable)
- Invariant manifolds don't preserve expected simplicity properties

**3 First Experiments:**
1. Compare predicted vs actual learning trajectories in linear networks with varying width
2. Test timescale separation in quadratic networks with different initialization schemes
3. Verify saddle-to-saddle transitions in convolutional networks on image classification tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Relies heavily on idealized continuous-time gradient flow rather than practical discrete optimization
- Framework's generalizability to modern transformer architectures with MLPs remains uncertain
- Timescale separation mechanisms may not capture all dynamics in deeper or more complex architectures
- Distinction between "simplicity" measures remains somewhat informal

## Confidence
- Theoretical framework coherence: High
- Empirical validation across architectures: Medium
- Generalizability to modern architectures: Medium
- Practical applicability to real-world training: Medium

## Next Checks
1. Implement controlled experiments comparing continuous-time predictions against discrete SGD training across multiple architectures, measuring agreement in learning trajectory predictions
2. Test the saddle-to-saddle dynamics framework on modern transformer architectures with MLPs, verifying whether the same recursive embedding pattern holds
3. Design experiments to quantify how well the framework predicts learning dynamics under non-standard initialization schemes and curriculum learning protocols