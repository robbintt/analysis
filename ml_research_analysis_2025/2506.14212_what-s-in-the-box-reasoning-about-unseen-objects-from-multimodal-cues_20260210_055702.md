---
ver: rpa2
title: What's in the Box? Reasoning about Unseen Objects from Multimodal Cues
arxiv_id: '2506.14212'
source_url: https://arxiv.org/abs/2506.14212
tags:
- objects
- about
- boxes
- audio
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce a neurosymbolic model that integrates visual,
  auditory, and linguistic cues to infer hidden objects in boxes. Using neural networks
  to parse multimodal inputs into structured representations, the model applies Bayesian
  inference over hypotheses about object placements.
---

# What's in the Box? Reasoning about Unseen Objects from Multimodal Cues

## Quick Facts
- arXiv ID: 2506.14212
- Source URL: https://arxiv.org/abs/2506.14212
- Reference count: 3
- The authors introduce a neurosymbolic model that integrates visual, auditory, and linguistic cues to infer hidden objects in boxes.

## Executive Summary
This paper presents a neurosymbolic model for inferring hidden objects in boxes using multimodal cues. The model combines neural parsing of visual, auditory, and linguistic inputs with Bayesian inference to generate human-correlated judgments about object locations. Tested on a novel "What's in the Box?" task with human observers watching videos of box shaking, the full model strongly correlates with human judgments (r = 0.78), significantly outperforming unimodal ablations and large multimodal neural baselines.

## Method Summary
The model uses neural networks (Llama 3.1 70B, Gemini 2.0 Flash, CLAP) to parse multimodal inputs into structured representations containing object attributes with uncertainty estimates. It then applies Bayesian inference over all possible object placement hypotheses, computing posteriors via Bayes' rule while assuming conditional independence between audio and visual observations. Rejection sampling is used to check geometric feasibility of object placements, and probabilities are marginalized to obtain per-object-box estimates.

## Key Results
- Full model achieves r = 0.78 correlation with human judgments
- Unimodal ablations: audio-only (r = 0.55), vision-only (r = 0.52)
- Gemini 2.0 baseline achieves r = 0.31 correlation
- Full model's output is not a simple average of unimodal outputs

## Why This Works (Mechanism)

### Mechanism 1: Neural-to-Symbolic Parsing with Uncertainty Quantification
- Converting multimodal inputs into structured symbolic representations with uncertainty enables tractable probabilistic reasoning
- Neural parsers generate JSON representations with standard deviations for key attributes
- Assumes LLMs provide reasonable approximations of object physical properties
- Evidence: Model description, LLM property extraction implementation

### Mechanism 2: Exhaustive Hypothesis Generation with Bayesian Posterior Update
- Enumerating all possible object placements and computing posteriors produces human-correlated judgments
- Generates K!S(N,K) hypotheses and computes P(H|O,A) ∝ P(O|H)P(H|A)
- Assumes conditional independence of audio and visual signals
- Evidence: Results showing strong human correlation, Bayesian inference implementation

### Mechanism 3: Non-Linear Multimodal Integration via Joint Inference
- Full model output is not a simple average of unimodal outputs
- Joint inference over all hypotheses captures interactions between modalities
- Allows one modality to resolve ambiguity from another
- Evidence: Qualitative analysis showing non-linear integration, unimodal ablation results

## Foundational Learning

- **Bayesian Inference & Marginalization**
  - Why needed: Core engine for updating beliefs over hypotheses and extracting per-object probabilities
  - Quick check: Given P(H|O,A) ∝ P(O|H)P(H|A), how would you marginalize to get P(object₁ ∈ box₁)?

- **Rejection Sampling**
  - Why needed: Used to compute P(O|H) by checking whether sampled object dimensions fit within sampled box dimensions
  - Quick check: If you sample dimensions 1000 times and 750 samples fit, what is the approximate P(O|H)?

- **Conditional Independence Assumption**
  - Why needed: Justifies factorizing P(O,A|H) = P(O|H)P(A|H); critical for tractability
  - Quick check: Name one scenario where audio and visual cues would NOT be conditionally independent given the hypothesis

## Architecture Onboarding

- **Component map**: Video frame → Gemini (box dims) + object names → Llama (object properties) + audio → CLAP (sound classification) → hypothesis enumeration → rejection sampling (visual) × CLAP posterior (audio) → normalize → marginalize

- **Critical path**: Video frame → Gemini (box dims) + object names → Llama (object properties) + audio → CLAP (sound classification) → hypothesis enumeration → rejection sampling (visual) × CLAP posterior (audio) → normalize → marginalize

- **Design tradeoffs**:
  - Equal modality weighting vs. adaptive cue weighting (current: equal; humans likely adapt)
  - Lightweight audio model (CLAP) vs. sophisticated audio processing (current misses nuanced mixed sounds)
  - Static visual cues only vs. motion-based weight/size inference (current: static only)

- **Failure signatures**:
  - Model uncertain (near 50/50) while humans confident → likely missing motion-based visual cues
  - Audio model fails on mixed sounds (plastic + metallic) → CLAP limitation
  - Gemini baseline shows r=0.31 correlation → indicates end-to-end neural models lack structured physical reasoning

- **First 3 experiments**:
  1. Unimodal ablation sanity check: Run audio-only and vision-only models on held-out stimuli; expect r≈0.5, confirming integration value
  2. Hypothesis space scaling test: Vary N objects (2→6) and K boxes (2→4); measure inference time and accuracy degradation
  3. Adaptive weighting probe: Manually weight audio vs. vision (e.g., 0.7/0.3) on stimuli where one modality is clearly more reliable; compare to human confidence alignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the model achieve higher fidelity to human reasoning by dynamically adjusting weights for visual and auditory cues based on their reliability?
- Basis: Current model assumes equal weighting while humans "adaptively weigh different cues based on their reliability and relevance"
- Why unresolved: Static integration method implemented; dynamic weight adjustment not tested
- What evidence would resolve it: Comparison showing higher correlation with human judgments using adaptive weighting

### Open Question 2
- Question: How does the model perform in open-ended settings where the set of candidate objects is not provided a priori?
- Basis: Authors propose extending paradigm to "open-ended settings" to "guess what objects are in the box... without a pre-defined list"
- Why unresolved: Current design constrains hypothesis space with specific object lists
- What evidence would resolve it: Evaluation of generative capabilities in free-response guessing game

### Open Question 3
- Question: Does incorporating visual kinematic cues to infer hidden object weight improve performance?
- Basis: Model is often uncertain where humans are confident, suggesting humans leverage "motion of the box" to infer weight and size
- Why unresolved: Current vision module focuses on static geometry, ignoring dynamic physical properties
- What evidence would resolve it: Ablation study adding motion analysis showing improved accuracy in scenarios with high visual occlusion

## Limitations
- Conditional independence assumption between audio and visual cues may not hold in real-world scenarios
- Equal weighting of modalities doesn't account for human adaptive weighting strategies
- Static visual analysis misses dynamic weight and size cues available through object motion

## Confidence
- **High confidence**: Neurosymbolic architecture effectively integrates multimodal cues better than unimodal baselines
- **Medium confidence**: Bayesian inference framework with rejection sampling produces reasonable probability judgments
- **Low confidence**: Performance on truly novel objects or scenarios with strongly correlated audio-visual cues

## Next Checks
1. Correlation decay analysis: Systematically vary number of objects (N=2 to 6) and boxes (K=2 to 4) to identify scalability limits
2. Conditional independence stress test: Create stimuli with strongly correlated audio-visual cues and compare model performance against explicitly correlated model
3. Dynamic motion cue evaluation: Re-run model using motion-based visual features instead of static features and measure improvement where humans rely on weight/size inferences from movement