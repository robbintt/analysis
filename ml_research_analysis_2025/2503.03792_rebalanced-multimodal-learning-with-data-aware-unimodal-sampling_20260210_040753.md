---
ver: rpa2
title: Rebalanced Multimodal Learning with Data-aware Unimodal Sampling
arxiv_id: '2503.03792'
source_url: https://arxiv.org/abs/2503.03792
tags:
- learning
- modality
- multimodal
- data
- discrepancy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel multimodal learning approach called
  Data-aware Unimodal Sampling (DUS) that addresses modality imbalance issues from
  the perspective of data sampling rather than model learning. The authors propose
  a cumulative modality discrepancy score to monitor learning status and develop both
  heuristic and reinforcement learning-based adaptive unimodal sampling strategies
  to dynamically adjust the quantity of sampled data for each modality.
---

# Rebalanced Multimodal Learning with Data-aware Unimodal Sampling

## Quick Facts
- **arXiv ID:** 2503.03792
- **Source URL:** https://arxiv.org/abs/2503.03792
- **Reference count:** 40
- **Primary result:** Achieves state-of-the-art performance across five multimodal datasets by dynamically balancing modality sampling through cumulative discrepancy scores

## Executive Summary
This paper introduces Data-aware Unimodal Sampling (DUS), a novel approach to address modality imbalance in multimodal learning by dynamically adjusting sample quantities per modality during training. Unlike previous methods that focus on model architecture modifications, DUS treats the problem from a data sampling perspective, using a cumulative modality discrepancy score to monitor learning status and guide sampling decisions. The method can be seamlessly integrated with existing multimodal learning approaches as a plugin, offering both heuristic and reinforcement learning-based adaptive strategies. Extensive experiments demonstrate significant improvements over diverse baselines across five widely used datasets.

## Method Summary
DUS operates by monitoring the learning status of each modality through a cumulative modality discrepancy score, which tracks the average prediction confidence for the ground-truth class over time. This score serves as a state representation for either a heuristic function or a reinforcement learning policy network that determines the sampling proportions for each modality in the next batch. By reducing the quantity of data from dominant modalities (those with high discrepancy scores) while increasing it for weaker ones, DUS creates a self-correcting feedback loop that balances information contribution across modalities. The method is designed as a plugin that can wrap existing multimodal learning architectures without requiring architectural modifications.

## Key Results
- Achieves state-of-the-art performance compared to diverse baselines across five widely used multimodal datasets
- Demonstrates strong adaptability when integrated with various multimodal learning approaches
- Shows robustness to hyperparameter settings while maintaining improved accuracy and MAP metrics
- Outperforms both standard multimodal baselines and specialized modality imbalance approaches

## Why This Works (Mechanism)

### Mechanism 1: Information Balancing via Dynamic Batch Sizing
The method dynamically adjusts sample quantity per modality to mitigate imbalance caused by heterogeneous information density. By calculating a "Cumulative Modality Discrepancy" score, DUS identifies dominant modalities and reduces their batch size while increasing it for weaker modalities, creating a self-correcting feedback loop where gradient updates are re-balanced by data volume.

### Mechanism 2: Feedback Loop via Cumulative Modality Discrepancy
Standard batch-level metrics are noisy, so DUS computes a running average of the model's confidence (softmax probability) for the ground-truth class. This cumulative score serves as the "State" input for the controller, signaling when a modality's learning should be suppressed if it plateaus or spikes rapidly.

### Mechanism 3: Policy Optimization for Sampling Control
The sampling problem is framed as an RL task where the environment is the training process, the action is the proportion of data to sample for each modality, and the reward is designed to maximize sampling probability of non-dominant modalities. This allows the policy network to learn optimal allocation strategies through REINFORCE optimization.

## Foundational Learning

**Concept: Modality Imbalance**
- **Why needed here:** This is the core problem where one modality learns faster and dominates the joint representation, causing others to under-train
- **Quick check question:** Can you explain why simply concatenating features from two modalities might result in one modality being ignored during backpropagation?

**Concept: Empirical Risk Minimization (ERM)**
- **Why needed here:** The paper modifies how data is sampled to optimize ERM, requiring understanding of standard ERM to see how DUS acts as a modification to the data distribution
- **Quick check question:** How does changing the batch size for a specific modality alter the gradient flow compared to standard joint training?

**Concept: Policy Gradient (REINFORCE)**
- **Why needed here:** The "DUS" variant uses REINFORCE to learn the sampling policy
- **Quick check question:** In the context of this paper, what acts as the "Agent," what is the "Environment," and what constitutes the "Reward"?

## Architecture Onboarding

**Component map:** Unimodal Encoders -> Discrepancy Monitor -> Policy Network -> Data Sampler -> Fusion/Classifier

**Critical path:** The feedback loop between the Discrepancy Monitor and the Data Sampler. If the monitor does not update the state correctly, the policy network receives garbage input, and the rebalancing fails.

**Design tradeoffs:**
- Heuristic vs. RL: The heuristic is robust but static; RL is adaptive but unstable and requires tuning
- Plugin capability: DUS wraps existing methods, assuming base methods allow variable batch sizes per modality

**Failure signatures:**
- Unstable Loss: RL agent oscillates, causing batch sizes to swing wildly (e.g., 0 to 64)
- Dominance Persistence: Discrepancy scores fail to converge or accuracy drops below unimodal baselines

**First 3 experiments:**
1. **Sanity Check (Toy Experiment):** Manually set batch sizes on a single epoch to verify that reducing dominant batch size reduces the discrepancy gap
2. **Heuristic Baseline:** Implement DUS-H first as a simpler lower bound before attempting RL integration
3. **Ablation on NB:** Test the "Constant Batch Size" sensitivity to ensure the total data budget isn't restricting the agent's ability to maneuver

## Open Questions the Paper Calls Out

**Open Question 1:** How can DUS be adapted to support multimodal learning methods that strictly require paired data inputs? The current mechanism shuffles data and samples different quantities per modality, breaking the strict index alignment required by paired-loss architectures.

**Open Question 2:** Can the heuristic adaptive sampling strategy be refined to handle non-monotonic information shifts, particularly for datasets like CREMA-D? The heuristic relies on a fixed function assuming a specific learning curve, whereas the RL agent dynamically reacts to sudden shifts.

**Open Question 3:** Does the computational overhead of the reinforcement learning policy network negate the efficiency gains from reduced batch sizes? The paper focuses on accuracy but doesn't provide detailed complexity analysis of training the policy network alongside the multimodal model.

## Limitations
- The policy network architecture implementation details are not specified beyond input/output dimensions
- The effectiveness of the cumulative discrepancy score as a state representation lacks empirical validation in the paper
- The RL approach may suffer from non-stationarity issues in dynamic training environments
- The method assumes batch size adjustments directly translate to balanced information contribution, which may not hold for all modality pairs

## Confidence

**High:** The general concept of addressing modality imbalance through data sampling and the basic heuristic approach
**Medium:** The cumulative discrepancy metric's reliability as a state representation and the RL formulation's convergence properties
**Low:** The specific performance gains without detailed hyperparameter sensitivity analysis and robustness across diverse modality combinations

## Next Checks

1. Implement and test the heuristic DUS-H baseline first to establish a performance floor before attempting the RL variant
2. Conduct ablation studies on the cumulative discrepancy metric by comparing it against alternative state representations
3. Test the method's robustness by deliberately introducing synthetic modality imbalance scenarios beyond the natural imbalances present in the datasets