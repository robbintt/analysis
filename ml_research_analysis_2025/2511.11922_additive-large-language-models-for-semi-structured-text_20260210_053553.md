---
ver: rpa2
title: Additive Large Language Models for Semi-Structured Text
arxiv_id: '2511.11922'
source_url: https://arxiv.org/abs/2511.11922
tags:
- calm
- patient
- feature
- what
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of interpretability in large
  language models (LLMs) for clinical text classification, where physicians need to
  understand which parts of a patient's record drive risk predictions. The proposed
  method, CALM (Classification with Additive Large Language Models), is an inherently
  interpretable framework designed for semi-structured text inputs composed of semantically
  meaningful components, such as sections of admission notes or question-answer fields
  from intake forms.
---

# Additive Large Language Models for Semi-Structured Text

## Quick Facts
- arXiv ID: 2511.11922
- Source URL: https://arxiv.org/abs/2511.11922
- Reference count: 40
- Key outcome: CALM achieves interpretability through additive component contributions with minor performance drops (0.02-0.03 AUC-PR) compared to black-box fine-tuning

## Executive Summary
This paper introduces CALM (Classification with Additive Large Language Models), a framework for interpretable clinical text classification that enforces additive structure on component-level contributions. CALM processes semi-structured clinical text by encoding each semantically meaningful component independently and averaging their logits, making each component's influence directly observable. The framework achieves performance comparable to conventional black-box LLM classifiers while providing faithful explanations at both patient and population levels. Extensions like CALM2 (pairwise interactions) and CALM-Distill (knowledge distillation) further improve performance while preserving interpretability.

## Method Summary
CALM is an inherently interpretable framework for semi-structured text inputs composed of semantically meaningful components. The method encodes each component independently using a shared LLM backbone, maps them to class logits via component-specific heads, and produces predictions as the additive sum of these contributions. The final logit is the average of all component contributions plus a learned bias term. CALM2 extends this by adding pairwise interaction modules, while CALM-Distill incorporates knowledge distillation from fine-tuned teachers. The framework is evaluated on three clinical outcome tasks using seven open-source LLMs across three datasets.

## Key Results
- CALM achieves performance comparable to conventional LLM classifiers with minor performance drops (0.02-0.03 AUC-PR)
- CALM2 with pairwise interactions and CALM-Distill with knowledge distillation further improve performance while preserving interpretability
- The framework provides both global interpretability through feature importance scores and local interpretability through patient-level risk contributions
- Component-level risk curves enable clear visualizations of how different text sections contribute to predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Enforcing additive structure on component-level contributions produces inherently interpretable predictions without significant accuracy loss.
- Mechanism: Each input component is encoded independently by a shared LLM backbone, mapped to class logits via component-specific heads, and aggregated through averaging plus a bias term, making each component's influence directly observable.
- Core assumption: Input can be decomposed into semantically meaningful components whose contributions are approximately independent or can be modeled with limited pairwise interactions.
- Evidence anchors: [abstract] CALM predicts outcomes as additive sum enabling faithful explanations; [section 3.2] z = (1/M) Σ ℓᵢ + b shows the additive aggregation; Related work CHiRPE uses post-hoc methods vs. CALM's forward-pass interpretability.
- Break condition: Strong higher-order interactions that cannot be captured by pairwise terms degrade predictive performance.

### Mechanism 2
- Claim: Processing components independently reduces computational complexity relative to concatenating all text.
- Mechanism: Standard fine-tuning processes concatenated text in O(L²_tot) time. CALM processes each component independently for total cost Σ O(L²ᵢ), achieving M-fold speedup for M equal-length components.
- Core assumption: Component lengths are not highly imbalanced; practical batching with padding does not negate efficiency gains.
- Evidence anchors: [section 3.2] Σ L²ᵢ < L²_tot for M > 1; CALM achieves M-fold speedup for equal-length components.
- Break condition: Highly variable component lengths with padding may exceed O(L²_tot) cost in practice.

### Mechanism 3
- Claim: Knowledge distillation transfers predictive capability to the additive student while preserving interpretability.
- Mechanism: CALM-Distill trains the student using convex combination of cross-entropy (ground truth) and KL divergence (teacher's soft targets), guiding the student's representations without altering additive architecture.
- Core assumption: Teacher model captures patterns the additive student cannot express directly but can approximate through softened targets.
- Evidence anchors: [section 3.4] CALM-Distill leverages teacher's knowledge without modifying architecture; [section 4.3.3] Table 3 shows CALM-Distill consistently outperforms CALM across models and α settings.
- Break condition: Large teacher-student capacity gap limits distillation benefit; initialization from fine-tuned weights degrades CALM performance.

## Foundational Learning

- **Neural Additive Models (NAMs)**: CALM extends NAM principles from scalar features to textual components. Understanding NAMs clarifies why additive decomposition enables interpretability and expected performance trade-offs.
  - Quick check: Can you explain why averaging per-feature logits preserves interpretability while concatenating features before prediction does not?

- **Knowledge Distillation with Temperature Scaling**: CALM-Distill relies on temperature-scaled softmax to extract dark knowledge from teacher distributions.
  - Quick check: Why does increasing temperature T soften the probability distribution, and what does this reveal about the teacher's learned class relationships?

- **Block-Diagonal Attention Masks**: The packed implementation uses block-diagonal masks to process multiple components in one forward pass while enforcing isolation between them.
  - Quick check: How does a block-diagonal attention mask prevent tokens from one component attending to tokens in another, and why is per-feature positional re-indexing necessary?

## Architecture Onboarding

- **Component map**: Input decomposition → independent encoding → per-component logits → additive aggregation → softmax → loss computation
- **Critical path**: Input decomposition → independent encoding → per-component logits → additive aggregation → softmax → loss computation. For CALM2, interaction logits are computed from embedding-level pairs and combined with main logits using weight β.
- **Design tradeoffs**: Additive constraint limits expressivity but guarantees faithful interpretability; component-specific heads increase parameter count but enable per-component analysis; packed implementation improves efficiency but requires custom attention masks; CALM2 adds O(M²R) complexity balancing expressivity vs. interpretability.
- **Failure signatures**: Large performance drop (>0.05 AUC-PR) vs. fine-tuning suggests inappropriate component decomposition; highly imbalanced component lengths causing padding waste; feature importance scores contradicting clinical intuition may indicate spurious correlations; CALM2 showing no improvement suggests pairwise interactions are not informative.
- **First 3 experiments**:
  1. Baseline comparison: Run standard fine-tuning and CALM on same dataset with identical backbone/hyperparameters. Report AUC-PR gap. If gap >0.05, inspect component decomposition alignment.
  2. Ablation on component granularity: Test different decompositions (e.g., 8 sections vs. 50 Q-A pairs) to assess sensitivity to component definition. Monitor both performance and interpretability quality.
  3. CALM2 and CALM-Distill sweeps: For CALM2, sweep β ∈ {0.1, 0.3, 0.5} and R ∈ {8, 16}. For CALM-Distill, sweep α ∈ {0.2, 0.4, 0.6} with T=2. Identify settings minimizing performance gap while maintaining interpretable interaction visualizations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is CALM to errors in automatic segmentation when applied to free-text clinical notes?
- Basis in paper: [explicit] The paper states "similar structure can often be automatically extracted from free-text notes" and relies on tools like ClinStructor, but does not evaluate how segmentation errors affect prediction accuracy or explanation fidelity.
- Why unresolved: Experiments use pre-segmented datasets, leaving untested the real-world scenario where segmentation introduces errors.
- What evidence would resolve it: Experiments injecting varying levels of segmentation noise into inputs, measuring degradation in both AUC-PR and faithfulness of component-level attributions.

### Open Question 2
- Question: Can CALM's interpretability framework scale efficiently to higher-order interactions beyond pairwise (CALM2)?
- Basis in paper: [inferred] CALM2 addresses pairwise interactions with O(M²R) complexity, but the paper notes this approach "may become impractical for moderate or large M" for text-level interactions. Clinically, some diagnoses require 3+ factors jointly.
- Why unresolved: Quadratic scaling for pairwise terms suggests even higher-order interactions would be computationally prohibitive, yet their exclusion may miss important clinical patterns.
- What evidence would resolve it: Analysis of whether truncated or sparse approximations of higher-order interactions can capture most performance gains without full enumeration.

### Open Question 3
- Question: Do the component-level explanations produced by CALM align with clinician reasoning and improve clinical decision-making?
- Basis in paper: [explicit] The paper states "the interpretations produced by CALM... are specific to how the current model uses each feature, not to the inherent value or causal influence of the features themselves."
- Why unresolved: While CALM guarantees mathematical faithfulness, the paper does not include user studies or clinical validation to assess whether explanations match clinical intuition or aid decision-making.
- What evidence would resolve it: Clinician user studies measuring alignment between CALM's feature importance rankings and expert clinical reasoning, or randomized trials comparing decision quality with vs. without CALM explanations.

## Limitations
- Additive structure assumption may miss higher-order interactions between components, potentially limiting expressivity
- Empirical evaluation limited to three clinical datasets with binary classification tasks
- Performance gaps (0.02-0.03 AUC-PR) assumed acceptable without context-dependent validation
- Computational efficiency claims rely on theoretical analysis rather than measured runtime comparisons

## Confidence

**High Confidence**: The additive architecture enables faithful interpretability at both patient and population levels. The core mechanism—averaging per-component logits to produce final predictions—is mathematically sound and directly verifiable.

**Medium Confidence**: CALM achieves performance comparable to conventional LLM classifiers with minor drops (0.02-0.03 AUC-PR). While experimental results support this, evaluation is limited to specific datasets and configurations.

**Low Confidence**: Knowledge distillation consistently improves CALM performance while preserving interpretability. Results show variation across models, and the mechanism by which dark knowledge transfers to an inherently interpretable student is not fully explained.

## Next Checks

1. **Component Decomposition Sensitivity Analysis**: Systematically vary granularity of component decomposition to quantify relationship between component definition quality and both performance and interpretability. Measure how performance and interpretability quality change as components become too coarse or too fine.

2. **Higher-Order Interaction Validation**: Implement and evaluate CALM with triplet interactions beyond pairwise to test whether performance gap to black-box models persists. Compare interpretability quality of triplet interaction visualizations against pairwise ones.

3. **Cross-Domain Generalizability Test**: Apply CALM to non-clinical semi-structured text tasks (e.g., legal document classification, financial report analysis) where component boundaries are more naturally defined. Test whether additive interpretability advantage generalizes beyond clinical domain.