---
ver: rpa2
title: Offensive Language Detection on Social Media Using XLNet
arxiv_id: '2506.21795'
source_url: https://arxiv.org/abs/2506.21795
tags:
- language
- offensive
- classification
- learning
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of detecting offensive language
  on social media, particularly in the context of class imbalance and the limitations
  of manual moderation. It introduces an automatic detection system based on XLNet,
  a generalized autoregressive pretraining method, and compares its performance with
  BERT on the OLID dataset.
---

# Offensive Language Detection on Social Media Using XLNet

## Quick Facts
- **arXiv ID:** 2506.21795
- **Source URL:** https://arxiv.org/abs/2506.21795
- **Reference count:** 40
- **Primary result:** XLNet outperforms BERT on OLID dataset for offensive language detection, achieving Macro-F1 scores of 0.78 (Subtask A) and 0.71 (Subtask B).

## Executive Summary
This study addresses the challenge of detecting offensive language on social media, particularly in the context of class imbalance and the limitations of manual moderation. It introduces an automatic detection system based on XLNet, a generalized autoregressive pretraining method, and compares its performance with BERT on the OLID dataset. XLNet demonstrated superior performance in detecting offensive content and categorizing offense types, with Macro-F1 scores of 0.78 and 0.71, respectively, compared to BERT's 0.69 and 0.63. BERT slightly outperformed XLNet in identifying offense targets (Macro-F1: 0.37 vs. 0.36). Oversampling strategies improved classification performance, particularly for XLNet in offense categorization. The results highlight XLNet's potential for robust offensive language detection, especially in imbalanced scenarios, while underscoring the need for further research on underrepresented classes like 'OTH'.

## Method Summary
The study employs a hierarchical classification approach on the OLID dataset, which contains 14,100 annotated tweets. The framework consists of three subtasks: Subtask A (offensive vs. not offensive), Subtask B (type of offense: targeted or untargeted), and Subtask C (target of offense: individual, group, or other). The authors fine-tune both XLNet-Base and BERT-Base models, implementing random oversampling to address class imbalance. A critical methodological detail is the use of mean pooling of final hidden states followed by a feed-forward network for classification, rather than the standard [CLS] token approach. The models are trained for three epochs with specific hyperparameters: XLNet uses a learning rate of 2e-5, while BERT uses 5e-5, both with AdamW optimizer and weight decay of 0.01.

## Key Results
- XLNet achieves Macro-F1 of 0.78 for offensive vs. not offensive classification, outperforming BERT's 0.69
- For offense type categorization (Subtask B), XLNet reaches Macro-F1 of 0.71 versus BERT's 0.63
- BERT slightly outperforms XLNet in offense target identification (Subtask C) with Macro-F1 of 0.37 vs. 0.36

## Why This Works (Mechanism)
XLNet's superior performance stems from its permutation-based pretraining objective, which captures bidirectional context more effectively than BERT's masked language modeling. This allows XLNet to better understand the contextual nuances of offensive language, particularly in identifying subtle forms of aggression and the relationships between offensive content and its targets. The mean pooling strategy aggregates information across all tokens rather than relying solely on the [CLS] token, providing a more comprehensive representation of the tweet's semantic content. Additionally, the oversampling technique effectively addresses the class imbalance problem, particularly benefiting the offense type classification where the targeted and untargeted classes were imbalanced.

## Foundational Learning
- **Permutation Language Modeling (XLNet)**: A pretraining objective that captures bidirectional context without masking tokens, allowing the model to consider all possible token arrangements during training. This is crucial for understanding the full context of potentially offensive content.
- **Mean Pooling vs. [CLS] Token**: Aggregating all token representations through mean pooling provides a more comprehensive feature vector than relying on a single [CLS] token, which may not capture the full semantic content of shorter texts like tweets.
- **Hierarchical Classification Framework**: Breaking down the offensive language detection problem into three subtasks allows for more granular analysis and better handling of the complex nature of offensive content, where the type and target of offense are distinct but related concepts.
- **Random Oversampling**: A technique to address class imbalance by duplicating minority class samples in the training data, which is particularly important for the offense type classification where targeted and untargeted categories may be imbalanced.
- **Macro-F1 Metric**: An evaluation metric that calculates F1-score for each class independently and then averages them, providing a balanced assessment across all classes regardless of their frequency in the dataset.

## Architecture Onboarding
- **Component Map:** Input Text -> Preprocessing -> Tokenization -> XLNet Backbone -> Mean Pooling -> Feed-Forward Network -> Classification Output
- **Critical Path:** The mean pooling layer followed by the feed-forward network is critical, as it directly impacts classification performance and represents a key methodological difference from standard implementations.
- **Design Tradeoffs:** The choice of mean pooling over [CLS] token provides better performance but may lose some sequence-level information that the [CLS] token captures. Using XLNet-Base instead of XLNet-Large balances performance with computational constraints.
- **Failure Signatures:** Poor performance on Subtask C, particularly the 'OTH' class with 0.0 F1-score, suggests either genuine difficulty with underrepresented classes or issues with the classification head architecture.
- **Three First Experiments:**
  1. Verify mean pooling implementation vs. default [CLS] token usage
  2. Test classification head architecture variations (different dimensions, activation functions)
  3. Compare performance with and without oversampling for Subtasks B and C

## Open Questions the Paper Calls Out
- Can advanced ensemble strategies that combine the strengths of XLNet and BERT yield superior performance compared to individual models across all OLID subtasks?
- To what extent can domain adaptation techniques and larger, more diverse datasets improve the detection of underrepresented classes, specifically the 'OTH' (Other) category?
- Does scaling the architecture to XLNet-Large provide a significant performance advantage over XLNet-Base and BERT-Large in offensive language detection?

## Limitations
- The classification head architecture (feed-forward network) following mean pooling is not fully specified, making exact reproduction challenging
- The paper reports notably low performance on the "OTH" target class (0.0 F1), suggesting either genuine difficulty with this underrepresented class or potential issues in model configuration
- Preprocessing details lack specificity regarding exact implementation of hashtag segmentation and emoji replacement methods

## Confidence
- **High Confidence**: The comparative performance advantage of XLNet over BERT for overall offensive language detection and offense type classification
- **Medium Confidence**: The oversampling strategy's effectiveness and the specific contribution of mean pooling versus other design choices
- **Medium Confidence**: The hierarchical classification framework, though limited results for the "OTH" target class require careful validation

## Next Checks
1. Verify Pooling Implementation: Confirm that mean pooling of final hidden states is correctly implemented rather than using the default [CLS] token or XLNet's summary token
2. Validate Oversampling Application: Ensure random oversampling is correctly applied to training data for Subtasks B and C, testing both with and without oversampling
3. Test Classification Head Architecture: Experiment with different feed-forward network configurations to understand the impact of this unspecified component on overall performance, particularly for the challenging "OTH" target class