---
ver: rpa2
title: 'Shadows in the Code: Exploring the Risks and Defenses of LLM-based Multi-Agent
  Software Development Systems'
arxiv_id: '2511.18467'
source_url: https://arxiv.org/abs/2511.18467
tags:
- code
- software
- user
- agents
- malicious
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates security vulnerabilities in LLM-driven
  multi-agent software development systems through two attack scenarios: malicious
  users exploiting benign agents (MU-BA) and benign users interacting with compromised
  agents (BU-MA). The authors introduce Implicit Malicious Behavior Injection Attack
  (IMBIA) to inject hidden malicious capabilities into software, and Adversarial IMBIA
  (Adv-IMBIA) as a defense mechanism.'
---

# Shadows in the Code: Exploring the Risks and Defenses of LLM-based Multi-Agent Software Development Systems

## Quick Facts
- arXiv ID: 2511.18467
- Source URL: https://arxiv.org/abs/2511.18467
- Reference count: 27
- Authors: Xiaoqing Wang; Keman Huang; Bin Liang; Hongyu Li; Xiaoyong Du
- One-line primary result: IMBIA attack achieves up to 93% success rate in multi-agent systems; Adv-IMBIA defense reduces this by up to 73% when protecting critical agents

## Executive Summary
This paper investigates security vulnerabilities in LLM-driven multi-agent software development systems through two attack scenarios: malicious users exploiting benign agents (MU-BA) and benign users interacting with compromised agents (BU-MA). The authors introduce Implicit Malicious Behavior Injection Attack (IMBIA) to inject hidden malicious capabilities into software, and Adversarial IMBIA (Adv-IMBIA) as a defense mechanism. Experiments across ChatDev, MetaGPT, and AgentVerse frameworks show IMBIA achieving attack success rates of 93%, 45%, and 71% in MU-BA scenarios, and 71%, 84%, and 45% in BU-MA scenarios. The defense mechanism significantly reduced these rates, particularly in MU-BA scenarios. Analysis reveals that coding and testing phases are most vulnerable when infiltrated, and strategic protection of critical agents can achieve nearly equivalent security to defending all agents. The study highlights the urgent need for robust security measures in multi-agent software development systems and provides practical guidelines for implementing targeted defensive strategies.

## Method Summary
The study evaluates security vulnerabilities in LLM-based multi-agent software development systems through controlled attack and defense experiments. The researchers implemented IMBIA attacks by injecting malicious prompts into user requirements (MU-BA) or agent profiles (BU-MA) using a tripartite structure: task summary, detailed description, and code instructions. They tested across three frameworks (ChatDev, MetaGPT, AgentVerse) using GPT-4o-mini as the base model and GPT-4o for maliciousness evaluation. The defense mechanism, Adv-IMBIA, introduces adversarial prompts that explicitly prohibit malicious behaviors and integrates them at either agent profiles (MU-BA) or user interface (BU-MA). The evaluation used 40 benign software requirements from the Software Requirement Description Dataset combined with 12 malicious behavior prompts, measuring Attack Success Rate, Benign Utility, and Reject Rate.

## Key Results
- IMBIA attack achieved 93%, 45%, and 71% success rates in MU-BA scenarios across ChatDev, MetaGPT, and AgentVerse respectively
- Adv-IMBIA defense reduced attack success rates by 73%, 40%, and 49% in MU-BA scenarios, and 45%, 7%, and 42% in BU-MA scenarios
- Coding and testing phases present significantly higher security risks than design phases when agents are compromised
- Strategic protection of critical agents (test-phase in ChatDev, design-phase in MetaGPT, code-phase in AgentVerse) achieves nearly equivalent security to defending all agents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Malicious prompts injected into user requirements or agent profiles can propagate through multi-agent systems to produce executable malware while maintaining surface-level software quality.
- Mechanism: IMBIA structures malicious payloads as a tripartite prompt: (1) task summary that avoids triggering safety filters, (2) detailed task description providing execution context, and (3) code instructions/snippets for implementation. In MU-BA scenarios, this appends to user requirements (Pb ⊕ Pm). In BU-MA scenarios, it modifies agent profiles (ai ⊕ Pm). The attack succeeds because later-stage agents (coding, testing) execute instructions with memory of prior outputs, enabling malicious code integration without degrading perceived software utility.
- Core assumption: Agents will prioritize completing all stated tasks without discriminating between benign user intent and injected instructions.
- Evidence anchors:
  - [abstract]: "IMBIA achieving attack success rates of 93%, 45%, and 71% in MU-BA scenarios, and 71%, 84%, and 45% in BU-MA scenarios"
  - [section]: Ablation study (Figure 5) shows secret task summary alone exceeds 54% ASR, with descriptions contributing more than code instructions
  - [corpus]: "Multi-Agent Systems Execute Arbitrary Malicious Code" confirms multi-agent systems will execute arbitrary code from untrusted inputs, validating the attack surface
- Break condition: Agents with explicit security/ethical guidelines that reject secondary task instructions, or architectures where task propagation is restricted (MetaGPT's SOP-based design transmits user tasks only to initial agents, reducing MU-BA vulnerability to 45% vs. ChatDev's 93%)

### Mechanism 2
- Claim: Coding and testing phases present significantly higher security risks than design phases when agents are compromised.
- Mechanism: In waterfall-based multi-agent systems, design-phase outputs are abstract specifications. Coding agents translate these into executable code, and testing agents validate/modify implementations. Compromised coding agents can directly inject malicious functions, while testing agents—with memory of prior outputs—can add malicious code during final validation. Design-phase compromise has limited impact because subsequent agents may override or ignore malicious abstractions during concrete implementation.
- Core assumption: Malicious code injection is more effective during implementation than during specification.
- Evidence anchors:
  - [abstract]: "compromised agents in the coding and testing phases pose significantly greater security risks"
  - [section]: Figure 6 shows ChatDev design-only ASR ≈ 0.03, code-only = 0.45, test-only = 0.68; MetaGPT design-only = 0.14, code-only = 0.78; AgentVerse code-only highest at 0.45
  - [corpus]: Weak direct evidence; "Multi-Agent Security Tax" discusses security/capability tradeoffs but doesn't address phase-specific vulnerabilities
- Break condition: Agile-style architectures with iterative group discussions (AgentVerse) constrain single-agent control over entire phases, reducing BU-MA effectiveness to 45% vs. waterfall-based MetaGPT's 84%

### Mechanism 3
- Claim: Strategic defense of critical agents achieves nearly equivalent protection to defending all agents, with different critical agents identified per framework.
- Mechanism: Adv-IMBIA injects adversarial prompts (Padv) that explicitly prohibit malicious behaviors (data exfiltration, unauthorized access, self-replication, ransomware patterns). For MU-BA, these integrate into agent profiles; for BU-MA, they're added at the user interface level. The defense is more effective in MU-BA (73%, 40%, 49% ASR reduction) than BU-MA (45%, 7%, 42%) because agent-level security instructions can override user malicious prompts more reliably than user-level instructions can detect compromised agent outputs.
- Core assumption: Adversarial prompts successfully override or detect malicious instructions without significantly degrading legitimate software generation capability.
- Evidence anchors:
  - [abstract]: "strategic protection of critical agents can achieve nearly equivalent security to defending all agents"
  - [section]: Figure 7 shows MetaGPT optimal with design-agent defense (ASR-d 0.05), ChatDev with test-agent defense (ASR-d 0.20), AgentVerse with code-agent defense (ASR-d 0.22)—all comparable to all-agent defense
  - [corpus]: "BlindGuard: Safeguarding LLM-based Multi-Agent Systems" addresses unknown attacks via unsupervised methods, but doesn't validate targeted agent defense strategies
- Break condition: Corrupted adversarial prompts themselves, or malicious agents that can strip/bypass security instructions before processing user requirements

## Foundational Learning

- Concept: Waterfall vs. Agile multi-agent architectures
  - Why needed here: Framework vulnerability patterns directly correlate with architectural choices—waterfall (ChatDev, MetaGPT) shows phase-sequential vulnerability propagation; agile-style (AgentVerse) shows distributed resistance but different weak points
  - Quick check question: Can you explain why MetaGPT's SOP-based task propagation to only initial agents reduces MU-BA vulnerability but not BU-MA vulnerability?

- Concept: Prompt injection and adversarial prompting
  - Why needed here: Both attack and defense mechanisms are prompt-based; understanding how malicious instructions can bypass safety filters (implicit injection) and how adversarial instructions can counter them is essential
  - Quick check question: Why does the paper structure IMBIA as three components (summary, description, code) rather than a single malicious prompt?

- Concept: Attack success rate vs. utility tradeoffs
  - Why needed here: The paper shows attacks maintain software quality (consistency metrics don't significantly degrade), making malicious software harder to detect; defense mechanisms must balance security with maintaining benign utility
  - Quick check question: What does it mean that "consistency—a key quality indicator measuring cosine similarity between generated code and normal software requirements—showed no significant degradation during attacks"?

## Architecture Onboarding

- Component map:
  - Attack pipeline: Benign requirements (Pb) → IMBIA injection (summary Ts + description Td + code instructions Ci) → Multi-agent system (ChatDev/MetaGPT/AgentVerse) → Malicious software (S)
  - Defense pipeline: Adversarial prompts (Padv) → Integration point (agent profiles for MU-BA, user interface for BU-MA) → Protected multi-agent system → Sanitized software
  - Evaluation framework: GPT-4o-based maliciousness assessment (4-level severity: 0=no malice, 1=low, 2=moderate, 3=high) with 86.34% human-agreement consistency

- Critical path:
  1. MU-BA: User requirements → malicious prompt appended → broadcast to all agents (ChatDev/AgentVerse) or initial agent only (MetaGPT) → coding/testing phases integrate malicious code
  2. BU-MA: Benign requirements → compromised agent profiles → malicious behavior injected during that agent's phase → downstream agents incorporate/validate malicious code
  3. Defense: Identify framework-specific critical agents → inject adversarial prompts into those agents (MU-BA) or user interface (BU-MA) → agents reject/modify malicious code

- Design tradeoffs:
  - Task propagation: Broadcast (ChatDev, AgentVerse) enables broader collaboration but increases attack surface; SOP-restricted (MetaGPT) reduces MU-BA vulnerability but may limit legitimate functionality
  - Defense placement: Agent-level defense (MU-BA) is more effective but requires modifying agent configurations; user-level defense (BU-MA) is easier to deploy but less effective (7% reduction in MetaGPT vs. 40% in MU-BA)
  - Phase protection: All-phase defense is comprehensive but resource-intensive; targeted critical-agent defense achieves near-equivalent results with lower cost

- Failure signatures:
  - False negative defense: Generated software passes security checks but contains executable malicious code (Level 3 malice)—indicates adversarial prompt gaps
  - False positive defense: Benign software incorrectly flagged and rejected—indicates overly restrictive adversarial prompts
  - Quality degradation: Significant drops in completeness, executability, or consistency metrics under defense—indicates defense prompts interfering with legitimate task completion
  - Inconsistent attack effectiveness: All-phase attack yields lower ASR than single-phase—indicates agent decision conflicts between benign and malicious tasks

- First 3 experiments:
  1. Replicate IMBIA attack on ChatDev with GPT-4o-mini base model using provided 12 malicious behaviors (Table 1) across 40 benign tasks; measure ASR, RR, and UUA to validate baseline vulnerability (expected: ~93% ASR per paper)
  2. Implement targeted Adv-IMBIA defense on ChatDev testing-phase agents only; compare ASR-d and utility metrics against all-agent defense to validate critical-agent hypothesis (expected: ~0.20 ASR-d vs. ~0.02 all-agent)
  3. Test cross-framework transfer: Apply MetaGPT's optimal defense configuration (design-agent protection) to ChatDev; measure effectiveness to validate whether critical agents are framework-specific or generalizable (expected: lower effectiveness than ChatDev's optimal test-agent defense based on paper's framework-specific findings)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can defense mechanisms be specifically optimized for the Benign User with Malicious Agents (BU-MA) scenario, given that current user-level defenses are significantly less effective than agent-level defenses in the Malicious User with Benign Agents (MU-BA) scenario?
- Basis in paper: [explicit] The authors state that "Adv-IMBIA exhibits substantially greater efficacy in the MU-BA scenario compared to the BU-MA scenario," noting that "defending against compromised agents at the user level presents greater challenges."
- Why unresolved: The paper demonstrates a gap in defense performance (e.g., only a 7% reduction in ASR for MetaGPT in BU-MA vs. 40% in MU-BA) but does not propose a solution to close this specific gap.
- What evidence would resolve it: A modified defense strategy that achieves parity in Attack Success Rate (ASR) reduction between BU-MA and MU-BA scenarios across all tested frameworks.

### Open Question 2
- Question: Can the "decision-making conflicts" observed in all-phase attacks be reliably synthesized as a defensive strategy to induce agent hesitation without degrading software utility?
- Basis in paper: [inferred] The paper notes a counterintuitive finding that attacking all agents simultaneously often fails to yield the highest effectiveness due to "decision-making conflicts" where agents "oscillate between fulfilling benign objectives and performing malicious tasks."
- Why unresolved: While the phenomenon is identified as a cause of attack failure in all-phase infiltration, the study does not explore if this friction can be intentionally induced by the system (e.g., via "canary" agents) to thwart attacks.
- What evidence would resolve it: Experiments showing that introducing specific conflicting sub-tasks or oversight agents increases the "hesitation" or rejection rates of malicious behaviors without lowering the Benign Utility (BU).

### Open Question 3
- Question: To what extent do the identified critical phase vulnerabilities (coding and testing) generalize to models with stronger inherent safety alignment or different reasoning architectures?
- Basis in paper: [inferred] The primary detailed analysis of phase vulnerabilities (RQ1/RQ2) utilizes GPT-4o-mini. While Table 2 briefly compares base models on ASR, it does not break down vulnerability by development phase (design vs. code vs. test) for other models like Claude or Llama.
- Why unresolved: It is unclear if the "coding" and "testing" phases are universally the weak points due to the nature of code generation, or if this is an artifact of the specific GPT-4o-mini architecture/alignment used.
- What evidence would resolve it: A replication of the RQ1 analysis (phase-specific ASR) across diverse model families (e.g., Claude, Llama) to see if the "later stages" remain the primary attack vectors.

### Open Question 4
- Question: How robust is the Adv-IMBIA defense against adaptive attacks designed to obfuscate malicious intent specifically to bypass the listed "security and ethical guidelines"?
- Basis in paper: [inferred] The defense mechanism (Adv-IMBIA) relies on explicit adversarial prompts listing forbidden behaviors (e.g., "do not access external URLs," "do not delete files").
- Why unresolved: The paper evaluates the defense against the standard IMBIA attacks but does not test "adaptive" attacks where the prompt is rewritten to disguise malicious actions as benign or administrative tasks that might technically comply with the defense's text while violating its intent.
- What evidence would resolve it: Evaluation results using attack prompts specifically optimized against the Adv-IMBIA defensive instructions, showing whether the explicit rule-based prompting is susceptible to semantic bypassing.

## Limitations
- The study relies heavily on GPT-4o-mini as the base model and GPT-4o for evaluation, raising questions about generalizability across different LLM architectures and versions
- The evaluation methodology depends on automated maliciousness classification, which may miss subtle attack variations despite 86.34% human agreement
- The study focuses on waterfall-style development systems, with limited exploration of agile or iterative architectures
- The defense mechanisms show effectiveness but introduce potential utility trade-offs that require further investigation

## Confidence
- **High Confidence:** IMBIA attack mechanism effectiveness (93%, 45%, 71% ASR across frameworks), phase-specific vulnerability findings (coding/testing phases most vulnerable), and critical-agent defense strategy (near-equivalent protection with strategic targeting)
- **Medium Confidence:** Cross-framework generalizability of findings, long-term effectiveness of Adv-IMBIA under evolving threat landscapes, and utility-maintenance claims under defense
- **Low Confidence:** Defense mechanism robustness against sophisticated adversarial attacks, performance on non-waterfall architectures, and scalability to larger, more complex software systems

## Next Checks
1. **Cross-Model Validation:** Replicate the IMBIA attack and Adv-IMBIA defense across multiple LLM models (e.g., Claude, Llama, Gemini) to assess whether observed vulnerabilities and defenses are model-specific or generalizable patterns.

2. **Defense Robustness Testing:** Systematically probe Adv-IMBIA's limitations by designing adversarial prompts specifically engineered to bypass the prohibited behavior keywords and test the defense's resilience against evolving attack strategies.

3. **Real-World Deployment Simulation:** Implement a controlled deployment of the defended multi-agent system in a realistic software development environment, measuring not only attack success rates but also developer productivity, code quality consistency, and false positive/negative rates over extended use.