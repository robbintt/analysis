---
ver: rpa2
title: Unsupervised Decomposition and Recombination with Discriminator-Driven Diffusion
  Models
arxiv_id: '2601.22057'
source_url: https://arxiv.org/abs/2601.22057
tags:
- diffusion
- latent
- recombination
- discriminator
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces an adversarial training signal for diffusion-based
  models to improve unsupervised decomposition and compositional generation. The method
  uses a discriminator trained to distinguish between single-source and recombined
  samples, guiding the generator to produce physically and semantically consistent
  recombinations.
---

# Unsupervised Decomposition and Recombination with Discriminator-Driven Diffusion Models

## Quick Facts
- arXiv ID: 2601.22057
- Source URL: https://arxiv.org/abs/2601.22057
- Reference count: 40
- Primary result: Improved FID scores and disentanglement metrics through discriminator-driven adversarial training for unsupervised decomposition

## Executive Summary
This work introduces an adversarial training signal for diffusion-based models to improve unsupervised decomposition and compositional generation. The method uses a discriminator trained to distinguish between single-source and recombined samples, guiding the generator to produce physically and semantically consistent recombinations. Experiments across multiple image datasets show improved Fréchet Inception Distance scores and better disentanglement metrics compared to Decomp Diffusion. The approach is also applied to robotic video trajectories, where recombining learned action components significantly increases state-space coverage for exploration.

## Method Summary
The method builds upon diffusion-based decomposition models by introducing an adversarial discriminator as a training signal. The discriminator learns to distinguish between original single-source samples and recombined samples generated by the model. This adversarial loss guides the diffusion generator to produce decompositions that can be recombined into physically and semantically consistent outputs. The approach operates in an unsupervised manner, requiring only the raw data without supervision signals indicating which components should be extracted or how they should combine.

## Key Results
- Improved Fréchet Inception Distance scores across CelebA-HQ, Virtual KITTI, CLEVR, and Falcor3D datasets compared to Decomp Diffusion baseline
- Better disentanglement metrics (MIG, MCC) demonstrating more effective component separation
- Successful application to robotic video trajectories, where recombined action components increased state-space coverage for exploration in the LIBERO benchmark

## Why This Works (Mechanism)
The adversarial discriminator provides a direct optimization signal for the quality of recombinations, addressing a key weakness in previous decomposition approaches where the generator lacked feedback on whether its extracted components could be meaningfully recombined. By training the discriminator to distinguish between authentic single-source samples and model-generated recombinations, the system learns to produce decompositions that preserve the essential characteristics needed for coherent reconstruction.

## Foundational Learning
- **Diffusion models**: Why needed - form the base generative framework for decomposition; Quick check - understand forward noising and reverse denoising processes
- **Adversarial training**: Why needed - provides the critical signal for evaluating recombination quality; Quick check - understand GAN-style discriminator-generator dynamics
- **Unsupervised disentanglement**: Why needed - enables learning without explicit component labels; Quick check - grasp the challenge of separating mixed factors without supervision
- **Fréchet Inception Distance**: Why needed - standard metric for evaluating generative model quality; Quick check - understand how FID compares feature distributions
- **Disentanglement metrics (MIG, MCC)**: Why needed - quantify how well components separate semantically meaningful factors; Quick check - understand mutual information-based evaluation
- **Compositional generation**: Why needed - the ultimate goal of creating new samples by recombining learned components; Quick check - grasp how component recombination enables creative generation

## Architecture Onboarding
Component map: Data -> Diffusion Generator -> Recombined Samples + Discriminator -> Adversarial Loss -> Generator Update

Critical path: The adversarial loop between generator and discriminator is the critical path. The discriminator provides real-time feedback on recombination quality, which directly shapes the generator's decomposition strategy.

Design tradeoffs: The method trades off the stability of pure likelihood-based diffusion training for the improved recombination quality enabled by adversarial signals. This introduces potential training instability but gains in semantic coherence.

Failure signatures: Mode collapse in the discriminator, where it becomes too easily fooled, would lead to poor decomposition quality. Conversely, if the discriminator is too strong, it may prevent meaningful learning by making the adversarial loss dominate.

First experiments:
1. Test discriminator's ability to distinguish single-source vs recombined samples on a simple dataset
2. Evaluate decomposition quality with and without adversarial loss on a toy dataset
3. Measure FID improvement when adding discriminator to a baseline diffusion decomposition model

## Open Questions the Paper Calls Out
None

## Limitations
- The method's effectiveness depends heavily on discriminator architecture and training stability, with risks of mode collapse
- Performance on natural images with complex, entangled backgrounds remains untested beyond controlled datasets
- Quantitative metrics don't fully capture whether extracted components are semantically meaningful or align with human intuitions

## Confidence
- High confidence in technical implementation and experimental methodology on benchmark datasets
- Medium confidence in generalization capability to more complex, real-world scenarios
- Medium confidence in semantic meaningfulness of extracted components, pending further validation

## Next Checks
1. Test the method on natural scene datasets with complex backgrounds (e.g., COCO, Cityscapes) to evaluate robustness beyond controlled environments
2. Conduct human perceptual studies to validate whether recombined samples appear physically plausible and whether extracted components align with human semantic understanding
3. Analyze the method's sensitivity to hyperparameter choices and discriminator architecture variations to establish more robust training guidelines