---
ver: rpa2
title: 'CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in
  Large Language Models'
arxiv_id: '2509.09675'
source_url: https://arxiv.org/abs/2509.09675
tags:
- bonus
- exploration
- arxiv
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CDE (Curiosity-Driven Exploration) tackles the challenge of poor
  exploration in RLVR training of LLMs, where models tend to converge prematurely
  and suffer from entropy collapse. It introduces a framework that uses intrinsic
  curiosity signals from both the actor (measured by perplexity over generated responses)
  and the critic (measured by variance across multi-head value estimates) as exploration
  bonuses.
---

# CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models

## Quick Facts
- arXiv ID: 2509.09675
- Source URL: https://arxiv.org/abs/2509.09675
- Authors: Runpeng Dai, Linfeng Song, Haolin Liu, Zhenwen Liang, Dian Yu, Haitao Mi, Zhaopeng Tu, Rui Liu, Tong Zheng, Hongtu Zhu, Dong Yu
- Reference count: 24
- Primary result: +3 points improvement on AIME benchmarks over standard GRPO/PPO

## Executive Summary
CDE (Curiosity-Driven Exploration) addresses the critical challenge of poor exploration in RLVR training of LLMs, where models typically converge prematurely and suffer from entropy collapse. The framework introduces intrinsic curiosity signals derived from both the actor (measured by perplexity over generated responses) and the critic (measured by variance across multi-head value estimates) as exploration bonuses. These bonuses guide the model to explore novel or uncertain reasoning paths while maintaining performance. Empirically, CDE achieves approximately +3 points improvement over standard GRPO/PPO on AIME benchmarks, with multi-head PPO yielding consistent gains and PPL bonus improving calibration by maintaining separation between correct and incorrect responses.

## Method Summary
CDE introduces a curiosity-driven exploration framework that augments standard RLVR training with intrinsic curiosity bonuses. The actor bonus uses perplexity over generated responses to encourage exploration of novel reasoning paths, while the critic bonus leverages variance across multi-head value estimates to capture uncertainty. These bonuses are integrated into the policy gradient objective, providing additional incentives for the model to explore diverse and uncertain regions of the solution space. The framework is implemented within the GRPO/PPO framework, with specific modifications to incorporate the curiosity signals into the advantage estimation and policy update steps.

## Key Results
- Achieves approximately +3 points improvement over standard GRPO/PPO on AIME benchmarks
- Multi-head PPO yields consistent performance gains across experiments
- PPL bonus improves calibration by maintaining separation between correct and incorrect responses

## Why This Works (Mechanism)
CDE addresses the exploration-exploitation dilemma in RLVR by providing intrinsic motivation signals that guide the model toward uncertain or novel reasoning paths. The actor bonus (perplexity-based) encourages the model to explore responses that are surprising or difficult to predict, effectively penalizing overconfidence in incorrect answers while promoting diverse correct responses. The critic bonus (variance-based) captures epistemic uncertainty in value estimates, directing exploration toward states where the model's value predictions are most uncertain. Together, these bonuses create a richer reward signal that helps prevent premature convergence and entropy collapse.

## Foundational Learning
- **RLVR Training**: Reinforcement Learning from Verbatim Feedback - needed to understand the specific training paradigm where LLMs learn from human-provided reasoning traces; quick check: verify understanding of how feedback is incorporated into policy updates.
- **Entropy Collapse**: The phenomenon where RL models converge too quickly to suboptimal policies with low diversity; needed to appreciate why standard RLVR methods fail; quick check: identify signs of entropy collapse in training curves.
- **Intrinsic Curiosity**: Using model-derived signals (perplexity, uncertainty) as exploration bonuses; needed to grasp the core innovation beyond standard extrinsic rewards; quick check: distinguish between intrinsic and extrinsic reward signals.
- **Multi-head Value Networks**: Maintaining multiple value estimators to capture epistemic uncertainty; needed to understand the critic bonus mechanism; quick check: explain how variance across heads measures uncertainty.
- **Perplexity as Exploration Signal**: Using prediction difficulty as a proxy for novelty; needed to understand the actor bonus mechanism; quick check: calculate perplexity for sample sequences.

## Architecture Onboarding
**Component Map**: LLM (Actor) -> Perplexity Module -> Actor Bonus -> PPO Update; LLM (Critic) -> Multi-head Value Network -> Variance Calculator -> Critic Bonus -> PPO Update
**Critical Path**: Generate response → Calculate perplexity → Compute actor bonus → Update policy; Generate value estimates across heads → Calculate variance → Compute critic bonus → Update policy
**Design Tradeoffs**: 
- Actor bonus trades computational overhead (perplexity calculation) for improved exploration
- Multi-head critic trades memory and compute for better uncertainty estimation
- Combined bonuses may introduce hyperparameter tuning complexity
**Failure Signatures**: 
- Premature convergence despite curiosity bonuses (bonus magnitude too low)
- Unstable training (bonus magnitude too high, overwhelming task rewards)
- No improvement over baseline (curiosity signals not well-calibrated)
**First Experiments**:
1. Compare single-head vs multi-head PPO on AIME to verify variance-based exploration benefits
2. Test perplexity bonus alone vs combined bonuses to isolate actor contribution
3. Measure entropy over training to verify prevention of collapse

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical validation limited to small dataset (24 AIME problems) and narrow model range (3B-8B parameters)
- Theoretical analysis relies on strong linear MDP assumptions that may not hold in practice
- Computational overhead of multi-head PPO and curiosity calculations not characterized

## Confidence
- **High Confidence**: RLVR entropy collapse problem is well-established; curiosity-based exploration is theoretically sound
- **Medium Confidence**: +3 point improvement on AIME is statistically supported but limited by small dataset and model range
- **Low Confidence**: Theoretical guarantees rely on strong assumptions; calibration improvements need broader validation

## Next Checks
1. Replicate experiments on larger, more diverse reasoning datasets (MATH, GSM8K) and across broader model sizes (1B-70B parameters)
2. Test framework on non-linear state representations to validate theoretical assumptions
3. Measure computational overhead (time, memory, gradients) across different model scales and batch sizes