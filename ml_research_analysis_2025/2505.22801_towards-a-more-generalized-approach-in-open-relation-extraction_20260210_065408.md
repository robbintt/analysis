---
ver: rpa2
title: Towards a More Generalized Approach in Open Relation Extraction
arxiv_id: '2505.22801'
source_url: https://arxiv.org/abs/2505.22801
tags:
- novel
- relation
- relations
- known
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of Open Relation Extraction (OpenRE),
  which seeks to identify and extract novel relational facts between named entities
  from unlabeled data without pre-defined relation schemas. Existing OpenRE methods
  typically assume that the unlabeled data consists solely of novel relations or is
  pre-divided into known and novel instances.
---

# Towards a More Generalized Approach in Open Relation Extraction

## Quick Facts
- arXiv ID: 2505.22801
- Source URL: https://arxiv.org/abs/2505.22801
- Reference count: 15
- Primary result: MixORE achieves F1 scores of 0.8328-0.9156 for known relations and B3 F1 scores of 0.9585-0.9779 for novel relations across three benchmark datasets.

## Executive Summary
This paper addresses the challenge of Open Relation Extraction (OpenRE), where the goal is to identify and extract novel relational facts between named entities from unlabeled data without pre-defined relation schemas. The proposed MixORE framework introduces a two-phase approach that first detects potential novel relations using a Semantic Autoencoder and Gaussian Mixture Model, then performs joint learning of known and novel relations through contrastive learning. The method demonstrates state-of-the-art performance on benchmark datasets, effectively balancing the dual objectives of known relation classification and novel relation discovery.

## Method Summary
MixORE operates in two phases: Novel Relation Detection (NRD) and Open-World Semi-Supervised (OW-SS) Joint Learning. In Phase 1, a Semantic Autoencoder (SAE) projects labeled instances into a latent space where known relations cluster around one-hot vectors, identifying outliers as potential novel relations. These outliers are clustered using Gaussian Mixture Models (GMM) and filtered by posterior probability to generate weak labels. In Phase 2, the framework employs contrastive learning with triplet margin loss for labeled data and clustering exemplar loss to progressively refine the model, using a rehearsal-based continual learning strategy to avoid catastrophic forgetting. The final model classifies known relations directly while clustering novel relations using K-Means.

## Key Results
- MixORE achieves F1 scores of 0.8328, 0.8833, and 0.9156 on FewRel, TACRED, and Re-TACRED datasets respectively for known relation classification
- The method obtains B3 F1 scores of 0.9585, 0.8973, and 0.9779 for novel relation clustering across the three datasets
- Outperforms competitive baselines consistently across all benchmark datasets
- Demonstrates effectiveness in balancing known relation classification and novel relation discovery

## Why This Works (Mechanism)

### Mechanism 1: Semantic Autoencoder for Outlier Detection
Novel relations are detected as outliers in a constrained latent space built from known relations. The Semantic Autoencoder projects labeled instances into a |C_known|-dimensional latent space where each known relation aligns with a one-hot vector. During inference, unlabeled instances that map poorly to these one-hot vectors are flagged as outliers since novel relations lack corresponding anchor points.

### Mechanism 2: High-Confidence Weak Label Extraction via GMM
After SAE identifies outliers (lowest 5% mapping scores), GMM clusters them into |C_novel| groups. Only instances with posterior probability >0.95 are retained as weak labels, reducing noise propagation. Despite cluster purity ranges of 0.556-0.636, final performance remains strong, suggesting noise tolerance.

### Mechanism 3: Dual-Path Contrastive Learning for Joint Optimization
The framework combines instance-level contrastive learning (triplet margin loss) with distribution-level clustering exemplar loss. Triplet margin loss pulls same-relation labeled instances closer while pushing different-relation instances apart. Clustering exemplar loss aligns instances with multi-granularity K-Means centroids. The combined OW-SS loss jointly optimizes both objectives during continual learning.

## Foundational Learning

- **Concept: Open-World vs Closed-World Relation Extraction**
  - Why needed here: MixORE operates in a setting where unlabeled data contains both known and novel relations, unlike traditional RE that assumes all relations are pre-defined.
  - Quick check question: Can you explain why standard supervised RE fails when novel relations appear at inference time?

- **Concept: Contrastive Learning with Positive/Negative Pairs**
  - Why needed here: The OW-SS joint learning phase relies on triplet margin loss and clustering exemplar loss—both contrastive objectives.
  - Quick check question: In triplet margin loss, what would happen if the margin γ is set too low? How would representation quality be affected?

- **Concept: Continual Learning with Rehearsal**
  - Why needed here: MixORE uses a rehearsal-based continual learning strategy to avoid catastrophic forgetting when incorporating weak-labeled novel instances.
  - Quick check question: Why can't we simply train on D_l and D_w simultaneously from the start instead of using warmup + continual learning?

## Architecture Onboarding

- **Component map:** Frozen BERT_base encoder → SAE projection → outlier detection (5% lowest mapping scores) → GMM clustering → weak label extraction (posterior >0.95) → Fine-tuned BERT_base encoder + linear classifier → warmup on D_l → continual training on D_l ∪ D_w → OW-SS loss → Inference with trained encoder → K-Means clustering for novel relations

- **Critical path:** SAE training on labeled data (must converge before NRD) → Weak label quality (directly impacts Phase 2 effectiveness) → Warmup on D_l (stabilizes encoder before mixed training) → Continual learning with D_w (final performance depends on this)

- **Design tradeoffs:** SAE keeps BERT frozen for efficiency vs fine-tuned encoder in Phase 2: Phase 1 prioritizes speed; Phase 2 prioritizes representation quality. Positive pairs only from D_l vs including D_w: Reduces noise but limits positive pair diversity. Pre-defined |C_novel| vs adaptive estimation: Simpler implementation but less flexible.

- **Failure signatures:** Low cluster purity (>50% noise) indicates SAE outlier detection is not discriminative enough; large performance gap between known F1 and novel clustering suggests overfitting to known relations; known relation F1 drops during continual learning indicates catastrophic forgetting.

- **First 3 experiments:** 1) Sanity check: Reproduce SAE outlier detection on FewRel and verify ground-truth novel instances have significantly lower mapping scores. 2) Weak label quality analysis: Run GMM clustering on detected outliers and measure cluster purity before/after posterior filtering. 3) Ablation validation: Remove one loss component at a time on held-out validation set and compare F1 drops to reported values.

## Open Questions the Paper Calls Out
- How can the number of novel relations be automatically determined rather than pre-defined?
- How can hierarchical dependencies between relations be explicitly modeled in the OpenRE framework?
- How sensitive is MixORE's performance to the 5% outlier detection threshold and posterior probability cutoff of 0.95?
- How does MixORE perform when the proportion and distribution of novel relations differ substantially from the experimental setting?

## Limitations
- The method requires pre-defining the number of novel relations, which assumes prior knowledge of the problem space
- SAE outlier detection shows modest effectiveness with only 3-5 of 6 novel relations successfully identified per dataset
- The fixed 5% threshold for outlier selection is somewhat arbitrary and may not generalize across datasets
- The method requires substantial unlabeled data to be effective

## Confidence
- **High Confidence**: Overall framework architecture and experimental results demonstrating MixORE's superior performance across all three benchmark datasets
- **Medium Confidence**: Specific mechanisms of Semantic Autoencoder outlier detection and weak label extraction, given modest cluster purity values
- **Low Confidence**: Generalizability of the pre-defined |C_novel| approach to real-world scenarios where the number of novel relations is truly unknown

## Next Checks
1. Test MixORE with different outlier detection thresholds (2.5%, 7.5%, 10%) on FewRel to determine optimal selection percentage and its impact on novel relation discovery rates
2. Implement alternative clustering methods (HDBSCAN, DBSCAN) for outlier post-processing and compare cluster purity and downstream performance against GMM
3. Evaluate MixORE's performance on reduced unlabeled data subsets (25%, 50%, 75% of original) to quantify minimum data requirements for effective novel relation discovery