---
ver: rpa2
title: 'UIPro: Unleashing Superior Interaction Capability For GUI Agents'
arxiv_id: '2509.17328'
source_url: https://arxiv.org/abs/2509.17328
tags:
- action
- data
- uipro
- agent
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UIPro, a generalist GUI agent designed to
  interact with graphical user interfaces across multiple platforms. The authors address
  the challenges of limited scenario diversity and heterogeneous action spaces in
  existing GUI datasets by curating a massive 20.6 million-sample GUI understanding
  dataset and proposing a unified action space for training.
---

# UIPro: Unleashing Superior Interaction Capability For GUI Agents

## Quick Facts
- arXiv ID: 2509.17328
- Source URL: https://arxiv.org/abs/2509.17328
- Reference count: 40
- UIPro achieves SOTA performance on multiple GUI interaction benchmarks across mobile, web, and desktop platforms

## Executive Summary
UIPro is a generalist GUI agent that addresses the challenges of limited scenario diversity and heterogeneous action spaces in existing GUI datasets. The authors curate a massive 20.6 million-sample GUI understanding dataset and propose a unified action space for training. UIPro is pre-trained on this dataset to develop strong GUI grounding capabilities, then fine-tuned on merged agent task data with unified action spaces. Experimental results show that UIPro achieves superior performance across multiple GUI interaction benchmarks, outperforming existing methods on mobile and web platforms. Ablation studies confirm the benefits of both the large-scale GUI understanding data and the unified action space in enhancing GUI interaction capabilities.

## Method Summary
UIPro employs a two-stage training approach: first pre-training on a massive 20.6M GUI grounding dataset to establish visual understanding capabilities, then fine-tuning on unified agent task data. The key innovations include systematic denoising of GUI elements to remove noise, a unified action space schema that harmonizes heterogeneous datasets, and a VLM architecture (visual encoder + LLM) that generates JSON-formatted actions. The model is evaluated across multiple benchmarks (AITW, AndroidControl, ScreenSpot, etc.) demonstrating superior performance through enhanced grounding and cross-dataset knowledge transfer.

## Key Results
- UIPro achieves state-of-the-art performance on AITW, AndroidControl, ScreenSpot, and other GUI interaction benchmarks
- Pre-training on 20.6M grounding tasks shows positive correlation with downstream agent performance (Fig. 4)
- Unified action space improves accuracy by enabling cross-dataset knowledge transfer (Fig. 5, Table 7)
- Denoising procedure reduces GUI element noise from up to 29.0% to improve grounding accuracy (Table 8)

## Why This Works (Mechanism)

### Mechanism 1: Grounding-First Pre-Training Establishes Visual Understanding
Large-scale GUI grounding pre-training establishes fundamental visual understanding capabilities that transfer to downstream agent tasks. The model learns to map visual elements to spatial coordinates and textual descriptions through 20.6M grounding tasks (element description, user intent, contextual functionality). This creates robust visual representations that generalize across platforms. When fine-tuned on agent tasks, the model doesn't need to learn basic element localization from scratch, allowing it to focus on action prediction. Evidence shows positive correlation between grounding accuracy and downstream performance (Fig. 4).

### Mechanism 2: Unified Action Space Enables Cross-Dataset Knowledge Transfer
A unified action space schema harmonizes heterogeneous datasets, enabling effective multi-task learning and knowledge transfer across platforms. Different datasets define similar actions inconsistently (e.g., `swipe` vs. `scroll`, different coordinate formats). The unified action space maps these to a canonical schema (e.g., `swipe(start, direction, distance)`). This reduces action definition conflicts, allows the model to learn from more diverse trajectories, and provides regularization benefits. Knowledge from common actions transfers to less common ones. Ablation shows significant performance declines without unification (Fig. 5).

### Mechanism 3: Systematic Denoising Reduces Training Confusion
Removing noisy GUI elements (blank, invisible, oversized, duplicate) improves model grounding accuracy by preventing the model from learning from invalid training samples. Raw GUI data contains high noise levels (up to 29.0% in some sources). The denoising procedure removes: invalid bounding boxes, oversized elements (likely containers), tiny elements (<18px), blank elements (low color std), duplicates, and invisible text. This ensures the model trains on valid, meaningful visual patterns. Table 8 shows denoising contributes notable grounding accuracy gains across all benchmarks.

## Foundational Learning

- **Vision-Language Model (VLM) Architecture**
  - Why needed here: UIPro is built on a VLM architecture (visual encoder + LLM) like LLaVA. Understanding how visual features are projected into the language model's embedding space is essential for comprehending how UIPro processes screenshots and outputs actions.
  - Quick check question: Can you explain how a pre-trained ViT and an LLM are connected in a typical VLM architecture?

- **Transfer Learning & Fine-Tuning**
  - Why needed here: UIPro uses a two-stage training: pre-training on grounding tasks, then fine-tuning on agent tasks. Understanding how representations learned in one stage transfer to another is critical for grasping the paper's methodology.
  - Quick check question: What is the difference between pre-training and fine-tuning, and why might pre-training on a large, general dataset help with downstream specialized tasks?

- **Action Space Design**
  - Why needed here: A core contribution is the unified action space. Understanding how actions are parameterized (e.g., `click(x,y)`, `swipe(start, direction, distance)`) and formatted (JSON) is key to implementing and extending the system.
  - Quick check question: Why would different GUI datasets have different action space definitions, and what problems does a unified schema solve?

## Architecture Onboarding

- **Component map**: Visual Encoder (ViT) -> LLM Backbone (Qwen2-VL/SLiME) -> JSON Action Output (normalized coordinates)

- **Critical path**:
  1. Data Curation: Collect raw GUI data from multiple sources. Apply the denoising procedure.
  2. Pre-training Dataset: Generate 20.6M grounding/referring tasks from cleaned data.
  3. Unified Action Dataset: Convert heterogeneous agent trajectory datasets into unified action format.
  4. Training: Pre-train VLM on grounding data (1 epoch). Fine-tune on unified agent data (6 epochs).
  5. Inference: Given task, history, and screenshot, model predicts JSON action. Execute in environment.

- **Design tradeoffs**:
  - Separate schemas for Mobile/Web/Desktop vs. fully unified schema (generality vs. precision)
  - Large dataset scale vs. noise levels (mitigated by denoising)
  - Single pre-training epoch (compute cost vs. overfitting risk)
  - Excluding action definitions from prompt during fine-tuning (efficiency vs. explicit guidance)

- **Failure signatures**:
  - "Almost hit the target": Predicted click outside but near ground truth bounding box (grounding precision issues)
  - Difficulty with long-tailed actions (drag, hotkey): Low accuracy on rare actions
  - Action Definition Conflicts: Model confuses similar actions without unified schema

- **First 3 experiments**:
  1. Reproduce Grounding Accuracy vs. Pre-training Data Scale: Pre-train with different amounts of grounding data and plot grounding accuracy.
  2. Ablate Unified Action Space: Fine-tune without unifying action spaces and compare Step SR against unified version.
  3. Evaluate Cross-Platform Generalization: Zero-shot evaluate on desktop benchmark after training only on mobile+web data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can evaluation benchmarks be adapted to fairly assess agents that generate alternative, yet valid, action trajectories differing from the single ground truth?
- Basis in paper: Section 4.2.3 states that benchmarks like AITW "often fail to consider alternative solutions, leading to slightly inaccurate evaluations."
- Why unresolved: Current metrics typically rely on exact matches with a single recorded human trajectory, penalizing agents that achieve the same goal through a different valid sequence of steps.
- What evidence would resolve it: Development of benchmark evaluation suite using goal-state verifiers that accept multiple valid action sequences as correct.

### Open Question 2
- Question: What specific data augmentation or architectural modifications are required to improve the prediction accuracy of long-tailed actions, such as drag and hotkey usage?
- Basis in paper: Section 4.2.3 identifies "Difficulty in using long-tailed actions... mainly due to insufficient training data" as a significant error pattern.
- Why unresolved: The current dataset distribution is heavily skewed toward common actions like clicking and typing, leaving complex, infrequent actions under-represented and poorly learned.
- What evidence would resolve it: Experiments demonstrating improved performance on "drag" and "hotkey" tasks after applying targeted data balancing or specialized action heads.

### Open Question 3
- Question: Does the dominance of mobile and web data in the unified action space introduce a bias that limits the sample efficiency of GUI agents in desktop environments?
- Basis in paper: Section 3.2.2 notes that authors "mainly merge datasets in mobile and web scenarios" because desktop data is significantly scarcer.
- Why unresolved: While transfer learning to desktop is shown in appendix, the imbalance raises questions about whether the "unified" space is truly platform-agnostic or optimized primarily for over-represented modalities.
- What evidence would resolve it: Comparative analysis of training dynamics where desktop data is up-sampled to parity with mobile/web data, measuring convergence speed and peak performance.

## Limitations

- Generalizability across drastically different platforms (mobile/web vs. desktop) remains partially validated, with weak cross-platform transfer observed
- Denoising procedure relies on heuristic rules with empirically chosen thresholds that lack theoretical justification
- Two-stage training assumes strong transfer from grounding tasks to agent tasks, but alternative pre-training objectives haven't been explored

## Confidence

- **High confidence**: Correlation between grounding accuracy and downstream performance (Fig. 4), positive impact of denoising on grounding accuracy (Table 8), and superior benchmark performance compared to baselines
- **Medium confidence**: Effectiveness of unified action space demonstrated through ablation studies, but specific design choices and impact on rare actions could benefit from more systematic exploration
- **Low confidence**: Optimal hyperparameters for denoising (threshold values) and pre-training (number of epochs) are not thoroughly explored; claims based on empirical observations rather than systematic ablation studies

## Next Checks

1. **Cross-Platform Transfer Validation**: Design controlled experiment testing UIPro's zero-shot performance on desktop GUI benchmarks after training only on mobile+web data. Measure performance drop to quantify domain gap and identify poorly transferring action types.

2. **Denoising Ablation Study**: Systematically vary denoising thresholds (area ratio, color std, size thresholds) and measure impact on grounding accuracy and downstream agent performance. Identify optimal tradeoff between noise removal and data retention.

3. **Alternative Pre-training Objectives**: Compare grounding pre-training approach against alternative self-supervised objectives like masked element prediction or contrastive learning on the same 20.6M dataset. Measure which objective yields better transfer to agent tasks on AITW benchmark.