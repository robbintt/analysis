---
ver: rpa2
title: 'Evalita-LLM: Benchmarking Large Language Models on Italian'
arxiv_id: '2502.02289'
source_url: https://arxiv.org/abs/2502.02289
tags:
- task
- dataset
- prompt
- tasks
- italian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Evalita-LLM introduces a benchmark for evaluating Large Language
  Models (LLMs) on Italian language tasks, addressing issues of translation quality,
  cultural bias, and prompt sensitivity. The benchmark includes native Italian tasks,
  both multiple-choice and generative, evaluated against multiple prompts to ensure
  fairness.
---

# Evalita-LLM: Benchmarking Large Language Models on Italian

## Quick Facts
- arXiv ID: 2502.02289
- Source URL: https://arxiv.org/abs/2502.02289
- Reference count: 24
- Evalita-LLM introduces a benchmark for evaluating Large Language Models (LLMs) on Italian language tasks, addressing issues of translation quality, cultural bias, and prompt sensitivity

## Executive Summary
Evalita-LLM introduces a comprehensive benchmark for evaluating Large Language Models on Italian language tasks. The benchmark addresses critical issues in existing evaluations, including translation artifacts, cultural bias from Anglo-American sources, and model sensitivity to specific prompts. The development methodology involves iterative validation of tasks and prompts using a set of development LLMs to ensure benchmark difficulty is calibrated appropriately. Results demonstrate that the benchmark is challenging yet solvable for development LLMs, with varying performance across different prompt templates.

## Method Summary
Evalita-LLM uses the lm-evaluation-harness library to evaluate models on 10 native Italian tasks (6 multiple-choice, 4 generative) across 10 different prompt templates. The evaluation employs zero-shot or few-shot (5-shot) settings without fine-tuning. Key metrics include MaxP (maximum performance across prompts), AvgP (average performance), and CPS (Combined Performance Score). The benchmark implements compositional prompting with 6 templates for multiple-choice tasks and 4 templates for generative tasks, derived from task description, request, and output format elements.

## Key Results
- Native Italian tasks reduce cultural and linguistic bias compared to translated benchmarks
- Multi-prompt averaging produces more stable and fair estimates of model capability
- Iterative validation with dev LLMs successfully calibrates benchmark difficulty to be challenging but solvable
- Output formatting requirements for generative tasks significantly impact evaluation reliability

## Why This Works (Mechanism)

### Mechanism 1: Native Italian Tasks Reduce Cultural and Linguistic Bias
Using tasks created natively in Italian mitigates evaluation artifacts from machine translation and Anglo-American cultural assumptions. Native datasets bypass translation errors and embed local cultural context, testing LLMs on material they would encounter in real Italian use.

### Mechanism 2: Multi-Prompt Averaging Increases Evaluation Robustness
Aggregating scores across multiple prompt templates produces a fairer, more stable estimate of model capability than single-prompt evaluation. Since LLMs show sensitivity to prompt wording, evaluating with several templates reduces variance from prompt selection luck.

### Mechanism 3: Iterative Validation with Dev LLMs Calibrates Benchmark Difficulty
Testing candidate tasks and prompts against development LLMs helps ensure benchmark tasks are solvable yet challenging. By selecting tasks where dev LLMs perform above baseline but below ceiling, the benchmark aims to remain discriminative for future models.

## Foundational Learning

- **Concept: Benchmarking as Distribution Testing**
  - Why needed here: Evaluating LLMs requires defining a distribution of tasks the model should solve. Evalita-LLM explicitly selects for native Italian distribution.
  - Quick check question: Why is evaluating on translated benchmarks considered problematic for a model trained primarily on Italian data?

- **Concept: Prompt Sensitivity and Template Variance**
  - Why needed here: The paper's core design is predicated on the fact that LLM scores vary with prompt phrasing; understanding this variance is key to interpreting multi-prompt metrics.
  - Quick check question: Given the results in Table 14, why might a single-prompt leaderboard be misleading for comparing LLM-1 and LLM-5?

- **Concept: Generative vs. Multiple-Choice Evaluation Paradigms**
  - Why needed here: Evalita-LLM includes both. Each has trade-offs: multiple-choice uses log-probabilities (simple, but can be gamed), generative requires output parsing and metrics like ROUGE (harder to normalize).
  - Quick check question: What is the role of "stop tokens" in generative tasks, and why are they critical for evaluation reliability?

## Architecture Onboarding

- **Component map:** Source Datasets (10 native Italian tasks) -> Prompt Templates (10 templates) -> Evaluation Harness (lm-evaluation-harness) -> Metrics (Accuracy, F1, ROUGE, MaxP, AvgP, CPS)
- **Critical path:** (1) Select model and task; (2) Run inference across all prompt templates; (3) Aggregate per-prompt scores into final metric (CPS)
- **Design tradeoffs:**
  - Multiple-choice vs. Generative: Multiple-choice is computationally cheaper but may not reflect real-world use; generative is more natural but computationally expensive
  - Prompt quantity vs. compute: Using 6 prompts per MC task provides robustness but increases runtime 6x
  - Native vs. Translated: Native tasks ensure cultural relevance but limit available datasets
- **Failure signatures:**
  - Prompt leakage/saturation: Near-perfect scores on specific prompt indicate contamination
  - Format failure in generative tasks: Model outputs that cannot be parsed score zero regardless of correctness
  - Metric instability: High variance across prompt templates suggests prompt sensitivity
- **First 3 experiments:**
  1. Baseline Run: Run llm-evaluation-harness with base Italian LLM on simple task to understand output format and metric calculation
  2. Prompt Variance Analysis: Run all prompt templates for chosen task and calculate standard deviation of scores
  3. Zero-shot vs. Few-shot Comparison: Run task in both zero-shot and few-shot modes to see performance delta

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset composition uncertainty: Exact distribution of domains and their relative difficulty is not reported
- Prompt sensitivity characterization incomplete: Does not quantify statistical significance of performance differences
- Validation methodology opacity: Specific dev models used for validation are not identified

## Confidence
- **High confidence:** The benchmark successfully implements native Italian tasks evaluated with multiple prompts
- **Medium confidence:** The claim that native tasks reduce cultural bias is logically sound but not empirically validated
- **Low confidence:** The assertion that the benchmark is "challenging yet solvable" is based on internal validation without public reproducibility data

## Next Checks
1. Reproduce prompt sensitivity analysis: Run subset of tasks across all prompt templates using public Italian LLM and calculate standard deviation of scores
2. Parse failure rate measurement: Instrument evaluation harness to log parsing success/failure rates separately from content accuracy
3. Cross-lingual bias comparison: Implement parallel evaluation using translated tasks and compare model rankings and absolute scores against native Italian versions