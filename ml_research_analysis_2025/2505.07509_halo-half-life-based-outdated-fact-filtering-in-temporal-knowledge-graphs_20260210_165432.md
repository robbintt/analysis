---
ver: rpa2
title: 'HALO: Half Life-Based Outdated Fact Filtering in Temporal Knowledge Graphs'
arxiv_id: '2505.07509'
source_url: https://arxiv.org/abs/2505.07509
tags:
- facts
- fact
- outdated
- temporal
- halo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes HALO, a framework that filters outdated facts
  in temporal knowledge graphs (TKGs) to improve reasoning performance. The method
  uses half-life theory to quantify the temporal validity of facts: facts whose validity
  falls below a threshold are considered outdated and removed.'
---

# HALO: Half Life-Based Outdated Fact Filtering in Temporal Knowledge Graphs

## Quick Facts
- arXiv ID: 2505.07509
- Source URL: https://arxiv.org/abs/2505.07509
- Reference count: 9
- Primary result: HALO improves state-of-the-art TKG reasoning methods with 0.29%-3.86% gains in MRR and Hits@1

## Executive Summary
This paper introduces HALO, a framework that filters outdated facts in temporal knowledge graphs using half-life theory to improve reasoning performance. The method identifies facts whose temporal validity falls below a threshold and removes them before reasoning. HALO consists of three modules: a temporal fact attention module that captures fact evolution over time, a dynamic relation-aware encoder that predicts half-life for each fact, and an outdated fact filtering module that applies exponential decay to identify and remove outdated facts. Experiments on ICEWS datasets demonstrate consistent improvements across multiple state-of-the-art TKG reasoning methods.

## Method Summary
HALO operates by first encoding temporal information between current time and fact timestamps using multi-head attention with periodic activation functions. It then constructs a facts-to-nodes graph where nodes share relations, applies GCN layers for message passing, and classifies facts as active or inactive. The half-life for each category is computed as half the average update interval. Finally, a time decay function with exponential form is applied to calculate temporal validity, and facts below a threshold θ are filtered out. The framework is designed as preprocessing for downstream TKG reasoning tasks.

## Key Results
- MRR improvements range from 0.29% to 3.86% across all tested datasets and methods
- HALO consistently improves performance on ICEWS14, ICEWS18, and ICEWS05-15 datasets
- Optimal half-life varies by dataset: ~80 days for ICEWS14 (more active) vs. ~160 days for ICEWS18 (more inactive)
- Lower threshold values generally yield better performance, indicating more aggressive filtering is beneficial

## Why This Works (Mechanism)

### Mechanism 1: Temporal Fact Attention for Evolution Capture
Attention-based encoding captures how facts evolve over time, enabling identification of relevant facts vs. noise. The module encodes time intervals using periodic activation function φ(d) = cos(dω_t + b_t), then computes attention scores via softmax over learned fact embeddings. Multi-head attention aggregates information from neighboring entities across timestamps. Core assumption: Facts with similar temporal patterns share relevance; periodic time encoding captures cyclical dependencies better than linear encoding.

### Mechanism 2: Relation-Aware Half-Life Prediction via Activity Classification
Grouping facts by relation and classifying them as "active" or "inactive" enables per-fact half-life prediction. A facts-to-nodes graph G_F2N is constructed where nodes share relations; GCN layers propagate information to produce embeddings. A classifier labels facts, and half-life is computed as mean update interval Δt for each category. Active facts receive shorter half-lives (faster decay). Core assumption: Facts sharing the same relation exhibit similar update patterns; historical frequency predicts future validity decay rate.

### Mechanism 3: Exponential Decay Filtering Based on Half-Life Theory
A fact's temporal validity decays exponentially over time; filtering facts below threshold θ removes outdated information. Temporal validity V(t_i, t_H^F) = V_0 · e^(-λ·(t_c - t_i)), where λ = ln(2)/t_H^F is derived from half-life. When V falls below threshold θ, the fact is marked outdated and removed. Core assumption: Decay follows exponential model; validity reaching half-life corresponds to 50% of initial influence (V_0 = 1).

## Foundational Learning

- **Concept: Temporal Knowledge Graphs (TKGs)**
  - Why needed here: HALO operates on quadruples (subject, relation, object, timestamp); understanding temporal dependencies is essential for validity reasoning
  - Quick check question: Can you explain why a TKG differs from a static KG and why timestamping matters for reasoning?

- **Concept: Half-Life Theory**
  - Why needed here: Core mathematical model for decay; understanding exponential decay is critical for interpreting λ and threshold selection
  - Quick check question: If a fact has half-life 100 days, what is its validity after 200 days?

- **Concept: Graph Neural Networks (GCNs)**
  - Why needed here: The dynamic relation-aware encoder uses multi-layer GCNs for message passing; understanding D̂^(-1/2) Â D̂^(-1/2) normalization is required
  - Quick check question: What does adding a self-loop (I_N) to the adjacency matrix achieve in GCN propagation?

## Architecture Onboarding

- **Component map:** Input TKG → Temporal Fact Attention Module → Fact embeddings F with temporal encoding → Dynamic Relation-Aware Encoder → GCN on facts-to-nodes graph → Activity classification → Half-Life Prediction → Compute t_H^Fact per category → Outdated Fact Filtering → Apply decay function → Filter if V < θ → Output: Filtered TKG for downstream reasoning

- **Critical path:** The half-life prediction depends on correct activity classification; if classifier error is high, decay rates will be misassigned, leading to under- or over-filtering

- **Design tradeoffs:**
  - Lower θ → more aggressive filtering, less training cost, but risk of removing still-valid facts
  - Higher number of GCN layers → better relation context, but potential over-smoothing
  - Assumption: Active facts share half-life; inactive facts share another—this reduces parameters but may oversimplify heterogeneous update patterns

- **Failure signatures:**
  - Performance degrades as θ → 0 (over-filtering removes useful signal)
  - MRR/Hits@1 plateau despite tuning t_H^F (half-life prediction may be misaligned with ground truth update patterns)
  - Attention weights uniform across timestamps (temporal encoding may not capture relevant patterns)

- **First 3 experiments:**
  1. Baseline sanity check: Run HALO on ICEWS14 with θ = 0.5 and default t_H^F; verify MRR improvement over unfiltered baseline per Table 1
  2. Threshold sensitivity sweep: Vary θ from 0.1 to 0.9 on ICEWS18; plot MRR vs. θ to find optimal operating point (expect peak at moderate θ per Figure 3)
  3. Half-life ablation: Randomly shuffle t_H^F values across facts; compare MRR degradation vs. learned t_H^F to validate half-life prediction importance

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the HALO framework be adapted to handle multi-modal knowledge graphs, and does the definition of "half-life" change when applied to data sources like text or images?
  - Basis in paper: The authors state in the Conclusion, "In future work, we will focus on expanding the framework to handle multi-modal KGs, incorporating data from different sources like text, images, or sensor data."
  - Why unresolved: The current implementation is designed solely for structured quadruples (s, r, o, t) and does not account for the semantic complexity or differing decay rates of unstructured multi-modal data.
  - What evidence would resolve it: A modified version of HALO applied to a multi-modal TKG benchmark, demonstrating how temporal validity is calculated for non-relational data.

- **Open Question 2:** Does the binary classification of facts into "active" and "inactive" groups with shared half-lives (Eq. 8-9) limit accuracy compared to learning fine-grained, instance-specific half-lives?
  - Basis in paper: The methodology assigns a single average half-life to entire groups of facts (t_H^Fact for active, t_H^Ina for inactive) rather than predicting a unique half-life for every distinct fact, which assumes homogeneous temporal behavior within these groups.
  - Why unresolved: Averaging half-lives may misrepresent the true validity of outlier facts (e.g., an "active" fact that remains relevant much longer than the group average), potentially leading to premature filtering or retention errors.
  - What evidence would resolve it: An ablation study comparing the current group-based half-life prediction against a regression model that predicts a specific half-life for each individual fact.

- **Open Question 3:** How does the assumption of an exponential decay function affect performance on temporal knowledge graphs characterized by cyclic events or sudden state changes rather than gradual obsolescence?
  - Basis in paper: The paper utilizes a strict exponential decay function V(t_i, t_H) = V_0 · e^(-λ···) (Eq. 10) derived from half-life theory. This assumes validity decays smoothly and continuously, which may not align with cyclic relations (e.g., seasonal events) or facts with binary validity (e.g., a position held until a specific termination date).
  - Why unresolved: An exponential function cannot model the "resurrection" of validity in cyclic patterns or the sharp drop-offs in discontinuous events, potentially filtering out facts that become relevant again.
  - What evidence would resolve it: Performance analysis of HALO on datasets specifically rich in cyclic/repeating events to see if the exponential filter erroneously degrades recall for recurring facts.

## Limitations
- Missing implementation details: hyperparameters (θ, t_H^F, GCN layers, attention heads, learning rates), embedding dimensions, and training procedures
- Limited ablation studies: No analysis of individual module contributions or performance without HALO filtering
- Assumption sensitivity: Half-life prediction relies on stable update intervals within relations, which may not hold for all TKG domains

## Confidence
- High confidence: Experimental setup and dataset usage are clearly specified; core mathematical formulations (Equations 1-10) are reproducible given code access
- Medium confidence: Mechanisms for temporal fact attention and GCN-based half-life prediction are theoretically sound but lack direct validation of individual components
- Low confidence: Claims about exponential decay model applicability to all TKG facts and optimal threshold values are not empirically justified

## Next Checks
1. Hyperparameter sensitivity analysis: Systematically vary θ, GCN layers, and attention heads to identify optimal configurations and robustness ranges
2. Individual module ablation: Test reasoning performance with only temporal attention, only GCN-based classification, and only decay filtering to isolate contributions
3. Domain generalization test: Apply HALO to a non-ICEWS TKG dataset (e.g., GDELT) to evaluate whether exponential decay assumptions hold across domains