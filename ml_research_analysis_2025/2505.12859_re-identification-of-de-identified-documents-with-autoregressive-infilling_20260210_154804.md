---
ver: rpa2
title: Re-identification of De-identified Documents with Autoregressive Infilling
arxiv_id: '2505.12859'
source_url: https://arxiv.org/abs/2505.12859
tags:
- text
- knowledge
- background
- infilling
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a retrieval-augmented method for re-identifying
  text documents that have been de-identified by masking personally identifiable information
  (PII). The approach uses a three-step pipeline: first, a sparse retriever finds
  relevant background documents; then, a dense retriever extracts the most useful
  passages for each masked span; finally, an LLM-based infiller generates re-identification
  candidates.'
---

# Re-identification of De-identified Documents with Autoregressive Infilling

## Quick Facts
- arXiv ID: 2505.12859
- Source URL: https://arxiv.org/abs/2505.12859
- Reference count: 13
- Primary result: Up to 80% re-identification accuracy for masked spans using retrieval-augmented LLM infilling

## Executive Summary
This paper presents a retrieval-augmented method for re-identifying text documents that have been de-identified by masking personally identifiable information (PII). The approach uses a three-step pipeline: first, a sparse retriever finds relevant background documents; then, a dense retriever extracts the most useful passages for each masked span; finally, an LLM-based infiller generates re-identification candidates. The method is evaluated on three datasets (Wikipedia biographies, court rulings, and clinical notes) across four levels of background knowledge. Results show that up to 80% of de-identified text spans can be successfully recovered, with accuracy increasing along with the level of background knowledge. The approach can be applied using either a fine-tuned GLM or an instruction-tuned Mistral model without domain-specific fine-tuning.

## Method Summary
The method employs a three-step pipeline to re-identify masked PII spans in de-identified documents. First, a sparse retriever (BMX) retrieves the top 100 documents from background knowledge using lexical/semantic similarity. Second, a dense retriever (fine-tuned ColBERT) ranks passages within those documents specifically for each masked span, using positive/negative training pairs where positives contain the original span content. Third, an infilling model (either fine-tuned GLM-RoBERTa-Large or zero-shot Mistral-Nemo-Instruct-2407) generates re-identification candidates using the top retrieved passages. The system is evaluated across four knowledge levels: L1 (no retrieval), L2 (general knowledge), L3 (all texts except target), and L4 (all texts including target). Results show progressive improvement in re-identification accuracy as background knowledge becomes more specific, with exact match rates reaching 80% on Wikipedia biographies at L4.

## Key Results
- Re-identification accuracy increases monotonically with background knowledge specificity (L1→L4)
- Fine-tuned GLM achieves 80.08% exact match on Wikipedia at L4, Mistral achieves 37.34% without fine-tuning
- Sparse retrieval alone achieves 51-65% span recall across datasets at L2-L3 levels
- Clinical notes show highest re-identification rates (91.1% exact match at L4) due to structured format

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A two-stage retrieval pipeline (sparse then dense) substantially increases the likelihood that relevant context for a masked span reaches the infilling model.
- Mechanism: Sparse retrieval (BMX, a BM25 variant) first narrows a large background corpus to ~100 documents using lexical/semantic similarity. Dense retrieval (fine-tuned ColBERT) then ranks passages within those documents specifically for each masked span, using positive/negative training pairs where positives contain the original span content. This hierarchical filtering balances recall (finding any relevant document) with precision (locating the exact passage needed).
- Core assumption: The background knowledge contains at least one passage mentioning the masked information in a retrievable form.
- Evidence anchors:
  - [abstract] "A retriever first selects from the background knowledge passages deemed relevant... Those passages are then provided to an infilling model"
  - [Section 3.1-3.2] Details sparse retriever retrieving N=100 documents, dense retriever using ColBERT with positive pairs defined as passages containing original span content
  - [corpus] Related work "Protecting De-identified Documents from Search-based Linkage Attacks" confirms linkage risks from phrase extraction, supporting retrieval-based attack viability
- Break condition: If sparse retrieval misses all documents containing the span (Table 1 shows 51-65% for general knowledge), dense retrieval cannot recover.

### Mechanism 2
- Claim: Re-identification accuracy scales monotonically with background knowledge specificity, with a sharp discontinuity when the original document is included.
- Mechanism: Four knowledge levels were tested (L1: no retrieval, L2: general knowledge, L3: all texts except target, L4: all texts including target). Performance jumps dramatically at L4 (exact match: ~80% for Wikipedia, ~66% for TAB, ~91% for clinical notes) because the infilling model receives passages containing the exact masked string. L2-L3 gains are modest but consistent, suggesting quasi-identifiers are inferable from related contexts.
- Core assumption: Adversaries have access to background knowledge at one of these levels.
- Evidence anchors:
  - [abstract] "re-identification accuracy increases along with the level of background knowledge"
  - [Table 3-5] Shows progression from L1 (6.26% GLM exact match on Wikipedia) to L4 (80.08%)
  - [corpus] "Stronger Re-identification Attacks through Reasoning and Aggregation" corroborates that aggregation of multiple signals improves re-identification
- Break condition: If background knowledge is genuinely disjoint from the target individual, re-identification drops to near-random for direct identifiers.

### Mechanism 3
- Claim: Instruction-tuned LLMs can perform infilling without domain-specific fine-tuning, achieving competitive token recall though lower exact match.
- Mechanism: Mistral-Nemo-Instruct-2407 (12B) was tested zero-shot with top-10 retrieved passages. It achieved higher token recall than GLM at L1-L3 (e.g., 47.43% vs 21.35% on TAB at L2) but lower exact match at L4 (37.34% vs 66.04%), suggesting it produces reformulations rather than verbatim recovery. Fine-tuned GLM (335M) exploits retrieved passages more precisely for exact string recovery.
- Core assumption: The infilling model can attend to retrieved passages and distinguish relevant context from noise.
- Evidence anchors:
  - [Section 3.3] Describes both GLM with fine-tuning and Mistral without domain-specific fine-tuning
  - [Table 4] Shows Mistral token recall exceeding GLM at lower knowledge levels but degrading at L4
  - [corpus] Evidence weak; related papers focus on de-identification rather than infilling architectures
- Break condition: When retrieved passages are structurally similar but substantively irrelevant (e.g., clinical notes for different patients with similar formats), Mistral's attention may misallocate.

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**
  - Why needed here: The entire architecture is RAG-inspired; understanding retrieval-then-generation patterns is prerequisite to grasping why this works.
  - Quick check question: Can you explain why RAG separates knowledge storage (retrieval index) from reasoning (LLM)?

- **Text Infilling / Fill-in-the-Middle**
  - Why needed here: The core task is predicting masked spans autoregressively; distinguish from masked language modeling (single token) vs. span infilling (variable length).
  - Quick check question: How does GLM's autoregressive blank infilling differ from BERT's masked token prediction?

- **ColBERT Late Interaction**
  - Why needed here: Dense retrieval uses ColBERT's contextualized late interaction; understanding token-level embeddings and MaxSim operations is necessary to debug retrieval quality.
  - Quick check question: Why might ColBERT outperform bi-encoder retrieval for finding passages mentioning specific entity names?

## Architecture Onboarding

- **Component map**: Sparse Retriever (BMX) -> Document Chunker -> Dense Retriever (ColBERT) -> Infilling Model (GLM/Mistral) -> (Optional) Ranking Model
- **Critical path**: Sparse retrieval recall → Dense retrieval precision → Infilling exact match → (Optional) Identity ranking. Failure at any stage propagates; sparse retrieval is the gating factor.
- **Design tradeoffs**:
  - GLM fine-tuned vs. Mistral zero-shot: GLM gives higher exact match at L4; Mistral gives higher token recall at L1-L3 without training data privacy risks
  - 1 vs. 2 retrieved passages for GLM: +1-2% exact match for ~40% longer sequences
  - Chunk size (600 chars): Balances context completeness vs. retrieval granularity
- **Failure signatures**:
  - Low sparse retrieval recall (<60% spans found in top-100): Background knowledge insufficient or query formulation weak
  - High token recall but low exact match: Model reformulating rather than copying (Mistral signature)
  - Near-perfect L4 but near-zero L2-L3: System memorizing rather than inferencing
- **First 3 experiments**:
  1. **Sparse retrieval ablation**: Measure span recall at top-10, top-50, top-100 documents to calibrate N parameter for your corpus size
  2. **Dense retriever positive/negative ratio**: Test ColBERT fine-tuning with varying hard negative sampling rates; current setup uses all non-containing passages as negatives
  3. **Infilling model comparison**: Run both GLM (fine-tuned) and Mistral (zero-shot) on a held-out validation set to determine which suits your privacy constraints (fine-tuning may leak data vs. zero-shot lower performance)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the dense retriever's fine-tuning be optimized to distinguish between contexts that merely contain a string match and those that provide semantically useful information for infilling?
- Basis in paper: [explicit] The Conclusion states the retriever "has a number of shortcomings such as retrieving texts containing the correct span but in irrelevant contexts" and suggests "Improving the fine-tuning of this retriever could lead to better downstream results."
- Why unresolved: Current training defines positive pairs simply by the presence of the original span string, ignoring the semantic utility of the surrounding context.
- What evidence would resolve it: A comparison of re-identification accuracy between the current ColBERT model and a version fine-tuned with a context-aware semantic relevance objective.

### Open Question 2
- Question: To what extent does incorporating structured data sources (tables, knowledge graphs) into the background knowledge improve re-identification accuracy compared to text-only retrieval?
- Basis in paper: [explicit] The Conclusion proposes that "Extending the background knowledge with other types of information (such as information derived from structured databases) could also enhance the re-identification performance."
- Why unresolved: The current approach relies exclusively on unstructured text documents for the sparse and dense retrieval steps.
- What evidence would resolve it: Evaluation metrics (Exact Match/Token Recall) from experiments where structured database entries are added to the background knowledge pool.

### Open Question 3
- Question: Can In-Context Learning (ICL) with larger instruction-tuned models significantly outperform the fine-tuned GLM, particularly in low-knowledge scenarios (L1-L2)?
- Basis in paper: [inferred] The Limitations section notes the fine-tuned model is "relatively small (335M)" and suggests "Using In-context Learning or fine-tuning larger models could result in better re-identification."
- Why unresolved: The paper tested Mistral in a zero-shot manner but did not evaluate ICL or fine-tuning for larger models, leaving the potential performance gain unknown.
- What evidence would resolve it: Ablation studies comparing the small fine-tuned GLM against large LLMs using ICL on the Wikipedia and TAB datasets.

### Open Question 4
- Question: How much of the reported re-identification success is attributable to data memorization from the model's pre-training rather than the retrieval mechanism?
- Basis in paper: [explicit] The Limitations section acknowledges a "possibility that some of the data has been leaked to the infilling model during the pre-training... thereby inflating the re-identification performance."
- Why unresolved: Datasets like Wikipedia and court rulings are public and likely present in the pre-training corpora of standard LLMs, making it hard to isolate the RAG contribution.
- What evidence would resolve it: Experiments using strictly synthetic or temporally held-out documents guaranteed to be absent from the models' pre-training data.

## Limitations
- BMX algorithm implementation details not provided, limiting exact reproduction
- Performance heavily dependent on background knowledge quality and coverage
- Clinical note results based on synthetic data which may not capture real-world complexity
- Method assumes complete de-identification annotations, not realistic attack scenarios

## Confidence

- **High Confidence**: The monotonic relationship between background knowledge specificity and re-identification accuracy (L1→L4) is strongly supported by consistent experimental results across all three datasets. The architectural design (sparse→dense→infilling) is well-specified and reproducible.
- **Medium Confidence**: The zero-shot performance of Mistral-Nemo-12B is credible but requires careful interpretation—high token recall with low exact match suggests the model reformulates rather than retrieves exact strings, which may be acceptable for some use cases but not others. The computational efficiency claims (30min vs 1.5h inference) are specific but hardware-dependent.
- **Low Confidence**: The clinical note results depend entirely on synthetic data quality; real-world performance may differ substantially. The method's generalization to languages other than English remains untested despite ColBERT's multilingual capabilities.

## Next Checks

1. **Background Knowledge Coverage Test**: Systematically measure sparse retrieval recall across varying document corpus sizes (1K, 10K, 100K, 1M documents) to identify the inflection point where retrieval performance plateaus, establishing minimum viable background knowledge requirements.

2. **Real-World Clinical Data Validation**: Replace Synthea-generated clinical notes with a small set of real de-identified EHR notes from a different healthcare system to test whether the L2-L3 performance patterns hold when document structure and terminology vary substantially from training conditions.

3. **Adversarial Boundary Detection Evaluation**: Design an experiment where the model must first identify PII spans before infilling them, measuring the compound failure rate when both detection and re-identification steps are required, to better simulate realistic attack scenarios.