---
ver: rpa2
title: 3D Gaussian and Diffusion-Based Gaze Redirection
arxiv_id: '2511.11231'
source_url: https://arxiv.org/abs/2511.11231
tags:
- gaze
- redirection
- head
- pose
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DiT-Gaze advances 3D gaze redirection by replacing the U-Net renderer
  in GazeGaussian with a Latent Diffusion Transformer, adding an orthogonality constraint
  loss for feature disentanglement, and introducing a weak supervision strategy that
  interpolates intermediate gaze angles. This combination enables higher-fidelity
  synthesis of subtle gaze shifts and better preservation of identity and head pose.
---

# 3D Gaussian and Diffusion-Based Gaze Redirection

## Quick Facts
- **arXiv ID:** 2511.11231
- **Source URL:** https://arxiv.org/abs/2511.11231
- **Reference count:** 32
- **Primary result:** Reduces gaze error to 6.353° (4.1% improvement) with SOTA perceptual quality

## Executive Summary
DiT-Gaze advances 3D gaze redirection by replacing the U-Net renderer in GazeGaussian with a Latent Diffusion Transformer (DiT), adding an orthogonality constraint loss for feature disentanglement, and introducing a weak supervision strategy that interpolates intermediate gaze angles. This combination enables higher-fidelity synthesis of subtle gaze shifts and better preservation of identity and head pose. Evaluated on ETH-XGaze and cross-dataset benchmarks (ColumbiaGaze, MPIIFaceGaze, GazeCapture), DiT-Gaze achieves state-of-the-art results, reducing gaze error to 6.353° (4.1% improvement) and improving perceptual quality with gains in PSNR, LPIPS, and Identity Similarity. Ablation studies confirm each component's contribution. The approach provides a superior method for generating synthetic training data to improve gaze estimator robustness.

## Method Summary
DiT-Gaze extends the GazeGaussian framework by replacing its U-Net renderer with a Latent Diffusion Transformer (DiT) that uses self-attention to capture long-range dependencies. The method introduces an orthogonality constraint loss that enforces disentanglement between gaze, pose, and expression representations by minimizing their cosine similarity. A weak supervision strategy generates intermediate gaze angles through mixed sampling (grid + random) with progressive training from central to extreme gazes. The model uses 3D Gaussian splatting for geometric representation, rasterizing two streams (face deformation and eye rotation) into high-dimensional feature maps. These are encoded into latent space, processed by the DiT with AdaLN conditioning, and decoded to synthesize the redirected gaze image. Training combines identity, gaze, and orthogonality losses with the intermediate gaze sampler.

## Key Results
- Achieves state-of-the-art gaze error of 6.353° on ETH-XGaze (4.1% improvement over baseline)
- Improves perceptual quality with significant gains in LPIPS, PSNR, and Identity Similarity metrics
- Demonstrates strong cross-dataset generalization on ColumbiaGaze, MPIIFaceGaze, and GazeCapture
- Ablation studies confirm DiT renderer (13.4% LPIPS drop) and orthogonality constraint contribute significantly

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing U-Net with Latent Diffusion Transformer improves perceptual fidelity and global coherence
- **Mechanism:** DiT uses self-attention over latent patches rather than localized convolutions, capturing long-range dependencies via AdaLN conditioning
- **Core assumption:** Transformer scaling properties transfer from general image synthesis to facial gaze manipulation
- **Evidence:** Abstract states "DiT allows higher-fidelity image synthesis"; section 3.3 explains long-range dependency modeling; corpus confirms DiT trend in robotics
- **Break condition:** Input feature map incoherence or VAE compression artifacts may cause hallucination

### Mechanism 2
- **Claim:** Orthogonality Constraint Loss improves identity preservation by decoupling control features
- **Mechanism:** Minimizes cosine similarity between gaze, pose, and expression latent vectors, forcing orthogonal subspaces
- **Core assumption:** Baseline model has correlated attribute representations causing identity drift
- **Evidence:** Abstract mentions "mathematically enforces disentanglement"; section 3.5 details orthogonality loss; corpus lacks domain-specific validation
- **Break condition:** Excessive weighting restricts natural correlations, causing unnatural rigidity

### Mechanism 3
- **Claim:** Weak supervision via intermediate gaze sampling creates smooth gaze manifold
- **Mechanism:** Synthesizes intermediate gaze vectors using mixed sampling with progressive training schedule
- **Core assumption:** Dataset lacks density for intermediate states without synthetic interpolation
- **Evidence:** Abstract states "smooth manifold"; section 3.4 explains continuous manifold teaching; corpus shows limited specific validation
- **Break condition:** Synthesized angles outside valid rotation range produce degenerate feature maps

## Foundational Learning

- **Concept: 3D Gaussian Splatting (3DGS)**
  - **Why needed:** Underlying geometric representation for diagnosing rendering failures
  - **Quick check:** Can you explain how "Eye Rotation Field" modifies Gaussians differently than "Face Deformation Field"?

- **Concept: Latent Diffusion Models (LDM)**
  - **Why needed:** DiT operates in compressed latent space; understanding VAE compression is key for debugging
  - **Quick check:** Why does DiT process "patches" of latent codes rather than full image, and how does VAE decoder finalize image?

- **Concept: Feature Disentanglement**
  - **Why needed:** Core improvement in identity preservation comes from forcing gaze, pose, and expression independence
  - **Quick check:** If $v_{gaze}$ and $v_{expr}$ were perfectly orthogonal, what would cosine similarity be, and how does that affect gradients?

## Architecture Onboarding

- **Component map:** Input -> Geometry Stage (Face Stream + Eye Stream) -> Orthogonality Module -> Rendering Stage (Rasterizer + DiT Renderer -> Output
- **Critical path:** Intermediate Gaze Sampler -> Eye Rotation Field -> DiT Renderer is most fragile
- **Design tradeoffs:** Fidelity vs. Pose Stability (0.22° regression noted); Speed vs. Quality (DiT computationally heavier)
- **Failure signatures:** Blur/Artifacts (U-Net baseline behavior); Identity Drift (Orthogonality Loss too weak); Rigid/Dull Eyes (sampler range too narrow)
- **First 3 experiments:**
  1. Ablate Renderer: Swap DiT for U-Net to isolate 13.4% LPIPS improvement
  2. Visualize Orthogonality: Plot $v_{gaze}$ vs. $v_{pose}$ embeddings with/without $L_{ortho}$
  3. Stress Test Interpolation: Input continuous gaze sweep to check for jitter/discontinuities

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can model distillation or quantization techniques successfully compress DiT-Gaze for real-time inference (>25 FPS) without compromising gaze accuracy?
- **Basis:** Authors explicitly state in "Future Work" that exploring model distillation is necessary for lightweight, real-time version
- **Why unresolved:** DiT introduces significant computational overhead compared to U-Net baseline
- **What evidence would resolve it:** Benchmarking distilled/quantized variant showing frame rates comparable to GazeGaussian while maintaining Gaze Error below 6.4°

### Open Question 2
- **Question:** Can trade-off between gaze accuracy and head pose stability be resolved through modified regularization?
- **Basis:** Conclusion notes "minor, 0.22° regression in head pose stability" as specific avenue for future research
- **Why unresolved:** Multi-objective training improves gaze accuracy but creates optimization landscape that slightly destabilizes head pose
- **What evidence would resolve it:** Ablation study showing configuration where Head Pose Error decreases/stays stable while Gaze Error improvement is retained

### Open Question 3
- **Question:** Does pre-trained VAE encoder impose upper bound on fidelity of fine-grained eye textures (e.g., sclera vascularity) compared to pixel-space rendering?
- **Basis:** Method operates in compressed latent space; while results are SOTA, VAE compression could theoretically limit reconstruction of high-frequency biological details
- **Why unresolved:** Standard metrics may not capture loss of semantic biological detail caused by latent compression
- **What evidence would resolve it:** Comparison of high-resolution eye-region crops between latent renderer and pixel-space renderer measuring reconstruction error of fine textural details

## Limitations
- Orthogonality Constraint Loss mechanism is novel with limited external validation
- Reports 0.22° increase in head pose error as trade-off for improved gaze accuracy
- Significant computational cost requires A100-class hardware for deployment

## Confidence

- **High:** Claims about improved perceptual quality (LPIPS, PSNR, ID Similarity) and gaze error reduction directly supported by quantitative results
- **Medium:** Architectural benefits of DiT over U-Net well-supported by literature but domain-specific performance depends on implementation details
- **Low:** Orthogonality Constraint Loss mechanism lacks external validation and optimal weighting parameters not disclosed

## Next Checks

1. **Ablation Validation:** Replace DiT with baseline U-Net renderer to verify claimed 13.4% LPIPS improvement
2. **Orthogonality Verification:** Visualize and measure cosine similarity between gaze, pose, and expression latent vectors with and without orthogonality constraint
3. **Cross-Dataset Generalization:** Test model on additional datasets beyond ColumbiaGaze, MPIIFaceGaze, and GazeCapture to verify robustness of SOTA claims