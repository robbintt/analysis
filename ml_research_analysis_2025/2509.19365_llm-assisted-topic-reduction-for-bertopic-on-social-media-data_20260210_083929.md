---
ver: rpa2
title: LLM-Assisted Topic Reduction for BERTopic on Social Media Data
arxiv_id: '2509.19365'
source_url: https://arxiv.org/abs/2509.19365
tags:
- topic
- topics
- bertopic
- reduction
- tweets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses excessive topic redundancy in BERTopic when
  applied to social media data. It proposes combining BERTopic with LLM-assisted topic
  reduction, where semantically similar topics are iteratively merged using LLM prompts.
---

# LLM-Assisted Topic Reduction for BERTopic on Social Media Data

## Quick Facts
- **arXiv ID:** 2509.19365
- **Source URL:** https://arxiv.org/abs/2509.19365
- **Reference count:** 25
- **Primary result:** LLM-assisted topic reduction improves topic diversity and coherence in BERTopic on noisy social media data compared to baseline HDBSCAN tuning.

## Executive Summary
This paper addresses the problem of excessive topic redundancy in BERTopic when applied to social media data, where HDBSCAN clustering often produces many semantically overlapping topics. The proposed solution combines BERTopic with an LLM-assisted topic reduction framework that iteratively merges semantically similar topics based on their keyword representations. Experiments on three Twitter datasets and four different LLMs show that this approach outperforms the baseline of simply increasing HDBSCAN's minimum cluster size in enhancing topic diversity, and in many cases improves coherence as well. The method is computationally more efficient than end-to-end LLM topic modeling while maintaining semantic quality.

## Method Summary
The approach works by first running BERTopic with standard parameters to generate an initial set of topics, then using an LLM to iteratively identify and merge the two most semantically similar topics until a target count is reached. Topic representations are extracted using c-TF-IDF (top 10 keywords) and fed to the LLM via prompt to determine similarity. The merged topics are updated by recalculating their c-TF-IDF representation from combined documents. This hybrid approach leverages BERTopic's efficient clustering while using LLM's semantic understanding for refinement, tested across different LLMs including GPT-4o-mini, Llama3-8b, Gemma3-12b, and Qwen3-30b-a3b.

## Key Results
- LLM-assisted reduction consistently improved topic diversity (ratio of unique words) compared to baseline HDBSCAN tuning across all three Twitter datasets.
- Coherence (measured by NPMI) improved in most cases, though on the Cyberbullying dataset diversity increased while coherence decreased, indicating dataset sensitivity.
- The method achieved better computational efficiency than end-to-end LLM topic modeling by limiting LLM usage to semantic refinement rather than full document processing.
- Different LLMs showed varying performance, with GPT-4o-mini demonstrating robustness across datasets while some open models (Llama-3) failed on sensitive content due to safety filters.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing density-based cluster pruning with semantic text merging reduces topic redundancy.
- **Mechanism:** BERTopic's HDBSCAN clustering groups documents by vector proximity, which in noisy social media data often splits semantically similar themes into multiple clusters. The baseline approach increases `minimum_cluster_size` based on geometric density, ignoring semantic nuance. This method extracts topic representations (keywords/labels) and uses an LLM to identify semantic equivalence missed by vector distance alone.
- **Core assumption:** Topic representations (top-10 keywords or LLM labels) capture sufficient semantic meaning for the LLM to judge similarity accurately.
- **Evidence anchors:** [abstract] "Semantically similar topics are iteratively merged using LLM prompts." [section 1] "HDBSCAN tends to produce an excessive number of topics... many of which are semantically overlapping."
- **Break condition:** If c-TF-IDF extraction fails to capture the topic concept (e.g., extracts stopwords), the LLM receives garbage input and makes invalid merge decisions.

### Mechanism 2
- **Claim:** Iterative agglomeration prevents context saturation and preserves topic quality.
- **Mechanism:** Rather than asking the LLM to map all pairwise similarities at once (computationally expensive and prone to hallucination), the framework iteratively selects the single best pair to merge, updating the topic list for fresh context in each step.
- **Core assumption:** Sequentially merging the "two most similar" topics leads to a globally optimal topic set.
- **Evidence anchors:** [section 3] "Iterative agglomerative merging... helps prevent overwhelming the LLM with very large input lists." [section 4] Results show diversity increases consistently as topics are reduced.
- **Break condition:** If the initial number of topics is massive (e.g., >500), the O(N) iterative steps might become a latency bottleneck or accumulate merge errors.

### Mechanism 3
- **Claim:** Decoupling generation from reduction allows for domain-agnostic robustness.
- **Mechanism:** Standard End-to-End LLM topic modeling forces a choice between cost and granularity. By letting BERTopic handle document clustering (cheap, scalable) and the LLM handle only schema refinement (expensive, semantic), the system achieves better scalability.
- **Core assumption:** BERTopic clustering provides a "good enough" superset of topics that merely needs consolidation, not generation from scratch.
- **Evidence anchors:** [abstract] "More computationally efficient than end-to-end LLM topic modeling." [section 2.2] "End-to-end approaches... require processing each document sequentially... leading to prohibitive costs."
- **Break condition:** If BERTopic misses a topic entirely (documents marked as noise), the LLM reduction phase cannot recover it.

## Foundational Learning

- **Concept: HDBSCAN `minimum_cluster_size` & `cluster_selection_method`**
  - **Why needed here:** These determine initial granularity. If set too low, you flood the LLM with noise; if set too high, you lose minority topics before the LLM sees them.
  - **Quick check question:** If I increase `minimum_cluster_size`, am I making topics more specific or more general?

- **Concept: c-TF-IDF (Class-based TF-IDF)**
  - **Why needed here:** This is the "language" the LLM speaks. It creates topic representation by prioritizing words unique to a specific cluster relative to the whole corpus.
  - **Quick check question:** How does c-TF-IDF differ from standard TF-IDF in treating a cluster of documents?

- **Concept: NPMI (Normalized Pointwise Mutual Information)**
  - **Why needed here:** This is the paper's primary metric for "coherence." It measures how often words in a topic appear together in external corpus vs. by chance.
  - **Quick check question:** Why might high NPMI not always equal "interpretable" topics to a human?

## Architecture Onboarding

- **Component map:** Raw Tweets -> BERTopic (SBERT Embeddings -> UMAP -> HDBSCAN) -> c-TF-IDF (Top 10 Keywords) OR LLM Labels -> LLM Prompt ("Identify 2 most similar") -> Merge -> Update List -> NPMI (Coherence) & Unique Words (Diversity)
- **Critical path:** The **Iterative Merge Loop**. You must ensure the LLM output is strictly parsed (e.g., `[2, 8]`) to map back to topic indices. A failure in parsing breaks the loop.
- **Design tradeoffs:**
  - **Prompt Format:** "Top-10 Keywords" is cheaper/faster but relies on c-TF-IDF accuracy. "LLM Labels" is semantically richer but adds latency/cost to every iteration step.
  - **Model Selection:** Open models (Llama/Gemma) offer privacy but failed on sensitive content (Cyberbullying). Closed models (GPT) are robust but incur API costs and data egress.
- **Failure signatures:**
  - **The "Safety Refusal":** LLM refuses to process topics related to hate speech/cyberbullying (observed with Llama-3). *Mitigation:* Filter representations or use a model with adjustable safety thresholds.
  - **Coherence Collapse:** On highly noisy data (Cyberbullying), semantic merging actually hurt NPMI scores compared to the baseline. *Mitigation:* Do not blindly apply to all datasets; validate on a sample first.
- **First 3 experiments:**
  1. **Sanity Check:** Run BERTopic on a small sample (1k tweets). Extract keywords. Manually verify if "redundant" topics exist.
  2. **Loop Calibration:** Implement the LLM-reducer. Set a target reduction (e.g., 50 -> 25). Compare "Top-10 Keywords" input vs. "LLM Label" input for latency and merge quality.
  3. **Boundary Test:** Feed the reducer a dataset with known distinct topics (e.g., Sports vs. Politics). Ensure the LLM does *not* merge them, validating precision.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do dynamic or context-aware prompting strategies improve the identification of overlapping topics compared to static prompts?
- **Basis in paper:** [explicit] The authors state that future work could explore "more advanced prompting strategies... such as dynamic or context-aware prompting" to enhance the merging process.
- **Why unresolved:** The current study relies on a static prompt structure ("Return the index of the two most similar topics...") for all iterations and datasets.
- **What evidence would resolve it:** A comparative analysis of topic merging accuracy and coherence using static prompts versus prompts that adapt based on dataset context or previous merge history.

### Open Question 2
- **Question:** Does LLM-assisted topic reduction correlate with human judgment of topic quality better than automated metrics like NPMI?
- **Basis in paper:** [explicit] The authors note that "incorporating human evaluation may offer a more nuanced assessment of topic quality compared to reliance on automated metrics alone."
- **Why unresolved:** The evaluation relies exclusively on automated metrics (NPMI and diversity), which showed potential limitations on the Cyberbullying dataset (where diversity improved but coherence dropped).
- **What evidence would resolve it:** A user study where human annotators rate the interpretability and semantic validity of topics reduced by LLMs versus the baseline.

### Open Question 3
- **Question:** How does the method perform against alternative topic reduction techniques beyond increasing the HDBSCAN minimum cluster size?
- **Basis in paper:** [explicit] The paper suggests that "comparison with... additional topic reduction techniques beyond simply increasing the minimum cluster size would provide a more comprehensive evaluation."
- **Why unresolved:** The current baseline is restricted to a single method (parameter tuning); other algorithmic reduction strategies were not tested.
- **What evidence would resolve it:** Benchmarking the LLM-assisted approach against other reduction methods, such as agglomerative clustering on topic embeddings or manual pruning heuristics.

## Limitations
- The method's performance on sensitive content (Cyberbullying) was problematic, with LLM safety filters causing failures and coherence metrics showing degradation.
- The approach relies on opaque LLM judgments of semantic similarity that may vary across model versions or prompt phrasings.
- The greedy sequential merging strategy assumes pairwise optimality leads to global optimality, which may not always hold.
- The generalizability to non-English content and other social media platforms beyond Twitter remains untested.

## Confidence
- **High confidence:** Computational efficiency claim (hybrid vs. pure LLM) is well-supported; basic LLM-based semantic similarity mechanism is reproducible.
- **Medium confidence:** Experimental results show improved diversity and coherence, but effect sizes vary significantly across datasets and LLMs.
- **Low confidence:** Generalizability to other platforms and languages is untested; long-term stability of reduced topics is unknown.

## Next Checks
1. **Cross-model validation:** Run the same reduction pipeline using different LLM models (both open and closed) on identical datasets to quantify variance in merge decisions and resulting topic quality.
2. **Sensitivity analysis:** Systematically vary the initial `min_cluster_size` parameter and document its impact on the final reduced topic set's coherence and diversity to identify optimal starting conditions.
3. **Human evaluation study:** Conduct a small-scale user study where domain experts assess the interpretability and usefulness of topics before and after reduction, supplementing the NPMI metric with qualitative feedback.