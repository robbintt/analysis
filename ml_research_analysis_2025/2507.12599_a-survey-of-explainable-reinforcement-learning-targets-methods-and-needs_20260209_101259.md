---
ver: rpa2
title: 'A Survey of Explainable Reinforcement Learning: Targets, Methods and Needs'
arxiv_id: '2507.12599'
source_url: https://arxiv.org/abs/2507.12599
tags:
- agent
- learning
- policy
- state
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comprehensive survey of eXplainable Reinforcement
  Learning (XRL), categorizing over 250 papers using a taxonomy based on two key questions:
  "What" (the target of explanation: policy, sequence, or action) and "How" (the method
  of explanation). The survey reveals a significant imbalance in research focus, with
  far more methods targeting policy and action-level explanations than sequence-level
  ones.'
---

# A Survey of Explainable Reinforcement Learning: Targets, Methods and Needs

## Quick Facts
- arXiv ID: 2507.12599
- Source URL: https://arxiv.org/abs/2507.12599
- Authors: Léo Saulières
- Reference count: 40
- Key outcome: Comprehensive survey categorizing over 250 XRL papers using a taxonomy based on "What" (target: policy, sequence, action) and "How" (explanation method)

## Executive Summary
This paper presents a comprehensive survey of eXplainable Reinforcement Learning (XRL), categorizing over 250 papers using a taxonomy based on two key questions: "What" (the target of explanation: policy, sequence, or action) and "How" (the method of explanation). The survey reveals a significant imbalance in research focus, with far more methods targeting policy and action-level explanations than sequence-level ones. The paper identifies trends such as the predominance of feature importance methods using saliency maps for action explanations and interpretable policy methods for policy explanations. It also highlights related domains like Explainable Planning and Algorithmic Recourse as potential sources of inspiration for future XRL methods. Finally, the survey outlines key needs for the field, including the development of unified metrics for comparison, more user studies, and dedicated user interfaces.

## Method Summary
This survey paper systematically reviews over 250 XRL papers and categorizes them using a two-dimensional taxonomy: "What" (the target of explanation - policy, sequence, or action) and "How" (the explanation method). The methodology involves literature synthesis rather than empirical experimentation, identifying trends, gaps, and methodological patterns across the XRL landscape. The paper draws connections to related fields including Hierarchical RL, Relational RL, Causal RL, Explainable Planning, and Algorithmic Recourse to identify potential sources of inspiration for future XRL development.

## Key Results
- Significant research imbalance: 175 papers focus on policy-level explanations, 89 on action-level, and only 11 on sequence-level explanations
- Feature importance methods (saliency maps) dominate action-level explanations
- Interpretable policy methods are the primary approach for policy-level explanations
- Critical gaps identified in sequence-level explanations, standardized metrics, user studies, and dedicated user interfaces

## Why This Works (Mechanism)

### Mechanism 1: Surrogate Model Mimicry (Policy Level)
- **Claim:** Opaque DRL policies can be approximated by simpler, interpretable models without total performance collapse
- **Mechanism:** A transparent surrogate model is trained via imitation learning to replicate the input-output mapping of the complex DRL agent, distilling it into explicit rules or branching logic
- **Core assumption:** The DRL agent's decision boundary can be adequately captured by a simpler function without losing critical fidelity
- **Evidence anchors:** Abstract mentions opacity of deep neural networks; Section 2.1.1 discusses surrogate models as interpretable substitutes learned to understand agent behavior
- **Break condition:** If the surrogate requires high complexity (e.g., deep trees) to match DRL performance, interpretability is lost

### Mechanism 2: Feature Attribution via Salience (Action Level)
- **Claim:** Specific state features responsible for a specific action can be isolated by perturbing inputs or back-propagating gradients
- **Mechanism:** By systematically masking or altering parts of the state or analyzing gradients, the method identifies "salient" regions representing input features that most significantly impact the action decision
- **Core assumption:** The agent's policy relies on local, detectable correlations rather than distributed abstract representations
- **Evidence anchors:** Section 4.1 discusses feature importance for identifying state features impacting agent decisions; Section 4.1.1 covers saliency maps for image-based states
- **Break condition:** If the agent relies on "spurious correlations," saliency maps may highlight unrelated features, providing false confidence

### Mechanism 3: Counterfactual Trajectory Comparison (Sequence Level)
- **Claim:** Agent behavior is best explained by contrasting actual trajectories with counterfactual sequences where actions differ
- **Mechanism:** The system generates alternative trajectories starting from the same state but with modified actions or policies, comparing outcomes to highlight the specific utility of the agent's chosen path
- **Core assumption:** Users possess the cognitive capacity to compare and reason about branching temporal sequences
- **Evidence anchors:** Section 3 covers counterfactual sequences for comparing agent behavior with alternative policies; Section 4.1.3 discusses why agents choose specific actions over alternatives
- **Break condition:** If environment dynamics are stochastic or chaotic, generating meaningful deterministic counterfactual trajectories becomes computationally infeasible or misleading

## Foundational Learning

- **Concept:** Markov Decision Process (MDP)
  - **Why needed here:** XRL targets often decompose the MDP; understanding "Human-readable MDP" explanations requires knowing the base tuple ⟨S, A, R, p⟩ (State, Action, Reward, Transition)
  - **Quick check question:** Can you define the "Transition Function" p and how it differs from the "Reward Function" R in the context of an RL agent?

- **Concept:** Imitation Learning (IL) / Policy Distillation
  - **Why needed here:** The survey identifies Surrogate Models as a primary XRL method, which rely heavily on IL - training a student (interpretable) model to mimic a teacher (DRL) model
  - **Quick check question:** If a "student" decision tree matches a "teacher" neural network's actions 95% of the time, does that guarantee the student has learned the same internal logic?

- **Concept:** The Fidelity vs. Interpretability Trade-off
  - **Why needed here:** Sections 2.1 and 2.1.1 explicitly discuss this; high performance usually requires complex (uninterpretable) models, while extracting interpretable rules often sacrifices performance
  - **Quick check question:** In the context of Section 2.1.1, what happens to the interpretability of a Soft Decision Tree as its depth increases to match DRL performance?

## Architecture Onboarding

- **Component map:** RL Core (Agent + Environment) -> Explanation Target Selector (Policy/Sequence/Action) -> Explanation Generator (Method) -> User Interface (Visualizer/Textual Output)

- **Critical path:** Selecting the correct Target (Policy vs. Action) is the first critical step; the path is: Identify User Query → Select Taxonomy Target → Apply "Inherently Understandable" or "Post-hoc" Method

- **Design tradeoffs:**
  - Intrinsic vs. Post-hoc: Training "Inherently Understandable" agents constrains learning capacity vs. Post-hoc methods that might be unfaithful to actual agent logic
  - Global vs. Local: Policy summaries give high-level views but miss specific action rationales

- **Failure signatures:**
  - Causal Confusion: Explainer highlights features that correlate with success but don't cause it; agent fails when those features are manipulated
  - Unfaithful Surrogate: Decision tree mimics output but uses completely different split criteria than the neural network actually uses internally

- **First 3 experiments:**
  1. **Sanity Check Saliency:** Train a DQN on an Atari game. Use gradient-based saliency map. Randomize network weights and check if saliency map changes. If it stays the same, the map is independent of learned policy.
  2. **Fidelity-F1 Score:** Train DRL agent and Decision Tree surrogate. Calculate Fidelity-1 score: percentage of states where Tree's action matches DRL agent's action. Aim for >90% before trusting explanation.
  3. **User "Mental Model" Test:** Present policy summary to user. Perturb environment in way predicted by explanation. Ask user to predict outcome. If they fail, explanation did not improve "Mental Model."

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the research community develop and standardize XRL methods to explain sequences of agent interactions, given the current overwhelming focus on action and policy-level explanations?
- **Basis in paper:** [explicit] Authors highlight "low interest of researchers in explaining sequences of agent interactions" and encourage explaining agent's action choice sequences to provide better heterogeneity of XRL methods
- **Why unresolved:** Only 11 works target sequence-level methods compared to 175 for policy and 89 for action levels; undue influence of classification-based XAI focusing on single decisions
- **What evidence would resolve it:** Publication of diverse new XRL methods targeting the "sequence" target category, along with benchmarks designed to evaluate temporal explanation validity

### Open Question 2
- **Question:** What unified metrics can be established to effectively compare different XRL methods and measure the quality of their explanations?
- **Basis in paper:** [explicit] Section 6 identifies "Provide metrics" as a key need, stating that "One of the shortcomings of XRL is the lack of a unified way of comparing methods"
- **Why unresolved:** Current evaluations rely on disparate approaches, often borrowing metrics from explainable classification that may not capture RL-specific nuances like state evolution or future reward prediction
- **What evidence would resolve it:** Development of a standardized benchmark suite that includes consistent metrics for stability, fidelity, and computational cost applicable across different XRL targets

### Open Question 3
- **Question:** To what extent do dedicated user interfaces and rigorous user studies improve the interpretability and adoption of XRL systems compared to algorithmic-only approaches?
- **Basis in paper:** [explicit] Section 6 lists "Perform user studies" and "Develop user interface" as distinct needs, noting that "a relatively small proportion validate their approach with a user study"
- **Why unresolved:** While many algorithmic methods exist, their actual utility to end-users is rarely verified; providing good XRL method is as important as providing user interface
- **What evidence would resolve it:** Comparative studies showing XRL methods integrated with specific user interfaces yield significantly higher user trust, task performance, or understanding than raw explanation data without dedicated UI

## Limitations

- The survey's comprehensive taxonomy provides structural insight but lacks standardized quantitative metrics for method comparison
- Claims about mechanism effectiveness are supported by literature references but lack direct empirical validation within the survey itself
- The field's reliance on qualitative categorization rather than systematic comparison affects confidence in conclusions about method utility

## Confidence

**High Confidence:** The taxonomic structure (What/How framework) and identification of research imbalances are well-supported by corpus analysis and methodology appears sound

**Medium Confidence:** Claims about specific mechanism effectiveness (surrogate models, saliency maps, counterfactuals) are theoretically sound but may not translate to practical utility without empirical validation

**Low Confidence:** Identification of "needs" for the field (unified metrics, more user studies, dedicated interfaces) represents reasonable speculation based on observed gaps rather than systematic investigation

## Next Checks

1. **Fidelity-Interpretability Trade-off Validation:** Implement multiple explanation methods on the same DRL agent and environment, then measure both explanation fidelity and human interpretability through controlled user studies

2. **Cross-Method Comparison Benchmark:** Develop a standardized evaluation protocol using common environments and metrics to compare explanation methods across the taxonomy, addressing the need for unified benchmarks

3. **Causal Attribution Accuracy Test:** Design experiments specifically targeting the "Causal Confusion" problem by creating environments where spurious correlations exist, then test whether explanation methods correctly identify true causal factors versus correlated features