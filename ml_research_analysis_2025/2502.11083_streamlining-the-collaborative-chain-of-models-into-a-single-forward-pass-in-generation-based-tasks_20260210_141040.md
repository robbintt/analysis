---
ver: rpa2
title: Streamlining the Collaborative Chain of Models into A Single Forward Pass in
  Generation-Based Tasks
arxiv_id: '2502.11083'
source_url: https://arxiv.org/abs/2502.11083
tags:
- hidden
- arxiv
- states
- chain
- fthss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the inefficiency of sequential "Chain of Models"
  in RAG and agent-based tasks, where each model's KV hidden states are recomputed
  during inference due to parameter differences, leading to redundant computation
  and increased KV cache storage. The proposed method, FTHSS, enables direct sharing
  of KV hidden states across models in a chain by modifying input and attention masks
  during training, allowing downstream models to reuse upstream KV caches.
---

# Streamlining the Collaborative Chain of Models into A Single Forward Pass in Generation-Based Tasks

## Quick Facts
- arXiv ID: 2502.11083
- Source URL: https://arxiv.org/abs/2502.11083
- Reference count: 31
- Primary result: Achieves 3× speedup at 3000 tokens while maintaining performance in multi-model RAG/agent chains

## Executive Summary
This paper addresses the inefficiency of sequential "Chain of Models" in RAG and agent-based tasks, where each model's KV hidden states are recomputed during inference due to parameter differences, leading to redundant computation and increased KV cache storage. The proposed method, FTHSS, enables direct sharing of KV hidden states across models in a chain by modifying input and attention masks during training, allowing downstream models to reuse upstream KV caches. Experiments on four tasks show FTHSS achieves performance comparable to traditional model chains while reducing inference latency (e.g., 3× speedup for 3,000 tokens) and eliminating redundant KV cache storage, especially in multi-round scenarios. Fine-tuning on small datasets suffices to adapt models to noisy KV states, further improving efficiency.

## Method Summary
FTHSS enables KV hidden state sharing across prompt-tuned models by modifying attention masks during training. The method works by training downstream models to interpret upstream KV states through exposure to noisy inputs, using cascade attention masks that allow in-memory recomputation of KV states without offline storage. Position ID offsets are handled through RoPE's relative position encoding. The approach supports both single-round (sequential) and multi-round (synchronous) training scenarios, with models sharing a common base architecture and differing only in soft prompt tokens.

## Key Results
- Achieves 3× inference speedup at 3000 tokens compared to traditional model chains
- Reduces KV cache storage from n copies to single shared cache
- Performance comparable to baseline with only 0.4-1.5 EM drop on complex datasets
- Fine-tuning on 5,000 examples sufficient to adapt to noisy KV states

## Why This Works (Mechanism)

### Mechanism 1: Cross-Model KV State Compatibility via Noise Adaptation
Downstream models can interpret upstream KV hidden states despite parameter differences from prompt tuning. In prompt-tuning setups, models share identical base parameters and differ only in learnable prompt tokens. The output KV states from model Mi differ marginally from what Mi+1 would compute via prefilling. FTHSS trains downstream models on noisy KV inputs (recalculated in memory via attention mask manipulation), teaching them to attend past noise tokens to task-relevant content. Core assumption: Attention sparsity implies a few noisy tokens minimally impact final outputs; models can learn to suppress irrelevant attention contributions.

### Mechanism 2: Online KV Recomputation via Cascade Attention Masking
Storing upstream KV states offline is unnecessary; they can be recomputed in-memory during downstream model training with negligible overhead. FTHSS concatenates shared content, upstream prompt tokens, upstream input/output, and downstream prompt tokens into one sequence. A custom attention mask ensures: (1) upstream KV states are computed correctly via causal attention over upstream content, and (2) downstream prompt tokens attend to those precomputed states but mask upstream prompt tokens. This avoids disk I/O for KV caches. Core assumption: GPU memory suffices to hold intermediate activations for recomputation.

### Mechanism 3: Position ID Shift Invariance under RoPE
Position IDs for downstream models can start at l+1 (where l is upstream's last position) without affecting attention accuracy. Rotary Position Embedding (RoPE) encodes relative positions via rotation matrices. The attention score Score(m, n) depends only on n−m (relative offset), not absolute m or n. Thus, shifting all downstream position IDs by a constant preserves attention patterns. Appendix D provides the mathematical proof. Core assumption: The base model uses RoPE or another relative position encoding.

## Foundational Learning

- **KV Cache in Transformers:** Understanding what KV caches store (keys/values per layer for all prior tokens) is prerequisite to grasping why sharing them saves computation. Quick check: During autoregressive decoding, which components are cached and reused versus recomputed at each step?
- **Prompt Tuning (PEFT):** FTHSS relies on a shared base model with only prompt tokens differing per sub-task. Understanding how prompt tokens are prepended and optimized clarifies why parameter differences are small enough for KV compatibility. Quick check: In prompt tuning, are the prompt token embeddings learned via gradient descent, or are they fixed discrete tokens?
- **Attention Masking in Transformers:** The cascade attention mask is the key implementation detail. Engineers must understand how causal vs. bidirectional masking controls information flow before implementing the custom mask patterns. Quick check: What does a causal mask prevent a token at position i from attending to?

## Architecture Onboarding

- **Component map:** Shared Base Transformer (Mθ) -> Per-Task Prompt Tokens (Pi) -> Cascade Attention Mask Module -> Unified KV Cache Manager
- **Critical path:**
  1. Training (Single-Round): Train upstream model A → Generate outputs → Recompute A's KV states in-memory with cascade mask → Train downstream model B to read from A's KV cache
  2. Training (Multi-Round): Initialize all prompt token sets → Construct unified sequence with interleaved prompt tokens → Apply round-specific masking → Train all prompts synchronously
  3. Inference: Precompute KV cache for shared content → Switch prompt tokens per sub-task → Append generated tokens to shared cache → Continue autoregressively without re-prefilling upstream outputs
- **Design tradeoffs:**
  - Storage vs. Compute in Training: Offline storage of KV states reduces training compute but increases I/O; online recomputation increases compute but simplifies pipeline
  - Fine-Tuning Data Scale: Full dataset retraining yields best results, but 5,000 examples suffice for noise adaptation
  - Single-Round vs. Multi-Round Complexity: Multi-round requires synchronous training and more complex masking; single-round allows sequential training
- **Failure signatures:**
  1. Attention Sink Collapse: If noise tokens dominate attention, downstream outputs degrade. Symptom: performance drops on complex datasets without fine-tuning
  2. Position Encoding Mismatch: Using absolute position embeddings causes scrambled attention after position ID shift. Symptom: generated text becomes incoherent at transition points
  3. Cache Contamination: Prompt tokens from one sub-task leaking into another's attention window. Symptom: task-irrelevant content in outputs
- **First 3 experiments:**
  1. Validate KV Compatibility on Synthetic Task: Create a 2-model chain (A→B) with toy task. Train with and without FTHSS. Compare accuracy and latency.
  2. Ablate Fine-Tuning Data Size: Using the Context Compression & QA task, train downstream model with 0, 1K, 5K, and full training examples. Plot performance curve.
  3. Stress Test Memory for Long Contexts: Construct a chain where upstream output is 4K–8K tokens. Measure peak GPU memory during online KV recomputation vs. offline storage.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the efficiency and performance of FTHSS scale effectively to Large Language Models with significantly larger parameter counts (e.g., 70B parameters or more)?
- Basis in paper: The authors state in the Limitations section that experiments were not conducted on large models due to resource constraints and explicitly call for future work to "explore the generalizability of hidden state sharing methods in larger models."
- Why unresolved: The current experiments are restricted to the Llama-3-8B architecture, leaving the behavior of shared KV hidden states in larger, more complex parameter spaces unverified.
- What evidence would resolve it: Empirical results showing latency speedups and task performance metrics when applying FTHSS to 70B+ parameter models on the same RAG and agent-based benchmarks.

### Open Question 2
- Question: Can the KV hidden state sharing mechanism be adapted for use with closed-source models where internal weights and states are accessible only via API?
- Basis in paper: The paper lists the inapplicability to closed-source models as a primary limitation, noting the method cannot be directly applied in such settings.
- Why unresolved: The current method relies on modifying attention masks and directly manipulating inputs during training on a shared base model, which is impossible with API-only access.
- What evidence would resolve it: A proposed framework or protocol that approximates state sharing or distills this capability into a scenario where the underlying model architecture is hidden.

### Open Question 3
- Question: How robust is the FTHSS method when applied to more diverse, high-quality, and challenging datasets beyond the standard RAG and agent tasks evaluated?
- Basis in paper: The Conclusion and Limitations section explicitly suggests that future work should explore generalizability "across diverse, high-quality, and challenging datasets."
- Why unresolved: The current study validates the method on specific tasks like Context Compression and Query Rewriting, which may not represent the full complexity of potential real-world applications.
- What evidence would resolve it: Evaluation of FTHSS performance on a broader suite of complex generation tasks, demonstrating that "noisy" KV states do not degrade output quality in high-precision domains.

## Limitations

- Method requires shared base architecture with only prompt differences, limiting applicability to other parameter-efficient fine-tuning methods
- Online KV recomputation introduces training-time memory overhead that could become prohibitive for very long contexts
- Cannot be directly applied to closed-source models where internal weights and states are inaccessible

## Confidence

- **High Confidence:** Experimental results showing 3× speedup and single KV cache usage are well-supported by methodology and measurements. Theoretical justification for position ID shift invariance under RoPE is mathematically rigorous.
- **Medium Confidence:** Claim that downstream models can effectively interpret upstream KV states with minimal fine-tuning (5,000 examples) is supported but may be task-dependent. Cascade attention masking mechanism appears sound but lacks detailed implementation specifications.
- **Low Confidence:** Generalizability beyond tested prompt-tuning paradigm to other parameter-efficient fine-tuning methods or substantially different model architectures is not demonstrated.

## Next Checks

1. **Multi-Architecture Compatibility Test:** Apply FTHSS to a chain where upstream and downstream models use different base architectures (e.g., Llama-3→Mistral) with parameter-efficient fine-tuning. Measure whether position ID shift invariance holds and whether performance degradation exceeds the observed 0.4-1.5 EM drop.

2. **Long-Context Stress Test:** Construct a 3-model chain where upstream output exceeds 8,000 tokens. Measure training memory usage during online KV recomputation and compare against the offline storage baseline. Identify the exact token threshold where online recomputation becomes infeasible.

3. **Attention Pattern Analysis:** Using the Context Compression→QA task, visualize attention weights in the downstream model when processing upstream KV states versus when processing its own inputs. Quantify the proportion of attention mass on noisy versus task-relevant tokens, and correlate this with performance degradation across different upstream output lengths.