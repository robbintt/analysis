---
ver: rpa2
title: LLM-driven Knowledge Distillation for Dynamic Text-Attributed Graphs
arxiv_id: '2502.10914'
source_url: https://arxiv.org/abs/2502.10914
tags:
- edge
- temporal
- text
- graph
- link
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LKD4DyTAG addresses the challenge of effectively encoding temporal,
  structural, and textual information in Dynamic Text-Attributed Graphs (DyTAGs) for
  future link prediction and edge classification tasks. The core idea is to use temporal
  encoding in edges combined with knowledge distillation from LLM-driven textual representations
  to a lightweight GNN model, allowing the GNN to capture both spatio-temporal and
  semantic information.
---

# LLM-driven Knowledge Distillation for Dynamic Text-Attributed Graphs

## Quick Facts
- **arXiv ID**: 2502.10914
- **Source URL**: https://arxiv.org/abs/2502.10914
- **Authors**: Amit Roy; Ning Yan; Masood Mortazavi
- **Reference count**: 40
- **Primary result**: LKD4DyTAG achieves up to 99.98% ROC-AUC for future link prediction and improved F1 scores for edge classification in Dynamic Text-Attributed Graphs using temporal encoding and LLM-driven knowledge distillation.

## Executive Summary
LKD4DyTAG introduces a novel framework for encoding temporal, structural, and textual information in Dynamic Text-Attributed Graphs (DyTAGs) to improve future link prediction and edge classification. The method leverages temporal edge encoding combined with knowledge distillation from LLM-generated textual representations into a lightweight GNN, enabling the model to capture both spatio-temporal and semantic features. Extensive experiments on six real-world datasets demonstrate significant performance gains over state-of-the-art approaches, with up to 99.98% ROC-AUC for link prediction and improved F1 scores for edge classification, particularly in transductive settings.

## Method Summary
The proposed approach integrates temporal encoding into edge representations and uses knowledge distillation to transfer information from LLM-generated textual embeddings to a GNN model. This allows the GNN to learn rich, temporally-aware representations that incorporate both structural and semantic information from the graph and its associated text. The framework is specifically designed for Dynamic Text-Attributed Graphs, where both the graph structure and textual content evolve over time. The knowledge distillation process involves training the GNN to mimic the output of a pre-trained LLM on textual data, enabling the model to effectively utilize large-scale linguistic knowledge while maintaining computational efficiency during inference.

## Key Results
- Achieves up to 99.98% ROC-AUC for future link prediction on benchmark datasets
- Demonstrates improved F1 scores for edge classification tasks, especially in transductive settings
- Outperforms state-of-the-art baselines across six real-world DyTAG datasets

## Why This Works (Mechanism)
The method works by combining temporal encoding with knowledge distillation, allowing the GNN to capture both the evolving structure of the graph and the semantic content of associated text. Temporal encoding ensures that the model is aware of the time dimension in dynamic graphs, while knowledge distillation leverages the rich linguistic representations generated by LLMs. This dual approach enables the model to make more accurate predictions by integrating both temporal patterns and textual semantics.

## Foundational Learning
- **Dynamic Text-Attributed Graphs (DyTAGs)**: Graphs with evolving structure and associated textual data over time
  - Why needed: To model real-world scenarios where both graph topology and textual content change dynamically
  - Quick check: Verify that the dataset contains timestamped edges and associated text for each edge
- **Temporal Encoding**: Adding time-aware features to graph representations
  - Why needed: To capture the evolution of graph structure and relationships over time
  - Quick check: Ensure temporal features are correctly integrated into edge representations
- **Knowledge Distillation**: Transferring knowledge from a large model (LLM) to a smaller one (GNN)
  - Why needed: To leverage rich textual representations without the computational cost of running the LLM during inference
  - Quick check: Validate that the GNN's output approximates the LLM's predictions on training data
- **Graph Neural Networks (GNNs)**: Neural networks designed to operate on graph-structured data
  - Why needed: To learn node and edge representations by aggregating information from the graph structure
  - Quick check: Confirm the GNN architecture is suitable for the graph size and task
- **Future Link Prediction**: Predicting edges that will appear in the graph at a future time step
  - Why needed: To evaluate the model's ability to forecast graph evolution
  - Quick check: Ensure the train/test split respects temporal order
- **Edge Classification**: Assigning labels to edges based on their features
  - Why needed: To assess the model's ability to understand and categorize relationships
  - Quick check: Verify the classification task is well-defined and balanced

## Architecture Onboarding

**Component Map**
LLM Textual Encoder -> Knowledge Distillation Module -> GNN with Temporal Encoding -> Prediction Layer

**Critical Path**
1. LLM processes text associated with each edge to generate rich embeddings
2. Knowledge distillation trains the GNN to mimic LLM outputs using these embeddings
3. GNN incorporates temporal encoding and structural information to make predictions

**Design Tradeoffs**
- Computational cost: LLM inference during training is expensive but necessary for knowledge distillation
- Model size: Lightweight GNN keeps inference efficient at the cost of some expressiveness
- Temporal granularity: Balancing the frequency of graph snapshots with model complexity

**Failure Signatures**
- Poor link prediction: Likely due to insufficient temporal encoding or ineffective knowledge transfer
- Overfitting: May occur if the GNN is too complex relative to the amount of training data
- High training time: Expected due to repeated LLM inference during knowledge distillation

**First Experiments**
1. Train the model on a small subset of the data to verify the end-to-end pipeline works
2. Perform an ablation study removing temporal encoding to measure its impact
3. Compare performance with and without knowledge distillation to quantify its contribution

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on LLM inference during training introduces significant computational overhead, limiting scalability to very large graphs
- Evaluation focuses on link prediction and edge classification, leaving node-level and graph-level tasks unexplored
- The individual contributions of the LLM and GNN components are somewhat entangled, making it difficult to isolate their effects

## Confidence
- **High**: The method's effectiveness for link prediction is well-supported by consistent improvements across multiple datasets and strong margins over baselines
- **Medium**: Claims about general applicability to DyTAGs are reasonable but not fully validated due to limited task and dataset diversity
- **Low**: Scalability assertions lack rigorous computational analysis, particularly regarding LLM inference costs

## Next Checks
1. Conduct runtime and memory profiling to quantify the computational overhead of LLM-based knowledge distillation across different graph sizes and text lengths
2. Test the method on node classification and graph classification tasks to evaluate its versatility beyond link prediction and edge classification
3. Perform experiments on datasets with varying temporal granularities and text-to-graph ratio to assess robustness across different DyTAG characteristics