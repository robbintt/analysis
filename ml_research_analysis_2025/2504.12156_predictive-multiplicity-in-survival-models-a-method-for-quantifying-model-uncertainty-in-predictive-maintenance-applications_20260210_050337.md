---
ver: rpa2
title: 'Predictive Multiplicity in Survival Models: A Method for Quantifying Model
  Uncertainty in Predictive Maintenance Applications'
arxiv_id: '2504.12156'
source_url: https://arxiv.org/abs/2504.12156
tags:
- survival
- predictive
- rashomon
- maintenance
- uncertainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper extends predictive multiplicity to survival analysis\
  \ for predictive maintenance, introducing formal metrics\u2014ambiguity, discrepancy,\
  \ and obscurity\u2014to quantify model uncertainty. Using the CMAPSS dataset, it\
  \ trains 22,500 Random Survival Forests models and constructs Rashomon sets with\
  \ varying performance tolerances (\u03F5 from 0.01 to 1)."
---

# Predictive Multiplicity in Survival Models: A Method for Quantifying Model Uncertainty in Predictive Maintenance Applications

## Quick Facts
- arXiv ID: 2504.12156
- Source URL: https://arxiv.org/abs/2504.12156
- Reference count: 11
- This paper extends predictive multiplicity to survival analysis for predictive maintenance, introducing formal metrics—ambiguity, discrepancy, and obscurity—to quantify model uncertainty.

## Executive Summary
This paper addresses the challenge of quantifying uncertainty in survival models used for predictive maintenance. While Rashomon sets identify multiple models with near-optimal performance, this work introduces three metrics—ambiguity, discrepancy, and obscurity—to measure prediction divergence among these models. Using the CMAPSS dataset, the study trains 22,500 Random Survival Forests models and demonstrates that even high-performing models can yield significantly different failure risk estimates, particularly in certain engine subsets where ambiguity reaches 100% at higher performance tolerances.

## Method Summary
The method constructs Rashomon sets of near-optimal Random Survival Forests models by defining a performance tolerance ε around the best model (measured via integrated Brier score). For each observation, predictions are compared against a reference model using a domain-specific conflict threshold δ. Three metrics quantify multiplicity: ambiguity (fraction of observations with any conflicting prediction), discrepancy (maximum disagreement between any two models), and obscurity (average conflict rate across all model pairs). The approach is applied to the CMAPSS dataset with 22,500 hyperparameter configurations spanning tree counts, feature sampling, node sizes, and split rules.

## Key Results
- Ambiguity increases up to 40-45% of observations across datasets, reaching 100% in FD003 at high ε
- Discrepancy remains lower but substantial, indicating severe worst-case prediction conflicts
- Obscurity remains mild but dataset-dependent, showing concentrated conflicts in few model pairs
- FD003 subset exhibits extreme prediction divergence among near-optimal models

## Why This Works (Mechanism)

### Mechanism 1: Rashomon Set Construction Captures Model Uncertainty
- Claim: Defining a performance tolerance (ε) around the best model creates a set of near-optimal models whose prediction variance reflects epistemic uncertainty.
- Mechanism: The Rashomon set H_ε(f_R) = {f ∈ H : Φ(f) ≤ Φ(f_R) + ε} includes all models within ε of the reference model's performance. When many models exist in this set with divergent predictions, it signals underspecification—multiple valid solutions to the same learning problem.
- Core assumption: Near-optimal performance (within ε) implies models are equally plausible; prediction differences reflect genuine uncertainty rather than model deficiency.

### Mechanism 2: Conflict Threshold (δ) Operationalizes Meaningful Disagreement
- Claim: A domain-specified deviation threshold (δ) converts continuous survival risk differences into binary "conflict" decisions, making multiplicity measurable.
- Mechanism: For each observation, if |f(x_i, t_i) - f_R(x_i, t_i)| ≥ δ, the prediction is "conflicting." This threshold is domain-dependent—industrial prognostics may require tighter δ than lower-stakes applications.
- Core assumption: The δ threshold meaningfully captures when prediction differences would lead to different decisions.

### Mechanism 3: Three Complementary Metrics Capture Distinct Uncertainty Dimensions
- Claim: Ambiguity, discrepancy, and obscurity measure different aspects of prediction variability—coverage, worst-case, and average disagreement respectively.
- Mechanism: Ambiguity measures what fraction of observations have any conflicting prediction, discrepancy measures the maximum disagreement between any two models, and obscurity measures the average conflict rate across all model pairs.
- Core assumption: These three metrics together provide a complete picture; no single metric suffices.

## Foundational Learning

- **Concept: Survival Analysis and Censoring**
  - Why needed here: The paper extends multiplicity to time-to-event prediction where not all failures are observed (censored data). Understanding survival functions S(t), hazard functions h(t), and cumulative incidence f(x,t) is essential.
  - Quick check question: If an engine hasn't failed by cycle 250, how does a survival model use that information vs. discarding it?

- **Concept: Rashomon Effect vs. Predictive Multiplicity**
  - Why needed here: Rashomon effect = existence of many near-optimal models. Predictive multiplicity = those models making conflicting predictions. They're related but distinct.
  - Quick check question: Can you have Rashomon effect without predictive multiplicity? (Yes—if all near-optimal models agree on predictions.)

- **Concept: Epistemic vs. Aleatoric Uncertainty**
  - Why needed here: Predictive multiplicity measures epistemic uncertainty (model uncertainty reducible with more/better data), distinct from aleatoric (inherent data noise). This framing guides interpretation and mitigation.
  - Quick check question: If you collect more sensor data and multiplicity decreases, what type of uncertainty were you measuring?

## Architecture Onboarding

- **Component map:** Raw sensor data → Feature engineering → Train 22,500 RSF models (hyperparameter grid) → Select reference model f_R (best Brier score) → Define ε tolerance → Build Rashomon set H_ε(f_R) → Define δ threshold → For each observation, mark conflicts → Compute: Ambiguity (coverage), Discrepancy (worst-case), Obscurity (average)

- **Critical path:**
  1. Performance metric selection (paper uses integrated Brier score; c-index reported for context)
  2. ε selection determines Rashomon set size—too small = no models; too large = everything included
  3. δ selection defines "conflict"—must be domain-informed

- **Design tradeoffs:**
  - **ε vs. Rashomon set size**: ε=0.01 yielded 1-7 models; ε=0.10 yielded 3,396-11,328 models. Larger sets capture more uncertainty but dilute interpretability.
  - **δ vs. sensitivity**: Low δ (0.01) flags minor differences as conflicts; high δ (0.10) only captures severe disagreements.
  - **Performance metric choice**: Brier score vs. c-index may yield different Rashomon sets. Paper uses Brier score but doesn't compare alternatives.

- **Failure signatures:**
  - Rashomon set size = 1: Multiplicity cannot be measured; model is uniquely optimal (or ε too tight)
  - Ambiguity = 1.0 at low ε: Severe underspecification—even near-identical models disagree (seen in FD003)
  - Obscurity → 0 while ambiguity > 0: Conflicts exist but are concentrated in few model pairs

- **First 3 experiments:**
  1. Replicate on FD001 with ε ∈ {0.01, 0.05, 0.10} and δ ∈ {0.01, 0.05, 0.10} to verify multiplicity patterns match Table 3.
  2. Vary the performance metric (use c-index instead of Brier score) and compare Rashomon set composition and multiplicity values.
  3. Apply to a single held-out engine across its lifecycle: compute multiplicity at t=100, 200, 300 cycles to see if uncertainty changes as degradation progresses.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ensemble methods or model aggregation strategies effectively mitigate predictive multiplicity in survival models, and what trade-offs exist between reducing multiplicity and preserving prediction accuracy?
- Basis in paper: [explicit] "Future research may explore strategies to mitigate or manage predictive multiplicity, such as model aggregation via ensembles, robust selection within the Rashomon set, or decision-making frameworks that incorporate ambiguity, discrepancy, and obscurity explicitly."
- Why unresolved: The paper introduces metrics to quantify multiplicity but does not propose or evaluate mitigation strategies.

### Open Question 2
- Question: How do the multiplicity metrics behave when applied to alternative survival model classes such as deep survival networks or Cox proportional hazards models?
- Basis in paper: [explicit] "Our analysis focuses exclusively on random survival forests models applied to the CMAPSS datasets. Extending this approach to other model classes (e.g., deep survival networks) and domains (e.g., healthcare, finance) is a promising direction for future work."
- Why unresolved: The experimental evaluation uses only Random Survival Forests; model class-specific properties may affect Rashomon set structure and multiplicity patterns.

### Open Question 3
- Question: How sensitive are ambiguity, discrepancy, and obscurity to the choice of performance metric (e.g., integrated Brier score vs. concordance index) used to define the Rashomon set?
- Basis in paper: [explicit] "Our multiplicity metrics depend on choices of performance metric (e.g., integrated Brier score) and conflict threshold; alternative definitions may yield different Rashomon set structures and multiplicity profiles."
- Why unresolved: Only the integrated Brier score was used to construct Rashomon sets; different performance metrics may rank models differently, altering set membership and multiplicity measurements.

## Limitations
- The paper does not provide guidance for selecting the conflict threshold δ in practice, leaving it domain-dependent without systematic methods.
- Feature engineering pipeline and IBS integration range remain unspecified, creating potential reproducibility gaps.
- The dramatic ambiguity values in FD003 (reaching 1.0 at ε=0.10) suggest sensitivity to hyperparameter configurations not fully explored.

## Confidence
- **High Confidence**: The mathematical framework for ambiguity, discrepancy, and obscurity metrics is well-defined and internally consistent.
- **Medium Confidence**: The empirical findings are robust within the CMAPSS context, though generalizability to other datasets and maintenance domains requires validation.
- **Low Confidence**: The paper lacks systematic sensitivity analysis for hyperparameter choices and does not provide guidance for selecting the conflict threshold δ in practice.

## Next Checks
1. Replicate the full analysis on FD001 with ε ∈ {0.01, 0.05, 0.10} and δ ∈ {0.01, 0.05, 0.10} to verify multiplicity patterns match Table 3.
2. Apply the methodology to a single held-out engine across its lifecycle (cycles 100, 200, 300) to assess how uncertainty evolves as degradation progresses.
3. Test sensitivity by using c-index instead of Brier score for Rashomon set construction and compare resulting multiplicity values.