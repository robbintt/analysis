---
ver: rpa2
title: 'JGU Mainz''s Submission to the WMT25 Shared Task on LLMs with Limited Resources
  for Slavic Languages: MT and QA'
arxiv_id: '2509.22490'
source_url: https://arxiv.org/abs/2509.22490
tags:
- ukrainian
- language
- data
- translation
- qwen2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents JGU Mainz's submission to the WMT25 Shared\
  \ Task on LLMs with Limited Resources for Slavic Languages, focusing on Ukrainian,\
  \ Upper Sorbian, and Lower Sorbian. The authors address the challenge of developing\
  \ small LLMs (\u22643B parameters) capable of both machine translation (MT) and\
  \ multiple-choice question answering (QA) for low-resource Slavic languages."
---

# JGU Mainz's Submission to the WMT25 Shared Task on LLMs with Limited Resources for Slavic Languages: MT and QA

## Quick Facts
- **arXiv ID:** 2509.22490
- **Source URL:** https://arxiv.org/abs/2509.22490
- **Reference count:** 11
- **Primary result:** JGU Mainz achieves consistent improvements over baselines across all tasks, with ChrF++ improvements of over 55/65 points for DE–DSB/HSB translation, 12.34/10.27 percentage points for DSB/HSB QA, and 4.61/2.7 ChrF++ for CS–UK EN–UK MT.

## Executive Summary
This paper presents JGU Mainz's submission to the WMT25 Shared Task on LLMs with Limited Resources for Slavic Languages, focusing on Ukrainian, Upper Sorbian, and Lower Sorbian. The authors address the challenge of developing small LLMs (≤3B parameters) capable of both machine translation and multiple-choice question answering for low-resource Slavic languages. Their core method involves jointly fine-tuning a Qwen2.5-3B-Instruct model using parameter-efficient LoRA for both tasks, integrating additional MT and QA data, applying retrieval-augmented generation for Ukrainian QA, and using ensembling for QA in Sorbian languages. The primary results show consistent improvements over baseline systems across all tasks, demonstrating the viability of small LLMs for multilingual low-resource settings.

## Method Summary
The approach uses Qwen2.5-3B-Instruct with LoRA adapters applied to all projection layers for parameter-efficient adaptation. The training involves a two-stage fine-tuning process: Stage 1 jointly trains on combined MT and general QA data, while Stage 2 adapts on in-domain QA examples plus a subset of MT data with learning rate search. For Ukrainian, retrieval-augmented generation supplements the model with domain-specific knowledge from Wikipedia and books. The inference pipeline includes few-shot ICL for MT, probability averaging over answer permutations for QA, and subject-specific RAG for Ukrainian.

## Key Results
- For DE–DSB translation, ChrF++ improves by over 55 points
- For DE–HSB translation, ChrF++ improves by over 65 points
- For DSB QA, accuracy increases by up to 12.34 percentage points
- For HSB QA, accuracy increases by up to 10.27 percentage points
- For Ukrainian tasks, CS–UK EN–UK MT improves by 4.61/2.7 ChrF++ while QA accuracy increases by 4.66 points

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Joint fine-tuning on MT and QA data with LoRA enables small LLMs (≤3B) to perform reasonably on both tasks for low-resource Slavic languages, with a two-stage approach preserving task balance.
- **Mechanism:** LoRA adapters applied to all projection layers allow parameter-efficient adaptation while preserving the pretrained multilingual knowledge in Qwen2.5-3B-Instruct. The first fine-tuning round (S1) establishes both capabilities using mixed MT+general QA data. A second round (S2) then adapts with in-domain QA examples plus a subset of MT data to improve domain alignment for QA while mitigating catastrophic forgetting of translation capability.
- **Core assumption:** The base model's multilingual pretraining provides sufficient foundational representations for Slavic languages that LoRA can adapt without full parameter updates.
- **Evidence anchors:**
  - [abstract] "jointly fine-tune a Qwen2.5-3B-Instruct model for both tasks with parameter-efficient finetuning"
  - [section 3.2-3.3] Details two-stage LoRA training: S1 on combined MT/QA data, S2 on 158 in-domain QA examples + 3k translation pairs with learning rate search (1e-4 for DSB, 1e-6 for HSB)
  - [corpus] Weak corpus support—related WMT25 submissions (In2x, SALAMANDRATA) use similar fine-tuning paradigms but for different language families and larger scales
- **Break condition:** If base model has poor tokenization for target language (e.g., Cyrillic over-splitting for Ukrainian noted in Section 4), LoRA adaptations will struggle regardless of training data quantity.

### Mechanism 2
- **Claim:** Averaging predicted probabilities across multiple answer-option permutations reduces positional bias in multiple-choice QA for low-resource languages.
- **Mechanism:** LLMs exhibit sensitivity to option ordering in multiple-choice settings. By generating predictions under all permutations (for 2-3 options) or 20 random permutations (for >3 options), then averaging the probability distributions before selecting the highest-likelihood answer, the model's preference for certain positions is neutralized.
- **Core assumption:** The model's underlying knowledge is position-invariant, and observed ordering biases are artifacts of the inference process rather than true capability differences.
- **Evidence anchors:**
  - [abstract] "apply ensembling for QA in Upper and Lower Sorbian"
  - [section 3.4] "We compute the probability distribution over the answer options under the model for each order and average them"
  - [section 4, Table 3] Averaging improves accuracy from 48.3%→51.7% (DSB) and 50.5%→55.2% (HSB) after S2
  - [corpus] No direct corpus support for permutation ensembling in related WMT25 submissions
- **Break condition:** If the model lacks sufficient language understanding to assign meaningful probabilities regardless of position, averaging will not help—the paper notes non-finetuned Qwen outperforms some fine-tuned variants on QA, suggesting knowledge extraction, not just bias, matters.

### Mechanism 3
- **Claim:** Retrieval-augmented generation with domain-specific knowledge bases improves QA accuracy for knowledge-intensive questions in mid-resource languages, but gains are constrained by embedding quality.
- **Mechanism:** Subject-specific ChromaDB indexes (history vs. language/literature) store chunked Wikipedia pages and books. At inference, questions are embedded via mean-pooled last hidden states, and the 5 most relevant chunks are retrieved as context. This external knowledge supplements the model's parametric knowledge.
- **Core assumption:** The embedding model produces semantically meaningful representations for the target language that enable accurate similarity search.
- **Evidence anchors:**
  - [abstract] "retrieval-augmented generation for Ukrainian QA"
  - [section 2.3] Uses 30k Wikipedia pages + 10 books on Ukrainian history/language/literature
  - [section 4] RAG improves Ukrainian QA from 31.16%→35.82%, but gains are "smaller than for DSB and HSB QA" due to "poor-quality embeddings for Ukrainian sentences" from Cyrillic tokenization issues
  - [corpus] GETALP@AutoMin 2025 (arXiv:2508.00476) similarly uses RAG for QA but on meeting transcripts, not multilingual knowledge retrieval
- **Break condition:** If tokenization poorly handles the target script, embeddings will be degraded, retrieval quality drops, and injected context becomes less helpful—the authors explicitly note this limitation for Ukrainian.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** The entire architecture depends on parameter-efficient fine-tuning within the 3B parameter constraint. Understanding that LoRA adds small adapter matrices rather than modifying all weights is essential for grasping why this approach is feasible.
  - **Quick check question:** Given a 3B parameter model, why might LoRA allow adaptation where full fine-tuning would be impractical?

- **Concept: Positional/Selection Bias in LLM Multiple-Choice QA**
  - **Why needed here:** The paper's QA improvements partially come from averaging over answer permutations. Without understanding that models favor certain positions regardless of content, this technique appears unmotivated.
  - **Quick check question:** If an LLM always prefers option "A" regardless of question content, how would averaging predictions across shuffled option orders help?

- **Concept: Embedding Quality and Retrieval Effectiveness**
  - **Why needed here:** The limited RAG gains for Ukrainian are attributed to poor embeddings from suboptimal tokenization. Understanding the embedding→retrieval→generation chain explains why RAG helped less than expected.
  - **Quick check question:** Why would poor tokenization (e.g., over-splitting words) lead to worse semantic embeddings and thus worse retrieval?

## Architecture Onboarding

- **Component map:** Qwen2.5-3B-Instruct -> LoRA adapters (all projection layers) -> Training data pipeline (provided + synthetic + translated) -> RAG subsystem (ChromaDB indexes) -> Inference augmentation (ICL + permutation averaging + RAG)
- **Critical path:**
  1. Prepare language-specific training data (translate MCQs, create synthetic parallel data via back-translation)
  2. Stage-1 LoRA fine-tuning on combined MT + general QA data
  3. Stage-2 LoRA fine-tuning on in-domain QA + MT subset with learning rate search
  4. At inference: Apply few-shot retrieval for MT, permutation averaging for QA, RAG for Ukrainian QA

- **Design tradeoffs:**
  - Joint MT+QA training vs. separate models: Single model per language satisfies constraints but introduces task interference (QA accuracy slightly drops after LoRA vs. non-finetuned for Sorbian)
  - Two-stage vs. single-stage fine-tuning: S2 improves QA domain alignment but can slightly reduce MT performance (DSB drops 67.5→66.6 ChrF++)
  - RAG index granularity: Subject-specific indexes improve precision but require accurate subject classification

- **Failure signatures:**
  - Cyrillic tokenization over-splitting → poor embeddings → weak retrieval → RAG provides little benefit
  - Insufficient in-domain QA data → model underperforms on target question types
  - Aggressive learning rate in S2 → catastrophic forgetting of MT capability
  - Missing translation pairs for few-shot ICL → MT quality degrades at inference

- **First 3 experiments:**
  1. **Baseline replication:** Fine-tune Qwen2.5-3B-Instruct with LoRA on provided Sorbian MT data only (no QA, no augmentation) to establish task-specific baseline vs. joint training.
  2. **Ablation on permutation averaging:** Run QA evaluation with and without probability averaging (fixed random seed, single order) to quantify bias mitigation contribution.
  3. **Embedding quality probe:** For Ukrainian, compare retrieval recall@5 using Qwen embeddings vs. a multilingual embedding model (e.g., LaBSE) to isolate whether embedding quality limits RAG effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does replacing sentence-level training data with document-level or conversational data mitigate the performance ceiling observed in Ukrainian machine translation?
- Basis in paper: [explicit] The authors explicitly state that a "possible reason" for the lower improvement margins in Ukrainian MT is a "mismatch between the training, development, and test sets: we train our models on sentences, but the test set consists of large documents and lengthy conversations."
- Why unresolved: The submitted model was trained exclusively on sentence-level data retrieved from sources like OpenSubtitles, and the paper does not test document-aware training methods.
- What evidence would resolve it: A comparison of performance on the test set between the current model and a model fine-tuned on a dataset of Ukrainian conversational dialogues or documents.

### Open Question 2
- Question: Can specific data mixing ratios or loss-weighting strategies prevent the degradation of question-answering accuracy during joint fine-tuning with machine translation data?
- Basis in paper: [explicit] The authors note that LoRA fine-tuning "slightly reduces overall accuracy compared to the non-finetuned model, likely due to the trade-off introduced by joint MT and QA finetuning."
- Why unresolved: The paper implements a joint training