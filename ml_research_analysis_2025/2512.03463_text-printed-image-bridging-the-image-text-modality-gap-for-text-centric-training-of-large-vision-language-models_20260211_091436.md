---
ver: rpa2
title: 'Text-Printed Image: Bridging the Image-Text Modality Gap for Text-centric
  Training of Large Vision-Language Models'
arxiv_id: '2512.03463'
source_url: https://arxiv.org/abs/2512.03463
tags:
- image
- training
- text
- images
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Text-Printed Image (TPI) bridges the image-text modality gap in
  text-centric training of large vision-language models (LVLMs) by rendering textual
  descriptions as images, enabling effective learning without real images. Experiments
  across four LVLMs and seven benchmarks show TPI outperforms text-only and text-to-image
  training baselines, with average gains of up to 10 points and approaching ground-truth
  image performance.
---

# Text-Printed Image: Bridging the Image-Text Modality Gap for Text-centric Training of Large Vision-Language Models

## Quick Facts
- **arXiv ID:** 2512.03463
- **Source URL:** https://arxiv.org/abs/2512.03463
- **Reference count:** 40
- **Primary result:** TPI bridges the image-text modality gap in text-centric training of LVLMs by rendering textual descriptions as images, achieving up to 10-point gains over text-only training and approaching ground-truth image performance across seven VQA benchmarks.

## Executive Summary
This paper addresses the fundamental challenge of training large vision-language models (LVLMs) when only textual descriptions of images are available, rather than the images themselves. The proposed Text-Printed Image (TPI) method renders textual descriptions as images and processes them through the frozen vision encoder, effectively projecting text semantics into the visual embedding space. This simple approach bridges the image-text modality gap that prevents text-only training from improving visual reasoning performance. Experiments across four LVLMs and seven benchmarks demonstrate that TPI outperforms both text-only and text-to-image training baselines, achieving average gains of up to 10 points while being three orders of magnitude faster to generate than synthetic images.

## Method Summary
The method generates textual descriptions from ground-truth images using Qwen2.5-VL-32B, then renders these descriptions as 336×336 pixel RGB images with black text on white background using Pillow. These TPI images are processed through the frozen vision encoder of pre-trained LVLMs, with LoRA fine-tuning applied only to the LLM backbone. The approach leverages the existing OCR capabilities of the vision encoder to extract semantic information from rendered text, aligning the training distribution with inference-time visual distributions. Four LVLMs (LLaVA 1.5 7B/13B, Qwen2.5 VL 7B Instruct, LLaMA 3.2 11B Vision) are fine-tuned across seven VQA benchmarks using AdamW optimizer with r=256 LoRA parameters and α=512.

## Key Results
- TPI achieves up to 10-point improvements over text-only training baselines across seven VQA benchmarks
- TPI performance approaches ground-truth image training (e.g., 73.03% vs 74.46% on ScienceQA for LLaVA-1.5-7B)
- TPI is 1000× faster to generate than text-to-image models while preserving 90.8% of semantic relevance compared to 46.3% for T2I
- TPI mitigates representation drift in intermediate layers, maintaining higher CKA similarity to ground-truth image-trained models
- Data efficiency: TPI training with just 1% of the original dataset achieves 94% of the performance of using the full dataset

## Why This Works (Mechanism)

### Mechanism 1
Rendering text as an image projects textual semantics into the visual embedding space, bridging the modality gap by forcing text-derived features to occupy the same latent space as natural images through the frozen vision encoder.

### Mechanism 2
Direct text rendering preserves semantic fidelity better than generative image synthesis because deterministic rendering ensures 100% of semantic content is visually present, whereas T2I models introduce stochasticity and hallucinations that omit critical details.

### Mechanism 3
TPI mitigates representation drift in the LLM backbone compared to text-only training by providing inputs that reside in the visual manifold, preventing the LLM from adapting to the distinct statistical distribution of pure text embeddings.

## Foundational Learning

- **Concept: Image-Text Modality Gap**
  - Why needed here: This is the core problem TPI solves—vision and text embeddings occupy disjoint regions in latent space, preventing text-only training from improving visual reasoning
  - Quick check question: Why does training on raw text fail to improve VQA performance? (Answer: The model learns representations in the text manifold that don't transfer to the image manifold)

- **Concept: Vision Encoder (Frozen)**
  - Why needed here: TPI relies on the pre-trained visual pathway to extract features; learning happens only in the LLM/Projector, not the encoder
  - Quick check question: Does TPI require fine-tuning the Vision Encoder? (Answer: No, it leverages the existing encoder to process rendered text)

- **Concept: Semantic Fidelity vs. Visual Realism**
  - Why needed here: To justify TPI over T2I, distinguish between "looking like a photo" (T2I) and "containing the right information" (TPI)
  - Quick check question: Why would a blurry image of text train a better VQA model than a high-res synthetic photo that hallucinates objects? (Answer: Training depends on semantic consistency between image and QA, not photorealism)

## Architecture Onboarding

- **Component map:** Input Text → Renderer (Pillow/Python) → TPI Image (RGB) → Vision Encoder (CLIP/ViT) → Projector (MLP) → LLM Backbone (LoRA fine-tuned)
- **Critical path:** The Renderer—font size and canvas resolution (336×336px) must match the training resolution of the Vision Encoder to ensure OCR success
- **Design tradeoffs:**
  - TPI vs. Text-only: TPI aligns modality but requires OCR overhead; Text-only is fastest but suffers from modality gap
  - TPI vs. T2I: TPI is 1000× faster and semantically accurate but lacks visual textures; T2I offers visual diversity but is slow and semantically lossy
- **Failure signatures:**
  - High training loss, Low eval score: Vision Encoder cannot "read" the font (OCR failure)—try increasing font size
  - Performance drops on non-text VQA: TPI provides no shape/texture supervision—don't use for tasks requiring pure visual geometry
- **First 3 experiments:**
  1. Render Verification: Take 10 samples, render to images, manually verify text is legible and fits canvas
  2. Relevance Score Check: Compute Relevance Score for small batch using helper LVLM to ensure semantic preservation
  3. Gap Visualization: Train proxy model (1 epoch) and plot t-SNE of hidden states to confirm TPI clusters with Image embeddings

## Open Questions the Paper Calls Out

- **Open Question 1:** Can TPI enable LVLMs to learn entirely new visual concepts absent from pre-training data, or is its utility strictly limited to task-specific specialization? (The paper states TPI assumes a foundation model setting rather than teaching new visual concepts from scratch)

- **Open Question 2:** What is the minimum OCR capability required in a vision encoder for TPI to effectively bridge the modality gap? (Section 4.4.3 notes TPI requires minimum OCR ability and shows correlation with OCRBench scores, but doesn't isolate the specific performance threshold)

- **Open Question 3:** Does the factual correctness of rendered text contribute more to performance than descriptive density or length? (Appendix D shows increasing description length yields minimal gains while stronger generation models significantly improve performance, suggesting accuracy outweighs volume)

## Limitations
- TPI's effectiveness scales directly with the LVLM's OCR capabilities, creating a hard ceiling on achievable performance for weaker OCR models
- The computational efficiency claims are relative comparisons that don't account for full pipeline costs including LLM inference for text generation and evaluation
- TPI assumes text descriptions capture all necessary visual information, which may not hold for tasks requiring spatial reasoning or fine-grained visual details

## Confidence

- **High Confidence:** The fundamental mechanism of bridging modality gaps through visual embedding space alignment is well-supported by geometric analysis and controlled experiments
- **Medium Confidence:** The representation drift mitigation claims show consistent CKA similarity patterns but rely on proxy metrics that don't directly measure downstream task performance
- **Low Confidence:** The universal applicability claim across diverse VQA tasks assumes text descriptions capture all necessary visual information, which may not hold for certain visual reasoning tasks

## Next Checks

1. **OCR Capability Benchmark:** Systematically evaluate TPI performance across LVLMs with known OCR strength differences (e.g., GPT-4V vs. LLaVA-1.5) on identical datasets to quantify the relationship between OCR capability and TPI effectiveness

2. **Ground-Truth Upper Bound Analysis:** Train identical LVLMs on ground-truth images for each benchmark and compare absolute performance gaps between GT-Image, TPI, and Text-only conditions to establish realistic performance ceilings

3. **Cross-Domain Generalization Test:** Apply TPI training on a VQA dataset and evaluate on a structurally similar but semantically distinct dataset (e.g., train on ScienceQA, test on general VQA) to measure modality bridging persistence across domain shifts