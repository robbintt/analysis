---
ver: rpa2
title: Policy Regularized Distributionally Robust Markov Decision Processes with Linear
  Function Approximation
arxiv_id: '2510.14246'
source_url: https://arxiv.org/abs/2510.14246
tags:
- policy
- theorem
- linear
- have
- dr-rpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of decision-making under distribution
  shift in reinforcement learning, where training and deployment environments differ.
  This problem is formulated through robust Markov decision processes (RMDPs), which
  optimize performance against adversarial transition dynamics.
---

# Policy Regularized Distributionally Robust Markov Decision Processes with Linear Function Approximation

## Quick Facts
- arXiv ID: 2510.14246
- Source URL: https://arxiv.org/abs/2510.14246
- Reference count: 10
- The paper proposes DR-RPO, a model-free online policy optimization method that achieves polynomial suboptimality bounds and sample efficiency in robust RL.

## Executive Summary
This paper addresses distribution shift in reinforcement learning by developing a policy optimization algorithm for distributionally robust Markov decision processes (DRMDPs). The proposed DR-RPO method incorporates reference-policy regularization to maintain tractable softmax policies while optimizing against adversarial transition dynamics. Under the d-rectangular linear MDP assumption, the algorithm combines linear function approximation with upper confidence bonus exploration to achieve sublinear regret bounds.

The key innovation is demonstrating that policy optimization can match the theoretical performance of value-based approaches in robust RL settings. The method achieves an average suboptimality bound of Õ(d²H²/√K) while maintaining practical tractability through KL-regularized softmax policies and optimistic exploration.

## Method Summary
DR-RPO is a model-free online policy optimization algorithm for d-rectangular linear DRMDP and RRMDP settings. The method maintains a reference policy and performs KL-regularized softmax updates to ensure tractable optimization. It uses linear function approximation with ridge regression to estimate robust Q-functions and adds UCB bonuses for optimistic exploration. The algorithm operates in episodes, updating its policy through backward induction based on robust Bellman equations that incorporate both regularization and distributional robustness.

## Key Results
- DR-RPO achieves average suboptimality bound of Õ(d²H²/√K) under d-rectangular linear DRMDP and RRMDP settings
- Policy optimization matches the sample efficiency of value-based approaches in robust RL
- Empirical results show DR-RPO outperforms non-robust baselines (LSVI-UCB, OPPO) as distribution shift increases
- The method maintains robustness across diverse domains while preserving theoretical guarantees

## Why This Works (Mechanism)

### Mechanism 1: Policy Regularization with KL Divergence
- Claim: KL regularization constrains the policy to remain near a reference policy, ensuring tractable optimization and stochasticity.
- Mechanism: The algorithm incorporates a KL divergence penalty into the robust value function, modifying the optimal policy from deterministic to softmax form: π*(a|s) ∝ π_ref(a|s)exp(ηQ̃(s,a)).
- Core assumption: Access to a valid reference policy π_ref and bounded feature mapping.
- Evidence anchors: [abstract] "...DR-RPO incorporates reference-policy regularization..."; [section 3.3] Defines robust policy-regularized value function.
- Break condition: If η is too high without sufficient data, the policy may deviate too aggressively from π_ref, reducing safety benefits.

### Mechanism 2: Linear Function Approximation with UCB Bonus
- Claim: Linear function approximation with UCB bonus enables sample-efficient exploration in large state-action spaces.
- Mechanism: Maintains Gram matrix Λ of visited features, estimates robust Q-function via ridge regression, adds bonus term Γ(s,a) = βΣᵢφᵢ(s,a)√(1ᵢᵀΛ⁻¹1ᵢ) for optimistic exploration.
- Core assumption: Environment satisfies d-rectangular linear MDP assumption with linear transitions and rewards.
- Evidence anchors: [abstract] "...adopts the d-rectangular linear MDP formulation..."; [algorithm 1] Details Gram matrix update and bonus calculation.
- Break condition: If covariance matrix Λ is rank-deficient, the inverse Λ⁻¹ becomes unstable, causing exploration to fail.

### Mechanism 3: Dual Formulation of Robust Bellman Equation
- Claim: Dual formulation transforms intractable infimum over transitions into tractable linear optimization.
- Mechanism: Under d-rectangular assumption and TV divergence, rewrites worst-case transition operator into linear form ⟨φ(s,a), θ + ν⟩, where ν captures robust penalty.
- Core assumption: Fail-state assumption (Assumption 4.2) required to simplify dual optimization to closed-form solution.
- Evidence anchors: [section 4.1] "Proposition 4.1... has a dual formulation..."; [proposition 4.4] Shows linear form of robust Q-function.
- Break condition: Without fail-state assumption, the simplified dual form is invalid and requires explicit optimization of min_s'[V(s')].

## Foundational Learning

**Concept: d-Rectangular Linear MDP**
- Why needed here: Structural backbone enabling linear regret bounds and function approximation guarantees
- Quick check question: Can you identify the feature mapping φ(s,a) and verify that P(s'|s,a) is linear in φ?

**Concept: Total Variation (TV) Divergence**
- Why needed here: Defines robustness using TV divergence to constrain uncertainty set
- Quick check question: How does TV divergence differ from KL divergence in terms of uncertainty set shape?

**Concept: Fail-State Assumption**
- Why needed here: Critical simplification step in proofs, allows complex dual optimization to collapse into simple linear term
- Quick check question: Does your environment contain a terminal state with zero reward reachable from all states?

## Architecture Onboarding

**Component map:**
Feature Encoder → Statistics Buffer (Λ, reward sums) → Robust Q-Estimator (ridge regression) → Bonus Calculator → Softmax Policy Head

**Critical path:**
1. Rollout: Agent acts via Softmax Policy Head
2. Update: Collect trajectory; update Gram Matrix Λ
3. Regress: Backward pass to compute ν and update Q-Estimator parameters
4. Bonus: Recalculate exploration bonus Γ
5. Policy Step: Update Softmax weights using new Q̃ + Γ

**Design tradeoffs:**
- η (Regularization Strength): High η allows rapid deviation from π_ref (higher reward potential) but risks instability; low η prioritizes safety
- ρ vs. σ: Use ρ (DRMDP) for hard constraints on uncertainty; use σ (RRMDP) for soft penalties if uncertainty level is ambiguous

**Failure signatures:**
- Collapse of Exploration: Bonus Γ too small or η too low → agent sticks to π_ref and never explores
- Numerical Instability: Near-singular Λ → bonus Γ explodes, causing Q-values to overflow

**First 3 experiments:**
1. Linear MDP Validation: Implement "Simulated Linear MDP" with low perturbation level q to verify algorithm recovers optimal policy under minimal noise
2. Perturbation Stress Test: Sweep perturbation level q to confirm DR-RPO outperforms non-robust baselines as distribution shift increases
3. Reference Policy Ablation: Vary π_ref (Uniform vs. Target-Optimal) to validate DR-RPO optimizes "on top of" reference without being strictly bound by its suboptimality

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can DR-RPO be adapted to remove the dependency on the fail-state assumption while maintaining tractable optimization and theoretical guarantees?
- Basis in paper: [inferred] Paper introduces fail-state assumption to simplify dual formulation, noting it avoids computational expense but may restrict environment class
- Why unresolved: Current theoretical derivation relies on this assumption to zero-out specific terms in dual formulation
- What evidence would resolve it: Modified DR-RPO algorithm with proof of suboptimality bounds not invoking Assumption 4.2

**Open Question 2**
- Question: Does the theoretical framework extend to general f-divergences beyond Total Variation divergence?
- Basis in paper: [inferred] Definitions of uncertainty set and dual formulation explicitly constructed using TV divergence
- Why unresolved: Tractable closed-form solution depends heavily on specific dual form of TV divergence
- What evidence would resolve it: Derivation of robust Bellman operator and regret bounds for DR-RPO under general f-divergence constraint

**Open Question 3**
- Question: Is the derived suboptimality bound of Õ(d²H²/√K) minimax optimal for policy optimization in d-rectangular linear RMDP setting?
- Basis in paper: [inferred] Paper provides upper bound matching value-based methods but does not provide lower bound
- Why unresolved: Fundamental limits of sample efficiency for robust policy optimization with linear function approximation remain unproven
- What evidence would resolve it: Theoretical proof establishing lower bound of Ω(d²H²/√K) for any online policy optimization algorithm in this setting

## Limitations
- Theoretical guarantees depend heavily on d-rectangular linear MDP and fail-state assumptions that may not hold in practical environments
- Numerical constants in bonus term and exact dual optimization implementation are not fully specified
- Empirical validation limited to synthetic environments without real-world applications
- Assumes access to feature mapping satisfying linear MDP structure, requiring domain knowledge for construction

## Confidence

**High confidence**: Policy regularization constraining optimization within tractable softmax class is well-established in RL literature

**Medium confidence**: Sublinear regret bounds under d-rectangular linear MDP assumptions are theoretically sound but rely on idealized conditions

**Low confidence**: Practical applicability of fail-state assumption and performance in non-synthetic domains remain uncertain without further empirical validation

## Next Checks

1. Test DR-RPO on continuous control benchmarks (e.g., MuJoCo tasks) with varying levels of transition noise to evaluate robustness beyond linear synthetic environments

2. Conduct ablation studies on reference policy (uniform vs. learned) and regularization strength η to quantify their impact on both performance and safety

3. Verify dual optimization implementation by comparing against ground-truth robust Q-function in small, fully-observable environment where exact computation is feasible