---
ver: rpa2
title: Scaling Laws for Speculative Decoding
arxiv_id: '2505.07858'
source_url: https://arxiv.org/abs/2505.07858
tags:
- decoding
- scaling
- rate
- draft
- acceptance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes log-linear scaling laws for speculative
  decoding, demonstrating that acceptance rate improves predictably with increased
  pretraining tokens, draft model capacity, and batch size. By scaling pretraining
  to 10B tokens, expanding draft model decoders, and optimizing batch size-dependent
  top-k path selection, the proposed Scylla method achieves 1.5-2.2 higher acceptance
  rates than EAGLE2 and 0.3 higher than EAGLE3 at temperature T=0.
---

# Scaling Laws for Speculative Decoding

## Quick Facts
- arXiv ID: 2505.07858
- Source URL: https://arxiv.org/abs/2505.07858
- Reference count: 40
- Primary result: Log-linear scaling laws for speculative decoding acceptance rates with pretraining tokens, draft capacity, and batch size

## Executive Summary
This paper establishes three log-linear scaling laws for speculative decoding that predictably improve token acceptance rates through pretraining, draft model capacity, and hardware-aware optimization. By pretraining draft models to 10B tokens, expanding decoder layers, and dynamically adjusting speculative tree width based on batch size, the Scylla method achieves 1.5-2.2× higher acceptance rates than EAGLE2 and 0.3× higher than EAGLE3 at temperature T=0. Industrial deployments demonstrate 2× decoding throughput improvements through hardware-aware batch optimization that prevents performance collapse at scale.

## Method Summary
The Scylla method employs a two-stage training pipeline: first pretraining the draft model on large-scale web data (CC 80%, C4 15%, GitHub 5%), then fine-tuning on ShareGPT-68K for two epochs. The draft model uses a modified EAGLE architecture with embedding/output normalization and scaled transformers (1-10 decoder layers). Key innovations include adaptive top-k tree selection that inversely scales with batch size to prevent compute saturation, and comprehensive benchmarking across MT-bench, HumanEval, GSM8K, Alpaca, CNN/DM, and Natural Questions datasets.

## Key Results
- Acceptance rates improve log-linearly with pretraining tokens: scaling from 1B to 10B tokens increases acceptance by 0.8-1.2×
- Draft model capacity scaling follows log-linear pattern: increasing decoder layers from 1 to 5 improves acceptance by 0.4-0.6×
- Hardware-aware top-k optimization prevents throughput collapse at large batch sizes, achieving 2× improvements over EAGLE2
- Scylla achieves 1.5-2.2× higher acceptance rates than EAGLE2 and 0.3× higher than EAGLE3 at temperature T=0

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Transfer via Pretraining
- **Claim:** Increasing pretraining tokens improves token acceptance rates log-linearly by aligning draft model knowledge with the target LLM.
- **Core assumption:** Acceptance rate is bottlenecked by draft model's lack of foundational knowledge rather than architectural capacity.
- **Evidence:** Theorem 1.1 shows acceptance rate = α·log₁₀(T_pretrain) + β; scaling pretraining data requires only 4.5× training iterations vs EAGLE3's 8×.
- **Break condition:** Performance saturates between 50-100B tokens with diminishing returns.

### Mechanism 2: Draft Capacity Scaling
- **Claim:** Increasing draft model depth improves acceptance rates log-linearly.
- **Core assumption:** Additional latency from deeper draft models is offset by reduced target verification steps.
- **Evidence:** Theorem 1.2 shows acceptance rate = α·log₁₀(D) + β; Figure 7 demonstrates quasi-linear progression on log-scale.
- **Break condition:** Draft latency approaches target model verification time, decreasing net throughput.

### Mechanism 3: Hardware-Aware Dynamic Tree Selection
- **Claim:** Optimal throughput requires dynamically reducing top-k paths with increasing batch size.
- **Core assumption:** Inference engine operates near theoretical roofline where memory bandwidth and compute peaks are distinct constraints.
- **Evidence:** Theorem 1.3 defines inverse square root scaling for optimal top-k; Figure 5 shows throughput collapse at batch=64 when top-k > 16.
- **Break condition:** Communication overhead dominates, invalidating roofline model predictions.

## Foundational Learning

- **Concept: Speculative Decoding (Draft-Verify Loop)**
  - **Why needed:** Optimizes draft model and verification path; understanding draft speculates multiple tokens verified in single target forward pass is essential.
  - **Quick check:** Why does verifying 10 draft tokens in one target forward pass usually cost less than generating 1 token autoregressively?

- **Concept: The Roofline Model**
  - **Why needed:** Section 3.2 uses this to explain performance collapse at large batch sizes.
  - **Quick check:** In a memory-bound operation, does increasing arithmetic intensity typically increase or decrease throughput?

- **Concept: Log-Linear Scaling**
  - **Why needed:** Paper claims acceptance rates scale with log(tokens) rather than linearly.
  - **Quick check:** If acceptance rate scales log-linearly with pretrain tokens, what happens to "return on investment" as we add more data?

## Architecture Onboarding

- **Component map:** Token Embeddings E(T) + Target Model Hidden States H → Linear Projection (2h→h) → Scaled Transformer Decoders → Target Model LM Head → Verification with attention mask
- **Critical path:** Draft Phase: Input projection → Draft Decoders → Sample Top-K paths → Verify Phase: Target Model Forward Pass (masked parallel) → Accept/Reject
- **Design tradeoffs:** Depth vs. Speed (deeper models increase acceptance but add latency); Batch vs. Tree Width (cannot scale batch without shrinking top-k to stay on compute roofline)
- **Failure signatures:** Throughput Collapse (batch=64 with high top-k causes drastic throughput drop); Stagnation (acceptance rates stop improving beyond 50B tokens)
- **First 3 experiments:**
  1. Validate Pretraining Law: Train draft models with 1B vs 10B tokens, plot acceptance rates to verify log-linear slope α ≈ 0.08
  2. Batch Collapse Test: Run inference at Batch=32 vs Batch=64 with fixed top-k=40, observe throughput drop, then apply Equation 4 to adjust top-k and confirm recovery
  3. Capacity Ablation: Compare 1-layer vs 3-layer draft models on same target, ensure latency increase is less than time saved by higher acceptance rate

## Open Questions the Paper Calls Out

- **Question:** Does incorporating Reinforcement Learning from Human Feedback (RLHF) into draft model training improve acceptance rates?
  - **Basis:** Introduction states "The impact of RLHF on draft accuracy presents an intriguing open question" and lists investigating RL fine-tuning as future work
  - **Why unresolved:** Study used Pretraining → SFT paradigm to isolate scaling laws, excluding RLHF stage
  - **Evidence needed:** Empirical measurements of acceptance rate changes when Scylla draft model undergoes RLHF training vs SFT-only baseline

- **Question:** Do scaling laws apply to Mixture-of-Experts (MoE) backbone architectures?
  - **Basis:** Discussion notes work "primarily concentrates on dense LLM architectures, leaving draft model optimization for MoE LLMs as an open research direction"
  - **Why unresolved:** MoE models use sparse activation and expert routing that may fundamentally alter computational intensity assumptions
  - **Evidence needed:** Validation of Theorems 1.1-1.3 on MoE models (DeepSeek-V3 or Mixtral) to check if log-linear relationships persist

- **Question:** Are scaling laws universal across different speculative decoding architectures?
  - **Basis:** Authors state scaling laws "are verified only in Transformer-based Scylla, more experiments on other speculative decoding methods are left for the future"
  - **Why unresolved:** Competing methods (Medusa, n-gram based) use different structural approaches for drafting
  - **Evidence needed:** Applying same scaling variables to alternative draft architectures to determine if they yield similar log-linear improvements

## Limitations

- Architectural Detail Ambiguity: Draft model input processing is underspecified, particularly the exact mechanism for merging token embeddings with target hidden states
- Hyperparameter Transfer: SFT stage hyperparameters are stated as "identical to EAGLE2" without documentation of learning rates, batch sizes, or optimizer settings
- Hardware-Dependent Optimization: Theorem 1.3's inverse square root top-k scaling assumes specific roofline profile that likely varies with different accelerator architectures

## Confidence

**High Confidence:** Pretraining scaling law (Theorem 1.1) and acceptance rate measurements are well-supported with clear demonstrations across multiple experiments and standard, reproducible methodology.

**Medium Confidence:** Draft capacity scaling (Theorem 1.2) shows clear trends but lacks isolated architectural ablation studies to confirm decoder depth specifically drives improvements over other parameters.

**Low Confidence:** Hardware-aware tree selection (Theorem 1.3) is most architecture-dependent claim; while inverse square root relationship is mathematically derived, critical threshold I_crit and roofline model parameters are not specified.

## Next Checks

1. **Architecture Fidelity Test:** Implement two draft models with identical pretraining data (10B tokens) but different input processing methods—one matching described concatenation+projection, another using simpler concatenation without projection. Compare acceptance rates to isolate architectural contribution.

2. **Hardware Profile Mapping:** For different GPU architecture (A100), measure memory-bandwidth and compute peaks, recalculate optimal top-k scaling using Eq. 5, and validate whether inverse square root relationship holds or requires adjustment.

3. **Pretraining Saturation Point:** Extend pretraining beyond 100B tokens to empirically identify saturation threshold. Train draft models at 50B, 100B, and 200B tokens with identical SFT procedures, then measure acceptance rate deltas to confirm diminishing returns observation.