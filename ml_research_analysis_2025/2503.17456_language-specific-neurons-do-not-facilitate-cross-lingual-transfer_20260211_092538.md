---
ver: rpa2
title: Language-specific Neurons Do Not Facilitate Cross-Lingual Transfer
arxiv_id: '2503.17456'
source_url: https://arxiv.org/abs/2503.17456
tags:
- language
- neurons
- languages
- neuron
- language-specific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether language-specific neurons identified
  in multilingual LLMs can be leveraged to improve cross-lingual transfer performance
  for low-resource languages. The authors evaluate test-time interventions and LoRA
  fine-tuning targeting language-specific neurons using methods like Language Activation
  Probability Entropy (LAPE) and activation probability-based thresholding.
---

# Language-specific Neurons Do Not Facilitate Cross-Lingual Transfer

## Quick Facts
- arXiv ID: 2503.17456
- Source URL: https://arxiv.org/abs/2503.17456
- Reference count: 19
- This paper finds that targeting language-specific neurons via test-time interventions or LoRA fine-tuning fails to improve cross-lingual transfer performance.

## Executive Summary
This paper investigates whether language-specific neurons in multilingual LLMs can be leveraged to improve cross-lingual transfer for low-resource languages. Despite prior work showing these neurons are important for generation tasks, the authors find that manipulating them—either through test-time interventions or fine-tuning—fails to yield meaningful improvements on XNLI and XQuAD benchmarks. The key insight is that these neurons are polysemantic, encoding both language-specific and task-relevant information, which limits the effectiveness of targeted adjustments.

## Method Summary
The authors use two neuron identification methods: Language Activation Probability Entropy (LAPE) and Activation Probability 90p, which rank neurons based on language-specific activation patterns in MLP layers. They evaluate test-time interventions that overwrite neuron activations with statistical aggregates (mean, P90, P10, zero) computed from target-language Wikipedia data during inference. They also employ masked LoRA fine-tuning that restricts parameter updates to identified language-specific neurons. Performance is measured on zero-shot cross-lingual transfer tasks (XNLI for classification, XQuAD for QA) after English fine-tuning.

## Key Results
- Test-time interventions on language-specific neurons fail to improve zero-shot transfer performance and often disrupt task-specific information
- LoRA fine-tuning of identified language-specific neurons produces no improvement compared to random neuron fine-tuning
- Setting neuron activations to zero does not significantly degrade performance, suggesting zero is not an effective deactivation signal
- Neurons identified by LAPE and Activation Probability 90p methods largely overlap, indicating consistent identification of polysemantic neurons

## Why This Works (Mechanism)

### Mechanism 1: Language Activation Probability Entropy (LAPE) for Neuron Identification
- Claim: Neurons with low LAPE values are language-specific because they exhibit high activation probabilities for only a limited subset of languages
- Mechanism: Computes entropy over normalized activation probabilities P^l_{i,j} across k languages; neurons with low entropy activate selectively for specific languages
- Core assumption: Language-specific features localize to identifiable neuron subsets in MLP layers
- Evidence anchors:
  - [abstract]: "existing language-specific neuron identification techniques (such as Language Activation Probability Entropy...)"
  - [section 2.1]: Formal LAPE definition shows entropy-based selection criterion
  - [corpus]: Related work (Tang et al. 2024, Kojima et al. 2024) shows similar identification methods affect generation tasks
- Break condition: Identified neurons are polysemantic—encoding both language AND task information, limiting isolation effectiveness

### Mechanism 2: Test-Time Activation Intervention
- Claim: Modifying activations of target-language neurons during inference can amplify cross-lingual transfer signals
- Mechanism: Replace neuron activations with statistical aggregates (mean μ, P90, P10, zero) computed from target language Wikipedia corpus during forward pass
- Core assumption: Overwriting activations with target-language statistics will steer representations toward better cross-lingual task performance
- Evidence anchors:
  - [abstract]: "manipulating these neurons—either through test-time interventions or fine-tuning—fails to yield meaningful improvements"
  - [section 4.2]: "test-time interventions fail to consistently improve zero-shot transfer performance... they often disrupt the task-specific information encoded in the activations"
  - [corpus]: Weak—neighbor papers show mixed results on amplification effects
- Break condition: Polysemantic neurons mean interventions destroy task-relevant computations co-encoded with language features

### Mechanism 3: Masked LoRA for Neuron-Specific Fine-Tuning
- Claim: Restricting LoRA updates to language-specific neurons via binary mask M enables targeted language enhancement without full model fine-tuning
- Mechanism: Apply ∆W = M ⊗ (BA) where M_{i,j}=1 only for language-specific neurons; attention layers and classification head also fine-tuned
- Core assumption: Targeted parameter updates can enhance language representations without disrupting shared task knowledge
- Evidence anchors:
  - [section 2.2]: Formal masked LoRA formulation with element-wise multiplication constraint
  - [section 4.3]: "fine-tuned the identified language-specific neurons using LoRA but observed no improvement in performance"
  - [corpus]: LANDeRMT (Zhu et al. 2024) shows neuron routing helps translation, but this doesn't transfer to NLI/QA tasks
- Break condition: Random neuron fine-tuning produces equivalent results, suggesting attention-layer LoRA already saturates task-specific gains

## Foundational Learning

- **Polysemantic Neurons**
  - Why needed here: Central explanation for why targeted interventions fail—neurons don't encode single features
  - Quick check question: If neuron (i,j) activates for both Hindi text AND entailment reasoning, what happens when you overwrite its activation with Hindi-only statistics?

- **Cross-Lingual Zero-Shot Transfer**
  - Why needed here: Evaluation paradigm—train on source language (English), evaluate on target languages without target-language task data
  - Quick check question: Your model achieves 80.5% on English XNLI but 66.8% on Urdu. What intervention strategy would meaningfully close this gap?

- **Activation Statistics vs. True Deactivation**
  - Why needed here: Paper shows setting activations to zero doesn't degrade performance, challenging prior assumptions about "deactivation"
  - Quick check question: If zero-activation doesn't hurt task performance but P10-activation causes severe degradation, what does this imply about the information carried in activation magnitude versus sign?

## Architecture Onboarding

- **Component map:**
  - MLP feedforward layers: Location of identified language-specific neurons (L layers × df hidden dim)
  - Attention layers: Fine-tuned alongside neuron-specific LoRA (not targeted for neuron identification)
  - LoRA adapter: Low-rank BA matrices masked by M to restrict updates to language-specific positions
  - Classification head: Task-specific output layer, always fine-tuned

- **Critical path:**
  1. Compute activation statistics (mean, P90) per neuron per language from Wikipedia corpus
  2. Rank neurons via LAPE (entropy-based) or Activation Probability 90p (percentile-based)
  3. Select top-m neurons as language-specific for target language
  4. Apply test-time intervention OR fine-tune with masked LoRA
  5. Evaluate on XNLI (accuracy) or XQuAD (EM/F1) in target language

- **Design tradeoffs:**
  - LAPE: Language-set dependent—requires specifying comparison language set L (Set1 vs Set6 affect results)
  - Act Prob 90p: Language-set independent but may capture different neuron populations (largely disjoint from LAPE selections per Figure 18-19)
  - Zero vs. low-percentile deactivation: Zero is computationally simple but not true deactivation; P10 causes severe performance drops

- **Failure signatures:**
  - Intervention improves perplexity but not task accuracy (language generation ≠ task reasoning)
  - LAPE and Act Prob 90p identify non-overlapping neurons yet both fail to improve transfer
  - Fine-tuning random neurons matches language-specific neuron fine-tuning performance

- **First 3 experiments:**
  1. **Baseline zero-shot**: Fine-tune on English XNLI/XQuAD, evaluate on vi/hi/ur/zh without any intervention to establish reference performance
  2. **Test-time ablation**: Apply Int μ, Int P90, Int 0, Int P10 separately to compare which statistical aggregates cause degradation vs. neutrality
  3. **Identification method comparison**: Run LAPE on Set1 vs. Set6 vs. Act Prob 90p, measure neuron overlap (Jaccard similarity) and correlate with any performance differences to validate polysemantic hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can attention-layer neurons be targeted to improve cross-lingual transfer, or do they exhibit similar polysemantic limitations as MLP neurons?
- Basis in paper: [explicit] The authors state in Limitations that they "focus on language-specific neurons in the MLP layers of multilingual LLMs, excluding attention mechanisms, which may also play a significant role."
- Why unresolved: Prior work has shown language-specific representations exist in feedforward networks, but attention mechanisms remain unexplored for targeted cross-lingual interventions.
- What evidence would resolve it: Experiments applying the same LAPE and Activation Probability 90p identification methods to attention heads, followed by test-time interventions and fine-tuning on the same XNLI/XQuAD benchmarks.

### Open Question 2
- Question: What activation values constitute true deactivation of language-specific neurons, if setting to zero is ineffective?
- Basis in paper: [inferred] The authors find that "setting activations to zero does not significantly degrade performance, suggesting zero is not a true indicator of deactivation," while lower percentiles (P10) cause clear drops.
- Why unresolved: The relationship between activation magnitude, deactivation effectiveness, and task performance remains unclear; zero may be within the normal operating range for these neurons.
- What evidence would resolve it: Systematic sweep of replacement values (including negative activations) measuring both perplexity changes and downstream task performance to identify thresholds where deactivation meaningfully impacts behavior.

### Open Question 3
- Question: Can methods be developed to disentangle the polysemantic representations in language-specific neurons to enable more effective targeted interventions?
- Basis in paper: [inferred] The authors hypothesize that "the polysemantic nature of neuron activations limits the effectiveness of targeted adjustments," as neurons encode both language-specific and task-relevant information.
- Why unresolved: Overwriting activations with statistical aggregates disrupts task-relevant computations; current neuron identification methods cannot separate language-only features from mixed representations.
- What evidence would resolve it: Development and evaluation of decomposition techniques (e.g., sparse autoencoders, disentangled representation learning) that isolate language-specific dimensions within neurons before applying interventions.

## Limitations

- The study focuses on MLP layer neurons but doesn't investigate attention mechanisms or embeddings, which may contain complementary language information
- Results are limited to zero-shot transfer scenarios and may not generalize to few-shot or full fine-tuning paradigms with target-language supervision
- The computational scope is constrained to specific identification methods (LAPE, Activation Probability 90p) that may not capture all aspects of language-specific representations

## Confidence

**High Confidence**: The core finding that language-specific neurons identified by LAPE/activation probability methods are polysemantic and encode task information beyond language identity. The experimental evidence (zero degradation when setting activations to zero, failure of both test-time interventions and LoRA fine-tuning) is robust across multiple benchmarks and languages.

**Medium Confidence**: The broader claim that language-specific neurons cannot facilitate cross-lingual transfer. While convincingly demonstrated for zero-shot scenarios on XNLI/XQuAD, the results may not generalize to other task types (generation, translation) or fine-tuning paradigms where target-language supervision exists.

**Low Confidence**: The interpretation that zero is not a "true deactivation" signal. While the empirical observation is sound, the theoretical explanation for why P10 causes severe degradation while zero does not could benefit from deeper investigation into activation distribution properties.

## Next Checks

1. **Neuron Functional Ablation Study**: Systematically ablate (zero out) identified language-specific neurons during both training and inference to determine whether task performance degrades when these neurons are never available, versus when they're available but overwritten during inference.

2. **Cross-Architecture Generalization**: Replicate the experiments on different multilingual model architectures (e.g., BLOOM, OPT-175B, LLaMA variants) to verify that the polysemantic nature of language-specific neurons is a general phenomenon rather than model-specific.

3. **Alternative Language Identification Methods**: Compare LAPE/activation probability methods against newer neuron identification techniques that explicitly account for polysemanticity (e.g., sparse coding approaches, subspace clustering) to determine if more sophisticated identification could yield neurons that better isolate language features from task features.