---
ver: rpa2
title: 'ReAcTree: Hierarchical LLM Agent Trees with Control Flow for Long-Horizon
  Task Planning'
arxiv_id: '2511.02424'
source_url: https://arxiv.org/abs/2511.02424
tags:
- kitchen
- table
- agent
- wine
- cabinet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of long-horizon task planning
  in embodied autonomous agents using large language models (LLMs). The proposed method,
  ReAcTree, introduces a hierarchical planning framework that dynamically constructs
  an agent tree in the subgoal space, where each agent node independently handles
  a subgoal while control flow nodes coordinate their execution.
---

# ReAcTree: Hierarchical LLM Agent Trees with Control Flow for Long-Horizon Task Planning

## Quick Facts
- arXiv ID: 2511.02424
- Source URL: https://arxiv.org/abs/2511.02424
- Reference count: 40
- Primary result: 61% goal success rate on WAH-NL with Qwen 2.5 72B, nearly doubling ReAct's 31%

## Executive Summary
This paper addresses the challenge of long-horizon task planning for embodied autonomous agents using large language models. The proposed ReAcTree framework introduces a hierarchical planning approach that dynamically constructs agent trees in the subgoal space, where each node independently handles a subgoal while control flow nodes coordinate execution. The method integrates two complementary memory systems—episodic memory for in-context learning and working memory for information sharing—to enhance planning robustness. Experiments demonstrate that ReAcTree consistently outperforms strong baselines like ReAct across diverse LLMs and datasets, achieving state-of-the-art performance while remaining effective even with smaller models.

## Method Summary
ReAcTree is a hierarchical LLM agent framework that dynamically constructs an agent tree in the subgoal space to tackle long-horizon tasks. Each agent node can independently handle a subgoal, while control flow nodes (Sequence, Fallback, Parallel) coordinate the execution of child nodes. The framework incorporates two memory systems: episodic memory for in-context learning through Sentence-BERT retrieval of subgoal-level trajectories (capped at 5K tokens), and working memory for sharing object location information across the tree via a shared Python dictionary. The system uses constrained generation through the Guidance library with temperature set to 0.0, and was evaluated on WAH-NL and ALFRED datasets using Qwen 2.5 and LLaMA 3.1 models.

## Key Results
- Achieved 61% goal success rate on WAH-NL with Qwen 2.5 72B, nearly doubling ReAct's 31%
- Even smaller Qwen 2.5 7B model achieved 37% success, demonstrating framework efficiency
- Showed consistent improvement over ReAct across all tested LLMs and datasets
- Ablation studies confirmed the importance of both hierarchical decomposition and dual memory systems

## Why This Works (Mechanism)
The hierarchical structure enables decomposition of complex tasks into manageable subgoals while maintaining coordination through control flow nodes. The episodic memory provides relevant context from past experiences, enabling in-context learning without retraining. Working memory allows agents to share critical information like object locations across the tree, reducing redundant exploration. The combination of independent subgoal handling with coordinated execution through control flow nodes allows the system to adapt to partial observability and complex task dependencies.

## Foundational Learning
- **Hierarchical Task Decomposition**: Breaking long-horizon tasks into subgoals reduces cognitive load on the LLM and enables parallel planning. Why needed: LLMs struggle with maintaining coherent plans across hundreds of steps. Quick check: Verify that subgoals are meaningful units that can be executed independently.
- **Control Flow Coordination**: Sequence, Fallback, and Parallel nodes orchestrate subgoal execution. Why needed: Complex tasks require conditional execution and fallback strategies. Quick check: Ensure control flow nodes correctly handle success/failure signals from children.
- **Dual Memory Architecture**: Episodic memory for learning from past experiences, working memory for real-time information sharing. Why needed: Single memory systems are insufficient for both learning and coordination. Quick check: Verify episodic memory retrieval and working memory updates during execution.
- **In-Context Learning via Retrieval**: Using Sentence-BERT to retrieve relevant trajectories for few-shot learning. Why needed: Fine-tuning is expensive and inflexible. Quick check: Measure retrieval accuracy and its impact on planning quality.
- **Constrained Generation**: Using Guidance library with temperature=0.0 for deterministic outputs. Why needed: Reduces hallucination and ensures reproducibility. Quick check: Verify that outputs adhere to the expected format.

## Architecture Onboarding

**Component Map**: User Input -> Task Analysis -> Hierarchical Tree Construction -> Control Flow Execution -> Environment Interaction -> Memory Update

**Critical Path**: The core execution loop involves constructing the agent tree, traversing control flow nodes, executing agent nodes with memory support, and updating memories based on outcomes.

**Design Tradeoffs**: The framework trades computational overhead (maintaining tree structures and dual memories) for improved planning quality and robustness. The use of 5K token limit on episodic memory balances context richness with inference speed.

**Failure Signatures**: Common failures include search loops ("SameRm"), plan failures from missing steps ("PlanFail"), object confusion ("Execution"), and failure to revise incorrect subgoals. These typically manifest as infinite loops, skipped required actions, or inability to adapt to unexpected situations.

**First Experiments**:
1. Run ReAct baseline on WAH-NL with Qwen 2.5 7B to establish baseline performance
2. Implement and test the hierarchical tree construction with simple control flow (Sequence only)
3. Add episodic memory with manually collected seed trajectories and measure impact on success rate

## Open Questions the Paper Calls Out
- **Subgoal Revision**: How can the framework be extended to dynamically revise incorrectly expanded subgoals during execution? Currently, if the LLM decomposes a goal incorrectly, the hierarchical structure commits to that path without self-correction.
- **Clarification Dialogues**: Can interactive clarification dialogues be integrated to handle instruction ambiguity? The system currently executes vague commands without querying for specifics.
- **Hallucination Mitigation**: How can the framework effectively mitigate LLM hallucinations during task planning? Object confusion and skipped targets remain persistent issues despite correct subgoal decomposition.

## Limitations
- High sensitivity to initial episodic memory content, with success rates heavily dependent on representative seed trajectories
- Performance on ALFRED (21-38% GSR) lags significantly behind WAH-NL (37-61% GSR), indicating domain-specific challenges
- Persistent issues with search strategy (getting stuck in loops) and object instance confusion that limit practical deployment

## Confidence
- **High confidence** in the core architectural contribution and its validity as a planning framework
- **Medium confidence** in reported performance metrics due to potential hardware constraints for running 72B models
- **Low confidence** in the exact seed trajectories used for episodic memory bootstrapping, which are critical for few-shot performance

## Next Checks
1. **Memory initialization validation**: Reproduce the episodic memory with the exact seed trajectories specified in Appendix E to verify the few-shot learning contribution
2. **Hardware scaling experiment**: Run Qwen 2.5 7B on both datasets to confirm that the 4× performance gap between 7B and 72B models holds across implementations
3. **Failure mode isolation**: Instrument the agent to log search patterns and object instance references to quantify the contribution of search failures vs. hallucination errors to overall performance degradation