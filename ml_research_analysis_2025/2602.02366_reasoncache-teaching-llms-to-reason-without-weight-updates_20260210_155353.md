---
ver: rpa2
title: 'ReasonCACHE: Teaching LLMs To Reason Without Weight Updates'
arxiv_id: '2602.02366'
source_url: https://arxiv.org/abs/2602.02366
tags:
- prefix
- arxiv
- reasoning
- learning
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReasonCACHE, a method that enables large
  language models to learn reasoning skills through in-context learning without any
  weight updates. By distilling demonstrations into a compact, learnable key-value
  cache using prefix tuning, ReasonCACHE overcomes the scaling limits of standard
  in-context learning, such as quadratic attention costs and degraded performance
  with longer contexts.
---

# ReasonCACHE: Teaching LLMs To Reason Without Weight Updates

## Quick Facts
- arXiv ID: 2602.02366
- Source URL: https://arxiv.org/abs/2602.02366
- Reference count: 36
- Key result: Enables LLMs to learn reasoning skills without weight updates using a learnable key-value cache that outperforms LoRA and fine-tuning on reasoning benchmarks

## Executive Summary
ReasonCACHE introduces a method for teaching large language models reasoning skills through in-context learning without any weight updates. The approach addresses fundamental scaling limitations of standard in-context learning by distilling demonstrations into a compact, learnable key-value cache using prefix tuning. This enables LLMs to acquire reasoning capabilities efficiently while avoiding the quadratic attention costs and degraded performance associated with longer contexts in traditional approaches.

The method achieves state-of-the-art performance on challenging reasoning benchmarks including GPQA-Diamond, GSM8K, MATH, and AIME, matching or surpassing traditional in-weight adaptation methods. ReasonCACHE demonstrates superior efficiency across multiple dimensions: requiring significantly less training data and fewer parameters than LoRA, generating shorter reasoning chains while improving accuracy, and using substantially less inference compute than standard in-context learning approaches.

## Method Summary
ReasonCACHE works by compressing multiple reasoning demonstrations into a compact, learnable key-value cache that can be efficiently retrieved during inference. The method uses prefix tuning to learn this cache representation, allowing the model to access distilled reasoning patterns without the computational overhead of processing full demonstrations. This approach overcomes the quadratic scaling limitations of standard in-context learning while maintaining or improving performance. The key insight is that by learning a compressed representation of reasoning demonstrations, the model can effectively transfer reasoning skills without modifying its pretrained weights, achieving both efficiency and performance gains across multiple reasoning benchmarks.

## Key Results
- Matches or surpasses in-weight adaptation approaches like supervised fine-tuning and LoRA on reasoning benchmarks
- Requires 59% less training data and 46% fewer trainable parameters than LoRA on GSM8K
- Generates 34% shorter reasoning chains while improving accuracy by 11% on GPQA-Diamond
- Uses 90% less inference compute than standard in-context learning

## Why This Works (Mechanism)
ReasonCACHE leverages the principle that reasoning skills can be effectively transferred through demonstration distillation rather than weight modification. By compressing multiple reasoning examples into a learnable cache, the method creates an efficient retrieval mechanism that bypasses the quadratic attention complexity of standard in-context learning. The prefix-tuning approach allows the model to learn optimal representations for reasoning patterns without the constraints of low-rank weight updates, enabling more expressive and efficient reasoning skill acquisition.

## Foundational Learning
- In-context learning (ICL): Understanding how LLMs use demonstration examples during inference without weight updates - needed to grasp baseline limitations and ReasonCACHE's improvements
- Prefix tuning: Learning additional parameters that modify model behavior without updating weights - critical for understanding how ReasonCACHE learns the demonstration cache
- Attention mechanisms: Quadratic complexity in standard attention - essential for appreciating why ReasonCACHE's cache approach is more efficient
- Low-rank adaptation (LoRA): Weight-efficient fine-tuning method - provides comparison baseline for evaluating ReasonCACHE's expressiveness
- Key-value caching: Efficient retrieval of demonstration information - fundamental to ReasonCACHE's core mechanism
- Reasoning chain distillation: Compressing multi-step reasoning into compact representations - key innovation enabling efficiency gains

Quick check: Verify that ReasonCACHE's cache mechanism truly avoids quadratic attention scaling by analyzing computational complexity as context length increases.

## Architecture Onboarding

Component map: Input -> Prefix Tuning Layer -> Learnable Cache -> LLM Reasoning Module -> Output

Critical path: Demonstration examples → Cache compression → Prefix tuning → Learned cache → Reasoning task execution

Design tradeoffs: The method sacrifices some flexibility of full in-context learning for efficiency gains through cache compression. The prefix-tuning mechanism adds complexity but enables better expressiveness than low-rank methods. The cache size must balance between comprehensive coverage and computational efficiency.

Failure signatures: Degraded performance on tasks requiring diverse demonstration types, cache miss scenarios where reasoning patterns don't match learned representations, and potential overfitting to specific reasoning styles in the training demonstrations.

First experiments to run:
1. Ablation study removing prefix tuning to isolate its contribution to performance gains
2. Stress test with maximum-length demonstrations to evaluate cache capacity limits
3. Cross-domain transfer experiment applying cached reasoning skills to novel task types

## Open Questions the Paper Calls Out
The paper acknowledges several important open questions regarding ReasonCACHE's broader applicability. Performance on non-reasoning tasks like code generation or fact-heavy knowledge tasks remains unexplored. The method's effectiveness with extremely long or numerous demonstrations hasn't been thoroughly evaluated. Questions also remain about the practical deployment considerations of the prefix-tuning mechanism, including memory management and serving infrastructure requirements.

## Limitations
- Evaluation focuses primarily on reasoning tasks, leaving uncertainty about performance on code generation or factual knowledge tasks
- Efficiency metrics are reported relative to baselines rather than absolute terms, limiting interpretability
- The prefix-tuning mechanism adds complexity that may affect practical deployment and memory management
- Scalability to tasks requiring hundreds of demonstrations remains untested, raising questions about practical limits

## Confidence
- Theoretical expressiveness claims: High (supported by formal proofs and empirical rank analysis)
- Reasoning task performance: Medium (strong on evaluated tasks, unknown generalizability to other domains)
- Efficiency metrics: Medium (impressive relative gains, limited absolute context provided)

## Next Checks
1. Test ReasonCACHE on non-reasoning tasks (code generation, factual QA) to assess generalizability beyond mathematical reasoning
2. Compare against more recent efficient inference methods like FlashAttention-2 or other KV cache optimization techniques
3. Evaluate performance degradation when scaling to tasks requiring hundreds of demonstrations, testing the practical limits of the distillation approach