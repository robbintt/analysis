---
ver: rpa2
title: The role of self-supervised pretraining in differentially private medical image
  analysis
arxiv_id: '2601.19618'
source_url: https://arxiv.org/abs/2601.19618
tags:
- initialization
- privacy
- training
- across
- auroc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study demonstrates that initialization strategy is a critical
  determinant of utility, fairness, and generalization in differentially private medical
  imaging. Across more than 800,000 chest radiographs, self-supervised DINOv3 initialization
  consistently outperformed ImageNet initialization under DP, but remained inferior
  to domain-specific supervised pretraining, which achieved performance closest to
  non-private baselines.
---

# The role of self-supervised pretraining in differentially private medical image analysis

## Quick Facts
- arXiv ID: 2601.19618
- Source URL: https://arxiv.org/abs/2601.19618
- Reference count: 40
- Self-supervised pretraining improves fairness and generalization under differential privacy, but domain-specific supervised pretraining remains superior

## Executive Summary
This study evaluates the impact of different pretraining strategies on the performance of differentially private medical image analysis. Using six large chest X-ray datasets (~805,000 images), the authors compare ImageNet-supervised, self-supervised DINOv3, and domain-specific MIMIC-CXR pretraining under full-model differential privacy with DP-SGD. DINOv3 initialization consistently improves fairness metrics and cross-dataset generalization compared to ImageNet, though domain-specific pretraining achieves the best utility and fairness outcomes. The findings demonstrate that while self-supervised pretraining is valuable for privacy-preserving medical AI, task-aligned supervision remains critical for optimal performance.

## Method Summary
The study uses ConvNeXt-Small and ConvNeXt-Tiny architectures for multi-label chest radiograph classification across six datasets (VinDr-CXR, ChestX-ray14, PadChest, CheXpert, UKA-CXR, MIMIC-CXR). Three initialization strategies are compared: ImageNet-supervised, DINOv3 self-supervised, and MIMIC-CXR domain-specific supervised pretraining. Models are trained under DP-SGD with Poisson sampling, per-sample gradient clipping (norm 4 for Small, 3.5 for Tiny), and Gaussian noise injection. Training uses AdamW optimizer (learning rate 5e-6 or 1e-5), weight decay of 0.01, and no data augmentation. Privacy budgets are evaluated across three ε ranges: 0<ε<1, 1<ε<3, and 3<ε<10. Fairness is assessed using AUROC disparity, equalized odds difference, and positive predictive value parity across sex and age subgroups.

## Key Results
- DINOv3 initialization reduces age-based AUROC disparity by up to 15 percentage points compared to ImageNet initialization
- Domain-specific pretraining achieves AUROC closest to non-private baselines, outperforming both DINOv3 and ImageNet initialization
- DINOv3 improves cross-dataset generalization under privacy constraints, maintaining better performance when tested on unseen datasets
- Fairness metrics (EOD, OD, PtD) show substantial improvement with DINOv3 initialization across all privacy regimes

## Why This Works (Mechanism)
Self-supervised pretraining on large unlabeled datasets helps models learn generalizable visual features that are less sensitive to the specific distribution of the target dataset. This makes them more robust to the noise injection required for differential privacy. Domain-specific pretraining provides the strongest initialization because it aligns pretraining tasks with the downstream medical imaging domain, creating feature representations that are both generalizable and task-relevant.

## Foundational Learning
- Differential Privacy: mathematical framework for protecting individual data privacy while allowing statistical analysis. Needed because medical data contains sensitive patient information. Quick check: verify ε bounds are correctly computed using Rényi DP accountant.
- DP-SGD: extension of stochastic gradient descent that adds Gaussian noise to gradients and clips per-sample gradients. Needed to train models under differential privacy constraints. Quick check: confirm gradient clipping is applied per-sample, not per-batch.
- Rényi Differential Privacy: accounting method that provides tighter privacy bounds than traditional methods. Needed for accurate privacy budget tracking during training. Quick check: verify δ parameter is set appropriately (6e-6 used here).

## Architecture Onboarding

**Component map:** Data preprocessing -> Model initialization -> DP-SGD training -> Privacy accounting -> Evaluation (utility + fairness)

**Critical path:** Initialization choice → DP training stability → Privacy-utility tradeoff → Fairness outcomes

**Design tradeoffs:** Self-supervised pretraining vs. supervised pretraining balances generalization capability with task alignment; tighter privacy budgets improve privacy but reduce utility and fairness; domain-specific pretraining requires additional labeled data but provides superior initialization.

**Failure signatures:** DP training divergence (learning rate too high), unstable privacy-utility tradeoff (incorrect noise calibration), fairness degradation (inappropriate initialization or sampling), poor cross-dataset generalization (insufficient domain alignment).

**First experiments:** 1) Train with only noise injection (no clipping) to verify privacy mechanism; 2) Compare uniform vs. Poisson sampling effects on DP-SGD stability; 3) Ablation study removing pretraining to quantify initialization impact.

## Open Questions the Paper Calls Out
None

## Limitations
- Exact noise multiplier values for achieving target ε ranges are not specified, requiring empirical calibration
- Training duration and stopping criteria are underspecified, creating potential variability in reported results
- Absolute AUROC values are sensitive to implementation details that are not fully documented

## Confidence

**High confidence:** Initialization strategy effects on fairness metrics, domain-specific pretraining superiority under DP, cross-dataset generalization patterns

**Medium confidence:** DINOv3 vs. ImageNet comparisons within studied privacy budgets (ε < 10)

**Low confidence:** Absolute AUROC values for DP models due to underspecified implementation details

## Next Checks
1. Implement systematic calibration procedure to empirically determine noise multipliers for target ε ranges
2. Conduct ablation studies comparing Poisson vs. uniform sampling under DP-SGD
3. Evaluate DP model fairness on additional protected attributes (sex-age intersection, socioeconomic proxies)