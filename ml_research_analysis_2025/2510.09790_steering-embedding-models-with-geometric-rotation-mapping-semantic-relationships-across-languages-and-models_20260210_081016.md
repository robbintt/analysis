---
ver: rpa2
title: 'Steering Embedding Models with Geometric Rotation: Mapping Semantic Relationships
  Across Languages and Models'
arxiv_id: '2510.09790'
source_url: https://arxiv.org/abs/2510.09790
tags:
- semantic
- language
- rise
- across
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Rotor-Invariant Shift Estimation (RISE),
  a geometric approach for steering embedding models that treats semantic transformations
  as consistent rotational operations on the unit hypersphere where embeddings reside.
  RISE uses Riemannian geometry to align different linguistic contexts into a common
  geometric frame, enabling cross-lingual and cross-model semantic control.
---

# Steering Embedding Models with Geometric Rotation: Mapping Semantic Relationships Across Languages and Models

## Quick Facts
- arXiv ID: 2510.09790
- Source URL: https://arxiv.org/abs/2510.09790
- Reference count: 40
- Key result: Geometric rotations on hyperspheres enable consistent semantic steering across 7 languages and 3 embedding models

## Executive Summary
This paper introduces Rotor-Invariant Shift Estimation (RISE), a geometric approach for steering embedding models that treats semantic transformations as consistent rotational operations on the unit hypersphere where embeddings reside. RISE uses Riemannian geometry to align different linguistic contexts into a common geometric frame, enabling cross-lingual and cross-model semantic control. Evaluated across three embedding models (text-embedding-3-large, bge-m3, mBERT), three datasets, and seven languages (English, Spanish, Japanese, Tamil, Thai, Arabic, Zulu), RISE achieves mean rotor alignment scores of 0.788 for negation, 0.780 for conditionality, and 0.762 for politeness. Cross-language transfer performance ranges from 77%-95% of monolingual effectiveness, demonstrating that RISE captures universal semantic patterns despite linguistic diversity.

## Method Summary
RISE treats semantic transformations as rotational displacements on the unit hypersphere, using Riemannian logarithmic and exponential maps to preserve manifold curvature. For each neutral-transformed sentence pair, RISE computes a rotor that canonicalizes the neutral embedding to a reference direction, applies this to the tangent vector between embeddings, and averages these canonicalized vectors to form a prototype. This prototype can then be applied to new embeddings via inverse rotor and exponential map to predict semantic transformations. The method requires normalized embeddings and uses Householder reflections for efficient rotor computation.

## Key Results
- RISE achieves rotor alignment scores of 0.788 (negation), 0.780 (conditionality), and 0.762 (politeness)
- Cross-language transfer reaches 77%-95% of monolingual effectiveness
- Outperforms random baselines by 5.1× to 26.2×, validating meaningful semantic learning
- Demonstrates consistent geometric operations across multilingual embedding spaces

## Why This Works (Mechanism)

### Mechanism 1: Riemannian Operations on Hyperspherical Embeddings
- Semantic transformations correspond to rotational displacements on the unit hypersphere, not Euclidean translations
- Normalized sentence embeddings lie on S^(d-1), using Riemannian log/exp maps to preserve curvature
- Core assumption: embeddings are approximately unit-normalized
- Break condition: non-normalized embeddings degrade geodesic operations to arbitrary shifts

### Mechanism 2: Canonicalization via Rotor Alignment
- Aligning all source embeddings to a fixed reference direction isolates the semantic change component
- Rotor R(n) maps n → e_1, then applies to log_n(v) so all tangent vectors share a common frame
- Core assumption: semantic transformation is directionally consistent when viewed canonically
- Break condition: highly context-dependent or magnitude-changing transformations won't cluster

### Mechanism 3: Prototype Transfer via Shared Geometric Structure
- A single prototype vector learned in one language/model can predict transformations in others with 77–95% effectiveness
- Averaged canonicalized tangent vector captures universal "semantic direction"
- Core assumption: multilingual models encode semantically analogous transformations geometrically
- Break condition: high cultural/contextual variability (e.g., politeness) shows higher variance and lower transfer

## Foundational Learning

- Concept: Riemannian manifolds and tangent spaces
  - Why needed: RISE operates on the unit hypersphere using geodesics; without this, log/exp maps are opaque
  - Quick check: Given two points on S^2, describe the tangent vector at one pointing toward the other

- Concept: Exponential and logarithmic maps (geodesic operations)
  - Why needed: Convert between curved manifold points and flat tangent vectors for averaging
  - Quick check: What does exp_n(log_n(v)) return, and what fails if v = -n?

- Concept: Rotation matrices / Householder reflections
  - Why needed: Rotor alignment uses Householder reflections or Givens rotations for O(d) canonicalization
  - Quick check: Construct a Householder reflection mapping vector a to e_1

## Architecture Onboarding

- Component map: Input sentences -> Multilingual encoder -> Normalized embeddings (n,v) -> Rotor module -> Tangent projection -> Prototype store -> Prediction
- Critical path: 1) Embedding normalization, 2) Rotor construction with antipodal handling, 3) Prototype quality from diverse pairs
- Design tradeoffs: Householder (simpler) vs Givens (orientation-preserving); high vs low dimensional embeddings; monolingual vs cross-lingual training
- Failure signatures: Non-normalized inputs, antipodal cases (n ≈ -e_1), high-variability phenomena, language/resource imbalance
- First 3 experiments: 1) Validate manifold assumption by plotting embedding norms, 2) Rotor sanity check vs random baseline, 3) Cross-language transfer test (English→Japanese)

## Open Questions the Paper Calls Out

- Does geometric consistency generalize to semantic phenomena beyond negation, conditionality, and politeness?
- Does reliance on LLM-generated synthetic data introduce biases that artificially inflate RISE's reported geometric consistency?
- Can the English-centric bias observed in cross-model transfer be mitigated through architectural or training changes?

## Limitations

- Model architecture dependency: Different models show varying transfer effectiveness, suggesting architectural differences affect universal geometric assumptions
- Dataset generation methodology: Synthetic data from GPT-4.5 may introduce model bias and may not capture natural language variation
- Antipodal case handling: Two-step construction for n ≈ -e_1 mentioned but implementation details are omitted

## Confidence

- **High Confidence**: Geometric framework is mathematically sound; core mechanism clearly specified; 5-26× improvement over baselines
- **Medium Confidence**: Cross-lingual transfer results demonstrate practical utility but depend on specific embedding characteristics; "universal semantic patterns" supported but not absolute
- **Low Confidence**: Transfer effectiveness based on synthetic data may not generalize to natural language; antipodal case handling underspecified

## Next Checks

1. **Manifold Assumption Validation**: Plot embedding norm distribution for target models; confirm clustering around unit length within 0.05
2. **Antipodal Case Implementation**: Implement and test two-step construction for n ≈ -e_1; measure frequency and impact on performance
3. **Natural Language Generalization**: Apply RISE to naturally occurring sentence pairs from BLiMP/SICK; compare performance to synthetic data results