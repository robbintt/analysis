---
ver: rpa2
title: 'Guardian: Decoupling Exploration from Safety in Reinforcement Learning'
arxiv_id: '2510.22859'
source_url: https://arxiv.org/abs/2510.22859
tags:
- learning
- safe
- safety
- offline
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the instability in hybrid offline-online reinforcement
  learning caused by distribution shifts between offline and online data. It proposes
  RLPD-GX, which decouples exploration (via a reward-seeking learner) from safety
  enforcement (via a projection-based guardian that ensures safe execution and guarded
  value backups).
---

# Guardian: Decoupling Exploration from Safety in Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.22859
- Source URL: https://arxiv.org/abs/2510.22859
- Authors: Kaitong Cai; Jusheng Zhang; Jing Yang; Keze Wang
- Reference count: 37
- Normalized mean score: 3.02 on Atari-100k (outperforming offline, online, and hybrid baselines)

## Executive Summary
This paper addresses instability in hybrid offline-online reinforcement learning caused by distribution shifts between offline and online data. It introduces RLPD-GX, which decouples exploration (via a reward-seeking learner) from safety enforcement (via a projection-based guardian). The method ensures safe execution and guarded value backups while maintaining exploration freedom. Dynamic temporal and symmetric sampling curricula stabilize training, and the guarded Bellman operator is proven to be a contraction, ensuring convergence. On the Atari-100k benchmark, RLPD-GX achieves state-of-the-art performance with superior safety and stability.

## Method Summary
RLPD-GX introduces a projection-based guardian that separates exploration from safety enforcement. The learner proposes unconstrained actions, which the guardian projects onto a safe set using a predefined safety predicate. Only the projected actions are executed and stored in the replay buffer. The method employs dynamic temporal and symmetric sampling curricula to gradually transition from offline to online data mixing. The guarded Bellman operator restricts backups to safe actions and is proven to be a contraction, ensuring convergence. This decoupled architecture allows for unconstrained exploration while maintaining safety guarantees throughout learning.

## Key Results
- Achieves normalized mean score of 3.02 on Atari-100k benchmark
- Outperforms offline (2.39), online (1.27), and hybrid (2.07) baselines by up to 45%
- Demonstrates superior safety and stability across ablation studies
- Proves convergence via contraction property of the guarded Bellman operator

## Why This Works (Mechanism)

### Mechanism 1: Decoupled Projection-Based Safety Enforcement
The Guardian projection preserves learning signal quality while preventing catastrophic actions by sanitizing the data stream without constraining policy expressivity. The Learner proposes unconstrained actions $a_t \sim \pi_\phi$, and the Guardian projects these onto a safe set: $a_t^{exec} = \arg\min_{a' \in A_{safe}(s_t)} \|a' - a_t\|_2^2$. This approach assumes a pre-defined safety predicate $g(s,a)$ exists and $A_{safe}(s)$ is non-empty for all states.

### Mechanism 2: Guarded Bellman Operator as Contraction
Restricting Bellman backups to safe actions preserves convergence guarantees by replacing standard max-over-actions with max-over-safe-actions: $(\mathcal{T}_\Pi Q)(s,a) = R(s,a) + \gamma \mathbb{E}_{s'}[\max_{a' \in A_{safe}(s')} Q(s',a')]$. Theorem 1 proves this is a $\gamma$-contraction in $\|\cdot\|_\infty$ under assumptions of finite state/action spaces, bounded rewards, valid transitions, and non-empty safe sets.

### Mechanism 3: Dynamic Sampling Curricula (DTS + DSS)
Gradual curriculum over temporal horizon and data mixing reduces variance and distribution shock through DTS expanding sampling interval $\Delta(t)$ from short to long horizons, and DSS annealing the offline-online mixing ratio $\lambda(t)$ from offline-biased to balanced. This provides low-variance early learning and smooth transition, assuming offline data provides useful local-dynamics priors and training duration $T$ is known or bounded.

## Foundational Learning

- **Bellman Operators and Contraction**
  - Why needed here: The core theoretical contribution is proving the guarded operator remains a contraction
  - Quick check question: Given two Q-functions $Q_1, Q_2$, show $\|\mathcal{T} Q_1 - \mathcal{T} Q_2\|_\infty \leq \gamma \|Q_1 - Q_2\|_\infty$

- **Distribution Shift in Offline RL**
  - Why needed here: The paper targets instability from $d_{off}$ vs. $d_{on}$ mismatch
  - Quick check question: Why does querying Q-values on OOD actions cause overestimation in offline RL?

- **Maximum Entropy RL (Soft Actor-Critic)**
  - Why needed here: The objective includes entropy bonus $\alpha H(\pi_\phi)$; guarded backup uses soft value
  - Quick check question: Derive the soft Bellman backup for a maximum-entropy objective

## Architecture Onboarding

- **Component map**: Learner $\pi_\phi$, Q-ensemble $\{Q_{\theta_i}\}$ -> Guardian $g(s,a)$, $A_{safe}(s)$, $\Pi_{safe}$ -> Buffers $D_{off}$, $B_{on}$ -> Samplers DTS, DSS

- **Critical path**: 
  1. Observe $s_t$
  2. Learner proposes $a_t \sim \pi_\phi(\cdot|s_t)$
  3. Guardian projects: $a_t^{exec} = \Pi_{safe}(s_t, a_t)$
  4. Execute $a_t^{exec}$, store $(s_t, a_t^{exec}, r_t, s_{t+1})$ in $B_{on}$
  5. Sample batches from $D_{off}$ and $B_{on}$ using DTS/DSS
  6. Compute guarded target $y$ using safe policy $\pi_\phi^{safe}$
  7. Update critics and actor
  8. Soft-update target networks

- **Design tradeoffs**: 
  - Predicate granularity: Hand-crafted rules are simple but domain-specific; learned classifiers generalize but may be imperfect
  - Guarded backup vs. execution-only shielding: Guarded backup is critical for stability (execution-only destabilizes value learning)
  - Schedule aggressiveness: Faster annealing improves sample efficiency but risks shock; slower annealing is safer but may delay convergence

- **Failure signatures**:
  - TD error divergence: Indicates value function corruption from OOD signals
  - High Q-ensemble variance: Suggests epistemic uncertainty on OOD states
  - Premature safety violations: Safety predicate may be incomplete
  - Stagnant performance in long-horizon tasks: DTS may be stuck on short horizons

- **First 3 experiments**:
  1. Ablate guarded backup: Compare RLPD-GX vs. SAC+Shield on Seaquest (expect SAC+Shield to collapse mid-training)
  2. Vary DTS schedule: Compare Uniform, Fixed-$k$, and DTS on BankHeist (expect DTS to dominate)
  3. Safety generalization test: Train on offline data with simple rules, evaluate on held-out critical states (expect high TTFV and decision accuracy)

## Open Questions the Paper Calls Out

### Open Question 1
How can safety predicates be automatically learned or inferred in domains where explicit safety rules are unavailable? The framework critically depends on accurate predicate definitions for both execution-time projection and guarded backups, yet real-world applications often lack pre-specified safety rules.

### Open Question 2
Does the Guardian framework scale to continuous action spaces where the projection operation becomes an optimization problem over potentially uncountable safe action sets? All experiments use discrete Atari action spaces, and the theoretical analysis explicitly assumes finite state/action spaces.

### Open Question 3
What happens to convergence guarantees when safe action sets become extremely sparse or when the optimal policy operates near constraint boundaries? The contraction proof assumes non-empty safe sets, but does not analyze scenarios where $A_{safe}(s)$ contains very few actions.

## Limitations
- Safety predicate completeness: Method assumes a pre-defined, exhaustive safety predicate $g(s,a)$; if $A_{safe}(s) = \emptyset$ for any state, the projection fails
- Static offline data assumption: Offline buffer $D_{off}$ is fixed and must contain sufficient safe demonstrations for the curriculum to bootstrap
- Finite horizon and known $T$: Dynamic sampling curricula assume bounded training time $T$ to schedule $\lambda(t)$ and $\Delta(t)$

## Confidence

- **High confidence**: Guarded Bellman operator contraction proof, offline-online mixing benefit, DTS/DSS curriculum effectiveness, safety enforcement preventing violations, and performance gains over baselines
- **Medium confidence**: Generalization of safety predicates beyond Atari, stability under varying offline data quality, and computational overhead estimates
- **Low confidence**: Real-world deployment readiness, performance in continuous control or safety-critical domains, and scalability to large state spaces

## Next Checks
1. **Safety predicate robustness test**: Train on Atari-100k with simple life-penalty rules, then evaluate on held-out critical states. Measure True-Trigger-First-Value (TTFV) and decision accuracy to quantify predicate coverage.

2. **Offline data scarcity stress test**: Reduce offline data size (e.g., 10% of Atari-100k) and evaluate RLPD-GX vs. SAC+Shield. Check if DTS/DSS can still stabilize learning without sufficient safe priors.

3. **Continuous control transfer**: Port RLPD-GX to a benchmark like Safety Gym or MuJoCo with safety constraints. Compare performance and safety metrics against pure online RL and offline RL baselines.