---
ver: rpa2
title: Improving the Accuracy of Amortized Model Comparison with Self-Consistency
arxiv_id: '2508.20614'
source_url: https://arxiv.org/abs/2508.20614
tags:
- training
- likelihood
- data
- marginal
- posterior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates self-consistency (SC) training to improve
  amortized model comparison under model misspecification. Four methods are compared:
  NPE, NPLE (both using parameter posteriors), NEE, and NPMP (direct marginal likelihood
  estimation).'
---

# Improving the Accuracy of Amortized Model Comparison with Self-Consistency

## Quick Facts
- **arXiv ID**: 2508.20614
- **Source URL**: https://arxiv.org/abs/2508.20614
- **Reference count**: 29
- **Primary result**: NPE with self-consistency training outperforms baseline amortized methods, especially under model misspecification.

## Executive Summary
This paper investigates self-consistency (SC) training to improve amortized model comparison under model misspecification. Four methods are compared: NPE, NPLE (both using parameter posteriors), NEE, and NPMP (direct marginal likelihood estimation). Across synthetic and real-world experiments, NPE and NPLE consistently outperform NEE and NPMP. SC training improves NPE accuracy significantly, especially for data near the SC training distribution, yielding near-perfect alignment with bridge sampling benchmarks in real applications. For NPLE, SC effects are inconsistent, likely due to the improper SC loss when both posterior and likelihood are approximate. NEE and NPMP show limited improvement from SC and are poorly calibrated under OOD conditions. The results suggest preferring NPE or NPLE for amortized model comparison and augmenting NPE with SC training on empirical data to enhance robustness under model misspecification.

## Method Summary
The paper compares four amortized Bayesian model comparison methods: Neural Posterior Estimation (NPE), Neural Posterior and Likelihood Estimation (NPLE), Neural Empirical Bayes (NEE), and Neural Posterior Marginal Prior (NPMP). All methods use normalizing flows with 6 affine coupling layers and DeepSet summary networks to learn summary statistics. The SC training approach uses a warm-start schedule where the SC loss is gradually introduced from epochs 21-30, with variance estimated over 16 posterior samples during training. Evaluation is performed on synthetic multivariate Gaussian data (D=1,10,20; N=1,10,100), two-vector Gaussian models for Bayes factors, racing diffusion data (17 participants), and air passenger traffic (15 countries). Performance is measured by relative error against analytic or bridge sampling benchmarks.

## Key Results
- NPE and NPLE consistently outperform NEE and NPMP across all experimental conditions
- SC training significantly improves NPE accuracy, particularly for data near the SC training distribution
- NPLE shows inconsistent improvements from SC training, likely due to improper SC loss when both posterior and likelihood are approximate
- NEE and NPMP demonstrate limited SC improvement and poor calibration under out-of-distribution conditions
- Real-world applications show NPE+SC achieves near-perfect alignment with bridge sampling benchmarks

## Why This Works (Mechanism)
The self-consistency training improves amortized model comparison by enforcing that posterior samples drawn from the approximate posterior yield consistent marginal likelihood estimates. This consistency constraint helps calibrate the network's uncertainty estimates, particularly under model misspecification where the assumed model deviates from the true data-generating process. The warm-start schedule (introducing SC loss gradually from epochs 21-30) allows the network to first learn reasonable posterior approximations before refining them through consistency constraints, preventing early training instability.

## Foundational Learning
- **Amortized Bayesian inference**: Neural networks learn to approximate posterior distributions across multiple datasets, enabling fast inference at test time without expensive MCMC sampling
- **Self-consistency training**: Enforces that posterior samples drawn from the approximate posterior yield consistent marginal likelihood estimates, improving calibration under model misspecification
- **Coupling flows**: Normalizing flow architecture that allows efficient density estimation through invertible transformations, enabling exact likelihood computation
- **DeepSet summaries**: Permutation-invariant neural network architecture for learning summary statistics from datasets of varying sizes
- **Model misspecification**: When the true data-generating process deviates from assumed model structure, requiring robust estimation methods
- **Bridge sampling**: Accurate but computationally expensive method for marginal likelihood estimation used as ground truth for comparison

## Architecture Onboarding
- **Component map**: Data -> Summary Network (DeepSet) -> Normalizing Flow -> Posterior Samples -> Marginal Likelihood Estimate
- **Critical path**: Training data → NPE/NPLE/NEE/NPMP network → Posterior samples → SC loss computation → Parameter updates
- **Design tradeoffs**: Posterior-based methods (NPE/NPLE) vs direct estimation (NEE/NPMP); computational efficiency vs accuracy under misspecification
- **Failure signatures**: NPLE+SC instability due to improper loss; NEE/NPMP poor calibration under OOD; convergence issues without SC warm-start
- **First experiments**: 1) Implement NPE with coupling flow and DeepSet summary; 2) Add SC loss with warm-start schedule; 3) Evaluate on Gaussian synthetic data with analytic ground truth

## Open Questions the Paper Calls Out
- How does the improper SC loss affect NPLE performance under different levels of model misspecification?
- What is the theoretical justification for the warm-start schedule in SC training?
- Can alternative consistency constraints improve NEE and NPMP performance?
- How do different coupling flow architectures impact the effectiveness of SC training?

## Limitations
- NPLE benefits inconsistently from SC training due to theoretical issues with the improper SC loss when both posterior and likelihood are approximate
- Real-world experiment implementations may have variations that affect result reproducibility
- Limited number of independent runs prevents quantification of result variance and statistical significance
- Performance under extreme model misspecification beyond tested scenarios remains unexplored

## Confidence
- **High**: Architectural specifications, SC training procedure, synthetic experiment design
- **Medium**: Real-world experiment implementations, cross-method performance comparisons
- **Low**: Specific hyperparameter choices (learning rate, random seeds), variance estimates

## Next Checks
1. Verify optimizer settings by implementing Adam with learning rate 1e-3 and compare training stability against alternative schedules
2. Run NPE+SC with 5 independent seeds to quantify result variance and ensure reported improvements are reproducible
3. Implement the improper SC loss for NPLE and verify it produces the inconsistent improvements observed in the paper