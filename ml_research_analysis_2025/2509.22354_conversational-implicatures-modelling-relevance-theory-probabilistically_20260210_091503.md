---
ver: rpa2
title: 'Conversational Implicatures: Modelling Relevance Theory Probabilistically'
arxiv_id: '2509.22354'
source_url: https://arxiv.org/abs/2509.22354
tags:
- mary
- relevance
- drink
- peter
- utterance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper develops a probabilistic model of implicature comprehension
  based on Relevance Theory, implemented in ProbLog. It formalizes how speakers convey
  implicit meanings through conversational implicatures, where listeners infer unstated
  assumptions to make sense of an utterance.
---

# Conversational Implicatures: Modelling Relevance Theory Probabilistically

## Quick Facts
- **arXiv ID**: 2509.22354
- **Source URL**: https://arxiv.org/abs/2509.22354
- **Reference count**: 40
- **Primary result**: Probabilistic model of implicature comprehension based on Relevance Theory, implemented in ProbLog.

## Executive Summary
This paper develops a probabilistic model of conversational implicature comprehension grounded in Relevance Theory. The model is implemented in ProbLog and formalizes how listeners infer implicit meanings from utterances by evaluating interpretive hypotheses against relevance expectations. It captures the interplay between explicatures (developed from the utterance's logical form) and implicatures (derived via contextual assumptions), with interpretations evaluated based on Kullback-Leibler divergence and processing effort. The approach bridges cognitive science and pragmatics by treating comprehension as a relevance-guided heuristic process, offering quantitative predictions for empirical testing.

## Method Summary
The method encodes knowledge graphs and inference rules in ProbLog, with interpretive hypotheses manually constructed by the experimenter. The utterance is set as evidence, and posterior probabilities for implicated conclusions are queried. Relevance is quantified using Kullback-Leibler divergence between prior and posterior probability distributions, with processing effort approximated by the size of the Herbrand base. The model evaluates hypotheses by maximizing KL divergence while minimizing Herbrand base size relative to relevance expectations.

## Key Results
- The model successfully captures the parallel adjustment of explicatures and implicatures via interacting conditional inferences.
- Interpretive hypotheses are evaluated quantitatively using KL divergence and Herbrand base size as proxies for cognitive effects and processing effort.
- The implementation demonstrates that comprehension can be modeled as a satisficing "Take the Best" heuristic rather than exhaustive search.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model operationalizes the Relevance-Theoretic comprehension heuristic by simulating a "path of least effort" search that terminates when specific relevance expectations are satisfied.
- Mechanism: The system defines a set of interpretive hypotheses ordered by accessibility. It samples from these hypotheses—conceptualized as probabilistic graphs of encyclopedic knowledge—and evaluates them against a set of queries. The process halts when the evidence provided by the utterance shifts the probability distribution of these queries sufficiently to meet pre-defined expectations.
- Core assumption: Comprehension is a satisficing process ("Take the Best" heuristic) rather than an exhaustive search, where the audience accepts the first hypothesis that yields adequate cognitive effects.
- Evidence anchors:
  - [Section 3.4]: Defines the relevance-theory processing heuristic (access, check, stop).
  - [Section 4.1.2]: Argues the heuristic is a "fast and frugal" cognitive module.
- Break condition: The mechanism fails if the knowledge graph lacks the specific encyclopedic link required to connect the utterance to the expected cognitive effect.

### Mechanism 2
- Claim: The model captures the parallel adjustment of explicatures and implicatures via interacting conditional inferences, rather than sequential processing.
- Mechanism: Implemented in ProbLog, the system links the utterance to an explicature and contextual assumptions simultaneously. Inference rules define dependencies (e.g., "Mary wants coffee" depends on "Mary doesn't drink energizing drinks" AND "Coffee is energizing"). Probabilistic inference updates the entire network simultaneously.
- Core assumption: Comprehension involves mutually constraining inferences where early hypotheses about implicit conclusions can trigger backwards inference to resolve explicit content.
- Evidence anchors:
  - [Section 4.2]: Explicitly models comprehension as "parallel adjustment" using bottom-up and top-down conditional inference.
  - [Section 5.3]: Details the Bayesian network structure linking Sentence, Adjustment, Explicature, and Implicature.
- Break condition: The mechanism distorts meaning if the probabilistic dependencies are misspecified.

### Mechanism 3
- Claim: The "optimal relevance" of an interpretation is quantifiable as a function of KL divergence (utility) and Herbrand base size (processing effort).
- Mechanism: The utility of an utterance is measured by the shift in the listener's probability distribution from prior to posterior beliefs (KL divergence). Processing effort is approximated by the size of the Herbrand base. The winning interpretation maximizes KL divergence while minimizing the Herbrand base size relative to relevance expectations.
- Core assumption: "Relevance" is a non-representational property that can be mathematically approximated by information gain relative to the complexity of the knowledge structure required to achieve it.
- Evidence anchors:
  - [Section 4.3.2]: Proposes KL divergence as a viable proxy for the utility component of relevance.
  - [Section 5.4.6]: Formulates comprehension modeling as an optimization model based on these metrics.
- Break condition: The metric fails to predict human intuition if two interpretations have identical KL divergence but vastly different cognitive "effort" not captured by the Herbrand base.

## Foundational Learning

### Relevance Theory (Sperber & Wilson)
- Why needed here: This is the theoretical backbone. You must understand that "Relevance" is a technical term balancing *cognitive effects* against *processing effort*, not just general "importance."
- Quick check question: In the context of this paper, does a "relevant" utterance require maximum truth or maximum efficiency?

### ProbLog (Probabilistic Logic Programming)
- Why needed here: The implementation language. You need to grasp how it extends Prolog by allowing facts to have probabilities and how inference queries return probability values rather than binary true/false.
- Quick check question: If a ProbLog rule states `0.8:: edge(coffee, energizing)`, what does the query `query(energizing(coffee))` return?

### KL-Divergence (Kullback-Leibler Divergence)
- Why needed here: The primary evaluation metric. It measures how one probability distribution diverges from a second, expected probability distribution.
- Quick check question: If the "Prior" probability for "Mary wants coffee" is 0.5 and the "Posterior" (after the utterance) is 0.9, does the KL divergence increase or decrease compared to a Posterior of 0.6?

## Architecture Onboarding

### Component map:
Knowledge Graph -> Inference Rules -> Utterance Model -> Query Interface -> Optimizer

### Critical path:
The "Utterance Model" injects evidence into the "Knowledge Graph" via "Inference Rules," updating the probabilities of the "Query Interface" propositions. The optimizer then evaluates the KL divergence between these priors and posteriors.

### Design tradeoffs:
The model uses a "manually constructed hypothesis set" (Section 5). It does not generate hypotheses; it evaluates them. This limits scalability but ensures semantic validity of the logical forms being tested.

### Failure signatures:
- **Infinite KL-Divergence**: Occurs when a query in the relevance expectation cannot be answered by the current knowledge graph.
- **Inconsistent Evidence**: Occurs in ProbLog if you assert evidence with a prior probability of 0 or 1; priors must be non-extreme (e.g., 0.01).

### First 3 experiments:
1. **Run Int 1 vs. Int 2**: Execute the provided code for the optimal hypothesis (Int 1) and the insufficient context hypothesis (Int 2) to observe the difference in KL divergence and the "inf" failure mode.
2. **Tuning Accessibility**: Modify the probabilities in the Knowledge Graph (e.g., lower the edge probability between 'coffee' and 'energizing') to simulate a listener with different encyclopedic knowledge and observe the impact on implicature strength.
3. **Effort Simulation**: Implement Int 3 (adding the "hot drink" node) and verify that while logical inferences remain possible, the KL-divergence/utility drops relative to the Herbrand base size increase, demonstrating the "effort" penalty.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the inference processes involved in metacognition be modeled to capture how communicators estimate the audience's expectations of relevance?
- Basis in paper: [explicit] The conclusion explicitly identifies this as a "strongly desirable goal."
- Why unresolved: The current model focuses exclusively on the audience's comprehension phase because the "asymmetry of responsibilities" makes the comprehension phase more amenable to study than the speaker's perspective.
- What evidence would resolve it: A computational model that successfully predicts speaker utterance choices based on inferred listener relevance expectations.

### Open Question 2
- Question: Can the model be extended to handle higher-level explicatures and metarepresentational utterance uses, such as speech acts, irony, and indirect quotation?
- Basis in paper: [explicit] The conclusion lists "higher-level explicature comprehension" and "metarepresentational utterance use" as specific areas where the model needs extension.
- Why unresolved: These phenomena involve complex layers of representation and dissociation that go beyond the basic enrichment of logical forms and derivation of implicatures currently implemented.
- What evidence would resolve it: A successful ProbLog implementation that models the comprehension of ironic utterances where the speaker dissociates themselves from the expressed proposition.

### Open Question 3
- Question: Can the hypothesis generation phase be automated within the model, rather than relying on manually constructed interpretive hypotheses?
- Basis in paper: [inferred] The paper states that the model "requires candidate interpretive hypotheses of utterances to be manually constructed by the experimenter."
- Why unresolved: The current ProbLog implementation requires the programmer to define the knowledge graph and query set a priori, rather than the system inferring the context and candidate interpretations dynamically from the input.
- What evidence would resolve it: A model that generates the encyclopedic knowledge graph and the set of possible implicatures dynamically based on the linguistic input and a broader knowledge base.

### Open Question 4
- Question: Do the quantitative predictions of the model (KL divergence and Herbrand base size) correlate with human behavior in empirical testing?
- Basis in paper: [explicit] The paper concludes that the model "offers quantitative predictions for empirical testing."
- Why unresolved: The paper provides a computational proof-of-concept and theoretical argument but does not present data from human subjects to validate that these specific mathematical metrics reflect cognitive reality.
- What evidence would resolve it: Experimental data showing that interpretations with higher KL divergence and smaller Herbrand bases are consistently preferred by human subjects or processed with less effort.

## Limitations
- The model relies on manually constructed interpretive hypotheses rather than automated hypothesis generation, constraining scalability.
- The choice of KL divergence as a relevance metric, while theoretically grounded, lacks empirical validation against human comprehension data.
- The probabilistic knowledge graph is hand-crafted, raising questions about how well it represents real-world encyclopedic knowledge distributions.

## Confidence
- **High Confidence**: The formal mapping between Relevance Theory's comprehension heuristic and the ProbLog implementation is internally consistent and mechanistically sound.
- **Medium Confidence**: The KL divergence/Herbrand base metric for quantifying optimal relevance is mathematically coherent but unproven as a cognitive model of human processing effort.
- **Low Confidence**: Claims about the model's ability to handle pragmatic phenomena "beyond humans" (e.g., with LLMs) are speculative without empirical validation on such systems.

## Next Checks
1. **Empirical Grounding**: Test the KL divergence metric against human comprehension data by having participants judge the relevance of utterances and comparing their ratings to the model's predictions.
2. **Knowledge Graph Generalization**: Replace the hand-crafted knowledge graph with automatically extracted probabilistic knowledge (e.g., from ConceptNet) and evaluate if implicature predictions remain stable.
3. **Hypothesis Generation**: Implement a simple automated hypothesis generator that creates contextual assumptions from related facts in the knowledge graph, then compare its performance to the manual hypotheses on a standard pragmatics dataset.