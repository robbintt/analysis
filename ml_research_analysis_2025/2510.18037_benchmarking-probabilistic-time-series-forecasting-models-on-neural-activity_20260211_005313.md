---
ver: rpa2
title: Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity
arxiv_id: '2510.18037'
source_url: https://arxiv.org/abs/2510.18037
tags:
- forecasting
- neural
- time
- series
- activity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks probabilistic deep learning models for forecasting
  neural activity from widefield calcium imaging data in mice. The authors evaluate
  eight deep learning models, including two foundation models, against four classical
  statistical models and two baselines across five experimental sessions.
---

# Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity

## Quick Facts
- **arXiv ID**: 2510.18037
- **Source URL**: https://arxiv.org/abs/2510.18037
- **Reference count**: 40
- **Primary result**: Deep learning models, particularly PatchTST, TiDE, and fine-tuned Chronos, consistently outperformed classical approaches for forecasting neural activity up to 1.5 seconds into the future.

## Executive Summary
This paper benchmarks probabilistic deep learning models for forecasting neural activity from widefield calcium imaging data in mice. The authors evaluate eight deep learning models, including two foundation models, against four classical statistical models and two baselines across five experimental sessions. The models are compared using multiple metrics including Mean Weighted Quantile Loss, Mean Scaled Interval Score, and correlation. Several deep learning models, particularly PatchTST, TiDE, and fine-tuned Chronos, consistently outperformed classical approaches across different prediction horizons. The best model produced informative forecasts up to 1.5 seconds into the future. Foundation models pretrained on other domains showed poor zero-shot performance but improved after fine-tuning on neural data.

## Method Summary
The study evaluates 14 models on univariate probabilistic forecasting of spontaneous mouse cortical neural activity from widefield calcium imaging (35 Hz). Models are trained to predict interval [i, i+L) from history [i−H, i) with L ∈ {18, 35, 70} steps (~0.5, 1, 2 seconds). Data comes from 5 sessions of ~51K timesteps each, with activity traces from 4 brain regions (SS, MO, VIS, RSP) registered to Allen CCFv3. The chronological split is 60% train, 20% validation, 20% test. Models include Naive/Average baselines; AR, ARIMA, AR-HMM, Theta (classical); DeepAR, DLinear, TFT, PatchTST, TiDE, WaveNet (DL via GluonTS); and Chronos and Moirai (foundation models, zero-shot + fine-tuned). Hyperparameters are tuned by random search (40 configs) using validation MWQL; 5 random seeds are used with early stopping at 10 epochs without improvement.

## Key Results
- PatchTST, TiDE, and fine-tuned Chronos consistently achieved higher accuracy across different prediction horizons
- Foundation models pretrained on weather, finance, and transportation domains transferred poorly to neural activity in zero-shot settings
- The best model produced informative forecasts up to 1.5 seconds into the future
- Channel-independence (modeling each brain region separately) outperformed cross-channel modeling for this task

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Patch-based tokenization of neural time series improves forecasting by capturing local temporal structure while reducing sequence length for transformer attention.
- **Mechanism**: PatchTST groups neighboring timesteps into patches (tokens) before feeding them into a vanilla transformer encoder. This reduces the effective sequence length from H timesteps to H/patch_len tokens, making attention computation tractable while preserving local temporal correlations that matter for neural dynamics.
- **Core assumption**: Neural activity exhibits structure at multi-timestep scales (tens of milliseconds) that can be captured by fixed-length patches.
- **Evidence anchors**: [abstract] "several deep learning models consistently outperformed classical approaches, with the best model producing informative forecasts up to 1.5 seconds into the future"; [section A.2.3] "For each variate i, neighboring timesteps are grouped together as one 'patch' (or token), and then fed into the transformer encoder"

### Mechanism 2
- **Claim**: Channel-independence (modeling each brain region separately with shared parameters) outperforms cross-channel modeling for this neural forecasting task.
- **Mechanism**: PatchTST and TiDE model each variate (brain region) independently using shared weights: y^(i),future = F(y^(i),history). This avoids learning spurious cross-regional correlations that may not generalize.
- **Core assumption**: Shared temporal dynamics exist across brain regions, but cross-regional dependencies are either weak or require more data to learn reliably.
- **Evidence anchors**: [section A.2.3] "PatchTST models each variate individually using a shared model F ('channel-independence')... TiDE operates in a channel-independent manner, as PatchTST, and does not model the relationship between different time series"; [section 3] "several deep learning models – PatchTST, TiDE, and fine-tuned Chronos – consistently achieved higher accuracy across different prediction horizons"

### Mechanism 3
- **Claim**: Foundation models pretrained on non-neural domains (weather, finance, transportation) fail zero-shot on neural data but succeed after fine-tuning, indicating architecture transferability but domain-specific representations.
- **Mechanism**: Chronos (T5-based) and Moirai learn general temporal patterns from diverse time series, but neural data characteristics (millisecond resolution, lack of seasonality/trends, different noise structure) diverge from pretraining distributions. Fine-tuning adapts learned representations to neural dynamics.
- **Core assumption**: The transformer architecture itself is appropriate for neural forecasting; only the learned weights require adaptation.
- **Evidence anchors**: [section 3] "foundation models (Chronos and Moirai) pretrained on these domains transferred poorly to neural activity in a zero-shot setting... Nevertheless, Chronos becomes competitive after fine-tuning on neural activity, suggesting that its architecture is sufficiently flexible to capture neural dynamics"

## Foundational Learning

- **Concept**: Probabilistic forecasting vs point forecasting
  - **Why needed here**: The paper evaluates models on quantile loss and prediction intervals, not just accuracy. Understanding why uncertainty quantification matters for neural control applications is essential.
  - **Quick check question**: Can you explain why a model with lower MSE might still be worse for closed-loop neural control than one with well-calibrated prediction intervals?

- **Concept**: Quantile loss and the quantile crossing problem
  - **Why needed here**: TFT and Chronos-Bolt optimize quantile loss but suffer from quantile crossing (e.g., 0.5-quantile > 0.9-quantile). Understanding this helps interpret results and select appropriate models.
  - **Quick check question**: Why does minimizing quantile loss for each quantile independently not guarantee ordered quantiles?

- **Concept**: Autoregressive vs direct multi-step forecasting
  - **Why needed here**: The paper compares AR-style models (DeepAR, WaveNet) that generate forecasts one step at a time against direct models (PatchTST, TiDE, DLinear) that predict all future steps simultaneously. Error accumulation differs fundamentally between these approaches.
  - **Quick check question**: For a 70-step forecast, why might an autoregressive model accumulate error differently than a direct multi-step model?

## Architecture Onboarding

- **Component map**: Raw widefield imaging (560×560 @ 35Hz) → Hemodynamic correction (405nm/470nm subtraction) → SVD compression (top 500 components) → Regional averaging (SS, MO, VIS, RSP via Allen CCF) → Univariate time series per region → Train/val/test split (60/20/20 chronological) → Sliding window sampling (history H → target L) → Model training with probabilistic output distribution

- **Critical path**: The baseline comparison is your first checkpoint. Before training any deep model, implement Naive (repeat last value) and Average (historical mean) baselines. If your fancy model can't beat these, stop and debug.

- **Design tradeoffs**:
  - PatchTST: Best accuracy, but requires tuning patch_len, stride, and context_length. Direct multi-step means separate models for each horizon.
  - TiDE: Simpler MLP architecture, faster training, competitive performance. Good default choice.
  - AR(valQL): Strong classical baseline. Use validation-based order selection, not AICc, for this data.
  - Fine-tuned Chronos: Competitive but requires careful learning rate tuning (10^-5 worked best). Zero-shot is useless for neural data.

- **Failure signatures**:
  - Model performs worse than Naive: Check data preprocessing, verify chronological split wasn't violated, ensure no data leakage.
  - Foundation model zero-shot fails: Expected behavior—pretraining domains don't match neural data characteristics.
  - Prediction intervals too narrow/wide: Check output distribution choice (Student-t worked well here) and verify quantile loss implementation.

- **First 3 experiments**:
  1. Replicate the baseline comparison (Naive, Average, AR) on a single session and region. Verify your MWQL computation matches reported relative performance.
  2. Train PatchTST with the hyperparameters from Table 1 for 35-step prediction. Compare step-wise error curves against Figure 1d to validate your implementation.
  3. Fine-tune Chronos on one session using the specified learning rate (10^-5) and varying fine_tune_steps. Confirm zero-shot performs poorly and fine-tuned version approaches PatchTST performance.

## Open Questions the Paper Calls Out

- **Question**: Does the ~1.5-second limit on informative forecasting stem from constraints in current model architectures or intrinsic variability in neural dynamics?
  - **Basis in paper**: [explicit] The Discussion states: "Whether this limit comes from model design constraints of existing methods or instead reflects intrinsic sources of variability and time scales in neural activity remains an open question."
  - **Why unresolved**: The study establishes the existence of the performance horizon but cannot disentangle whether the plateau is a failure of the deep learning models to capture long-range dependencies or a fundamental property of the neural data's chaotic dynamics.
  - **What evidence would resolve it**: Developing specialized architectures that extend the horizon beyond 1.5s would support the "model constraint" hypothesis; conversely, theoretical analysis linking the horizon to the system's Lyapunov exponents would support the "intrinsic variability" hypothesis.

- **Question**: Can a foundation model pre-trained specifically on diverse neural data outperform general-purpose foundation models fine-tuned on neural activity?
  - **Basis in paper**: [explicit] The Discussion suggests: "It may be of interest to develop neural time series forecasting foundation models by combining strong backbones identified here with neuroscience-specific innovations, such as cross-subject training."
  - **Why unresolved**: The benchmark showed that general foundation models (Chronos, Moirai) failed in zero-shot settings and only became competitive after fine-tuning, leaving the potential of a domain-specific pre-training approach untested.
  - **What evidence would resolve it**: Training a large-scale transformer on a multi-session, multi-subject neural dataset and demonstrating superior zero-shot or few-shot performance compared to the fine-tuned general models evaluated in this paper.

- **Question**: Can current forecasting models effectively drive closed-loop control systems for neural intervention?
  - **Basis in paper**: [inferred] The Discussion mentions: "Our findings point toward future control applications" and suggests that "closed-loop control experiments informed by forecasting performance may help reveal the temporal structure inherent to neural system."
  - **Why unresolved**: The paper evaluates forecasting accuracy offline using historical data, but it does not validate whether these predictions are sufficiently fast, accurate, or robust to serve as input for a real-time feedback loop (e.g., for preemptive stimulation).
  - **What evidence would resolve it**: Integrating the top-performing models (e.g., PatchTST) into a real-time optogenetic setup to test if predicted activity states can be successfully triggered or suppressed by targeted stimulation.

## Limitations

- Foundation models showed poor zero-shot performance, suggesting domain transfer remains challenging between non-neural and neural time series
- The paper does not address computational efficiency comparisons, which may be critical for real-time closed-loop applications
- Evaluation focuses on four brain regions from spontaneous activity without considering task-driven neural dynamics or inter-animal variability

## Confidence

- **High Confidence**: The finding that PatchTST and TiDE consistently outperformed classical approaches across multiple horizons is well-supported by the experimental design and multiple evaluation metrics.
- **Medium Confidence**: The claim about channel-independence being superior to cross-channel modeling is based on observed performance but lacks direct ablation studies comparing these architectural choices.
- **Low Confidence**: The interpretation that foundation models' zero-shot failure is primarily due to pretraining data characteristics (hourly vs. millisecond resolution) is plausible but not directly tested through controlled experiments.

## Next Checks

1. **Cross-validation of preprocessing pipeline**: Replicate the hemodynamic correction, SVD compression, and regional averaging steps on held-out data to verify the reported performance isn't dependent on specific preprocessing choices.

2. **Temporal cross-validation**: Evaluate model performance using time-series cross-validation (rolling origin) rather than single train/val/test split to assess robustness across different temporal segments.

3. **Computational cost analysis**: Measure and compare training/inference times and memory usage across all models, particularly for PatchTST and foundation models, to assess practical feasibility for closed-loop applications.