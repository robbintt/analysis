---
ver: rpa2
title: A Computational Framework to Identify Self-Aspects in Text
arxiv_id: '2507.13115'
source_url: https://arxiv.org/abs/2507.13115
tags:
- self
- self-aspects
- language
- will
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a computational framework for detecting Self-aspects
  in text. The authors plan to develop an ontology of Self-aspects and create a gold-standard
  annotated dataset.
---

# A Computational Framework to Identify Self-Aspects in Text

## Quick Facts
- arXiv ID: 2507.13115
- Source URL: https://arxiv.org/abs/2507.13115
- Authors: Jaya Caporusso; Matthew Purver; Senja Pollak
- Reference count: 38
- Key outcome: Proposed framework for detecting Self-aspects with preliminary SVM+LICW achieving 0.83 F1-score on Social Self classification

## Executive Summary
This paper proposes a computational framework for detecting Self-aspects in text through the development of an ontology and gold-standard annotated dataset. The authors plan to evaluate three distinct approaches - conventional discriminative models, generative LLMs, and embedding-based retrieval - against four criteria: interpretability, ground-truth adherence, accuracy, and computational efficiency. Preliminary experiments using SVMs with LIWC features achieved a macro-averaged F1-score of 0.83 for classifying Social Self in diary entries. The framework aims to advance understanding of the Self in language and enable applications in mental health and phenomenology research.

## Method Summary
The framework involves constructing an ontology of Self-aspects through literature review and expert validation, creating a gold-standard annotated dataset, and evaluating three computational approaches. The three approaches include conventional discriminative models (SVMs, Random Forests, Logistic Regression), generative LLMs (fine-tuned models), and embedding-based retrieval methods. Evaluation will assess models against four criteria: interpretability (D1), ground-truth adherence (D2), accuracy (D3), and computational efficiency (D4). The preliminary experiments used SVM classifiers with LIWC features to classify Social Self in diary entries, achieving 0.83 macro-averaged F1-score.

## Key Results
- Preliminary SVM+LICW model achieved 0.83 macro-averaged F1-score for Social Self classification
- Framework establishes four evaluation criteria: interpretability, ground-truth adherence, accuracy, and computational efficiency
- Identifies three computational approaches for Self-aspect detection: discriminative models, generative LLMs, and embedding-based retrieval

## Why This Works (Mechanism)
None provided

## Foundational Learning
- Self-aspect ontology: A formal representation of different dimensions of selfhood (bodily, social, agency, etc.) that enables computational detection of self-related content in text. Why needed: Provides structured vocabulary for identifying specific aspects of self-representation. Quick check: Can the ontology consistently categorize diverse examples of self-related text?
- Ground-truth adherence: The extent to which model predictions can be traced back to specific evidence in annotated training data rather than model pre-training. Why needed: Ensures transparency and reliability of model outputs for sensitive applications. Quick check: Can predictions be validated against specific annotated examples?
- Interpretability in computational models: The ability to understand and explain how models make decisions, particularly important for psychological and phenomenological applications. Why needed: Critical for trust and validation in mental health and research contexts. Quick check: Can domain experts follow the model's reasoning process?

## Architecture Onboarding

Component map: Ontology Construction -> Dataset Annotation -> Model Development -> Evaluation

Critical path: The framework requires sequential progression from ontology construction through dataset annotation to model development, with evaluation occurring iteratively across all stages.

Design tradeoffs: High-performing generative models offer accuracy but lack interpretability (D1) and ground-truth adherence (D2), while interpretable models may sacrifice nuance in handling complex Self-aspects.

Failure signatures: Models may conflate overlapping Self-aspects (e.g., agency applied to bodily self), produce outputs derived from pre-training rather than ground-truth data, or fail to generalize across diverse text sources.

First experiments:
1. Implement and evaluate the proposed Mixture of Experts approach to assess whether it can achieve LLM-level accuracy while satisfying interpretability and ground-truth criteria
2. Conduct inter-annotator agreement studies using the finalized ontology on diverse text samples to validate category distinctions
3. Perform ablation studies comparing feature contributions (LIWC, embeddings, LLM outputs) to classification performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a computational framework be designed to reconcile the trade-off between high classification accuracy and the strict requirements for interpretability and ground-truth adherence?
- Basis in paper: [explicit] Section 10 states that "Reconciling the need for interpretability and ground-truth adherence with high classification performance remains a central challenge in our methodological design."
- Why unresolved: High-performing generative models often function as black boxes, while interpretable models may lack the nuance to handle complex Self-aspects effectively.
- What evidence would resolve it: Empirical results demonstrating a model (e.g., the proposed Mixture of Experts) that achieves accuracy comparable to LLMs while satisfying strict interpretability (D1) and ground-truth (D2) criteria.

### Open Question 2
- Question: How can an ontology of the Self account for aspects that are not mutually exclusive (e.g., agency applying to the bodily self) while remaining consistent across different disciplinary traditions?
- Basis in paper: [explicit] Section 4 notes that "Constructing the Self ontology presents various challenges, most of all regarding how the different components relate with each other," specifically that "most of the aspects... appear to not be mutually exclusive."
- Why unresolved: The phenomenological nature of the Self involves overlapping experiences that are difficult to formalize into discrete, computationally detectable categories.
- What evidence would resolve it: High inter-annotator agreement scores on a diverse dataset using the finalized ontology, indicating that overlapping categories can be reliably distinguished.

### Open Question 3
- Question: To what extent do fine-tuned LLM predictions for Self-aspects derive from the provided ground-truth data versus the models' opaque pre-training corpora?
- Basis in paper: [explicit] Section 7.2 states, "Even when fine-tuned, it remains unclear whether these modelsâ€™ predictions are derived from the data used for fine-tuning or the huge corpora used for pre-training."
- Why unresolved: The internal reasoning of LLMs is difficult to trace, making it hard to verify if outputs are grounded in the specific study data or general statistical patterns.
- What evidence would resolve it: Ablation studies or specific evaluation metrics demonstrating that model outputs are causally linked to the annotated examples rather than hallucinated from pre-training.

## Limitations
- Preliminary results limited to single Self-aspect (Social Self) and simple classifier (SVM+LICW)
- Generalizability to diverse text sources beyond diary entries remains unproven
- Claims about applications in mental health and phenomenology research require empirical validation

## Confidence
- High confidence: The general computational approach and evaluation criteria are sound and well-motivated
- Medium confidence: The initial SVM results on Social Self classification are reliable but limited in scope
- Low confidence: Claims about the framework's applicability to mental health and phenomenology research require empirical validation

## Next Checks
1. Conduct cross-validation studies using diverse text corpora (e.g., social media posts, clinical interviews, narrative therapy sessions) to assess generalizability
2. Perform ablation studies to determine the relative contribution of different feature sets (LIWC, embeddings, LLM outputs) to classification performance
3. Implement a systematic bias audit to evaluate model performance across different demographic groups and cultural contexts