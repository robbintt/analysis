---
ver: rpa2
title: Dutch Metaphor Extraction from Cancer Patients' Interviews and Forum Data using
  LLMs and Human in the Loop
arxiv_id: '2511.06427'
source_url: https://arxiv.org/abs/2511.06427
tags:
- metaphors
- metaphor
- cancer
- data
- patients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents HealthQuote.NL, a Dutch metaphor corpus extracted
  from cancer patient narratives using large language models with human-in-the-loop
  verification. The authors developed refined prompting strategies including chain-of-thought
  reasoning and self-prompting to address challenges like hallucination and figurative
  language confusion.
---

# Dutch Metaphor Extraction from Cancer Patients' Interviews and Forum Data using LLMs and Human in the Loop

## Quick Facts
- arXiv ID: 2511.06427
- Source URL: https://arxiv.org/abs/2511.06427
- Reference count: 0
- Primary result: Developed HealthQuote.NL corpus of 130 Dutch metaphors from cancer narratives using LLMs with human verification

## Executive Summary
This study presents HealthQuote.NL, a Dutch metaphor corpus extracted from cancer patient narratives using large language models with human-in-the-loop verification. The authors developed refined prompting strategies including chain-of-thought reasoning and self-prompting to address challenges like hallucination and figurative language confusion. From 13 patient interview transcripts and 100 forum posts, they extracted 130 metaphors (65 from each source) at word, phrase, and sentence levels, categorized by source domain and communicative function. The metaphors were mapped to an existing English metaphor menu and shown to be relevant for patient care applications including shared decision-making and improved communication.

## Method Summary
The authors extracted Dutch metaphors from 13 transcribed oncology interviews (5,596-13,777 words each) and 100 kanker.nl forum posts using local LLMs via ollama platform. They developed a prompting pipeline progressing from basic instruction prompts to refined prompts with chain-of-thought reasoning and self-prompting. Metaphors were categorized by type (word/phrase/sentence), source domain (violence, journey, nature, etc.), and function (explanation, coping, empowerment). Human verification was used to filter hallucinations, idioms, and over-interpretations, resulting in HealthQuote.NL corpus with confidence scores and metadata.

## Key Results
- Extracted 130 verified metaphors (65 from interviews, 65 from forum posts)
- Chain-of-thought prompting reduced hallucination and improved extraction fidelity
- English Metaphor Menu insertion improved category alignment but reduced output diversity
- Human verification caught 7 documented failure modes including idiom confusion and role hallucination

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-thought reasoning with structured extraction protocols reduces metaphor hallucination and improves extraction fidelity.
- Mechanism: CoT forces intermediate reasoning steps before output, constraining the model to justify each metaphor identification against source text, which reduces abstractive generation.
- Core assumption: Explicit reasoning steps improve verification traceability even in small local LLMs (7-12B parameters).
- Evidence anchors: [section 3] "In the refined prompts (RPs), we added CoT and ISP by asking LLMs to generate prompts for such a task." [section 4.4] "The output metaphors from RPs are more closely related to the original context and have less hallucination."

### Mechanism 2
- Claim: Iterative self-prompting (LLM-assisted prompting) generates more effective task-specific prompts than manual design alone.
- Mechanism: An LLM generates candidate prompts for its own task, which are then refined through iteration, leveraging the model's implicit understanding of its own reasoning patterns.
- Core assumption: LLMs can articulate useful task decompositions when prompted meta-cognitively.
- Evidence anchors: [section 3] "we added CoT and ISP by asking LLMs to generate prompts for such a task." [abstract] "self-prompting" listed among successful strategies for addressing hallucination and figurative language confusion.

### Mechanism 3
- Claim: Knowledge insertion (English Metaphor Menu as in-context examples) guides extraction while introducing category bias.
- Mechanism: Providing the full 17-item English Metaphor Menu as few-shot examples primes the model toward specific metaphor categories (violence, journey, etc.) while reducing exploration of novel categories.
- Core assumption: Cross-linguistic metaphor categories share sufficient structure for Dutch-to-English transfer.
- Evidence anchors: [section 3] "For v2, we inserted the entire English Metaphor Menu list (17 from the public website) with our categorisation." [section 3] "The different settings for RP-v1 and RP-v2 are to investigate 1) if the English Metaphor Menu can guide the LLMs to extract better metaphors; 2) if the English Metaphor Menu will introduce bias."

## Foundational Learning

- Concept: Metaphor Identification Procedure (MIP) and cross-domain mapping
  - Why needed here: The paper references Lakoff's contemporary theory of metaphor and MIP extensions for Dutch; understanding "cross-domain mapping" is prerequisite to interpreting extraction categories.
  - Quick check question: Can you explain why "fighting cancer" maps illness→war while "treatment is a journey" maps process→travel?

- Concept: Human-in-the-loop (HITL) verification in NLP pipelines
  - Why needed here: The entire extraction framework relies on manual post-filtering to separate metaphors from idioms, hallucinations, and figurative language.
  - Quick check question: What specific error types does HITL catch that automated verification cannot?

- Concept: Temperature parameter in LLM sampling
  - Why needed here: Paper uses temperature=0.8 to encourage "alternative metaphor suggestions" while acknowledging increased hallucination risk.
  - Quick check question: If you observed too many hallucinated metaphors, would you increase or decrease temperature?

## Architecture Onboarding

- Component map: Data ingestion -> Chunking (max-token=4000, overlap=40) -> Prompt layer (Instruction Prompt → Refined Prompt v1 → Refined Prompt v2) -> Model ensemble (8 local LLMs via ollama) -> Automated verification (auto.verify) -> Human verification -> Output corpus

- Critical path: Prompt design → Chunking → Model inference → Automated verification → Human filtering → Corpus compilation
  - Bottleneck: Human verification (manual review of all LLM outputs)

- Design tradeoffs:
  - Local vs commercial LLMs: Local prioritizes privacy but limits model capability; GPT-5 used only on anonymized/paraphrased data
  - Temperature=0.8: Increases diversity but raises hallucination rate
  - English Metaphor Menu insertion: Improves category alignment but introduces bias
  - Verification tool: Adds precision but may filter valid novel metaphors

- Failure signatures:
  - Abstractive vs extractive: Model paraphrases rather than extracts exact text (Section 4.3, Point 1)
  - Idiom confusion: "baal als een stekker" extracted as metaphor (Section 4.5)
  - Role hallucination: "doctor's assistant" → "doctor" (Section 4.3, Point 7)
  - Over-interpretation: Generating metaphors from minimal context (Section 4.3, Point 5)

- First 3 experiments:
  1. Baseline extraction: Run Instruction Prompt (no CoT, no examples) on 3 interview docs, manually classify error types using the 7 failure categories.
  2. Ablation on prompt complexity: Compare RP-v1 vs RP-v2 on same 100 forum posts, measuring metaphor count, source domain distribution, and hallucination rate via human review.
  3. Model ensemble variance: Run all 8 local LLMs on 1 interview transcript with RP-v2, measure overlap coefficient and unique metaphors per model to validate "collective of LLMs" strategy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the prompting strategies developed on 100 forum posts generalize to the full dataset of 15,653 blogs, and does the distribution of metaphor source domains remain consistent at scale?
- Basis in paper: [explicit] The authors state "For the current study, we only used 100 posts of the forum data we have. For future work, we will explore more data from the blog post for extended experiments."
- Why unresolved: Only 65 metaphors from 100 posts were extracted; the full dataset may reveal different metaphor patterns, domain distributions, or LLM failure modes not observed in the pilot sample.
- What evidence would resolve it: Running the same RP-v1/RP-v2 prompting pipeline on the complete forum corpus and comparing metaphor type distributions, extraction quality metrics, and inter-model agreement rates.

### Open Question 2
- Question: Does inserting the English Metaphor Menu as in-context knowledge systematically bias LLMs toward Anglophone metaphorical conceptualizations at the expense of Dutch-specific metaphorical patterns?
- Basis in paper: [explicit] The authors designed RP-v1 vs RP-v2 specifically "to investigate 1) if the English Metaphor Menu can guide the LLMs to extract better metaphors; 2) if the English Metaphor Menu will introduce bias to LLMs to extract only metaphors like the English metaphors."
- Why unresolved: The authors reported outputs became "less diverse or creative" with knowledge insertion but did not systematically measure bias toward English categories versus novel Dutch metaphors.
- What evidence would resolve it: A controlled comparison quantifying metaphors mapped to English Menu categories versus culturally-specific Dutch metaphors emerging under RP-v1 (without menu) versus RP-v2 (with menu).

### Open Question 3
- Question: How can multi-word expressions (MWEs), idioms, conventionalized "dead" metaphors, and novel metaphors be systematically disambiguated in Dutch healthcare discourse?
- Basis in paper: [explicit] The authors identify idiom/metaphor confusion as a key challenge (Points 4, 6) and state "For future work... We will look into some of the MWEs identified by LLMs, since they always overlap with idioms and metaphors."
- Why unresolved: The paper documents LLMs extracting idioms like "te horen krijgen" and "ik baal als een stekker" as metaphors, but provides no systematic method for distinguishing them from conceptually novel metaphors relevant to patient care.
- What evidence would resolve it: Developing annotation guidelines with explicit criteria for Dutch MWE/idiom boundaries, validated through inter-annotator agreement on a held-out corpus subset.

### Open Question 4
- Question: What quantitative evaluation metrics can reliably assess Dutch metaphor extraction quality beyond human verification?
- Basis in paper: [inferred] The methodology relies entirely on human-in-the-loop verification with no reported quantitative metrics (precision, recall, F1) or inter-annotator agreement scores; different LLMs extracted different metaphors with no systematic comparison framework.
- Why unresolved: Without standardized metrics, it remains unclear whether improvements from refined prompts reflect genuine extraction quality gains or simply reduced obvious errors, and whether results are reproducible across annotators.
- What evidence would resolve it: Developing a gold-standard annotated test set with multiple annotators, calculating inter-annotator agreement (Cohen's Kappa), and reporting extraction precision/recall against this benchmark.

## Limitations
- Human-in-the-loop verification introduces subjectivity and doesn't scale to larger datasets
- Cross-linguistic validity of mapping Dutch metaphors to English Metaphor Menu untested
- Reported "less diverse" outputs suggest trade-off between precision and recall not systematically evaluated

## Confidence
- **High confidence**: Chain-of-thought reasoning mechanism (qualitative verification support), technical implementation details (well-specified and reproducible)
- **Medium confidence**: Self-prompting effectiveness (lacks quantitative comparison), cross-linguistic metaphor category mapping (no validation of equivalence)
- **Low confidence**: Generalizability to patient care applications (empirical connection not tested), scalability of human verification pipeline

## Next Checks
1. **Inter-rater reliability test**: Have three independent annotators verify the same 20 extracted metaphors using the paper's 7 failure criteria. Calculate Cohen's kappa to establish annotation consistency and identify ambiguous cases.

2. **Prompt ablation study**: Run RP-v1 and RP-v2 on identical 50 forum posts, measuring metaphor count, category distribution, and hallucination rate. Compare against manual prompt variants to quantify self-prompting benefits.

3. **Generalizability validation**: Apply the refined prompting pipeline to a different medical domain (e.g., diabetes or heart disease narratives) using the same local LLMs. Measure extraction yield, error types, and category coverage to test domain transferability.