---
ver: rpa2
title: On the Fundamental Limits of LLMs at Scale
arxiv_id: '2511.12869'
source_url: https://arxiv.org/abs/2511.12869
tags:
- arxiv
- language
- reasoning
- preprint
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper establishes that large language model failures stem
  from five fundamental theoretical limits: hallucination, context compression, reasoning
  degradation, retrieval fragility, and multimodal misalignment. Through rigorous
  mathematical proofs, the authors demonstrate that these failures are inevitable
  consequences of computability theory, information theory, and statistical learning
  rather than engineering artifacts.'
---

# On the Fundamental Limits of LLMs at Scale

## Quick Facts
- **arXiv ID**: 2511.12869
- **Source URL**: https://arxiv.org/abs/2511.12869
- **Reference count**: 40
- **Primary result**: Five fundamental theoretical limits (hallucination, context compression, reasoning degradation, retrieval fragility, multimodal misalignment) constrain LLM scaling, proven via computability theory and information theory

## Executive Summary
This paper establishes that large language model failures stem from five fundamental theoretical limits: hallucination, context compression, reasoning degradation, retrieval fragility, and multimodal misalignment. Through rigorous mathematical proofs, the authors demonstrate that these failures are inevitable consequences of computability theory, information theory, and statistical learning rather than engineering artifacts. The analysis reveals that hallucination arises from diagonalization and uncomputability arguments, context compression from positional undertraining and softmax crowding, reasoning degradation from likelihood training's preference for correlation over entailment, retrieval fragility from token budget constraints and adversarial contamination, and multimodal misalignment from linguistic dominance and representation bottlenecks. These findings reframe scaling as bounded by intrinsic theoretical ceilings rather than solvable engineering problems, providing a unified framework that connects disparate failure modes to their mathematical foundations.

## Method Summary
The paper establishes theoretical limits on LLM scaling through formal mathematical proofs and synthesis of empirical evidence from existing literature. The methodology involves constructing diagonalization arguments to prove inevitable hallucination, analyzing VC-dimension bounds for context compression, and applying causal mediation analysis to reasoning degradation. The approach combines computability theory, information theory, and statistical learning to prove that five failure modes are fundamental rather than engineering artifacts. Empirical validation comes from synthesizing results across 40 referenced works rather than conducting novel experiments, with theoretical claims supported by aggregation of existing empirical failure laws and scaling studies.

## Key Results
- Hallucination is mathematically inevitable for computably enumerable model classes due to diagonalization and uncomputability arguments
- Effective context length is strictly smaller than nominal window due to positional undertraining and softmax crowding (requiring Θ(ln N) logit margin)
- Likelihood-based training optimizes for correlation over entailment, causing reasoning degradation through disposable mediator steps
- Retrieval fragility emerges from token budget constraints and adversarial contamination in RAG systems
- Multimodal misalignment results from linguistic dominance and representation bottlenecks in cross-modal learning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Hallucination is mathematically inevitable for any computably enumerable model class, rather than a solvable training artifact.
- **Mechanism**: Diagonalization arguments (Theorems 1–3) demonstrate that for any set of models, an adversarial ground-truth function can be constructed such that every model fails on at least one input. Furthermore, undecidable problems (e.g., the Halting Problem) force infinite failure sets.
- **Core assumption**: The set of all possible ground-truth functions is uncountable or strictly larger than the countable set of computable models.
- **Evidence anchors**:
  - [abstract] Establishes hallucination as arising from "diagonalization and uncomputability arguments."
  - [Section 2.1] Formalizes Theorem 1 (Inevitability for enumerable LLMs) and Theorem 3 (Undecidable problems force hallucination).
  - [corpus] Supports the view that compression behaviors in LLMs are linked to scaling limits and information theory.
- **Break condition**: If the model class is not enumerable or if the task domain is restricted to strictly decidable, bounded-complexity functions, the strict diagonalization guarantee may not apply, though statistical limits (Lemma 1) persist.

### Mechanism 2
- **Claim**: The "effective" context length is strictly smaller than the nominal window due to positional undertraining and attention saturation.
- **Mechanism**: Left-skewed training distributions (Lemma 2) yield vanishing gradients for distant tokens (positional undertraining). Simultaneously, softmax crowding (Lemma 4) requires a log(N) score margin to distinguish a relevant token among N distractors, which standard training fails to provide.
- **Core assumption**: Training data exhibits a heavy bias toward short-range dependencies, and attention scores for distractors follow a distribution allowing them to drown out relevant signals.
- **Evidence anchors**:
  - [abstract] Attributes context compression to "positional undertraining and softmax crowding."
  - [Section 3.4] Lemma 4 proves that without a growing score margin, attention on the relevant token vanishes as sequence length N increases.
  - [corpus] *Glyph: Scaling Context Windows via Visual-Text Compression* and *How Much Information Can a Vision Token Hold?* support the idea that token capacity and context length are bottlenecks requiring architectural mitigation.
- **Break condition**: If specific curricula (e.g., "positional curricula" mentioned in the abstract) or sparse attention mechanisms are successfully implemented to up-weight long-range dependencies, the effective context may approach the nominal limit.

### Mechanism 3
- **Claim**: Reasoning degradation occurs because likelihood-based training optimizes for correlation (pattern completion) rather than entailment (logical validity).
- **Mechanism**: The standard loss function (Eq. 26) marginalizes over reasoning paths, meaning a "fluent but non-causal" Chain-of-Thought (CoT) can persist if it maintains high output likelihood (Eq. 28). Consequently, CoT steps often act as "disposable mediators" (Eq. 29) with low causal influence on the final answer.
- **Core assumption**: Models exploit statistical shortcuts in the training data rather than learning rigid logical rules; there is no explicit verification signal for intermediate steps.
- **Evidence anchors**:
  - [abstract] Notes likelihood training's preference for correlation over entailment.
  - [Section 4.2] Defines the "Disposable Mediator" problem where the Indirect Effect (IE) of reasoning steps on the outcome is near zero.
  - [corpus] *Understanding LLM Behaviors via Compression* supports the general view that LLM behaviors are emergent properties of compression and data scaling.
- **Break condition**: If process-supervision or verifiable rewards (mentioned in Section 4.3) are applied to enforce validity at every intermediate step, the correlation shortcut is broken.

## Foundational Learning

- **Concept**: **Kolmogorov Complexity**
  - **Why needed here**: To understand Lemma 1, which explains why hallucination is inevitable even for decidable problems if the complexity of the function exceeds the descriptive capacity of the model.
  - **Quick check question**: How does the ratio of function complexity to model capacity affect generalization error?

- **Concept**: **Softmax Saturation & Rank Bottlenecks**
  - **Why needed here**: To grasp why increasing context window size (N) creates a "race to the bottom" where signal is lost in noise without specific architectural interventions (Lemma 4).
  - **Quick check question**: Why does a fixed score margin fail to distinguish relevant tokens as context length scales linearly?

- **Concept**: **Causal Mediation Analysis**
  - **Why needed here**: To diagnose "Reasoning Degradation" by quantifying whether intermediate tokens (Z) actually cause the output (Y) or are merely correlated spurious artifacts.
  - **Quick check question**: What is the difference between the "Direct Effect" and "Indirect Effect" in the context of a Chain-of-Thought?

## Architecture Onboarding

- **Component map**: Input: Prompts + Retrieved Passages (RAG) -> Encoder: Vision/Text tokenizers (Vulnerable to "Granularity Mismatch") -> Core: Transformer Backbone (Vulnerable to "Softmax Crowding" and "Positional Undertraining") -> Objective: Likelihood Maximization (Drives "Reasoning Degradation")

- **Critical path**: The interaction between the **Retrieval Module** and the **Context Window**. If the retriever injects noise or the context window saturates (Section 3), the model must hallucinate or lose information before generation begins.

- **Design tradeoffs**:
  - **Coverage vs. Relevance (Section 5.1.1)**: High recall retrieval increases coverage but lowers signal-to-noise ratio due to fixed token budgets.
  - **Creativity vs. Factuality (Section 2.4)**: High temperature sampling improves novelty but increases hallucination risk.

- **Failure signatures**:
  - **"Lost in the Middle":** Performance drops when critical information is placed in the center of a long context (Section 3).
  - **Spurious Correlations:** The model answers correctly using a flawed or hallucinated reasoning chain (Section 4.2).

- **First 3 experiments**:
  1. **Positional Stress Test**: Measure accuracy on retrieval tasks when the "needle" is moved from position 0% to 100% of the context window to verify effective context limits (Section 3.4).
  2. **Mediation Analysis**: Perturb intermediate reasoning steps (Z) in a CoT and measure the change in final answer (Y) to quantify "disposability" (Section 4.2).
  3. **Adversarial Hallucination Trigger**: Construct adversarial inputs via diagonalization (Theorem 1) to test if the model fails on specific low-complexity but unseen queries.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can irreducible hallucination rates be translated from abstract existence proofs into measurable lower bounds for specific query distributions?
- **Basis in paper**: [explicit] Section 8 identifies the need to "Move from existence proofs to measurable lower bounds on irreducible failure rates."
- **Why unresolved**: Theorems 1–3 prove the *existence* of hallucinations but do not quantify the probability density of failure sets under natural data distributions.
- **What evidence would resolve it**: A rigorous framework linking theoretical undecidability to empirical failure frequencies in specific task domains.

### Open Question 2
- **Question**: Can Retrieval-Augmented Generation (RAG) be formalized as a constrained optimization problem with provable approximation guarantees?
- **Basis in paper**: [explicit] Section 8 calls for formalizing RAG to derive "approximation guarantees for multi-hop coverage and robustness."
- **Why unresolved**: Current RAG systems rely on heuristics like top-k selection, which lack theoretical guarantees for maintaining relevance and coverage under strict token budgets.
- **What evidence would resolve it**: Algorithms that mathematically bound the trade-off between retrieval breadth (token budget) and evidence coverage.

### Open Question 3
- **Question**: Does the multimodal interaction term in scaling laws ($\Delta_{interaction}$) grow or stabilize as parameters and datasets increase?
- **Basis in paper**: [inferred] Section 6.4.1 introduces a "fractured" scaling law with an interaction term but leaves its asymptotic behavior undefined.
- **Why unresolved**: Modality competition suggests that scaling one modality might degrade another via the interaction term, potentially bounding overall performance.
- **What evidence would resolve it**: Empirical scaling curves showing whether multimodal performance is strictly bounded by the slowest-scaling modality.

## Limitations

- Proof completeness issues: Some proofs rely on idealized assumptions (e.g., adversarial construction) that may not reflect real-world data distributions
- Empirical validation gaps: Claims about effective context length and reasoning degradation lack direct experimental measurements from controlled experiments
- Mitigation implementation ambiguity: Section 5 outlines potential mitigations without concrete algorithmic specifications or implementation details

## Confidence

**High confidence**: The mathematical foundations of diagonalization arguments (Theorems 1-3) and basic information-theoretic constraints (Kolmogorov complexity arguments in Lemma 1) are well-established in computability theory. The softmax crowding analysis (Lemma 4) follows directly from softmax properties and attention mechanics. The retrieval fragility analysis based on token budget constraints is mathematically sound.

**Medium confidence**: The connection between likelihood training and reasoning degradation (Section 4.2) is theoretically plausible but relies on specific assumptions about training data distributions and the prevalence of spurious correlations. The positional undertraining argument (Lemma 2) is mechanistically sound but depends on unverified assumptions about training corpus statistics.

**Low confidence**: The multimodal misalignment analysis (Section 6) relies heavily on anecdotal evidence and lacks rigorous mathematical formalization. Claims about linguistic dominance and representation bottlenecks are qualitative rather than quantitative.

## Next Checks

1. **Direct empirical validation of effective context limits**: Implement the positional stress test described in Section 3.4 using a standard model (e.g., Llama-2-7B). Measure retrieval accuracy as a function of token position across different context lengths (8K, 32K, 128K). Plot accuracy vs. position to empirically verify the sharp degradation pattern predicted by positional undertraining theory.

2. **Causal mediation analysis for reasoning steps**: Following Section 4.2's methodology, select a dataset with verifiable Chain-of-Thought reasoning (e.g., GSM8K). For each reasoning step, systematically perturb the intermediate tokens while keeping other inputs fixed, then measure the causal impact on final answer accuracy using the indirect effect metric. Quantify the proportion of "disposable" reasoning steps where IE approaches zero.

3. **Adversarial input generation via diagonalization**: Implement Theorem 1's construction to generate adversarial inputs for a specific model (e.g., GPT-2-small). Create a dataset of these inputs and measure actual failure rates. Compare against baseline performance on non-adversarial inputs to empirically validate the inevitability claim for real models rather than abstract function classes.