---
ver: rpa2
title: 'Multilingual Text-to-SQL: Benchmarking the Limits of Language Models with
  Collaborative Language Agents'
arxiv_id: '2509.24405'
source_url: https://arxiv.org/abs/2509.24405
tags:
- language
- multispider
- text-to-sql
- multilingual
- schema
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MultiSpider 2.0, a multilingual Text-to-SQL
  benchmark extending Spider 2.0 to eight languages (English, German, French, Spanish,
  Portuguese, Japanese, Chinese, Vietnamese). It features enterprise-scale, cross-domain
  schemas and compositional SQL complexity, adding linguistic and dialectal variation
  for realistic multilingual evaluation.
---

# Multilingual Text-to-SQL: Benchmarking the Limits of Language Models with Collaborative Language Agents

## Quick Facts
- **arXiv ID:** 2509.24405
- **Source URL:** https://arxiv.org/abs/2509.24405
- **Reference count:** 40
- **Primary result:** Introduces MultiSpider 2.0, a multilingual Text-to-SQL benchmark, and demonstrates a 4% to 15% accuracy improvement using the COLA framework.

## Executive Summary
This paper introduces MultiSpider 2.0, a multilingual Text-to-SQL benchmark extending Spider 2.0 to eight languages (English, German, French, Spanish, Portuguese, Japanese, Chinese, Vietnamese). It features enterprise-scale, cross-domain schemas and compositional SQL complexity, adding linguistic and dialectal variation for realistic multilingual evaluation. The benchmark reveals a significant multilingual gap: state-of-the-art LLMs like DeepSeek-R1 and OpenAI o1 achieve only 4% execution accuracy on MultiSpider 2.0 versus 60% on MultiSpider 1.0. To address this, the authors propose Collaborative Language Agents (COLA), a modular framework that iteratively refines SQL queries through specialized agents (Classifier, Analyzer, Corrector), improving accuracy to 15% without task-specific fine-tuning. COLA demonstrates consistent gains across languages and models, highlighting the need for robust, multilingual Text-to-SQL methods in real-world enterprise environments.

## Method Summary
The paper introduces MultiSpider 2.0, a multilingual Text-to-SQL benchmark with 5,056 examples across 200 databases in eight languages. It evaluates state-of-the-art LLMs and proposes the Collaborative Language Agents (COLA) framework to address the multilingual performance gap. COLA uses three agents: (1) Classifier for schema pruning, (2) Analyzer for query decomposition via chain-of-thought reasoning, and (3) Corrector for execution-guided iterative refinement. The framework is tested with backbone models like OpenAI o1 and DeepSeek-R1, demonstrating improved accuracy without task-specific fine-tuning.

## Key Results
- MultiSpider 2.0 reveals a significant multilingual gap: state-of-the-art LLMs achieve only 4% execution accuracy versus 60% on MultiSpider 1.0.
- COLA framework improves accuracy to 15% without task-specific fine-tuning through iterative refinement.
- Schema linking errors are the dominant failure mode (33%), particularly in multilingual contexts.

## Why This Works (Mechanism)

### Mechanism 1: Schema Noise Reduction via Context Partitioning
Reducing the schema search space via a dedicated classification agent improves generation accuracy by minimizing irrelevant context in large enterprise databases. The Classifier agent filters the full database schema into a smaller, question-relevant sub-database before SQL generation begins. By presenting the generator with only relevant tables/columns, the model is less likely to hallucinate incorrect joins or select wrong columns. The core assumption is that the Classifier agent can accurately map natural language entities to schema elements with higher precision than the generator itself. Evidence anchors include the abstract mentioning "specialized agents (Classifier, Analyzer, Corrector)" and section 3.3 describing the Classifier's role. A break condition occurs if the Classifier prunes a table required for a complex multi-hop join, leading to an irrecoverable failure state.

### Mechanism 2: Iterative Error Correction via Execution Feedback
Grounding the generation process in database execution feedback allows for the self-correction of syntactic and simple semantic errors that single-pass models miss. The Corrector agent executes the generated SQL candidate. If an error is returned, the error message and query are fed back into the LLM context, allowing the model to debug logic based on actual runtime constraints. The core assumption is that the underlying LLM possesses sufficient reasoning capability to interpret database error messages and map them back to the original natural language intent. Evidence anchors include the abstract noting "iteratively refines SQL queries" and section 4.3 identifying the Corrector as "the most impactful component." A break condition occurs if the error is structural, causing the iteration to oscillate between syntactically different but semantically identical wrong queries.

### Mechanism 3: Chain-of-Thought Query Decomposition
Decomposing complex queries into structured sub-problems improves handling of compositional SQL complexity compared to direct generation. The Analyzer agent breaks down the natural language question into intermediate reasoning steps (Chain-of-Thought) rather than predicting the SQL token sequence directly. This forces the model to explicitly plan the query structure before synthesizing the final code. The core assumption is that the natural language question contains sufficient explicit cues to be decomposed linearly. Evidence anchors include the abstract's mention of "demanding deeper reasoning for complex SQL" and section 3.3 describing the Analyzer's role. A break condition occurs if the question contains linguistic ambiguity, causing the decomposition step to propagate a misunderstanding into the sub-questions.

## Foundational Learning

- **Concept: Schema Linking**
  - **Why needed here:** The paper identifies "Wrong schema linking" as the #1 error source (33% of failures). You must understand how to map a phrase like "visitors in Feb" to specific columns in a complex database schema.
  - **Quick check question:** Given a user question "Show average sales for Tokyo," how do you determine if "Tokyo" refers to a `Cities` table, a `Region` column, or a string literal in a `Sales` table?

- **Concept: Execution Accuracy vs. Exact Match**
  - **Why needed here:** The paper highlights a massive gap between these metrics (e.g., 95% EX vs 74% EM on Spider 1.0). You need to distinguish between "the SQL code is textually identical" (EM) and "the SQL code produces the correct data table" (EX).
  - **Quick check question:** If a query uses `ORDER BY` but the gold standard doesn't, would Execution Accuracy (EX) typically penalize this? (Hint: Check if the result set content changes).

- **Concept: Cross-Lingual Transfer / Code-Switching**
  - **Why needed here:** The paper evaluates 8 languages and notes a 6.1% drop for non-English languages. The difficulty involves handling non-English questions while database schemas and values often remain English or require translation.
  - **Quick check question:** If a database has English column names (e.g., `product_id`) but the user asks in Chinese, what specific alignment challenge does the model face compared to an English-only query?

## Architecture Onboarding

- **Component map:** Input (NL Question + Database Schema + Auxiliary Docs) -> Classifier (prunes schema -> Reduced Schema) -> Analyzer (decomposes question + generates draft SQL using CoT) -> Corrector (executes draft -> catches errors -> refines SQL iteratively) -> Output (Final SQL Query)

- **Critical path:** The Classifier's output is the bottleneck. If the "Reduced Schema" excludes a necessary table, the Analyzer and Corrector cannot succeed regardless of their reasoning capability. Verification of schema completeness is a priority.

- **Design tradeoffs:**
  - **Latency vs. Accuracy:** The COLA framework significantly increases inference time compared to single-pass prompting, but boosts accuracy from ~4% to ~15%.
  - **EM vs. EX Optimization:** The system optimizes for Execution Accuracy (EX), often at the expense of Exact Match (EM), tolerating syntactic variations if the result table is correct.

- **Failure signatures:**
  - **Wrong Schema Linking (33%):** Mapping NL to the wrong column/table.
  - **Erroneous Analysis (20%):** Misinterpreting the query intent (e.g., aggregation logic).
  - **Code-Switching Failures:** Specific to non-English languages (Zh, Ja, Vi); models fail to align localized questions with canonical schema IDs.

- **First 3 experiments:**
  1. **Ablation on Classifier:** Run the Analyzer + Corrector on the full schema (no pruning) vs. the Classifier-reduced schema to measure the noise reduction impact.
  2. **Corrector Iteration Cap:** Vary the maximum number of self-correction iterations (e.g., 0, 1, 3, 5) to find the point of diminishing returns on Execution Accuracy.
  3. **Cross-lingual consistency:** Compare performance when translating the schema/content to the target language (localized snapshot) vs. keeping the schema in English while querying in a target language (code-switching stress test).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can stepwise schema-grounded planning be implemented to effectively bind entities and select plausible join paths in enterprise-scale databases with dozens of tables?
- **Basis in paper:** The conclusion explicitly lists "Schema-grounded planning" as a priority, noting that current models suffer from "join-path overreach" and need to "propagate constraints through joins, aggregation, grouping, and nesting."
- **Why unresolved:** Current LLMs struggle to reason over large, heterogeneous schemas without getting lost in irrelevant relations, contributing to the "Wrong schema linking" (33%) error rate observed in the analysis.
- **What evidence would resolve it:** A method that explicitly maps natural language entities to schema elements early in the generation process and restricts the search space to valid foreign-key paths, demonstrating improved accuracy on the "Hard" (>160 tokens) subset of MultiSpider 2.0.

### Open Question 2
- **Question:** What specific normalization techniques are required to handle code-switching and transliteration in non-English queries, particularly for high-drop languages like Chinese, Japanese, and Vietnamese?
- **Basis in paper:** The authors identify "Dialect-aware normalization" as a key future direction, specifically citing the need to support "code-switching, transliteration, and regional synonyms (e.g., enâ€“zh variants)" which caused significant performance degradation in East Asian languages.
- **Why unresolved:** Standard tokenization and pre-training often fail when technical English terms (schema names) are mixed with non-English natural language queries, leading to lexical ambiguity.
- **What evidence would resolve it:** Integrating lightweight lexicons or learned normalizers into the parsing loop that successfully align mixed-language tokens to database identifiers, reducing the performance gap between European and Asian languages.

### Open Question 3
- **Question:** Can execution-grounded learning (e.g., RLHF or verifier-guided search) be integrated into agent frameworks to improve accuracy without incurring the prohibitive latency of extended test-time computation?
- **Basis in paper:** The paper highlights "Execution-grounded learning" as a future priority, suggesting "verifier-guided search and RLHF" while simultaneously warning of the "higher computational costs" associated with extended reasoning in models like OpenAI o1.
- **Why unresolved:** While COLA improves accuracy through iterative correction, it relies on multiple LLM calls (Classifier, Analyzer, Corrector). Balancing the depth of verification with the cost of additional inference steps remains an open optimization problem.
- **What evidence would resolve it:** A training pipeline that uses execution results as reward signals to fine-tune a single model or a lighter agent system, achieving comparable accuracy to COLA (approx. 15%) with significantly reduced inference time and token usage.

## Limitations

- The paper demonstrates significant multilingual performance degradation (4% EX vs. 60% EX on MultiSpider 1.0), but the causes are not fully dissected.
- The effectiveness of COLA's iterative correction is impressive (improving to 15% EX), but the scalability and convergence properties of the correction loop are not characterized.
- The generalizability of the 4% to 15% improvement range to other multilingual Text-to-SQL datasets or larger enterprise schemas is uncertain.

## Confidence

- **High confidence:** The empirical observation of multilingual performance gaps and the general effectiveness of the COLA framework in improving accuracy over baseline single-pass methods.
- **Medium confidence:** The specific attribution of 33% of failures to "Wrong Schema Linking" across all languages, as this may conflate language-specific and schema-specific issues.
- **Low confidence:** The generalizability of the 4% to 15% improvement range to other multilingual Text-to-SQL datasets or to scenarios with even larger enterprise schemas than those in MultiSpider 2.0.

## Next Checks

1. **Schema Linking Error Analysis:** Perform a fine-grained error classification to determine if schema linking failures are driven by linguistic ambiguity (e.g., polysemy in non-English queries) or by model limitations in cross-lingual entity recognition, stratified by language pair.
2. **Corrector Convergence Study:** Systematically vary the maximum number of self-correction iterations (0, 1, 3, 5, 10) and plot the EX accuracy and iteration count per example to identify the point of diminishing returns and detect potential oscillation patterns.
3. **Schema Localization Impact:** Evaluate a variant of the pipeline where the database schema and values are translated into the target language (e.g., German schema for German queries) versus the current code-switching setup, to isolate the impact of schema localization on schema linking accuracy.